- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1810.05587] A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1810.05587] 多智能体深度强化学习的调查与批评¹¹ 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1810.05587](https://ar5iv.labs.arxiv.org/html/1810.05587)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1810.05587](https://ar5iv.labs.arxiv.org/html/1810.05587)
- en: 'A Survey and Critique of Multiagent Deep Reinforcement Learning¹¹1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多智能体深度强化学习的调查与批评¹¹ 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”
- en: Pablo Hernandez-Leal, Bilal Kartal and Matthew E. Taylor
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pablo Hernandez-Leal, Bilal Kartal 和 Matthew E. Taylor
- en: '{pablo.hernandez,bilal.kartal,matthew.taylor}@borealisai.com Borealis AI'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{pablo.hernandez,bilal.kartal,matthew.taylor}@borealisai.com Borealis AI'
- en: Edmonton, Canada
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大，埃德蒙顿
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep reinforcement learning (RL) has achieved outstanding results in recent
    years. This has led to a dramatic increase in the number of applications and methods.
    Recent works have explored learning beyond single-agent scenarios and have considered
    multiagent learning (MAL) scenarios. Initial results report successes in complex
    multiagent domains, although there are several challenges to be addressed. The
    primary goal of this article is to provide a clear overview of current multiagent
    deep reinforcement learning (MDRL) literature. Additionally, we complement the
    overview with a broader analysis: (i) we revisit previous key components, originally
    presented in MAL and RL, and highlight how they have been adapted to multiagent
    deep reinforcement learning settings. (ii) We provide general guidelines to new
    practitioners in the area: describing lessons learned from MDRL works, pointing
    to recent benchmarks, and outlining open avenues of research. (iii) We take a
    more critical tone raising practical challenges of MDRL (e.g., implementation
    and computational demands). We expect this article will help unify and motivate
    future research to take advantage of the abundant literature that exists (e.g.,
    RL and MAL) in a joint effort to promote fruitful research in the multiagent community.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（RL）近年来取得了卓越的成果。这导致了应用和方法数量的显著增加。近期工作探索了超越单一智能体场景的学习，并考虑了多智能体学习（MAL）场景。初步结果报告了在复杂的多智能体领域中的成功，尽管存在若干挑战需要解决。本文的主要目标是提供当前多智能体深度强化学习（MDRL）文献的清晰概述。此外，我们通过更广泛的分析来补充这一概述：（i）我们重新审视了最初在MAL和RL中提出的关键组件，并强调了它们如何被适应到多智能体深度强化学习设置中。（ii）我们向新从业者提供一般性指南：描述从MDRL工作中获得的经验教训，指向最近的基准，并概述开放的研究途径。（iii）我们采取更批判的语气，提出MDRL的实际挑战（例如，实施和计算需求）。我们希望本文能帮助统一和激励未来的研究，利用现有的丰富文献（例如，RL和MAL），共同推动多智能体社区的有益研究。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Almost 20 years ago Stone and Veloso’s seminal survey [[1](#bib.bib1)] laid
    the groundwork for defining the area of multiagent systems (MAS) and its open
    problems in the context of AI. About ten years ago, Shoham, Powers, and Grenager [[2](#bib.bib2)]
    noted that the literature on multiagent learning (MAL) was growing and it was
    not possible to enumerate all relevant articles. Since then, the number of published
    MAL works continues to steadily rise, which led to different surveys on the area,
    ranging from analyzing the basics of MAL and their challenges [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], to addressing specific subareas: game theory
    and MAL [[2](#bib.bib2), [6](#bib.bib6)], cooperative scenarios [[7](#bib.bib7),
    [8](#bib.bib8)], and evolutionary dynamics of MAL [[9](#bib.bib9)]. In just the
    last couple of years, three surveys related to MAL have been published: learning
    in non-stationary environments [[10](#bib.bib10)], agents modeling agents [[11](#bib.bib11)],
    and transfer learning in multiagent RL [[12](#bib.bib12)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎20年前，Stone和Veloso的开创性调查[[1](#bib.bib1)]为定义多智能体系统（MAS）领域及其在AI背景下的开放问题奠定了基础。大约十年前，Shoham、Powers和Grenager[[2](#bib.bib2)]指出，多智能体学习（MAL）文献在增长，无法列举所有相关文章。从那时起，已发表的MAL作品数量持续上升，这导致了该领域的不同调查，从分析MAL的基础及其挑战[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]，到解决特定子领域：博弈论和MAL[[2](#bib.bib2), [6](#bib.bib6)]，合作场景[[7](#bib.bib7),
    [8](#bib.bib8)]，以及MAL的进化动态[[9](#bib.bib9)]。仅在过去几年内，已发布了三项与MAL相关的调查：非平稳环境中的学习[[10](#bib.bib10)]，智能体对智能体建模[[11](#bib.bib11)]，以及多智能体RL中的迁移学习[[12](#bib.bib12)]。
- en: The research interest in MAL has been accompanied by successes in artificial
    intelligence, first, in single-agent video games [[13](#bib.bib13)]; more recently,
    in two-player games, for example, playing Go [[14](#bib.bib14), [15](#bib.bib15)],
    poker [[16](#bib.bib16), [17](#bib.bib17)], and games of two competing teams,
    e.g., DOTA 2 [[18](#bib.bib18)] and StarCraft II [[19](#bib.bib19)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对MAL的研究兴趣与人工智能的成功相伴而生，首先是在单智能体视频游戏中[[13](#bib.bib13)]；更近期的成功则是在双人游戏中，例如围棋[[14](#bib.bib14),
    [15](#bib.bib15)]，扑克[[16](#bib.bib16), [17](#bib.bib17)]，以及两个竞争团队的游戏，例如DOTA 2[[18](#bib.bib18)]和StarCraft
    II[[19](#bib.bib19)]。
- en: 'While different techniques and algorithms were used in the above scenarios,
    in general, they are all a combination of techniques from two main areas: reinforcement
    learning (RL) [[20](#bib.bib20)] and deep learning [[21](#bib.bib21), [22](#bib.bib22)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在上述场景中使用了不同的技术和算法，但一般而言，它们都是来自两个主要领域的技术组合：强化学习（RL）[[20](#bib.bib20)]和深度学习[[21](#bib.bib21),
    [22](#bib.bib22)]。
- en: RL is an area of machine learning where an agent learns by interacting (i.e.,
    taking actions) within a dynamic environment. However, one of the main challenges
    to RL, and traditional machine learning in general, is the need for manually designing
    quality features on which to learn. Deep learning enables efficient representation
    learning, thus allowing the automatic discovery of features [[21](#bib.bib21),
    [22](#bib.bib22)]. In recent years, deep learning has had successes in different
    areas such as computer vision and natural language processing [[21](#bib.bib21),
    [22](#bib.bib22)]. One of the key aspects of deep learning is the use of *neural
    networks* (NNs) that can find compact representations in high-dimensional data [[23](#bib.bib23)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个领域，其中一个智能体通过在动态环境中交互（即采取行动）来学习。然而，RL以及传统机器学习的主要挑战之一是需要手动设计质量特征来进行学习。深度学习实现了高效的表示学习，从而允许自动发现特征[[21](#bib.bib21),
    [22](#bib.bib22)]。近年来，深度学习在计算机视觉和自然语言处理等不同领域取得了成功[[21](#bib.bib21), [22](#bib.bib22)]。深度学习的一个关键方面是使用*神经网络*（NNs），它们可以在高维数据中找到紧凑的表示[[23](#bib.bib23)]。
- en: In deep reinforcement learning (DRL) [[23](#bib.bib23), [24](#bib.bib24)] deep
    neural networks are trained to approximate the optimal policy and/or the value
    function. In this way the deep NN, serving as function approximator, enables powerful
    generalization. One of the key advantages of DRL is that it enables RL to scale
    to problems with high-dimensional state and action spaces. However, most existing
    successful DRL applications so far have been on visual domains (e.g., Atari games),
    and there is still a lot of work to be done for more realistic applications [[25](#bib.bib25),
    [26](#bib.bib26)] with complex dynamics, which are not necessarily vision-based.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习（DRL）中[[23](#bib.bib23), [24](#bib.bib24)]，深度神经网络被训练来逼近最优策略和/或价值函数。这种方式中，作为函数逼近器的深度神经网络能够实现强大的泛化能力。DRL的一个关键优势是，它使强化学习能够扩展到具有高维状态和动作空间的问题。然而，迄今为止，大多数现有成功的DRL应用都集中在视觉领域（例如Atari游戏），对于更加现实的、复杂动态的应用仍需大量工作[[25](#bib.bib25),
    [26](#bib.bib26)]，这些应用不一定是基于视觉的。
- en: 'DRL has been regarded as an important component in constructing general AI
    systems [[27](#bib.bib27)] and has been successfully integrated with other techniques,
    e.g., search [[14](#bib.bib14)], planning [[28](#bib.bib28)], and more recently
    with multiagent systems, with an emerging area of *multiagent deep reinforcement
    learning* *(MDRL)*[[29](#bib.bib29), [30](#bib.bib30)].²²2We have noted inconsistency
    in abbreviations such as: D-MARL, MADRL, deep-multiagent RL and MA-DRL.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）被视为构建通用人工智能系统的重要组成部分[[27](#bib.bib27)]，并已成功地与其他技术集成，例如搜索[[14](#bib.bib14)]、规划[[28](#bib.bib28)]，以及最近的多智能体系统，其中包括新兴领域*多智能体深度强化学习*（*MDRL*）[[29](#bib.bib29),
    [30](#bib.bib30)]。²²2我们注意到缩写存在不一致，如：D-MARL，MADRL，深度多智能体RL和MA-DRL。
- en: Learning in multiagent settings is fundamentally more difficult than the single-agent
    case due to the presence of multiagent pathologies, e.g., the moving target problem
    (non-stationarity) [[2](#bib.bib2), [5](#bib.bib5), [10](#bib.bib10)], curse of
    dimensionality [[2](#bib.bib2), [5](#bib.bib5)], multiagent credit assignment [[31](#bib.bib31),
    [32](#bib.bib32)], global exploration [[8](#bib.bib8)], and relative overgeneralization [[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]. Despite this complexity, top AI conferences
    like AAAI, ICML, ICLR, IJCAI and NeurIPS, and specialized conferences such as
    AAMAS, have published works reporting successes in MDRL. In light of these works,
    we believe it is pertinent to first, have an overview of the recent MDRL works,
    and second, understand how these recent works relate to the existing literature.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体环境中学习基本上比单一智能体情况更加困难，因为存在多智能体病态问题，例如移动目标问题（非静态性）[[2](#bib.bib2), [5](#bib.bib5),
    [10](#bib.bib10)]，维度诅咒[[2](#bib.bib2), [5](#bib.bib5)]，多智能体信用分配[[31](#bib.bib31),
    [32](#bib.bib32)]，全局探索[[8](#bib.bib8)]，以及相对泛化过度[[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35)]。尽管存在这些复杂性，像AAAI、ICML、ICLR、IJCAI和NeurIPS这样的顶级人工智能会议，以及专门的会议如AAMAS，都发表了关于MDRL成功的作品。鉴于这些作品，我们认为首先，概述最近的MDRL作品是相关的，其次，理解这些最近的作品如何与现有文献相关。
- en: This article contributes to the state of the art with a brief survey of the
    current works in MDRL in an effort to complement existing surveys on multiagent
    learning [[36](#bib.bib36), [10](#bib.bib10)], cooperative learning [[7](#bib.bib7),
    [8](#bib.bib8)], agents modeling agents [[11](#bib.bib11)], knowledge reuse in
    multiagent RL [[12](#bib.bib12)], and (single-agent) deep reinforcement learning [[23](#bib.bib23),
    [37](#bib.bib37)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过对当前MDRL工作的简要调查，为现有关于多智能体学习[[36](#bib.bib36), [10](#bib.bib10)]、合作学习[[7](#bib.bib7),
    [8](#bib.bib8)]、代理建模代理[[11](#bib.bib11)]、多智能体RL中的知识重用[[12](#bib.bib12)]以及（单一智能体）深度强化学习[[23](#bib.bib23),
    [37](#bib.bib37)]的调查进行补充。
- en: 'First, we provide a short review of key algorithms in RL such as Q-learning
    and REINFORCE (see Section [2.1](#S2.SS1 "2.1 Reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Second, we review DRL
    highlighting the challenges in this setting and reviewing recent works (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). Third, we present the multiagent setting
    and give an overview of key challenges and results (see Section [3.1](#S3.SS1
    "3.1 Multiagent Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Then, we present the
    identified four categories to group recent MDRL works (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们简要回顾了强化学习中的关键算法，如 Q-learning 和 REINFORCE（见第 [2.1](#S2.SS1 "2.1 强化学习 ‣ 2
    单智能体学习 ‣ 多智能体深度强化学习的调查与评估1footnote 11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。其次，我们回顾了深度强化学习（DRL），突出介绍了这一领域中的挑战，并评审了近期的研究工作（见第 [2.2](#S2.SS2 "2.2 深度强化学习
    ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与评估1footnote 11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。第三，我们介绍了多智能体环境，并概述了关键挑战和结果（见第 [3.1](#S3.SS1 "3.1 多智能体学习 ‣ 3 多智能体深度强化学习 (MDRL)
    ‣ 多智能体深度强化学习的调查与评估1footnote 11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。然后，我们展示了用于归类近期 MDRL 研究的四个类别（见图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 多智能体深度强化学习的调查与评估1footnote
    11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")）：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Analysis of emergent behaviors: evaluate single-agent DRL algorithms in multiagent
    scenarios (e.g., Atari games, social dilemmas, 3D competitive games).'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应对新兴行为的分析：在多智能体场景中评估单智能体 DRL 算法（例如，Atari 游戏、社会困境、3D 竞争游戏）。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Learning communication: agents learn communication protocols to solve cooperative
    tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习沟通：智能体学习沟通协议以解决合作任务。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Learning cooperation: agents learn to cooperate using only actions and (local)
    observations.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习合作：智能体通过仅使用动作和（局部）观察来学习合作。
- en: '4.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Agents modeling agents: agents reason about others to fulfill a task (e.g.,
    best response learners).'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智能体建模智能体：智能体推理其他智能体以完成任务（例如，最佳响应学习者）。
- en: 'For each category we provide a description as well as outline the recent works
    (see Section [3.2](#S3.SS2 "3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and Tables [2](#S3.T2 "Table
    2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")–[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")). Then, we take a step back and
    reflect on how these new works relate to the existing literature. In that context,
    first, we present examples on how methods and algorithms originally introduced
    in RL and MAL were successfully been scaled to MDRL (see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Second, we provide some
    pointers for new practitioners in the area by describing general *lessons learned*
    from the existing MDRL works (see Section [4.2](#S4.SS2 "4.2 Lessons learned ‣
    4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”"))
    and point to recent multiagent benchmarks (see Section [4.3](#S4.SS3 "4.3 Benchmarks
    for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). Third, we take a more critical view and describe practical
    challenges in MDRL, such as reproducibility, hyperparameter tunning, and computational
    demands (see Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Then, we outline some
    open research questions (see Section [4.5](#S4.SS5 "4.5 Open questions ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Lastly, we present our
    conclusions from this work (see Section [5](#S5 "5 Conclusions ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个类别，我们提供了描述，并概述了最近的研究（见第[3.2](#S3.SS2 "3.2 MDRL categorization ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节和表格[2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")–[4](#S3.T4 "Table 4 ‣ 3.2 MDRL categorization ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")）。然后，我们回顾这些新研究如何与现有文献相关。在这一背景下，首先，我们展示了如何将最初在RL和MAL中引入的方法和算法成功扩展到MDRL（见第[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节）。其次，我们通过描述现有MDRL研究中的一般*经验教训*（见第[4.2](#S4.SS2
    "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")节）为新从业者提供一些指引，并指出最近的多智能体基准（见第[4.3](#S4.SS3 "4.3
    Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节）。第三，我们采取更为批判的视角，描述MDRL中的实际挑战，如可重复性、超参数调优和计算需求（见第[4.4](#S4.SS4
    "4.4 Practical challenges in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")节）。然后，我们概述了一些未解的研究问题（见第[4.5](#S4.SS5
    "4.5 Open questions ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节）。最后，我们展示了本研究的结论（见第[5](#S5 "5 Conclusions ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")节）。'
- en: Our goal is to outline a recent and active area (i.e., MDRL), as well as to
    motivate future research to take advantage of the ample and existing literature
    in multiagent learning. We aim to enable researchers with experience in either
    DRL or MAL to gain a common understanding about recent works, and open problems
    in MDRL, and to avoid having scattered sub-communities with little interaction [[2](#bib.bib2),
    [10](#bib.bib10), [11](#bib.bib11), [38](#bib.bib38)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是概述一个近期活跃的领域（即 MDRL），并激励未来研究利用多智能体学习中的丰富现有文献。我们旨在使有 DRL 或 MAL 经验的研究人员对最新工作和
    MDRL 中的开放问题有共同的理解，并避免形成相互隔绝的小圈子 [[2](#bib.bib2), [10](#bib.bib10), [11](#bib.bib11),
    [38](#bib.bib38)]。
- en: '![Refer to caption](img/05ff41b4091bf1206fa5a001eb009d06.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/05ff41b4091bf1206fa5a001eb009d06.png)'
- en: (a) Analysis of emergent behaviors
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 产生行为的分析
- en: '![Refer to caption](img/92d13bf4a92b65ba059ac044451f2559.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/92d13bf4a92b65ba059ac044451f2559.png)'
- en: (b) Learning communication
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 学习沟通
- en: '![Refer to caption](img/0552b30d046b5206bb4d8f8b4fc3798f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0552b30d046b5206bb4d8f8b4fc3798f.png)'
- en: (c) Learning cooperation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 学习合作
- en: '![Refer to caption](img/848d22f0f88e0c4c34d32daabb3340f8.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/848d22f0f88e0c4c34d32daabb3340f8.png)'
- en: (d) Agents modeling agents
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 智能体建模智能体
- en: 'Figure 1: Categories of different MDRL works. (a) Analysis of emergent behaviors:
    evaluate single-agent DRL algorithms in multiagent scenarios. (b) Learning communication:
    agents learn with actions and through messages. (c) Learning cooperation: agents
    learn to cooperate using only actions and (local) observations. (d) Agents modeling
    agents: agents reason about others to fulfill a task (e.g., cooperative or competitive).
    For a more detailed description see Sections [3.3](#S3.SS3 "3.3 Emergent behaviors
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")–[3.6](#S3.SS6 "3.6 Agents modeling agents ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and Tables [2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")–[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同 MDRL 工作的分类。(a) 产生行为的分析：在多智能体场景中评估单智能体 DRL 算法。(b) 学习沟通：智能体通过行动和消息进行学习。(c)
    学习合作：智能体仅通过行动和（局部）观察来学习合作。(d) 智能体建模智能体：智能体推理其他智能体以完成任务（例如，合作或竞争）。有关更详细的描述，请参见章节 [3.3](#S3.SS3
    "3.3 产生行为 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")–[3.6](#S3.SS6
    "3.6 智能体建模智能体 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")
    以及表格 [2](#S3.T2 "表 2 ‣ 3.2 MDRL 分类 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注
    11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")–[4](#S3.T4 "表 4 ‣ 3.2 MDRL 分类 ‣ 3 多智能体深度强化学习
    (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")。
- en: 2 Single-agent learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 单智能体学习
- en: This section presents the formalism of reinforcement learning and its main components
    before outlining *deep* reinforcement learning along with its particular challenges
    and recent algorithms. For a more detailed description we refer the reader to
    excellent books and surveys on the area [[39](#bib.bib39), [20](#bib.bib20), [23](#bib.bib23),
    [40](#bib.bib40), [24](#bib.bib24)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了强化学习的形式化以及其主要组成部分，然后概述了*深度*强化学习及其特定挑战和最新算法。有关更详细的描述，我们推荐读者参考该领域的优秀书籍和综述文章 [[39](#bib.bib39),
    [20](#bib.bib20), [23](#bib.bib23), [40](#bib.bib40), [24](#bib.bib24)]。
- en: 2.1 Reinforcement learning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习
- en: RL formalizes the interaction of an agent with an environment using a Markov
    decision process (MDP) [[41](#bib.bib41)]. An MDP is defined by the tuple <math
    id="S2.SS1.p1.1.m1.5" class="ltx_Math" alttext="\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle"
    display="inline"><semantics id="S2.SS1.p1.1.m1.5a"><mrow id="S2.SS1.p1.1.m1.5.6.2"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.1"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟨</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1"
    xref="S2.SS1.p1.1.m1.1.1.cmml">𝒮</mi><mo id="S2.SS1.p1.1.m1.5.6.2.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">𝒜</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.3" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.3.3"
    xref="S2.SS1.p1.1.m1.3.3.cmml">R</mi><mo id="S2.SS1.p1.1.m1.5.6.2.4" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    id="S2.SS1.p1.1.m1.4.4" xref="S2.SS1.p1.1.m1.4.4.cmml">T</mi><mo id="S2.SS1.p1.1.m1.5.6.2.5"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.5.5" xref="S2.SS1.p1.1.m1.5.5.cmml">γ</mi><mo
    stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.6" xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.1.m1.5b"><list id="S2.SS1.p1.1.m1.5.6.1.cmml"
    xref="S2.SS1.p1.1.m1.5.6.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝒮</ci><ci
    id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">𝒜</ci><ci id="S2.SS1.p1.1.m1.3.3.cmml"
    xref="S2.SS1.p1.1.m1.3.3">𝑅</ci><ci id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4">𝑇</ci><ci
    id="S2.SS1.p1.1.m1.5.5.cmml" xref="S2.SS1.p1.1.m1.5.5">𝛾</ci></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.1.m1.5c">\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle</annotation></semantics></math>
    where <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics
    id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1"
    xref="S2.SS1.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>
    represents a finite set of states. <math id="S2.SS1.p1.3.m3.1" class="ltx_Math"
    alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝒜</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math>
    represents a finite set of actions. The transition function <math id="S2.SS1.p1.4.m4.2"
    class="ltx_Math" alttext="T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]"
    display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3"
    xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">T</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">:</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3" xref="S2.SS1.p1.4.m4.2.3.3.cmml"><mrow id="S2.SS1.p1.4.m4.2.3.3.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2.cmml">𝒮</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.2.3.cmml">𝒜</mi><mo lspace="0.222em"
    rspace="0.222em" id="S2.SS1.p1.4.m4.2.3.3.2.1a" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">×</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.4" xref="S2.SS1.p1.4.m4.2.3.3.2.4.cmml">𝒮</mi></mrow><mo
    stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.1" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml">→</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3.3.2" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml"><mo stretchy="false"
    id="S2.SS1.p1.4.m4.2.3.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">[</mo><mn
    id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">0</mn><mo id="S2.SS1.p1.4.m4.2.3.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">,</mo><mn id="S2.SS1.p1.4.m4.2.2" xref="S2.SS1.p1.4.m4.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.4.m4.2b"><apply id="S2.SS1.p1.4.m4.2.3.cmml"
    xref="S2.SS1.p1.4.m4.2.3"><ci id="S2.SS1.p1.4.m4.2.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.1">:</ci><ci
    id="S2.SS1.p1.4.m4.2.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.2">𝑇</ci><apply id="S2.SS1.p1.4.m4.2.3.3.cmml"
    xref="S2.SS1.p1.4.m4.2.3.3"><ci id="S2.SS1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.1">→</ci><apply
    id="S2.SS1.p1.4.m4.2.3.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2"><ci id="S2.SS1.p1.4.m4.2.3.3.2.2.cmml"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2">𝒮</ci><ci id="S2.SS1.p1.4.m4.2.3.3.2.3.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2.3">𝒜</ci><ci
    id="S2.SS1.p1.4.m4.2.3.3.2.4.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2.4">𝒮</ci></apply><interval
    closure="closed" id="S2.SS1.p1.4.m4.2.3.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.3.2"><cn
    type="integer" id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">0</cn><cn
    type="integer" id="S2.SS1.p1.4.m4.2.2.cmml" xref="S2.SS1.p1.4.m4.2.2">1</cn></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.4.m4.2c">T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]</annotation></semantics></math>
    determines the probability of a transition from any state <math id="S2.SS1.p1.5.m5.1"
    class="ltx_Math" alttext="s\in\mathcal{S}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mrow
    id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2"
    xref="S2.SS1.p1.5.m5.1.1.2.cmml">s</mi><mo id="S2.SS1.p1.5.m5.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.cmml">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">𝒮</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml"
    xref="S2.SS1.p1.5.m5.1.1"><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑠</ci><ci
    id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝒮</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">s\in\mathcal{S}</annotation></semantics></math>
    to any state <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="s^{\prime}\in\mathcal{S}"
    display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1"
    xref="S2.SS1.p1.6.m6.1.1.cmml"><msup id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml"><mi
    id="S2.SS1.p1.6.m6.1.1.2.2" xref="S2.SS1.p1.6.m6.1.1.2.2.cmml">s</mi><mo id="S2.SS1.p1.6.m6.1.1.2.3"
    xref="S2.SS1.p1.6.m6.1.1.2.3.cmml">′</mo></msup><mo id="S2.SS1.p1.6.m6.1.1.1"
    xref="S2.SS1.p1.6.m6.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.6.m6.1.1.3"
    xref="S2.SS1.p1.6.m6.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><apply
    id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous"
    id="S2.SS1.p1.6.m6.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.1.1.2">superscript</csymbol><ci
    id="S2.SS1.p1.6.m6.1.1.2.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2.2">𝑠</ci><ci id="S2.SS1.p1.6.m6.1.1.2.3.cmml"
    xref="S2.SS1.p1.6.m6.1.1.2.3">′</ci></apply><ci id="S2.SS1.p1.6.m6.1.1.3.cmml"
    xref="S2.SS1.p1.6.m6.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.6.m6.1c">s^{\prime}\in\mathcal{S}</annotation></semantics></math>
    given any possible action <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="a\in\mathcal{A}"
    display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1"
    xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">a</mi><mo
    id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">𝒜</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml"
    xref="S2.SS1.p1.7.m7.1.1"><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">𝑎</ci><ci
    id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝒜</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">a\in\mathcal{A}</annotation></semantics></math>.
    The reward function <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}"
    display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mrow id="S2.SS1.p1.8.m8.1.1"
    xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">R</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mrow id="S2.SS1.p1.8.m8.1.1.3.2"
    xref="S2.SS1.p1.8.m8.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.3.2.2"
    xref="S2.SS1.p1.8.m8.1.1.3.2.2.cmml">𝒮</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.8.m8.1.1.3.2.1" xref="S2.SS1.p1.8.m8.1.1.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.8.m8.1.1.3.2.3" xref="S2.SS1.p1.8.m8.1.1.3.2.3.cmml">𝒜</mi><mo lspace="0.222em"
    rspace="0.222em" id="S2.SS1.p1.8.m8.1.1.3.2.1a" xref="S2.SS1.p1.8.m8.1.1.3.2.1.cmml">×</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.3.2.4" xref="S2.SS1.p1.8.m8.1.1.3.2.4.cmml">𝒮</mi></mrow><mo
    stretchy="false" id="S2.SS1.p1.8.m8.1.1.3.1" xref="S2.SS1.p1.8.m8.1.1.3.1.cmml">→</mo><mi
    id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml">ℝ</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml"
    xref="S2.SS1.p1.8.m8.1.1"><ci id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1">:</ci><ci
    id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑅</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml"
    xref="S2.SS1.p1.8.m8.1.1.3"><ci id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.1">→</ci><apply
    id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2"><ci id="S2.SS1.p1.8.m8.1.1.3.2.2.cmml"
    xref="S2.SS1.p1.8.m8.1.1.3.2.2">𝒮</ci><ci id="S2.SS1.p1.8.m8.1.1.3.2.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2.3">𝒜</ci><ci
    id="S2.SS1.p1.8.m8.1.1.3.2.4.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2.4">𝒮</ci></apply><ci
    id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3">ℝ</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}</annotation></semantics></math>
    defines the immediate and possibly stochastic reward that an agent would receive
    given that the agent executes action <math id="S2.SS1.p1.9.m9.1" class="ltx_Math"
    alttext="a" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mi id="S2.SS1.p1.9.m9.1.1"
    xref="S2.SS1.p1.9.m9.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">𝑎</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">a</annotation></semantics></math>
    while in state <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.p1.10.m10.1a"><mi id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml"
    xref="S2.SS1.p1.10.m10.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.10.m10.1c">s</annotation></semantics></math> and it is transitioned
    to state <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics
    id="S2.SS1.p1.11.m11.1a"><msup id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml"><mi
    id="S2.SS1.p1.11.m11.1.1.2" xref="S2.SS1.p1.11.m11.1.1.2.cmml">s</mi><mo id="S2.SS1.p1.11.m11.1.1.3"
    xref="S2.SS1.p1.11.m11.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.11.m11.1b"><apply id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.p1.11.m11.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">superscript</csymbol><ci
    id="S2.SS1.p1.11.m11.1.1.2.cmml" xref="S2.SS1.p1.11.m11.1.1.2">𝑠</ci><ci id="S2.SS1.p1.11.m11.1.1.3.cmml"
    xref="S2.SS1.p1.11.m11.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.11.m11.1c">s^{\prime}</annotation></semantics></math>, <math id="S2.SS1.p1.12.m12.2"
    class="ltx_Math" alttext="\gamma\in[0,1]" display="inline"><semantics id="S2.SS1.p1.12.m12.2a"><mrow
    id="S2.SS1.p1.12.m12.2.3" xref="S2.SS1.p1.12.m12.2.3.cmml"><mi id="S2.SS1.p1.12.m12.2.3.2"
    xref="S2.SS1.p1.12.m12.2.3.2.cmml">γ</mi><mo id="S2.SS1.p1.12.m12.2.3.1" xref="S2.SS1.p1.12.m12.2.3.1.cmml">∈</mo><mrow
    id="S2.SS1.p1.12.m12.2.3.3.2" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml"><mo stretchy="false"
    id="S2.SS1.p1.12.m12.2.3.3.2.1" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">[</mo><mn
    id="S2.SS1.p1.12.m12.1.1" xref="S2.SS1.p1.12.m12.1.1.cmml">0</mn><mo id="S2.SS1.p1.12.m12.2.3.3.2.2"
    xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">,</mo><mn id="S2.SS1.p1.12.m12.2.2" xref="S2.SS1.p1.12.m12.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.p1.12.m12.2.3.3.2.3" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.12.m12.2b"><apply id="S2.SS1.p1.12.m12.2.3.cmml"
    xref="S2.SS1.p1.12.m12.2.3"><ci id="S2.SS1.p1.12.m12.2.3.2.cmml" xref="S2.SS1.p1.12.m12.2.3.2">𝛾</ci><interval
    closure="closed" id="S2.SS1.p1.12.m12.2.3.3.1.cmml" xref="S2.SS1.p1.12.m12.2.3.3.2"><cn
    type="integer" id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1">0</cn><cn
    type="integer" id="S2.SS1.p1.12.m12.2.2.cmml" xref="S2.SS1.p1.12.m12.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.12.m12.2c">\gamma\in[0,1]</annotation></semantics></math>
    represents the discount factor that balances the trade-off between immediate rewards
    and future rewards.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）通过马尔可夫决策过程（MDP）[[41](#bib.bib41)]形式化了代理与环境的交互。MDP由元组<math id="S2.SS1.p1.1.m1.5"
    class="ltx_Math" alttext="\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle" display="inline"><semantics
    id="S2.SS1.p1.1.m1.5a"><mrow id="S2.SS1.p1.1.m1.5.6.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml"><mo
    stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.1" xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟨</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">𝒮</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">𝒜</mi><mo id="S2.SS1.p1.1.m1.5.6.2.3"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml">R</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.4" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.4.4"
    xref="S2.SS1.p1.1.m1.4.4.cmml">T</mi><mo id="S2.SS1.p1.1.m1.5.6.2.5" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    id="S2.SS1.p1.1.m1.5.5" xref="S2.SS1.p1.1.m1.5.5.cmml">γ</mi><mo stretchy="false"
    id="S2.SS1.p1.1.m1.5.6.2.6" xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.1.m1.5b"><list id="S2.SS1.p1.1.m1.5.6.1.cmml"
    xref="S2.SS1.p1.1.m1.5.6.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝒮</ci><ci
    id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">𝒜</ci><ci id="S2.SS1.p1.1.m1.3.3.cmml"
    xref="S2.SS1.p1.1.m1.3.3">𝑅</ci><ci id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4">𝑇</ci><ci
    id="S2.SS1.p1.1.m1.5.5.cmml" xref="S2.SS1.p1.1.m1.5.5">𝛾</ci></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.1.m1.5c">\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle</annotation></semantics></math>定义，其中<math
    id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics
    id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1"
    xref="S2.SS1.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>表示有限状态集合。<math
    id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics
    id="S2.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.3.m3.1.1"
    xref="S2.SS1.p1.3.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝒜</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math>表示有限动作集合。转移函数<math
    id="S2.SS1.p1.4.m4.2" class="ltx_Math" alttext="T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]"
    display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3"
    xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">T</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">:</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3" xref="S2.SS1.p1.4.m4.2.3.3.cmml"><mrow id="S2.SS1.p1.4.m4.2.3.3.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2.cmml">𝒮</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.4.m4.2.
- en: MDPs are adequate models to obtain optimal decisions in *single* agent fully
    observable environments.³³3A Partially Observable Markov Decision Process (POMDP) [[42](#bib.bib42),
    [43](#bib.bib43)] explicitly models environments where the agent no longer sees
    the true system state and instead receives an *observation* (generated from the
    underlying system state). Solving an MDP will yield a policy <math id="S2.SS1.p2.1.m1.1"
    class="ltx_Math" alttext="\pi:\mathcal{S}\rightarrow\mathcal{A}" display="inline"><semantics
    id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi
    id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">π</mi><mo lspace="0.278em"
    rspace="0.278em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">𝒮</mi><mo stretchy="false"
    id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">𝒜</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml"
    xref="S2.SS1.p2.1.m1.1.1"><ci id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">:</ci><ci
    id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝜋</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3"><ci id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1">→</ci><ci
    id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝒮</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3.3">𝒜</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\pi:\mathcal{S}\rightarrow\mathcal{A}</annotation></semantics></math>,
    which is a mapping from states to actions. An optimal policy <math id="S2.SS1.p2.2.m2.1"
    class="ltx_Math" alttext="\pi^{*}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msup
    id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2"
    xref="S2.SS1.p2.2.m2.1.1.2.cmml">π</mi><mo id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml"
    xref="S2.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml"
    xref="S2.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.cmml"
    xref="S2.SS1.p2.2.m2.1.1.2">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p2.2.m2.1c">\pi^{*}</annotation></semantics></math> is the one that
    maximizes the expected discounted sum of rewards. There are different techniques
    for solving MDPs assuming a complete description of all its elements. One of the
    most common techniques is the value iteration algorithm [[44](#bib.bib44)], which
    requires a complete and accurate representation of states, actions, rewards, and
    transitions. However, this may be difficult to obtain in many domains. For this
    reason, RL algorithms often learn from experience interacting with the environment
    in discrete time steps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MDPs 是适用于在*单一*代理完全可观察环境中获得最佳决策的模型。³³3部分可观察马尔可夫决策过程（POMDP）[[42](#bib.bib42),
    [43](#bib.bib43)] 明确建模了代理无法再看到真实系统状态，而是接收*观测*（由基础系统状态生成）的环境。解决 MDP 将产生一个策略 <math
    id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\pi:\mathcal{S}\rightarrow\mathcal{A}"
    display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1"
    xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">π</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">𝒮</mi><mo stretchy="false"
    id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">𝒜</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml"
    xref="S2.SS1.p2.1.m1.1.1"><ci id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">:</ci><ci
    id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝜋</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3"><ci id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1">→</ci><ci
    id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝒮</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3.3">𝒜</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\pi:\mathcal{S}\rightarrow\mathcal{A}</annotation></semantics></math>，它是从状态到动作的映射。一个最优策略
    <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\pi^{*}" display="inline"><semantics
    id="S2.SS1.p2.2.m2.1a"><msup id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi
    id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">π</mi><mo id="S2.SS1.p2.2.m2.1.1.3"
    xref="S2.SS1.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">superscript</csymbol><ci
    id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\pi^{*}</annotation></semantics></math>
    是最大化期望折扣回报总和的策略。对于 MDPs 的解决有不同的技术，前提是对其所有元素有完整描述。最常见的技术之一是价值迭代算法 [[44](#bib.bib44)]，它需要对状态、动作、奖励和转移有完整而准确的表示。然而，在许多领域，这可能很难获得。因此，RL
    算法通常通过在离散时间步中与环境交互的经验来学习。
- en: Q-learning
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q-学习
- en: 'One of the most well known algorithms for RL is Q-learning [[45](#bib.bib45)].
    It has been devised for stationary, single-agent, fully observable environments
    with discrete actions. A Q-learning agent keeps the estimate of its expected payoff
    starting in state <math id="S2.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="s"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">s</annotation></semantics></math>,
    taking action <math id="S2.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1"
    xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">𝑎</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">a</annotation></semantics></math>
    as <math id="S2.SS1.SSS0.Px1.p1.3.m3.2" class="ltx_Math" alttext="\hat{Q}(s,a)"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.2a"><mrow id="S2.SS1.SSS0.Px1.p1.3.m3.2.3"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"><mover accent="true" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.2b"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2">𝑄</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">𝑠</ci><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2">𝑎</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.2c">\hat{Q}(s,a)</annotation></semantics></math>.
    Each tabular entry <math id="S2.SS1.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="\hat{Q}(s,a)"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.4.m4.2a"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.3"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"><mover accent="true" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.2b"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2">𝑄</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">𝑠</ci><ci
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2">𝑎</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.2c">\hat{Q}(s,a)</annotation></semantics></math>
    is an estimate of the corresponding optimal <math id="S2.SS1.SSS0.Px1.p1.5.m5.1"
    class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a"><msup
    id="S2.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2"
    xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3"
    xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">superscript</csymbol><ci
    id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2">𝑄</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">Q^{*}</annotation></semantics></math>
    function that maps state-action pairs to the discounted sum of future rewards
    starting with action <math id="S2.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.6.m6.1a"><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.1"
    xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.6.m6.1b"><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1">𝑎</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.6.m6.1c">a</annotation></semantics></math>
    at state <math id="S2.SS1.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.7.m7.1a"><mi id="S2.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.7.m7.1b"><ci id="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.7.m7.1c">s</annotation></semantics></math> and following
    the optimal policy thereafter. Each time the agent transitions from a state <math
    id="S2.SS1.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.8.m8.1a"><mi id="S2.SS1.SSS0.Px1.p1.8.m8.1.1" xref="S2.SS1.SSS0.Px1.p1.8.m8.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.8.m8.1b"><ci id="S2.SS1.SSS0.Px1.p1.8.m8.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.8.m8.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.8.m8.1c">s</annotation></semantics></math> to a state <math
    id="S2.SS1.SSS0.Px1.p1.9.m9.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.9.m9.1a"><msup id="S2.SS1.SSS0.Px1.p1.9.m9.1.1" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.9.m9.1b"><apply id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2">𝑠</ci><ci id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.9.m9.1c">s^{\prime}</annotation></semantics></math>
    via action <math id="S2.SS1.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.10.m10.1a"><mi id="S2.SS1.SSS0.Px1.p1.10.m10.1.1"
    xref="S2.SS1.SSS0.Px1.p1.10.m10.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.10.m10.1b"><ci id="S2.SS1.SSS0.Px1.p1.10.m10.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.10.m10.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.10.m10.1c">a</annotation></semantics></math> receiving
    payoff <math id="S2.SS1.SSS0.Px1.p1.11.m11.1" class="ltx_Math" alttext="r" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.11.m11.1a"><mi id="S2.SS1.SSS0.Px1.p1.11.m11.1.1" xref="S2.SS1.SSS0.Px1.p1.11.m11.1.1.cmml">r</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.11.m11.1b"><ci id="S2.SS1.SSS0.Px1.p1.11.m11.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.11.m11.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.11.m11.1c">r</annotation></semantics></math>, the <math
    id="S2.SS1.SSS0.Px1.p1.12.m12.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.12.m12.1a"><mi id="S2.SS1.SSS0.Px1.p1.12.m12.1.1" xref="S2.SS1.SSS0.Px1.p1.12.m12.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.12.m12.1b"><ci id="S2.SS1.SSS0.Px1.p1.12.m12.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.12.m12.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.12.m12.1c">Q</annotation></semantics></math> table is updated
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E1.m1.7" class="ltx_Math" alttext="\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]"
    display="block"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml"><mrow
    id="S2.E1.m1.7.7.3" xref="S2.E1.m1.7.7.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.3.2"
    xref="S2.E1.m1.7.7.3.2.cmml"><mi id="S2.E1.m1.7.7.3.2.2" xref="S2.E1.m1.7.7.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.3.2.1" xref="S2.E1.m1.7.7.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.3.1" xref="S2.E1.m1.7.7.3.1.cmml">​</mo><mrow id="S2.E1.m1.7.7.3.3.2"
    xref="S2.E1.m1.7.7.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.1"
    xref="S2.E1.m1.7.7.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">s</mi><mo
    id="S2.E1.m1.7.7.3.3.2.2" xref="S2.E1.m1.7.7.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2"
    xref="S2.E1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.3"
    xref="S2.E1.m1.7.7.3.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.7.7.2"
    xref="S2.E1.m1.7.7.2.cmml">←</mo><mrow id="S2.E1.m1.7.7.1" xref="S2.E1.m1.7.7.1.cmml"><mrow
    id="S2.E1.m1.7.7.1.3" xref="S2.E1.m1.7.7.1.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.1.3.2"
    xref="S2.E1.m1.7.7.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.3.2.2" xref="S2.E1.m1.7.7.1.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.3.2.1" xref="S2.E1.m1.7.7.1.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.3.1" xref="S2.E1.m1.7.7.1.3.1.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.3.3.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.3.3.2.1" xref="S2.E1.m1.7.7.1.3.3.1.cmml">(</mo><mi id="S2.E1.m1.3.3"
    xref="S2.E1.m1.3.3.cmml">s</mi><mo id="S2.E1.m1.7.7.1.3.3.2.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml">,</mo><mi
    id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.1.3.3.2.3"
    xref="S2.E1.m1.7.7.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.2"
    xref="S2.E1.m1.7.7.1.2.cmml">+</mo><mrow id="S2.E1.m1.7.7.1.1" xref="S2.E1.m1.7.7.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.3" xref="S2.E1.m1.7.7.1.1.3.cmml">α</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.1.2" xref="S2.E1.m1.7.7.1.1.2.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.2.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.7.7.1.1.1.1.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">(</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml">r</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml">γ</mi><mo
    lspace="0.167em" rspace="0em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml"><munder
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.2.cmml">max</mi><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2.cmml">a</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3.cmml">′</mo></msup></munder><mo
    lspace="0.167em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5a" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml">⁡</mo><mover
    accent="true" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1.cmml">^</mo></mover></mrow><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3a" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">a</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">′</mo></msup><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.5" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.2.cmml">−</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.3.cmml"><mover accent="true"
    id="S2.E1.m1.7.7.1.1.1.1.1.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.1.1.1.1.3.2.2"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.2.cmml">Q</mi><mo id="S2.E1.m1.7.7.1.1.1.1.1.3.2.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.E1.m1.7.7.1.1.1.1.1.3.1" xref="S2.E1.m1.7.7.1.1.1.1.1.3.1.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.1" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">(</mo><mi
    id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">s</mi><mo id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.2"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">,</mo><mi id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml">a</mi><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7"><ci
    id="S2.E1.m1.7.7.2.cmml" xref="S2.E1.m1.7.7.2">←</ci><apply id="S2.E1.m1.7.7.3.cmml"
    xref="S2.E1.m1.7.7.3"><apply id="S2.E1.m1.7.7.3.2.cmml" xref="S2.E1.m1.7.7.3.2"><ci
    id="S2.E1.m1.7.7.3.2.1.cmml" xref="S2.E1.m1.7.7.3.2.1">^</ci><ci id="S2.E1.m1.7.7.3.2.2.cmml"
    xref="S2.E1.m1.7.7.3.2.2">𝑄</ci></apply><interval closure="open" id="S2.E1.m1.7.7.3.3.1.cmml"
    xref="S2.E1.m1.7.7.3.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑠</ci><ci
    id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝑎</ci></interval></apply><apply id="S2.E1.m1.7.7.1.cmml"
    xref="S2.E1.m1.7.7.1"><apply id="S2.E1.m1.7.7.1.3.cmml" xref="S2.E1.m1.7.7.1.3"><apply
    id="S2.E1.m1.7.7.1.3.2.cmml" xref="S2.E1.m1.7.7.1.3.2"><ci id="S2.E1.m1.7.7.1.3.2.1.cmml"
    xref="S2.E1.m1.7.7.1.3.2.1">^</ci><ci id="S2.E1.m1.7.7.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.3.2.2">𝑄</ci></apply><interval
    closure="open" id="S2.E1.m1.7.7.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.3.3.2"><ci
    id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑠</ci><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">𝑎</ci></interval></apply><apply
    id="S2.E1.m1.7.7.1.1.cmml" xref="S2.E1.m1.7.7.1.1"><ci id="S2.E1.m1.7.7.1.1.3.cmml"
    xref="S2.E1.m1.7.7.1.1.3">𝛼</ci><apply id="S2.E1.m1.7.7.1.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1"><csymbol
    cd="latexml" id="S2.E1.m1.7.7.1.1.1.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.2">delimited-[]</csymbol><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1"><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4">𝑟</ci><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4">𝛾</ci><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5"><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1">subscript</csymbol><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3">superscript</csymbol><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2">𝑎</ci><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3">′</ci></apply></apply><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1">^</ci><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2">𝑄</ci></apply></apply><interval closure="open"
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2"><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑠</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3">′</ci></apply><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2">𝑎</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3">′</ci></apply></interval></apply></apply><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3"><apply id="S2.E1.m1.7.7.1.1.1.1.1.3.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.1">^</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.2">𝑄</ci></apply><interval
    closure="open" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.2"><ci
    id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">𝑠</ci><ci id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">𝑎</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E1.m1.7c">\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]</annotation></semantics></math>
    |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: 'with the learning rate <math id="S2.SS1.SSS0.Px1.p1.13.m1.2" class="ltx_Math"
    alttext="\alpha\in[0,1]" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.13.m1.2a"><mrow
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml">α</mi><mo
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1.cmml">∈</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">[</mo><mn
    id="S2.SS1.SSS0.Px1.p1.13.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml">0</mn><mo
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">,</mo><mn
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.13.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3"><ci id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2">𝛼</ci><interval closure="closed" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1">0</cn><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.13.m1.2c">\alpha\in[0,1]</annotation></semantics></math>.
    Q-learning is proven to converge to <math id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1"
    class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1a"><msup
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml">Q</mi><mo
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2">𝑄</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1c">Q^{*}</annotation></semantics></math>
    if state and action spaces are discrete and finite, the sum of the learning rates
    goes to infinity (so that each state-action pair is visited infinitely often)
    and that the sum of the squares of the learning rates is finite (which is required
    to show that the convergence is with probability one) [[46](#bib.bib46), [45](#bib.bib45),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)].
    The convergence of single-step on-policy RL algorithms, i.e, SARSA (<math id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1"
    class="ltx_Math" alttext="\lambda=0" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1a"><mrow
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml">λ</mi><mo
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1.cmml">=</mo><mn
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1b"><apply id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1"><ci id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2">𝜆</ci><cn type="integer" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1c">\lambda=0</annotation></semantics></math>),
    for both decaying exploration (greedy in the limit with infinite exploration)
    and persistent exploration (selecting actions probabilistically according to the
    ranks of the Q values) was demonstrated by Singh et al. [[52](#bib.bib52)]. Furthermore,
    Van Seijen [[53](#bib.bib53)] has proven convergence for Expected SARSA (see Section [3.1](#S3.SS1
    "3.1 Multiagent Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") for convergence results
    in multiagent domains).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率<math id="S2.SS1.SSS0.Px1.p1.13.m1.2" class="ltx_Math" alttext="\alpha\in[0,1]"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.13.m1.2a"><mrow id="S2.SS1.SSS0.Px1.p1.13.m1.2.3"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml">α</mi><mo id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1.cmml">∈</mo><mrow id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">[</mo><mn id="S2.SS1.SSS0.Px1.p1.13.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml">0</mn><mo id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">,</mo><mn id="S2.SS1.SSS0.Px1.p1.13.m1.2.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.3"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.13.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3"><ci id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2">𝛼</ci><interval closure="closed" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1">0</cn><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.13.m1.2c">\alpha\in[0,1]</annotation></semantics></math>。Q-learning已被证明在状态和动作空间为离散和有限时收敛至<math
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1" class="ltx_Math" alttext="Q^{*}" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1a"><msup id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2">𝑄</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1c">Q^{*}</annotation></semantics></math>，如果学习率之和趋向于无穷大（以便每个状态-动作对被无限次访问），并且学习率的平方和有限（这是显示收敛概率为一所需的）
    [[46](#bib.bib46], [45](#bib.bib45], [47](#bib.bib47], [48](#bib.bib48], [49](#bib.bib49],
    [50](#bib.bib50], [51](#bib.bib51)]。单步在策略RL算法的收敛性，即SARSA（<math id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1"
    class="ltx_Math" alttext="\lambda=0" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1a"><mrow
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml">λ</mi><mo
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1.cmml">=</mo><mn
    id="S2.S
- en: REINFORCE (Monte Carlo policy gradient)
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: REINFORCE（蒙特卡洛策略梯度）
- en: In contrast to value-based methods, which do not try to optimize directly over
    a policy space [[54](#bib.bib54)], policy gradient methods can learn parameterized
    policies without using intermediate value estimates.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与不直接优化策略空间的基于价值的方法相比[[54](#bib.bib54)]，策略梯度方法可以在不使用中间价值估计的情况下学习参数化策略。
- en: Policy parameters are learned by following the gradient of some performance
    measure with gradient descent [[55](#bib.bib55)]. For example, REINFORCE [[56](#bib.bib56)]
    uses estimated return by Monte Carlo (MC) methods with full episode trajectories
    to learn policy parameters <math id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1" class="ltx_Math"
    alttext="\theta" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1a"><mi
    id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1c">\theta</annotation></semantics></math>, with
    <math id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5" class="ltx_Math" alttext="\pi(a;s,\theta)\approx\pi(a;s)"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5a"><mrow id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.cmml"><mrow id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2.cmml">π</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1.cmml">a</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">;</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3.cmml">θ</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.4" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">)</mo></mrow></mrow><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.1.cmml">≈</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2.cmml">π</mi><mo
    lspace="0em" rspace="0em" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4.cmml">a</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">;</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5b"><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6"><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2">𝜋</ci><list id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1">𝑎</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2">𝑠</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3">𝜃</ci></list></apply><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2">𝜋</ci><list id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4">𝑎</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5">𝑠</ci></list></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5c">\pi(a;s,\theta)\approx\pi(a;s)</annotation></semantics></math>,
    as follows
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E2.m1.6" class="ltx_Math" alttext="\theta_{t+1}=\theta_{t}+\alpha
    G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}" display="block"><semantics
    id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.7" xref="S2.E2.m1.6.7.cmml"><msub id="S2.E2.m1.6.7.2"
    xref="S2.E2.m1.6.7.2.cmml"><mi id="S2.E2.m1.6.7.2.2" xref="S2.E2.m1.6.7.2.2.cmml">θ</mi><mrow
    id="S2.E2.m1.6.7.2.3" xref="S2.E2.m1.6.7.2.3.cmml"><mi id="S2.E2.m1.6.7.2.3.2"
    xref="S2.E2.m1.6.7.2.3.2.cmml">t</mi><mo id="S2.E2.m1.6.7.2.3.1" xref="S2.E2.m1.6.7.2.3.1.cmml">+</mo><mn
    id="S2.E2.m1.6.7.2.3.3" xref="S2.E2.m1.6.7.2.3.3.cmml">1</mn></mrow></msub><mo
    id="S2.E2.m1.6.7.1" xref="S2.E2.m1.6.7.1.cmml">=</mo><mrow id="S2.E2.m1.6.7.3"
    xref="S2.E2.m1.6.7.3.cmml"><msub id="S2.E2.m1.6.7.3.2" xref="S2.E2.m1.6.7.3.2.cmml"><mi
    id="S2.E2.m1.6.7.3.2.2" xref="S2.E2.m1.6.7.3.2.2.cmml">θ</mi><mi id="S2.E2.m1.6.7.3.2.3"
    xref="S2.E2.m1.6.7.3.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.7.3.1" xref="S2.E2.m1.6.7.3.1.cmml">+</mo><mrow
    id="S2.E2.m1.6.7.3.3" xref="S2.E2.m1.6.7.3.3.cmml"><mi id="S2.E2.m1.6.7.3.3.2"
    xref="S2.E2.m1.6.7.3.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.7.3.3.1"
    xref="S2.E2.m1.6.7.3.3.1.cmml">​</mo><msub id="S2.E2.m1.6.7.3.3.3" xref="S2.E2.m1.6.7.3.3.3.cmml"><mi
    id="S2.E2.m1.6.7.3.3.3.2" xref="S2.E2.m1.6.7.3.3.3.2.cmml">G</mi><mi id="S2.E2.m1.6.7.3.3.3.3"
    xref="S2.E2.m1.6.7.3.3.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.7.3.3.1a"
    xref="S2.E2.m1.6.7.3.3.1.cmml">​</mo><mfrac id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml"><mrow
    id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><mrow id="S2.E2.m1.3.3.3.5" xref="S2.E2.m1.3.3.3.5.cmml"><mo
    rspace="0.167em" id="S2.E2.m1.3.3.3.5.1" xref="S2.E2.m1.3.3.3.5.1.cmml">∇</mo><mi
    id="S2.E2.m1.3.3.3.5.2" xref="S2.E2.m1.3.3.3.5.2.cmml">π</mi></mrow><mo lspace="0em"
    rspace="0em" id="S2.E2.m1.3.3.3.4" xref="S2.E2.m1.3.3.3.4.cmml">​</mo><mrow id="S2.E2.m1.3.3.3.3.3"
    xref="S2.E2.m1.3.3.3.3.4.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.3.3.3.4"
    xref="S2.E2.m1.3.3.3.3.4.cmml">(</mo><msub id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mi
    id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.1.1.1.1.1.1.3"
    xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.5"
    xref="S2.E2.m1.3.3.3.3.4.cmml">;</mo><msub id="S2.E2.m1.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.cmml"><mi
    id="S2.E2.m1.2.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.2.cmml">S</mi><mi id="S2.E2.m1.2.2.2.2.2.2.3"
    xref="S2.E2.m1.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.6"
    xref="S2.E2.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.E2.m1.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.3.cmml"><mi
    id="S2.E2.m1.3.3.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.3.3.2.cmml">θ</mi><mi id="S2.E2.m1.3.3.3.3.3.3.3"
    xref="S2.E2.m1.3.3.3.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E2.m1.3.3.3.3.3.7"
    xref="S2.E2.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S2.E2.m1.6.6.6" xref="S2.E2.m1.6.6.6.cmml"><mi
    id="S2.E2.m1.6.6.6.5" xref="S2.E2.m1.6.6.6.5.cmml">π</mi><mo lspace="0em" rspace="0em"
    id="S2.E2.m1.6.6.6.4" xref="S2.E2.m1.6.6.6.4.cmml">​</mo><mrow id="S2.E2.m1.6.6.6.3.3"
    xref="S2.E2.m1.6.6.6.3.4.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.6.3.3.4"
    xref="S2.E2.m1.6.6.6.3.4.cmml">(</mo><msub id="S2.E2.m1.4.4.4.1.1.1" xref="S2.E2.m1.4.4.4.1.1.1.cmml"><mi
    id="S2.E2.m1.4.4.4.1.1.1.2" xref="S2.E2.m1.4.4.4.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.4.4.4.1.1.1.3"
    xref="S2.E2.m1.4.4.4.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.6.6.3.3.5"
    xref="S2.E2.m1.6.6.6.3.4.cmml">;</mo><msub id="S2.E2.m1.5.5.5.2.2.2" xref="S2.E2.m1.5.5.5.2.2.2.cmml"><mi
    id="S2.E2.m1.5.5.5.2.2.2.2" xref="S2.E2.m1.5.5.5.2.2.2.2.cmml">S</mi><mi id="S2.E2.m1.5.5.5.2.2.2.3"
    xref="S2.E2.m1.5.5.5.2.2.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.6.6.3.3.6"
    xref="S2.E2.m1.6.6.6.3.4.cmml">,</mo><msub id="S2.E2.m1.6.6.6.3.3.3" xref="S2.E2.m1.6.6.6.3.3.3.cmml"><mi
    id="S2.E2.m1.6.6.6.3.3.3.2" xref="S2.E2.m1.6.6.6.3.3.3.2.cmml">θ</mi><mi id="S2.E2.m1.6.6.6.3.3.3.3"
    xref="S2.E2.m1.6.6.6.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E2.m1.6.6.6.3.3.7"
    xref="S2.E2.m1.6.6.6.3.4.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.7.cmml" xref="S2.E2.m1.6.7"><apply
    id="S2.E2.m1.6.7.2.cmml" xref="S2.E2.m1.6.7.2"><csymbol cd="ambiguous" id="S2.E2.m1.6.7.2.1.cmml"
    xref="S2.E2.m1.6.7.2">subscript</csymbol><ci id="S2.E2.m1.6.7.2.2.cmml" xref="S2.E2.m1.6.7.2.2">𝜃</ci><apply
    id="S2.E2.m1.6.7.2.3.cmml" xref="S2.E2.m1.6.7.2.3"><ci id="S2.E2.m1.6.7.2.3.2.cmml"
    xref="S2.E2.m1.6.7.2.3.2">𝑡</ci><cn type="integer" id="S2.E2.m1.6.7.2.3.3.cmml"
    xref="S2.E2.m1.6.7.2.3.3">1</cn></apply></apply><apply id="S2.E2.m1.6.7.3.cmml"
    xref="S2.E2.m1.6.7.3"><apply id="S2.E2.m1.6.7.3.2.cmml" xref="S2.E2.m1.6.7.3.2"><csymbol
    cd="ambiguous" id="S2.E2.m1.6.7.3.2.1.cmml" xref="S2.E2.m1.6.7.3.2">subscript</csymbol><ci
    id="S2.E2.m1.6.7.3.2.2.cmml" xref="S2.E2.m1.6.7.3.2.2">𝜃</ci><ci id="S2.E2.m1.6.7.3.2.3.cmml"
    xref="S2.E2.m1.6.7.3.2.3">𝑡</ci></apply><apply id="S2.E2.m1.6.7.3.3.cmml" xref="S2.E2.m1.6.7.3.3"><ci
    id="S2.E2.m1.6.7.3.3.2.cmml" xref="S2.E2.m1.6.7.3.3.2">𝛼</ci><apply id="S2.E2.m1.6.7.3.3.3.cmml"
    xref="S2.E2.m1.6.7.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.7.3.3.3.1.cmml"
    xref="S2.E2.m1.6.7.3.3.3">subscript</csymbol><ci id="S2.E2.m1.6.7.3.3.3.2.cmml"
    xref="S2.E2.m1.6.7.3.3.3.2">𝐺</ci><ci id="S2.E2.m1.6.7.3.3.3.3.cmml" xref="S2.E2.m1.6.7.3.3.3.3">𝑡</ci></apply><apply
    id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6"><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><apply
    id="S2.E2.m1.3.3.3.5.cmml" xref="S2.E2.m1.3.3.3.5"><ci id="S2.E2.m1.3.3.3.5.1.cmml"
    xref="S2.E2.m1.3.3.3.5.1">∇</ci><ci id="S2.E2.m1.3.3.3.5.2.cmml" xref="S2.E2.m1.3.3.3.5.2">𝜋</ci></apply><list
    id="S2.E2.m1.3.3.3.3.4.cmml" xref="S2.E2.m1.3.3.3.3.3"><apply id="S2.E2.m1.1.1.1.1.1.1.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1.2">𝐴</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">𝑡</ci></apply><apply
    id="S2.E2.m1.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous"
    id="S2.E2.m1.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2">subscript</csymbol><ci
    id="S2.E2.m1.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2.2">𝑆</ci><ci id="S2.E2.m1.2.2.2.2.2.2.3.cmml"
    xref="S2.E2.m1.2.2.2.2.2.2.3">𝑡</ci></apply><apply id="S2.E2.m1.3.3.3.3.3.3.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.3.3.3.1.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S2.E2.m1.3.3.3.3.3.3.2.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3.2">𝜃</ci><ci id="S2.E2.m1.3.3.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3.3.3">𝑡</ci></apply></list></apply><apply
    id="S2.E2.m1.6.6.6.cmml" xref="S2.E2.m1.6.6.6"><ci id="S2.E2.m1.6.6.6.5.cmml"
    xref="S2.E2.m1.6.6.6.5">𝜋</ci><list id="S2.E2.m1.6.6.6.3.4.cmml" xref="S2.E2.m1.6.6.6.3.3"><apply
    id="S2.E2.m1.4.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.4.1.1.1"><csymbol cd="ambiguous"
    id="S2.E2.m1.4.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.4.1.1.1">subscript</csymbol><ci
    id="S2.E2.m1.4.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.4.1.1.1.2">𝐴</ci><ci id="S2.E2.m1.4.4.4.1.1.1.3.cmml"
    xref="S2.E2.m1.4.4.4.1.1.1.3">𝑡</ci></apply><apply id="S2.E2.m1.5.5.5.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.5.2.2.2.1.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2">subscript</csymbol><ci id="S2.E2.m1.5.5.5.2.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2.2">𝑆</ci><ci id="S2.E2.m1.5.5.5.2.2.2.3.cmml" xref="S2.E2.m1.5.5.5.2.2.2.3">𝑡</ci></apply><apply
    id="S2.E2.m1.6.6.6.3.3.3.cmml" xref="S2.E2.m1.6.6.6.3.3.3"><csymbol cd="ambiguous"
    id="S2.E2.m1.6.6.6.3.3.3.1.cmml" xref="S2.E2.m1.6.6.6.3.3.3">subscript</csymbol><ci
    id="S2.E2.m1.6.6.6.3.3.3.2.cmml" xref="S2.E2.m1.6.6.6.3.3.3.2">𝜃</ci><ci id="S2.E2.m1.6.6.6.3.3.3.3.cmml"
    xref="S2.E2.m1.6.6.6.3.3.3.3">𝑡</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E2.m1.6c">\theta_{t+1}=\theta_{t}+\alpha G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}</annotation></semantics></math>
    |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="S2.E2.m1.6" class="ltx_Math" alttext="\theta_{t+1}=\theta_{t}+\alpha
    G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}"'
- en: where <math id="S2.SS1.SSS0.Px2.p2.3.m1.1" class="ltx_Math" alttext="G_{t}"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.3.m1.1a"><msub id="S2.SS1.SSS0.Px2.p2.3.m1.1.1"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml">G</mi><mi id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px2.p2.3.m1.1b"><apply id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1">subscript</csymbol><ci
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2">𝐺</ci><ci
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m1.1c">G_{t}</annotation></semantics></math>
    represents the return, <math id="S2.SS1.SSS0.Px2.p2.4.m2.1" class="ltx_Math" alttext="\alpha"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.4.m2.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m2.1.1"
    xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px2.p2.4.m2.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1">𝛼</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.4.m2.1c">\alpha</annotation></semantics></math>
    is the learning rate, and <math id="S2.SS1.SSS0.Px2.p2.5.m3.1" class="ltx_Math"
    alttext="A_{t}\sim\pi" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.5.m3.1a"><mrow
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"><msub
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml">A</mi><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml">t</mi></msub><mo
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml">∼</mo><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml">π</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m3.1b"><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1">similar-to</csymbol><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2">𝐴</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m3.1c">A_{t}\sim\pi</annotation></semantics></math>.
    A main limitation is that policy gradient methods can have high variance [[54](#bib.bib54)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math id="S2.SS1.SSS0.Px2.p2.3.m1.1" class="ltx_Math" alttext="G_{t}" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.3.m1.1a"><msub id="S2.SS1.SSS0.Px2.p2.3.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml">G</mi><mi
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml">t</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.3.m1.1b"><apply id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2">𝐺</ci><ci id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m1.1c">G_{t}</annotation></semantics></math>
    代表回报，<math id="S2.SS1.SSS0.Px2.p2.4.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.4.m2.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml">α</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.4.m2.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p2.4.m2.1c">\alpha</annotation></semantics></math> 是学习率，而
    <math id="S2.SS1.SSS0.Px2.p2.5.m3.1" class="ltx_Math" alttext="A_{t}\sim\pi" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.5.m3.1a"><mrow id="S2.SS1.SSS0.Px2.p2.5.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"><msub
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml">A</mi><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml">t</mi></msub><mo
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml">∼</mo><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml">π</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m3.1b"><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1">similar-to</csymbol><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2">𝐴</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m3.1c">A_{t}\sim\pi</annotation></semantics></math>。一个主要的限制是策略梯度方法可能具有较高的方差
    [[54](#bib.bib54)]。
- en: 'The policy gradient update can be generalized to include a comparison to an
    arbitrary *baseline* of the state [[56](#bib.bib56)]. The baseline, <math id="S2.SS1.SSS0.Px2.p3.1.m1.1"
    class="ltx_Math" alttext="b(s)" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.1.m1.1a"><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mi id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml">b</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2"><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2">𝑏</ci><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1">𝑠</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p3.1.m1.1c">b(s)</annotation></semantics></math>,
    can be any function, as long as it does not vary with the action; the baseline
    leaves the expected value of the update unchanged, but it can have an effect on
    its variance [[20](#bib.bib20)]. A natural choice for the baseline is a learned
    state-value function, this reduces the variance, and it is bias-free if learned
    by MC.⁴⁴4Action-dependant baselines had been proposed [[57](#bib.bib57), [58](#bib.bib58)],
    however, a recent study by Tucker et al. [[59](#bib.bib59)] found that in many
    works the reason of good performance was because of bugs or errors in the code,
    rather than the proposed method itself. Moreover, when using the state-value function
    for bootstrapping (updating the value estimate for a state from the estimated
    values of subsequent states) it assigns credit (reducing the variance but introducing
    bias), i.e., criticizes the policy’s action selections. Thus, in actor-critic
    methods [[54](#bib.bib54)], the actor represents the policy, i.e., action-selection
    mechanism, whereas a critic is used for the value function learning. In the case
    when the critic learns a state-action function (<math id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1a"><mi
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1c">Q</annotation></semantics></math> function)
    and a state value function (<math id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1" class="ltx_Math"
    alttext="V" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1a"><mi
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml">V</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1b"><ci id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1c">V</annotation></semantics></math> function),
    an *advantage function* can be computed by subtracting state values from the state-action
    values [[20](#bib.bib20), [60](#bib.bib60)]. The advantage function indicates
    the relative quality of an action compared to other available actions computed
    from the baseline, i.e., state value function. An example of an actor-critic algorithm
    is Deterministic Policy Gradient (DPG) [[61](#bib.bib61)]. In DPG [[61](#bib.bib61)]
    the critic follows the standard Q-learning and the actor is updated following
    the gradient of the policy’s performance [[62](#bib.bib62)], DPG was later extended
    to DRL (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and MDRL (see Section [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). For multiagent learning
    settings the variance is further increased as all the agents’ rewards depend on
    the rest of the agents, and it is formally shown that as the number of agents
    increase, the probability of taking a correct gradient direction decreases exponentially [[63](#bib.bib63)].
    Recent MDRL works addressed this high variance issue, e.g., COMA [[64](#bib.bib64)]
    and MADDPG [[63](#bib.bib63)] (see Section [3.5](#S3.SS5 "3.5 Learning cooperation
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient methods have a clear connection with deep reinforcement learning
    since *the policy might be represented by a neural network* whose input is a representation
    of the state, whose output are action selection probabilities or values for continuous
    control [[65](#bib.bib65)], and whose weights are the policy parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法与深度强化学习有着清晰的联系，因为*策略可能由神经网络表示*，其输入是状态的表示，输出是动作选择的概率或连续控制的值[[65](#bib.bib65)]，权重则是策略参数。
- en: 2.2 Deep reinforcement learning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度强化学习
- en: 'While tabular RL methods such as Q-learning are successful in domains that
    do not suffer from the curse of dimensionality, there are many limitations: learning
    in large state spaces can be prohibitively slow, methods do not generalize (across
    the state space), and state representations need to be hand-specified [[20](#bib.bib20)].
    Function approximators tried to address those limitations, using for example,
    decision trees [[66](#bib.bib66)], tile coding [[67](#bib.bib67)], radial basis
    functions [[68](#bib.bib68)], and locally weighted regression [[69](#bib.bib69)]
    to approximate the value function.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像Q学习这样的表格化RL方法在不受维度灾难影响的领域中取得了成功，但存在许多限制：在大型状态空间中学习可能非常缓慢，方法无法进行广泛的泛化（跨状态空间），状态表示需要手动指定[[20](#bib.bib20)]。函数逼近器尝试解决这些限制，例如使用决策树[[66](#bib.bib66)]、平铺编码[[67](#bib.bib67)]、径向基函数[[68](#bib.bib68)]和局部加权回归[[69](#bib.bib69)]来逼近价值函数。
- en: Similarly, these challenges can be addressed by using deep learning, i.e., neural
    networks [[69](#bib.bib69), [66](#bib.bib66)] as function approximators. For example,
    <math id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="Q(s,a;\theta)" display="inline"><semantics
    id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi
    id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">Q</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">​</mo><mrow
    id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo stretchy="false"
    id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1"
    xref="S2.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi
    id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3"
    xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">θ</mi><mo
    stretchy="false" id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml"
    xref="S2.SS2.p2.1.m1.3.4"><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">𝑄</ci><vector
    id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.p2.1.m1.1.1">𝑠</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">𝑎</ci><ci
    id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">𝜃</ci></vector></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">Q(s,a;\theta)</annotation></semantics></math>
    can be used to approximate the state-action values with <math id="S2.SS2.p2.2.m2.1"
    class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi
    id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\theta</annotation></semantics></math>
    representing the neural network weights. This has two advantages, first, deep
    learning helps to generalize across states improving the sample efficiency for
    large state-space RL problems. Second, deep learning can be used to reduce (or
    eliminate) the need for manually designing features to represent state information [[21](#bib.bib21),
    [22](#bib.bib22)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，这些挑战可以通过使用深度学习，即神经网络[[69](#bib.bib69), [66](#bib.bib66)]作为函数逼近器来解决。例如，<math
    id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="Q(s,a;\theta)" display="inline"><semantics
    id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi
    id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">Q</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">​</mo><mrow
    id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo stretchy="false"
    id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1"
    xref="S2.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi
    id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3"
    xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">θ</mi><mo
    stretchy="false" id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml"
    xref="S2.SS2.p2.1.m1.3.4"><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">𝑄</ci><vector
    id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.p2.1.m1.1.1">𝑠</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">𝑎</ci><ci
    id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">𝜃</ci></vector></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">Q(s,a;\theta)</annotation></semantics></math>可以用来近似状态-动作值，其中<math
    id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics
    id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.p2.2.m2.1c">\theta</annotation></semantics></math>代表神经网络的权重。这有两个优点，首先，深度学习有助于在状态间泛化，提高大状态空间强化学习问题的样本效率。其次，深度学习可用于减少（或消除）手动设计用于表示状态信息的特征的需求[[21](#bib.bib21),
    [22](#bib.bib22)]。
- en: However, extending deep learning to RL problems comes with additional challenges
    including non-i.i.d. (not independently and identically distributed) data. Many
    supervised learning methods assume that training data is from an i.i.d. stationary
    distribution [[70](#bib.bib70), [22](#bib.bib22), [71](#bib.bib71)]. However,
    in RL, training data consists of highly correlated sequential agent-environment
    interactions, which violates the *independence* condition. Moreover, RL training
    data distribution is non-stationary as the agent actively learns while exploring
    different parts of the state space, violating the condition of sampled data being
    *identically distributed* [[72](#bib.bib72)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将深度学习扩展到强化学习问题也带来了额外的挑战，包括非独立同分布（non-i.i.d.）的数据。许多监督学习方法假设训练数据来自独立同分布的静态分布
    [[70](#bib.bib70), [22](#bib.bib22), [71](#bib.bib71)]。然而，在强化学习中，训练数据由高度相关的顺序代理-环境交互组成，违反了*独立性*条件。此外，强化学习的训练数据分布是非静态的，因为代理在探索状态空间的不同部分时，主动学习，违反了数据*同分布*的条件
    [[72](#bib.bib72)]。
- en: 'In practice, using function approximators in RL requires making crucial representational
    decisions and poor design choices can result in estimates that diverge from the
    optimal value function [[73](#bib.bib73), [69](#bib.bib69), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]. In particular, function
    approximation, bootstrapping, and off-policy learning are considered the three
    main properties that when combined, can make the learning to diverge and are known
    as *the deadly triad* [[77](#bib.bib77), [20](#bib.bib20)]. Recently, some works
    have shown that non-linear (i.e., deep) function approximators poorly estimate
    the value function [[78](#bib.bib78), [59](#bib.bib59), [79](#bib.bib79)] and
    another work found problems with Q-learning using function approximation (over/under-estimation,
    instability and even divergence) due to the *delusional* bias: “delusional bias
    occurs whenever a backed-up value estimate is derived from action choices that
    are not realizable in the underlying policy class”[[80](#bib.bib80)]. Additionally,
    convergence results for reinforcement learning using function approximation are
    still scarce [[74](#bib.bib74), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [80](#bib.bib80)]; in general, stronger convergence guarantees are available for
    policy-gradient methods [[55](#bib.bib55)] than for value-based methods [[20](#bib.bib20)].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用RL中的函数逼近器需要做出关键的表示决策，而糟糕的设计选择可能导致估计值函数偏离最优值函数 [[73](#bib.bib73), [69](#bib.bib69),
    [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]。特别是，函数逼近、自举和离策略学习被认为是导致学习发散的三个主要属性，并被称为*致命三连击*
    [[77](#bib.bib77), [20](#bib.bib20)]。最近的一些研究表明，非线性（即深度）函数逼近器对价值函数的估计能力较差 [[78](#bib.bib78),
    [59](#bib.bib59), [79](#bib.bib79)]，另一项研究发现Q学习在使用函数逼近时存在问题（过度/欠估计、不稳定甚至发散），原因是*妄想偏差*：“每当从不可实现的动作选择导出支持的值估计时，就会出现妄想偏差”[[80](#bib.bib80)]。此外，使用函数逼近进行强化学习的收敛结果仍然很少见
    [[74](#bib.bib74), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [80](#bib.bib80)]；一般来说，与基于值的方法
    [[20](#bib.bib20)] 相比，策略梯度方法提供更强的收敛保证 [[55](#bib.bib55)]。
- en: Below we mention how the existing DRL methods aim to address these challenges
    when briefly reviewing value-based methods, such as DQN [[13](#bib.bib13)]; policy
    gradient methods, like Proximal Policy Optimization (PPO) [[60](#bib.bib60)];
    and actor-critic methods like Asynchronous Advantage Actor-Critic (A3C) [[84](#bib.bib84)].
    We refer the reader to recent surveys on single-agent DRL [[23](#bib.bib23), [37](#bib.bib37),
    [24](#bib.bib24)] for a more detailed discussion of the literature.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们提及现有的深度强化学习方法在简要回顾基于价值的方法时如何解决这些挑战，例如DQN [[13](#bib.bib13)]；策略梯度方法，如Proximal
    Policy Optimization（PPO） [[60](#bib.bib60)]；以及演员-评论者方法，如异步优势演员评论者（A3C） [[84](#bib.bib84)]。我们建议读者参阅关于单一代理DRL的最新调查
    [[23](#bib.bib23), [37](#bib.bib37), [24](#bib.bib24)]，以获取更详细的文献讨论。
- en: Value-based methods
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于价值的方法
- en: '![Refer to caption](img/c7d77c308940bcefcd4044d97716b880.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7d77c308940bcefcd4044d97716b880.png)'
- en: 'Figure 2: Deep Q-Network (DQN) [[13](#bib.bib13)]: Inputs are four stacked
    frames; the network is composed of several layers: *Convolutional* layers employ
    filters to learn features from high-dimensional data with a much smaller number
    of neurons and *Dense* layers are fully-connected layers. The last layer represents
    the actions the agent can take (in this case, <math id="S2.F2.3.1.m1.1" class="ltx_Math"
    alttext="10" display="inline"><semantics id="S2.F2.3.1.m1.1b"><mn id="S2.F2.3.1.m1.1.1"
    xref="S2.F2.3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content"
    id="S2.F2.3.1.m1.1c"><cn type="integer" id="S2.F2.3.1.m1.1.1.cmml" xref="S2.F2.3.1.m1.1.1">10</cn></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.3.1.m1.1d">10</annotation></semantics></math>
    possible actions). Deep Recurrent Q-Network (DRQN) [[85](#bib.bib85)], which extends
    DQN to partially observable domains [[42](#bib.bib42)], is identical to this setup
    except the penultimate layer (<math id="S2.F2.4.2.m2.1" class="ltx_Math" alttext="1\times
    256" display="inline"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.1"
    xref="S2.F2.4.2.m2.1.1.cmml"><mn id="S2.F2.4.2.m2.1.1.2" xref="S2.F2.4.2.m2.1.1.2.cmml">1</mn><mo
    lspace="0.222em" rspace="0.222em" id="S2.F2.4.2.m2.1.1.1" xref="S2.F2.4.2.m2.1.1.1.cmml">×</mo><mn
    id="S2.F2.4.2.m2.1.1.3" xref="S2.F2.4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.1.cmml"
    xref="S2.F2.4.2.m2.1.1"><cn type="integer" id="S2.F2.4.2.m2.1.1.2.cmml" xref="S2.F2.4.2.m2.1.1.2">1</cn><cn
    type="integer" id="S2.F2.4.2.m2.1.1.3.cmml" xref="S2.F2.4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.4.2.m2.1d">1\times 256</annotation></semantics></math>
    Dense layer) is replaced with a recurrent LSTM layer [[86](#bib.bib86)].'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度 Q 网络 (DQN) [[13](#bib.bib13)]：输入为四个堆叠的帧；网络由多个层组成：*卷积* 层使用滤波器从高维数据中学习特征，具有较少的神经元，而*全连接*
    层是完全连接的层。最后一层表示代理可以采取的动作（在此情况下为<math id="S2.F2.3.1.m1.1" class="ltx_Math" alttext="10"
    display="inline"><semantics id="S2.F2.3.1.m1.1b"><mn id="S2.F2.3.1.m1.1.1" xref="S2.F2.3.1.m1.1.1.cmml">10</mn><annotation-xml
    encoding="MathML-Content" id="S2.F2.3.1.m1.1c"><cn type="integer" id="S2.F2.3.1.m1.1.1.cmml"
    xref="S2.F2.3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex"
    id="S2.F2.3.1.m1.1d">10</annotation></semantics></math> 种可能动作)。深度递归 Q 网络 (DRQN) [[85](#bib.bib85)]，它将
    DQN 扩展到部分可观察领域 [[42](#bib.bib42)]，除了倒数第二层（<math id="S2.F2.4.2.m2.1" class="ltx_Math"
    alttext="1\times 256" display="inline"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.1"
    xref="S2.F2.4.2.m2.1.1.cmml"><mn id="S2.F2.4.2.m2.1.1.2" xref="S2.F2.4.2.m2.1.1.2.cmml">1</mn><mo
    lspace="0.222em" rspace="0.222em" id="S2.F2.4.2.m2.1.1.1" xref="S2.F2.4.2.m2.1.1.1.cmml">×</mo><mn
    id="S2.F2.4.2.m2.1.1.3" xref="S2.F2.4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.1.cmml"
    xref="S2.F2.4.2.m2.1.1"><cn type="integer" id="S2.F2.4.2.m2.1.1.2.cmml" xref="S2.F2.4.2.m2.1.1.2">1</cn><cn
    type="integer" id="S2.F2.4.2.m2.1.1.3.cmml" xref="S2.F2.4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.4.2.m2.1d">1\times 256</annotation></semantics></math>
    Dense 层) 被替换为递归 LSTM 层 [[86](#bib.bib86)]。
- en: 'The major breakthrough work combining deep learning with Q-learning was the
    Deep Q-Network (DQN) [[13](#bib.bib13)]. DQN uses a deep neural network for function
    approximation [[87](#bib.bib87)]⁵⁵5Before DQN, many approaches used neural networks
    for representing the Q-value function [[88](#bib.bib88)], such as Neural Fitted
    Q-learning [[87](#bib.bib87)] and NEAT+Q [[75](#bib.bib75)]. (see Figure [2](#S2.F2
    "Figure 2 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and maintains an *experience
    replay* (ER) buffer [[89](#bib.bib89), [90](#bib.bib90)] to store interactions
    <math id="S2.SS2.SSS0.Px1.p1.1.m1.4" class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle"
    display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">⟨</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.4"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.3.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">r</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.5"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><msup id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false"
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.6" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.4b"><list id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3">𝑟</ci><apply id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>.
    DQN keeps an additional copy of neural network parameters, <math id="S2.SS2.SSS0.Px1.p1.2.m2.1"
    class="ltx_Math" alttext="\theta^{-}" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.2.m2.1a"><msup
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">θ</mi><mo id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝜃</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.2.m2.1c">\theta^{-}</annotation></semantics></math>,
    for the target network in addition to the <math id="S2.SS2.SSS0.Px1.p1.3.m3.1"
    class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.3.m3.1a"><mi
    id="S2.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p1.3.m3.1c">\theta</annotation></semantics></math> parameters
    to stabilize the learning, i.e., to alleviate the non-stationary data distribution.⁶⁶6Double
    Q-learning [[91](#bib.bib91)] originally proposed keeping two <math id="footnote6.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="footnote6.m1.1b"><mi
    id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="footnote6.m1.1c"><ci id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="footnote6.m1.1d">Q</annotation></semantics></math>
    functions (estimators) to reduce the overestimation bias in RL, while still keeping
    the convergence guarantees, later it was extended to DRL in Double DQN [[92](#bib.bib92)]
    (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")).
    For each training iteration <math id="S2.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math"
    alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.4.m4.1a"><mi id="S2.SS2.SSS0.Px1.p1.4.m4.1.1"
    xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p1.4.m4.1b"><ci id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.4.m4.1c">i</annotation></semantics></math>,
    DQN minimizes the mean-squared error (MSE) between the Q-network and its target
    network using the loss function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E3.m1.9" class="ltx_Math" alttext="L_{i}(\theta_{i})=\operatorname{\mathbb{E}}_{s,a,r,s^{\prime}}[(r+\gamma
    max_{a^{\prime}}Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]"
    display="block"><semantics id="S2.E3.m1.9a"><mrow id="S2.E3.m1.9.9" xref="S2.E3.m1.9.9.cmml"><mrow
    id="S2.E3.m1.7.7.1" xref="S2.E3.m1.7.7.1.cmml"><msub id="S2.E3.m1.7.7.1.3" xref="S2.E3.m1.7.7.1.3.cmml"><mi
    id="S2.E3.m1.7.7.1.3.2" xref="S2.E3.m1.7.7.1.3.2.cmml">L</mi><mi id="S2.E3.m1.7.7.1.3.3"
    xref="S2.E3.m1.7.7.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.7.7.1.2"
    xref="S2.E3.m1.7.7.1.2.cmml">​</mo><mrow id="S2.E3.m1.7.7.1.1.1" xref="S2.E3.m1.7.7.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E3.m1.7.7.1.1.1.2" xref="S2.E3.m1.7.7.1.1.1.1.cmml">(</mo><msub
    id="S2.E3.m1.7.7.1.1.1.1" xref="S2.E3.m1.7.7.1.1.1.1.cmml"><mi id="S2.E3.m1.7.7.1.1.1.1.2"
    xref="S2.E3.m1.7.7.1.1.1.1.2.cmml">θ</mi><mi id="S2.E3.m1.7.7.1.1.1.1.3" xref="S2.E3.m1.7.7.1.1.1.1.3.cmml">i</mi></msub><mo
    stretchy="false" id="S2.E3.m1.7.7.1.1.1.3" xref="S2.E3.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow><mo
    id="S2.E3.m1.9.9.4" xref="S2.E3.m1.9.9.4.cmml">=</mo><mrow id="S2.E3.m1.9.9.3.2"
    xref="S2.E3.m1.9.9.3.3.cmml"><msub id="S2.E3.m1.8.8.2.1.1" xref="S2.E3.m1.8.8.2.1.1.cmml"><mi
    id="S2.E3.m1.8.8.2.1.1.2" xref="S2.E3.m1.8.8.2.1.1.2.cmml">𝔼</mi><mrow id="S2.E3.m1.4.4.4.4"
    xref="S2.E3.m1.4.4.4.5.cmml"><mi id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml">s</mi><mo
    id="S2.E3.m1.4.4.4.4.2" xref="S2.E3.m1.4.4.4.5.cmml">,</mo><mi id="S2.E3.m1.2.2.2.2"
    xref="S2.E3.m1.2.2.2.2.cmml">a</mi><mo id="S2.E3.m1.4.4.4.4.3" xref="S2.E3.m1.4.4.4.5.cmml">,</mo><mi
    id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">r</mi><mo id="S2.E3.m1.4.4.4.4.4"
    xref="S2.E3.m1.4.4.4.5.cmml">,</mo><msup id="S2.E3.m1.4.4.4.4.1" xref="S2.E3.m1.4.4.4.4.1.cmml"><mi
    id="S2.E3.m1.4.4.4.4.1.2" xref="S2.E3.m1.4.4.4.4.1.2.cmml">s</mi><mo id="S2.E3.m1.4.4.4.4.1.3"
    xref="S2.E3.m1.4.4.4.4.1.3.cmml">′</mo></msup></mrow></msub><mo id="S2.E3.m1.9.9.3.2a"
    xref="S2.E3.m1.9.9.3.3.cmml">⁡</mo><mrow id="S2.E3.m1.9.9.3.2.2" xref="S2.E3.m1.9.9.3.3.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.2" xref="S2.E3.m1.9.9.3.3.cmml">[</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1" xref="S2.E3.m1.9.9.3.2.2.1.cmml"><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.2"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.cmml"><mi id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5.cmml">r</mi><mo id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.4"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.4.cmml">+</mo><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5.cmml">γ</mi><mo lspace="0em" rspace="0em"
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4a" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4b" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><msub
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2.cmml">x</mi><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3.cmml">′</mo></msup></msub><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4c" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4d" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">(</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">,</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3.cmml">′</mo></msup><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.6" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">;</mo><msubsup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2.cmml">θ</mi><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3.cmml">i</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.3.cmml">−</mo></msubsup><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.7" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.5.cmml">−</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.2.cmml">​</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">(</mo><mi
    id="S2.E3.m1.5.5" xref="S2.E3.m1.5.5.cmml">s</mi><mo id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">,</mo><mi id="S2.E3.m1.6.6" xref="S2.E3.m1.6.6.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">;</mo><msub
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2.cmml">θ</mi><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3.cmml">i</mi></msub><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml">)</mo></mrow><mn
    id="S2.E3.m1.9.9.3.2.2.1.3" xref="S2.E3.m1.9.9.3.2.2.1.3.cmml">2</mn></msup><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.3" xref="S2.E3.m1.9.9.3.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E3.m1.9b"><apply id="S2.E3.m1.9.9.cmml" xref="S2.E3.m1.9.9"><apply
    id="S2.E3.m1.7.7.1.cmml" xref="S2.E3.m1.7.7.1"><apply id="S2.E3.m1.7.7.1.3.cmml"
    xref="S2.E3.m1.7.7.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.1.3.1.cmml" xref="S2.E3.m1.7.7.1.3">subscript</csymbol><ci
    id="S2.E3.m1.7.7.1.3.2.cmml" xref="S2.E3.m1.7.7.1.3.2">𝐿</ci><ci id="S2.E3.m1.7.7.1.3.3.cmml"
    xref="S2.E3.m1.7.7.1.3.3">𝑖</ci></apply><apply id="S2.E3.m1.7.7.1.1.1.1.cmml"
    xref="S2.E3.m1.7.7.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.1.1.1.1.1.cmml"
    xref="S2.E3.m1.7.7.1.1.1">subscript</csymbol><ci id="S2.E3.m1.7.7.1.1.1.1.2.cmml"
    xref="S2.E3.m1.7.7.1.1.1.1.2">𝜃</ci><ci id="S2.E3.m1.7.7.1.1.1.1.3.cmml" xref="S2.E3.m1.7.7.1.1.1.1.3">𝑖</ci></apply></apply><apply
    id="S2.E3.m1.9.9.3.3.cmml" xref="S2.E3.m1.9.9.3.2"><apply id="S2.E3.m1.8.8.2.1.1.cmml"
    xref="S2.E3.m1.8.8.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.8.8.2.1.1.1.cmml"
    xref="S2.E3.m1.8.8.2.1.1">subscript</csymbol><ci id="S2.E3.m1.8.8.2.1.1.2.cmml"
    xref="S2.E3.m1.8.8.2.1.1.2">𝔼</ci><list id="S2.E3.m1.4.4.4.5.cmml" xref="S2.E3.m1.4.4.4.4"><ci
    id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1">𝑠</ci><ci id="S2.E3.m1.2.2.2.2.cmml"
    xref="S2.E3.m1.2.2.2.2">𝑎</ci><ci id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3">𝑟</ci><apply
    id="S2.E3.m1.4.4.4.4.1.cmml" xref="S2.E3.m1.4.4.4.4.1"><csymbol cd="ambiguous"
    id="S2.E3.m1.4.4.4.4.1.1.cmml" xref="S2.E3.m1.4.4.4.4.1">superscript</csymbol><ci
    id="S2.E3.m1.4.4.4.4.1.2.cmml" xref="S2.E3.m1.4.4.4.4.1.2">𝑠</ci><ci id="S2.E3.m1.4.4.4.4.1.3.cmml"
    xref="S2.E3.m1.4.4.4.4.1.3">′</ci></apply></list></apply><apply id="S2.E3.m1.9.9.3.2.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1">superscript</csymbol><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1"><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3"><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5">𝑟</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3"><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5">𝛾</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6">𝑚</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7">𝑎</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8">subscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2">𝑥</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2">𝑎</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3">′</ci></apply></apply><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9">𝑄</ci><vector id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3"><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3">′</ci></apply><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2">𝑎</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3">′</ci></apply><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3">superscript</csymbol><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2">𝜃</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3">𝑖</ci></apply></apply></vector></apply></apply><apply
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4"><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3">𝑄</ci><vector
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1"><ci
    id="S2.E3.m1.5.5.cmml" xref="S2.E3.m1.5.5">𝑠</ci><ci id="S2.E3.m1.6.6.cmml" xref="S2.E3.m1.6.6">𝑎</ci><apply
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1"><csymbol
    cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.1.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1">subscript</csymbol><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2">𝜃</ci><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3">𝑖</ci></apply></vector></apply></apply><cn
    type="integer" id="S2.E3.m1.9.9.3.2.2.1.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.3">2</cn></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E3.m1.9c">L_{i}(\theta_{i})=\operatorname{\mathbb{E}}_{s,a,r,s^{\prime}}[(r+\gamma
    max_{a^{\prime}}Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]</annotation></semantics></math>
    |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: 'where target network parameters <math id="S2.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math"
    alttext="\theta^{-}" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><msup
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">θ</mi><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2">𝜃</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">\theta^{-}</annotation></semantics></math>
    are set to Q-network parameters <math id="S2.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math"
    alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mi
    id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p2.2.m2.1c">\theta</annotation></semantics></math> periodically
    and mini-batches of <math id="S2.SS2.SSS0.Px1.p2.3.m3.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.3.m3.4a"><mrow
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">⟨</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.2.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml">a</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.4" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.3.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml">r</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.5" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><msup
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.6" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.3.m3.4b"><list id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1"><ci id="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3">𝑟</ci><apply id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.3.m3.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    tuples are sampled from the ER buffer, as depicted in Figure [3](#S2.F3 "Figure
    3 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The ER buffer provides stability for learning as random batches sampled from
    the buffer helps alleviating the problems caused by the non-i.i.d. data. However,
    it comes with disadvantages, such as higher memory requirements and computation
    per real interaction [[93](#bib.bib93)]. The ER buffer is mainly used for off-policy
    RL methods as it can cause a mismatch between buffer content from earlier policy
    and from the current policy for on-policy methods [[93](#bib.bib93)]. Extending
    the ER buffer for the multiagent case is not trivial, see Sections [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”"), [4.1](#S4.SS1 "4.1 Avoiding
    deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”") and [4.2](#S4.SS2 "4.2 Lessons learned
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”").
    Recent works were designed to reduce the problem of catastrophic forgetting (this
    occurs when the trained neural network performs poorly on previously learned tasks
    due to a non-stationary training distribution [[94](#bib.bib94), [95](#bib.bib95)])
    and the ER buffer, in DRL [[96](#bib.bib96)] and MDRL [[97](#bib.bib97)].'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ER 缓冲区为学习提供了稳定性，因为从缓冲区随机抽样的批次有助于缓解由非独立同分布数据造成的问题。然而，它也有一些缺点，例如更高的内存需求和每次真实交互的计算量 [[93](#bib.bib93)]。ER
    缓冲区主要用于离线策略 RL 方法，因为它可能导致早期策略与当前策略之间的缓冲区内容不匹配，对于在线策略方法 [[93](#bib.bib93)]。将 ER
    缓冲区扩展到多智能体情况并非易事，请参见第 [3.5](#S3.SS5 "3.5 Learning cooperation ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")、[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") 和 [4.2](#S4.SS2 "4.2 Lessons
    learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") 部分。近期的研究旨在减少灾难性遗忘的问题（当训练神经网络在之前学到的任务上表现不佳，由于训练分布非静态 [[94](#bib.bib94),
    [95](#bib.bib95)]），以及 ER 缓冲区在 DRL [[96](#bib.bib96)] 和 MDRL [[97](#bib.bib97)]
    中的应用。'
- en: 'DQN has been extended in many ways, for example, by using double estimators [[91](#bib.bib91)]
    to reduce the overestimation bias with Double DQN [[92](#bib.bib92)] (see Section
    [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and by decomposing the
    Q-function with a *dueling*-DQN architecture [[98](#bib.bib98)], where two streams
    are learned, one estimates state values and another one advantages, those are
    combined in the final layer to form <math id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1a"><mi
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1b"><ci id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1c">Q</annotation></semantics></math> values (this
    method improved over Double DQN).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In practice, DQN is trained using an input of four stacked frames (last four
    frames the agent has encountered). If a game requires a memory of more than four
    frames it will appear non-Markovian to DQN because the future game states (and
    rewards) do not depend only on the input (four frames) but rather on the history [[99](#bib.bib99)].
    Thus, DQN’s performance declines when given incomplete state observations (e.g.,
    one input frame) since DQN assumes full state observability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-world tasks often feature incomplete and noisy state information resulting
    from *partial observability* (see Section [2.1](#S2.SS1 "2.1 Reinforcement learning
    ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")).
    Deep Recurrent Q-Networks (DRQN) [[85](#bib.bib85)] proposed using *recurrent
    neural networks*, in particular, Long Short-Term Memory (LSTMs) cells [[86](#bib.bib86)]
    in DQN, for this setting. Consider the architecture in Figure [2](#S2.F2 "Figure
    2 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") with the first dense layer
    after convolution replaced by a layer of LSTM cells. With this addition, DRQN
    has memory capacity so that it can even work with only one input frame rather
    than a stacked input of consecutive frames. This idea has been extended to MDRL,
    see Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and Section [4.2](#S4.SS2 "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”"). There are also other approaches
    to deal with partial observability such as finite state controllers [[100](#bib.bib100)]
    (where action selection is performed according to the complete observation history)
    and using an initiation set of options conditioned on the previously employed
    option [[101](#bib.bib101)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c1c2a9ce10012396bd49fcdd20cc483d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Representation of a DQN agent that uses an experience replay buffer [[89](#bib.bib89),
    [90](#bib.bib90)] to keep <math id="S2.F3.2.1.m1.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.F3.2.1.m1.4b"><mrow
    id="S2.F3.2.1.m1.4.4.1" xref="S2.F3.2.1.m1.4.4.2.cmml"><mo stretchy="false" id="S2.F3.2.1.m1.4.4.1.2"
    xref="S2.F3.2.1.m1.4.4.2.cmml">⟨</mo><mi id="S2.F3.2.1.m1.1.1" xref="S2.F3.2.1.m1.1.1.cmml">s</mi><mo
    id="S2.F3.2.1.m1.4.4.1.3" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi id="S2.F3.2.1.m1.2.2"
    xref="S2.F3.2.1.m1.2.2.cmml">a</mi><mo id="S2.F3.2.1.m1.4.4.1.4" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi
    id="S2.F3.2.1.m1.3.3" xref="S2.F3.2.1.m1.3.3.cmml">r</mi><mo id="S2.F3.2.1.m1.4.4.1.5"
    xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><msup id="S2.F3.2.1.m1.4.4.1.1" xref="S2.F3.2.1.m1.4.4.1.1.cmml"><mi
    id="S2.F3.2.1.m1.4.4.1.1.2" xref="S2.F3.2.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.F3.2.1.m1.4.4.1.1.3"
    xref="S2.F3.2.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S2.F3.2.1.m1.4.4.1.6"
    xref="S2.F3.2.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S2.F3.2.1.m1.4c"><list id="S2.F3.2.1.m1.4.4.2.cmml" xref="S2.F3.2.1.m1.4.4.1"><ci
    id="S2.F3.2.1.m1.1.1.cmml" xref="S2.F3.2.1.m1.1.1">𝑠</ci><ci id="S2.F3.2.1.m1.2.2.cmml"
    xref="S2.F3.2.1.m1.2.2">𝑎</ci><ci id="S2.F3.2.1.m1.3.3.cmml" xref="S2.F3.2.1.m1.3.3">𝑟</ci><apply
    id="S2.F3.2.1.m1.4.4.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1"><csymbol cd="ambiguous"
    id="S2.F3.2.1.m1.4.4.1.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S2.F3.2.1.m1.4.4.1.1.2.cmml" xref="S2.F3.2.1.m1.4.4.1.1.2">𝑠</ci><ci id="S2.F3.2.1.m1.4.4.1.1.3.cmml"
    xref="S2.F3.2.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F3.2.1.m1.4d">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    tuples for minibatch updates. The Q-values are parameterized with a NN and a policy
    is obtained by selecting (greedily) over those at every timestep.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient methods
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For many tasks, particularly for physical control, the action space is continuous
    and high dimensional where DQN is not suitable. Deep Deterministic Policy Gradient
    (DDPG) [[65](#bib.bib65)] is a model-free off-policy actor-critic algorithm for
    such domains, based on the DPG algorithm [[61](#bib.bib61)] (see Section [2.1](#S2.SS1
    "2.1 Reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). Additionally, it proposes a new method for
    updating the networks, i.e., the target network parameters slowly change (this
    could also be applicable to DQN), in contrast to the hard reset (direct weight
    copy) used in DQN. Given the off-policy nature, DDPG generates exploratory behavior
    by adding sampled noise from some noise processes to its actor policy. The authors
    also used batch normalization [[102](#bib.bib102)] to ensure generalization across
    many different tasks without performing manual normalizations. However, note that
    other works have shown batch normalization can cause divergence in DRL [[103](#bib.bib103),
    [104](#bib.bib104)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous Advantage Actor-Critic (A3C) [[93](#bib.bib93)] is an algorithm
    that employs a *parallelized* asynchronous training scheme (using multiple CPU
    threads) for efficiency. It is an on-policy RL method that does not use an experience
    replay buffer. A3C allows multiple workers to simultaneously interact with the
    environment and compute gradients locally. All the workers pass their computed
    local gradients to a global NN which performs the optimization and synchronizes
    with the workers asynchronously (see Figure [4](#S2.F4 "Figure 4 ‣ Policy gradient
    methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")). There is also the Advantage Actor-Critic
    (A2C) method [[105](#bib.bib105)] that combines all the gradients from all the
    workers to update the global NN *synchronously*. The loss function for A3C is
    composed of two terms: policy loss (actor), <math id="S2.SS2.SSS0.Px2.p2.1.m1.1"
    class="ltx_Math" alttext="\mathcal{L}_{\pi}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.1.m1.1a"><msub
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">π</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.1.m1.1c">\mathcal{L}_{\pi}</annotation></semantics></math>,
    and value loss (critic), <math id="S2.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math"
    alttext="\mathcal{L}_{v}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.2.m2.1a"><msub
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.2.m2.1c">\mathcal{L}_{v}</annotation></semantics></math>.
    A3C parameters are updated using the *advantage* function <math id="S2.SS2.SSS0.Px2.p2.3.m3.6"
    class="ltx_Math" alttext="A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)" display="inline"><semantics
    id="S2.SS2.SSS0.Px2.p2.3.m3.6a"><mrow id="S2.SS2.SSS0.Px2.p2.3.m3.6.6" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.4.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">(</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml">s</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2.cmml">a</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.6" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">;</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2.cmml">θ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3.cmml">v</mi></msub><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.7" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.4.cmml">=</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.cmml"><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.1.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">)</mo></mrow></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.1.cmml">−</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2.cmml">V</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.1.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.3.3.cmml">s</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.3.m3.6b"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3"><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5">𝐴</ci><vector id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2">𝜃</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3">𝑣</ci></apply></vector></apply><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5"><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2">𝑄</ci><interval
    closure="open" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.1.1">𝑠</ci><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.2.2">𝑎</ci></interval></apply><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2">𝑉</ci><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.3.m3.6c">A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)</annotation></semantics></math>,
    commonly used to reduce variance (see Section [2.1](#S2.SS1 "2.1 Reinforcement
    learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). An entropy loss for the policy, <math id="S2.SS2.SSS0.Px2.p2.4.m4.1"
    class="ltx_Math" alttext="H(\pi)" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.4.m4.1a"><mrow
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"><mi id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.1.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.1.cmml">π</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2"><ci id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2">𝐻</ci><ci id="S2.SS2.SSS0.Px2.p2.4.m4.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.1">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.4.m4.1c">H(\pi)</annotation></semantics></math>,
    is also commonly added, which helps to improve exploration by discouraging premature
    convergence to suboptimal deterministic policies [[93](#bib.bib93)]. Thus, the
    loss function is given by: <math id="S2.SS2.SSS0.Px2.p2.5.m5.3" class="ltx_math_unparsed"
    alttext="\mathcal{L}_{\text{A3C}}=\lambda_{v}\mathcal{L}_{v}+\lambda_{\pi}\mathcal{L}_{\pi}-\lambda_{H}\mathbb{E}_{s\sim\pi}[H(\pi(s,\cdot,\theta)]"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.5.m5.3a"><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3b"><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.4"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.4.2">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.4.3">A3C</mtext></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.5">=</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.6"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.6.2">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.6.3">v</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.7"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.7.2">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.7.3">v</mi></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.8">+</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.9"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.9.2">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.9.3">π</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.10"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.10.2">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.10.3">π</mi></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.11">−</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.12"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.12.2">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.12.3">H</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.13"><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.2">𝔼</mi><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3"><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.2">s</mi><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.1">∼</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.3">π</mi></mrow></msub><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.14"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.1">[</mo><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.2">H</mi><mrow
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3"><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.1">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.2">π</mi><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.1">(</mo><mi id="S2.SS2.SSS0.Px2.p2.5.m5.1.1">s</mi><mo
    rspace="0em" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.2">,</mo><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p2.5.m5.2.2">⋅</mo><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.3">,</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.3">θ</mi><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.4">)</mo></mrow><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.4">]</mo></mrow></mrow></mrow><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.5.m5.3c">\mathcal{L}_{\text{A3C}}=\lambda_{v}\mathcal{L}_{v}+\lambda_{\pi}\mathcal{L}_{\pi}-\lambda_{H}\mathbb{E}_{s\sim\pi}[H(\pi(s,\cdot,\theta)]</annotation></semantics></math>
    with <math id="S2.SS2.SSS0.Px2.p2.6.m6.1" class="ltx_Math" alttext="\lambda_{v},\lambda_{\pi},"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.6.m6.1a"><mrow id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1"><mrow
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml">v</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2.cmml">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3.cmml">π</mi></msub></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.6.m6.1b"><list id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2"><apply id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2">𝜆</ci><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2">𝜆</ci><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3">𝜋</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.6.m6.1c">\lambda_{v},\lambda_{\pi},</annotation></semantics></math>
    and <math id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1" class="ltx_Math" alttext="\lambda_{H}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2.cmml">λ</mi><mi id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2">𝜆</ci><ci id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1c">\lambda_{H}</annotation></semantics></math>,
    being weighting terms on the individual loss components. Wang et al. [[106](#bib.bib106)]
    took A3C’s framework but used off-policy learning to create the Actor-critic with
    experience replay (ACER) algorithm. Gu et al. [[107](#bib.bib107)] introduced
    the Interpolated Policy Gradient (IPG) algorithm and showed a connection between
    ACER and DDPG: they are a pair of reparametrization terms (they are special cases
    of IPG) when they are put under the same stochastic policy setting, and when the
    policy is deterministic they collapse into DDPG.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07d5c0b6b6c365bbbd09b85f59844ef8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Asynchronous Advantage Actor-Critic (A3C) employs multiple (CPUs)
    workers without needing an ER buffer. Each worker has its own NN and independently
    interacts with the environment to compute the loss and gradients. Workers then
    pass computed gradients to the global NN that optimizes the parameters and synchronizes
    with the worker *asynchronously*. This distributed system is designed for single-agent
    deep RL. Compared to different DQN variants, A3C obtains better performance on
    a variety of Atari games using substantially less training time with multiple
    CPU cores of standard laptops without a GPU [[93](#bib.bib93)]. However, we note
    that more recent approaches use both multiple CPU cores for more efficient training
    data generation and GPUs for more efficient learning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Jaderberg et al. [[84](#bib.bib84)] built the Unsupervised Reinforcement and
    Auxiliary Learning (UNREAL) framework on top of A3C and introduced unsupervised
    *auxiliary tasks* (e.g., reward prediction) to speed up the learning process.
    Auxiliary tasks in general are not used for anything other than shaping the features
    of the agent, i.e., facilitating and regularizing the representation learning
    process [[108](#bib.bib108), [109](#bib.bib109)]; their formalization in RL is
    related to the concept of *general value functions* [[20](#bib.bib20), [110](#bib.bib110)].
    The UNREAL framework optimizes a combined loss function <math id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1"
    class="ltx_Math" alttext="\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1a"><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml">UNREAL</mtext></msub><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1.cmml">≈</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml">A3C</mtext></msub><mo rspace="0.055em"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1.cmml">+</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml"><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2.cmml">∑</mo><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml">i</mi></msub><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml">λ</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1.cmml">​</mo><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2.cmml">T</mi><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3.cmml">i</mi></msub></mrow></msub><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2.cmml">ℒ</mi><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2.cmml">A</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3.cmml">i</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"><apply id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3">UNREAL</mtext></ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2">ℒ</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"><mtext
    mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3">A3C</mtext></ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3">𝑖</ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2">𝜆</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3"><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2">𝐴</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2">𝑇</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3">𝑖</ci></apply></apply></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2">ℒ</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3"><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2">𝐴</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2">𝑇</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1c">\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}</annotation></semantics></math>,
    that combines the A3C loss, <math id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1" class="ltx_Math"
    alttext="\mathcal{L}_{\text{A3C}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1a"><msub
    id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2.cmml">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3a.cmml">A3C</mtext></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3a.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3">A3C</mtext></ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1c">\mathcal{L}_{\text{A3C}}</annotation></semantics></math>,
    together with auxiliary task losses <math id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1"
    class="ltx_Math" alttext="\mathcal{L}_{AT_{i}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1a"><msub
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2.cmml">ℒ</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3.cmml">i</mi></msub></mrow></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1b"><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2">ℒ</ci><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3"><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2">𝐴</ci><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2">𝑇</ci><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1c">\mathcal{L}_{AT_{i}}</annotation></semantics></math>,
    where <math id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1" class="ltx_Math" alttext="\lambda_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1a"><msub id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2.cmml">λ</mi><mrow id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3.cmml">i</mi></msub></mrow></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2">𝜆</ci><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3"><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2">𝐴</ci><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2">𝑇</ci><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1c">\lambda_{AT_{i}}</annotation></semantics></math>
    are weight terms (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") for use of auxiliary tasks in MDRL). In contrast to A3C, UNREAL
    uses a prioritized ER buffer, in which transitions with positive reward are given
    higher probability of being sampled. This approach can be viewed as a simple form
    of prioritized replay [[111](#bib.bib111)], which was in turn inspired by model-based
    RL algorithms like prioritized sweeping [[112](#bib.bib112), [113](#bib.bib113)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Another distributed architecture is the Importance Weighted Actor-Learner Architecture
    (IMPALA) [[114](#bib.bib114)]. Unlike A3C or UNREAL, IMPALA actors communicate
    *trajectories of experience* (sequences of states, actions, and rewards) to a
    centralized learner, thus IMPALA decouples acting from learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Trust Region Policy Optimization (TRPO) [[60](#bib.bib60)] and Proximal Policy
    Optimization (PPO) [[115](#bib.bib115)] are recently proposed policy gradient
    algorithms where the latter represents the state-of-the art with advantages such
    as being simpler to implement and having better empirical sample complexity. Interestingly,
    a recent work [[79](#bib.bib79)] studying PPO and TRPO arrived at the surprising
    conclusion that these methods often deviate from what the theoretical framework
    would predict: gradient estimates are poorly correlated with the true gradient
    and value networks tend to produce inaccurate predictions for the true value function.
    Compared to vanilla policy gradient algorithms, PPO prevents abrupt changes in
    policies during training through the loss function, similar to early work by Kakade [[116](#bib.bib116)].
    Another advantage of PPO is that it can be used in a distributed fashion, i.e,
    Distributed PPO (DPPO) [[117](#bib.bib117)]. Note that *distributed approaches*
    like DPPO or A3C use parallelization only to improve the learning by more efficient
    training data generation through multiple CPU cores for single agent DRL and they
    should not be considered multiagent approaches (except for recent work which tries
    to exploit this parallelization in a multiagent environment [[118](#bib.bib118)]).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, there’s a connection between policy gradient algorithms and Q-learning [[119](#bib.bib119)]
    within the framework of entropy-regularized reinforcement learning [[120](#bib.bib120)]
    where the value and <math id="S2.SS2.SSS0.Px2.p6.1.m1.1" class="ltx_Math" alttext="Q"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p6.1.m1.1a"><mi id="S2.SS2.SSS0.Px2.p6.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p6.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p6.1.m1.1c">Q</annotation></semantics></math>
    functions are slightly altered to consider the entropy of the policy. In this
    vein, Soft Actor-Critic (SAC) [[121](#bib.bib121)] is a recent algorithm that
    concurrently learns a stochastic policy, two Q-functions (taking inspiration from
    Double Q-learning) and a value function. SAC alternates between collecting experience
    with the current policy and updating from batches sampled from the ER buffer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: We have reviewed recent algorithms in DRL, while the list is not exhaustive,
    it provides an overview of the different state-of-art techniques and algorithms
    which will become useful while describing the MDRL techniques in the next section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 3 Multiagent Deep Reinforcement Learning (MDRL)
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we briefly introduce the general framework on multiagent learning and
    then we dive into the categories and the research on MDRL.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Multiagent Learning
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning in a multiagent environment is inherently more complex than in the
    single-agent case, as agents interact at the same time with environment and potentially
    with each other [[5](#bib.bib5)]. The *independent* learners, a.k.a. *decentralized*
    learners approach [[122](#bib.bib122)] directly uses single-agent algorithms in
    the multi-agent setting despite the underlying assumptions of these algorithms
    being violated (each agent independently learns its own policy, treating other
    agents as part of the environment). In particular the *Markov property* (the future
    dynamics, transitions, and rewards depend only on the current state) becomes invalid
    since the environment is no longer stationary [[4](#bib.bib4), [6](#bib.bib6),
    [123](#bib.bib123)]. This approach ignores the multiagent nature of the setting
    entirely and it can fail when an opponent adapts or learns, for example, based
    on the past history of interactions [[2](#bib.bib2)]. Despite the lack of guarantees,
    independent learners have been used in practice, providing advantages with regards
    to scalability while often achieving good results [[8](#bib.bib8)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: To understand why multiagent domains are non-stationary from agents’ local perspectives,
    consider a simple stochastic (also known as Markov) game <math id="S3.SS1.p2.1.m1.5"
    class="ltx_Math" alttext="(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})"
    display="inline"><semantics id="S3.SS1.p2.1.m1.5a"><mrow id="S3.SS1.p2.1.m1.5.6.2"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.1"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1"
    xref="S3.SS1.p2.1.m1.1.1.cmml">𝒮</mi><mo id="S3.SS1.p2.1.m1.5.6.2.2" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">𝒩</mi><mo
    id="S3.SS1.p2.1.m1.5.6.2.3" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml">𝒜</mi><mo id="S3.SS1.p2.1.m1.5.6.2.4"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.4.4"
    xref="S3.SS1.p2.1.m1.4.4.cmml">𝒯</mi><mo id="S3.SS1.p2.1.m1.5.6.2.5" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.5.5" xref="S3.SS1.p2.1.m1.5.5.cmml">ℛ</mi><mo
    stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.6" xref="S3.SS1.p2.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.1.m1.5b"><vector id="S3.SS1.p2.1.m1.5.6.1.cmml"
    xref="S3.SS1.p2.1.m1.5.6.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝒮</ci><ci
    id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">𝒩</ci><ci id="S3.SS1.p2.1.m1.3.3.cmml"
    xref="S3.SS1.p2.1.m1.3.3">𝒜</ci><ci id="S3.SS1.p2.1.m1.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4">𝒯</ci><ci
    id="S3.SS1.p2.1.m1.5.5.cmml" xref="S3.SS1.p2.1.m1.5.5">ℛ</ci></vector></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.1.m1.5c">(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})</annotation></semantics></math>,
    which can be seen as an extension of an MDP to multiple agents [[124](#bib.bib124),
    [125](#bib.bib125)]. One key distinction is that the transition, <math id="S3.SS1.p2.2.m2.1"
    class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝒯</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml"
    xref="S3.SS1.p2.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p2.2.m2.1c">\mathcal{T}</annotation></semantics></math>, and reward
    function, <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics
    id="S3.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1"
    xref="S3.SS1.p2.3.m3.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ℛ</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{R}</annotation></semantics></math>,
    depend on the actions <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1"
    xref="S3.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.2"
    xref="S3.SS1.p2.4.m4.1.1.2.cmml">𝒜</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><msub id="S3.SS1.p2.4.m4.1.1.3.2"
    xref="S3.SS1.p2.4.m4.1.1.3.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2.2" xref="S3.SS1.p2.4.m4.1.1.3.2.2.cmml">A</mi><mn
    id="S3.SS1.p2.4.m4.1.1.3.2.3" xref="S3.SS1.p2.4.m4.1.1.3.2.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.1" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">×</mo><mi
    mathvariant="normal" id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.4.m4.1.1.3.4" xref="S3.SS1.p2.4.m4.1.1.3.4.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.4.2"
    xref="S3.SS1.p2.4.m4.1.1.3.4.2.cmml">A</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.4.m4.1.1.3.4.3" xref="S3.SS1.p2.4.m4.1.1.3.4.3.cmml">𝒩</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml"
    xref="S3.SS1.p2.4.m4.1.1"><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝒜</ci><apply
    id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><apply id="S3.SS1.p2.4.m4.1.1.3.2.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.2.1.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.2.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2.2">𝐴</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.2.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p2.4.m4.1.1.3.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.3">…</ci><apply id="S3.SS1.p2.4.m4.1.1.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4"><csymbol
    cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4">subscript</csymbol><ci
    id="S3.SS1.p2.4.m4.1.1.3.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4.2">𝐴</ci><ci id="S3.SS1.p2.4.m4.1.1.3.4.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.4.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}</annotation></semantics></math> of all, <math id="S3.SS1.p2.5.m5.1"
    class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">𝒩</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml"
    xref="S3.SS1.p2.5.m5.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p2.5.m5.1c">\mathcal{N}</annotation></semantics></math>, agents, this
    means, <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{R}=R_{1}\times...\times
    R_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1"
    xref="S3.SS1.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.2"
    xref="S3.SS1.p2.6.m6.1.1.2.cmml">ℛ</mi><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml"><msub id="S3.SS1.p2.6.m6.1.1.3.2"
    xref="S3.SS1.p2.6.m6.1.1.3.2.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.2.2" xref="S3.SS1.p2.6.m6.1.1.3.2.2.cmml">R</mi><mn
    id="S3.SS1.p2.6.m6.1.1.3.2.3" xref="S3.SS1.p2.6.m6.1.1.3.2.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.1" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">×</mo><mi
    mathvariant="normal" id="S3.SS1.p2.6.m6.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.1a" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.6.m6.1.1.3.4" xref="S3.SS1.p2.6.m6.1.1.3.4.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.4.2"
    xref="S3.SS1.p2.6.m6.1.1.3.4.2.cmml">R</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.6.m6.1.1.3.4.3" xref="S3.SS1.p2.6.m6.1.1.3.4.3.cmml">𝒩</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml"
    xref="S3.SS1.p2.6.m6.1.1"><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ℛ</ci><apply
    id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"><apply id="S3.SS1.p2.6.m6.1.1.3.2.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.2.1.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.2.2.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2.2">𝑅</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.2.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p2.6.m6.1.1.3.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.3">…</ci><apply id="S3.SS1.p2.6.m6.1.1.3.4.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4"><csymbol
    cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.4.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4">subscript</csymbol><ci
    id="S3.SS1.p2.6.m6.1.1.3.4.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4.2">𝑅</ci><ci id="S3.SS1.p2.6.m6.1.1.3.4.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.4.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{R}=R_{1}\times...\times
    R_{\mathcal{N}}</annotation></semantics></math> and <math id="S3.SS1.p2.7.m7.1"
    class="ltx_Math" alttext="\mathcal{T}=\mathcal{S}\times A_{1}\times...\times A_{\mathcal{N}}"
    display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1"
    xref="S3.SS1.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1.2"
    xref="S3.SS1.p2.7.m7.1.1.2.cmml">𝒯</mi><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">𝒮</mi><mo lspace="0.222em"
    rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.3.2"
    xref="S3.SS1.p2.7.m7.1.1.3.3.2.cmml">A</mi><mn id="S3.SS1.p2.7.m7.1.1.3.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1a" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">×</mo><mi
    mathvariant="normal" id="S3.SS1.p2.7.m7.1.1.3.4" xref="S3.SS1.p2.7.m7.1.1.3.4.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1b" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.7.m7.1.1.3.5" xref="S3.SS1.p2.7.m7.1.1.3.5.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.5.2"
    xref="S3.SS1.p2.7.m7.1.1.3.5.2.cmml">A</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.7.m7.1.1.3.5.3" xref="S3.SS1.p2.7.m7.1.1.3.5.3.cmml">𝒩</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml"
    xref="S3.SS1.p2.7.m7.1.1"><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝒯</ci><apply
    id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.2">𝒮</ci><apply id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3"><csymbol
    cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">subscript</csymbol><ci
    id="S3.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2">𝐴</ci><cn type="integer"
    id="S3.SS1.p2.7.m7.1.1.3.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.3">1</cn></apply><ci
    id="S3.SS1.p2.7.m7.1.1.3.4.cmml" xref="S3.SS1.p2.7.m7.1.1.3.4">…</ci><apply id="S3.SS1.p2.7.m7.1.1.3.5.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.5.1.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.5.2.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5.2">𝐴</ci><ci id="S3.SS1.p2.7.m7.1.1.3.5.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.5.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathcal{T}=\mathcal{S}\times
    A_{1}\times...\times A_{\mathcal{N}}</annotation></semantics></math>.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Given a learning agent <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="i"
    display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1"
    xref="S3.SS1.p3.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">i</annotation></semantics></math>
    and using the common shorthand notation <math id="S3.SS1.p3.2.m2.1" class="ltx_Math"
    alttext="\bm{-i}=\mathcal{N}\setminus\{i\}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow
    id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mrow id="S3.SS1.p3.2.m2.1.2.2"
    xref="S3.SS1.p3.2.m2.1.2.2.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.SS1.p3.2.m2.1.2.2a" xref="S3.SS1.p3.2.m2.1.2.2.cmml">−</mo><mi id="S3.SS1.p3.2.m2.1.2.2.2"
    xref="S3.SS1.p3.2.m2.1.2.2.2.cmml">𝒊</mi></mrow><mo id="S3.SS1.p3.2.m2.1.2.1"
    xref="S3.SS1.p3.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS1.p3.2.m2.1.2.3" xref="S3.SS1.p3.2.m2.1.2.3.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.2.3.2" xref="S3.SS1.p3.2.m2.1.2.3.2.cmml">𝒩</mi><mo
    id="S3.SS1.p3.2.m2.1.2.3.1" xref="S3.SS1.p3.2.m2.1.2.3.1.cmml">∖</mo><mrow id="S3.SS1.p3.2.m2.1.2.3.3.2"
    xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.2.3.3.2.1"
    xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">{</mo><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">i</mi><mo
    stretchy="false" id="S3.SS1.p3.2.m2.1.2.3.3.2.2" xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2"><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2"><ci
    id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2">𝒊</ci></apply><apply
    id="S3.SS1.p3.2.m2.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.2.3"><ci id="S3.SS1.p3.2.m2.1.2.3.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2.3.2">𝒩</ci><set id="S3.SS1.p3.2.m2.1.2.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.2.3.3.2"><ci
    id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑖</ci></set></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\bm{-i}=\mathcal{N}\setminus\{i\}</annotation></semantics></math>
    for the set of opponents, the value function now depends on the joint action <math
    id="S3.SS1.p3.3.m3.2" class="ltx_Math" alttext="\bm{a}=(a_{i},\bm{a_{-i}})" display="inline"><semantics
    id="S3.SS1.p3.3.m3.2a"><mrow id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.4" xref="S3.SS1.p3.3.m3.2.2.4.cmml">𝒂</mi><mo id="S3.SS1.p3.3.m3.2.2.3"
    xref="S3.SS1.p3.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml"><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">(</mo><msub
    id="S3.SS1.p3.3.m3.1.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.2"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.3"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.3.m3.2.2.2.2.4"
    xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.3.m3.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml">𝒂</mi><mrow
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p3.3.m3.2.2.2.2.2.3a" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml">−</mo><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.5" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.3.m3.2b"><apply id="S3.SS1.p3.3.m3.2.2.cmml"
    xref="S3.SS1.p3.3.m3.2.2"><ci id="S3.SS1.p3.3.m3.2.2.4.cmml" xref="S3.SS1.p3.3.m3.2.2.4">𝒂</ci><interval
    closure="open" id="S3.SS1.p3.3.m3.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2"><apply
    id="S3.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1">subscript</csymbol><ci
    id="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.SS1.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2"><csymbol cd="ambiguous"
    id="S3.SS1.p3.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2">subscript</csymbol><ci
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2">𝒂</ci><apply
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3"><ci id="S3.SS1.p3.3.m3.2.2.2.2.2.3.2.cmml"
    xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.2">𝒊</ci></apply></apply></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.3.m3.2c">\bm{a}=(a_{i},\bm{a_{-i}})</annotation></semantics></math>,
    and the joint policy <math id="S3.SS1.p3.4.m4.4" class="ltx_Math" alttext="\bm{\pi}(s,\bm{a})=\prod_{j}\pi_{j}(s,a_{j})"
    display="inline"><semantics id="S3.SS1.p3.4.m4.4a"><mrow id="S3.SS1.p3.4.m4.4.4"
    xref="S3.SS1.p3.4.m4.4.4.cmml"><mrow id="S3.SS1.p3.4.m4.4.4.3" xref="S3.SS1.p3.4.m4.4.4.3.cmml"><mi
    id="S3.SS1.p3.4.m4.4.4.3.2" xref="S3.SS1.p3.4.m4.4.4.3.2.cmml">𝝅</mi><mo lspace="0em"
    rspace="0em" id="S3.SS1.p3.4.m4.4.4.3.1" xref="S3.SS1.p3.4.m4.4.4.3.1.cmml">​</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.3.3.2" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml"><mo stretchy="false"
    id="S3.SS1.p3.4.m4.4.4.3.3.2.1" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">(</mo><mi
    id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">s</mi><mo id="S3.SS1.p3.4.m4.4.4.3.3.2.2"
    xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">,</mo><mi id="S3.SS1.p3.4.m4.2.2" xref="S3.SS1.p3.4.m4.2.2.cmml">𝒂</mi><mo
    stretchy="false" id="S3.SS1.p3.4.m4.4.4.3.3.2.3" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo
    rspace="0.111em" id="S3.SS1.p3.4.m4.4.4.2" xref="S3.SS1.p3.4.m4.4.4.2.cmml">=</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.1" xref="S3.SS1.p3.4.m4.4.4.1.cmml"><msub id="S3.SS1.p3.4.m4.4.4.1.2"
    xref="S3.SS1.p3.4.m4.4.4.1.2.cmml"><mo id="S3.SS1.p3.4.m4.4.4.1.2.2" xref="S3.SS1.p3.4.m4.4.4.1.2.2.cmml">∏</mo><mi
    id="S3.SS1.p3.4.m4.4.4.1.2.3" xref="S3.SS1.p3.4.m4.4.4.1.2.3.cmml">j</mi></msub><mrow
    id="S3.SS1.p3.4.m4.4.4.1.1" xref="S3.SS1.p3.4.m4.4.4.1.1.cmml"><msub id="S3.SS1.p3.4.m4.4.4.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.4.4.1.1.3.2" xref="S3.SS1.p3.4.m4.4.4.1.1.3.2.cmml">π</mi><mi
    id="S3.SS1.p3.4.m4.4.4.1.1.3.3" xref="S3.SS1.p3.4.m4.4.4.1.1.3.3.cmml">j</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.4.4.1.1.2" xref="S3.SS1.p3.4.m4.4.4.1.1.2.cmml">​</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1" xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml"><mo stretchy="false"
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.2" xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">(</mo><mi
    id="S3.SS1.p3.4.m4.3.3" xref="S3.SS1.p3.4.m4.3.3.cmml">s</mi><mo id="S3.SS1.p3.4.m4.4.4.1.1.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS1.p3.4.m4.4.4.1.1.1.1.4"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.4.m4.4b"><apply id="S3.SS1.p3.4.m4.4.4.cmml"
    xref="S3.SS1.p3.4.m4.4.4"><apply id="S3.SS1.p3.4.m4.4.4.3.cmml" xref="S3.SS1.p3.4.m4.4.4.3"><ci
    id="S3.SS1.p3.4.m4.4.4.3.2.cmml" xref="S3.SS1.p3.4.m4.4.4.3.2">𝝅</ci><interval
    closure="open" id="S3.SS1.p3.4.m4.4.4.3.3.1.cmml" xref="S3.SS1.p3.4.m4.4.4.3.3.2"><ci
    id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑠</ci><ci id="S3.SS1.p3.4.m4.2.2.cmml"
    xref="S3.SS1.p3.4.m4.2.2">𝒂</ci></interval></apply><apply id="S3.SS1.p3.4.m4.4.4.1.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1"><apply id="S3.SS1.p3.4.m4.4.4.1.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2"><csymbol
    cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.2.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2">subscript</csymbol><csymbol
    cd="latexml" id="S3.SS1.p3.4.m4.4.4.1.2.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2.2">product</csymbol><ci
    id="S3.SS1.p3.4.m4.4.4.1.2.3.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2.3">𝑗</ci></apply><apply
    id="S3.SS1.p3.4.m4.4.4.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1"><apply id="S3.SS1.p3.4.m4.4.4.1.1.3.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.1.3.1.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.4.m4.4.4.1.1.3.2.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.2">𝜋</ci><ci id="S3.SS1.p3.4.m4.4.4.1.1.3.3.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.3">𝑗</ci></apply><interval closure="open" id="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1"><ci id="S3.SS1.p3.4.m4.3.3.cmml" xref="S3.SS1.p3.4.m4.3.3">𝑠</ci><apply
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1">subscript</csymbol><ci
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3">𝑗</ci></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.4.m4.4c">\bm{\pi}(s,\bm{a})=\prod_{j}\pi_{j}(s,a_{j})</annotation></semantics></math>:⁷⁷7In
    this setting each agent independently executes a policy, however, there are other
    cases where this does not hold, for example when agents have a coordinated exploration
    strategy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.E4.m1.6" class="ltx_Math" alttext="V^{\bm{\pi}}_{i}(s)=\sum_{\bm{a}\in\mathcal{A}}\bm{\pi}(s,\bm{a})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V_{i}(s^{\prime})]." display="block"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6.1"
    xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow
    id="S3.E4.m1.6.6.1.1.6" xref="S3.E4.m1.6.6.1.1.6.cmml"><msubsup id="S3.E4.m1.6.6.1.1.6.2"
    xref="S3.E4.m1.6.6.1.1.6.2.cmml"><mi id="S3.E4.m1.6.6.1.1.6.2.2.2" xref="S3.E4.m1.6.6.1.1.6.2.2.2.cmml">V</mi><mi
    id="S3.E4.m1.6.6.1.1.6.2.3" xref="S3.E4.m1.6.6.1.1.6.2.3.cmml">i</mi><mi id="S3.E4.m1.6.6.1.1.6.2.2.3"
    xref="S3.E4.m1.6.6.1.1.6.2.2.3.cmml">𝝅</mi></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E4.m1.6.6.1.1.6.1" xref="S3.E4.m1.6.6.1.1.6.1.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.6.3.2"
    xref="S3.E4.m1.6.6.1.1.6.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.6.3.2.1"
    xref="S3.E4.m1.6.6.1.1.6.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">s</mi><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.6.3.2.2" xref="S3.E4.m1.6.6.1.1.6.cmml">)</mo></mrow></mrow><mo
    rspace="0.111em" id="S3.E4.m1.6.6.1.1.5" xref="S3.E4.m1.6.6.1.1.5.cmml">=</mo><mrow
    id="S3.E4.m1.6.6.1.1.4" xref="S3.E4.m1.6.6.1.1.4.cmml"><munder id="S3.E4.m1.6.6.1.1.4.5"
    xref="S3.E4.m1.6.6.1.1.4.5.cmml"><mo movablelimits="false" id="S3.E4.m1.6.6.1.1.4.5.2"
    xref="S3.E4.m1.6.6.1.1.4.5.2.cmml">∑</mo><mrow id="S3.E4.m1.6.6.1.1.4.5.3" xref="S3.E4.m1.6.6.1.1.4.5.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.5.3.2" xref="S3.E4.m1.6.6.1.1.4.5.3.2.cmml">𝒂</mi><mo id="S3.E4.m1.6.6.1.1.4.5.3.1"
    xref="S3.E4.m1.6.6.1.1.4.5.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.5.3.3" xref="S3.E4.m1.6.6.1.1.4.5.3.3.cmml">𝒜</mi></mrow></munder><mrow
    id="S3.E4.m1.6.6.1.1.4.4" xref="S3.E4.m1.6.6.1.1.4.4.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.6"
    xref="S3.E4.m1.6.6.1.1.4.4.6.cmml">𝝅</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.5"
    xref="S3.E4.m1.6.6.1.1.4.4.5.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.4.4.7.2" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.7.2.1" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">(</mo><mi
    id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.7.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">,</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">𝒂</mi><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.7.2.3" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.5a" xref="S3.E4.m1.6.6.1.1.4.4.5.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.cmml"><munder id="S3.E4.m1.6.6.1.1.4.4.4.5"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.cmml"><mo movablelimits="false" id="S3.E4.m1.6.6.1.1.4.4.4.5.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.2.cmml">∑</mo><mrow id="S3.E4.m1.6.6.1.1.4.4.4.5.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.cmml"><msup id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3.cmml">′</mo></msup><mo id="S3.E4.m1.6.6.1.1.4.4.4.5.3.1"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.3.cmml">𝒮</mi></mrow></munder><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.4.4.4.6" xref="S3.E4.m1.6.6.1.1.4.4.4.4.6.cmml">𝒯</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.5.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.4" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">(</mo><mi
    id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.5"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.6"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2.cmml">𝒂</mi><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3a" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.7" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msup
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.8" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.5a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.5.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml">[</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.cmml"><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.cmml"><msub
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2.cmml">R</mi><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.4.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">(</mo><mi
    id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.5"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.6"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2.cmml">𝒂</mi><mrow id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.7" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msup
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.8" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.5.cmml">+</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3.cmml">γ</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2.cmml">​</mo><msub
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2.cmml">V</mi><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml">(</mo><msup
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    lspace="0em" id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1"><apply
    id="S3.E4.m1.6.6.1.1.6.cmml" xref="S3.E4.m1.6.6.1.1.6"><apply id="S3.E4.m1.6.6.1.1.6.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.6.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2">subscript</csymbol><apply id="S3.E4.m1.6.6.1.1.6.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.6.2.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2">superscript</csymbol><ci id="S3.E4.m1.6.6.1.1.6.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2.2.2">𝑉</ci><ci id="S3.E4.m1.6.6.1.1.6.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.6.2.2.3">𝝅</ci></apply><ci
    id="S3.E4.m1.6.6.1.1.6.2.3.cmml" xref="S3.E4.m1.6.6.1.1.6.2.3">𝑖</ci></apply><ci
    id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑠</ci></apply><apply id="S3.E4.m1.6.6.1.1.4.cmml"
    xref="S3.E4.m1.6.6.1.1.4"><apply id="S3.E4.m1.6.6.1.1.4.5.cmml" xref="S3.E4.m1.6.6.1.1.4.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.5">subscript</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.5.3"><ci id="S3.E4.m1.6.6.1.1.4.5.3.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.5.3.2">𝒂</ci><ci id="S3.E4.m1.6.6.1.1.4.5.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.5.3.3">𝒜</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4"><ci id="S3.E4.m1.6.6.1.1.4.4.6.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.6">𝝅</ci><interval closure="open" id="S3.E4.m1.6.6.1.1.4.4.7.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.7.2"><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑠</ci><ci
    id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">𝒂</ci></interval><apply id="S3.E4.m1.6.6.1.1.4.4.4.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4"><apply id="S3.E4.m1.6.6.1.1.4.4.4.5.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5">subscript</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3">′</ci></apply><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.3">𝒮</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4"><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.6.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.6">𝒯</ci><vector id="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3"><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">𝑠</ci><apply
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2">𝒂</ci><apply
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3"><ci
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3">′</ci></apply></vector><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1"><csymbol
    cd="latexml" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.2">delimited-[]</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2">𝑅</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3">𝑖</ci></apply><vector
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3"><ci
    id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">𝑠</ci><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2">𝒂</ci><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3"><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3">′</ci></apply></vector></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4"><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3">𝛾</ci><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2">𝑉</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3">𝑖</ci></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3">′</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E4.m1.6c">V^{\bm{\pi}}_{i}(s)=\sum_{\bm{a}\in\mathcal{A}}\bm{\pi}(s,\bm{a})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V_{i}(s^{\prime})].</annotation></semantics></math> |  | (4) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: Consequently, the optimal policy is dependent on the other agents’ policies,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | <math id="S3.E5X.2.1.1.m1.6" class="ltx_Math" alttext="\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)="
    display="inline"><semantics id="S3.E5X.2.1.1.m1.6a"><mrow id="S3.E5X.2.1.1.m1.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.2" xref="S3.E5X.2.1.1.m1.6.6.2.cmml"><msubsup
    id="S3.E5X.2.1.1.m1.6.6.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.4.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml">i</mi><mo id="S3.E5X.2.1.1.m1.6.6.2.4.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.3.cmml">∗</mo></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.2.3" xref="S3.E5X.2.1.1.m1.6.6.2.3.cmml">​</mo><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml"><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.3.3" xref="S3.E5X.2.1.1.m1.3.3.cmml">s</mi><mo
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub
    id="S3.E5X.2.1.1.m1.5.5.1.1.1.1" xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml">a</mi><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.6.6.2.2.2.5"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.6.6.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml">𝝅</mi><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.6" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow><mo
    id="S3.E5X.2.1.1.m1.6.6.4" xref="S3.E5X.2.1.1.m1.6.6.4.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.6.6.5"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml"><munder
    id="S3.E5X.2.1.1.m1.6.6.5.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2.1.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1.cmml">​</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3.cmml">max</mi></mrow><msub
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3.cmml">i</mi></msub></munder><mo id="S3.E5X.2.1.1.m1.6.6.5.2a"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml">⁡</mo><msubsup id="S3.E5X.2.1.1.m1.6.6.5.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2.cmml">V</mi><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.2.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.3.cmml">i</mi><mrow
    id="S3.E5X.2.1.1.m1.2.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml"><mo stretchy="false"
    id="S3.E5X.2.1.1.m1.2.2.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">(</mo><msub
    id="S3.E5X.2.1.1.m1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.2.2.2.2.4"
    xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.2.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.2.cmml">𝝅</mi><mrow
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5X.2.1.1.m1.2.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.2.2.2.2.5" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.6.6.5.1" xref="S3.E5X.2.1.1.m1.6.6.5.1.cmml">​</mo><mrow
    id="S3.E5X.2.1.1.m1.6.6.5.3.2" xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mo stretchy="false"
    id="S3.E5X.2.1.1.m1.6.6.5.3.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.4.4"
    xref="S3.E5X.2.1.1.m1.4.4.cmml">s</mi><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.5.3.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml">)</mo></mrow></mrow><mo id="S3.E5X.2.1.1.m1.6.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.6.cmml">=</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.E5X.2.1.1.m1.6b"><apply id="S3.E5X.2.1.1.m1.6.6.cmml" xref="S3.E5X.2.1.1.m1.6.6"><apply
    id="S3.E5X.2.1.1.m1.6.6b.cmml" xref="S3.E5X.2.1.1.m1.6.6"><apply id="S3.E5X.2.1.1.m1.6.6.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2"><apply id="S3.E5X.2.1.1.m1.6.6.2.4.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.2.4.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4">superscript</csymbol><apply
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4"><csymbol cd="ambiguous"
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2">𝜋</ci><ci
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3">𝑖</ci></apply></apply><vector
    id="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.2.2"><ci id="S3.E5X.2.1.1.m1.3.3.cmml"
    xref="S3.E5X.2.1.1.m1.3.3">𝑠</ci><apply id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.1.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2">𝑎</ci><ci id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2">𝝅</ci><apply id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"><ci id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2">𝒊</ci></apply></apply></vector></apply><apply
    id="S3.E5X.2.1.1.m1.6.6.5.cmml" xref="S3.E5X.2.1.1.m1.6.6.5"><apply id="S3.E5X.2.1.1.m1.6.6.5.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2"><apply id="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.1.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1">subscript</csymbol><apply
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2"><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2">arg</ci><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3">max</ci></apply><apply id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2">𝜋</ci><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3">𝑖</ci></apply></apply><apply id="S3.E5X.2.1.1.m1.6.6.5.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2">subscript</csymbol><apply id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2">superscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2">𝑉</ci><interval closure="open" id="S3.E5X.2.1.1.m1.2.2.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.2.2.2.2"><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.2">𝜋</ci><ci
    id="S3.E5X.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.2">𝝅</ci><apply
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3"><ci id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2">𝒊</ci></apply></apply></interval></apply><ci
    id="S3.E5X.2.1.1.m1.6.6.5.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.3">𝑖</ci></apply></apply><ci
    id="S3.E5X.2.1.1.m1.4.4.cmml" xref="S3.E5X.2.1.1.m1.4.4">𝑠</ci></apply></apply><apply
    id="S3.E5X.2.1.1.m1.6.6c.cmml" xref="S3.E5X.2.1.1.m1.6.6"><csymbol cd="latexml"
    id="S3.E5X.2.1.1.m1.6.6.7.cmml" xref="S3.E5X.2.1.1.m1.6.6.7">absent</csymbol></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E5X.2.1.1.m1.6c">\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)=</annotation></semantics></math>
    |  | (5) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '|  |  | <math id="S3.E5Xa.2.1.1.m1.7" class="ltx_Math" alttext="\displaystyle\operatorname*{arg\,max}_{\pi_{i}}\sum_{\bm{a}\in\mathcal{A}}\pi_{i}(s,a_{i})\bm{\pi_{-i}}(s,\bm{a_{-i}})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s^{\prime})]." display="inline"><semantics id="S3.E5Xa.2.1.1.m1.7a"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"><munder id="S3.E5Xa.2.1.1.m1.7.7.1.1.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.cmml"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2.cmml">arg</mi><mo
    lspace="0.170em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.1.cmml">​</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3.cmml">max</mi></mrow><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2.cmml">π</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3.cmml">i</mi></msub></munder><mo
    lspace="0.167em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.cmml"><mstyle
    displaystyle="true" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml"><munder
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml"><mo
    movablelimits="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.2.cmml">∑</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2.cmml">𝒂</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.1.cmml">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3.cmml">𝒜</mi></mrow></munder></mstyle><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.cmml"><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2.cmml">π</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.3.3" xref="S3.E5Xa.2.1.1.m1.3.3.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2.cmml">𝝅</mi><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3a"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml">−</mo><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2.cmml">𝒊</mi></mrow></msub><mo lspace="0em"
    rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7b" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.4.4" xref="S3.E5Xa.2.1.1.m1.4.4.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2.cmml">𝒂</mi><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml">−</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7c" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.cmml"><mstyle
    displaystyle="true" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml"><munder
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml"><mo
    movablelimits="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.2.cmml">∑</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.cmml"><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3.cmml">′</mo></msup><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.1.cmml">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3.cmml">𝒮</mi></mrow></munder></mstyle><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6.cmml">𝒯</mi><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.5.5" xref="S3.E5Xa.2.1.1.m1.5.5.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.5"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.6"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2.cmml">𝒂</mi><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml">[</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.cmml"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.cmml"><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2.cmml">R</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.4.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.6.6" xref="S3.E5Xa.2.1.1.m1.6.6.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.5"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2.cmml">𝒂</mi><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3a"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml">−</mo><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.5.cmml">+</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3.cmml">γ</mi><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2.cmml">​</mo><msubsup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2.cmml">V</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3.cmml">i</mi><mrow
    id="S3.E5Xa.2.1.1.m1.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml"><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">(</mo><msub
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2.cmml">π</mi><mi id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5Xa.2.1.1.m1.2.2.2.2.4"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.2.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2.cmml">𝝅</mi><mrow
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3a"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml">−</mo><mi id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.5" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml">(</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    lspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E5Xa.2.1.1.m1.7b"><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1"><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2">arg</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3">max</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2">𝜋</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3">𝑖</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2">𝒂</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3">𝒜</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2">𝜋</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3">𝑖</ci></apply><interval
    closure="open" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1"><ci
    id="S3.E5Xa.2.1.1.m1.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.3.3">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3">𝑖</ci></apply></interval><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2">𝝅</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2">𝒊</ci></apply></apply><interval closure="open"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1"><ci
    id="S3.E5Xa.2.1.1.m1.4.4.cmml" xref="S3.E5Xa.2.1.1.m1.4.4">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2">𝒂</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2">𝒊</ci></apply></apply></interval><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3">′</ci></apply><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3">𝒮</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6">𝒯</ci><vector
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3"><ci
    id="S3.E5Xa.2.1.1.m1.5.5.cmml" xref="S3.E5Xa.2.1.1.m1.5.5">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2">𝑎</ci><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2">𝒂</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3">′</ci></apply></vector><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1"><csymbol
    cd="latexml" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.2">delimited-[]</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2">𝑅</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3">𝑖</ci></apply><vector
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3"><ci
    id="S3.E5Xa.2.1.1.m1.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.6.6">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2">𝒂</ci><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3">′</ci></apply></vector></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3">𝛾</ci><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2">𝑉</ci><interval
    closure="open" id="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2"><apply
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2">𝜋</ci><ci
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2">𝝅</ci><apply
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3"><ci
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2">𝒊</ci></apply></apply></interval></apply><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3">𝑖</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3">′</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E5Xa.2.1.1.m1.7c">\displaystyle\operatorname*{arg\,max}_{\pi_{i}}\sum_{\bm{a}\in\mathcal{A}}\pi_{i}(s,a_{i})\bm{\pi_{-i}}(s,\bm{a_{-i}})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s^{\prime})].</annotation></semantics></math>
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: Specifically, the opponents’ joint policy <math id="S3.SS1.p4.1.m1.2" class="ltx_Math"
    alttext="\bm{\pi_{-i}}(s,\bm{a_{-i}})" display="inline"><semantics id="S3.SS1.p4.1.m1.2a"><mrow
    id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml"><msub id="S3.SS1.p4.1.m1.2.2.3"
    xref="S3.SS1.p4.1.m1.2.2.3.cmml"><mi id="S3.SS1.p4.1.m1.2.2.3.2" xref="S3.SS1.p4.1.m1.2.2.3.2.cmml">𝝅</mi><mrow
    id="S3.SS1.p4.1.m1.2.2.3.3" xref="S3.SS1.p4.1.m1.2.2.3.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p4.1.m1.2.2.3.3a" xref="S3.SS1.p4.1.m1.2.2.3.3.cmml">−</mo><mi
    id="S3.SS1.p4.1.m1.2.2.3.3.2" xref="S3.SS1.p4.1.m1.2.2.3.3.2.cmml">𝒊</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S3.SS1.p4.1.m1.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.cmml">​</mo><mrow
    id="S3.SS1.p4.1.m1.2.2.1.1" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml"><mo stretchy="false"
    id="S3.SS1.p4.1.m1.2.2.1.1.2" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p4.1.m1.1.1"
    xref="S3.SS1.p4.1.m1.1.1.cmml">s</mi><mo id="S3.SS1.p4.1.m1.2.2.1.1.3" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">,</mo><msub
    id="S3.SS1.p4.1.m1.2.2.1.1.1" xref="S3.SS1.p4.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.2.2.1.1.1.2"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS1.p4.1.m1.2.2.1.1.1.3"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.SS1.p4.1.m1.2.2.1.1.1.3a" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml">−</mo><mi
    id="S3.SS1.p4.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.SS1.p4.1.m1.2.2.1.1.4" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><apply id="S3.SS1.p4.1.m1.2.2.cmml"
    xref="S3.SS1.p4.1.m1.2.2"><apply id="S3.SS1.p4.1.m1.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.3"><csymbol
    cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.2.3">subscript</csymbol><ci
    id="S3.SS1.p4.1.m1.2.2.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.3.2">𝝅</ci><apply id="S3.SS1.p4.1.m1.2.2.3.3.cmml"
    xref="S3.SS1.p4.1.m1.2.2.3.3"><ci id="S3.SS1.p4.1.m1.2.2.3.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.3.3.2">𝒊</ci></apply></apply><interval
    closure="open" id="S3.SS1.p4.1.m1.2.2.1.2.cmml" xref="S3.SS1.p4.1.m1.2.2.1.1"><ci
    id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑠</ci><apply id="S3.SS1.p4.1.m1.2.2.1.1.1.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.1.1.1.1.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.2.2.1.1.1.2.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.2">𝒂</ci><apply id="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.3"><ci id="S3.SS1.p4.1.m1.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.2">𝒊</ci></apply></apply></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">\bm{\pi_{-i}}(s,\bm{a_{-i}})</annotation></semantics></math>
    can be non-stationary, i.e., changes as the opponents’ policies change over time,
    for example with learning opponents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Convergence results
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Littman [[125](#bib.bib125)] studied convergence properties of reinforcement
    learning joint action agents [[126](#bib.bib126)] in Markov games with the following
    conclusions: in adversarial environments (zero-sum games) an optimal play can
    be guaranteed against an arbitrary opponent, i.e., Minimax Q-learning [[124](#bib.bib124)].
    In coordination environments (e.g., in cooperative games all agents share the
    same reward function), strong assumptions need be made about other agents to guarantee
    convergence to optimal behavior [[125](#bib.bib125)], e.g., Nash Q-learning [[127](#bib.bib127)]
    and Friend-or-Foe Q-learning [[128](#bib.bib128)]. In other types of environments
    no value-based RL algorithms with guaranteed convergence properties are known [[125](#bib.bib125)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Recent work on MDRL have addressed scalability and have focused significantly
    less on convergence guarantees, with few exceptions [[129](#bib.bib129), [130](#bib.bib130),
    [131](#bib.bib131), [132](#bib.bib132)]. One notable work has shown a connection
    between update rules for actor-critic algorithms for multiagent partially observable
    settings and (counterfactual) regret minimization:⁸⁸8 Counterfactual regret minimization
    is a technique for solving large games based on regret minimization [[133](#bib.bib133),
    [134](#bib.bib134)] due to a well-known connection between regret and Nash equilibria [[135](#bib.bib135)].
    It has been one of the reasons of successes in Poker [[16](#bib.bib16), [17](#bib.bib17)].
    the advantage values are scaled counterfactual regrets. This lead to new convergence
    properties of independent RL algorithms in zero-sum games with imperfect information [[136](#bib.bib136)].
    The result is also used to support policy gradient optimization against worst-case
    opponents, in a new algorithm called Exploitability Descent [[137](#bib.bib137)].⁹⁹9This
    algorithm is similar to CFR-BR [[138](#bib.bib138)] and has the main advantage
    that the current policy convergences rather than the average policy, so there
    is no need to learn the average strategy, which requires large reservoir buffers
    or many past networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: We refer the interested reader to seminal works about convergence in multiagent
    domains [[139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)]. Note that instead
    of convergence, some MAL algorithms have proved learning a best response against
    classes of opponents [[150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other common problems in MAL, including action shadowing [[34](#bib.bib34),
    [33](#bib.bib33)], the curse of dimensionality [[5](#bib.bib5)], and multiagent
    credit assignment [[32](#bib.bib32)]. Describing each problem is out of the scope
    of this survey. However, we refer the interested reader to excellent resources
    on general MAL [[4](#bib.bib4), [153](#bib.bib153), [154](#bib.bib154)], as well
    as surveys in specific areas: game theory and multiagent reinforcement learning [[5](#bib.bib5),
    [6](#bib.bib6)], cooperative scenarios [[7](#bib.bib7), [8](#bib.bib8)], evolutionary
    dynamics of multiagent learning [[9](#bib.bib9)], learning in non-stationary environments [[10](#bib.bib10)],
    agents modeling agents [[11](#bib.bib11)], and transfer learning in multiagent
    RL [[12](#bib.bib12)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 MDRL categorization
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") we outlined some recent
    works in single-agent DRL since an exhaustive list is out of the scope of this
    article. This explosion of works has led DRL to be extended and combined with
    other techniques [[23](#bib.bib23), [37](#bib.bib37), [29](#bib.bib29)]. One natural
    extension to DRL is to test whether these approaches could be applied in a multiagent
    environment.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'We analyzed the most recent works (that are not covered by previous MAL surveys [[10](#bib.bib10),
    [11](#bib.bib11)] and we do not consider genetic algorithms or swarm intelligence
    in this survey) that have a clear connection with MDRL. We propose 4 categories
    which take inspiration from previous surveys [[1](#bib.bib1), [5](#bib.bib5),
    [7](#bib.bib7), [11](#bib.bib11)] and that conveniently describe and represent
    current works. Note that some of these works fit into more than one category (they
    are not mutually exclusive), therefore their summaries are presented in all applicable
    Tables [2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")-[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”"), however, for the ease of exposition
    when describing them in the text we only do so in one category. Additionally,
    for each work we present its learning type, either a value-based method (e.g.,
    DQN) or a policy gradient method (e.g., actor-critic); also, we mention if the
    setting is evaluated in a fully cooperative, fully competitive or mixed environment
    (both cooperative and competitive).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Analysis of emergent behaviors*. These works, in general, do not propose learning
    algorithms — their main focus is to analyze and evaluate DRL algorithms, e.g.,
    DQN [[155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)], PPO [[158](#bib.bib158),
    [157](#bib.bib157)] and others [[159](#bib.bib159), [157](#bib.bib157), [160](#bib.bib160)],
    in a multiagent environment. In this category we found works which analyze behaviors
    in the three major settings: cooperative, competitive and mixed scenarios; see
    Section [3.3](#S3.SS3 "3.3 Emergent behaviors ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and Table [2](#S3.T2 "Table
    2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning communication* [[161](#bib.bib161), [160](#bib.bib160), [162](#bib.bib162),
    [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165)]. These works explore
    a sub-area in which agents can share information with communication protocols,
    for example through direct messages [[162](#bib.bib162)] or via a shared memory [[165](#bib.bib165)].
    This area is attracting attention and it had not been explored much in the MAL
    literature. See Section [3.4](#S3.SS4 "3.4 Learning communication ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") and Table [2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3
    Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning cooperation*. While learning to communicate is an emerging area,
    fostering cooperation in learning agents has a long history of research in MAL [[7](#bib.bib7),
    [8](#bib.bib8)]. In this category the analyzed works are evaluated in either cooperative
    or mixed settings. Some works in this category take inspiration from MAL (e.g.,
    leniency, hysteresis, and difference rewards concepts) and extend them to the
    MDRL setting [[35](#bib.bib35), [166](#bib.bib166), [167](#bib.bib167)]. A notable
    exception [[168](#bib.bib168)] takes a key component from RL (i.e., experience
    replay buffer) and adapts it for MDRL. See Section [3.5](#S3.SS5 "3.5 Learning
    cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”") and Table [3](#S3.T3 "Table 3 ‣ 3.2 MDRL categorization
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Agents modeling agents*. Albrecht and Stone [[11](#bib.bib11)] presented a
    thorough survey in this topic and we have found many works that fit into this
    category in the MDRL setting, some taking inspiration from DRL [[169](#bib.bib169),
    [170](#bib.bib170), [171](#bib.bib171)], and others from MAL [[172](#bib.bib172),
    [173](#bib.bib173), [64](#bib.bib64), [174](#bib.bib174), [175](#bib.bib175)].
    Modeling agents is helpful not only to cooperate, but also for modeling opponents [[172](#bib.bib172),
    [169](#bib.bib169), [171](#bib.bib171), [173](#bib.bib173)], inferring goals [[170](#bib.bib170)],
    and accounting for the learning behavior of other agents [[64](#bib.bib64)]. In
    this category the analyzed algorithms present their results in either a competitive
    setting or a mixed one (cooperative and competitive). See Section [3.6](#S3.SS6
    "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and Table [4](#S3.T4 "Table
    4 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the rest of this section we describe each category along with the summaries
    of related works.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: These papers analyze *emergent behaviors* in MDRL. Learning type is
    either value-based (VB) or policy gradient (PG). Setting where experiments were
    performed: cooperative (CO), competitive (CMP) or mixed. A detailed description
    is given in Section [3.3](#S3.SS3 "3.3 Emergent behaviors ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”").'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '| Work | Summary | Learning | Setting |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Tampuu et al. [[155](#bib.bib155)] | Train DQN agents to play Pong. | VB
    | CO&CMP |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Leibo et al. [[156](#bib.bib156)] | Train DQN agents to play sequential social
    dilemmas. | VB | Mixed |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Lerer and Peysakhovich [[176](#bib.bib176)] | Propose DRL agents able to
    cooperate in social dilemmas. | VB | Mixed |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Leibo et al. [[159](#bib.bib159)] | Propose Malthusian reinforcement learning
    which extends self-play to population dynamics. | VB | Mixed |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Bansal et al. [[158](#bib.bib158)] | Train PPO agents in competitive MuJoCo
    scenarios. | PG | CMP |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| Raghu et al. [[157](#bib.bib157)] | Train PPO, A3C, and DQN agents in attacker-defender
    games. | VB, PG | CMP |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Lazaridou et al. [[161](#bib.bib161)] | Train agents represented with NN
    to learn a communication language. | PG | CO |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Mordatch and Abbeel [[160](#bib.bib160)] | Learn communication with an end-to-end
    differentiable model to train with backpropagation. | PG | CO |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: 'Table 2: These papers propose algorithms for *learning communication*. Learning
    type is either value-based (VB) or policy gradient (PG). Setting were experiments
    were performed: cooperative (CO) or mixed. A more detailed description is given
    in Section [3.4](#S3.SS4 "3.4 Learning communication ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Lazaridou et al. [[161](#bib.bib161)] | Train agents represented with NN
    to learn a communication language. | PG | CO |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Mordatch and Abbeel [[160](#bib.bib160)] | Learn communication with an end-to-end
    differentiable model to train with backpropagation. | PG | CO |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| RIAL [[162](#bib.bib162)] | Use a single network (parameter sharing) to train
    agents that take environmental and communication actions. | VB | CO |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| DIAL [[162](#bib.bib162)] | Use gradient sharing during learning and communication
    actions during execution. | VB | CO |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| CommNet [[163](#bib.bib163)] | Use a continuous vector channel for communication
    on a single network. | PG | CO |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| BiCNet [[164](#bib.bib164)] | Use the actor-critic paradigm where communication
    occurs in the latent space. | PG | Mixed |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| MD-MADDPG [[165](#bib.bib165)] | Use of a shared memory as a means to multiagent
    communication. | PG | CO |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| MADDPG-MD [[177](#bib.bib177)] | Extend dropout technique to robustify communication
    when applied in multiagent scenarios with direct communication. | PG | CO |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: 'Table 3: These papers aim to *learn cooperation*. Learning type is either value-based
    (VB) or policy gradient (PG). Setting where experiments were performed: cooperative
    (CO), competitive (CMP) or mixed. A more detailed description is given in Section [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| Lerer and Peysakhovich [[176](#bib.bib176)] | Propose DRL agents able to
    cooperate in social dilemmas. | VB | Mixed |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| MD-MADDPG [[165](#bib.bib165)] | Use of a shared memory as a means to multiagent
    communication. | PG | CO |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| MADDPG-MD [[177](#bib.bib177)] | Extend dropout technique to robustify communication
    when applied in multiagent scenarios with direct communication. | PG | CO |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| RIAL [[162](#bib.bib162)] | Use a single network (parameter sharing) to train
    agents that take environmental and communication actions. | VB | CO |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| DIAL [[162](#bib.bib162)] | Use gradient sharing during learning and communication
    actions during execution. | VB | CO |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| DCH/PSRO [[172](#bib.bib172)] | Policies can overfit to opponents: better
    compute approximate best responses to a mixture of policies. | VB | CO & CMP |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Fingerprints [[168](#bib.bib168)] | Deal with ER problems in MDRL by conditioning
    the value function on a fingerprint that disambiguates the age of the sampled
    data. | VB | CO |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Lenient-DQN [[35](#bib.bib35)] | Achieve cooperation by leniency, optimism
    in the value function by forgiving suboptimal (low-rewards) actions. | VB | CO
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Hysteretic-DRQN [[166](#bib.bib166)] | Achieve cooperation by using two learning
    rates, depending on the updated values together with multitask learning via policy
    distillation. | VB | CO |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| WDDQN [[178](#bib.bib178)] | Achieve cooperation by leniency, weighted double
    estimators, and a modified prioritized experience replay buffer. | VB | CO |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| FTW [[179](#bib.bib179)] | Agents act in a mixed environment (composed of
    teammates and opponents), it proposes a two-level architecture and population-based
    learning. | PG | Mixed |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| VDN [[180](#bib.bib180)] | Decompose the team action-value function into
    pieces across agents, where the pieces can be easily added. | VB | Mixed |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| QMIX [[181](#bib.bib181)] | Decompose the team action-value function together
    with a mixing network that can recombine them. | VB | Mixed |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| COMA [[167](#bib.bib167)] | Use a centralized critic and a counter-factual
    advantage function based on solving the multiagent credit assignment. | PG | Mixed
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| PS-DQN, PS-TRPO, PS-A3C [[182](#bib.bib182)] | Propose parameter sharing
    for learning cooperative tasks. | VB, PG | CO |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| MADDPG [[63](#bib.bib63)] | Use an actor-critic approach where the critic
    is augmented with information from other agents, the actions of all agents. |
    PG | Mixed |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: 'Table 4: These papers consider *agents modeling agents*. Learning type is either
    value-based (VB) or policy gradient (PG). Setting where experiments were performed:
    cooperative (CO), competitive (CMP) or mixed. A more detailed description is given
    in Section [3.6](#S3.SS6 "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| MADDPG [[63](#bib.bib63)] | Use an actor-critic approach where the critic
    is augmented with information from other agents, the actions of all agents. |
    PG | Mixed |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| DRON [[169](#bib.bib169)] | Have a network to infer the opponent behavior
    together with the standard DQN architecture. | VB | Mixed |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| DPIQN, DPIRQN [[171](#bib.bib171)] | Learn policy features from raw observations
    that represent high-level opponent behaviors via auxiliary tasks. | VB | Mixed
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| SOM [[170](#bib.bib170)] | Assume the reward function depends on a hidden
    goal of both agents and then use an agent’s own policy to infer the goal of the
    other agent. | PG | Mixed |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| NFSP [[173](#bib.bib173)] | Compute approximate Nash equilibria via self-play
    and two neural networks. | VB | CMP |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| PSRO/DCH [[172](#bib.bib172)] | Policies can overfit to opponents: better
    compute approximate best responses to a mixture of policies. | PG | CO & CMP |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| M3DDPG [[183](#bib.bib183)] | Extend MADDPG with minimax objective to robustify
    the learned policy. | PG | Mixed |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| LOLA [[64](#bib.bib64)] | Use a learning rule where the agent accounts for
    the parameter update of other agents to maximize its own reward. | PG | Mixed
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| ToMnet [[174](#bib.bib174)] | Use an architecture for end-to-end learning
    and inference of diverse opponent types. | PG | Mixed |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Deep Bayes-ToMoP [[175](#bib.bib175)] | Best respond to opponents using Bayesian
    policy reuse, theory of mind, and deep networks. | VB | CMP |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| Deep BPR+[[184](#bib.bib184)] | Bayesian policy reuse and policy distillation
    to quickly best respond to opponents. | VB | CO & CMP |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: 3.3 Emergent behaviors
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some recent works have analyzed the previously mentioned *independent* DRL
    agents (see Section [3.1](#S3.SS1 "3.1 Multiagent Learning ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”"))
    from the perspective of types of emerging behaviors (e.g., cooperative or competitive).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: One of the earliest MDRL works is by Tampuu et al. [[155](#bib.bib155)], which
    had two independent DQN learning agents to play the Atari Pong game. Their focus
    was to adapt the reward function for the learning agents, which resulted in either
    cooperative or competitive emergent behaviors.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Leibo et al. [[156](#bib.bib156)] meanwhile studied independent DQNs in the
    context of *sequential social dilemmas*: a Markov game that satisfies certain
    inequalities [[156](#bib.bib156)]. The focus of this work was to highlight that
    cooperative or competitive behaviors exist not only as discrete (atomic) actions,
    but they are temporally extended (over policies). In the related setting of one
    shot Markov social dilemmas, Lerer and Peysakhovich [[176](#bib.bib176)] extended
    the famous Tit-for-Tat (TFT)^(10)^(10)10TFT originated in an iterated prisoner’s
    dilemma tournament and later inspired different strategies in MAL [[185](#bib.bib185)],
    its generalization, Godfather, is a representative of *leader strategies* [[186](#bib.bib186)].
    strategy [[187](#bib.bib187)] for DRL (using function approximators) and showed
    (theoretically and experimentally) that such agents can maintain cooperation.
    To construct the agents they used self-play and two reward schemes: selfish and
    cooperative. Previously, different MAL algorithms were designed to foster cooperation
    in social dilemmas with Q-learning agents [[188](#bib.bib188), [189](#bib.bib189)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-play is a useful concept for learning algorithms (e.g., fictitious play [[190](#bib.bib190)])
    since under certain classes of games it can guarantee convergence^(11)^(11)11The
    average strategy profile of fictitious players converges to a Nash equilibrium
    in certain classes of games, e.g., two-player zero-sum and potential games [[191](#bib.bib191)].
    and it has been used as a standard technique in previous RL and MAL works [[192](#bib.bib192),
    [14](#bib.bib14), [193](#bib.bib193)]. Despite its common usage self-play can
    be brittle to forgetting past knowledge [[194](#bib.bib194), [172](#bib.bib172),
    [195](#bib.bib195)] (see Section [4.5](#S4.SS5 "4.5 Open questions ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") for a note on the role
    of self-play as an open question in MDRL). To overcome this issue, Leibo et al. [[159](#bib.bib159)]
    proposed Malthusian reinforcement learning as an extension of self-play to population
    dynamics. The approach can be thought of as community coevolution and has been
    shown to produce better results (avoiding local optima) than independent agents
    with intrinsic motivation [[196](#bib.bib196)]. A limitation of this work is that
    it does not place itself within the state of the art in evolutionary and genetic
    algorithms. Evolutionary strategies have been employed for solving reinforcement
    learning problems [[197](#bib.bib197)] and for evolving function approximators [[75](#bib.bib75)].
    Similarly, they have been used multiagent scenarios to compute approximate Nash
    equilibria [[198](#bib.bib198)] and as metaheuristic optimization algorithms [[199](#bib.bib199),
    [200](#bib.bib200), [7](#bib.bib7), [201](#bib.bib201)].'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Bansal et al. [[158](#bib.bib158)] explored the emergent behaviors in competitive
    scenarios using the MuJoCo simulator [[202](#bib.bib202)]. They trained independent
    learning agents with PPO and incorporated two main modifications to deal with
    the MAL nature of the problem. First, they used *exploration rewards* [[203](#bib.bib203)]
    which are dense rewards that allow agents to learn basic (non-competitive) behaviors
    — this type of reward is annealed through time giving more weight to the environmental
    (competitive) reward. Exploration rewards come from early work in robotics [[204](#bib.bib204)]
    and single-agent RL [[205](#bib.bib205)], and their goal is to provide dense feedback
    for the learning algorithm to improve sample efficiency (Ng et al. [[206](#bib.bib206)]
    studied the theoretical conditions under which modifications of the reward function
    of an MDP preserve the optimal policy). For multiagent scenarios, these dense
    rewards help agents in the beginning phase of the training to learn basic non-competitive
    skills, increasing the probability of random actions from the agent yielding a
    positive reward. The second contribution was *opponent sampling* which maintains
    a pool of older versions of the opponent to sample from, in contrast to using
    the most recent version.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Raghu et al. [[157](#bib.bib157)] investigated how DRL algorithms (DQN, A2C,
    and PPO) performed in a family of two-player zero-sum games with tunable complexity,
    called Erdos-Selfridge-Spencer games [[207](#bib.bib207), [208](#bib.bib208)].
    Their reasoning is threefold: (i) these games provide a parameterized family of
    environments where (ii) optimal behavior can be completely characterized, and
    (iii) support multiagent play. Their work showed that algorithms can exhibit wide
    variation in performance as the algorithms are tuned to the game’s difficulty.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Lazaridou et al. [[161](#bib.bib161)] proposed a framework for language learning
    that relies on multiagent communication. The agents, represented by (feed-forward)
    neural networks, need to develop an *emergent language* to solve a task. The task
    is formalized as a *signaling game* [[209](#bib.bib209)] in which two agents,
    a sender and a receiver, obtain a pair of images. The sender is told one of them
    is the target and is allowed to send a message (from a fixed vocabulary) to the
    receiver. Only when the receiver identifies the target image do both agents receive
    a positive reward. The results show that agents can coordinate for the experimented
    visual-based domain. To analyze the semantic properties^(12)^(12)12The vocabulary
    that agents use was arbitrary and had no initial meaning. To understand its emerging
    semantics they looked at the relationship between symbols and the sets of images
    they referred to [[161](#bib.bib161)]. of the learned communication protocol they
    looked whether symbol usage reflects the semantics of the visual space, and that
    despite some variation, many high level objects groups correspond to the same
    learned symbols using a t-SNE [[210](#bib.bib210)] based analysis (t-SNE is a
    visualization technique for high-dimensional data and it has also been used to
    better understand the behavior of trained DRL agents [[211](#bib.bib211), [212](#bib.bib212)]).
    A key objective of this work was to determine if the agent’s language could be
    human-interpretable. To achieve this, learned symbols were grounded with natural
    language by extending the signaling game with a supervised image labelling task
    (the sender will be encouraged to use conventional names, making communication
    more transparent to humans). To measure the interpretability of the extended game,
    a crowdsourced survey was performed, and in essence, the trained agent receiver
    was replaced with a human. The results showed that 68% of the cases, human participants
    picked the correct image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Mordatch and Abbeel [[160](#bib.bib160)] investigated the emergence
    of language with the difference that in their setting there were no explicit roles
    for the agents (i.e., sender or receiver). To learn, they proposed an end-to-end
    differentiable model of all agent and environment state dynamics over time to
    calculate the gradient of the return with backpropagation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Learning communication
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in the previous section, one of the desired emergent behaviors
    of multiagent interaction is the emergence of communication [[161](#bib.bib161),
    [160](#bib.bib160)]. This setting usually considers a set of *cooperative agents*
    in a *partially observable* environment (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement
    learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")) where agents need to maximize their shared utility by means
    of communicating information.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning
    (DIAL) are two methods using deep networks to learn to communicate [[162](#bib.bib162)].
    Both methods use a neural net that outputs the agent’s <math id="S3.SS4.p2.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi
    id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">Q</annotation></semantics></math>
    values (as done in standard DRL algorithms) and a message to communicate to other
    agents in the next timestep. RIAL is based on DRQN and also uses the concept of
    *parameter sharing*, i.e., using a single network whose parameters are shared
    among all agents. In contrast, DIAL directly passes gradients via the communication
    channel during learning, and messages are discretized and mapped to the set of
    communication actions during execution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '*Memory-driven* (MD) communication was proposed on top of the Multi-Agent Deep
    Deterministic Policy Gradient (MADDPG) [[63](#bib.bib63)] method. In MD-MADDPG
    [[165](#bib.bib165)], the agents use a shared memory as a communication channel:
    before taking an action, the agent first reads the memory, then writes a response.
    In this case the agent’s policy becomes dependent on its private observation and
    its interpretation of the collective memory. Experiments were performed with two
    agents in cooperative scenarios. The results highlighted the fact that the communication
    channel was used differently in each environment, e.g., in simpler tasks agents
    significantly decrease their memory activity near the end of the task as there
    are no more changes in the environment; in more complex environments, the changes
    in memory usage appear at a much higher frequency due to the presence of many
    sub-tasks.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Dropout [[213](#bib.bib213)] is a technique to prevent overfitting (in supervised
    learning this happens when the learning algorithm achieves good performance only
    on a specific data set and fails to generalize) in neural networks which is based
    on randomly dropping units and their connections during training time. Inspired
    by dropout, Kim et al. [[177](#bib.bib177)] proposed a similar approach in multiagent
    environments where direct communication through messages is allowed. In this case,
    the messages of other agents are dropped out at training time, thus the authors
    proposed the Message-Dropout MADDPG algorithm [[177](#bib.bib177)]. This method
    is expected to work in fully or limited communication environments. The empirical
    results show that with properly chosen message dropout rate, the proposed method
    both significantly improves the training speed and the robustness of learned policies
    (by introducing communication errors) during execution time. This capability is
    important as MDRL agents trained in simulated or controlled environments will
    be less fragile when transferred to more realistic environments.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'While RIAL and DIAL used a discrete communication channel, CommNet [[163](#bib.bib163)]
    used a continuous vector channel. Through this channel agents receive the summed
    transmissions of other agents. The authors assume full cooperation and train a
    single network for all the agents. There are two distinctive characteristics of
    CommNet from previous works: it allows multiple communication cycles at each timestep
    and a dynamic variation of agents at run time, i.e., agents come and go in the
    environment.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to previous approaches, in Multiagent Bidirectionally Coordinated
    Network (BiCNet) [[164](#bib.bib164)], communication takes place in the latent
    space (i.e., in the hidden layers). It also uses parameter sharing, however, it
    proposes bidirectional recurrent neural networks [[214](#bib.bib214)] to model
    the actor and critic networks of their model. Note that in BiCNet agents do not
    *explicitly* share a message and thus it can be considered a method for learning
    cooperation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Learning communication is an active area in MDRL with many open questions, in
    this context, we refer the interested reader to a recent work by Lowe et al. [[215](#bib.bib215)]
    where it discusses common pitfalls (and recommendations to avoid those) while
    measuring communication in multiagent environments.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Learning cooperation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although *explicit communication* is a new emerging trend in MDRL, there has
    already been a large amount of work in MAL for cooperative settings^(13)^(13)13There
    is a large body of research on coordinating multiagent teams by specifying communication
    protocols [[216](#bib.bib216), [217](#bib.bib217)]: these expect agents to know
    the team’s goal as well as the tasks required to accomplish the goal. that do
    not involve communication [[7](#bib.bib7), [8](#bib.bib8)]. Therefore, it was
    a natural starting point for many recent MDRL works.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Foerster et al. [[168](#bib.bib168)] studied the simple scenario of *cooperation
    with independent Q-learning agents* (see Section [3.1](#S3.SS1 "3.1 Multiagent
    Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")), where the agents use the standard DQN architecture
    of neural networks and an experience replay buffer (see Figure [3](#S2.F3 "Figure
    3 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). However, for the ER to
    work, the data distribution needs to follow certain assumptions (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")) which are no loger valid due to the multiagent
    nature of the world: the dynamics that generated the data in the ER no longer
    reflect the current dynamics, making the experience obsolete [[168](#bib.bib168),
    [90](#bib.bib90)]. Their solution is to add information to the experience tuple
    that can help to *disambiguate the age of the sampled data* from the replay memory.
    Two approaches were proposed. The first is Multiagent Importance Sampling which
    adds the probability of the joint action so an importance sampling correction [[70](#bib.bib70),
    [218](#bib.bib218)] can computed when the tuple is later sampled for training.
    This was similar to previous works in adaptive importance sampling [[219](#bib.bib219),
    [220](#bib.bib220)] and off-environment RL [[221](#bib.bib221)]. The second approach
    is Multiagent Fingerprints which adds the estimate (i.e., fingerprint) of other
    agents’ policies (loosely inspired by Hyper-Q [[150](#bib.bib150)], see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). For the practical implementation,
    good results were obtained by using the training iteration number and exploration
    rate as the fingerprint.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Gupta et al. [[182](#bib.bib182)] tackled cooperative environments in partially
    observable domains without explicit communication. They proposed *parameter sharing*
    (PS) as a way to improve learning in homogeneous multiagent environments (where
    agents have the same set of actions). The idea is to have one globally shared
    learning network that can still behave differently in execution time, i.e., because
    its inputs (individual agent observation and agent index) will be different. They
    tested three variations of this approach with parameter sharing: PS-DQN, PS-DDPG
    and PS-TRPO, which extended single-agent DQN, DDPG and TRPO algorithms, respectively.
    The results showed that PS-TRPO outperformed the other two. Note that Foerster
    et al. [[162](#bib.bib162)] concurrently proposed a similar concept, see Section [3.4](#S3.SS4
    "3.4 Learning communication ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Lenient-DQN (LDQN) [[35](#bib.bib35)] took the *leniency* concept [[222](#bib.bib222)]
    (originally presented in MAL) and extended their use to MDRL. The purpose of leniency
    is to overcome a pathology called relative overgeneralization [[34](#bib.bib34),
    [223](#bib.bib223), [224](#bib.bib224)]. Similar to other approaches designed
    to overcome relative overgeneralization (e.g., distributed Q-learning [[225](#bib.bib225)]
    and hysteretic Q-learning [[8](#bib.bib8)]) lenient learners initially maintain
    an optimistic disposition to mitigate the noise from transitions resulting in
    miscoordination, preventing agents from being drawn towards sub-optimal but wide
    peaks in the reward search space [[97](#bib.bib97)]. However, similar to other
    MDRL works [[168](#bib.bib168)], the LDQN authors experienced problems with the
    ER buffer and arrived at a similar solution: adding information to the experience
    tuple, in their case, the leniency value. When sampling from the ER buffer, this
    value is used to determine a leniency condition; if the condition is not met then
    the sample is ignored.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar vein, Decentralized-Hysteretic Deep Recurrent Q-Networks (DEC-HDRQNs) [[166](#bib.bib166)]
    were proposed for fostering cooperation among independent learners. The motivation
    is similar to LDQN, making an optimistic value update, however, their solution
    is different. Here, the authors took inspiration from Hysteretic Q-learning [[8](#bib.bib8)],
    originally presented in MAL, where two learning rates were used. A difference
    between lenient agents and hysteretic Q-learning is that lenient agents are only
    *initially* forgiving towards teammates. Lenient learners over time apply less
    leniency towards updates that would lower utility values, taking into account
    how frequently observation-action pairs have been encountered. The idea being
    that the transition from optimistic to average reward learner will help make lenient
    learners more robust towards misleading stochastic rewards [[222](#bib.bib222)].
    Additionally, in DEC-HDRQNs the ER buffer is also extended into *concurrent experience
    replay trajectories*, which are composed of three dimensions: agent index, the
    episode, and the timestep; when training, the sampled traces have the same starting
    timesteps. Moreover, to improve on generalization over different tasks, i.e.,
    multi-task learning[[226](#bib.bib226)], DEC-HDRQNs make use of policy distillation [[227](#bib.bib227),
    [228](#bib.bib228)] (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). In contrast to other approaches, DEC-HDRQNS are fully decentralized
    during learning and execution.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted Double Deep Q-Network (WDDQN) [[178](#bib.bib178)] is based on having
    double estimators. This idea was originally introduced in Double Q-learning [[91](#bib.bib91)]
    and aims to remove the existing overestimation bias caused by using the maximum
    action value as an approximation for the maximum expected action value (see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). It also uses a *lenient*
    reward [[222](#bib.bib222)] to be optimistic during initial phase of coordination
    and proposes a *scheduled* replay strategy in which samples closer to the terminal
    states are heuristically given higher priority; this strategy might not be applicable
    for any domain. For other works extending the ER to multiagent settings see MADDPG [[63](#bib.bib63)],
    Sections [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL ‣
    4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and [4.2](#S4.SS2 "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”").'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d8897e26b6d80a96325b534c94c1ee5.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A schematic view of the architecture used in FTW (For the Win) [[179](#bib.bib179)]:
    two unrolled recurrent neural networks (RNNs) operate at different time-scales,
    the idea is that the *Slow RNN* helps with long term temporal correlations. Observations
    are latent space output of some convolutional neural network to learn non-linear
    features. Feudal Networks [[229](#bib.bib229)] is another work in single-agent
    DRL that also maintains a multi-time scale hierarchy where the slower network
    sets the goal, and the faster network tries to achieve them. Fedual Networks were
    in turn, inspired by early work in RL which proposed a hierarchy of Q-learners [[230](#bib.bib230),
    [231](#bib.bib231)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'While previous approaches were mostly inspired by how MAL algorithms could
    be extended to MDRL, other works take as base the results by single-agent DRL.
    One example is the For The Win (FTW) [[179](#bib.bib179)] agent which is based
    on the actor-learner structure of IMPALA [[114](#bib.bib114)] (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). The authors test FTW in a game where two
    opposing teams compete to capture each other’s flags [[232](#bib.bib232)]. To
    deal with the MAL problem they propose two main additions: a *hierarchical two-level
    representation* with recurrent neural networks operating at different timescales,
    as depicted in Figure [5](#S3.F5 "Figure 5 ‣ 3.5 Learning cooperation ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”"), and a *population based training* [[233](#bib.bib233), [234](#bib.bib234),
    [235](#bib.bib235)] where 30 agents were trained in parallel together with a stochastic
    matchmaking scheme that biases agents to be of similar *skills*. The Elo rating
    system [[236](#bib.bib236)] was originally devised to rate chess player skills,^(14)^(14)14Elo
    uses a normal distribution for each player skill, and after each match, both players’
    distributions are updated based on measure of *surprise*, i.e., if a user with
    previously lower (predicted) skill beats a high skilled one, the low-skilled player
    is significantly increased. TrueSkill [[237](#bib.bib237)] extended Elo by tracking
    uncertainty in skill rating, supporting draws, and matches beyond 1 vs 1; <math
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\alpha-" display="inline"><semantics
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1a"><mrow id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"><mi
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml">α</mi><mo
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.1.1.1.1.1.1.m1.1b"><apply id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1">limit-from</csymbol><ci id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2">𝛼</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p7.1.1.1.1.1.1.m1.1c">\alpha-</annotation></semantics></math>Rank
    is a more recent alternative to ELO [[238](#bib.bib238)]. FTW did not use TrueSkill
    but a simpler extension of Elo for <math id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1" class="ltx_Math"
    alttext="n" display="inline"><semantics id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1a"><mi
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1" xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1b"><ci id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml"
    xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1c">n</annotation></semantics></math> vs <math
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1a"><mi id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1" xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1b"><ci id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml"
    xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1c">n</annotation></semantics></math> games (by
    adding individual agent ratings to compute the team skill). Hierarchical approaches
    were previously proposed in RL, e.g., Feudal RL [[230](#bib.bib230), [231](#bib.bib231)],
    and were later extended to DRL in Feudal networks [[229](#bib.bib229)]; population
    based training can be considered analogous to evolutionary strategies that employ
    self-adaptive hyperparameter tuning to modify how the genetic algorithm itself
    operates [[234](#bib.bib234), [239](#bib.bib239), [240](#bib.bib240)]. An interesting
    result from FTW is that the population-based training obtained better results
    than training via self-play [[192](#bib.bib192)], which was a standard concept
    in previous works [[14](#bib.bib14), [193](#bib.bib193)]. FTW used heavy compute
    resources, it used 30 agents (processes) in parallel where every training game
    lasted 4500 agent steps (<math id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1" class="ltx_Math"
    alttext="\approx" display="inline"><semantics id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1a"><mo
    id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1.1" xref="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1c">\approx</annotation></semantics></math>
    five minutes) and agents were trained for two billion steps (<math id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1"
    class="ltx_Math" alttext="\approx" display="inline"><semantics id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1a"><mo
    id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1.1" xref="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1c">\approx</annotation></semantics></math>
    450K games).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Lowe et al. [[63](#bib.bib63)] noted that using standard policy gradient methods
    (see Section [2.1](#S2.SS1 "2.1 Reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) on multiagent environments
    yields high variance and performs poorly. This occurs because the variance is
    further increased as all the agents’ rewards depend on the rest of the agents,
    and it is formally shown that as the number of agents increase, the probability
    of taking a correct gradient direction decreases exponentially [[63](#bib.bib63)].
    Therefore, to overcome this issue Lowe et al. proposed the Multi-Agent Deep Deterministic
    Policy Gradient (MADDPG) [[63](#bib.bib63)], building on DDPG [[65](#bib.bib65)]
    (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")), to train a centralized
    critic per agent that is given all agents’ policies during training to reduce
    the variance by removing the non-stationarity caused by the concurrently learning
    agents. Here, the actor only has local information (turning the method into a
    centralized training with decentralized execution) and the ER buffer records experiences
    of *all* agents. MADDPG was tested in both cooperative and competitive scenarios,
    experimental results show that it performs better than several decentralized methods
    (such as DQN, DDPG, and TRPO). The authors mention that traditional RL methods
    do not produce consistent gradient signals. This is exemplified in a challenging
    competitive scenarios where agents continuously adapt to each other causing the
    learned best-response policies oscillate — for such a domain, MADDPG is shown
    to learn more robustly than DDPG.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach based on policy gradients is the Counterfactual Multi-Agent
    Policy Gradients (COMA) [[167](#bib.bib167)]. COMA was designed for the fully
    centralized setting and the *multiagent credit assignment problem* [[241](#bib.bib241)],
    i.e., how the agents should deduce their contributions when learning in a cooperative
    setting in the presence of only global rewards. Their proposal is to compute a
    *counterfactual baseline*, that is, marginalize out the action of the agent while
    keeping the rest of the other agents’ actions fixed. Then, an advantage function
    can be computed comparing the current <math id="S3.SS5.p9.1.1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S3.SS5.p9.1.1.1.m1.1a"><mi id="S3.SS5.p9.1.1.1.m1.1.1"
    xref="S3.SS5.p9.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS5.p9.1.1.1.m1.1b"><ci id="S3.SS5.p9.1.1.1.m1.1.1.cmml" xref="S3.SS5.p9.1.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p9.1.1.1.m1.1c">Q</annotation></semantics></math>
    value to the counterfactual. This counterfactual baseline has its roots in *difference
    rewards*, which is a method for obtaining the individual contribution of an agent
    in a cooperative multiagent team [[241](#bib.bib241)]. In particular, the *aristocrat*
    utility aims to measure the difference between an agent’s actual action and the
    average action [[31](#bib.bib31)]. The intention would be equivalent to sideline
    the agent by having the agent perform an action where the reward does not depend
    on the agent’s actions, i.e., to consider the reward that would have arisen assuming
    a world without that agent having ever existed (see Section [4.2](#S4.SS2 "4.2
    Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'On the one hand, fully centralized approaches (e.g., COMA) do not suffer from
    non-stationarity but have constrained scalability. On the other hand, independent
    learning agents are better suited to scale but suffer from non-stationarity issues.
    There are some hybrid approaches that learn *a centralized but factored* <math
    id="S3.SS5.p10.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS5.p10.1.m1.1a"><mi id="S3.SS5.p10.1.m1.1.1" xref="S3.SS5.p10.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p10.1.m1.1b"><ci id="S3.SS5.p10.1.m1.1.1.cmml"
    xref="S3.SS5.p10.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p10.1.m1.1c">Q</annotation></semantics></math> value function [[242](#bib.bib242),
    [243](#bib.bib243)]. Value Decomposition Networks (VDNs) [[180](#bib.bib180)]
    decompose a team value function into an additive decomposition of the individual
    value functions. Similarly, QMIX [[181](#bib.bib181)] relies on the idea of factorizing,
    however, instead of sum, QMIX assumes a *mixing network* that combines the local
    values in a non-linear way, which can represent monotonic action-value functions.
    While the mentioned approaches have obtained good empirical results, the factorization
    of value-functions in multiagent scenarios using function approximators (MDRL)
    is an ongoing research topic, with open questions such as how well factorizations
    capture complex coordination problems and how to learn those factorizations [[244](#bib.bib244)]
    (see Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging RL,
    MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Agents modeling agents
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c33d4d724302e5f821fb15a7441cfe4d.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: (a) Deep Policy Inference Q-Network: receives four stacked frames
    as input (similar to DQN, see Figure [2](#S2.F2 "Figure 2 ‣ Value-based methods
    ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). (b) Deep Policy Inference Recurrent Q-Network:
    receives one frame as input and has an LSTM layer instead of a fully connected
    layer (FC). Both approaches [[171](#bib.bib171)] condition the <math id="S3.F6.4.1.m1.1"
    class="ltx_Math" alttext="Q_{M}" display="inline"><semantics id="S3.F6.4.1.m1.1b"><msub
    id="S3.F6.4.1.m1.1.1" xref="S3.F6.4.1.m1.1.1.cmml"><mi id="S3.F6.4.1.m1.1.1.2"
    xref="S3.F6.4.1.m1.1.1.2.cmml">Q</mi><mi id="S3.F6.4.1.m1.1.1.3" xref="S3.F6.4.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml
    encoding="MathML-Content" id="S3.F6.4.1.m1.1c"><apply id="S3.F6.4.1.m1.1.1.cmml"
    xref="S3.F6.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.4.1.m1.1.1.1.cmml" xref="S3.F6.4.1.m1.1.1">subscript</csymbol><ci
    id="S3.F6.4.1.m1.1.1.2.cmml" xref="S3.F6.4.1.m1.1.1.2">𝑄</ci><ci id="S3.F6.4.1.m1.1.1.3.cmml"
    xref="S3.F6.4.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.4.1.m1.1d">Q_{M}</annotation></semantics></math> value outputs on the
    policy features, <math id="S3.F6.5.2.m2.1" class="ltx_Math" alttext="h^{PI}" display="inline"><semantics
    id="S3.F6.5.2.m2.1b"><msup id="S3.F6.5.2.m2.1.1" xref="S3.F6.5.2.m2.1.1.cmml"><mi
    id="S3.F6.5.2.m2.1.1.2" xref="S3.F6.5.2.m2.1.1.2.cmml">h</mi><mrow id="S3.F6.5.2.m2.1.1.3"
    xref="S3.F6.5.2.m2.1.1.3.cmml"><mi id="S3.F6.5.2.m2.1.1.3.2" xref="S3.F6.5.2.m2.1.1.3.2.cmml">P</mi><mo
    lspace="0em" rspace="0em" id="S3.F6.5.2.m2.1.1.3.1" xref="S3.F6.5.2.m2.1.1.3.1.cmml">​</mo><mi
    id="S3.F6.5.2.m2.1.1.3.3" xref="S3.F6.5.2.m2.1.1.3.3.cmml">I</mi></mrow></msup><annotation-xml
    encoding="MathML-Content" id="S3.F6.5.2.m2.1c"><apply id="S3.F6.5.2.m2.1.1.cmml"
    xref="S3.F6.5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F6.5.2.m2.1.1.1.cmml" xref="S3.F6.5.2.m2.1.1">superscript</csymbol><ci
    id="S3.F6.5.2.m2.1.1.2.cmml" xref="S3.F6.5.2.m2.1.1.2">ℎ</ci><apply id="S3.F6.5.2.m2.1.1.3.cmml"
    xref="S3.F6.5.2.m2.1.1.3"><ci id="S3.F6.5.2.m2.1.1.3.2.cmml" xref="S3.F6.5.2.m2.1.1.3.2">𝑃</ci><ci
    id="S3.F6.5.2.m2.1.1.3.3.cmml" xref="S3.F6.5.2.m2.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.F6.5.2.m2.1d">h^{PI}</annotation></semantics></math>,
    which are also used to learn the opponent policy <math id="S3.F6.6.3.m3.1" class="ltx_Math"
    alttext="\pi_{o}" display="inline"><semantics id="S3.F6.6.3.m3.1b"><msub id="S3.F6.6.3.m3.1.1"
    xref="S3.F6.6.3.m3.1.1.cmml"><mi id="S3.F6.6.3.m3.1.1.2" xref="S3.F6.6.3.m3.1.1.2.cmml">π</mi><mi
    id="S3.F6.6.3.m3.1.1.3" xref="S3.F6.6.3.m3.1.1.3.cmml">o</mi></msub><annotation-xml
    encoding="MathML-Content" id="S3.F6.6.3.m3.1c"><apply id="S3.F6.6.3.m3.1.1.cmml"
    xref="S3.F6.6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F6.6.3.m3.1.1.1.cmml" xref="S3.F6.6.3.m3.1.1">subscript</csymbol><ci
    id="S3.F6.6.3.m3.1.1.2.cmml" xref="S3.F6.6.3.m3.1.1.2">𝜋</ci><ci id="S3.F6.6.3.m3.1.1.3.cmml"
    xref="S3.F6.6.3.m3.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.6.3.m3.1d">\pi_{o}</annotation></semantics></math>.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'An important ability for agents to have is to reason about the behaviors of
    other agents by constructing models that make predictions about the modeled agents [[11](#bib.bib11)].
    An early work for modeling agents while using deep neural networks was the Deep
    Reinforcement Opponent Network (DRON) [[169](#bib.bib169)]. The idea is to have
    two networks: one which evaluates <math id="S3.SS6.p1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1"
    xref="S3.SS6.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">Q</annotation></semantics></math>-values
    and a second one that *learns a representation of the opponent’s policy*. Moreover,
    the authors proposed to have several expert networks to combine their predictions
    to get the estimated <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="Q"
    display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1"
    xref="S3.SS6.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">Q</annotation></semantics></math>
    value, the idea being that each expert network captures one type of opponent strategy [[245](#bib.bib245)].
    This is related to previous works in type-based reasoning from game theory [[246](#bib.bib246),
    [139](#bib.bib139)] later applied in AI [[245](#bib.bib245), [11](#bib.bib11),
    [247](#bib.bib247)]. The mixture of experts idea was presented in supervised learning
    where each expert handled a subset of the data (a subtask), and then a gating
    network decided which of the experts should be used [[248](#bib.bib248)].'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'DRON uses hand-crafted features to define the opponent network. In contrast,
    Deep Policy Inference Q-Network (DPIQN) and its recurrent version, DPIRQN [[171](#bib.bib171)]
    learn *policy features* directly from raw observations of the other agents. The
    way to learn these policy features is by means of auxiliary tasks [[84](#bib.bib84),
    [110](#bib.bib110)] (see Sections [2.2](#S2.SS2 "2.2 Deep reinforcement learning
    ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) that provide additional
    learning goals, in this case, the auxiliary task is to learn the opponents’ policies.
    This auxiliary task modifies the loss function by computing an auxiliary loss:
    the cross entropy loss between the inferred opponent policy and the ground truth
    (one-hot action vector) of the opponent. Then, the <math id="S3.SS6.p2.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><mi
    id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p2.1.m1.1b"><ci id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">Q</annotation></semantics></math>
    value function of the learning agent is conditioned on the opponent’s policy features
    (see Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")),
    which aims to reduce the non-stationarity of the environment. The authors used
    an adaptive training procedure to adjust the attention (a weight on the loss function)
    to either emphasize learning the policy features (of the opponent) or the respective
    <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml"
    xref="S3.SS6.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p2.2.m2.1c">Q</annotation></semantics></math> values of the agent.
    An advantage of these approaches is that modeling the agents can work for both
    opponents and teammates [[171](#bib.bib171)].'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: In many previous works an opponent model is learned from observations. Self
    Other Modeling (SOM) [[170](#bib.bib170)] proposed a different approach, this
    is, using *the agent’s own policy as a means to predict the opponent’s actions*.
    SOM can be used in cooperative and competitive settings (with an arbitrary number
    of agents) and infers other agents’ goals. This is important because in the evaluated
    domains, the reward function depends on the goal of the agents. SOM uses two networks,
    one used for computing the agents’ own policy, and a second one used to infer
    the opponent’s goal. The idea is that these networks have the same input parameters
    but with different values (the agent’s or the opponent’s). In contrast to previous
    approaches, SOM is not focused on learning the opponent policy, i.e., a probability
    distribution over next actions, but rather on estimating the opponent’s goal.
    SOM is expected to work best when agents share a set of goals from which each
    agent gets assigned one at the beginning of the episode and the reward structure
    depends on both of their assigned goals. Despite its simplicity, training takes
    longer as an additional optimization step is performed given the other agent’s
    observed actions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a long-standing history of combining game theory and MAL [[2](#bib.bib2),
    [6](#bib.bib6), [193](#bib.bib193)]. From that context, some approaches were inspired
    by influential game theory approaches. Neural Fictitious Self-Play (NFSP) [[173](#bib.bib173)]
    builds on fictitious (self-) play [[190](#bib.bib190), [249](#bib.bib249)], together
    with two deep networks to find *approximate Nash equilibria*^(15)^(15)15Nash equilibrium [[250](#bib.bib250)]
    is a solution concept in game theory in which no agent would choose to deviate
    from its strategy (they are a best response to others’ strategies). This concept
    has been explored in seminal MAL algorithms like Nash-Q learning [[127](#bib.bib127)]
    and Minimax-Q learning [[124](#bib.bib124), [128](#bib.bib128)]. in two-player
    imperfect information games [[251](#bib.bib251)] (for example, consider Poker:
    when it is an agent’s turn to move it does not have access to all information
    about the world). One network learns an *approximate best response* (<math id="S3.SS6.p4.1.m1.1"
    class="ltx_Math" alttext="\epsilon-" display="inline"><semantics id="S3.SS6.p4.1.m1.1a"><mrow
    id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml"><mi id="S3.SS6.p4.1.m1.1.1.2"
    xref="S3.SS6.p4.1.m1.1.1.2.cmml">ϵ</mi><mo id="S3.SS6.p4.1.m1.1.1.3" xref="S3.SS6.p4.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.1.m1.1b"><apply id="S3.SS6.p4.1.m1.1.1.cmml"
    xref="S3.SS6.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS6.p4.1.m1.1.1.1.cmml"
    xref="S3.SS6.p4.1.m1.1.1">limit-from</csymbol><ci id="S3.SS6.p4.1.m1.1.1.2.cmml"
    xref="S3.SS6.p4.1.m1.1.1.2">italic-ϵ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">\epsilon-</annotation></semantics></math>greedy
    over <math id="S3.SS6.p4.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p4.2.m2.1a"><mi id="S3.SS6.p4.2.m2.1.1" xref="S3.SS6.p4.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.2.m2.1b"><ci id="S3.SS6.p4.2.m2.1.1.cmml"
    xref="S3.SS6.p4.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p4.2.m2.1c">Q</annotation></semantics></math> values) to the historical
    behavior of other agents and the second one (called the average network) learns
    to imitate its own past best response behaviour using supervised classification.
    The agent behaves using a mixture of the average and the best response networks
    depending on the probability of an anticipatory parameter [[252](#bib.bib252)].
    Comparisons with DQN in Leduc Hold’em Poker revealed that DQN’s deterministic
    strategy is highly exploitable. Such strategies are sufficient to behave optimally
    in single-agent domains, i.e., MDPs for which DQN was designed. However, imperfect-information
    games generally require stochastic strategies to achieve optimal behaviour [[173](#bib.bib173)].
    DQN learning experiences are both highly correlated over time, and highly focused
    on a narrow state distribution. In contrast to NFSP agents whose experience varies
    more smoothly, resulting in a more stable data distribution, more stable neural
    networks and better performance.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The (N)FSP concept was further generalized in Policy-Space Response Oracles
    (PSRO) [[172](#bib.bib172)], where it was shown that fictitious play is one specific
    meta-strategy distribution over a set of previous (approximate) best responses
    (summarized by a meta-game obtained by empirical game theoretic analysis [[253](#bib.bib253)]),
    but there are a wide variety to choose from. One reason to use mixed meta-strategies
    is that it prevents overfitting^(16)^(16)16Johanson et al. [[254](#bib.bib254)]
    also found “overfitting” when solving large extensive games (e.g., poker) — the
    performance in an abstract game improved but it was worse in the full game. the
    responses to one specific policy, and hence provides a form of opponent/teammate
    regularization. An approximate scalable version of the algorithm leads to a graph
    of agents best-responding independently called Deep Cognitive Hierarchies (DCHs) [[172](#bib.bib172)]
    due to its similarity to behavioral game-theoretic models [[255](#bib.bib255),
    [256](#bib.bib256)].
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimax is a paramount concept in game theory that is roughly described as
    minimizing the worst case scenario (maximum loss) [[251](#bib.bib251)]. Li et
    al. [[183](#bib.bib183)] took the minimax idea as an approach to robustify learning
    in multiagent environments so that the learned robust policy should be able to
    behave well even with strategies not seen during training. They extended the MADDPG
    algorithm [[63](#bib.bib63)] to Minimax Multiagent Deep Deterministic Policy Gradients
    (M3DDPG), which updates policies considering a worst-case scenario: assuming that
    all other agents act adversarially. This yields a minimax learning objective which
    is computationally intractable to directly optimize. They address this issue by
    taking ideas from robust reinforcement learning [[257](#bib.bib257)] which implicitly
    adopts the minimax idea by using the *worst noise* concept [[258](#bib.bib258)].
    In MAL different approaches were proposed to assess the robustness of an algorithm,
    e.g., guarantees of safety [[152](#bib.bib152), [259](#bib.bib259)], security [[260](#bib.bib260)]
    or exploitability [[261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263)].'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches usually learned a model of the other agents as a way to
    predict their behavior. However, they do not explicitly *account for anticipated
    learning of the other agents*, which is the objective of Learning with Opponent-Learning
    Awareness (LOLA) [[64](#bib.bib64)]. LOLA optimizes the expected return *after
    the opponent updates its policy one step*. Therefore, a LOLA agent directly shapes
    the policy updates of other agents to maximize its own reward. One of LOLA’s assumptions
    is having access to opponents’ policy parameters. LOLA builds on previous ideas
    by Zhang and Lesser [[264](#bib.bib264)] where the learning agent predicts the
    opponent’s policy parameter update but only uses it to learn a best response (to
    the anticipated updated parameters).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Theory of mind is part of a group of *recursive reasoning* approaches[[265](#bib.bib265),
    [245](#bib.bib245), [266](#bib.bib266), [267](#bib.bib267)] in which agents have
    explicit beliefs about the mental states of other agents. The mental states of
    other agents may, in turn, also contain beliefs and mental states of other agents,
    leading to a nesting of beliefs [[11](#bib.bib11)]. Theory of Mind Network (ToMnet) [[174](#bib.bib174)]
    starts with a simple premise: when encountering a novel opponent, *the agent should
    already have a strong and rich prior about how the opponent should behave*. ToMnet
    has an architecture composed of three networks: (i) a character network that learns
    from historical information, (ii) a mental state network that takes the character
    output and the recent trajectory, and (iii) the prediction network that takes
    the current state as well as the outputs of the other networks as its input. The
    output of the architecture is open for different problems but in general its goal
    is to predict the opponent’s next action. A main advantage of ToMnet is that it
    can predict general behavior, for all agents; or specific, for a particular agent.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Deep Bayesian Theory of Mind Policy (Bayes-ToMoP) [[175](#bib.bib175)] is another
    algorithm that takes inspiration from theory of mind [[268](#bib.bib268)]. The
    algorithm assumes the opponent has different stationary strategies to act and
    changes among them over time [[269](#bib.bib269)]. Earlier work in MAL dealt with
    this setting, e.g., BPR+ [[270](#bib.bib270)] extends the Bayesian policy reuse^(17)^(17)17Bayesian
    policy reuse assumes an agent with prior experience in the form of a library of
    policies. When a novel task instance occurs, the objective is to reuse a policy
    from its library based on observed signals which correlate to policy performance [[271](#bib.bib271)].
    framework [[271](#bib.bib271)] to multiagent settings (BPR assumes a single-agent
    environment; BPR+ aims to best respond to the opponent in a multiagent game).
    A limitation of BPR+ is that it behaves poorly against itself (self-play), thus,
    Deep Bayes-ToMoP uses theory of mind to provide a higher-level reasoning strategy
    which provides an optimal behavior against BPR+ agents.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Deep BPR+ [[184](#bib.bib184)] is another work inspired by BPR+ which uses neural
    networks as value-function approximators. It not only uses the environment reward
    but also uses the online learned opponent model [[272](#bib.bib272), [273](#bib.bib273)]
    to construct a rectified belief over the opponent strategy. Additionally, it leverages
    ideas from policy distillation [[227](#bib.bib227), [228](#bib.bib228)] and extends
    them to the multiagent case to create a distilled policy network. In this case,
    whenever a new acting policy is learned, distillation is applied to consolidate
    the new updated library which improves in terms of storage and generalization
    (over opponents).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 4 Bridging RL, MAL and MDRL
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section aims to provide directions to promote fruitful cooperations between
    sub-communities. First, we address the pitfall of *deep learning amnesia*, roughly
    described as missing citations to the original works and not exploiting the advancements
    that have been made in the past. We present examples on how ideas originated earlier,
    for example in RL and MAL, were successfully extended to MDRL (see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Second, we outline *lessons
    learned* from the works analyzed in this survey (see Section [4.2](#S4.SS2 "4.2
    Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). Then we point the readers to recent benchmarks for MDRL (see
    Section [4.3](#S4.SS3 "4.3 Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and we discuss the practical
    challenges that arise in MDRL like high computational demands and reproducibility
    (see Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging RL,
    MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Lastly, we pose some
    open research challenges and reflect on their relation with previous open questions
    in MAL [[11](#bib.bib11)] (see Section [4.5](#S4.SS5 "4.5 Open questions ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Avoiding deep learning amnesia: examples in MDRL'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This survey focuses on recent *deep* works, however, in previous sections, when
    describing recent algorithms, we also point to original works that inspired them.
    Schmidhuber said “Machine learning is the science of credit assignment. The machine
    learning community itself profits from proper credit assignment to its members” [[274](#bib.bib274)].
    In this context, we want to avoid committing the pitfall of not giving credit
    to original ideas that were proposed earlier, a.k.a. *deep learning amnesia*.
    Here, we provide some specific examples of research milestones that were studied
    earlier, e.g., RL or MAL, and that now became highly relevant for MDRL. Our purpose
    is to highlight that existent literature contains pertinent ideas and algorithms
    that should not be ignored. On the contrary, they should be examined and cited [[275](#bib.bib275),
    [276](#bib.bib276)] to understand recent developments [[277](#bib.bib277)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with non-stationarity in independent learners
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is well known that using independent learners makes the environment non-stationary
    from the agent’s point of view [[4](#bib.bib4), [123](#bib.bib123)]. Many MAL
    algorithms tried to solve this problem in different ways [[10](#bib.bib10)]. One
    example is *Hyper-Q* [[150](#bib.bib150)] which accounts for the (values of mixed)
    strategies of other agents and includes that information in the state representation,
    which effectively turns the learning problem into a stationary one. Note that
    in this way it is possible to even consider adaptive agents. Foerster et al. [[162](#bib.bib162)]
    make use of this insight to propose their *fingerprint* algorithm in an MDRL problem
    (see Section [3.5](#S3.SS5 "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Other examples include
    the leniency concept [[222](#bib.bib222)] and Hysteretic Q-learning [[8](#bib.bib8)]
    originally presented in MAL, which now have their “deep” counterparts, LDQNs [[35](#bib.bib35)]
    and DEC-HDRQNs[[166](#bib.bib166)], see Section [3.5](#S3.SS5 "3.5 Learning cooperation
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Multiagent credit assignment
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In cooperative multiagent scenarios, it is common to use either *local rewards*,
    unique for each agent, or *global rewards*, which represent the entire group’s
    performance [[278](#bib.bib278)]. However, local rewards are usually harder to
    obtain, therefore, it is common to rely only on the global ones. This raises the
    problem of *credit assignment*: how does a single agent’s actions contribute to
    a system that involves the actions of many agents [[32](#bib.bib32)]. A solution
    that came from MAL research that has proven successful in many scenarios is *difference
    rewards* [[241](#bib.bib241), [278](#bib.bib278), [279](#bib.bib279)], which aims
    to capture an agent’s contribution to the system’s global performance. In particular
    the *aristocrat* utility aims to measure the difference between an agent’s actual
    action and the average action [[31](#bib.bib31)], however, it has a self-consistency
    problem and in practice it is more common to compute the *wonderful life utility* [[280](#bib.bib280),
    [31](#bib.bib31)], which proposes to use a clamping operation that would be equivalent
    to removing that player from the team. COMA [[167](#bib.bib167)] builds on these
    concepts to propose an *advantage function* based on the contribution of the agent,
    which can be efficiently computed with deep neural networks (see Section [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Multitask learning
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the context of RL, multitask learning [[226](#bib.bib226)] is an area that
    develops agents that can act in *several related tasks* rather than just in a
    single one [[281](#bib.bib281)]. *Distillation*, roughly defined as transferring
    the knowledge from a large model to a small model, was a concept originally introduced
    for supervised learning and model compression [[282](#bib.bib282), [228](#bib.bib228)].
    Inspired by those works, Policy distillation [[227](#bib.bib227)] was extended
    to the DRL realm. Policy distillation was used to train a much smaller network
    and to merge *several task-specific policies* into a single policy, i.e., for
    multitask learning. In the MDRL setting, Omidshafiei et al. [[166](#bib.bib166)]
    successfully adapted policy distillation within Dec-HDRQNs to obtain a more general
    multitask multiagent network (see Section [3.5](#S3.SS5 "3.5 Learning cooperation
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). Another example is Deep BPR+ [[184](#bib.bib184)] which uses
    distillation to generalize over multiple opponents (see Section [3.6](#S3.SS6
    "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary tasks
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Jaderberg et al. [[84](#bib.bib84)] introduced the term auxiliary task with
    the insight that (single-agent) environments contain a variety of possible training
    signals (e.g., pixel changes). These tasks are naturally implemented in DRL in
    which the last layer is split into multiple parts (heads), each working on a different
    task. All heads propagate errors into the same shared preceding part of the network,
    which would then try to form representations, in its next-to-last layer, to support
    all the heads [[20](#bib.bib20)]. However, the idea of multiple predictions about
    arbitrary signals was originally suggested for RL, in the context of general value
    functions [[110](#bib.bib110), [20](#bib.bib20)] and there still open problems,
    for example, better theoretical understanding [[109](#bib.bib109), [283](#bib.bib283)].
    In the context of neural networks, early work proposed *hints* that improved the
    network performance and learning time. Suddarth and Kergosien [[284](#bib.bib284)]
    presented a minimal example of a small neural network where it was shown that
    adding an auxiliary task effectively removed local minima. One could think of
    extending these auxiliary tasks to modeling other agents’ behaviors [[285](#bib.bib285),
    [160](#bib.bib160)], which is one of the key ideas that DPIQN and DRPIQN [[171](#bib.bib171)]
    proposed in MDRL settings (see Section [3.6](#S3.SS6 "3.6 Agents modeling agents
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lin [[90](#bib.bib90), [89](#bib.bib89)] proposed the concept of experience
    replay to speed up the credit assignment propagation process in single agent RL.
    This concept became central to many DRL works [[72](#bib.bib72)] (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). However, Lin stated that a condition for
    the ER to be useful is that “the environment should not change over time because
    this makes past experiences irrelevant or even harmful” [[90](#bib.bib90)]. This
    is a problem in domains where many agents are learning since the environment becomes
    non-stationary from the point of view of each agent. Since DRL relies heavily
    on experience replay, this is an issue in MDRL: the non-stationarity introduced
    means that the dynamics that generated the data in the agent’s replay memory no
    longer reflect the current dynamics in which it is learning [[162](#bib.bib162)].
    To overcome this problem different methods have been proposed [[168](#bib.bib168),
    [35](#bib.bib35), [166](#bib.bib166), [178](#bib.bib178)], see Section [4.2](#S4.SS2
    "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”").'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Double estimators
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Double Q-learning [[91](#bib.bib91)] proposed to reduce the overestimation
    of action values in Q-learning, this is caused by using the maximum action value
    as an approximation for the maximum expected action value. Double Q-learning works
    by keeping two Q functions and was proven to convergence to the optimal policy [[91](#bib.bib91)].
    Later this idea was applied to arbitrary function approximators, including deep
    neural networks, i.e., Double DQN [[92](#bib.bib92)], which were naturally applied
    since two networks were already used in DQN (see Section [2.2](#S2.SS2 "2.2 Deep
    reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). These ideas have also been recently applied to MDRL [[178](#bib.bib178)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Lessons learned
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have exemplified how RL and MAL can be extended for MDRL settings. Now, we
    outline general *best practices* learned from the works analyzed throughout this
    paper.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experience replay buffer in MDRL. While some works removed the ER buffer in
    MDRL [[162](#bib.bib162)] it is an important component in many DRL and MDRL algorithms.
    However, using the standard buffer (i.e., keeping <math id="S4.I1.i1.p1.1.m1.4"
    class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle" display="inline"><semantics
    id="S4.I1.i1.p1.1.m1.4a"><mrow id="S4.I1.i1.p1.1.m1.4.4.1" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml"><mo
    stretchy="false" id="S4.I1.i1.p1.1.m1.4.4.1.2" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">⟨</mo><mi
    id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">s</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.3"
    xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.2.2" xref="S4.I1.i1.p1.1.m1.2.2.cmml">a</mi><mo
    id="S4.I1.i1.p1.1.m1.4.4.1.4" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.3.3"
    xref="S4.I1.i1.p1.1.m1.3.3.cmml">r</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.5" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><msup
    id="S4.I1.i1.p1.1.m1.4.4.1.1" xref="S4.I1.i1.p1.1.m1.4.4.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.4.4.1.1.2"
    xref="S4.I1.i1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.1.3"
    xref="S4.I1.i1.p1.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S4.I1.i1.p1.1.m1.4.4.1.6"
    xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S4.I1.i1.p1.1.m1.4b"><list id="S4.I1.i1.p1.1.m1.4.4.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1"><ci
    id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">𝑠</ci><ci id="S4.I1.i1.p1.1.m1.2.2.cmml"
    xref="S4.I1.i1.p1.1.m1.2.2">𝑎</ci><ci id="S4.I1.i1.p1.1.m1.3.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3">𝑟</ci><apply
    id="S4.I1.i1.p1.1.m1.4.4.1.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous"
    id="S4.I1.i1.p1.1.m1.4.4.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S4.I1.i1.p1.1.m1.4.4.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1.2">𝑠</ci><ci
    id="S4.I1.i1.p1.1.m1.4.4.1.1.3.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>)
    will probably fail due to a lack of theoretical guarantees under this setting,
    see Sections [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and [4.1](#S4.SS1 "4.1
    Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and MDRL
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”"). *Adding information in
    the experience tuple* that can help disambiguate the sample is the solution adopted
    in many works, whether a value based method [[168](#bib.bib168), [35](#bib.bib35),
    [166](#bib.bib166), [178](#bib.bib178)] or a policy gradient method [[63](#bib.bib63)].
    In this regard, it is an open question to consider how new DRL ideas could be
    best integrated into the ER [[286](#bib.bib286), [111](#bib.bib111), [287](#bib.bib287),
    [288](#bib.bib288), [96](#bib.bib96)] and how those ideas would fare in a MDRL
    setting.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Centralized learning with decentralized execution. Many MAL works were either
    fully centralized or fully decentralized approaches. However, inspired by *decentralized
    partially observable Markov decison processes* (DEC-POMDPs) [[289](#bib.bib289),
    [290](#bib.bib290)],^(18)^(18)18Centralized planning and decentralized execution
    is also a standard paradigm for multiagent planning [[291](#bib.bib291)]. in MDRL
    this new mixed paradigm has been commonly used  [[168](#bib.bib168), [35](#bib.bib35),
    [181](#bib.bib181), [172](#bib.bib172), [167](#bib.bib167), [63](#bib.bib63)]
    (a notable exception are DEC-HDRQNs [[166](#bib.bib166)] which perform learning
    and execution in a decentralized manner, see Section [3.5](#S3.SS5 "3.5 Learning
    cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). Note that not all real-world problems fit
    into this paradigm and it is more common for robotics or games where a simulator
    is generally available [[162](#bib.bib162)]. The main benefit is that during learning
    *additional information can be used* (e.g., global state, action, or rewards)
    and during execution this information is removed.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter sharing. Another frequent component in many MDRL works is the idea
    of sharing parameters, i.e., training a single network in which agents share their
    weights. Note that, since agents could receive different observations (e.g., in
    partially observable scenarios), they can still behave differently. This method
    was proposed concurrently in different works [[292](#bib.bib292), [162](#bib.bib162)]
    and later it has been successfully applied in many others [[163](#bib.bib163),
    [164](#bib.bib164), [168](#bib.bib168), [180](#bib.bib180), [181](#bib.bib181)].
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recurrent networks. Recurrent neural networks (RNNs) enhanced neural networks
    with a memory capability, however, they suffer from the vanishing gradient problem,
    which renders them inefficient for long-term dependencies [[293](#bib.bib293)].
    However, RNN variants such as LSTMs [[86](#bib.bib86), [294](#bib.bib294)] and
    GRUs (Gated Recurrent Unit) [[295](#bib.bib295)] addressed this challenge. In
    single-agent DRL, DRQN [[85](#bib.bib85)] initially proposed idea of using recurrent
    networks in single-agent *partially observable* environments. Then, Feudal Networks [[229](#bib.bib229)]
    proposed a hierarchical approach [[230](#bib.bib230)], *multiple LSTM networks
    with different time-scales*, i.e., the observation input schedule is different
    for each LSTM network, to create a temporal hierarchy so that it can better address
    the long-term credit assignment challenge for RL problems. Recently, the use of
    recurrent networks has been extended to MDRL to address the challenge of partially
    observability [[158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164), [166](#bib.bib166),
    [180](#bib.bib180), [181](#bib.bib181), [170](#bib.bib170), [171](#bib.bib171),
    [174](#bib.bib174)] for example, in FTW [[179](#bib.bib179)], depicted in Figure [5](#S3.F5
    "Figure 5 ‣ 3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and DRPIRQN [[171](#bib.bib171)]
    depicted in Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”"). See Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    for practical challenges (e.g., training issues) of recurrent networks in MDRL.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting in MAL. In single-agent RL, agents can overfit to the environment [[296](#bib.bib296)].
    A similar problem can occur in multiagent settings [[254](#bib.bib254)], agents
    can overfit, i.e., an agent’s policy can easily get stuck in a local optima and
    the learned policy may be only locally optimal to other agents’ current policies [[183](#bib.bib183)].
    This has the effect of limiting the generalization of the learned policies [[172](#bib.bib172)].
    To reduce this problem, a solution is to have a set of policies (an ensemble)
    and learn from them or best respond to the mixture of them [[172](#bib.bib172),
    [63](#bib.bib63), [169](#bib.bib169)]. Another solution has been to robustify
    algorithms — a robust policy should be able to behave well even with strategies
    different from its training (better generalization) [[183](#bib.bib183)].
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Benchmarks for MDRL
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standardized environments such as the Arcade Learning Environment (ALE) [[297](#bib.bib297),
    [298](#bib.bib298)] and OpenAI Gym [[299](#bib.bib299)] have allowed single-agent
    RL to move beyond toy domains. For DRL there are open-source frameworks that provide
    compact and reliable implementations of some state-of-the-art DRL algorithms [[300](#bib.bib300)].
    Even though MDRL is a recent area, there are now a number of open sourced simulators
    and benchmarks to use with different characteristics, which we describe below.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91df0d0e84687b786d6dc17105cf6138.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: (a) Multiagent object transportation
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d77570179eaad9ecdf463801adbe644.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: (b) Pommerman
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: (a) A fully cooperative benchmark with two agents, Multiagent Object
    Trasportation. (b) A mixed cooperative-competitive domain with four agents, Pommerman.
    For more MDRL benchmarks see Section [4.3](#S4.SS3 "4.3 Benchmarks for MDRL ‣
    4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”").'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fully Cooperative Multiagent Object Transporation Problems (CMOTPs)^(19)^(19)19[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    were originally presented by Busoniu et al. [[36](#bib.bib36)] as a simple two-agent
    coordination problem in MAL. Palmer et al. [[35](#bib.bib35)] proposed two pixel-based
    extensions to the original setting which include narrow passages that test the
    agents’ ability to master fully-cooperative sub-tasks, stochastic rewards and
    noisy observations, see Figure [7(a)](#S4.F7.sf1 "In Figure 7 ‣ 4.3 Benchmarks
    for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Apprentice Firemen Game^(20)^(20)20[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    (inspired by the classic climb game [[126](#bib.bib126)]) is another two-agent
    pixel-based environment that simultaneously confronts learners with four pathologies
    in MAL: relative overgeneralization, stochasticity, the moving target problem,
    and alter exploration problem [[97](#bib.bib97)].'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pommerman [[301](#bib.bib301)] is a multiagent benchmark useful for testing
    cooperative, competitive and mixed (cooperative and competitive) scenarios. It
    supports partial observability and communication among agents, see Figure [7(b)](#S4.F7.sf2
    "In Figure 7 ‣ 4.3 Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”"). Pommerman is a very challenging
    domain from the exploration perspective as the rewards are very sparse and delayed [[302](#bib.bib302)].
    A recent competition was held during NeurIPS-2018^(21)^(21)21[https://www.pommerman.com/](https://www.pommerman.com/)
    and the top agents from that competition are available for training purposes.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starcraft Multiagent Challenge [[303](#bib.bib303)] is based on the real-time
    strategy game StarCraft II and focuses on micromanagement challenges,^(22)^(22)22[https://github.com/oxwhirl/smac](https://github.com/oxwhirl/smac)
    that is, fine-grained control of individual units, where each unit is controlled
    by an independent agent that must act based on local observations. It is accompanied
    by a MDRL framework including state-of-the-art algorithms (e.g., QMIX and COMA).^(23)^(23)23[https://github.com/oxwhirl/pymarl](https://github.com/oxwhirl/pymarl)
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Multi-Agent Reinforcement Learning in Malmö (MARLÖ) competition [[304](#bib.bib304)]
    is another multiagent challenge with multiple cooperative 3D games^(24)^(24)24[https://github.com/crowdAI/marlo-single-agent-starter-kit/](https://github.com/crowdAI/marlo-single-agent-starter-kit/)
    within Minecraft. The scenarios were created with the open source Malmö platform [[305](#bib.bib305)],
    providing examples of how a wider range of multiagent cooperative, competitive
    and mixed scenarios can be experimented on within Minecraft.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanabi is a cooperative multiplayer card game (two to five players). The main
    characteristic of the game is that players do not observe their own cards but
    other players can reveal information about them. This makes an interesting challenge
    for learning algorithms in particular in the context of self-play learning and
    ad-hoc teams [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)]. The
    Hanabi Learning Environment [[309](#bib.bib309)] was recently released^(25)^(25)25[https://github.com/deepmind/hanabi-learning-environment](https://github.com/deepmind/hanabi-learning-environment)
    and it is accompanied with a baseline (deep RL) agent [[310](#bib.bib310)].
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arena [[311](#bib.bib311)] is platform for multiagent research^(26)^(26)26[https://github.com/YuhangSong/Arena-BuildingToolkit](https://github.com/YuhangSong/Arena-BuildingToolkit)
    based on the Unity engine [[312](#bib.bib312)]. It has 35 multiagent games (e.g.,
    social dilemmas) and supports communication among agents. It has basseline implementations
    of recent DRL algorithms such as independent PPO learners.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MuJoCo Multiagent Soccer [[313](#bib.bib313)] uses the MuJoCo physics engine [[202](#bib.bib202)].
    The environment simulates a 2 vs. 2 soccer game with agents having a 3-dimensional
    action space.^(27)^(27)27[https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer)
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural MMO [[314](#bib.bib314)] is a research platform^(28)^(28)28[https://github.com/openai/neural-mmo](https://github.com/openai/neural-mmo)
    inspired by the human game genre of Massively Multiplayer Online (MMO) Role-Playing
    Games. These games involve a large, variable number of players competing to survive.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4 Practical challenges in MDRL
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we take a more critical view with respect to MDRL and highlight
    different practical challenges that already happen in DRL and that are likely
    to occur in MDRL such as reproducibility, hyperparameter tuning, the need of computational
    resources and conflation of results. We provide pointers on how we think those
    challenges could be (partially) addressed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility, troubling trends and negative results
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducibility is a challenge in RL which is only aggravated in DRL due to
    different sources of stochasticity: baselines, hyperparameters, architectures [[315](#bib.bib315),
    [316](#bib.bib316)] and random seeds [[317](#bib.bib317)]. Moreover, DRL does
    not have common practices for statistical testing [[318](#bib.bib318)] which has
    led to bad practices such as only reporting the results when algorithms perform
    well, sometimes referred as *cherry picking* [[319](#bib.bib319)] (Azizzadenesheli
    also describes *cherry planting* as adapting an environment to a specific algorithm [[319](#bib.bib319)]).
    We believe that together with following the advice on how to design experiments
    and report results [[320](#bib.bib320)], the community would also benefit from
    reporting *negative results* [[321](#bib.bib321), [322](#bib.bib322), [318](#bib.bib318),
    [323](#bib.bib323)] for carefully designed hypothesis and experiments.^(29)^(29)29This
    idea was initially inspired by the Workshop “Critiquing and Correcting Trends
    in Machine Learning” at NeurIPS 2018 where it was possible to submit *Negative
    results* papers: “Papers which show failure modes of existing algorithms or suggest
    new approaches which one might expect to perform well but which do not. The aim
    is to provide a venue for work which might otherwise go unpublished but which
    is still of interest to the community.” [https://ml-critique-correct.github.io/](https://ml-critique-correct.github.io/)
    However, we found very few papers with this characteristic[[324](#bib.bib324),
    [325](#bib.bib325), [326](#bib.bib326)] — we note that this is not encouraged
    in the ML community; moreover, negative results reduce the chance of paper acceptance [[320](#bib.bib320)].
    In this regard, we ask the community to reflect on these practices and find ways
    to remove these obstacles.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Implementation challenges and hyperparameter tuning
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One problem is that canonical implementations of DRL algorithms often contain
    additional non-trivial optimizations — these are sometimes necessary for the algorithms
    to achieve good performance [[79](#bib.bib79)]. A recent study by Tucker et al. [[59](#bib.bib59)]
    found that several published works on action-dependant baselines contained bugs
    and errors — those were the real reason of the high performance in the experimental
    results, not the proposed method. Melis et al. [[327](#bib.bib327)] compared a
    series of works with increasing innovations in network architectures and the vanilla
    LSTMs [[86](#bib.bib86)] (originally proposed in 1997). The results showed that,
    when properly tuned, LSTMs outperformed the more recent models. In this context,
    Lipton and Steinhardt noted that the community may have benefited more by learning
    the details of the hyperparameter tuning [[320](#bib.bib320)]. A partial reason
    for this surprising result might be that this type of networks are known for being
    difficult to train [[293](#bib.bib293)] and there are recent works in DRL that
    report problems when using recurrent networks [[182](#bib.bib182), [328](#bib.bib328),
    [329](#bib.bib329), [330](#bib.bib330)]. Another known complication is catastrophic
    forgetting (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) with recent examples in
    DRL [[157](#bib.bib157), [92](#bib.bib92)] — we expect that these issues would
    likely occur in MDRL. The effects of hyperparameter tuning were analyzed in more
    detail in DRL by Henderson et al. [[315](#bib.bib315)], who arrived at the conclusion
    that hyperparameters can have significantly different effects across algorithms
    (they tested TRPO, DDPG, PPO and ACKTR) and environments since there is an intricate
    interplay among them [[315](#bib.bib315)]. The authors urge the community to report
    *all* parameters used in the experimental evaluations for accurate comparison
    — we encourage a similar behavior for MDRL. Note that hyperparameter tuning is
    related to the troubling trend of cherry picking in that it can show a carefully
    picked set of parameters that make an algorithm work (see previous challenge).
    Lastly, note that hyperparameter tuning is computationally very expensive, which
    relates to the connection with the following challenge of computational demands.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Computational resources
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep RL usually requires millions of interactions for an agent to learn [[331](#bib.bib331)],
    i.e., low sample efficiency [[332](#bib.bib332)], which highlights the need for
    large computational infrastructure in general. The original A3C implementation [[93](#bib.bib93)]
    uses 16 CPU workers for 4 days to learn to play an Atari game with a total of
    200M training frames^(30)^(30)30It is sometimes unclear in the literature what
    is the meaning of frame due to the “frame skip” technique. It is therefore suggested
    to refer to “game frames” and “training frames” [[333](#bib.bib333)]. (results
    are reported for 57 Atari games). Distributed PPO used 64 workers (presumably
    one CPU per worker, although this is not clearly stated in the paper) for 100
    hours (more than 4 days) to learn locomotion tasks [[117](#bib.bib117)]. In MDRL,
    for example, the Atari Pong game, agents were trained for 50 epochs, 250k time
    steps each, for a total of 1.25M training frames [[155](#bib.bib155)]. The FTW
    agent [[179](#bib.bib179)] uses 30 agents (processes) in parallel and every training
    game lasts for five minues; FTW agents were trained for approximately 450K games
    <math id="S4.SS4.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics
    id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">\approx</annotation></semantics></math>4.2
    years. These examples highlight the computational demands sometimes needed within
    DRL and MDRL.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent works have reduced the learning of an Atari game to minutes (Stooke
    and Abbeel [[334](#bib.bib334)] trained DRL agents in less than one hour with
    hardware consisting of 8 GPUs and 40 cores). However, this is (for now) the exception
    and computational infrastructure is a major bottleneck for doing DRL and MDRL,
    especially for those who do not have such large compute power (e.g., most companies
    and most academic research groups) [[212](#bib.bib212), [322](#bib.bib322)].^(31)^(31)31One
    recent effort by Beeching et al. [[212](#bib.bib212)] proposes to use only “mid-range
    hardware” (8 CPUs and 1 GPU) to train deep RL agents. Within this context we propose
    two ways to address this problem. (1) Raising awareness: For DRL we found few
    works that study the computational demands of recent algorithms [[335](#bib.bib335),
    [331](#bib.bib331)]. For MDRL most published works do not provide information
    regarding computational resources used such as CPU/GPU usage, memory demands,
    and wall-clock computation. Therefore, the first way to tackle this issue is by
    raising awareness and encouraging authors to report metrics about computational
    demands for accurately comparison and evaluation. (2) Delve into algorithmic contributions.
    Another way to address these issues is to prioritize the algorithmic contribution
    for the new MDRL algorithms rather than the computational resources spent. Indeed,
    for this to work, it needs to be accompanied with high-quality reviewers.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'We have argued to raise awareness on the computational demands and report results,
    however, there is still the open question on *how* and *what* to measure/report.
    There are several dimensions to measure efficiency: sample efficiency is commonly
    measured by counting state-action pairs used for training; computational efficiency
    could be measured by number of CPUs/GPUs and days used for training. How do we
    measure the impact of other resources, such as external data sources or annotations?^(32)^(32)32NeurIPS
    2019 hosts the “MineRL Competition on Sample Efficient Reinforcement Learning
    using Human Priors” where the primary goal of the competition is to foster the
    development of algorithms which can efficiently leverage human demonstrations
    to drastically reduce the number of samples needed to solve complex, hierarchical,
    and sparse environments [[336](#bib.bib336)]. Similarly, do we need to differentiate
    the computational needs of the algorithm itself versus the environment it is run
    in? We do not have the answers, however, we point out that current standard metrics
    might not be entirely comprehensive.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we believe that high compute based methods act as a frontier to
    showcase benchmarks [[19](#bib.bib19), [18](#bib.bib18)], i.e., they show what
    results are possible as data and compute is scaled up (e.g., OpenAI Five generates
    180 years of gameplay data each day using 128,000 CPU cores and 256 GPUs [[18](#bib.bib18)];
    AlphaStar uses 200 years of Starcraft II gameplay [[19](#bib.bib19)]); however,
    lighter compute based algorithmic methods can also yield significant contributions
    to better tackle real-world problems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Occam’s razor and ablative analysis
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finding the simplest context that exposes the innovative research idea remains
    challenging, and if ignored leads to a conflation of fundamental research (working
    principles in the most abstract setting) and applied research (working systems
    as complete as possible). In particular, some deep learning papers are presented
    as learning from pixels without further explanation, while object-level representations
    would have already exposed the algorithmic contribution. This still makes sense
    to remain comparable with established benchmarks (e.g., OpenAI Gym [[299](#bib.bib299)]),
    but less so if custom simulations are written without open source access, as it
    introduces unnecessary variance in pixel-level representations and artificially
    inflates computational resources (see previous point about *computational resources*).^(33)^(33)33Cuccu,
    Togelius and Cudré-Mauroux achieved state-of-the-art policy learning in Atari
    games with only 6 to 18 neurons [[337](#bib.bib337)]. The main idea was to decouple
    image processing from decision-making. In this context there are some notable
    exceptions where the algorithmic contribution is presented in a minimal setting
    and then results are scaled into complex settings: LOLA [[64](#bib.bib64)] first
    presented a minimalist setting with a two-player two-action game and then with
    a more complex variant; similarly, QMIX [[181](#bib.bib181)] presented its results
    in a two-step (matrix) game and then in the more involved Starcraft II micromanagement
    domain [[303](#bib.bib303)].'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Open questions
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, here we present some open questions for MDRL and point to suggestions
    on how to approach them. We believe that there are solid ideas in earlier literature
    and we refer the reader to Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") to avoid deep learning amnesia.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the challenge of sparse and delayed rewards.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Recent MDRL competitions and environments have complex scenarios where many
    actions are taken before a reward signal is available (see Section [4.3](#S4.SS3
    "4.3 Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). This sparseness is already a challenge for
    RL [[20](#bib.bib20), [338](#bib.bib338)] where approaches such as count-based
    exploration/intrinsic motivation [[196](#bib.bib196), [339](#bib.bib339), [340](#bib.bib340),
    [341](#bib.bib341), [342](#bib.bib342)] and hierarchical learning [[343](#bib.bib343),
    [344](#bib.bib344), [111](#bib.bib111)] have been proposed to address it — in
    MDRL this is even more problematic since the agents not only need to learn basic
    behaviors (like in DRL), but also to learn the strategic element (e.g., competitive/collaborative)
    embedded in the multiagent setting. To address this issue, recent MDRL approaches
    applied *dense* rewards [[206](#bib.bib206), [205](#bib.bib205), [204](#bib.bib204)]
    (a concept originated in RL) at each step to allow the agents to learn basic motor
    skills and then decrease these *dense* rewards over time in favor of the environmental
    reward [[158](#bib.bib158)], see Section [3.3](#S3.SS3 "3.3 Emergent behaviors
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”"). Recent works like OpenAI Five [[18](#bib.bib18)] uses hand-crafted
    intermediate rewards to accelerate the learning and FTW [[179](#bib.bib179)] lets
    the agents learn their internal rewards by a hierarchical two-tier optimization.
    In single agent domains, RUDDER [[345](#bib.bib345)] has been recently proposed
    for such delayed sparse reward problems. RUDDER generates a new MDP with *more
    intermediate rewards* whose optimal solution is still an optimal solution to the
    original MDP. This is achieved by using LSTM networks to redistribute the original
    sparse reward to earlier state-action pairs and automatically provide reward shaping.
    How to best extend RUDDER to multiagent domains is an open avenue of research.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the role of self-play.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Self-play is a cornerstone in MAL with impressive results [[147](#bib.bib147),
    [127](#bib.bib127), [145](#bib.bib145), [346](#bib.bib346), [143](#bib.bib143)].
    While notable results had also been shown in MDRL [[173](#bib.bib173), [193](#bib.bib193)],
    recent works have also shown that *plain* self-play does not yield the best results.
    However, adding diversity, i.e., evolutionary methods [[239](#bib.bib239), [240](#bib.bib240),
    [233](#bib.bib233), [234](#bib.bib234)] or sampling-based methods, have shown
    good results [[158](#bib.bib158), [179](#bib.bib179), [159](#bib.bib159)]. A drawback
    of these solutions is the additional computational requirements since they need
    either parallel training (more CPU computation) or memory requirements. Then,
    it is still an open problem to improve the computational efficiency of these previously
    proposed successful methods, i.e., achieving similar training stability with smaller
    population sizes that uses fewer CPU workers in MAL and MDRL (see Section [4.4](#S4.SS4
    "4.4 Practical challenges in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”") and Albrecht et al. [[11](#bib.bib11),
    Section 5.5]).'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the challenge of the combinatorial nature of MDRL.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Monte Carlo tree search (MCTS) [[347](#bib.bib347)] has been the backbone of
    the major breakthroughs behind AlphaGo [[14](#bib.bib14)] and AlphaGo Zero [[15](#bib.bib15)]
    that combined search and DRL. A recent work [[348](#bib.bib348)] has outlined
    how search and RL can be better combined for potentially new methods. However,
    for multiagent scenarios, there is an additional challenge of the exponential
    growth of all the agents’ action spaces for centralized methods [[349](#bib.bib349)].
    One way to tackle this challenge within multiagent scenarios is the use of search
    parallelization [[350](#bib.bib350), [351](#bib.bib351)]. Given more scalable
    planners, there is room for research in combining these techniques in MDRL settings.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To learn complex multiagent interactions some type of abstraction [[352](#bib.bib352)]
    is often needed, for example, factored value functions [[353](#bib.bib353), [354](#bib.bib354),
    [242](#bib.bib242), [243](#bib.bib243), [355](#bib.bib355), [356](#bib.bib356)]
    (see QMIX and VDN in Section [3.5](#S3.SS5 "3.5 Learning cooperation ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") for recent work in MDRL) try to exploit independence among agents
    through (factored) structure; however, in MDRL there are still open questions
    such as understanding their representational power [[244](#bib.bib244)] (e.g.,
    the accuracy of the learned Q-function approximations) and how to learn those
    factorizations, where ideas from transfer planning techniques could be useful [[357](#bib.bib357),
    [103](#bib.bib103)]. In transfer planning the idea is to define a simpler “source
    problem” (e.g., with fewer agents), in which the agent(s) can plan [[357](#bib.bib357)]
    or learn [[103](#bib.bib103)]; since it is less complex than the real multiagent
    problem, issues such as the non-stationarity of the environment can be reduced/removed.
    Lastly, another related idea are *influence* abstractions [[358](#bib.bib358),
    [359](#bib.bib359), [10](#bib.bib10)], where instead of learning a complex multiagent
    model, these methods try to build smaller models based on the influence agents
    can exert on one another. While this has not been sufficiently explored in actual
    multiagent settings, there is some evidence that these ideas can lead to effective
    inductive biases, improving effectiveness of DRL in such local abstractions [[360](#bib.bib360)].'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Conclusions
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep reinforcement learning has shown recent success on many fronts [[13](#bib.bib13),
    [14](#bib.bib14), [16](#bib.bib16)] and a natural next step is to test multiagent
    scenarios. However, learning in multiagent environments is fundamentally more
    difficult due to non-stationarity, the increase of dimensionality, and the credit-assignment
    problem, among other factors [[1](#bib.bib1), [5](#bib.bib5), [10](#bib.bib10),
    [147](#bib.bib147), [241](#bib.bib241), [361](#bib.bib361), [97](#bib.bib97)].
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey provides broad overview of recent works in the emerging area of
    Multiagent Deep Reinforcement Learning (MDRL). First, we categorized recent works
    into four different topics: emergent behaviors, learning communication, learning
    cooperation, and agents modeling agents. Then, we exemplified how key components
    (e.g., experience replay and difference rewards) originated in RL and MAL need
    to be adapted to work in MDRL. We provided general lessons learned applicable
    to MDRL, pointed to recent multiagent benchmarks and highlighted some open research
    problems. Finally, we also reflected on the practical challenges such as computational
    demands and reproducibility in MDRL.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Our conclusions of this work are that while the number of works in DRL and
    MDRL are notable and represent important milestones for AI, at the same time we
    acknowledge there are also open questions in both (deep) single-agent learning [[38](#bib.bib38),
    [298](#bib.bib298), [362](#bib.bib362), [79](#bib.bib79)] and multiagent learning [[363](#bib.bib363),
    [364](#bib.bib364), [365](#bib.bib365), [366](#bib.bib366), [367](#bib.bib367),
    [368](#bib.bib368)]. Our view is that there are practical issues within MDRL that
    hinder its scientific progress: the necessity of high compute power, complicated
    reproducibility (e.g., hyperparameter tuning), and the lack of sufficient encouragement
    for publishing negative results. However, we remain highly optimistic of the multiagent
    community and hope this work serves to raise those issues, encounter good solutions,
    and ultimately take advantage of the existing literature and resources available
    to move the area in the right direction.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Chao Gao, Nidhi Hegde, Gregory Palmer, Felipe Leno Da
    Silva and Craig Sherstan for reading earlier versions of this work and providing
    feedback, to April Cooper for her visual designs for the figures in the article,
    to Frans Oliehoek, Sam Devlin, Marc Lanctot, Nolan Bard, Roberta Raileanu, Angeliki
    Lazaridou, and Yuhang Song for clarifications in their areas of expertise, to
    Baoxiang Wang for his suggestions on recent deep RL works, to Michael Kaisers,
    Daan Bloembergen, and Katja Hofmann for their comments about the practical challenges
    of MDRL, and to the editor and three anonymous reviewers whose comments and suggestions
    increased the quality of this work.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] P. Stone, M. M. Veloso, Multiagent Systems - A Survey from a Machine Learning
    Perspective., Autonomous Robots 8 (3) (2000) 345–383.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer,
    what is the question?, Artificial Intelligence 171 (7) (2007) 365–377.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] E. Alonso, M. D’inverno, D. Kudenko, M. Luck, J. Noble, Learning in multi-agent
    systems, Knowledge Engineering Review 16 (03) (2002) 1–8.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K. Tuyls, G. Weiss, Multiagent learning: Basics, challenges, and prospects,
    AI Magazine 33 (3) (2012) 41–52.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. Busoniu, R. Babuska, B. De Schutter, A Comprehensive Survey of Multiagent
    Reinforcement Learning, IEEE Transactions on Systems, Man and Cybernetics, Part
    C (Applications and Reviews) 38 (2) (2008) 156–172.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Nowé, P. Vrancx, Y.-M. De Hauwere, Game theory and multi-agent reinforcement
    learning, in: Reinforcement Learning, Springer, 2012, pp. 441–470.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] L. Panait, S. Luke, Cooperative Multi-Agent Learning: The State of the
    Art, Autonomous Agents and Multi-Agent Systems 11 (3).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] L. Matignon, G. J. Laurent, N. Le Fort-Piat, Independent reinforcement
    learners in cooperative Markov games: a survey regarding coordination problems,
    Knowledge Engineering Review 27 (1) (2012) 1–31.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] D. Bloembergen, K. Tuyls, D. Hennes, M. Kaisers, Evolutionary Dynamics
    of Multi-Agent Learning: A Survey., Journal of Artificial Intelligence Research
    53 (2015) 659–697.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] P. Hernandez-Leal, M. Kaisers, T. Baarslag, E. Munoz de Cote, [A Survey
    of Learning in Multiagent Environments - Dealing with Non-Stationarity](http://arxiv.org/abs/1707.09183).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1707.09183](http://arxiv.org/abs/1707.09183)
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[11] S. V. Albrecht, P. Stone, Autonomous agents modelling other agents: A
    comprehensive survey and open problems, Artificial Intelligence 258 (2018) 66–95.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] F. L. Silva, A. H. R. Costa, A survey on transfer learning for multiagent
    reinforcement learning systems, Journal of Artificial Intelligence Research 64
    (2019) 645–703.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis,
    Human-level control through deep reinforcement learning, Nature 518 (7540) (2015)
    529–533.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe,
    J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,
    T. Graepel, D. Hassabis, Mastering the game of Go with deep neural networks and
    tree search, Nature 529 (7587) (2016) 484–489.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton, et al., Mastering the game of Go without
    human knowledge, Nature 550 (7676) (2017) 354.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill, N. Bard, T. Davis,
    K. Waugh, M. Johanson, M. Bowling, DeepStack: Expert-level artificial intelligence
    in heads-up no-limit poker, Science 356 (6337) (2017) 508–513.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N. Brown, T. Sandholm, Superhuman AI for heads-up no-limit poker: Libratus
    beats top professionals, Science 359 (6374) (2018) 418–424.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Open AI Five, [https://blog.openai.com/openai-five](https://blog.openai.com/openai-five),
    [Online; accessed 7-September-2018] (2018).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki,
    A. Dudzik, A. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan, M. Kroiss,
    I. Danihelka, J. Agapiou, J. Oh, V. Dalibard, D. Choi, L. Sifre, Y. Sulsky, S. Vezhnevets,
    J. Molloy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z. Wang, T. Pfaff, T. Pohlen,
    Y. Wu, D. Yogatama, J. Cohen, K. McKinney, O. Smith, T. Schaul, T. Lillicrap,
    C. Apps, K. Kavukcuoglu, D. Hassabis, D. Silver, AlphaStar: Mastering the Real-Time
    Strategy Game StarCraft II, [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
    (2019).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, 2nd
    Edition, MIT Press, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015)
    436.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Schmidhuber, Deep learning in neural networks: An overview, Neural
    networks 61 (2015) 85–117.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, [A Brief
    Survey of Deep Reinforcement Learning](http://arXiv.org/abs/1708.05866v2) .'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1708.05866v2](http://arXiv.org/abs/1708.05866v2)
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[24] V. François-Lavet, P. Henderson, R. Islam, M. G. Bellemare, J. Pineau,
    et al., An introduction to deep reinforcement learning, Foundations and Trends®
    in Machine Learning 11 (3-4) (2018) 219–354.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Yang, J. Hao, M. Sun, Z. Wang, C. Fan, G. Strbac, Recurrent Deep Multiagent
    Q-Learning for Autonomous Brokers in Smart Grid, in: Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence, Stockholm, Sweden,
    2018.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Zhao, G. Qiu, Z. Guan, W. Zhao, X. He, Deep reinforcement learning
    for sponsored search real-time bidding, in: Proceedings of the 24th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining, ACM, 2018, pp.
    1021–1030.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] B. M. Lake, T. D. Ullman, J. Tenenbaum, S. Gershman, Building machines
    that learn and think like people, Behavioral and Brain Sciences 40.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. Tamar, S. Levine, P. Abbeel, Y. Wu, G. Thomas, Value Iteration Networks.,
    NIPS (2016) 2154–2162.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] G. Papoudakis, F. Christianos, A. Rahman, S. V. Albrecht, Dealing with
    non-stationarity in multi-agent deep reinforcement learning, arXiv preprint arXiv:1906.04737.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] T. T. Nguyen, N. D. Nguyen, S. Nahavandi, Deep reinforcement learning
    for multi-agent systems: A review of challenges, solutions and applications, arXiv
    preprint arXiv:1812.11794.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. H. Wolpert, K. Tumer, Optimal payoff functions for members of collectives,
    in: Modeling complexity in economic and social systems, 2002, pp. 355–369.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. K. Agogino, K. Tumer, Unifying Temporal and Structural Credit Assignment
    Problems., in: Proceedings of 17th International Conference on Autonomous Agents
    and Multiagent Systems, 2004.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] N. Fulda, D. Ventura, Predicting and Preventing Coordination Problems
    in Cooperative Q-learning Systems, in: Proceedings of the Twentieth International
    Joint Conference on Artificial Intelligence, Hyderabad, India, 2007, pp. 780–785.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] E. Wei, S. Luke, Lenient Learning in Independent-Learner Stochastic Cooperative
    Games., Journal of Machine Learning Research.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] G. Palmer, K. Tuyls, D. Bloembergen, R. Savani, Lenient Multi-Agent Deep
    Reinforcement Learning., in: International Conference on Autonomous Agents and
    Multiagent Systems, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] L. Busoniu, R. Babuska, B. De Schutter, Multi-agent reinforcement learning:
    An overview, in: D. Srinivasan, L. C. Jain (Eds.), Innovations in Multi-Agent
    Systems and Applications - 1, Springer Berlin Heidelberg, Berlin, Heidelberg,
    2010, pp. 183–221.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. Li, [Deep reinforcement learning: An overview](http://arxiv.org/abs/1701.07274),
    CoRR abs/1701.07274. [arXiv:1701.07274](http://arxiv.org/abs/1701.07274).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1701.07274](http://arxiv.org/abs/1701.07274)
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[38] A. Darwiche, Human-level intelligence or animal-like abilities?, Commun.
    ACM 61 (10) (2018) 56–67.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M. Wiering, M. Van Otterlo, Reinforcement learning, Adaptation, learning,
    and optimization 12.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L. P. Kaelbling, M. L. Littman, A. W. Moore, Reinforcement learning: A
    survey, Journal of artificial intelligence research 4 (1996) 237–285.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. L. Puterman, Markov decision processes: Discrete stochastic dynamic
    programming, John Wiley & Sons, Inc., 1994.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. R. Cassandra, Exact and approximate algorithms for partially observable
    Markov decision processes, Ph.D. thesis, Computer Science Department, Brown University
    (May 1998).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] K. J. Astrom, Optimal control of Markov processes with incomplete state
    information, Journal of mathematical analysis and applications 10 (1) (1965) 174–205.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. Bellman, A Markovian decision process, Journal of Mathematics and Mechanics
    6 (5) (1957) 679–684.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Watkins, Learning from delayed rewards, Ph.D. thesis, King’s College,
    Cambridge, UK (Apr. 1989).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] T. Kamihigashi, C. Le Van, [Necessary and Sufficient Conditions for a
    Solution of the Bellman Equation to be the Value Function: A General Principle](https://halshs.archives-ouvertes.fr/halshs-01159177).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://halshs.archives-ouvertes.fr/halshs-01159177](https://halshs.archives-ouvertes.fr/halshs-01159177)
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[47] J. Tsitsiklis, Asynchronous stochastic approximation and Q-learning, Machine
    Learning 16 (3) (1994) 185–202.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. Jaakkola, M. I. Jordan, S. P. Singh, Convergence of stochastic iterative
    dynamic programming algorithms, in: Advances in neural information processing
    systems, 1994, pp. 703–710.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C. Szepesvári, M. L. Littman, A unified analysis of value-function-based
    reinforcement-learning algorithms, Neural computation 11 (8) (1999) 2017–2060.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] E. Even-Dar, Y. Mansour, Learning rates for Q-learning, Journal of Machine
    Learning Research 5 (Dec) (2003) 1–25.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Szepesvári, Algorithms for reinforcement learning, Synthesis lectures
    on artificial intelligence and machine learning 4 (1) (2010) 1–103.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S. Singh, T. Jaakkola, M. L. Littman, C. Szepesvári, Convergence results
    for single-step on-policy reinforcement-learning algorithms, Machine learning
    38 (3) (2000) 287–308.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] H. Van Seijen, H. Van Hasselt, S. Whiteson, M. Wiering, A theoretical
    and empirical analysis of Expected Sarsa, in: IEEE Symposium on Adaptive Dynamic
    Programming and Reinforcement Learning, Nashville, TN, USA, 2009, pp. 177–184.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] V. R. Konda, J. Tsitsiklis, Actor-critic algorithms, in: Advances in Neural
    Information Processing Systems, 2000.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, Policy Gradient
    Methods for Reinforcement Learning with Function Approximation., in: Advances
    in Neural Information Processing Systems, 2000.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] R. J. Williams, Simple statistical gradient-following algorithms for connectionist
    reinforcement learning, Machine learning 8 (3-4) (1992) 229–256.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Liu, Y. Feng, Y. Mao, D. Zhou, J. Peng, Q. Liu, Action-depedent control
    variates for policy optimization via stein’s identity, in: International Conference
    on Learning Representations, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, S. Levine, Q-prop: Sample-efficient
    policy gradient with an off-policy critic, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] G. Tucker, S. Bhupatiraju, S. Gu, R. E. Turner, Z. Ghahramani, S. Levine,
    The mirage of action-dependent baselines in reinforcement learning, in: International
    Conference on Machine Learning, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, P. Moritz, Trust Region
    Policy Optimization., in: 31st International Conference on Machine Learning, Lille,
    France, 2015.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller,
    Deterministic policy gradient algorithms, in: ICML, 2014.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] R. Hafner, M. Riedmiller, Reinforcement learning in feedback control,
    Machine learning 84 (1-2) (2011) 137–169.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, I. Mordatch, Multi-Agent
    Actor-Critic for Mixed Cooperative-Competitive Environments., in: Advances in
    Neural Information Processing Systems, 2017, pp. 6379–6390.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, I. Mordatch,
    Learning with Opponent-Learning Awareness., in: Proceedings of 17th International
    Conference on Autonomous Agents and Multiagent Systems, Stockholm, Sweden, 2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, in:
    International Conference on Learning Representations, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] L. D. Pyeatt, A. E. Howe, et al., Decision tree function approximation
    in reinforcement learning, in: Proceedings of the third international symposium
    on adaptive systems: evolutionary computation and probabilistic graphical models,
    Vol. 2, Cuba, 2001, pp. 70–77.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. S. Sutton, Generalization in reinforcement learning: Successful examples
    using sparse coarse coding, in: Advances in neural information processing systems,
    1996, pp. 1038–1044.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. M. Kretchmar, C. W. Anderson, Comparison of CMACs and radial basis
    functions for local function approximators in reinforcement learning, in: Proceedings
    of International Conference on Neural Networks (ICNN’97), Vol. 2, IEEE, 1997,
    pp. 834–837.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. A. Boyan, A. W. Moore, Generalization in reinforcement learning: Safely
    approximating the value function, in: Advances in neural information processing
    systems, 1995, pp. 369–376.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. M. Bishop, Pattern recognition and machine learning, Springer, 2006.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, G. Tesauro, [Learning
    to learn without forgetting by maximizing transfer and minimizing interference](http://arxiv.org/abs/1810.11910),
    CoRR abs/1810.11910. [arXiv:1810.11910](http://arxiv.org/abs/1810.11910).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1810.11910](http://arxiv.org/abs/1810.11910)
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[72] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    M. Riedmiller, [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602v1).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1312.5602v1](http://arxiv.org/abs/1312.5602v1)
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[73] G. J. Gordon, Approximate solutions to Markov decision processes, Tech.
    rep., Carnegie-Mellon University (1999).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. Baird, Residual algorithms: Reinforcement learning with function approximation,
    in: Machine Learning Proceedings 1995, 1995, pp. 30–37.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. Whiteson, P. Stone, Evolutionary function approximation for reinforcement
    learning, Journal of Machine Learning Research 7 (May) (2006) 877–917.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J. Achiam, E. Knight, P. Abbeel, [Towards Characterizing Divergence in
    Deep Q-Learning](http://arxiv.org/abs/1903.08894), CoRR abs/1903.08894. [arXiv:1903.08894](http://arxiv.org/abs/1903.08894).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.08894](http://arxiv.org/abs/1903.08894)
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[77] H. van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, J. Modayil,
    [Deep reinforcement learning and the deadly triad](http://arxiv.org/abs/1812.02648),
    CoRR abs/1812.02648. [arXiv:1812.02648](http://arxiv.org/abs/1812.02648).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1812.02648](http://arxiv.org/abs/1812.02648)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[78] S. Fujimoto, H. van Hoof, D. Meger, Addressing function approximation
    error in actor-critic methods, in: International Conference on Machine Learning,
    2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Ilyas, L. Engstrom, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph,
    A. Madry, [Are deep policy gradient algorithms truly policy gradient algorithms?](http://arxiv.org/abs/1811.02553),
    CoRR abs/1811.02553. [arXiv:1811.02553](http://arxiv.org/abs/1811.02553).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1811.02553](http://arxiv.org/abs/1811.02553)
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[80] T. Lu, D. Schuurmans, C. Boutilier, Non-delusional Q-learning and value-iteration,
    in: Advances in Neural Information Processing Systems, 2018, pp. 9949–9959.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. N. Tsitsiklis, B. Van Roy, Analysis of temporal-diffference learning
    with function approximation, in: Advances in neural information processing systems,
    1997, pp. 1075–1081.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] F. S. Melo, S. P. Meyn, M. I. Ribeiro, An analysis of reinforcement learning
    with function approximation, in: Proceedings of the 25th international conference
    on Machine learning, ACM, 2008, pp. 664–671.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Ernst, P. Geurts, L. Wehenkel, Tree-based batch mode reinforcement
    learning, Journal of Machine Learning Research 6 (Apr) (2005) 503–556.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    K. Kavukcuoglu, Reinforcement Learning with Unsupervised Auxiliary Tasks., in:
    International Conference on Learning Representations, 2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] M. Hausknecht, P. Stone, Deep Recurrent Q-Learning for Partially Observable
    MDPs, in: International Conference on Learning Representations, 2015.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation
    9 (8) (1997) 1735–1780.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. Riedmiller, Neural fitted Q iteration–first experiences with a data
    efficient neural reinforcement learning method, in: European Conference on Machine
    Learning, Springer, 2005, pp. 317–328.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] R. H. Crites, A. G. Barto, Elevator group control using multiple reinforcement
    learning agents, Machine learning 33 (2-3) (1998) 235–262.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] L. J. Lin, Programming robots using reinforcement learning and teaching.,
    in: AAAI, 1991, pp. 781–786.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] L.-J. Lin, Self-improving reactive agents based on reinforcement learning,
    planning and teaching, Machine learning 8 (3-4) (1992) 293–321.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H. V. Hasselt, Double Q-learning, in: Advances in Neural Information Processing
    Systems, 2010, pp. 2613–2621.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double
    Q-learning, in: Thirtieth AAAI Conference on Artificial Intelligence, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in: International
    Conference on Machine Learning, 2016, pp. 1928–1937.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. McCloskey, N. J. Cohen, Catastrophic interference in connectionist
    networks: The sequential learning problem, in: Psychology of learning and motivation,
    Vol. 24, Elsevier, 1989, pp. 109–165.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, Y. Bengio, [An empirical
    investigation of catastrophic forgetting in gradient-based neural networks](https://arxiv.org/abs/1312.6211).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://arxiv.org/abs/1312.6211](https://arxiv.org/abs/1312.6211)
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[96] D. Isele, A. Cosgun, Selective experience replay for lifelong learning,
    in: Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] G. Palmer, R. Savani, K. Tuyls, Negative update intervals in deep multi-agent
    reinforcement learning, in: 18th International Conference on Autonomous Agents
    and Multiagent Systems, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, N. De Freitas,
    Dueling network architectures for deep reinforcement learning, in: International
    Conference on Machine Learning, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Hauskrecht, Value-function approximations for partially observable
    Markov decision processes, Journal of Artificial Intelligence Research 13 (1).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Meuleau, L. Peshkin, K.-E. Kim, L. P. Kaelbling, Learning finite-state
    controllers for partially observable environments, in: Proceedings of the Fifteenth
    conference on Uncertainty in artificial intelligence, 1999, pp. 427–436.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D. Steckelmacher, D. M. Roijers, A. Harutyunyan, P. Vrancx, H. Plisnier,
    A. Nowé, Reinforcement learning in pomdps with memoryless options and option-observation
    initiation sets, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] S. Ioffe, C. Szegedy, Batch normalization: Accelerating deep network
    training by reducing internal covariate shift, Proceedings of the 32nd International
    Conference on Machine Learning (2015) 448–456.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] E. Van der Pol, F. A. Oliehoek, Coordinated deep reinforcement learners
    for traffic light control, in: Proceedings of Learning, Inference and Control
    of Multi-Agent Systems at NIPS, 2016.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Salimans, D. P. Kingma, Weight normalization: A simple reparameterization
    to accelerate training of deep neural networks, in: Advances in Neural Information
    Processing Systems, 2016, pp. 901–909.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] OpenAI Baselines: ACKTR & A2C, [https://openai.com/blog/baselines-acktr-a2c/](https://openai.com/blog/baselines-acktr-a2c/),
    [Online; accessed 29-April-2019] (2017).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, N. de Freitas,
    Sample efficient actor-critic with experience replay, arXiv preprint arXiv:1611.01224.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] S. S. Gu, T. Lillicrap, R. E. Turner, Z. Ghahramani, B. Schölkopf, S. Levine,
    Interpolated policy gradient: Merging on-policy and off-policy gradient estimation
    for deep reinforcement learning, in: Advances in Neural Information Processing
    Systems, 2017, pp. 3846–3855.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] E. Shelhamer, P. Mahmoudieh, M. Argus, T. Darrell, Loss is its own reward:
    Self-supervision for reinforcement learning, ICLR workshops.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. G. Bellemare, W. Dabney, R. Dadashi, A. A. Taïga, P. S. Castro, N. L.
    Roux, D. Schuurmans, T. Lattimore, C. Lyle, [A Geometric Perspective on Optimal
    Representations for Reinforcement Learning](http://arxiv.org/abs/1901.11530),
    CoRR abs/1901.11530. [arXiv:1901.11530](http://arxiv.org/abs/1901.11530).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.11530](http://arxiv.org/abs/1901.11530)
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[110] R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White,
    D. Precup, Horde: A scalable real-time architecture for learning knowledge from
    unsupervised sensorimotor interaction, in: The 10th International Conference on
    Autonomous Agents and Multiagent Systems-Volume 2, International Foundation for
    Autonomous Agents and Multiagent Systems, 2011, pp. 761–768.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T. Schaul, J. Quan, I. Antonoglou, D. Silver, Prioritized Experience
    Replay, in: International Conference on Learning Representations, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. W. Moore, C. G. Atkeson, Prioritized sweeping: Reinforcement learning
    with less data and less time, Machine learning 13 (1) (1993) 103–130.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. Andre, N. Friedman, R. Parr, Generalized prioritized sweeping, in:
    Advances in Neural Information Processing Systems, 1998, pp. 1001–1007.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    V. Firoiu, T. Harley, I. Dunning, et al., IMPALA: Scalable distributed Deep-RL
    with importance weighted actor-learner architectures, in: International Conference
    on Machine Learning, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, [Proximal
    Policy Optimization Algorithms](http://arxiv.org/abs/1707.06347).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[116] S. M. Kakade, A natural policy gradient, in: Advances in neural information
    processing systems, 2002, pp. 1531–1538.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa,
    T. Erez, Z. Wang, S. M. A. Eslami, M. A. Riedmiller, D. Silver, [Emergence of
    Locomotion Behaviours in Rich Environments.](http://arXiv.org/abs/1707.02286v2)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1707.02286v2](http://arXiv.org/abs/1707.02286v2)
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[118] G. Bacchiani, D. Molinari, M. Patander, Microscopic traffic simulation
    by cooperative multi-agent deep reinforcement learning, in: AAMAS, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. Schulman, P. Abbeel, X. Chen, [Equivalence Between Policy Gradients
    and Soft Q-Learning](http://arxiv.org/abs/1704.06440), CoRR abs/1704.06440. [arXiv:1704.06440](http://arxiv.org/abs/1704.06440).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1704.06440](http://arxiv.org/abs/1704.06440)
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[120] T. Haarnoja, H. Tang, P. Abbeel, S. Levine, Reinforcement learning with
    deep energy-based policies, in: Proceedings of the 34th International Conference
    on Machine Learning-Volume 70, 2017, pp. 1352–1361.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor, in: International
    Conference on Machine Learning, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] M. Tan, Multi-Agent Reinforcement Learning: Independent vs. Cooperative
    Agents, in: Machine Learning Proceedings 1993 Proceedings of the Tenth International
    Conference, University of Massachusetts, Amherst, June 27–29, 1993, 1993, pp.
    330–337.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] G. J. Laurent, L. Matignon, L. Fort-Piat, et al., The world of independent
    learners is not Markovian, International Journal of Knowledge-based and Intelligent
    Engineering Systems 15 (1) (2011) 55–64.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] M. L. Littman, Markov games as a framework for multi-agent reinforcement
    learning, in: Proceedings of the 11th International Conference on Machine Learning,
    New Brunswick, NJ, USA, 1994, pp. 157–163.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. L. Littman, Value-function reinforcement learning in Markov games,
    Cognitive Systems Research 2 (1) (2001) 55–66.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative
    multiagent systems, in: Proceedings of the 15th National Conference on Artificial
    Intelligence, Madison, Wisconsin, USA, 1998, pp. 746–752.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Hu, M. P. Wellman, Nash Q-learning for general-sum stochastic games,
    The Journal of Machine Learning Research 4 (2003) 1039–1069.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] M. L. Littman, Friend-or-foe Q-learning in general-sum games, in: Proceedings
    of 17th International Conference on Autonomous Agents and Multiagent Systems,
    Williamstown, MA, USA, 2001, pp. 322–328.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] X. Song, T. Wang, C. Zhang, Convergence of multi-agent learning with
    a finite step size in general-sum games, in: 18th International Conference on
    Autonomous Agents and Multiagent Systems, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, T. Graepel,
    The mechanics of n-player differentiable games, in: Proceedings of the 35th International
    Conference on Machine Learning, Proceedings of Machine Learning Research, Stockholm,
    Sweden, 2018, pp. 354–363.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Pérolat, B. Piot, O. Pietquin, Actor-critic fictitious play in simultaneous
    move multistage games, in: 21st International Conference on Artificial Intelligence
    and Statistics, 2018.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] G. Bono, J. S. Dibangoye, L. Matignon, F. Pereyron, O. Simonin, Cooperative
    multi-agent policy gradient, in: European Conference on Machine Learning, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] T. W. Neller, M. Lanctot, An introduction to counterfactual regret minimization,
    in: Proceedings of Model AI Assignments, The Fourth Symposium on Educational Advances
    in Artificial Intelligence (EAAI-2013), 2013.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] M. Zinkevich, M. Johanson, M. Bowling, C. Piccione, Regret minimization
    in games with incomplete information, in: Advances in neural information processing
    systems, 2008, pp. 1729–1736.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Blum, Y. Monsour, Learning, regret minimization, and equilibria, in:
    Algorithmic Game Theory, Cambridge University Press, 2007, Ch. 4.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S. Srinivasan, M. Lanctot, V. Zambaldi, J. Pérolat, K. Tuyls, R. Munos,
    M. Bowling, Actor-critic policy optimization in partially observable multiagent
    environments, in: Advances in Neural Information Processing Systems, 2018, pp.
    3422–3435.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] E. Lockhart, M. Lanctot, J. Pérolat, J. Lespiau, D. Morrill, F. Timbers,
    K. Tuyls, [Computing approximate equilibria in sequential adversarial games by
    exploitability descent](http://arxiv.org/abs/1903.05614), CoRR abs/1903.05614.
    [arXiv:1903.05614](http://arxiv.org/abs/1903.05614).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.05614](http://arxiv.org/abs/1903.05614)
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[138] M. Johanson, N. Bard, N. Burch, M. Bowling, Finding optimal abstract
    strategies in extensive-form games, in: Twenty-Sixth AAAI Conference on Artificial
    Intelligence, 2012.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] E. Kalai, E. Lehrer, Rational learning leads to Nash equilibrium, Econometrica:
    Journal of the Econometric Society (1993) 1019–1045.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T. W. Sandholm, R. H. Crites, Multiagent reinforcement learning in the
    iterated prisoner’s dilemma, Biosystems 37 (1-2) (1996) 147–166.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] S. Singh, M. Kearns, Y. Mansour, Nash convergence of gradient dynamics
    in general-sum games, in: Proceedings of the Sixteenth conference on Uncertainty
    in artificial intelligence, Morgan Kaufmann Publishers Inc., 2000, pp. 541–548.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] B. Banerjee, J. Peng, Adaptive policy gradient in multiagent learning,
    in: Proceedings of the second international joint conference on Autonomous agents
    and multiagent systems, ACM, 2003, pp. 686–692.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Greenwald, K. Hall, Correlated Q-learning, in: Proceedings of 17th
    International Conference on Autonomous Agents and Multiagent Systems, Washington,
    DC, USA, 2003, pp. 242–249.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M. Bowling, Convergence problems of general-sum multiagent reinforcement
    learning, in: International Conference on Machine Learning, 2000, pp. 89–94.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Bowling, Convergence and no-regret in multiagent learning, in: Advances
    in Neural Information Processing Systems, Vancouver, Canada, 2004, pp. 209–216.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. Zinkevich, A. Greenwald, M. L. Littman, Cyclic equilibria in Markov
    games, in: Advances in Neural Information Processing Systems, 2006, pp. 1641–1648.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. Bowling, M. Veloso, Multiagent learning using a variable learning
    rate, Artificial Intelligence 136 (2) (2002) 215–250.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] M. Kaisers, K. Tuyls, FAQ-learning in matrix games: demonstrating convergence
    near Nash equilibria, and bifurcation of attractors in the battle of sexes, in:
    AAAI Workshop on Interactive Decision Theory and Game Theory, San Francisco, CA,
    USA, 2011, pp. 309–316.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Wunder, M. L. Littman, M. Babes, Classes of Multiagent Q-learning
    Dynamics with epsilon-greedy Exploration, in: Proceedings of the 35th International
    Conference on Machine Learning, Haifa, Israel, 2010, pp. 1167–1174.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] G. Tesauro, Extending Q-learning to general adaptive multi-agent systems,
    in: Advances in Neural Information Processing Systems, Vancouver, Canada, 2003,
    pp. 871–878.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] M. Weinberg, J. S. Rosenschein, Best-response multiagent learning in
    non-stationary environments, in: Proceedings of the 3rd International Conference
    on Autonomous Agents and Multiagent Systems, New York, NY, USA, 2004, pp. 506–513.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] D. Chakraborty, P. Stone, Multiagent learning in the presence of memory-bounded
    agents, Autonomous Agents and Multi-Agent Systems 28 (2) (2013) 182–213.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] G. Weiss (Ed.),  Multiagent Systems, 2nd Edition, (Intelligent Robotics
    and Autonomous Agents series), MIT Press, Cambridge, MA, USA, 2013.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Multiagent Learning, Foundations and Recent Trends, [https://www.cs.utexas.edu/~larg/ijcai17_tutorial/multiagent_learning.pdf%****␣main.bbl␣Line␣775␣****](https://www.cs.utexas.edu/~larg/ijcai17_tutorial/multiagent_learning.pdf%****%20main.bbl%20Line%20775%20****),
    [Online; accessed 7-September-2018] (2017).'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru,
    R. Vicente, Multiagent cooperation and competition with deep reinforcement learning,
    PLOS ONE 12 (4) (2017) e0172395.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, Multi-agent Reinforcement
    Learning in Sequential Social Dilemmas, in: Proceedings of the 16th Conference
    on Autonomous Agents and Multiagent Systems, Sao Paulo, 2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M. Raghu, A. Irpan, J. Andreas, R. Kleinberg, Q. Le, J. Kleinberg, Can
    Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?, in: Proceedings
    of the 35th International Conference on Machine Learning, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, I. Mordatch, Emergent
    Complexity via Multi-Agent Competition., in: International Conference on Machine
    Learning, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J. Z. Leibo, J. Perolat, E. Hughes, S. Wheelwright, A. H. Marblestone,
    E. Duéñez-Guzmán, P. Sunehag, I. Dunning, T. Graepel, Malthusian reinforcement
    learning, in: 18th International Conference on Autonomous Agents and Multiagent
    Systems, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] I. Mordatch, P. Abbeel, Emergence of grounded compositional language
    in multi-agent populations, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A. Lazaridou, A. Peysakhovich, M. Baroni, Multi-Agent Cooperation and
    the Emergence of (Natural) Language, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. N. Foerster, Y. M. Assael, N. De Freitas, S. Whiteson, Learning to
    communicate with deep multi-agent reinforcement learning, in: Advances in Neural
    Information Processing Systems, 2016, pp. 2145–2153.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] S. Sukhbaatar, A. Szlam, R. Fergus, Learning Multiagent Communication
    with Backpropagation, in: Advances in Neural Information Processing Systems, 2016,
    pp. 2244–2252.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] P. Peng, Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, J. Wang, [Multiagent
    Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games.](http://arxiv.org/abs/1703.10069)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1703.10069](http://arxiv.org/abs/1703.10069)
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[165] E. Pesce, G. Montana, [Improving coordination in multi-agent deep reinforcement
    learning through memory-driven communication](http://arxiv.org/abs/1901.03887),
    CoRR abs/1901.03887. [arXiv:1901.03887](http://arxiv.org/abs/1901.03887).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.03887](http://arxiv.org/abs/1901.03887)
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[166] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, J. Vian, Deep Decentralized
    Multi-task Multi-Agent Reinforcement Learning under Partial Observability, in:
    Proceedings of the 34th International Conference on Machine Learning, Sydney,
    2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, S. Whiteson, Counterfactual
    Multi-Agent Policy Gradients., in: 32nd AAAI Conference on Artificial Intelligence,
    2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] J. N. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr,
    P. Kohli, S. Whiteson, Stabilising Experience Replay for Deep Multi-Agent Reinforcement
    Learning., in: International Conference on Machine Learning, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. He, J. Boyd-Graber, K. Kwok, H. Daume, Opponent modeling in deep reinforcement
    learning, in: 33rd International Conference on Machine Learning, 2016, pp. 2675–2684.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] R. Raileanu, E. Denton, A. Szlam, R. Fergus, Modeling Others using Oneself
    in Multi-Agent Reinforcement Learning., in: International Conference on Machine
    Learning, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, C.-Y. Lee, A Deep Policy
    Inference Q-Network for Multi-Agent Systems, in: International Conference on Autonomous
    Agents and Multiagent Systems, 2018.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Lanctot, V. F. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat,
    D. Silver, T. Graepel, A Unified Game-Theoretic Approach to Multiagent Reinforcement
    Learning., in: Advances in Neural Information Processing Systems, 2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] J. Heinrich, D. Silver, [Deep Reinforcement Learning from Self-Play in
    Imperfect-Information Games](http://arxiv.org/abs/1603.01121).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1603.01121](http://arxiv.org/abs/1603.01121)
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[174] N. C. Rabinowitz, F. Perbet, H. F. Song, C. Zhang, S. M. A. Eslami, M. Botvinick,
    Machine Theory of Mind., in: International Conference on Machine Learning, Stockholm,
    Sweden, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] T. Yang, J. Hao, Z. Meng, C. Zhang, Y. Z. Z. Zheng, Towards Efficient
    Detection and Optimal Response against Sophisticated Opponents, in: IJCAI, 2019.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] A. Lerer, A. Peysakhovich, [Maintaining cooperation in complex social
    dilemmas using deep reinforcement learning](http://arxiv.org/abs/1707.01068),
    CoRR abs/1707.01068. [arXiv:1707.01068](http://arxiv.org/abs/1707.01068).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1707.01068](http://arxiv.org/abs/1707.01068)
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[177] W. Kim, M. Cho, Y. Sung, Message-Dropout: An Efficient Training Method
    for Multi-Agent Deep Reinforcement Learning, in: 33rd AAAI Conference on Artificial
    Intelligence, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Y. Zheng, J. Hao, Z. Zhang, [Weighted double deep multiagent reinforcement
    learning in stochastic cooperative environments](http://arXiv.org/abs/1802.08534).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1802.08534](http://arXiv.org/abs/1802.08534)
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[179] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.
    Castañeda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat,
    T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, T. Graepel,
    [Human-level performance in 3d multiplayer games with population-based reinforcement
    learning](https://science.sciencemag.org/content/364/6443/859), Science 364 (6443)
    (2019) 859–865. [arXiv:https://science.sciencemag.org/content/364/6443/859.full.pdf](http://arxiv.org/abs/https://science.sciencemag.org/content/364/6443/859.full.pdf),
    [doi:10.1126/science.aau6249](https://doi.org/10.1126/science.aau6249).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://science.sciencemag.org/content/364/6443/859](https://science.sciencemag.org/content/364/6443/859)
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[180] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M. Jaderberg,
    M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, T. Graepel, Value-Decomposition
    Networks For Cooperative Multi-Agent Learning Based On Team Reward., in: Proceedings
    of 17th International Conference on Autonomous Agents and Multiagent Systems,
    Stockholm, Sweden, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. N. Foerster,
    S. Whiteson, QMIX - Monotonic Value Function Factorisation for Deep Multi-Agent
    Reinforcement Learning., in: International Conference on Machine Learning, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J. K. Gupta, M. Egorov, M. Kochenderfer, Cooperative multi-agent control
    using deep reinforcement learning, in: G. Sukthankar, J. A. Rodriguez-Aguilar
    (Eds.), Autonomous Agents and Multiagent Systems, Springer International Publishing,
    Cham, 2017, pp. 66–83.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, S. Russell, Robust multi-agent
    reinforcement learning via minimax deep deterministic policy gradient, in: AAAI
    Conference on Artificial Intelligence, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Y. Zheng, Z. Meng, J. Hao, Z. Zhang, T. Yang, C. Fan, A Deep Bayesian
    Policy Reuse Approach Against Non-Stationary Agents, in: Advances in Neural Information
    Processing Systems, 2018, pp. 962–972.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] R. Powers, Y. Shoham, Learning against opponents with bounded memory,
    in: Proceedings of the 19th International Joint Conference on Artificial Intelligence,
    Edinburg, Scotland, UK, 2005, pp. 817–822.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] M. L. Littman, P. Stone, Implicit Negotiation in Repeated Games, ATAL
    ’01: Revised Papers from the 8th International Workshop on Intelligent Agents
    VIII.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] R. Axelrod, W. D. Hamilton, The evolution of cooperation, Science 211 (27)
    (1981) 1390–1396.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] E. Munoz de Cote, A. Lazaric, M. Restelli, Learning to cooperate in multi-agent
    social dilemmas, in: Proceedings of the 5th International Conference on Autonomous
    Agents and Multiagent Systems, Hakodate, Hokkaido, Japan, 2006, pp. 783–785.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J. L. Stimpson, M. A. Goodrich, Learning to cooperate in a social dilemma:
    A satisficing approach to bargaining, in: Proceedings of the 20th International
    Conference on Machine Learning (ICML-03), 2003, pp. 728–735.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] G. W. Brown, Iterative solution of games by fictitious play, Activity
    analysis of production and allocation 13 (1) (1951) 374–376.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] D. Monderer, L. S. Shapley, Fictitious play property for games with identical
    interests, Journal of economic theory 68 (1) (1996) 258–265.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] G. Tesauro, Temporal difference learning and TD-Gammon, Communications
    of the ACM 38 (3) (1995) 58–68.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] M. Bowling, N. Burch, M. Johanson, O. Tammelin, Heads-up limit hold’em
    poker is solved, Science 347 (6218) (2015) 145–149.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. Z. Leibo, E. Hughes, M. Lanctot, T. Graepel, [Autocurricula and the
    emergence of innovation from social interaction: A manifesto for multi-agent intelligence
    research](http://arxiv.org/abs/1903.00742), CoRR abs/1903.00742. [arXiv:1903.00742](http://arxiv.org/abs/1903.00742).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.00742](http://arxiv.org/abs/1903.00742)
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[195] S. Samothrakis, S. Lucas, T. Runarsson, D. Robles, Coevolving game-playing
    agents: Measuring performance and intransitivities, IEEE Transactions on Evolutionary
    Computation 17 (2) (2013) 213–226.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, R. Munos,
    Unifying count-based exploration and intrinsic motivation, in: Advances in Neural
    Information Processing Systems, 2016, pp. 1471–1479.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] D. E. Moriarty, A. C. Schultz, J. J. Grefenstette, Evolutionary algorithms
    for reinforcement learning, Journal of Artificial Intelligence Research 11 (1999)
    241–276.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] F. A. Oliehoek, E. D. De Jong, N. Vlassis, The parallel Nash memory for
    asymmetric games, in: Proceedings of the 8th annual conference on Genetic and
    evolutionary computation, ACM, 2006, pp. 337–344.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] L. Bull, T. C. Fogarty, M. Snaith, Evolution in multi-agent systems:
    Evolving communicating classifier systems for gait in a quadrupedal robot, in:
    Proceedings of the 6th International Conference on Genetic Algorithms, Morgan
    Kaufmann Publishers Inc., 1995, pp. 382–388.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] L. Bull, Evolutionary computing in multi-agent environments: Operators,
    in: International Conference on Evolutionary Programming, Springer, 1998, pp.
    43–52.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] H. Iba, Emergent cooperation for multiple agents using genetic programming,
    in: International Conference on Parallel Problem Solving from Nature, Springer,
    1996, pp. 32–41.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] E. Todorov, T. Erez, Y. Tassa, MuJoCo - A physics engine for model-based
    control, Intelligent Robots and Systems (2012) 5026–5033.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] V. Gullapalli, A. G. Barto, Shaping as a method for accelerating reinforcement
    learning, in: Proceedings of the 1992 IEEE international symposium on intelligent
    control, IEEE, 1992, pp. 554–559.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] S. Mahadevan, J. Connell, Automatic programming of behavior-based robots
    using reinforcement learning, Artificial intelligence 55 (2-3) (1992) 311–365.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] G. Konidaris, A. Barto, Autonomous shaping: Knowledge transfer in reinforcement
    learning, in: Proceedings of the 23rd international conference on Machine learning,
    ACM, 2006, pp. 489–496.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A. Y. Ng, D. Harada, S. J. Russell, Policy invariance under reward transformations:
    Theory and application to reward shaping, in: Proceedings of the Sixteenth International
    Conference on Machine Learning, 1999, pp. 278–287.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] P. Erdös, J. L. Selfridge, On a combinatorial game, Journal of Combinatorial
    Theory, Series A 14 (3) (1973) 298–301.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] J. Spencer, Randomization, derandomization and antirandomization: three
    games, Theoretical Computer Science 131 (2) (1994) 415–429.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] D. Fudenberg, J. Tirole, Game Theory, The MIT Press, 1991.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] L. v. d. Maaten, G. Hinton, Visualizing data using t-SNE, Journal of
    machine learning research 9 (Nov) (2008) 2579–2605.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] T. Zahavy, N. Ben-Zrihem, S. Mannor, Graying the black box: Understanding
    DQNs, in: International Conference on Machine Learning, 2016, pp. 1899–1908.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] E. Beeching, C. Wolf, J. Dibangoye, O. Simonin, [Deep Reinforcement Learning
    on a Budget: 3D Control and Reasoning Without a Supercomputer](http://arxiv.org/abs/1904.01806),
    CoRR abs/1904.01806. [arXiv:1904.01806](http://arxiv.org/abs/1904.01806).'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1904.01806](http://arxiv.org/abs/1904.01806)
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[213] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
    Dropout: a simple way to prevent neural networks from overfitting, The Journal
    of Machine Learning Research 15 (1) (2014) 1929–1958.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] M. Schuster, K. K. Paliwal, Bidirectional recurrent neural networks,
    IEEE Transactions on Signal Processing 45 (11) (1997) 2673–2681.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] R. Lowe, J. Foerster, Y.-L. Boureau, J. Pineau, Y. Dauphin, On the pitfalls
    of measuring emergent communication, in: 18th International Conference on Autonomous
    Agents and Multiagent Systems, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] M. Tambe, Towards flexible teamwork, Journal of artificial intelligence
    research 7 (1997) 83–124.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] B. J. Grosz, S. Kraus, Collaborative plans for complex group action,
    Artificial Intelligence 86 (2) (1996) 269–357.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] D. Precup, R. S. Sutton, S. Singh, Eligibility traces for off-policy
    policy evaluation, in: Proceedings of the Seventeenth International Conference
    on Machine Learning., 2000.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J. Frank, S. Mannor, D. Precup, Reinforcement learning in the presence
    of rare events, in: Proceedings of the 25th international conference on Machine
    learning, ACM, 2008, pp. 336–343.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] T. I. Ahamed, V. S. Borkar, S. Juneja, Adaptive importance sampling technique
    for markov chains using stochastic approximation, Operations Research 54 (3) (2006)
    489–504.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] K. A. Ciosek, S. Whiteson, Offer: Off-environment reinforcement learning,
    in: Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] D. Bloembergen, M. Kaisers, K. Tuyls, Lenient frequency adjusted Q-learning,
    in: Proceedings of the 22nd Belgian/Netherlands Artificial Intelligence Conference,
    2010.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] L. Panait, K. Sullivan, S. Luke, Lenience towards teammates helps in
    cooperative multiagent learning, in: Proceedings of the 5th International Conference
    on Autonomous Agents and Multiagent Systems, Hakodate, Japan, 2006.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] L. Panait, K. Tuyls, S. Luke, Theoretical advantages of lenient learners:
    An evolutionary game theoretic perspective, JMLR 9 (Mar) (2008) 423–457.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] M. Lauer, M. Riedmiller, An algorithm for distributed reinforcement learning
    in cooperative multi-agent systems, in: In Proceedings of the Seventeenth International
    Conference on Machine Learning, 2000.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] R. Caruana, Multitask learning, Machine learning 28 (1) (1997) 41–75.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, R. Hadsell, Policy Distillation, in: International
    Conference on Learning Representations, 2016.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural
    network, in: NIPS Deep Learning Workshop, 2014.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver,
    K. Kavukcuoglu, FeUdal Networks for Hierarchical Reinforcement Learning., International
    Conference On Machine Learning.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] P. Dayan, G. E. Hinton, Feudal reinforcement learning, in: Advances in
    neural information processing systems, 1993, pp. 271–278.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] S. P. Singh, Transfer of learning by composing solutions of elemental
    sequential tasks, Machine Learning 8 (3-4) (1992) 323–339.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Capture the Flag: the emergence of complex cooperative agents, [https://deepmind.com/blog/capture-the-flag/](https://deepmind.com/blog/capture-the-flag/),
    [Online; accessed 7-September-2018] (2018).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] C. D. Rosin, R. K. Belew, New methods for competitive coevolution, Evolutionary
    computation 5 (1) (1997) 1–29.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] J. Lehman, K. O. Stanley, Exploiting open-endedness to solve problems
    through the search for novelty., in: ALIFE, 2008, pp. 329–336.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,
    A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, et al., [Population
    based training of neural networks](http://arxiv.org/abs/1711.09846).'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1711.09846](http://arxiv.org/abs/1711.09846)
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[236] A. E. Elo, The rating of chessplayers, past and present, Arco Pub., 1978.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] R. Herbrich, T. Minka, T. Graepel, TrueSkill™: a Bayesian skill rating
    system, in: Advances in neural information processing systems, 2007, pp. 569–576.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] S. Omidshafiei, C. Papadimitriou, G. Piliouras, K. Tuyls, M. Rowland,
    J.-B. Lespiau, W. M. Czarnecki, M. Lanctot, J. Perolat, R. Munos, <math id="bib.bib238.1.m1.1"
    class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib238.1.m1.1a"><mi
    id="bib.bib238.1.m1.1.1" xref="bib.bib238.1.m1.1.1.cmml">α</mi><annotation-xml
    encoding="MathML-Content" id="bib.bib238.1.m1.1b"><ci id="bib.bib238.1.m1.1.1.cmml"
    xref="bib.bib238.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="bib.bib238.1.m1.1c">\alpha</annotation></semantics></math>-Rank: Multi-Agent
    Evaluation by Evolution, Scientific Reports 9.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] T. Back, Evolutionary algorithms in theory and practice: evolution strategies,
    evolutionary programming, genetic algorithms, Oxford university press, 1996.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] K. A. De Jong, Evolutionary computation: a unified approach, MIT press,
    2006.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] K. Tumer, A. Agogino, Distributed agent-based air traffic flow management,
    in: Proceedings of the 6th International Conference on Autonomous Agents and Multiagent
    Systems, Honolulu, Hawaii, 2007.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factored MDPs,
    in: Advances in neural information processing systems, 2002, pp. 1523–1530.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] J. R. Kok, N. Vlassis, Sparse cooperative Q-learning, in: Proceedings
    of the twenty-first international conference on Machine learning, ACM, 2004, p. 61.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] J. Castellini, F. A. Oliehoek, R. Savani, S. Whiteson, The Representational
    Capacity of Action-Value Networks for Multi-Agent Reinforcement Learning, in:
    18th International Conference on Autonomous Agents and Multiagent Systems, 2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] P. J. Gmytrasiewicz, P. Doshi, A framework for sequential planning in
    multiagent settings, Journal of Artificial Intelligence Research 24 (1) (2005)
    49–79.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] J. C. Harsanyi, Games with incomplete information played by “Bayesian”
    players, I–III Part I. The basic model, Management science 14 (3) (1967) 159–182.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] S. Barrett, P. Stone, S. Kraus, A. Rosenfeld, Teamwork with Limited Knowledge
    of Teammates., in: Proceedings of the Twenty-Seventh AAAI Conference on Artificial
    Intelligence, Bellevue, WS, USA, 2013, pp. 102–108.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, G. E. Hinton, et al., Adaptive
    mixtures of local experts., Neural computation 3 (1) (1991) 79–87.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] J. Heinrich, M. Lanctot, D. Silver, Fictitious self-play in extensive-form
    games, in: International Conference on Machine Learning, 2015, pp. 805–813.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J. F. Nash, Equilibrium points in n-person games, Proceedings of the
    National Academy of Sciences 36 (1) (1950) 48–49.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J. Von Neumann, O. Morgenstern, Theory of games and economic behavior,
    Vol. 51, Bull. Amer. Math. Soc, 1945.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] J. S. Shamma, G. Arslan, Dynamic fictitious play, dynamic gradient play,
    and distributed convergence to Nash equilibria, IEEE Transactions on Automatic
    Control 50 (3) (2005) 312–327.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] W. E. Walsh, R. Das, G. Tesauro, J. O. Kephart, Analyzing complex strategic
    interactions in multi-agent systems, AAAI-02 Workshop on Game-Theoretic and Decision-Theoretic
    Agents (2002) 109–118.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] M. Johanson, K. Waugh, M. Bowling, M. Zinkevich, Accelerating best response
    calculation in large extensive games, in: Twenty-Second International Joint Conference
    on Artificial Intelligence, 2011.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] C. F. Camerer, T.-H. Ho, J.-K. Chong, A cognitive hierarchy model of
    games, The Quarterly Journal of Economics 119 (3) (2004) 861.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] M. Costa Gomes, V. P. Crawford, B. Broseta, Cognition and Behavior in
    Normal–Form Games: An Experimental Study, Econometrica 69 (5) (2001) 1193–1235.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] J. Morimoto, K. Doya, Robust reinforcement learning, Neural computation
    17 (2) (2005) 335–359.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] L. Pinto, J. Davidson, R. Sukthankar, A. Gupta, Robust adversarial reinforcement
    learning, in: Proceedings of the 34th International Conference on Machine Learning-Volume
    70, JMLR. org, 2017, pp. 2817–2826.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] R. Powers, Y. Shoham, T. Vu, A general criterion and an algorithmic framework
    for learning in multi-agent systems, Machine Learning 67 (1-2) (2007) 45–76.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] J. W. Crandall, M. A. Goodrich, Learning to compete, coordinate, and
    cooperate in repeated games using reinforcement learning, Machine Learning 82 (3)
    (2011) 281–314.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] M. Johanson, M. A. Zinkevich, M. Bowling, Computing Robust Counter-Strategies.,
    in: Advances in Neural Information Processing Systems, Vancouver, BC, Canada,
    2007, pp. 721–728.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] P. McCracken, M. Bowling, Safe strategies for agent modelling in games,
    in: AAAI Fall Symposium, 2004, pp. 103–110.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] S. Damer, M. Gini, Safely using predictions in general-sum normal form
    games, in: Proceedings of the 16th Conference on Autonomous Agents and Multiagent
    Systems, Sao Paulo, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] C. Zhang, V. Lesser, Multi-agent learning with policy prediction, in:
    Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] P. J. Gmytrasiewicz, E. H. Durfee, Rational Coordination in Multi-Agent
    Environments, Autonomous Agents and Multi-Agent Systems 3 (4) (2000) 319–350.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] C. F. Camerer, T.-H. Ho, J.-K. Chong, Behavioural Game Theory: Thinking,
    Learning and Teaching, in: Advances in Understanding Strategic Behavior, New York,
    2004, pp. 120–180.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] D. Carmel, S. Markovitch, Incorporating opponent models into adversary
    search, in: AAAI/IAAI, Vol. 1, 1996, pp. 120–125.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] H. de Weerd, R. Verbrugge, B. Verheij, How much does it help to know
    what she knows you know? An agent-based simulation study, Artificial Intelligence
    199-200 (C) (2013) 67–92.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] P. Hernandez-Leal, M. Kaisers, Towards a Fast Detection of Opponents
    in Repeated Stochastic Games, in: G. Sukthankar, J. A. Rodriguez-Aguilar (Eds.),
    Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, Sao
    Paulo, Brazil, May 8-12, 2017, Revised Selected Papers, 2017, pp. 239–257.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] P. Hernandez-Leal, M. E. Taylor, B. Rosman, L. E. Sucar, E. Munoz de
    Cote, Identifying and Tracking Switching, Non-stationary Opponents: a Bayesian
    Approach, in: Multiagent Interaction without Prior Coordination Workshop at AAAI,
    Phoenix, AZ, USA, 2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] B. Rosman, M. Hawasly, S. Ramamoorthy, Bayesian Policy Reuse, Machine
    Learning 104 (1) (2016) 99–127.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] P. Hernandez-Leal, Y. Zhan, M. E. Taylor, L. E. Sucar, E. Munoz de Cote,
    Efficiently detecting switches against non-stationary opponents, Autonomous Agents
    and Multi-Agent Systems 31 (4) (2017) 767–789.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] P. Hernandez-Leal, M. Kaisers, Learning against sequential opponents
    in repeated stochastic games, in: The 3rd Multi-disciplinary Conference on Reinforcement
    Learning and Decision Making, Ann Arbor, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] J. Schmidhuber, Critique of Paper by “Deep Learning Conspiracy” (Nature
    521 p 436), [http://people.idsia.ch/~juergen/deep-learning-conspiracy.html](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html)
    (2015).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Do I really have to cite an arXiv paper?, [http://approximatelycorrect.com/2017/08/01/do-i-have-to-cite-arxiv-paper/](http://approximatelycorrect.com/2017/08/01/do-i-have-to-cite-arxiv-paper/),
    [Online; accessed 21-May-2019] (2017).'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Collaboration & Credit Principles, How can we be good stewards of collaborative
    trust?, [http://colah.github.io/posts/2019-05-Collaboration/index.html](http://colah.github.io/posts/2019-05-Collaboration/index.html),
    [Online; accessed 31-May-2019] (2019).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] H. Wang, B. Raj, E. P. Xing, [On the origin of deep learning](http://arxiv.org/abs/1702.07800),
    CoRR abs/1702.07800. [arXiv:1702.07800](http://arxiv.org/abs/1702.07800).'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1702.07800](http://arxiv.org/abs/1702.07800)
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[278] A. K. Agogino, K. Tumer, Analyzing and visualizing multiagent rewards
    in dynamic and stochastic domains, Autonomous Agents and Multi-Agent Systems 17 (2)
    (2008) 320–338.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] S. Devlin, L. M. Yliniemi, D. Kudenko, K. Tumer, Potential-based difference
    rewards for multiagent reinforcement learning., in: 13th International Conference
    on Autonomous Agents and Multiagent Systems, AAMAS 2014, Paris, France, 2014.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] D. H. Wolpert, K. R. Wheeler, K. Tumer, General principles of learning-based
    multi-agent systems, in: Proceedings of the Third International Conference on
    Autonomous Agents, 1999.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] M. E. Taylor, P. Stone, Transfer learning for reinforcement learning
    domains: A survey, The Journal of Machine Learning Research 10 (2009) 1633–1685.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] C. Bucilua, R. Caruana, A. Niculescu-Mizil, Model compression, in: Proceedings
    of the 12th ACM SIGKDD international conference on Knowledge discovery and data
    mining, ACM, 2006, pp. 535–541.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Y. Du, W. M. Czarnecki, S. M. Jayakumar, R. Pascanu, B. Lakshminarayanan,
    Adapting auxiliary losses using gradient similarity, arXiv preprint arXiv:1812.02224.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] S. C. Suddarth, Y. Kergosien, Rule-injection hints as a means of improving
    network performance and learning time, in: Neural Networks, Springer, 1990, pp.
    120–129.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] P. Hernandez-Leal, B. Kartal, M. E. Taylor, Agent Modeling as Auxiliary
    Task for Deep Reinforcement Learning, in: AAAI Conference on Artificial Intelligence
    and Interactive Digital Entertainment, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,
    B. McGrew, J. Tobin, P. Abbeel, W. Zaremba, Hindsight experience replay, in: Advances
    in Neural Information Processing Systems, 2017.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Z. C. Lipton, K. Azizzadenesheli, A. Kumar, L. Li, J. Gao, L. Deng, [Combating
    Reinforcement Learning’s Sisyphean Curse with Intrinsic Fear](http://arxiv.org/abs/1611.01211v8).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1611.01211v8](http://arxiv.org/abs/1611.01211v8)
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[288] T. De Bruin, J. Kober, K. Tuyls, R. Babuška, Experience selection in
    deep reinforcement learning for control, The Journal of Machine Learning Research
    19 (1) (2018) 347–402.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] D. S. Bernstein, R. Givan, N. Immerman, S. Zilberstein, The complexity
    of decentralized control of Markov decision processes, Mathematics of operations
    research 27 (4) (2002) 819–840.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized
    POMDPs, Springer, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] F. A. Oliehoek, M. T. Spaan, N. Vlassis, Optimal and approximate Q-value
    functions for decentralized POMDPs, Journal of Artificial Intelligence Research
    32 (2008) 289–353.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] J. K. Gupta, M. Egorov, M. J. Kochenderfer, Cooperative Multi-agent Control
    using deep reinforcement learning, in: Adaptive Learning Agents at AAMAS, Sao
    Paulo, 2017.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] R. Pascanu, T. Mikolov, Y. Bengio, On the difficulty of training recurrent
    neural networks, in: International conference on machine learning, 2013, pp. 1310–1318.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] K. Greff, R. K. Srivastava, J. Koutnik, B. R. Steunebrink, J. Schmidhuber,
    LSTM: A Search Space Odyssey, IEEE Transactions on Neural Networks and Learning
    Systems 28 (10) (2017) 2222–2232.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of gated
    recurrent neural networks on sequence modeling, in: Deep Learning and Representation
    Learning Workshop, 2014.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] S. Whiteson, B. Tanner, M. E. Taylor, P. Stone, Protecting against evaluation
    overfitting in empirical reinforcement learning, in: 2011 IEEE Symposium on Adaptive
    Dynamic Programming and Reinforcement Learning (ADPRL), IEEE, 2011, pp. 120–127.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] M. G. Bellemare, Y. Naddaf, J. Veness, M. Bowling, The arcade learning
    environment: An evaluation platform for general agents, Journal of Artificial
    Intelligence Research 47 (2013) 253–279.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht,
    M. Bowling, Revisiting the arcade learning environment: Evaluation protocols and
    open problems for general agents, Journal of Artificial Intelligence Research
    61 (2018) 523–562.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang,
    W. Zaremba, OpenAI Gym, arXiv preprint arXiv:1606.01540.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] P. S. Castro, S. Moitra, C. Gelada, S. Kumar, M. G. Bellemare, [Dopamine:
    A Research Framework for Deep Reinforcement Learning](http://arxiv.org/abs/1812.06110).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1812.06110](http://arxiv.org/abs/1812.06110)
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[301] C. Resnick, W. Eldridge, D. Ha, D. Britz, J. Foerster, J. Togelius, K. Cho,
    J. Bruna, [Pommerman: A Multi-Agent Playground](http://arxiv.org/abs/1809.07124).'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.07124](http://arxiv.org/abs/1809.07124)
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[302] C. Gao, B. Kartal, P. Hernandez-Leal, M. E. Taylor, On Hard Exploration
    for Reinforcement Learning: a Case Study in Pommerman, in: AAAI Conference on
    Artificial Intelligence and Interactive Digital Entertainment, 2019.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. J.
    Rudner, C. Hung, P. H. S. Torr, J. N. Foerster, S. Whiteson, [The StarCraft Multi-Agent
    Challenge](http://arxiv.org/abs/1902.04043), CoRR abs/1902.04043. [arXiv:1902.04043](http://arxiv.org/abs/1902.04043).'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1902.04043](http://arxiv.org/abs/1902.04043)
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[304] D. Pérez-Liébana, K. Hofmann, S. P. Mohanty, N. Kuno, A. Kramer, S. Devlin,
    R. D. Gaina, D. Ionita, [The multi-agent reinforcement learning in Malmö (MARLÖ)
    competition](http://arxiv.org/abs/1901.08129), CoRR abs/1901.08129. [arXiv:1901.08129](http://arxiv.org/abs/1901.08129).'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.08129](http://arxiv.org/abs/1901.08129)
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[305] M. Johnson, K. Hofmann, T. Hutton, D. Bignell, The Malmo platform for
    artificial intelligence experimentation., in: IJCAI, 2016, pp. 4246–4247.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] P. Stone, G. Kaminka, S. Kraus, J. S. Rosenschein, Ad Hoc Autonomous
    Agent Teams: Collaboration without Pre-Coordination., in: 32nd AAAI Conference
    on Artificial Intelligence, Atlanta, Georgia, USA, 2010, pp. 1504–1509.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] M. Bowling, P. McCracken, Coordination and adaptation in impromptu teams,
    in: Proceedings of the Nineteenth Conference on Artificial Intelligence, Vol. 5,
    2005, pp. 53–58.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] S. V. Albrecht, S. Ramamoorthy, A game-theoretic model and best-response
    learning method for ad hoc coordination in multiagent systems, in: Proceedings
    of the 12th International Conference on Autonomous Agents and Multi-agent Systems,
    Saint Paul, MN, USA, 2013.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] N. Bard, J. N. Foerster, S. Chandar, N. Burch, M. Lanctot, H. F. Song,
    E. Parisotto, V. Dumoulin, S. Moitra, E. Hughes, I. Dunning, S. Mourad, H. Larochelle,
    M. G. Bellemare, M. Bowling, [The Hanabi Challenge: A New Frontier for AI Research](https://arxiv.org/abs/1902.00506).'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://arxiv.org/abs/1902.00506](https://arxiv.org/abs/1902.00506)
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[310] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, D. Silver, Rainbow: Combining improvements in deep
    reinforcement learning, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Y. Song, J. Wang, T. Lukasiewicz, Z. Xu, M. Xu, Z. Ding, L. Wu, [Arena:
    A general evaluation platform and building toolkit for multi-agent intelligence](http://arxiv.org/abs/1905.08085),
    CoRR abs/1905.08085. [arXiv:1905.08085](http://arxiv.org/abs/1905.08085).'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1905.08085](http://arxiv.org/abs/1905.08085)
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[312] A. Juliani, V. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, D. Lange,
    [Unity: A general platform for intelligent agents](http://arxiv.org/abs/1809.02627),
    CoRR abs/1809.02627. [arXiv:1809.02627](http://arxiv.org/abs/1809.02627).'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.02627](http://arxiv.org/abs/1809.02627)
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[313] S. Liu, G. Lever, J. Merel, S. Tunyasuvunakool, N. Heess, T. Graepel,
    Emergent coordination through competition, in: International Conference on Learning
    Representations, 2019.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] J. Suarez, Y. Du, P. Isola, I. Mordatch, [Neural MMO: A massively multiagent
    game environment for training and evaluating intelligent agents](http://arxiv.org/abs/1903.00784),
    CoRR abs/1903.00784. [arXiv:1903.00784](http://arxiv.org/abs/1903.00784).'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.00784](http://arxiv.org/abs/1903.00784)
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[315] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, D. Meger, Deep
    Reinforcement Learning That Matters., in: 32nd AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] P. Nagarajan, G. Warnell, P. Stone, [Deterministic implementations for
    reproducibility in deep reinforcement learning](http://arxiv.org/abs/1809.05676).'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.05676](http://arxiv.org/abs/1809.05676)
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[317] K. Clary, E. Tosch, J. Foley, D. Jensen, Let’s play again: Variability
    of deep reinforcement learning agents in Atari environments, in: NeurIPS Critiquing
    and Correcting Trends Workshop, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] J. Z. Forde, M. Paganini, The scientific method in the science of machine
    learning, in: ICLR Debugging Machine Learning Models workshop, 2019.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] K. Azizzadenesheli, Maybe a few considerations in reinforcement learning
    research?, in: Reinforcement Learning for Real Life Workshop, 2019.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Z. C. Lipton, J. Steinhardt, Troubling trends in machine learning scholarship,
    in: ICML Machine Learning Debates workshop, 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] R. Rosenthal, The file drawer problem and tolerance for null results.,
    Psychological bulletin 86 (3) (1979) 638.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] D. Sculley, J. Snoek, A. Wiltschko, A. Rahimi, Winner’s curse? on pace,
    progress, and empirical rigor, in: ICLR Workshop, 2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] O. Gencoglu, M. van Gils, E. Guldogan, C. Morikawa, M. Süzen, M. Gruber,
    J. Leinonen, H. Huttunen, Hark side of deep learning–from grad student descent
    to automated machine learning, arXiv preprint arXiv:1904.07633.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] K. Azizzadenesheli, B. Yang, W. Liu, E. Brunskill, Z. Lipton, A. Anandkumar,
    Surprising negative results for generative adversarial tree search, in: Critiquing
    and Correcting Trends in Machine Learning Workshop, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] C. Lyle, P. S. Castro, M. G. Bellemare, A comparative analysis of expected
    and distributional reinforcement learning, in: Thirty-Third AAAI Conference on
    Artificial Intelligence, 2019.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] B. Kartal, P. Hernandez-Leal, M. E. Taylor, Using Monte Carlo tree search
    as a demonstrator within asynchronous deep RL, in: AAAI Workshop on Reinforcement
    Learning in Games, 2019.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] G. Melis, C. Dyer, P. Blunsom, On the state of the art of evaluation
    in neural language models, in: International Conference on Learning Representations,
    2018.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Deep Reinforcement Learning: Pong from Pixels, [https://karpathy.github.io/2016/05/31/rl/](https://karpathy.github.io/2016/05/31/rl/),
    [Online; accessed 7-May-2019] (2016).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] V. Firoiu, W. F. Whitney, J. B. Tenenbaum, [Beating the World’s Best
    at Super Smash Bros. with Deep Reinforcement Learning](http://arxiv.org/abs/1702.06230),
    CoRR abs/1702.06230. [arXiv:1702.06230](http://arxiv.org/abs/1702.06230).'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1702.06230](http://arxiv.org/abs/1702.06230)
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[330] C. Gao, P. Hernandez-Leal, B. Kartal, M. E. Taylor, Skynet: A Top Deep
    RL Agent in the Inaugural Pommerman Team Competition, in: 4th Multidisciplinary
    Conference on Reinforcement Learning and Decision Making, 2019.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] D. Amodei, D. Hernandez, [AI and Compute](https://blog.%20openai.%20com/ai-and-compute)
    (2018).'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://blog.openai.com/ai-and-compute](https://blog.openai.com/ai-and-compute)
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[332] Y. Yu, Towards sample efficient reinforcement learning., in: IJCAI, 2018,
    pp. 5739–5743.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, J. Clune,
    [Deep neuroevolution: Genetic algorithms are a competitive alternative for training
    deep neural networks for reinforcement learning](http://arxiv.org/abs/1712.06567),
    CoRR abs/1712.06567.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1712.06567](http://arxiv.org/abs/1712.06567)
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[334] A. Stooke, P. Abbeel, [Accelerated methods for deep reinforcement learning](http://arxiv.org/abs/1803.02811),
    CoRR abs/1803.02811. [arXiv:1803.02811](http://arxiv.org/abs/1803.02811).'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1803.02811](http://arxiv.org/abs/1803.02811)
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[335] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, J. Kautz, Reinforcement
    learning through asynchronous advantage actor-critic on a GPU, in: International
    Conference on Learning Representations, 2017.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] W. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. P.
    Mohanty, D. P. Liebana, R. Salakhutdinov, N. Topin, M. Veloso, P. Wang, [The MineRL
    Competition on Sample Efficient Reinforcement Learning using Human Priors](http://arxiv.org/abs/1904.10079),
    CoRR abs/1904.10079. [arXiv:1904.10079](http://arxiv.org/abs/1904.10079).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1904.10079](http://arxiv.org/abs/1904.10079)
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[337] G. Cuccu, J. Togelius, P. Cudré-Mauroux, Playing Atari with six neurons,
    in: Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems, International Foundation for Autonomous Agents and Multiagent
    Systems, 2019, pp. 998–1006.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, J. Clune, Go-explore:
    a new approach for hard-exploration problems, arXiv preprint arXiv:1901.10995.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] R. I. Brafman, M. Tennenholtz, R-max-a general polynomial time algorithm
    for near-optimal reinforcement learning, Journal of Machine Learning Research
    3 (Oct) (2002) 213–231.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] A. L. Strehl, M. L. Littman, An analysis of model-based interval estimation
    for Markov decision processes, Journal of Computer and System Sciences 74 (8)
    (2008) 1309–1331.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] J. Schmidhuber, A possibility for implementing curiosity and boredom
    in model-building neural controllers, in: Proc. of the international conference
    on simulation of adaptive behavior: From animals to animats, 1991, pp. 222–227.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] A. G. Barto, Intrinsic motivation and reinforcement learning, in: Intrinsically
    motivated learning in natural and artificial systems, Springer, 2013, pp. 17–47.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] T. D. Kulkarni, K. Narasimhan, A. Saeedi, J. Tenenbaum, Hierarchical
    deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,
    in: Advances in neural information processing systems, 2016, pp. 3675–3683.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] T. G. Dietterich, Ensemble Methods in Machine Learning, in: MCS Proceedings
    of the First International Workshop on Multiple Classifier Systems, Springer Berlin
    Heidelberg, Cagliari, Italy, 2000, pp. 1–15.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, S. Hochreiter,
    [RUDDER: Return Decomposition for Delayed Rewards](http://arxiv.org/abs/1806.07857).'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1806.07857](http://arxiv.org/abs/1806.07857)
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[346] V. Conitzer, T. Sandholm, AWESOME: A general multiagent learning algorithm
    that converges in self-play and learns a best response against stationary opponents,
    Machine Learning 67 (1-2) (2006) 23–43.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen,
    S. Tavener, D. Perez, S. Samothrakis, S. Colton, A survey of Monte Carlo tree
    search methods, IEEE Transactions on Computational Intelligence and AI in games
    4 (1) (2012) 1–43.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] T. Vodopivec, S. Samothrakis, B. Ster, On Monte Carlo tree search and
    reinforcement learning, Journal of Artificial Intelligence Research 60 (2017)
    881–936.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] B. Kartal, J. Godoy, I. Karamouzas, S. J. Guy, Stochastic tree search
    with useful cycles for patrolling problems, in: Robotics and Automation (ICRA),
    2015 IEEE International Conference on, IEEE, 2015, pp. 1289–1294.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] B. Kartal, E. Nunes, J. Godoy, M. Gini, Monte Carlo tree search with
    branch and bound for multi-robot task allocation, in: The IJCAI-16 Workshop on
    Autonomous Mobile Service Robots, 2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] G. Best, O. M. Cliff, T. Patten, R. R. Mettu, R. Fitch, Dec-MCTS: Decentralized
    planning for multi-robot active perception, The International Journal of Robotics
    Research 38 (2-3) (2019) 316–337.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Y.-M. De Hauwere, P. Vrancx, A. Nowe, Learning multi-agent state space
    representations, in: Proceedings of the 9th International Conference on Autonomous
    Agents and Multiagent Systems, Toronto, Canada, 2010, pp. 715–722.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] C. Guestrin, M. Lagoudakis, R. Parr, Coordinated reinforcement learning,
    in: ICML, Vol. 2, 2002, pp. 227–234.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] C. Guestrin, D. Koller, R. Parr, S. Venkataraman, Efficient solution
    algorithms for factored MDPs, Journal of Artificial Intelligence Research 19 (2003)
    399–468.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] C. Amato, F. A. Oliehoek, Scalable Planning and Learning for Multiagent
    POMDPs, in: AAAI, 2015, pp. 1995–2002.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] F. A. Oliehoek, Interactive Learning and Decision Making - Foundations,
    Insights & Challenges., International Joint Conference on Artificial Intelligence.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] F. A. Oliehoek, S. Whiteson, M. T. Spaan, Approximate solutions for factored
    Dec-POMDPs with many agents, in: Proceedings of the 2013 international conference
    on Autonomous agents and multi-agent systems, International Foundation for Autonomous
    Agents and Multiagent Systems, 2013, pp. 563–570.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] R. Becker, S. Zilberstein, V. Lesser, C. V. Goldman, Solving transition
    independent decentralized Markov decision processes, Journal of Artificial Intelligence
    Research 22 (2004) 423–455.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] F. A. Oliehoek, S. J. Witwicki, L. P. Kaelbling, Influence-based abstraction
    for multiagent systems, in: Twenty-Sixth AAAI Conference on Artificial Intelligence,
    2012.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] M. Suau de Castro, E. Congeduti, R. A. Starre, A. Czechowski, F. A. Oliehoek,
    Influence-based abstraction in deep reinforcement learning, in: Adaptive, Learning
    Agents workshop, 2019.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] E. Wei, D. Wicke, D. Freelan, S. Luke, [Multiagent Soft Q-Learning](http://arXiv.org/abs/1804.09817).'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1804.09817](http://arXiv.org/abs/1804.09817)
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[362] R. R. Torrado, P. Bontrager, J. Togelius, J. Liu, D. Perez-Liebana, [Deep
    Reinforcement Learning for General Video Game AI](http://arxiv.org/abs/1806.02448).'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1806.02448](http://arxiv.org/abs/1806.02448)
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[363] P. A. Ortega, S. Legg, [Modeling friends and foes](http://arxiv.org/abs/1807.00196).'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1807.00196](http://arxiv.org/abs/1807.00196)
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[364] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, J. Wang, Mean field multi-agent
    reinforcement learning, in: Proceedings of the 35th International Conference on
    Machine Learning, Stockholm Sweden, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] A. Grover, M. Al-Shedivat, J. K. Gupta, Y. Burda, H. Edwards, Learning
    Policy Representations in Multiagent Systems., in: International Conference on
    Machine Learning, 2018.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] C. K. Ling, F. Fang, J. Z. Kolter, What game are we playing? end-to-end
    learning in normal and extensive form games, in: Twenty-Seventh International
    Joint Conference on Artificial Intelligence, 2018.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] S. Omidshafiei, D. Hennes, D. Morrill, R. Munos, J. Perolat, M. Lanctot,
    A. Gruslys, J.-B. Lespiau, K. Tuyls, Neural Replicator Dynamics, arXiv e-prints
    (2019) arXiv:1906.00190[arXiv:1906.00190](http://arxiv.org/abs/1906.00190).'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] S. Khadka, S. Majumdar, K. Tumer, Evolutionary Reinforcement Learning
    for Sample-Efficient Multiagent Coordination, arXiv e-prints (2019) arXiv:1906.07315[arXiv:1906.07315](http://arxiv.org/abs/1906.07315).'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[◄](/html/1810.05586) [![ar5iv homepage](img/ed0f3cf5a019c4f8e48e41de62929bb0.png)](/)
    [Feeling'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: lucky?](/feeling_lucky) [Conversion
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: report](/log/1810.05587) [Report
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: an issue](https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1810.05587)
    [View original
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: on arXiv](https://arxiv.org/abs/1810.05587)[►](/html/1810.05588)[](javascript:toggleColorScheme()
    "Toggle ar5iv color scheme")[Copyright](https://arxiv.org/help/license) [Privacy
    Policy](https://arxiv.org/help/policies/privacy_policy)Generated on Tue Mar 19
    07:19:24 2024 by [LaTeXML![Mascot Sammy](img/70e087b9e50c3aa663763c3075b0d6c5.png)](http://dlmf.nist.gov/LaTeXML/)
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
