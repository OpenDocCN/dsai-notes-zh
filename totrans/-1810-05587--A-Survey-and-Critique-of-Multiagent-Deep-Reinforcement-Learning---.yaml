- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1810.05587] A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1810.05587] 多智能体深度强化学习的调查与批评¹¹ 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1810.05587](https://ar5iv.labs.arxiv.org/html/1810.05587)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1810.05587](https://ar5iv.labs.arxiv.org/html/1810.05587)
- en: 'A Survey and Critique of Multiagent Deep Reinforcement Learning¹¹1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多智能体深度强化学习的调查与批评¹¹ 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”
- en: Pablo Hernandez-Leal, Bilal Kartal and Matthew E. Taylor
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pablo Hernandez-Leal, Bilal Kartal 和 Matthew E. Taylor
- en: '{pablo.hernandez,bilal.kartal,matthew.taylor}@borealisai.com Borealis AI'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{pablo.hernandez,bilal.kartal,matthew.taylor}@borealisai.com Borealis AI'
- en: Edmonton, Canada
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大，埃德蒙顿
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep reinforcement learning (RL) has achieved outstanding results in recent
    years. This has led to a dramatic increase in the number of applications and methods.
    Recent works have explored learning beyond single-agent scenarios and have considered
    multiagent learning (MAL) scenarios. Initial results report successes in complex
    multiagent domains, although there are several challenges to be addressed. The
    primary goal of this article is to provide a clear overview of current multiagent
    deep reinforcement learning (MDRL) literature. Additionally, we complement the
    overview with a broader analysis: (i) we revisit previous key components, originally
    presented in MAL and RL, and highlight how they have been adapted to multiagent
    deep reinforcement learning settings. (ii) We provide general guidelines to new
    practitioners in the area: describing lessons learned from MDRL works, pointing
    to recent benchmarks, and outlining open avenues of research. (iii) We take a
    more critical tone raising practical challenges of MDRL (e.g., implementation
    and computational demands). We expect this article will help unify and motivate
    future research to take advantage of the abundant literature that exists (e.g.,
    RL and MAL) in a joint effort to promote fruitful research in the multiagent community.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（RL）近年来取得了卓越的成果。这导致了应用和方法数量的显著增加。近期工作探索了超越单一智能体场景的学习，并考虑了多智能体学习（MAL）场景。初步结果报告了在复杂的多智能体领域中的成功，尽管存在若干挑战需要解决。本文的主要目标是提供当前多智能体深度强化学习（MDRL）文献的清晰概述。此外，我们通过更广泛的分析来补充这一概述：（i）我们重新审视了最初在MAL和RL中提出的关键组件，并强调了它们如何被适应到多智能体深度强化学习设置中。（ii）我们向新从业者提供一般性指南：描述从MDRL工作中获得的经验教训，指向最近的基准，并概述开放的研究途径。（iii）我们采取更批判的语气，提出MDRL的实际挑战（例如，实施和计算需求）。我们希望本文能帮助统一和激励未来的研究，利用现有的丰富文献（例如，RL和MAL），共同推动多智能体社区的有益研究。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Almost 20 years ago Stone and Veloso’s seminal survey [[1](#bib.bib1)] laid
    the groundwork for defining the area of multiagent systems (MAS) and its open
    problems in the context of AI. About ten years ago, Shoham, Powers, and Grenager [[2](#bib.bib2)]
    noted that the literature on multiagent learning (MAL) was growing and it was
    not possible to enumerate all relevant articles. Since then, the number of published
    MAL works continues to steadily rise, which led to different surveys on the area,
    ranging from analyzing the basics of MAL and their challenges [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], to addressing specific subareas: game theory
    and MAL [[2](#bib.bib2), [6](#bib.bib6)], cooperative scenarios [[7](#bib.bib7),
    [8](#bib.bib8)], and evolutionary dynamics of MAL [[9](#bib.bib9)]. In just the
    last couple of years, three surveys related to MAL have been published: learning
    in non-stationary environments [[10](#bib.bib10)], agents modeling agents [[11](#bib.bib11)],
    and transfer learning in multiagent RL [[12](#bib.bib12)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎20年前，Stone和Veloso的开创性调查[[1](#bib.bib1)]为定义多智能体系统（MAS）领域及其在AI背景下的开放问题奠定了基础。大约十年前，Shoham、Powers和Grenager[[2](#bib.bib2)]指出，多智能体学习（MAL）文献在增长，无法列举所有相关文章。从那时起，已发表的MAL作品数量持续上升，这导致了该领域的不同调查，从分析MAL的基础及其挑战[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]，到解决特定子领域：博弈论和MAL[[2](#bib.bib2), [6](#bib.bib6)]，合作场景[[7](#bib.bib7),
    [8](#bib.bib8)]，以及MAL的进化动态[[9](#bib.bib9)]。仅在过去几年内，已发布了三项与MAL相关的调查：非平稳环境中的学习[[10](#bib.bib10)]，智能体对智能体建模[[11](#bib.bib11)]，以及多智能体RL中的迁移学习[[12](#bib.bib12)]。
- en: The research interest in MAL has been accompanied by successes in artificial
    intelligence, first, in single-agent video games [[13](#bib.bib13)]; more recently,
    in two-player games, for example, playing Go [[14](#bib.bib14), [15](#bib.bib15)],
    poker [[16](#bib.bib16), [17](#bib.bib17)], and games of two competing teams,
    e.g., DOTA 2 [[18](#bib.bib18)] and StarCraft II [[19](#bib.bib19)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对MAL的研究兴趣与人工智能的成功相伴而生，首先是在单智能体视频游戏中[[13](#bib.bib13)]；更近期的成功则是在双人游戏中，例如围棋[[14](#bib.bib14),
    [15](#bib.bib15)]，扑克[[16](#bib.bib16), [17](#bib.bib17)]，以及两个竞争团队的游戏，例如DOTA 2[[18](#bib.bib18)]和StarCraft
    II[[19](#bib.bib19)]。
- en: 'While different techniques and algorithms were used in the above scenarios,
    in general, they are all a combination of techniques from two main areas: reinforcement
    learning (RL) [[20](#bib.bib20)] and deep learning [[21](#bib.bib21), [22](#bib.bib22)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在上述场景中使用了不同的技术和算法，但一般而言，它们都是来自两个主要领域的技术组合：强化学习（RL）[[20](#bib.bib20)]和深度学习[[21](#bib.bib21),
    [22](#bib.bib22)]。
- en: RL is an area of machine learning where an agent learns by interacting (i.e.,
    taking actions) within a dynamic environment. However, one of the main challenges
    to RL, and traditional machine learning in general, is the need for manually designing
    quality features on which to learn. Deep learning enables efficient representation
    learning, thus allowing the automatic discovery of features [[21](#bib.bib21),
    [22](#bib.bib22)]. In recent years, deep learning has had successes in different
    areas such as computer vision and natural language processing [[21](#bib.bib21),
    [22](#bib.bib22)]. One of the key aspects of deep learning is the use of *neural
    networks* (NNs) that can find compact representations in high-dimensional data [[23](#bib.bib23)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个领域，其中一个智能体通过在动态环境中交互（即采取行动）来学习。然而，RL以及传统机器学习的主要挑战之一是需要手动设计质量特征来进行学习。深度学习实现了高效的表示学习，从而允许自动发现特征[[21](#bib.bib21),
    [22](#bib.bib22)]。近年来，深度学习在计算机视觉和自然语言处理等不同领域取得了成功[[21](#bib.bib21), [22](#bib.bib22)]。深度学习的一个关键方面是使用*神经网络*（NNs），它们可以在高维数据中找到紧凑的表示[[23](#bib.bib23)]。
- en: In deep reinforcement learning (DRL) [[23](#bib.bib23), [24](#bib.bib24)] deep
    neural networks are trained to approximate the optimal policy and/or the value
    function. In this way the deep NN, serving as function approximator, enables powerful
    generalization. One of the key advantages of DRL is that it enables RL to scale
    to problems with high-dimensional state and action spaces. However, most existing
    successful DRL applications so far have been on visual domains (e.g., Atari games),
    and there is still a lot of work to be done for more realistic applications [[25](#bib.bib25),
    [26](#bib.bib26)] with complex dynamics, which are not necessarily vision-based.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习（DRL）中[[23](#bib.bib23), [24](#bib.bib24)]，深度神经网络被训练来逼近最优策略和/或价值函数。这种方式中，作为函数逼近器的深度神经网络能够实现强大的泛化能力。DRL的一个关键优势是，它使强化学习能够扩展到具有高维状态和动作空间的问题。然而，迄今为止，大多数现有成功的DRL应用都集中在视觉领域（例如Atari游戏），对于更加现实的、复杂动态的应用仍需大量工作[[25](#bib.bib25),
    [26](#bib.bib26)]，这些应用不一定是基于视觉的。
- en: 'DRL has been regarded as an important component in constructing general AI
    systems [[27](#bib.bib27)] and has been successfully integrated with other techniques,
    e.g., search [[14](#bib.bib14)], planning [[28](#bib.bib28)], and more recently
    with multiagent systems, with an emerging area of *multiagent deep reinforcement
    learning* *(MDRL)*[[29](#bib.bib29), [30](#bib.bib30)].²²2We have noted inconsistency
    in abbreviations such as: D-MARL, MADRL, deep-multiagent RL and MA-DRL.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）被视为构建通用人工智能系统的重要组成部分[[27](#bib.bib27)]，并已成功地与其他技术集成，例如搜索[[14](#bib.bib14)]、规划[[28](#bib.bib28)]，以及最近的多智能体系统，其中包括新兴领域*多智能体深度强化学习*（*MDRL*）[[29](#bib.bib29),
    [30](#bib.bib30)]。²²2我们注意到缩写存在不一致，如：D-MARL，MADRL，深度多智能体RL和MA-DRL。
- en: Learning in multiagent settings is fundamentally more difficult than the single-agent
    case due to the presence of multiagent pathologies, e.g., the moving target problem
    (non-stationarity) [[2](#bib.bib2), [5](#bib.bib5), [10](#bib.bib10)], curse of
    dimensionality [[2](#bib.bib2), [5](#bib.bib5)], multiagent credit assignment [[31](#bib.bib31),
    [32](#bib.bib32)], global exploration [[8](#bib.bib8)], and relative overgeneralization [[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]. Despite this complexity, top AI conferences
    like AAAI, ICML, ICLR, IJCAI and NeurIPS, and specialized conferences such as
    AAMAS, have published works reporting successes in MDRL. In light of these works,
    we believe it is pertinent to first, have an overview of the recent MDRL works,
    and second, understand how these recent works relate to the existing literature.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体环境中学习基本上比单一智能体情况更加困难，因为存在多智能体病态问题，例如移动目标问题（非静态性）[[2](#bib.bib2), [5](#bib.bib5),
    [10](#bib.bib10)]，维度诅咒[[2](#bib.bib2), [5](#bib.bib5)]，多智能体信用分配[[31](#bib.bib31),
    [32](#bib.bib32)]，全局探索[[8](#bib.bib8)]，以及相对泛化过度[[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35)]。尽管存在这些复杂性，像AAAI、ICML、ICLR、IJCAI和NeurIPS这样的顶级人工智能会议，以及专门的会议如AAMAS，都发表了关于MDRL成功的作品。鉴于这些作品，我们认为首先，概述最近的MDRL作品是相关的，其次，理解这些最近的作品如何与现有文献相关。
- en: This article contributes to the state of the art with a brief survey of the
    current works in MDRL in an effort to complement existing surveys on multiagent
    learning [[36](#bib.bib36), [10](#bib.bib10)], cooperative learning [[7](#bib.bib7),
    [8](#bib.bib8)], agents modeling agents [[11](#bib.bib11)], knowledge reuse in
    multiagent RL [[12](#bib.bib12)], and (single-agent) deep reinforcement learning [[23](#bib.bib23),
    [37](#bib.bib37)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过对当前MDRL工作的简要调查，为现有关于多智能体学习[[36](#bib.bib36), [10](#bib.bib10)]、合作学习[[7](#bib.bib7),
    [8](#bib.bib8)]、代理建模代理[[11](#bib.bib11)]、多智能体RL中的知识重用[[12](#bib.bib12)]以及（单一智能体）深度强化学习[[23](#bib.bib23),
    [37](#bib.bib37)]的调查进行补充。
- en: 'First, we provide a short review of key algorithms in RL such as Q-learning
    and REINFORCE (see Section [2.1](#S2.SS1 "2.1 Reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Second, we review DRL
    highlighting the challenges in this setting and reviewing recent works (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). Third, we present the multiagent setting
    and give an overview of key challenges and results (see Section [3.1](#S3.SS1
    "3.1 Multiagent Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Then, we present the
    identified four categories to group recent MDRL works (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们简要回顾了强化学习中的关键算法，如 Q-learning 和 REINFORCE（见第 [2.1](#S2.SS1 "2.1 强化学习 ‣ 2
    单智能体学习 ‣ 多智能体深度强化学习的调查与评估1footnote 11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。其次，我们回顾了深度强化学习（DRL），突出介绍了这一领域中的挑战，并评审了近期的研究工作（见第 [2.2](#S2.SS2 "2.2 深度强化学习
    ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与评估1footnote 11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。第三，我们介绍了多智能体环境，并概述了关键挑战和结果（见第 [3.1](#S3.SS1 "3.1 多智能体学习 ‣ 3 多智能体深度强化学习 (MDRL)
    ‣ 多智能体深度强化学习的调查与评估1footnote 11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。然后，我们展示了用于归类近期 MDRL 研究的四个类别（见图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 多智能体深度强化学习的调查与评估1footnote
    11footnote 1 本文早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")）：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Analysis of emergent behaviors: evaluate single-agent DRL algorithms in multiagent
    scenarios (e.g., Atari games, social dilemmas, 3D competitive games).'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应对新兴行为的分析：在多智能体场景中评估单智能体 DRL 算法（例如，Atari 游戏、社会困境、3D 竞争游戏）。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Learning communication: agents learn communication protocols to solve cooperative
    tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习沟通：智能体学习沟通协议以解决合作任务。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Learning cooperation: agents learn to cooperate using only actions and (local)
    observations.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习合作：智能体通过仅使用动作和（局部）观察来学习合作。
- en: '4.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Agents modeling agents: agents reason about others to fulfill a task (e.g.,
    best response learners).'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智能体建模智能体：智能体推理其他智能体以完成任务（例如，最佳响应学习者）。
- en: 'For each category we provide a description as well as outline the recent works
    (see Section [3.2](#S3.SS2 "3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and Tables [2](#S3.T2 "Table
    2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")–[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")). Then, we take a step back and
    reflect on how these new works relate to the existing literature. In that context,
    first, we present examples on how methods and algorithms originally introduced
    in RL and MAL were successfully been scaled to MDRL (see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Second, we provide some
    pointers for new practitioners in the area by describing general *lessons learned*
    from the existing MDRL works (see Section [4.2](#S4.SS2 "4.2 Lessons learned ‣
    4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”"))
    and point to recent multiagent benchmarks (see Section [4.3](#S4.SS3 "4.3 Benchmarks
    for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). Third, we take a more critical view and describe practical
    challenges in MDRL, such as reproducibility, hyperparameter tunning, and computational
    demands (see Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Then, we outline some
    open research questions (see Section [4.5](#S4.SS5 "4.5 Open questions ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Lastly, we present our
    conclusions from this work (see Section [5](#S5 "5 Conclusions ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个类别，我们提供了描述，并概述了最近的研究（见第[3.2](#S3.SS2 "3.2 MDRL categorization ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节和表格[2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")–[4](#S3.T4 "Table 4 ‣ 3.2 MDRL categorization ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")）。然后，我们回顾这些新研究如何与现有文献相关。在这一背景下，首先，我们展示了如何将最初在RL和MAL中引入的方法和算法成功扩展到MDRL（见第[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节）。其次，我们通过描述现有MDRL研究中的一般*经验教训*（见第[4.2](#S4.SS2
    "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")节）为新从业者提供一些指引，并指出最近的多智能体基准（见第[4.3](#S4.SS3 "4.3
    Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节）。第三，我们采取更为批判的视角，描述MDRL中的实际挑战，如可重复性、超参数调优和计算需求（见第[4.4](#S4.SS4
    "4.4 Practical challenges in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")节）。然后，我们概述了一些未解的研究问题（见第[4.5](#S4.SS5
    "4.5 Open questions ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节）。最后，我们展示了本研究的结论（见第[5](#S5 "5 Conclusions ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")节）。'
- en: Our goal is to outline a recent and active area (i.e., MDRL), as well as to
    motivate future research to take advantage of the ample and existing literature
    in multiagent learning. We aim to enable researchers with experience in either
    DRL or MAL to gain a common understanding about recent works, and open problems
    in MDRL, and to avoid having scattered sub-communities with little interaction [[2](#bib.bib2),
    [10](#bib.bib10), [11](#bib.bib11), [38](#bib.bib38)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是概述一个近期活跃的领域（即 MDRL），并激励未来研究利用多智能体学习中的丰富现有文献。我们旨在使有 DRL 或 MAL 经验的研究人员对最新工作和
    MDRL 中的开放问题有共同的理解，并避免形成相互隔绝的小圈子 [[2](#bib.bib2), [10](#bib.bib10), [11](#bib.bib11),
    [38](#bib.bib38)]。
- en: '![Refer to caption](img/05ff41b4091bf1206fa5a001eb009d06.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/05ff41b4091bf1206fa5a001eb009d06.png)'
- en: (a) Analysis of emergent behaviors
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 产生行为的分析
- en: '![Refer to caption](img/92d13bf4a92b65ba059ac044451f2559.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/92d13bf4a92b65ba059ac044451f2559.png)'
- en: (b) Learning communication
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 学习沟通
- en: '![Refer to caption](img/0552b30d046b5206bb4d8f8b4fc3798f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0552b30d046b5206bb4d8f8b4fc3798f.png)'
- en: (c) Learning cooperation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 学习合作
- en: '![Refer to caption](img/848d22f0f88e0c4c34d32daabb3340f8.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/848d22f0f88e0c4c34d32daabb3340f8.png)'
- en: (d) Agents modeling agents
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 智能体建模智能体
- en: 'Figure 1: Categories of different MDRL works. (a) Analysis of emergent behaviors:
    evaluate single-agent DRL algorithms in multiagent scenarios. (b) Learning communication:
    agents learn with actions and through messages. (c) Learning cooperation: agents
    learn to cooperate using only actions and (local) observations. (d) Agents modeling
    agents: agents reason about others to fulfill a task (e.g., cooperative or competitive).
    For a more detailed description see Sections [3.3](#S3.SS3 "3.3 Emergent behaviors
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")–[3.6](#S3.SS6 "3.6 Agents modeling agents ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and Tables [2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")–[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同 MDRL 工作的分类。(a) 产生行为的分析：在多智能体场景中评估单智能体 DRL 算法。(b) 学习沟通：智能体通过行动和消息进行学习。(c)
    学习合作：智能体仅通过行动和（局部）观察来学习合作。(d) 智能体建模智能体：智能体推理其他智能体以完成任务（例如，合作或竞争）。有关更详细的描述，请参见章节 [3.3](#S3.SS3
    "3.3 产生行为 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")–[3.6](#S3.SS6
    "3.6 智能体建模智能体 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")
    以及表格 [2](#S3.T2 "表 2 ‣ 3.2 MDRL 分类 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注
    11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")–[4](#S3.T4 "表 4 ‣ 3.2 MDRL 分类 ‣ 3 多智能体深度强化学习
    (MDRL) ‣ 多智能体深度强化学习的综述与批判1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要综述”")。
- en: 2 Single-agent learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 单智能体学习
- en: This section presents the formalism of reinforcement learning and its main components
    before outlining *deep* reinforcement learning along with its particular challenges
    and recent algorithms. For a more detailed description we refer the reader to
    excellent books and surveys on the area [[39](#bib.bib39), [20](#bib.bib20), [23](#bib.bib23),
    [40](#bib.bib40), [24](#bib.bib24)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了强化学习的形式化以及其主要组成部分，然后概述了*深度*强化学习及其特定挑战和最新算法。有关更详细的描述，我们推荐读者参考该领域的优秀书籍和综述文章 [[39](#bib.bib39),
    [20](#bib.bib20), [23](#bib.bib23), [40](#bib.bib40), [24](#bib.bib24)]。
- en: 2.1 Reinforcement learning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习
- en: RL formalizes the interaction of an agent with an environment using a Markov
    decision process (MDP) [[41](#bib.bib41)]. An MDP is defined by the tuple <math
    id="S2.SS1.p1.1.m1.5" class="ltx_Math" alttext="\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle"
    display="inline"><semantics id="S2.SS1.p1.1.m1.5a"><mrow id="S2.SS1.p1.1.m1.5.6.2"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.1"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟨</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1"
    xref="S2.SS1.p1.1.m1.1.1.cmml">𝒮</mi><mo id="S2.SS1.p1.1.m1.5.6.2.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">𝒜</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.3" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.3.3"
    xref="S2.SS1.p1.1.m1.3.3.cmml">R</mi><mo id="S2.SS1.p1.1.m1.5.6.2.4" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    id="S2.SS1.p1.1.m1.4.4" xref="S2.SS1.p1.1.m1.4.4.cmml">T</mi><mo id="S2.SS1.p1.1.m1.5.6.2.5"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.5.5" xref="S2.SS1.p1.1.m1.5.5.cmml">γ</mi><mo
    stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.6" xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.1.m1.5b"><list id="S2.SS1.p1.1.m1.5.6.1.cmml"
    xref="S2.SS1.p1.1.m1.5.6.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝒮</ci><ci
    id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">𝒜</ci><ci id="S2.SS1.p1.1.m1.3.3.cmml"
    xref="S2.SS1.p1.1.m1.3.3">𝑅</ci><ci id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4">𝑇</ci><ci
    id="S2.SS1.p1.1.m1.5.5.cmml" xref="S2.SS1.p1.1.m1.5.5">𝛾</ci></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.1.m1.5c">\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle</annotation></semantics></math>
    where <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics
    id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1"
    xref="S2.SS1.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>
    represents a finite set of states. <math id="S2.SS1.p1.3.m3.1" class="ltx_Math"
    alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝒜</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math>
    represents a finite set of actions. The transition function <math id="S2.SS1.p1.4.m4.2"
    class="ltx_Math" alttext="T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]"
    display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3"
    xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">T</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">:</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3" xref="S2.SS1.p1.4.m4.2.3.3.cmml"><mrow id="S2.SS1.p1.4.m4.2.3.3.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2.cmml">𝒮</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.2.3.cmml">𝒜</mi><mo lspace="0.222em"
    rspace="0.222em" id="S2.SS1.p1.4.m4.2.3.3.2.1a" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">×</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.4" xref="S2.SS1.p1.4.m4.2.3.3.2.4.cmml">𝒮</mi></mrow><mo
    stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.1" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml">→</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3.3.2" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml"><mo stretchy="false"
    id="S2.SS1.p1.4.m4.2.3.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">[</mo><mn
    id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">0</mn><mo id="S2.SS1.p1.4.m4.2.3.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">,</mo><mn id="S2.SS1.p1.4.m4.2.2" xref="S2.SS1.p1.4.m4.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.4.m4.2b"><apply id="S2.SS1.p1.4.m4.2.3.cmml"
    xref="S2.SS1.p1.4.m4.2.3"><ci id="S2.SS1.p1.4.m4.2.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.1">:</ci><ci
    id="S2.SS1.p1.4.m4.2.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.2">𝑇</ci><apply id="S2.SS1.p1.4.m4.2.3.3.cmml"
    xref="S2.SS1.p1.4.m4.2.3.3"><ci id="S2.SS1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.1">→</ci><apply
    id="S2.SS1.p1.4.m4.2.3.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2"><ci id="S2.SS1.p1.4.m4.2.3.3.2.2.cmml"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2">𝒮</ci><ci id="S2.SS1.p1.4.m4.2.3.3.2.3.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2.3">𝒜</ci><ci
    id="S2.SS1.p1.4.m4.2.3.3.2.4.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2.4">𝒮</ci></apply><interval
    closure="closed" id="S2.SS1.p1.4.m4.2.3.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.3.2"><cn
    type="integer" id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">0</cn><cn
    type="integer" id="S2.SS1.p1.4.m4.2.2.cmml" xref="S2.SS1.p1.4.m4.2.2">1</cn></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.4.m4.2c">T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]</annotation></semantics></math>
    determines the probability of a transition from any state <math id="S2.SS1.p1.5.m5.1"
    class="ltx_Math" alttext="s\in\mathcal{S}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mrow
    id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2"
    xref="S2.SS1.p1.5.m5.1.1.2.cmml">s</mi><mo id="S2.SS1.p1.5.m5.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.cmml">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">𝒮</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml"
    xref="S2.SS1.p1.5.m5.1.1"><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑠</ci><ci
    id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝒮</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">s\in\mathcal{S}</annotation></semantics></math>
    to any state <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="s^{\prime}\in\mathcal{S}"
    display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1"
    xref="S2.SS1.p1.6.m6.1.1.cmml"><msup id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml"><mi
    id="S2.SS1.p1.6.m6.1.1.2.2" xref="S2.SS1.p1.6.m6.1.1.2.2.cmml">s</mi><mo id="S2.SS1.p1.6.m6.1.1.2.3"
    xref="S2.SS1.p1.6.m6.1.1.2.3.cmml">′</mo></msup><mo id="S2.SS1.p1.6.m6.1.1.1"
    xref="S2.SS1.p1.6.m6.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.6.m6.1.1.3"
    xref="S2.SS1.p1.6.m6.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><apply
    id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous"
    id="S2.SS1.p1.6.m6.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.1.1.2">superscript</csymbol><ci
    id="S2.SS1.p1.6.m6.1.1.2.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2.2">𝑠</ci><ci id="S2.SS1.p1.6.m6.1.1.2.3.cmml"
    xref="S2.SS1.p1.6.m6.1.1.2.3">′</ci></apply><ci id="S2.SS1.p1.6.m6.1.1.3.cmml"
    xref="S2.SS1.p1.6.m6.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.6.m6.1c">s^{\prime}\in\mathcal{S}</annotation></semantics></math>
    given any possible action <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="a\in\mathcal{A}"
    display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1"
    xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">a</mi><mo
    id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">𝒜</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml"
    xref="S2.SS1.p1.7.m7.1.1"><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">𝑎</ci><ci
    id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝒜</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">a\in\mathcal{A}</annotation></semantics></math>.
    The reward function <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}"
    display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mrow id="S2.SS1.p1.8.m8.1.1"
    xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">R</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mrow id="S2.SS1.p1.8.m8.1.1.3.2"
    xref="S2.SS1.p1.8.m8.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.3.2.2"
    xref="S2.SS1.p1.8.m8.1.1.3.2.2.cmml">𝒮</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.8.m8.1.1.3.2.1" xref="S2.SS1.p1.8.m8.1.1.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.8.m8.1.1.3.2.3" xref="S2.SS1.p1.8.m8.1.1.3.2.3.cmml">𝒜</mi><mo lspace="0.222em"
    rspace="0.222em" id="S2.SS1.p1.8.m8.1.1.3.2.1a" xref="S2.SS1.p1.8.m8.1.1.3.2.1.cmml">×</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.3.2.4" xref="S2.SS1.p1.8.m8.1.1.3.2.4.cmml">𝒮</mi></mrow><mo
    stretchy="false" id="S2.SS1.p1.8.m8.1.1.3.1" xref="S2.SS1.p1.8.m8.1.1.3.1.cmml">→</mo><mi
    id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml">ℝ</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml"
    xref="S2.SS1.p1.8.m8.1.1"><ci id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1">:</ci><ci
    id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑅</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml"
    xref="S2.SS1.p1.8.m8.1.1.3"><ci id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.1">→</ci><apply
    id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2"><ci id="S2.SS1.p1.8.m8.1.1.3.2.2.cmml"
    xref="S2.SS1.p1.8.m8.1.1.3.2.2">𝒮</ci><ci id="S2.SS1.p1.8.m8.1.1.3.2.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2.3">𝒜</ci><ci
    id="S2.SS1.p1.8.m8.1.1.3.2.4.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2.4">𝒮</ci></apply><ci
    id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3">ℝ</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}</annotation></semantics></math>
    defines the immediate and possibly stochastic reward that an agent would receive
    given that the agent executes action <math id="S2.SS1.p1.9.m9.1" class="ltx_Math"
    alttext="a" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mi id="S2.SS1.p1.9.m9.1.1"
    xref="S2.SS1.p1.9.m9.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">𝑎</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">a</annotation></semantics></math>
    while in state <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.p1.10.m10.1a"><mi id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml"
    xref="S2.SS1.p1.10.m10.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.10.m10.1c">s</annotation></semantics></math> and it is transitioned
    to state <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics
    id="S2.SS1.p1.11.m11.1a"><msup id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml"><mi
    id="S2.SS1.p1.11.m11.1.1.2" xref="S2.SS1.p1.11.m11.1.1.2.cmml">s</mi><mo id="S2.SS1.p1.11.m11.1.1.3"
    xref="S2.SS1.p1.11.m11.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.11.m11.1b"><apply id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.p1.11.m11.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">superscript</csymbol><ci
    id="S2.SS1.p1.11.m11.1.1.2.cmml" xref="S2.SS1.p1.11.m11.1.1.2">𝑠</ci><ci id="S2.SS1.p1.11.m11.1.1.3.cmml"
    xref="S2.SS1.p1.11.m11.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.11.m11.1c">s^{\prime}</annotation></semantics></math>, <math id="S2.SS1.p1.12.m12.2"
    class="ltx_Math" alttext="\gamma\in[0,1]" display="inline"><semantics id="S2.SS1.p1.12.m12.2a"><mrow
    id="S2.SS1.p1.12.m12.2.3" xref="S2.SS1.p1.12.m12.2.3.cmml"><mi id="S2.SS1.p1.12.m12.2.3.2"
    xref="S2.SS1.p1.12.m12.2.3.2.cmml">γ</mi><mo id="S2.SS1.p1.12.m12.2.3.1" xref="S2.SS1.p1.12.m12.2.3.1.cmml">∈</mo><mrow
    id="S2.SS1.p1.12.m12.2.3.3.2" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml"><mo stretchy="false"
    id="S2.SS1.p1.12.m12.2.3.3.2.1" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">[</mo><mn
    id="S2.SS1.p1.12.m12.1.1" xref="S2.SS1.p1.12.m12.1.1.cmml">0</mn><mo id="S2.SS1.p1.12.m12.2.3.3.2.2"
    xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">,</mo><mn id="S2.SS1.p1.12.m12.2.2" xref="S2.SS1.p1.12.m12.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.p1.12.m12.2.3.3.2.3" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.12.m12.2b"><apply id="S2.SS1.p1.12.m12.2.3.cmml"
    xref="S2.SS1.p1.12.m12.2.3"><ci id="S2.SS1.p1.12.m12.2.3.2.cmml" xref="S2.SS1.p1.12.m12.2.3.2">𝛾</ci><interval
    closure="closed" id="S2.SS1.p1.12.m12.2.3.3.1.cmml" xref="S2.SS1.p1.12.m12.2.3.3.2"><cn
    type="integer" id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1">0</cn><cn
    type="integer" id="S2.SS1.p1.12.m12.2.2.cmml" xref="S2.SS1.p1.12.m12.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.12.m12.2c">\gamma\in[0,1]</annotation></semantics></math>
    represents the discount factor that balances the trade-off between immediate rewards
    and future rewards.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）通过马尔可夫决策过程（MDP）[[41](#bib.bib41)]形式化了代理与环境的交互。MDP由元组<math id="S2.SS1.p1.1.m1.5"
    class="ltx_Math" alttext="\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle" display="inline"><semantics
    id="S2.SS1.p1.1.m1.5a"><mrow id="S2.SS1.p1.1.m1.5.6.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml"><mo
    stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.1" xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟨</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">𝒮</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">𝒜</mi><mo id="S2.SS1.p1.1.m1.5.6.2.3"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml">R</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.4" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.4.4"
    xref="S2.SS1.p1.1.m1.4.4.cmml">T</mi><mo id="S2.SS1.p1.1.m1.5.6.2.5" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    id="S2.SS1.p1.1.m1.5.5" xref="S2.SS1.p1.1.m1.5.5.cmml">γ</mi><mo stretchy="false"
    id="S2.SS1.p1.1.m1.5.6.2.6" xref="S2.SS1.p1.1.m1.5.6.1.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.1.m1.5b"><list id="S2.SS1.p1.1.m1.5.6.1.cmml"
    xref="S2.SS1.p1.1.m1.5.6.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝒮</ci><ci
    id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">𝒜</ci><ci id="S2.SS1.p1.1.m1.3.3.cmml"
    xref="S2.SS1.p1.1.m1.3.3">𝑅</ci><ci id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4">𝑇</ci><ci
    id="S2.SS1.p1.1.m1.5.5.cmml" xref="S2.SS1.p1.1.m1.5.5">𝛾</ci></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.1.m1.5c">\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle</annotation></semantics></math>定义，其中<math
    id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics
    id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1"
    xref="S2.SS1.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>表示有限状态集合。<math
    id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics
    id="S2.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.3.m3.1.1"
    xref="S2.SS1.p1.3.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝒜</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math>表示有限动作集合。转移函数<math
    id="S2.SS1.p1.4.m4.2" class="ltx_Math" alttext="T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]"
    display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3"
    xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">T</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">:</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3" xref="S2.SS1.p1.4.m4.2.3.3.cmml"><mrow id="S2.SS1.p1.4.m4.2.3.3.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2.cmml">𝒮</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.4.m4.2.
- en: MDPs are adequate models to obtain optimal decisions in *single* agent fully
    observable environments.³³3A Partially Observable Markov Decision Process (POMDP) [[42](#bib.bib42),
    [43](#bib.bib43)] explicitly models environments where the agent no longer sees
    the true system state and instead receives an *observation* (generated from the
    underlying system state). Solving an MDP will yield a policy <math id="S2.SS1.p2.1.m1.1"
    class="ltx_Math" alttext="\pi:\mathcal{S}\rightarrow\mathcal{A}" display="inline"><semantics
    id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi
    id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">π</mi><mo lspace="0.278em"
    rspace="0.278em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">𝒮</mi><mo stretchy="false"
    id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">𝒜</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml"
    xref="S2.SS1.p2.1.m1.1.1"><ci id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">:</ci><ci
    id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝜋</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3"><ci id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1">→</ci><ci
    id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝒮</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3.3">𝒜</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\pi:\mathcal{S}\rightarrow\mathcal{A}</annotation></semantics></math>,
    which is a mapping from states to actions. An optimal policy <math id="S2.SS1.p2.2.m2.1"
    class="ltx_Math" alttext="\pi^{*}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msup
    id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2"
    xref="S2.SS1.p2.2.m2.1.1.2.cmml">π</mi><mo id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml"
    xref="S2.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml"
    xref="S2.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.cmml"
    xref="S2.SS1.p2.2.m2.1.1.2">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p2.2.m2.1c">\pi^{*}</annotation></semantics></math> is the one that
    maximizes the expected discounted sum of rewards. There are different techniques
    for solving MDPs assuming a complete description of all its elements. One of the
    most common techniques is the value iteration algorithm [[44](#bib.bib44)], which
    requires a complete and accurate representation of states, actions, rewards, and
    transitions. However, this may be difficult to obtain in many domains. For this
    reason, RL algorithms often learn from experience interacting with the environment
    in discrete time steps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MDPs 是适用于在*单一*代理完全可观察环境中获得最佳决策的模型。³³3部分可观察马尔可夫决策过程（POMDP）[[42](#bib.bib42),
    [43](#bib.bib43)] 明确建模了代理无法再看到真实系统状态，而是接收*观测*（由基础系统状态生成）的环境。解决 MDP 将产生一个策略 <math
    id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\pi:\mathcal{S}\rightarrow\mathcal{A}"
    display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1"
    xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">π</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">𝒮</mi><mo stretchy="false"
    id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">𝒜</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml"
    xref="S2.SS1.p2.1.m1.1.1"><ci id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">:</ci><ci
    id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝜋</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3"><ci id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1">→</ci><ci
    id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝒮</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3.3">𝒜</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\pi:\mathcal{S}\rightarrow\mathcal{A}</annotation></semantics></math>，它是从状态到动作的映射。一个最优策略
    <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\pi^{*}" display="inline"><semantics
    id="S2.SS1.p2.2.m2.1a"><msup id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi
    id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">π</mi><mo id="S2.SS1.p2.2.m2.1.1.3"
    xref="S2.SS1.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">superscript</csymbol><ci
    id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\pi^{*}</annotation></semantics></math>
    是最大化期望折扣回报总和的策略。对于 MDPs 的解决有不同的技术，前提是对其所有元素有完整描述。最常见的技术之一是价值迭代算法 [[44](#bib.bib44)]，它需要对状态、动作、奖励和转移有完整而准确的表示。然而，在许多领域，这可能很难获得。因此，RL
    算法通常通过在离散时间步中与环境交互的经验来学习。
- en: Q-learning
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q-学习
- en: 'One of the most well known algorithms for RL is Q-learning [[45](#bib.bib45)].
    It has been devised for stationary, single-agent, fully observable environments
    with discrete actions. A Q-learning agent keeps the estimate of its expected payoff
    starting in state <math id="S2.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="s"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">s</annotation></semantics></math>,
    taking action <math id="S2.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1"
    xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">𝑎</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">a</annotation></semantics></math>
    as <math id="S2.SS1.SSS0.Px1.p1.3.m3.2" class="ltx_Math" alttext="\hat{Q}(s,a)"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.2a"><mrow id="S2.SS1.SSS0.Px1.p1.3.m3.2.3"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"><mover accent="true" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.2b"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2">𝑄</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">𝑠</ci><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2">𝑎</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.2c">\hat{Q}(s,a)</annotation></semantics></math>.
    Each tabular entry <math id="S2.SS1.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="\hat{Q}(s,a)"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.4.m4.2a"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.3"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"><mover accent="true" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.2b"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2">𝑄</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">𝑠</ci><ci
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2">𝑎</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.2c">\hat{Q}(s,a)</annotation></semantics></math>
    is an estimate of the corresponding optimal <math id="S2.SS1.SSS0.Px1.p1.5.m5.1"
    class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a"><msup
    id="S2.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2"
    xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3"
    xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">superscript</csymbol><ci
    id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2">𝑄</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">Q^{*}</annotation></semantics></math>
    function that maps state-action pairs to the discounted sum of future rewards
    starting with action <math id="S2.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.6.m6.1a"><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.1"
    xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.6.m6.1b"><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1">𝑎</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.6.m6.1c">a</annotation></semantics></math>
    at state <math id="S2.SS1.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.7.m7.1a"><mi id="S2.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.7.m7.1b"><ci id="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.7.m7.1c">s</annotation></semantics></math> and following
    the optimal policy thereafter. Each time the agent transitions from a state <math
    id="S2.SS1.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.8.m8.1a"><mi id="S2.SS1.SSS0.Px1.p1.8.m8.1.1" xref="S2.SS1.SSS0.Px1.p1.8.m8.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.8.m8.1b"><ci id="S2.SS1.SSS0.Px1.p1.8.m8.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.8.m8.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.8.m8.1c">s</annotation></semantics></math> to a state <math
    id="S2.SS1.SSS0.Px1.p1.9.m9.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.9.m9.1a"><msup id="S2.SS1.SSS0.Px1.p1.9.m9.1.1" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.9.m9.1b"><apply id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2">𝑠</ci><ci id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.9.m9.1c">s^{\prime}</annotation></semantics></math>
    via action <math id="S2.SS1.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.10.m10.1a"><mi id="S2.SS1.SSS0.Px1.p1.10.m10.1.1"
    xref="S2.SS1.SSS0.Px1.p1.10.m10.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.10.m10.1b"><ci id="S2.SS1.SSS0.Px1.p1.10.m10.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.10.m10.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.10.m10.1c">a</annotation></semantics></math> receiving
    payoff <math id="S2.SS1.SSS0.Px1.p1.11.m11.1" class="ltx_Math" alttext="r" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.11.m11.1a"><mi id="S2.SS1.SSS0.Px1.p1.11.m11.1.1" xref="S2.SS1.SSS0.Px1.p1.11.m11.1.1.cmml">r</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.11.m11.1b"><ci id="S2.SS1.SSS0.Px1.p1.11.m11.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.11.m11.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.11.m11.1c">r</annotation></semantics></math>, the <math
    id="S2.SS1.SSS0.Px1.p1.12.m12.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.12.m12.1a"><mi id="S2.SS1.SSS0.Px1.p1.12.m12.1.1" xref="S2.SS1.SSS0.Px1.p1.12.m12.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.12.m12.1b"><ci id="S2.SS1.SSS0.Px1.p1.12.m12.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.12.m12.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.12.m12.1c">Q</annotation></semantics></math> table is updated
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中最著名的算法之一是Q学习 [[45](#bib.bib45)]。它是为具有离散动作的稳定、单代理、完全可观测环境设计的。Q学习代理通过估计其从状态<math
    id="S2.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.1.m1.1c">s</annotation></semantics></math>开始，并采取动作<math
    id="S2.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">a</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.2.m2.1c">a</annotation></semantics></math> 作为<math id="S2.SS1.SSS0.Px1.p1.3.m3.2"
    class="ltx_Math" alttext="\hat{Q}(s,a)" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.2a"><mrow
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"><mover
    accent="true" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml">Q</mi><mo
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml">^</mo></mover><mo
    lspace="0em" rspace="0em" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.2b"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2">𝑄</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">𝑠</ci><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2">𝑎</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.2c">\hat{Q}(s,a)</annotation></semantics></math>。每个表格条目<math
    id="S2.SS1.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="\hat{Q}(s,a)" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.4.m4.2a"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"><mover
    accent="true" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2" xref="S
- en: '|  | <math id="S2.E1.m1.7" class="ltx_Math" alttext="\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]"
    display="block"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml"><mrow
    id="S2.E1.m1.7.7.3" xref="S2.E1.m1.7.7.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.3.2"
    xref="S2.E1.m1.7.7.3.2.cmml"><mi id="S2.E1.m1.7.7.3.2.2" xref="S2.E1.m1.7.7.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.3.2.1" xref="S2.E1.m1.7.7.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.3.1" xref="S2.E1.m1.7.7.3.1.cmml">​</mo><mrow id="S2.E1.m1.7.7.3.3.2"
    xref="S2.E1.m1.7.7.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.1"
    xref="S2.E1.m1.7.7.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">s</mi><mo
    id="S2.E1.m1.7.7.3.3.2.2" xref="S2.E1.m1.7.7.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2"
    xref="S2.E1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.3"
    xref="S2.E1.m1.7.7.3.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.7.7.2"
    xref="S2.E1.m1.7.7.2.cmml">←</mo><mrow id="S2.E1.m1.7.7.1" xref="S2.E1.m1.7.7.1.cmml"><mrow
    id="S2.E1.m1.7.7.1.3" xref="S2.E1.m1.7.7.1.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.1.3.2"
    xref="S2.E1.m1.7.7.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.3.2.2" xref="S2.E1.m1.7.7.1.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.3.2.1" xref="S2.E1.m1.7.7.1.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.3.1" xref="S2.E1.m1.7.7.1.3.1.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.3.3.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.3.3.2.1" xref="S2.E1.m1.7.7.1.3.3.1.cmml">(</mo><mi id="S2.E1.m1.3.3"
    xref="S2.E1.m1.3.3.cmml">s</mi><mo id="S2.E1.m1.7.7.1.3.3.2.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml">,</mo><mi
    id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.1.3.3.2.3"
    xref="S2.E1.m1.7.7.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.2"
    xref="S2.E1.m1.7.7.1.2.cmml">+</mo><mrow id="S2.E1.m1.7.7.1.1" xref="S2.E1.m1.7.7.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.3" xref="S2.E1.m1.7.7.1.1.3.cmml">α</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.1.2" xref="S2.E1.m1.7.7.1.1.2.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.2.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.7.7.1.1.1.1.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">(</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml">r</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml">γ</mi><mo
    lspace="0.167em" rspace="0em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml"><munder
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.2.cmml">max</mi><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2.cmml">a</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3.cmml">′</mo></msup></munder><mo
    lspace="0.167em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5a" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml">⁡</mo><mover
    accent="true" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1.cmml">^</mo></mover></mrow><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3a" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">a</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">′</mo></msup><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.5" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.2.cmml">−</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.3.cmml"><mover accent="true"
    id="S2.E1.m1.7.7.1.1.1.1.1.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.1.1.1.1.3.2.2"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.2.cmml">Q</mi><mo id="S2.E1.m1.7.7.1.1.1.1.1.3.2.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.E1.m1.7.7.1.1.1.1.1.3.1" xref="S2.E1.m1.7.7.1.1.1.1.1.3.1.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.1" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">(</mo><mi
    id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">s</mi><mo id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.2"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">,</mo><mi id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml">a</mi><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7"><ci
    id="S2.E1.m1.7.7.2.cmml" xref="S2.E1.m1.7.7.2">←</ci><apply id="S2.E1.m1.7.7.3.cmml"
    xref="S2.E1.m1.7.7.3"><apply id="S2.E1.m1.7.7.3.2.cmml" xref="S2.E1.m1.7.7.3.2"><ci
    id="S2.E1.m1.7.7.3.2.1.cmml" xref="S2.E1.m1.7.7.3.2.1">^</ci><ci id="S2.E1.m1.7.7.3.2.2.cmml"
    xref="S2.E1.m1.7.7.3.2.2">𝑄</ci></apply><interval closure="open" id="S2.E1.m1.7.7.3.3.1.cmml"
    xref="S2.E1.m1.7.7.3.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑠</ci><ci
    id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝑎</ci></interval></apply><apply id="S2.E1.m1.7.7.1.cmml"
    xref="S2.E1.m1.7.7.1"><apply id="S2.E1.m1.7.7.1.3.cmml" xref="S2.E1.m1.7.7.1.3"><apply
    id="S2.E1.m1.7.7.1.3.2.cmml" xref="S2.E1.m1.7.7.1.3.2"><ci id="S2.E1.m1.7.7.1.3.2.1.cmml"
    xref="S2.E1.m1.7.7.1.3.2.1">^</ci><ci id="S2.E1.m1.7.7.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.3.2.2">𝑄</ci></apply><interval
    closure="open" id="S2.E1.m1.7.7.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.3.3.2"><ci
    id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑠</ci><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">𝑎</ci></interval></apply><apply
    id="S2.E1.m1.7.7.1.1.cmml" xref="S2.E1.m1.7.7.1.1"><ci id="S2.E1.m1.7.7.1.1.3.cmml"
    xref="S2.E1.m1.7.7.1.1.3">𝛼</ci><apply id="S2.E1.m1.7.7.1.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1"><csymbol
    cd="latexml" id="S2.E1.m1.7.7.1.1.1.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.2">delimited-[]</csymbol><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1"><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4">𝑟</ci><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4">𝛾</ci><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5"><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1">subscript</csymbol><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3">superscript</csymbol><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2">𝑎</ci><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3">′</ci></apply></apply><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1">^</ci><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2">𝑄</ci></apply></apply><interval closure="open"
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2"><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑠</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3">′</ci></apply><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2">𝑎</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3">′</ci></apply></interval></apply></apply><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3"><apply id="S2.E1.m1.7.7.1.1.1.1.1.3.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.1">^</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.2">𝑄</ci></apply><interval
    closure="open" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.2"><ci
    id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">𝑠</ci><ci id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">𝑎</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E1.m1.7c">\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]</annotation></semantics></math>
    |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="S2.E1.m1.7" class="ltx_Math" alttext="\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]"
    display="block"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml"><mrow
    id="S2.E1.m1.7.7.3" xref="S2.E1.m1.7.7.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.3.2"
    xref="S2.E1.m1.7.7.3.2.cmml"><mi id="S2.E1.m1.7.7.3.2.2" xref="S2.E1.m1.7.7.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.3.2.1" xref="S2.E1.m1.7.7.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.3.1" xref="S2.E1.m1.7.7.3.1.cmml">​</mo><mrow id="S2.E1.m1.7.7.3.3.2"
    xref="S2.E1.m1.7.7.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.1"
    xref="S2.E1.m1.7.7.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">s</mi><mo
    id="S2.E1.m1.7.7.3.3.2.2" xref="S2.E1.m1.7.7.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2"
    xref="S2.E1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.3"
    xref="S2.E1.m1.7.7.3.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.7.7.2"
    xref="S2.E1.m1.7.7.2.cmml">←</mo><mrow id="S2.E1.m1.7.7.1" xref="S2.E1.m1.7.7.1.cmml"><mrow
    id="S2.E1.m1.7.7.1.3" xref="S2.E1.m1.7.7.1.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.1.3.2"
    xref="S2.E1.m1.7.7.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.3.2.2" xref="S2.E1.m1.7.7.1.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.3.2.1" xref="S2.E1.m1.7.7.1.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.3.1" xref="S2.E1.m1.7.7.1.3.1.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.3.3.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.3.3.2.1" xref="S2.E1.m1.7.7.1.3.3.1.cmml">(</mo><mi id="S2.E1.m1.3.3"
    xref="S2.E1.m1.3.3.cmml">s</mi><mo id="S2.E1.m1.7.7.1.3.3.2.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml">,</mo><mi
    id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.1.3.3.2.3"
    xref="S2.E1.m1.7.7.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.2"
    xref="S2.E1.m1.7.7.1.2.cmml">+</mo><mrow id="S2.E1.m1.7.7.1.1" xref="S2.E1.m1.7.7.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.3" xref="S2.E1.m1.7.7.1.1.3.cmml">α</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.1.2" xref="S2.E1.m1.7.7.1.1.2.cmml">​</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.2.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.7.7.1.1.1.1.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">(</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml">r</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml">γ</mi><mo
    l'
- en: 'with the learning rate <math id="S2.SS1.SSS0.Px1.p1.13.m1.2" class="ltx_Math"
    alttext="\alpha\in[0,1]" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.13.m1.2a"><mrow
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml">α</mi><mo
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1.cmml">∈</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">[</mo><mn
    id="S2.SS1.SSS0.Px1.p1.13.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml">0</mn><mo
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">,</mo><mn
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.13.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3"><ci id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2">𝛼</ci><interval closure="closed" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1">0</cn><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.13.m1.2c">\alpha\in[0,1]</annotation></semantics></math>.
    Q-learning is proven to converge to <math id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1"
    class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1a"><msup
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml">Q</mi><mo
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2">𝑄</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1c">Q^{*}</annotation></semantics></math>
    if state and action spaces are discrete and finite, the sum of the learning rates
    goes to infinity (so that each state-action pair is visited infinitely often)
    and that the sum of the squares of the learning rates is finite (which is required
    to show that the convergence is with probability one) [[46](#bib.bib46), [45](#bib.bib45),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)].
    The convergence of single-step on-policy RL algorithms, i.e, SARSA (<math id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1"
    class="ltx_Math" alttext="\lambda=0" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1a"><mrow
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml">λ</mi><mo
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1.cmml">=</mo><mn
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1b"><apply id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1"><ci id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2">𝜆</ci><cn type="integer" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1c">\lambda=0</annotation></semantics></math>),
    for both decaying exploration (greedy in the limit with infinite exploration)
    and persistent exploration (selecting actions probabilistically according to the
    ranks of the Q values) was demonstrated by Singh et al. [[52](#bib.bib52)]. Furthermore,
    Van Seijen [[53](#bib.bib53)] has proven convergence for Expected SARSA (see Section [3.1](#S3.SS1
    "3.1 Multiagent Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") for convergence results
    in multiagent domains).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率<math id="S2.SS1.SSS0.Px1.p1.13.m1.2" class="ltx_Math" alttext="\alpha\in[0,1]"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.13.m1.2a"><mrow id="S2.SS1.SSS0.Px1.p1.13.m1.2.3"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml">α</mi><mo id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1.cmml">∈</mo><mrow id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">[</mo><mn id="S2.SS1.SSS0.Px1.p1.13.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml">0</mn><mo id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">,</mo><mn id="S2.SS1.SSS0.Px1.p1.13.m1.2.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.3"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.13.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3"><ci id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2">𝛼</ci><interval closure="closed" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1">0</cn><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.13.m1.2c">\alpha\in[0,1]</annotation></semantics></math>。Q-learning已被证明在状态和动作空间为离散和有限时收敛至<math
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1" class="ltx_Math" alttext="Q^{*}" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1a"><msup id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2">𝑄</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1c">Q^{*}</annotation></semantics></math>，如果学习率之和趋向于无穷大（以便每个状态-动作对被无限次访问），并且学习率的平方和有限（这是显示收敛概率为一所需的）
    [[46](#bib.bib46], [45](#bib.bib45], [47](#bib.bib47], [48](#bib.bib48], [49](#bib.bib49],
    [50](#bib.bib50], [51](#bib.bib51)]。单步在策略RL算法的收敛性，即SARSA（<math id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1"
    class="ltx_Math" alttext="\lambda=0" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1a"><mrow
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml">λ</mi><mo
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1.cmml">=</mo><mn
    id="S2.S
- en: REINFORCE (Monte Carlo policy gradient)
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: REINFORCE（蒙特卡洛策略梯度）
- en: In contrast to value-based methods, which do not try to optimize directly over
    a policy space [[54](#bib.bib54)], policy gradient methods can learn parameterized
    policies without using intermediate value estimates.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与不直接优化策略空间的基于价值的方法相比[[54](#bib.bib54)]，策略梯度方法可以在不使用中间价值估计的情况下学习参数化策略。
- en: Policy parameters are learned by following the gradient of some performance
    measure with gradient descent [[55](#bib.bib55)]. For example, REINFORCE [[56](#bib.bib56)]
    uses estimated return by Monte Carlo (MC) methods with full episode trajectories
    to learn policy parameters <math id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1" class="ltx_Math"
    alttext="\theta" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1a"><mi
    id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1c">\theta</annotation></semantics></math>, with
    <math id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5" class="ltx_Math" alttext="\pi(a;s,\theta)\approx\pi(a;s)"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5a"><mrow id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.cmml"><mrow id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2.cmml">π</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1.cmml">a</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">;</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3.cmml">θ</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.4" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">)</mo></mrow></mrow><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.1.cmml">≈</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2.cmml">π</mi><mo
    lspace="0em" rspace="0em" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4.cmml">a</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">;</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5b"><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6"><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2">𝜋</ci><list id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1">𝑎</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2">𝑠</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3">𝜃</ci></list></apply><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2">𝜋</ci><list id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4">𝑎</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5">𝑠</ci></list></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5c">\pi(a;s,\theta)\approx\pi(a;s)</annotation></semantics></math>,
    as follows
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E2.m1.6" class="ltx_Math" alttext="\theta_{t+1}=\theta_{t}+\alpha
    G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}" display="block"><semantics
    id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.7" xref="S2.E2.m1.6.7.cmml"><msub id="S2.E2.m1.6.7.2"
    xref="S2.E2.m1.6.7.2.cmml"><mi id="S2.E2.m1.6.7.2.2" xref="S2.E2.m1.6.7.2.2.cmml">θ</mi><mrow
    id="S2.E2.m1.6.7.2.3" xref="S2.E2.m1.6.7.2.3.cmml"><mi id="S2.E2.m1.6.7.2.3.2"
    xref="S2.E2.m1.6.7.2.3.2.cmml">t</mi><mo id="S2.E2.m1.6.7.2.3.1" xref="S2.E2.m1.6.7.2.3.1.cmml">+</mo><mn
    id="S2.E2.m1.6.7.2.3.3" xref="S2.E2.m1.6.7.2.3.3.cmml">1</mn></mrow></msub><mo
    id="S2.E2.m1.6.7.1" xref="S2.E2.m1.6.7.1.cmml">=</mo><mrow id="S2.E2.m1.6.7.3"
    xref="S2.E2.m1.6.7.3.cmml"><msub id="S2.E2.m1.6.7.3.2" xref="S2.E2.m1.6.7.3.2.cmml"><mi
    id="S2.E2.m1.6.7.3.2.2" xref="S2.E2.m1.6.7.3.2.2.cmml">θ</mi><mi id="S2.E2.m1.6.7.3.2.3"
    xref="S2.E2.m1.6.7.3.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.7.3.1" xref="S2.E2.m1.6.7.3.1.cmml">+</mo><mrow
    id="S2.E2.m1.6.7.3.3" xref="S2.E2.m1.6.7.3.3.cmml"><mi id="S2.E2.m1.6.7.3.3.2"
    xref="S2.E2.m1.6.7.3.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.7.3.3.1"
    xref="S2.E2.m1.6.7.3.3.1.cmml">​</mo><msub id="S2.E2.m1.6.7.3.3.3" xref="S2.E2.m1.6.7.3.3.3.cmml"><mi
    id="S2.E2.m1.6.7.3.3.3.2" xref="S2.E2.m1.6.7.3.3.3.2.cmml">G</mi><mi id="S2.E2.m1.6.7.3.3.3.3"
    xref="S2.E2.m1.6.7.3.3.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.7.3.3.1a"
    xref="S2.E2.m1.6.7.3.3.1.cmml">​</mo><mfrac id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml"><mrow
    id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><mrow id="S2.E2.m1.3.3.3.5" xref="S2.E2.m1.3.3.3.5.cmml"><mo
    rspace="0.167em" id="S2.E2.m1.3.3.3.5.1" xref="S2.E2.m1.3.3.3.5.1.cmml">∇</mo><mi
    id="S2.E2.m1.3.3.3.5.2" xref="S2.E2.m1.3.3.3.5.2.cmml">π</mi></mrow><mo lspace="0em"
    rspace="0em" id="S2.E2.m1.3.3.3.4" xref="S2.E2.m1.3.3.3.4.cmml">​</mo><mrow id="S2.E2.m1.3.3.3.3.3"
    xref="S2.E2.m1.3.3.3.3.4.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.3.3.3.4"
    xref="S2.E2.m1.3.3.3.3.4.cmml">(</mo><msub id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mi
    id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.1.1.1.1.1.1.3"
    xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.5"
    xref="S2.E2.m1.3.3.3.3.4.cmml">;</mo><msub id="S2.E2.m1.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.cmml"><mi
    id="S2.E2.m1.2.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.2.cmml">S</mi><mi id="S2.E2.m1.2.2.2.2.2.2.3"
    xref="S2.E2.m1.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.6"
    xref="S2.E2.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.E2.m1.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.3.cmml"><mi
    id="S2.E2.m1.3.3.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.3.3.2.cmml">θ</mi><mi id="S2.E2.m1.3.3.3.3.3.3.3"
    xref="S2.E2.m1.3.3.3.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E2.m1.3.3.3.3.3.7"
    xref="S2.E2.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S2.E2.m1.6.6.6" xref="S2.E2.m1.6.6.6.cmml"><mi
    id="S2.E2.m1.6.6.6.5" xref="S2.E2.m1.6.6.6.5.cmml">π</mi><mo lspace="0em" rspace="0em"
    id="S2.E2.m1.6.6.6.4" xref="S2.E2.m1.6.6.6.4.cmml">​</mo><mrow id="S2.E2.m1.6.6.6.3.3"
    xref="S2.E2.m1.6.6.6.3.4.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.6.3.3.4"
    xref="S2.E2.m1.6.6.6.3.4.cmml">(</mo><msub id="S2.E2.m1.4.4.4.1.1.1" xref="S2.E2.m1.4.4.4.1.1.1.cmml"><mi
    id="S2.E2.m1.4.4.4.1.1.1.2" xref="S2.E2.m1.4.4.4.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.4.4.4.1.1.1.3"
    xref="S2.E2.m1.4.4.4.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.6.6.3.3.5"
    xref="S2.E2.m1.6.6.6.3.4.cmml">;</mo><msub id="S2.E2.m1.5.5.5.2.2.2" xref="S2.E2.m1.5.5.5.2.2.2.cmml"><mi
    id="S2.E2.m1.5.5.5.2.2.2.2" xref="S2.E2.m1.5.5.5.2.2.2.2.cmml">S</mi><mi id="S2.E2.m1.5.5.5.2.2.2.3"
    xref="S2.E2.m1.5.5.5.2.2.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.6.6.3.3.6"
    xref="S2.E2.m1.6.6.6.3.4.cmml">,</mo><msub id="S2.E2.m1.6.6.6.3.3.3" xref="S2.E2.m1.6.6.6.3.3.3.cmml"><mi
    id="S2.E2.m1.6.6.6.3.3.3.2" xref="S2.E2.m1.6.6.6.3.3.3.2.cmml">θ</mi><mi id="S2.E2.m1.6.6.6.3.3.3.3"
    xref="S2.E2.m1.6.6.6.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E2.m1.6.6.6.3.3.7"
    xref="S2.E2.m1.6.6.6.3.4.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.7.cmml" xref="S2.E2.m1.6.7"><apply
    id="S2.E2.m1.6.7.2.cmml" xref="S2.E2.m1.6.7.2"><csymbol cd="ambiguous" id="S2.E2.m1.6.7.2.1.cmml"
    xref="S2.E2.m1.6.7.2">subscript</csymbol><ci id="S2.E2.m1.6.7.2.2.cmml" xref="S2.E2.m1.6.7.2.2">𝜃</ci><apply
    id="S2.E2.m1.6.7.2.3.cmml" xref="S2.E2.m1.6.7.2.3"><ci id="S2.E2.m1.6.7.2.3.2.cmml"
    xref="S2.E2.m1.6.7.2.3.2">𝑡</ci><cn type="integer" id="S2.E2.m1.6.7.2.3.3.cmml"
    xref="S2.E2.m1.6.7.2.3.3">1</cn></apply></apply><apply id="S2.E2.m1.6.7.3.cmml"
    xref="S2.E2.m1.6.7.3"><apply id="S2.E2.m1.6.7.3.2.cmml" xref="S2.E2.m1.6.7.3.2"><csymbol
    cd="ambiguous" id="S2.E2.m1.6.7.3.2.1.cmml" xref="S2.E2.m1.6.7.3.2">subscript</csymbol><ci
    id="S2.E2.m1.6.7.3.2.2.cmml" xref="S2.E2.m1.6.7.3.2.2">𝜃</ci><ci id="S2.E2.m1.6.7.3.2.3.cmml"
    xref="S2.E2.m1.6.7.3.2.3">𝑡</ci></apply><apply id="S2.E2.m1.6.7.3.3.cmml" xref="S2.E2.m1.6.7.3.3"><ci
    id="S2.E2.m1.6.7.3.3.2.cmml" xref="S2.E2.m1.6.7.3.3.2">𝛼</ci><apply id="S2.E2.m1.6.7.3.3.3.cmml"
    xref="S2.E2.m1.6.7.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.7.3.3.3.1.cmml"
    xref="S2.E2.m1.6.7.3.3.3">subscript</csymbol><ci id="S2.E2.m1.6.7.3.3.3.2.cmml"
    xref="S2.E2.m1.6.7.3.3.3.2">𝐺</ci><ci id="S2.E2.m1.6.7.3.3.3.3.cmml" xref="S2.E2.m1.6.7.3.3.3.3">𝑡</ci></apply><apply
    id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6"><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><apply
    id="S2.E2.m1.3.3.3.5.cmml" xref="S2.E2.m1.3.3.3.5"><ci id="S2.E2.m1.3.3.3.5.1.cmml"
    xref="S2.E2.m1.3.3.3.5.1">∇</ci><ci id="S2.E2.m1.3.3.3.5.2.cmml" xref="S2.E2.m1.3.3.3.5.2">𝜋</ci></apply><list
    id="S2.E2.m1.3.3.3.3.4.cmml" xref="S2.E2.m1.3.3.3.3.3"><apply id="S2.E2.m1.1.1.1.1.1.1.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1.2">𝐴</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">𝑡</ci></apply><apply
    id="S2.E2.m1.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous"
    id="S2.E2.m1.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2">subscript</csymbol><ci
    id="S2.E2.m1.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2.2">𝑆</ci><ci id="S2.E2.m1.2.2.2.2.2.2.3.cmml"
    xref="S2.E2.m1.2.2.2.2.2.2.3">𝑡</ci></apply><apply id="S2.E2.m1.3.3.3.3.3.3.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.3.3.3.1.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S2.E2.m1.3.3.3.3.3.3.2.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3.2">𝜃</ci><ci id="S2.E2.m1.3.3.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3.3.3">𝑡</ci></apply></list></apply><apply
    id="S2.E2.m1.6.6.6.cmml" xref="S2.E2.m1.6.6.6"><ci id="S2.E2.m1.6.6.6.5.cmml"
    xref="S2.E2.m1.6.6.6.5">𝜋</ci><list id="S2.E2.m1.6.6.6.3.4.cmml" xref="S2.E2.m1.6.6.6.3.3"><apply
    id="S2.E2.m1.4.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.4.1.1.1"><csymbol cd="ambiguous"
    id="S2.E2.m1.4.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.4.1.1.1">subscript</csymbol><ci
    id="S2.E2.m1.4.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.4.1.1.1.2">𝐴</ci><ci id="S2.E2.m1.4.4.4.1.1.1.3.cmml"
    xref="S2.E2.m1.4.4.4.1.1.1.3">𝑡</ci></apply><apply id="S2.E2.m1.5.5.5.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.5.2.2.2.1.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2">subscript</csymbol><ci id="S2.E2.m1.5.5.5.2.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2.2">𝑆</ci><ci id="S2.E2.m1.5.5.5.2.2.2.3.cmml" xref="S2.E2.m1.5.5.5.2.2.2.3">𝑡</ci></apply><apply
    id="S2.E2.m1.6.6.6.3.3.3.cmml" xref="S2.E2.m1.6.6.6.3.3.3"><csymbol cd="ambiguous"
    id="S2.E2.m1.6.6.6.3.3.3.1.cmml" xref="S2.E2.m1.6.6.6.3.3.3">subscript</csymbol><ci
    id="S2.E2.m1.6.6.6.3.3.3.2.cmml" xref="S2.E2.m1.6.6.6.3.3.3.2">𝜃</ci><ci id="S2.E2.m1.6.6.6.3.3.3.3.cmml"
    xref="S2.E2.m1.6.6.6.3.3.3.3">𝑡</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E2.m1.6c">\theta_{t+1}=\theta_{t}+\alpha G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}</annotation></semantics></math>
    |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="S2.E2.m1.6" class="ltx_Math" alttext="\theta_{t+1}=\theta_{t}+\alpha
    G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}"'
- en: where <math id="S2.SS1.SSS0.Px2.p2.3.m1.1" class="ltx_Math" alttext="G_{t}"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.3.m1.1a"><msub id="S2.SS1.SSS0.Px2.p2.3.m1.1.1"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml">G</mi><mi id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px2.p2.3.m1.1b"><apply id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1">subscript</csymbol><ci
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2">𝐺</ci><ci
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m1.1c">G_{t}</annotation></semantics></math>
    represents the return, <math id="S2.SS1.SSS0.Px2.p2.4.m2.1" class="ltx_Math" alttext="\alpha"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.4.m2.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m2.1.1"
    xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px2.p2.4.m2.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1">𝛼</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.4.m2.1c">\alpha</annotation></semantics></math>
    is the learning rate, and <math id="S2.SS1.SSS0.Px2.p2.5.m3.1" class="ltx_Math"
    alttext="A_{t}\sim\pi" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.5.m3.1a"><mrow
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"><msub
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml">A</mi><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml">t</mi></msub><mo
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml">∼</mo><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml">π</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m3.1b"><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1">similar-to</csymbol><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2">𝐴</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m3.1c">A_{t}\sim\pi</annotation></semantics></math>.
    A main limitation is that policy gradient methods can have high variance [[54](#bib.bib54)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math id="S2.SS1.SSS0.Px2.p2.3.m1.1" class="ltx_Math" alttext="G_{t}" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.3.m1.1a"><msub id="S2.SS1.SSS0.Px2.p2.3.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml">G</mi><mi
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml">t</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.3.m1.1b"><apply id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2">𝐺</ci><ci id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m1.1c">G_{t}</annotation></semantics></math>
    代表回报，<math id="S2.SS1.SSS0.Px2.p2.4.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.4.m2.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml">α</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.4.m2.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p2.4.m2.1c">\alpha</annotation></semantics></math> 是学习率，而
    <math id="S2.SS1.SSS0.Px2.p2.5.m3.1" class="ltx_Math" alttext="A_{t}\sim\pi" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.5.m3.1a"><mrow id="S2.SS1.SSS0.Px2.p2.5.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"><msub
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml">A</mi><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml">t</mi></msub><mo
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml">∼</mo><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml">π</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m3.1b"><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1">similar-to</csymbol><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2">𝐴</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m3.1c">A_{t}\sim\pi</annotation></semantics></math>。一个主要的限制是策略梯度方法可能具有较高的方差
    [[54](#bib.bib54)]。
- en: 'The policy gradient update can be generalized to include a comparison to an
    arbitrary *baseline* of the state [[56](#bib.bib56)]. The baseline, <math id="S2.SS1.SSS0.Px2.p3.1.m1.1"
    class="ltx_Math" alttext="b(s)" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.1.m1.1a"><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mi id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml">b</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2"><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2">𝑏</ci><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1">𝑠</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p3.1.m1.1c">b(s)</annotation></semantics></math>,
    can be any function, as long as it does not vary with the action; the baseline
    leaves the expected value of the update unchanged, but it can have an effect on
    its variance [[20](#bib.bib20)]. A natural choice for the baseline is a learned
    state-value function, this reduces the variance, and it is bias-free if learned
    by MC.⁴⁴4Action-dependant baselines had been proposed [[57](#bib.bib57), [58](#bib.bib58)],
    however, a recent study by Tucker et al. [[59](#bib.bib59)] found that in many
    works the reason of good performance was because of bugs or errors in the code,
    rather than the proposed method itself. Moreover, when using the state-value function
    for bootstrapping (updating the value estimate for a state from the estimated
    values of subsequent states) it assigns credit (reducing the variance but introducing
    bias), i.e., criticizes the policy’s action selections. Thus, in actor-critic
    methods [[54](#bib.bib54)], the actor represents the policy, i.e., action-selection
    mechanism, whereas a critic is used for the value function learning. In the case
    when the critic learns a state-action function (<math id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1a"><mi
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1c">Q</annotation></semantics></math> function)
    and a state value function (<math id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1" class="ltx_Math"
    alttext="V" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1a"><mi
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml">V</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1b"><ci id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1c">V</annotation></semantics></math> function),
    an *advantage function* can be computed by subtracting state values from the state-action
    values [[20](#bib.bib20), [60](#bib.bib60)]. The advantage function indicates
    the relative quality of an action compared to other available actions computed
    from the baseline, i.e., state value function. An example of an actor-critic algorithm
    is Deterministic Policy Gradient (DPG) [[61](#bib.bib61)]. In DPG [[61](#bib.bib61)]
    the critic follows the standard Q-learning and the actor is updated following
    the gradient of the policy’s performance [[62](#bib.bib62)], DPG was later extended
    to DRL (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and MDRL (see Section [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). For multiagent learning
    settings the variance is further increased as all the agents’ rewards depend on
    the rest of the agents, and it is formally shown that as the number of agents
    increase, the probability of taking a correct gradient direction decreases exponentially [[63](#bib.bib63)].
    Recent MDRL works addressed this high variance issue, e.g., COMA [[64](#bib.bib64)]
    and MADDPG [[63](#bib.bib63)] (see Section [3.5](#S3.SS5 "3.5 Learning cooperation
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 策策政策梯度更新可以被推广到与任意*基准*进行比较[[56](#bib.bib56)]。基准，<math id="S2.SS1.SSS0.Px2.p3.1.m1.1"
    class="ltx_Math" alttext="b(s)" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.1.m1.1a"><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mi id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml">b</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1.cmml">​</mo><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p3.1.m1.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2"><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2">𝑏</ci><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.1.m1.1c">b(s)</annotation></semantics></math>，可以是任何函数，只要它不随动作变化；基准不改变更新的期望值，但可能会影响其方差[[20](#bib.bib20)]。自然的基准选择是学习得到的状态值函数，这会减少方差，并且如果通过MC学习则无偏。⁴⁴以前提出过基于动作的基准[[57](#bib.bib57)，[58](#bib.bib58)]，然而，Tucker等人的一项近期研究[[59](#bib.bib59)]发现，许多作品中的良好性能是由于代码中的错误或缺陷，而不是提出的方法本身。此外，当使用状态值函数进行自举（从后续状态的估计值更新状态值估计）时，它会分配信用（减少方差但引入偏差），即，批评策略的动作选择。因此，在演员-评论员方法[[54](#bib.bib54)]中，演员代表策略，即动作选择机制，而评论员用于值函数学习。当评论员学习状态-动作函数（<math
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1a"><mi id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1c">Q</annotation></semantics></math>函数）和状态值函数（<math
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1a"><mi id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml">V</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1b"><ci id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1c">V</annotation></semantics></math>函数），可以通过从状态-动作值中减去状态值来计算*优势函数*[[20](#bib.bib20)，[60](#bib.bib60)]。优势函数表示相对于基准，即状态值函数，动作的相对质量。一个演员-评论员算法的例子是确定性策略梯度（DPG）[[61](#bib.bib61)]。在DPG[[61](#bib.bib61)]中，评论员遵循标准的Q学习，而演员则根据策略表现的梯度进行更新[[62](#bib.bib62)]，DPG后来扩展到DRL（见第[2.2节](#S2.SS2
    "2.2 深度强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与评析1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")）和MDRL（见第[3.5节](#S3.SS5
    "3.5 学习合作 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与评析1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")）。对于多智能体学习设置，由于所有代理的奖励都依赖于其他代理，方差进一步增加，并且正式证明，随着代理数量的增加，采取正确梯度方向的概率呈指数级下降[[63](#bib.bib63)]。最近的MDRL工作解决了这个高方差问题，例如COMA[[64](#bib.bib64)]和MADDPG[[63](#bib.bib63)]（见第[3.5节](#S3.SS5
    "3.5 学习合作 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与评析1脚注 11
- en: Policy gradient methods have a clear connection with deep reinforcement learning
    since *the policy might be represented by a neural network* whose input is a representation
    of the state, whose output are action selection probabilities or values for continuous
    control [[65](#bib.bib65)], and whose weights are the policy parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法与深度强化学习有着清晰的联系，因为*策略可能由神经网络表示*，其输入是状态的表示，输出是动作选择的概率或连续控制的值[[65](#bib.bib65)]，权重则是策略参数。
- en: 2.2 Deep reinforcement learning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度强化学习
- en: 'While tabular RL methods such as Q-learning are successful in domains that
    do not suffer from the curse of dimensionality, there are many limitations: learning
    in large state spaces can be prohibitively slow, methods do not generalize (across
    the state space), and state representations need to be hand-specified [[20](#bib.bib20)].
    Function approximators tried to address those limitations, using for example,
    decision trees [[66](#bib.bib66)], tile coding [[67](#bib.bib67)], radial basis
    functions [[68](#bib.bib68)], and locally weighted regression [[69](#bib.bib69)]
    to approximate the value function.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像Q学习这样的表格化RL方法在不受维度灾难影响的领域中取得了成功，但存在许多限制：在大型状态空间中学习可能非常缓慢，方法无法进行广泛的泛化（跨状态空间），状态表示需要手动指定[[20](#bib.bib20)]。函数逼近器尝试解决这些限制，例如使用决策树[[66](#bib.bib66)]、平铺编码[[67](#bib.bib67)]、径向基函数[[68](#bib.bib68)]和局部加权回归[[69](#bib.bib69)]来逼近价值函数。
- en: Similarly, these challenges can be addressed by using deep learning, i.e., neural
    networks [[69](#bib.bib69), [66](#bib.bib66)] as function approximators. For example,
    <math id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="Q(s,a;\theta)" display="inline"><semantics
    id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi
    id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">Q</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">​</mo><mrow
    id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo stretchy="false"
    id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1"
    xref="S2.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi
    id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3"
    xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">θ</mi><mo
    stretchy="false" id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml"
    xref="S2.SS2.p2.1.m1.3.4"><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">𝑄</ci><vector
    id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.p2.1.m1.1.1">𝑠</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">𝑎</ci><ci
    id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">𝜃</ci></vector></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">Q(s,a;\theta)</annotation></semantics></math>
    can be used to approximate the state-action values with <math id="S2.SS2.p2.2.m2.1"
    class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi
    id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\theta</annotation></semantics></math>
    representing the neural network weights. This has two advantages, first, deep
    learning helps to generalize across states improving the sample efficiency for
    large state-space RL problems. Second, deep learning can be used to reduce (or
    eliminate) the need for manually designing features to represent state information [[21](#bib.bib21),
    [22](#bib.bib22)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，这些挑战可以通过使用深度学习，即神经网络[[69](#bib.bib69), [66](#bib.bib66)]作为函数逼近器来解决。例如，<math
    id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="Q(s,a;\theta)" display="inline"><semantics
    id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi
    id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">Q</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">​</mo><mrow
    id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo stretchy="false"
    id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1"
    xref="S2.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi
    id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3"
    xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">θ</mi><mo
    stretchy="false" id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml"
    xref="S2.SS2.p2.1.m1.3.4"><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">𝑄</ci><vector
    id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.p2.1.m1.1.1">𝑠</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">𝑎</ci><ci
    id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">𝜃</ci></vector></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">Q(s,a;\theta)</annotation></semantics></math>可以用来近似状态-动作值，其中<math
    id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics
    id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.p2.2.m2.1c">\theta</annotation></semantics></math>代表神经网络的权重。这有两个优点，首先，深度学习有助于在状态间泛化，提高大状态空间强化学习问题的样本效率。其次，深度学习可用于减少（或消除）手动设计用于表示状态信息的特征的需求[[21](#bib.bib21),
    [22](#bib.bib22)]。
- en: However, extending deep learning to RL problems comes with additional challenges
    including non-i.i.d. (not independently and identically distributed) data. Many
    supervised learning methods assume that training data is from an i.i.d. stationary
    distribution [[70](#bib.bib70), [22](#bib.bib22), [71](#bib.bib71)]. However,
    in RL, training data consists of highly correlated sequential agent-environment
    interactions, which violates the *independence* condition. Moreover, RL training
    data distribution is non-stationary as the agent actively learns while exploring
    different parts of the state space, violating the condition of sampled data being
    *identically distributed* [[72](#bib.bib72)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将深度学习扩展到强化学习问题也带来了额外的挑战，包括非独立同分布（non-i.i.d.）的数据。许多监督学习方法假设训练数据来自独立同分布的静态分布
    [[70](#bib.bib70), [22](#bib.bib22), [71](#bib.bib71)]。然而，在强化学习中，训练数据由高度相关的顺序代理-环境交互组成，违反了*独立性*条件。此外，强化学习的训练数据分布是非静态的，因为代理在探索状态空间的不同部分时，主动学习，违反了数据*同分布*的条件
    [[72](#bib.bib72)]。
- en: 'In practice, using function approximators in RL requires making crucial representational
    decisions and poor design choices can result in estimates that diverge from the
    optimal value function [[73](#bib.bib73), [69](#bib.bib69), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]. In particular, function
    approximation, bootstrapping, and off-policy learning are considered the three
    main properties that when combined, can make the learning to diverge and are known
    as *the deadly triad* [[77](#bib.bib77), [20](#bib.bib20)]. Recently, some works
    have shown that non-linear (i.e., deep) function approximators poorly estimate
    the value function [[78](#bib.bib78), [59](#bib.bib59), [79](#bib.bib79)] and
    another work found problems with Q-learning using function approximation (over/under-estimation,
    instability and even divergence) due to the *delusional* bias: “delusional bias
    occurs whenever a backed-up value estimate is derived from action choices that
    are not realizable in the underlying policy class”[[80](#bib.bib80)]. Additionally,
    convergence results for reinforcement learning using function approximation are
    still scarce [[74](#bib.bib74), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [80](#bib.bib80)]; in general, stronger convergence guarantees are available for
    policy-gradient methods [[55](#bib.bib55)] than for value-based methods [[20](#bib.bib20)].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用RL中的函数逼近器需要做出关键的表示决策，而糟糕的设计选择可能导致估计值函数偏离最优值函数 [[73](#bib.bib73), [69](#bib.bib69),
    [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]。特别是，函数逼近、自举和离策略学习被认为是导致学习发散的三个主要属性，并被称为*致命三连击*
    [[77](#bib.bib77), [20](#bib.bib20)]。最近的一些研究表明，非线性（即深度）函数逼近器对价值函数的估计能力较差 [[78](#bib.bib78),
    [59](#bib.bib59), [79](#bib.bib79)]，另一项研究发现Q学习在使用函数逼近时存在问题（过度/欠估计、不稳定甚至发散），原因是*妄想偏差*：“每当从不可实现的动作选择导出支持的值估计时，就会出现妄想偏差”[[80](#bib.bib80)]。此外，使用函数逼近进行强化学习的收敛结果仍然很少见
    [[74](#bib.bib74), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [80](#bib.bib80)]；一般来说，与基于值的方法
    [[20](#bib.bib20)] 相比，策略梯度方法提供更强的收敛保证 [[55](#bib.bib55)]。
- en: Below we mention how the existing DRL methods aim to address these challenges
    when briefly reviewing value-based methods, such as DQN [[13](#bib.bib13)]; policy
    gradient methods, like Proximal Policy Optimization (PPO) [[60](#bib.bib60)];
    and actor-critic methods like Asynchronous Advantage Actor-Critic (A3C) [[84](#bib.bib84)].
    We refer the reader to recent surveys on single-agent DRL [[23](#bib.bib23), [37](#bib.bib37),
    [24](#bib.bib24)] for a more detailed discussion of the literature.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们提及现有的深度强化学习方法在简要回顾基于价值的方法时如何解决这些挑战，例如DQN [[13](#bib.bib13)]；策略梯度方法，如Proximal
    Policy Optimization（PPO） [[60](#bib.bib60)]；以及演员-评论者方法，如异步优势演员评论者（A3C） [[84](#bib.bib84)]。我们建议读者参阅关于单一代理DRL的最新调查
    [[23](#bib.bib23), [37](#bib.bib37), [24](#bib.bib24)]，以获取更详细的文献讨论。
- en: Value-based methods
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于价值的方法
- en: '![Refer to caption](img/c7d77c308940bcefcd4044d97716b880.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7d77c308940bcefcd4044d97716b880.png)'
- en: 'Figure 2: Deep Q-Network (DQN) [[13](#bib.bib13)]: Inputs are four stacked
    frames; the network is composed of several layers: *Convolutional* layers employ
    filters to learn features from high-dimensional data with a much smaller number
    of neurons and *Dense* layers are fully-connected layers. The last layer represents
    the actions the agent can take (in this case, <math id="S2.F2.3.1.m1.1" class="ltx_Math"
    alttext="10" display="inline"><semantics id="S2.F2.3.1.m1.1b"><mn id="S2.F2.3.1.m1.1.1"
    xref="S2.F2.3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content"
    id="S2.F2.3.1.m1.1c"><cn type="integer" id="S2.F2.3.1.m1.1.1.cmml" xref="S2.F2.3.1.m1.1.1">10</cn></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.3.1.m1.1d">10</annotation></semantics></math>
    possible actions). Deep Recurrent Q-Network (DRQN) [[85](#bib.bib85)], which extends
    DQN to partially observable domains [[42](#bib.bib42)], is identical to this setup
    except the penultimate layer (<math id="S2.F2.4.2.m2.1" class="ltx_Math" alttext="1\times
    256" display="inline"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.1"
    xref="S2.F2.4.2.m2.1.1.cmml"><mn id="S2.F2.4.2.m2.1.1.2" xref="S2.F2.4.2.m2.1.1.2.cmml">1</mn><mo
    lspace="0.222em" rspace="0.222em" id="S2.F2.4.2.m2.1.1.1" xref="S2.F2.4.2.m2.1.1.1.cmml">×</mo><mn
    id="S2.F2.4.2.m2.1.1.3" xref="S2.F2.4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.1.cmml"
    xref="S2.F2.4.2.m2.1.1"><cn type="integer" id="S2.F2.4.2.m2.1.1.2.cmml" xref="S2.F2.4.2.m2.1.1.2">1</cn><cn
    type="integer" id="S2.F2.4.2.m2.1.1.3.cmml" xref="S2.F2.4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.4.2.m2.1d">1\times 256</annotation></semantics></math>
    Dense layer) is replaced with a recurrent LSTM layer [[86](#bib.bib86)].'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度 Q 网络 (DQN) [[13](#bib.bib13)]：输入为四个堆叠的帧；网络由多个层组成：*卷积* 层使用滤波器从高维数据中学习特征，具有较少的神经元，而*全连接*
    层是完全连接的层。最后一层表示代理可以采取的动作（在此情况下为<math id="S2.F2.3.1.m1.1" class="ltx_Math" alttext="10"
    display="inline"><semantics id="S2.F2.3.1.m1.1b"><mn id="S2.F2.3.1.m1.1.1" xref="S2.F2.3.1.m1.1.1.cmml">10</mn><annotation-xml
    encoding="MathML-Content" id="S2.F2.3.1.m1.1c"><cn type="integer" id="S2.F2.3.1.m1.1.1.cmml"
    xref="S2.F2.3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex"
    id="S2.F2.3.1.m1.1d">10</annotation></semantics></math> 种可能动作)。深度递归 Q 网络 (DRQN) [[85](#bib.bib85)]，它将
    DQN 扩展到部分可观察领域 [[42](#bib.bib42)]，除了倒数第二层（<math id="S2.F2.4.2.m2.1" class="ltx_Math"
    alttext="1\times 256" display="inline"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.1"
    xref="S2.F2.4.2.m2.1.1.cmml"><mn id="S2.F2.4.2.m2.1.1.2" xref="S2.F2.4.2.m2.1.1.2.cmml">1</mn><mo
    lspace="0.222em" rspace="0.222em" id="S2.F2.4.2.m2.1.1.1" xref="S2.F2.4.2.m2.1.1.1.cmml">×</mo><mn
    id="S2.F2.4.2.m2.1.1.3" xref="S2.F2.4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.1.cmml"
    xref="S2.F2.4.2.m2.1.1"><cn type="integer" id="S2.F2.4.2.m2.1.1.2.cmml" xref="S2.F2.4.2.m2.1.1.2">1</cn><cn
    type="integer" id="S2.F2.4.2.m2.1.1.3.cmml" xref="S2.F2.4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.4.2.m2.1d">1\times 256</annotation></semantics></math>
    Dense 层) 被替换为递归 LSTM 层 [[86](#bib.bib86)]。
- en: 'The major breakthrough work combining deep learning with Q-learning was the
    Deep Q-Network (DQN) [[13](#bib.bib13)]. DQN uses a deep neural network for function
    approximation [[87](#bib.bib87)]⁵⁵5Before DQN, many approaches used neural networks
    for representing the Q-value function [[88](#bib.bib88)], such as Neural Fitted
    Q-learning [[87](#bib.bib87)] and NEAT+Q [[75](#bib.bib75)]. (see Figure [2](#S2.F2
    "Figure 2 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and maintains an *experience
    replay* (ER) buffer [[89](#bib.bib89), [90](#bib.bib90)] to store interactions
    <math id="S2.SS2.SSS0.Px1.p1.1.m1.4" class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle"
    display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">⟨</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.4"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.3.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">r</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.5"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><msup id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false"
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.6" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.4b"><list id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3">𝑟</ci><apply id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>.
    DQN keeps an additional copy of neural network parameters, <math id="S2.SS2.SSS0.Px1.p1.2.m2.1"
    class="ltx_Math" alttext="\theta^{-}" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.2.m2.1a"><msup
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">θ</mi><mo id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝜃</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.2.m2.1c">\theta^{-}</annotation></semantics></math>,
    for the target network in addition to the <math id="S2.SS2.SSS0.Px1.p1.3.m3.1"
    class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.3.m3.1a"><mi
    id="S2.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p1.3.m3.1c">\theta</annotation></semantics></math> parameters
    to stabilize the learning, i.e., to alleviate the non-stationary data distribution.⁶⁶6Double
    Q-learning [[91](#bib.bib91)] originally proposed keeping two <math id="footnote6.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="footnote6.m1.1b"><mi
    id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="footnote6.m1.1c"><ci id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="footnote6.m1.1d">Q</annotation></semantics></math>
    functions (estimators) to reduce the overestimation bias in RL, while still keeping
    the convergence guarantees, later it was extended to DRL in Double DQN [[92](#bib.bib92)]
    (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")).
    For each training iteration <math id="S2.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math"
    alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.4.m4.1a"><mi id="S2.SS2.SSS0.Px1.p1.4.m4.1.1"
    xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p1.4.m4.1b"><ci id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.4.m4.1c">i</annotation></semantics></math>,
    DQN minimizes the mean-squared error (MSE) between the Q-network and its target
    network using the loss function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '结合深度学习与Q学习的重大突破性工作是深度Q网络（DQN）[[13](#bib.bib13)]。DQN使用深度神经网络进行函数近似[[87](#bib.bib87)]⁵⁵在DQN之前，许多方法使用神经网络来表示Q值函数[[88](#bib.bib88)]，如神经网络拟合Q学习[[87](#bib.bib87)]和NEAT+Q[[75](#bib.bib75)]。（见图[2](#S2.F2
    "Figure 2 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")），并保持一个*经验回放*（ER）缓冲区[[89](#bib.bib89),
    [90](#bib.bib90)]，以存储交互<math id="S2.SS2.SSS0.Px1.p1.1.m1.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.4a"><mrow
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">⟨</mo><mi
    id="S2.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p1.1.m1.2.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">a</mi><mo
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.4" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p1.1.m1.3.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">r</mi><mo
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.5" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><msup
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.6" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.4b"><list id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3">𝑟</ci><apply id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>。DQN还为目标网络保留了一个神经网络参数的额外副本<math
    id="S2.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\theta^{-}" display="inline"><semantics
    id="S2.SS2.SSS0.Px1.p1.2.m2.1a"><msup id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">θ</mi><mo
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml"
    xref'
- en: '|  | <math id="S2.E3.m1.9" class="ltx_Math" alttext="L_{i}(\theta_{i})=\operatorname{\mathbb{E}}_{s,a,r,s^{\prime}}[(r+\gamma
    max_{a^{\prime}}Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]"
    display="block"><semantics id="S2.E3.m1.9a"><mrow id="S2.E3.m1.9.9" xref="S2.E3.m1.9.9.cmml"><mrow
    id="S2.E3.m1.7.7.1" xref="S2.E3.m1.7.7.1.cmml"><msub id="S2.E3.m1.7.7.1.3" xref="S2.E3.m1.7.7.1.3.cmml"><mi
    id="S2.E3.m1.7.7.1.3.2" xref="S2.E3.m1.7.7.1.3.2.cmml">L</mi><mi id="S2.E3.m1.7.7.1.3.3"
    xref="S2.E3.m1.7.7.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.7.7.1.2"
    xref="S2.E3.m1.7.7.1.2.cmml">​</mo><mrow id="S2.E3.m1.7.7.1.1.1" xref="S2.E3.m1.7.7.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E3.m1.7.7.1.1.1.2" xref="S2.E3.m1.7.7.1.1.1.1.cmml">(</mo><msub
    id="S2.E3.m1.7.7.1.1.1.1" xref="S2.E3.m1.7.7.1.1.1.1.cmml"><mi id="S2.E3.m1.7.7.1.1.1.1.2"
    xref="S2.E3.m1.7.7.1.1.1.1.2.cmml">θ</mi><mi id="S2.E3.m1.7.7.1.1.1.1.3" xref="S2.E3.m1.7.7.1.1.1.1.3.cmml">i</mi></msub><mo
    stretchy="false" id="S2.E3.m1.7.7.1.1.1.3" xref="S2.E3.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow><mo
    id="S2.E3.m1.9.9.4" xref="S2.E3.m1.9.9.4.cmml">=</mo><mrow id="S2.E3.m1.9.9.3.2"
    xref="S2.E3.m1.9.9.3.3.cmml"><msub id="S2.E3.m1.8.8.2.1.1" xref="S2.E3.m1.8.8.2.1.1.cmml"><mi
    id="S2.E3.m1.8.8.2.1.1.2" xref="S2.E3.m1.8.8.2.1.1.2.cmml">𝔼</mi><mrow id="S2.E3.m1.4.4.4.4"
    xref="S2.E3.m1.4.4.4.5.cmml"><mi id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml">s</mi><mo
    id="S2.E3.m1.4.4.4.4.2" xref="S2.E3.m1.4.4.4.5.cmml">,</mo><mi id="S2.E3.m1.2.2.2.2"
    xref="S2.E3.m1.2.2.2.2.cmml">a</mi><mo id="S2.E3.m1.4.4.4.4.3" xref="S2.E3.m1.4.4.4.5.cmml">,</mo><mi
    id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">r</mi><mo id="S2.E3.m1.4.4.4.4.4"
    xref="S2.E3.m1.4.4.4.5.cmml">,</mo><msup id="S2.E3.m1.4.4.4.4.1" xref="S2.E3.m1.4.4.4.4.1.cmml"><mi
    id="S2.E3.m1.4.4.4.4.1.2" xref="S2.E3.m1.4.4.4.4.1.2.cmml">s</mi><mo id="S2.E3.m1.4.4.4.4.1.3"
    xref="S2.E3.m1.4.4.4.4.1.3.cmml">′</mo></msup></mrow></msub><mo id="S2.E3.m1.9.9.3.2a"
    xref="S2.E3.m1.9.9.3.3.cmml">⁡</mo><mrow id="S2.E3.m1.9.9.3.2.2" xref="S2.E3.m1.9.9.3.3.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.2" xref="S2.E3.m1.9.9.3.3.cmml">[</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1" xref="S2.E3.m1.9.9.3.2.2.1.cmml"><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.2"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.cmml"><mi id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5.cmml">r</mi><mo id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.4"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.4.cmml">+</mo><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5.cmml">γ</mi><mo lspace="0em" rspace="0em"
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4a" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4b" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><msub
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2.cmml">x</mi><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3.cmml">′</mo></msup></msub><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4c" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4d" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">​</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">(</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">,</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3.cmml">′</mo></msup><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.6" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">;</mo><msubsup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2.cmml">θ</mi><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3.cmml">i</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.3.cmml">−</mo></msubsup><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.7" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.5.cmml">−</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.2.cmml">​</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">(</mo><mi
    id="S2.E3.m1.5.5" xref="S2.E3.m1.5.5.cmml">s</mi><mo id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">,</mo><mi id="S2.E3.m1.6.6" xref="S2.E3.m1.6.6.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">;</mo><msub
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2.cmml">θ</mi><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3.cmml">i</mi></msub><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml">)</mo></mrow><mn
    id="S2.E3.m1.9.9.3.2.2.1.3" xref="S2.E3.m1.9.9.3.2.2.1.3.cmml">2</mn></msup><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.3" xref="S2.E3.m1.9.9.3.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E3.m1.9b"><apply id="S2.E3.m1.9.9.cmml" xref="S2.E3.m1.9.9"><apply
    id="S2.E3.m1.7.7.1.cmml" xref="S2.E3.m1.7.7.1"><apply id="S2.E3.m1.7.7.1.3.cmml"
    xref="S2.E3.m1.7.7.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.1.3.1.cmml" xref="S2.E3.m1.7.7.1.3">subscript</csymbol><ci
    id="S2.E3.m1.7.7.1.3.2.cmml" xref="S2.E3.m1.7.7.1.3.2">𝐿</ci><ci id="S2.E3.m1.7.7.1.3.3.cmml"
    xref="S2.E3.m1.7.7.1.3.3">𝑖</ci></apply><apply id="S2.E3.m1.7.7.1.1.1.1.cmml"
    xref="S2.E3.m1.7.7.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.1.1.1.1.1.cmml"
    xref="S2.E3.m1.7.7.1.1.1">subscript</csymbol><ci id="S2.E3.m1.7.7.1.1.1.1.2.cmml"
    xref="S2.E3.m1.7.7.1.1.1.1.2">𝜃</ci><ci id="S2.E3.m1.7.7.1.1.1.1.3.cmml" xref="S2.E3.m1.7.7.1.1.1.1.3">𝑖</ci></apply></apply><apply
    id="S2.E3.m1.9.9.3.3.cmml" xref="S2.E3.m1.9.9.3.2"><apply id="S2.E3.m1.8.8.2.1.1.cmml"
    xref="S2.E3.m1.8.8.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.8.8.2.1.1.1.cmml"
    xref="S2.E3.m1.8.8.2.1.1">subscript</csymbol><ci id="S2.E3.m1.8.8.2.1.1.2.cmml"
    xref="S2.E3.m1.8.8.2.1.1.2">𝔼</ci><list id="S2.E3.m1.4.4.4.5.cmml" xref="S2.E3.m1.4.4.4.4"><ci
    id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1">𝑠</ci><ci id="S2.E3.m1.2.2.2.2.cmml"
    xref="S2.E3.m1.2.2.2.2">𝑎</ci><ci id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3">𝑟</ci><apply
    id="S2.E3.m1.4.4.4.4.1.cmml" xref="S2.E3.m1.4.4.4.4.1"><csymbol cd="ambiguous"
    id="S2.E3.m1.4.4.4.4.1.1.cmml" xref="S2.E3.m1.4.4.4.4.1">superscript</csymbol><ci
    id="S2.E3.m1.4.4.4.4.1.2.cmml" xref="S2.E3.m1.4.4.4.4.1.2">𝑠</ci><ci id="S2.E3.m1.4.4.4.4.1.3.cmml"
    xref="S2.E3.m1.4.4.4.4.1.3">′</ci></apply></list></apply><apply id="S2.E3.m1.9.9.3.2.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1">superscript</csymbol><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1"><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3"><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5">𝑟</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3"><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5">𝛾</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6">𝑚</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7">𝑎</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8">subscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2">𝑥</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2">𝑎</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3">′</ci></apply></apply><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9">𝑄</ci><vector id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3"><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3">′</ci></apply><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2">𝑎</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3">′</ci></apply><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3">superscript</csymbol><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2">𝜃</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3">𝑖</ci></apply></apply></vector></apply></apply><apply
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4"><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3">𝑄</ci><vector
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1"><ci
    id="S2.E3.m1.5.5.cmml" xref="S2.E3.m1.5.5">𝑠</ci><ci id="S2.E3.m1.6.6.cmml" xref="S2.E3.m1.6.6">𝑎</ci><apply
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1"><csymbol
    cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.1.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1">subscript</csymbol><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2">𝜃</ci><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3">𝑖</ci></apply></vector></apply></apply><cn
    type="integer" id="S2.E3.m1.9.9.3.2.2.1.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.3">2</cn></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E3.m1.9c">L_{i}(\theta_{i})=\operatorname{\mathbb{E}}_{s,a,r,s^{\prime}}[(r+\gamma
    max_{a^{\prime}}Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]</annotation></semantics></math>
    |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: 该公式表示了**均方误差**的计算方式，它基于经验期望值 `L_{i}(\theta_{i})`。
- en: 'where target network parameters <math id="S2.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math"
    alttext="\theta^{-}" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><msup
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">θ</mi><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2">𝜃</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">\theta^{-}</annotation></semantics></math>
    are set to Q-network parameters <math id="S2.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math"
    alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mi
    id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p2.2.m2.1c">\theta</annotation></semantics></math> periodically
    and mini-batches of <math id="S2.SS2.SSS0.Px1.p2.3.m3.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.3.m3.4a"><mrow
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">⟨</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.2.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml">a</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.4" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.3.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml">r</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.5" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><msup
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.6" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.3.m3.4b"><list id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1"><ci id="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3">𝑟</ci><apply id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.3.m3.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    tuples are sampled from the ER buffer, as depicted in Figure [3](#S2.F3 "Figure
    3 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络参数<math id="S2.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\theta^{-}"
    display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><msup id="S2.SS2.SSS0.Px1.p2.1.m1.1.1"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">θ</mi><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.2">𝜃</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">\theta^{-}</annotation></semantics></math>被设置为Q网络参数<math
    id="S2.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics
    id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mi id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">θ</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p2.2.m2.1c">\theta</annotation></semantics></math>，并且小批量的<math
    id="S2.SS2.SSS0.Px1.p2.3.m3.4" class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle"
    display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.3.m3.4a"><mrow id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.2"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">⟨</mo><mi id="S2.SS2.SSS0.Px1.p2.3.m3.1.1"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.3"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p2.3.m3.2.2"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml">a</mi><mo id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.4"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p2.3.m3.3.3"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml">r</mi><mo id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.5"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><msup id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false"
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.6" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.3.m3.4b"><list id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1"><ci id="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1">𝑠</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3">𝑟</ci><apply id="S2.SS2.SSS0.Px1.p2.3.m3.4.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.3.m3.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.1.2">𝑠</ci><ci
- en: 'The ER buffer provides stability for learning as random batches sampled from
    the buffer helps alleviating the problems caused by the non-i.i.d. data. However,
    it comes with disadvantages, such as higher memory requirements and computation
    per real interaction [[93](#bib.bib93)]. The ER buffer is mainly used for off-policy
    RL methods as it can cause a mismatch between buffer content from earlier policy
    and from the current policy for on-policy methods [[93](#bib.bib93)]. Extending
    the ER buffer for the multiagent case is not trivial, see Sections [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”"), [4.1](#S4.SS1 "4.1 Avoiding
    deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”") and [4.2](#S4.SS2 "4.2 Lessons learned
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”").
    Recent works were designed to reduce the problem of catastrophic forgetting (this
    occurs when the trained neural network performs poorly on previously learned tasks
    due to a non-stationary training distribution [[94](#bib.bib94), [95](#bib.bib95)])
    and the ER buffer, in DRL [[96](#bib.bib96)] and MDRL [[97](#bib.bib97)].'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ER 缓冲区为学习提供了稳定性，因为从缓冲区随机抽样的批次有助于缓解由非独立同分布数据造成的问题。然而，它也有一些缺点，例如更高的内存需求和每次真实交互的计算量 [[93](#bib.bib93)]。ER
    缓冲区主要用于离线策略 RL 方法，因为它可能导致早期策略与当前策略之间的缓冲区内容不匹配，对于在线策略方法 [[93](#bib.bib93)]。将 ER
    缓冲区扩展到多智能体情况并非易事，请参见第 [3.5](#S3.SS5 "3.5 Learning cooperation ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")、[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") 和 [4.2](#S4.SS2 "4.2 Lessons
    learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") 部分。近期的研究旨在减少灾难性遗忘的问题（当训练神经网络在之前学到的任务上表现不佳，由于训练分布非静态 [[94](#bib.bib94),
    [95](#bib.bib95)]），以及 ER 缓冲区在 DRL [[96](#bib.bib96)] 和 MDRL [[97](#bib.bib97)]
    中的应用。'
- en: 'DQN has been extended in many ways, for example, by using double estimators [[91](#bib.bib91)]
    to reduce the overestimation bias with Double DQN [[92](#bib.bib92)] (see Section
    [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and by decomposing the
    Q-function with a *dueling*-DQN architecture [[98](#bib.bib98)], where two streams
    are learned, one estimates state values and another one advantages, those are
    combined in the final layer to form <math id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1a"><mi
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1b"><ci id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1c">Q</annotation></semantics></math> values (this
    method improved over Double DQN).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'DQN 已经通过多种方式进行了扩展，例如，通过使用双重估计器[[91](#bib.bib91)]来减少双重 DQN [[92](#bib.bib92)]
    的过度估计偏差（参见[4.1节](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")），以及通过使用*对抗*
    DQN 架构[[98](#bib.bib98)]来分解 Q 函数，其中学习了两个流，一个用于估计状态值，另一个用于估计优势，这些在最终层中结合形成<math
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1a"><mi id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1b"><ci id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1c">Q</annotation></semantics></math> 值（该方法在 Double
    DQN 上有所改进）。'
- en: In practice, DQN is trained using an input of four stacked frames (last four
    frames the agent has encountered). If a game requires a memory of more than four
    frames it will appear non-Markovian to DQN because the future game states (and
    rewards) do not depend only on the input (four frames) but rather on the history [[99](#bib.bib99)].
    Thus, DQN’s performance declines when given incomplete state observations (e.g.,
    one input frame) since DQN assumes full state observability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，DQN 使用四个堆叠帧作为输入（即代理人遇到的最后四帧）。如果游戏需要记忆超过四帧，它对 DQN 来说将表现为非马尔可夫过程，因为未来的游戏状态（和奖励）不仅仅依赖于输入的四帧，而是依赖于历史[[99](#bib.bib99)]。因此，当
    DQN 给出的状态观察不完整（例如，仅一个输入帧）时，其性能会下降，因为 DQN 假设完全的状态可观察性。
- en: 'Real-world tasks often feature incomplete and noisy state information resulting
    from *partial observability* (see Section [2.1](#S2.SS1 "2.1 Reinforcement learning
    ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")).
    Deep Recurrent Q-Networks (DRQN) [[85](#bib.bib85)] proposed using *recurrent
    neural networks*, in particular, Long Short-Term Memory (LSTMs) cells [[86](#bib.bib86)]
    in DQN, for this setting. Consider the architecture in Figure [2](#S2.F2 "Figure
    2 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") with the first dense layer
    after convolution replaced by a layer of LSTM cells. With this addition, DRQN
    has memory capacity so that it can even work with only one input frame rather
    than a stacked input of consecutive frames. This idea has been extended to MDRL,
    see Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and Section [4.2](#S4.SS2 "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”"). There are also other approaches
    to deal with partial observability such as finite state controllers [[100](#bib.bib100)]
    (where action selection is performed according to the complete observation history)
    and using an initiation set of options conditioned on the previously employed
    option [[101](#bib.bib101)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的任务常常由于*部分可观测性*（见第[2.1](#S2.SS1 "2.1 强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与批判1脚注
    11脚注 1 该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节）的影响，表现出不完整和嘈杂的状态信息。深度递归 Q 网络（DRQN）[[85](#bib.bib85)]
    提出了在 DQN 中使用*递归神经网络*，特别是长短期记忆（LSTM）单元[[86](#bib.bib86)]，以应对这种情况。考虑图[2](#S2.F2
    "图 2 ‣ 基于价值的方法 ‣ 2.2 深度强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与批判1脚注 11脚注 1 该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")中显示的架构，将卷积后的第一个密集层替换为一个
    LSTM 单元层。通过这种添加，DRQN 具有记忆能力，因此它即使在只有一个输入帧的情况下也能工作，而不必依赖连续帧的堆叠输入。这一想法已扩展到 MDRL，见图[6](#S3.F6
    "图 6 ‣ 3.6 代理建模代理 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与批判1脚注 11脚注 1 该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")和第[4.2](#S4.SS2
    "4.2 经验教训 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多智能体深度强化学习的调查与批判1脚注 11脚注 1 该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节。还有其他处理部分可观测性的方法，例如有限状态控制器[[100](#bib.bib100)]（其中动作选择是根据完整的观察历史进行的）以及使用基于先前使用的选项的初始选项集[[101](#bib.bib101)]。
- en: '![Refer to caption](img/c1c2a9ce10012396bd49fcdd20cc483d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c1c2a9ce10012396bd49fcdd20cc483d.png)'
- en: 'Figure 3: Representation of a DQN agent that uses an experience replay buffer [[89](#bib.bib89),
    [90](#bib.bib90)] to keep <math id="S2.F3.2.1.m1.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.F3.2.1.m1.4b"><mrow
    id="S2.F3.2.1.m1.4.4.1" xref="S2.F3.2.1.m1.4.4.2.cmml"><mo stretchy="false" id="S2.F3.2.1.m1.4.4.1.2"
    xref="S2.F3.2.1.m1.4.4.2.cmml">⟨</mo><mi id="S2.F3.2.1.m1.1.1" xref="S2.F3.2.1.m1.1.1.cmml">s</mi><mo
    id="S2.F3.2.1.m1.4.4.1.3" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi id="S2.F3.2.1.m1.2.2"
    xref="S2.F3.2.1.m1.2.2.cmml">a</mi><mo id="S2.F3.2.1.m1.4.4.1.4" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi
    id="S2.F3.2.1.m1.3.3" xref="S2.F3.2.1.m1.3.3.cmml">r</mi><mo id="S2.F3.2.1.m1.4.4.1.5"
    xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><msup id="S2.F3.2.1.m1.4.4.1.1" xref="S2.F3.2.1.m1.4.4.1.1.cmml"><mi
    id="S2.F3.2.1.m1.4.4.1.1.2" xref="S2.F3.2.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.F3.2.1.m1.4.4.1.1.3"
    xref="S2.F3.2.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S2.F3.2.1.m1.4.4.1.6"
    xref="S2.F3.2.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S2.F3.2.1.m1.4c"><list id="S2.F3.2.1.m1.4.4.2.cmml" xref="S2.F3.2.1.m1.4.4.1"><ci
    id="S2.F3.2.1.m1.1.1.cmml" xref="S2.F3.2.1.m1.1.1">𝑠</ci><ci id="S2.F3.2.1.m1.2.2.cmml"
    xref="S2.F3.2.1.m1.2.2">𝑎</ci><ci id="S2.F3.2.1.m1.3.3.cmml" xref="S2.F3.2.1.m1.3.3">𝑟</ci><apply
    id="S2.F3.2.1.m1.4.4.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1"><csymbol cd="ambiguous"
    id="S2.F3.2.1.m1.4.4.1.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S2.F3.2.1.m1.4.4.1.1.2.cmml" xref="S2.F3.2.1.m1.4.4.1.1.2">𝑠</ci><ci id="S2.F3.2.1.m1.4.4.1.1.3.cmml"
    xref="S2.F3.2.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F3.2.1.m1.4d">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    tuples for minibatch updates. The Q-values are parameterized with a NN and a policy
    is obtained by selecting (greedily) over those at every timestep.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用经验回放缓冲区的 DQN 代理的表示[[89](#bib.bib89), [90](#bib.bib90)]，用于保持 <math id="S2.F3.2.1.m1.4"
    class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle" display="inline"><semantics
    id="S2.F3.2.1.m1.4b"><mrow id="S2.F3.2.1.m1.4.4.1" xref="S2.F3.2.1.m1.4.4.2.cmml"><mo
    stretchy="false" id="S2.F3.2.1.m1.4.4.1.2" xref="S2.F3.2.1.m1.4.4.2.cmml">⟨</mo><mi
    id="S2.F3.2.1.m1.1.1" xref="S2.F3.2.1.m1.1.1.cmml">s</mi><mo id="S2.F3.2.1.m1.4.4.1.3"
    xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi id="S2.F3.2.1.m1.2.2" xref="S2.F3.2.1.m1.2.2.cmml">a</mi><mo
    id="S2.F3.2.1.m1.4.4.1.4" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi id="S2.F3.2.1.m1.3.3"
    xref="S2.F3.2.1.m1.3.3.cmml">r</mi><mo id="S2.F3.2.1.m1.4.4.1.5" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><msup
    id="S2.F3.2.1.m1.4.4.1.1" xref="S2.F3.2.1.m1.4.4.1.1.cmml"><mi id="S2.F3.2.1.m1.4.4.1.1.2"
    xref="S2.F3.2.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.F3.2.1.m1.4.4.1.1.3" xref="S2.F3.2.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S2.F3.2.1.m1.4.4.1.6" xref="S2.F3.2.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F3.2.1.m1.4c"><list id="S2.F3.2.1.m1.4.4.2.cmml"
    xref="S2.F3.2.1.m1.4.4.1"><ci id="S2.F3.2.1.m1.1.1.cmml" xref="S2.F3.2.1.m1.1.1">𝑠</ci><ci
    id="S2.F3.2.1.m1.2.2.cmml" xref="S2.F3.2.1.m1.2.2">𝑎</ci><ci id="S2.F3.2.1.m1.3.3.cmml"
    xref="S2.F3.2.1.m1.3.3">𝑟</ci><apply id="S2.F3.2.1.m1.4.4.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.F3.2.1.m1.4.4.1.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S2.F3.2.1.m1.4.4.1.1.2.cmml" xref="S2.F3.2.1.m1.4.4.1.1.2">𝑠</ci><ci id="S2.F3.2.1.m1.4.4.1.1.3.cmml"
    xref="S2.F3.2.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F3.2.1.m1.4d">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    元组用于小批量更新。Q 值通过神经网络参数化，策略是通过在每个时间步上选择（贪婪地）这些值来获得的。
- en: Policy gradient methods
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 策略梯度方法
- en: 'For many tasks, particularly for physical control, the action space is continuous
    and high dimensional where DQN is not suitable. Deep Deterministic Policy Gradient
    (DDPG) [[65](#bib.bib65)] is a model-free off-policy actor-critic algorithm for
    such domains, based on the DPG algorithm [[61](#bib.bib61)] (see Section [2.1](#S2.SS1
    "2.1 Reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). Additionally, it proposes a new method for
    updating the networks, i.e., the target network parameters slowly change (this
    could also be applicable to DQN), in contrast to the hard reset (direct weight
    copy) used in DQN. Given the off-policy nature, DDPG generates exploratory behavior
    by adding sampled noise from some noise processes to its actor policy. The authors
    also used batch normalization [[102](#bib.bib102)] to ensure generalization across
    many different tasks without performing manual normalizations. However, note that
    other works have shown batch normalization can cause divergence in DRL [[103](#bib.bib103),
    [104](#bib.bib104)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多任务，特别是对于物理控制，动作空间是连续且高维的，这种情况下 DQN 并不适用。深度确定性策略梯度（Deep Deterministic Policy
    Gradient，DDPG）[[65](#bib.bib65)] 是一个适用于这类领域的无模型离策略演员-评论者算法，基于 DPG 算法[[61](#bib.bib61)]（参见第
    [2.1](#S2.SS1 "2.1 强化学习 ‣ 2 单智体学习 ‣ 多智能体深度强化学习调查与评述1footnote 11footnote 1本工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”"）节）。此外，它提出了一种更新网络的新方法，即目标网络参数缓慢变化（这也适用于
    DQN），与 DQN 中使用的硬重置（直接权重复制）形成对比。由于其离策略的特性，DDPG 通过向其演员策略添加来自某些噪声过程的样本噪声生成探索性行为。作者还使用批归一化[[102](#bib.bib102)]来确保在许多不同任务上的泛化能力，而不需要手动归一化。然而，需要注意的是，其他作品表明批归一化可能导致深度强化学习中的发散[[103](#bib.bib103),
    [104](#bib.bib104)]。
- en: 'Asynchronous Advantage Actor-Critic (A3C) [[93](#bib.bib93)] is an algorithm
    that employs a *parallelized* asynchronous training scheme (using multiple CPU
    threads) for efficiency. It is an on-policy RL method that does not use an experience
    replay buffer. A3C allows multiple workers to simultaneously interact with the
    environment and compute gradients locally. All the workers pass their computed
    local gradients to a global NN which performs the optimization and synchronizes
    with the workers asynchronously (see Figure [4](#S2.F4 "Figure 4 ‣ Policy gradient
    methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")). There is also the Advantage Actor-Critic
    (A2C) method [[105](#bib.bib105)] that combines all the gradients from all the
    workers to update the global NN *synchronously*. The loss function for A3C is
    composed of two terms: policy loss (actor), <math id="S2.SS2.SSS0.Px2.p2.1.m1.1"
    class="ltx_Math" alttext="\mathcal{L}_{\pi}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.1.m1.1a"><msub
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">π</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.1.m1.1c">\mathcal{L}_{\pi}</annotation></semantics></math>,
    and value loss (critic), <math id="S2.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math"
    alttext="\mathcal{L}_{v}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.2.m2.1a"><msub
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.2.m2.1c">\mathcal{L}_{v}</annotation></semantics></math>.
    A3C parameters are updated using the *advantage* function <math id="S2.SS2.SSS0.Px2.p2.3.m3.6"
    class="ltx_Math" alttext="A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)" display="inline"><semantics
    id="S2.SS2.SSS0.Px2.p2.3.m3.6a"><mrow id="S2.SS2.SSS0.Px2.p2.3.m3.6.6" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.4.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">(</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml">s</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2.cmml">a</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.6" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">;</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2.cmml">θ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3.cmml">v</mi></msub><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.7" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.4.cmml">=</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.cmml"><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.1.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">)</mo></mrow></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.1.cmml">−</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2.cmml">V</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.1.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.3.3.cmml">s</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.3.m3.6b"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3"><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5">𝐴</ci><vector id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2">𝑎</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2">𝜃</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3">𝑣</ci></apply></vector></apply><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5"><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2">𝑄</ci><interval
    closure="open" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.1.1">𝑠</ci><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.2.2">𝑎</ci></interval></apply><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2">𝑉</ci><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.3.m3.6c">A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)</annotation></semantics></math>,
    commonly used to reduce variance (see Section [2.1](#S2.SS1 "2.1 Reinforcement
    learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). An entropy loss for the policy, <math id="S2.SS2.SSS0.Px2.p2.4.m4.1"
    class="ltx_Math" alttext="H(\pi)" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.4.m4.1a"><mrow
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"><mi id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.1.cmml">​</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.1.cmml">π</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2"><ci id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2">𝐻</ci><ci id="S2.SS2.SSS0.Px2.p2.4.m4.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.1">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.4.m4.1c">H(\pi)</annotation></semantics></math>,
    is also commonly added, which helps to improve exploration by discouraging premature
    convergence to suboptimal deterministic policies [[93](#bib.bib93)]. Thus, the
    loss function is given by: <math id="S2.SS2.SSS0.Px2.p2.5.m5.3" class="ltx_math_unparsed"
    alttext="\mathcal{L}_{\text{A3C}}=\lambda_{v}\mathcal{L}_{v}+\lambda_{\pi}\mathcal{L}_{\pi}-\lambda_{H}\mathbb{E}_{s\sim\pi}[H(\pi(s,\cdot,\theta)]"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.5.m5.3a"><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3b"><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.4"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.4.2">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.4.3">A3C</mtext></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.5">=</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.6"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.6.2">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.6.3">v</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.7"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.7.2">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.7.3">v</mi></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.8">+</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.9"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.9.2">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.9.3">π</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.10"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.10.2">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.10.3">π</mi></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.11">−</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.12"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.12.2">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.12.3">H</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.13"><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.2">𝔼</mi><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3"><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.2">s</mi><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.1">∼</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.3">π</mi></mrow></msub><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.14"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.1">[</mo><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.2">H</mi><mrow
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3"><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.1">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.2">π</mi><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.1">(</mo><mi id="S2.SS2.SSS0.Px2.p2.5.m5.1.1">s</mi><mo
    rspace="0em" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.2">,</mo><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p2.5.m5.2.2">⋅</mo><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.3">,</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.3">θ</mi><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.4">)</mo></mrow><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.4">]</mo></mrow></mrow></mrow><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.5.m5.3c">\mathcal{L}_{\text{A3C}}=\lambda_{v}\mathcal{L}_{v}+\lambda_{\pi}\mathcal{L}_{\pi}-\lambda_{H}\mathbb{E}_{s\sim\pi}[H(\pi(s,\cdot,\theta)]</annotation></semantics></math>
    with <math id="S2.SS2.SSS0.Px2.p2.6.m6.1" class="ltx_Math" alttext="\lambda_{v},\lambda_{\pi},"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.6.m6.1a"><mrow id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1"><mrow
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml">v</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2.cmml">λ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3.cmml">π</mi></msub></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.6.m6.1b"><list id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2"><apply id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2">𝜆</ci><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2">𝜆</ci><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3">𝜋</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.6.m6.1c">\lambda_{v},\lambda_{\pi},</annotation></semantics></math>
    and <math id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1" class="ltx_Math" alttext="\lambda_{H}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2.cmml">λ</mi><mi id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2">𝜆</ci><ci id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1c">\lambda_{H}</annotation></semantics></math>,
    being weighting terms on the individual loss components. Wang et al. [[106](#bib.bib106)]
    took A3C’s framework but used off-policy learning to create the Actor-critic with
    experience replay (ACER) algorithm. Gu et al. [[107](#bib.bib107)] introduced
    the Interpolated Policy Gradient (IPG) algorithm and showed a connection between
    ACER and DDPG: they are a pair of reparametrization terms (they are special cases
    of IPG) when they are put under the same stochastic policy setting, and when the
    policy is deterministic they collapse into DDPG.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '异步优势演员-评论家（A3C）[[93](#bib.bib93)] 是一种算法，它采用*并行化*异步训练方案（使用多个 CPU 线程）以提高效率。这是一种在线策略的强化学习方法，不使用经验回放缓冲区。A3C
    允许多个工作者同时与环境互动并在本地计算梯度。所有工作者将其计算的本地梯度传递给一个全局神经网络，该网络执行优化并与工作者异步同步（参见图[4](#S2.F4
    "Figure 4 ‣ Policy gradient methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")）。还有一种优势演员-评论家（A2C）方法[[105](#bib.bib105)]，它将所有工作者的梯度合并，以*同步*的方式更新全局神经网络。A3C
    的损失函数由两个部分组成：策略损失（演员），<math id="S2.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\pi}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p2.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">ℒ</mi><mi id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">π</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2">ℒ</ci><ci
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.1.m1.1c">\mathcal{L}_{\pi}</annotation></semantics></math>，以及价值损失（评论家），<math
    id="S2.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{v}" display="inline"><semantics
    id="S2.SS2.SSS0.Px2.p2.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">ℒ</mi><mi
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.2.m2.1c">\mathcal{L}_{v}</annotation></semantics></math>。A3C
    参数使用*优势*函数<math id="S2.SS2.SSS0.Px2.p2.3.m3.6" class="ltx_Math" alttext="A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.3.m3.6a"><mrow id="S2.SS2.SSS0.Px2.p2.3.m3.6.6"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"><mrow id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">(</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml">s</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2'
- en: '![Refer to caption](img/07d5c0b6b6c365bbbd09b85f59844ef8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/07d5c0b6b6c365bbbd09b85f59844ef8.png)'
- en: 'Figure 4: Asynchronous Advantage Actor-Critic (A3C) employs multiple (CPUs)
    workers without needing an ER buffer. Each worker has its own NN and independently
    interacts with the environment to compute the loss and gradients. Workers then
    pass computed gradients to the global NN that optimizes the parameters and synchronizes
    with the worker *asynchronously*. This distributed system is designed for single-agent
    deep RL. Compared to different DQN variants, A3C obtains better performance on
    a variety of Atari games using substantially less training time with multiple
    CPU cores of standard laptops without a GPU [[93](#bib.bib93)]. However, we note
    that more recent approaches use both multiple CPU cores for more efficient training
    data generation and GPUs for more efficient learning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：异步优势行动者-评论者（A3C）使用多个（CPU）工作者，无需ER缓冲区。每个工作者都有自己的神经网络（NN），独立与环境交互以计算损失和梯度。工作者然后将计算得到的梯度传递给全局神经网络（NN），优化参数并异步与工作者同步。这种分布式系统设计用于单一代理深度强化学习。与不同的DQN变体相比，A3C在多个Atari游戏中表现更好，使用标准笔记本电脑的多个CPU核心显著减少训练时间，而无需GPU
    [[93](#bib.bib93)]。但是，我们注意到最近的方法使用多个CPU核心来更高效地生成训练数据，并使用GPU来更高效地学习。
- en: 'Jaderberg et al. [[84](#bib.bib84)] built the Unsupervised Reinforcement and
    Auxiliary Learning (UNREAL) framework on top of A3C and introduced unsupervised
    *auxiliary tasks* (e.g., reward prediction) to speed up the learning process.
    Auxiliary tasks in general are not used for anything other than shaping the features
    of the agent, i.e., facilitating and regularizing the representation learning
    process [[108](#bib.bib108), [109](#bib.bib109)]; their formalization in RL is
    related to the concept of *general value functions* [[20](#bib.bib20), [110](#bib.bib110)].
    The UNREAL framework optimizes a combined loss function <math id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1"
    class="ltx_Math" alttext="\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1a"><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml">UNREAL</mtext></msub><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1.cmml">≈</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml">A3C</mtext></msub><mo rspace="0.055em"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1.cmml">+</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml"><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2.cmml">∑</mo><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml">i</mi></msub><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml">λ</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1.cmml">​</mo><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2.cmml">T</mi><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3.cmml">i</mi></msub></mrow></msub><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2.cmml">ℒ</mi><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2.cmml">A</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3.cmml">i</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"><apply id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3">UNREAL</mtext></ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2">ℒ</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"><mtext
    mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3">A3C</mtext></ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3">𝑖</ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2">𝜆</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3"><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2">𝐴</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2">𝑇</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3">𝑖</ci></apply></apply></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2">ℒ</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3"><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2">𝐴</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2">𝑇</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1c">\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}</annotation></semantics></math>,
    that combines the A3C loss, <math id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1" class="ltx_Math"
    alttext="\mathcal{L}_{\text{A3C}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1a"><msub
    id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2.cmml">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3a.cmml">A3C</mtext></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3a.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3">A3C</mtext></ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1c">\mathcal{L}_{\text{A3C}}</annotation></semantics></math>,
    together with auxiliary task losses <math id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1"
    class="ltx_Math" alttext="\mathcal{L}_{AT_{i}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1a"><msub
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2.cmml">ℒ</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3.cmml">i</mi></msub></mrow></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1b"><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2">ℒ</ci><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3"><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2">𝐴</ci><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2">𝑇</ci><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1c">\mathcal{L}_{AT_{i}}</annotation></semantics></math>,
    where <math id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1" class="ltx_Math" alttext="\lambda_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1a"><msub id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2.cmml">λ</mi><mrow id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.1.cmml">​</mo><msub
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3.cmml">i</mi></msub></mrow></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2">𝜆</ci><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3"><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2">𝐴</ci><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2">𝑇</ci><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1c">\lambda_{AT_{i}}</annotation></semantics></math>
    are weight terms (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") for use of auxiliary tasks in MDRL). In contrast to A3C, UNREAL
    uses a prioritized ER buffer, in which transitions with positive reward are given
    higher probability of being sampled. This approach can be viewed as a simple form
    of prioritized replay [[111](#bib.bib111)], which was in turn inspired by model-based
    RL algorithms like prioritized sweeping [[112](#bib.bib112), [113](#bib.bib113)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Jaderberg 等人[[84](#bib.bib84)] 在 A3C 的基础上构建了**无监督强化学习和辅助学习（UNREAL）**框架，并引入了无监督的*辅助任务*（例如，奖励预测）来加快学习过程。一般而言，辅助任务除了塑造智能体的特征外，没有其他用途，即促进和规范化表示学习过程[[108](#bib.bib108),
    [109](#bib.bib109)]；它们在强化学习中的形式化与*通用价值函数*的概念相关[[20](#bib.bib20), [110](#bib.bib110)]。UNREAL
    框架优化了一个组合损失函数 <math id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1a"><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml">UNREAL</mtext></msub><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1.cmml">≈</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml">A3C</mtext></msub><mo rspace="0.055em"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1.cmml">+</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml"><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2.cmml">∑</mo><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml">i</mi></msub><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml">λ</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1.cmml">​</mo><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"
    xref="S2.SS2.SSS
- en: Another distributed architecture is the Importance Weighted Actor-Learner Architecture
    (IMPALA) [[114](#bib.bib114)]. Unlike A3C or UNREAL, IMPALA actors communicate
    *trajectories of experience* (sequences of states, actions, and rewards) to a
    centralized learner, thus IMPALA decouples acting from learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个分布式架构是重要性加权演员-学习者架构 (IMPALA) [[114](#bib.bib114)]。与A3C或UNREAL不同，IMPALA演员将*经验轨迹*（状态、动作和奖励的序列）传送给集中式学习者，从而使IMPALA将行动与学习解耦。
- en: 'Trust Region Policy Optimization (TRPO) [[60](#bib.bib60)] and Proximal Policy
    Optimization (PPO) [[115](#bib.bib115)] are recently proposed policy gradient
    algorithms where the latter represents the state-of-the art with advantages such
    as being simpler to implement and having better empirical sample complexity. Interestingly,
    a recent work [[79](#bib.bib79)] studying PPO and TRPO arrived at the surprising
    conclusion that these methods often deviate from what the theoretical framework
    would predict: gradient estimates are poorly correlated with the true gradient
    and value networks tend to produce inaccurate predictions for the true value function.
    Compared to vanilla policy gradient algorithms, PPO prevents abrupt changes in
    policies during training through the loss function, similar to early work by Kakade [[116](#bib.bib116)].
    Another advantage of PPO is that it can be used in a distributed fashion, i.e,
    Distributed PPO (DPPO) [[117](#bib.bib117)]. Note that *distributed approaches*
    like DPPO or A3C use parallelization only to improve the learning by more efficient
    training data generation through multiple CPU cores for single agent DRL and they
    should not be considered multiagent approaches (except for recent work which tries
    to exploit this parallelization in a multiagent environment [[118](#bib.bib118)]).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Trust Region Policy Optimization (TRPO) [[60](#bib.bib60)] 和 Proximal Policy
    Optimization (PPO) [[115](#bib.bib115)] 是最近提出的策略梯度算法，其中后者代表了当前的最先进技术，具有如实现更简单和具有更好的经验样本复杂度等优点。有趣的是，一项最近的研究 [[79](#bib.bib79)]
    研究了PPO和TRPO，并得出了一个令人惊讶的结论：这些方法往往偏离理论框架的预测：梯度估计与真实梯度的相关性较差，值网络往往对真实值函数的预测不准确。与普通策略梯度算法相比，PPO通过损失函数防止训练期间策略的急剧变化，类似于Kakade早期的工作 [[116](#bib.bib116)]。PPO的另一个优点是它可以以分布式的方式使用，即分布式PPO
    (DPPO) [[117](#bib.bib117)]。请注意，像DPPO或A3C这样的*分布式方法*仅通过多个CPU核心为单一智能体DRL生成更高效的训练数据来改进学习，它们不应被视为多智能体方法（除非是最近的工作试图在多智能体环境中利用这种并行化 [[118](#bib.bib118)]）。
- en: Lastly, there’s a connection between policy gradient algorithms and Q-learning [[119](#bib.bib119)]
    within the framework of entropy-regularized reinforcement learning [[120](#bib.bib120)]
    where the value and <math id="S2.SS2.SSS0.Px2.p6.1.m1.1" class="ltx_Math" alttext="Q"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p6.1.m1.1a"><mi id="S2.SS2.SSS0.Px2.p6.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p6.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p6.1.m1.1c">Q</annotation></semantics></math>
    functions are slightly altered to consider the entropy of the policy. In this
    vein, Soft Actor-Critic (SAC) [[121](#bib.bib121)] is a recent algorithm that
    concurrently learns a stochastic policy, two Q-functions (taking inspiration from
    Double Q-learning) and a value function. SAC alternates between collecting experience
    with the current policy and updating from batches sampled from the ER buffer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在熵正则化强化学习的框架下，策略梯度算法与Q学习之间存在联系 [[119](#bib.bib119)]，在该框架中，值函数和<math id="S2.SS2.SSS0.Px2.p6.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS2.SSS0.Px2.p6.1.m1.1a"><mi
    id="S2.SS2.SSS0.Px2.p6.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p6.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px2.p6.1.m1.1c">Q</annotation></semantics></math>函数会略微调整，以考虑策略的熵。在这一方面，Soft
    Actor-Critic (SAC) [[121](#bib.bib121)] 是一种新近提出的算法，它同时学习一个随机策略、两个Q函数（从Double Q-learning中获得灵感）和一个值函数。SAC在使用当前策略收集经验和从ER缓冲区中采样的批次中更新之间交替进行。
- en: We have reviewed recent algorithms in DRL, while the list is not exhaustive,
    it provides an overview of the different state-of-art techniques and algorithms
    which will become useful while describing the MDRL techniques in the next section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了近期的DRL算法，虽然列表并不详尽，但它提供了不同最先进技术和算法的概述，这将有助于在下一节描述MDRL技术时使用。
- en: 3 Multiagent Deep Reinforcement Learning (MDRL)
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 多智能体深度强化学习（MDRL）
- en: First, we briefly introduce the general framework on multiagent learning and
    then we dive into the categories and the research on MDRL.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们简要介绍多智能体学习的总体框架，然后深入探讨MDRL的分类和研究。
- en: 3.1 Multiagent Learning
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 多智能体学习
- en: Learning in a multiagent environment is inherently more complex than in the
    single-agent case, as agents interact at the same time with environment and potentially
    with each other [[5](#bib.bib5)]. The *independent* learners, a.k.a. *decentralized*
    learners approach [[122](#bib.bib122)] directly uses single-agent algorithms in
    the multi-agent setting despite the underlying assumptions of these algorithms
    being violated (each agent independently learns its own policy, treating other
    agents as part of the environment). In particular the *Markov property* (the future
    dynamics, transitions, and rewards depend only on the current state) becomes invalid
    since the environment is no longer stationary [[4](#bib.bib4), [6](#bib.bib6),
    [123](#bib.bib123)]. This approach ignores the multiagent nature of the setting
    entirely and it can fail when an opponent adapts or learns, for example, based
    on the past history of interactions [[2](#bib.bib2)]. Despite the lack of guarantees,
    independent learners have been used in practice, providing advantages with regards
    to scalability while often achieving good results [[8](#bib.bib8)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体环境中学习比在单智能体环境中更为复杂
- en: To understand why multiagent domains are non-stationary from agents’ local perspectives,
    consider a simple stochastic (also known as Markov) game <math id="S3.SS1.p2.1.m1.5"
    class="ltx_Math" alttext="(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})"
    display="inline"><semantics id="S3.SS1.p2.1.m1.5a"><mrow id="S3.SS1.p2.1.m1.5.6.2"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.1"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1"
    xref="S3.SS1.p2.1.m1.1.1.cmml">𝒮</mi><mo id="S3.SS1.p2.1.m1.5.6.2.2" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">𝒩</mi><mo
    id="S3.SS1.p2.1.m1.5.6.2.3" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml">𝒜</mi><mo id="S3.SS1.p2.1.m1.5.6.2.4"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.4.4"
    xref="S3.SS1.p2.1.m1.4.4.cmml">𝒯</mi><mo id="S3.SS1.p2.1.m1.5.6.2.5" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.5.5" xref="S3.SS1.p2.1.m1.5.5.cmml">ℛ</mi><mo
    stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.6" xref="S3.SS1.p2.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.1.m1.5b"><vector id="S3.SS1.p2.1.m1.5.6.1.cmml"
    xref="S3.SS1.p2.1.m1.5.6.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝒮</ci><ci
    id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">𝒩</ci><ci id="S3.SS1.p2.1.m1.3.3.cmml"
    xref="S3.SS1.p2.1.m1.3.3">𝒜</ci><ci id="S3.SS1.p2.1.m1.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4">𝒯</ci><ci
    id="S3.SS1.p2.1.m1.5.5.cmml" xref="S3.SS1.p2.1.m1.5.5">ℛ</ci></vector></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.1.m1.5c">(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})</annotation></semantics></math>,
    which can be seen as an extension of an MDP to multiple agents [[124](#bib.bib124),
    [125](#bib.bib125)]. One key distinction is that the transition, <math id="S3.SS1.p2.2.m2.1"
    class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝒯</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml"
    xref="S3.SS1.p2.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p2.2.m2.1c">\mathcal{T}</annotation></semantics></math>, and reward
    function, <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics
    id="S3.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1"
    xref="S3.SS1.p2.3.m3.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ℛ</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{R}</annotation></semantics></math>,
    depend on the actions <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1"
    xref="S3.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.2"
    xref="S3.SS1.p2.4.m4.1.1.2.cmml">𝒜</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><msub id="S3.SS1.p2.4.m4.1.1.3.2"
    xref="S3.SS1.p2.4.m4.1.1.3.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2.2" xref="S3.SS1.p2.4.m4.1.1.3.2.2.cmml">A</mi><mn
    id="S3.SS1.p2.4.m4.1.1.3.2.3" xref="S3.SS1.p2.4.m4.1.1.3.2.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.1" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">×</mo><mi
    mathvariant="normal" id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.4.m4.1.1.3.4" xref="S3.SS1.p2.4.m4.1.1.3.4.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.4.2"
    xref="S3.SS1.p2.4.m4.1.1.3.4.2.cmml">A</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.4.m4.1.1.3.4.3" xref="S3.SS1.p2.4.m4.1.1.3.4.3.cmml">𝒩</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml"
    xref="S3.SS1.p2.4.m4.1.1"><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝒜</ci><apply
    id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><apply id="S3.SS1.p2.4.m4.1.1.3.2.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.2.1.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.2.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2.2">𝐴</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.2.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p2.4.m4.1.1.3.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.3">…</ci><apply id="S3.SS1.p2.4.m4.1.1.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4"><csymbol
    cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4">subscript</csymbol><ci
    id="S3.SS1.p2.4.m4.1.1.3.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4.2">𝐴</ci><ci id="S3.SS1.p2.4.m4.1.1.3.4.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.4.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}</annotation></semantics></math> of all, <math id="S3.SS1.p2.5.m5.1"
    class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">𝒩</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml"
    xref="S3.SS1.p2.5.m5.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p2.5.m5.1c">\mathcal{N}</annotation></semantics></math>, agents, this
    means, <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{R}=R_{1}\times...\times
    R_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1"
    xref="S3.SS1.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.2"
    xref="S3.SS1.p2.6.m6.1.1.2.cmml">ℛ</mi><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml"><msub id="S3.SS1.p2.6.m6.1.1.3.2"
    xref="S3.SS1.p2.6.m6.1.1.3.2.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.2.2" xref="S3.SS1.p2.6.m6.1.1.3.2.2.cmml">R</mi><mn
    id="S3.SS1.p2.6.m6.1.1.3.2.3" xref="S3.SS1.p2.6.m6.1.1.3.2.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.1" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">×</mo><mi
    mathvariant="normal" id="S3.SS1.p2.6.m6.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.1a" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.6.m6.1.1.3.4" xref="S3.SS1.p2.6.m6.1.1.3.4.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.4.2"
    xref="S3.SS1.p2.6.m6.1.1.3.4.2.cmml">R</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.6.m6.1.1.3.4.3" xref="S3.SS1.p2.6.m6.1.1.3.4.3.cmml">𝒩</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml"
    xref="S3.SS1.p2.6.m6.1.1"><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ℛ</ci><apply
    id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"><apply id="S3.SS1.p2.6.m6.1.1.3.2.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.2.1.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.2.2.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2.2">𝑅</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.2.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p2.6.m6.1.1.3.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.3">…</ci><apply id="S3.SS1.p2.6.m6.1.1.3.4.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4"><csymbol
    cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.4.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4">subscript</csymbol><ci
    id="S3.SS1.p2.6.m6.1.1.3.4.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4.2">𝑅</ci><ci id="S3.SS1.p2.6.m6.1.1.3.4.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.4.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{R}=R_{1}\times...\times
    R_{\mathcal{N}}</annotation></semantics></math> and <math id="S3.SS1.p2.7.m7.1"
    class="ltx_Math" alttext="\mathcal{T}=\mathcal{S}\times A_{1}\times...\times A_{\mathcal{N}}"
    display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1"
    xref="S3.SS1.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1.2"
    xref="S3.SS1.p2.7.m7.1.1.2.cmml">𝒯</mi><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">𝒮</mi><mo lspace="0.222em"
    rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.3.2"
    xref="S3.SS1.p2.7.m7.1.1.3.3.2.cmml">A</mi><mn id="S3.SS1.p2.7.m7.1.1.3.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1a" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">×</mo><mi
    mathvariant="normal" id="S3.SS1.p2.7.m7.1.1.3.4" xref="S3.SS1.p2.7.m7.1.1.3.4.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1b" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">×</mo><msub
    id="S3.SS1.p2.7.m7.1.1.3.5" xref="S3.SS1.p2.7.m7.1.1.3.5.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.5.2"
    xref="S3.SS1.p2.7.m7.1.1.3.5.2.cmml">A</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.7.m7.1.1.3.5.3" xref="S3.SS1.p2.7.m7.1.1.3.5.3.cmml">𝒩</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml"
    xref="S3.SS1.p2.7.m7.1.1"><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝒯</ci><apply
    id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.2">𝒮</ci><apply id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3"><csymbol
    cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">subscript</csymbol><ci
    id="S3.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2">𝐴</ci><cn type="integer"
    id="S3.SS1.p2.7.m7.1.1.3.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.3">1</cn></apply><ci
    id="S3.SS1.p2.7.m7.1.1.3.4.cmml" xref="S3.SS1.p2.7.m7.1.1.3.4">…</ci><apply id="S3.SS1.p2.7.m7.1.1.3.5.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.5.1.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.5.2.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5.2">𝐴</ci><ci id="S3.SS1.p2.7.m7.1.1.3.5.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.5.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathcal{T}=\mathcal{S}\times
    A_{1}\times...\times A_{\mathcal{N}}</annotation></semantics></math>.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么多智能体领域从代理的本地视角来看是非静态的，考虑一个简单的随机（也称为马尔可夫）博弈<math id="S3.SS1.p2.1.m1.5"
    class="ltx_Math" alttext="(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})"
    display="inline"><semantics id="S3.SS1.p2.1.m1.5a"><mrow id="S3.SS1.p2.1.m1.5.6.2"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.1"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1"
    xref="S3.SS1.p2.1.m1.1.1.cmml">𝒮</mi><mo id="S3.SS1.p2.1.m1.5.6.2.2" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">𝒩</mi><mo
    id="S3.SS1.p2.1.m1.5.6.2.3" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml">𝒜</mi><mo id="S3.SS1.p2.1.m1.5.6.2.4"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.4.4"
    xref="S3.SS1.p2.1.m1.4.4.cmml">𝒯</mi><mo id="S3.SS1.p2.1.m1.5.6.2.5" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.5.5" xref="S3.SS1.p2.1.m1.5.5.cmml">ℛ</mi><mo
    stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.6" xref="S3.SS1.p2.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.1.m1.5b"><vector id="S3.SS1.p2.1.m1.5.6.1.cmml"
    xref="S3.SS1.p2.1.m1.5.6.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝒮</ci><ci
    id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">𝒩</ci><ci id="S3.SS1.p2.1.m1.3.3.cmml"
    xref="S3.SS1.p2.1.m1.3.3">𝒜</ci><ci id="S3.SS1.p2.1.m1.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4">𝒯</ci><ci
    id="S3.SS1.p2.1.m1.5.5.cmml" xref="S3.SS1.p2.1.m1.5.5">ℛ</ci></vector></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.1.m1.5c">(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})</annotation></semantics></math>，可以视为多智能体MDP的扩展[[124](#bib.bib124),
    [125](#bib.bib125)]。其中一个关键区别在于转移<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{T}"
    display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝒯</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathcal{T}</annotation></semantics></math>
    和奖励函数<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics
    id="S3.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1"
    xref="S3.SS1.p2.3.m3.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ℛ</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{R}</annotation></semantics></math>
    依赖于行动<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1"
    xref="S3.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.2"
    xref="S3.SS1.p2.4.m4.1.1.2.cmml">𝒜</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><msub id="S3.SS1.p2.4.m4.1.1.3.2"
    xref="S3.SS1.p2.4.m4.1.1.3.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2.2" xref="S3.SS1.p2.4.m4.1.1.3.2.2.cmml">A</mi><mn
    id="S3.SS1.p2.4.m4.1.1.3.2.3" xref="S
- en: Given a learning agent <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="i"
    display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1"
    xref="S3.SS1.p3.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">i</annotation></semantics></math>
    and using the common shorthand notation <math id="S3.SS1.p3.2.m2.1" class="ltx_Math"
    alttext="\bm{-i}=\mathcal{N}\setminus\{i\}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow
    id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mrow id="S3.SS1.p3.2.m2.1.2.2"
    xref="S3.SS1.p3.2.m2.1.2.2.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.SS1.p3.2.m2.1.2.2a" xref="S3.SS1.p3.2.m2.1.2.2.cmml">−</mo><mi id="S3.SS1.p3.2.m2.1.2.2.2"
    xref="S3.SS1.p3.2.m2.1.2.2.2.cmml">𝒊</mi></mrow><mo id="S3.SS1.p3.2.m2.1.2.1"
    xref="S3.SS1.p3.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS1.p3.2.m2.1.2.3" xref="S3.SS1.p3.2.m2.1.2.3.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.2.3.2" xref="S3.SS1.p3.2.m2.1.2.3.2.cmml">𝒩</mi><mo
    id="S3.SS1.p3.2.m2.1.2.3.1" xref="S3.SS1.p3.2.m2.1.2.3.1.cmml">∖</mo><mrow id="S3.SS1.p3.2.m2.1.2.3.3.2"
    xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.2.3.3.2.1"
    xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">{</mo><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">i</mi><mo
    stretchy="false" id="S3.SS1.p3.2.m2.1.2.3.3.2.2" xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2"><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2"><ci
    id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2">𝒊</ci></apply><apply
    id="S3.SS1.p3.2.m2.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.2.3"><ci id="S3.SS1.p3.2.m2.1.2.3.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2.3.2">𝒩</ci><set id="S3.SS1.p3.2.m2.1.2.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.2.3.3.2"><ci
    id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑖</ci></set></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\bm{-i}=\mathcal{N}\setminus\{i\}</annotation></semantics></math>
    for the set of opponents, the value function now depends on the joint action <math
    id="S3.SS1.p3.3.m3.2" class="ltx_Math" alttext="\bm{a}=(a_{i},\bm{a_{-i}})" display="inline"><semantics
    id="S3.SS1.p3.3.m3.2a"><mrow id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.4" xref="S3.SS1.p3.3.m3.2.2.4.cmml">𝒂</mi><mo id="S3.SS1.p3.3.m3.2.2.3"
    xref="S3.SS1.p3.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml"><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">(</mo><msub
    id="S3.SS1.p3.3.m3.1.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.2"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.3"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.3.m3.2.2.2.2.4"
    xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.3.m3.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml">𝒂</mi><mrow
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p3.3.m3.2.2.2.2.2.3a" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml">−</mo><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.5" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.3.m3.2b"><apply id="S3.SS1.p3.3.m3.2.2.cmml"
    xref="S3.SS1.p3.3.m3.2.2"><ci id="S3.SS1.p3.3.m3.2.2.4.cmml" xref="S3.SS1.p3.3.m3.2.2.4">𝒂</ci><interval
    closure="open" id="S3.SS1.p3.3.m3.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2"><apply
    id="S3.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1">subscript</csymbol><ci
    id="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.SS1.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2"><csymbol cd="ambiguous"
    id="S3.SS1.p3.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2">subscript</csymbol><ci
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2">𝒂</ci><apply
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3"><ci id="S3.SS1.p3.3.m3.2.2.2.2.2.3.2.cmml"
    xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.2">𝒊</ci></apply></apply></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.3.m3.2c">\bm{a}=(a_{i},\bm{a_{-i}})</annotation></semantics></math>,
    and the joint policy <math id="S3.SS1.p3.4.m4.4" class="ltx_Math" alttext="\bm{\pi}(s,\bm{a})=\prod_{j}\pi_{j}(s,a_{j})"
    display="inline"><semantics id="S3.SS1.p3.4.m4.4a"><mrow id="S3.SS1.p3.4.m4.4.4"
    xref="S3.SS1.p3.4.m4.4.4.cmml"><mrow id="S3.SS1.p3.4.m4.4.4.3" xref="S3.SS1.p3.4.m4.4.4.3.cmml"><mi
    id="S3.SS1.p3.4.m4.4.4.3.2" xref="S3.SS1.p3.4.m4.4.4.3.2.cmml">𝝅</mi><mo lspace="0em"
    rspace="0em" id="S3.SS1.p3.4.m4.4.4.3.1" xref="S3.SS1.p3.4.m4.4.4.3.1.cmml">​</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.3.3.2" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml"><mo stretchy="false"
    id="S3.SS1.p3.4.m4.4.4.3.3.2.1" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">(</mo><mi
    id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">s</mi><mo id="S3.SS1.p3.4.m4.4.4.3.3.2.2"
    xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">,</mo><mi id="S3.SS1.p3.4.m4.2.2" xref="S3.SS1.p3.4.m4.2.2.cmml">𝒂</mi><mo
    stretchy="false" id="S3.SS1.p3.4.m4.4.4.3.3.2.3" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo
    rspace="0.111em" id="S3.SS1.p3.4.m4.4.4.2" xref="S3.SS1.p3.4.m4.4.4.2.cmml">=</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.1" xref="S3.SS1.p3.4.m4.4.4.1.cmml"><msub id="S3.SS1.p3.4.m4.4.4.1.2"
    xref="S3.SS1.p3.4.m4.4.4.1.2.cmml"><mo id="S3.SS1.p3.4.m4.4.4.1.2.2" xref="S3.SS1.p3.4.m4.4.4.1.2.2.cmml">∏</mo><mi
    id="S3.SS1.p3.4.m4.4.4.1.2.3" xref="S3.SS1.p3.4.m4.4.4.1.2.3.cmml">j</mi></msub><mrow
    id="S3.SS1.p3.4.m4.4.4.1.1" xref="S3.SS1.p3.4.m4.4.4.1.1.cmml"><msub id="S3.SS1.p3.4.m4.4.4.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.4.4.1.1.3.2" xref="S3.SS1.p3.4.m4.4.4.1.1.3.2.cmml">π</mi><mi
    id="S3.SS1.p3.4.m4.4.4.1.1.3.3" xref="S3.SS1.p3.4.m4.4.4.1.1.3.3.cmml">j</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.4.4.1.1.2" xref="S3.SS1.p3.4.m4.4.4.1.1.2.cmml">​</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1" xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml"><mo stretchy="false"
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.2" xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">(</mo><mi
    id="S3.SS1.p3.4.m4.3.3" xref="S3.SS1.p3.4.m4.3.3.cmml">s</mi><mo id="S3.SS1.p3.4.m4.4.4.1.1.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS1.p3.4.m4.4.4.1.1.1.1.4"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.4.m4.4b"><apply id="S3.SS1.p3.4.m4.4.4.cmml"
    xref="S3.SS1.p3.4.m4.4.4"><apply id="S3.SS1.p3.4.m4.4.4.3.cmml" xref="S3.SS1.p3.4.m4.4.4.3"><ci
    id="S3.SS1.p3.4.m4.4.4.3.2.cmml" xref="S3.SS1.p3.4.m4.4.4.3.2">𝝅</ci><interval
    closure="open" id="S3.SS1.p3.4.m4.4.4.3.3.1.cmml" xref="S3.SS1.p3.4.m4.4.4.3.3.2"><ci
    id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑠</ci><ci id="S3.SS1.p3.4.m4.2.2.cmml"
    xref="S3.SS1.p3.4.m4.2.2">𝒂</ci></interval></apply><apply id="S3.SS1.p3.4.m4.4.4.1.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1"><apply id="S3.SS1.p3.4.m4.4.4.1.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2"><csymbol
    cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.2.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2">subscript</csymbol><csymbol
    cd="latexml" id="S3.SS1.p3.4.m4.4.4.1.2.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2.2">product</csymbol><ci
    id="S3.SS1.p3.4.m4.4.4.1.2.3.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2.3">𝑗</ci></apply><apply
    id="S3.SS1.p3.4.m4.4.4.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1"><apply id="S3.SS1.p3.4.m4.4.4.1.1.3.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.1.3.1.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.4.m4.4.4.1.1.3.2.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.2">𝜋</ci><ci id="S3.SS1.p3.4.m4.4.4.1.1.3.3.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.3">𝑗</ci></apply><interval closure="open" id="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1"><ci id="S3.SS1.p3.4.m4.3.3.cmml" xref="S3.SS1.p3.4.m4.3.3">𝑠</ci><apply
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1">subscript</csymbol><ci
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3">𝑗</ci></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.4.m4.4c">\bm{\pi}(s,\bm{a})=\prod_{j}\pi_{j}(s,a_{j})</annotation></semantics></math>:⁷⁷7In
    this setting each agent independently executes a policy, however, there are other
    cases where this does not hold, for example when agents have a coordinated exploration
    strategy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个学习代理<math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics
    id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">i</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml"
    xref="S3.SS1.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p3.1.m1.1c">i</annotation></semantics></math>，并使用常见的简写符号<math id="S3.SS1.p3.2.m2.1"
    class="ltx_Math" alttext="\bm{-i}=\mathcal{N}\setminus\{i\}" display="inline"><semantics
    id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mrow
    id="S3.SS1.p3.2.m2.1.2.2" xref="S3.SS1.p3.2.m2.1.2.2.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p3.2.m2.1.2.2a" xref="S3.SS1.p3.2.m2.1.2.2.cmml">−</mo><mi
    id="S3.SS1.p3.2.m2.1.2.2.2" xref="S3.SS1.p3.2.m2.1.2.2.2.cmml">𝒊</mi></mrow><mo
    id="S3.SS1.p3.2.m2.1.2.1" xref="S3.SS1.p3.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS1.p3.2.m2.1.2.3"
    xref="S3.SS1.p3.2.m2.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.2.3.2"
    xref="S3.SS1.p3.2.m2.1.2.3.2.cmml">𝒩</mi><mo id="S3.SS1.p3.2.m2.1.2.3.1" xref="S3.SS1.p3.2.m2.1.2.3.1.cmml">∖</mo><mrow
    id="S3.SS1.p3.2.m2.1.2.3.3.2" xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml"><mo stretchy="false"
    id="S3.SS1.p3.2.m2.1.2.3.3.2.1" xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">{</mo><mi
    id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">i</mi><mo stretchy="false"
    id="S3.SS1.p3.2.m2.1.2.3.3.2.2" xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2"><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2"><ci
    id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2">𝒊</ci></apply><apply
    id="S3.SS1.p3.2.m2.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.2.3"><ci id="S3.SS1.p3.2.m2.1.2.3.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2.3.2">𝒩</ci><set id="S3.SS1.p3.2.m2.1.2.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.2.3.3.2"><ci
    id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑖</ci></set></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\bm{-i}=\mathcal{N}\setminus\{i\}</annotation></semantics></math>，对于对手的集合，现在值函数依赖于联合行动<math
    id="S3.SS1.p3.3.m3.2" class="ltx_Math" alttext="\bm{a}=(a_{i},\bm{a_{-i}})" display="inline"><semantics
    id="S3.SS1.p3.3.m3.2a"><mrow id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.4" xref="S3.SS1.p3.3.m3.2.2.4.cmml">𝒂</mi><mo id="S3.SS1.p3.3.m3.2.2.3"
    xref="S3.SS1.p3.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml"><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">(</mo><msub
    id="S3.SS1.p3.3.m3.1.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.2"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.3"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.3.m3.2.2.2.2.4"
    xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.3.m3.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml">𝒂</mi><mrow
    id="S3.SS1.p3.3.m3
- en: '|  | <math id="S3.E4.m1.6" class="ltx_Math" alttext="V^{\bm{\pi}}_{i}(s)=\sum_{\bm{a}\in\mathcal{A}}\bm{\pi}(s,\bm{a})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V_{i}(s^{\prime})]." display="block"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6.1"
    xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow
    id="S3.E4.m1.6.6.1.1.6" xref="S3.E4.m1.6.6.1.1.6.cmml"><msubsup id="S3.E4.m1.6.6.1.1.6.2"
    xref="S3.E4.m1.6.6.1.1.6.2.cmml"><mi id="S3.E4.m1.6.6.1.1.6.2.2.2" xref="S3.E4.m1.6.6.1.1.6.2.2.2.cmml">V</mi><mi
    id="S3.E4.m1.6.6.1.1.6.2.3" xref="S3.E4.m1.6.6.1.1.6.2.3.cmml">i</mi><mi id="S3.E4.m1.6.6.1.1.6.2.2.3"
    xref="S3.E4.m1.6.6.1.1.6.2.2.3.cmml">𝝅</mi></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E4.m1.6.6.1.1.6.1" xref="S3.E4.m1.6.6.1.1.6.1.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.6.3.2"
    xref="S3.E4.m1.6.6.1.1.6.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.6.3.2.1"
    xref="S3.E4.m1.6.6.1.1.6.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">s</mi><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.6.3.2.2" xref="S3.E4.m1.6.6.1.1.6.cmml">)</mo></mrow></mrow><mo
    rspace="0.111em" id="S3.E4.m1.6.6.1.1.5" xref="S3.E4.m1.6.6.1.1.5.cmml">=</mo><mrow
    id="S3.E4.m1.6.6.1.1.4" xref="S3.E4.m1.6.6.1.1.4.cmml"><munder id="S3.E4.m1.6.6.1.1.4.5"
    xref="S3.E4.m1.6.6.1.1.4.5.cmml"><mo movablelimits="false" id="S3.E4.m1.6.6.1.1.4.5.2"
    xref="S3.E4.m1.6.6.1.1.4.5.2.cmml">∑</mo><mrow id="S3.E4.m1.6.6.1.1.4.5.3" xref="S3.E4.m1.6.6.1.1.4.5.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.5.3.2" xref="S3.E4.m1.6.6.1.1.4.5.3.2.cmml">𝒂</mi><mo id="S3.E4.m1.6.6.1.1.4.5.3.1"
    xref="S3.E4.m1.6.6.1.1.4.5.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.5.3.3" xref="S3.E4.m1.6.6.1.1.4.5.3.3.cmml">𝒜</mi></mrow></munder><mrow
    id="S3.E4.m1.6.6.1.1.4.4" xref="S3.E4.m1.6.6.1.1.4.4.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.6"
    xref="S3.E4.m1.6.6.1.1.4.4.6.cmml">𝝅</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.5"
    xref="S3.E4.m1.6.6.1.1.4.4.5.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.4.4.7.2" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.7.2.1" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">(</mo><mi
    id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.7.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">,</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">𝒂</mi><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.7.2.3" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.5a" xref="S3.E4.m1.6.6.1.1.4.4.5.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.cmml"><munder id="S3.E4.m1.6.6.1.1.4.4.4.5"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.cmml"><mo movablelimits="false" id="S3.E4.m1.6.6.1.1.4.4.4.5.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.2.cmml">∑</mo><mrow id="S3.E4.m1.6.6.1.1.4.4.4.5.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.cmml"><msup id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3.cmml">′</mo></msup><mo id="S3.E4.m1.6.6.1.1.4.4.4.5.3.1"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.3.cmml">𝒮</mi></mrow></munder><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.4.4.4.6" xref="S3.E4.m1.6.6.1.1.4.4.4.4.6.cmml">𝒯</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.5.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.4" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">(</mo><mi
    id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.5"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.6"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2.cmml">𝒂</mi><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3a" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.7" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msup
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.8" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.5a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.5.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml">[</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.cmml"><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.cmml"><msub
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2.cmml">R</mi><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.4.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">(</mo><mi
    id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.5"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.6"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2.cmml">𝒂</mi><mrow id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.7" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msup
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.8" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.5.cmml">+</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3.cmml">γ</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2.cmml">​</mo><msub
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2.cmml">V</mi><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2.cmml">​</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml">(</mo><msup
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    lspace="0em" id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1"><apply
    id="S3.E4.m1.6.6.1.1.6.cmml" xref="S3.E4.m1.6.6.1.1.6"><apply id="S3.E4.m1.6.6.1.1.6.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.6.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2">subscript</csymbol><apply id="S3.E4.m1.6.6.1.1.6.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.6.2.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2">superscript</csymbol><ci id="S3.E4.m1.6.6.1.1.6.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2.2.2">𝑉</ci><ci id="S3.E4.m1.6.6.1.1.6.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.6.2.2.3">𝝅</ci></apply><ci
    id="S3.E4.m1.6.6.1.1.6.2.3.cmml" xref="S3.E4.m1.6.6.1.1.6.2.3">𝑖</ci></apply><ci
    id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑠</ci></apply><apply id="S3.E4.m1.6.6.1.1.4.cmml"
    xref="S3.E4.m1.6.6.1.1.4"><apply id="S3.E4.m1.6.6.1.1.4.5.cmml" xref="S3.E4.m1.6.6.1.1.4.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.5">subscript</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.5.3"><ci id="S3.E4.m1.6.6.1.1.4.5.3.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.5.3.2">𝒂</ci><ci id="S3.E4.m1.6.6.1.1.4.5.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.5.3.3">𝒜</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4"><ci id="S3.E4.m1.6.6.1.1.4.4.6.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.6">𝝅</ci><interval closure="open" id="S3.E4.m1.6.6.1.1.4.4.7.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.7.2"><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑠</ci><ci
    id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">𝒂</ci></interval><apply id="S3.E4.m1.6.6.1.1.4.4.4.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4"><apply id="S3.E4.m1.6.6.1.1.4.4.4.5.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5">subscript</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3">′</ci></apply><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.3">𝒮</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4"><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.6.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.6">𝒯</ci><vector id="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3"><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">𝑠</ci><apply
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2">𝒂</ci><apply
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3"><ci
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3">′</ci></apply></vector><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1"><csymbol
    cd="latexml" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.2">delimited-[]</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2">𝑅</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3">𝑖</ci></apply><vector
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3"><ci
    id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">𝑠</ci><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2">𝒂</ci><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3"><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3">′</ci></apply></vector></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4"><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3">𝛾</ci><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2">𝑉</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3">𝑖</ci></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2">𝑠</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3">′</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E4.m1.6c">V^{\bm{\pi}}_{i}(s)=\sum_{\bm{a}\in\mathcal{A}}\bm{\pi}(s,\bm{a})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V_{i}(s^{\prime})].</annotation></semantics></math> |  | (4) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id'
- en: Consequently, the optimal policy is dependent on the other agents’ policies,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**最优**策略依赖于其他参与者的策略，
- en: '|  |  | <math id="S3.E5X.2.1.1.m1.6" class="ltx_Math" alttext="\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)="
    display="inline"><semantics id="S3.E5X.2.1.1.m1.6a"><mrow id="S3.E5X.2.1.1.m1.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.2" xref="S3.E5X.2.1.1.m1.6.6.2.cmml"><msubsup
    id="S3.E5X.2.1.1.m1.6.6.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.4.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml">i</mi><mo id="S3.E5X.2.1.1.m1.6.6.2.4.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.3.cmml">∗</mo></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.2.3" xref="S3.E5X.2.1.1.m1.6.6.2.3.cmml">​</mo><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml"><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.3.3" xref="S3.E5X.2.1.1.m1.3.3.cmml">s</mi><mo
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub
    id="S3.E5X.2.1.1.m1.5.5.1.1.1.1" xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml">a</mi><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.6.6.2.2.2.5"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.6.6.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml">𝝅</mi><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.6" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow><mo
    id="S3.E5X.2.1.1.m1.6.6.4" xref="S3.E5X.2.1.1.m1.6.6.4.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.6.6.5"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml"><munder
    id="S3.E5X.2.1.1.m1.6.6.5.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2.1.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1.cmml">​</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3.cmml">max</mi></mrow><msub
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3.cmml">i</mi></msub></munder><mo id="S3.E5X.2.1.1.m1.6.6.5.2a"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml">⁡</mo><msubsup id="S3.E5X.2.1.1.m1.6.6.5.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2.cmml">V</mi><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.2.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.3.cmml">i</mi><mrow
    id="S3.E5X.2.1.1.m1.2.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml"><mo stretchy="false"
    id="S3.E5X.2.1.1.m1.2.2.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">(</mo><msub
    id="S3.E5X.2.1.1.m1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.2.2.2.2.4"
    xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.2.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.2.cmml">𝝅</mi><mrow
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5X.2.1.1.m1.2.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.2.2.2.2.5" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.6.6.5.1" xref="S3.E5X.2.1.1.m1.6.6.5.1.cmml">​</mo><mrow
    id="S3.E5X.2.1.1.m1.6.6.5.3.2" xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mo stretchy="false"
    id="S3.E5X.2.1.1.m1.6.6.5.3.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.4.4"
    xref="S3.E5X.2.1.1.m1.4.4.cmml">s</mi><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.5.3.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml">)</mo></mrow></mrow><mo id="S3.E5X.2.1.1.m1.6.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.6.cmml">=</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.E5X.2.1.1.m1.6b"><apply id="S3.E5X.2.1.1.m1.6.6.cmml" xref="S3.E5X.2.1.1.m1.6.6"><apply
    id="S3.E5X.2.1.1.m1.6.6b.cmml" xref="S3.E5X.2.1.1.m1.6.6"><apply id="S3.E5X.2.1.1.m1.6.6.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2"><apply id="S3.E5X.2.1.1.m1.6.6.2.4.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.2.4.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4">superscript</csymbol><apply
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4"><csymbol cd="ambiguous"
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2">𝜋</ci><ci
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3">𝑖</ci></apply></apply><vector
    id="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.2.2"><ci id="S3.E5X.2.1.1.m1.3.3.cmml"
    xref="S3.E5X.2.1.1.m1.3.3">𝑠</ci><apply id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.1.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2">𝑎</ci><ci id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2">𝝅</ci><apply id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"><ci id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2">𝒊</ci></apply></apply></vector></apply><apply
    id="S3.E5X.2.1.1.m1.6.6.5.cmml" xref="S3.E5X.2.1.1.m1.6.6.5"><apply id="S3.E5X.2.1.1.m1.6.6.5.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2"><apply id="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.1.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1">subscript</csymbol><apply
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2"><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2">arg</ci><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3">max</ci></apply><apply id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2">𝜋</ci><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3">𝑖</ci></apply></apply><apply id="S3.E5X.2.1.1.m1.6.6.5.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2">subscript</csymbol><apply id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2">superscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2">𝑉</ci><interval closure="open" id="S3.E5X.2.1.1.m1.2.2.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.2.2.2.2"><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.2">𝜋</ci><ci
    id="S3.E5X.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.2">𝝅</ci><apply
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3"><ci id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2">𝒊</ci></apply></apply></interval></apply><ci
    id="S3.E5X.2.1.1.m1.6.6.5.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.3">𝑖</ci></apply></apply><ci
    id="S3.E5X.2.1.1.m1.4.4.cmml" xref="S3.E5X.2.1.1.m1.4.4">𝑠</ci></apply></apply><apply
    id="S3.E5X.2.1.1.m1.6.6c.cmml" xref="S3.E5X.2.1.1.m1.6.6"><csymbol cd="latexml"
    id="S3.E5X.2.1.1.m1.6.6.7.cmml" xref="S3.E5X.2.1.1.m1.6.6.7">absent</csymbol></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E5X.2.1.1.m1.6c">\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)=</annotation></semantics></math>
    |  | (5) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | <math id="S3.E5X.2.1.1.m1.6" class="ltx_Math" alttext="\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)="
    display="inline"><semantics id="S3.E5X.2.1.1.m1.6a"><mrow id="S3.E5X.2.1.1.m1.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.2" xref="S3.E5X.2.1.1.m1.6.6.2.cmml"><msubsup
    id="S3.E5X.2.1.1.m1.6.6.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.4.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml">π</mi><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml">i</mi><mo id="S3.E5X.2.1.1.m1.6.6.2.4.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.3.cmml">∗</mo></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.2.3" xref="S3.E5X.2.1.1.m1.6.6.2.3.cmml">​</mo><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml"><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.3.3" xref="S3.E5X.2.1.1.m1.3.3.cmml">s</mi><mo
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub
    id="S3.E5X.2.1.1.m1.5.5.1.1.1.1" xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml">a</mi><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.6.6.2.2.2.5"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.6.6.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml">𝝅</mi><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.6" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow><mo
    id="S3.E5X.2.1.1.m1.6.6.4" xref="S3.E5X.2.1.1.m1.6.6.4.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.6.6.5"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml"><munder
    id="S3.E5X.2.1.1.m1.6.6.5.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2.1.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1.cmml">​</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml">max</mi></mrow><msub
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2"
    xref="S3.E5X.2.1.1'
- en: '|  |  | <math id="S3.E5Xa.2.1.1.m1.7" class="ltx_Math" alttext="\displaystyle\operatorname*{arg\,max}_{\pi_{i}}\sum_{\bm{a}\in\mathcal{A}}\pi_{i}(s,a_{i})\bm{\pi_{-i}}(s,\bm{a_{-i}})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s^{\prime})]." display="inline"><semantics id="S3.E5Xa.2.1.1.m1.7a"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"><munder id="S3.E5Xa.2.1.1.m1.7.7.1.1.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.cmml"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2.cmml">arg</mi><mo
    lspace="0.170em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.1.cmml">​</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3.cmml">max</mi></mrow><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2.cmml">π</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3.cmml">i</mi></msub></munder><mo
    lspace="0.167em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.cmml"><mstyle
    displaystyle="true" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml"><munder
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml"><mo
    movablelimits="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.2.cmml">∑</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2.cmml">𝒂</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.1.cmml">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3.cmml">𝒜</mi></mrow></munder></mstyle><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.cmml"><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2.cmml">π</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.3.3" xref="S3.E5Xa.2.1.1.m1.3.3.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2.cmml">𝝅</mi><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3a"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml">−</mo><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2.cmml">𝒊</mi></mrow></msub><mo lspace="0em"
    rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7b" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.4.4" xref="S3.E5Xa.2.1.1.m1.4.4.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2.cmml">𝒂</mi><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml">−</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7c" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.cmml"><mstyle
    displaystyle="true" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml"><munder
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml"><mo
    movablelimits="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.2.cmml">∑</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.cmml"><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3.cmml">′</mo></msup><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.1.cmml">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3.cmml">𝒮</mi></mrow></munder></mstyle><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6.cmml">𝒯</mi><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.5.5" xref="S3.E5Xa.2.1.1.m1.5.5.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.5"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.6"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2.cmml">𝒂</mi><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml">−</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml">[</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.cmml"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.cmml"><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2.cmml">R</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.4.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.6.6" xref="S3.E5Xa.2.1.1.m1.6.6.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.5"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2.cmml">𝒂</mi><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3a"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml">−</mo><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.5.cmml">+</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3.cmml">γ</mi><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2.cmml">​</mo><msubsup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2.cmml">V</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3.cmml">i</mi><mrow
    id="S3.E5Xa.2.1.1.m1.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml"><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">(</mo><msub
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2.cmml">π</mi><mi id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5Xa.2.1.1.m1.2.2.2.2.4"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.2.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2.cmml">𝝅</mi><mrow
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3a"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml">−</mo><mi id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml">𝒊</mi></mrow></msub><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.5" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2.cmml">​</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml">(</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3.cmml">′</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    lspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E5Xa.2.1.1.m1.7b"><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1"><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2">arg</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3">max</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2">𝜋</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3">𝑖</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2">𝒂</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3">𝒜</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2">𝜋</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3">𝑖</ci></apply><interval
    closure="open" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1"><ci
    id="S3.E5Xa.2.1.1.m1.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.3.3">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3">𝑖</ci></apply></interval><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2">𝝅</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2">𝒊</ci></apply></apply><interval closure="open"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1"><ci
    id="S3.E5Xa.2.1.1.m1.4.4.cmml" xref="S3.E5Xa.2.1.1.m1.4.4">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2">𝒂</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2">𝒊</ci></apply></apply></interval><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3">′</ci></apply><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3">𝒮</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6">𝒯</ci><vector
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3"><ci
    id="S3.E5Xa.2.1.1.m1.5.5.cmml" xref="S3.E5Xa.2.1.1.m1.5.5">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2">𝑎</ci><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2">𝒂</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3">′</ci></apply></vector><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1"><csymbol
    cd="latexml" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.2">delimited-[]</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2">𝑅</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3">𝑖</ci></apply><vector
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3"><ci
    id="S3.E5Xa.2.1.1.m1.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.6.6">𝑠</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2">𝑎</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2">𝒂</ci><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2">𝒊</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3">′</ci></apply></vector></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3">𝛾</ci><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2">𝑉</ci><interval
    closure="open" id="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2"><apply
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2">𝜋</ci><ci
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2">𝝅</ci><apply
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3"><ci
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2">𝒊</ci></apply></apply></interval></apply><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3">𝑖</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2">𝑠</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3">′</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E5Xa.2.1.1.m1.7c">\displaystyle\operatorname*{arg\,max}_{\pi_{i}}\sum_{\bm{a}\in\mathcal{A}}\pi_{i}(s,a_{i})\bm{\pi_{-i}}(s,\bm{a_{-i}})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s^{\prime})].</annotation></semantics></math>
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: Specifically, the opponents’ joint policy <math id="S3.SS1.p4.1.m1.2" class="ltx_Math"
    alttext="\bm{\pi_{-i}}(s,\bm{a_{-i}})" display="inline"><semantics id="S3.SS1.p4.1.m1.2a"><mrow
    id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml"><msub id="S3.SS1.p4.1.m1.2.2.3"
    xref="S3.SS1.p4.1.m1.2.2.3.cmml"><mi id="S3.SS1.p4.1.m1.2.2.3.2" xref="S3.SS1.p4.1.m1.2.2.3.2.cmml">𝝅</mi><mrow
    id="S3.SS1.p4.1.m1.2.2.3.3" xref="S3.SS1.p4.1.m1.2.2.3.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p4.1.m1.2.2.3.3a" xref="S3.SS1.p4.1.m1.2.2.3.3.cmml">−</mo><mi
    id="S3.SS1.p4.1.m1.2.2.3.3.2" xref="S3.SS1.p4.1.m1.2.2.3.3.2.cmml">𝒊</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S3.SS1.p4.1.m1.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.cmml">​</mo><mrow
    id="S3.SS1.p4.1.m1.2.2.1.1" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml"><mo stretchy="false"
    id="S3.SS1.p4.1.m1.2.2.1.1.2" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p4.1.m1.1.1"
    xref="S3.SS1.p4.1.m1.1.1.cmml">s</mi><mo id="S3.SS1.p4.1.m1.2.2.1.1.3" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">,</mo><msub
    id="S3.SS1.p4.1.m1.2.2.1.1.1" xref="S3.SS1.p4.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.2.2.1.1.1.2"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS1.p4.1.m1.2.2.1.1.1.3"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.SS1.p4.1.m1.2.2.1.1.1.3a" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml">−</mo><mi
    id="S3.SS1.p4.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.2.cmml">𝒊</mi></mrow></msub><mo
    stretchy="false" id="S3.SS1.p4.1.m1.2.2.1.1.4" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><apply id="S3.SS1.p4.1.m1.2.2.cmml"
    xref="S3.SS1.p4.1.m1.2.2"><apply id="S3.SS1.p4.1.m1.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.3"><csymbol
    cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.2.3">subscript</csymbol><ci
    id="S3.SS1.p4.1.m1.2.2.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.3.2">𝝅</ci><apply id="S3.SS1.p4.1.m1.2.2.3.3.cmml"
    xref="S3.SS1.p4.1.m1.2.2.3.3"><ci id="S3.SS1.p4.1.m1.2.2.3.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.3.3.2">𝒊</ci></apply></apply><interval
    closure="open" id="S3.SS1.p4.1.m1.2.2.1.2.cmml" xref="S3.SS1.p4.1.m1.2.2.1.1"><ci
    id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑠</ci><apply id="S3.SS1.p4.1.m1.2.2.1.1.1.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.1.1.1.1.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.2.2.1.1.1.2.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.2">𝒂</ci><apply id="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.3"><ci id="S3.SS1.p4.1.m1.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.2">𝒊</ci></apply></apply></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">\bm{\pi_{-i}}(s,\bm{a_{-i}})</annotation></semantics></math>
    can be non-stationary, i.e., changes as the opponents’ policies change over time,
    for example with learning opponents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对手的联合策略 <math id
- en: Convergence results
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收敛性结果
- en: 'Littman [[125](#bib.bib125)] studied convergence properties of reinforcement
    learning joint action agents [[126](#bib.bib126)] in Markov games with the following
    conclusions: in adversarial environments (zero-sum games) an optimal play can
    be guaranteed against an arbitrary opponent, i.e., Minimax Q-learning [[124](#bib.bib124)].
    In coordination environments (e.g., in cooperative games all agents share the
    same reward function), strong assumptions need be made about other agents to guarantee
    convergence to optimal behavior [[125](#bib.bib125)], e.g., Nash Q-learning [[127](#bib.bib127)]
    and Friend-or-Foe Q-learning [[128](#bib.bib128)]. In other types of environments
    no value-based RL algorithms with guaranteed convergence properties are known [[125](#bib.bib125)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Littman[[125](#bib.bib125)]研究了Markov游戏中强化学习联合动作智能体的收敛性质，得出如下结论：在对抗环境（零和游戏）中，可以保证对任意对手的最佳游戏，即Minimax
    Q学习[[124](#bib.bib124)]。在协调环境（例如，在合作游戏中所有智能体共享相同的奖励函数）中，需要对其他智能体做出强假设，以保证收敛到最佳行为[[125](#bib.bib125)]，例如，Nash
    Q学习[[127](#bib.bib127)]和Friend-or-Foe Q学习[[128](#bib.bib128)]。在其他类型的环境中，没有已知具有保证收敛性质的基于值的RL算法[[125](#bib.bib125)]。
- en: Recent work on MDRL have addressed scalability and have focused significantly
    less on convergence guarantees, with few exceptions [[129](#bib.bib129), [130](#bib.bib130),
    [131](#bib.bib131), [132](#bib.bib132)]. One notable work has shown a connection
    between update rules for actor-critic algorithms for multiagent partially observable
    settings and (counterfactual) regret minimization:⁸⁸8 Counterfactual regret minimization
    is a technique for solving large games based on regret minimization [[133](#bib.bib133),
    [134](#bib.bib134)] due to a well-known connection between regret and Nash equilibria [[135](#bib.bib135)].
    It has been one of the reasons of successes in Poker [[16](#bib.bib16), [17](#bib.bib17)].
    the advantage values are scaled counterfactual regrets. This lead to new convergence
    properties of independent RL algorithms in zero-sum games with imperfect information [[136](#bib.bib136)].
    The result is also used to support policy gradient optimization against worst-case
    opponents, in a new algorithm called Exploitability Descent [[137](#bib.bib137)].⁹⁹9This
    algorithm is similar to CFR-BR [[138](#bib.bib138)] and has the main advantage
    that the current policy convergences rather than the average policy, so there
    is no need to learn the average strategy, which requires large reservoir buffers
    or many past networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的MDRL研究关注了可扩展性，并且在收敛性保证方面的关注显著减少，只有少数例外[[129](#bib.bib129), [130](#bib.bib130),
    [131](#bib.bib131), [132](#bib.bib132)]。一项值得注意的研究展示了多智能体部分可观测设置中的演员-评论家算法更新规则与（反事实）遗憾最小化之间的联系：⁸⁸8
    反事实遗憾最小化是一种基于遗憾最小化的解决大型游戏的技术[[133](#bib.bib133), [134](#bib.bib134)]，由于遗憾与纳什均衡之间的著名联系[[135](#bib.bib135)]。这也是扑克中成功的原因之一[[16](#bib.bib16),
    [17](#bib.bib17)]。优势值被缩放为反事实遗憾。这导致了在具有不完全信息的零和游戏中独立RL算法的新收敛性质[[136](#bib.bib136)]。该结果还被用于支持针对最坏情况对手的策略梯度优化，在一个名为Exploitability
    Descent的新算法中[[137](#bib.bib137)]。⁹⁹9 这个算法类似于CFR-BR[[138](#bib.bib138)]，其主要优点是当前策略收敛而不是平均策略，因此不需要学习平均策略，这需要大量的储备缓冲区或多个历史网络。
- en: We refer the interested reader to seminal works about convergence in multiagent
    domains [[139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)]. Note that instead
    of convergence, some MAL algorithms have proved learning a best response against
    classes of opponents [[150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议对多智能体领域收敛性的感兴趣的读者参考一些开创性的著作[[139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)]。需要注意的是，一些MAL算法已经证明了针对对手类别学习最佳回应，而不是收敛性[[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)]。
- en: 'There are other common problems in MAL, including action shadowing [[34](#bib.bib34),
    [33](#bib.bib33)], the curse of dimensionality [[5](#bib.bib5)], and multiagent
    credit assignment [[32](#bib.bib32)]. Describing each problem is out of the scope
    of this survey. However, we refer the interested reader to excellent resources
    on general MAL [[4](#bib.bib4), [153](#bib.bib153), [154](#bib.bib154)], as well
    as surveys in specific areas: game theory and multiagent reinforcement learning [[5](#bib.bib5),
    [6](#bib.bib6)], cooperative scenarios [[7](#bib.bib7), [8](#bib.bib8)], evolutionary
    dynamics of multiagent learning [[9](#bib.bib9)], learning in non-stationary environments [[10](#bib.bib10)],
    agents modeling agents [[11](#bib.bib11)], and transfer learning in multiagent
    RL [[12](#bib.bib12)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在MAL中还有其他常见问题，包括动作跟随[[34](#bib.bib34), [33](#bib.bib33)]、维度灾难[[5](#bib.bib5)]和多智能体信用分配[[32](#bib.bib32)]。描述每个问题超出了本调查的范围。然而，我们建议有兴趣的读者查阅关于一般MAL的优秀资源[[4](#bib.bib4),
    [153](#bib.bib153), [154](#bib.bib154)]，以及特定领域的调查：博弈论和多智能体强化学习[[5](#bib.bib5),
    [6](#bib.bib6)]、合作场景[[7](#bib.bib7), [8](#bib.bib8)]、多智能体学习的进化动态[[9](#bib.bib9)]、非平稳环境中的学习[[10](#bib.bib10)]、智能体建模智能体[[11](#bib.bib11)]和多智能体RL中的迁移学习[[12](#bib.bib12)]。
- en: 3.2 MDRL categorization
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 MDRL分类
- en: 'In Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") we outlined some recent
    works in single-agent DRL since an exhaustive list is out of the scope of this
    article. This explosion of works has led DRL to be extended and combined with
    other techniques [[23](#bib.bib23), [37](#bib.bib37), [29](#bib.bib29)]. One natural
    extension to DRL is to test whether these approaches could be applied in a multiagent
    environment.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.2节](#S2.SS2 "2.2 深度强化学习 ‣ 2 单一智能体学习 ‣ 多智能体深度强化学习的调查与批评1footnote 11footnote
    1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")中，我们概述了一些单一智能体DRL的最新工作，因为详尽的列表超出了本文的范围。这些工作的激增导致DRL被扩展并与其他技术结合[[23](#bib.bib23),
    [37](#bib.bib37), [29](#bib.bib29)]。对DRL的一个自然扩展是测试这些方法是否可以应用于多智能体环境。
- en: 'We analyzed the most recent works (that are not covered by previous MAL surveys [[10](#bib.bib10),
    [11](#bib.bib11)] and we do not consider genetic algorithms or swarm intelligence
    in this survey) that have a clear connection with MDRL. We propose 4 categories
    which take inspiration from previous surveys [[1](#bib.bib1), [5](#bib.bib5),
    [7](#bib.bib7), [11](#bib.bib11)] and that conveniently describe and represent
    current works. Note that some of these works fit into more than one category (they
    are not mutually exclusive), therefore their summaries are presented in all applicable
    Tables [2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")-[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”"), however, for the ease of exposition
    when describing them in the text we only do so in one category. Additionally,
    for each work we present its learning type, either a value-based method (e.g.,
    DQN) or a policy gradient method (e.g., actor-critic); also, we mention if the
    setting is evaluated in a fully cooperative, fully competitive or mixed environment
    (both cooperative and competitive).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们分析了最新的研究（这些研究未被之前的 MAL 调查涵盖 [[10](#bib.bib10), [11](#bib.bib11)]，且本调查不考虑遗传算法或群体智能），这些研究与
    MDRL 有明确的联系。我们提出了 4 个类别，这些类别借鉴了之前的调查 [[1](#bib.bib1), [5](#bib.bib5), [7](#bib.bib7),
    [11](#bib.bib11)]，并便利地描述和代表了当前的研究。请注意，这些研究中的一些适合多个类别（它们不是互斥的），因此它们的总结在所有适用的表格[2](#S3.T2
    "Table 2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")-[4](#S3.T4 "Table 4 ‣ 3.2
    MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")中展示，然而，为了便于描述，在文本中我们仅在一个类别中进行描述。此外，我们为每项工作呈现其学习类型，无论是基于价值的方法（例如
    DQN）还是策略梯度方法（例如 actor-critic）；此外，我们还提及设置是否在完全合作、完全竞争或混合环境（既包括合作又包括竞争）中进行评估。'
- en: '1.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*Analysis of emergent behaviors*. These works, in general, do not propose learning
    algorithms — their main focus is to analyze and evaluate DRL algorithms, e.g.,
    DQN [[155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)], PPO [[158](#bib.bib158),
    [157](#bib.bib157)] and others [[159](#bib.bib159), [157](#bib.bib157), [160](#bib.bib160)],
    in a multiagent environment. In this category we found works which analyze behaviors
    in the three major settings: cooperative, competitive and mixed scenarios; see
    Section [3.3](#S3.SS3 "3.3 Emergent behaviors ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and Table [2](#S3.T2 "Table
    2 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*新兴行为的分析*。这些研究通常不提出学习算法——它们的主要关注点是分析和评估 DRL 算法，例如 DQN [[155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157)], PPO [[158](#bib.bib158), [157](#bib.bib157)]
    等，在多智能体环境中。在这一类别中，我们发现了分析三大主要情境的行为的研究：合作、竞争和混合场景；请参见第[3.3节](#S3.SS3 "3.3 Emergent
    behaviors ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")和表[2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")。'
- en: '2.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*Learning communication* [[161](#bib.bib161), [160](#bib.bib160), [162](#bib.bib162),
    [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165)]. These works explore
    a sub-area in which agents can share information with communication protocols,
    for example through direct messages [[162](#bib.bib162)] or via a shared memory [[165](#bib.bib165)].
    This area is attracting attention and it had not been explored much in the MAL
    literature. See Section [3.4](#S3.SS4 "3.4 Learning communication ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") and Table [2](#S3.T2 "Table 2 ‣ 3.2 MDRL categorization ‣ 3
    Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*学习沟通* [[161](#bib.bib161), [160](#bib.bib160), [162](#bib.bib162), [163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165)]。这些工作探讨了一个子领域，其中智能体可以通过通信协议共享信息，例如通过直接消息[[162](#bib.bib162)]或通过共享记忆[[165](#bib.bib165)]。这一领域正在引起关注，而在MAL文献中尚未得到充分探索。请参阅第[3.4节](#S3.SS4
    "3.4 Learning communication ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")和表[2](#S3.T2 "Table 2 ‣
    3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")。'
- en: '3.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: '*Learning cooperation*. While learning to communicate is an emerging area,
    fostering cooperation in learning agents has a long history of research in MAL [[7](#bib.bib7),
    [8](#bib.bib8)]. In this category the analyzed works are evaluated in either cooperative
    or mixed settings. Some works in this category take inspiration from MAL (e.g.,
    leniency, hysteresis, and difference rewards concepts) and extend them to the
    MDRL setting [[35](#bib.bib35), [166](#bib.bib166), [167](#bib.bib167)]. A notable
    exception [[168](#bib.bib168)] takes a key component from RL (i.e., experience
    replay buffer) and adapts it for MDRL. See Section [3.5](#S3.SS5 "3.5 Learning
    cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”") and Table [3](#S3.T3 "Table 3 ‣ 3.2 MDRL categorization
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*学习合作*。尽管学习沟通是一个新兴领域，但在MAL中促进学习体的合作已有悠久的研究历史[[7](#bib.bib7), [8](#bib.bib8)]。在这一类别中，分析的工作要么在合作环境中，要么在混合环境中进行评估。该类别中的一些工作受到MAL的启发（例如，宽容、滞后和差异奖励概念），并将其扩展到MDRL设置中[[35](#bib.bib35),
    [166](#bib.bib166), [167](#bib.bib167)]。一个显著的例外[[168](#bib.bib168)]从RL中提取了一个关键组件（即经验重放缓冲区），并将其适应于MDRL。请参阅第[3.5节](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")和表[3](#S3.T3 "Table 3 ‣
    3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")。'
- en: '4.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: '*Agents modeling agents*. Albrecht and Stone [[11](#bib.bib11)] presented a
    thorough survey in this topic and we have found many works that fit into this
    category in the MDRL setting, some taking inspiration from DRL [[169](#bib.bib169),
    [170](#bib.bib170), [171](#bib.bib171)], and others from MAL [[172](#bib.bib172),
    [173](#bib.bib173), [64](#bib.bib64), [174](#bib.bib174), [175](#bib.bib175)].
    Modeling agents is helpful not only to cooperate, but also for modeling opponents [[172](#bib.bib172),
    [169](#bib.bib169), [171](#bib.bib171), [173](#bib.bib173)], inferring goals [[170](#bib.bib170)],
    and accounting for the learning behavior of other agents [[64](#bib.bib64)]. In
    this category the analyzed algorithms present their results in either a competitive
    setting or a mixed one (cooperative and competitive). See Section [3.6](#S3.SS6
    "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and Table [4](#S3.T4 "Table
    4 ‣ 3.2 MDRL categorization ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*代理建模代理*。Albrecht 和 Stone [[11](#bib.bib11)] 在这一主题上进行了详细的调查，我们在 MDRL 设置中发现了许多适合此类别的工作，有些受到深度强化学习的启发 [[169](#bib.bib169),
    [170](#bib.bib170), [171](#bib.bib171)]，还有一些受到多代理学习的启发 [[172](#bib.bib172), [173](#bib.bib173),
    [64](#bib.bib64), [174](#bib.bib174), [175](#bib.bib175)]。建模代理不仅对合作有帮助，也有助于建模对手 [[172](#bib.bib172),
    [169](#bib.bib169), [171](#bib.bib171), [173](#bib.bib173)]，推断目标 [[170](#bib.bib170)]，以及考虑其他代理的学习行为 [[64](#bib.bib64)]。在这一类别中，分析的算法在竞争性环境或混合环境（合作与竞争）中展示其结果。请参见第[3.6节](#S3.SS6
    "3.6 代理建模代理 ‣ 3 多代理深度强化学习（MDRL） ‣ 多代理深度强化学习的调查与批评1脚注 11脚注 1本工作的早期版本标题为：“多代理深度强化学习是答案还是问题？简要调查”")和表[4](#S3.T4
    "表 4 ‣ 3.2 MDRL 分类 ‣ 3 多代理深度强化学习（MDRL） ‣ 多代理深度强化学习的调查与批评1脚注 11脚注 1本工作的早期版本标题为：“多代理深度强化学习是答案还是问题？简要调查”")。'
- en: In the rest of this section we describe each category along with the summaries
    of related works.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的其余部分将描述每个类别以及相关工作的总结。
- en: 'Table 1: These papers analyze *emergent behaviors* in MDRL. Learning type is
    either value-based (VB) or policy gradient (PG). Setting where experiments were
    performed: cooperative (CO), competitive (CMP) or mixed. A detailed description
    is given in Section [3.3](#S3.SS3 "3.3 Emergent behaviors ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”").'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：这些论文分析了 MDRL 中的 *新兴行为*。学习类型为值基（VB）或策略梯度（PG）。实验进行的环境：合作（CO）、竞争（CMP）或混合。详细描述见第[3.3节](#S3.SS3
    "3.3 新兴行为 ‣ 3 多代理深度强化学习（MDRL） ‣ 多代理深度强化学习的调查与批评1脚注 11脚注 1本工作的早期版本标题为：“多代理深度强化学习是答案还是问题？简要调查”")。
- en: '| Work | Summary | Learning | Setting |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 总结 | 学习 | 环境 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Tampuu et al. [[155](#bib.bib155)] | Train DQN agents to play Pong. | VB
    | CO&CMP |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Tampuu 等人 [[155](#bib.bib155)] | 训练 DQN 代理玩 Pong 游戏。 | VB | CO&CMP |'
- en: '| Leibo et al. [[156](#bib.bib156)] | Train DQN agents to play sequential social
    dilemmas. | VB | Mixed |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Leibo 等人 [[156](#bib.bib156)] | 训练 DQN 代理玩序列社会困境。 | VB | Mixed |'
- en: '| Lerer and Peysakhovich [[176](#bib.bib176)] | Propose DRL agents able to
    cooperate in social dilemmas. | VB | Mixed |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Lerer 和 Peysakhovich [[176](#bib.bib176)] | 提出了能够在社会困境中进行合作的深度强化学习代理。 | VB
    | Mixed |'
- en: '| Leibo et al. [[159](#bib.bib159)] | Propose Malthusian reinforcement learning
    which extends self-play to population dynamics. | VB | Mixed |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Leibo 等人 [[159](#bib.bib159)] | 提出了马尔萨斯强化学习，将自我对弈扩展到群体动态。 | VB | Mixed |'
- en: '| Bansal et al. [[158](#bib.bib158)] | Train PPO agents in competitive MuJoCo
    scenarios. | PG | CMP |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Bansal 等人 [[158](#bib.bib158)] | 在竞争性的 MuJoCo 场景中训练 PPO 代理。 | PG | CMP |'
- en: '| Raghu et al. [[157](#bib.bib157)] | Train PPO, A3C, and DQN agents in attacker-defender
    games. | VB, PG | CMP |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Raghu 等人 [[157](#bib.bib157)] | 在攻击者-防御者游戏中训练 PPO、A3C 和 DQN 代理。 | VB, PG
    | CMP |'
- en: '| Lazaridou et al. [[161](#bib.bib161)] | Train agents represented with NN
    to learn a communication language. | PG | CO |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Lazaridou 等人 [[161](#bib.bib161)] | 训练使用神经网络表示的代理学习沟通语言。 | PG | CO |'
- en: '| Mordatch and Abbeel [[160](#bib.bib160)] | Learn communication with an end-to-end
    differentiable model to train with backpropagation. | PG | CO |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Mordatch 和 Abbeel [[160](#bib.bib160)] | 通过端到端可微分模型学习通信，以便用反向传播进行训练。 | PG
    | CO |'
- en: 'Table 2: These papers propose algorithms for *learning communication*. Learning
    type is either value-based (VB) or policy gradient (PG). Setting were experiments
    were performed: cooperative (CO) or mixed. A more detailed description is given
    in Section [3.4](#S3.SS4 "3.4 Learning communication ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 这些论文提出了*学习通信*的算法。学习类型包括基于价值的 (VB) 或策略梯度 (PG)。实验设置包括合作 (CO) 或混合。更详细的描述见第[3.4节](#S3.SS4
    "3.4 学习通信 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与评析1footnote 11footnote 1该工作的早期版本标题为：‘多智能体深度强化学习是答案还是问题？简要调查’")。'
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 总结 | 学习 | 设置 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Lazaridou et al. [[161](#bib.bib161)] | Train agents represented with NN
    to learn a communication language. | PG | CO |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Lazaridou 等 [[161](#bib.bib161)] | 训练由 NN 表示的代理以学习一种通信语言。 | PG | CO |'
- en: '| Mordatch and Abbeel [[160](#bib.bib160)] | Learn communication with an end-to-end
    differentiable model to train with backpropagation. | PG | CO |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Mordatch 和 Abbeel [[160](#bib.bib160)] | 通过端到端可微分模型学习通信，以便用反向传播进行训练。 | PG
    | CO |'
- en: '| RIAL [[162](#bib.bib162)] | Use a single network (parameter sharing) to train
    agents that take environmental and communication actions. | VB | CO |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| RIAL [[162](#bib.bib162)] | 使用单一网络 (参数共享) 来训练进行环境和通信动作的代理。 | VB | CO |'
- en: '| DIAL [[162](#bib.bib162)] | Use gradient sharing during learning and communication
    actions during execution. | VB | CO |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| DIAL [[162](#bib.bib162)] | 在学习过程中使用梯度共享，在执行过程中使用通信动作。 | VB | CO |'
- en: '| CommNet [[163](#bib.bib163)] | Use a continuous vector channel for communication
    on a single network. | PG | CO |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| CommNet [[163](#bib.bib163)] | 在单一网络上使用连续向量通道进行通信。 | PG | CO |'
- en: '| BiCNet [[164](#bib.bib164)] | Use the actor-critic paradigm where communication
    occurs in the latent space. | PG | Mixed |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| BiCNet [[164](#bib.bib164)] | 使用演员-评论家范式，其中通信发生在潜在空间中。 | PG | 混合 |'
- en: '| MD-MADDPG [[165](#bib.bib165)] | Use of a shared memory as a means to multiagent
    communication. | PG | CO |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| MD-MADDPG [[165](#bib.bib165)] | 使用共享内存作为多智能体通信的一种手段。 | PG | CO |'
- en: '| MADDPG-MD [[177](#bib.bib177)] | Extend dropout technique to robustify communication
    when applied in multiagent scenarios with direct communication. | PG | CO |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MADDPG-MD [[177](#bib.bib177)] | 扩展丢弃技术，以增强在具有直接通信的多智能体场景中的通信鲁棒性。 | PG |
    CO |'
- en: 'Table 3: These papers aim to *learn cooperation*. Learning type is either value-based
    (VB) or policy gradient (PG). Setting where experiments were performed: cooperative
    (CO), competitive (CMP) or mixed. A more detailed description is given in Section [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 这些论文旨在*学习合作*。学习类型包括基于价值的 (VB) 或策略梯度 (PG)。实验设置包括合作 (CO)、竞争 (CMP) 或混合。更详细的描述见第[3.5节](#S3.SS5
    "3.5 学习合作 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与评析1footnote 11footnote 1该工作的早期版本标题为：‘多智能体深度强化学习是答案还是问题？简要调查’")。'
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 总结 | 学习 | 设置 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Lerer and Peysakhovich [[176](#bib.bib176)] | Propose DRL agents able to
    cooperate in social dilemmas. | VB | Mixed |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Lerer 和 Peysakhovich [[176](#bib.bib176)] | 提出能够在社会困境中合作的 DRL 代理。 | VB |
    混合 |'
- en: '| MD-MADDPG [[165](#bib.bib165)] | Use of a shared memory as a means to multiagent
    communication. | PG | CO |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MD-MADDPG [[165](#bib.bib165)] | 使用共享内存作为多智能体通信的一种手段。 | PG | CO |'
- en: '| MADDPG-MD [[177](#bib.bib177)] | Extend dropout technique to robustify communication
    when applied in multiagent scenarios with direct communication. | PG | CO |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MADDPG-MD [[177](#bib.bib177)] | 扩展丢弃技术，以增强在具有直接通信的多智能体场景中的通信鲁棒性。 | PG |
    CO |'
- en: '| RIAL [[162](#bib.bib162)] | Use a single network (parameter sharing) to train
    agents that take environmental and communication actions. | VB | CO |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| RIAL [[162](#bib.bib162)] | 使用单一网络（参数共享）来训练采取环境和通信行动的智能体。 | VB | CO |'
- en: '| DIAL [[162](#bib.bib162)] | Use gradient sharing during learning and communication
    actions during execution. | VB | CO |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| DIAL [[162](#bib.bib162)] | 在学习期间使用梯度共享，在执行期间进行通信行动。 | VB | CO |'
- en: '| DCH/PSRO [[172](#bib.bib172)] | Policies can overfit to opponents: better
    compute approximate best responses to a mixture of policies. | VB | CO & CMP |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| DCH/PSRO [[172](#bib.bib172)] | 策略可能会过度拟合对手：更好地计算对一组策略的近似最佳回应。 | VB | CO
    & CMP |'
- en: '| Fingerprints [[168](#bib.bib168)] | Deal with ER problems in MDRL by conditioning
    the value function on a fingerprint that disambiguates the age of the sampled
    data. | VB | CO |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Fingerprints [[168](#bib.bib168)] | 通过将价值函数条件化为一个指纹来解决MDRL中的ER问题，这个指纹可以消除样本数据的年龄歧义。
    | VB | CO |'
- en: '| Lenient-DQN [[35](#bib.bib35)] | Achieve cooperation by leniency, optimism
    in the value function by forgiving suboptimal (low-rewards) actions. | VB | CO
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Lenient-DQN [[35](#bib.bib35)] | 通过宽容、价值函数中的乐观态度来实现合作，通过宽容次优（低回报）行动。 | VB
    | CO |'
- en: '| Hysteretic-DRQN [[166](#bib.bib166)] | Achieve cooperation by using two learning
    rates, depending on the updated values together with multitask learning via policy
    distillation. | VB | CO |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Hysteretic-DRQN [[166](#bib.bib166)] | 通过使用两个学习率来实现合作，依据更新的值，以及通过策略蒸馏进行多任务学习。
    | VB | CO |'
- en: '| WDDQN [[178](#bib.bib178)] | Achieve cooperation by leniency, weighted double
    estimators, and a modified prioritized experience replay buffer. | VB | CO |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| WDDQN [[178](#bib.bib178)] | 通过宽容、加权双重估计器和修改过的优先经验回放缓冲区实现合作。 | VB | CO |'
- en: '| FTW [[179](#bib.bib179)] | Agents act in a mixed environment (composed of
    teammates and opponents), it proposes a two-level architecture and population-based
    learning. | PG | Mixed |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| FTW [[179](#bib.bib179)] | 智能体在一个混合环境中行动（由队友和对手组成），提出了一个双层架构和基于人群的学习。 | PG
    | Mixed |'
- en: '| VDN [[180](#bib.bib180)] | Decompose the team action-value function into
    pieces across agents, where the pieces can be easily added. | VB | Mixed |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| VDN [[180](#bib.bib180)] | 将团队动作价值函数分解为各个部分，这些部分可以轻松添加。 | VB | Mixed |'
- en: '| QMIX [[181](#bib.bib181)] | Decompose the team action-value function together
    with a mixing network that can recombine them. | VB | Mixed |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| QMIX [[181](#bib.bib181)] | 将团队动作价值函数分解，并通过混合网络重新组合。 | VB | Mixed |'
- en: '| COMA [[167](#bib.bib167)] | Use a centralized critic and a counter-factual
    advantage function based on solving the multiagent credit assignment. | PG | Mixed
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| COMA [[167](#bib.bib167)] | 使用中心化评论者和基于解决多智能体信用分配的反事实优势函数。 | PG | Mixed |'
- en: '| PS-DQN, PS-TRPO, PS-A3C [[182](#bib.bib182)] | Propose parameter sharing
    for learning cooperative tasks. | VB, PG | CO |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| PS-DQN, PS-TRPO, PS-A3C [[182](#bib.bib182)] | 提出了用于学习合作任务的参数共享。 | VB, PG
    | CO |'
- en: '| MADDPG [[63](#bib.bib63)] | Use an actor-critic approach where the critic
    is augmented with information from other agents, the actions of all agents. |
    PG | Mixed |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| MADDPG [[63](#bib.bib63)] | 使用演员-评论者方法，其中评论者通过其他智能体的信息来增强，并考虑所有智能体的行动。 |
    PG | Mixed |'
- en: 'Table 4: These papers consider *agents modeling agents*. Learning type is either
    value-based (VB) or policy gradient (PG). Setting where experiments were performed:
    cooperative (CO), competitive (CMP) or mixed. A more detailed description is given
    in Section [3.6](#S3.SS6 "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 这些论文考虑了*智能体对智能体的建模*。学习类型是基于价值（VB）或策略梯度（PG）。实验的设置包括：合作（CO）、竞争（CMP）或混合。详细描述见第[3.6](#S3.SS6
    "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”)节。'
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 总结 | 学习 | 设置 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MADDPG [[63](#bib.bib63)] | Use an actor-critic approach where the critic
    is augmented with information from other agents, the actions of all agents. |
    PG | Mixed |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MADDPG [[63](#bib.bib63)] | 使用演员-评论者方法，其中评论者通过其他智能体的信息来增强，并考虑所有智能体的行动。 |
    PG | Mixed |'
- en: '| DRON [[169](#bib.bib169)] | Have a network to infer the opponent behavior
    together with the standard DQN architecture. | VB | Mixed |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| DRON [[169](#bib.bib169)] | 拥有一个网络来推断对手行为，并结合标准DQN架构。 | VB | Mixed |'
- en: '| DPIQN, DPIRQN [[171](#bib.bib171)] | Learn policy features from raw observations
    that represent high-level opponent behaviors via auxiliary tasks. | VB | Mixed
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| DPIQN, DPIRQN [[171](#bib.bib171)] | 从原始观察中学习代表高级对手行为的策略特征，通过辅助任务。 | VB |
    Mixed |'
- en: '| SOM [[170](#bib.bib170)] | Assume the reward function depends on a hidden
    goal of both agents and then use an agent’s own policy to infer the goal of the
    other agent. | PG | Mixed |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| SOM [[170](#bib.bib170)] | 假设奖励函数依赖于两个代理的隐藏目标，然后使用代理自己的策略推断其他代理的目标。 | PG
    | Mixed |'
- en: '| NFSP [[173](#bib.bib173)] | Compute approximate Nash equilibria via self-play
    and two neural networks. | VB | CMP |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| NFSP [[173](#bib.bib173)] | 通过自我博弈和两个神经网络计算近似纳什均衡。 | VB | CMP |'
- en: '| PSRO/DCH [[172](#bib.bib172)] | Policies can overfit to opponents: better
    compute approximate best responses to a mixture of policies. | PG | CO & CMP |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| PSRO/DCH [[172](#bib.bib172)] | 策略可能对对手过拟合：更好地计算对策略混合的近似最佳响应。 | PG | CO &
    CMP |'
- en: '| M3DDPG [[183](#bib.bib183)] | Extend MADDPG with minimax objective to robustify
    the learned policy. | PG | Mixed |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| M3DDPG [[183](#bib.bib183)] | 扩展 MADDPG 以最小化目标来增强学到的策略的鲁棒性。 | PG | Mixed
    |'
- en: '| LOLA [[64](#bib.bib64)] | Use a learning rule where the agent accounts for
    the parameter update of other agents to maximize its own reward. | PG | Mixed
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LOLA [[64](#bib.bib64)] | 使用一种学习规则，代理考虑其他代理的参数更新以最大化自己的奖励。 | PG | Mixed |'
- en: '| ToMnet [[174](#bib.bib174)] | Use an architecture for end-to-end learning
    and inference of diverse opponent types. | PG | Mixed |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ToMnet [[174](#bib.bib174)] | 使用一种端到端学习和推理多样对手类型的架构。 | PG | Mixed |'
- en: '| Deep Bayes-ToMoP [[175](#bib.bib175)] | Best respond to opponents using Bayesian
    policy reuse, theory of mind, and deep networks. | VB | CMP |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Deep Bayes-ToMoP [[175](#bib.bib175)] | 使用贝叶斯策略重用、心智理论和深度网络最佳响应对手。 | VB |
    CMP |'
- en: '| Deep BPR+[[184](#bib.bib184)] | Bayesian policy reuse and policy distillation
    to quickly best respond to opponents. | VB | CO & CMP |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Deep BPR+[[184](#bib.bib184)] | 贝叶斯策略重用和策略蒸馏以快速最佳响应对手。 | VB | CO & CMP |'
- en: 3.3 Emergent behaviors
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 涌现行为
- en: 'Some recent works have analyzed the previously mentioned *independent* DRL
    agents (see Section [3.1](#S3.SS1 "3.1 Multiagent Learning ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”"))
    from the perspective of types of emerging behaviors (e.g., cooperative or competitive).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一些近期的研究从新兴行为类型（例如，合作或竞争）的角度分析了之前提到的*独立* DRL 代理（见第 [3.1](#S3.SS1 "3.1 多智能体学习
    ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。
- en: One of the earliest MDRL works is by Tampuu et al. [[155](#bib.bib155)], which
    had two independent DQN learning agents to play the Atari Pong game. Their focus
    was to adapt the reward function for the learning agents, which resulted in either
    cooperative or competitive emergent behaviors.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的 MDRL 研究之一是 Tampuu 等人 [[155](#bib.bib155)] 的工作，他们让两个独立的 DQN 学习代理玩 Atari Pong
    游戏。他们的重点是为学习代理调整奖励函数，从而产生合作或竞争的涌现行为。
- en: 'Leibo et al. [[156](#bib.bib156)] meanwhile studied independent DQNs in the
    context of *sequential social dilemmas*: a Markov game that satisfies certain
    inequalities [[156](#bib.bib156)]. The focus of this work was to highlight that
    cooperative or competitive behaviors exist not only as discrete (atomic) actions,
    but they are temporally extended (over policies). In the related setting of one
    shot Markov social dilemmas, Lerer and Peysakhovich [[176](#bib.bib176)] extended
    the famous Tit-for-Tat (TFT)^(10)^(10)10TFT originated in an iterated prisoner’s
    dilemma tournament and later inspired different strategies in MAL [[185](#bib.bib185)],
    its generalization, Godfather, is a representative of *leader strategies* [[186](#bib.bib186)].
    strategy [[187](#bib.bib187)] for DRL (using function approximators) and showed
    (theoretically and experimentally) that such agents can maintain cooperation.
    To construct the agents they used self-play and two reward schemes: selfish and
    cooperative. Previously, different MAL algorithms were designed to foster cooperation
    in social dilemmas with Q-learning agents [[188](#bib.bib188), [189](#bib.bib189)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Leibo 等人[[156](#bib.bib156)]同时在*序列社会困境*的背景下研究了独立 DQN：一个满足特定不等式的马尔可夫游戏[[156](#bib.bib156)]。这项工作的重点是强调合作或竞争行为不仅存在于离散（原子）动作中，而且还在时间上延展（跨越策略）。在一次性马尔可夫社会困境的相关背景下，Lerer
    和 Peysakhovich[[176](#bib.bib176)] 扩展了著名的Tit-for-Tat (TFT)^(10)^(10)10TFT，该策略起源于一个重复囚徒困境锦标赛，并在
    MAL [[185](#bib.bib185)] 中激发了不同的策略，其推广形式 Godfather 是 *领导者策略*[[186](#bib.bib186)]
    的代表。策略[[187](#bib.bib187)]用于 DRL（使用函数逼近器），并理论和实验上证明这些代理可以维持合作。为了构建这些代理，他们使用了自我对战和两种奖励方案：自私和合作。之前，不同的
    MAL 算法被设计用于在社会困境中促进 Q-learning 代理的合作[[188](#bib.bib188), [189](#bib.bib189)]。
- en: 'Self-play is a useful concept for learning algorithms (e.g., fictitious play [[190](#bib.bib190)])
    since under certain classes of games it can guarantee convergence^(11)^(11)11The
    average strategy profile of fictitious players converges to a Nash equilibrium
    in certain classes of games, e.g., two-player zero-sum and potential games [[191](#bib.bib191)].
    and it has been used as a standard technique in previous RL and MAL works [[192](#bib.bib192),
    [14](#bib.bib14), [193](#bib.bib193)]. Despite its common usage self-play can
    be brittle to forgetting past knowledge [[194](#bib.bib194), [172](#bib.bib172),
    [195](#bib.bib195)] (see Section [4.5](#S4.SS5 "4.5 Open questions ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") for a note on the role
    of self-play as an open question in MDRL). To overcome this issue, Leibo et al. [[159](#bib.bib159)]
    proposed Malthusian reinforcement learning as an extension of self-play to population
    dynamics. The approach can be thought of as community coevolution and has been
    shown to produce better results (avoiding local optima) than independent agents
    with intrinsic motivation [[196](#bib.bib196)]. A limitation of this work is that
    it does not place itself within the state of the art in evolutionary and genetic
    algorithms. Evolutionary strategies have been employed for solving reinforcement
    learning problems [[197](#bib.bib197)] and for evolving function approximators [[75](#bib.bib75)].
    Similarly, they have been used multiagent scenarios to compute approximate Nash
    equilibria [[198](#bib.bib198)] and as metaheuristic optimization algorithms [[199](#bib.bib199),
    [200](#bib.bib200), [7](#bib.bib7), [201](#bib.bib201)].'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '自我对弈是学习算法的一个有用概念（例如，虚拟对弈 [[190](#bib.bib190)]），因为在某些类别的游戏中，它可以保证收敛^(11)^(11)11虚拟玩家的平均策略配置在某些类别的游戏中收敛到纳什均衡，例如，两人零和游戏和潜在游戏 [[191](#bib.bib191)]。并且它已被用作以前
    RL 和 MAL 工作的标准技术 [[192](#bib.bib192), [14](#bib.bib14), [193](#bib.bib193)]。尽管自我对弈的使用很普遍，但它可能容易忘记过去的知识 [[194](#bib.bib194),
    [172](#bib.bib172), [195](#bib.bib195)]（有关自我对弈在 MDRL 中作为开放问题的作用，请参见第 [4.5](#S4.SS5
    "4.5 Open questions ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") 节）。为了解决这个问题，Leibo 等人 [[159](#bib.bib159)] 提出了马尔萨斯强化学习，作为自我对弈在种群动态中的扩展。这种方法可以被看作是社区共同进化，并且已被证明比具有内在动机的独立代理人产生更好的结果（避免局部最优） [[196](#bib.bib196)]。该工作的一个限制是，它没有将自己置于进化和遗传算法的最前沿。进化策略已被用于解决强化学习问题 [[197](#bib.bib197)]
    和进化函数逼近器 [[75](#bib.bib75)]。类似地，它们也被用于多代理场景中以计算近似纳什均衡 [[198](#bib.bib198)] 和作为元启发式优化算法 [[199](#bib.bib199),
    [200](#bib.bib200), [7](#bib.bib7), [201](#bib.bib201)]。'
- en: Bansal et al. [[158](#bib.bib158)] explored the emergent behaviors in competitive
    scenarios using the MuJoCo simulator [[202](#bib.bib202)]. They trained independent
    learning agents with PPO and incorporated two main modifications to deal with
    the MAL nature of the problem. First, they used *exploration rewards* [[203](#bib.bib203)]
    which are dense rewards that allow agents to learn basic (non-competitive) behaviors
    — this type of reward is annealed through time giving more weight to the environmental
    (competitive) reward. Exploration rewards come from early work in robotics [[204](#bib.bib204)]
    and single-agent RL [[205](#bib.bib205)], and their goal is to provide dense feedback
    for the learning algorithm to improve sample efficiency (Ng et al. [[206](#bib.bib206)]
    studied the theoretical conditions under which modifications of the reward function
    of an MDP preserve the optimal policy). For multiagent scenarios, these dense
    rewards help agents in the beginning phase of the training to learn basic non-competitive
    skills, increasing the probability of random actions from the agent yielding a
    positive reward. The second contribution was *opponent sampling* which maintains
    a pool of older versions of the opponent to sample from, in contrast to using
    the most recent version.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Bansal 等人 [[158](#bib.bib158)] 使用 MuJoCo 模拟器 [[202](#bib.bib202)] 探索了竞争场景中的突现行为。他们用
    PPO 训练了独立的学习代理，并对问题的 MAL 特性进行了两个主要的修改。首先，他们使用了 *探索奖励* [[203](#bib.bib203)]，这是一种稠密奖励，允许代理学习基本（非竞争）行为——这种奖励会随着时间的推移逐渐减小，给予环境（竞争）奖励更多的权重。探索奖励源于早期的机器人研究
    [[204](#bib.bib204)] 和单代理强化学习 [[205](#bib.bib205)]，其目的是为学习算法提供稠密反馈，从而提高样本效率（Ng
    等人 [[206](#bib.bib206)] 研究了在什么理论条件下，MDP 的奖励函数修改能够保持最优策略）。对于多代理场景，这些稠密奖励帮助代理在训练初期学习基本的非竞争技能，增加了代理采取随机行动获得正奖励的概率。第二个贡献是
    *对手采样*，它维护了一个旧版对手的池以供采样，这与使用最新版本的对手形成对比。
- en: 'Raghu et al. [[157](#bib.bib157)] investigated how DRL algorithms (DQN, A2C,
    and PPO) performed in a family of two-player zero-sum games with tunable complexity,
    called Erdos-Selfridge-Spencer games [[207](#bib.bib207), [208](#bib.bib208)].
    Their reasoning is threefold: (i) these games provide a parameterized family of
    environments where (ii) optimal behavior can be completely characterized, and
    (iii) support multiagent play. Their work showed that algorithms can exhibit wide
    variation in performance as the algorithms are tuned to the game’s difficulty.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Raghu 等人 [[157](#bib.bib157)] 研究了 DRL 算法（DQN、A2C 和 PPO）在一组可调复杂度的两人零和游戏中的表现，这些游戏被称为
    Erdos-Selfridge-Spencer 游戏 [[207](#bib.bib207), [208](#bib.bib208)]。他们的理由有三方面：（i）这些游戏提供了一组参数化的环境，（ii）可以完全表征最优行为，以及（iii）支持多代理游戏。他们的工作表明，随着算法对游戏难度的调节，算法的表现可能会出现较大差异。
- en: Lazaridou et al. [[161](#bib.bib161)] proposed a framework for language learning
    that relies on multiagent communication. The agents, represented by (feed-forward)
    neural networks, need to develop an *emergent language* to solve a task. The task
    is formalized as a *signaling game* [[209](#bib.bib209)] in which two agents,
    a sender and a receiver, obtain a pair of images. The sender is told one of them
    is the target and is allowed to send a message (from a fixed vocabulary) to the
    receiver. Only when the receiver identifies the target image do both agents receive
    a positive reward. The results show that agents can coordinate for the experimented
    visual-based domain. To analyze the semantic properties^(12)^(12)12The vocabulary
    that agents use was arbitrary and had no initial meaning. To understand its emerging
    semantics they looked at the relationship between symbols and the sets of images
    they referred to [[161](#bib.bib161)]. of the learned communication protocol they
    looked whether symbol usage reflects the semantics of the visual space, and that
    despite some variation, many high level objects groups correspond to the same
    learned symbols using a t-SNE [[210](#bib.bib210)] based analysis (t-SNE is a
    visualization technique for high-dimensional data and it has also been used to
    better understand the behavior of trained DRL agents [[211](#bib.bib211), [212](#bib.bib212)]).
    A key objective of this work was to determine if the agent’s language could be
    human-interpretable. To achieve this, learned symbols were grounded with natural
    language by extending the signaling game with a supervised image labelling task
    (the sender will be encouraged to use conventional names, making communication
    more transparent to humans). To measure the interpretability of the extended game,
    a crowdsourced survey was performed, and in essence, the trained agent receiver
    was replaced with a human. The results showed that 68% of the cases, human participants
    picked the correct image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Lazaridou 等人[[161](#bib.bib161)] 提出了一个依赖于多智能体沟通的语言学习框架。这些智能体由（前馈）神经网络表示，需要发展一种*新兴语言*来解决任务。任务被形式化为一个*信号博弈*[[209](#bib.bib209)]，其中两个智能体，一个发送者和一个接收者，获得一对图像。发送者被告知其中一个是目标，并可以向接收者发送一条信息（来自固定词汇表）。只有当接收者识别出目标图像时，两个智能体才会获得正向奖励。结果表明，智能体可以协调处理实验的视觉基础领域。为了分析所学沟通协议的语义属性^(12)^(12)12智能体使用的词汇是任意的，并没有初始意义。为了理解其新兴的语义，他们查看了符号与它们所指的图像集合之间的关系[[161](#bib.bib161)]。他们检查了符号使用是否反映了视觉空间的语义，尽管存在一些变异，但许多高层次的对象组对应于相同的学习符号，使用了基于
    t-SNE 的分析[[210](#bib.bib210)]（t-SNE 是一种高维数据可视化技术，也用于更好地理解训练过的 DRL 智能体[[211](#bib.bib211),
    [212](#bib.bib212)]）。这项工作的关键目标之一是确定智能体的语言是否可以被人类理解。为实现这一目标，通过扩展信号博弈为一个监督图像标注任务，将学习到的符号与自然语言对接（鼓励发送者使用常规名称，使沟通对人类更加透明）。为了衡量扩展游戏的可解释性，进行了众包调查，实际上，训练的智能体接收者被替换为人类。结果显示，在68%的案例中，人类参与者选择了正确的图像。
- en: Similarly, Mordatch and Abbeel [[160](#bib.bib160)] investigated the emergence
    of language with the difference that in their setting there were no explicit roles
    for the agents (i.e., sender or receiver). To learn, they proposed an end-to-end
    differentiable model of all agent and environment state dynamics over time to
    calculate the gradient of the return with backpropagation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Mordatch 和 Abbeel [[160](#bib.bib160)] 研究了语言的出现，不同之处在于他们的设置中没有明确的智能体角色（即，发送者或接收者）。为了学习，他们提出了一个端到端可微分的模型，描述了所有智能体和环境状态随时间的动态，以计算回报的梯度并进行反向传播。
- en: 3.4 Learning communication
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 学习沟通
- en: 'As we discussed in the previous section, one of the desired emergent behaviors
    of multiagent interaction is the emergence of communication [[161](#bib.bib161),
    [160](#bib.bib160)]. This setting usually considers a set of *cooperative agents*
    in a *partially observable* environment (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement
    learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")) where agents need to maximize their shared utility by means
    of communicating information.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在上一节中讨论的，多智能体交互的一个期望的突现行为是通信的出现 [[161](#bib.bib161), [160](#bib.bib160)]。这种设置通常考虑一组*合作智能体*在*部分可观察*的环境中（见第[2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")节），其中智能体需要通过通信信息来最大化其共享效用。'
- en: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning
    (DIAL) are two methods using deep networks to learn to communicate [[162](#bib.bib162)].
    Both methods use a neural net that outputs the agent’s <math id="S3.SS4.p2.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi
    id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">Q</annotation></semantics></math>
    values (as done in standard DRL algorithms) and a message to communicate to other
    agents in the next timestep. RIAL is based on DRQN and also uses the concept of
    *parameter sharing*, i.e., using a single network whose parameters are shared
    among all agents. In contrast, DIAL directly passes gradients via the communication
    channel during learning, and messages are discretized and mapped to the set of
    communication actions during execution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 强化的智能体间学习（RIAL）和可微分的智能体间学习（DIAL）是两种使用深度网络学习通信的方法 [[162](#bib.bib162)]。这两种方法都使用神经网络输出智能体的<math
    id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml"
    xref="S3.SS4.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS4.p2.1.m1.1c">Q</annotation></semantics></math> 值（如标准DRL算法中所做的那样）以及在下一个时间步与其他智能体通信的消息。RIAL
    基于 DRQN，并且使用*参数共享*的概念，即使用一个在所有智能体中共享参数的网络。相比之下，DIAL 在学习过程中通过通信通道直接传递梯度，消息在执行期间被离散化并映射到通信动作集。
- en: '*Memory-driven* (MD) communication was proposed on top of the Multi-Agent Deep
    Deterministic Policy Gradient (MADDPG) [[63](#bib.bib63)] method. In MD-MADDPG
    [[165](#bib.bib165)], the agents use a shared memory as a communication channel:
    before taking an action, the agent first reads the memory, then writes a response.
    In this case the agent’s policy becomes dependent on its private observation and
    its interpretation of the collective memory. Experiments were performed with two
    agents in cooperative scenarios. The results highlighted the fact that the communication
    channel was used differently in each environment, e.g., in simpler tasks agents
    significantly decrease their memory activity near the end of the task as there
    are no more changes in the environment; in more complex environments, the changes
    in memory usage appear at a much higher frequency due to the presence of many
    sub-tasks.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于记忆*（MD）通信方法是在多智能体深度确定性策略梯度（MADDPG）[[63](#bib.bib63)] 方法的基础上提出的。在MD-MADDPG
    [[165](#bib.bib165)] 中，智能体使用共享记忆作为通信通道：在采取行动之前，智能体首先读取记忆，然后写入响应。在这种情况下，智能体的策略变得依赖于其私人观察和对集体记忆的解释。实验是在合作场景下进行的，结果突出显示了在不同环境中通信通道的使用方式不同，例如，在较简单的任务中，智能体在任务结束时显著减少其记忆活动，因为环境中没有更多变化；在更复杂的环境中，由于存在许多子任务，记忆使用的变化出现频率要高得多。'
- en: Dropout [[213](#bib.bib213)] is a technique to prevent overfitting (in supervised
    learning this happens when the learning algorithm achieves good performance only
    on a specific data set and fails to generalize) in neural networks which is based
    on randomly dropping units and their connections during training time. Inspired
    by dropout, Kim et al. [[177](#bib.bib177)] proposed a similar approach in multiagent
    environments where direct communication through messages is allowed. In this case,
    the messages of other agents are dropped out at training time, thus the authors
    proposed the Message-Dropout MADDPG algorithm [[177](#bib.bib177)]. This method
    is expected to work in fully or limited communication environments. The empirical
    results show that with properly chosen message dropout rate, the proposed method
    both significantly improves the training speed and the robustness of learned policies
    (by introducing communication errors) during execution time. This capability is
    important as MDRL agents trained in simulated or controlled environments will
    be less fragile when transferred to more realistic environments.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout [[213](#bib.bib213)] 是一种防止过拟合的技术（在监督学习中，这种情况发生在学习算法在特定数据集上表现良好，但无法泛化）在神经网络中，它基于在训练期间随机丢弃单元及其连接。受到
    Dropout 启发，Kim 等人[[177](#bib.bib177)] 提出了在允许通过消息进行直接通信的多智能体环境中类似的方法。在这种情况下，其他智能体的消息在训练时会被丢弃，因此作者提出了
    Message-Dropout MADDPG 算法[[177](#bib.bib177)]。预计这种方法适用于完全或有限通信环境。实证结果表明，通过适当选择消息丢弃率，该方法显著提高了训练速度和学习策略的鲁棒性（通过引入通信错误）在执行期间。这种能力很重要，因为在模拟或受控环境中训练的
    MDRL 智能体在转移到更现实的环境时将更不易脆弱。
- en: 'While RIAL and DIAL used a discrete communication channel, CommNet [[163](#bib.bib163)]
    used a continuous vector channel. Through this channel agents receive the summed
    transmissions of other agents. The authors assume full cooperation and train a
    single network for all the agents. There are two distinctive characteristics of
    CommNet from previous works: it allows multiple communication cycles at each timestep
    and a dynamic variation of agents at run time, i.e., agents come and go in the
    environment.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RIAL 和 DIAL 使用了离散的通信通道，但 CommNet [[163](#bib.bib163)] 使用了连续的向量通道。通过这个通道，智能体接收其他智能体的汇总传输。作者假设完全合作，并为所有智能体训练一个单一网络。CommNet
    相对于之前的工作有两个显著特点：它允许在每个时间步进行多个通信周期，并且在运行时智能体会动态变化，即智能体在环境中出现和消失。
- en: In contrast to previous approaches, in Multiagent Bidirectionally Coordinated
    Network (BiCNet) [[164](#bib.bib164)], communication takes place in the latent
    space (i.e., in the hidden layers). It also uses parameter sharing, however, it
    proposes bidirectional recurrent neural networks [[214](#bib.bib214)] to model
    the actor and critic networks of their model. Note that in BiCNet agents do not
    *explicitly* share a message and thus it can be considered a method for learning
    cooperation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法相比，在多智能体双向协调网络（BiCNet）[[164](#bib.bib164)]中，通信发生在潜在空间中（即，在隐藏层中）。它也使用参数共享，但提出了双向递归神经网络[[214](#bib.bib214)]
    来建模其模型的演员和评论家网络。需要注意的是，在 BiCNet 中，智能体不*显式*地共享消息，因此可以视为一种学习合作的方法。
- en: Learning communication is an active area in MDRL with many open questions, in
    this context, we refer the interested reader to a recent work by Lowe et al. [[215](#bib.bib215)]
    where it discusses common pitfalls (and recommendations to avoid those) while
    measuring communication in multiagent environments.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 学习通信是 MDRL 中一个活跃的领域，存在许多未解的问题，在这种背景下，我们推荐感兴趣的读者参考 Lowe 等人的近期工作[[215](#bib.bib215)]，该工作讨论了在多智能体环境中测量通信时的常见陷阱（以及避免这些陷阱的建议）。
- en: 3.5 Learning cooperation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 学习合作
- en: 'Although *explicit communication* is a new emerging trend in MDRL, there has
    already been a large amount of work in MAL for cooperative settings^(13)^(13)13There
    is a large body of research on coordinating multiagent teams by specifying communication
    protocols [[216](#bib.bib216), [217](#bib.bib217)]: these expect agents to know
    the team’s goal as well as the tasks required to accomplish the goal. that do
    not involve communication [[7](#bib.bib7), [8](#bib.bib8)]. Therefore, it was
    a natural starting point for many recent MDRL works.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*显式通信*是 MDRL 中的新兴趋势，但在 MAL 中已经有大量关于合作设置的工作^(13)^(13)13 关于通过指定通信协议来协调多智能体团队的研究[[216](#bib.bib216),
    [217](#bib.bib217)]：这些研究期望智能体了解团队的目标以及实现目标所需的任务。此外，还有一些研究没有涉及通信[[7](#bib.bib7),
    [8](#bib.bib8)]。因此，这是许多最近 MDRL 工作的自然出发点。
- en: 'Foerster et al. [[168](#bib.bib168)] studied the simple scenario of *cooperation
    with independent Q-learning agents* (see Section [3.1](#S3.SS1 "3.1 Multiagent
    Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")), where the agents use the standard DQN architecture
    of neural networks and an experience replay buffer (see Figure [3](#S2.F3 "Figure
    3 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). However, for the ER to
    work, the data distribution needs to follow certain assumptions (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")) which are no loger valid due to the multiagent
    nature of the world: the dynamics that generated the data in the ER no longer
    reflect the current dynamics, making the experience obsolete [[168](#bib.bib168),
    [90](#bib.bib90)]. Their solution is to add information to the experience tuple
    that can help to *disambiguate the age of the sampled data* from the replay memory.
    Two approaches were proposed. The first is Multiagent Importance Sampling which
    adds the probability of the joint action so an importance sampling correction [[70](#bib.bib70),
    [218](#bib.bib218)] can computed when the tuple is later sampled for training.
    This was similar to previous works in adaptive importance sampling [[219](#bib.bib219),
    [220](#bib.bib220)] and off-environment RL [[221](#bib.bib221)]. The second approach
    is Multiagent Fingerprints which adds the estimate (i.e., fingerprint) of other
    agents’ policies (loosely inspired by Hyper-Q [[150](#bib.bib150)], see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). For the practical implementation,
    good results were obtained by using the training iteration number and exploration
    rate as the fingerprint.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 'Foerster等人[[168](#bib.bib168)]研究了*与独立Q学习代理的合作*的简单场景（见第[3.1节](#S3.SS1 "3.1 Multiagent
    Learning ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")），其中代理使用标准的DQN神经网络架构和经验回放缓冲区（见图[3](#S2.F3 "Figure
    3 ‣ Value-based methods ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")）。然而，为了使经验回放有效，数据分布需要遵循某些假设（见第[2.2节](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")），但由于世界的多代理性质，这些假设不再有效：生成数据的动态不再反映当前的动态，使得经验变得过时[[168](#bib.bib168),
    [90](#bib.bib90)]。他们的解决方案是向经验元组中添加信息，以帮助*消除从回放记忆中采样数据的年龄*。提出了两种方法。第一种是多代理重要性采样，它添加了联合动作的概率，以便在后续采样用于训练时，可以计算重要性采样修正[[70](#bib.bib70),
    [218](#bib.bib218)]。这类似于以前的自适应重要性采样工作[[219](#bib.bib219), [220](#bib.bib220)]和离环境RL[[221](#bib.bib221)]。第二种方法是多代理指纹，它添加了其他代理政策的估计（即指纹）（灵感来源于Hyper-Q[[150](#bib.bib150)]，见第[4.1节](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")）。在实际实施中，通过使用训练迭代次数和探索率作为指纹，获得了良好的结果。'
- en: 'Gupta et al. [[182](#bib.bib182)] tackled cooperative environments in partially
    observable domains without explicit communication. They proposed *parameter sharing*
    (PS) as a way to improve learning in homogeneous multiagent environments (where
    agents have the same set of actions). The idea is to have one globally shared
    learning network that can still behave differently in execution time, i.e., because
    its inputs (individual agent observation and agent index) will be different. They
    tested three variations of this approach with parameter sharing: PS-DQN, PS-DDPG
    and PS-TRPO, which extended single-agent DQN, DDPG and TRPO algorithms, respectively.
    The results showed that PS-TRPO outperformed the other two. Note that Foerster
    et al. [[162](#bib.bib162)] concurrently proposed a similar concept, see Section [3.4](#S3.SS4
    "3.4 Learning communication ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gupta 等人 [[182](#bib.bib182)] 解决了在部分可观察领域中没有明确通信的合作环境问题。他们提出了*参数共享*（PS）作为在同质多智能体环境中（其中智能体具有相同的动作集）改进学习的方法。其理念是有一个全球共享的学习网络，该网络在执行时仍可以表现不同，即因为其输入（单个智能体观察和智能体索引）会有所不同。他们测试了三种具有参数共享的变体：PS-DQN、PS-DDPG
    和 PS-TRPO，它们分别扩展了单智能体 DQN、DDPG 和 TRPO 算法。结果表明，PS-TRPO 优于其他两者。注意，Foerster 等人 [[162](#bib.bib162)]
    同时提出了类似的概念，见第 [3.4](#S3.SS4 "3.4 Learning communication ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”) 节。'
- en: 'Lenient-DQN (LDQN) [[35](#bib.bib35)] took the *leniency* concept [[222](#bib.bib222)]
    (originally presented in MAL) and extended their use to MDRL. The purpose of leniency
    is to overcome a pathology called relative overgeneralization [[34](#bib.bib34),
    [223](#bib.bib223), [224](#bib.bib224)]. Similar to other approaches designed
    to overcome relative overgeneralization (e.g., distributed Q-learning [[225](#bib.bib225)]
    and hysteretic Q-learning [[8](#bib.bib8)]) lenient learners initially maintain
    an optimistic disposition to mitigate the noise from transitions resulting in
    miscoordination, preventing agents from being drawn towards sub-optimal but wide
    peaks in the reward search space [[97](#bib.bib97)]. However, similar to other
    MDRL works [[168](#bib.bib168)], the LDQN authors experienced problems with the
    ER buffer and arrived at a similar solution: adding information to the experience
    tuple, in their case, the leniency value. When sampling from the ER buffer, this
    value is used to determine a leniency condition; if the condition is not met then
    the sample is ignored.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Lenient-DQN (LDQN) [[35](#bib.bib35)] 采用了*宽容*概念 [[222](#bib.bib222)]（最初在 MAL
    中提出）并将其扩展到 MDRL。宽容的目的是克服一种称为相对过度概括 [[34](#bib.bib34), [223](#bib.bib223), [224](#bib.bib224)]
    的病理现象。类似于其他旨在克服相对过度概括的方案（例如，分布式 Q 学习 [[225](#bib.bib225)] 和滞后 Q 学习 [[8](#bib.bib8)]），宽容学习者最初保持乐观的态度，以缓解由于过渡造成的噪声，从而防止智能体被引导到奖励搜索空间中的次优但宽广的峰值
    [[97](#bib.bib97)]。然而，类似于其他 MDRL 工作 [[168](#bib.bib168)]，LDQN 的作者在 ER 缓冲区中遇到了问题，并得出了类似的解决方案：向经验元组中添加信息，在他们的情况下是宽容值。在从
    ER 缓冲区采样时，这个值用于确定宽容条件；如果条件不满足，则忽略该样本。
- en: 'In a similar vein, Decentralized-Hysteretic Deep Recurrent Q-Networks (DEC-HDRQNs) [[166](#bib.bib166)]
    were proposed for fostering cooperation among independent learners. The motivation
    is similar to LDQN, making an optimistic value update, however, their solution
    is different. Here, the authors took inspiration from Hysteretic Q-learning [[8](#bib.bib8)],
    originally presented in MAL, where two learning rates were used. A difference
    between lenient agents and hysteretic Q-learning is that lenient agents are only
    *initially* forgiving towards teammates. Lenient learners over time apply less
    leniency towards updates that would lower utility values, taking into account
    how frequently observation-action pairs have been encountered. The idea being
    that the transition from optimistic to average reward learner will help make lenient
    learners more robust towards misleading stochastic rewards [[222](#bib.bib222)].
    Additionally, in DEC-HDRQNs the ER buffer is also extended into *concurrent experience
    replay trajectories*, which are composed of three dimensions: agent index, the
    episode, and the timestep; when training, the sampled traces have the same starting
    timesteps. Moreover, to improve on generalization over different tasks, i.e.,
    multi-task learning[[226](#bib.bib226)], DEC-HDRQNs make use of policy distillation [[227](#bib.bib227),
    [228](#bib.bib228)] (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). In contrast to other approaches, DEC-HDRQNS are fully decentralized
    during learning and execution.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，去中心化滞后深度递归 Q 网络（DEC-HDRQNs）[[166](#bib.bib166)] 被提出用于促进独立学习者之间的合作。其动机与 LDQN
    类似，都是进行乐观的价值更新，但其解决方案有所不同。在这里，作者从滞后 Q 学习[[8](#bib.bib8)] 中获得了灵感，该方法最初在 MAL 中提出，使用了两种学习率。宽容代理与滞后
    Q 学习的不同在于，宽容代理对队友只有*最初*的宽容。宽容学习者随着时间的推移对那些可能降低效用值的更新应用的宽容度逐渐减少，同时考虑了观察-动作对的出现频率。其理念是，从乐观奖励学习者过渡到平均奖励学习者将有助于使宽容学习者对误导性随机奖励更加稳健[[222](#bib.bib222)]。此外，在
    DEC-HDRQNs 中，ER 缓冲区还扩展为*并发经验重放轨迹*，这些轨迹由三个维度组成：代理索引、回合和时间步；在训练时，采样的轨迹具有相同的起始时间步。此外，为了在不同任务上提高泛化能力，即多任务学习[[226](#bib.bib226)]，DEC-HDRQNs
    利用策略蒸馏[[227](#bib.bib227), [228](#bib.bib228)]（参见第[4.1节](#S4.SS1 "4.1 避免深度学习遗忘：MDRL
    中的示例 ‣ 4 RL、MAL 和 MDRL 的桥接 ‣ 多智能体深度强化学习的调查与评论1脚注 11脚注 1本工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”"））。与其他方法相比，DEC-HDRQNs
    在学习和执行过程中完全去中心化。
- en: 'Weighted Double Deep Q-Network (WDDQN) [[178](#bib.bib178)] is based on having
    double estimators. This idea was originally introduced in Double Q-learning [[91](#bib.bib91)]
    and aims to remove the existing overestimation bias caused by using the maximum
    action value as an approximation for the maximum expected action value (see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). It also uses a *lenient*
    reward [[222](#bib.bib222)] to be optimistic during initial phase of coordination
    and proposes a *scheduled* replay strategy in which samples closer to the terminal
    states are heuristically given higher priority; this strategy might not be applicable
    for any domain. For other works extending the ER to multiagent settings see MADDPG [[63](#bib.bib63)],
    Sections [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL ‣
    4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and [4.2](#S4.SS2 "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”").'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '加权双重深度Q网络（WDDQN）[[178](#bib.bib178)]基于双重估计器。这一思想最早在双重Q学习中提出 [[91](#bib.bib91)]，旨在消除由于使用最大动作值作为最大期望动作值的近似值而产生的过高估计偏差（见第[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节）。它还使用*宽容*的奖励 [[222](#bib.bib222)]，在协调初期持乐观态度，并提出了一种*调度*重放策略，其中更接近终端状态的样本在启发式上被给予更高的优先级；该策略可能不适用于任何领域。有关将ER扩展到多智能体设置的其他工作，请参见MADDPG [[63](#bib.bib63)]，第[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节和[4.2](#S4.SS2 "4.2 Lessons
    learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节。'
- en: '![Refer to caption](img/4d8897e26b6d80a96325b534c94c1ee5.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4d8897e26b6d80a96325b534c94c1ee5.png)'
- en: 'Figure 5: A schematic view of the architecture used in FTW (For the Win) [[179](#bib.bib179)]:
    two unrolled recurrent neural networks (RNNs) operate at different time-scales,
    the idea is that the *Slow RNN* helps with long term temporal correlations. Observations
    are latent space output of some convolutional neural network to learn non-linear
    features. Feudal Networks [[229](#bib.bib229)] is another work in single-agent
    DRL that also maintains a multi-time scale hierarchy where the slower network
    sets the goal, and the faster network tries to achieve them. Fedual Networks were
    in turn, inspired by early work in RL which proposed a hierarchy of Q-learners [[230](#bib.bib230),
    [231](#bib.bib231)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：FTW（For the Win）中使用的架构示意图 [[179](#bib.bib179)]：两个展开的递归神经网络（RNNs）在不同的时间尺度上运行，其思想是*慢速RNN*有助于长期时间相关性。观察结果是一些卷积神经网络的潜在空间输出，用于学习非线性特征。Feudal
    Networks [[229](#bib.bib229)]是另一个单智能体DRL的工作，它也维持了一个多时间尺度的层次结构，其中较慢的网络设定目标，而较快的网络尝试实现这些目标。Feudal
    Networks反过来受到早期RL工作的启发，该工作提出了一个Q学习者的层次结构 [[230](#bib.bib230), [231](#bib.bib231)]。
- en: 'While previous approaches were mostly inspired by how MAL algorithms could
    be extended to MDRL, other works take as base the results by single-agent DRL.
    One example is the For The Win (FTW) [[179](#bib.bib179)] agent which is based
    on the actor-learner structure of IMPALA [[114](#bib.bib114)] (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). The authors test FTW in a game where two
    opposing teams compete to capture each other’s flags [[232](#bib.bib232)]. To
    deal with the MAL problem they propose two main additions: a *hierarchical two-level
    representation* with recurrent neural networks operating at different timescales,
    as depicted in Figure [5](#S3.F5 "Figure 5 ‣ 3.5 Learning cooperation ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”"), and a *population based training* [[233](#bib.bib233), [234](#bib.bib234),
    [235](#bib.bib235)] where 30 agents were trained in parallel together with a stochastic
    matchmaking scheme that biases agents to be of similar *skills*. The Elo rating
    system [[236](#bib.bib236)] was originally devised to rate chess player skills,^(14)^(14)14Elo
    uses a normal distribution for each player skill, and after each match, both players’
    distributions are updated based on measure of *surprise*, i.e., if a user with
    previously lower (predicted) skill beats a high skilled one, the low-skilled player
    is significantly increased. TrueSkill [[237](#bib.bib237)] extended Elo by tracking
    uncertainty in skill rating, supporting draws, and matches beyond 1 vs 1; <math
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\alpha-" display="inline"><semantics
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1a"><mrow id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"><mi
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml">α</mi><mo
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.1.1.1.1.1.1.m1.1b"><apply id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1">limit-from</csymbol><ci id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2">𝛼</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p7.1.1.1.1.1.1.m1.1c">\alpha-</annotation></semantics></math>Rank
    is a more recent alternative to ELO [[238](#bib.bib238)]. FTW did not use TrueSkill
    but a simpler extension of Elo for <math id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1" class="ltx_Math"
    alttext="n" display="inline"><semantics id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1a"><mi
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1" xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1b"><ci id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml"
    xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1c">n</annotation></semantics></math> vs <math
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1a"><mi id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1" xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1b"><ci id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml"
    xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1c">n</annotation></semantics></math> games (by
    adding individual agent ratings to compute the team skill). Hierarchical approaches
    were previously proposed in RL, e.g., Feudal RL [[230](#bib.bib230), [231](#bib.bib231)],
    and were later extended to DRL in Feudal networks [[229](#bib.bib229)]; population
    based training can be considered analogous to evolutionary strategies that employ
    self-adaptive hyperparameter tuning to modify how the genetic algorithm itself
    operates [[234](#bib.bib234), [239](#bib.bib239), [240](#bib.bib240)]. An interesting
    result from FTW is that the population-based training obtained better results
    than training via self-play [[192](#bib.bib192)], which was a standard concept
    in previous works [[14](#bib.bib14), [193](#bib.bib193)]. FTW used heavy compute
    resources, it used 30 agents (processes) in parallel where every training game
    lasted 4500 agent steps (<math id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1" class="ltx_Math"
    alttext="\approx" display="inline"><semantics id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1a"><mo
    id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1.1" xref="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1c">\approx</annotation></semantics></math>
    five minutes) and agents were trained for two billion steps (<math id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1"
    class="ltx_Math" alttext="\approx" display="inline"><semantics id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1a"><mo
    id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1.1" xref="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1c">\approx</annotation></semantics></math>
    450K games).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '以前的方法大多受到如何将MAL算法扩展到MDRL的启发，而其他研究则以单代理DRL的结果为基础。一个例子是基于IMPALA的演员-学习者结构的For
    The Win (FTW) [[179](#bib.bib179)]代理（见第[2.2节](#S2.SS2 "2.2 Deep reinforcement
    learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")）。作者在一个两个对立团队竞争夺取对方旗帜的游戏中测试了FTW [[232](#bib.bib232)]。为了处理MAL问题，他们提出了两个主要的补充：一个*分层的两级表示*，使用在不同时间尺度上操作的递归神经网络，如图[5](#S3.F5
    "Figure 5 ‣ 3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")所示，以及一个*基于群体的训练* [[233](#bib.bib233),
    [234](#bib.bib234), [235](#bib.bib235)]，其中30个代理同时训练，并结合一个随机匹配机制，偏向于让代理具有相似的*技能*。Elo评级系统 [[236](#bib.bib236)]最初用于评级国际象棋选手的技能，^(14)^(14)14Elo使用每个玩家技能的正态分布，在每场比赛后，根据*惊讶*的度量更新两个玩家的分布，即如果一个之前预测技能较低的用户击败了一个高技能玩家，则低技能玩家的评级会显著提高。TrueSkill [[237](#bib.bib237)]扩展了Elo，通过跟踪技能评级的不确定性、支持平局以及1对1之外的比赛；<math
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\alpha-" display="inline"><semantics
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1a"><mrow id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"><mi
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml">α</mi><mo
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.1.1.1.1.1.1.m1.1b"><apply id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1">limit-from</csymbol><ci id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2">𝛼</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p7.1.1.1.1.1.1.m1.1c">\alpha-</annotation></semantics></math>Rank是对ELO的较新替代方案 [[238](#bib.bib238)]。FTW没有使用TrueSkill，而是对Elo进行了更简单的扩展，用于<math
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1a"><mi id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1" xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1b"><ci id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml"
    xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1c">n</annotation></semantics></math>对<math id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1"
    class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1a"><mi
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1" xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1b"><ci id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml"
    xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1c">n</annotation></semantics></math>游戏（通过添加单独代理的评级来计算团队技能）。分层方法以前在RL中提出，例如，Feudal
    RL [[230](#bib.bib230), [231](#bib.bib231)]，后来在Feudal网络 [[229](#bib.bib229)]中扩展到DRL；基于群体的训练可以被视为类似于进化策略，采用自适应超参数调整来修改遗传算法的操作方式 [[234](#bib.bib234),
    [239](#bib.bib239), [240](#bib.bib240)]。FTW的一个有趣结果是，基于群体的训练比通过自我对抗训练获得了更好的结果 [[192](#bib.bib192)]，这在之前的研究中是一个标准概念 [[14](#bib.bib14),
    [193](#bib.bib193)]。FTW使用了大量计算资源，使用了30个代理（进程）并行训练，每场训练游戏持续4500个代理步骤（<math id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1"
    class="ltx_Math" alttext="\approx" display="inline"><semantics id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1a"><mo
    id="S3.SS5.p7.4.4'
- en: 'Lowe et al. [[63](#bib.bib63)] noted that using standard policy gradient methods
    (see Section [2.1](#S2.SS1 "2.1 Reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) on multiagent environments
    yields high variance and performs poorly. This occurs because the variance is
    further increased as all the agents’ rewards depend on the rest of the agents,
    and it is formally shown that as the number of agents increase, the probability
    of taking a correct gradient direction decreases exponentially [[63](#bib.bib63)].
    Therefore, to overcome this issue Lowe et al. proposed the Multi-Agent Deep Deterministic
    Policy Gradient (MADDPG) [[63](#bib.bib63)], building on DDPG [[65](#bib.bib65)]
    (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")), to train a centralized
    critic per agent that is given all agents’ policies during training to reduce
    the variance by removing the non-stationarity caused by the concurrently learning
    agents. Here, the actor only has local information (turning the method into a
    centralized training with decentralized execution) and the ER buffer records experiences
    of *all* agents. MADDPG was tested in both cooperative and competitive scenarios,
    experimental results show that it performs better than several decentralized methods
    (such as DQN, DDPG, and TRPO). The authors mention that traditional RL methods
    do not produce consistent gradient signals. This is exemplified in a challenging
    competitive scenarios where agents continuously adapt to each other causing the
    learned best-response policies oscillate — for such a domain, MADDPG is shown
    to learn more robustly than DDPG.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lowe 等人 [[63](#bib.bib63)] 指出，在多智能体环境中使用标准策略梯度方法（参见第 [2.1](#S2.SS1 "2.1 Reinforcement
    learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") 节）会导致高方差且表现不佳。这是因为方差进一步增加，因为所有智能体的奖励都依赖于其他智能体，并且正式表明，随着智能体数量的增加，采取正确梯度方向的概率呈指数下降
    [[63](#bib.bib63)]。因此，为了克服这个问题，Lowe 等人提出了多智能体深度确定性策略梯度（MADDPG） [[63](#bib.bib63)]，以
    DDPG [[65](#bib.bib65)]（参见第 [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2
    Single-agent learning ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    节）为基础，训练一个集中式评论员，在训练期间提供所有智能体的策略，以通过消除由并发学习的智能体引起的非平稳性来减少方差。在这里，演员仅具有局部信息（将方法转变为集中训练与分散执行），而
    ER 缓冲区记录了*所有*智能体的经验。MADDPG 在合作和竞争场景中均进行了测试，实验结果表明，它比多种分散方法（如 DQN、DDPG 和 TRPO）表现更好。作者提到，传统的
    RL 方法不会产生一致的梯度信号。这在具有挑战性的竞争场景中表现突出，在这些场景中，智能体不断适应彼此，导致学到的最佳响应策略发生波动——对于这样的领域，MADDPG
    比 DDPG 更具鲁棒性。'
- en: 'Another approach based on policy gradients is the Counterfactual Multi-Agent
    Policy Gradients (COMA) [[167](#bib.bib167)]. COMA was designed for the fully
    centralized setting and the *multiagent credit assignment problem* [[241](#bib.bib241)],
    i.e., how the agents should deduce their contributions when learning in a cooperative
    setting in the presence of only global rewards. Their proposal is to compute a
    *counterfactual baseline*, that is, marginalize out the action of the agent while
    keeping the rest of the other agents’ actions fixed. Then, an advantage function
    can be computed comparing the current <math id="S3.SS5.p9.1.1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S3.SS5.p9.1.1.1.m1.1a"><mi id="S3.SS5.p9.1.1.1.m1.1.1"
    xref="S3.SS5.p9.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS5.p9.1.1.1.m1.1b"><ci id="S3.SS5.p9.1.1.1.m1.1.1.cmml" xref="S3.SS5.p9.1.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p9.1.1.1.m1.1c">Q</annotation></semantics></math>
    value to the counterfactual. This counterfactual baseline has its roots in *difference
    rewards*, which is a method for obtaining the individual contribution of an agent
    in a cooperative multiagent team [[241](#bib.bib241)]. In particular, the *aristocrat*
    utility aims to measure the difference between an agent’s actual action and the
    average action [[31](#bib.bib31)]. The intention would be equivalent to sideline
    the agent by having the agent perform an action where the reward does not depend
    on the agent’s actions, i.e., to consider the reward that would have arisen assuming
    a world without that agent having ever existed (see Section [4.2](#S4.SS2 "4.2
    Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '基于策略梯度的另一种方法是**反事实多智能体策略梯度（COMA）**[[167](#bib.bib167)]。COMA 是为完全集中化的设置而设计的，解决*多智能体信用分配问题*[[241](#bib.bib241)]，即在只有全局奖励的合作环境中，智能体应如何推断其贡献。他们的建议是计算*反事实基线*，即在保持其他智能体动作不变的情况下边际化掉当前智能体的动作。然后，可以计算一个优势函数，将当前<math
    id="S3.SS5.p9.1.1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS5.p9.1.1.1.m1.1a"><mi id="S3.SS5.p9.1.1.1.m1.1.1" xref="S3.SS5.p9.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p9.1.1.1.m1.1b"><ci id="S3.SS5.p9.1.1.1.m1.1.1.cmml"
    xref="S3.SS5.p9.1.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p9.1.1.1.m1.1c">Q</annotation></semantics></math>值与反事实进行比较。这个反事实基线源于*差异奖励*，这是一种在合作多智能体团队中获得个体贡献的方法[[241](#bib.bib241)]。具体来说，*贵族*效用旨在衡量智能体实际动作与平均动作之间的差异[[31](#bib.bib31)]。其意图相当于通过让智能体执行一个与智能体动作无关的动作来忽略该智能体，即考虑在没有该智能体存在的世界中所产生的奖励（见第[4.2节](#S4.SS2
    "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”)）。'
- en: 'On the one hand, fully centralized approaches (e.g., COMA) do not suffer from
    non-stationarity but have constrained scalability. On the other hand, independent
    learning agents are better suited to scale but suffer from non-stationarity issues.
    There are some hybrid approaches that learn *a centralized but factored* <math
    id="S3.SS5.p10.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS5.p10.1.m1.1a"><mi id="S3.SS5.p10.1.m1.1.1" xref="S3.SS5.p10.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p10.1.m1.1b"><ci id="S3.SS5.p10.1.m1.1.1.cmml"
    xref="S3.SS5.p10.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p10.1.m1.1c">Q</annotation></semantics></math> value function [[242](#bib.bib242),
    [243](#bib.bib243)]. Value Decomposition Networks (VDNs) [[180](#bib.bib180)]
    decompose a team value function into an additive decomposition of the individual
    value functions. Similarly, QMIX [[181](#bib.bib181)] relies on the idea of factorizing,
    however, instead of sum, QMIX assumes a *mixing network* that combines the local
    values in a non-linear way, which can represent monotonic action-value functions.
    While the mentioned approaches have obtained good empirical results, the factorization
    of value-functions in multiagent scenarios using function approximators (MDRL)
    is an ongoing research topic, with open questions such as how well factorizations
    capture complex coordination problems and how to learn those factorizations [[244](#bib.bib244)]
    (see Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging RL,
    MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '一方面，完全集中式的方法（例如，COMA）没有非平稳性的问题，但具有有限的可扩展性。另一方面，独立学习代理更适合扩展，但面临非平稳性的问题。有一些混合方法学习*集中但分解的*
    <math id="S3.SS5.p10.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS5.p10.1.m1.1a"><mi id="S3.SS5.p10.1.m1.1.1" xref="S3.SS5.p10.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p10.1.m1.1b"><ci id="S3.SS5.p10.1.m1.1.1.cmml"
    xref="S3.SS5.p10.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p10.1.m1.1c">Q</annotation></semantics></math> 价值函数 [[242](#bib.bib242),
    [243](#bib.bib243)]。价值分解网络（VDNs）[[180](#bib.bib180)] 将团队价值函数分解为个体价值函数的加法分解。类似地，QMIX
    [[181](#bib.bib181)] 依赖于分解的思想，但 QMIX 假设一个*混合网络*，以非线性的方式结合局部值，这可以表示单调的动作-价值函数。虽然上述方法取得了良好的实证结果，但在多智能体场景中使用函数逼近器（MDRL）对价值函数的分解仍然是一个正在研究的课题，存在一些开放性问题，如分解如何捕捉复杂的协调问题以及如何学习这些分解
    [[244](#bib.bib244)]（见第 [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”) 节）。'
- en: 3.6 Agents modeling agents
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 代理建模代理
- en: '![Refer to caption](img/c33d4d724302e5f821fb15a7441cfe4d.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c33d4d724302e5f821fb15a7441cfe4d.png)'
- en: 'Figure 6: (a) Deep Policy Inference Q-Network: receives four stacked frames
    as input (similar to DQN, see Figure [2](#S2.F2 "Figure 2 ‣ Value-based methods
    ‣ 2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). (b) Deep Policy Inference Recurrent Q-Network:
    receives one frame as input and has an LSTM layer instead of a fully connected
    layer (FC). Both approaches [[171](#bib.bib171)] condition the <math id="S3.F6.4.1.m1.1"
    class="ltx_Math" alttext="Q_{M}" display="inline"><semantics id="S3.F6.4.1.m1.1b"><msub
    id="S3.F6.4.1.m1.1.1" xref="S3.F6.4.1.m1.1.1.cmml"><mi id="S3.F6.4.1.m1.1.1.2"
    xref="S3.F6.4.1.m1.1.1.2.cmml">Q</mi><mi id="S3.F6.4.1.m1.1.1.3" xref="S3.F6.4.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml
    encoding="MathML-Content" id="S3.F6.4.1.m1.1c"><apply id="S3.F6.4.1.m1.1.1.cmml"
    xref="S3.F6.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.4.1.m1.1.1.1.cmml" xref="S3.F6.4.1.m1.1.1">subscript</csymbol><ci
    id="S3.F6.4.1.m1.1.1.2.cmml" xref="S3.F6.4.1.m1.1.1.2">𝑄</ci><ci id="S3.F6.4.1.m1.1.1.3.cmml"
    xref="S3.F6.4.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.4.1.m1.1d">Q_{M}</annotation></semantics></math> value outputs on the
    policy features, <math id="S3.F6.5.2.m2.1" class="ltx_Math" alttext="h^{PI}" display="inline"><semantics
    id="S3.F6.5.2.m2.1b"><msup id="S3.F6.5.2.m2.1.1" xref="S3.F6.5.2.m2.1.1.cmml"><mi
    id="S3.F6.5.2.m2.1.1.2" xref="S3.F6.5.2.m2.1.1.2.cmml">h</mi><mrow id="S3.F6.5.2.m2.1.1.3"
    xref="S3.F6.5.2.m2.1.1.3.cmml"><mi id="S3.F6.5.2.m2.1.1.3.2" xref="S3.F6.5.2.m2.1.1.3.2.cmml">P</mi><mo
    lspace="0em" rspace="0em" id="S3.F6.5.2.m2.1.1.3.1" xref="S3.F6.5.2.m2.1.1.3.1.cmml">​</mo><mi
    id="S3.F6.5.2.m2.1.1.3.3" xref="S3.F6.5.2.m2.1.1.3.3.cmml">I</mi></mrow></msup><annotation-xml
    encoding="MathML-Content" id="S3.F6.5.2.m2.1c"><apply id="S3.F6.5.2.m2.1.1.cmml"
    xref="S3.F6.5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F6.5.2.m2.1.1.1.cmml" xref="S3.F6.5.2.m2.1.1">superscript</csymbol><ci
    id="S3.F6.5.2.m2.1.1.2.cmml" xref="S3.F6.5.2.m2.1.1.2">ℎ</ci><apply id="S3.F6.5.2.m2.1.1.3.cmml"
    xref="S3.F6.5.2.m2.1.1.3"><ci id="S3.F6.5.2.m2.1.1.3.2.cmml" xref="S3.F6.5.2.m2.1.1.3.2">𝑃</ci><ci
    id="S3.F6.5.2.m2.1.1.3.3.cmml" xref="S3.F6.5.2.m2.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.F6.5.2.m2.1d">h^{PI}</annotation></semantics></math>,
    which are also used to learn the opponent policy <math id="S3.F6.6.3.m3.1" class="ltx_Math"
    alttext="\pi_{o}" display="inline"><semantics id="S3.F6.6.3.m3.1b"><msub id="S3.F6.6.3.m3.1.1"
    xref="S3.F6.6.3.m3.1.1.cmml"><mi id="S3.F6.6.3.m3.1.1.2" xref="S3.F6.6.3.m3.1.1.2.cmml">π</mi><mi
    id="S3.F6.6.3.m3.1.1.3" xref="S3.F6.6.3.m3.1.1.3.cmml">o</mi></msub><annotation-xml
    encoding="MathML-Content" id="S3.F6.6.3.m3.1c"><apply id="S3.F6.6.3.m3.1.1.cmml"
    xref="S3.F6.6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F6.6.3.m3.1.1.1.cmml" xref="S3.F6.6.3.m3.1.1">subscript</csymbol><ci
    id="S3.F6.6.3.m3.1.1.2.cmml" xref="S3.F6.6.3.m3.1.1.2">𝜋</ci><ci id="S3.F6.6.3.m3.1.1.3.cmml"
    xref="S3.F6.6.3.m3.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.6.3.m3.1d">\pi_{o}</annotation></semantics></math>.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: (a) 深度策略推断 Q 网络：接收四个堆叠的帧作为输入（类似于 DQN，见图 [2](#S2.F2 "图 2 ‣ 基于价值的方法 ‣ 2.2
    深度强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的综述与批评1脚注 11脚注 1该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")）。(b)
    深度策略推断递归 Q 网络：接收一个帧作为输入，并且有一个 LSTM 层代替全连接层 (FC)。这两种方法 [[171](#bib.bib171)] 都基于策略特征上的
    <math id="S3.F6.4.1.m1.1" class="ltx_Math" alttext="Q_{M}" display="inline"><semantics
    id="S3.F6.4.1.m1.1b"><msub id="S3.F6.4.1.m1.1.1" xref="S3.F6.4.1.m1.1.1.cmml"><mi
    id="S3.F6.4.1.m1.1.1.2" xref="S3.F6.4.1.m1.1.1.2.cmml">Q</mi><mi id="S3.F6.4.1.m1.1.1.3"
    xref="S3.F6.4.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content"
    id="S3.F6.4.1.m1.1c"><apply id="S3.F6.4.1.m1.1.1.cmml" xref="S3.F6.4.1.m1.1.1"><csymbol
    cd="ambiguous" id="S3.F6.4.1.m1.1.1.1.cmml" xref="S3.F6.4.1.m1.1.1">subscript</csymbol><ci
    id="S3.F6.4.1.m1.1.2" xref="S3.F6.4.1.m1.1.2.cmml">𝑄</ci><ci id="S3.F6.4.1.m1.1.3"
    xref="S3.F6.4.1.m1.1.3.cmml">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.4.1.m1.1d">Q_{M}</annotation></semantics></math> 值输出，<math id="S3.F6.5.2.m2.1"
    class="ltx_Math" alttext="h^{PI}" display="inline"><semantics id="S3.F6.5.2.m2.1b"><msup
    id="S3.F6.5.2.m2.1.1" xref="S3.F6.5.2.m2.1.1.cmml"><mi id="S3.F6.5.2.m2.1.1.2"
    xref="S3.F6.5.2.m2.1.1.2.cmml">h</mi><mrow id="S3.F6.5.2.m2.1.1.3" xref="S3.F6.5.2.m2.1.1.3.cmml"><mi
    id="S3.F6.5.2.m2.1.1.3.2" xref="S3.F6.5.2.m2.1.1.3.2.cmml">P</mi><mo lspace="0em"
    rspace="0em" id="S3.F6.5.2.m2.1.1.3.1" xref="S3.F6.5.2.m2.1.1.3.1.cmml">​</mo><mi
    id="S3.F6.5.2.m2.1.1.3.3" xref="S3.F6.5.2.m2.1.1.3.3.cmml">I</mi></mrow></msup><annotation-xml
    encoding="MathML-Content" id="S3.F6.5.2.m2.1c"><apply id="S3.F6.5.2.m2.1.1.cmml"
    xref="S3.F6.5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F6.5.2.m2.1.1.1.cmml" xref="S3.F6.5.2.m2.1.1">superscript</csymbol><ci
    id="S3.F6.5.2.m2.1.1.2.cmml" xref="S3.F6.5.2.m2.1.1.2">ℎ</ci><apply id="S3.F6.5.2.m2.1.1.3.cmml"
    xref="S3.F6.5.2.m2.1.1.3"><ci id="S3.F6.5.2.m2.1.1.3.2.cmml" xref="S3.F6.5.2.m2.1.1.3.2">𝑃</ci><ci
    id="S3.F6.5.2.m2.1.1.3.3.cmml" xref="S3.F6.5.2.m2.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.F6.5.2.m2.1d">h^{PI}</annotation></semantics></math>，这些值也用于学习对手的策略
    <math id="S3.F6.6.3.m3.1" class="ltx_Math" alttext="\pi_{o}" display="inline"><semantics
    id="S3.F6.6.3.m3.1b"><msub id="S3.F6.6.3.m3.1.1" xref="S3.F6.6.3.m3.1.1.cmml"><mi
    id="S3.F6.6.3.m3.1.1.2" xref="S3.F6.6.3.m3.1.1.2.cmml">π</mi><mi id="S3.F6.6.3.m3.1.1.3"
    xref="S3.F6.6.3.m3.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content"
    id="S3.F6.6.3.m3.1c"><apply id="S3.F6.6.3.m3.1.1.cmml" xref="S3.F6.6.3.m3.1.1"><csymbol
    cd="ambiguous" id="S3.F6.6.3.m3.1.1.1.cmml" xref="S3.F6.6.3.m3.1.1">subscript</csymbol><ci
    id="S3.F6.6.3.m3.1.1.2.cmml" xref="S3.F6.6.3.m3.1.1.2">𝜋</ci><ci id="S3.F6.6.3.m3.1.1.3.cmml"
    xref="S3.F6.6.3.m3.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.6.3.m3.1d">\pi_{o}</annotation></semantics></math>。'
- en: 'An important ability for agents to have is to reason about the behaviors of
    other agents by constructing models that make predictions about the modeled agents [[11](#bib.bib11)].
    An early work for modeling agents while using deep neural networks was the Deep
    Reinforcement Opponent Network (DRON) [[169](#bib.bib169)]. The idea is to have
    two networks: one which evaluates <math id="S3.SS6.p1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1"
    xref="S3.SS6.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">Q</annotation></semantics></math>-values
    and a second one that *learns a representation of the opponent’s policy*. Moreover,
    the authors proposed to have several expert networks to combine their predictions
    to get the estimated <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="Q"
    display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1"
    xref="S3.SS6.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">Q</annotation></semantics></math>
    value, the idea being that each expert network captures one type of opponent strategy [[245](#bib.bib245)].
    This is related to previous works in type-based reasoning from game theory [[246](#bib.bib246),
    [139](#bib.bib139)] later applied in AI [[245](#bib.bib245), [11](#bib.bib11),
    [247](#bib.bib247)]. The mixture of experts idea was presented in supervised learning
    where each expert handled a subset of the data (a subtask), and then a gating
    network decided which of the experts should be used [[248](#bib.bib248)].'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对代理来说，一个重要的能力是通过构建模型来推理其他代理的行为，以对建模的代理进行预测 [[11](#bib.bib11)]。早期利用深度神经网络建模代理的工作是深度强化对手网络（DRON） [[169](#bib.bib169)]。其理念是拥有两个网络：一个评估<math
    id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml"
    xref="S3.SS6.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p1.1.m1.1c">Q</annotation></semantics></math>值，另一个*学习对手策略的表示*。此外，作者提出了使用多个专家网络来结合它们的预测，以获取估计的<math
    id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml"
    xref="S3.SS6.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p1.2.m2.1c">Q</annotation></semantics></math>值，每个专家网络捕捉一种对手策略 [[245](#bib.bib245)]。这与游戏理论中的基于类型的推理的早期工作有关 [[246](#bib.bib246),
    [139](#bib.bib139)]，后来被应用于AI [[245](#bib.bib245), [11](#bib.bib11), [247](#bib.bib247)]。专家混合的理念在监督学习中被提出，其中每个专家处理数据的一个子集（一个子任务），然后一个门控网络决定使用哪个专家 [[248](#bib.bib248)]。
- en: 'DRON uses hand-crafted features to define the opponent network. In contrast,
    Deep Policy Inference Q-Network (DPIQN) and its recurrent version, DPIRQN [[171](#bib.bib171)]
    learn *policy features* directly from raw observations of the other agents. The
    way to learn these policy features is by means of auxiliary tasks [[84](#bib.bib84),
    [110](#bib.bib110)] (see Sections [2.2](#S2.SS2 "2.2 Deep reinforcement learning
    ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    and [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) that provide additional
    learning goals, in this case, the auxiliary task is to learn the opponents’ policies.
    This auxiliary task modifies the loss function by computing an auxiliary loss:
    the cross entropy loss between the inferred opponent policy and the ground truth
    (one-hot action vector) of the opponent. Then, the <math id="S3.SS6.p2.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><mi
    id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p2.1.m1.1b"><ci id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">Q</annotation></semantics></math>
    value function of the learning agent is conditioned on the opponent’s policy features
    (see Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent Deep
    Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")),
    which aims to reduce the non-stationarity of the environment. The authors used
    an adaptive training procedure to adjust the attention (a weight on the loss function)
    to either emphasize learning the policy features (of the opponent) or the respective
    <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml"
    xref="S3.SS6.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p2.2.m2.1c">Q</annotation></semantics></math> values of the agent.
    An advantage of these approaches is that modeling the agents can work for both
    opponents and teammates [[171](#bib.bib171)].'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: DRON 使用手工特征来定义对手网络。相比之下，深度策略推断 Q 网络（DPIQN）及其递归版本 DPIRQN [[171](#bib.bib171)]
    直接从其他智能体的原始观测中学习*策略特征*。学习这些策略特征的方式是通过辅助任务 [[84](#bib.bib84), [110](#bib.bib110)]（参见第[2.2](#S2.SS2
    "2.2 深度强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的综述与批评1脚注 11脚注 1该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要综述”")节和第[4.1](#S4.SS1
    "4.1 避免深度学习遗忘症：MDRL 中的示例 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多智能体深度强化学习的综述与批评1脚注 11脚注 1该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要综述”")节），这些任务提供了额外的学习目标，在这种情况下，辅助任务是学习对手的策略。这一辅助任务通过计算辅助损失来修改损失函数：即推断的对手策略与对手的真实标签（独热编码向量）之间的交叉熵损失。然后，学习智能体的<math
    id="S3.SS6.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p2.1.m1.1a"><mi id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><ci id="S3.SS6.p2.1.m1.1.1.cmml"
    xref="S3.SS6.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p2.1.m1.1c">Q</annotation></semantics></math> 值函数以对手的策略特征为条件（参见图[6](#S3.F6
    "图 6 ‣ 3.6 代理建模代理 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的综述与批评1脚注 11脚注 1该工作的早期版本标题为：“多智能体深度强化学习是答案还是问题？简要综述”")），旨在减少环境的非平稳性。作者使用了一种自适应训练程序来调整注意力（即在损失函数上的权重），以强调学习对手的策略特征或智能体的相应<math
    id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml"
    xref="S3.SS6.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p2.2.m2.1c">Q</annotation></semantics></math> 值。此方法的一个优点是建模代理可以同时适用于对手和队友
    [[171](#bib.bib171)]。
- en: In many previous works an opponent model is learned from observations. Self
    Other Modeling (SOM) [[170](#bib.bib170)] proposed a different approach, this
    is, using *the agent’s own policy as a means to predict the opponent’s actions*.
    SOM can be used in cooperative and competitive settings (with an arbitrary number
    of agents) and infers other agents’ goals. This is important because in the evaluated
    domains, the reward function depends on the goal of the agents. SOM uses two networks,
    one used for computing the agents’ own policy, and a second one used to infer
    the opponent’s goal. The idea is that these networks have the same input parameters
    but with different values (the agent’s or the opponent’s). In contrast to previous
    approaches, SOM is not focused on learning the opponent policy, i.e., a probability
    distribution over next actions, but rather on estimating the opponent’s goal.
    SOM is expected to work best when agents share a set of goals from which each
    agent gets assigned one at the beginning of the episode and the reward structure
    depends on both of their assigned goals. Despite its simplicity, training takes
    longer as an additional optimization step is performed given the other agent’s
    observed actions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多以往的工作中，通过观察学习对手模型。Self Other Modeling (SOM) [[170](#bib.bib170)] 提出了不同的方法，即*利用代理自身的策略来预测对手的行动*。SOM
    可以用于合作和竞争环境（与任意数量的代理），并推断其他代理的目标。这很重要，因为在评估领域中，奖励函数依赖于代理的目标。SOM 使用两个网络，一个用于计算代理自身的策略，另一个用于推断对手的目标。其思想是这些网络具有相同的输入参数，但值不同（代理的或对手的）。与以往的方法不同，SOM
    不是专注于学习对手的策略，即下一步行动的概率分布，而是专注于估计对手的目标。预计当代理共享一组目标，从中每个代理在开始阶段被分配一个目标，并且奖励结构依赖于他们分配的目标时，SOM
    的表现最好。尽管其简单性，训练时间较长，因为在给定其他代理观察到的行动时，会进行额外的优化步骤。
- en: 'There is a long-standing history of combining game theory and MAL [[2](#bib.bib2),
    [6](#bib.bib6), [193](#bib.bib193)]. From that context, some approaches were inspired
    by influential game theory approaches. Neural Fictitious Self-Play (NFSP) [[173](#bib.bib173)]
    builds on fictitious (self-) play [[190](#bib.bib190), [249](#bib.bib249)], together
    with two deep networks to find *approximate Nash equilibria*^(15)^(15)15Nash equilibrium [[250](#bib.bib250)]
    is a solution concept in game theory in which no agent would choose to deviate
    from its strategy (they are a best response to others’ strategies). This concept
    has been explored in seminal MAL algorithms like Nash-Q learning [[127](#bib.bib127)]
    and Minimax-Q learning [[124](#bib.bib124), [128](#bib.bib128)]. in two-player
    imperfect information games [[251](#bib.bib251)] (for example, consider Poker:
    when it is an agent’s turn to move it does not have access to all information
    about the world). One network learns an *approximate best response* (<math id="S3.SS6.p4.1.m1.1"
    class="ltx_Math" alttext="\epsilon-" display="inline"><semantics id="S3.SS6.p4.1.m1.1a"><mrow
    id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml"><mi id="S3.SS6.p4.1.m1.1.1.2"
    xref="S3.SS6.p4.1.m1.1.1.2.cmml">ϵ</mi><mo id="S3.SS6.p4.1.m1.1.1.3" xref="S3.SS6.p4.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.1.m1.1b"><apply id="S3.SS6.p4.1.m1.1.1.cmml"
    xref="S3.SS6.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS6.p4.1.m1.1.1.1.cmml"
    xref="S3.SS6.p4.1.m1.1.1">limit-from</csymbol><ci id="S3.SS6.p4.1.m1.1.1.2.cmml"
    xref="S3.SS6.p4.1.m1.1.1.2">italic-ϵ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">\epsilon-</annotation></semantics></math>greedy
    over <math id="S3.SS6.p4.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p4.2.m2.1a"><mi id="S3.SS6.p4.2.m2.1.1" xref="S3.SS6.p4.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.2.m2.1b"><ci id="S3.SS6.p4.2.m2.1.1.cmml"
    xref="S3.SS6.p4.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p4.2.m2.1c">Q</annotation></semantics></math> values) to the historical
    behavior of other agents and the second one (called the average network) learns
    to imitate its own past best response behaviour using supervised classification.
    The agent behaves using a mixture of the average and the best response networks
    depending on the probability of an anticipatory parameter [[252](#bib.bib252)].
    Comparisons with DQN in Leduc Hold’em Poker revealed that DQN’s deterministic
    strategy is highly exploitable. Such strategies are sufficient to behave optimally
    in single-agent domains, i.e., MDPs for which DQN was designed. However, imperfect-information
    games generally require stochastic strategies to achieve optimal behaviour [[173](#bib.bib173)].
    DQN learning experiences are both highly correlated over time, and highly focused
    on a narrow state distribution. In contrast to NFSP agents whose experience varies
    more smoothly, resulting in a more stable data distribution, more stable neural
    networks and better performance.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 结合博弈论和MAL的历史悠久[[2](#bib.bib2), [6](#bib.bib6), [193](#bib.bib193)]。在这种背景下，一些方法受到了有影响力的博弈论方法的启发。神经虚拟自我博弈（NFSP）[[173](#bib.bib173)]基于虚拟（自我）博弈[[190](#bib.bib190),
    [249](#bib.bib249)]，结合了两个深度网络来寻找*近似纳什均衡*^(15)^(15)15纳什均衡[[250](#bib.bib250)]是博弈论中的一个解概念，其中没有代理会选择偏离其策略（它们是对其他策略的最佳回应）。这一概念在开创性的MAL算法中得到了探讨，如Nash-Q学习[[127](#bib.bib127)]和Minimax-Q学习[[124](#bib.bib124),
    [128](#bib.bib128)]。在两人不完全信息博弈中[[251](#bib.bib251)]（例如，考虑扑克：当代理轮到行动时，它无法获得关于世界的所有信息）。一个网络学习*近似最佳回应*（<math
    id="S3.SS6.p4.1.m1.1" class="ltx_Math" alttext="\epsilon-" display="inline"><semantics
    id="S3.SS6.p4.1.m1.1a"><mrow id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml"><mi
    id="S3.SS6.p4.1.m1.1.1.2" xref="S3.SS6.p4.1.m1.1.2.cmml">ϵ</mi><mo id="S3.SS6.p4.1.m1.1.3"
    xref="S3.SS6.p4.1.m1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p4.1.m1.1b"><apply id="S3.SS6.p4.1.m1.1.1.cmml" xref="S3.SS6.p4.1.m1.1.1"><csymbol
    cd="latexml" id="S3.SS6.p4.1.m1.1.1.1.cmml" xref="S3.SS6.p4.1.m1.1.1">limit-from</csymbol><ci
    id="S3.SS6.p4.1.m1.1.1.2.cmml" xref="S3.SS6.p4.1.m1.1.1.2">italic-ϵ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">\epsilon-</annotation></semantics></math>贪婪于<math
    id="S3.SS6.p4.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p4.2.m2.1a"><mi id="S3.SS6.p4.2.m2.1.1" xref="S3.SS6.p4.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.2.m2.1b"><ci id="S3.SS6.p4.2.m2.1.1.cmml"
    xref="S3.SS6.p4.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p4.2.m2.1c">Q</annotation></semantics></math>值）对其他代理的历史行为，第二个网络（称为平均网络）学习使用监督分类来模仿其自身过去的最佳回应行为。代理根据预测参数的概率使用平均网络和最佳回应网络的混合策略[[252](#bib.bib252)]。与DQN在Leduc
    Hold’em扑克中的比较表明，DQN的确定性策略高度可被利用。这些策略足以在单代理领域，即DQN设计的MDP中实现最佳行为。然而，不完全信息博弈通常需要随机策略来实现最佳行为[[173](#bib.bib173)]。DQN学习经验在时间上高度相关，并且高度集中于狭窄的状态分布。与NFSP代理相比，NFSP代理的经验更平滑，从而导致更稳定的数据分布、更稳定的神经网络和更好的性能。
- en: The (N)FSP concept was further generalized in Policy-Space Response Oracles
    (PSRO) [[172](#bib.bib172)], where it was shown that fictitious play is one specific
    meta-strategy distribution over a set of previous (approximate) best responses
    (summarized by a meta-game obtained by empirical game theoretic analysis [[253](#bib.bib253)]),
    but there are a wide variety to choose from. One reason to use mixed meta-strategies
    is that it prevents overfitting^(16)^(16)16Johanson et al. [[254](#bib.bib254)]
    also found “overfitting” when solving large extensive games (e.g., poker) — the
    performance in an abstract game improved but it was worse in the full game. the
    responses to one specific policy, and hence provides a form of opponent/teammate
    regularization. An approximate scalable version of the algorithm leads to a graph
    of agents best-responding independently called Deep Cognitive Hierarchies (DCHs) [[172](#bib.bib172)]
    due to its similarity to behavioral game-theoretic models [[255](#bib.bib255),
    [256](#bib.bib256)].
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: (N)FSP概念在政策空间响应神谕（PSRO）[[172](#bib.bib172)]中得到了进一步的概括，其中显示了虚拟游戏是一种特定的元策略分布，涵盖了一组之前（近似）的最佳响应（通过经验游戏理论分析[[253](#bib.bib253)]获得的元游戏的总结），但可以选择的种类非常广泛。使用混合元策略的一个原因是，它可以防止过拟合^(16)^(16)16Johanson等人[[254](#bib.bib254)]在解决大型广泛游戏（例如扑克）时也发现了“过拟合”——在抽象游戏中的表现提高，但在完整游戏中的表现却更差。这种策略响应对特定策略的反应，从而提供了一种对手/队友正则化的形式。该算法的一个近似可扩展版本形成了一个代理独立最佳响应的图，称为深度认知层级（DCHs）[[172](#bib.bib172)]，因其类似于行为博弈理论模型[[255](#bib.bib255),
    [256](#bib.bib256)]。
- en: 'Minimax is a paramount concept in game theory that is roughly described as
    minimizing the worst case scenario (maximum loss) [[251](#bib.bib251)]. Li et
    al. [[183](#bib.bib183)] took the minimax idea as an approach to robustify learning
    in multiagent environments so that the learned robust policy should be able to
    behave well even with strategies not seen during training. They extended the MADDPG
    algorithm [[63](#bib.bib63)] to Minimax Multiagent Deep Deterministic Policy Gradients
    (M3DDPG), which updates policies considering a worst-case scenario: assuming that
    all other agents act adversarially. This yields a minimax learning objective which
    is computationally intractable to directly optimize. They address this issue by
    taking ideas from robust reinforcement learning [[257](#bib.bib257)] which implicitly
    adopts the minimax idea by using the *worst noise* concept [[258](#bib.bib258)].
    In MAL different approaches were proposed to assess the robustness of an algorithm,
    e.g., guarantees of safety [[152](#bib.bib152), [259](#bib.bib259)], security [[260](#bib.bib260)]
    or exploitability [[261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263)].'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Minimax是博弈论中的一个重要概念，大致描述为最小化最坏情况（最大损失）[[251](#bib.bib251)]。Li等人[[183](#bib.bib183)]将minimax思想作为一种增强多智能体环境中学习稳健性的方式，以便学习到的稳健策略即使在训练过程中未见过的策略下也能表现良好。他们将MADDPG算法[[63](#bib.bib63)]扩展为Minimax
    Multiagent Deep Deterministic Policy Gradients (M3DDPG)，它在考虑最坏情况（假设所有其他代理采取对抗性行为）时更新策略。这产生了一个在计算上难以直接优化的minimax学习目标。他们通过借鉴稳健强化学习[[257](#bib.bib257)]中的思想来解决这个问题，该思想通过使用*最坏噪声*概念[[258](#bib.bib258)]隐式地采用了minimax思想。在MAL中，提出了不同的方法来评估算法的稳健性，例如安全性[[152](#bib.bib152),
    [259](#bib.bib259)]、安全[[260](#bib.bib260)]或可利用性[[261](#bib.bib261), [262](#bib.bib262),
    [263](#bib.bib263)]。
- en: Previous approaches usually learned a model of the other agents as a way to
    predict their behavior. However, they do not explicitly *account for anticipated
    learning of the other agents*, which is the objective of Learning with Opponent-Learning
    Awareness (LOLA) [[64](#bib.bib64)]. LOLA optimizes the expected return *after
    the opponent updates its policy one step*. Therefore, a LOLA agent directly shapes
    the policy updates of other agents to maximize its own reward. One of LOLA’s assumptions
    is having access to opponents’ policy parameters. LOLA builds on previous ideas
    by Zhang and Lesser [[264](#bib.bib264)] where the learning agent predicts the
    opponent’s policy parameter update but only uses it to learn a best response (to
    the anticipated updated parameters).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的方法通常会学习其他代理的模型，以预测其行为。然而，它们并没有明确*考虑到其他代理的预期学习*，这是具有对手学习意识（LOLA）的目标[[64](#bib.bib64)]。LOLA优化对手更新其策略一步后的*预期回报*。因此，LOLA代理直接调整其他代理的策略更新，以最大化自身奖励。LOLA的一个假设是可以访问对手的策略参数。LOLA基于Zhang和Lesser的前期思想[[264](#bib.bib264)]，其中学习代理预测对手的策略参数更新，但只使用它来学习最佳响应（对预期更新的参数）。
- en: 'Theory of mind is part of a group of *recursive reasoning* approaches[[265](#bib.bib265),
    [245](#bib.bib245), [266](#bib.bib266), [267](#bib.bib267)] in which agents have
    explicit beliefs about the mental states of other agents. The mental states of
    other agents may, in turn, also contain beliefs and mental states of other agents,
    leading to a nesting of beliefs [[11](#bib.bib11)]. Theory of Mind Network (ToMnet) [[174](#bib.bib174)]
    starts with a simple premise: when encountering a novel opponent, *the agent should
    already have a strong and rich prior about how the opponent should behave*. ToMnet
    has an architecture composed of three networks: (i) a character network that learns
    from historical information, (ii) a mental state network that takes the character
    output and the recent trajectory, and (iii) the prediction network that takes
    the current state as well as the outputs of the other networks as its input. The
    output of the architecture is open for different problems but in general its goal
    is to predict the opponent’s next action. A main advantage of ToMnet is that it
    can predict general behavior, for all agents; or specific, for a particular agent.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 思维理论是*递归推理*方法的一部分[[265](#bib.bib265), [245](#bib.bib245), [266](#bib.bib266),
    [267](#bib.bib267)]，其中代理对其他代理的心理状态有明确的信念。其他代理的心理状态可能也包含对其他代理的信念和心理状态，从而导致信念的嵌套[[11](#bib.bib11)]。思维理论网络（ToMnet）[[174](#bib.bib174)]以一个简单的前提开始：当遇到一个新的对手时，*代理应该已经对对手的行为有一个强大而丰富的先验知识*。ToMnet的架构由三个网络组成：(i)
    一个从历史信息中学习的角色网络，(ii) 一个接受角色输出和最近轨迹的心理状态网络，以及 (iii) 一个接受当前状态以及其他网络输出作为输入的预测网络。该架构的输出可以用于不同的问题，但一般目标是预测对手的下一步行动。ToMnet的一个主要优势是它能够预测所有代理的一般行为；或特定于某个代理的行为。
- en: Deep Bayesian Theory of Mind Policy (Bayes-ToMoP) [[175](#bib.bib175)] is another
    algorithm that takes inspiration from theory of mind [[268](#bib.bib268)]. The
    algorithm assumes the opponent has different stationary strategies to act and
    changes among them over time [[269](#bib.bib269)]. Earlier work in MAL dealt with
    this setting, e.g., BPR+ [[270](#bib.bib270)] extends the Bayesian policy reuse^(17)^(17)17Bayesian
    policy reuse assumes an agent with prior experience in the form of a library of
    policies. When a novel task instance occurs, the objective is to reuse a policy
    from its library based on observed signals which correlate to policy performance [[271](#bib.bib271)].
    framework [[271](#bib.bib271)] to multiagent settings (BPR assumes a single-agent
    environment; BPR+ aims to best respond to the opponent in a multiagent game).
    A limitation of BPR+ is that it behaves poorly against itself (self-play), thus,
    Deep Bayes-ToMoP uses theory of mind to provide a higher-level reasoning strategy
    which provides an optimal behavior against BPR+ agents.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 深度贝叶斯思维理论政策（Bayes-ToMoP）[[175](#bib.bib175)]是另一个受到思维理论[[268](#bib.bib268)]启发的算法。该算法假设对手有不同的静态策略来行动，并随着时间的推移在它们之间变化[[269](#bib.bib269)]。早期在MAL中的工作处理了这种情况，例如，BPR+[[270](#bib.bib270)]将贝叶斯政策重用^(17)^(17)17贝叶斯政策重用假设代理具有以策略库形式存在的先验经验。当出现新的任务实例时，目标是根据观察到的信号从其库中重用一个政策，这些信号与政策表现相关[[271](#bib.bib271)]。框架[[271](#bib.bib271)]扩展到多代理设置（BPR假设单代理环境；BPR+旨在在多代理游戏中最佳响应对手）。BPR+的一个限制是它在自我对战（自我游戏）中表现不佳，因此，Deep
    Bayes-ToMoP利用思维理论提供一种更高层次的推理策略，以便对抗BPR+代理时表现出最佳行为。
- en: Deep BPR+ [[184](#bib.bib184)] is another work inspired by BPR+ which uses neural
    networks as value-function approximators. It not only uses the environment reward
    but also uses the online learned opponent model [[272](#bib.bib272), [273](#bib.bib273)]
    to construct a rectified belief over the opponent strategy. Additionally, it leverages
    ideas from policy distillation [[227](#bib.bib227), [228](#bib.bib228)] and extends
    them to the multiagent case to create a distilled policy network. In this case,
    whenever a new acting policy is learned, distillation is applied to consolidate
    the new updated library which improves in terms of storage and generalization
    (over opponents).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Deep BPR+[[184](#bib.bib184)]是另一个受BPR+启发的工作，它使用神经网络作为价值函数近似器。它不仅使用环境奖励，还使用在线学习的对手模型[[272](#bib.bib272),
    [273](#bib.bib273)]来构建对对手策略的修正信念。此外，它利用了政策蒸馏[[227](#bib.bib227), [228](#bib.bib228)]的思想，并将其扩展到多代理情况下，创建了一个蒸馏政策网络。在这种情况下，每当学习到新的行动策略时，就应用蒸馏来巩固新的更新库，从而在存储和泛化（对抗对手）方面有所改进。
- en: 4 Bridging RL, MAL and MDRL
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 桥接RL、MAL和MDRL
- en: 'This section aims to provide directions to promote fruitful cooperations between
    sub-communities. First, we address the pitfall of *deep learning amnesia*, roughly
    described as missing citations to the original works and not exploiting the advancements
    that have been made in the past. We present examples on how ideas originated earlier,
    for example in RL and MAL, were successfully extended to MDRL (see Section [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and
    MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Second, we outline *lessons
    learned* from the works analyzed in this survey (see Section [4.2](#S4.SS2 "4.2
    Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). Then we point the readers to recent benchmarks for MDRL (see
    Section [4.3](#S4.SS3 "4.3 Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) and we discuss the practical
    challenges that arise in MDRL like high computational demands and reproducibility
    (see Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL ‣ 4 Bridging RL,
    MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Lastly, we pose some
    open research challenges and reflect on their relation with previous open questions
    in MAL [[11](#bib.bib11)] (see Section [4.5](#S4.SS5 "4.5 Open questions ‣ 4 Bridging
    RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在提供促进子社区之间富有成效合作的方向。首先，我们讨论了*深度学习遗忘症*的陷阱，大致描述为遗漏对原始工作的引用以及未利用过去取得的进展。我们展示了如何将早期的思想，例如在
    RL 和 MAL 中产生的思想，成功扩展到 MDRL 的示例（见第[4.1](#S4.SS1 "4.1 避免深度学习遗忘症：MDRL 中的示例 ‣ 4 连接
    RL、MAL 和 MDRL ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节）。其次，我们概述了从本调查中分析的工作中获得的*经验教训*（见第[4.2](#S4.SS2
    "4.2 经验教训 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节）。然后，我们引导读者查看
    MDRL 的最新基准（见第[4.3](#S4.SS3 "4.3 MDRL 的基准 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多智能体深度强化学习的调查与批评1脚注
    11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节），并讨论 MDRL 中出现的实际挑战，如高计算需求和可重复性（见第[4.4](#S4.SS4
    "4.4 MDRL 中的实际挑战 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节）。最后，我们提出了一些未解研究挑战，并反思它们与
    MAL 中先前未解问题的关系[[11](#bib.bib11)]（见第[4.5](#S4.SS5 "4.5 未解问题 ‣ 4 连接 RL、MAL 和 MDRL
    ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节）。
- en: '4.1 Avoiding deep learning amnesia: examples in MDRL'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 避免深度学习遗忘症：MDRL 中的示例
- en: This survey focuses on recent *deep* works, however, in previous sections, when
    describing recent algorithms, we also point to original works that inspired them.
    Schmidhuber said “Machine learning is the science of credit assignment. The machine
    learning community itself profits from proper credit assignment to its members” [[274](#bib.bib274)].
    In this context, we want to avoid committing the pitfall of not giving credit
    to original ideas that were proposed earlier, a.k.a. *deep learning amnesia*.
    Here, we provide some specific examples of research milestones that were studied
    earlier, e.g., RL or MAL, and that now became highly relevant for MDRL. Our purpose
    is to highlight that existent literature contains pertinent ideas and algorithms
    that should not be ignored. On the contrary, they should be examined and cited [[275](#bib.bib275),
    [276](#bib.bib276)] to understand recent developments [[277](#bib.bib277)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查聚焦于最近的*深度*工作，然而，在描述近期算法时，我们也指向了激发这些算法的原始工作。Schmidhuber曾说：“机器学习是信用分配的科学。机器学习社区本身也从对其成员的适当信用分配中受益”[[274](#bib.bib274)]。在这种背景下，我们希望避免犯下未给予早期提出的原始想法应有的信用的陷阱，即*深度学习遗忘症*。在这里，我们提供了一些早期研究里程碑的具体例子，例如RL或MAL，这些现在对于MDRL变得高度相关。我们的目的是突出现有文献中包含的相关想法和算法，这些不应被忽视。相反，它们应被审查并引用[[275](#bib.bib275),
    [276](#bib.bib276)]，以理解近期发展[[277](#bib.bib277)]。
- en: Dealing with non-stationarity in independent learners
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理独立学习者中的非平稳性
- en: 'It is well known that using independent learners makes the environment non-stationary
    from the agent’s point of view [[4](#bib.bib4), [123](#bib.bib123)]. Many MAL
    algorithms tried to solve this problem in different ways [[10](#bib.bib10)]. One
    example is *Hyper-Q* [[150](#bib.bib150)] which accounts for the (values of mixed)
    strategies of other agents and includes that information in the state representation,
    which effectively turns the learning problem into a stationary one. Note that
    in this way it is possible to even consider adaptive agents. Foerster et al. [[162](#bib.bib162)]
    make use of this insight to propose their *fingerprint* algorithm in an MDRL problem
    (see Section [3.5](#S3.SS5 "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement
    Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")). Other examples include
    the leniency concept [[222](#bib.bib222)] and Hysteretic Q-learning [[8](#bib.bib8)]
    originally presented in MAL, which now have their “deep” counterparts, LDQNs [[35](#bib.bib35)]
    and DEC-HDRQNs[[166](#bib.bib166)], see Section [3.5](#S3.SS5 "3.5 Learning cooperation
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，使用独立学习者会使环境从代理的角度来看变得非平稳[[4](#bib.bib4), [123](#bib.bib123)]。许多MAL算法尝试以不同的方式解决这个问题[[10](#bib.bib10)]。一个例子是*Hyper-Q*[[150](#bib.bib150)]，它考虑了其他代理的（混合）策略的值，并将这些信息包含在状态表示中，这有效地将学习问题转化为一个平稳问题。请注意，通过这种方式，甚至可以考虑自适应代理。Foerster等人[[162](#bib.bib162)]利用这一见解在MDRL问题中提出了他们的*fingerprint*算法（见第[3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节）。其他例子包括在MAL中首次提出的宽容概念[[222](#bib.bib222)]和Hysteretic
    Q-learning[[8](#bib.bib8)]，现在它们有了“深度”对应物，LDQNs[[35](#bib.bib35)]和DEC-HDRQNs[[166](#bib.bib166)]，见第[3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节。'
- en: Multiagent credit assignment
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多智能体信用分配
- en: 'In cooperative multiagent scenarios, it is common to use either *local rewards*,
    unique for each agent, or *global rewards*, which represent the entire group’s
    performance [[278](#bib.bib278)]. However, local rewards are usually harder to
    obtain, therefore, it is common to rely only on the global ones. This raises the
    problem of *credit assignment*: how does a single agent’s actions contribute to
    a system that involves the actions of many agents [[32](#bib.bib32)]. A solution
    that came from MAL research that has proven successful in many scenarios is *difference
    rewards* [[241](#bib.bib241), [278](#bib.bib278), [279](#bib.bib279)], which aims
    to capture an agent’s contribution to the system’s global performance. In particular
    the *aristocrat* utility aims to measure the difference between an agent’s actual
    action and the average action [[31](#bib.bib31)], however, it has a self-consistency
    problem and in practice it is more common to compute the *wonderful life utility* [[280](#bib.bib280),
    [31](#bib.bib31)], which proposes to use a clamping operation that would be equivalent
    to removing that player from the team. COMA [[167](#bib.bib167)] builds on these
    concepts to propose an *advantage function* based on the contribution of the agent,
    which can be efficiently computed with deep neural networks (see Section [3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '在合作的多智能体场景中，通常使用*局部奖励*，即每个智能体独有的奖励，或*全局奖励*，即代表整个团队表现的奖励[[278](#bib.bib278)]。然而，局部奖励通常更难获取，因此通常只依赖全局奖励。这引发了*信用分配*的问题：一个单一智能体的行为如何对涉及多个智能体的系统做出贡献[[32](#bib.bib32)]。来自MAL研究的一种在许多场景中已证明成功的解决方案是*差异奖励*[[241](#bib.bib241),
    [278](#bib.bib278), [279](#bib.bib279)]，其目的是捕捉智能体对系统全局表现的贡献。特别是*贵族*效用旨在衡量智能体实际行动与平均行动之间的差异[[31](#bib.bib31)]，然而，它存在自洽性问题，实际中更常计算*美好生活效用*[[280](#bib.bib280),
    [31](#bib.bib31)]，该效用提议使用一个夹紧操作，相当于将该玩家从团队中移除。COMA[[167](#bib.bib167)]基于这些概念提出了一种*优势函数*，该函数基于智能体的贡献，可以通过深度神经网络高效计算（见第[3.5节](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”)）。'
- en: Multitask learning
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多任务学习
- en: 'In the context of RL, multitask learning [[226](#bib.bib226)] is an area that
    develops agents that can act in *several related tasks* rather than just in a
    single one [[281](#bib.bib281)]. *Distillation*, roughly defined as transferring
    the knowledge from a large model to a small model, was a concept originally introduced
    for supervised learning and model compression [[282](#bib.bib282), [228](#bib.bib228)].
    Inspired by those works, Policy distillation [[227](#bib.bib227)] was extended
    to the DRL realm. Policy distillation was used to train a much smaller network
    and to merge *several task-specific policies* into a single policy, i.e., for
    multitask learning. In the MDRL setting, Omidshafiei et al. [[166](#bib.bib166)]
    successfully adapted policy distillation within Dec-HDRQNs to obtain a more general
    multitask multiagent network (see Section [3.5](#S3.SS5 "3.5 Learning cooperation
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). Another example is Deep BPR+ [[184](#bib.bib184)] which uses
    distillation to generalize over multiple opponents (see Section [3.6](#S3.SS6
    "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning (MDRL)
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的背景下，多任务学习[[226](#bib.bib226)]是一个发展能够在*多个相关任务*中执行的智能体的领域，而不仅仅是在单一任务中[[281](#bib.bib281)]。*蒸馏*，大致定义为将知识从大模型转移到小模型，是最初为监督学习和模型压缩引入的概念[[282](#bib.bib282),
    [228](#bib.bib228)]。受到这些工作的启发，策略蒸馏[[227](#bib.bib227)]被扩展到深度强化学习领域。策略蒸馏被用来训练一个更小的网络，并将*多个特定任务的策略*合并成一个单一策略，即用于多任务学习。在多任务深度强化学习（MDRL）设置中，Omidshafiei
    等人[[166](#bib.bib166)]成功地在 Dec-HDRQNs 中应用了策略蒸馏，以获得一个更通用的多任务多智能体网络（见第[3.5节](#S3.SS5
    "3.5 学习合作 ‣ 3 多智能体深度强化学习（MDRL） ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”)）另一个例子是
    Deep BPR+[[184](#bib.bib184)]，它使用蒸馏来在多个对手之间进行泛化（见第[3.6节](#S3.SS6 "3.6 智能体建模智能体
    ‣ 3 多智能体深度强化学习（MDRL） ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1 早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”)）。
- en: Auxiliary tasks
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 辅助任务
- en: 'Jaderberg et al. [[84](#bib.bib84)] introduced the term auxiliary task with
    the insight that (single-agent) environments contain a variety of possible training
    signals (e.g., pixel changes). These tasks are naturally implemented in DRL in
    which the last layer is split into multiple parts (heads), each working on a different
    task. All heads propagate errors into the same shared preceding part of the network,
    which would then try to form representations, in its next-to-last layer, to support
    all the heads [[20](#bib.bib20)]. However, the idea of multiple predictions about
    arbitrary signals was originally suggested for RL, in the context of general value
    functions [[110](#bib.bib110), [20](#bib.bib20)] and there still open problems,
    for example, better theoretical understanding [[109](#bib.bib109), [283](#bib.bib283)].
    In the context of neural networks, early work proposed *hints* that improved the
    network performance and learning time. Suddarth and Kergosien [[284](#bib.bib284)]
    presented a minimal example of a small neural network where it was shown that
    adding an auxiliary task effectively removed local minima. One could think of
    extending these auxiliary tasks to modeling other agents’ behaviors [[285](#bib.bib285),
    [160](#bib.bib160)], which is one of the key ideas that DPIQN and DRPIQN [[171](#bib.bib171)]
    proposed in MDRL settings (see Section [3.6](#S3.SS6 "3.6 Agents modeling agents
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jaderberg 等人 [[84](#bib.bib84)] 引入了“辅助任务”这一术语，洞察到（单一智能体）环境中包含多种可能的训练信号（例如像素变化）。这些任务自然地在
    DRL 中实现，其中最后一层被分成多个部分（头），每个部分处理不同的任务。所有头将误差传播到相同的共享前部网络，然后在其倒数第二层形成表示，以支持所有头 [[20](#bib.bib20)]。然而，对任意信号的多个预测的想法最初是为
    RL 提出的，涉及一般价值函数 [[110](#bib.bib110), [20](#bib.bib20)]，目前仍存在开放问题，例如更好的理论理解 [[109](#bib.bib109),
    [283](#bib.bib283)]。在神经网络的背景下，早期工作提出了*提示*，以改善网络性能和学习时间。Suddarth 和 Kergosien [[284](#bib.bib284)]
    提出了一个小型神经网络的最小示例，显示添加辅助任务有效地消除了局部最小值。可以考虑将这些辅助任务扩展到建模其他智能体的行为 [[285](#bib.bib285),
    [160](#bib.bib160)]，这是 DPIQN 和 DRPIQN [[171](#bib.bib171)] 在 MDRL 设置中提出的关键思想之一（参见第
    [3.6](#S3.SS6 "3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”) 节）。'
- en: Experience replay
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经验重放
- en: 'Lin [[90](#bib.bib90), [89](#bib.bib89)] proposed the concept of experience
    replay to speed up the credit assignment propagation process in single agent RL.
    This concept became central to many DRL works [[72](#bib.bib72)] (see Section [2.2](#S2.SS2
    "2.2 Deep reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). However, Lin stated that a condition for
    the ER to be useful is that “the environment should not change over time because
    this makes past experiences irrelevant or even harmful” [[90](#bib.bib90)]. This
    is a problem in domains where many agents are learning since the environment becomes
    non-stationary from the point of view of each agent. Since DRL relies heavily
    on experience replay, this is an issue in MDRL: the non-stationarity introduced
    means that the dynamics that generated the data in the agent’s replay memory no
    longer reflect the current dynamics in which it is learning [[162](#bib.bib162)].
    To overcome this problem different methods have been proposed [[168](#bib.bib168),
    [35](#bib.bib35), [166](#bib.bib166), [178](#bib.bib178)], see Section [4.2](#S4.SS2
    "4.2 Lessons learned ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”").'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Lin [[90](#bib.bib90), [89](#bib.bib89)] 提出了经验回放的概念，以加速单智能体 RL 中的信用分配传播过程。这个概念成为了许多
    DRL 工作的核心 [[72](#bib.bib72)]（见第 [2.2](#S2.SS2 "2.2 深度强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与批评1footnote
    11footnote 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”") 节）。然而，Lin 指出，经验回放有用的一个条件是“环境不应随时间变化，因为这会使过去的经验变得无关甚至有害”
    [[90](#bib.bib90)]。在许多智能体进行学习的领域，这成为了一个问题，因为从每个智能体的角度来看，环境变得非平稳。由于 DRL 在很大程度上依赖于经验回放，这在
    MDRL 中是一个问题：引入的非平稳性意味着在智能体回放记忆中生成数据的动态不再反映当前学习中的动态 [[162](#bib.bib162)]。为解决这个问题，已经提出了不同的方法
    [[168](#bib.bib168), [35](#bib.bib35), [166](#bib.bib166), [178](#bib.bib178)]，见第
    [4.2](#S4.SS2 "4.2 学到的经验 ‣ 4 RL、MAL 和 MDRL 的桥接 ‣ 多智能体深度强化学习的调查与批评1footnote 11footnote
    1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”") 节。
- en: Double estimators
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双重估计器
- en: 'Double Q-learning [[91](#bib.bib91)] proposed to reduce the overestimation
    of action values in Q-learning, this is caused by using the maximum action value
    as an approximation for the maximum expected action value. Double Q-learning works
    by keeping two Q functions and was proven to convergence to the optimal policy [[91](#bib.bib91)].
    Later this idea was applied to arbitrary function approximators, including deep
    neural networks, i.e., Double DQN [[92](#bib.bib92)], which were naturally applied
    since two networks were already used in DQN (see Section [2.2](#S2.SS2 "2.2 Deep
    reinforcement learning ‣ 2 Single-agent learning ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")). These ideas have also been recently applied to MDRL [[178](#bib.bib178)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Double Q-learning [[91](#bib.bib91)] 提出了减少 Q-learning 中动作值过高估计的问题，这种问题是由于使用最大动作值作为最大期望动作值的近似值。Double
    Q-learning 通过保持两个 Q 函数来工作，并被证明能够收敛到最优策略 [[91](#bib.bib91)]。后来，这一理念被应用于任意函数逼近器，包括深度神经网络，即
    Double DQN [[92](#bib.bib92)]，因为在 DQN 中已经使用了两个网络（见第 [2.2](#S2.SS2 "2.2 深度强化学习
    ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与批评1footnote 11footnote 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")
    节）。这些思想最近也被应用于 MDRL [[178](#bib.bib178)]。
- en: 4.2 Lessons learned
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 学到的经验
- en: We have exemplified how RL and MAL can be extended for MDRL settings. Now, we
    outline general *best practices* learned from the works analyzed throughout this
    paper.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们举例说明了 RL 和 MAL 如何扩展到 MDRL 设置。现在，我们概述了从本文分析的工作中学到的一般 *最佳实践*。
- en: '1.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Experience replay buffer in MDRL. While some works removed the ER buffer in
    MDRL [[162](#bib.bib162)] it is an important component in many DRL and MDRL algorithms.
    However, using the standard buffer (i.e., keeping <math id="S4.I1.i1.p1.1.m1.4"
    class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle" display="inline"><semantics
    id="S4.I1.i1.p1.1.m1.4a"><mrow id="S4.I1.i1.p1.1.m1.4.4.1" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml"><mo
    stretchy="false" id="S4.I1.i1.p1.1.m1.4.4.1.2" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">⟨</mo><mi
    id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">s</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.3"
    xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.2.2" xref="S4.I1.i1.p1.1.m1.2.2.cmml">a</mi><mo
    id="S4.I1.i1.p1.1.m1.4.4.1.4" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.3.3"
    xref="S4.I1.i1.p1.1.m1.3.3.cmml">r</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.5" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><msup
    id="S4.I1.i1.p1.1.m1.4.4.1.1" xref="S4.I1.i1.p1.1.m1.4.4.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.4.4.1.1.2"
    xref="S4.I1.i1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.1.3"
    xref="S4.I1.i1.p1.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S4.I1.i1.p1.1.m1.4.4.1.6"
    xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S4.I1.i1.p1.1.m1.4b"><list id="S4.I1.i1.p1.1.m1.4.4.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1"><ci
    id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">𝑠</ci><ci id="S4.I1.i1.p1.1.m1.2.2.cmml"
    xref="S4.I1.i1.p1.1.m1.2.2">𝑎</ci><ci id="S4.I1.i1.p1.1.m1.3.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3">𝑟</ci><apply
    id="S4.I1.i1.p1.1.m1.4.4.1.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous"
    id="S4.I1.i1.p1.1.m1.4.4.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S4.I1.i1.p1.1.m1.4.4.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1.2">𝑠</ci><ci
    id="S4.I1.i1.p1.1.m1.4.4.1.1.3.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>)
    will probably fail due to a lack of theoretical guarantees under this setting,
    see Sections [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent learning
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and [4.1](#S4.SS1 "4.1
    Avoiding deep learning amnesia: examples in MDRL ‣ 4 Bridging RL, MAL and MDRL
    ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”"). *Adding information in
    the experience tuple* that can help disambiguate the sample is the solution adopted
    in many works, whether a value based method [[168](#bib.bib168), [35](#bib.bib35),
    [166](#bib.bib166), [178](#bib.bib178)] or a policy gradient method [[63](#bib.bib63)].
    In this regard, it is an open question to consider how new DRL ideas could be
    best integrated into the ER [[286](#bib.bib286), [111](#bib.bib111), [287](#bib.bib287),
    [288](#bib.bib288), [96](#bib.bib96)] and how those ideas would fare in a MDRL
    setting.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Centralized learning with decentralized execution. Many MAL works were either
    fully centralized or fully decentralized approaches. However, inspired by *decentralized
    partially observable Markov decison processes* (DEC-POMDPs) [[289](#bib.bib289),
    [290](#bib.bib290)],^(18)^(18)18Centralized planning and decentralized execution
    is also a standard paradigm for multiagent planning [[291](#bib.bib291)]. in MDRL
    this new mixed paradigm has been commonly used  [[168](#bib.bib168), [35](#bib.bib35),
    [181](#bib.bib181), [172](#bib.bib172), [167](#bib.bib167), [63](#bib.bib63)]
    (a notable exception are DEC-HDRQNs [[166](#bib.bib166)] which perform learning
    and execution in a decentralized manner, see Section [3.5](#S3.SS5 "3.5 Learning
    cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). Note that not all real-world problems fit
    into this paradigm and it is more common for robotics or games where a simulator
    is generally available [[162](#bib.bib162)]. The main benefit is that during learning
    *additional information can be used* (e.g., global state, action, or rewards)
    and during execution this information is removed.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '集中学习与分散执行。许多MAL研究要么完全集中，要么完全分散。然而，受到*部分可观察的马尔可夫决策过程*（DEC-POMDPs）[[289](#bib.bib289),
    [290](#bib.bib290)]的启发，^(18)^(18)18集中规划与分散执行也是多智能体规划的标准范式[[291](#bib.bib291)]。在MDRL中，这种新的混合范式已被广泛使用[[168](#bib.bib168),
    [35](#bib.bib35), [181](#bib.bib181), [172](#bib.bib172), [167](#bib.bib167),
    [63](#bib.bib63)]（一个显著的例外是DEC-HDRQNs[[166](#bib.bib166)]，它们以分散的方式进行学习和执行，见第[3.5](#S3.SS5
    "3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”)节）。注意，并非所有现实世界的问题都适合这种范式，这种方法在机器人或游戏中更为常见，因为这些领域通常有可用的模拟器[[162](#bib.bib162)]。其主要优点是，在学习期间可以使用*额外的信息*（例如，全局状态、动作或奖励），而在执行过程中，这些信息被移除。'
- en: '3.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: Parameter sharing. Another frequent component in many MDRL works is the idea
    of sharing parameters, i.e., training a single network in which agents share their
    weights. Note that, since agents could receive different observations (e.g., in
    partially observable scenarios), they can still behave differently. This method
    was proposed concurrently in different works [[292](#bib.bib292), [162](#bib.bib162)]
    and later it has been successfully applied in many others [[163](#bib.bib163),
    [164](#bib.bib164), [168](#bib.bib168), [180](#bib.bib180), [181](#bib.bib181)].
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数共享。许多MDRL研究中的另一个常见组件是共享参数的思想，即训练一个网络，其中智能体共享它们的权重。注意，由于智能体可能接收到不同的观察（例如，在部分可观察的场景中），它们仍然可能表现得不同。这种方法在不同的研究中同时提出[[292](#bib.bib292),
    [162](#bib.bib162)]，后来在许多其他研究中成功应用[[163](#bib.bib163), [164](#bib.bib164), [168](#bib.bib168),
    [180](#bib.bib180), [181](#bib.bib181)]。
- en: '4.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Recurrent networks. Recurrent neural networks (RNNs) enhanced neural networks
    with a memory capability, however, they suffer from the vanishing gradient problem,
    which renders them inefficient for long-term dependencies [[293](#bib.bib293)].
    However, RNN variants such as LSTMs [[86](#bib.bib86), [294](#bib.bib294)] and
    GRUs (Gated Recurrent Unit) [[295](#bib.bib295)] addressed this challenge. In
    single-agent DRL, DRQN [[85](#bib.bib85)] initially proposed idea of using recurrent
    networks in single-agent *partially observable* environments. Then, Feudal Networks [[229](#bib.bib229)]
    proposed a hierarchical approach [[230](#bib.bib230)], *multiple LSTM networks
    with different time-scales*, i.e., the observation input schedule is different
    for each LSTM network, to create a temporal hierarchy so that it can better address
    the long-term credit assignment challenge for RL problems. Recently, the use of
    recurrent networks has been extended to MDRL to address the challenge of partially
    observability [[158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164), [166](#bib.bib166),
    [180](#bib.bib180), [181](#bib.bib181), [170](#bib.bib170), [171](#bib.bib171),
    [174](#bib.bib174)] for example, in FTW [[179](#bib.bib179)], depicted in Figure [5](#S3.F5
    "Figure 5 ‣ 3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”") and DRPIRQN [[171](#bib.bib171)]
    depicted in Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”"). See Section [4.4](#S4.SS4 "4.4 Practical challenges in MDRL
    ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”")
    for practical challenges (e.g., training issues) of recurrent networks in MDRL.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '循环网络。循环神经网络（RNNs）增强了神经网络的记忆能力，但它们遭遇了梯度消失问题，这使得它们在处理长期依赖关系时效率低下[[293](#bib.bib293)]。然而，RNN变体如LSTMs[[86](#bib.bib86),
    [294](#bib.bib294)]和GRUs（门控循环单元）[[295](#bib.bib295)]解决了这一挑战。在单一智能体DRL中，DRQN[[85](#bib.bib85)]最初提出了在单智能体*部分可观测*环境中使用循环网络的想法。然后，Feudal
    Networks[[229](#bib.bib229)]提出了一种分层方法[[230](#bib.bib230)]，即*多个具有不同时间尺度的LSTM网络*，即每个LSTM网络的观察输入时间表不同，以创建时间层级，从而更好地解决RL问题中的长期信用分配挑战。最近，循环网络的使用已扩展到MDRL，以解决*部分可观测性*的挑战[[158](#bib.bib158),
    [162](#bib.bib162), [164](#bib.bib164), [166](#bib.bib166), [180](#bib.bib180),
    [181](#bib.bib181), [170](#bib.bib170), [171](#bib.bib171), [174](#bib.bib174)]，例如，FTW[[179](#bib.bib179)]，如图[5](#S3.F5
    "Figure 5 ‣ 3.5 Learning cooperation ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")和DRPIRQN[[171](#bib.bib171)]，如图[6](#S3.F6
    "Figure 6 ‣ 3.6 Agents modeling agents ‣ 3 Multiagent Deep Reinforcement Learning
    (MDRL) ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")。有关MDRL中循环网络的实际挑战（例如训练问题），请参见第[4.4](#S4.SS4
    "4.4 Practical challenges in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”")节。'
- en: '5.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: Overfitting in MAL. In single-agent RL, agents can overfit to the environment [[296](#bib.bib296)].
    A similar problem can occur in multiagent settings [[254](#bib.bib254)], agents
    can overfit, i.e., an agent’s policy can easily get stuck in a local optima and
    the learned policy may be only locally optimal to other agents’ current policies [[183](#bib.bib183)].
    This has the effect of limiting the generalization of the learned policies [[172](#bib.bib172)].
    To reduce this problem, a solution is to have a set of policies (an ensemble)
    and learn from them or best respond to the mixture of them [[172](#bib.bib172),
    [63](#bib.bib63), [169](#bib.bib169)]. Another solution has been to robustify
    algorithms — a robust policy should be able to behave well even with strategies
    different from its training (better generalization) [[183](#bib.bib183)].
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 MAL 中的过拟合。在单代理强化学习中，代理可能会过拟合环境 [[296](#bib.bib296)]。在多代理设置中，也可能出现类似的问题，代理可能会过拟合，即代理的策略可能会轻易陷入局部最优，所学策略可能仅对其他代理的当前策略局部最优 [[183](#bib.bib183)]。这会限制所学策略的泛化能力 [[172](#bib.bib172)]。为减少此问题，一种解决方案是使用一组策略（集成）并从中学习或对其混合进行最佳响应 [[172](#bib.bib172),
    [63](#bib.bib63), [169](#bib.bib169)]。另一种解决方案是增强算法的鲁棒性——一个鲁棒的策略应该能够在与其训练策略不同的情况下表现良好（更好的泛化） [[183](#bib.bib183)]。
- en: 4.3 Benchmarks for MDRL
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 MDRL的基准
- en: Standardized environments such as the Arcade Learning Environment (ALE) [[297](#bib.bib297),
    [298](#bib.bib298)] and OpenAI Gym [[299](#bib.bib299)] have allowed single-agent
    RL to move beyond toy domains. For DRL there are open-source frameworks that provide
    compact and reliable implementations of some state-of-the-art DRL algorithms [[300](#bib.bib300)].
    Even though MDRL is a recent area, there are now a number of open sourced simulators
    and benchmarks to use with different characteristics, which we describe below.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化环境如 Arcade Learning Environment (ALE) [[297](#bib.bib297), [298](#bib.bib298)]
    和 OpenAI Gym [[299](#bib.bib299)] 使得单代理强化学习可以超越玩具领域。对于深度强化学习（DRL），有开源框架提供了一些最先进的
    DRL 算法的紧凑而可靠的实现 [[300](#bib.bib300)]。尽管多代理强化学习（MDRL）是一个较新的领域，但现在有许多开源的模拟器和基准可以使用，我们将在下文中描述它们的不同特性。
- en: '![Refer to caption](img/91df0d0e84687b786d6dc17105cf6138.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/91df0d0e84687b786d6dc17105cf6138.png)'
- en: (a) Multiagent object transportation
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 多代理物体运输
- en: '![Refer to caption](img/7d77570179eaad9ecdf463801adbe644.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7d77570179eaad9ecdf463801adbe644.png)'
- en: (b) Pommerman
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Pommerman
- en: 'Figure 7: (a) A fully cooperative benchmark with two agents, Multiagent Object
    Trasportation. (b) A mixed cooperative-competitive domain with four agents, Pommerman.
    For more MDRL benchmarks see Section [4.3](#S4.SS3 "4.3 Benchmarks for MDRL ‣
    4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: “Is
    multiagent deep reinforcement learning the answer or the question? A brief survey”").'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '图7: (a) 一个完全合作的基准，包含两个代理的多代理物体运输。 (b) 一个混合合作-竞争领域，包含四个代理的 Pommerman。更多 MDRL
    基准见第 [4.3](#S4.SS3 "4.3 MDRL的基准 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多代理深度强化学习的调查与批评1脚注 11脚注
    1 本工作早期版本的标题为：“多代理深度强化学习是答案还是问题？简要调查”")节。'
- en: '1.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Fully Cooperative Multiagent Object Transporation Problems (CMOTPs)^(19)^(19)19[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    were originally presented by Busoniu et al. [[36](#bib.bib36)] as a simple two-agent
    coordination problem in MAL. Palmer et al. [[35](#bib.bib35)] proposed two pixel-based
    extensions to the original setting which include narrow passages that test the
    agents’ ability to master fully-cooperative sub-tasks, stochastic rewards and
    noisy observations, see Figure [7(a)](#S4.F7.sf1 "In Figure 7 ‣ 4.3 Benchmarks
    for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”").'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完全合作的多代理物体运输问题 (CMOTPs)^(19)^(19)19[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    最初由 Busoniu 等人提出 [[36](#bib.bib36)]，作为 MAL 中一个简单的双代理协调问题。Palmer 等人 [[35](#bib.bib35)]
    提出了对原始设置的两个基于像素的扩展，包括测试代理掌握完全合作子任务的狭窄通道、随机奖励和嘈杂观察的环境，见图 [7(a)](#S4.F7.sf1 "在图7
    ‣ 4.3 MDRL的基准 ‣ 4 连接 RL、MAL 和 MDRL ‣ 多代理深度强化学习的调查与批评1脚注 11脚注 1 本工作早期版本的标题为：“多代理深度强化学习是答案还是问题？简要调查”")。
- en: '2.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'The Apprentice Firemen Game^(20)^(20)20[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    (inspired by the classic climb game [[126](#bib.bib126)]) is another two-agent
    pixel-based environment that simultaneously confronts learners with four pathologies
    in MAL: relative overgeneralization, stochasticity, the moving target problem,
    and alter exploration problem [[97](#bib.bib97)].'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学徒消防员游戏^(20)^(20)20[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)（灵感来源于经典的攀爬游戏 [[126](#bib.bib126)]）是另一个双智能体像素基础环境，给学习者同时带来四种MAL病态：相对过度概括、随机性、移动目标问题和改变探索问题 [[97](#bib.bib97)]。
- en: '3.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Pommerman [[301](#bib.bib301)] is a multiagent benchmark useful for testing
    cooperative, competitive and mixed (cooperative and competitive) scenarios. It
    supports partial observability and communication among agents, see Figure [7(b)](#S4.F7.sf2
    "In Figure 7 ‣ 4.3 Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”"). Pommerman is a very challenging
    domain from the exploration perspective as the rewards are very sparse and delayed [[302](#bib.bib302)].
    A recent competition was held during NeurIPS-2018^(21)^(21)21[https://www.pommerman.com/](https://www.pommerman.com/)
    and the top agents from that competition are available for training purposes.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pommerman [[301](#bib.bib301)] 是一个多智能体基准，适用于测试合作、竞争和混合（合作与竞争）场景。它支持部分可观察性和智能体之间的通信，见图 [7(b)](#S4.F7.sf2
    "在图7 ‣ 4.3 MDRL基准 ‣ 4 连接RL、MAL和MDRL ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")。从探索角度来看，Pommerman是一个非常具有挑战性的领域，因为奖励非常稀疏且延迟 [[302](#bib.bib302)]。最近的一场竞赛在NeurIPS-2018^(21)^(21)21[https://www.pommerman.com/](https://www.pommerman.com/)
    举行，竞赛中的顶级智能体可用于训练目的。
- en: '4.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: Starcraft Multiagent Challenge [[303](#bib.bib303)] is based on the real-time
    strategy game StarCraft II and focuses on micromanagement challenges,^(22)^(22)22[https://github.com/oxwhirl/smac](https://github.com/oxwhirl/smac)
    that is, fine-grained control of individual units, where each unit is controlled
    by an independent agent that must act based on local observations. It is accompanied
    by a MDRL framework including state-of-the-art algorithms (e.g., QMIX and COMA).^(23)^(23)23[https://github.com/oxwhirl/pymarl](https://github.com/oxwhirl/pymarl)
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 星际争霸多智能体挑战 [[303](#bib.bib303)] 基于实时战略游戏《星际争霸II》，专注于微观管理挑战，^(22)^(22)22[https://github.com/oxwhirl/smac](https://github.com/oxwhirl/smac)
    即对单个单位的精细控制，每个单位由一个独立的智能体控制，必须根据局部观察做出行动。它配有一个包括最新算法（例如QMIX和COMA）的MDRL框架。^(23)^(23)23[https://github.com/oxwhirl/pymarl](https://github.com/oxwhirl/pymarl)
- en: '5.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: The Multi-Agent Reinforcement Learning in Malmö (MARLÖ) competition [[304](#bib.bib304)]
    is another multiagent challenge with multiple cooperative 3D games^(24)^(24)24[https://github.com/crowdAI/marlo-single-agent-starter-kit/](https://github.com/crowdAI/marlo-single-agent-starter-kit/)
    within Minecraft. The scenarios were created with the open source Malmö platform [[305](#bib.bib305)],
    providing examples of how a wider range of multiagent cooperative, competitive
    and mixed scenarios can be experimented on within Minecraft.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 马尔默的多智能体强化学习（MARLÖ）竞赛 [[304](#bib.bib304)] 是另一个多智能体挑战，涉及多个合作的3D游戏^(24)^(24)24[https://github.com/crowdAI/marlo-single-agent-starter-kit/](https://github.com/crowdAI/marlo-single-agent-starter-kit/)
    在Minecraft中进行。这些场景使用开源的马尔默平台创建 [[305](#bib.bib305)]，提供了在Minecraft中实验更广泛的多智能体合作、竞争和混合场景的示例。
- en: '6.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6.'
- en: Hanabi is a cooperative multiplayer card game (two to five players). The main
    characteristic of the game is that players do not observe their own cards but
    other players can reveal information about them. This makes an interesting challenge
    for learning algorithms in particular in the context of self-play learning and
    ad-hoc teams [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)]. The
    Hanabi Learning Environment [[309](#bib.bib309)] was recently released^(25)^(25)25[https://github.com/deepmind/hanabi-learning-environment](https://github.com/deepmind/hanabi-learning-environment)
    and it is accompanied with a baseline (deep RL) agent [[310](#bib.bib310)].
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hanabi 是一个合作多人纸牌游戏（2到5名玩家）。游戏的主要特点是玩家无法观察自己的纸牌，但其他玩家可以透露有关他们的信息。这对学习算法，特别是在自我对弈学习和临时团队的背景下，提出了有趣的挑战。Hanabi
    Learning Environment [[309](#bib.bib309)] 最近发布^(25)^(25)25[https://github.com/deepmind/hanabi-learning-environment](https://github.com/deepmind/hanabi-learning-environment)，并且配有一个基线（深度强化学习）代理 [[310](#bib.bib310)]。
- en: '7.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7.'
- en: Arena [[311](#bib.bib311)] is platform for multiagent research^(26)^(26)26[https://github.com/YuhangSong/Arena-BuildingToolkit](https://github.com/YuhangSong/Arena-BuildingToolkit)
    based on the Unity engine [[312](#bib.bib312)]. It has 35 multiagent games (e.g.,
    social dilemmas) and supports communication among agents. It has basseline implementations
    of recent DRL algorithms such as independent PPO learners.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Arena [[311](#bib.bib311)] 是一个基于 Unity 引擎的多智能体研究平台^(26)^(26)26[https://github.com/YuhangSong/Arena-BuildingToolkit](https://github.com/YuhangSong/Arena-BuildingToolkit)。它包含35个多智能体游戏（例如社会困境），并支持智能体之间的通信。它还包括一些最新的深度强化学习算法的基线实现，如独立PPO学习者。
- en: '8.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8.'
- en: MuJoCo Multiagent Soccer [[313](#bib.bib313)] uses the MuJoCo physics engine [[202](#bib.bib202)].
    The environment simulates a 2 vs. 2 soccer game with agents having a 3-dimensional
    action space.^(27)^(27)27[https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer)
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MuJoCo Multiagent Soccer [[313](#bib.bib313)] 使用 MuJoCo 物理引擎 [[202](#bib.bib202)]。该环境模拟了一个2对2的足球比赛，智能体具有三维动作空间。^(27)^(27)27[https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer)
- en: '9.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '9.'
- en: Neural MMO [[314](#bib.bib314)] is a research platform^(28)^(28)28[https://github.com/openai/neural-mmo](https://github.com/openai/neural-mmo)
    inspired by the human game genre of Massively Multiplayer Online (MMO) Role-Playing
    Games. These games involve a large, variable number of players competing to survive.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Neural MMO [[314](#bib.bib314)] 是一个受大规模多人在线（MMO）角色扮演游戏启发的研究平台^(28)^(28)28[https://github.com/openai/neural-mmo](https://github.com/openai/neural-mmo)。这些游戏涉及大量可变数量的玩家在竞争中求生存。
- en: 4.4 Practical challenges in MDRL
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 MDRL中的实际挑战
- en: In this section we take a more critical view with respect to MDRL and highlight
    different practical challenges that already happen in DRL and that are likely
    to occur in MDRL such as reproducibility, hyperparameter tuning, the need of computational
    resources and conflation of results. We provide pointers on how we think those
    challenges could be (partially) addressed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们对 MDRL 采取了更加批判的视角，并突出了 DRL 中已经出现的以及可能在 MDRL 中出现的不同实际挑战，如可重复性、超参数调整、计算资源需求和结果混淆。我们提供了关于如何（部分）解决这些挑战的建议。
- en: Reproducibility, troubling trends and negative results
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可重复性、令人担忧的趋势和负面结果
- en: 'Reproducibility is a challenge in RL which is only aggravated in DRL due to
    different sources of stochasticity: baselines, hyperparameters, architectures [[315](#bib.bib315),
    [316](#bib.bib316)] and random seeds [[317](#bib.bib317)]. Moreover, DRL does
    not have common practices for statistical testing [[318](#bib.bib318)] which has
    led to bad practices such as only reporting the results when algorithms perform
    well, sometimes referred as *cherry picking* [[319](#bib.bib319)] (Azizzadenesheli
    also describes *cherry planting* as adapting an environment to a specific algorithm [[319](#bib.bib319)]).
    We believe that together with following the advice on how to design experiments
    and report results [[320](#bib.bib320)], the community would also benefit from
    reporting *negative results* [[321](#bib.bib321), [322](#bib.bib322), [318](#bib.bib318),
    [323](#bib.bib323)] for carefully designed hypothesis and experiments.^(29)^(29)29This
    idea was initially inspired by the Workshop “Critiquing and Correcting Trends
    in Machine Learning” at NeurIPS 2018 where it was possible to submit *Negative
    results* papers: “Papers which show failure modes of existing algorithms or suggest
    new approaches which one might expect to perform well but which do not. The aim
    is to provide a venue for work which might otherwise go unpublished but which
    is still of interest to the community.” [https://ml-critique-correct.github.io/](https://ml-critique-correct.github.io/)
    However, we found very few papers with this characteristic[[324](#bib.bib324),
    [325](#bib.bib325), [326](#bib.bib326)] — we note that this is not encouraged
    in the ML community; moreover, negative results reduce the chance of paper acceptance [[320](#bib.bib320)].
    In this regard, we ask the community to reflect on these practices and find ways
    to remove these obstacles.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，可重复性是一个挑战，在深度强化学习（DRL）中由于不同的随机因素（如基线、超参数、架构[[315](#bib.bib315), [316](#bib.bib316)]
    和随机种子[[317](#bib.bib317)]）而被进一步加剧。此外，DRL 缺乏统计测试的共同实践[[318](#bib.bib318)]，这导致了不良实践，比如只有在算法表现良好时才报告结果，有时被称为*选择性报告*[[319](#bib.bib319)]（Azizzadenesheli
    还将*环境调整*描述为将环境适应特定算法[[319](#bib.bib319)]）。我们相信，除了遵循关于如何设计实验和报告结果的建议[[320](#bib.bib320)]，社区也会从报告*负面结果*[[321](#bib.bib321),
    [322](#bib.bib322), [318](#bib.bib318), [323](#bib.bib323)]中受益，这些结果是针对精心设计的假设和实验的。^(29)^(29)29这一想法最初受到了
    NeurIPS 2018 研讨会“批判和纠正机器学习趋势”的启发，当时可以提交*负面结果*论文：“展示现有算法失败模式或提出期望表现良好的新方法但未达到预期的论文。目的是为可能否则无法发表但仍对社区感兴趣的工作提供一个平台。”
    [https://ml-critique-correct.github.io/](https://ml-critique-correct.github.io/)
    然而，我们发现具有这种特征的论文非常少[[324](#bib.bib324), [325](#bib.bib325), [326](#bib.bib326)]——我们注意到这在机器学习社区中并不受鼓励；此外，负面结果减少了论文被接受的机会[[320](#bib.bib320)]。在这方面，我们请求社区反思这些实践并寻找去除这些障碍的方法。
- en: Implementation challenges and hyperparameter tuning
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现挑战和超参数调优
- en: 'One problem is that canonical implementations of DRL algorithms often contain
    additional non-trivial optimizations — these are sometimes necessary for the algorithms
    to achieve good performance [[79](#bib.bib79)]. A recent study by Tucker et al. [[59](#bib.bib59)]
    found that several published works on action-dependant baselines contained bugs
    and errors — those were the real reason of the high performance in the experimental
    results, not the proposed method. Melis et al. [[327](#bib.bib327)] compared a
    series of works with increasing innovations in network architectures and the vanilla
    LSTMs [[86](#bib.bib86)] (originally proposed in 1997). The results showed that,
    when properly tuned, LSTMs outperformed the more recent models. In this context,
    Lipton and Steinhardt noted that the community may have benefited more by learning
    the details of the hyperparameter tuning [[320](#bib.bib320)]. A partial reason
    for this surprising result might be that this type of networks are known for being
    difficult to train [[293](#bib.bib293)] and there are recent works in DRL that
    report problems when using recurrent networks [[182](#bib.bib182), [328](#bib.bib328),
    [329](#bib.bib329), [330](#bib.bib330)]. Another known complication is catastrophic
    forgetting (see Section [2.2](#S2.SS2 "2.2 Deep reinforcement learning ‣ 2 Single-agent
    learning ‣ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")) with recent examples in
    DRL [[157](#bib.bib157), [92](#bib.bib92)] — we expect that these issues would
    likely occur in MDRL. The effects of hyperparameter tuning were analyzed in more
    detail in DRL by Henderson et al. [[315](#bib.bib315)], who arrived at the conclusion
    that hyperparameters can have significantly different effects across algorithms
    (they tested TRPO, DDPG, PPO and ACKTR) and environments since there is an intricate
    interplay among them [[315](#bib.bib315)]. The authors urge the community to report
    *all* parameters used in the experimental evaluations for accurate comparison
    — we encourage a similar behavior for MDRL. Note that hyperparameter tuning is
    related to the troubling trend of cherry picking in that it can show a carefully
    picked set of parameters that make an algorithm work (see previous challenge).
    Lastly, note that hyperparameter tuning is computationally very expensive, which
    relates to the connection with the following challenge of computational demands.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是，DRL算法的标准实现通常包含额外的非平凡优化——这些优化有时是算法实现良好性能所必需的[[79](#bib.bib79)]。Tucker等人最近的一项研究[[59](#bib.bib59)]发现，一些关于动作依赖基线的已发表工作包含了错误和缺陷——这些才是实验结果中高性能的真正原因，而不是所提出的方法。Melis等人[[327](#bib.bib327)]比较了一系列具有不断创新的网络架构的工作与原始LSTMs[[86](#bib.bib86)]（最初在1997年提出）。结果显示，当适当调整时，LSTMs的表现优于更新的模型。在这种情况下，Lipton和Steinhardt指出，社区可能从学习超参数调整的细节中获益更多[[320](#bib.bib320)]。这种令人惊讶结果的部分原因可能是这类网络通常难以训练[[293](#bib.bib293)]，且最近的DRL工作报告了使用递归网络时的问题[[182](#bib.bib182),
    [328](#bib.bib328), [329](#bib.bib329), [330](#bib.bib330)]。另一个已知的复杂问题是灾难性遗忘（见第[2.2](#S2.SS2
    "2.2 深度强化学习 ‣ 2 单智能体学习 ‣ 多智能体深度强化学习的调查与批判1脚注 11脚注 1早期版本标题为：“多智能体深度强化学习是答案还是问题？简要调查”")节），DRL中有近期的例子[[157](#bib.bib157),
    [92](#bib.bib92)]——我们预计这些问题也可能在MDRL中发生。Henderson等人[[315](#bib.bib315)]对DRL中的超参数调整效果进行了更详细的分析，得出的结论是超参数在不同算法（他们测试了TRPO、DDPG、PPO和ACKTR）和环境中可能有显著不同的效果，因为它们之间存在复杂的相互作用[[315](#bib.bib315)]。作者敦促社区报告实验评估中使用的*所有*参数以进行准确比较——我们鼓励对MDRL采取类似的做法。请注意，超参数调整与令人担忧的挑选趋势相关，因为它可能显示出一组精心挑选的参数使算法有效（见前述挑战）。最后，请注意，超参数调整计算成本非常高，这与接下来讨论的计算需求挑战相关。
- en: Computational resources
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算资源
- en: Deep RL usually requires millions of interactions for an agent to learn [[331](#bib.bib331)],
    i.e., low sample efficiency [[332](#bib.bib332)], which highlights the need for
    large computational infrastructure in general. The original A3C implementation [[93](#bib.bib93)]
    uses 16 CPU workers for 4 days to learn to play an Atari game with a total of
    200M training frames^(30)^(30)30It is sometimes unclear in the literature what
    is the meaning of frame due to the “frame skip” technique. It is therefore suggested
    to refer to “game frames” and “training frames” [[333](#bib.bib333)]. (results
    are reported for 57 Atari games). Distributed PPO used 64 workers (presumably
    one CPU per worker, although this is not clearly stated in the paper) for 100
    hours (more than 4 days) to learn locomotion tasks [[117](#bib.bib117)]. In MDRL,
    for example, the Atari Pong game, agents were trained for 50 epochs, 250k time
    steps each, for a total of 1.25M training frames [[155](#bib.bib155)]. The FTW
    agent [[179](#bib.bib179)] uses 30 agents (processes) in parallel and every training
    game lasts for five minues; FTW agents were trained for approximately 450K games
    <math id="S4.SS4.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics
    id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">\approx</annotation></semantics></math>4.2
    years. These examples highlight the computational demands sometimes needed within
    DRL and MDRL.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习通常需要数百万次交互才能让代理学习[[331](#bib.bib331)]，即样本效率低[[332](#bib.bib332)]，这突出了一般需要大规模计算基础设施的需求。原始的A3C实现[[93](#bib.bib93)]使用了16个CPU工作节点，经过4天的训练来学习玩一个Atari游戏，总共使用了200M训练帧^(30)^(30)30由于“帧跳过”技术，文献中有时不清楚帧的具体含义。因此建议参考“游戏帧”和“训练帧”[[333](#bib.bib333)]。（结果是针对57个Atari游戏报告的）。分布式PPO使用了64个工作节点（假设每个工作节点一个CPU，尽管论文中未明确说明）进行了100小时（超过4天）的训练来学习运动任务[[117](#bib.bib117)]。在MDRL中，例如，Atari
    Pong游戏，代理经过50轮训练，每轮250k时间步，总共1.25M训练帧[[155](#bib.bib155)]。FTW代理[[179](#bib.bib179)]使用了30个并行代理（进程），每个训练游戏持续五分钟；FTW代理训练了大约450K个游戏
    <math id="S4.SS4.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics
    id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml">≈</mo><annotation
    encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">\approx</annotation></semantics></math>4.2年。这些例子突出了DRL和MDRL中有时所需的计算需求。
- en: 'Recent works have reduced the learning of an Atari game to minutes (Stooke
    and Abbeel [[334](#bib.bib334)] trained DRL agents in less than one hour with
    hardware consisting of 8 GPUs and 40 cores). However, this is (for now) the exception
    and computational infrastructure is a major bottleneck for doing DRL and MDRL,
    especially for those who do not have such large compute power (e.g., most companies
    and most academic research groups) [[212](#bib.bib212), [322](#bib.bib322)].^(31)^(31)31One
    recent effort by Beeching et al. [[212](#bib.bib212)] proposes to use only “mid-range
    hardware” (8 CPUs and 1 GPU) to train deep RL agents. Within this context we propose
    two ways to address this problem. (1) Raising awareness: For DRL we found few
    works that study the computational demands of recent algorithms [[335](#bib.bib335),
    [331](#bib.bib331)]. For MDRL most published works do not provide information
    regarding computational resources used such as CPU/GPU usage, memory demands,
    and wall-clock computation. Therefore, the first way to tackle this issue is by
    raising awareness and encouraging authors to report metrics about computational
    demands for accurately comparison and evaluation. (2) Delve into algorithmic contributions.
    Another way to address these issues is to prioritize the algorithmic contribution
    for the new MDRL algorithms rather than the computational resources spent. Indeed,
    for this to work, it needs to be accompanied with high-quality reviewers.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究将 Atari 游戏的学习时间缩短到了分钟级别（Stooke 和 Abbeel [[334](#bib.bib334)] 在不到一小时内用包含
    8 个 GPU 和 40 个核心的硬件训练了 DRL 代理）。然而，这（目前）是一个例外，计算基础设施是进行 DRL 和 MDRL 的主要瓶颈，尤其是对于那些没有如此大计算能力的机构（例如，大多数公司和大多数学术研究团队）[[212](#bib.bib212),
    [322](#bib.bib322)]。^(31)^(31)31 Beeching 等人 [[212](#bib.bib212)] 最近的一项努力提议仅使用“中档硬件”（8
    个 CPU 和 1 个 GPU）来训练深度 RL 代理。在这种背景下，我们提出两种方法来解决这个问题。 (1) 提高认识：对于 DRL，我们发现很少有研究研究最近算法的计算需求[[335](#bib.bib335),
    [331](#bib.bib331)]。对于 MDRL，大多数已发表的工作没有提供关于计算资源使用的信息，如 CPU/GPU 使用情况、内存需求和实际计算时间。因此，解决这个问题的第一种方法是提高认识，并鼓励作者报告计算需求的指标，以便进行准确的比较和评估。
    (2) 深入算法贡献。另一种解决这些问题的方法是优先考虑新 MDRL 算法的算法贡献，而不是计算资源的花费。的确，要实现这一点，需要配合高质量的审稿人。
- en: 'We have argued to raise awareness on the computational demands and report results,
    however, there is still the open question on *how* and *what* to measure/report.
    There are several dimensions to measure efficiency: sample efficiency is commonly
    measured by counting state-action pairs used for training; computational efficiency
    could be measured by number of CPUs/GPUs and days used for training. How do we
    measure the impact of other resources, such as external data sources or annotations?^(32)^(32)32NeurIPS
    2019 hosts the “MineRL Competition on Sample Efficient Reinforcement Learning
    using Human Priors” where the primary goal of the competition is to foster the
    development of algorithms which can efficiently leverage human demonstrations
    to drastically reduce the number of samples needed to solve complex, hierarchical,
    and sparse environments [[336](#bib.bib336)]. Similarly, do we need to differentiate
    the computational needs of the algorithm itself versus the environment it is run
    in? We do not have the answers, however, we point out that current standard metrics
    might not be entirely comprehensive.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了提高对计算需求的认识并报告结果的问题，但仍然存在*如何*和*什么*来衡量/报告的开放性问题。有几个维度可以用来衡量效率：样本效率通常通过计算用于训练的状态-动作对来衡量；计算效率可以通过用于训练的CPU/GPUs数量和天数来衡量。我们如何衡量其他资源的影响，比如外部数据源或注释？^(32)^(32)32NeurIPS
    2019 举办了“利用人类先验的样本高效强化学习 MineRL 竞赛”，该竞赛的主要目标是促进能够高效利用人类演示以显著减少解决复杂、分层和稀疏环境所需样本数量的算法的发展[[336](#bib.bib336)]。类似地，我们是否需要区分算法本身的计算需求与运行环境的计算需求？我们没有答案，但我们指出，目前的标准度量可能并不完全全面。
- en: In the end, we believe that high compute based methods act as a frontier to
    showcase benchmarks [[19](#bib.bib19), [18](#bib.bib18)], i.e., they show what
    results are possible as data and compute is scaled up (e.g., OpenAI Five generates
    180 years of gameplay data each day using 128,000 CPU cores and 256 GPUs [[18](#bib.bib18)];
    AlphaStar uses 200 years of Starcraft II gameplay [[19](#bib.bib19)]); however,
    lighter compute based algorithmic methods can also yield significant contributions
    to better tackle real-world problems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们认为高计算方法作为一个前沿，展示了基准测试[[19](#bib.bib19), [18](#bib.bib18)]，即它们展示了随着数据和计算的增加（例如，OpenAI
    Five 每天使用128,000个CPU核心和256个GPU生成180年的游戏数据[[18](#bib.bib18)]; AlphaStar使用200年的Starcraft
    II游戏数据[[19](#bib.bib19)]）可能实现的结果。然而，较轻的计算基础算法方法也能对更好地解决现实世界问题做出显著贡献。
- en: Occam’s razor and ablative analysis
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奥卡姆剃刀与消融分析
- en: 'Finding the simplest context that exposes the innovative research idea remains
    challenging, and if ignored leads to a conflation of fundamental research (working
    principles in the most abstract setting) and applied research (working systems
    as complete as possible). In particular, some deep learning papers are presented
    as learning from pixels without further explanation, while object-level representations
    would have already exposed the algorithmic contribution. This still makes sense
    to remain comparable with established benchmarks (e.g., OpenAI Gym [[299](#bib.bib299)]),
    but less so if custom simulations are written without open source access, as it
    introduces unnecessary variance in pixel-level representations and artificially
    inflates computational resources (see previous point about *computational resources*).^(33)^(33)33Cuccu,
    Togelius and Cudré-Mauroux achieved state-of-the-art policy learning in Atari
    games with only 6 to 18 neurons [[337](#bib.bib337)]. The main idea was to decouple
    image processing from decision-making. In this context there are some notable
    exceptions where the algorithmic contribution is presented in a minimal setting
    and then results are scaled into complex settings: LOLA [[64](#bib.bib64)] first
    presented a minimalist setting with a two-player two-action game and then with
    a more complex variant; similarly, QMIX [[181](#bib.bib181)] presented its results
    in a two-step (matrix) game and then in the more involved Starcraft II micromanagement
    domain [[303](#bib.bib303)].'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 找到揭示创新研究思想的最简单背景仍然具有挑战性，如果忽视这一点，将导致基础研究（在最抽象的设置中工作的原理）和应用研究（尽可能完整的工作系统）之间的混淆。特别是，一些深度学习论文被呈现为从像素中学习而没有进一步解释，而对象级表示本可以揭示算法贡献。这仍然有意义，以便与既定基准（例如，OpenAI
    Gym[[299](#bib.bib299)]）保持可比性，但如果自定义模拟没有开放源代码访问，则不那么合理，因为这会引入不必要的像素级表示变异，并人为地增加计算资源（参见关于*计算资源*的前一点）。^(33)^(33)33Cuccu、Togelius和Cudré-Mauroux在Atari游戏中仅用6到18个神经元实现了最先进的策略学习[[337](#bib.bib337)]。主要思想是将图像处理与决策制定解耦。在这种情况下，有一些显著的例外，其中算法贡献在最小设置中呈现，然后结果扩展到复杂设置：LOLA[[64](#bib.bib64)]首先在一个两玩家两动作的游戏中呈现了一个极简设置，然后是一个更复杂的变体；类似地，QMIX[[181](#bib.bib181)]在一个两步（矩阵）游戏中展示了其结果，然后在更复杂的Starcraft
    II微观管理领域[[303](#bib.bib303)]中展示了结果。
- en: 4.5 Open questions
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 未解问题
- en: 'Finally, here we present some open questions for MDRL and point to suggestions
    on how to approach them. We believe that there are solid ideas in earlier literature
    and we refer the reader to Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") to avoid deep learning amnesia.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提出了一些关于MDRL的未解问题，并指出了如何应对这些问题的建议。我们相信，早期文献中有可靠的思想，我们建议读者参考第[4.1节](#S4.SS1
    "4.1 避免深度学习遗忘：MDRL中的示例 ‣ 4 连接RL、MAL和MDRL ‣ 多智能体深度强化学习的调查与批评1脚注 11脚注 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")以避免深度学习遗忘。
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: On the challenge of sparse and delayed rewards.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于稀疏和延迟奖励的挑战。
- en: 'Recent MDRL competitions and environments have complex scenarios where many
    actions are taken before a reward signal is available (see Section [4.3](#S4.SS3
    "4.3 Benchmarks for MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: “Is multiagent deep reinforcement learning the answer
    or the question? A brief survey”")). This sparseness is already a challenge for
    RL [[20](#bib.bib20), [338](#bib.bib338)] where approaches such as count-based
    exploration/intrinsic motivation [[196](#bib.bib196), [339](#bib.bib339), [340](#bib.bib340),
    [341](#bib.bib341), [342](#bib.bib342)] and hierarchical learning [[343](#bib.bib343),
    [344](#bib.bib344), [111](#bib.bib111)] have been proposed to address it — in
    MDRL this is even more problematic since the agents not only need to learn basic
    behaviors (like in DRL), but also to learn the strategic element (e.g., competitive/collaborative)
    embedded in the multiagent setting. To address this issue, recent MDRL approaches
    applied *dense* rewards [[206](#bib.bib206), [205](#bib.bib205), [204](#bib.bib204)]
    (a concept originated in RL) at each step to allow the agents to learn basic motor
    skills and then decrease these *dense* rewards over time in favor of the environmental
    reward [[158](#bib.bib158)], see Section [3.3](#S3.SS3 "3.3 Emergent behaviors
    ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”"). Recent works like OpenAI Five [[18](#bib.bib18)] uses hand-crafted
    intermediate rewards to accelerate the learning and FTW [[179](#bib.bib179)] lets
    the agents learn their internal rewards by a hierarchical two-tier optimization.
    In single agent domains, RUDDER [[345](#bib.bib345)] has been recently proposed
    for such delayed sparse reward problems. RUDDER generates a new MDP with *more
    intermediate rewards* whose optimal solution is still an optimal solution to the
    original MDP. This is achieved by using LSTM networks to redistribute the original
    sparse reward to earlier state-action pairs and automatically provide reward shaping.
    How to best extend RUDDER to multiagent domains is an open avenue of research.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '最近的MDRL竞赛和环境中出现了复杂的场景，其中许多动作在奖励信号可用之前已经进行（见第[4.3](#S4.SS3 "4.3 Benchmarks for
    MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”")节）。这种稀疏性对RL [[20](#bib.bib20), [338](#bib.bib338)]已经构成挑战，尽管已提出诸如基于计数的探索/内在动机 [[196](#bib.bib196),
    [339](#bib.bib339), [340](#bib.bib340), [341](#bib.bib341), [342](#bib.bib342)]和层次学习 [[343](#bib.bib343),
    [344](#bib.bib344), [111](#bib.bib111)]等方法来解决这一问题——而在MDRL中，这一问题更加严重，因为代理不仅需要学习基本行为（如在DRL中），还需要学习多代理设置中嵌入的战略元素（例如竞争/协作）。为了解决这个问题，最近的MDRL方法在每一步应用了*密集*奖励 [[206](#bib.bib206),
    [205](#bib.bib205), [204](#bib.bib204)]（这一概念源于RL），以使代理学习基本的运动技能，然后随着时间的推移减少这些*密集*奖励，以便于环境奖励 [[158](#bib.bib158)]，见第[3.3](#S3.SS3
    "3.3 Emergent behaviors ‣ 3 Multiagent Deep Reinforcement Learning (MDRL) ‣ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: “Is multiagent deep reinforcement
    learning the answer or the question? A brief survey”")节。最近的研究如OpenAI Five [[18](#bib.bib18)]使用了手工设计的中间奖励来加速学习，而FTW [[179](#bib.bib179)]让代理通过层次化的两级优化来学习其内部奖励。在单代理领域中，RUDDER [[345](#bib.bib345)]最近被提出用于解决这种延迟稀疏奖励问题。RUDDER生成了一个具有*更多中间奖励*的新MDP，其最优解仍然是原始MDP的最优解。这是通过使用LSTM网络将原始稀疏奖励重新分配到早期状态-动作对，并自动提供奖励塑形来实现的。如何最好地将RUDDER扩展到多代理领域仍然是一个开放的研究方向。'
- en: '2.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: On the role of self-play.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于自我对弈的角色。
- en: 'Self-play is a cornerstone in MAL with impressive results [[147](#bib.bib147),
    [127](#bib.bib127), [145](#bib.bib145), [346](#bib.bib346), [143](#bib.bib143)].
    While notable results had also been shown in MDRL [[173](#bib.bib173), [193](#bib.bib193)],
    recent works have also shown that *plain* self-play does not yield the best results.
    However, adding diversity, i.e., evolutionary methods [[239](#bib.bib239), [240](#bib.bib240),
    [233](#bib.bib233), [234](#bib.bib234)] or sampling-based methods, have shown
    good results [[158](#bib.bib158), [179](#bib.bib179), [159](#bib.bib159)]. A drawback
    of these solutions is the additional computational requirements since they need
    either parallel training (more CPU computation) or memory requirements. Then,
    it is still an open problem to improve the computational efficiency of these previously
    proposed successful methods, i.e., achieving similar training stability with smaller
    population sizes that uses fewer CPU workers in MAL and MDRL (see Section [4.4](#S4.SS4
    "4.4 Practical challenges in MDRL ‣ 4 Bridging RL, MAL and MDRL ‣ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: “Is multiagent deep reinforcement learning
    the answer or the question? A brief survey”") and Albrecht et al. [[11](#bib.bib11),
    Section 5.5]).'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the challenge of the combinatorial nature of MDRL.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Monte Carlo tree search (MCTS) [[347](#bib.bib347)] has been the backbone of
    the major breakthroughs behind AlphaGo [[14](#bib.bib14)] and AlphaGo Zero [[15](#bib.bib15)]
    that combined search and DRL. A recent work [[348](#bib.bib348)] has outlined
    how search and RL can be better combined for potentially new methods. However,
    for multiagent scenarios, there is an additional challenge of the exponential
    growth of all the agents’ action spaces for centralized methods [[349](#bib.bib349)].
    One way to tackle this challenge within multiagent scenarios is the use of search
    parallelization [[350](#bib.bib350), [351](#bib.bib351)]. Given more scalable
    planners, there is room for research in combining these techniques in MDRL settings.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To learn complex multiagent interactions some type of abstraction [[352](#bib.bib352)]
    is often needed, for example, factored value functions [[353](#bib.bib353), [354](#bib.bib354),
    [242](#bib.bib242), [243](#bib.bib243), [355](#bib.bib355), [356](#bib.bib356)]
    (see QMIX and VDN in Section [3.5](#S3.SS5 "3.5 Learning cooperation ‣ 3 Multiagent
    Deep Reinforcement Learning (MDRL) ‣ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: “Is multiagent deep reinforcement learning the answer or the question?
    A brief survey”") for recent work in MDRL) try to exploit independence among agents
    through (factored) structure; however, in MDRL there are still open questions
    such as understanding their representational power [[244](#bib.bib244)] (e.g.,
    the accuracy of the learned Q-function approximations) and how to learn those
    factorizations, where ideas from transfer planning techniques could be useful [[357](#bib.bib357),
    [103](#bib.bib103)]. In transfer planning the idea is to define a simpler “source
    problem” (e.g., with fewer agents), in which the agent(s) can plan [[357](#bib.bib357)]
    or learn [[103](#bib.bib103)]; since it is less complex than the real multiagent
    problem, issues such as the non-stationarity of the environment can be reduced/removed.
    Lastly, another related idea are *influence* abstractions [[358](#bib.bib358),
    [359](#bib.bib359), [10](#bib.bib10)], where instead of learning a complex multiagent
    model, these methods try to build smaller models based on the influence agents
    can exert on one another. While this has not been sufficiently explored in actual
    multiagent settings, there is some evidence that these ideas can lead to effective
    inductive biases, improving effectiveness of DRL in such local abstractions [[360](#bib.bib360)].'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习复杂的多智能体互动通常需要某种类型的抽象[[352](#bib.bib352)]，例如，分解值函数[[353](#bib.bib353), [354](#bib.bib354),
    [242](#bib.bib242), [243](#bib.bib243), [355](#bib.bib355), [356](#bib.bib356)]（参见第[3.5节](#S3.SS5
    "3.5 学习合作 ‣ 3 多智能体深度强化学习 (MDRL) ‣ 多智能体深度强化学习的调查与评析1footnote 11footnote 1早期版本的标题为：“多智能体深度强化学习是答案还是问题？简要调查”")中的QMIX和VDN）尝试通过（分解的）结构来利用智能体之间的独立性。然而，在MDRL中仍存在未解的问题，例如理解它们的表示能力[[244](#bib.bib244)]（例如，学习到的Q函数近似的准确性）以及如何学习这些分解，其中转移规划技术中的想法可能会有用[[357](#bib.bib357),
    [103](#bib.bib103)]。在转移规划中，定义一个更简单的“源问题”（例如，少数几个智能体），在其中智能体可以进行规划[[357](#bib.bib357)]或学习[[103](#bib.bib103)]；由于其复杂性低于实际的多智能体问题，因此环境的非平稳性等问题可以减少或消除。最后，另一个相关的想法是*影响*抽象[[358](#bib.bib358),
    [359](#bib.bib359), [10](#bib.bib10)]，这些方法尝试基于智能体之间可以施加的影响来构建较小的模型，而不是学习复杂的多智能体模型。尽管在实际多智能体环境中这尚未得到充分探索，但有证据表明这些想法可以带来有效的归纳偏差，提升DRL在这些局部抽象中的有效性[[360](#bib.bib360)]。
- en: 5 Conclusions
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: Deep reinforcement learning has shown recent success on many fronts [[13](#bib.bib13),
    [14](#bib.bib14), [16](#bib.bib16)] and a natural next step is to test multiagent
    scenarios. However, learning in multiagent environments is fundamentally more
    difficult due to non-stationarity, the increase of dimensionality, and the credit-assignment
    problem, among other factors [[1](#bib.bib1), [5](#bib.bib5), [10](#bib.bib10),
    [147](#bib.bib147), [241](#bib.bib241), [361](#bib.bib361), [97](#bib.bib97)].
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习在许多方面取得了近期成功[[13](#bib.bib13), [14](#bib.bib14), [16](#bib.bib16)]，自然的下一步是测试多智能体场景。然而，由于非平稳性、维度增加和信用分配问题等因素[[1](#bib.bib1),
    [5](#bib.bib5), [10](#bib.bib10), [147](#bib.bib147), [241](#bib.bib241), [361](#bib.bib361),
    [97](#bib.bib97)]，在多智能体环境中进行学习本质上更加困难。
- en: 'This survey provides broad overview of recent works in the emerging area of
    Multiagent Deep Reinforcement Learning (MDRL). First, we categorized recent works
    into four different topics: emergent behaviors, learning communication, learning
    cooperation, and agents modeling agents. Then, we exemplified how key components
    (e.g., experience replay and difference rewards) originated in RL and MAL need
    to be adapted to work in MDRL. We provided general lessons learned applicable
    to MDRL, pointed to recent multiagent benchmarks and highlighted some open research
    problems. Finally, we also reflected on the practical challenges such as computational
    demands and reproducibility in MDRL.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述提供了关于新兴领域多智能体深度强化学习（MDRL）近期工作的广泛概述。首先，我们将近期工作分类为四个不同主题：新兴行为、学习通信、学习合作和智能体建模智能体。然后，我们举例说明了
    RL 和 MAL 中的一些关键组件（例如经验回放和差异奖励）如何需要调整以适应 MDRL。我们提供了适用于 MDRL 的一般经验教训，指出了近期的多智能体基准，并强调了一些开放的研究问题。最后，我们还反思了
    MDRL 中的实际挑战，如计算需求和可重复性。
- en: 'Our conclusions of this work are that while the number of works in DRL and
    MDRL are notable and represent important milestones for AI, at the same time we
    acknowledge there are also open questions in both (deep) single-agent learning [[38](#bib.bib38),
    [298](#bib.bib298), [362](#bib.bib362), [79](#bib.bib79)] and multiagent learning [[363](#bib.bib363),
    [364](#bib.bib364), [365](#bib.bib365), [366](#bib.bib366), [367](#bib.bib367),
    [368](#bib.bib368)]. Our view is that there are practical issues within MDRL that
    hinder its scientific progress: the necessity of high compute power, complicated
    reproducibility (e.g., hyperparameter tuning), and the lack of sufficient encouragement
    for publishing negative results. However, we remain highly optimistic of the multiagent
    community and hope this work serves to raise those issues, encounter good solutions,
    and ultimately take advantage of the existing literature and resources available
    to move the area in the right direction.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结论是，尽管 DRL 和 MDRL 的工作数量显著，并且代表了 AI 的重要里程碑，但我们同时也承认在 (深度) 单智能体学习 [[38](#bib.bib38),
    [298](#bib.bib298), [362](#bib.bib362), [79](#bib.bib79)] 和多智能体学习 [[363](#bib.bib363),
    [364](#bib.bib364), [365](#bib.bib365), [366](#bib.bib366), [367](#bib.bib367),
    [368](#bib.bib368)] 中仍然存在开放性问题。我们的观点是，MDRL 中存在一些实际问题，这些问题阻碍了其科学进展：高计算能力的必要性、复杂的可重复性（例如超参数调整）以及对负面结果缺乏足够的鼓励。然而，我们对多智能体社区保持高度乐观，希望这项工作能引起对这些问题的关注，找到良好的解决方案，并最终利用现有文献和资源推动该领域朝着正确方向发展。
- en: Acknowledgements
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Chao Gao, Nidhi Hegde, Gregory Palmer, Felipe Leno Da
    Silva and Craig Sherstan for reading earlier versions of this work and providing
    feedback, to April Cooper for her visual designs for the figures in the article,
    to Frans Oliehoek, Sam Devlin, Marc Lanctot, Nolan Bard, Roberta Raileanu, Angeliki
    Lazaridou, and Yuhang Song for clarifications in their areas of expertise, to
    Baoxiang Wang for his suggestions on recent deep RL works, to Michael Kaisers,
    Daan Bloembergen, and Katja Hofmann for their comments about the practical challenges
    of MDRL, and to the editor and three anonymous reviewers whose comments and suggestions
    increased the quality of this work.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Chao Gao、Nidhi Hegde、Gregory Palmer、Felipe Leno Da Silva 和 Craig Sherstan
    阅读早期版本的工作并提供反馈，感谢 April Cooper 为文章中的图形设计提供的视觉设计，感谢 Frans Oliehoek、Sam Devlin、Marc
    Lanctot、Nolan Bard、Roberta Raileanu、Angeliki Lazaridou 和 Yuhang Song 在他们各自领域的澄清，感谢
    Baoxiang Wang 对最近深度 RL 工作的建议，感谢 Michael Kaisers、Daan Bloembergen 和 Katja Hofmann
    对 MDRL 实际挑战的评论，以及感谢编辑和三位匿名评审员，他们的评论和建议提高了本工作的质量。
- en: References
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] P. Stone, M. M. Veloso, Multiagent Systems - A Survey from a Machine Learning
    Perspective., Autonomous Robots 8 (3) (2000) 345–383.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. Stone, M. M. Veloso, 《多智能体系统 - 从机器学习的角度看综述》，自主机器人 8 (3) (2000) 345–383。'
- en: '[2] Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer,
    what is the question?, Artificial Intelligence 171 (7) (2007) 365–377.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Shoham, R. Powers, T. Grenager, 《如果多智能体学习是答案，那么问题是什么？》，人工智能 171 (7)
    (2007) 365–377。'
- en: '[3] E. Alonso, M. D’inverno, D. Kudenko, M. Luck, J. Noble, Learning in multi-agent
    systems, Knowledge Engineering Review 16 (03) (2002) 1–8.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] E. Alonso, M. D’inverno, D. Kudenko, M. Luck, J. Noble, 《多智能体系统中的学习》，知识工程评论
    16 (03) (2002) 1–8。'
- en: '[4] K. Tuyls, G. Weiss, Multiagent learning: Basics, challenges, and prospects,
    AI Magazine 33 (3) (2012) 41–52.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] K. Tuyls, G. Weiss, 《多智能体学习：基础、挑战和前景》，人工智能杂志 33 (3) (2012) 41–52。'
- en: '[5] L. Busoniu, R. Babuska, B. De Schutter, A Comprehensive Survey of Multiagent
    Reinforcement Learning, IEEE Transactions on Systems, Man and Cybernetics, Part
    C (Applications and Reviews) 38 (2) (2008) 156–172.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. Busoniu, R. Babuska, B. De Schutter, 多智能体强化学习的全面调查，IEEE Transactions
    on Systems, Man and Cybernetics, Part C (Applications and Reviews) 38 (2) (2008)
    156–172。'
- en: '[6] A. Nowé, P. Vrancx, Y.-M. De Hauwere, Game theory and multi-agent reinforcement
    learning, in: Reinforcement Learning, Springer, 2012, pp. 441–470.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Nowé, P. Vrancx, Y.-M. De Hauwere, 博弈论与多智能体强化学习，见：强化学习，Springer，2012，第441–470页。'
- en: '[7] L. Panait, S. Luke, Cooperative Multi-Agent Learning: The State of the
    Art, Autonomous Agents and Multi-Agent Systems 11 (3).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Panait, S. Luke, 合作多智能体学习：现状，Autonomous Agents and Multi-Agent Systems
    11 (3)。'
- en: '[8] L. Matignon, G. J. Laurent, N. Le Fort-Piat, Independent reinforcement
    learners in cooperative Markov games: a survey regarding coordination problems,
    Knowledge Engineering Review 27 (1) (2012) 1–31.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] L. Matignon, G. J. Laurent, N. Le Fort-Piat, 合作马尔可夫博弈中的独立强化学习者：关于协调问题的调查，Knowledge
    Engineering Review 27 (1) (2012) 1–31。'
- en: '[9] D. Bloembergen, K. Tuyls, D. Hennes, M. Kaisers, Evolutionary Dynamics
    of Multi-Agent Learning: A Survey., Journal of Artificial Intelligence Research
    53 (2015) 659–697.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Bloembergen, K. Tuyls, D. Hennes, M. Kaisers, 多智能体学习的进化动态：一个调查，Journal
    of Artificial Intelligence Research 53 (2015) 659–697。'
- en: '[10] P. Hernandez-Leal, M. Kaisers, T. Baarslag, E. Munoz de Cote, [A Survey
    of Learning in Multiagent Environments - Dealing with Non-Stationarity](http://arxiv.org/abs/1707.09183).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] P. Hernandez-Leal, M. Kaisers, T. Baarslag, E. Munoz de Cote, [多智能体环境中的学习调查
    - 处理非平稳性](http://arxiv.org/abs/1707.09183)。'
- en: URL [http://arxiv.org/abs/1707.09183](http://arxiv.org/abs/1707.09183)
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1707.09183](http://arxiv.org/abs/1707.09183)
- en: '[11] S. V. Albrecht, P. Stone, Autonomous agents modelling other agents: A
    comprehensive survey and open problems, Artificial Intelligence 258 (2018) 66–95.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. V. Albrecht, P. Stone, 自主代理建模其他代理：一个全面的调查和开放问题，Artificial Intelligence
    258 (2018) 66–95。'
- en: '[12] F. L. Silva, A. H. R. Costa, A survey on transfer learning for multiagent
    reinforcement learning systems, Journal of Artificial Intelligence Research 64
    (2019) 645–703.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] F. L. Silva, A. H. R. Costa, 关于多智能体强化学习系统的迁移学习调查，Journal of Artificial
    Intelligence Research 64 (2019) 645–703。'
- en: '[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis,
    Human-level control through deep reinforcement learning, Nature 518 (7540) (2015)
    529–533.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis,
    通过深度强化学习实现人类水平的控制，Nature 518 (7540) (2015) 529–533。'
- en: '[14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe,
    J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,
    T. Graepel, D. Hassabis, Mastering the game of Go with deep neural networks and
    tree search, Nature 529 (7587) (2016) 484–489.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D.
    Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,
    T. Graepel, D. Hassabis, 使用深度神经网络和树搜索掌握围棋，Nature 529 (7587) (2016) 484–489。'
- en: '[15] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton, et al., Mastering the game of Go without
    human knowledge, Nature 550 (7676) (2017) 354.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A.
    Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, 等， 在没有人类知识的情况下掌握围棋，Nature 550 (7676)
    (2017) 354。'
- en: '[16] M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill, N. Bard, T. Davis,
    K. Waugh, M. Johanson, M. Bowling, DeepStack: Expert-level artificial intelligence
    in heads-up no-limit poker, Science 356 (6337) (2017) 508–513.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill, N. Bard, T. Davis,
    K. Waugh, M. Johanson, M. Bowling, DeepStack：在无上限扑克中的专家级人工智能，Science 356 (6337)
    (2017) 508–513。'
- en: '[17] N. Brown, T. Sandholm, Superhuman AI for heads-up no-limit poker: Libratus
    beats top professionals, Science 359 (6374) (2018) 418–424.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] N. Brown, T. Sandholm, 超越人类的AI在无上限扑克中的表现：Libratus击败顶级职业玩家，Science 359
    (6374) (2018) 418–424。'
- en: '[18] Open AI Five, [https://blog.openai.com/openai-five](https://blog.openai.com/openai-five),
    [Online; accessed 7-September-2018] (2018).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Open AI Five, [https://blog.openai.com/openai-five](https://blog.openai.com/openai-five)，[在线；访问日期：2018年9月7日]
    (2018)。'
- en: '[19] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki,
    A. Dudzik, A. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan, M. Kroiss,
    I. Danihelka, J. Agapiou, J. Oh, V. Dalibard, D. Choi, L. Sifre, Y. Sulsky, S. Vezhnevets,
    J. Molloy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z. Wang, T. Pfaff, T. Pohlen,
    Y. Wu, D. Yogatama, J. Cohen, K. McKinney, O. Smith, T. Schaul, T. Lillicrap,
    C. Apps, K. Kavukcuoglu, D. Hassabis, D. Silver, AlphaStar: Mastering the Real-Time
    Strategy Game StarCraft II, [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
    (2019).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki,
    A. Dudzik, A. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan, M. Kroiss,
    I. Danihelka, J. Agapiou, J. Oh, V. Dalibard, D. Choi, L. Sifre, Y. Sulsky, S.
    Vezhnevets, J. Molloy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z. Wang, T. Pfaff,
    T. Pohlen, Y. Wu, D. Yogatama, J. Cohen, K. McKinney, O. Smith, T. Schaul, T.
    Lillicrap, C. Apps, K. Kavukcuoglu, D. Hassabis, D. Silver, AlphaStar：掌握实时战略游戏《星际争霸II》，[https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)（2019）。'
- en: '[20] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, 2nd
    Edition, MIT Press, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. S. Sutton, A. G. Barto, 强化学习：导论，第2版，MIT出版社，2018。'
- en: '[21] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015)
    436.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. LeCun, Y. Bengio, G. Hinton, 深度学习，《自然》521（7553）（2015）436。'
- en: '[22] J. Schmidhuber, Deep learning in neural networks: An overview, Neural
    networks 61 (2015) 85–117.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Schmidhuber, 神经网络中的深度学习：概述，《神经网络》61（2015）85–117。'
- en: '[23] K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, [A Brief
    Survey of Deep Reinforcement Learning](http://arXiv.org/abs/1708.05866v2) .'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, [深度强化学习简要调查](http://arXiv.org/abs/1708.05866v2)。'
- en: URL [http://arXiv.org/abs/1708.05866v2](http://arXiv.org/abs/1708.05866v2)
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arXiv.org/abs/1708.05866v2](http://arXiv.org/abs/1708.05866v2)
- en: '[24] V. François-Lavet, P. Henderson, R. Islam, M. G. Bellemare, J. Pineau,
    et al., An introduction to deep reinforcement learning, Foundations and Trends®
    in Machine Learning 11 (3-4) (2018) 219–354.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] V. François-Lavet, P. Henderson, R. Islam, M. G. Bellemare, J. Pineau,
    等，《深度强化学习介绍》，《机器学习基础与趋势®》11（3-4）（2018）219–354。'
- en: '[25] Y. Yang, J. Hao, M. Sun, Z. Wang, C. Fan, G. Strbac, Recurrent Deep Multiagent
    Q-Learning for Autonomous Brokers in Smart Grid, in: Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence, Stockholm, Sweden,
    2018.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Yang, J. Hao, M. Sun, Z. Wang, C. Fan, G. Strbac, 智能电网中的递归深度多智能体Q学习，载于第二十七届国际人工智能联合会议论文集，瑞典斯德哥尔摩，2018。'
- en: '[26] J. Zhao, G. Qiu, Z. Guan, W. Zhao, X. He, Deep reinforcement learning
    for sponsored search real-time bidding, in: Proceedings of the 24th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining, ACM, 2018, pp.
    1021–1030.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Zhao, G. Qiu, Z. Guan, W. Zhao, X. He, 针对赞助搜索实时竞标的深度强化学习，载于第24届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集，ACM，2018，第1021–1030页。'
- en: '[27] B. M. Lake, T. D. Ullman, J. Tenenbaum, S. Gershman, Building machines
    that learn and think like people, Behavioral and Brain Sciences 40.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] B. M. Lake, T. D. Ullman, J. Tenenbaum, S. Gershman, 构建像人类一样学习和思考的机器，《行为与脑科学》40。'
- en: '[28] A. Tamar, S. Levine, P. Abbeel, Y. Wu, G. Thomas, Value Iteration Networks.,
    NIPS (2016) 2154–2162.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Tamar, S. Levine, P. Abbeel, Y. Wu, G. Thomas, 值迭代网络，《神经信息处理系统》（2016）2154–2162。'
- en: '[29] G. Papoudakis, F. Christianos, A. Rahman, S. V. Albrecht, Dealing with
    non-stationarity in multi-agent deep reinforcement learning, arXiv preprint arXiv:1906.04737.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] G. Papoudakis, F. Christianos, A. Rahman, S. V. Albrecht, 处理多智能体深度强化学习中的非平稳性，arXiv
    预印本 arXiv:1906.04737。'
- en: '[30] T. T. Nguyen, N. D. Nguyen, S. Nahavandi, Deep reinforcement learning
    for multi-agent systems: A review of challenges, solutions and applications, arXiv
    preprint arXiv:1812.11794.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. T. Nguyen, N. D. Nguyen, S. Nahavandi, 多智能体系统的深度强化学习：挑战、解决方案与应用综述，arXiv
    预印本 arXiv:1812.11794。'
- en: '[31] D. H. Wolpert, K. Tumer, Optimal payoff functions for members of collectives,
    in: Modeling complexity in economic and social systems, 2002, pp. 355–369.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. H. Wolpert, K. Tumer, 集体成员的最优回报函数，载于经济与社会系统复杂性建模，2002，第355–369页。'
- en: '[32] A. K. Agogino, K. Tumer, Unifying Temporal and Structural Credit Assignment
    Problems., in: Proceedings of 17th International Conference on Autonomous Agents
    and Multiagent Systems, 2004.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. K. Agogino, K. Tumer, 统一时间和结构信用分配问题，载于第17届国际自主代理与多智能体系统会议论文集，2004。'
- en: '[33] N. Fulda, D. Ventura, Predicting and Preventing Coordination Problems
    in Cooperative Q-learning Systems, in: Proceedings of the Twentieth International
    Joint Conference on Artificial Intelligence, Hyderabad, India, 2007, pp. 780–785.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] N. Fulda, D. Ventura, 预测与防止合作Q学习系统中的协调问题，发表于：第二十届国际人工智能联合会议论文集，印度海得拉巴，2007年，页码
    780–785。'
- en: '[34] E. Wei, S. Luke, Lenient Learning in Independent-Learner Stochastic Cooperative
    Games., Journal of Machine Learning Research.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] E. Wei, S. Luke, 在独立学习者随机合作游戏中的宽容学习，《机器学习研究杂志》。'
- en: '[35] G. Palmer, K. Tuyls, D. Bloembergen, R. Savani, Lenient Multi-Agent Deep
    Reinforcement Learning., in: International Conference on Autonomous Agents and
    Multiagent Systems, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] G. Palmer, K. Tuyls, D. Bloembergen, R. Savani, 宽容多智能体深度强化学习，发表于：国际自主代理与多智能体系统会议，2018。'
- en: '[36] L. Busoniu, R. Babuska, B. De Schutter, Multi-agent reinforcement learning:
    An overview, in: D. Srinivasan, L. C. Jain (Eds.), Innovations in Multi-Agent
    Systems and Applications - 1, Springer Berlin Heidelberg, Berlin, Heidelberg,
    2010, pp. 183–221.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] L. Busoniu, R. Babuska, B. De Schutter, 多智能体强化学习：概述，发表于：D. Srinivasan,
    L. C. Jain (编辑)，《多智能体系统与应用的创新 - 1》，Springer Berlin Heidelberg，柏林，海德堡，2010，页码 183–221。'
- en: '[37] Y. Li, [Deep reinforcement learning: An overview](http://arxiv.org/abs/1701.07274),
    CoRR abs/1701.07274. [arXiv:1701.07274](http://arxiv.org/abs/1701.07274).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Li, [深度强化学习概述](http://arxiv.org/abs/1701.07274)，CoRR abs/1701.07274。
    [arXiv:1701.07274](http://arxiv.org/abs/1701.07274)。'
- en: URL [http://arxiv.org/abs/1701.07274](http://arxiv.org/abs/1701.07274)
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1701.07274](http://arxiv.org/abs/1701.07274)
- en: '[38] A. Darwiche, Human-level intelligence or animal-like abilities?, Commun.
    ACM 61 (10) (2018) 56–67.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Darwiche, 人类级智能还是类似动物的能力？，《计算机通讯》61(10) (2018) 56–67。'
- en: '[39] M. Wiering, M. Van Otterlo, Reinforcement learning, Adaptation, learning,
    and optimization 12.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Wiering, M. Van Otterlo, 强化学习，《适应、学习与优化》12。'
- en: '[40] L. P. Kaelbling, M. L. Littman, A. W. Moore, Reinforcement learning: A
    survey, Journal of artificial intelligence research 4 (1996) 237–285.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. P. Kaelbling, M. L. Littman, A. W. Moore, 强化学习：一项综述，《人工智能研究杂志》4 (1996)
    237–285。'
- en: '[41] M. L. Puterman, Markov decision processes: Discrete stochastic dynamic
    programming, John Wiley & Sons, Inc., 1994.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. L. Puterman, 马尔可夫决策过程：离散随机动态规划，John Wiley & Sons, Inc., 1994。'
- en: '[42] A. R. Cassandra, Exact and approximate algorithms for partially observable
    Markov decision processes, Ph.D. thesis, Computer Science Department, Brown University
    (May 1998).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. R. Cassandra, 部分可观察马尔可夫决策过程的精确和近似算法，博士论文，布朗大学计算机科学系 (1998年5月)。'
- en: '[43] K. J. Astrom, Optimal control of Markov processes with incomplete state
    information, Journal of mathematical analysis and applications 10 (1) (1965) 174–205.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] K. J. Astrom, 具有不完全状态信息的马尔可夫过程的最优控制，《数学分析与应用杂志》10(1) (1965) 174–205。'
- en: '[44] R. Bellman, A Markovian decision process, Journal of Mathematics and Mechanics
    6 (5) (1957) 679–684.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Bellman, 马尔可夫决策过程，《数学与力学杂志》6(5) (1957) 679–684。'
- en: '[45] J. Watkins, Learning from delayed rewards, Ph.D. thesis, King’s College,
    Cambridge, UK (Apr. 1989).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Watkins, 从延迟奖励中学习，博士论文，剑桥大学国王学院，英国 (1989年4月)。'
- en: '[46] T. Kamihigashi, C. Le Van, [Necessary and Sufficient Conditions for a
    Solution of the Bellman Equation to be the Value Function: A General Principle](https://halshs.archives-ouvertes.fr/halshs-01159177).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] T. Kamihigashi, C. Le Van, [贝尔曼方程解为价值函数的必要与充分条件：一个通用原则](https://halshs.archives-ouvertes.fr/halshs-01159177)。'
- en: URL [https://halshs.archives-ouvertes.fr/halshs-01159177](https://halshs.archives-ouvertes.fr/halshs-01159177)
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://halshs.archives-ouvertes.fr/halshs-01159177](https://halshs.archives-ouvertes.fr/halshs-01159177)
- en: '[47] J. Tsitsiklis, Asynchronous stochastic approximation and Q-learning, Machine
    Learning 16 (3) (1994) 185–202.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Tsitsiklis, 异步随机逼近与Q学习，《机器学习》16(3) (1994) 185–202。'
- en: '[48] T. Jaakkola, M. I. Jordan, S. P. Singh, Convergence of stochastic iterative
    dynamic programming algorithms, in: Advances in neural information processing
    systems, 1994, pp. 703–710.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Jaakkola, M. I. Jordan, S. P. Singh, 随机迭代动态规划算法的收敛性，发表于：神经信息处理系统进展，1994年，页码
    703–710。'
- en: '[49] C. Szepesvári, M. L. Littman, A unified analysis of value-function-based
    reinforcement-learning algorithms, Neural computation 11 (8) (1999) 2017–2060.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Szepesvári, M. L. Littman, 基于价值函数的强化学习算法的统一分析，《神经计算》11(8) (1999) 2017–2060。'
- en: '[50] E. Even-Dar, Y. Mansour, Learning rates for Q-learning, Journal of Machine
    Learning Research 5 (Dec) (2003) 1–25.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] E. Even-Dar, Y. Mansour, Q学习的学习速率，《机器学习研究杂志》5 (12月) (2003) 1–25。'
- en: '[51] C. Szepesvári, Algorithms for reinforcement learning, Synthesis lectures
    on artificial intelligence and machine learning 4 (1) (2010) 1–103.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Szepesvári，《强化学习算法》，《人工智能与机器学习综合讲座》4 (1) (2010) 1–103。'
- en: '[52] S. Singh, T. Jaakkola, M. L. Littman, C. Szepesvári, Convergence results
    for single-step on-policy reinforcement-learning algorithms, Machine learning
    38 (3) (2000) 287–308.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Singh, T. Jaakkola, M. L. Littman, C. Szepesvári，《单步在政策强化学习算法中的收敛结果》，《机器学习》38
    (3) (2000) 287–308。'
- en: '[53] H. Van Seijen, H. Van Hasselt, S. Whiteson, M. Wiering, A theoretical
    and empirical analysis of Expected Sarsa, in: IEEE Symposium on Adaptive Dynamic
    Programming and Reinforcement Learning, Nashville, TN, USA, 2009, pp. 177–184.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Van Seijen, H. Van Hasselt, S. Whiteson, M. Wiering，《期望Sarsa的理论与实证分析》，在：IEEE自适应动态规划与强化学习研讨会，纳什维尔，田纳西州，美国，2009，第177–184页。'
- en: '[54] V. R. Konda, J. Tsitsiklis, Actor-critic algorithms, in: Advances in Neural
    Information Processing Systems, 2000.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] V. R. Konda, J. Tsitsiklis，《演员-评论员算法》，在：神经信息处理系统进展，2000。'
- en: '[55] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, Policy Gradient
    Methods for Reinforcement Learning with Function Approximation., in: Advances
    in Neural Information Processing Systems, 2000.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour，《带函数逼近的强化学习策略梯度方法》，在：神经信息处理系统进展，2000。'
- en: '[56] R. J. Williams, Simple statistical gradient-following algorithms for connectionist
    reinforcement learning, Machine learning 8 (3-4) (1992) 229–256.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] R. J. Williams，《用于连接主义强化学习的简单统计梯度跟随算法》，《机器学习》8 (3-4) (1992) 229–256。'
- en: '[57] H. Liu, Y. Feng, Y. Mao, D. Zhou, J. Peng, Q. Liu, Action-depedent control
    variates for policy optimization via stein’s identity, in: International Conference
    on Learning Representations, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Liu, Y. Feng, Y. Mao, D. Zhou, J. Peng, Q. Liu，《通过斯坦因恒等式进行策略优化的动作依赖控制变量》，在：国际学习表征会议，2018。'
- en: '[58] S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, S. Levine, Q-prop: Sample-efficient
    policy gradient with an off-policy critic, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, S. Levine，《Q-prop：一种具有离政策评论员的样本高效策略梯度》，在：国际学习表征会议，2017。'
- en: '[59] G. Tucker, S. Bhupatiraju, S. Gu, R. E. Turner, Z. Ghahramani, S. Levine,
    The mirage of action-dependent baselines in reinforcement learning, in: International
    Conference on Machine Learning, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] G. Tucker, S. Bhupatiraju, S. Gu, R. E. Turner, Z. Ghahramani, S. Levine，《强化学习中依赖动作的基线的幻影》，在：国际机器学习会议，2018。'
- en: '[60] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, P. Moritz, Trust Region
    Policy Optimization., in: 31st International Conference on Machine Learning, Lille,
    France, 2015.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, P. Moritz，《信任域策略优化》，在：第31届国际机器学习会议，里尔，法国，2015。'
- en: '[61] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller,
    Deterministic policy gradient algorithms, in: ICML, 2014.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller，《确定性策略梯度算法》，在：ICML，2014。'
- en: '[62] R. Hafner, M. Riedmiller, Reinforcement learning in feedback control,
    Machine learning 84 (1-2) (2011) 137–169.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. Hafner, M. Riedmiller，《反馈控制中的强化学习》，《机器学习》84 (1-2) (2011) 137–169。'
- en: '[63] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, I. Mordatch, Multi-Agent
    Actor-Critic for Mixed Cooperative-Competitive Environments., in: Advances in
    Neural Information Processing Systems, 2017, pp. 6379–6390.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, I. Mordatch，《混合合作-竞争环境中的多智能体演员-评论员》，在：神经信息处理系统进展，2017，第6379–6390页。'
- en: '[64] J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, I. Mordatch,
    Learning with Opponent-Learning Awareness., in: Proceedings of 17th International
    Conference on Autonomous Agents and Multiagent Systems, Stockholm, Sweden, 2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, I.
    Mordatch，《具有对手学习意识的学习》，在：第17届国际自主代理与多智能体系统会议，斯德哥尔摩，瑞典，2018。'
- en: '[65] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, in:
    International Conference on Learning Representations, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, D. Wierstra，《深度强化学习中的连续控制》，在：国际学习表征会议，2016。'
- en: '[66] L. D. Pyeatt, A. E. Howe, et al., Decision tree function approximation
    in reinforcement learning, in: Proceedings of the third international symposium
    on adaptive systems: evolutionary computation and probabilistic graphical models,
    Vol. 2, Cuba, 2001, pp. 70–77.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. S. Sutton, Generalization in reinforcement learning: Successful examples
    using sparse coarse coding, in: Advances in neural information processing systems,
    1996, pp. 1038–1044.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. M. Kretchmar, C. W. Anderson, Comparison of CMACs and radial basis
    functions for local function approximators in reinforcement learning, in: Proceedings
    of International Conference on Neural Networks (ICNN’97), Vol. 2, IEEE, 1997,
    pp. 834–837.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. A. Boyan, A. W. Moore, Generalization in reinforcement learning: Safely
    approximating the value function, in: Advances in neural information processing
    systems, 1995, pp. 369–376.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. M. Bishop, Pattern recognition and machine learning, Springer, 2006.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, G. Tesauro, [Learning
    to learn without forgetting by maximizing transfer and minimizing interference](http://arxiv.org/abs/1810.11910),
    CoRR abs/1810.11910. [arXiv:1810.11910](http://arxiv.org/abs/1810.11910).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1810.11910](http://arxiv.org/abs/1810.11910)
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[72] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    M. Riedmiller, [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602v1).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1312.5602v1](http://arxiv.org/abs/1312.5602v1)
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[73] G. J. Gordon, Approximate solutions to Markov decision processes, Tech.
    rep., Carnegie-Mellon University (1999).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. Baird, Residual algorithms: Reinforcement learning with function approximation,
    in: Machine Learning Proceedings 1995, 1995, pp. 30–37.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. Whiteson, P. Stone, Evolutionary function approximation for reinforcement
    learning, Journal of Machine Learning Research 7 (May) (2006) 877–917.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J. Achiam, E. Knight, P. Abbeel, [Towards Characterizing Divergence in
    Deep Q-Learning](http://arxiv.org/abs/1903.08894), CoRR abs/1903.08894. [arXiv:1903.08894](http://arxiv.org/abs/1903.08894).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.08894](http://arxiv.org/abs/1903.08894)
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[77] H. van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, J. Modayil,
    [Deep reinforcement learning and the deadly triad](http://arxiv.org/abs/1812.02648),
    CoRR abs/1812.02648. [arXiv:1812.02648](http://arxiv.org/abs/1812.02648).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1812.02648](http://arxiv.org/abs/1812.02648)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[78] S. Fujimoto, H. van Hoof, D. Meger, Addressing function approximation
    error in actor-critic methods, in: International Conference on Machine Learning,
    2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Ilyas, L. Engstrom, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph,
    A. Madry, [Are deep policy gradient algorithms truly policy gradient algorithms?](http://arxiv.org/abs/1811.02553),
    CoRR abs/1811.02553. [arXiv:1811.02553](http://arxiv.org/abs/1811.02553).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1811.02553](http://arxiv.org/abs/1811.02553)
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1811.02553](http://arxiv.org/abs/1811.02553)
- en: '[80] T. Lu, D. Schuurmans, C. Boutilier, Non-delusional Q-learning and value-iteration,
    in: Advances in Neural Information Processing Systems, 2018, pp. 9949–9959.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] T. Lu, D. Schuurmans, C. Boutilier, 《非妄想Q学习和价值迭代》，发表于：神经信息处理系统进展，2018年，第9949–9959页。'
- en: '[81] J. N. Tsitsiklis, B. Van Roy, Analysis of temporal-diffference learning
    with function approximation, in: Advances in neural information processing systems,
    1997, pp. 1075–1081.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. N. Tsitsiklis, B. Van Roy, 《具有函数逼近的时间差分学习分析》，发表于：神经信息处理系统进展，1997年，第1075–1081页。'
- en: '[82] F. S. Melo, S. P. Meyn, M. I. Ribeiro, An analysis of reinforcement learning
    with function approximation, in: Proceedings of the 25th international conference
    on Machine learning, ACM, 2008, pp. 664–671.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] F. S. Melo, S. P. Meyn, M. I. Ribeiro, 《具有函数逼近的强化学习分析》，发表于：第25届国际机器学习会议论文集，ACM，2008年，第664–671页。'
- en: '[83] D. Ernst, P. Geurts, L. Wehenkel, Tree-based batch mode reinforcement
    learning, Journal of Machine Learning Research 6 (Apr) (2005) 503–556.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] D. Ernst, P. Geurts, L. Wehenkel, 《基于树的批量模式强化学习》，《机器学习研究杂志》6（四月）（2005年）第503–556页。'
- en: '[84] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    K. Kavukcuoglu, Reinforcement Learning with Unsupervised Auxiliary Tasks., in:
    International Conference on Learning Representations, 2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    K. Kavukcuoglu, 《带有无监督辅助任务的强化学习》，发表于：国际学习表征会议，2017年。'
- en: '[85] M. Hausknecht, P. Stone, Deep Recurrent Q-Learning for Partially Observable
    MDPs, in: International Conference on Learning Representations, 2015.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] M. Hausknecht, P. Stone, 《部分可观察MDP的深度递归Q学习》，发表于：国际学习表征会议，2015年。'
- en: '[86] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation
    9 (8) (1997) 1735–1780.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. Hochreiter, J. Schmidhuber, 《长短期记忆》，《神经计算》9（8）（1997年）第1735–1780页。'
- en: '[87] M. Riedmiller, Neural fitted Q iteration–first experiences with a data
    efficient neural reinforcement learning method, in: European Conference on Machine
    Learning, Springer, 2005, pp. 317–328.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Riedmiller, 《神经网络拟合Q迭代——数据高效神经强化学习方法的首次经验》，发表于：欧洲机器学习会议，施普林格，2005年，第317–328页。'
- en: '[88] R. H. Crites, A. G. Barto, Elevator group control using multiple reinforcement
    learning agents, Machine learning 33 (2-3) (1998) 235–262.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] R. H. Crites, A. G. Barto, 《使用多个强化学习代理的电梯群控制》，《机器学习》33（2-3）（1998年）第235–262页。'
- en: '[89] L. J. Lin, Programming robots using reinforcement learning and teaching.,
    in: AAAI, 1991, pp. 781–786.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] L. J. Lin, 《使用强化学习和教学编程机器人》，发表于：AAAI，1991年，第781–786页。'
- en: '[90] L.-J. Lin, Self-improving reactive agents based on reinforcement learning,
    planning and teaching, Machine learning 8 (3-4) (1992) 293–321.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L.-J. Lin, 《基于强化学习、自我提升的反应性代理，规划和教学》，《机器学习》8(3-4)（1992年）第293–321页。'
- en: '[91] H. V. Hasselt, Double Q-learning, in: Advances in Neural Information Processing
    Systems, 2010, pp. 2613–2621.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] H. V. Hasselt, 《双重Q学习》，发表于：神经信息处理系统进展，2010年，第2613–2621页。'
- en: '[92] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double
    Q-learning, in: Thirtieth AAAI Conference on Artificial Intelligence, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] H. Van Hasselt, A. Guez, D. Silver, 《带双重Q学习的深度强化学习》，发表于：第三十届AAAI人工智能会议，2016年。'
- en: '[93] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in: International
    Conference on Machine Learning, 2016, pp. 1928–1937.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver, K. Kavukcuoglu, 《深度强化学习的异步方法》，发表于：国际机器学习会议，2016年，第1928–1937页。'
- en: '[94] M. McCloskey, N. J. Cohen, Catastrophic interference in connectionist
    networks: The sequential learning problem, in: Psychology of learning and motivation,
    Vol. 24, Elsevier, 1989, pp. 109–165.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. McCloskey, N. J. Cohen, 《连接主义网络中的灾难性干扰：序列学习问题》，发表于：《学习和动机心理学》，第24卷，Elsevier，1989年，第109–165页。'
- en: '[95] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, Y. Bengio, [An empirical
    investigation of catastrophic forgetting in gradient-based neural networks](https://arxiv.org/abs/1312.6211).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, Y. Bengio, [《基于梯度的神经网络中的灾难性遗忘的实证研究》](https://arxiv.org/abs/1312.6211)。'
- en: URL [https://arxiv.org/abs/1312.6211](https://arxiv.org/abs/1312.6211)
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://arxiv.org/abs/1312.6211](https://arxiv.org/abs/1312.6211)
- en: '[96] D. Isele, A. Cosgun, Selective experience replay for lifelong learning,
    in: Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D. Isele, A. Cosgun, 《用于终身学习的选择性经验重放》，发表于：第三十二届AAAI人工智能会议，2018年。'
- en: '[97] G. Palmer, R. Savani, K. Tuyls, Negative update intervals in deep multi-agent
    reinforcement learning, in: 18th International Conference on Autonomous Agents
    and Multiagent Systems, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] G. Palmer, R. Savani, K. Tuyls, 深度多智能体强化学习中的负更新间隔，发表于：第18届自主代理与多智能体系统国际会议，2019年。'
- en: '[98] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, N. De Freitas,
    Dueling network architectures for deep reinforcement learning, in: International
    Conference on Machine Learning, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, N. De Freitas,
    用于深度强化学习的对决网络架构，发表于：国际机器学习大会，2016年。'
- en: '[99] M. Hauskrecht, Value-function approximations for partially observable
    Markov decision processes, Journal of Artificial Intelligence Research 13 (1).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Hauskrecht, 对部分可观察马尔可夫决策过程的价值函数近似，《人工智能研究杂志》13 (1)。'
- en: '[100] N. Meuleau, L. Peshkin, K.-E. Kim, L. P. Kaelbling, Learning finite-state
    controllers for partially observable environments, in: Proceedings of the Fifteenth
    conference on Uncertainty in artificial intelligence, 1999, pp. 427–436.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] N. Meuleau, L. Peshkin, K.-E. Kim, L. P. Kaelbling, 学习有限状态控制器用于部分可观察环境，发表于：第十五届人工智能不确定性会议论文，1999年，页码427–436。'
- en: '[101] D. Steckelmacher, D. M. Roijers, A. Harutyunyan, P. Vrancx, H. Plisnier,
    A. Nowé, Reinforcement learning in pomdps with memoryless options and option-observation
    initiation sets, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] D. Steckelmacher, D. M. Roijers, A. Harutyunyan, P. Vrancx, H. Plisnier,
    A. Nowé, 在POMDP中使用无记忆选项和选项-观察初始化集合的强化学习，发表于：第三十二届AAAI人工智能大会，2018年。'
- en: '[102] S. Ioffe, C. Szegedy, Batch normalization: Accelerating deep network
    training by reducing internal covariate shift, Proceedings of the 32nd International
    Conference on Machine Learning (2015) 448–456.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] S. Ioffe, C. Szegedy, 批量归一化：通过减少内部协变量偏移加速深度网络训练，发表于：第32届国际机器学习大会会议录（2015年）448–456页。'
- en: '[103] E. Van der Pol, F. A. Oliehoek, Coordinated deep reinforcement learners
    for traffic light control, in: Proceedings of Learning, Inference and Control
    of Multi-Agent Systems at NIPS, 2016.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] E. Van der Pol, F. A. Oliehoek, 协调的深度强化学习者用于交通灯控制，发表于：NIPS多智能体系统的学习、推理和控制会议论文，2016年。'
- en: '[104] T. Salimans, D. P. Kingma, Weight normalization: A simple reparameterization
    to accelerate training of deep neural networks, in: Advances in Neural Information
    Processing Systems, 2016, pp. 901–909.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Salimans, D. P. Kingma, 权重归一化：加速深度神经网络训练的简单重参数化，发表于：神经信息处理系统进展，2016年，页码901–909。'
- en: '[105] OpenAI Baselines: ACKTR & A2C, [https://openai.com/blog/baselines-acktr-a2c/](https://openai.com/blog/baselines-acktr-a2c/),
    [Online; accessed 29-April-2019] (2017).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] OpenAI Baselines: ACKTR & A2C, [https://openai.com/blog/baselines-acktr-a2c/](https://openai.com/blog/baselines-acktr-a2c/)，[在线；访问日期：2019年4月29日]（2017年）。'
- en: '[106] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, N. de Freitas,
    Sample efficient actor-critic with experience replay, arXiv preprint arXiv:1611.01224.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, N. de
    Freitas, 样本高效的演员-评论家与经验回放，arXiv预印本 arXiv:1611.01224。'
- en: '[107] S. S. Gu, T. Lillicrap, R. E. Turner, Z. Ghahramani, B. Schölkopf, S. Levine,
    Interpolated policy gradient: Merging on-policy and off-policy gradient estimation
    for deep reinforcement learning, in: Advances in Neural Information Processing
    Systems, 2017, pp. 3846–3855.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] S. S. Gu, T. Lillicrap, R. E. Turner, Z. Ghahramani, B. Schölkopf, 插值策略梯度：融合策略和非策略梯度估计用于深度强化学习，发表于：神经信息处理系统进展，2017年，页码3846–3855。'
- en: '[108] E. Shelhamer, P. Mahmoudieh, M. Argus, T. Darrell, Loss is its own reward:
    Self-supervision for reinforcement learning, ICLR workshops.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] E. Shelhamer, P. Mahmoudieh, M. Argus, T. Darrell, 损失是它自己的奖励：用于强化学习的自监督，ICLR研讨会。'
- en: '[109] M. G. Bellemare, W. Dabney, R. Dadashi, A. A. Taïga, P. S. Castro, N. L.
    Roux, D. Schuurmans, T. Lattimore, C. Lyle, [A Geometric Perspective on Optimal
    Representations for Reinforcement Learning](http://arxiv.org/abs/1901.11530),
    CoRR abs/1901.11530. [arXiv:1901.11530](http://arxiv.org/abs/1901.11530).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. G. Bellemare, W. Dabney, R. Dadashi, A. A. Taïga, P. S. Castro, N.
    L. Roux, D. Schuurmans, T. Lattimore, C. Lyle, [关于强化学习的最佳表示的几何视角](http://arxiv.org/abs/1901.11530)，CoRR
    abs/1901.11530。 [arXiv:1901.11530](http://arxiv.org/abs/1901.11530)。'
- en: URL [http://arxiv.org/abs/1901.11530](http://arxiv.org/abs/1901.11530)
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1901.11530](http://arxiv.org/abs/1901.11530)
- en: '[110] R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White,
    D. Precup, Horde: A scalable real-time architecture for learning knowledge from
    unsupervised sensorimotor interaction, in: The 10th International Conference on
    Autonomous Agents and Multiagent Systems-Volume 2, International Foundation for
    Autonomous Agents and Multiagent Systems, 2011, pp. 761–768.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White,
    D. Precup，《Horde：一个可扩展的实时架构用于从无监督传感-运动交互中学习知识》，第十届国际自主代理与多智能体系统会议-第2卷，国际自主代理与多智能体系统基金会，2011年，第761-768页。'
- en: '[111] T. Schaul, J. Quan, I. Antonoglou, D. Silver, Prioritized Experience
    Replay, in: International Conference on Learning Representations, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] T. Schaul, J. Quan, I. Antonoglou, D. Silver，《优先级经验重放》，国际学习表征会议，2016年。'
- en: '[112] A. W. Moore, C. G. Atkeson, Prioritized sweeping: Reinforcement learning
    with less data and less time, Machine learning 13 (1) (1993) 103–130.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. W. Moore, C. G. Atkeson，《优先级扫荡：用更少的数据和时间进行强化学习》，《机器学习》13（1）（1993年）第103-130页。'
- en: '[113] D. Andre, N. Friedman, R. Parr, Generalized prioritized sweeping, in:
    Advances in Neural Information Processing Systems, 1998, pp. 1001–1007.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] D. Andre, N. Friedman, R. Parr，《广义优先级扫荡》，《神经信息处理系统进展》，1998年，第1001-1007页。'
- en: '[114] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    V. Firoiu, T. Harley, I. Dunning, et al., IMPALA: Scalable distributed Deep-RL
    with importance weighted actor-learner architectures, in: International Conference
    on Machine Learning, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    V. Firoiu, T. Harley, I. Dunning, 等，《IMPALA：可扩展的分布式深度强化学习与重要性加权的演员-学习者架构》，国际机器学习会议，2018年。'
- en: '[115] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, [Proximal
    Policy Optimization Algorithms](http://arxiv.org/abs/1707.06347).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov，《近端策略优化算法》（http://arxiv.org/abs/1707.06347）。'
- en: URL [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
- en: '[116] S. M. Kakade, A natural policy gradient, in: Advances in neural information
    processing systems, 2002, pp. 1531–1538.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. M. Kakade，《自然策略梯度》，《神经信息处理系统进展》，2002年，第1531-1538页。'
- en: '[117] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa,
    T. Erez, Z. Wang, S. M. A. Eslami, M. A. Riedmiller, D. Silver, [Emergence of
    Locomotion Behaviours in Rich Environments.](http://arXiv.org/abs/1707.02286v2)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa,
    T. Erez, Z. Wang, S. M. A. Eslami, M. A. Riedmiller, D. Silver，《在丰富环境中运动行为的出现。](http://arXiv.org/abs/1707.02286v2)'
- en: URL [http://arXiv.org/abs/1707.02286v2](http://arXiv.org/abs/1707.02286v2)
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arXiv.org/abs/1707.02286v2](http://arXiv.org/abs/1707.02286v2)
- en: '[118] G. Bacchiani, D. Molinari, M. Patander, Microscopic traffic simulation
    by cooperative multi-agent deep reinforcement learning, in: AAMAS, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] G. Bacchiani, D. Molinari, M. Patander，《通过合作多智能体深度强化学习进行微观交通模拟》，AAMAS，2019年。'
- en: '[119] J. Schulman, P. Abbeel, X. Chen, [Equivalence Between Policy Gradients
    and Soft Q-Learning](http://arxiv.org/abs/1704.06440), CoRR abs/1704.06440. [arXiv:1704.06440](http://arxiv.org/abs/1704.06440).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. Schulman, P. Abbeel, X. Chen，《策略梯度与软Q学习的等价性》（http://arxiv.org/abs/1704.06440），CoRR
    abs/1704.06440。 [arXiv:1704.06440](http://arxiv.org/abs/1704.06440)。'
- en: URL [http://arxiv.org/abs/1704.06440](http://arxiv.org/abs/1704.06440)
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1704.06440](http://arxiv.org/abs/1704.06440)
- en: '[120] T. Haarnoja, H. Tang, P. Abbeel, S. Levine, Reinforcement learning with
    deep energy-based policies, in: Proceedings of the 34th International Conference
    on Machine Learning-Volume 70, 2017, pp. 1352–1361.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] T. Haarnoja, H. Tang, P. Abbeel, S. Levine，《基于深度能量模型的强化学习》，《第34届国际机器学习会议论文集-第70卷》，2017年，第1352-1361页。'
- en: '[121] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor, in: International
    Conference on Machine Learning, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine，《软演员-评论家：具有随机演员的离策略最大熵深度强化学习》，国际机器学习会议，2018年。'
- en: '[122] M. Tan, Multi-Agent Reinforcement Learning: Independent vs. Cooperative
    Agents, in: Machine Learning Proceedings 1993 Proceedings of the Tenth International
    Conference, University of Massachusetts, Amherst, June 27–29, 1993, 1993, pp.
    330–337.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] M. Tan, 多智能体强化学习：独立智能体与合作智能体，《机器学习会议论文集1993》第十届国际会议论文集，美国马萨诸塞大学，安姆斯特，1993年6月27-29日，1993年，第330-337页。'
- en: '[123] G. J. Laurent, L. Matignon, L. Fort-Piat, et al., The world of independent
    learners is not Markovian, International Journal of Knowledge-based and Intelligent
    Engineering Systems 15 (1) (2011) 55–64.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] G. J. Laurent, L. Matignon, L. Fort-Piat 等，《独立学习者的世界不是马尔可夫的》，《知识基础与智能工程系统国际期刊》15（1）（2011）55–64。'
- en: '[124] M. L. Littman, Markov games as a framework for multi-agent reinforcement
    learning, in: Proceedings of the 11th International Conference on Machine Learning,
    New Brunswick, NJ, USA, 1994, pp. 157–163.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] M. L. Littman, 马尔可夫博弈作为多智能体强化学习的框架，载于：第11届国际机器学习大会论文集，新泽西州新布伦瑞克，美国，1994年，第157–163页。'
- en: '[125] M. L. Littman, Value-function reinforcement learning in Markov games,
    Cognitive Systems Research 2 (1) (2001) 55–66.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. L. Littman, 马尔可夫博弈中的价值函数强化学习，《认知系统研究》2（1）（2001）55–66。'
- en: '[126] C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative
    multiagent systems, in: Proceedings of the 15th National Conference on Artificial
    Intelligence, Madison, Wisconsin, USA, 1998, pp. 746–752.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] C. Claus, C. Boutilier, 合作多智能体系统中的强化学习动态，载于：第15届全国人工智能大会论文集，威斯康星州麦迪逊，美国，1998年，第746–752页。'
- en: '[127] J. Hu, M. P. Wellman, Nash Q-learning for general-sum stochastic games,
    The Journal of Machine Learning Research 4 (2003) 1039–1069.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Hu, M. P. Wellman, 纳什Q学习用于一般和随机博弈，《机器学习研究期刊》4（2003）1039–1069。'
- en: '[128] M. L. Littman, Friend-or-foe Q-learning in general-sum games, in: Proceedings
    of 17th International Conference on Autonomous Agents and Multiagent Systems,
    Williamstown, MA, USA, 2001, pp. 322–328.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] M. L. Littman, 一般和博弈中的友或敌Q学习，载于：第17届国际自主智能体与多智能体系统会议论文集，马萨诸塞州威廉斯敦，美国，2001年，第322–328页。'
- en: '[129] X. Song, T. Wang, C. Zhang, Convergence of multi-agent learning with
    a finite step size in general-sum games, in: 18th International Conference on
    Autonomous Agents and Multiagent Systems, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] X. Song, T. Wang, C. Zhang, 一般和博弈中有限步长的多智能体学习的收敛性，载于：第18届国际自主智能体与多智能体系统会议，2019年。'
- en: '[130] D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, T. Graepel,
    The mechanics of n-player differentiable games, in: Proceedings of the 35th International
    Conference on Machine Learning, Proceedings of Machine Learning Research, Stockholm,
    Sweden, 2018, pp. 354–363.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, T. Graepel,
    n玩家可微分博弈的机制，载于：第35届国际机器学习大会论文集，机器学习研究论文集，瑞典斯德哥尔摩，2018年，第354–363页。'
- en: '[131] J. Pérolat, B. Piot, O. Pietquin, Actor-critic fictitious play in simultaneous
    move multistage games, in: 21st International Conference on Artificial Intelligence
    and Statistics, 2018.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Pérolat, B. Piot, O. Pietquin, 同步移动多阶段博弈中的演员-评论家虚拟游戏，载于：第21届国际人工智能与统计会议，2018年。'
- en: '[132] G. Bono, J. S. Dibangoye, L. Matignon, F. Pereyron, O. Simonin, Cooperative
    multi-agent policy gradient, in: European Conference on Machine Learning, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] G. Bono, J. S. Dibangoye, L. Matignon, F. Pereyron, O. Simonin, 合作多智能体策略梯度，载于：欧洲机器学习会议，2018年。'
- en: '[133] T. W. Neller, M. Lanctot, An introduction to counterfactual regret minimization,
    in: Proceedings of Model AI Assignments, The Fourth Symposium on Educational Advances
    in Artificial Intelligence (EAAI-2013), 2013.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] T. W. Neller, M. Lanctot, 反事实遗憾最小化简介，载于：模型人工智能任务会议，第四届人工智能教育进展研讨会（EAAI-2013），2013年。'
- en: '[134] M. Zinkevich, M. Johanson, M. Bowling, C. Piccione, Regret minimization
    in games with incomplete information, in: Advances in neural information processing
    systems, 2008, pp. 1729–1736.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Zinkevich, M. Johanson, M. Bowling, C. Piccione, 不完全信息博弈中的遗憾最小化，载于：神经信息处理系统进展，2008年，第1729–1736页。'
- en: '[135] A. Blum, Y. Monsour, Learning, regret minimization, and equilibria, in:
    Algorithmic Game Theory, Cambridge University Press, 2007, Ch. 4.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Blum, Y. Monsour, 学习、遗憾最小化与均衡，载于：算法博弈论，剑桥大学出版社，2007年，第4章。'
- en: '[136] S. Srinivasan, M. Lanctot, V. Zambaldi, J. Pérolat, K. Tuyls, R. Munos,
    M. Bowling, Actor-critic policy optimization in partially observable multiagent
    environments, in: Advances in Neural Information Processing Systems, 2018, pp.
    3422–3435.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] S. Srinivasan, M. Lanctot, V. Zambaldi, J. Pérolat, K. Tuyls, R. Munos,
    M. Bowling, 部分可观察的多智能体环境中的演员-评论家策略优化，载于：神经信息处理系统进展，2018年，第3422–3435页。'
- en: '[137] E. Lockhart, M. Lanctot, J. Pérolat, J. Lespiau, D. Morrill, F. Timbers,
    K. Tuyls, [Computing approximate equilibria in sequential adversarial games by
    exploitability descent](http://arxiv.org/abs/1903.05614), CoRR abs/1903.05614.
    [arXiv:1903.05614](http://arxiv.org/abs/1903.05614).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] E. Lockhart, M. Lanctot, J. Pérolat, J. Lespiau, D. Morrill, F. Timbers,
    K. Tuyls, [通过可利用性下降计算顺序对抗游戏中的近似均衡](http://arxiv.org/abs/1903.05614)，CoRR abs/1903.05614.
    [arXiv:1903.05614](http://arxiv.org/abs/1903.05614)。'
- en: URL [http://arxiv.org/abs/1903.05614](http://arxiv.org/abs/1903.05614)
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1903.05614](http://arxiv.org/abs/1903.05614)
- en: '[138] M. Johanson, N. Bard, N. Burch, M. Bowling, Finding optimal abstract
    strategies in extensive-form games, in: Twenty-Sixth AAAI Conference on Artificial
    Intelligence, 2012.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] M. Johanson, N. Bard, N. Burch, M. Bowling, 在扩展形式游戏中寻找最优抽象策略，收录于：第26届AAAI人工智能会议，2012年。'
- en: '[139] E. Kalai, E. Lehrer, Rational learning leads to Nash equilibrium, Econometrica:
    Journal of the Econometric Society (1993) 1019–1045.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] E. Kalai, E. Lehrer, 理性学习导致纳什均衡，《计量经济学》：计量经济学会期刊（1993）1019–1045。'
- en: '[140] T. W. Sandholm, R. H. Crites, Multiagent reinforcement learning in the
    iterated prisoner’s dilemma, Biosystems 37 (1-2) (1996) 147–166.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] T. W. Sandholm, R. H. Crites, 迭代囚徒困境中的多智能体强化学习，《生物系统》37 (1-2) (1996)
    147–166。'
- en: '[141] S. Singh, M. Kearns, Y. Mansour, Nash convergence of gradient dynamics
    in general-sum games, in: Proceedings of the Sixteenth conference on Uncertainty
    in artificial intelligence, Morgan Kaufmann Publishers Inc., 2000, pp. 541–548.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] S. Singh, M. Kearns, Y. Mansour, 梯度动态在一般总和博弈中的纳什收敛性，收录于：第十六届人工智能不确定性会议论文集，Morgan
    Kaufmann Publishers Inc.，2000年，页码 541–548。'
- en: '[142] B. Banerjee, J. Peng, Adaptive policy gradient in multiagent learning,
    in: Proceedings of the second international joint conference on Autonomous agents
    and multiagent systems, ACM, 2003, pp. 686–692.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] B. Banerjee, J. Peng, 多智能体学习中的自适应策略梯度，收录于：第二届国际联合会议关于自主代理与多智能体系统，ACM，2003年，页码
    686–692。'
- en: '[143] A. Greenwald, K. Hall, Correlated Q-learning, in: Proceedings of 17th
    International Conference on Autonomous Agents and Multiagent Systems, Washington,
    DC, USA, 2003, pp. 242–249.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Greenwald, K. Hall, 相关的 Q 学习，收录于：第17届国际自主代理与多智能体系统会议论文集，华盛顿特区，美国，2003年，页码
    242–249。'
- en: '[144] M. Bowling, Convergence problems of general-sum multiagent reinforcement
    learning, in: International Conference on Machine Learning, 2000, pp. 89–94.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] M. Bowling, 一般总和多智能体强化学习的收敛问题，收录于：国际机器学习会议，2000年，页码 89–94。'
- en: '[145] M. Bowling, Convergence and no-regret in multiagent learning, in: Advances
    in Neural Information Processing Systems, Vancouver, Canada, 2004, pp. 209–216.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Bowling, 多智能体学习中的收敛性与无悔性，收录于：神经信息处理系统进展，温哥华，加拿大，2004年，页码 209–216。'
- en: '[146] M. Zinkevich, A. Greenwald, M. L. Littman, Cyclic equilibria in Markov
    games, in: Advances in Neural Information Processing Systems, 2006, pp. 1641–1648.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] M. Zinkevich, A. Greenwald, M. L. Littman, 马尔可夫博弈中的循环均衡，收录于：神经信息处理系统进展，2006年，页码
    1641–1648。'
- en: '[147] M. Bowling, M. Veloso, Multiagent learning using a variable learning
    rate, Artificial Intelligence 136 (2) (2002) 215–250.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] M. Bowling, M. Veloso, 使用可变学习率的多智能体学习，《人工智能》136 (2) (2002) 215–250。'
- en: '[148] M. Kaisers, K. Tuyls, FAQ-learning in matrix games: demonstrating convergence
    near Nash equilibria, and bifurcation of attractors in the battle of sexes, in:
    AAAI Workshop on Interactive Decision Theory and Game Theory, San Francisco, CA,
    USA, 2011, pp. 309–316.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] M. Kaisers, K. Tuyls, 矩阵博弈中的FAQ学习：演示在纳什均衡附近的收敛性，以及在性别对抗中的吸引子分岔，收录于：AAAI互动决策理论与博弈理论研讨会，旧金山，加利福尼亚州，美国，2011年，页码
    309–316。'
- en: '[149] M. Wunder, M. L. Littman, M. Babes, Classes of Multiagent Q-learning
    Dynamics with epsilon-greedy Exploration, in: Proceedings of the 35th International
    Conference on Machine Learning, Haifa, Israel, 2010, pp. 1167–1174.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. Wunder, M. L. Littman, M. Babes, 具有 epsilon-greedy 探索的多智能体 Q 学习动态类别，收录于：第35届国际机器学习会议论文集，海法，以色列，2010年，页码
    1167–1174。'
- en: '[150] G. Tesauro, Extending Q-learning to general adaptive multi-agent systems,
    in: Advances in Neural Information Processing Systems, Vancouver, Canada, 2003,
    pp. 871–878.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] G. Tesauro, 将 Q 学习扩展到一般自适应多智能体系统，收录于：神经信息处理系统进展，温哥华，加拿大，2003年，页码 871–878。'
- en: '[151] M. Weinberg, J. S. Rosenschein, Best-response multiagent learning in
    non-stationary environments, in: Proceedings of the 3rd International Conference
    on Autonomous Agents and Multiagent Systems, New York, NY, USA, 2004, pp. 506–513.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] M. Weinberg, J. S. Rosenschein, 非静态环境下的最佳响应多智能体学习，收录于：第3届国际自主代理与多智能体系统会议论文集，纽约，纽约州，美国，2004年，页码
    506–513。'
- en: '[152] D. Chakraborty, P. Stone, Multiagent learning in the presence of memory-bounded
    agents, Autonomous Agents and Multi-Agent Systems 28 (2) (2013) 182–213.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] G. Weiss (Ed.),  Multiagent Systems, 2nd Edition, (Intelligent Robotics
    and Autonomous Agents series), MIT Press, Cambridge, MA, USA, 2013.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Multiagent Learning, Foundations and Recent Trends, [https://www.cs.utexas.edu/~larg/ijcai17_tutorial/multiagent_learning.pdf%****␣main.bbl␣Line␣775␣****](https://www.cs.utexas.edu/~larg/ijcai17_tutorial/multiagent_learning.pdf%****%20main.bbl%20Line%20775%20****),
    [Online; accessed 7-September-2018] (2017).'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru,
    R. Vicente, Multiagent cooperation and competition with deep reinforcement learning,
    PLOS ONE 12 (4) (2017) e0172395.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, Multi-agent Reinforcement
    Learning in Sequential Social Dilemmas, in: Proceedings of the 16th Conference
    on Autonomous Agents and Multiagent Systems, Sao Paulo, 2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M. Raghu, A. Irpan, J. Andreas, R. Kleinberg, Q. Le, J. Kleinberg, Can
    Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?, in: Proceedings
    of the 35th International Conference on Machine Learning, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, I. Mordatch, Emergent
    Complexity via Multi-Agent Competition., in: International Conference on Machine
    Learning, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J. Z. Leibo, J. Perolat, E. Hughes, S. Wheelwright, A. H. Marblestone,
    E. Duéñez-Guzmán, P. Sunehag, I. Dunning, T. Graepel, Malthusian reinforcement
    learning, in: 18th International Conference on Autonomous Agents and Multiagent
    Systems, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] I. Mordatch, P. Abbeel, Emergence of grounded compositional language
    in multi-agent populations, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A. Lazaridou, A. Peysakhovich, M. Baroni, Multi-Agent Cooperation and
    the Emergence of (Natural) Language, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. N. Foerster, Y. M. Assael, N. De Freitas, S. Whiteson, Learning to
    communicate with deep multi-agent reinforcement learning, in: Advances in Neural
    Information Processing Systems, 2016, pp. 2145–2153.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] S. Sukhbaatar, A. Szlam, R. Fergus, Learning Multiagent Communication
    with Backpropagation, in: Advances in Neural Information Processing Systems, 2016,
    pp. 2244–2252.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] P. Peng, Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, J. Wang, [Multiagent
    Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games.](http://arxiv.org/abs/1703.10069)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1703.10069](http://arxiv.org/abs/1703.10069)
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[165] E. Pesce, G. Montana, [Improving coordination in multi-agent deep reinforcement
    learning through memory-driven communication](http://arxiv.org/abs/1901.03887),
    CoRR abs/1901.03887. [arXiv:1901.03887](http://arxiv.org/abs/1901.03887).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.03887](http://arxiv.org/abs/1901.03887)
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[166] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, J. Vian, Deep Decentralized
    Multi-task Multi-Agent Reinforcement Learning under Partial Observability, in:
    Proceedings of the 34th International Conference on Machine Learning, Sydney,
    2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, J. Vian, 部分可观测下的深度去中心化多任务多智能体强化学习，发表于：第34届国际机器学习会议，悉尼，2017。'
- en: '[167] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, S. Whiteson, Counterfactual
    Multi-Agent Policy Gradients., in: 32nd AAAI Conference on Artificial Intelligence,
    2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, S. Whiteson, 反事实多智能体策略梯度，发表于：第32届AAAI人工智能会议，2017。'
- en: '[168] J. N. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr,
    P. Kohli, S. Whiteson, Stabilising Experience Replay for Deep Multi-Agent Reinforcement
    Learning., in: International Conference on Machine Learning, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] J. N. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr,
    P. Kohli, S. Whiteson, 深度多智能体强化学习中的经验重放稳定性研究，发表于：国际机器学习会议，2017。'
- en: '[169] H. He, J. Boyd-Graber, K. Kwok, H. Daume, Opponent modeling in deep reinforcement
    learning, in: 33rd International Conference on Machine Learning, 2016, pp. 2675–2684.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] H. He, J. Boyd-Graber, K. Kwok, H. Daume, 深度强化学习中的对手建模，发表于：第33届国际机器学习会议，2016，第2675–2684页。'
- en: '[170] R. Raileanu, E. Denton, A. Szlam, R. Fergus, Modeling Others using Oneself
    in Multi-Agent Reinforcement Learning., in: International Conference on Machine
    Learning, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] R. Raileanu, E. Denton, A. Szlam, R. Fergus, 在多智能体强化学习中使用自身建模他人，发表于：国际机器学习会议，2018。'
- en: '[171] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, C.-Y. Lee, A Deep Policy
    Inference Q-Network for Multi-Agent Systems, in: International Conference on Autonomous
    Agents and Multiagent Systems, 2018.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, C.-Y. Lee, 针对多智能体系统的深度策略推断Q网络，发表于：国际自主代理和多智能体系统会议，2018。'
- en: '[172] M. Lanctot, V. F. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat,
    D. Silver, T. Graepel, A Unified Game-Theoretic Approach to Multiagent Reinforcement
    Learning., in: Advances in Neural Information Processing Systems, 2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Lanctot, V. F. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat,
    D. Silver, T. Graepel, 统一博弈论方法的多智能体强化学习，发表于：神经信息处理系统进展，2017。'
- en: '[173] J. Heinrich, D. Silver, [Deep Reinforcement Learning from Self-Play in
    Imperfect-Information Games](http://arxiv.org/abs/1603.01121).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] J. Heinrich, D. Silver, [在不完全信息游戏中进行自我对战的深度强化学习](http://arxiv.org/abs/1603.01121)。'
- en: URL [http://arxiv.org/abs/1603.01121](http://arxiv.org/abs/1603.01121)
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1603.01121](http://arxiv.org/abs/1603.01121)
- en: '[174] N. C. Rabinowitz, F. Perbet, H. F. Song, C. Zhang, S. M. A. Eslami, M. Botvinick,
    Machine Theory of Mind., in: International Conference on Machine Learning, Stockholm,
    Sweden, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] N. C. Rabinowitz, F. Perbet, H. F. Song, C. Zhang, S. M. A. Eslami, M.
    Botvinick, 机器心智理论，发表于：国际机器学习会议，斯德哥尔摩，瑞典，2018。'
- en: '[175] T. Yang, J. Hao, Z. Meng, C. Zhang, Y. Z. Z. Zheng, Towards Efficient
    Detection and Optimal Response against Sophisticated Opponents, in: IJCAI, 2019.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] T. Yang, J. Hao, Z. Meng, C. Zhang, Y. Z. Z. Zheng, 针对复杂对手的高效检测与优化响应，发表于：IJCAI，2019。'
- en: '[176] A. Lerer, A. Peysakhovich, [Maintaining cooperation in complex social
    dilemmas using deep reinforcement learning](http://arxiv.org/abs/1707.01068),
    CoRR abs/1707.01068. [arXiv:1707.01068](http://arxiv.org/abs/1707.01068).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] A. Lerer, A. Peysakhovich, [在复杂社会困境中使用深度强化学习维持合作](http://arxiv.org/abs/1707.01068)，CoRR
    abs/1707.01068. [arXiv:1707.01068](http://arxiv.org/abs/1707.01068)。'
- en: URL [http://arxiv.org/abs/1707.01068](http://arxiv.org/abs/1707.01068)
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1707.01068](http://arxiv.org/abs/1707.01068)
- en: '[177] W. Kim, M. Cho, Y. Sung, Message-Dropout: An Efficient Training Method
    for Multi-Agent Deep Reinforcement Learning, in: 33rd AAAI Conference on Artificial
    Intelligence, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] W. Kim, M. Cho, Y. Sung, 消息丢失：多智能体深度强化学习的高效训练方法，发表于：第33届AAAI人工智能会议，2019。'
- en: '[178] Y. Zheng, J. Hao, Z. Zhang, [Weighted double deep multiagent reinforcement
    learning in stochastic cooperative environments](http://arXiv.org/abs/1802.08534).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Y. Zheng, J. Hao, Z. Zhang, [加权双深度多智能体强化学习在随机合作环境中的应用](http://arXiv.org/abs/1802.08534)。'
- en: URL [http://arXiv.org/abs/1802.08534](http://arXiv.org/abs/1802.08534)
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arXiv.org/abs/1802.08534](http://arXiv.org/abs/1802.08534)
- en: '[179] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.
    Castañeda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat,
    T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, T. Graepel,
    [Human-level performance in 3d multiplayer games with population-based reinforcement
    learning](https://science.sciencemag.org/content/364/6443/859), Science 364 (6443)
    (2019) 859–865. [arXiv:https://science.sciencemag.org/content/364/6443/859.full.pdf](http://arxiv.org/abs/https://science.sciencemag.org/content/364/6443/859.full.pdf),
    [doi:10.1126/science.aau6249](https://doi.org/10.1126/science.aau6249).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.
    Castañeda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat,
    T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, T. Graepel,
    [人类水平的3D多人游戏中的基于群体的强化学习](https://science.sciencemag.org/content/364/6443/859)，科学
    364 (6443) (2019) 859–865。 [arXiv:https://science.sciencemag.org/content/364/6443/859.full.pdf](http://arxiv.org/abs/https://science.sciencemag.org/content/364/6443/859.full.pdf)，[doi:10.1126/science.aau6249](https://doi.org/10.1126/science.aau6249)。'
- en: URL [https://science.sciencemag.org/content/364/6443/859](https://science.sciencemag.org/content/364/6443/859)
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://science.sciencemag.org/content/364/6443/859](https://science.sciencemag.org/content/364/6443/859)
- en: '[180] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M. Jaderberg,
    M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, T. Graepel, Value-Decomposition
    Networks For Cooperative Multi-Agent Learning Based On Team Reward., in: Proceedings
    of 17th International Conference on Autonomous Agents and Multiagent Systems,
    Stockholm, Sweden, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M.
    Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, T. Graepel, 基于团队奖励的合作多智能体学习中的价值分解网络，见：第17届国际自主智能体和多智能体系统大会论文集，瑞典斯德哥尔摩，2018年。'
- en: '[181] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. N. Foerster,
    S. Whiteson, QMIX - Monotonic Value Function Factorisation for Deep Multi-Agent
    Reinforcement Learning., in: International Conference on Machine Learning, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. N. Foerster,
    S. Whiteson, QMIX - 单调价值函数分解用于深度多智能体强化学习，见：2018年国际机器学习大会。'
- en: '[182] J. K. Gupta, M. Egorov, M. Kochenderfer, Cooperative multi-agent control
    using deep reinforcement learning, in: G. Sukthankar, J. A. Rodriguez-Aguilar
    (Eds.), Autonomous Agents and Multiagent Systems, Springer International Publishing,
    Cham, 2017, pp. 66–83.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. K. Gupta, M. Egorov, M. Kochenderfer, 使用深度强化学习进行合作多智能体控制，见：G. Sukthankar,
    J. A. Rodriguez-Aguilar（编），《自主智能体与多智能体系统》，施普林格国际出版，Cham，2017年，页码：66–83。'
- en: '[183] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, S. Russell, Robust multi-agent
    reinforcement learning via minimax deep deterministic policy gradient, in: AAAI
    Conference on Artificial Intelligence, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, S. Russell, 通过最小化深度确定性策略梯度的鲁棒多智能体强化学习，见：2019年美国人工智能会议。'
- en: '[184] Y. Zheng, Z. Meng, J. Hao, Z. Zhang, T. Yang, C. Fan, A Deep Bayesian
    Policy Reuse Approach Against Non-Stationary Agents, in: Advances in Neural Information
    Processing Systems, 2018, pp. 962–972.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. Zheng, Z. Meng, J. Hao, Z. Zhang, T. Yang, C. Fan, 针对非平稳智能体的深度贝叶斯策略重用方法，见：2018年神经信息处理系统进展，页码：962–972。'
- en: '[185] R. Powers, Y. Shoham, Learning against opponents with bounded memory,
    in: Proceedings of the 19th International Joint Conference on Artificial Intelligence,
    Edinburg, Scotland, UK, 2005, pp. 817–822.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] R. Powers, Y. Shoham, 在有限记忆对手中学习，见：第19届国际联合人工智能大会论文集，英国爱丁堡，2005年，页码：817–822。'
- en: '[186] M. L. Littman, P. Stone, Implicit Negotiation in Repeated Games, ATAL
    ’01: Revised Papers from the 8th International Workshop on Intelligent Agents
    VIII.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M. L. Littman, P. Stone, 重复博弈中的隐式谈判，ATAL ’01：第8届智能体国际研讨会修订论文集。'
- en: '[187] R. Axelrod, W. D. Hamilton, The evolution of cooperation, Science 211 (27)
    (1981) 1390–1396.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] R. Axelrod, W. D. Hamilton, 合作的进化，科学 211 (27) (1981) 1390–1396。'
- en: '[188] E. Munoz de Cote, A. Lazaric, M. Restelli, Learning to cooperate in multi-agent
    social dilemmas, in: Proceedings of the 5th International Conference on Autonomous
    Agents and Multiagent Systems, Hakodate, Hokkaido, Japan, 2006, pp. 783–785.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] E. Munoz de Cote, A. Lazaric, M. Restelli, 在多智能体社会困境中学习合作，见：第5届国际自主智能体和多智能体系统大会论文集，日本函馆，2006年，页码：783–785。'
- en: '[189] J. L. Stimpson, M. A. Goodrich, Learning to cooperate in a social dilemma:
    A satisficing approach to bargaining, in: Proceedings of the 20th International
    Conference on Machine Learning (ICML-03), 2003, pp. 728–735.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J. L. Stimpson, M. A. Goodrich, 在社会困境中学习合作：一种满意的谈判方法，见：第20届国际机器学习大会（ICML-03），2003年，页码：728–735。'
- en: '[190] G. W. Brown, Iterative solution of games by fictitious play, Activity
    analysis of production and allocation 13 (1) (1951) 374–376.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] G. W. Brown, 通过虚拟游戏的迭代解法，生产与分配的活动分析 13 (1) (1951) 374–376。'
- en: '[191] D. Monderer, L. S. Shapley, Fictitious play property for games with identical
    interests, Journal of economic theory 68 (1) (1996) 258–265.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] D. Monderer, L. S. Shapley, 具有相同利益的游戏的虚拟游戏属性，经济理论杂志 68 (1) (1996) 258–265。'
- en: '[192] G. Tesauro, Temporal difference learning and TD-Gammon, Communications
    of the ACM 38 (3) (1995) 58–68.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] G. Tesauro, 时序差分学习与TD-Gammon，ACM通讯 38 (3) (1995) 58–68。'
- en: '[193] M. Bowling, N. Burch, M. Johanson, O. Tammelin, Heads-up limit hold’em
    poker is solved, Science 347 (6218) (2015) 145–149.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] M. Bowling, N. Burch, M. Johanson, O. Tammelin, heads-up limit hold’em扑克已被解决，Science
    347 (6218) (2015) 145–149。'
- en: '[194] J. Z. Leibo, E. Hughes, M. Lanctot, T. Graepel, [Autocurricula and the
    emergence of innovation from social interaction: A manifesto for multi-agent intelligence
    research](http://arxiv.org/abs/1903.00742), CoRR abs/1903.00742. [arXiv:1903.00742](http://arxiv.org/abs/1903.00742).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] J. Z. Leibo, E. Hughes, M. Lanctot, T. Graepel, [自适应课程与社会互动中创新的出现：多智能体智能研究的宣言](http://arxiv.org/abs/1903.00742)，CoRR
    abs/1903.00742. [arXiv:1903.00742](http://arxiv.org/abs/1903.00742)。'
- en: URL [http://arxiv.org/abs/1903.00742](http://arxiv.org/abs/1903.00742)
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1903.00742](http://arxiv.org/abs/1903.00742)
- en: '[195] S. Samothrakis, S. Lucas, T. Runarsson, D. Robles, Coevolving game-playing
    agents: Measuring performance and intransitivities, IEEE Transactions on Evolutionary
    Computation 17 (2) (2013) 213–226.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] S. Samothrakis, S. Lucas, T. Runarsson, D. Robles, 共同进化游戏代理：性能与非传递性测量，IEEE进化计算交易
    17 (2) (2013) 213–226。'
- en: '[196] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, R. Munos,
    Unifying count-based exploration and intrinsic motivation, in: Advances in Neural
    Information Processing Systems, 2016, pp. 1471–1479.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, R. Munos,
    统一基于计数的探索与内在动机，载于：神经信息处理系统进展，2016年，页1471–1479。'
- en: '[197] D. E. Moriarty, A. C. Schultz, J. J. Grefenstette, Evolutionary algorithms
    for reinforcement learning, Journal of Artificial Intelligence Research 11 (1999)
    241–276.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] D. E. Moriarty, A. C. Schultz, J. J. Grefenstette, 强化学习的进化算法，人工智能研究杂志
    11 (1999) 241–276。'
- en: '[198] F. A. Oliehoek, E. D. De Jong, N. Vlassis, The parallel Nash memory for
    asymmetric games, in: Proceedings of the 8th annual conference on Genetic and
    evolutionary computation, ACM, 2006, pp. 337–344.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] F. A. Oliehoek, E. D. De Jong, N. Vlassis, 不对称博弈的平行纳什记忆，载于：第八届年度遗传与进化计算会议论文集，ACM，2006年，页337–344。'
- en: '[199] L. Bull, T. C. Fogarty, M. Snaith, Evolution in multi-agent systems:
    Evolving communicating classifier systems for gait in a quadrupedal robot, in:
    Proceedings of the 6th International Conference on Genetic Algorithms, Morgan
    Kaufmann Publishers Inc., 1995, pp. 382–388.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] L. Bull, T. C. Fogarty, M. Snaith, 多智能体系统中的进化：用于四足机器人步态的进化通信分类器系统，载于：第六届国际遗传算法会议论文集，Morgan
    Kaufmann Publishers Inc.，1995年，页382–388。'
- en: '[200] L. Bull, Evolutionary computing in multi-agent environments: Operators,
    in: International Conference on Evolutionary Programming, Springer, 1998, pp.
    43–52.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] L. Bull, 多智能体环境中的进化计算：操作符，载于：进化编程国际会议，Springer，1998年，页43–52。'
- en: '[201] H. Iba, Emergent cooperation for multiple agents using genetic programming,
    in: International Conference on Parallel Problem Solving from Nature, Springer,
    1996, pp. 32–41.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] H. Iba, 使用遗传编程的多智能体的涌现合作，载于：自然并行问题解决国际会议，Springer，1996年，页32–41。'
- en: '[202] E. Todorov, T. Erez, Y. Tassa, MuJoCo - A physics engine for model-based
    control, Intelligent Robots and Systems (2012) 5026–5033.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] E. Todorov, T. Erez, Y. Tassa, MuJoCo - 基于模型的控制物理引擎，智能机器人与系统 (2012) 5026–5033。'
- en: '[203] V. Gullapalli, A. G. Barto, Shaping as a method for accelerating reinforcement
    learning, in: Proceedings of the 1992 IEEE international symposium on intelligent
    control, IEEE, 1992, pp. 554–559.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] V. Gullapalli, A. G. Barto, 作为加速强化学习的方法的塑造，载于：1992年IEEE国际智能控制研讨会论文集，IEEE，1992年，页554–559。'
- en: '[204] S. Mahadevan, J. Connell, Automatic programming of behavior-based robots
    using reinforcement learning, Artificial intelligence 55 (2-3) (1992) 311–365.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] S. Mahadevan, J. Connell, 使用强化学习自动编程基于行为的机器人，人工智能 55 (2-3) (1992) 311–365。'
- en: '[205] G. Konidaris, A. Barto, Autonomous shaping: Knowledge transfer in reinforcement
    learning, in: Proceedings of the 23rd international conference on Machine learning,
    ACM, 2006, pp. 489–496.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] G. Konidaris, A. Barto, 自主塑造：强化学习中的知识转移，载于：第23届国际机器学习会议论文集，ACM，2006年，页489–496。'
- en: '[206] A. Y. Ng, D. Harada, S. J. Russell, Policy invariance under reward transformations:
    Theory and application to reward shaping, in: Proceedings of the Sixteenth International
    Conference on Machine Learning, 1999, pp. 278–287.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] A. Y. Ng, D. Harada, S. J. Russell, 奖励转换下的策略不变性：理论与奖励塑造的应用，见：第十六届国际机器学习会议论文集，1999年，第278–287页。'
- en: '[207] P. Erdös, J. L. Selfridge, On a combinatorial game, Journal of Combinatorial
    Theory, Series A 14 (3) (1973) 298–301.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] P. Erdös, J. L. Selfridge, 关于一个组合博弈，组合博弈理论期刊，系列A 14 (3) (1973) 298–301。'
- en: '[208] J. Spencer, Randomization, derandomization and antirandomization: three
    games, Theoretical Computer Science 131 (2) (1994) 415–429.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] J. Spencer, 随机化、去随机化与反随机化：三种博弈，理论计算机科学 131 (2) (1994) 415–429。'
- en: '[209] D. Fudenberg, J. Tirole, Game Theory, The MIT Press, 1991.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] D. Fudenberg, J. Tirole, 博弈论，麻省理工学院出版社，1991年。'
- en: '[210] L. v. d. Maaten, G. Hinton, Visualizing data using t-SNE, Journal of
    machine learning research 9 (Nov) (2008) 2579–2605.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] L. v. d. Maaten, G. Hinton, 使用t-SNE可视化数据，机器学习研究期刊 9 (Nov) (2008) 2579–2605。'
- en: '[211] T. Zahavy, N. Ben-Zrihem, S. Mannor, Graying the black box: Understanding
    DQNs, in: International Conference on Machine Learning, 2016, pp. 1899–1908.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] T. Zahavy, N. Ben-Zrihem, S. Mannor, 揭开黑箱的神秘面纱：理解DQN，见：国际机器学习会议，2016年，第1899–1908页。'
- en: '[212] E. Beeching, C. Wolf, J. Dibangoye, O. Simonin, [Deep Reinforcement Learning
    on a Budget: 3D Control and Reasoning Without a Supercomputer](http://arxiv.org/abs/1904.01806),
    CoRR abs/1904.01806. [arXiv:1904.01806](http://arxiv.org/abs/1904.01806).'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] E. Beeching, C. Wolf, J. Dibangoye, O. Simonin, [预算下的深度强化学习：无需超级计算机的3D控制与推理](http://arxiv.org/abs/1904.01806)，CoRR
    abs/1904.01806。 [arXiv:1904.01806](http://arxiv.org/abs/1904.01806)。'
- en: URL [http://arxiv.org/abs/1904.01806](http://arxiv.org/abs/1904.01806)
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1904.01806](http://arxiv.org/abs/1904.01806)
- en: '[213] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
    Dropout: a simple way to prevent neural networks from overfitting, The Journal
    of Machine Learning Research 15 (1) (2014) 1929–1958.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, Dropout：一种简单的防止神经网络过拟合的方法，机器学习研究期刊
    15 (1) (2014) 1929–1958。'
- en: '[214] M. Schuster, K. K. Paliwal, Bidirectional recurrent neural networks,
    IEEE Transactions on Signal Processing 45 (11) (1997) 2673–2681.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] M. Schuster, K. K. Paliwal, 双向递归神经网络，IEEE信号处理学报 45 (11) (1997) 2673–2681。'
- en: '[215] R. Lowe, J. Foerster, Y.-L. Boureau, J. Pineau, Y. Dauphin, On the pitfalls
    of measuring emergent communication, in: 18th International Conference on Autonomous
    Agents and Multiagent Systems, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] R. Lowe, J. Foerster, Y.-L. Boureau, J. Pineau, Y. Dauphin, 测量突现通信的陷阱，见：第18届自主代理与多智能体系统国际会议，2019年。'
- en: '[216] M. Tambe, Towards flexible teamwork, Journal of artificial intelligence
    research 7 (1997) 83–124.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] M. Tambe, 朝着灵活的团队合作，人工智能研究期刊 7 (1997) 83–124。'
- en: '[217] B. J. Grosz, S. Kraus, Collaborative plans for complex group action,
    Artificial Intelligence 86 (2) (1996) 269–357.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] B. J. Grosz, S. Kraus, 复杂群体行动的协作计划，人工智能 86 (2) (1996) 269–357。'
- en: '[218] D. Precup, R. S. Sutton, S. Singh, Eligibility traces for off-policy
    policy evaluation, in: Proceedings of the Seventeenth International Conference
    on Machine Learning., 2000.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] D. Precup, R. S. Sutton, S. Singh, 用于离策略策略评估的资格迹，见：第十七届国际机器学习会议论文集，2000年。'
- en: '[219] J. Frank, S. Mannor, D. Precup, Reinforcement learning in the presence
    of rare events, in: Proceedings of the 25th international conference on Machine
    learning, ACM, 2008, pp. 336–343.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] J. Frank, S. Mannor, D. Precup, 在稀有事件存在下的强化学习，见：第25届国际机器学习会议论文集，ACM，2008年，第336–343页。'
- en: '[220] T. I. Ahamed, V. S. Borkar, S. Juneja, Adaptive importance sampling technique
    for markov chains using stochastic approximation, Operations Research 54 (3) (2006)
    489–504.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] T. I. Ahamed, V. S. Borkar, S. Juneja, 使用随机逼近的马尔可夫链自适应重要性抽样技术，运筹学 54
    (3) (2006) 489–504。'
- en: '[221] K. A. Ciosek, S. Whiteson, Offer: Off-environment reinforcement learning,
    in: Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] K. A. Ciosek, S. Whiteson, Offer: 离环境强化学习，见：第三十一届AAAI人工智能会议，2017年。'
- en: '[222] D. Bloembergen, M. Kaisers, K. Tuyls, Lenient frequency adjusted Q-learning,
    in: Proceedings of the 22nd Belgian/Netherlands Artificial Intelligence Conference,
    2010.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] D. Bloembergen, M. Kaisers, K. Tuyls, 宽松的频率调整Q学习，见：第22届比利时/荷兰人工智能会议论文集，2010年。'
- en: '[223] L. Panait, K. Sullivan, S. Luke, Lenience towards teammates helps in
    cooperative multiagent learning, in: Proceedings of the 5th International Conference
    on Autonomous Agents and Multiagent Systems, Hakodate, Japan, 2006.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] L. Panait, K. Sullivan, S. Luke, 对队友的宽容有助于合作多智能体学习，见：第五届国际自主代理和多智能体系统会议论文集，日本函馆，2006年。'
- en: '[224] L. Panait, K. Tuyls, S. Luke, Theoretical advantages of lenient learners:
    An evolutionary game theoretic perspective, JMLR 9 (Mar) (2008) 423–457.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] L. Panait, K. Tuyls, S. Luke, 宽容学习者的理论优势：一种进化博弈理论视角，《JMLR》9 (Mar) (2008)
    423–457。'
- en: '[225] M. Lauer, M. Riedmiller, An algorithm for distributed reinforcement learning
    in cooperative multi-agent systems, in: In Proceedings of the Seventeenth International
    Conference on Machine Learning, 2000.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] M. Lauer, M. Riedmiller, 一种用于合作多智能体系统中的分布式强化学习算法，见：第十七届国际机器学习会议论文集，2000年。'
- en: '[226] R. Caruana, Multitask learning, Machine learning 28 (1) (1997) 41–75.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] R. Caruana, 多任务学习，《机器学习》28 (1) (1997) 41–75。'
- en: '[227] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, R. Hadsell, Policy Distillation, in: International
    Conference on Learning Representations, 2016.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, R. Hadsell, 政策蒸馏，见：国际学习表征会议，2016年。'
- en: '[228] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural
    network, in: NIPS Deep Learning Workshop, 2014.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] G. Hinton, O. Vinyals, J. Dean, 提炼神经网络中的知识，见：NIPS深度学习研讨会，2014年。'
- en: '[229] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver,
    K. Kavukcuoglu, FeUdal Networks for Hierarchical Reinforcement Learning., International
    Conference On Machine Learning.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D.
    Silver, K. Kavukcuoglu, 层级强化学习的FeUdal网络，国际机器学习会议。'
- en: '[230] P. Dayan, G. E. Hinton, Feudal reinforcement learning, in: Advances in
    neural information processing systems, 1993, pp. 271–278.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] P. Dayan, G. E. Hinton, 封建强化学习，见：神经信息处理系统进展，1993年，第271–278页。'
- en: '[231] S. P. Singh, Transfer of learning by composing solutions of elemental
    sequential tasks, Machine Learning 8 (3-4) (1992) 323–339.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] S. P. Singh, 通过组合基本顺序任务的解决方案进行学习转移，《机器学习》8 (3-4) (1992) 323–339。'
- en: '[232] Capture the Flag: the emergence of complex cooperative agents, [https://deepmind.com/blog/capture-the-flag/](https://deepmind.com/blog/capture-the-flag/),
    [Online; accessed 7-September-2018] (2018).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] 捕捉旗帜：复杂合作智能体的出现，[https://deepmind.com/blog/capture-the-flag/](https://deepmind.com/blog/capture-the-flag/)，[在线；访问日期：2018年9月7日]
    (2018)。'
- en: '[233] C. D. Rosin, R. K. Belew, New methods for competitive coevolution, Evolutionary
    computation 5 (1) (1997) 1–29.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] C. D. Rosin, R. K. Belew, 竞争性共同进化的新方法，《进化计算》5 (1) (1997) 1–29。'
- en: '[234] J. Lehman, K. O. Stanley, Exploiting open-endedness to solve problems
    through the search for novelty., in: ALIFE, 2008, pp. 329–336.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] J. Lehman, K. O. Stanley, 利用开放性来通过寻求新颖性解决问题，见：ALIFE，2008年，第329–336页。'
- en: '[235] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,
    A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, et al., [Population
    based training of neural networks](http://arxiv.org/abs/1711.09846).'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,
    A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, 等，[基于人群的神经网络训练](http://arxiv.org/abs/1711.09846)。'
- en: URL [http://arxiv.org/abs/1711.09846](http://arxiv.org/abs/1711.09846)
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1711.09846](http://arxiv.org/abs/1711.09846)
- en: '[236] A. E. Elo, The rating of chessplayers, past and present, Arco Pub., 1978.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] A. E. Elo, 国际象棋选手的评级，Arco出版公司，1978年。'
- en: '[237] R. Herbrich, T. Minka, T. Graepel, TrueSkill™: a Bayesian skill rating
    system, in: Advances in neural information processing systems, 2007, pp. 569–576.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] R. Herbrich, T. Minka, T. Graepel, TrueSkill™：一种贝叶斯技能评分系统，见：神经信息处理系统进展，2007年，第569–576页。'
- en: '[238] S. Omidshafiei, C. Papadimitriou, G. Piliouras, K. Tuyls, M. Rowland,
    J.-B. Lespiau, W. M. Czarnecki, M. Lanctot, J. Perolat, R. Munos, <math id="bib.bib238.1.m1.1"
    class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib238.1.m1.1a"><mi
    id="bib.bib238.1.m1.1.1" xref="bib.bib238.1.m1.1.1.cmml">α</mi><annotation-xml
    encoding="MathML-Content" id="bib.bib238.1.m1.1b"><ci id="bib.bib238.1.m1.1.1.cmml"
    xref="bib.bib238.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="bib.bib238.1.m1.1c">\alpha</annotation></semantics></math>-Rank: Multi-Agent
    Evaluation by Evolution, Scientific Reports 9.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] S. Omidshafiei, C. Papadimitriou, G. Piliouras, K. Tuyls, M. Rowland,
    J.-B. Lespiau, W. M. Czarnecki, M. Lanctot, J. Perolat, R. Munos, <math id="bib.bib238.1.m1.1"
    class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib238.1.m1.1a"><mi
    id="bib.bib238.1.m1.1.1" xref="bib.bib238.1.m1.1.1.cmml">α</mi><annotation-xml
    encoding="MathML-Content" id="bib.bib238.1.m1.1b"><ci id="bib.bib238.1.m1.1.1.cmml"
    xref="bib.bib238.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="bib.bib238.1.m1.1c">\alpha</annotation></semantics></math>-Rank: 通过进化进行多智能体评估，《科学报告》9。'
- en: '[239] T. Back, Evolutionary algorithms in theory and practice: evolution strategies,
    evolutionary programming, genetic algorithms, Oxford university press, 1996.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] T. Back, 理论与实践中的进化算法：进化策略、进化编程、遗传算法，牛津大学出版社，1996年。'
- en: '[240] K. A. De Jong, Evolutionary computation: a unified approach, MIT press,
    2006.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] K. A. De Jong, 进化计算：统一方法，MIT出版社，2006年。'
- en: '[241] K. Tumer, A. Agogino, Distributed agent-based air traffic flow management,
    in: Proceedings of the 6th International Conference on Autonomous Agents and Multiagent
    Systems, Honolulu, Hawaii, 2007.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] K. Tumer, A. Agogino, 基于分布式智能体的空中交通流管理，发表于：第6届国际自主代理与多智能体系统会议论文集，夏威夷檀香山，2007年。'
- en: '[242] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factored MDPs,
    in: Advances in neural information processing systems, 2002, pp. 1523–1530.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] C. Guestrin, D. Koller, R. Parr, 基于分解MDP的多智能体规划，发表于：神经信息处理系统进展，2002年，第1523–1530页。'
- en: '[243] J. R. Kok, N. Vlassis, Sparse cooperative Q-learning, in: Proceedings
    of the twenty-first international conference on Machine learning, ACM, 2004, p. 61.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] J. R. Kok, N. Vlassis, 稀疏合作Q学习，发表于：第21届国际机器学习会议论文集，ACM，2004年，第61页。'
- en: '[244] J. Castellini, F. A. Oliehoek, R. Savani, S. Whiteson, The Representational
    Capacity of Action-Value Networks for Multi-Agent Reinforcement Learning, in:
    18th International Conference on Autonomous Agents and Multiagent Systems, 2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] J. Castellini, F. A. Oliehoek, R. Savani, S. Whiteson, 多智能体强化学习中行动值网络的表示能力，发表于：第18届国际自主代理与多智能体系统会议，2019年。'
- en: '[245] P. J. Gmytrasiewicz, P. Doshi, A framework for sequential planning in
    multiagent settings, Journal of Artificial Intelligence Research 24 (1) (2005)
    49–79.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] P. J. Gmytrasiewicz, P. Doshi, 多智能体环境下的序贯规划框架，《人工智能研究杂志》24 (1) (2005)
    49–79。'
- en: '[246] J. C. Harsanyi, Games with incomplete information played by “Bayesian”
    players, I–III Part I. The basic model, Management science 14 (3) (1967) 159–182.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] J. C. Harsanyi, “贝叶斯”玩家进行的非完全信息博弈，I–III 第一部分：基本模型，《管理科学》14 (3) (1967)
    159–182。'
- en: '[247] S. Barrett, P. Stone, S. Kraus, A. Rosenfeld, Teamwork with Limited Knowledge
    of Teammates., in: Proceedings of the Twenty-Seventh AAAI Conference on Artificial
    Intelligence, Bellevue, WS, USA, 2013, pp. 102–108.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] S. Barrett, P. Stone, S. Kraus, A. Rosenfeld, 有限队友知识的团队合作，发表于：第27届AAAI人工智能会议论文集，华盛顿州贝尔维尤，美国，2013年，第102–108页。'
- en: '[248] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, G. E. Hinton, et al., Adaptive
    mixtures of local experts., Neural computation 3 (1) (1991) 79–87.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, G. E. Hinton 等，局部专家的自适应混合，《神经计算》3
    (1) (1991) 79–87。'
- en: '[249] J. Heinrich, M. Lanctot, D. Silver, Fictitious self-play in extensive-form
    games, in: International Conference on Machine Learning, 2015, pp. 805–813.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] J. Heinrich, M. Lanctot, D. Silver, 广泛形式博弈中的虚拟自我博弈，发表于：国际机器学习会议，2015年，第805–813页。'
- en: '[250] J. F. Nash, Equilibrium points in n-person games, Proceedings of the
    National Academy of Sciences 36 (1) (1950) 48–49.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] J. F. Nash, n人博弈中的均衡点，《美国国家科学院院刊》36 (1) (1950) 48–49。'
- en: '[251] J. Von Neumann, O. Morgenstern, Theory of games and economic behavior,
    Vol. 51, Bull. Amer. Math. Soc, 1945.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] J. Von Neumann, O. Morgenstern, 博弈论与经济行为，第51卷，《美国数学学会公告》，1945年。'
- en: '[252] J. S. Shamma, G. Arslan, Dynamic fictitious play, dynamic gradient play,
    and distributed convergence to Nash equilibria, IEEE Transactions on Automatic
    Control 50 (3) (2005) 312–327.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] J. S. Shamma, G. Arslan, 动态虚拟博弈、动态梯度博弈以及向纳什均衡的分布式收敛，《IEEE自动控制学报》50 (3)
    (2005) 312–327。'
- en: '[253] W. E. Walsh, R. Das, G. Tesauro, J. O. Kephart, Analyzing complex strategic
    interactions in multi-agent systems, AAAI-02 Workshop on Game-Theoretic and Decision-Theoretic
    Agents (2002) 109–118.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] M. Johanson, K. Waugh, M. Bowling, M. Zinkevich, Accelerating best response
    calculation in large extensive games, in: Twenty-Second International Joint Conference
    on Artificial Intelligence, 2011.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] C. F. Camerer, T.-H. Ho, J.-K. Chong, A cognitive hierarchy model of
    games, The Quarterly Journal of Economics 119 (3) (2004) 861.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] M. Costa Gomes, V. P. Crawford, B. Broseta, Cognition and Behavior in
    Normal–Form Games: An Experimental Study, Econometrica 69 (5) (2001) 1193–1235.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] J. Morimoto, K. Doya, Robust reinforcement learning, Neural computation
    17 (2) (2005) 335–359.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] L. Pinto, J. Davidson, R. Sukthankar, A. Gupta, Robust adversarial reinforcement
    learning, in: Proceedings of the 34th International Conference on Machine Learning-Volume
    70, JMLR. org, 2017, pp. 2817–2826.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] R. Powers, Y. Shoham, T. Vu, A general criterion and an algorithmic framework
    for learning in multi-agent systems, Machine Learning 67 (1-2) (2007) 45–76.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] J. W. Crandall, M. A. Goodrich, Learning to compete, coordinate, and
    cooperate in repeated games using reinforcement learning, Machine Learning 82 (3)
    (2011) 281–314.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] M. Johanson, M. A. Zinkevich, M. Bowling, Computing Robust Counter-Strategies.,
    in: Advances in Neural Information Processing Systems, Vancouver, BC, Canada,
    2007, pp. 721–728.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] P. McCracken, M. Bowling, Safe strategies for agent modelling in games,
    in: AAAI Fall Symposium, 2004, pp. 103–110.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] S. Damer, M. Gini, Safely using predictions in general-sum normal form
    games, in: Proceedings of the 16th Conference on Autonomous Agents and Multiagent
    Systems, Sao Paulo, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] C. Zhang, V. Lesser, Multi-agent learning with policy prediction, in:
    Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] P. J. Gmytrasiewicz, E. H. Durfee, Rational Coordination in Multi-Agent
    Environments, Autonomous Agents and Multi-Agent Systems 3 (4) (2000) 319–350.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] C. F. Camerer, T.-H. Ho, J.-K. Chong, Behavioural Game Theory: Thinking,
    Learning and Teaching, in: Advances in Understanding Strategic Behavior, New York,
    2004, pp. 120–180.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] D. Carmel, S. Markovitch, Incorporating opponent models into adversary
    search, in: AAAI/IAAI, Vol. 1, 1996, pp. 120–125.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] H. de Weerd, R. Verbrugge, B. Verheij, How much does it help to know
    what she knows you know? An agent-based simulation study, Artificial Intelligence
    199-200 (C) (2013) 67–92.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] P. Hernandez-Leal, M. Kaisers, Towards a Fast Detection of Opponents
    in Repeated Stochastic Games, in: G. Sukthankar, J. A. Rodriguez-Aguilar (Eds.),
    Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, Sao
    Paulo, Brazil, May 8-12, 2017, Revised Selected Papers, 2017, pp. 239–257.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] P. Hernandez-Leal, M. E. Taylor, B. Rosman, L. E. Sucar, E. Munoz de
    Cote, Identifying and Tracking Switching, Non-stationary Opponents: a Bayesian
    Approach, in: Multiagent Interaction without Prior Coordination Workshop at AAAI,
    Phoenix, AZ, USA, 2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] B. Rosman, M. Hawasly, S. Ramamoorthy, Bayesian Policy Reuse, Machine
    Learning 104 (1) (2016) 99–127.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] P. Hernandez-Leal, Y. Zhan, M. E. Taylor, L. E. Sucar, E. Munoz de Cote,
    Efficiently detecting switches against non-stationary opponents, Autonomous Agents
    and Multi-Agent Systems 31 (4) (2017) 767–789.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] P. Hernandez-Leal, M. Kaisers, Learning against sequential opponents
    in repeated stochastic games, in: The 3rd Multi-disciplinary Conference on Reinforcement
    Learning and Decision Making, Ann Arbor, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] J. Schmidhuber, Critique of Paper by “Deep Learning Conspiracy” (Nature
    521 p 436), [http://people.idsia.ch/~juergen/deep-learning-conspiracy.html](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html)
    (2015).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Do I really have to cite an arXiv paper?, [http://approximatelycorrect.com/2017/08/01/do-i-have-to-cite-arxiv-paper/](http://approximatelycorrect.com/2017/08/01/do-i-have-to-cite-arxiv-paper/),
    [Online; accessed 21-May-2019] (2017).'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Collaboration & Credit Principles, How can we be good stewards of collaborative
    trust?, [http://colah.github.io/posts/2019-05-Collaboration/index.html](http://colah.github.io/posts/2019-05-Collaboration/index.html),
    [Online; accessed 31-May-2019] (2019).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] H. Wang, B. Raj, E. P. Xing, [On the origin of deep learning](http://arxiv.org/abs/1702.07800),
    CoRR abs/1702.07800. [arXiv:1702.07800](http://arxiv.org/abs/1702.07800).'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1702.07800](http://arxiv.org/abs/1702.07800)
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[278] A. K. Agogino, K. Tumer, Analyzing and visualizing multiagent rewards
    in dynamic and stochastic domains, Autonomous Agents and Multi-Agent Systems 17 (2)
    (2008) 320–338.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] S. Devlin, L. M. Yliniemi, D. Kudenko, K. Tumer, Potential-based difference
    rewards for multiagent reinforcement learning., in: 13th International Conference
    on Autonomous Agents and Multiagent Systems, AAMAS 2014, Paris, France, 2014.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] D. H. Wolpert, K. R. Wheeler, K. Tumer, General principles of learning-based
    multi-agent systems, in: Proceedings of the Third International Conference on
    Autonomous Agents, 1999.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] M. E. Taylor, P. Stone, Transfer learning for reinforcement learning
    domains: A survey, The Journal of Machine Learning Research 10 (2009) 1633–1685.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] C. Bucilua, R. Caruana, A. Niculescu-Mizil, Model compression, in: Proceedings
    of the 12th ACM SIGKDD international conference on Knowledge discovery and data
    mining, ACM, 2006, pp. 535–541.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Y. Du, W. M. Czarnecki, S. M. Jayakumar, R. Pascanu, B. Lakshminarayanan,
    Adapting auxiliary losses using gradient similarity, arXiv preprint arXiv:1812.02224.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] S. C. Suddarth, Y. Kergosien, Rule-injection hints as a means of improving
    network performance and learning time, in: Neural Networks, Springer, 1990, pp.
    120–129.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] P. Hernandez-Leal, B. Kartal, M. E. Taylor, Agent Modeling as Auxiliary
    Task for Deep Reinforcement Learning, in: AAAI Conference on Artificial Intelligence
    and Interactive Digital Entertainment, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,
    B. McGrew, J. Tobin, P. Abbeel, W. Zaremba, Hindsight experience replay, in: Advances
    in Neural Information Processing Systems, 2017.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Z. C. Lipton, K. Azizzadenesheli, A. Kumar, L. Li, J. Gao, L. Deng, [Combating
    Reinforcement Learning’s Sisyphean Curse with Intrinsic Fear](http://arxiv.org/abs/1611.01211v8).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1611.01211v8](http://arxiv.org/abs/1611.01211v8)
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[288] T. De Bruin, J. Kober, K. Tuyls, R. Babuška, Experience selection in
    deep reinforcement learning for control, The Journal of Machine Learning Research
    19 (1) (2018) 347–402.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] D. S. Bernstein, R. Givan, N. Immerman, S. Zilberstein, The complexity
    of decentralized control of Markov decision processes, Mathematics of operations
    research 27 (4) (2002) 819–840.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized
    POMDPs, Springer, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] F. A. Oliehoek, M. T. Spaan, N. Vlassis, Optimal and approximate Q-value
    functions for decentralized POMDPs, Journal of Artificial Intelligence Research
    32 (2008) 289–353.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] J. K. Gupta, M. Egorov, M. J. Kochenderfer, Cooperative Multi-agent Control
    using deep reinforcement learning, in: Adaptive Learning Agents at AAMAS, Sao
    Paulo, 2017.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] R. Pascanu, T. Mikolov, Y. Bengio, On the difficulty of training recurrent
    neural networks, in: International conference on machine learning, 2013, pp. 1310–1318.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] K. Greff, R. K. Srivastava, J. Koutnik, B. R. Steunebrink, J. Schmidhuber,
    LSTM: A Search Space Odyssey, IEEE Transactions on Neural Networks and Learning
    Systems 28 (10) (2017) 2222–2232.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of gated
    recurrent neural networks on sequence modeling, in: Deep Learning and Representation
    Learning Workshop, 2014.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] S. Whiteson, B. Tanner, M. E. Taylor, P. Stone, Protecting against evaluation
    overfitting in empirical reinforcement learning, in: 2011 IEEE Symposium on Adaptive
    Dynamic Programming and Reinforcement Learning (ADPRL), IEEE, 2011, pp. 120–127.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] M. G. Bellemare, Y. Naddaf, J. Veness, M. Bowling, The arcade learning
    environment: An evaluation platform for general agents, Journal of Artificial
    Intelligence Research 47 (2013) 253–279.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht,
    M. Bowling, Revisiting the arcade learning environment: Evaluation protocols and
    open problems for general agents, Journal of Artificial Intelligence Research
    61 (2018) 523–562.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang,
    W. Zaremba, OpenAI Gym, arXiv preprint arXiv:1606.01540.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] P. S. Castro, S. Moitra, C. Gelada, S. Kumar, M. G. Bellemare, [Dopamine:
    A Research Framework for Deep Reinforcement Learning](http://arxiv.org/abs/1812.06110).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1812.06110](http://arxiv.org/abs/1812.06110)
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[301] C. Resnick, W. Eldridge, D. Ha, D. Britz, J. Foerster, J. Togelius, K. Cho,
    J. Bruna, [Pommerman: A Multi-Agent Playground](http://arxiv.org/abs/1809.07124).'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.07124](http://arxiv.org/abs/1809.07124)
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[302] C. Gao, B. Kartal, P. Hernandez-Leal, M. E. Taylor, On Hard Exploration
    for Reinforcement Learning: a Case Study in Pommerman, in: AAAI Conference on
    Artificial Intelligence and Interactive Digital Entertainment, 2019.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. J.
    Rudner, C. Hung, P. H. S. Torr, J. N. Foerster, S. Whiteson, [The StarCraft Multi-Agent
    Challenge](http://arxiv.org/abs/1902.04043), CoRR abs/1902.04043. [arXiv:1902.04043](http://arxiv.org/abs/1902.04043).'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1902.04043](http://arxiv.org/abs/1902.04043)
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[304] D. Pérez-Liébana, K. Hofmann, S. P. Mohanty, N. Kuno, A. Kramer, S. Devlin,
    R. D. Gaina, D. Ionita, [The multi-agent reinforcement learning in Malmö (MARLÖ)
    competition](http://arxiv.org/abs/1901.08129), CoRR abs/1901.08129. [arXiv:1901.08129](http://arxiv.org/abs/1901.08129).'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.08129](http://arxiv.org/abs/1901.08129)
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[305] M. Johnson, K. Hofmann, T. Hutton, D. Bignell, The Malmo platform for
    artificial intelligence experimentation., in: IJCAI, 2016, pp. 4246–4247.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] P. Stone, G. Kaminka, S. Kraus, J. S. Rosenschein, Ad Hoc Autonomous
    Agent Teams: Collaboration without Pre-Coordination., in: 32nd AAAI Conference
    on Artificial Intelligence, Atlanta, Georgia, USA, 2010, pp. 1504–1509.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] M. Bowling, P. McCracken, Coordination and adaptation in impromptu teams,
    in: Proceedings of the Nineteenth Conference on Artificial Intelligence, Vol. 5,
    2005, pp. 53–58.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] S. V. Albrecht, S. Ramamoorthy, A game-theoretic model and best-response
    learning method for ad hoc coordination in multiagent systems, in: Proceedings
    of the 12th International Conference on Autonomous Agents and Multi-agent Systems,
    Saint Paul, MN, USA, 2013.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] N. Bard, J. N. Foerster, S. Chandar, N. Burch, M. Lanctot, H. F. Song,
    E. Parisotto, V. Dumoulin, S. Moitra, E. Hughes, I. Dunning, S. Mourad, H. Larochelle,
    M. G. Bellemare, M. Bowling, [The Hanabi Challenge: A New Frontier for AI Research](https://arxiv.org/abs/1902.00506).'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://arxiv.org/abs/1902.00506](https://arxiv.org/abs/1902.00506)
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[310] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, D. Silver, Rainbow: Combining improvements in deep
    reinforcement learning, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Y. Song, J. Wang, T. Lukasiewicz, Z. Xu, M. Xu, Z. Ding, L. Wu, [Arena:
    A general evaluation platform and building toolkit for multi-agent intelligence](http://arxiv.org/abs/1905.08085),
    CoRR abs/1905.08085. [arXiv:1905.08085](http://arxiv.org/abs/1905.08085).'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] Y. Song, J. Wang, T. Lukasiewicz, Z. Xu, M. Xu, Z. Ding, L. Wu, [**Arena：一个通用的多智能体智能评估平台和构建工具包**](http://arxiv.org/abs/1905.08085),
    CoRR abs/1905.08085. [arXiv:1905.08085](http://arxiv.org/abs/1905.08085).'
- en: URL [http://arxiv.org/abs/1905.08085](http://arxiv.org/abs/1905.08085)
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1905.08085](http://arxiv.org/abs/1905.08085)
- en: '[312] A. Juliani, V. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, D. Lange,
    [Unity: A general platform for intelligent agents](http://arxiv.org/abs/1809.02627),
    CoRR abs/1809.02627. [arXiv:1809.02627](http://arxiv.org/abs/1809.02627).'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] A. Juliani, V. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, D. Lange,
    [**Unity：一个通用的智能体平台**](http://arxiv.org/abs/1809.02627), CoRR abs/1809.02627.
    [arXiv:1809.02627](http://arxiv.org/abs/1809.02627).'
- en: URL [http://arxiv.org/abs/1809.02627](http://arxiv.org/abs/1809.02627)
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1809.02627](http://arxiv.org/abs/1809.02627)
- en: '[313] S. Liu, G. Lever, J. Merel, S. Tunyasuvunakool, N. Heess, T. Graepel,
    Emergent coordination through competition, in: International Conference on Learning
    Representations, 2019.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] S. Liu, G. Lever, J. Merel, S. Tunyasuvunakool, N. Heess, T. Graepel,
    **通过竞争的自发协调**，收录于：国际学习表示大会, 2019.'
- en: '[314] J. Suarez, Y. Du, P. Isola, I. Mordatch, [Neural MMO: A massively multiagent
    game environment for training and evaluating intelligent agents](http://arxiv.org/abs/1903.00784),
    CoRR abs/1903.00784. [arXiv:1903.00784](http://arxiv.org/abs/1903.00784).'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] J. Suarez, Y. Du, P. Isola, I. Mordatch, [**Neural MMO：一个用于训练和评估智能体的大规模多智能体游戏环境**](http://arxiv.org/abs/1903.00784),
    CoRR abs/1903.00784. [arXiv:1903.00784](http://arxiv.org/abs/1903.00784).'
- en: URL [http://arxiv.org/abs/1903.00784](http://arxiv.org/abs/1903.00784)
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1903.00784](http://arxiv.org/abs/1903.00784)
- en: '[315] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, D. Meger, Deep
    Reinforcement Learning That Matters., in: 32nd AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, D. Meger, **深度强化学习的重要性**，收录于：第32届AAAI人工智能会议,
    2018.'
- en: '[316] P. Nagarajan, G. Warnell, P. Stone, [Deterministic implementations for
    reproducibility in deep reinforcement learning](http://arxiv.org/abs/1809.05676).'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] P. Nagarajan, G. Warnell, P. Stone, [**深度强化学习中的确定性实现以保证可重复性**](http://arxiv.org/abs/1809.05676).'
- en: URL [http://arxiv.org/abs/1809.05676](http://arxiv.org/abs/1809.05676)
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://arxiv.org/abs/1809.05676](http://arxiv.org/abs/1809.05676)
- en: '[317] K. Clary, E. Tosch, J. Foley, D. Jensen, Let’s play again: Variability
    of deep reinforcement learning agents in Atari environments, in: NeurIPS Critiquing
    and Correcting Trends Workshop, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] K. Clary, E. Tosch, J. Foley, D. Jensen, **再来一局：Atari环境中深度强化学习智能体的变异性**，收录于：NeurIPS
    Critiquing and Correcting Trends Workshop, 2018.'
- en: '[318] J. Z. Forde, M. Paganini, The scientific method in the science of machine
    learning, in: ICLR Debugging Machine Learning Models workshop, 2019.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] J. Z. Forde, M. Paganini, **科学方法在机器学习科学中的应用**，收录于：ICLR调试机器学习模型研讨会, 2019.'
- en: '[319] K. Azizzadenesheli, Maybe a few considerations in reinforcement learning
    research?, in: Reinforcement Learning for Real Life Workshop, 2019.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] K. Azizzadenesheli, **也许在强化学习研究中需要考虑几个方面**，收录于：Reinforcement Learning
    for Real Life Workshop, 2019.'
- en: '[320] Z. C. Lipton, J. Steinhardt, Troubling trends in machine learning scholarship,
    in: ICML Machine Learning Debates workshop, 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] Z. C. Lipton, J. Steinhardt, **机器学习学术中的令人担忧的趋势**，收录于：ICML机器学习辩论研讨会, 2018.'
- en: '[321] R. Rosenthal, The file drawer problem and tolerance for null results.,
    Psychological bulletin 86 (3) (1979) 638.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] R. Rosenthal, **档案抽屉问题与对无效结果的容忍度**，心理学公告 86 (3) (1979) 638.'
- en: '[322] D. Sculley, J. Snoek, A. Wiltschko, A. Rahimi, Winner’s curse? on pace,
    progress, and empirical rigor, in: ICLR Workshop, 2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] D. Sculley, J. Snoek, A. Wiltschko, A. Rahimi, **赢家的诅咒？** 论进度、发展和实证严谨性，收录于：ICLR
    Workshop, 2018.'
- en: '[323] O. Gencoglu, M. van Gils, E. Guldogan, C. Morikawa, M. Süzen, M. Gruber,
    J. Leinonen, H. Huttunen, Hark side of deep learning–from grad student descent
    to automated machine learning, arXiv preprint arXiv:1904.07633.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] O. Gencoglu, M. van Gils, E. Guldogan, C. Morikawa, M. Süzen, M. Gruber,
    J. Leinonen, H. Huttunen, **深度学习的暗面——从研究生的下降到自动化机器学习**，arXiv预印本 arXiv:1904.07633.'
- en: '[324] K. Azizzadenesheli, B. Yang, W. Liu, E. Brunskill, Z. Lipton, A. Anandkumar,
    Surprising negative results for generative adversarial tree search, in: Critiquing
    and Correcting Trends in Machine Learning Workshop, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] K. Azizzadenesheli, B. Yang, W. Liu, E. Brunskill, Z. Lipton, A. Anandkumar,
    **生成对抗树搜索的惊人负面结果**，收录于：Critiquing and Correcting Trends in Machine Learning Workshop,
    2018.'
- en: '[325] C. Lyle, P. S. Castro, M. G. Bellemare, A comparative analysis of expected
    and distributional reinforcement learning, in: Thirty-Third AAAI Conference on
    Artificial Intelligence, 2019.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] B. Kartal, P. Hernandez-Leal, M. E. Taylor, Using Monte Carlo tree search
    as a demonstrator within asynchronous deep RL, in: AAAI Workshop on Reinforcement
    Learning in Games, 2019.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] G. Melis, C. Dyer, P. Blunsom, On the state of the art of evaluation
    in neural language models, in: International Conference on Learning Representations,
    2018.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Deep Reinforcement Learning: Pong from Pixels, [https://karpathy.github.io/2016/05/31/rl/](https://karpathy.github.io/2016/05/31/rl/),
    [Online; accessed 7-May-2019] (2016).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] V. Firoiu, W. F. Whitney, J. B. Tenenbaum, [Beating the World’s Best
    at Super Smash Bros. with Deep Reinforcement Learning](http://arxiv.org/abs/1702.06230),
    CoRR abs/1702.06230. [arXiv:1702.06230](http://arxiv.org/abs/1702.06230).'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1702.06230](http://arxiv.org/abs/1702.06230)
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[330] C. Gao, P. Hernandez-Leal, B. Kartal, M. E. Taylor, Skynet: A Top Deep
    RL Agent in the Inaugural Pommerman Team Competition, in: 4th Multidisciplinary
    Conference on Reinforcement Learning and Decision Making, 2019.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] D. Amodei, D. Hernandez, [AI and Compute](https://blog.%20openai.%20com/ai-and-compute)
    (2018).'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://blog.openai.com/ai-and-compute](https://blog.openai.com/ai-and-compute)
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[332] Y. Yu, Towards sample efficient reinforcement learning., in: IJCAI, 2018,
    pp. 5739–5743.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, J. Clune,
    [Deep neuroevolution: Genetic algorithms are a competitive alternative for training
    deep neural networks for reinforcement learning](http://arxiv.org/abs/1712.06567),
    CoRR abs/1712.06567.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1712.06567](http://arxiv.org/abs/1712.06567)
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[334] A. Stooke, P. Abbeel, [Accelerated methods for deep reinforcement learning](http://arxiv.org/abs/1803.02811),
    CoRR abs/1803.02811. [arXiv:1803.02811](http://arxiv.org/abs/1803.02811).'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1803.02811](http://arxiv.org/abs/1803.02811)
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[335] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, J. Kautz, Reinforcement
    learning through asynchronous advantage actor-critic on a GPU, in: International
    Conference on Learning Representations, 2017.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] W. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. P.
    Mohanty, D. P. Liebana, R. Salakhutdinov, N. Topin, M. Veloso, P. Wang, [The MineRL
    Competition on Sample Efficient Reinforcement Learning using Human Priors](http://arxiv.org/abs/1904.10079),
    CoRR abs/1904.10079. [arXiv:1904.10079](http://arxiv.org/abs/1904.10079).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1904.10079](http://arxiv.org/abs/1904.10079)
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[337] G. Cuccu, J. Togelius, P. Cudré-Mauroux, Playing Atari with six neurons,
    in: Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems, International Foundation for Autonomous Agents and Multiagent
    Systems, 2019, pp. 998–1006.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, J. Clune, Go-explore:
    a new approach for hard-exploration problems, arXiv preprint arXiv:1901.10995.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] R. I. Brafman, M. Tennenholtz, R-max-a general polynomial time algorithm
    for near-optimal reinforcement learning, Journal of Machine Learning Research
    3 (Oct) (2002) 213–231.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] A. L. Strehl, M. L. Littman, An analysis of model-based interval estimation
    for Markov decision processes, Journal of Computer and System Sciences 74 (8)
    (2008) 1309–1331.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] J. Schmidhuber, A possibility for implementing curiosity and boredom
    in model-building neural controllers, in: Proc. of the international conference
    on simulation of adaptive behavior: From animals to animats, 1991, pp. 222–227.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] A. G. Barto, Intrinsic motivation and reinforcement learning, in: Intrinsically
    motivated learning in natural and artificial systems, Springer, 2013, pp. 17–47.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] T. D. Kulkarni, K. Narasimhan, A. Saeedi, J. Tenenbaum, Hierarchical
    deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,
    in: Advances in neural information processing systems, 2016, pp. 3675–3683.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] T. G. Dietterich, Ensemble Methods in Machine Learning, in: MCS Proceedings
    of the First International Workshop on Multiple Classifier Systems, Springer Berlin
    Heidelberg, Cagliari, Italy, 2000, pp. 1–15.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, S. Hochreiter,
    [RUDDER: Return Decomposition for Delayed Rewards](http://arxiv.org/abs/1806.07857).'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1806.07857](http://arxiv.org/abs/1806.07857)
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[346] V. Conitzer, T. Sandholm, AWESOME: A general multiagent learning algorithm
    that converges in self-play and learns a best response against stationary opponents,
    Machine Learning 67 (1-2) (2006) 23–43.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen,
    S. Tavener, D. Perez, S. Samothrakis, S. Colton, A survey of Monte Carlo tree
    search methods, IEEE Transactions on Computational Intelligence and AI in games
    4 (1) (2012) 1–43.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] T. Vodopivec, S. Samothrakis, B. Ster, On Monte Carlo tree search and
    reinforcement learning, Journal of Artificial Intelligence Research 60 (2017)
    881–936.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] B. Kartal, J. Godoy, I. Karamouzas, S. J. Guy, Stochastic tree search
    with useful cycles for patrolling problems, in: Robotics and Automation (ICRA),
    2015 IEEE International Conference on, IEEE, 2015, pp. 1289–1294.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] B. Kartal, E. Nunes, J. Godoy, M. Gini, Monte Carlo tree search with
    branch and bound for multi-robot task allocation, in: The IJCAI-16 Workshop on
    Autonomous Mobile Service Robots, 2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] G. Best, O. M. Cliff, T. Patten, R. R. Mettu, R. Fitch, Dec-MCTS: Decentralized
    planning for multi-robot active perception, The International Journal of Robotics
    Research 38 (2-3) (2019) 316–337.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Y.-M. De Hauwere, P. Vrancx, A. Nowe, Learning multi-agent state space
    representations, in: Proceedings of the 9th International Conference on Autonomous
    Agents and Multiagent Systems, Toronto, Canada, 2010, pp. 715–722.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] C. Guestrin, M. Lagoudakis, R. Parr, Coordinated reinforcement learning,
    in: ICML, Vol. 2, 2002, pp. 227–234.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] C. Guestrin, D. Koller, R. Parr, S. Venkataraman, Efficient solution
    algorithms for factored MDPs, Journal of Artificial Intelligence Research 19 (2003)
    399–468.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] C. Amato, F. A. Oliehoek, Scalable Planning and Learning for Multiagent
    POMDPs, in: AAAI, 2015, pp. 1995–2002.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] F. A. Oliehoek, Interactive Learning and Decision Making - Foundations,
    Insights & Challenges., International Joint Conference on Artificial Intelligence.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] F. A. Oliehoek, S. Whiteson, M. T. Spaan, Approximate solutions for factored
    Dec-POMDPs with many agents, in: Proceedings of the 2013 international conference
    on Autonomous agents and multi-agent systems, International Foundation for Autonomous
    Agents and Multiagent Systems, 2013, pp. 563–570.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] R. Becker, S. Zilberstein, V. Lesser, C. V. Goldman, Solving transition
    independent decentralized Markov decision processes, Journal of Artificial Intelligence
    Research 22 (2004) 423–455.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] F. A. Oliehoek, S. J. Witwicki, L. P. Kaelbling, Influence-based abstraction
    for multiagent systems, in: Twenty-Sixth AAAI Conference on Artificial Intelligence,
    2012.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] M. Suau de Castro, E. Congeduti, R. A. Starre, A. Czechowski, F. A. Oliehoek,
    Influence-based abstraction in deep reinforcement learning, in: Adaptive, Learning
    Agents workshop, 2019.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] E. Wei, D. Wicke, D. Freelan, S. Luke, [Multiagent Soft Q-Learning](http://arXiv.org/abs/1804.09817).'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1804.09817](http://arXiv.org/abs/1804.09817)
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[362] R. R. Torrado, P. Bontrager, J. Togelius, J. Liu, D. Perez-Liebana, [Deep
    Reinforcement Learning for General Video Game AI](http://arxiv.org/abs/1806.02448).'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1806.02448](http://arxiv.org/abs/1806.02448)
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[363] P. A. Ortega, S. Legg, [Modeling friends and foes](http://arxiv.org/abs/1807.00196).'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1807.00196](http://arxiv.org/abs/1807.00196)
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[364] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, J. Wang, Mean field multi-agent
    reinforcement learning, in: Proceedings of the 35th International Conference on
    Machine Learning, Stockholm Sweden, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] A. Grover, M. Al-Shedivat, J. K. Gupta, Y. Burda, H. Edwards, Learning
    Policy Representations in Multiagent Systems., in: International Conference on
    Machine Learning, 2018.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] C. K. Ling, F. Fang, J. Z. Kolter, What game are we playing? end-to-end
    learning in normal and extensive form games, in: Twenty-Seventh International
    Joint Conference on Artificial Intelligence, 2018.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] S. Omidshafiei, D. Hennes, D. Morrill, R. Munos, J. Perolat, M. Lanctot,
    A. Gruslys, J.-B. Lespiau, K. Tuyls, Neural Replicator Dynamics, arXiv e-prints
    (2019) arXiv:1906.00190[arXiv:1906.00190](http://arxiv.org/abs/1906.00190).'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] S. Khadka, S. Majumdar, K. Tumer, Evolutionary Reinforcement Learning
    for Sample-Efficient Multiagent Coordination, arXiv e-prints (2019) arXiv:1906.07315[arXiv:1906.07315](http://arxiv.org/abs/1906.07315).'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[◄](/html/1810.05586) [![ar5iv homepage](img/ed0f3cf5a019c4f8e48e41de62929bb0.png)](/)
    [Feeling'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: lucky?](/feeling_lucky) [Conversion
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: report](/log/1810.05587) [Report
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: an issue](https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1810.05587)
    [View original
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: on arXiv](https://arxiv.org/abs/1810.05587)[►](/html/1810.05588)[](javascript:toggleColorScheme()
    "Toggle ar5iv color scheme")[Copyright](https://arxiv.org/help/license) [Privacy
    Policy](https://arxiv.org/help/policies/privacy_policy)Generated on Tue Mar 19
    07:19:24 2024 by [LaTeXML![Mascot Sammy](img/70e087b9e50c3aa663763c3075b0d6c5.png)](http://dlmf.nist.gov/LaTeXML/)
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
