- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:38:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:38:33'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2306.16021] Structure in Reinforcement Learning: A Survey and Open Problems'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2306.16021] 强化学习中的结构：调查与开放问题'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.16021](https://ar5iv.labs.arxiv.org/html/2306.16021)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.16021](https://ar5iv.labs.arxiv.org/html/2306.16021)
- en: \pdfcolInitStack
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfcolInitStack
- en: tcb@breakable
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: tcb@breakable
- en: 'Structure in Reinforcement Learning:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的结构：
- en: A Survey and Open Problems
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 调查与开放问题
- en: \nameAditya Mohan \emaila.mohan@ai.uni-hannover.de
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \name阿迪亚·莫汉 \emaila.mohan@ai.uni-hannover.de
- en: \addrInstitute of Artificial Intelligence
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addr人工智能研究所
- en: Leibniz University Hannover
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 莱布尼茨大学汉诺威
- en: \nameAmy Zhang \emailamy.zhang@austin.utexas.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \name艾米·张 \emailamy.zhang@austin.utexas.edu
- en: \addrUniversity of Texas at Austin, Meta AI
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \addr德克萨斯大学奥斯汀分校，Meta AI
- en: \nameMarius Lindauer \emailm.lindauer@ai.uni-hannover.de
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \name马里乌斯·林道尔 \emailm.lindauer@ai.uni-hannover.de
- en: \addrInstitute of Artificial Intelligence
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \addr人工智能研究所
- en: Leibniz University Hannover
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 莱布尼茨大学汉诺威
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep
    Neural Networks (DNNs) for function approximation, has demonstrated considerable
    success in numerous applications. However, its practicality in addressing various
    real-world scenarios, characterized by diverse and unpredictable dynamics, noisy
    signals, and large state and action spaces, remains limited. This limitation stems
    from issues such as poor data efficiency, limited generalization capabilities,
    a lack of safety guarantees, and the absence of interpretability, among other
    factors. To overcome these challenges and improve performance across these crucial
    metrics, one promising avenue is to incorporate additional structural information
    about the problem into the RL learning process. Various sub-fields of RL have
    proposed methods for incorporating such inductive biases. We amalgamate these
    diverse methodologies under a unified framework, shedding light on the role of
    structure in the learning problem, and classify these methods into distinct patterns
    of incorporating structure. By leveraging this comprehensive framework, we provide
    valuable insights into the challenges of structured RL and lay the groundwork
    for a design pattern perspective on RL research. This novel perspective paves
    the way for future advancements and aids in developing more effective and efficient
    RL algorithms that can potentially handle real-world scenarios better.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）借助深度神经网络（DNNs）在函数逼近中的表现能力，在许多应用中取得了显著成功。然而，它在处理各种现实世界场景时的实用性仍然有限，这些场景具有多样和不可预测的动态、噪声信号以及大型状态和动作空间。这些限制源于数据效率差、泛化能力有限、安全保障不足以及缺乏可解释性等问题。为克服这些挑战并在这些关键指标上提高性能，一个有前途的方向是将问题的额外结构信息纳入RL学习过程。RL的各种子领域已经提出了将这种归纳偏置纳入的方法。我们将这些不同的方法整合到一个统一的框架下，阐明结构在学习问题中的作用，并将这些方法分类为不同的结构纳入模式。通过利用这个全面的框架，我们提供了有关结构化RL挑战的宝贵见解，并为RL研究的设计模式视角奠定了基础。这一新颖的视角为未来的进展铺平了道路，并有助于开发能够更好应对现实世界场景的更有效和高效的RL算法。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Reinforcement Learning (RL) has contributed to a range of sequential decision-making
    and control problems like games (?), robotic manipulation (?), optimizing chemical
    reactions (?), and RNA folding (?). Most of the traditional research in RL focuses
    on designing agents that learn to solve a sequential decision problem induced
    by the inherent dynamics of a task, e.g., the differential equations governing
    the cart pole task in the classic control suite (?, ?). However, their performance
    significantly degrades when even small aspects of the environment change (?, ?).
    Moreover, deploying RL agents for real-world learning-based optimization has additional
    challenges, such as complicated dynamics, intractable and computationally expensive
    state and action spaces, and noisy reward signals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）在一系列顺序决策和控制问题中做出了贡献，如游戏（？）、机器人操作（？）、化学反应优化（？）和RNA折叠（？）。大多数传统的RL研究集中于设计能够学习解决由任务固有动态引发的顺序决策问题的智能体，例如，经典控制套件中的推车摆杆任务的微分方程（？，？）。然而，当环境的即使是微小方面发生变化时，它们的性能会显著下降（？，？）。此外，将RL智能体用于现实世界学习优化还面临额外挑战，如复杂的动态、难以处理和计算成本高昂的状态和动作空间以及噪声奖励信号。
- en: 'Thus, research in RL has started to address these issues through methods that
    can generally be categorized into two dogmas (?): (i) Generalization:Methods developed
    to solve a broader class of problems where the agent is trained on various tasks
    and environments (?, ?). (ii) Deployability:Methods that are specifically engineered
    towards concrete real-world problems by incorporating additional aspects such
    as feature engineering, computational budget optimization, safety, etc. The intersection
    of generalization and deployability is particularly interesting since it covers
    a class of problems where we require methods to handle sufficient diversity in
    the task while being deployable for specific applications. To foster research
    in this area, ? (?) argue for a design-pattern oriented approach, where methods
    can be abstracted into patterns that are specialized to specific kinds of problems.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RL 研究已经开始通过可以大致归类为两种学说的方法来解决这些问题：(i) 泛化：开发用于解决更广泛问题的方法，其中代理在各种任务和环境中进行训练。
    (ii) 可部署性：专门针对具体实际问题进行工程化的方法，通过加入额外方面如特征工程、计算预算优化、安全性等。泛化与可部署性的交集特别有趣，因为它涵盖了一类问题，我们需要方法处理任务中的足够多样性，同时又能针对具体应用进行部署。为了促进这一领域的研究，?
    认为采用设计模式导向的方法，其中方法可以抽象成专门针对特定类型问题的模式。
- en: However, the path to RL design patterns is hindered by gaps in our understanding
    of the relationship between the design decisions for RL methods and the properties
    of the problems they might be suited for. While decisions like using state abstractions
    for high-dimensional spaces seem obvious, decisions like using relational neural
    architectures for problems are not so obvious to a designer. One way to add principle
    to this process is to understand how to incorporate additional domain knowledge
    into the learning pipeline. A strong source of such domain knowledge is the structure
    present in the learning problem itself, including priors about the state space,
    the action space, the reward function, or the dynamics of the environment. While
    such methods have been research subjects throughout the history of RL (?), approaches
    for doing so in Deep RL are scattered across the various sub-fields in the vast
    and disparate landscape of modern RL research.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RL 设计模式的道路受到我们对 RL 方法设计决策与这些方法可能适用问题属性之间关系理解的限制。虽然使用状态抽象来处理高维空间的决策似乎很明显，但像使用关系神经网络架构解决问题的决策对设计师来说并不那么明显。将额外的领域知识融入学习流程是一种为这一过程增添原则的方法。这样领域知识的一个强大来源是学习问题本身的结构，包括有关状态空间、动作空间、奖励函数或环境动态的先验信息。虽然这些方法一直是
    RL 研究的主题，但在深度 RL 中处理这些问题的方法散布在现代 RL 研究广阔而不同的子领域中。
- en: '![Refer to caption](img/cc5551af6f8b92e2a5caefd645883dba.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/cc5551af6f8b92e2a5caefd645883dba.png)'
- en: 'Figure 1: Overview of our framework. Domain knowledge can generally be incorporated
    into an RL pipeline as side information and can be used to achieve improved performance
    across metrics such as *Sample Efficiency*, *Generalization*, *Interpretability*,
    and *Safety*. We discuss this process in [Section 3](#S3 "3 Side Information and
    its Usage ‣ Structure in Reinforcement Learning: A Survey and Open Problems").
    A particular source of side information is decomposability in a learning problem,
    which can be categorized into four archetypes along a spectrum - *Latent*, *Factored*,
    *Relational*, and *Modular* - explained further in [Section 4.1](#S4.SS1 "4.1
    Decomposability and Structural Archetypes ‣ 4 Structure as Side Information ‣
    Structure in Reinforcement Learning: A Survey and Open Problems"). Incorporating
    side information about decomposability amounts to adding structure to a learning
    pipeline, and this process can be categorized into seven different patterns -
    *Abstraction*, *Augmentation*, *Auxiliary Optimization*, *Auxiliary Model*, *Warehouse*,
    *Environment Generation*, and *Explicitly Designed* - discussed further in [Section 5](#S5
    "5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: 我们框架的概述。领域知识通常可以作为附加信息并用于在诸如*样本效率*、*泛化*、*可解释性*和*安全性*等指标上实现改进的性能。我们在[第3节](#S3
    "3 附加信息及其使用 ‣ 强化学习中的结构：调查与开放问题")中讨论了这个过程。附加信息的一个特定来源是学习问题中的可分解性，它可以被分类为沿着光谱的四个典型
    - *潜在*、*因子*、*关联*和*模块化* - 在[第4.1节](#S4.SS1 "4.1 可分解性与结构原型 ‣ 强化学习中的结构：调查与开放问题")中有更详细的解释。将可分解性的附加信息并入到学习流程中相当于为学习流程添加结构，这个过程可以被分类为七种不同的模式
    - *抽象*、*增强*、*辅助优化*、*辅助模型*、*仓库*、*环境生成*和*明确设计* - 在[第5节](#S5 "5 结构并入模式 ‣ 强化学习中的结构：调查与开放问题")中有进一步的讨论。'
- en: 'Contributions and Structure of the Paper. In this work, we take the first steps
    in amalgamating these approaches under our pattern-centric framework for incorporating
    structure in RL. [Figure 1](#S1.F1 "In 1 Introduction ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems") shows a general overview of three elements
    of understanding the role of incorporating structure into a learning problem that
    we cover in this work. In [Section 2](#S2 "2 Preliminaries ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems"), we describe the background and notation
    needed to formalize the relevant aspects of the RL problems. In [Section 3](#S3
    "3 Side Information and its Usage ‣ Structure in Reinforcement Learning: A Survey
    and Open Problems"), we introduce the notion of side information and define different
    additional metrics that can be addressed by incorporating side information into
    an RL pipeline. We then formulate structure as a particular kind of side information
    about decomposability in a problem in [Section 4](#S4 "4 Structure as Side Information
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems"), and categorize
    decompositions in the literature into four major archetypes. In [Section 5](#S5
    "5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems"), we formulate seven patterns of incorporating structure
    into the RL learning process and provide an overview of each pattern by connecting
    it to the relevant surveyed literature. The framework developed in this work opens
    new avenues for research while providing a common reference point for understanding
    what kind of design decisions work under which situations. We discuss these aspects
    further in [Section 6](#S6 "6 Open Problems in Structured Reinforcement Learning
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems") for more concrete
    takeaways for researchers and practitioners.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的贡献与结构。在这项工作中，我们在我们的模式中心框架下，将这些方法进行整合，首次迈出了将结构融入RL的第一步。[图1](#S1.F1 "在 1 引言
    ‣ 强化学习中的结构：综述与开放问题")展示了我们在这项工作中涵盖的将结构融入学习问题的三个要素的总体概览。在[第2节](#S2 "2 前言 ‣ 强化学习中的结构：综述与开放问题")中，我们描述了形式化RL问题相关方面所需的背景和符号。在[第3节](#S3
    "3 侧信息及其应用 ‣ 强化学习中的结构：综述与开放问题")中，我们引入了侧信息的概念，并定义了通过将侧信息纳入RL流程可以解决的不同附加度量。然后，在[第4节](#S4
    "4 结构作为侧信息 ‣ 强化学习中的结构：综述与开放问题")中，我们将结构形式化为关于问题可分解性的特定类型的侧信息，并将文献中的分解分类为四种主要类型。在[第5节](#S5
    "5 融入结构的模式 ‣ 强化学习中的结构：综述与开放问题")中，我们提出了将结构融入RL学习过程的七种模式，并通过将其与相关文献联系起来，概述了每种模式。本研究中开发的框架为研究开辟了新的途径，同时提供了一个共同的参考点，以了解在不同情况下哪些设计决策有效。我们在[第6节](#S6
    "6 结构化强化学习中的开放问题 ‣ 强化学习中的结构：综述与开放问题")中进一步讨论这些方面，以便为研究人员和从业者提供更具体的收获。
- en: 2 Preliminaries
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: 'The following sections summarize the main background necessary for our approach
    to studying structural decompositions and related patterns. In [Section 2.1](#S2.SS1
    "2.1 Markov Decision Processes ‣ 2 Preliminaries ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems"), we formalize the sequential decision-making
    problem as an MDP. [Section 2.2](#S2.SS2 "2.2 Reinforcement Learning ‣ 2 Preliminaries
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems") then presents
    the RL framework for solving MDPs and introduces the RL pipeline.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分总结了我们研究结构分解及相关模式所需的主要背景。在[第2.1节](#S2.SS1 "2.1 马尔可夫决策过程 ‣ 2 前言 ‣ 强化学习中的结构：综述与开放问题")中，我们将序贯决策问题形式化为MDP。[第2.2节](#S2.SS2
    "2.2 强化学习 ‣ 2 前言 ‣ 强化学习中的结构：综述与开放问题")随后介绍了解决MDP的RL框架，并引入了RL流程。
- en: 2.1 Markov Decision Processes
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 马尔可夫决策过程
- en: Sequential decision-making problems are usually formalized using the notion
    of a Markov Decision Process (MDP) (?, ?), which can be written down as a 5-tuple
    $\mathcal{M}=\langle\mathcal{S},\mathcal{A},R,P,\rho\rangle$. At any timestep,
    the environment exists in a state $s\in\mathcal{S}$, with $\rho$ being the initial
    state distribution. The agent takes an action $a\in\mathcal{A}$ which *transitions*
    the environment to a new state $s^{\prime}\in\mathcal{S}$. The stochastic transition
    function governs the dynamics of such transitions $P:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$,
    which takes the state $s$ and action $a$ as input and outputs a probability distribution
    over the next states $\Delta(.)$ from which the next state $s^{\prime}$ can be
    sampled. For each transition, the agent receives a reward $R:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$,
    with $R\in\mathcal{R}$. The sequence $(s,a,r,s^{\prime})$ is called an experience.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 序列决策问题通常通过马尔可夫决策过程（MDP）的概念来形式化（？，？），其可以写作一个5元组$\mathcal{M}=\langle\mathcal{S},\mathcal{A},R,P,\rho\rangle$。在任何时间步，环境处于状态$s\in\mathcal{S}$，其中$\rho$是初始状态分布。智能体采取一个动作$a\in\mathcal{A}$，这会*使*环境转移到一个新的状态$s^{\prime}\in\mathcal{S}$。随机转移函数控制这种转移的动态$P:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$，它将状态$s$和动作$a$作为输入，并输出一个在下一状态$s^{\prime}$上取样的概率分布$\Delta(.)$。对于每一次转移，智能体会获得一个奖励$R:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$，其中$R\in\mathcal{R}$。序列$(s,a,r,s^{\prime})$称为一次经历。
- en: The agent acts according to a policy $\pi:\mathcal{S}\to\Delta(\mathcal{A})$,
    in a space of policies $\Pi$, that produces a probability distribution over actions
    given a state. This distribution is a delta distribution for deterministic policies,
    which leads to the policy outputting a single action. Using the current policy,
    an agent can repeatedly generate experiences, and a sequence of such experiences
    is also called a *trajectory* ($\tau$). In episodic RL, the trajectory consists
    of experiences collected over multiple episodes with environment resets, while
    in continual settings, the trajectory encompasses experiences collected over some
    horizon in a single episode. The rewards in $\tau$ can be accumulated into an
    expected sum called the return $G$, which can be calculated for any starting state
    $s$ as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体根据策略$\pi:\mathcal{S}\to\Delta(\mathcal{A})$行动，策略空间为$\Pi$，该策略在给定状态下生成一个动作的概率分布。对于确定性策略，这个分布是一个delta分布，这导致策略输出一个单一动作。使用当前策略，智能体可以重复生成经历，这些经历的序列也称为*轨迹*（$\tau$）。在
    episodic RL 中，轨迹由在多个环境重置的回合中收集的经历组成，而在持续设置中，轨迹包括在单个回合中的某个时间范围内收集的经历。$\tau$中的奖励可以累计成一个期望和，称为回报$G$，可以为任何起始状态$s$计算如下：
- en: '|  | $G(\pi,s)=\mathbb{E}_{(s_{0}=s,a_{1},r_{1},\dots)\sim\mathcal{M}}\bigg{[}\sum_{t=0}^{\infty}r_{t}\bigg{]}.$
    |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $G(\pi,s)=\mathbb{E}_{(s_{0}=s,a_{1},r_{1},\dots)\sim\mathcal{M}}\bigg{[}\sum_{t=0}^{\infty}r_{t}\bigg{]}.$
    |  | (1) |'
- en: 'For the sum in [Equation 1](#S2.E1 "In 2.1 Markov Decision Processes ‣ 2 Preliminaries
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems") to be tractable,
    we either assume the horizon of the problem to be of a fixed length $T$ (finite-horizon
    return), i.e., the trajectory to terminate after $T$-steps, or we discount the
    future rewards by a discount factor $\gamma$ (infinite horizon return). Discounting,
    however, can also be applied to finite horizons. Solving an MDP amounts to determining
    the policy $\pi^{*}\in\Pi$ that maximizes the expectation over the returns of
    its trajectory. This expectation can be captured by the notion of the (state-action)
    value function $Q\in\mathcal{Q}$. Given a policy $\pi$, the expectation can be
    written recursively:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使[方程1](#S2.E1 "在2.1马尔可夫决策过程 ‣ 2 基础 ‣ 强化学习中的结构：调查与开放问题")中的求和是可处理的，我们要么假设问题的时间范围是固定长度$T$（有限时间范围回报），即轨迹在$T$步后终止，要么通过折扣因子$\gamma$对未来的回报进行折扣（无限时间范围回报）。然而，折扣也可以应用于有限时间范围。解决MDP的问题在于确定一个策略$\pi^{*}\in\Pi$，该策略最大化其轨迹回报的期望。这个期望可以通过（状态-动作）价值函数$Q\in\mathcal{Q}$来捕捉。给定策略$\pi$，期望可以递归地表示：
- en: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{s\sim\rho}\big{[}G_{t}\mid s,a\big{]}=\mathbb{E}_{s^{\prime}\sim\tau}\big{[}R(s,a)+\gamma\mathbb{E}_{a^{\prime}\sim\pi(\cdot\mid
    s^{\prime})}[Q^{\pi}(s^{\prime},a^{\prime})]\big{]}.$ |  | (2) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{s\sim\rho}\big{[}G_{t}\mid s,a\big{]}=\mathbb{E}_{s^{\prime}\sim\tau}\big{[}R(s,a)+\gamma\mathbb{E}_{a^{\prime}\sim\pi(\cdot\mid
    s^{\prime})}[Q^{\pi}(s^{\prime},a^{\prime})]\big{]}.$ |  | (2) |'
- en: 'Thus, the goal can now be formulated as the task of finding an optimal policy
    that can maximize the $Q^{\pi}(s,a)$:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标现在可以被表述为找到一个最优策略，以最大化 $Q^{\pi}(s,a)$：
- en: '|  | $\pi^{*}\in\operatorname*{arg\,max}_{\pi\in\Pi}Q^{\pi}(s,a).$ |  | (3)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}\in\operatorname*{arg\,max}_{\pi\in\Pi}Q^{\pi}(s,a).$ |  | (3)
    |'
- en: We also consider partially observable MDPs (POMDPs), which model situations
    where the state is not fully observable. A POMDP is defined as a 7-tuple $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{O},R,P,\xi,\rho\rangle$,
    where $\mathcal{S},\mathcal{A},R,P,\rho$ remain the same as defined above. Instead
    of observing the state $s\in\mathcal{S}$, the agent now has access to observation
    $o\in\mathcal{O}$ that is generated from the true state through an emission function
    $\xi:\mathcal{S}\to\mathcal{O}$. Thus, the observation takes the state’s role
    in the experience generation process, and the rest of the learning process can
    now be conditioned on $o$ instead of $s$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考虑了部分可观测的马尔可夫决策过程（POMDP），它们建模了状态不能完全观察的情况。POMDP 被定义为 7 元组 $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{O},R,P,\xi,\rho\rangle$，其中
    $\mathcal{S},\mathcal{A},R,P,\rho$ 与上述定义相同。代理现在不再观察状态 $s\in\mathcal{S}$，而是通过一个发射函数
    $\xi:\mathcal{S}\to\mathcal{O}$ 从真实状态生成观察值 $o\in\mathcal{O}$。因此，观察值在经验生成过程中承担了状态的角色，学习过程的其余部分现在可以以
    $o$ 作为条件，而不是 $s$。
- en: 2.2 Reinforcement Learning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 强化学习
- en: 'The task of an RL algorithm is to interact with the MDP by simulating its transition
    dynamics $P(s^{\prime}\mid s,a)$ and reward function $R(s,a)$ and learn the optimal
    policy mentioned in [Equation 3](#S2.E3 "In 2.1 Markov Decision Processes ‣ 2
    Preliminaries ‣ Structure in Reinforcement Learning: A Survey and Open Problems").
    In Deep RL, the policy is a Deep Neural Network (?) that is used to generate $\tau$.
    We can optimize such a policy by minimizing an appropriate objective $J\in\mathcal{J}$.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: RL 算法的任务是通过模拟其转移动态 $P(s^{\prime}\mid s,a)$ 和奖励函数 $R(s,a)$ 来与 MDP 进行交互，并学习[公式
    3](#S2.E3 "在 2.1 马尔可夫决策过程 ‣ 2 预备知识 ‣ 强化学习中的结构：综述与开放问题") 中提到的最优策略。在深度 RL 中，策略是一个深度神经网络
    (?)，用于生成 $\tau$。我们可以通过最小化一个适当的目标 $J\in\mathcal{J}$ 来优化这种策略。
- en: '![Refer to caption](img/e208b8299c729ef6a86b3821f5def762.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e208b8299c729ef6a86b3821f5def762.png)'
- en: 'Figure 2: The anatomy of an RL pipeline.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：RL 流水线的结构图。
- en: A model of an MDP $\hat{\mathcal{M}}$ allows an agent to *plan* a trajectory
    by simulating it to generate experiences. RL methods that use such models are
    categorized into *Model-Based RL* (?). On the other hand, not having such a model
    necessitates learning the policy directly from experiences, and such methods fall
    into the category of *Model-free RL*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 的模型 $\hat{\mathcal{M}}$ 允许代理通过模拟来*规划*轨迹，从而生成经验。使用这种模型的 RL 方法被归类为 *基于模型的
    RL*（?）。另一方面，没有这种模型就需要直接从经验中学习策略，这些方法属于 *无模型 RL* 类别。
- en: RL methods can additionally be categorized based on the type of objective $J$.
    Methods that use a value function, and correspondingly the Temporal Difference
    error (?), to learn a policy fall into the category of *Value-based RL*. A key
    idea in these methods is *bootstrapping*, where they use a learned value estimate
    to improve the estimate of a state that precedes it. *On-policy* methods directly
    update the policy that generated the experiences, while *Off-policy* methods use
    a separate policy to generate experiences. *Policy-Based* Methods parameterize
    the policy directly and use the Policy Gradient (?, ?) to create $J$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RL 方法还可以根据目标 $J$ 的类型进行分类。使用价值函数及相应的时间差分误差 (?) 来学习策略的方法属于 *基于价值的 RL* 类别。这些方法中的一个关键思想是
    *自举*，即它们使用学习到的价值估计来改进其前置状态的估计。*在策略* 方法直接更新生成经验的策略，而 *离策略* 方法使用单独的策略来生成经验。*基于策略*
    方法直接对策略进行参数化，并使用策略梯度 (?, ?) 来创建 $J$。
- en: 'A central research theme in practical RL methods focuses on approximating a
    global solution by iteratively learning one or more of the aforementioned quantities
    using supervised learning and function approximations. We use the notion of a
    pipeline to talk about different RL methods. [Figure 2](#S2.F2 "In 2.2 Reinforcement
    Learning ‣ 2 Preliminaries ‣ Structure in Reinforcement Learning: A Survey and
    Open Problems") shows the anatomy of an RL pipeline. The pipeline can be defined
    as a mathematical tuple $\Omega=\langle\mathcal{S},\mathcal{A},R,P,Q,\pi,\hat{\mathcal{M}},J,\mathcal{E}\rangle$,
    where all definitions remain the same as before. To solve an MDP, the pipeline
    operates on given an environment $\mathcal{E}$ by taking the state $s\in\mathcal{S}$
    as input and producing an action $a\in\mathcal{A}$ as an output. The environment
    operates with the dynamics $P$ and a reward function $R$. The pipeline might generate
    experiences by directly interacting with $\mathcal{E}$, i.e., *learning* from
    experiences or by simulating a learned model $\hat{\mathcal{M}}$ of the environment.
    The optimization procedure encompasses the interplay between the current policy
    $\pi$, its value function $Q$, the reward $R$, and the learning objective $J$.
    With a slight abuse of notation, we refer to any of the components of a pipeline
    as $X$ and assume the space in which it exists as $\mathcal{X}$.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实际RL方法中的一个核心研究主题集中于通过迭代学习一个或多个上述量来逼近全局解，这些量使用监督学习和函数逼近技术。我们使用“管道”这一概念来讨论不同的RL方法。[图2](#S2.F2
    "在2.2 强化学习 ‣ 2 基础知识 ‣ 强化学习中的结构：综述与开放问题")展示了RL管道的结构。管道可以定义为数学元组$\Omega=\langle\mathcal{S},\mathcal{A},R,P,Q,\pi,\hat{\mathcal{M}},J,\mathcal{E}\rangle$，其中所有定义保持不变。为了解决MDP，管道在给定环境$\mathcal{E}$的情况下，通过将状态$s\in\mathcal{S}$作为输入并生成动作$a\in\mathcal{A}$作为输出来操作。环境通过动态$P$和奖励函数$R$进行操作。管道可能通过直接与$\mathcal{E}$交互，即*从经验中学习*，或通过模拟环境的学习模型$\hat{\mathcal{M}}$来生成经验。优化过程涵盖了当前策略$\pi$、其价值函数$Q$、奖励$R$和学习目标$J$之间的相互作用。稍有滥用符号，我们将管道的任何组件称为$X$，并假设其存在的空间为$\mathcal{X}$。
- en: 3 Side Information and its Usage
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 辅助信息及其使用
- en: In addition to the characterization of the problem by an MDP, there can still
    be additional information that could potentially improve performance on additional
    metrics such as *Sample Efficiency*, *Generalization*, *Interpretability*, and
    *Safety*. We call this *Side Information* (also referred to as privileged information).
    For the (semi-) supervised and unsupervised settings, side information is any
    additional information $z\in\mathcal{Z}$ that, while being neither part of the
    input nor the output space, can potentially contribute to the learning process
    (?).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过MDP对问题进行表征外，仍然可能存在额外的信息，这些信息可能会改善在*样本效率*、*泛化*、*可解释性*和*安全性*等附加指标上的性能。我们称这些为*辅助信息*（也称为特权信息）。对于（半）监督和无监督设置，辅助信息是任何额外的信息$z\in\mathcal{Z}$，虽然它既不是输入也不是输出空间的一部分，但可能对学习过程有贡献（？）。
- en: 'Translated to the RL setting, this can be understood as additional information
    $z$ not provided in the original MDP definition $\mathcal{M}$. Such information
    can be incorporated into the learning process by a function $\beta:\mathcal{Z}\to\mathcal{X}$,
    where $\mathcal{X}\in\Omega$. Thus, side information can be incorporated into
    the RL pipeline by biasing one or more of the components shown in [Figure 2](#S2.F2
    "In 2.2 Reinforcement Learning ‣ 2 Preliminaries ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 转化为RL设置，这可以理解为原始MDP定义$\mathcal{M}$中未提供的额外信息$z$。这种信息可以通过函数$\beta:\mathcal{Z}\to\mathcal{X}$纳入学习过程，其中$\mathcal{X}\in\Omega$。因此，辅助信息可以通过偏置[图2](#S2.F2
    "在2.2 强化学习 ‣ 2 基础知识 ‣ 强化学习中的结构：综述与开放问题")中所示的一个或多个组件来纳入RL管道。
- en: The natural follow-up question, then, becomes the impact of incorporating side
    information into the learning pipeline. In this work, we specifically focus on
    four ways in which side information can be used and formally define them in the
    following sections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 自然接下来的问题是将辅助信息纳入学习管道的影响。因此，本研究特别关注辅助信息可以使用的四种方式，并在以下章节中正式定义它们。
- en: 3.1 Sample Efficiency
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 样本效率
- en: 'Sample Efficiency is intimately tied to the idea of the Sample Complexity of
    RL. To formally define it, we use the notion of the *Sample Complexity of Exploration*
    (?): Given fixed parameters $\epsilon,\delta>0$, if the difference between the
    value functions of a learned policy $\pi$ and an optimal policy $\pi^{*}$ is $||Q^{\pi}(s,a)-Q^{\star}(s,a)||_{\infty}>\epsilon$,
    then we call this a mistake. If, with a probability of $1-\delta$, the total number
    of mistakes at a timestep $t$ is $\zeta(\epsilon,\delta)$, then we call $\zeta$
    the sample complexity of exploration.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率与强化学习的样本复杂性概念密切相关。为了正式定义它，我们使用*探索的样本复杂性*（？）的概念：给定固定的参数 $\epsilon,\delta>0$，如果学习到的策略
    $\pi$ 和最优策略 $\pi^{*}$ 的价值函数之间的差异为 $||Q^{\pi}(s,a)-Q^{\star}(s,a)||_{\infty}>\epsilon$，则称之为一个错误。如果在时间步
    $t$ 时，错误的总数为 $\zeta(\epsilon,\delta)$，且概率为 $1-\delta$，则 $\zeta$ 称为探索的样本复杂性。
- en: Incorporating side information leads to a reduction in $\zeta$, thus, improving
    the sample efficiency. Intuitively, if a pipeline demonstrates a higher reward
    than a baseline for the same number of timesteps, then we consider it more sample
    efficient. However, methods can additionally make specific claims on $\zeta$ by
    utilizing certain assumptions about the problem itself (?, ?, ?).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 纳入旁侧信息会减少 $\zeta$，从而提高样本效率。直观上，如果一个管道在相同时间步数下表现出的奖励高于基线，则我们认为它更具样本效率。然而，方法还可以通过利用对问题本身的某些假设来对
    $\zeta$ 做出具体声明 (？，？，？)。
- en: Exploration.
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索。
- en: 'One specific way to improve the sample complexity of exploration is to impact
    the exploration mechanism using side information directly. ? (?) categorize exploration
    methods based on the type of information that an agent uses to explore the world
    into the following categories: (i) *Reward-Free Exploration*methods in which extrinsic
    rewards do not affect the choice of action. Instead, they rely on intrinsic forms
    of exploration. (ii) *Randomized Action Selection*methods use estimated value
    functions, policies, or rewards to induce exploratory behavior. (iii) *Optimism/Bonus-Based
    Exploration*methods use the *optimism in the face of uncertainty* paradigm to
    prefer actions with higher uncertain values. (iv) *Deliberate Exploration*methods
    that either use posterior distributions over dynamics (Bayesian setup) or meta-learning
    techniques to optimally solve exploration and (v) *Probability Matching*methods
    that use heuristics to select the next action. Incorporating side information
    into any of these methods generally leads to better sample efficiency, particularly
    by improving the state-space coverage of the exploration mechanism.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提高样本复杂性的一个具体方法是直接使用旁侧信息影响探索机制。? (?) 将探索方法根据代理使用的信息类型分为以下几类：（i）*无奖励探索*方法，其中外部奖励不会影响行动选择。相反，它们依赖于内部形式的探索。（ii）*随机行动选择*方法使用估计的价值函数、策略或奖励来引发探索行为。（iii）*乐观/奖金基础探索*方法使用*面对不确定性的乐观*范式来偏好具有更高不确定值的行动。（iv）*深思熟虑的探索*方法利用动态的后验分布（贝叶斯设置）或元学习技术来优化解决探索问题。（v）*概率匹配*方法使用启发式来选择下一个行动。将旁侧信息纳入这些方法中的任何一种通常会提高样本效率，特别是通过改善探索机制的状态空间覆盖。
- en: 3.2 Transfer and Generalization
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 迁移和泛化
- en: 'Transfer and generalization encompass performance metrics that measure how
    an RL agent performs on a set of different MDPs: Transfer evaluates how well an
    agent, trained on some MDP $\mathcal{M}_{i}$ performs on some other MDP $\mathcal{M}_{j}$.
    This can be either done in a zero-shot manner, where the agent is not fine-tuned
    on $\mathcal{M}_{j}$, or in a few-shot manner, where the agent gets to make some
    policy updates on $\mathcal{M}_{j}$ to learn as fast as possible. Generally, the
    performance gap between the two MDPs determines the transfer performance.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移和泛化包括衡量强化学习代理在不同MDP集上表现的性能指标：迁移评估一个在某些MDP $\mathcal{M}_{i}$ 上训练的代理在其他MDP $\mathcal{M}_{j}$
    上的表现。这可以是零样本方式，即代理不在$\mathcal{M}_{j}$ 上进行微调，或是少样本方式，即代理对$\mathcal{M}_{j}$ 进行一些策略更新以尽可能快地学习。通常，两个MDP之间的性能差距决定了迁移性能。
- en: '|  | $J_{\text{transfer}}(\pi):=\bm{G}(\pi,\mathcal{M}_{i})-\bm{G}(\pi,\mathcal{M}_{j}).$
    |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{\text{transfer}}(\pi):=\bm{G}(\pi,\mathcal{M}_{i})-\bm{G}(\pi,\mathcal{M}_{j}).$
    |  | (4) |'
- en: Generalization extends this idea to training an agent on a set of training MDPs
    $\bm{\mathcal{M}}_{train}$, and then evaluating its performance on a separate
    set of MDPs $\bm{\mathcal{M}}_{test}$. Consequently, generalization (?) can then
    be measured by the metric.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化将这一思想扩展到在一组训练MDP $\bm{\mathcal{M}}_{train}$ 上训练代理，然后在一组单独的MDP $\bm{\mathcal{M}}_{test}$
    上评估其性能。因此，泛化（?）可以通过该指标来衡量。
- en: '|  | $\text{Gen}(\pi):=\bm{G}(\pi,\bm{\mathcal{M}}_{train})-\bm{G}(\pi,\bm{\mathcal{M}}_{test}).$
    |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Gen}(\pi):=\bm{G}(\pi,\bm{\mathcal{M}}_{train})-\bm{G}(\pi,\bm{\mathcal{M}}_{test}).$
    |  | (5) |'
- en: A more restrictive form of generalization can be evaluated when the training
    and testing MDPs are sampled from the same distribution, i.e., $\bm{\mathcal{M}}_{train},\bm{\mathcal{M}}_{test}\sim
    p(\bm{\mathcal{M}})$. Depending on how the transfer is done (zero-shot, few-shot,
    etc.), this notion covers any form of distribution of MDPs, including multi-task
    settings. Incorporating side information into the learning can minimize $\text{Gen}(\pi)$
    by capturing similarities between $\bm{\mathcal{M}}_{train}$ and $\bm{\mathcal{M}}_{test}$
    in many different ways.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练和测试MDP从相同的分布中抽样时，即 $\bm{\mathcal{M}}_{train},\bm{\mathcal{M}}_{test}\sim
    p(\bm{\mathcal{M}})$，可以评估更严格的泛化形式。根据转移的方式（零-shot、少-shot等），该概念涵盖了MDP的任何分布形式，包括多任务设置。将侧信息融入学习可以通过以多种方式捕捉
    $\bm{\mathcal{M}}_{train}$ 和 $\bm{\mathcal{M}}_{test}$ 之间的相似性来最小化 $\text{Gen}(\pi)$。
- en: 3.3 Interpretability
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 可解释性
- en: 'Interpretability refers to a mechanistic understanding of a system to make
    it more transparent. ? (?) enumerate three fundamental properties of model interpretability:
    (i) Simulatabilityrefers to the ability of a human to simulate the inner working
    of a system, (ii) Decomposabiltiyrefers to adding intuitive understanding to individual
    working parts of a system, (iii) Transparencyrefers to improving the understanding
    of a system’s function (e.g., quantifying its convergence properties).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性指的是对系统进行机制性理解，以使其更加透明。 ?（?）列举了模型可解释性的三个基本属性：（i）可模拟性指人类能够模拟系统的内部工作，（ii）可分解性指对系统的各个工作部分增加直观理解，（iii）透明性指改善对系统功能的理解（例如，量化其收敛特性）。
- en: Given the coupled nature of individual parts of an RL pipeline, adding interpretability
    amounts to being able to learn a policy for the MDP that adheres to one of multiple
    such properties. Incorporating side information can help take steps towards improved
    performance on all three of these metrics, depending on the nature of side information
    and what it encompasses. We consider claims on interpretability based on whether
    a given work additionally addresses at least two of these metrics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到RL管道中各个部分的耦合特性，增加可解释性意味着能够学习一个符合多个此类属性的MDP策略。结合侧信息可以帮助在这三项指标上改进性能，具体取决于侧信息的性质及其涵盖内容。我们基于是否有额外处理至少这两个指标来考虑关于可解释性的声明。
- en: 3.4 Safety
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 安全性
- en: Safety is the idea of learning policies that maximize the expectation of the
    return in problems in which it is important to ensure reasonable system performance
    and respect safety constraints during the learning and or deployment processes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性是学习策略以最大化回报期望的概念，在这些问题中，确保系统性能合理并在学习和/或部署过程中尊重安全约束是重要的。
- en: 'While Safety in RL is a vast field in and of itself (?), we consider two specific
    categories in this work: *Safe Learning with constraints* and *Safe Exploration*.
    The former modifies the expected return account for one or more constraints $c_{i}\in
    C$, and the general form can be written as'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RL中的安全性是一个广泛的领域（?），我们在本研究中考虑了两个具体的类别：*带约束的安全学习*和*安全探索*。前者通过修改预期回报来考虑一个或多个约束
    $c_{i}\in C$，其一般形式可以写成
- en: '|  | $\max_{\pi\in\Pi}\mathbb{E}_{\pi}(G)\,\,\,s.t.\,\,\,c_{i}=\{h_{i}\leq\alpha\}$
    |  | (6) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi\in\Pi}\mathbb{E}_{\pi}(G)\,\,\,s.t.\,\,\,c_{i}=\{h_{i}\leq\alpha\}$
    |  | (6) |'
- en: where $h_{i}$ is a function related to the return and $\alpha$ is the threshold
    restricting the values of this function. Consequently, side information can be
    used in the formulation of such constraints.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{i}$ 是与回报相关的函数，$\alpha$ 是限制该函数值的阈值。因此，可以在这些约束的制定中使用侧信息。
- en: On the other hand, Safe Exploration modifies the exploration process subject
    to external knowledge, which in our case translates to incorporating side information
    into the exploration process. While intuitively, this overlaps with the usage
    of side information for directed exploration, a distinguishing feature of this
    work is the final goal of this directed exploration to be safety, which might
    potentially come at the cost of sample efficiency and/or generalization.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，安全探索修改了受外部知识影响的探索过程，在我们的案例中，这转化为将附加信息纳入探索过程。虽然直观上，这与用于有目的探索的附加信息的使用重叠，但这项工作的一个显著特征是这种有目的探索的最终目标是安全，这可能会以样本效率和/或泛化能力的损失为代价。
- en: 4 Structure as Side Information
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结构作为附加信息
- en: Structure can be considered a particular kind of side information available
    to a learning agent. To build an intuition about what we mean by this, consider
    the task of managing a large factory with many production cells ¹¹1example taken
    from ? (?). If a cell positioned early in the production line generates faulty
    parts, the whole factory may be affected. However, the quality of the parts a
    cell generates depends directly only on the state of this cell and the quality
    of the parts it receives from neighboring cells. Additionally, the cost of running
    the factory depends, among other things, on the sum of the costs of maintaining
    each local cell. Finally, while a cell responsible for anodization may receive
    parts directly from any other cell in the factory, a work order for a cylindrical
    part may restrict this dependency to cells with a lathe. In the context of producing
    cylindrical parts, the quality of the anodized parts depends directly only on
    the state of cells with a lathe. Thus, by incorporating information about the
    additive nature of production, costs, and the context of the part that needs to
    be produced, the learning pipeline can be imbued with better objectives such as
    improved sample efficiency or robustness of a learned policy to changing factory
    conditions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结构可以被视为对学习主体提供的一种特定类型的附加信息。为了建立对这一点的直观理解，可以考虑管理一个有多个生产单元的大型工厂的任务¹¹1例子取自？（？）。如果生产线早期的一个单元产生了缺陷部件，整个工厂可能会受到影响。然而，一个单元所生成部件的质量直接取决于该单元的状态和它从邻近单元接收到的部件质量。此外，工厂运营的成本还取决于维护每个本地单元的成本总和。最后，虽然负责阳极化的单元可能会直接从工厂中的任何其他单元接收部件，但针对圆柱形部件的工作订单可能会将这一依赖关系限制在具有车床的单元中。在生产圆柱形部件的背景下，阳极化部件的质量仅直接取决于具有车床的单元的状态。因此，通过结合关于生产的附加性质、成本和需要生产的部件的背景的信息，学习流程可以赋予更好的目标，如提高样本效率或对变化的工厂条件的学习策略的鲁棒性。
- en: The vanilla MDP framework does not require incorporating such additive information
    into the learning pipeline. Thus, biasing the pipeline with this amounts to incorporating
    side information. However, this information particularly helps decompose the complicated
    learning problem into additive sub-parts that can be learned independently and,
    potentially, more efficiently. Thus, structure is a particular kind of side information
    that provides the learning pipeline with knowledge about decomposability in the
    learning problem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的MDP框架不要求将这种附加信息纳入学习流程。因此，偏向于该流程就是引入附加信息。然而，这种信息特别有助于将复杂的学习问题分解成可以独立学习且潜在地更高效的附加子部分。因此，结构是一种特定类型的附加信息，它为学习流程提供了关于学习问题可分解性的知识。
- en: 'In this section, we discuss the relationship between structure and decomposability.
    In [Section 4.1](#S4.SS1 "4.1 Decomposability and Structural Archetypes ‣ 4 Structure
    as Side Information ‣ Structure in Reinforcement Learning: A Survey and Open Problems"),
    we explain the impact of structural side information by explaining how it decomposes
    complex systems and categorizes such decompositions into four archetypes. In [Section 4.2](#S4.SS2
    "4.2 Latent Decomposition ‣ 4 Structure as Side Information ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems") - [Section 4.5](#S4.SS5 "4.5 Modular Decomposition
    ‣ 4 Structure as Side Information ‣ Structure in Reinforcement Learning: A Survey
    and Open Problems"), we discuss these archetypes further to connect them with
    existing literature.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论结构与可分解性之间的关系。在[第4.1节](#S4.SS1 "4.1 可分解性与结构原型 ‣ 4 结构作为附加信息 ‣ 强化学习中的结构：调查与开放问题")中，我们通过解释结构附加信息的影响来解释它如何分解复杂系统，并将这些分解分类为四种原型。在[第4.2节](#S4.SS2
    "4.2 潜在分解 ‣ 4 结构作为附加信息 ‣ 强化学习中的结构：调查与开放问题") - [第4.5节](#S4.SS5 "4.5 模块化分解 ‣ 4 结构作为附加信息
    ‣ 强化学习中的结构：调查与开放问题")中，我们进一步讨论这些原型，并将其与现有文献联系起来。
- en: 4.1 Decomposability and Structural Archetypes
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 可分解性与结构原型
- en: Decomposability is the property of a system that allows breaking it down into
    smaller components or subsystems that can be analyzed, understood, and potentially
    learned more efficiently than the larger system, independently (?). In a decomposable
    system, the short-term behavior of each subsystem is approximately independent
    of the short-term behavior of the other subsystems. In the long run, the behavior
    of any one subsystem depends on the behavior of the other subsystems only in an
    aggregated way.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可分解性是指系统允许将其分解为更小的组件或子系统的属性，这些组件或子系统可以被分析、理解，并可能比较大的系统更高效地学习。在一个可分解的系统中，每个子系统的短期行为大致独立于其他子系统的短期行为。从长远来看，任何一个子系统的行为仅以汇总的方式依赖于其他子系统的行为。
- en: 'Concerning the RL pipeline in [Figure 2](#S2.F2 "In 2.2 Reinforcement Learning
    ‣ 2 Preliminaries ‣ Structure in Reinforcement Learning: A Survey and Open Problems"),
    we can see decomposability along two axes: (i) *Problem Decomposition*i.e., the
    environment parameterization, states, actions, transitions, and rewards; (ii)
    *Solution Decomposition*i.e., the learned policies, value functions, models, and
    training procedures. The spectrum of decomposability (?) provides an intuitive
    way to understand where a system lies in this regard. On one end of the spectrum,
    problems are non-decomposable, while on the other end, problems can be decomposed
    into weakly interacting sub-problems. Similarly, solutions on the former are monolithic,
    while those on the latter are modular. We capture this problem-solution interplay
    by marking four different archetypes of decomposability, as shown in [Figure 3](#S4.F3
    "In 4.1 Decomposability and Structural Archetypes ‣ 4 Structure as Side Information
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[图2](#S2.F2 "在2.2 强化学习 ‣ 2 基础 ‣ 强化学习中的结构：调查与开放问题")中的RL管道，我们可以看到沿两个轴的可分解性：(i)
    *问题分解*，即环境参数化、状态、动作、转换和奖励；(ii) *解决方案分解*，即学习到的策略、价值函数、模型和训练过程。可分解性的光谱（?）提供了一种直观的方法来理解系统在这方面的位置。在光谱的一端，问题是不可分解的，而在另一端，问题可以分解为弱相互作用的子问题。类似地，前者的解决方案是整体的，而后者的解决方案是模块化的。我们通过标记四种不同的可分解性原型来捕捉这一问题-解决方案的互动，如[图3](#S4.F3
    "在4.1 可分解性与结构原型 ‣ 4 结构作为附加信息 ‣ 强化学习中的结构：调查与开放问题")所示。
- en: '![Refer to caption](img/71d01ff23ecf44acac0e39501e2c515f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/71d01ff23ecf44acac0e39501e2c515f.png)'
- en: 'Figure 3: Spectrum of Decomposability and Structural Archetypes. On the left
    end of the spectrum exist monolithic structural decompositions where knowledge
    about a *latent* subspace of $\mathcal{X}$ can be learned and incorporated as
    an inductive bias. Moving towards the right, we can learn multiple independent
    subspaces, albeit in a monolithic solution. These are *factored* decompositions.
    Further ahead, we see the emergence of interactionally complex decompositions,
    where knowledge about factorization and how they relate to each other can be incorporated
    into the learning process. We call these *relational decompositions*. Finally,
    we see fully distributed subsystems that can be incorporated and learned using
    individual policies. We call these *modular decompositions*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：可分解性谱和结构原型。在谱的左端存在整体结构分解，其中关于$\mathcal{X}$的*潜在*子空间的知识可以被学习并作为归纳偏置融入。向右移动，我们可以学习多个独立的子空间，尽管是在整体解决方案中。这些是*因式分解*。进一步前进，我们看到互动复杂的分解的出现，其中关于因式分解及其相互关系的知识可以融入学习过程。我们称这些为*关系分解*。最后，我们看到完全分布的子系统，这些子系统可以通过个体策略进行整合和学习。我们称这些为*模块化分解*。
- en: 4.2 Latent Decomposition
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 潜在分解
- en: Latent decompositions are monolithic and can be useful in complex environments
    where the underlying structure is either unclear or non-stationary. Under this
    view, a pipeline component $\mathcal{X}$ can be approximated by a latent representation
    $\kappa$, which can then be integrated into the learning process. The quantities
    in the update that depend on $\mathcal{X}$ can now be re-conditioned on $\kappa$,
    which helps in improving performance by reducing the dimensionality of $\mathcal{X}$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在分解是整体性的，并且在基础结构不明确或非平稳的复杂环境中可能非常有用。从这个角度来看，管道组件$\mathcal{X}$可以通过潜在表示$\kappa$进行近似，然后可以将其集成到学习过程中。现在，依赖于$\mathcal{X}$的更新量可以重新条件化在$\kappa$上，这有助于通过减少$\mathcal{X}$的维度来提高性能。
- en: Latent States and Actions.
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在状态和动作。
- en: Latent state spaces have been classically explored through the Latent MDP literature (?),
    where the aim is to discover a latent representation of the state space that is
    sufficient to learn an optimal policy. Latent states are used for tackling rich
    observation spaces, where Block MDPs (?) and Contextual MDPs (?) have shown success
    in generalization problems. Latent actions have been similarly explored in settings
    with stochastic action sets (?).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在状态空间经典上通过潜在MDP文献 (?) 进行了探索，其目标是发现足够学习最优策略的状态空间的潜在表示。潜在状态用于解决丰富的观察空间，其中Block
    MDPs (?) 和Contextual MDPs (?) 在泛化问题上表现成功。潜在动作在具有随机动作集 (?) 的设置中也得到了类似的探索。
- en: Given an encoder $\phi:\mathcal{S}\times\mathcal{A}\to\mathcal{\kappa}$ and
    a decoder $\beta:\mathcal{\kappa}\to\mathcal{S}$, the latent state-action formulation
    allows decomposing transition dynamics into a low-rank approximation,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个编码器$\phi:\mathcal{S}\times\mathcal{A}\to\mathcal{\kappa}$和一个解码器$\beta:\mathcal{\kappa}\to\mathcal{S}$，潜在状态-动作表述允许将转移动态分解为低秩近似，
- en: '|  | $P(s^{\prime}\mid s,a)=\langle\phi(\kappa\mid s,a)\beta(s^{\prime}\mid\kappa)\rangle.$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(s^{\prime}\mid s,a)=\langle\phi(\kappa\mid s,a)\beta(s^{\prime}\mid\kappa)\rangle.$
    |  |'
- en: Latent Transition and Rewards.
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在转移和奖励。
- en: 'While latent states allow decomposing transition matrices, another way to approach
    the problem directly is to decompose transition matrices into low-rank approximations:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然潜在状态允许将转移矩阵分解，另一种直接解决问题的方法是将转移矩阵分解为低秩近似：
- en: '|  | $P(s^{\prime}\mid s,a)=\phi(s^{\prime}\mid s,a)\beta(s^{\prime}\mid s,a).$
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(s^{\prime}\mid s,a)=\phi(s^{\prime}\mid s,a)\beta(s^{\prime}\mid s,a).$
    |  |'
- en: Linear MDPs (?) and corresponding applications in Model-based RL (?, ?) have
    studied this form of direct decomposition.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 线性MDPs (?) 和相应的模型基RL应用 (?) 已研究这种直接分解形式。
- en: A similar decomposition can be applied to rewards as well. ? (?) have primarily
    explored this in noisy reward settings where the reward signal is assumed to be
    generated from a latent function that can be learned as an auxiliary learning
    objective.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的分解也可以应用于奖励。? (?) 主要在噪声奖励设置中探索了这一点，其中奖励信号被假定为从一个可以作为辅助学习目标进行学习的潜在函数生成的。
- en: 4.3 Factored Decomposition
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 因式分解
- en: The factored decomposition moves slightly away from the monolithic nature by
    decomposing $\mathcal{X}$ into (latent) factors $\kappa_{1},\dots,\kappa_{n}$.
    Thus, the spaces become inner products of the individual factor spaces. A crucial
    aspect of factorization is that the factors can potentially impose conditional
    independence in their effects on the learning dynamics.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分解的分解略微偏离了整体性质，通过将$\mathcal{X}$分解为（潜在的）因子$\kappa_{1},\dots,\kappa_{n}$。因此，这些空间变成了各个因子空间的内积。分解的一个关键方面是，这些因子可能会在对学习动态的影响中施加条件独立性。
- en: Factored States and Actions.
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事实状态和行动。
- en: Factored state and action spaces have been explored in the Factored MDPs (?, ?, ?).
    Methods in this setting traditionally capture next state distribution using mechanisms
    such as Dynamic Bayesian Networks (?).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在分解马尔可夫决策过程（MDP）中，已经探索了分解状态和行动空间（?，?，?）。在这种情况下的方法传统上使用动态贝叶斯网络（?）等机制来捕捉下一个状态的分布。
- en: Factorization in the action space has also been used for tackling high-dimensional
    actions (?). These methods either impose a factorized structure on subsets of
    a high-dimensional action set (?) or impose this structure through the Q-values
    that lead to the final action (?). Crucially, these methods can potentially exploit
    some form of independence resulting from such factorization, either in the state
    representations or transitions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在行动空间中的分解也已被用于解决高维行动（?）。这些方法要么对高维行动集的子集施加分解结构（?），要么通过导致最终行动的Q值施加这一结构（?）。关键是，这些方法可能利用因分解而产生的某种形式的独立性，无论是在状态表示还是过渡中。
- en: Factored Transitions and Rewards.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分解过渡和奖励。
- en: Combined with factored states or modeled independently, factored rewards have
    been used to model perturbed rewards and multi-objective settings (?). While Factored
    MDPs do not naturally lead to factored policies, combining state and reward factorization
    can lead to factorization of Q-values (?, ?).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 结合分解状态或独立建模，分解奖励已被用于建模扰动奖励和多目标设置（?）。虽然分解马尔可夫决策过程（MDP）自然不会导致分解策略，但结合状态和奖励分解可以导致Q值的分解（?，?）。
- en: 4.4 Relational Decomposition
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 关系分解
- en: Relational decompositions add a further notion of separability where in addition
    to the factored subsets, capturing immutable relations between them becomes important
    as well. Usually, these relations exist between entities in a scene and are used
    to formulate learning methods based on inductive logic (?). Traditionally, such
    relations were limited to first-order logic, but the relational structure has
    also been captured through graphs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关系分解增加了进一步的可分离性概念，除了分解子集外，捕捉它们之间不可变的关系也变得重要。通常，这些关系存在于场景中的实体之间，并用于基于归纳逻辑（?）的学习方法的制定。传统上，这些关系限于一阶逻辑，但关系结构也通过图捕捉到了。
- en: The relational assumption posits that a space of predicates $\beta$ can ground
    these entities, and it can be modeled as a set of rules (such as inductive logic)
    that define how $z_{i},z_{j}\in Z$ interact with each other. An extension of this
    is by capturing $\beta$ as a graph $G=\langle V,E\rangle$ where the vertices are
    $z_{i}\in Z$ and the edges represent the relationship between them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 关系假设认为，谓词空间$\beta$可以为这些实体提供基础，它可以被建模为一组规则（如归纳逻辑），定义$z_{i},z_{j}\in Z$之间的相互作用。其扩展是通过捕捉$\beta$作为图$G=\langle
    V,E\rangle$，其中顶点是$z_{i}\in Z$，边表示它们之间的关系。
- en: Using such a representation allows us to talk about generalization over the
    entities $\kappa_{i},\kappa_{j}$, and forms of $\beta$. This helps us circumvent
    the dimensionality of enumerative spaces.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样的表示允许我们讨论实体$\kappa_{i},\kappa_{j}$和$\beta$的形式上的泛化。这有助于我们绕过枚举空间的维度。
- en: Relational States and Actions
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关系状态和行动
- en: Classically, relational representations have been used to model state spaces
    in Relational MDPs (?) and Object-Oriented MDPs (?, ?). They represent factored
    state spaces using first-order representations consisting of objects, predicates,
    and functions on them to describe a set of ground MDPs. Such representations can
    capture interactionally complex structures between entities much more efficiently.
    Additionally, permutations of the interactions between the entities can help define
    new MDPs that differ in their dynamics, thus, contributing towards work in generalization.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，关系表示被用来建模关系型 MDP（?）和面向对象 MDP（?，?）中的状态空间。它们通过一阶表示来表示分解的状态空间，这些表示由对象、谓词及其上的函数组成，以描述一组基础
    MDP。这种表示方式可以更高效地捕捉实体之间的互动复杂结构。此外，实体之间互动的排列可以帮助定义在动态上有所不同的新 MDP，从而有助于泛化工作的推进。
- en: States can also be more generally represented as graphs (?, ?), or by using
    symbolic inductive biases (?) fed to a learning module in addition to the original
    state.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 状态也可以更一般地表示为图（?，?），或通过使用符号归纳偏差（?）输入到学习模块中，作为原始状态的补充。
- en: Action relations help tackle instances where the agent has multiple possible
    actions available, and the set of actions is significantly large. These methods
    capture relations using either attention mechanisms (?, ?) or graphs (?), thus
    offering scalability to high-dimensional action spaces. Additionally, relations
    between states and actions have helped define notions such as intents and affordances (?, ?).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 动作关系有助于解决代理有多个可能的动作可用，且动作集非常大的实例。这些方法通过注意机制（?，?）或图（?）来捕捉关系，从而提供对高维动作空间的可扩展性。此外，状态和动作之间的关系帮助定义了诸如意图和可用性（?，?）的概念。
- en: Relational Value Functions and Policies
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关系值函数和策略
- en: Traditional work in Relational MDPs has also explored ways to represent and
    construct first-order representations of value functions and/or policies to generalize
    to new instances. These include Regression Trees (?), Decision Lists (?), Algebraic
    Decision Diagrams (?), and Linear Basis Functions (?, ?). Recent approaches have
    started looking into DNN representations (?, ?), with extensions into modeling
    problem aspects such as morphology in Robotic tasks (?) in a relational manner,
    or using Graph-Laplacian (?) representations for intrinsic rewards (?).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的关系型 MDP 工作也探索了表示和构建价值函数和/或策略的一阶表示的方法，以便对新实例进行泛化。这些方法包括回归树（?），决策列表（?），代数决策图（?），和线性基函数（?，?）。最近的方法开始关注深度神经网络表示（?，?），以及在关系方式中对机器人任务中的形态学等问题方面的建模扩展（?），或使用图-拉普拉斯（?）表示来进行内在奖励（?）。
- en: Relational Tasks
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关系任务
- en: A parallel line of work looks at capturing relations in a multi-task setting,
    where task perturbations are either in the form of goals and corresponding rewards
    (?, ?, ?). Most work aims at integrating these relationships into the optimization
    procedure and/or additionally capturing them as models. We delve deeper into specifics
    in later sections.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作线条关注于在多任务设置中捕捉关系，其中任务扰动可以是目标和相应奖励（?，?，?）的形式。大多数工作旨在将这些关系整合到优化过程中和/或作为模型进行捕捉。我们将在后续部分深入探讨细节。
- en: 4.5 Modular Decomposition
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 模块化分解
- en: 'Modular decompositions exist at the other end of the spectrum of decomposability,
    where individual value functions and/or policies can be learned for each decomposed
    entity $X$. Specifically, a task can be broken down into individual subsystems
    $\kappa_{1},\dots,\kappa_{N}$ for which models, value functions, and policies
    can be subsequently learned. Such modularity can exist along the following axes:
    (i) *Spatial Modularity*allows learning quantities specific to parts of the state
    space, thus, effectively reducing the dimensionality of the states; (ii) *Temporal
    Modularity*allows breaking down tasks into sequences over a learning horizon and,
    thus, learning modular quantities in a sequence; (iii) *Functional Modularity*allows
    decomposing the policy architecture into functionally modular parts, even if the
    problem is spatially and temporally monolithic.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化分解存在于可分解性的另一端，其中可以为每个分解的实体 $X$ 学习个别的价值函数和/或政策。具体来说，一个任务可以被分解为若干个子系统 $\kappa_{1},\dots,\kappa_{N}$，随后可以为这些子系统学习模型、价值函数和政策。这种模块化可以沿以下轴线存在：(i)
    *空间模块化* 允许学习特定于状态空间部分的量，从而有效地减少状态的维度；(ii) *时间模块化* 允许将任务分解为学习范围内的序列，从而在序列中学习模块化的量；(iii)
    *功能模块化* 允许将策略架构分解为功能上模块化的部分，即使问题在空间和时间上是整体的。
- en: A potential consequence of such breakdown is the emergence of a hierarchy, and
    when learning problems exploit this hierarchical relationship, these problems
    come under the purview of Hierarchical RL (HRL) (?). The learned policies can
    also exhibit a hierarchy, where each policy can choose the lower-level policies
    to execute the subtasks. Each level can be treated as a planning problem (?) or
    a learning problem (?), thus, allowing solutions to combine planning and learning
    through the hierarchy. Hierarchy, however, is not a necessity for modularity.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解的一个潜在结果是层级的出现，当学习问题利用这种层级关系时，这些问题就属于层次化强化学习（HRL）的范畴（?）。学习到的策略也可以表现出层级结构，其中每个策略可以选择低层次的策略来执行子任务。每一层都可以被视为一个规划问题（?）或学习问题（?），从而允许解决方案通过层级结合规划和学习。然而，层级并不是模块化的必要条件。
- en: Modularity in States and Goals
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 状态和目标中的模块化
- en: Modular decomposition of state spaces is primarily studied at high-level planning
    and state-abstractions for HRL methods (?). Additionally, the literature on skills
    has looked into the direction of training policies for individual parts of the
    state-space (?). Similarly, partial models only make predictions or specific parts
    of the observation-action spaces in Model-Based settings (?, ?). Goals have been
    specifically considered in methods that either use goals as an interface between
    levels of hierarchy (?, ?, ?), or as outputs of task specification methods (?, ?).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间的模块化分解主要在高层次规划和HRL方法的状态抽象中进行研究（?）。此外，关于技能的文献也探讨了针对状态空间各个部分的策略训练方向（?）。类似地，部分模型仅在基于模型的设置中对观察-行动空间的特定部分进行预测（?，?）。目标在那些使用目标作为层级间接口的策略中（?，?，?），或作为任务规范方法的输出（?，?）中被特别考虑。
- en: Modularity in Actions
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 行动中的模块化
- en: Modularity in action spaces refers to conditioning policies on learned action
    abstractions. The classic example of such methods belongs to the realm of the
    options framework where policies are associated with temporal abstractions over
    actions (?). In HRL methods, learning and planning of the higher levels are based
    on the lower-level policies and termination conditions of their execution.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 行动空间中的模块化指的是将策略条件化为学习到的行动抽象。此类方法的经典例子属于选项框架的领域，其中策略与行动的时间抽象相关联 (?)。在层次化强化学习（HRL）方法中，高层次的学习和规划基于低层次的政策和其执行的终止条件。
- en: Compositional Policies
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组合策略
- en: Continual settings utilize policies compositionally by treating already learned
    policies as primitives. Such methods either feed these primitives to the discrete
    optimization problems for selection mechanisms or to continuous optimization settings
    involving ensembling (?) and distillation (?). Modularity in such settings manifests
    itself by construction and is a central factor in building solutions. Even though
    the final policy in such paradigms, obtained through ensembling, selection, and/or
    distillation, can be monolithic, the method of obtaining such policies is a purely
    distributed regime.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 持续设置通过将已学习的策略视为原语来利用策略的组合。这些方法要么将这些原语输入到离散优化问题中以进行选择机制，要么输入到涉及集成 (?) 和蒸馏 (?)
    的连续优化设置中。在这种设置中，模块化通过构建体现出来，并且是构建解决方案的核心因素。尽管在这种范式中，通过集成、选择和/或蒸馏获得的最终策略可以是整体的，但获得这些策略的方法是纯粹的分布式体制。
- en: 5 Patterns of Incorporating Structure
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种结构整合模式
- en: 'Having defined different forms of decomposability and the different objectives
    that side information can be used to accomplish, we now connect the two by understanding
    the ways of incorporating structure into a learning process. We assume that some
    form of structure exists in the problem and/or the solution space, which can be
    incorporated into the learning pipeline as an inductive bias. To understand how
    decomposability can be incorporated into the RL pipeline, we survey the literature
    with a very specific question in mind: *Do existing methods use structure in a
    repeatable manner?* The answer to this question, inspired by the categorization
    of ? (?) for the supervised learning case, brings us to *patterns of incorporating
    structure*.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了不同形式的可分解性和侧信息可以实现的不同目标之后，我们现在通过理解将结构纳入学习过程的方式来连接这两者。我们假设问题和/或解决方案空间中存在某种形式的结构，这些结构可以作为归纳偏置纳入学习流程。为了了解如何将可分解性纳入
    RL 流程，我们在考虑一个非常具体的问题时调查了文献：*现有方法是否以可重复的方式使用结构？* 这个问题的答案，受监督学习案例的分类的启发，带我们进入了*结构整合模式*。
- en: '![Refer to caption](img/94b1937ff46608a3d9903621bc0847cd.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94b1937ff46608a3d9903621bc0847cd.png)'
- en: (a) Abstraction
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 抽象
- en: '![Refer to caption](img/992ca57174930418188dbd13c9232fc8.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/992ca57174930418188dbd13c9232fc8.png)'
- en: (b) Augmentation
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 增强
- en: '![Refer to caption](img/10d6b3519e55a3df6a51b5d26134b480.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10d6b3519e55a3df6a51b5d26134b480.png)'
- en: (c) Aux. Optimization
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 辅助优化
- en: '![Refer to caption](img/1dae4187bca79cdc47629da9f338b82a.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1dae4187bca79cdc47629da9f338b82a.png)'
- en: (d) Aux. Model
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 辅助模型
- en: '![Refer to caption](img/daae5f668350bc37e3fdf580e1e37825.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/daae5f668350bc37e3fdf580e1e37825.png)'
- en: (e) Warehouse
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 仓库
- en: '![Refer to caption](img/f02b3d10bd9ed3e30ba2d4caaaa3add5.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f02b3d10bd9ed3e30ba2d4caaaa3add5.png)'
- en: (f) Environment Gen.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 环境生成
- en: '![Refer to caption](img/4d62173c04948b6d31402994c52ba5cf.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d62173c04948b6d31402994c52ba5cf.png)'
- en: (g) Explicitly Designed
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 明确设计
- en: 'Figure 4: Patterns of incorporating structural information. We categorize the
    methods of incorporating structure as inductive biases into the learning pipeline
    into patterns that can be applied for different kinds of usages.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：结构信息整合的模式。我们将将结构作为归纳偏置纳入学习流程的方法分为几种可以应用于不同用法的模式。
- en: 'A pattern is a principled change in the RL pipeline $\Omega$ that allows the
    pipeline to achieve one, or a combination of, the additional objectives: *Sample
    Efficiency*, *Generalization*, *Safety*, and *Interpretability*. We categorize
    the literature into seven patterns, an overview of which has been shown in [Figure 4](#S5.F4
    "In 5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems"). Our proposed patterns come from our literature survey
    and are meant to provide an initial direction for such categorization. We do not
    consider this list exhaustive but more as a starting point to build further upon.
    We present an overview of our meta-analysis on the patterns used for which of
    the four use cases in [Figure 5](#S5.F5 "In 5 Patterns of Incorporating Structure
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 模式是 RL 流程 $\Omega$ 中的原则性变化，使得流程能够实现一个或多个附加目标：*样本效率*、*泛化*、*安全性* 和 *可解释性*。我们将文献分为七种模式，其中的概述已显示在[图
    4](#S5.F4 "在 5 种结构融入模式中 ‣ 强化学习中的结构：调查与开放问题")中。我们提出的模式来自我们的文献调查，旨在为这种分类提供初步方向。我们不认为这份列表是详尽的，而是作为进一步构建的起点。我们展示了我们对[图
    5](#S5.F5 "在 5 种结构融入模式中 ‣ 强化学习中的结构：调查与开放问题")中四个使用案例的模式的元分析概述。
- en: '![Refer to caption](img/fba402d3757bcbf5be33b63280ad48e5.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fba402d3757bcbf5be33b63280ad48e5.png)'
- en: 'Figure 5: Proclivities. A meta-analysis of the proclivities of each pattern
    to the additional objectives. On the x-axis are the patterns discussed in this
    text, while on the y-axis are the percentage of publications for each additional
    objective that address it using a particular pattern.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：倾向。对每种模式在附加目标上的倾向的元分析。x 轴是本文讨论的模式，而 y 轴是每个附加目标中使用特定模式解决问题的出版物百分比。
- en: In the following subsections, we delve deeper into each pattern, explaining
    different lines of literature that apply each pattern for different use cases.
    To further provide intuition about this categorization, we will consider the running
    example of a taxi service, where the task of the RL agent (the taxi) is to pick
    up passengers from various locations and drop them at their desired destinations
    within a city grid. The agent receives a positive reward when a passenger is successfully
    dropped off at their destination and incurs a small penalty for each time step
    to encourage efficiency.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将深入探讨每种模式，解释不同的文献线如何将每种模式应用于不同的使用案例。为了进一步提供对这种分类的直观理解，我们将考虑一个出租车服务的运行示例，其中
    RL 代理（出租车）的任务是从城市网格中的各种地点接载乘客并将其送到他们期望的目的地。当乘客成功被送到目的地时，代理会获得积极的奖励，并且每个时间步都会产生小的惩罚以鼓励效率。
- en: 'For each of the following sections, we present a table of the surveyed methods
    that categorizes the work in the following manner: (i) The structured space, information
    about which is incorporated as side information; (ii) The type of decomposition
    exhibited for that structured space. We specifically categorize works that use
    structured task distributions through goals and/or rewards; (iii) The additional
    objectives for which the decomposition is utilized. In addition to demonstrating
    our categorization, our rationale behind the table format is to highlight the
    areas where further research might be lucrative. These are the spots in the tables
    where we could not yet find literature, and/or we believe additional work can
    be important.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下每个部分，我们呈现了一个调查方法的表格，按照以下方式对工作进行分类：(i) 结构化空间的相关信息作为附加信息；(ii) 该结构化空间的分解类型。我们特别分类了通过目标和/或奖励使用结构化任务分布的工作；(iii)
    分解被利用的附加目标。除了展示我们的分类外，我们表格格式的理由是突出进一步研究可能有利的领域。这些是表格中我们尚未找到文献的地方，和/或我们认为额外工作可能很重要的地方。
- en: 5.1 Abstraction Pattern
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 抽象模式
- en: '![[Uncaptioned image]](img/2531d347621afdf37764a85c6bb7a88f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/2531d347621afdf37764a85c6bb7a88f.png)'
- en: Abstraction pattern utilizes structural information to create abstract entities
    in the RL pipeline. For any entity, $X$, an abstraction utilizes the structural
    information to create $X_{abs}$, which takes over the role of $X$ in the learning
    procedure. In the taxi example, the state space can be abstracted to the current
    grid cell of the taxi, the destination grid cell of the current passenger, and
    whether the taxi is currently carrying a passenger. This significantly simplifies
    the state space compared to representing the full details of the city grid. The
    action space could also be abstracted to moving in the four cardinal directions,
    plus picking up and dropping off a passenger. Finding appropriate abstractions
    can be a challenging task in itself. Too much abstraction can lead to loss of
    critical information, while too little might not significantly reduce complexity (?).
    Consequently, learning-based methods that jointly learn abstractions factor this
    granularity into the learning process.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象模式利用结构信息在RL流程中创建抽象实体。对于任何实体$X$，一种抽象利用结构信息创建$X_{abs}$，它在学习过程中取代$X$的角色。在出租车示例中，状态空间可以被抽象为出租车的当前网格单元、当前乘客的目的地网格单元以及出租车是否正在载客。这显著简化了状态空间，相比于表示城市网格的全部细节。动作空间也可以被抽象为在四个主要方向上移动，加上接送乘客。找到适当的抽象本身就是一个具有挑战性的任务。过多的抽象可能导致关键信息丧失，而过少的抽象可能无法显著降低复杂性（？）。因此，基于学习的方法将这种粒度因素纳入学习过程。
- en: 'Abstractions have been thoroughly explored in the literature, with early work
    addressing a formal theory on state abstractions (?). Recent works have primarily
    used abstractions for tackling generalization. Thus, we see in [Figure 5](#S5.F5
    "In 5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems") that generalization is the most explored use case
    for abstractions. However, the aforementioned advantages of abstraction usually
    interleave these approaches with sample efficiency gains and safety as well. Given
    the widespread use of abstractions in the literature, we explore how different
    forms of abstractions impact each use case in the following paragraphs.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '抽象在文献中得到了彻底探讨，早期工作涉及了状态抽象的形式理论（？）。最近的工作主要使用抽象来处理泛化问题。因此，在[图5](#S5.F5 "In 5
    Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning: A Survey
    and Open Problems")中，我们看到泛化是抽象最常用的案例。然而，前述的抽象优势通常与样本效率和安全性交织在一起。鉴于文献中抽象的广泛使用，我们将在接下来的段落中探讨不同形式的抽象如何影响每个用例。'
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Goals | Latent | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 潜在 | ? (?) | ? (?), ? (?) |  |  |'
- en: '| Relational |  |  | ? (?) |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 |  |  | ? (?) |  |'
- en: '| Modular | ? (?) | ? (?) | ? (?), ? (?) |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?) | ? (?), ? (?) |  |'
- en: '| States | Latent | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?)
    | ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?) | ? (?), ? (?) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?),
    ? (?), ? (?), ? (?), ? (?) | ? (?) | ? (?), ? (?) |'
- en: '| Factored | ? (?) | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?) |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?) |  |'
- en: '| Relational | ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?),
    ? (?), ? (?), ? (?) | ? (?), ? (?) |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?),
    ? (?), ? (?) | ? (?), ? (?) |  |'
- en: '| Modular | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?)
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) |  |'
- en: '| Actions | Latent | ? (?), ? (?) |  |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 潜在 | ? (?), ? (?) |  |  |  |'
- en: '| Factored |  | ? (?) | ? (?) |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 分解 |  | ? (?) | ? (?) |  |'
- en: '| Relational | ? (?) | ? (?) |  |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?) | ? (?) |  |  |'
- en: '| Modular | ? (?) | ? (?), ? (?) | ? (?) |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?), ? (?) | ? (?) |  |'
- en: '| Rewards | Latent |  | ? (?), ? (?), ? (?), ? (?) |  |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 潜在 |  | ? (?), ? (?), ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?) | ? (?), ? (?), ? (?), | ? (?) | ? (?) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) | ? (?), ? (?), ? (?) | ? (?) | ? (?) |'
- en: '| Dynamics | Latent | ? (?) | ? (?), ? (?), ? (?), ? (?) |  |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 | ? (?) | ? (?), ? (?), ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?) | ? (?) |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) | ? (?) |  |  |'
- en: '| Modular | ? (?) | ? (?) |  |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?) |  |  |'
- en: Generalization.
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 泛化。
- en: State abstractions are a standard choice for improving generalization performance
    by capturing shared dynamics across MDPs into abstract state spaces using methods
    such as Invariant Causal Prediction (?, ?), similarity metrics (?, ?, ?, ?, ?, ?, ?),
    Free Energy Minimization (?), and disentanglement (?, ?).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 状态抽象是通过使用如不变因果预测、相似性度量、自由能最小化和解缠结等方法，将MDP中的共享动态捕捉到抽象状态空间中，从而提高泛化性能的标准选择。
- en: Value functions can serve as temporal abstractions for shared dynamics in Multi-task
    Settings. Successor Features (SF) (?, ?) exploit latent reward and dynamic decompositions
    by using value functions as an abstraction. Subsequent works have combined them
    with Generalized Policy Iteration (?) and Universal Value Function Approximators (?, ?).
    Factorization in value functions, other the other hand, can help improve sample
    efficiency and generalization both (?, ?).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务设置中，价值函数可以作为共享动态的时间抽象。后继特征（SF）通过使用价值函数作为抽象，利用潜在奖励和动态分解。后续工作将其与广义策略迭代和通用价值函数逼近器相结合。另一方面，价值函数的因式分解可以提高样本效率和泛化能力。
- en: Relational abstractions contribute to generalization by incorporating symbolic
    spaces into the RL pipeline. These can help incorporate planning approaches in
    hierarchical frameworks (?, ?). Additionally, relational abstractions can help
    abstract away general aspects of a collection of MDPs, thus allowing methods to
    learn generalizable Q-values over abstract states and actions that can be transferred
    to new tasks (?) or develop methods specifically for graph-structured spaces (?, ?).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽象通过将符号空间纳入RL流程来促进泛化。这些可以帮助将规划方法纳入层次框架中。此外，关系抽象可以帮助抽象出一组MDP的通用方面，从而允许方法在抽象状态和动作上学习可泛化的Q值，这些Q值可以转移到新任务中，或为图结构空间专门开发方法。
- en: Abstractions can additionally enable generalization in hierarchical settings
    by compressing state spaces (?), abstract automata (?, ?), templates of dynamics
    across tasks (?), or even be combined with options to preserve optimal values (?).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象还可以通过压缩状态空间、抽象自动机、任务间动态模板，或甚至与选项结合以保留最佳值，在层次设置中实现泛化。
- en: Sample Efficiency.
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 样本效率。
- en: Latent variable models improve sample efficiency across the RL pipeline. Latent
    state abstractions can improve sample efficiency in Model-based RL (?) and also
    help improve the tractability of policy learning over options in HRL (?). In model-free
    tasks, these can also be learned as inverse models for visual features (?) or
    control in a latent space (?). Latent transition models demonstrate efficiency
    gains by capturing task-relevant information in noisy settings (?), by preserving
    bisimulation distances between original states (?), or by utilizing factorized
    abstractions (?). Learned latent abstractions (?) can also contribute to the exploration
    mechanism in the Go-Explore regime (?).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在变量模型提高了RL流程中的样本效率。潜在状态抽象可以提高基于模型的RL中的样本效率，并有助于提高在HRL中选项上的策略学习的可处理性。在无模型任务中，这些也可以作为视觉特征或潜在空间中的控制的逆模型进行学习。潜在转移模型通过在噪声环境中捕捉任务相关信息、保持原始状态之间的双模拟距离或利用因式分解抽象来展示效率提升。学习到的潜在抽象也可以有助于Go-Explore机制中的探索。
- en: Latent action models can expedite convergence of policy gradient methods such
    as REINFORCE (?)) by shortening the learning horizon in stochastic scenarios like
    dialog generation ? (?). Action embeddings, on the other hand, can help reduce
    the dimensionality of large action spaces ? (?)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在动作模型可以通过缩短在随机场景（如对话生成）中的学习范围，加快策略梯度方法（如REINFORCE）的收敛速度。另一方面，动作嵌入可以帮助减少大型动作空间的维度。
- en: Safety and Interpretability.
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全性和可解释性。
- en: Relational abstractions are a very good choice for interpretability since they
    capture interactionally complex decompositions. The combination of object-centric
    representations and learned abstractions can help add transparency (?) while symbolic
    interjections, such as tracking the relational distance between objects, can help
    improve performance  (?).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽象是实现可解释性的非常好的选择，因为它们捕捉了交互复杂的分解。对象中心表示与学习到的抽象的结合可以帮助增加透明度，同时，诸如跟踪对象之间关系距离的符号插入可以帮助提高性能。
- en: State and rewards abstractions can help with safety. Latent states can help
    to learn safe causal inference models by embedding confounders (?) On the other
    hand, meshes (?, ?) help benchmark metrics such as robustness in a learned policy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 状态和奖励的抽象可以帮助提升安全性。潜在状态可以通过嵌入混杂因素来帮助学习安全的因果推断模型。另一方面，网格（?, ?）可以帮助基准测试学习策略中的鲁棒性等指标。
- en: 5.2 Augmentation Pattern
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 扩展模式
- en: '![[Uncaptioned image]](img/bdb5799491df297ffdd8623d2326f85c.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/bdb5799491df297ffdd8623d2326f85c.png)'
- en: The augmentation pattern treats $X$ and $z$ as separate input entities for the
    action-selection mechanism. The combination can range from the simple concatenation
    of structural information to the state or actions to more involved methods of
    conditioning policy or value functions on additional information. Crucially, the
    structural information neither directly influences the optimization procedure
    nor changes the nature of $X$. It simply augments the already existing entities.
    In this view, abstractions that are learned in an auxiliary manner and concatenated
    to states, actions, or models can also be considered augmentations since the original
    entity remains unchanged.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展模式将 $X$ 和 $z$ 视为动作选择机制的独立输入实体。组合可以从简单的结构信息与状态或动作的串联，到更复杂的根据附加信息调整策略或价值函数的方法。关键是，结构信息既不会直接影响优化过程，也不会改变
    $X$ 的性质。它仅仅是对已存在实体的扩展。从这个角度看，以辅助方式学习并串联到状态、动作或模型的抽象也可以被视为扩展，因为原始实体保持不变。
- en: 'For the taxi example, one way to apply the augmentation pattern would be by
    conditioning the policy on additional information, such as the time of day or
    day of the week. This information could be useful because traffic conditions and
    passenger demands can vary depending on these factors. However, augmentations
    can increase the complexity of the policy, and care needs to be taken to ensure
    that the policy does not overfit the additional information. Due to this, this
    pattern is generally not explored to its fullest extent. While we see usages of
    augmentations equitably for most use cases in [Figure 5](#S5.F5 "In 5 Patterns
    of Incorporating Structure ‣ Structure in Reinforcement Learning: A Survey and
    Open Problems"), the number of papers utilizing this pattern still falls short
    compared to more established techniques, such as abstraction. In the next paragraphs,
    we delineate three kinds of augmentations in the surveyed work.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于出租车示例，一种应用扩展模式的方法是根据附加信息（如时间或星期几）来调整策略。这些信息可能有用，因为交通状况和乘客需求可能会因这些因素而有所不同。然而，扩展可能会增加策略的复杂性，需要谨慎以确保策略不会过拟合附加信息。因此，这种模式通常没有被充分探讨。尽管我们在[图5](#S5.F5
    "在 5 种结构融入模式中 ‣ 强化学习中的结构：调查与开放问题")中看到扩展的应用在大多数用例中是公平的，但使用这种模式的论文数量仍然不如像抽象等更成熟的技术。在接下来的段落中，我们将阐述调查中提到的三种扩展。
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全性 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Goals | Latent |  | ? (?), ? (?) |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 潜在 |  | ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?) | ? (?) |  |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) | ? (?) |  |  |'
- en: '| Relational | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?) | ? (?), ? (?) |  |  |'
- en: '| Modular | ? (?), ? (?) | ? (?), ? (?) | ? (?) |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?) | ? (?), ? (?) | ? (?) |  |'
- en: '| States | Latent | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?) |  |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?) |  |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) |  |  |  |'
- en: '| Relational | ? (?) | ? (?) |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?) | ? (?) |  |  |'
- en: '| Modular |  |  |  |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 |  |  |  |  |'
- en: '| Actions | Latent | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 潜在 | ? (?) | ? (?), ? (?) |  |  |'
- en: '| Relational |  | ? (?) |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 |  | ? (?) |  |  |'
- en: '| Modular | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?), ? (?) |  |  |'
- en: '| Rewards | Factored | ? (?) | ? (?) |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 分解 | ? (?) | ? (?) |  |  |'
- en: '| Dynamics | Latent | ? (?) | ? (?), ? (?), ? (?) |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 | ? (?) | ? (?), ? (?), ? (?) |  |  |'
- en: '| Factored |  | ? (?) |  |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 分解 |  | ? (?) |  |  |'
- en: '| Policies | Modular | ? (?), ? (?), ? (?) | ? (?) | ? (?) |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 模块化 | ? (?), ? (?), ? (?) | ? (?) | ? (?) |  |'
- en: Context-based Augmentations.
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于上下文的扩展。
- en: Contextual representations of dynamics (?, ?) and goal-related information (?, ?)
    can help with generalization and sample efficiency by exposing the agent to the
    necessary information for optimality. Goal augmentations additionally allow interpretable
    mechanisms for specifying goals (?). On the other hand, augmentation of meta-learned
    latent spaces to the normal state can promote temporally coherent exploration
    across tasks (?). Action histories (?) can directly help with sample efficiency,
    and action relations (?, ?) contribute to generalization over large action sets.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 动态的上下文表示(?, ?)和与目标相关的信息(?, ?)可以通过向代理暴露必要的最优信息来帮助泛化和样本效率。目标增强还允许对目标(?)的可解释机制进行指定。另一方面，元学习潜在空间的增强到正常状态可以促进任务之间的时间一致探索(?).
    行动历史(?)可以直接帮助样本效率，而行动关系(?, ?)则有助于在大型行动集上进行泛化。
- en: Language Augmentations.
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言增强。
- en: Language can explicitly capture relational metadata in the world. Latent language
    interpretation models (?) can utilize the compositionality of language to achieve
    better exploration and generalization to different relational settings, as represented
    by their language descriptions. On the other hand, goal descriptions (?) can help
    hierarchical settings by exploiting semantic relationships between different subtasks
    and producing better goals for lower-level policies. Augmentations can additionally
    help make existing methods more interpretable through methods such as ? (?) by
    guiding search over approximate policies written in human-readable formats.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 语言可以明确地捕捉世界中的关系元数据。潜在的语言解释模型(?)可以利用语言的组合性来实现更好的探索和对不同关系设置的泛化，如其语言描述所示。另一方面，目标描述(?)可以通过利用不同子任务之间的语义关系并为低层策略生成更好的目标，来帮助层级设置。增强还可以通过如?
    (?)等方法使现有方法更具可解释性，指导对以人类可读格式编写的近似策略的搜索。
- en: Control Augmentations.
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制增强。
- en: Augmentations can additionally help with primitive control, such as multi-level
    control in hierarchical settings. Augmenting internal latent variables conditioned
    on primitive skills (?, ?, ?) can help tackle sample efficiency in hierarchical
    settings. Augmentations can also help morphological control (?) through methods
    such as ? (?) that model the different limbs as individual agents that need to
    learn to join together into a morphology to solve a task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 增强还可以帮助原始控制，例如层级设置中的多级控制。基于原始技能(?, ?, ?) 条件的内部潜在变量的增强可以帮助解决层级设置中的样本效率问题。增强还可以通过如?
    (?)等方法帮助形态控制，这些方法将不同的肢体建模为需要学会结合在一起解决任务的单独代理。
- en: 5.3 Auxiliary Optimization Pattern
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 辅助优化模式
- en: This pattern uses structural side information to modify the optimization procedure.
    This includes methods involving contrastive losses, reward shaping, concurrent
    optimization, masking strategies, regularization, baselining, etc. However, given
    that the changes in the optimization can go hand-in-hand with modifications of
    other components, this pattern shares methods with many other patterns. For example,
    contrastive losses can be used to learn state abstractions. Similarly, a learned
    model can be utilized for reward shaping as well. Thus, methods that fall into
    this category simultaneously utilize both patterns.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式使用结构侧信息来修改优化过程。这包括涉及对比损失、奖励塑造、并行优化、掩蔽策略、正则化、基线等的方法。然而，由于优化中的变化可能与其他组件的修改同步进行，因此这个模式与许多其他模式共享方法。例如，对比损失可以用来学习状态抽象。同样，学习到的模型也可以用于奖励塑造。因此，属于这一类别的方法同时利用这两种模式。
- en: '![[Uncaptioned image]](img/9e72ee1dc309034fab511d6985c6523e.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/9e72ee1dc309034fab511d6985c6523e.png)'
- en: 'In the case of the taxi, reward shaping could help the policy to be reused
    for slight perturbances in the city grid, where the shaped reward encourages the
    taxi to stay near areas where passengers are frequently found when it does not
    have a passenger. It is crucial to ensure that the modified optimization process
    remains aligned with the original objective, i.e., there needs to exist some form
    of regularization that controls how the modification of the optimization procedure
    respects the original objective. For reward shaping techniques, this amounts to
    the invariance of the optimal policy under the shaped reward (?). For auxiliary
    objectives, this manifests in some form of entropy (?) or divergence regularization (?).
    Constraints ensure this through recursion (?), while baselines control the variance
    of updates (?). The strongest use of constraints is in the safety literature,
    where constraints either help control the updates using some safety criterion
    or constrain the exploration. Consequently, in [Figure 5](#S5.F5 "In 5 Patterns
    of Incorporating Structure ‣ Structure in Reinforcement Learning: A Survey and
    Open Problems"), we see that the auxiliary optimization pattern peaks in its proclivity
    towards addressing safety. In the following paragraphs, we cover methods that
    optimize individual aspects of the optimization procedure, namely, rewards, learning
    objectives, constraints, and parallel optimization.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在出租车的案例中，奖励塑造可以帮助策略在城市网格中的轻微扰动下重复使用，其中塑造的奖励鼓励出租车在没有乘客时待在乘客经常出现的区域。确保修改后的优化过程与原始目标保持一致至关重要，即需要存在某种形式的正则化来控制优化过程的修改如何尊重原始目标。对于奖励塑造技术，这相当于在塑造的奖励下保持最优策略的不变性 (?)。对于辅助目标，这表现为某种形式的熵 (?)
    或发散正则化 (?)。约束通过递归 (?) 确保这一点，而基线控制更新的方差 (?)。约束的最强用法是在安全文献中，其中约束要么通过某种安全标准帮助控制更新，要么约束探索。因此，在[图 5](#S5.F5
    "在5种结构纳入模式中 ‣ 强化学习中的结构：调查与开放问题")中，我们看到辅助优化模式在解决安全问题上的倾向达到顶峰。在接下来的段落中，我们将讨论优化优化过程的个别方面的方法，即奖励、学习目标、约束和并行优化。
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Goals | Latent |  | ? (?) |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 潜在 |  | ? (?) |  |  |'
- en: '| Relational |  | ? (?) |  |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 |  | ? (?) |  |  |'
- en: '| Factored |  |  | ? (?) |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 |  |  | ? (?) |  |'
- en: '|  | Modular | ? (?), ? (?), ? (?), ? (?) |  |  |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | 模块化 | ? (?), ? (?), ? (?), ? (?) |  |  |  |'
- en: '| States | Latent | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?),
    ? (?) |  | ? (?) | ? (?), ? (?) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) |  |
    ? (?) | ? (?), ? (?) |'
- en: '| Factored | ? (?), ? (?), ? (?), ? (?) |  |  | ? (?) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 | ? (?), ? (?), ? (?), ? (?) |  |  | ? (?) |'
- en: '| Relational | ? (?) |  |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?) |  |  |  |'
- en: '| Modular | ? (?), ? (?) |  | ? (?) |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?) |  | ? (?) |  |'
- en: '| Actions | Latent | ? (?), ? (?), ? (?), ? (?) | ? (?) | ? (?) | ? (?), ? (?),
    ? (?) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 潜在 | ? (?), ? (?), ? (?), ? (?) | ? (?) | ? (?) | ? (?), ? (?), ? (?)
    |'
- en: '| Factored | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) |  |  |  |'
- en: '| Modular | ? (?), ? (?) |  | ? (?) | ? (?) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?) |  | ? (?) | ? (?) |'
- en: '| Rewards | Factored | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?) |  |
    ? (?), ? (?) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 分解型 | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?) |  | ? (?), ? (?)
    |'
- en: '| Dynamics | Latent | ? (?), ? (?) | ? (?) |  |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 | ? (?), ? (?) | ? (?) |  |  |'
- en: '| Factored | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 | ? (?) | ? (?), ? (?) |  |  |'
- en: '| Relational | ? (?), ? (?) |  |  |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?), ? (?) |  |  |  |'
- en: '| Policy Space | Latent | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 策略空间 | 潜在 | ? (?) | ? (?), ? (?) |  |  |'
- en: Reward Modification.
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励修改。
- en: Reward shaping is a common way to incorporate additional information into the
    optimization procedure. Methods can gain sample efficiency by exploiting modular
    and relational decompositions through task descriptions (?), or goal information
    from a higher level policy with off-policy modification to the lower level transitions (?).
    Histories of rewards (?) can help learn symmetric relationships between states
    and, thus, improve the selection procedure for states in a mini-batch for optimization.
    Factorization of states and rewards into endogenous and exogenous factors (?),
    on the other hand, helps with safety and sample efficiency through reward corrections.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励塑造是将额外信息纳入优化过程的常用方式。通过任务描述(?)利用模块化和关系分解，或从更高层次策略获得的目标信息与对较低层次过渡的离策略修改（?），方法可以提高样本效率。奖励历史(?)可以帮助学习状态之间的对称关系，从而改善优化中迷你批次状态的选择过程。另一方面，将状态和奖励分解为内生和外生因素(?)有助于通过奖励修正提高安全性和样本效率。
- en: Extrinsic Rewards can also be used to guide the exploration process. Symbolic
    planning with relational representations can be used to interact with a primitive
    learning policy through extrinsic rewards in hierarchical settings, thus, adding
    interpretability while directly impacting the exploration through the extrinsic
    reward (?). Alternatively, additional reward sources can help determine the quality
    of counterfactual trajectories, which can help explain why an agent took certain
    kinds of actions (?). Additionally, running averages can rewards be used to adaptively
    tune exploration parameters for heterogeneous action spaces (?)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 外部奖励也可以用于指导探索过程。符号规划与关系表示可以用于通过外部奖励与原始学习策略进行交互，从而在层次化设置中增加可解释性，同时通过外部奖励直接影响探索(?).
    另一种方法是，额外的奖励来源可以帮助确定反事实轨迹的质量，这有助于解释代理为何采取某些行动(?). 此外，运行平均奖励可以用于自适应调整异质动作空间的探索参数(?).
- en: On the other hand, intrinsic rewards can specifically help with exploration
    in sparse reward environments. Latent decompositions can help improve such methods
    by directly impacting the exploration. Language abstractions can serve as latent
    decompositions that can be separately used for exploration (?). Alternatively,
    geometric structures can provide a way to compare state embeddings and provide
    episodic bonuses (?).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，内在奖励可以特别帮助在稀疏奖励环境中进行探索。潜在分解可以通过直接影响探索来改善这种方法。语言抽象可以作为潜在分解，单独用于探索(?)。另外，几何结构可以提供比较状态嵌入并提供情节奖励(?)的方法。
- en: Auxiliary Learning Objectives.
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 辅助学习目标。
- en: Skill-based methods transfer skills between agents with different morphology
    by learning invariant subspaces and using those to create a transfer auxiliary
    objective (through a reward signal) (?), or an entropy-based term for policy regularization (?).
    In hierarchical settings, discovering appropriate sub-tasks (?) can be a highly
    sample-inefficient process. ? (?) tackle this by composing values of the sub-trajectories
    under the current policy, which they subsequently use for behavior cloning. Latent
    decompositions can help with robustness and safety when used for some form of
    policy regularization (?). Auxiliary losses, which usually help with generalization,
    can also be a very good entry point for human-like inductive biases (?). Metrics
    inspired by the geometry of latent decompositions can help learn optimal values
    in multi-task settings (?).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 基于技能的方法通过学习不变子空间并利用这些子空间创建转移辅助目标（通过奖励信号）(?)，或用于策略正则化的基于熵的项（?），在形态不同的代理之间转移技能。在层次化设置中，发现适当的子任务(?)可能是一个高度样本低效的过程。?
    (?) 通过在当前策略下组合子轨迹的值来解决这个问题，然后利用这些值进行行为克隆。潜在分解在用于某种形式的策略正则化(?)时可以帮助提高鲁棒性和安全性。辅助损失通常有助于泛化，也可以是引入类人归纳偏见(?)的一个很好的切入点。受到潜在分解几何学启发的度量可以帮助在多任务设置中学习最优值(?).
- en: Constraints and Baselines.
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束与基线。
- en: Constrained optimization is commonplace in Safe RL, and incorporating structure
    can help improve the sample efficiency of such methods while making them more
    interpretable. Factorization of states into safe and unsafe states can help develop
    persistent safety conditions (?), or language abstractions (?). Recursive constraints (?)
    can help explicitly condition the optimization on a latent subset of safe actions
    using factored states. Restricting the exploration of options to non-risky states
    can help incorporate safety in hierarchical settings as well (?). Factorized actions
    can also help improve the sample efficiency of policy gradient methods through
    baselining (?, ?), offline methods through direct value conditioning (?), and
    value-based planning through matrix estimation (?)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 约束优化在安全强化学习中很常见，融入结构可以帮助提高这些方法的样本效率，同时使其更具可解释性。将状态因子化为安全状态和不安全状态可以帮助开发持久的安全条件（？），或语言抽象（？）。递归约束（？）可以通过因子状态明确条件化优化，专注于潜在的安全动作子集。将选项的探索限制在非风险状态中，也可以帮助在层次设置中融入安全性（？）。因子化的动作还可以通过基准化（？，？）来提高策略梯度方法的样本效率，通过直接值条件（？）提高离线方法的样本效率，并通过矩阵估计（？）提高基于值的规划效率。
- en: Methods can also directly incorporate expert domain knowledge directly in the
    action selection mechanism for safety and interpretability (?), or for directed
    exploration to improve sample efficiency (?). Hierarchical settings can benefit
    from latent state decompositions incorporated via modification of the termination
    condition (?). Additionally, state-action equivalences can help scale Q-learning
    to large spaces through factorization (?).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 方法也可以直接将专家领域知识纳入动作选择机制，以提高安全性和可解释性（？），或用于定向探索以提高样本效率（？）。层次设置可以通过修改终止条件来受益于潜在状态分解（？）。此外，状态-动作等价性可以通过因子化帮助将Q学习扩展到大空间（？）。
- en: Concurrent Optimization.
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并发优化。
- en: Parallelizing optimization using structural decompositions can specifically
    help with sample efficiency. Factored MDPs are a very good way to model factors
    that influence the content presented to users and can be used for ensembling methods
    in a parallel regime (?). Similarly, factored rewards in hierarchical settings
    can help decompose Multi-task problems into a linear combination of individual
    task MDPs (?). Alternatively, discretizing continuous sub-actions in multi-dimensional
    action spaces can help extend the MDP for each sub-action to an undiscounted lower-level
    MDP, modifying the backup for the Q values using decompositions (?). Relational
    decompositions can additionally help with masking strategies for Factored Neural
    Networks (?).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构分解来并行优化可以特别帮助提高样本效率。因子化MDP（马尔可夫决策过程）是建模影响用户内容呈现的因素的一个非常好的方法，并且可以用于并行方法中的集成（？）。类似地，分解的奖励在层次设置中可以帮助将多任务问题分解为个别任务MDP的线性组合（？）。或者，在多维动作空间中离散化连续子动作可以帮助将每个子动作的MDP扩展为非折扣的低层次MDP，通过分解来修改Q值的备份（？）。关系分解还可以帮助为因子化神经网络提供掩蔽策略（？）。
- en: 5.4 Auxiliary Model Pattern
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 辅助模型模式
- en: '![[Uncaptioned image]](img/d8c2f7605a30170a927e830b99f065ec.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图片]](img/d8c2f7605a30170a927e830b99f065ec.png)'
- en: This pattern represents using the structural information in a model. In using
    the term model, we specifically refer to methods that utilize a model of the world
    to generate experiences, either fully or partially. This notion allows us to capture
    a range of methods, from ones using full-scale world models to generate rewards
    and next-state transitions to ones that use these methods to generate full experience
    sequences. In our categorization, we specifically look at how the structure is
    incorporated into such models to help generate some parts of learning experiences.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这一模式代表了使用模型中的结构信息。在使用“模型”这一术语时，我们特别指的是那些利用世界模型生成体验的方法，无论是完全还是部分。这一概念使我们能够涵盖从使用全尺度世界模型生成奖励和下一状态转移的方法，到使用这些方法生成完整体验序列的方法。在我们的分类中，我们特别关注结构是如何融入这些模型中的，以帮助生成学习体验的某些部分。
- en: 'Our taxi agent could learn a latent model of city traffic based on past experiences.
    This model could be used to plan routes that avoid traffic and hence reach destinations
    faster. Alternatively, the agent could learn an ensembling technique to combine
    multiple models, each of which model-specific components of the traffic dynamics.
    With models, there is usually a trade-off between model complexity and accuracy,
    and it is essential to manage this carefully to avoid overfitting and maintain
    robustness. To this end, incorporating structure helps make the model-learning
    phase more efficient while allowing reuse for generalization. Hence, in [Figure 5](#S5.F5
    "In 5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems"), we see that the auxiliary model pattern shows a
    strong proclivity to utilizing structure for sample efficiency. In the following
    paragraphs, we explicitly discuss models that utilize decompositions and models
    used for creating decompositions.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的出租车代理可以根据过去的经验学习城市交通的潜在模型。这个模型可以用来规划避开交通的路线，从而更快地到达目的地。或者，代理可以学习一种集成技术来结合多个模型，每个模型都针对交通动态的特定组件。使用模型时，通常在模型复杂性和准确性之间存在权衡，必须仔细管理，以避免过拟合并保持鲁棒性。为此，纳入结构有助于使模型学习阶段更加高效，同时允许重用以进行泛化。因此，在
    [图 5](#S5.F5 "在 5 种结构化方式中 ‣ 强化学习中的结构：调查与开放问题") 中，我们看到辅助模型模式表现出强烈的利用结构以提高样本效率的倾向。在接下来的段落中，我们将明确讨论利用分解的模型和用于创建分解的模型。
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全性 |'
- en: '| Goals | Factored |  | ? (?) |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 分解型 |  | ? (?) |  |  |'
- en: '| Relational |  | ? (?), ? (?) |  |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 |  | ? (?), ? (?) |  |  |'
- en: '| Modular | ? (?) | ? (?) | ? (?) |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?) | ? (?) |  |'
- en: '| States | Latent | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?),
    ? (?), ? (?), ? (?), ? (?), ? (?) |  | ? (?) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?),
    ? (?), ? (?), ? (?), ? (?) |  | ? (?) |'
- en: '| Factored | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) |  |  |'
- en: '| Relational | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?),
    ? (?), ? (?), ? (?) | ? (?) |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?),
    ? (?), ? (?) | ? (?) |  |'
- en: '| Modular | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
- en: '| Actions | Latent | ? (?) | ? (?) |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 潜在 | ? (?) | ? (?) |  |  |'
- en: '| Factored | ? (?), ? (?), ? (?), ? (?) | ? (?) |  |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 | ? (?), ? (?), ? (?), ? (?) | ? (?) |  |  |'
- en: '| Relational | ? (?), ? (?) | ? (?) |  |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?), ? (?) | ? (?) |  |  |'
- en: '|  | Modular | ? (?), ? (?) | ? (?) | ? (?) |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | 模块化 | ? (?), ? (?) | ? (?) | ? (?) |  |'
- en: '| Rewards | Latent | ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?) |  |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 潜在 | ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?) |  |  |'
- en: '| Factored |  | ? (?) |  | ? (?), ? (?) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 |  | ? (?) |  | ? (?), ? (?) |'
- en: '| Dynamics | Latent | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?),
    ? (?), ? (?) | ? (?) |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?),
    ? (?) | ? (?) |  |'
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 分解型 | ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
- en: '|  | Relational | ? (?) |  |  |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | 关系型 | ? (?) |  |  |  |'
- en: '|  | Modular | ? (?), ? (?), ? (?) | ? (?) |  |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | 模块化 | ? (?), ? (?), ? (?) | ? (?) |  |  |'
- en: Models with structured representations.
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 具有结构化表示的模型。
- en: ? (?) utilize factored decomposition for state space to demonstrate the benefits
    of model-based methods in combinatorially complex environments. Similarly, the
    dreamer models (?, ?) utilize latent representations of pixel-based environments.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 利用分解型状态空间来展示基于模型的方法在组合复杂环境中的优势。同样，dreamer 模型 (?,?,?) 利用基于像素的环境的潜在表示。
- en: Object-oriented representation for states can help bypass the need to learn
    latent factors using CNNs in MBRL (?) or as random variables whose posterior can
    be refined using NNs (?). Graph (Convolutional) Networks (?) can capture rich
    higher-order interaction data, such as crowd navigation ? (?), or invariances (?).
    Action equivalences can help learn latent models (Abstract MDPs) (?) for planning
    and value iteration.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 状态的面向对象表示可以帮助绕过在 MBRL (?) 中使用 CNN 学习潜在因素的需求，或者作为可以通过 NNs (?) 改进的随机变量。图（卷积）网络 (?)
    可以捕捉丰富的高阶交互数据，如人群导航 ? (?)，或不变性 (?)。动作等效性可以帮助学习潜在模型（抽象 MDPs） (?) 以进行规划和价值迭代。
- en: Models for task-specific decompositions.
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特定任务分解的模型。
- en: Another way to utilize decompositions in models is to capture task-specific
    decompositions. Models that capture some form of relevance, such as observational
    and interventional data in Causal RL (?), or task-relevant vs. irrelevant data (?)
    can help with generalization and sample efficiency gains. Latent representations
    help models capture control-relevant information (?) or subtask dependencies (?).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 利用模型中的分解的另一种方法是捕捉任务特定的分解。捕捉某种形式的相关性，如因果强化学习中的观察性和干预性数据(?)，或任务相关与不相关数据(?)的模型可以帮助泛化和样本效率提升。潜在表示帮助模型捕捉控制相关信息(?)或子任务依赖关系(?)。
- en: Models for safety usually incorporate some measure of cost to abstract safe
    states (?), or unawareness to factor states and actions (?). Alternatively, models
    can also directly guide exploration mechanisms through latent causal decompositions (?)
    and state subspaces (?) to gain sample efficiency. Generative methods such as
    CycleGAN (?) are also very good ways to use Latent models of different components
    of an MDP to generate counterfactual trajectories (?)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 安全模型通常会包含一些成本度量，以抽象安全状态(?)，或通过潜在因果分解(?)和状态子空间(?)直接指导探索机制，从而提高样本效率。生成方法，如CycleGAN(?)，也是使用MDP不同组件的潜在模型生成反事实轨迹(?)的很好的方法。
- en: 5.5 Warehouse Pattern
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 仓储模式
- en: This pattern uses structural information to create a database of entities that
    can be combined to achieve a specific objective. These can be learned policies
    and value functions or even models. Given the online nature of such methods, they
    are often targeted toward continual and life-long learning problems. The inherent
    modularity in such methods often leads them to focus on knowledge reuse as a central
    theme.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 该模式利用结构信息创建一个可以组合以实现特定目标的实体数据库。这些可以是学习到的策略和价值函数，甚至是模型。鉴于这种方法的在线特性，它们通常针对持续和终身学习问题。这些方法固有的模块化通常使它们关注知识重用作为核心主题。
- en: The taxi from our running example could maintain a database of value functions
    or policies for different parts of the city or at different times of the day.
    These could be reused as the taxi navigates through the city, making learning
    more efficient. While warehousing generally can improve efficiency, it has primarily
    been explored through the skills and options framework for targeting generalization.
    An important consideration in warehousing is managing the warehouse’s size and
    diversity to avoid biasing the learning process too much toward past experiences.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行示例中的出租车可以维护一个关于城市不同部分或一天不同时间的价值函数或政策的数据库。这些可以在出租车穿行城市时被重用，使学习更高效。虽然仓储通常可以提高效率，但它主要通过技能和选项框架探索以针对泛化。在仓储中一个重要的考虑因素是管理仓库的大小和多样性，以避免将学习过程过度偏向过去的经验。
- en: '![[Uncaptioned image]](img/8d8ac2908ee567ea073ef1c5320e3b6e.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/8d8ac2908ee567ea073ef1c5320e3b6e.png)'
- en: 'So far, the warehousing pattern seems to be applied to sample efficiency and
    generalization. However, warehousing also overlaps with interpretability since
    the stored data can be easily used to analyze the agent’s behavior and understand
    the policy for novel scenarios. Consequently, these objectives are equitably distributed
    in [Figure 5](#S5.F5 "In 5 Patterns of Incorporating Structure ‣ Structure in
    Reinforcement Learning: A Survey and Open Problems").'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，仓储模式似乎被应用于样本效率和泛化。然而，仓储也与可解释性重叠，因为存储的数据可以很容易地用于分析代理的行为并理解新情境下的策略。因此，这些目标在[图5](#S5.F5
    "在5种结构整合模式中 ‣ 强化学习中的结构：调查与开放问题")中公平地分布。
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Goals | Factored |  | ? (?), ? (?) |  |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 分解 |  | ? (?), ? (?) |  |  |'
- en: '| Relational |  |  | ? (?) |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 关系 |  |  | ? (?) |  |'
- en: '| Modular | ? (?) | ? (?) | ? (?) |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?) | ? (?) |  |'
- en: '| States | Latent |  | ? (?), ? (?) |  |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 |  | ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?), ? (?) |  |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?), ? (?) | ? (?), ? (?), ? (?) |  |  |'
- en: '| Modular | ? (?) | ? (?), ? (?), ? (?) | ? (?) |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?) | ? (?), ? (?), ? (?) | ? (?) |  |'
- en: '| Actions | Latent |  | ? (?) |  |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 潜在 |  | ? (?) |  |  |'
- en: '| Modular | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?),
    ? (?) | ? (?) |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?)
    | ? (?) |  |'
- en: '| Rewards | Factored |  | ? (?), ? (?), ? (?), ? (?) |  |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 分解 |  | ? (?), ? (?), ? (?), ? (?) |  |  |'
- en: '| Dynamics | Latent |  | ? (?) |  |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 |  | ? (?) |  |  |'
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?) | ? (?) |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?), ? (?) | ? (?), ? (?) | ? (?) |  |'
- en: '|  | Modular | ? (?) | ? (?), ? (?), ? (?), ? (?) |  |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | 模块化 | ? (?) | ? (?), ? (?), ? (?), ? (?) |  |  |'
- en: '| Policies | Latent |  | ? (?) | ? (?) |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 潜在 |  | ? (?) | ? (?) |  |'
- en: '| Modular | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?),
    ? (?), ? (?), ? (?), ? (?) | ? (?) |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?),
    ? (?), ? (?), ? (?) | ? (?) |  |'
- en: Policy Warehousing.
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 策略仓储。
- en: Policy subspaces (?) is a relatively new concept that utilizes shared latent
    parameters in policies to learn a subspace that can be subsequently combined linearly
    to create new policies. Extending these subspaces by warehousing additional policies
    naturally extends them to continual settings (?).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 策略子空间 (?) 是一个相对较新的概念，它利用策略中的共享潜在参数学习一个子空间，这个子空间可以随后线性组合以创建新的策略。通过仓储额外策略扩展这些子空间自然地扩展到持续设置 (?).
- en: Using goals and rewards, task factorization endows warehousing policies and
    Q values in multi-task lifelong settings. The multi-task lifelong problem can
    also be treated as a relationship graph between existing tasks generated from
    latent space (?). On the other hand, methods such as ? (?) factor MDPs into agent-specific
    and task-specific degrees of variation, for which individual modules can be trained.
    Disentanglement using variational encoder-decoder models (?) can help control
    morphologically different agents by factorizing dynamics into shared and agent-specific
    factors. Additionally, methods such as ? (?) partition the agent’s problem into
    interconnected sub-agents that learn local control policies.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用目标和奖励，任务分解在多任务终身设置中赋予了策略和 Q 值仓储功能。多任务终身问题也可以被视为从潜在空间生成的现有任务之间的关系图 (?). 另一方面，方法如
    ? (?) 将 MDPs 分解为特定于代理和特定于任务的变化度，这些个别模块可以进行训练。使用变分编码器-解码器模型 (?) 的解缠绕可以通过将动态分解为共享和特定于代理的因素来帮助控制形态上不同的代理。此外，方法如
    ? (?) 将代理的问题划分为互相连接的子代理，学习局部控制策略。
- en: Methods that utilize the skills framework effectively warehouse learned primitives,
    similar to how options warehouse associated policies in HRL. These can subsequently
    be used for maximizing mutual information in lower layers (?), sketching together
    a policy (?), diversity-seeking priors in continual settings (?), or for partitioned
    states spaces (?). Similarly, ? (?) apply the warehouse pattern on a latent embedding
    space, learned using auxiliary optimization.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有效利用技能框架的方法将学到的原语进行仓储，这类似于选项在 HRL 中仓储关联策略。这些可以随后用于在较低层次上最大化互信息 (?), 组合策略 (?),
    在持续设置中寻求多样性先验 (?), 或用于分区状态空间 (?). 类似地，? (?) 将仓储模式应用于使用辅助优化学习的潜在嵌入空间。
- en: Decomposed Models.
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分解模型。
- en: Decompositions that inherently exist in models lead to approaches that often
    ensemble multiple models that individually reflect different aspects of the problem.
    Ensemble methods such as ? (?) capture the dynamics in individual modules that
    sparsely interact and use attention mechanisms (?). Ensembling dynamics can also
    help with few-shot adaptation to unseen MDPs (?). Factored models can also be
    combined with relational decompositions to bind actions to object-centric representations ? (?).
    Latent representations in hierarchical settings (?) can additionally improve the
    sample inefficiency of Deep Option Critic (?).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中固有的分解会导致采用多模型集成的方法，这些模型各自反映问题的不同方面。集成方法如 ? (?) 捕捉在个别模块中稀疏互动的动态，并使用注意机制 (?).
    集成动态也有助于在少量样本下适应未见过的 MDPs (?). 分解模型也可以与关系分解相结合，将动作绑定到以对象为中心的表示 ? (?). 在层次设置中的潜在表示 (?)
    还可以改善 Deep Option Critic 的样本低效性 (?).
- en: 5.6 Environment Generation Pattern
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 环境生成模式
- en: '![[Uncaptioned image]](img/6ce2b26e0b664f491124867ac8de0f54.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/6ce2b26e0b664f491124867ac8de0f54.png)'
- en: This pattern uses structural information to create task, goal, or dynamics distributions
    from which MDPs can be sampled. This subsumes the idea of procedurally generated
    environments while additionally incorporating methods that use auxiliary models
    inducing structure in the environment generation process. The decomposition is
    reflected in the aspects of the environment generation that are impacted by the
    generative process, such as dynamics, reward structure, state space, etc. Given
    the online nature of this pattern, methods in this pattern end up addressing curriculum
    learning in one way or another.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 该模式利用结构信息创建任务、目标或动态分布，从中可以抽样MDPs。这包括了程序生成环境的理念，同时还结合了使用辅助模型来引入环境生成过程中结构的方法。该分解体现在受生成过程影响的环境生成方面，如动态、奖励结构、状态空间等。鉴于该模式的在线特性，这一模式中的方法最终以某种方式处理课程学习。
- en: In the taxi example, a curriculum of tasks could be generated, starting with
    simple tasks (like navigating an empty grid) and gradually introducing complexity
    (like adding traffic and passengers with different destinations). Ensuring that
    the generated MDPs provide good coverage of the problem space is crucial to avoid
    overfitting to a specific subset of tasks. This necessitates additional diversity
    constraints that must be incorporated into the environment generation process.
    Structure, crucially, provides additional interpretability and controllability
    in the environment generation process, thus, making benchmarking easier than methods
    that use unsupervised techniques (?).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在出租车示例中，可以生成任务的课程，从简单任务（如在空网格中导航）开始，逐渐引入复杂性（如增加交通和不同目的地的乘客）。确保生成的MDPs能够良好覆盖问题空间对避免过拟合特定任务子集至关重要。这需要在环境生成过程中加入额外的多样性约束。结构至关重要，它为环境生成过程提供了额外的可解释性和可控性，从而使得基准测试比使用无监督技术的方法更为简单 (?).
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Goals | Relational | ? (?), ? (?) | ? (?) | ? (?) |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 关系型 | ? (?), ? (?) | ? (?) | ? (?) |  |'
- en: '| Modular | ? (?), ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?) | ? (?), ? (?) |  |  |'
- en: '| States | Latent |  | ? (?), ? (?) |  |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 |  | ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?), ? (?) | ? (?) | ? (?), ? (?) |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 因式分解 | ? (?), ? (?) | ? (?) | ? (?), ? (?) |  |'
- en: '| Relational | ? (?), ? (?) | ? (?) | ? (?) |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?), ? (?) | ? (?) | ? (?) |  |'
- en: '| Rewards | Latent |  | ? (?), ? (?) |  |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 潜在 |  | ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?) | ? (?) |  |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 因式分解 | ? (?) | ? (?) |  |  |'
- en: '| Dynamics | Latent |  | ? (?), ? (?) |  |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 |  | ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?), ? (?) | ? (?) |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 因式分解 | ? (?), ? (?) | ? (?), ? (?), ? (?) | ? (?) |  |'
- en: '| Relational | ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
- en: '| Modular | ? (?), ? (?) | ? (?) | ? (?) |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 | ? (?), ? (?) | ? (?) | ? (?) |  |'
- en: The compositional nature of learning problems can be modeled using rule-based
    grammar. ? (?) particularly utilize this to impact the transition dynamics and
    generate environments. This allows them to train agents with an implicit compositional
    curriculum. This is further used by ? (?) in their auxiliary optimization procedure.
    Another way to capture task dependencies is through latent graphical models, which
    can be used to generate the state-space, reward functions, and transition dynamics (?, ?).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 学习问题的组合性质可以使用基于规则的语法来建模。? (?)特别利用这一点来影响转移动态并生成环境。这使得它们能够用隐式组合课程训练智能体。这进一步被? (?)在其辅助优化程序中使用。另一种捕捉任务依赖性的方法是通过潜在图形模型，这可以用来生成状态空间、奖励函数和转移动态 (?, ?)。
- en: Latent dynamics models allow simulating task distributions, which can help with
    generalization (?). Clustering methods such as (?), on the other hand, explore
    task similarities by meta-learning a clustering method through an exploration
    policy. In a way, they recover a factored decomposition on the task space where
    individual clusters can be further used for policy adaptation.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在动态模型允许模拟任务分布，这有助于泛化 (?). 聚类方法如(?)，则通过探索策略的元学习来探索任务相似性。某种程度上，它们在任务空间上恢复了因式分解，其中单独的簇可以进一步用于策略调整。
- en: 5.7 Explicitly Designed
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 明确设计
- en: 'This pattern encompasses all methods where the inductive biases manifest in
    specific architectures or setups that reflect the decomposability of the problem
    that they aim to utilize. Naturally, this includes highly specific Neural architectures,
    but it also easily extends to other methods like sequential architectures to capture
    hierarchies, relations, etc. Crucially, the usage of structural information is
    limited to the specificity of the architecture and not any other part of the pipeline.
    In the case of the taxi, a neural architecture could be designed to process the
    city grid as an image and output a policy. Techniques like convolutional layers
    could be used to capture the spatial structure of the city grid. Different network
    parts could be specialized for different subtasks, like identifying passenger
    locations and planning routes. However, this pattern involves a considerable amount
    of manual tuning and experimentation, and it’s critical to ensure that these designs
    generalize well across different tasks. Designing specific neural architectures
    can provide better interpretability, enabling the ability to decompose different
    components and simulate them independently. Consequently, this pattern shows the
    highest proclivity to interpretability, with Generalization being a close second
    in [Figure 5](#S5.F5 "In 5 Patterns of Incorporating Structure ‣ Structure in
    Reinforcement Learning: A Survey and Open Problems").'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式涵盖了所有归纳偏置在特定架构或设置中显现的方法，这些架构或设置反映了它们旨在利用的问题的可分解性。这自然包括了高度特定的神经架构，但它也很容易扩展到其他方法，如顺序架构以捕捉层次结构、关系等。关键是，结构信息的使用仅限于架构的特异性，而不是管道的其他部分。在出租车的情况下，可以设计一个神经架构来将城市网格处理为图像并输出策略。可以使用卷积层等技术来捕捉城市网格的空间结构。不同的网络部分可以专门用于不同的子任务，如识别乘客位置和规划路线。然而，这种模式涉及大量的手动调优和实验，并且必须确保这些设计在不同任务中能够很好地泛化。设计特定的神经架构可以提供更好的可解释性，使得能够分解不同的组件并独立模拟它们。因此，这种模式表现出对可解释性的最高倾向，泛化则紧随其后，见[图5](#S5.F5
    "在结构纳入的5种模式中 ‣ 强化学习中的结构：调查与开放问题")。
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 空间 | 类型 | 效率 | 泛化 | 可解释性 | 安全性 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Goals | Factored | ? (?) | ? (?) | ? (?) |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 分解 | ? (?) | ? (?) | ? (?) |  |'
- en: '| Relational | ? (?) | ? (?) |  |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?) | ? (?) |  |  |'
- en: '| States | Latent | ? (?) | ? (?) |  |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 潜在 | ? (?) | ? (?) |  |  |'
- en: '| Factored | ? (?) | ? (?) |  |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) | ? (?) |  |  |'
- en: '| Relational | ? (?),? (?),? (?),? (?) | ? (?),? (?),? (?),? (?),? (?), ? (?)
    | ? (?), ? (?) |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?),? (?),? (?),? (?) | ? (?),? (?),? (?),? (?),? (?), ? (?) | ? (?),
    ? (?) |  |'
- en: '| Modular |  |  |  |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 |  |  |  |  |'
- en: '| Actions | Latent | ? (?) |  |  |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 潜在 | ? (?) |  |  |  |'
- en: '| Factored | ? (?) |  | ? (?) |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?) |  | ? (?) |  |'
- en: '| Relational | ? (?) |  | ? (?) |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 关系型 | ? (?) |  | ? (?) |  |'
- en: '| Rewards | Latent |  | ? (?) |  |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 潜在 |  | ? (?) |  |  |'
- en: '|  | Factored |  |  |  | ? (?) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | 分解 |  |  |  | ? (?) |'
- en: '| Dynamics | Latent | ? (?) | ? (?), ? (?) |  |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 潜在 | ? (?) | ? (?), ? (?) |  |  |'
- en: '| Factored | ? (?),? (?) |  |  |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 分解 | ? (?),? (?) |  |  |  |'
- en: '|  | Relational |  | ? (?) |  |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | 关系型 |  | ? (?) |  |  |'
- en: '| Policies | Relational | ? (?), ? (?) | ? (?) | ? (?) |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 关系型 | ? (?), ? (?) | ? (?) | ? (?) |  |'
- en: '|  | Modular |  | ? (?) | ? (?),? (?) |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | 模块化 |  | ? (?) | ? (?),? (?) |  |'
- en: Splitting Functionality.
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拆分功能性。
- en: One way to bias the architecture is to split its functionality into different
    parts. Most of the works that achieve such disambiguation are either Factored
    or Relational. Structured Control Nets (?) model linear and non-linear aspects
    of the dynamics individually and combine them additively to gain sample efficiency
    and generalization. Alternatively, Bi-linear Value Networks (?) architecturally
    decompose dynamics into state and goal-conditioned components to produce a goal-conditioned
    Q-function. Action Branching architectures (?) used a shared representation that
    is then factored into separate action branches for individual functionality. This
    approach bears similarity to capturing multi-task representations using bottlenecks (?).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置架构的一种方式是将其功能分为不同的部分。大多数实现这种消歧的作品要么是分解的，要么是关系的。结构化控制网络（?）分别对动态的线性和非线性方面进行建模，并将它们以加法的方式结合起来，以获得样本效率和泛化性。另一种方法是使用双线性价值网络（?）将动态体系架构分解为状态和目标条件组件，以生成目标条件Q功能。行动分支架构（?）使用一个共享表示，然后将其分解为单独的行动分支，以实现个别功能。这种方法类似于使用瓶颈捕获多任务表示。
- en: Relational and Modular biases manifest in hierarchical architectures. This also
    allows them to add more interpretability to the architecture. Two-step hybrid
    policies (?), for example, demonstrate an explicit method to make policies more
    interpretable through splitting actions into pruners and selector modules. On
    the other hand, routing hierarchies explicitly capture modularity using sub-modules
    that a separate policy can use for routing them (?, ?).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 关系和模块化偏见表现在分层架构中。这也使它们能够为架构增加更多的可解释性。举例来说，两步混合策略（?），通过将行动分为修剪器和选择器模块，明确展示了使政策更具解释性的方法。另一方面，路由层次结构可以明确地利用子模块捕捉模块化，这些模块可以由另一个政策用于它们的路由（?,
    ?）。
- en: Capturing Invariances in Architectures.
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在架构中捕获不变性。
- en: Specialized architectures can also help capture invariance in the problem. Symbolic
    Networks (?, ?) train a set of shared parameters for Relational MDPs by first
    converting them to Graphs and then capturing node embeddings using Neural Networks.
    Homomorphic Networks (?) capture symmetry into specialized MLP and CNN architectures.
    An alternate approach to incorporating symmetry is through basis functions (?).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 专门的架构也可以帮助捕捉问题中的不变性。符号网络（?, ?）通过首先将其转换为图形，然后使用神经网络捕捉节点嵌入来为关系MDPs训练一组共享参数。同态网络（?）将对称性捕捉到专门的MLP和CNN架构中。将对称性纳入的另一种方法是通过基函数（?）。
- en: Attention mechanisms can explicitly capture entity-factored scenarios (?, ?, ?).
    Relational and Graph Networks can capture additional relational inductive biases
    explicitly. Linear Relation Networks (?) provides an architecture that scales
    linearly with the number of objects. Graph networks have also been used to model
    an agent’s morphology in embodied control explicitly (?, ?).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制可以明确地捕捉实体因子化的情景（?, ?, ?）。关系和图网络可以明确地捕捉额外的关系归纳偏见。线性关系网络（?）提供了一种随着对象数量线性增长的架构。图网络还被用来明确地对具体行为进行建模代理人形态（?,
    ?）。
- en: Specialized Modules.
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 专门的模块。
- en: A class of methods combines the best of both worlds by capturing invariance
    in additional specialized modules. Such modules can capture relational structure
    in semantic meaning (?), relational encoders for auxiliary models (?), or specialized
    architectures for incorporating domain knowledge (?).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 一类方法结合了两种世界的优势，通过专门的模块来捕获不变性。这些模块可以捕捉语义意义中的关系结构（?），辅助模型的关系编码器（?），或者用于合并领域知识的专门架构（?）。
- en: 6 Open Problems in Structured Reinforcement Learning
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化强化学习中的6个开放问题
- en: 'Having discussed our patterns-oriented framework for understanding how to incorporate
    structure into the RL pipeline, we now turn to connect our framework with existing
    sub-fields of RL. We examine existing paradigms in these sub-fields from two major
    perspectives: Scalability and Robustness. These dimensions serve as a canvas upon
    which we can position and understand different RL paradigms in sub-fields such
    as Offline RL, Unsupervised RL, Foundation Models in RL, Partial observability,
    Big Worlds, Automated RL, and Meta-RL.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了我们面向模式的框架来理解如何将结构纳入RL管线之后，我们现在转向将我们的框架与现有的RL子领域连接起来。我们从可扩展性和鲁棒性这两个主要角度审视这些子领域中的现有范式。这些维度可以被视为一个画布，我们可以在上面定位和理解RL子领域中不同的RL范式，如离线RL，无监督RL，RL基础模型，部分可观察性，大世界，自动化RL和元RL。
- en: Sparse data scenarios require more intelligent ways to use limited experiences,
    while abundant data scenarios might suffer from data quality since they might
    be generated from noisy and often unreliable sources.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据场景需要更智能的方法来利用有限的经验，而丰富数据场景可能会受到数据质量的影响，因为数据可能来自噪声大且经常不可靠的来源。
- en: Scalability
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性
- en: measures how well methods scale with the increasing problem complexity in terms
    of the size of the state and action spaces, complex dynamics, noisy reward signals,
    and longer task horizons. On one hand, methods might specifically require low
    dimensional spaces and might not scale so well with increasing the size of these
    spaces, and on the other, some methods might be overkill for simple problems but
    better suited for large spaces.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量方法如何随着问题复杂性的增加在状态空间和动作空间的大小、复杂动态、噪声奖励信号和较长任务时间等方面进行扩展。一方面，方法可能特别需要低维空间，并且在这些空间的大小增加时扩展性可能较差；另一方面，一些方法可能对简单问题来说过于复杂，但更适合于大空间。
- en: Robustness
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 鲁棒性
- en: measures the response of methods to changes in the environment. While the notion
    overlaps with generalization, robustness for our purposes more holistically looks
    at central properties of data distribution, such as initial state distributions
    and multi-modal evaluation returns. Under this notion, fundamentally different
    learning dynamics might be robust to different kinds of changes in the environment.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量方法对环境变化的响应。虽然这一概念与泛化重叠，但在我们的目的下，鲁棒性更全面地考虑了数据分布的核心属性，如初始状态分布和多模态评估回报。在这一概念下，本质上不同的学习动态可能对环境中的不同变化具有鲁棒性。
- en: Structure of the Section.
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本节结构
- en: In the following subsections, we cover sub-fields of RL that lie at different
    points of the 2D space of Scalability and Robustness. We introduce the existing
    paradigms and the current challenges for each sub-field. We then present some
    examples in which our framework can bolster further research and practice in these
    fields. Finally, we collate this discussion into takeaways that can be combined
    into specific design patterns utilizing our framework.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们涵盖了在可扩展性和鲁棒性的二维空间中处于不同点的RL子领域。我们介绍了每个子领域的现有范式和当前挑战。然后，我们展示了一些示例，说明我们的框架如何推动这些领域的进一步研究和实践。最后，我们将讨论内容整理成可结合的设计模式，以利用我们的框架。
- en: 6.1 Offline RL
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 离线RL
- en: 'Offline Reinforcement Learning (also known as batch RL) (?) involves learning
    from a fixed dataset without further interaction with the environment. This approach
    can benefit when active exploration is costly, risky, or infeasible. Consequently,
    such methods are highly data-dependent due to their reliance on the collected
    dataset, and they do not generalize well due to the limitations of the pre-collected
    data. The three dominant paradigms in Offline RL – Behavior Cloning (?), Q-Learning (?),
    and Sequence Modelling (?) – uniformly degrade in performance as the state-space
    increases (?). Offline RL also comes with its own challenges, including overcoming
    distributional shifts and exploiting the available dataset effectively. Structural
    decomposition can play a crucial role in addressing these challenges in the following
    ways:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 离线强化学习（也称为批量RL） (?) 涉及从固定数据集中学习，而不与环境进一步互动。这种方法在主动探索成本高、风险大或不可行时尤其有用。因此，由于依赖于收集的数据集，这些方法对数据高度依赖，并且由于预先收集数据的局限性，它们的泛化能力较差。离线RL中的三种主要范式——行为克隆 (?)、Q学习 (?)
    和序列建模 (?)——在状态空间增加时普遍表现出性能下降的趋势 (?)。离线RL还面临自身的挑战，包括克服分布变化和有效利用可用数据集。结构分解在解决这些挑战中可以发挥关键作用，具体包括以下方面：
- en: Improved Exploitation of Dataset.
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的更有效利用。
- en: Task decomposition allows learning individual policies or value functions for
    different subtasks, which could potentially leverage the available data more effectively.
    For example, modular decomposition through warehousing separate policies for individual
    modules using the corresponding subset of the data might be more sample-efficient
    than learning a single policy for the entire task. Task decompositions, thus,
    open up new avenues for developing specialized algorithms that effectively learn
    from limited data about each subtask while balancing the effects of learning different
    subtasks. Practitioners can leverage such decompositions to maximize the utility
    of their available datasets by training models that effectively handle specific
    subtasks, potentially improving the overall system’s performance with the same
    dataset.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分解允许学习不同子任务的单独策略或值函数，可以更有效地利用可用数据。例如，通过仓储分解，使用数据的相应子集为各个模块单独建立策略可能比为整个任务学习单一策略更具样本效率。因此，任务分解为开发有效学习不同子任务效果平衡的专门算法开辟了新途径。从业者可以利用这样的分解来通过训练能有效处理特定子任务的模型，最大限度地提高可用数据集的效用，潜在地改善整个系统在相同数据集下的性能。
- en: Mitigating Distributional Shift.
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓解分布偏移。
- en: The structural information could potentially help mitigate the effect of distributional
    shifts. For instance, if some factors are less prone to distributional shifts
    in a factored decomposition, we could focus more on those factors during learning.
    This opens up venues for gaining theoretical insights into the complex interplay
    of structural decompositions, task distributions, and policy performance. On the
    other hand, practical methods for environments where distributional shifts are
    common could leverage structural decomposition to create more robust RL systems.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 结构信息可能有助于缓解分布偏移的影响。例如，如果在分解中某些因素不太容易受到分布偏移的影响，我们可以在学习过程中更加关注这些因素。这为我们获得有关结构分解、任务分布和策略表现之间复杂相互作用的理论洞见打开了机会。另一方面，在分布偏移普遍的环境中，实用方法可以利用结构分解来创建更加健壮的强化学习系统。
- en: Auxiliary Tasks for Exploration.
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索的辅助任务。
- en: Structural decomposition can be used to define auxiliary tasks that facilitate
    learning from the dataset. For instance, in a relational decomposition, we could
    define auxiliary tasks that involve predicting the relationships between different
    entities, which could help in learning a useful representation of the data. Using
    the proposed framework, researchers can explore how to define meaningful auxiliary
    tasks that help the agent learn a better representation of the environment. This
    could lead to new methods that efficiently exploit the available data by learning
    about these auxiliary tasks. Practitioners can design auxiliary tasks based on
    the specific decompositions of their problem. For example, if the task has a clear
    relational structure, auxiliary tasks that predict the relations between different
    entities can potentially improve the agent’s understanding of the environment
    and its overall performance.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 结构分解可用于定义辅助任务，以便从数据集中更好地学习。例如，在关系分解中，我们可以定义涉及预测不同实体之间关系的辅助任务，这有助于学习数据的有用表示。利用所提出的框架，研究人员可以探索如何定义有助于代理人更好地学习环境表示的有意义的辅助任务。这可能会导致新方法，通过学习这些辅助任务有效利用可用数据。从业者可以根据问题的特定分解设计辅助任务。例如，如果任务具有清晰的关系结构，那么可以预测不同实体之间的关系的辅助任务可能会改善代理人对环境的理解和整体性能。
- en: <svg   height="89.38" overflow="visible" version="1.1" width="600"><g transform="translate(0,89.38)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="61.04" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Offline RL Patterns • Use a Factored or Relational decomposition
    to create abstractions that can help with distribution shift and auxiliary interpretability.
    • Implement a Modular design with each module targeting a specific sub-problem,
    improving Scalability. • Employ policy reuse by warehousing policies learned for
    sub-problems across tasks. • If sufficient interaction data is available, employ
    data augmentation strategies for counterfactual scenarios using latent models.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="89.38" overflow="visible" version="1.1" width="600"><g transform="translate(0,89.38)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="61.04" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">离线强化学习模式 • 使用因式分解或关系分解来创建能够帮助应对分布偏移和辅助解释性的抽象。 • 实施模块化设计，每个模块针对特定的子问题，从而提高**可扩展性**。
    • 通过为子问题存储学习到的策略来利用策略重用。 • 如果有足够的交互数据，使用潜在模型的策略进行反事实场景的数据增强。
- en: 6.2 Unsupervised RL
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 无监督强化学习
- en: Unsupervised RL (?) refers to the sub-field of behavior learning in RL, where
    an agent learns to interact with an environment without receiving explicit feedback
    or guidance in the form of rewards. Methods in this area can be characterized
    based on the nature of the metrics that are used to evaluate performance intrinsically (?).
    *Knowledge-based* methods define a self-supervised task by making predictions
    on some aspect of the environment (?, ?, ?), *Data-based methods* maximize the
    state visitation entropy for exploring the environment (?, ?, ?, ?, ?), and *Competence-based*
    methods maximize the mutual information between the trajectories and space of
    learned skills (?, ?, ?, ?, ?). The pre-training phase allows these methods to
    learn the underlying structure of data. However, this phase also requires large
    amounts of data and, thus, impacts the scalability of such methods for problems
    where the learned representations are not very useful. Consequently, such methods
    currently handle medium complexity problems, with the avenue of better scalability
    being a topic of further research.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督强化学习（？）指的是强化学习中的一个子领域，其中代理在没有明确反馈或奖励指导的情况下学习与环境交互。该领域的方法可以根据用于评估性能的指标的性质进行特征化（？）。*基于知识*的方法通过对环境某些方面进行预测来定义自监督任务（？，？，？，），*基于数据*的方法通过最大化状态访问熵来探索环境（？，？，？，？，？），而*基于能力*的方法则最大化轨迹与学习技能空间之间的互信息（？，？，？，？，？）。预训练阶段使这些方法能够学习数据的潜在结构。然而，这一阶段也需要大量的数据，因此影响了这些方法在学习表示不太有用的问题上的可扩展性。因此，目前这些方法处理的是中等复杂性的问题，而更好的可扩展性仍然是进一步研究的主题。
- en: Structural decompositions can help such methods by improving the pre-training
    phase’s tractability and the fine-tuning phase’s generality. Latent decompositions
    could help exploit structure in unlabeled data, while relational decompositions
    could add interpretability to the learned representations. Through augmentation,
    conditioning policies on specific parts of the state space can reduce the amount
    of data needed for fine-tuning. Additionally, understanding problem decomposition
    can simplify complex problems into more manageable sub-problems, effectively reducing
    the perceived problem complexity while incorporating such decomposition in external
    curricula for fine-tuning. Incorporating warehousing guided by decompositions
    for competence-based methods can boost the fine-tuning process of the learned
    skills.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 结构性分解可以通过提高预训练阶段的可处理性和微调阶段的通用性来帮助这些方法。潜在分解可以帮助利用未标记数据中的结构，而关系分解可以为学习到的表示增加可解释性。通过增强，将策略条件化于状态空间的特定部分可以减少微调所需的数据量。此外，理解问题分解可以将复杂问题简化为更易于管理的子问题，有效减少感知问题的复杂性，同时在外部课程中纳入这种分解以进行微调。结合分解引导的仓储可以提升学习技能的微调过程。
- en: <svg   height="105.99" overflow="visible" version="1.1" width="600"><g transform="translate(0,105.99)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Unsupervised RL Patterns • Use latent decompositions to extract
    structure from unlabeled data, reducing Data Dependency. • Employ factored and
    modular decompositions and abstractions to manage scalability by focusing learning
    on different parts of the problem independently. • Warehouse skills across different
    modular sub-problems to reuse solutions and enhance Generality. • Manage Problem
    Complexity by leveraging problem decomposition to simplify the learning task and
    using decompositions for fine-tuning using curriculum learning.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="105.99" overflow="visible" version="1.1" width="600"><g transform="translate(0,105.99)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">无监督强化学习模式 • 使用潜在分解从未标记的数据中提取结构，减少数据依赖性。 • 采用分解和模块化的抽象来管理可扩展性，通过将学习集中在问题的不同部分上独立进行。
    • 在不同的模块子问题中储存技能，以便重用解决方案并提高通用性。 • 通过利用问题分解简化学习任务，并使用分解进行微调以实现课程学习，来管理问题复杂性。
- en: 6.3 Big Data and Foundation models in RL
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 大数据与强化学习中的基础模型
- en: Foundation models (?, ?, ?) refer to a paradigm where a large model is pre-trained
    on large and heterogeneous datasets and fine-tuned for specific tasks. These models
    are “foundational” in the sense that they can serve as the basis for a wide range
    of tasks, reducing the need for training separate models for each task from scratch.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型（?, ?, ?）指的是一种范式，其中一个大型模型在大量异质数据集上进行预训练，并针对特定任务进行微调。这些模型被称为“基础性”的，因为它们可以作为广泛任务的基础，减少为每个任务从头训练单独模型的需求。
- en: Foundation models for RL come increasingly closer to becoming a reality. Such
    RL models would follow a similar concept of training a large model on various
    tasks, environments, and behaviors to be fine-tuned for specific downstream tasks.
    SMART (?), one of the current contenders for such models, follows this paradigm
    by using a self-supervised and control-centric objective that encourages the transformer-based
    model to capture control-relevant representation and demonstrates superior performance
    when used for fine-tuning. AdA (?) trains an in-context learning agent on a vast
    distribution of tasks where the task factors are generated from a latent ruleset.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的基础模型越来越接近现实。这类强化学习模型会遵循类似的概念，即在各种任务、环境和行为上训练一个大型模型，然后针对特定的下游任务进行微调。SMART（?），目前这些模型中的一个有力竞争者，遵循这一范式，通过使用自监督和以控制为中心的目标，鼓励基于变换器的模型捕捉与控制相关的表示，并在用于微调时表现出优越的性能。AdA（?）在一个庞大的任务分布上训练一个上下文学习代理，其中任务因素是从潜在规则集中生成的。
- en: Given the pre-training paradigm, these methods are highly data-dependent in
    principle. However, incorporating large amounts of data can demonstrate scalability
    benefits by reducing fine-tuning costs for distributed applications. A natural
    question that arises is the role of Structured RL in the realm of end-end learning
    and big data. Even though such methods subscribe to an end-to-end paradigm, structural
    decompositions can benefit them differently.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于预训练范式，这些方法原则上高度依赖数据。然而，结合大量数据可以通过减少分布式应用的微调成本来展示可扩展性优势。一个自然的问题是结构化强化学习在端到端学习和大数据领域中的作用。即便这些方法遵循端到端范式，结构性分解也可以以不同的方式使其受益。
- en: Interpretability and Selection during Fine-tuning.
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调过程中的可解释性和选择。
- en: Researchers can better understand the decomposability in the pre-trained models
    by categorizing methods based on how they incorporate structure. Consequently,
    this can guide the selection processes for fine-tuning methods depending on the
    tasks at hand. Passive learning from pre-trained models can benefit from better
    explanations about what parts of a fine-tuning task space might be suited for
    what kind of warmstarting strategies. Additionally, incorporating interpretability-oriented
    decompositions such as relational representations can help design more interpretable
    fine-tuning methods.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员可以通过根据方法如何融入结构来对预训练模型中的可分解性进行分类，从而更好地理解这些模型。因此，这可以指导根据实际任务选择微调方法。对预训练模型的被动学习可以从关于微调任务空间的哪些部分适合什么样的预热策略的更好解释中受益。此外，融入以可解释性为导向的分解，例如关系表示，可以帮助设计更具可解释性的微调方法。
- en: Task-Specific Architectures and Algorithms.
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特定任务的架构和算法。
- en: Structural information can guide the development of novel architectures. With
    a better understanding of how different architectures and algorithms incorporate
    structural information, practitioners can more effectively adapt existing methods
    or contribute to designing novel solutions tailored to their specific tasks. For
    example, Action Branching architectures might provide modular functionality in
    downstream tasks, especially suited for multi-task settings. On the other hand,
    representation bottlenecks might suit settings that deviate from each other by
    small changes in contextual features.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 结构信息可以指导新架构的开发。通过更好地理解不同架构和算法如何融入结构信息，从业者可以更有效地调整现有方法或贡献于设计针对其特定任务的创新解决方案。例如，行动分支架构可能在下游任务中提供模块化功能，尤其适合多任务设置。另一方面，表示瓶颈可能适合那些在上下文特征上有细微变化的设置。
- en: Improved Fine-Tuning and Transfer Learning.
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 改进的微调和迁移学习。
- en: By understanding how to decompose tasks and incorporate structural information,
    foundational models can be fine-tuned more effectively for specific tasks or transferred
    to new tasks. The understanding of decompositions could guide how to structure
    the fine-tuning process or how to adapt the foundational model to a new task.
    By understanding how to incorporate structural information during fine-tuning,
    they can potentially achieve improved performance.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解如何分解任务和融入结构信息，基础模型可以更有效地进行特定任务的微调或迁移到新任务。对分解的理解可以指导如何构建微调过程或如何将基础模型适应到新任务中。通过理解在微调过程中如何融入结构信息，它们可能会实现更好的性能。
- en: Benchmarking and Evaluation.
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准测试和评估。
- en: By understanding the spectrum of decomposability and how various methods incorporate
    structure, we can create better benchmarks and evaluation protocols for foundational
    models. For instance, we can evaluate how well foundational models handle tasks
    of different decompositions and patterns of fine-tuning. Researchers can use this
    framework to design better evaluation protocols and benchmarks for foundational
    models. For practitioners, such benchmarks and evaluation protocols can guide
    the selection of models and algorithms for their specific tasks.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解可分解性的范围以及各种方法如何融入结构，我们可以为基础模型创建更好的基准测试和评估协议。例如，我们可以评估基础模型在处理不同分解任务和微调模式方面的表现。研究人员可以利用这一框架设计更好的评估协议和基准测试，以适用于基础模型。对于从业者而言，这些基准测试和评估协议可以指导他们为特定任务选择模型和算法。
- en: <svg   height="155.8" overflow="visible" version="1.1" width="600"><g transform="translate(0,155.8)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="127.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Foundation Model Patterns • Use Factored or Relational abstractions
    on the pre-trained foundation model for state abstractions to manage high-dimensional
    state spaces and, thus, reduce data dependency. • Condition the policy on additional
    task-specific information, such as goal information, representation of specific
    fine-tuning instructions, or control priors to improve scalability. • Regularize
    the fine-tuning process to prevent catastrophic forgetting of useful features
    learned during pre-training. • Maintain a warehouse of fine-tuned policies and
    value functions to help reuse previously learned skills and adapt them to new
    tasks, improving learning efficiency and generalization. • Incorporate a curriculum
    of increasingly complex fine-tuning environments based on the agent’s performance
    to help the agent gradually adapt the foundation model’s knowledge to the specific
    RL task. • Use explicit architectures that fine-tune different RL problem aspects,
    such as perception, policy learning, and value estimation.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="155.8" overflow="visible" version="1.1" width="600"><g transform="translate(0,155.8)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="127.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">基础模型模式 • 在预训练的基础模型上使用因子化或关系抽象进行状态抽象，以管理高维状态空间，从而减少数据依赖性。 • 根据额外的任务特定信息（例如目标信息、特定微调指令的表示或控制先验）对策略进行条件化，以提高可扩展性。
    • 对微调过程进行正则化，以防止在预训练过程中学到的有用特征的灾难性遗忘。 • 维护一个微调策略和价值函数的仓库，以帮助重用以前学到的技能，并将其适应新任务，从而提高学习效率和泛化能力。
    • 根据代理的表现，结合一系列逐渐复杂的微调环境，以帮助代理逐步将基础模型的知识适应特定的强化学习任务。 • 使用明确的架构来微调不同的强化学习问题方面，例如感知、策略学习和价值估计。
- en: 6.4 Partial observability and Big Worlds
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 部分可观测性与大世界
- en: In many real-world situations, the Markov property might not fully capture the
    dynamics of the environment (?, ?). This can happen in cases where the environment’s
    state or the rewards depend on more than just the most recent state and action
    or if the agent cannot fully observe the state of the environment at each time
    step. In such situations, methods must deal with non-Markovian dynamics and partial
    observability.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的情况下，马尔可夫性质可能无法完全捕捉环境的动态（？，？）。这种情况可能发生在环境的状态或奖励不仅依赖于最近的状态和动作，或者如果代理无法在每个时间步骤完全观察环境状态时。在这种情况下，方法必须处理非马尔可夫动态和部分可观测性。
- en: Abstractions.
  id: totrans-371
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 抽象。
- en: Abstractions can play a crucial role in such situations, where structural decompositions
    using abstraction patterns can make methods more sample efficient. Often used
    in options, temporal Abstractions allow the agent to make decisions over extended
    periods, thereby encapsulating potential temporal dependencies within these extended
    actions. This can effectively convert a non-Markovian problem into a Markovian
    one at the level of options. State Abstractions abstract away irrelevant aspects
    of the state and, thus, can sometimes ignore certain temporal dependencies, rendering
    the process Markovian at the level of the abstracted states. Thus, research into
    the role of decompositions in abstraction opens up possibilities to understand
    the dependencies between non-Markovian models and the kind of abstractions they
    use to solve problems with incomplete information. Abstraction can also simplify
    the observation space in POMDPs, reducing the complexity of the belief update
    process. The abstraction might involve grouping similar observations together,
    identifying higher-level features, or other simplifications. Abstractions can
    additionally allow us to break partial observability down into different types
    instead of always assuming the worst-case scenario. Utilizing such restricted
    assumptions on partial observability can help us build more specific algorithms
    and derive convergence and optimality guarantees for such scenarios.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，抽象可以发挥关键作用，结构分解利用抽象模式可以使方法更加样本高效。通常用于选项中的时间抽象允许智能体在较长时间内做出决策，从而将这些扩展动作中的潜在时间依赖性封装起来。这可以有效地将非马尔可夫问题转换为选项级别的马尔可夫问题。状态抽象抽象掉了状态中的无关方面，因此，有时可以忽略某些时间依赖性，从而在抽象状态级别使过程变为马尔可夫。因此，研究抽象中的分解角色为理解非马尔可夫模型之间的依赖关系及其解决不完整信息问题时使用的抽象类型打开了可能性。抽象还可以简化POMDP中的观测空间，从而减少信念更新过程的复杂性。抽象可能涉及将相似观测分组、识别更高级的特征或其他简化。抽象还可以使我们将部分可观测性分解为不同类型，而不是总是假设最坏情况。利用这种对部分可观测性的限制假设可以帮助我们构建更具体的算法，并为这些场景推导收敛性和最优性保证。
- en: Augmentations.
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强。
- en: Any additional information required, such as belief states or memory of past
    observations, can be used as abstractions or augmentations. This can also help
    with more efficient learning of transition models for planning. Hierarchical techniques
    that utilize optimization at different timescales can incorporate warehousing
    to reuse learned policies across various levels of abstraction. Environment generation
    patterns could also be used to generate a curriculum of increasingly complex tasks
    for the agent, starting with simpler MDPs and gradually introducing partial observability
    or other non-Markovian features.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 任何额外需要的信息，如信念状态或过去观察的记忆，都可以用作抽象或补充。这也可以帮助更高效地学习用于规划的转移模型。利用在不同时间尺度上进行优化的分层技术可以将仓储纳入其中，以便在各种抽象层次之间重用学习到的策略。环境生成模式也可以用来为智能体生成一系列越来越复杂的任务，从简单的MDP开始，逐步引入部分可观测性或其他非马尔可夫特性。
- en: Big worlds.
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大世界。
- en: As we extend the information content of the environment to its extremity, we
    delve into the realm of the big world hypothesis in RL (?), where the agent’s
    environment is multiple orders of magnitude larger than the agent, and the agent
    cannot represent the optimal value function and policy even in the limit of infinite
    data. In such scenarios, the agent must make decisions under significant uncertainty,
    which presents several challenges, including exploration, generalization, and
    efficient learning. Even though the hypothesis suggests that incorporating side
    information might not be beneficial in learning the optimal policy and value in
    such scenarios, structural decomposition of large environments in different ways
    can allow benchmarking methods along different axes while allowing a deeper study
    into the performance of algorithms on parts of the environment that the agent
    has not yet experienced.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将环境的信息内容扩展到其极限时，我们*深入*到RL中的大世界假设领域，其中智能体的环境比智能体大多个数量级，甚至在无限数据的极限下，智能体也无法表示最优价值函数和策略。在这种情况下，智能体必须在重大不确定性下做出决策，这带来了包括探索、泛化和高效学习等几个挑战。尽管该假设表明在这种情况下整合边际信息可能对学习最优策略和价值没有帮助，但以不同方式对大环境进行结构分解可以允许在不同轴上进行基准测试，同时深入研究智能体尚未体验过的环境部分上的算法性能。
- en: Modular decomposition can guide the agent’s exploration process by helping the
    agent explore different parts of the environment independently. Incorporating
    modularity opens a gateway to novel methods and theoretical insights about the
    relationships between task decomposition, exploration, and learning efficiency
    in large environments. Relational decompositions can help the agent learn relationships
    between different entities, bolstering its ability to generalize to unseen parts
    of the environment. Finally, Structural information can be used to facilitate
    more efficient learning. For instance, in an auxiliary optimization pattern, the
    agent could learn faster by optimizing auxiliary tasks that are easier to learn
    or provide useful information about the environment’s structure.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化分解可以通过帮助代理独立探索环境的不同部分来指导代理的探索过程。引入模块化为任务分解、探索和在大环境中学习效率之间关系提供了新方法和理论见解。关系分解可以帮助代理学习不同实体之间的关系，从而增强其对未见部分环境的泛化能力。最后，结构信息可以用于促进更高效的学习。例如，在辅助优化模式中，代理可以通过优化更容易学习或提供有关环境结构有用信息的辅助任务来加速学习。
- en: <svg   height="138.9" overflow="visible" version="1.1" width="600"><g transform="translate(0,138.9)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="110.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Patterns for Partial Observability and Big Worlds • Use temporal
    and state abstraction to abstract away temporal dependencies and non-Markovian
    aspects of the state. Utilize Modularity to tie these abstractions to learned
    primitives such as skills or options. • Use memory more efficiently as an abstraction
    or augmentation for learned transition models. • Warehouse policies and utilize
    them for optimization across timescales, such as in hierarchical methods, to make
    them more tractable. • Utilize modular decompositions for guiding separate and
    parallel exploration mechanisms for different parts of the state space. Utilize
    relational abstractions to make this knowledge more interpretable. • Utilize structure
    for task factorization to guide benchmarking methods along different axes of task
    complexity.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="138.9" overflow="visible" version="1.1" width="600"><g transform="translate(0,138.9)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="110.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">部分可观测性和大世界的模式 • 使用时间和状态抽象来抽象掉时间依赖性和非马尔可夫状态的方面。利用**模块化**将这些抽象与已学习的基本元素（如技能或选项）联系起来。
    • 更有效地利用**记忆**作为学习过渡模型的抽象或增强。 • 管理策略并利用它们进行跨时间尺度的优化，例如在**层次方法**中，以使其更易处理。 • 利用模块化分解来指导不同状态空间部分的独立和并行探索机制。利用**关系抽象**使这些知识更具可解释性。
    • 利用**结构**进行任务因式分解，以指导沿不同任务复杂性轴的基准方法。
- en: 6.5 Automated RL
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 自动化强化学习
- en: Automated RL (AutoRL) is a sub-field focused on methods to automate the process
    of designing and optimizing RL algorithms, including the agent’s architecture,
    reward function, and other hyperparameters (?). Methods in AutoRL can be placed
    on a spectrum of automation, where on one end would be methods to select pipelines
    and on the other would be methods that try to discover new algorithms ground-up
    in a data-driven manner (?). Techniques from the Automated Machine Learning literature (?)
    then transfer to the RL setting, including algorithm selection (?), hyperparameter
    optimization (?, ?, ?), dynamic configurations (?), learned optimizers (?), and
    neural architecture search (?). Similarly, techniques from the Evolutionary optimization
    and Meta-Learning literature naturally transfer to this setting with methods aiming
    to meta-learn parts of the RL pipeline such as update rules (?), loss functions (?, ?),
    symbolic representations of algorithms (?, ?, ?), or concept drift (?). However,
    there are still many open questions in AutoRL, such as properties of hyperparameter
    landscapes in RL (?), sound evaluation protocols (?), stability of training due
    to the non-stationary learning task and non-deterministic data collection on the
    fly. Consequently, most of these methods suffer from a lack of scalability.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化强化学习（AutoRL）是一个专注于自动化设计和优化强化学习算法过程的子领域，包括代理的架构、奖励函数以及其他超参数（？）。AutoRL中的方法可以被放置在一个自动化的范围上，一端是选择管道的方法，而另一端则是尝试从头开始以数据驱动的方式发现新算法的方法（？）。自动化机器学习文献中的技术（？）随后转移到强化学习的设置中，包括算法选择（？）、超参数优化（？，？，？）、动态配置（？）、学习优化器（？）和神经架构搜索（？）。类似地，进化优化和元学习文献中的技术自然转移到这种设置中，方法旨在元学习强化学习管道的部分，例如更新规则（？）、损失函数（？，？）、算法的符号表示（？，？，？）或概念漂移（？）。然而，AutoRL中仍存在许多未解之谜，例如强化学习中超参数景观的特性（？）、合理的评估协议（？）、由于非平稳学习任务和非确定性数据收集导致的训练稳定性。因此，大多数这些方法都存在可扩展性不足的问题。
- en: Algorithm Selection and Configuration.
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法选择与配置。
- en: Depending on the decomposability of the problem at hand, different RL methods
    could be more appropriate. Structural decompositions can guide the selection process
    in AutoRL by suggesting appropriate types of decompositions based on the problem
    characteristics. Understanding how different decomposition types influence the
    performance of RL methods can bridge the gap between selection and configuration
    by helping researchers understand the level of abstraction needed for selection
    conditioned on the task, aiding in developing more efficient and targeted search
    algorithms. Decomposability can also guide ranking procedures, where methods that
    cater to different decomposability can be ranked differently, given a problem.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头问题的可分解性，不同的强化学习方法可能更为合适。结构性分解可以通过根据问题特征建议适当的分解类型来指导AutoRL中的选择过程。了解不同分解类型如何影响强化学习方法的性能，可以弥合选择与配置之间的差距，帮助研究人员理解在特定任务下所需的抽象级别，从而有助于开发更高效和有针对性的搜索算法。可分解性也可以指导排名程序，其中根据问题的不同分解性，方法可以被不同地排名。
- en: Hyperparameter Optimization.
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数优化。
- en: Parameters related to structural decomposition (e.g., the number of subtasks
    in a modular decomposition) could be part of the hyperparameter optimization process
    in AutoRL. Researchers can investigate the interplay between configuration spaces
    of hyperparameters and various structural decomposition-related parameters. For
    example, high decomposability might require different exploration rates or learning
    rates than a low decomposability problem. This could lead to novel insights and
    methods for more effective hyperparameter optimization in AutoRL. Practitioners
    can use this understanding to guide the hyperparameter optimization process in
    their AutoRL system. By tuning parameters related to the decomposition, they can
    potentially improve the performance of their RL agent.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 与结构分解相关的参数（例如，模块化分解中的子任务数量）可能是AutoRL中超参数优化过程的一部分。研究人员可以探讨超参数配置空间与各种结构分解相关参数之间的相互作用。例如，高可分解性可能需要不同的探索率或学习率，而低可分解性问题可能需要不同的设置。这可能会带来对AutoRL中更有效的超参数优化的新见解和方法。实践者可以利用这一理解来指导其AutoRL系统中的超参数优化过程。通过调整与分解相关的参数，他们可能会改善其强化学习代理的性能。
- en: <svg   height="105.99" overflow="visible" version="1.1" width="600"><g transform="translate(0,105.99)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">AutoRL Patterns • Use methods to perform a decomposability assessment
    of a problem. This can help guide algorithm selection for problems with different
    types of decomposability. • Expedite the hyperparameter search by abstracting
    away task-irrelevant aspects. • Warehouse to reuse learned policy and value functions
    for landmarking performances of algorithms similar to the one being optimized.
    • Incorporate modularity information in the form of goals and task hierarchies
    in the search process. • Structure neural architecture search-space using decomposability
    in the problem.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="105.99" overflow="visible" version="1.1" width="600"><g transform="translate(0,105.99)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">AutoRL 模式 • 使用方法对问题进行可分解性评估。这可以帮助指导选择不同类型可分解性的问题的算法。 • 通过抽象掉与任务无关的方面来加速超参数搜索。
    • 仓库中重用学习到的策略和价值函数，用于标记类似于正在优化的算法的性能。 • 在搜索过程中结合以目标和任务层级形式的模块化信息。 • 使用问题中的可分解性来结构化神经架构搜索空间。
- en: 6.6 Meta-Reinforcement Learning
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 元强化学习
- en: Meta-Reinforcement Learning, while having overlaps with AutoRL, is a field in
    and of itself (?) that focuses on training agents to adapt and learn new tasks
    or environments quickly. The general Meta-RL setup involves a bi-level optimization
    procedure where an agent learns a set of parameters by training on a distribution
    of tasks or environments that help it adapt and perform well on new, unseen tasks
    that share some form of overlap with the training tasks. ? (?) outline different
    problem settings in Meta-RL based on the kind of feedback (supervised, unsupervised,
    rewards) that is provided to the agent during the training and adaptation phases.
    We particularly refer to the standard setting where extrinsic rewards act as feedback
    during the training and adaptation phases. However, decompositions can also be
    useful for other settings similar to those discussed in other sections.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习，尽管与 AutoRL 有重叠，但它本身是一个独立的领域（？），专注于训练代理快速适应和学习新任务或环境。一般的 Meta-RL 设置涉及一个双层优化过程，其中代理通过在任务或环境的分布上进行训练来学习一组参数，这些任务或环境帮助它适应并在与训练任务共享某种形式重叠的新任务上表现良好。？（？）根据提供给代理的反馈类型（监督、无监督、奖励）概述了
    Meta-RL 中的不同问题设置。我们特别参考了标准设置，其中外部奖励在训练和适应阶段作为反馈。然而，分解也可能对其他类似于其他部分讨论的设置有用。
- en: Task Decompositions.
  id: totrans-388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务分解。
- en: Depending on the meta-task’s decomposability, different task decomposition approaches
    could be employed to guide the meta-learning process. Consequently, understanding
    how task decomposition affects Meta-RL can guide the development of more effective
    meta-learning algorithms. It might also lead to new insights on balancing learning
    between different subtasks. By identifying suitable decompositions, practitioners
    can set up their system to learn in a way that is more aligned with the structure
    of the tasks, potentially leading to improved performance.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 根据元任务的可分解性，可以采用不同的任务分解方法来指导元学习过程。因此，理解任务分解如何影响 Meta-RL 可以指导更有效的元学习算法的开发。这也可能导致在不同子任务之间平衡学习的新见解。通过识别合适的分解，实践者可以设置他们的系统，以一种与任务结构更为一致的方式进行学习，从而可能提高性能。
- en: Adaptation Strategies.
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 适应策略。
- en: Decompositions could inform the way a Meta-RL agent adapts to a new task. For
    instance, if the new task is highly decomposable, a modular adaptation strategy
    could be more appropriate by guiding the agent to an appropriate latent space
    of the new task. Thus, our framework of decomposability can inspire new research
    into how the task’s decomposition, f can guide adaptation strategies in Meta-RL.
    This could lead to novel methods or theories on adapting to new tasks more effectively
    based on their structure.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 分解可以指引 Meta-RL 代理如何适应新任务。例如，如果新任务高度可分解，那么模块化适应策略可能更为合适，通过引导代理到新任务的适当潜在空间。因此，我们的可分解性框架可以激发关于任务分解如何在
    Meta-RL 中指导适应策略的新研究。这可能会导致基于任务结构更有效地适应新任务的新方法或理论。
- en: <svg   height="105.99" overflow="visible" version="1.1" width="600"><g transform="translate(0,105.99)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Meta-RL Patterns • Use decompositions for abstracting task distributions,
    which can be integrated into the adaptation process. • Compartmentalize the learning
    process into modules for highly decomposable problems. These modules can serve
    as abstract configurations for the meta-level and, thus, make the outer loop more
    tractable • Learn and warehouse models geared towards specific task clusters for
    different decomposability types to guide data augmentation during the adaptation
    phase. • Utilize decomposability to design learning curricula based on abstract
    types of tasks to train the warmstarting configurations
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="105.99" overflow="visible" version="1.1" width="600"><g transform="translate(0,105.99)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject
    width="555.91" height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Meta-RL模式 • 使用分解来抽象任务分布，这可以集成到适应过程当中。 • 将学习过程划分为模块以应对高度可分解的问题。这些模块可以作为元层次的抽象配置，从而使外循环更加易于处理。
    • 学习和存储针对特定任务集群的模型，以应对不同的可分解性类型，从而指导在适应阶段的数据增强。 • 利用可分解性设计基于任务抽象类型的学习课程，以训练预热配置
- en: 7 Conclusion and Future Work
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: Understanding the intricacies of Reinforcement Learning (RL) ’s complexities
    is challenging, exacerbated by the divergent methodologies employed across different
    problem domains. This fragmentation hinders the development of unifying principles
    and consistent practices in RL. To address this critical gap, we propose an innovative
    framework to understand different methods of effectively integrating the inherent
    structure of learning problems into RL algorithms. Our work serves as a pivotal
    step towards consolidating the multifaceted aspects of RL, ushering in a design
    pattern perspective for this domain.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 理解强化学习（RL）复杂性的细节是具有挑战性的，不同问题领域中采用的各种方法加剧了这一复杂性。这种碎片化阻碍了RL中统一原则和一致实践的发展。为了解决这一关键问题，我们提出了一个创新框架，以理解将学习问题固有结构有效地融入RL算法的不同方法。我们的工作是整合RL多方面特征的重要一步，为这一领域带来了设计模式的视角。
- en: We first conceptualized structure as side information about the decomposability
    of a learning problem and corresponding solutions. We have categorized decomposability
    into four distinct archetypes - latent, factored, relational, and modular. This
    classification delineates a spectrum that establishes insightful connections with
    existing literature, elucidating the diverse influence of structure within RL.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将结构概念化为关于学习问题及其对应解决方案的可分解性旁注。我们将可分解性分为四种不同的原型——潜在型、因子型、关系型和模块型。这种分类描绘了一个谱系，建立了与现有文献的深刻联系，阐明了结构在强化学习（RL）中的多样影响。
- en: We then presented seven key patterns following a thorough analysis of the RL
    landscape - abstraction, augmentation, auxiliary optimization, auxiliary model,
    warehousing, environment generation, and explicitly designed patterns. These patterns
    represent strategic approaches for the incorporation of structural knowledge into
    RL. Although our framework provides a comprehensive starting point, we acknowledge
    that these patterns are not exhaustive. We envisage this as an impetus for researchers
    to refine and develop new patterns, thereby expanding the repertoire of design
    patterns in RL.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们在对RL领域进行深入分析后，提出了七种关键模式——抽象、增强、辅助优化、辅助模型、仓储、环境生成和明确设计模式。这些模式代表了将结构性知识融入RL的战略方法。尽管我们的框架提供了一个全面的起点，但我们承认这些模式并非详尽无遗。我们期望这能激励研究人员进一步完善和开发新模式，从而扩展RL中的设计模式库。
- en: In conclusion, our work offers a pattern-centric perspective on RL, underlining
    the critical role of structural decompositions in shaping both present and future
    paradigms. By promoting this perspective, we aim to stimulate a new wave of research
    in RL, enriched by a deeper and more structured understanding of the field. While
    our proposed framework is a novel contribution, it should be viewed as an initial
    step in an ongoing process. We anticipate and encourage further development and
    refinement of our framework and eagerly await the emergence of new, innovative
    patterns that will undoubtedly shape the future of RL.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的工作提供了一个以模式为中心的强化学习视角，强调结构分解在塑造当前和未来范式中的关键作用。通过推动这一视角，我们旨在激发强化学习领域的一波新研究，丰富对该领域的深入和结构化理解。虽然我们提出的框架是一个新颖的贡献，但应视为持续过程中的初步步骤。我们期待并鼓励进一步发展和完善我们的框架，并热切期待那些无疑将塑造强化学习未来的新颖模式的出现。
- en: Acknowledgments
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors thank Robert Kirk and Rohan Chitnis for their discussion and comments
    on drafts of this work. We would also like to thank Vincent François-Lavet, Khimya
    Khetrapal, and Rishabh Aggarwal for providing additional relevant references in
    the literature.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Robert Kirk和Rohan Chitnis对本工作的草稿所提出的讨论和意见。我们还要感谢Vincent François-Lavet、Khimya
    Khetrapal和Rishabh Aggarwal提供的额外相关文献。
- en: References
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdulhai et al. Abdulhai, M., Kim, D., Riemer, M., Liu, M., Tesauro, G., and How,
    J. (2022). Context-specific representation abstraction for deep option learning..
    In Sycara et al. (?).
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdulhai等人 Abdulhai, M., Kim, D., Riemer, M., Liu, M., Tesauro, G., 和 How, J.
    (2022). 针对深度选项学习的上下文特定表示抽象. 见Sycara等人（?）。
- en: Abel, Hershkowitz, Barth-Maron, Brawner, O’Farrell, MacGlashan, and Tellex Abel,
    D., Hershkowitz, D., Barth-Maron, G., Brawner, S., O’Farrell, K., MacGlashan,
    J., and Tellex, S. (2015). Goal-based action priors. In Proceedings of the Twenty-Fifth
    International Conference on Automated Planning and Scheduling, (ICAPS’15).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abel, Hershkowitz, Barth-Maron, Brawner, O’Farrell, MacGlashan和Tellex Abel,
    D., Hershkowitz, D., Barth-Maron, G., Brawner, S., O’Farrell, K., MacGlashan,
    J., 和 Tellex, S. (2015). 基于目标的行动先验. 见第二十五届自动规划与调度国际会议论文集（ICAPS’15）。
- en: Abel, Umbanhowar, Khetarpal, Arumugam, Precup, and Littman Abel, D., Umbanhowar,
    N., Khetarpal, K., Arumugam, D., Precup, D., and Littman, M. (2020). Value preserving
    state-action abstractions. In Proceedings of the 23rd International Conference
    on Artificial Intelligence and Statistics, (AISTATS’20), pp. 1639–1650.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abel, Umbanhowar, Khetarpal, Arumugam, Precup和Littman Abel, D., Umbanhowar,
    N., Khetarpal, K., Arumugam, D., Precup, D., 和 Littman, M. (2020). 价值保持的状态-动作抽象.
    见第23届人工智能与统计国际会议论文集（AISTATS’20），第1639–1650页。
- en: Adjodah, Klinger, and Joseph Adjodah, D., Klinger, T., and Joseph, J. (2018).
    Symbolic relation networks for reinforcement learning. In Proceedings of the Workshop
    on Relational Representation Learning in Conference on Neural Information Processing
    Systems (NeurIPS).
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adjodah, Klinger和Joseph Adjodah, D., Klinger, T., 和 Joseph, J. (2018). 强化学习的符号关系网络.
    见神经信息处理系统会议（NeurIPS）关系表示学习研讨会论文集。
- en: Adriaensen, Biedenkapp, Shala, Awad, Eimer, Lindauer, and Hutter Adriaensen,
    S., Biedenkapp, A., Shala, G., Awad, N., Eimer, T., Lindauer, M., and Hutter,
    F. (2022). Automated dynamic algorithm configuration. Journal of Artificial Intelligence
    Research (JAIR), 75, 1633–1699.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adriaensen, Biedenkapp, Shala, Awad, Eimer, Lindauer和Hutter Adriaensen, S.,
    Biedenkapp, A., Shala, G., Awad, N., Eimer, T., Lindauer, M., 和 Hutter, F. (2022).
    自动动态算法配置. 人工智能研究期刊（JAIR），75，1633–1699。
- en: Agarwal, Machado, Castro, and Bellemare Agarwal, R., Machado, M., Castro, P.,
    and Bellemare, M. (2021). Contrastive behavioral similarity embeddings for generalization
    in reinforcement learning. In Proceedings of the Ninth International Conference
    on Learning Representations, (ICLR’21).
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal, Machado, Castro和Bellemare Agarwal, R., Machado, M., Castro, P., 和 Bellemare,
    M. (2021). 用于强化学习中泛化的对比行为相似性嵌入. 见第九届学习表示国际会议论文集（ICLR’21）。
- en: Alabdulkarim and Riedl Alabdulkarim, A.,  and Riedl, M. (2022). Experiential
    explanations for reinforcement learning. CoRR, abs/2210.04723.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alabdulkarim和Riedl Alabdulkarim, A., 和 Riedl, M. (2022). 强化学习的经验性解释. CoRR, abs/2210.04723。
- en: Alet, Schneider, Lozano-Pérez, and Kaelbling Alet, F., Schneider, M., Lozano-Pérez,
    T., and Kaelbling, L. (2020). Meta-learning curiosity algorithms. In Proceedings
    of the Eighth International Conference on Learning Representations (ICLR’20).
    OpenReview.net.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alet, Schneider, Lozano-Pérez和Kaelbling Alet, F., Schneider, M., Lozano-Pérez,
    T., 和 Kaelbling, L. (2020). 元学习好奇算法. 见第八届学习表示国际会议论文集（ICLR’20）。OpenReview.net。
- en: Allen, Parikh, Gottesman, and Konidaris Allen, C., Parikh, N., Gottesman, O.,
    and Konidaris, G. (2021). Learning markov state abstractions for deep reinforcement
    learning.. In Ranzato et al. (?).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allen, Parikh, Gottesman, 和 Konidaris Allen, C., Parikh, N., Gottesman, O.,
    和 Konidaris, G. (2021). 深度强化学习中的马尔可夫状态抽象学习。见 Ranzato 等（？）。
- en: Amin, Gomrokchi, Aboutalebi, Satija, and Precup Amin, S., Gomrokchi, M., Aboutalebi,
    H., Satija, H., and Precup, D. (2021a). Locally persistent exploration in continuous
    control tasks with sparse rewards.. In Meila, and Zhang (?).
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin, Gomrokchi, Aboutalebi, Satija, 和 Precup Amin, S., Gomrokchi, M., Aboutalebi,
    H., Satija, H., 和 Precup, D. (2021a). 连续控制任务中的局部持续探索与稀疏奖励。见 Meila 和 Zhang（？）。
- en: Amin, Gomrokchi, Satija, van Hoof, and Precup Amin, S., Gomrokchi, M., Satija,
    H., van Hoof, H., and Precup, D. (2021b). A survey of exploration methods in reinforcement
    learning. CoRR, abs/2109.00157.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin, Gomrokchi, Satija, van Hoof, 和 Precup Amin, S., Gomrokchi, M., Satija,
    H., van Hoof, H., 和 Precup, D. (2021b). 强化学习中的探索方法综述。CoRR，abs/2109.00157。
- en: Andersen and Konidaris Andersen, G.,  and Konidaris, G. (2017). Active exploration
    for learning symbolic representations.. In Guyon et al. (?).
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andersen 和 Konidaris Andersen, G., 和 Konidaris, G. (2017). 用于学习符号表示的主动探索。见 Guyon
    等（？）。
- en: 'Andreas, Klein, and Levine Andreas, J., Klein, D., and Levine, S. (2018). Learning
    with latent language. In Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreas, Klein, 和 Levine Andreas, J., Klein, D., 和 Levine, S. (2018). 通过潜在语言进行学习。见《2018年北美计算语言学协会：人类语言技术会议论文集》。
- en: Azizzadenesheli, Lazaric, and Anandkumar Azizzadenesheli, K., Lazaric, A., and Anandkumar,
    A. (2016). Reinforcement learning in rich-observation mdps using spectral methods.
    CoRR, abs/1611.03907.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azizzadenesheli, Lazaric, 和 Anandkumar Azizzadenesheli, K., Lazaric, A., 和 Anandkumar,
    A. (2016). 使用谱方法的富观测 MDP 中的强化学习。CoRR，abs/1611.03907。
- en: Bacon, Harb, and Precup Bacon, P., Harb, J., and Precup, D. (2017). The option-critic
    architecture. In S.Singh,  and Markovitch, S. (Eds.), Proceedings of the Thirty-First
    Conference on Artificial Intelligence (AAAI’17). AAAI Press.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bacon, Harb, 和 Precup Bacon, P., Harb, J., 和 Precup, D. (2017). 选项-批评架构。见 S.
    Singh 和 Markovitch, S. (编)，《第三十一届人工智能会议论文集》（AAAI’17）。AAAI出版社。
- en: 'Baheri Baheri, A. (2020). Safe reinforcement learning with mixture density
    network: A case study in autonomous highway driving. CoRR, abs/2007.01698.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baheri Baheri, A. (2020). 结合混合密度网络的安全强化学习：自动高速公路驾驶的案例研究。CoRR，abs/2007.01698。
- en: Bain and Sammut Bain, M.,  and Sammut, C. (1995). A framework for behavioural
    cloning. In Furukawa, K., Michie, D., and Muggleton, S. (Eds.), Machine Intelligence
    15, Intelligent Agents [St. Catherine’s College, Oxford, UK, July 1995], pp. 103–129\.
    Oxford University Press.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bain 和 Sammut Bain, M., 和 Sammut, C. (1995). 行为克隆框架。见 Furukawa, K., Michie,
    D., 和 Muggleton, S. (编)，《机器智能 15，智能代理》（圣凯瑟琳学院，牛津，英国，1995年7月），第 103–129 页。牛津大学出版社。
- en: 'Balaji, Christodoulou, Jeon, and Bell-Masterson Balaji, B., Christodoulou,
    P., Jeon, B., and Bell-Masterson, J. (2020). Factoredrl: Leveraging factored graphs
    for deep reinforcement learning. In NeurIPS Deep Reinforcement Learning Workshop.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balaji, Christodoulou, Jeon, 和 Bell-Masterson Balaji, B., Christodoulou, P.,
    Jeon, B., 和 Bell-Masterson, J. (2020). Factoredrl：利用分解图进行深度强化学习。见 NeurIPS 深度强化学习研讨会。
- en: Bapst, Sanchez-Gonzalez, Doersch, Stachenfeld, Kohli, Battaglia, and Hamrick
    Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K., Kohli, P., Battaglia,
    P., and Hamrick, J. (2019). Structured agents for physical construction.. In Chaudhuri, and Salakhutdinov
    (?).
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bapst, Sanchez-Gonzalez, Doersch, Stachenfeld, Kohli, Battaglia, 和 Hamrick Bapst,
    V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K., Kohli, P., Battaglia,
    P., 和 Hamrick, J. (2019). 用于物理构建的结构化代理。见 Chaudhuri 和 Salakhutdinov（？）。
- en: 'Barreto, Borsa, Hou, Comanici, Aygün, Hamel, Toyama, Hunt, Mourad, Silver, and Precup
    Barreto, A., Borsa, D., Hou, S., Comanici, G., Aygün, E., Hamel, P., Toyama, D.,
    Hunt, J., Mourad, S., Silver, D., and Precup, D. (2019). The option keyboard:
    Combining skills in reinforcement learning.. In Wallach et al. (?).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barreto, Borsa, Hou, Comanici, Aygün, Hamel, Toyama, Hunt, Mourad, Silver, 和
    Precup Barreto, A., Borsa, D., Hou, S., Comanici, G., Aygün, E., Hamel, P., Toyama,
    D., Hunt, J., Mourad, S., Silver, D., 和 Precup, D. (2019). 选项键盘：在强化学习中结合技能。见 Wallach
    等（？）。
- en: Barreto, Borsa, Quan, Schaul, Silver, Hessel, Mankowitz, Zidek, and Munos Barreto,
    A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek,
    A., and Munos, R. (2018). Transfer in deep reinforcement learning using successor
    features and generalised policy improvement.. In Dy, and Krause (?).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barreto, Borsa, Quan, Schaul, Silver, Hessel, Mankowitz, Zidek, 和 Munos Barreto,
    A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek,
    A., 和 Munos, R. (2018). 使用后继特征和广义策略改进的深度强化学习中的迁移。在 Dy 和 Krause (?)。
- en: Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt, and Silver Barreto, A., Dabney,
    W., Munos, R., Hunt, J., Schaul, T., van Hasselt, H., and Silver, D. (2017). Successor
    features for transfer in reinforcement learning.. In Guyon et al. (?).
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt, 和 Silver Barreto, A., Dabney,
    W., Munos, R., Hunt, J., Schaul, T., van Hasselt, H., 和 Silver, D. (2017). 强化学习中的后继特征迁移。在
    Guyon 等 (?)。
- en: Bauer, Baumli, Baveja, Behbahani, Bhoopchand, Bradley-Schmieg, Chang, Clay,
    Collister, Dasagi, Gonzalez, Gregor, Hughes, Kashem, Loks-Thompson, Openshaw,
    Parker-Holder, Pathak, Nieves, Rakicevic, Rocktäschel, Schroecker, Sygnowski,
    Tuyls, York, Zacherl, and Zhang Bauer, J., Baumli, K., Baveja, S., Behbahani,
    F., Bhoopchand, A., Bradley-Schmieg, N., Chang, M., Clay, N., Collister, A., Dasagi,
    V., Gonzalez, L., Gregor, K., Hughes, E., Kashem, S., Loks-Thompson, M., Openshaw,
    H., Parker-Holder, J., Pathak, S., Nieves, N., Rakicevic, N., Rocktäschel, T.,
    Schroecker, Y., Sygnowski, J., Tuyls, K., York, S., Zacherl, A., and Zhang, L.
    (2023). Human-timescale adaptation in an open-ended task space. CoRR, abs/2301.07608.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bauer, Baumli, Baveja, Behbahani, Bhoopchand, Bradley-Schmieg, Chang, Clay,
    Collister, Dasagi, Gonzalez, Gregor, Hughes, Kashem, Loks-Thompson, Openshaw,
    Parker-Holder, Pathak, Nieves, Rakicevic, Rocktäschel, Schroecker, Sygnowski,
    Tuyls, York, Zacherl, 和 Zhang Bauer, J., Baumli, K., Baveja, S., Behbahani, F.,
    Bhoopchand, A., Bradley-Schmieg, N., Chang, M., Clay, N., Collister, A., Dasagi,
    V., Gonzalez, L., Gregor, K., Hughes, E., Kashem, S., Loks-Thompson, M., Openshaw,
    H., Parker-Holder, J., Pathak, S., Nieves, N., Rakicevic, N., Rocktäschel, T.,
    Schroecker, Y., Sygnowski, J., Tuyls, K., York, S., Zacherl, A., 和 Zhang, L. (2023).
    人类时间尺度适应在开放式任务空间中的应用。CoRR, abs/2301.07608.
- en: Baumli, Warde-Farley, Hansen, and Mnih Baumli, K., Warde-Farley, D., Hansen,
    S., and Mnih, V. (2021). Relative variational intrinsic control.. In Yang et al.
    (?).
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baumli, Warde-Farley, Hansen, 和 Mnih Baumli, K., Warde-Farley, D., Hansen, S.,
    和 Mnih, V. (2021). 相对变分内在控制。在 Yang 等 (?)。
- en: Beck, Vuorio, Liu, Xiong, Zintgraf, Finn, and Whiteson Beck, J., Vuorio, R.,
    Liu, E., Xiong, Z., Zintgraf, L., Finn, C., and Whiteson, S. (2023). A survey
    of meta-reinforcement learning. CoRR, abs/2301.08028.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beck, Vuorio, Liu, Xiong, Zintgraf, Finn, 和 Whiteson Beck, J., Vuorio, R., Liu,
    E., Xiong, Z., Zintgraf, L., Finn, C., 和 Whiteson, S. (2023). 元强化学习的综述。CoRR, abs/2301.08028.
- en: Bellman Bellman, R. (1954). Some applications of the theory of dynamic programming
    - A review. Oper. Res., 2(3), 275–288.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman Bellman, R. (1954). 动态规划理论的一些应用 - 综述。运筹学, 2(3), 275–288.
- en: Belogolovsky, Korsunsky, Mannor, Tessler, and Zahavy Belogolovsky, S., Korsunsky,
    P., Mannor, S., Tessler, C., and Zahavy, T. (2021). Inverse reinforcement learning
    in contextual mdps. Mach. Learn., 110(9), 2295–2334.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belogolovsky, Korsunsky, Mannor, Tessler, 和 Zahavy Belogolovsky, S., Korsunsky,
    P., Mannor, S., Tessler, C., 和 Zahavy, T. (2021). 上下文 mdps 中的逆强化学习。机器学习, 110(9),
    2295–2334.
- en: Bengio, Wallach, Larochelle, Grauman, Cesa-Bianchi, and Garnett Bengio, S.,
    Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (Eds.).
    (2018). Proceedings of the 31st International Conference on Advances in Neural
    Information Processing Systems (NeurIPS’18). Curran Associates.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio, Wallach, Larochelle, Grauman, Cesa-Bianchi, 和 Garnett Bengio, S., Wallach,
    H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., 和 Garnett, R. (编). (2018).
    第31届国际神经信息处理系统大会（NeurIPS’18）论文集。Curran Associates.
- en: Benjamins, Eimer, Schubert, Mohan, Döhler, Biedenkapp, Rosenhahn, Hutter, and Lindauer
    Benjamins, C., Eimer, T., Schubert, F., Mohan, A., Döhler, S., Biedenkapp, A.,
    Rosenhahn, B., Hutter, F., and Lindauer, M. (2023). Contextualize me - the case
    for context in reinforcement learning. Transactions on Machine Learning Research,
    2835-8856.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benjamins, Eimer, Schubert, Mohan, Döhler, Biedenkapp, Rosenhahn, Hutter, 和
    Lindauer Benjamins, C., Eimer, T., Schubert, F., Mohan, A., Döhler, S., Biedenkapp,
    A., Rosenhahn, B., Hutter, F., 和 Lindauer, M. (2023). 语境化我 - 强化学习中的语境案例。机器学习研究论文集,
    2835-8856.
- en: Bewley and Lecune Bewley, T.,  and Lecune, F. (2022). Interpretable preference-based
    reinforcement learning with tree-structured reward functions. In 21st International
    Conference on Autonomous Agents and Multiagent Systems, AAMAS 2022, Auckland,
    New Zealand, May 9-13, 2022. International Foundation for Autonomous Agents and
    Multiagent Systems (IFAAMAS).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bewley 和 Lecune Bewley, T., 和 Lecune, F. (2022). 具有树结构奖励函数的可解释的基于偏好的强化学习。在第21届国际自主代理与多代理系统会议（AAMAS
    2022），新西兰奥克兰，2022年5月9-13日。国际自主代理与多代理系统基金会（IFAAMAS）。
- en: 'Beyret, Shafti, and Faisal Beyret, B., Shafti, A., and Faisal, A. (2019). Dot-to-dot:
    Explainable hierarchical reinforcement learning for robotic manipulation. In International
    Conference on Intelligent Robots and Systems, (IROS’19), pp. 5014–5019\. IEEE.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beyret, Shafti, 和 Faisal Beyret, B., Shafti, A., 和 Faisal, A. (2019). 点对点：用于机器人操作的可解释层次强化学习。在国际智能机器人与系统会议（IROS’19）中，第5014–5019页。IEEE。
- en: Bhargava, Chitnis, Geramifard, Sodhani, and Zhang Bhargava, P., Chitnis, R.,
    Geramifard, A., Sodhani, S., and Zhang, A. (2023). Sequence modeling is a robust
    contender for offline reinforcement learning. CoRR, abs/2305.14550.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhargava, Chitnis, Geramifard, Sodhani, 和 Zhang Bhargava, P., Chitnis, R., Geramifard,
    A., Sodhani, S., 和 Zhang, A. (2023). 序列建模是离线强化学习的有力竞争者。CoRR, abs/2305.14550。
- en: Bhatt, Tjanaka, Fontaine, and Nikolaidis Bhatt, V., Tjanaka, B., Fontaine, M.,
    and Nikolaidis, S. (2022). Deep surrogate assisted generation of environments.
    In Proceedings of the 35th International Conference on Advances in Neural Information
    Processing Systems (NeurIPS’22).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatt, Tjanaka, Fontaine, 和 Nikolaidis Bhatt, V., Tjanaka, B., Fontaine, M.,
    和 Nikolaidis, S. (2022). 深度代理辅助环境生成。在第35届神经信息处理系统国际会议（NeurIPS’22）论文集中。
- en: Biza, Kipf, Klee, Platt, van de Meent, and Wong Biza, O., Kipf, T., Klee, D.,
    Platt, R., van de Meent, J., and Wong, L. (2022a). Factored world models for zero-shot
    generalization in robotic manipulation. In arXiv preprint arXiv:2202.05333.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biza, Kipf, Klee, Platt, van de Meent, 和 Wong Biza, O., Kipf, T., Klee, D.,
    Platt, R., van de Meent, J., 和 Wong, L. (2022a). 用于零-shot泛化的分解世界模型。在arXiv预印本arXiv:2202.05333中。
- en: Biza, Platt, van de Meent, Wong, and Kipf Biza, O., Platt, R., van de Meent,
    J., Wong, L., and Kipf, T. (2022b). Binding actions to objects in world models.
    In arXiv preprint arXiv:2204.13022.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biza, Platt, van de Meent, Wong, 和 Kipf Biza, O., Platt, R., van de Meent, J.,
    Wong, L., 和 Kipf, T. (2022b). 在世界模型中将动作绑定到对象上。在arXiv预印本arXiv:2204.13022中。
- en: Borsa, Barreto, Quan, Mankowitz, van Hasselt, Munos, Silver, and Schaul Borsa,
    D., Barreto, A., Quan, J., Mankowitz, D., van Hasselt, H., Munos, R., Silver,
    D., and Schaul, T. (2019). Universal successor features approximators. In Proceedings
    of the Seventh International Conference on Learning Representations (ICLR’19).
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borsa, Barreto, Quan, Mankowitz, van Hasselt, Munos, Silver, 和 Schaul Borsa,
    D., Barreto, A., Quan, J., Mankowitz, D., van Hasselt, H., Munos, R., Silver,
    D., 和 Schaul, T. (2019). 通用后继特征近似器。在第七届国际学习表征会议（ICLR’19）论文集中。
- en: Borsa, Graepel, and Shawe-Taylor Borsa, D., Graepel, T., and Shawe-Taylor, J.
    (2016). Learning shared representations in multi-task reinforcement learning.
    CoRR, abs/1603.02041.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borsa, Graepel, 和 Shawe-Taylor Borsa, D., Graepel, T., 和 Shawe-Taylor, J. (2016).
    在多任务强化学习中学习共享表示。CoRR, abs/1603.02041。
- en: Boutilier, Cohen, Hassidim, Mansour, Meshi, Mladenov, and Schuurmans Boutilier,
    C., Cohen, A., Hassidim, A., Mansour, Y., Meshi, O., Mladenov, M., and Schuurmans,
    D. (2018). Planning and learning with stochastic action sets.. In Lang (?).
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boutilier, Cohen, Hassidim, Mansour, Meshi, Mladenov, 和 Schuurmans Boutilier,
    C., Cohen, A., Hassidim, A., Mansour, Y., Meshi, O., Mladenov, M., 和 Schuurmans,
    D. (2018). 使用随机动作集的规划和学习。在Lang (?)中。
- en: Boutilier, Dearden, and Goldszmidt Boutilier, C., Dearden, R., and Goldszmidt,
    M. (2000). Stochastic dynamic programming with factored representations. Artificial
    Intelligence, 121(1-2), 49–107.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boutilier, Dearden, 和 Goldszmidt Boutilier, C., Dearden, R., 和 Goldszmidt, M.
    (2000). 带有分解表示的随机动态规划。人工智能，121(1-2)，49–107。
- en: Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba Brockman,
    G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,
    W. (2016). OpenAI gym. In arxiv preprint arXiv:1606.01540.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, 和 Zaremba Brockman,
    G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., 和 Zaremba,
    W. (2016). OpenAI gym。在arxiv预印本arXiv:1606.01540中。
- en: Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell,
    Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter,
    Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford,
    Sutskever, and Amodei Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
    Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
    B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. (2020). Language models are few-shot learners.. In Larochelle et al. (?), pp. 1877–1901.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell,
    Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter,
    Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford,
    Sutskever, and Amodei Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
    Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
    B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. (2020). 语言模型是少样本学习者。在Larochelle等人（？），页1877-1901。
- en: Brunskill and Li Brunskill, E.,  and Li, L. (2013). Sample complexity of multi-task
    reinforcement learning. In Nicholson, A.,  and Smyth, P. (Eds.), Proceedings of
    the 29th conference on Uncertainty in Artificial Intelligence (UAI’13). AUAI Press.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunskill and Li Brunskill, E.,  and Li, L. (2013). 多任务强化学习的样本复杂度。在尼克尔森，A.，和Smyth，P（编），第29届不确定性人工智能大会（UAI'13）论文集。AUAI出版社。
- en: Buchholz and Scheftelowitsch Buchholz, P.,  and Scheftelowitsch, D. (2019).
    Computation of weighted sums of rewards for concurrent mdps. Math. Methods Oper.
    Res., 89(1), 1–42.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buchholz and Scheftelowitsch Buchholz, P.,  and Scheftelowitsch, D. (2019).
    加权奖励并发mdp的计算。数学方法运筹研究，89（1），1-42。
- en: 'Buesing, Weber, Zwols, Heess, Racanière, Guez, and Lespiau Buesing, L., Weber,
    T., Zwols, Y., Heess, N., Racanière, S., Guez, A., and Lespiau, J. (2019). Woulda,
    coulda, shoulda: Counterfactually-guided policy search. In Proceesings of the
    Seventh International Conference on Learning Representations (ICLR’19). OpenReview.net.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buesing, Weber, Zwols, Heess, Racanière, Guez, and Lespiau Buesing, L., Weber,
    T., Zwols, Y., Heess, N., Racanière, S., Guez, A., and Lespiau, J. (2019). 应该，能够，应该：反事实导向的策略搜索。在第七届国际学习表示学术会议（ICLR'19）论文集中。OpenReview.net。
- en: 'Burgess, Matthey, Watters, Kabra, Higgins, Botvinick, and Lerchner Burgess,
    C., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., and Lerchner,
    A. (2019). Monet: Unsupervised scene decomposition and representation. CoRR, abs/1901.11390.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Burgess, Matthey, Watters, Kabra, Higgins, Botvinick, and Lerchner Burgess,
    C., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., and Lerchner,
    A. (2019). Monet: 无监督场景分解和表示。CoRR，abs/1901.11390。'
- en: 'Castro, Kastner, Panangaden, and Rowland Castro, P., Kastner, T., Panangaden,
    P., and Rowland, M. (2021). Mico: Improved representations via sampling-based
    state similarity for markov decision processes.. In Ranzato et al. (?).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Castro, Kastner, Panangaden, and Rowland Castro, P., Kastner, T., Panangaden,
    P., and Rowland, M. (2021). Mico: 通过基于采样的状态相似性改善表示方法的马尔科夫决策过程。在Ranzato等人（？）中。'
- en: Castro, Kastner, Panangaden, and Rowland Castro, P., Kastner, T., Panangaden,
    P., and Rowland, M. (2023). A kernel perspective on behavioural metrics for markov
    decision processes. In Transactions on Machine Learning Research.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Castro, Kastner, Panangaden, and Rowland Castro, P., Kastner, T., Panangaden,
    P., and Rowland, M. (2023). 马尔科夫决策过程的行为度量的内核视角。在机器学习研究交易。 '
- en: Chandak, Theocharous, Kostas, Jordan, and Thomas Chandak, Y., Theocharous, G.,
    Kostas, J., Jordan, S., and Thomas, P. (2019). Learning action representations
    for reinforcement learning.. In Chaudhuri, and Salakhutdinov (?).
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandak, Theocharous, Kostas, Jordan, and Thomas Chandak, Y., Theocharous, G.,
    Kostas, J., Jordan, S., and Thomas, P. (2019). 为强化学习学习动作表示方式。在Chaudhuri，and Salakhutdinov（？）。
- en: Chaudhuri, Jegelka, Song, Szepesvári, Niu, and Sabato Chaudhuri, K., Jegelka,
    S., Song, L., Szepesvári, C., Niu, G., and Sabato, S. (Eds.). (2022). Proceedings
    of the 39th International Conference on Machine Learning (ICML’22), Vol. 162 of
    Proceedings of Machine Learning Research. PMLR.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhuri, Jegelka, Song, Szepesvári, Niu, and Sabato Chaudhuri, K., Jegelka,
    S., Song, L., Szepesvári, C., Niu, G., and Sabato, S.（编）。 （2022）。第39届国际机器学习会议(ICML'22)论文集，机器学习研究论文集第162卷。PMLR。
- en: Chaudhuri and Salakhutdinov Chaudhuri, K.,  and Salakhutdinov, R. (Eds.). (2019).
    Proceedings of the 36th International Conference on Machine Learning (ICML’19),
    Vol. 97\. Proceedings of Machine Learning Research.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhuri and Salakhutdinov Chaudhuri, K.,  and Salakhutdinov, R.（编）。 （2019）。第36届国际机器学习会议(ICML'19)论文集，机器学习研究论文集第97卷。PMLR。
- en: Chen, Gao, Xu, Yang, Li, Ding, Feng, and Wang Chen, C., Gao, Z., Xu, K., Yang,
    S., Li, Y., Ding, B., Feng, D., and Wang, H. (2022). Nuclear norm maximization
    based curiosity-driven learning. CoRR, abs/2205.10484.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈，高，徐，杨，李，丁，冯，以及王 陈，C.，高，Z.，徐，K.，杨，S.，李，Y.，丁，B.，冯，D.，和王，H.（2022）。基于核范数最大化的好奇驱动学习。CoRR，abs/2205.10484。
- en: Chen, Hu, Nikdel, Mori, and Savva Chen, C., Hu, S., Nikdel, P., Mori, G., and Savva,
    M. (2020). Relational graph learning for crowd navigation. In 2020 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS). IEEE.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈，胡，尼克德尔，森，以及萨瓦 陈，C.，胡，S.，尼克德尔，P.，森，G.，和萨瓦，M.（2020）。用于人群导航的关系图学习。在2020 IEEE/RSJ国际智能机器人与系统会议（IROS）。IEEE。
- en: Chen, Wan, Shi, Ding, Gao, and Feng Chen, C., Wan, T., Shi, P., Ding, B., Gao,
    Z., and Feng, D. (2022). Uncertainty estimation based intrinsic reward for efficient
    reinforcement learning. In 2022 IEEE International Conference on Joint Cloud Computing
    (JCC), pp. 1–8.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈，万，石，丁，高，以及冯 陈，C.，万，T.，石，P.，丁，B.，高，Z.，和冯，D.（2022）。基于不确定性估计的内在奖励以提高强化学习效率。在2022
    IEEE国际联合云计算会议（JCC），第1–8页。
- en: 'Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch Chen,
    L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas,
    A., and Mordatch, I. (2021). Decision transformer: Reinforcement learning via
    sequence modeling.. In Ranzato et al. (?).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈，陆，拉杰斯瓦兰，李，格罗弗，拉斯金，阿贝尔，斯里尼瓦斯，以及莫达奇 陈，L.，陆，K.，拉杰斯瓦兰，A.，李，K.，格罗弗，A.，拉斯金，M.，阿贝尔，P.，斯里尼瓦斯，A.，和莫达奇，I.（2021）。决策变换器：通过序列建模进行强化学习。在
    Ranzato 等（？）。
- en: 'Cheung et al. Cheung, W., Simchi-Levi, D., and Zhu, R. (2020). Reinforcement
    learning for non-stationary markov decision processes: The blessing of (more)
    optimism. In icml20.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheung 等 Cheung，W.，Simchi-Levi，D.，和 Zhu，R.（2020）。用于非平稳马尔科夫决策过程的强化学习：更多乐观的福音。在icml20。
- en: Christodoulou et al. Christodoulou, P., Lange, R., Shafti, A., and Faisal, A.
    (2019). Reinforcement learning with structured hierarchical grammar representations
    of actions. CoRR, abs/1910.02876.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christodoulou 等 Christodoulou，P.，Lange，R.，Shafti，A.，和Faisal，A.（2019）。带有结构化层次语法动作表示的强化学习。CoRR，abs/1910.02876。
- en: Chu and Wang Chu, Z.,  and Wang, H. (2023). Meta-reinforcement learning via
    exploratory task clustering. In arXiv preprint arXiv:2302.07958.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu 和 Wang Chu，Z.，和 Wang，H.（2023）。通过探索性任务聚类进行元强化学习。在arXiv预印本arXiv:2302.07958。
- en: Co-Reyes et al. Co-Reyes, J., Miao, Y., Peng, D., Real, E., Le, Q., Levine,
    S., Lee, H., and Faust, A. (2021). Evolving reinforcement learning algorithms.
    In Proceedings of the Ninth International Conference on Learning Representations
    (ICLR’20). OpenReview.net.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Co-Reyes 等 Co-Reyes，J.，苗，Y.，彭，D.，Real，E.，Le，Q.，Levine，S.，Lee，H.，和Faust，A.（2021）。进化强化学习算法。在第九届国际学习表征会议（ICLR’20）论文集中。OpenReview.net。
- en: 'Dayan Dayan, P. (1993). Improving generalization for temporal difference learning:
    The successor representation. Neural Comput., 5(4), 613–624.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dayan Dayan，P.（1993）。提高时间差分学习的泛化能力：继任者表示。Neural Comput.，5（4），613–624。
- en: 'der Pol et al. der Pol, E. V., Worrall, D., van Hoof, H., Oliehoek, F., and Welling,
    M. (2020). Mdp homomorphic networks: Group symmetries in reinforcement learning..
    In Larochelle et al. (?).'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: der Pol 等 der Pol，E. V.，沃勒，D.，范胡夫，H.，奥利赫克，F.，和韦林，M.（2020）。Mdp 同态网络：强化学习中的群对称性。在
    Larochelle 等（？）。
- en: D’Eramo, Tateo, Bonarini, Restelli, and J. Peters D’Eramo, C., Tateo, D., Bonarini,
    A., Restelli, M., and J. Peters, J. (2020). Sharing knowledge in multi-task deep
    reinforcement learning. In Proceedings of the Eighth International Conference
    on Learning Representations (ICLR’20).
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Eramo，Tateo，博纳里尼，雷斯特利，以及J.彼得斯 D’Eramo，C.，Tateo，D.，博纳里尼，A.，雷斯特利，M.，和J.彼得斯，J.（2020）。多任务深度强化学习中的知识共享。在第八届国际学习表征会议（ICLR’20）论文集中。
- en: 'Devin, Geng, Abbeel, Darrell, and Levine Devin, C., Geng, D., Abbeel, P., Darrell,
    T., and Levine, S. (2019). Plan arithmetic: Compositional plan vectors for multi-task
    control. CoRR, abs/1910.14033.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devin，耿，Abbeel，Darrell，以及Levine Devin，C.，耿，D.，Abbeel，P.，Darrell，T.，和Levine，S.（2019）。计划算术：用于多任务控制的组合计划向量。CoRR，abs/1910.14033。
- en: Devin, Gupta, Darrell, Abbeel, and Levine Devin, C., Gupta, A., Darrell, T.,
    Abbeel, P., and Levine, S. (2017). Learning modular neural network policies for
    multi-task and multi-robot transfer. In IEEE International Conference on Robotics
    and Automation (ICRA).
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devin，Gupta，Darrell，Abbeel，以及Levine Devin，C.，Gupta，A.，Darrell，T.，Abbeel，P.，和Levine，S.（2017）。为多任务和多机器人转移学习模块化神经网络策略。在IEEE国际机器人与自动化会议（ICRA）上。
- en: Ding, Lin, Li, and Zhao Ding, W., Lin, H., Li, B., and Zhao, D. (2022). Generalizing
    goal-conditioned reinforcement learning with variational causal reasoning. In
    Proceedings of the 35th International Conference on Advances in Neural Information
    Processing Systems (NeurIPS’22).
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding, Lin, Li 和 Zhao Ding, W., Lin, H., Li, B., 和 Zhao, D. (2022). 使用变分因果推理推广目标条件强化学习。发表于第35届神经信息处理系统国际会议（NeurIPS’22）论文集。
- en: Diuk, Cohen, and Littman Diuk, C., Cohen, A., and Littman, M. (2008). An object-oriented
    representation for efficient reinforcement learning. In Cohen, W., McCallum, A.,
    and Roweis, S. (Eds.), Proceedings of the 25th International Conference on Machine
    Learning (ICML’08). Omnipress.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diuk, Cohen 和 Littman Diuk, C., Cohen, A., 和 Littman, M. (2008). 高效强化学习的面向对象表示。发表于
    Cohen, W., McCallum, A., 和 Roweis, S. (Eds.), 第25届国际机器学习会议（ICML’08）论文集。Omnipress。
- en: Du, Krishnamurthy, Jiang, Agarwal, Dudík, and Langford Du, S., Krishnamurthy,
    A., Jiang, N., Agarwal, A., Dudík, M., and Langford, J. (2019). Provably efficient
    RL with rich observations via latent state decoding.. In Chaudhuri, and Salakhutdinov
    (?).
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du, Krishnamurthy, Jiang, Agarwal, Dudík 和 Langford Du, S., Krishnamurthy, A.,
    Jiang, N., Agarwal, A., Dudík, M., 和 Langford, J. (2019). 通过潜在状态解码的可证明高效RL。发表于
    Chaudhuri 和 Salakhutdinov (?)。
- en: Dy and Krause Dy, J.,  and Krause, A. (Eds.). (2018). Proceedings of the 35th
    International Conference on Machine Learning (ICML’18), Vol. 80\. Proceedings
    of Machine Learning Research.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dy 和 Krause Dy, J., 和 Krause, A. (Eds.). (2018). 第35届国际机器学习会议（ICML’18）论文集，第80卷。机器学习研究论文集。
- en: Dzeroski et al. Dzeroski, S., Raedt, L. D., and Driessens, K. (2001). Relational
    reinforcement learning. Machine Learning Journal, 43(1/2), 7–52.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dzeroski等人 Dzeroski, S., Raedt, L. D., 和 Driessens, K. (2001). 关系型强化学习。《机器学习杂志》，43(1/2),
    7–52。
- en: Ecoffet et al. Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K., and Clune,
    J. (2021). First return, then explore. Nature, 590(7847), 580–586.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ecoffet等人 Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K., 和 Clune, J. (2021).
    先返回，再探索。《自然》，590(7847), 580–586。
- en: Eimer et al. Eimer, T., Lindauer, M., and Raileanu, R. (2023). Hyperparameters
    in reinforcement learning and how to tune them. In Proceedings of the International
    Conference on Machine Learning (ICML’23).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eimer等人 Eimer, T., Lindauer, M., 和 Raileanu, R. (2023). 强化学习中的超参数及其调优方法。发表于国际机器学习会议（ICML’23）论文集。
- en: 'Eysenbach et al. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. (2019).
    Diversity is all you need: Learning skills without a reward function. In Proceedings
    of the Seventh International Conference on Learning Representations (ICLR’19).'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eysenbach等人 Eysenbach, B., Gupta, A., Ibarz, J., 和 Levine, S. (2019). 多样性即是全部：无奖励函数的技能学习。发表于第七届国际学习表征会议（ICLR’19）论文集。
- en: 'Fern et al. Fern, A., Yoon, S., and Givan, R. (2006). Approximate policy iteration
    with a policy language bias: Solving relational markov decision processes. Journal
    of Artificial Intelligence Research, 25, 75–118.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fern等人 Fern, A., Yoon, S., 和 Givan, R. (2006). 带有策略语言偏差的近似策略迭代：解决关系型马尔可夫决策过程。《人工智能研究杂志》，25,
    75–118。
- en: Florensa et al. Florensa, C., Duan, Y., and Abbeel, P. (2017). Stochastic neural
    networks for hierarchical reinforcement learning. In Proceedings of Fifth the
    International Conference on Learning Representations (ICLR’17).
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florensa等人 Florensa, C., Duan, Y., 和 Abbeel, P. (2017). 用于层次强化学习的随机神经网络。发表于第五届国际学习表征会议（ICLR’17）论文集。
- en: Fox et al. Fox, R., Pakman, A., and Tishby, N. (2016). Taming the noise in reinforcement
    learning via soft updates. In Ihler, A.,  and Janzing, D. (Eds.), Proceedings
    of the 32nd conference on Uncertainty in Artificial Intelligence (UAI’16). AUAI
    Press.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fox等人 Fox, R., Pakman, A., 和 Tishby, N. (2016). 通过软更新驯服强化学习中的噪声。发表于 Ihler, A.,
    和 Janzing, D. (Eds.), 第32届人工智能不确定性会议（UAI’16）论文集。AUAI出版社。
- en: Fu et al. Fu, X., Yang, G., Agrawal, P., and Jaakkola, T. (2021). Learning task
    informed abstractions.. In Meila, and Zhang (?).
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu等人 Fu, X., Yang, G., Agrawal, P., 和 Jaakkola, T. (2021). 学习任务信息化抽象。发表于 Meila
    和 Zhang (?)。
- en: Furelos-Blanco, Law, Jonsson, Broda, and Russo Furelos-Blanco, D., Law, M.,
    Jonsson, A., Broda, K., and Russo, A. (2021). Induction and exploitation of subgoal
    automata for reinforcement learning. J. Artif. Intell. Res., 70, 1031–1116.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Furelos-Blanco, Law, Jonsson, Broda 和 Russo Furelos-Blanco, D., Law, M., Jonsson,
    A., Broda, K., 和 Russo, A. (2021). 强化学习中的子目标自动机的归纳与利用。《人工智能研究杂志》，70, 1031–1116。
- en: Gallouedec and Dellandrea Gallouedec, Q.,  and Dellandrea, E. (2023). Cell-free
    latent go-explore. In Proceedings of the 40th International Conference on Machine
    Learning (ICML’23).
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gallouedec 和 Dellandrea Gallouedec, Q., 和 Dellandrea, E. (2023). 无细胞潜在Go-Explore。发表于第40届国际机器学习会议（ICML’23）论文集。
- en: Garcia and Fernandez Garcia, J.,  and Fernandez, F. (2015). A comprehensive
    survey on safe reinforcement learning. Journal of Machine Learning Research, 16,
    1437–1480.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia 和 Fernandez Garcia, J., 和 Fernandez, F. (2015). 关于安全强化学习的综合调查。机器学习研究期刊,
    16, 1437–1480.
- en: 'Garg, Bajpai, and Mausam Garg, S., Bajpai, A., and Mausam (2020). Symbolic
    network: Generalized neural policies for relational mdps.. In III, and Singh (?).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg, Bajpai 和 Mausam Garg, S., Bajpai, A., 和 Mausam (2020). 符号网络：面向关系 MDP 的广义神经策略。在
    III 和 Singh (?).
- en: Garnelo, Arulkumaran, and Shanahan Garnelo, M., Arulkumaran, K., and Shanahan,
    M. (2016). Towards deep symbolic reinforcement learning. In arXiv preprint arXiv:1609.05518.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garnelo, Arulkumaran 和 Shanahan Garnelo, M., Arulkumaran, K., 和 Shanahan, M.
    (2016). 朝着深度符号强化学习的方向发展。在 arXiv 预印本 arXiv:1609.05518.
- en: Gasse, Grasset, Gaudron, and Oudeyer Gasse, M., Grasset, D., Gaudron, G., and Oudeyer,
    P. (2021). Causal reinforcement learning using observational and interventional
    data. In arXiv preprint arXiv:2106.14421.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasse, Grasset, Gaudron 和 Oudeyer Gasse, M., Grasset, D., Gaudron, G., 和 Oudeyer,
    P. (2021). 使用观察和干预数据的因果强化学习。在 arXiv 预印本 arXiv:2106.14421.
- en: Gaya, Doan, Caccia, Soulier, Denoyer, and Raileanu Gaya, J., Doan, T., Caccia,
    L., Soulier, L., Denoyer, L., and Raileanu, R. (2022a). Building a subspace of
    policies for scalable continual learning. In arXiv preprint arXiv:2211.10445.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaya, Doan, Caccia, Soulier, Denoyer 和 Raileanu Gaya, J., Doan, T., Caccia,
    L., Soulier, L., Denoyer, L., 和 Raileanu, R. (2022a). 为可扩展的持续学习构建策略子空间。在 arXiv
    预印本 arXiv:2211.10445.
- en: Gaya, Soulier, and Denoyer Gaya, J., Soulier, L., and Denoyer, L. (2022b). Learning
    a subspace of policies for online adaptation in reinforcement learning. In Proceedings
    of the Tenth International Conference on Learning Representations (ICLR’22).
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaya, Soulier 和 Denoyer Gaya, J., Soulier, L., 和 Denoyer, L. (2022b). 在强化学习中学习策略子空间以进行在线适应。在第十届国际学习表征会议
    (ICLR’22) 的会议记录中.
- en: Gehring, Synnaeve, Krause, and Usunier Gehring, J., Synnaeve, G., Krause, A.,
    and Usunier, N. (2021). Hierarchical skills for efficient exploration.. In Ranzato
    et al. (?).
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring, Synnaeve, Krause 和 Usunier Gehring, J., Synnaeve, G., Krause, A., 和
    Usunier, N. (2021). 高效探索的层次技能。在 Ranzato 等 (?).
- en: Geißer et al. Geißer, F., Speck, D., and Keller, T. (2020). Trial-based heuristic
    tree search for mdps with factored action spaces. In Proceedings of the International
    Symposium on Combinatorial Search.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geißer 等 Geißer, F., Speck, D., 和 Keller, T. (2020). 基于试验的启发式树搜索用于具有因子化动作空间的
    MDP。在国际组合搜索研讨会的会议记录中.
- en: 'Gelada et al. Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare,
    M. (2019). Deepmdp: Learning continuous latent space models for representation
    learning.. In Chaudhuri, and Salakhutdinov (?).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gelada 等 Gelada, C., Kumar, S., Buckman, J., Nachum, O., 和 Bellemare, M. (2019).
    DeepMDP：学习用于表征学习的连续潜在空间模型。在 Chaudhuri 和 Salakhutdinov (?).
- en: Ghorbani et al. Ghorbani, M., Hosseini, R., Shariatpanahi, S., and Ahmadabadi,
    M. (2020). Reinforcement learning with subspaces using free energy paradigm. In
    arXiv preprint arXiv:2012.07091.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghorbani 等 Ghorbani, M., Hosseini, R., Shariatpanahi, S., 和 Ahmadabadi, M. (2020).
    使用自由能范式的子空间强化学习。在 arXiv 预印本 arXiv:2012.07091.
- en: Gillen and Byl Gillen, S.,  and Byl, K. (2021). Explicitly encouraging low fractional
    dimensional trajectories via reinforcement learning. In Conference on Robot Learning,
    pp. 2137–2147\. PMLR.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gillen 和 Byl Gillen, S., 和 Byl, K. (2021). 通过强化学习显式地鼓励低分数维度轨迹。在机器人学习会议上，pp.
    2137–2147\. PMLR.
- en: Goodfellow et al. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep
    Learning. MIT Press.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 Goodfellow, I., Bengio, Y., 和 Courville, A. (2016). 深度学习。MIT Press.
- en: Goyal et al. Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio,
    Y., and Schölkopf, B. (2021). Recurrent independent mechanisms. In Proceedings
    of the Ninth International Conference on Learning Representations (ICLR’21).
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等 Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio,
    Y., 和 Schölkopf, B. (2021). 递归独立机制。在第九届国际学习表征会议 (ICLR’21) 的会议记录中.
- en: Goyal et al. Goyal, A., Sodhani, S., Binas, J., Peng, X., Levine, S., and Bengio,
    Y. (2020). Reinforcement learning with competitive ensembles of information-constrained
    primitives. In Proceedings of the Eighth International Conference on Learning
    Representations (ICLR’20).
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等 Goyal, A., Sodhani, S., Binas, J., Peng, X., Levine, S., 和 Bengio, Y.
    (2020). 使用信息受限原语的竞争集成进行强化学习。在第八届国际学习表征会议 (ICLR’20) 的会议记录中.
- en: Gregor et al. Gregor, K., Rezende, D., and Wierstra, D. (2016). Variational
    intrinsic control. CoRR, abs/1611.07507.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gregor 等 Gregor, K., Rezende, D., 和 Wierstra, D. (2016). 变分内在控制。CoRR, abs/1611.07507.
- en: Guestrin et al. Guestrin, C., Koller, D., Gearhart, C., and Kanodia, N. (2003a).
    Generalizing plans to new environments in relational mdps. In Gottlob, G.,  and Walsh,
    T. (Eds.), Proceedings of the 18th International Joint Conference on Artificial
    Intelligence (IJCAI’03).
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guestrin 等人 Guestrin, C., Koller, D., Gearhart, C., 和 Kanodia, N. (2003a)。在关系
    MDP 中将计划推广到新环境。在 Gottlob, G. 和 Walsh, T.（编），第 18 届国际人工智能联合会议（IJCAI’03）论文集。
- en: Guestrin et al. Guestrin, C., Koller, D., Parr, R., and Venkataraman, S. (2003b).
    Efficient solution algorithms for factored mdps. Journal of Artificial Intelligence
    Research, 19, 399–468.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guestrin 等人 Guestrin, C., Koller, D., Parr, R., 和 Venkataraman, S. (2003b)。针对分解
    MDP 的高效解决算法。《人工智能研究杂志》，19，399–468。
- en: Guo et al. Guo, J., Gong, M., and Tao, D. (2022). A relational intervention
    approach for unsupervised dynamics generalization in model-based reinforcement
    learning. In Proceedings of the Ninth International Conference on Learning Representations
    (ICLR’21). OpenReview.net.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 Guo, J., Gong, M., 和 Tao, D. (2022)。一种用于模型基础强化学习中无监督动态泛化的关系干预方法。在第九届国际学习表征会议（ICLR’21）论文集。OpenReview.net。
- en: Guo et al. Guo, Z., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. Á.,
    Valko, M., Mesnard, T., Lattimore, T., and Munos, R. (2021). Geometric entropic
    exploration. CoRR, abs/2101.02055.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 Guo, Z., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. Á.,
    Valko, M., Mesnard, T., Lattimore, T., 和 Munos, R. (2021)。几何熵探索。CoRR, abs/2101.02055。
- en: Gupta et al. Gupta, A., Devin, C., Liu, Y., Abbeel, P., and Levine, S. (2017).
    Learning invariant feature spaces to transfer skills with reinforcement learning.
    In Proceedings of the Fifth International Conference on Learning Representations
    (ICLR’17).
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 Gupta, A., Devin, C., Liu, Y., Abbeel, P., 和 Levine, S. (2017)。学习不变特征空间以通过强化学习转移技能。在第五届国际学习表征会议（ICLR’17）论文集。
- en: Gupta et al. Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. (2018).
    Meta-reinforcement learning of structured exploration strategies.. In Bengio et al.
    (?).
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., 和 Levine, S. (2018)。结构化探索策略的元强化学习。在
    Bengio 等人（?）中。
- en: Gur et al. Gur, I., Jaques, N., Miao, Y., Choi, J., Tiwari, M., Lee, H., and Faust,
    A. (2021). Environment generation for zero-shot compositional reinforcement learning..
    In Ranzato et al. (?).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur 等人 Gur, I., Jaques, N., Miao, Y., Choi, J., Tiwari, M., Lee, H., 和 Faust,
    A. (2021)。用于零样本组合强化学习的环境生成。在 Ranzato 等人（?）中。
- en: Guyon et al. Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus, R.,
    Vishwanathan, S., and Garnett, R. (Eds.). (2017). Proceedings of the 30th International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’17).
    Curran Associates.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guyon 等人 Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
    S., 和 Garnett, R.（编）。(2017)。第 30 届神经信息处理系统国际会议（NeurIPS’17）论文集。Curran Associates。
- en: Haarnoja et al. Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a).
    Latent space policies for hierarchical reinforcement learning.. In Dy, and Krause
    (?).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja 等人 Haarnoja, T., Hartikainen, K., Abbeel, P., 和 Levine, S. (2018a)。用于层次强化学习的潜在空间策略。在
    Dy 和 Krause（?）中。
- en: Haarnoja, Pong, Zhou, Dalal, Abbeel, and Levine Haarnoja, T., Pong, V., Zhou,
    A., Dalal, M., Abbeel, P., and Levine, S. (2018b). Composable deep reinforcement
    learning for robotic manipulation. In 2018 IEEE International Conference on Robotics
    and Automation (ICRA’18).
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja, Pong, Zhou, Dalal, Abbeel 和 Levine Haarnoja, T., Pong, V., Zhou, A.,
    Dalal, M., Abbeel, P., 和 Levine, S. (2018b)。用于机器人操控的可组合深度强化学习。在 2018 年 IEEE 国际机器人与自动化会议（ICRA’18）上。
- en: 'Hafner, Lillicrap, Ba, and Norouzi Hafner, D., Lillicrap, T., Ba, J., and Norouzi,
    M. (2020). Dream to control: Learning behaviors by latent imagination.. In III, and Singh
    (?).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner, Lillicrap, Ba 和 Norouzi Hafner, D., Lillicrap, T., Ba, J., 和 Norouzi,
    M. (2020)。通过潜在想象控制：学习行为。在 III 和 Singh（?）中。
- en: Hafner, Pasukonis, Ba, and Lillicrap Hafner, D., Pasukonis, J., Ba, J., and Lillicrap,
    T. (2023). Mastering diverse domains through world models. In arXiv preprint arXiv:2301.04104.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner, Pasukonis, Ba 和 Lillicrap Hafner, D., Pasukonis, J., Ba, J., 和 Lillicrap,
    T. (2023)。通过世界模型掌握多样化领域。在 arXiv 预印本 arXiv:2301.04104 中。
- en: Hallak, Castro, and Mannor Hallak, A., Castro, D. D., and Mannor, S. (2015).
    Contextual markov decision processes. CoRR, abs/1502.02259.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hallak, Castro 和 Mannor Hallak, A., Castro, D. D., 和 Mannor, S. (2015)。上下文马尔可夫决策过程。CoRR,
    abs/1502.02259。
- en: Hansen-Estruch, Zhang, Nair, Yin, and Levine Hansen-Estruch, P., Zhang, A.,
    Nair, A., Yin, P., and Levine, S. (2022). Bisimulation makes analogies in goal-conditioned
    reinforcement learning.. In Chaudhuri et al. (?).
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen-Estruch, Zhang, Nair, Yin 和 Levine Hansen-Estruch, P., Zhang, A., Nair,
    A., Yin, P., 和 Levine, S. (2022)。双模拟在目标条件强化学习中的类比作用。在 Chaudhuri 等人（?）中。
- en: Harutyunyan, Dabney, Borsa, Heess, Munos, and Precup Harutyunyan, A., Dabney,
    W., Borsa, D., Heess, N., Munos, R., and Precup, D. (2019). The termination critic.
    In Chaudhuri, K.,  and Sugiyama, M. (Eds.), The 22nd International Conference
    on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha,
    Okinawa, Japan, Vol. 89 of Proceedings of Machine Learning Research, pp. 2231–2240\.
    PMLR.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harutyunyan, Dabney, Borsa, Heess, Munos 和 Precup Harutyunyan, A., Dabney, W.,
    Borsa, D., Heess, N., Munos, R., 和 Precup, D. (2019). 终止评论家。见 Chaudhuri, K., 和
    Sugiyama, M. (编), 第22届人工智能与统计国际会议，AISTATS 2019，2019年4月16-18日，冲绳县那霸市，日本，机器学习研究会议录第89卷，第2231–2240页。PMLR。
- en: Hausman, Springenberg, Wang, Heess, and Riedmiller Hausman, K., Springenberg,
    J., Wang, Z., Heess, N., and Riedmiller, M. (2018). Learning an embedding space
    for transferable robot skills. In Proceedings of the Sixth International Conference
    on Learning Representations (ICLR’18).
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausman, Springenberg, Wang, Heess 和 Riedmiller Hausman, K., Springenberg, J.,
    Wang, Z., Heess, N., 和 Riedmiller, M. (2018). 学习可转移机器人技能的嵌入空间。见第六届国际学习表示会议（ICLR’18）论文集。
- en: Hazan, Kakade, Singh, and van Soest Hazan, E., Kakade, S., Singh, K., and van
    Soest, A. (2019). Provably efficient maximum entropy exploration.. In Chaudhuri, and Salakhutdinov
    (?).
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hazan, Kakade, Singh 和 van Soest Hazan, E., Kakade, S., Singh, K., 和 van Soest,
    A. (2019). 可证明的高效最大熵探索。见 Chaudhuri 和 Salakhutdinov (?).
- en: Heess et al. Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller, M.,
    and Silver, D. (2016). Learning and transfer of modulated locomotor controllers.
    In arXiv preprint arXiv:1610.05182.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等人。Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller, M., 和 Silver,
    D. (2016). 学习和转移调制的运动控制器。见 arXiv 预印本 arXiv:1610.05182。
- en: Henaff et al. Henaff, M., Raileanu, R., Jiang, M., and Rocktäschel, T. (2022).
    Exploration via elliptical episodic bonuses. In neurips22.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henaff 等人。Henaff, M., Raileanu, R., Jiang, M., 和 Rocktäschel, T. (2022). 通过椭圆形情节奖励进行探索。见
    neurips22。
- en: 'Higgins et al. Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel,
    A., Botvinick, M., Blundell, C., and Lerchner, A. (2017). Darla: Improving zero-shot
    transfer in reinforcement learning.. In Precup, and Teh (?).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Higgins 等人。Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel,
    A., Botvinick, M., Blundell, C., 和 Lerchner, A. (2017). Darla: 改进强化学习中的零-shot
    转移。见 Precup 和 Teh (?).'
- en: Hofer Hofer, S. (2017). On Decomposability in Robot Reinforcement Learning.
    Technische University of Berlin (Germany).
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofer Hofer, S. (2017). 关于机器人强化学习中的可分解性。柏林工业大学（德国）。
- en: Hong et al. Hong, Z., Yang, G., and Agrawal, P. (2022). Bilinear value networks.
    CoRR, abs/2204.13695.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人。Hong, Z., Yang, G., 和 Agrawal, P. (2022). 双线性值网络。CoRR, abs/2204.13695.
- en: Hu and Montana Hu, Y.,  and Montana, G. (2019). Skill transfer in deep reinforcement
    learning under morphological heterogeneity. In arXiv preprint arXiv:1908.05265.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 和 Montana Hu, Y., 和 Montana, G. (2019). 在形态异质性下的深度强化学习技能转移。见 arXiv 预印本 arXiv:1908.05265。
- en: 'Huang et al. Huang, W., Mordatch, I., and Pathak, D. (2020). One policy to
    control them all: Shared modular policies for agent-agnostic control.. In III, and Singh
    (?).'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人。Huang, W., Mordatch, I., 和 Pathak, D. (2020). 一种策略控制所有：用于代理无关控制的共享模块化策略。见
    III 和 Singh (?).
- en: 'Hutter et al. Hutter, F., Kotthoff, L., and Vanschoren, J. (Eds.). (2019).
    Automated Machine Learning: Methods, Systems, Challenges. Springer. Available
    for free at http://automl.org/book.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutter 等人。Hutter, F., Kotthoff, L., 和 Vanschoren, J. (编). (2019). 自动化机器学习：方法、系统、挑战。Springer。可免费获取：http://automl.org/book。
- en: 'Icarte et al. Icarte, R., Klassen, T., Valenzano, R., and McIlraith, S. (2022).
    Reward machines: Exploiting reward function structure in reinforcement learning.
    J. Artif. Intell. Res., 73, 173–208.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Icarte 等人。Icarte, R., Klassen, T., Valenzano, R., 和 McIlraith, S. (2022). 奖励机器：利用奖励函数结构进行强化学习。J.
    Artif. Intell. Res., 73, 173–208.
- en: III and Singh III, H. D.,  and Singh, A. (Eds.). (2020). Proceedings of the
    37th International Conference on Machine Learning (ICML’20), Vol. 98\. Proceedings
    of Machine Learning Research.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: III 和 Singh III, H. D., 和 Singh, A. (编). (2020). 第37届国际机器学习会议（ICML’20）论文集，第98卷。机器学习研究会议录。
- en: Illanes et al. Illanes, L., Yan, X., Icarte, R., and McIlraith, S. (2020). Symbolic
    plans as high-level instructions for reinforcement learning. In Proceedings of
    the International Conference on Automated Planning and Scheduling.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Illanes 等人。Illanes, L., Yan, X., Icarte, R., 和 McIlraith, S. (2020). 符号计划作为强化学习的高级指令。见国际自动规划与调度会议论文集。
- en: Innes and Lascarides Innes, C.,  and Lascarides, A. (2020). Learning factored
    markov decision processes with unawareness. In Peters, J.,  and Sontag, D. (Eds.),
    Proceedings of The 36th Uncertainty in Artificial Intelligence Conference (UAI’20).
    PMLR.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Innes 和 Lascarides Innes, C., 和 Lascarides, A. (2020). 带有无意识的因子马尔可夫决策过程学习。载于
    Peters, J., 和 Sontag, D. (编)，第36届人工智能不确定性会议 (UAI’20) 论文集。PMLR。
- en: Islam et al. Islam, R., Zang, H., Goyal, A., Lamb, A., Kawaguchi, K., Li, X.,
    Laroche, R., Bengio, Y., and Combes, R. (2022). Discrete factorial representations
    as an abstraction for goal conditioned reinforcement learning. In arXiv preprint
    arXiv:2211.00247.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam 等 Islam, R., Zang, H., Goyal, A., Lamb, A., Kawaguchi, K., Li, X., Laroche,
    R., Bengio, Y., 和 Combes, R. (2022). 离散因子表示作为目标条件强化学习的抽象。载于 arXiv 预印本 arXiv:2211.00247。
- en: 'Jain et al. Jain, A., Khetarpal, K., and Precup, D. (2021a). Safe option-critic:
    Learning safety in the option-critic architecture. The Knowledge Engineering Review,
    36, e4.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等 Jain, A., Khetarpal, K., 和 Precup, D. (2021a). 安全选项-评论者：在选项-评论者架构中学习安全性。知识工程评论，36，e4。
- en: 'Jain et al. Jain, A., Kosaka, N., Kim, K., and Lim, J. (2021b). Know your action
    set: Learning action relations for reinforcement learning.. In Meila, and Zhang
    (?).'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等 Jain, A., Kosaka, N., Kim, K., 和 Lim, J. (2021b). 了解你的动作集：学习强化学习的动作关系。载于
    Meila 和 Zhang (?).
- en: Jain et al. Jain, A., Szot, A., and Lim, J. (2020). Generalization to new actions
    in reinforcement learning.. In III, and Singh (?).
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等 Jain, A., Szot, A., 和 Lim, J. (2020). 强化学习中新动作的泛化。载于 III 和 Singh (?).
- en: Janisch et al. Janisch, J., Pevnỳ, T., and Lisỳ, V. (2020). Symbolic relational
    deep reinforcement learning based on graph neural networks. In arXiv preprint
    arXiv:2009.12462.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janisch 等 Janisch, J., Pevnỳ, T., 和 Lisỳ, V. (2020). 基于图神经网络的符号关系深度强化学习。载于 arXiv
    预印本 arXiv:2009.12462。
- en: Javed Javed, K. (2023). The big world hypothesis and its ramifications on reinforcement
    learning..
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Javed Javed, K. (2023). 大世界假说及其对强化学习的影响。
- en: Jiang Jiang, N. (2018). Notes on state abstractions..
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang Jiang, N. (2018). 状态抽象的笔记。
- en: Jiang et al. Jiang, Y., Shane, S. G., Murphy, K., and Finn, C. (2019). Language
    as an abstraction for hierarchical deep reinforcement learning.. In Wallach et al.
    (?).
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 Jiang, Y., Shane, S. G., Murphy, K., 和 Finn, C. (2019). 语言作为层次化深度强化学习的抽象。载于
    Wallach 等 (?).
- en: Jiang, Gao, and Chen Jiang, Z., Gao, J., and Chen, J. (2022). Unsupervised skill
    discovery via recurrent skill training. In neurips22.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang, Gao 和 Chen Jiang, Z., Gao, J., 和 Chen, J. (2022). 通过递归技能训练的无监督技能发现。载于
    neurips22。
- en: Jonschkowski, Höfer, and Brock Jonschkowski, R., Höfer, S., and Brock, O. (2015).
    Patterns for learning with side information. In arXiv preprint arXiv:1511.06429.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jonschkowski, Höfer 和 Brock Jonschkowski, R., Höfer, S., 和 Brock, O. (2015).
    带有边信息的学习模式。载于 arXiv 预印本 arXiv:1511.06429。
- en: Joshi and Khardon Joshi, S.,  and Khardon, R. (2011). Probabilistic relational
    planning with first order decision diagrams. Journal of Artificial Intelligence
    Research, 41, 231–266.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 和 Khardon Joshi, S., 和 Khardon, R. (2011). 使用一阶决策图的概率关系规划。人工智能研究期刊，41，231–266。
- en: Kaiser, Otte, Runkler, and Ek Kaiser, M., Otte, C., Runkler, T., and Ek, C.
    (2019). Interpretable dynamics models for data-efficient reinforcement learning.
    In Proceedings of the 27th European Symposium on Artificial Neural Networks (ESANN’19).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser, Otte, Runkler 和 Ek Kaiser, M., Otte, C., Runkler, T., 和 Ek, C. (2019).
    用于数据高效强化学习的可解释动态模型。载于第27届欧洲人工神经网络研讨会 (ESANN’19) 论文集。
- en: Kakade Kakade, S. (2003). On the Sample Complexity of Reinforcement Learning.
    University of London, University College London (United Kingdom).
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kakade Kakade, S. (2003). 强化学习的样本复杂性。伦敦大学，伦敦大学学院（英国）。
- en: Kaplanis, Shanahan, and Clopath Kaplanis, C., Shanahan, M., and Clopath, C.
    (2019). Policy consolidation for continual reinforcement learning.. In Chaudhuri, and Salakhutdinov
    (?).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplanis, Shanahan 和 Clopath Kaplanis, C., Shanahan, M., 和 Clopath, C. (2019).
    连续强化学习中的策略整合。载于 Chaudhuri 和 Salakhutdinov (?).
- en: Karia and Srivastava Karia, R.,  and Srivastava, S. (2022). Relational abstractions
    for generalized reinforcement learning on symbolic problems. In arXiv preprint
    arXiv:2204.12665.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karia 和 Srivastava Karia, R., 和 Srivastava, S. (2022). 用于符号问题的广义强化学习的关系抽象。载于
    arXiv 预印本 arXiv:2204.12665。
- en: Kearns and Koller Kearns, M.,  and Koller, D. (1999). Efficient reinforcement
    learning in factored mdps. In Dean, T. (Ed.), Proceedings of the 16th International
    Joint Conference on Artificial Intelligence (IJCAI’99). Morgan Kaufmann Publishers.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kearns 和 Koller Kearns, M., 和 Koller, D. (1999). 在分解马尔可夫决策过程中高效的强化学习。载于 Dean,
    T. (编)，第16届国际人工智能联合会议论文集 (IJCAI’99)。摩根考夫曼出版社。
- en: Khamassi et al. Khamassi, M., Velentzas, G., Tsitsimis, T., and Tzafestas, C.
    (2017). Active exploration and parameterized reinforcement learning applied to
    a simulated human-robot interaction task. In First IEEE International Conference
    on Robotic Computing (IRC’17), pp. 28–35\. IEEE Computer Society.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khamassi et al. Khamassi, M., Velentzas, G., Tsitsimis, T., 和 Tzafestas, C.
    (2017). 应用于模拟人类-机器人交互任务的主动探索和参数化强化学习。在第一届 IEEE 国际机器人计算会议（IRC’17）上，pp. 28–35. IEEE
    计算机学会.
- en: Khetarpal et al. Khetarpal, K., Ahmed, Z., Comanici, G., Abel, D., and Precup,
    D. (2020). What can i do here? a theory of affordances in reinforcement learning..
    In III, and Singh (?).
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khetarpal et al. Khetarpal, K., Ahmed, Z., Comanici, G., Abel, D., 和 Precup,
    D. (2020). 我可以在这里做什么？强化学习中的可用性理论。在 III 和 Singh (?).
- en: Khetarpal et al. Khetarpal, K., Ahmed, Z., Comanici, G., and Precup, D. (2021).
    Temporally abstract partial models.. In Ranzato et al. (?).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khetarpal et al. Khetarpal, K., Ahmed, Z., Comanici, G., 和 Precup, D. (2021).
    时间抽象部分模型。在 Ranzato 等人 (?).
- en: 'Khetarpal et al. Khetarpal, K., Klissarov, M., Chevalier-Boisvert, M., Bacon,
    P., and Precup, D. (2020). Options of interest: Temporal abstraction with interest
    functions.. In Rossi et al. (?).'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khetarpal et al. Khetarpal, K., Klissarov, M., Chevalier-Boisvert, M., Bacon,
    P., 和 Precup, D. (2020). 关注的选项：具有兴趣函数的时间抽象。在 Rossi 等人 (?).
- en: Kim and Dean Kim, K.,  and Dean, T. (2002). Solving factored mdps with large
    action space using algebraic decision diagrams. In Trends in Artificial Intelligence.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Dean Kim, K., 和 Dean, T. (2002). 使用代数决策图解决大动作空间的因子化 MDP。在人工智能趋势中.
- en: Kipf et al. Kipf, T., van der Pol, E., and Welling, M. (2020). Contrastive learning
    of structured world models. In Proceedings of the Eighth International Conference
    on Learning Representations (ICLR’20).
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf et al. Kipf, T., van der Pol, E., 和 Welling, M. (2020). 结构化世界模型的对比学习。在第八届国际学习表示会议（ICLR’20）上.
- en: Kirillov et al. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson,
    L., Xiao, T., Whitehead, S., Berg, A., Lo, W., Dollár, P., and Girshick, R. (2023).
    Segment anything. CoRR, abs/2304.02643.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov et al. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson,
    L., Xiao, T., Whitehead, S., Berg, A., Lo, W., Dollár, P., 和 Girshick, R. (2023).
    分割一切。CoRR, abs/2304.02643.
- en: Kirk et al. Kirk, R., Zhang, A., Grefenstette, E., and Rocktäschel, T. (2023).
    A survey of zero-shot generalisation in deep reinforcement learning. Journal of
    Artificial Intelligence Research, 76, 201–264.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk et al. Kirk, R., Zhang, A., Grefenstette, E., 和 Rocktäschel, T. (2023).
    深度强化学习中的零样本泛化调查。人工智能研究杂志, 76, 201–264.
- en: Kirsch et al. Kirsch, L., van Steenkiste, S., and Schmidhuber, J. (2020). Improving
    generalization in meta reinforcement learning using learned objectives. In Proceedings
    of the Eighth International Conference on Learning Representations (ICLR’20).
    OpenReview.net.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirsch et al. Kirsch, L., van Steenkiste, S., 和 Schmidhuber, J. (2020). 使用学习目标改进元强化学习中的泛化。在第八届国际学习表示会议（ICLR’20）论文集中。OpenReview.net.
- en: Klissarov and Machado Klissarov, M.,  and Machado, M. (2023). Deep laplacian-based
    options for temporally-extended exploration. In Proceedings of the 40th International
    Conference on Machine Learning (ICML’23).
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klissarov 和 Machado Klissarov, M., 和 Machado, M. (2023). 基于深度拉普拉斯的时间扩展探索选项。在第40届国际机器学习大会（ICML’23）论文集中.
- en: 'Kokel et al. Kokel, H., Manoharan, A., Natarajan, S., Ravindran, B., and Tadepalli,
    P. (2021). Reprel: Integrating relational planning and reinforcement learning
    for effective abstraction. In Proceedings of the International Conference on Automated
    Planning and Scheduling.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kokel et al. Kokel, H., Manoharan, A., Natarajan, S., Ravindran, B., 和 Tadepalli,
    P. (2021). Reprel：将关系规划与强化学习集成以实现有效抽象。在国际自动化规划与调度会议论文集中.
- en: Koller and Parr Koller, D.,  and Parr, R. (1999). Computing factored value functions
    for policies in structured mdps. In IJCAI.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koller 和 Parr Koller, D., 和 Parr, R. (1999). 计算结构化 MDP 中策略的因子化值函数。在 IJCAI 上.
- en: Kooi et al. Kooi, J., Hoogendoorn, M., and François-Lavet, V. (2022). Disentangled
    (un)controllable features. CoRR, abs/2211.00086.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kooi et al. Kooi, J., Hoogendoorn, M., 和 François-Lavet, V. (2022). 解耦（不）可控特征。CoRR,
    abs/2211.00086.
- en: 'Kulkarni et al. Kulkarni, T., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
    (2016). Hierarchical deep reinforcement learning: Integrating temporal abstraction
    and intrinsic motivation. In Lee, D., Sugiyama, M., von Luxburg, U., Guyon, I.,
    and Garnett, R. (Eds.), Proceedings of the 29th International Conference on Advances
    in Neural Information Processing Systems (NeurIPS’16). Curran Associates.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni等人 Kulkarni, T., Narasimhan, K., Saeedi, A., 和 Tenenbaum, J.（2016）.
    分层深度强化学习：整合时间抽象和内在动机。在Lee, D., Sugiyama, M., von Luxburg, U., Guyon, I., 和 Garnett,
    R.（Eds.）所著的第29届国际神经信息处理系统大会（NeurIPS’16）论文集中。Curran Associates。
- en: Kumar et al. Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative
    q-learning for offline reinforcement learning.. In Larochelle et al. (?).
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar等人 Kumar, A., Zhou, A., Tucker, G., 和 Levine, S.（2020）. 用于离线强化学习的保守Q学习。在Larochelle等人（？）中。
- en: Kumar et al. Kumar, S., Correa, C., Dasgupta, I., Marjieh, R., Hu, M., Hawkins,
    R., Daw, N., Cohen, J., Narasimhan, K., and Griffiths, T. (2022). Using natural
    language and program abstractions to instill human inductive biases in machines.
    In Proceedings of the 35th International Conference on Advances in Neural Information
    Processing Systems (NeurIPS’22).
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar等人 Kumar, S., Correa, C., Dasgupta, I., Marjieh, R., Hu, M., Hawkins, R.,
    Daw, N., Cohen, J., Narasimhan, K., 和 Griffiths, T.（2022）. 用自然语言和程序抽象来灌输机器中人类归纳偏见。在第35届国际神经信息处理系统大会（NeurIPS’22）论文集中。
- en: Kumar et al. Kumar, S., Dasgupta, I., Cohen, J., Daw, N., and Griffiths, T.
    (2021). Meta-learning of structured task distributions in humans and machines.
    In Proceedings of the Ninth International Conference on Learning Representations
    (ICLR’21).
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar等人 Kumar, S., Dasgupta, I., Cohen, J., Daw, N., 和 Griffiths, T.（2021）.
    人类和机器中结构化任务分布的元学习。在第九届国际学习表示会议（ICLR’21）论文集中。
- en: 'Kwon et al. Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S. (2021). Rl
    for latent mdps: Regret guarantees and a lower bound.. In Ranzato et al. (?).'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon等人 Kwon, J., Efroni, Y., Caramanis, C., 和 Mannor, S.（2021）. 用于潜在MDP的强化学习：后悔保证和一个下限。在Ranzato等人（？）中。
- en: Lampinen et al. Lampinen, A., Roy, N., Dasgupta, I., Chan, S., Tam, A., Mcclelland,
    J., Yan, C., Santoro, A., Rabinowitz, N., J. Wang, J., and Hill, F. (2022). Tell
    me why! explanations support learning relational and causal structure.. In Chaudhuri
    et al. (?).
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lampinen等人 Lampinen, A., Roy, N., Dasgupta, I., Chan, S., Tam, A., Mcclelland,
    J., Yan, C., Santoro, A., Rabinowitz, N., J. Wang, J., 和 Hill, F.（2022）. 告诉我为什么！解释支持学习关系和因果结构。在Chaudhuri等人（？）中。
- en: 'Lan and Agarwal Lan, C.,  and Agarwal, R. (2023). Revisiting bisimulation:
    A sampling-based state similarity pseudo-metric. In The First Tiny Papers Track
    at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 和 Agarwal Lan, C.，和 Agarwal, R.（2023）. 重新审视双模拟：基于抽样的状态相似度伪度量。在2023年ICLR
    Tiny Papers @ ICLR第一届迷你论文赛特等论文，卢旺达基加利，5月5日。
- en: Lan, Bellemare, and Castro Lan, C., Bellemare, M., and Castro, P. (2021). Metrics
    and continuity in reinforcement learning.. In Yang et al. (?).
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan, Bellemare, 和 Castro Lan, C., Bellemare, M., 和 Castro, P.（2021）. 强化学习中的度量和连续性。在Yang等人（？）中。
- en: Lan, Mahmood, Yan, and Xu Lan, Q., Mahmood, A., Yan, S., and Xu, Z. (2023).
    Learning to optimize for reinforcement learning. CoRR, abs/2302.01470.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan, Mahmood, Yan, 和 Xu Lan, Q., Mahmood, A., Yan, S., 和 Xu, Z.（2023）. 为强化学习优化学习。CoRR,
    abs/2302.01470。
- en: Lang Lang, J. (Ed.). (2018). Proceedings of the 27th International Joint Conference
    on Artificial Intelligence (IJCAI’18).
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lang Lang, J.（Ed.）（2018）. 第27届国际人工智能联合会议（IJCAI’18）论文集中。
- en: Laroche and Feraud Laroche, R.,  and Feraud, R. (2022). Reinforcement learning
    algorithm selection. In Proceedings of the Sixth International Conference on Learning
    Representations (ICLR’22).
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laroche 和 Feraud Laroche, R.，和 Feraud, R.（2022）. 强化学习算法选择。在第六届国际学习表示会议（ICLR’22）论文集中。
- en: Larochelle, Ranzato, Hadsell, Balcan, and Lin Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M.-F., and Lin, H. (Eds.). (2020). Proceedings of the 33rd International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’20).
    Curran Associates.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Larochelle, Ranzato, Hadsell, Balcan, 和 Lin Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M.-F., 和 Lin, H.（Eds.）（2020）. 第33届国际神经信息处理系统大会（NeurIPS’20）论文集中。Curran
    Associates。
- en: 'Laskin, Yarats, Liu, Lee, Zhan, Lu, Cang, Pinto, and Abbeel Laskin, M., Yarats,
    D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. (2021).
    URLB: unsupervised reinforcement learning benchmark. In Vanschoren, J.,  and Yeung,
    S. (Eds.), Proceedings of the Neural Information Processing Systems Track on Datasets
    and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Laskin, Yarats, Liu, Lee, Zhan, Lu, Cang, Pinto 和 Abbeel Laskin, M., Yarats,
    D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., 和 Abbeel, P. (2021).
    URLB: 无监督强化学习基准。见 Vanschoren, J., 和 Yeung, S. (编辑), 神经信息处理系统数据集和基准轨道论文集 1, NeurIPS
    数据集和基准 2021, 2021年12月, 虚拟.'
- en: 'Lee, Nagabandi, Abbeel, and Levine Lee, A., Nagabandi, A., Abbeel, P., and Levine,
    S. (2020a). Stochastic latent actor-critic: Deep reinforcement learning with a
    latent variable model.. In Larochelle et al. (?).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee, Nagabandi, Abbeel 和 Levine Lee, A., Nagabandi, A., Abbeel, P., 和 Levine,
    S. (2020a). 随机潜在演员-评论家：带有潜在变量模型的深度强化学习。见 Larochelle 等人 (?).
- en: Lee et al. Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., and Hutter, M.
    (2020b). Learning quadrupedal locomotion over challenging terrain. Science in
    Robotics, 5.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., 和 Hutter, M. (2020b).
    在具有挑战性的地形上学习四足动物运动。科学机器人, 5.
- en: Lee et al. Lee, J., Sedwards, S., and Czarnecki, K. (2022). Recursive constraints
    to prevent instability in constrained reinforcement learning. In arXiv preprint
    arXiv:2201.07958.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 Lee, J., Sedwards, S., 和 Czarnecki, K. (2022). 递归约束以防止受限强化学习中的不稳定性。见
    arXiv 预印本 arXiv:2201.07958.
- en: Lee and Chung Lee, S.,  and Chung, S. (2021). Improving generalization in meta-rl
    with imaginary tasks from latent dynamics mixture.. In Ranzato et al. (?).
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 Chung Lee, S., 和 Chung, S. (2021). 通过潜在动态混合中的虚拟任务改善元强化学习中的泛化能力。见 Ranzato
    等人 (?).
- en: Li et al. Li, A., Spyra, O., Perel, S., Dalibard, V., Jaderberg, M., Gu, C.,
    Budden, D., Harley, T., and Gupta, P. (2019). A generalized framework for population
    based training. In Teredesai, A., Kumar, V., Li, Y., Rosales, R., Terzi, E., and Karypis,
    G. (Eds.), Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
    Discovery & Data Mining (KDD’19), p. 1791–1799\. ACM Press.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 Li, A., Spyra, O., Perel, S., Dalibard, V., Jaderberg, M., Gu, C., Budden,
    D., Harley, T., 和 Gupta, P. (2019). 一种通用的基于人群的训练框架。见 Teredesai, A., Kumar, V.,
    Li, Y., Rosales, R., Terzi, E., 和 Karypis, G. (编辑), 第25届 ACM SIGKDD 国际知识发现与数据挖掘会议
    (KDD’19) 论文集, 页码 1791–1799\. ACM Press.
- en: Li et al. Li, L., Walsh, T., and Littman, M. (2006). Towards a unified theory
    of state abstraction for mdps.. In AI&M.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 Li, L., Walsh, T., 和 Littman, M. (2006). 朝着 MDP 状态抽象的统一理论迈进。见 AI&M.
- en: 'Li et al. Li, T., Pan, J., Zhu, D., and Meng, M. (2018). Learning to interrupt:
    A hierarchical deep reinforcement learning framework for efficient exploration.
    In 2018 IEEE International Conference on Robotics and Biomimetics (ROBIO), pp. 648–653\.
    IEEE.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 Li, T., Pan, J., Zhu, D., 和 Meng, M. (2018). 学习中断：一种高效探索的层次化深度强化学习框架。见
    2018 IEEE 国际机器人与仿生学会议 (ROBIO), 页码 648–653\. IEEE.
- en: Li et al. Li, Y., Wu, Y., Xu, H., Wang, X., and Wu, Y. (2021). Solving compositional
    reinforcement learning problems via task reduction. In Proceedings of the Ninth
    International Conference on Learning Representations (ICLR’21).
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 Li, Y., Wu, Y., Xu, H., Wang, X., 和 Wu, Y. (2021). 通过任务缩减解决组合强化学习问题。见第九届国际学习表征会议
    (ICLR’21) 论文集.
- en: Liao et al. Liao, L., Fu, Z., Yang, Z., Wang, Y., Kolar, M., and Wang, Z. (2021).
    Instrumental variable value iteration for causal offline reinforcement learning.
    CoRR, abs/2102.09907.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等人 Liao, L., Fu, Z., Yang, Z., Wang, Y., Kolar, M., 和 Wang, Z. (2021).
    用于因果离线强化学习的工具变量价值迭代。CoRR, abs/2102.09907.
- en: Lipton Lipton, Z. (2018). The mythos of model interpretability. Commun. ACM,
    61(10), 36–43.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton Lipton, Z. (2018). 模型可解释性的神话。Commun. ACM, 61(10), 36–43.
- en: Lu et al. Lu, C., Kuba, J., Letcher, A., Metz, L., de Witt, C., and Foerster,
    J. (2022). Discovered policy optimisation. In neurips22.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 Lu, C., Kuba, J., Letcher, A., Metz, L., de Witt, C., 和 Foerster, J. (2022).
    发现策略优化。见 neurips22.
- en: Lu et al. Lu, K., Zhang, S., Stone, P., and Chen, X. (2018). Robot representation
    and reasoning with knowledge from reinforcement learning. CoRR, abs/1809.11074.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 Lu, K., Zhang, S., Stone, P., 和 Chen, X. (2018). 利用强化学习中的知识进行机器人表征和推理。CoRR,
    abs/1809.11074.
- en: Lu et al. Lu, M., Shahn, Z., Sow, D., Doshi-Velez, F., and Lehman, L. H. (2020).
    Is deep reinforcement learning ready for practical applications in healthcare?
    A sensitivity analysis of duel-ddqn for hemodynamic management in sepsis patients.
    In AMIA 2020, American Medical Informatics Association Annual Symposium, Virtual
    Event, USA, November 14-18, 2020. AMIA.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 Lu, M., Shahn, Z., Sow, D., Doshi-Velez, F., 和 Lehman, L. H. (2020). 深度强化学习是否为医疗实际应用做好了准备？对二重深度Q网络在脓毒症患者血流动力学管理中的敏感性分析。在
    AMIA 2020，美国医学信息学协会年会，虚拟活动，美国，2020年11月14-18日。AMIA。
- en: Luis et al. Luis, J., Miao, Y., Co-Reyes, J., Parisi, A., Tan, J., Real, E.,
    and Faust, A. (2022). Multi-objective evolution for generalizable policy gradient
    algorithms. CoRR, abs/2204.04292.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luis 等人 Luis, J., Miao, Y., Co-Reyes, J., Parisi, A., Tan, J., Real, E., 和 Faust,
    A. (2022). 用于通用策略梯度算法的多目标进化。CoRR, abs/2204.04292。
- en: 'Lyu et al. Lyu, D., Yang, F., Liu, B., and Gustafson, S. (2019). Sdrl: Interpretable
    and data-efficient deep reinforcement learning leveraging symbolic planning. In
    Hentenryck, P. V.,  and Zhou, Z. (Eds.), Proceedings of the Thirty-Third Conference
    on Artificial Intelligence (AAAI’19). AAAI Press.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu 等人 Lyu, D., Yang, F., Liu, B., 和 Gustafson, S. (2019). Sdrl：利用符号规划的可解释和数据高效的深度强化学习。在
    Hentenryck, P. V., 和 Zhou, Z. (编辑)，第三十三届人工智能会议（AAAI’19）论文集。AAAI Press。
- en: lyu et al. lyu, Y., Côme, A., Zhang, Y., and Talebi, M. (2023). Scaling up q-learning
    via exploiting state-action equivalence. Entropy, 25(4), 584.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lyu 等人 lyu, Y., Côme, A., Zhang, Y., 和 Talebi, M. (2023). 通过利用状态-动作等价性来扩展 Q
    学习。熵, 25(4), 584。
- en: 'Mahadevan and Maggioni Mahadevan, S.,  and Maggioni, M. (2007). Proto-value
    functions: A laplacian framework for learning representation and control in markov
    decision processes. J. Mach. Learn. Res., 8, 2169–2231.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahadevan 和 Maggioni Mahadevan, S., 和 Maggioni, M. (2007). 原型值函数：一种用于马尔可夫决策过程中的学习表示和控制的拉普拉斯框架。J.
    Mach. Learn. Res., 8, 2169–2231。
- en: Mahajan et al. Mahajan, A., Samvelyan, M., Mao, L., Makoviychuk, V., Garg, A.,
    Kossaifi, J., Whiteson, S., Zhu, Y., and Anandkumar, A. (2021). Reinforcement
    learning in factored action spaces using tensor decompositions. In arXiv preprint
    arXiv:2110.14538.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahajan 等人 Mahajan, A., Samvelyan, M., Mao, L., Makoviychuk, V., Garg, A., Kossaifi,
    J., Whiteson, S., Zhu, Y., 和 Anandkumar, A. (2021). 使用张量分解的分解动作空间中的强化学习。发表于 arXiv
    预印本 arXiv:2110.14538。
- en: Mahajan and Tulabandhula Mahajan, A.,  and Tulabandhula, T. (2017). Symmetry
    learning for function approximation in reinforcement learning. In arXiv preprint
    arXiv:1706.02999.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahajan 和 Tulabandhula Mahajan, A., 和 Tulabandhula, T. (2017). 强化学习中的函数逼近对称性学习。发表于
    arXiv 预印本 arXiv:1706.02999。
- en: Mambelli et al. Mambelli, D., Träuble, F., Bauer, S., Schölkopf, B., and Locatello,
    F. (2022). Compositional multi-object reinforcement learning with linear relation
    networks. In arXiv preprint arXiv:2201.13388.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mambelli 等人 Mambelli, D., Träuble, F., Bauer, S., Schölkopf, B., 和 Locatello,
    F. (2022). 具有线性关系网络的组合性多目标强化学习。发表于 arXiv 预印本 arXiv:2201.13388。
- en: Mankowitz et al. Mankowitz, D., Mann, T., and Mannor, S. (2015). Bootstrapping
    skills. In arXiv preprint arXiv:1506.03624.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mankowitz 等人 Mankowitz, D., Mann, T., 和 Mannor, S. (2015). 引导技能。发表于 arXiv 预印本
    arXiv:1506.03624。
- en: Mannor and Tamar Mannor, S.,  and Tamar, A. (2023). Towards deployable rl–what’s
    broken with rl research and a potential fix. In arXiv preprint arXiv:2301.01320.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mannor 和 Tamar Mannor, S., 和 Tamar, A. (2023). 迈向可部署的强化学习——强化学习研究中的问题及潜在解决方案。发表于
    arXiv 预印本 arXiv:2301.01320。
- en: Martinez et al. Martinez, D., Alenya, G., and Torras, C. (2017). Relational
    reinforcement learning with guided demonstrations. Artificial Intelligence, 247,
    295–312.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez 等人 Martinez, D., Alenya, G., 和 Torras, C. (2017). 带有引导演示的关系型强化学习。人工智能,
    247, 295–312。
- en: Marzi et al. Marzi, T., Khehra, A., Cini, A., and Alippi, C. (2023). Feudal
    graph reinforcement learning. CoRR, abs/2304.05099.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marzi 等人 Marzi, T., Khehra, A., Cini, A., 和 Alippi, C. (2023). 封建图强化学习。CoRR,
    abs/2304.05099。
- en: Mausam and Weld Mausam, D.,  and Weld, D. (2003). Solving relational mdps with
    first-order machine learning. In Proceedings of the ICAPS workshop on planning
    under uncertainty and incomplete information.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mausam 和 Weld Mausam, D., 和 Weld, D. (2003). 通过一阶机器学习解决关系型马尔可夫决策过程。在 ICAPS 规划不确定性和信息不完整研讨会的论文集中。
- en: Meila and Zhang Meila, M.,  and Zhang, T. (Eds.). (2021). Proceedings of the
    38th International Conference on Machine Learning (ICML’21), Vol. 139 of Proceedings
    of Machine Learning Research. PMLR.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meila 和 Zhang Meila, M., 和 Zhang, T. (编辑). (2021). 第38届国际机器学习大会（ICML’21）论文集，第139卷，机器学习研究论文集。PMLR。
- en: 'Mendez et al. Mendez, J., Hussing, M., Gummadi, M., and Eaton, E. (2022a).
    Composuite: A compositional reinforcement learning benchmark. In Chandar, S.,
    Pascanu, R., and Precup, D. (Eds.), Proceedings of the First Conference on Lifelong
    Learning Agents (CoLLAs’22), Vol. 199, pp. 982–1003\. PMLR.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendez等（Mendez, J., Hussing, M., Gummadi, M., 和Eaton, E.（2022a））。Composuite：一个组成性的强化学习基准。见Chandar,
    S., Pascanu, R., 和Precup, D.（编），《第一届终身学习代理会议论文集（CoLLAs’22）》，第199卷，第982–1003页。PMLR。
- en: Mendez et al. Mendez, J., van Seijen, H., and Eaton, E. (2022b). Modular lifelong
    reinforcement learning via neural composition. In Proceedings of the Tenth International
    Conference on Learning Representations (ICLR’22).
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendez等（Mendez, J., van Seijen, H., 和Eaton, E.（2022b））。通过神经组成实现模块化终身强化学习。见第十届国际学习表示大会（ICLR’22）论文集。
- en: Mendez et al. Mendez, J., Wang, B., and Eaton, E. (2020). Lifelong policy gradient
    learning of factored policies for faster training without forgetting.. In Larochelle
    et al. (?).
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendez等（Mendez, J., Wang, B., 和Eaton, E.（2020））。终身策略梯度学习因子化策略以加速训练且不遗忘。见Larochelle等（？）。
- en: Meng and Khushi Meng, T.,  and Khushi, M. (2019). Reinforcement learning in
    financial markets. Data, 4(3), 110.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng和Khushi（Meng, T., 和Khushi, M.（2019））。金融市场中的强化学习。《数据》，4（3），110。
- en: Metz et al. Metz, L., Ibarz, J., Jaitly, N., and Davidson, J. (2017). Discrete
    sequential prediction of continuous actions for deep rl. In arXiv preprint arXiv:1705.05035.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metz等（Metz, L., Ibarz, J., Jaitly, N., 和Davidson, J.（2017））。连续动作的离散序列预测用于深度强化学习。见arXiv预印本arXiv:1705.05035。
- en: 'Mihajlovic and Petkovic Mihajlovic, V.,  and Petkovic, M. (2001). Dynamic bayesian
    networks: A state of the art. In University of Twente Document Repository.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihajlovic和Petkovic（Mihajlovic, V., 和Petkovic, M.（2001））。动态贝叶斯网络：最新进展。见特温特大学文献库。
- en: Mirsky et al. Mirsky, R., Shperberg, S., Zhang, Y., Xu, Z., Jiang, Y., Cui,
    J., and Stone, P. (2022). Task factorization in curriculum learning. In Decision
    Awareness in Reinforcement Learning Workshop at ICML 2022.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirsky等（Mirsky, R., Shperberg, S., Zhang, Y., Xu, Z., Jiang, Y., Cui, J., 和Stone,
    P.（2022））。课程学习中的任务因子分解。见ICML 2022的强化学习决策意识研讨会。
- en: Misra et al. Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J. (2020).
    Kinematic state abstraction and provably efficient rich-observation reinforcement
    learning.. In III, and Singh (?).
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misra等（Misra, D., Henaff, M., Krishnamurthy, A., 和Langford, J.（2020））。运动状态抽象和可证明高效的丰富观测强化学习。见III和Singh（？）。
- en: Modi et al. Modi, A., Jiang, N., Singh, S., and Tewari, A. (2018). Markov decision
    processes with continuous side information. In Janoos, F., Mohri, M., and Sridharan,
    K. (Eds.), Algorithmic Learning Theory, ALT 2018, 7-9 April 2018, Lanzarote, Canary
    Islands, Spain, Vol. 83 of Proceedings of Machine Learning Research, pp. 597–618\.
    PMLR.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Modi等（Modi, A., Jiang, N., Singh, S., 和Tewari, A.（2018））。带有连续侧面信息的马尔可夫决策过程。见Janoos,
    F., Mohri, M., 和Sridharan, K.（编），《算法学习理论，ALT 2018，2018年4月7-9日，西班牙加纳利群岛兰萨罗特》，第83卷，机器学习研究论文集，第597–618页。PMLR。
- en: 'Moerland et al. Moerland, T., Broekens, J., Plaat, A., and Jonker, C. (2023).
    Model-based reinforcement learning: A survey. Found. Trends Mach. Learn., 16(1),
    1–118.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moerland等（Moerland, T., Broekens, J., Plaat, A., 和Jonker, C.（2023））。基于模型的强化学习：综述。《机器学习趋势基础》，16（1），1–118。
- en: Mohamed and Rezende Mohamed, S.,  and Rezende, D. (2015). Variational information
    maximisation for intrinsically motivated reinforcement learning. In Cortes, C.,
    Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (Eds.), Proceedings of the
    28th International Conference on Advances in Neural Information Processing Systems
    (NeurIPS’15). Curran Associates.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohamed和Rezende（Mohamed, S., 和Rezende, D.（2015））。用于内在激励的强化学习的变分信息最大化。见Cortes,
    C., Lawrence, N., Lee, D., Sugiyama, M., 和Garnett, R.（编），《第28届神经信息处理系统国际会议论文集（NeurIPS’15）》。Curran
    Associates。
- en: Mohan et al. Mohan, A., Benjamins, C., Wienecke, K., Dockhorn, A., and Lindauer,
    M. (2023). Autorl hyperparameter landscapes. In Faust, A., White, C., Hutter,
    F., Garnett, R., and Gardner, J. (Eds.), Proceedings of the Second International
    Conference on Automated Machine Learning. Proceedings of Machine Learning Research.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohan等（Mohan, A., Benjamins, C., Wienecke, K., Dockhorn, A., 和Lindauer, M.（2023））。Autorl超参数景观。见Faust,
    A., White, C., Hutter, F., Garnett, R., 和Gardner, J.（编），《第二届自动化机器学习国际会议论文集》。机器学习研究论文集。
- en: Mu et al. Mu, J., Zhong, V., Raileanu, R., Jiang, M., Goodman, N., Rocktäschel,
    T., and Grefenstette, E. (2022a). Improving intrinsic exploration with language
    abstractions. In Proceedings of the 35th International Conference on Advances
    in Neural Information Processing Systems (NeurIPS’22).
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu等 Mu, J., Zhong, V., Raileanu, R., Jiang, M., Goodman, N., Rocktäschel, T.,
    和 Grefenstette, E. (2022a). 利用语言抽象提升内在探索。收录于第35届神经信息处理系统国际会议论文集 (NeurIPS’22).
- en: Mu et al. Mu, T., Lin, K., Niu, F., and Thattai, G. (2022b). Learning two-step
    hybrid policy for graph-based interpretable reinforcement learning. Trans. Mach.
    Learn. Res., 2022.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu等 Mu, T., Lin, K., Niu, F., 和 Thattai, G. (2022b). 学习两步混合策略用于基于图的可解释强化学习。Trans.
    Mach. Learn. Res., 2022.
- en: Mutti et al. Mutti, M., Mancassola, M., and Restelli, M. (2022). Unsupervised
    reinforcement learning in multiple environments.. In Sycara et al. (?).
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mutti等 Mutti, M., Mancassola, M., 和 Restelli, M. (2022). 多环境中的无监督强化学习。收录于Sycara等
    (?).
- en: Mutti, Pratissoli, and Restelli Mutti, M., Pratissoli, L., and Restelli, M.
    (2021). Task-agnostic exploration via policy gradient of a non-parametric state
    entropy estimate.. In Yang et al. (?).
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mutti, Pratissoli, 和 Restelli Mutti, M., Pratissoli, L., 和 Restelli, M. (2021).
    通过非参数状态熵估计的策略梯度进行任务无关探索。收录于Yang等 (?).
- en: Nachum et al. Nachum, O., Shane, S. G., Lee, H., and Levine, S. (2018). Data-efficient
    hierarchical reinforcement learning.. In Bengio et al. (?).
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nachum等 Nachum, O., Shane, S. G., Lee, H., 和 Levine, S. (2018). 数据高效的层次化强化学习。收录于Bengio等
    (?).
- en: Nam, Sun, Pertsch, Hwang, and Lim Nam, T., Sun, S., Pertsch, K., Hwang, S. J.,
    and Lim, J. (2022). Skill-based meta-reinforcement learning. In Proceedings of
    the Tenth International Conference on Learning Representations (ICLR’22). OpenReview.net.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nam, Sun, Pertsch, Hwang, 和 Lim Nam, T., Sun, S., Pertsch, K., Hwang, S. J.,
    和 Lim, J. (2022). 基于技能的元强化学习。收录于《第十届学习表征国际会议论文集 (ICLR’22)》。OpenReview.net.
- en: Narvekar, Sinapov, Leonetti, and Stone Narvekar, S., Sinapov, J., Leonetti,
    M., and Stone, P. (2016). Source task creation for curriculum learning. In Jonker,
    C., Marsella, S., Thangarajah, J., and Tuyls, K. (Eds.), Proceedings of the International
    Conference on Autonomous Agents & Multiagent Systems (AAMAS’16), pp. 566–574\.
    ACM.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narvekar, Sinapov, Leonetti, 和 Stone Narvekar, S., Sinapov, J., Leonetti, M.,
    和 Stone, P. (2016). 用于课程学习的源任务创建。收录于Jonker, C., Marsella, S., Thangarajah, J.,
    和 Tuyls, K. (编), 《国际自主代理与多智能体系统会议论文集 (AAMAS’16)》，第566–574页. ACM.
- en: 'Ng, Harada, and Russell Ng, A., Harada, D., and Russell, S. (1999). Policy
    invariance under reward transformations: Theory and application to reward shaping.
    In Bratko, I. (Ed.), Proceedings of the Sixteenth International Conference on
    Machine Learning (ICML’99). Morgan Kaufmann Publishers.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng, Harada, 和 Russell Ng, A., Harada, D., 和 Russell, S. (1999). 奖励变换下的策略不变性：理论及其在奖励塑造中的应用。收录于Bratko,
    I. (编), 《第十六届国际机器学习会议论文集 (ICML’99)》。Morgan Kaufmann Publishers.
- en: Oh, Hessel, Czarnecki, Xu, van Hasselt, Singh, and Silver Oh, J., Hessel, M.,
    Czarnecki, W., Xu, Z., van Hasselt, H., Singh, S., and Silver, D. (2020). Discovering
    reinforcement learning algorithms.. In Larochelle et al. (?).
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh, Hessel, Czarnecki, Xu, van Hasselt, Singh, 和 Silver Oh, J., Hessel, M.,
    Czarnecki, W., Xu, Z., van Hasselt, H., Singh, S., 和 Silver, D. (2020). 发现强化学习算法。收录于Larochelle等
    (?).
- en: Ok et al. Ok, J., Proutière, A., and Tranos, D. (2018). Exploration in structured
    reinforcement learning.. In Bengio et al. (?).
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ok等 Ok, J., Proutière, A., 和 Tranos, D. (2018). 结构化强化学习中的探索。收录于Bengio等 (?).
- en: Oliva et al. Oliva, M., Banik, S., Josifovski, J., and Knoll, A. (2022). Graph
    neural networks for relational inductive bias in vision-based deep reinforcement
    learning of robot control. In International Joint Conference on Neural Networks,
    IJCNN 2022, Padua, Italy, July 18-23, 2022, pp. 1–9\. IEEE.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oliva等 Oliva, M., Banik, S., Josifovski, J., 和 Knoll, A. (2022). 用于视觉深度强化学习中机器控制的图神经网络。收录于2022年神经网络国际联合会议
    (IJCNN 2022), 意大利帕多瓦, 2022年7月18-23日，第1–9页. IEEE.
- en: OpenAI OpenAI (2023). GPT-4 technical report. CoRR, abs/2303.08774.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI OpenAI (2023). GPT-4技术报告。CoRR, abs/2303.08774.
- en: 'Papini et al. Papini, M., Tirinzoni, A., Pacchiano, A., Restelli, M., Lazaric,
    A., and Pirotta, M. (2021). Reinforcement learning in linear mdps: Constant regret
    and representation selection.. In Ranzato et al. (?).'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papini等 Papini, M., Tirinzoni, A., Pacchiano, A., Restelli, M., Lazaric, A.,
    和 Pirotta, M. (2021). 在线性马尔可夫决策过程中的强化学习：常量遗憾和表示选择。收录于Ranzato等 (?).
- en: Parker-Holder et al. Parker-Holder, J., Nguyen, V., and Roberts, S. J. (2020).
    Provably efficient online Hyperparameter Optimization with population-based bandits..
    In Larochelle et al. (?).
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parker-Holder等 Parker-Holder, J., Nguyen, V., 和 Roberts, S. J. (2020). 具有基于人群的赌徒的可证明高效在线超参数优化。收录于Larochelle等
    (?).
- en: 'Parker-Holder et al. Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A.,
    Miao, Y., Eimer, T., Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F.,
    and Lindauer, M. (2022). Automated reinforcement learning (AutoRL): A survey and
    open problems. Journal of Artificial Intelligence Research (JAIR), 74, 517–568.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parker-Holder 等人：Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A., Miao,
    Y., Eimer, T., Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F., 和 Lindauer,
    M. (2022)。自动化强化学习（AutoRL）：调查与开放问题。《人工智能研究杂志（JAIR）》，74，517–568.
- en: Parr and Russell Parr, R.,  and Russell, S. (1997). Reinforcement learning with
    hierarchies of machines. In Proceedings of the Tenth International Conference
    on Advances in Neural Information Processing Systems (NeurIPS’97).
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parr 和 Russell：Parr, R., 和 Russell, S. (1997)。具有机器层次的强化学习。见第十届神经信息处理系统国际会议论文集（NeurIPS’97）。
- en: 'Pateria et al. Pateria, S., Subagdja, B., Tan, A., and Quek, C. (2022). Hierarchical
    reinforcement learning: A comprehensive survey. ACM Computing Surveys, 54(5),
    109:1–109:35.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pateria 等人：Pateria, S., Subagdja, B., Tan, A., 和 Quek, C. (2022)。层次强化学习：综合调查。ACM计算调查，54(5)，109:1–109:35.
- en: Pathak et al. Pathak, D., Agrawal, P., Efros, A., and Darrell, T. (2017). Curiosity-driven
    exploration by self-supervised prediction.. In Precup, and Teh (?).
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak 等人：Pathak, D., Agrawal, P., Efros, A., 和 Darrell, T. (2017)。由自监督预测驱动的好奇探索..
    见 Precup 和 Teh (?).
- en: 'Pathak, Lu, Darrell, Isola, and Efros Pathak, D., Lu, C., Darrell, T., Isola,
    P., and Efros, A. (2019). Learning to control self-assembling morphologies: a
    study of generalization via modularity.. In Wallach et al. (?).'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak, Lu, Darrell, Isola 和 Efros：Pathak, D., Lu, C., Darrell, T., Isola, P.,
    和 Efros, A. (2019)。学习控制自组装形态：通过模块化研究泛化.. 见 Wallach 等人 (?).
- en: Payani and Fekri Payani, A.,  and Fekri, F. (2020). Incorporating relational
    background knowledge into reinforcement learning via differentiable inductive
    logic programming. CoRR, abs/2003.10386.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Payani 和 Fekri：Payani, A., 和 Fekri, F. (2020)。通过可微分的归纳逻辑编程将关系背景知识融入强化学习。CoRR,
    abs/2003.10386.
- en: 'Peng et al. Peng, X., Chang, M., Zhang, G., Abbeel, P., and Levine, S. (2019).
    MCP: learning composable hierarchical control with multiplicative compositional
    policies.. In Wallach et al. (?).'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人：Peng, X., Chang, M., Zhang, G., Abbeel, P., 和 Levine, S. (2019)。MCP：通过乘法组合策略学习可组合的层次控制..
    见 Wallach 等人 (?).
- en: Perez et al. Perez, C., Such, F., and Karaletsos, T. (2020). Generalized hidden
    parameter mdps transferable model-based rl in a handful of trials.. In Rossi et al.
    (?).
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等人：Perez, C., Such, F., 和 Karaletsos, T. (2020)。广义隐藏参数 MDPS 可迁移的基于模型的
    RL 只需少量试验.. 见 Rossi 等人 (?).
- en: 'Peters, Buhlmann, and Meinshausen Peters, J., Buhlmann, P., and Meinshausen,
    N. (2016). Causal inference by using invariant prediction: identification and
    confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical
    Methodology), 78(5), 947–1012.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters, Buhlmann 和 Meinshausen：Peters, J., Buhlmann, P., 和 Meinshausen, N. (2016)。通过使用不变预测进行因果推断：识别和置信区间。《皇家统计学会杂志：B系列（统计方法学）》，78(5)，947–1012.
- en: Pitis, Creager, and Garg Pitis, S., Creager, E., and Garg, A. (2020). Counterfactual
    data augmentation using locally factored dynamics.. In Larochelle et al. (?).
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pitis, Creager 和 Garg：Pitis, S., Creager, E., 和 Garg, A. (2020)。利用局部因子化动力学的反事实数据增强..
    见 Larochelle 等人 (?).
- en: Prakash et al. Prakash, B., Waytowich, N., Ganesan, A., Oates, T., and Mohsenin,
    T. (2020). Guiding safe reinforcement learning policies using structured language
    constraints. In Espinoza, H., Hernández-Orallo, J., Chen, X. C., ÓhÉigeartaigh,
    S. S., Huang, X., Castillo-Effen, M., Mallah, R., and McDermid, J. A. (Eds.),
    Proceedings of the Workshop on Artificial Intelligence Safety, co-located with
    34th AAAI Conference on Artificial Intelligence, SafeAI@AAAI 2020, New York City,
    NY, USA, February 7, 2020, Vol. 2560 of CEUR Workshop Proceedings, pp. 153–161\.
    CEUR-WS.org.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prakash 等人：Prakash, B., Waytowich, N., Ganesan, A., Oates, T., 和 Mohsenin, T.
    (2020)。通过结构化语言约束引导安全强化学习策略。见 Espinoza, H., Hernández-Orallo, J., Chen, X. C.,
    ÓhÉigeartaigh, S. S., Huang, X., Castillo-Effen, M., Mallah, R., 和 McDermid, J.
    A. (编辑)，《人工智能安全研讨会论文集》，与第34届AAAI人工智能会议同期举办，SafeAI@AAAI 2020，纽约市，美国，2020年2月7日，CEUR
    Workshop Proceedings第2560卷，第153–161页。CEUR-WS.org.
- en: Prakash et al. Prakash, B., Waytowich, N., Oates, T., and Mohsenin, T. (2022).
    Towards an interpretable hierarchical agent framework using semantic goals. CoRR,
    abs/2210.08412.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prakash 等人：Prakash, B., Waytowich, N., Oates, T., 和 Mohsenin, T. (2022)。朝着一个可解释的层次代理框架使用语义目标。CoRR,
    abs/2210.08412.
- en: Precup and Teh Precup, D.,  and Teh, Y. (Eds.). (2017). Proceedings of the 34th
    International Conference on Machine Learning (ICML’17), Vol. 70\. Proceedings
    of Machine Learning Research.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Precup 和 Teh Precup, D., 和 Teh, Y. (编). (2017). 第三十四届机器学习国际会议论文集（ICML’17），第
    70 卷。机器学习研究论文集。
- en: 'Prudencio et al. Prudencio, R., Máximo, M., and Colombini, E. (2022). A survey
    on offline reinforcement learning: Taxonomy, review, and open problems. CoRR,
    abs/2203.01387.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prudencio 等人 Prudencio, R., Máximo, M., 和 Colombini, E. (2022). 离线强化学习调查：分类、综述及开放问题。CoRR,
    abs/2203.01387。
- en: 'Puterman Puterman, M. (2014). Markov decision processes: discrete stochastic
    dynamic programming. John Wiley & Sons.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puterman Puterman, M. (2014). 马尔可夫决策过程：离散随机动态规划。John Wiley & Sons。
- en: Ranzato et al. Ranzato, M., Beygelzimer, A., Nguyen, K., Liang, P., Vaughan,
    J., and Dauphin, Y. (Eds.). (2021). Proceedings of the 34th International Conference
    on Advances in Neural Information Processing Systems (NeurIPS’21). Curran Associates.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ranzato 等人 Ranzato, M., Beygelzimer, A., Nguyen, K., Liang, P., Vaughan, J.,
    和 Dauphin, Y. (编). (2021). 第三十四届神经信息处理系统国际会议论文集（NeurIPS’21）。Curran Associates。
- en: Raza and Lin Raza, S.,  and Lin, M. (2019). Policy reuse in reinforcement learning
    for modular agents. In IEEE 2nd International Conference on Information and Computer
    Technologies (ICICT). IEEE.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raza 和 Lin Raza, S., 和 Lin, M. (2019). 模块化代理的强化学习中的政策重用。在 IEEE 第二届国际信息与计算机技术会议（ICICT）。IEEE。
- en: Ross and Pineau Ross, S.,  and Pineau, J. (2008). Model-based bayesian reinforcement
    learning in large structured domains. In McAllester, D. A.,  and Myllymäki, P.
    (Eds.), UAI 2008, Proceedings of the 24th Conference in Uncertainty in Artificial
    Intelligence, Helsinki, Finland, July 9-12, 2008, pp. 476–483\. AUAI Press.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 和 Pineau Ross, S., 和 Pineau, J. (2008). 大型结构化领域中的基于模型的贝叶斯强化学习。在 McAllester,
    D. A., 和 Myllymäki, P. (编), UAI 2008，第 24 届不确定性人工智能会议论文集，赫尔辛基，芬兰，2008年7月9-12日，页
    476–483。AUAI Press。
- en: Rossi et al. Rossi, F., Conitzer, V., and Sha, F. (Eds.). (2020). Proceedings
    of the Thirty-Fourth Conference on Artificial Intelligence (AAAI’20). Association
    for the Advancement of Artificial Intelligence, AAAI Press.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rossi 等人 Rossi, F., Conitzer, V., 和 Sha, F. (编). (2020). 第三十四届人工智能会议论文集（AAAI’20）。人工智能进步协会，AAAI
    Press。
- en: Rusu et al. Rusu, A., Colmenarejo, S., Gülçehre, C., Desjardins, G., Kirkpatrick,
    J., Pascanu, R., Mnih, V., Kavukcuoglu, K., and Hadsell, R. (2016). Policy distillation.
    In Proceedings of Fourth International Conference on Learning Representations
    (ICLR’16).
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusu 等人 Rusu, A., Colmenarejo, S., Gülçehre, C., Desjardins, G., Kirkpatrick,
    J., Pascanu, R., Mnih, V., Kavukcuoglu, K., 和 Hadsell, R. (2016). 策略蒸馏。在第四届国际学习表征会议论文集（ICLR’16）。
- en: Salimans et al. Salimans, T., Ho, J., Chen, X., and Sutskever, I. (2017). Evolution
    strategies as a scalable alternative to reinforcement learning. CoRR, abs/1703.03864.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans 等人 Salimans, T., Ho, J., Chen, X., 和 Sutskever, I. (2017). 演化策略作为一种可扩展的强化学习替代方法。CoRR,
    abs/1703.03864。
- en: Sanner and Boutilier Sanner, S.,  and Boutilier, C. (2012). Approximate linear
    programming for first-order mdps. In arXiv preprint arXiv:1207.1415.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanner 和 Boutilier Sanner, S., 和 Boutilier, C. (2012). 针对一阶马尔可夫决策过程的近似线性规划。在
    arXiv 预印本 arXiv:1207.1415。
- en: Saxe et al. Saxe, A., Earle, A., and Rosman, B. (2017). Hierarchy through composition
    with multitask lmdps.. In Precup, and Teh (?).
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxe 等人 Saxe, A., Earle, A., 和 Rosman, B. (2017). 通过组合实现的层次结构与多任务 LMDPs。在 Precup
    和 Teh (？)。
- en: Schaul et al. Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal
    value function approximators. In Bach, F.,  and Blei, D. (Eds.), International
    conference on machine learning, Vol. 37\. Omnipress.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul 等人 Schaul, T., Horgan, D., Gregor, K., 和 Silver, D. (2015). 通用价值函数逼近器。在
    Bach, F., 和 Blei, D. (编), 国际机器学习会议, 第 37 卷。Omnipress。
- en: Schiewer and Wiskott Schiewer, R.,  and Wiskott, L. (2021). Modular networks
    prevent catastrophic interference in model-based multi-task reinforcement learning.
    In Proceedings of the Seventh International Conference on Machine Learning, Optimization,
    and Data Science (LOD’21), Vol. 13164 of Lecture Notes in Computer Science, pp. 299–313\.
    Springer.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schiewer 和 Wiskott Schiewer, R., 和 Wiskott, L. (2021). 模块化网络在基于模型的多任务强化学习中防止灾难性干扰。在第七届机器学习、优化与数据科学国际会议论文集（LOD’21），第
    13164 卷计算机科学讲义笔记，页 299–313。Springer。
- en: Seitzer et al. Seitzer, M., Schölkopf, B., and Martius, G. (2021). Causal influence
    detection for improving efficiency in reinforcement learning.. In Ranzato et al.
    (?).
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seitzer 等人 Seitzer, M., Schölkopf, B., 和 Martius, G. (2021). 用于提升强化学习效率的因果影响检测。在
    Ranzato 等人 (？)。
- en: Shanahan et al. Shanahan, M., Nikiforou, K., Creswell, A., Kaplanis, C., Barrett,
    D., and Garnelo, M. (2020). An explicitly relational neural network architecture.
    In icml20.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanahan 等人 Shanahan, M., Nikiforou, K., Creswell, A., Kaplanis, C., Barrett,
    D., 和 Garnelo, M. (2020). 一种明确的关系神经网络架构。发表于 icml20。
- en: Sharma et al. Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. (2020).
    Dynamics-aware unsupervised discovery of skills. In Proceedings of the Eighth
    International Conference on Learning Representations (ICLR’20). OpenReview.net.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 Sharma, A., Gu, S., Levine, S., Kumar, V., 和 Hausman, K. (2020). 动态感知的无监督技能发现。发表于第八届国际学习表征会议（ICLR’20）会议论文集。OpenReview.net。
- en: 'Sharma et al. Sharma, V., Arora, D., Geisser, F., Mausam, A., and Singla, P.
    (2022). Symnet 2.0: Effectively handling non-fluents and actions in generalized
    neural policies for rddl relational mdps. In Uncertainty in Artificial Intelligence,
    pp. 1771–1781. PMLR.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 Sharma, V., Arora, D., Geisser, F., Mausam, A., 和 Singla, P. (2022).
    Symnet 2.0：在 RDDL 关系 MDPS 中有效处理非流动性和动作。发表于人工智能中的不确定性，第1771–1781页。PMLR。
- en: Shu et al. Shu, T., Xiong, C., and Socher, R. (2018). Hierarchical and interpretable
    skill acquisition in multi-task reinforcement learning. In Proceedings of the
    Sixth International Conference on Learning Representations (ICLR’18).
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等人 Shu, T., Xiong, C., 和 Socher, R. (2018). 多任务强化学习中的层次化和可解释的技能获取。发表于第六届国际学习表征会议（ICLR’18）会议论文集。
- en: Shyam et al. Shyam, P., Jaskowski, W., and Gomez, F. (2019). Model-based active
    exploration.. In Chaudhuri, and Salakhutdinov (?).
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shyam 等人 Shyam, P., Jaskowski, W., 和 Gomez, F. (2019). 基于模型的主动探索。发表于 Chaudhuri
    和 Salakhutdinov (?)。
- en: Silver et al. Silver, D., Huang, A., Maddison, C., Guez, A., Sifre, L., Driessche,
    G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,
    S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach,
    M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. (2016). Mastering the game
    of go with deep neural networks and tree search. Nature, 529(7587), 484–489.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等人 Silver, D., Huang, A., Maddison, C., Guez, A., Sifre, L., Driessche,
    G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,
    S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach,
    M., Kavukcuoglu, K., Graepel, T., 和 Hassabis, D. (2016). 利用深度神经网络和树搜索掌握围棋。自然，529(7587)，484–489。
- en: 'Simao et al. Simao, T., Jansen, N., and Spaan, M. (2021). Alwayssafe: Reinforcement
    learning without safety constraint violations during training. In Proceedings
    of the 20th International Conference on Autonomous Agents and MultiAgent Systems.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simao 等人 Simao, T., Jansen, N., 和 Spaan, M. (2021). Alwayssafe：在训练过程中无安全约束违反的强化学习。发表于第20届国际自主代理和多代理系统会议。
- en: Singh et al. Singh, G., Peri, S. V., Kim, J., Kim, H., and Ahn, S. (2021). Structured
    world belief for reinforcement learning in POMDP.. In Meila, and Zhang (?).
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人 Singh, G., Peri, S. V., Kim, J., Kim, H., 和 Ahn, S. (2021). POMDP 中的结构化世界信念用于强化学习。发表于
    Meila 和 Zhang (?)。
- en: 'Sodhani et al. Sodhani, S., Levine, S., and Zhang, A. (2022a). Improving generalization
    with approximate factored value functions. In ICLR2022 Workshop on the Elements
    of Reasoning: Objects, Structure and Causality.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sodhani 等人 Sodhani, S., Levine, S., 和 Zhang, A. (2022a). 通过近似分解值函数改善泛化能力。发表于
    ICLR2022 工作坊：推理的元素：对象、结构和因果关系。
- en: Sodhani et al. Sodhani, S., Meier, F., Pineau, J., and Zhang, A. (2022b). Block
    contextual mdps for continual learning. In Learning for Dynamics and Control Conference.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sodhani 等人 Sodhani, S., Meier, F., Pineau, J., 和 Zhang, A. (2022b). 用于持续学习的块上下文
    MDPS。发表于动态和控制学习会议。
- en: Sodhani et al. Sodhani, S., Zhang, A., and Pineau, J. (2021). Multi-task reinforcement
    learning with context-based representations.. In Meila, and Zhang (?).
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sodhani 等人 Sodhani, S., Zhang, A., 和 Pineau, J. (2021). 基于上下文的多任务强化学习。发表于 Meila
    和 Zhang (?)。
- en: Sohn et al. Sohn, S., Oh, J., and Lee, H. (2018). Hierarchical reinforcement
    learning for zero-shot generalization with subtask dependencies.. In Bengio et al.
    (?).
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn 等人 Sohn, S., Oh, J., 和 Lee, H. (2018). 用于零样本泛化的层次化强化学习及其子任务依赖性。发表于 Bengio
    等人 (?)。
- en: Sohn et al. Sohn, S., Woo, H., Choi, J., and Lee, H. (2020). Meta reinforcement
    learning with autonomous inference of subtask dependencies. In Proceedings of
    the Eighth International Conference on Learning Representations, (ICLR’20). OpenReview.net.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn 等人 Sohn, S., Woo, H., Choi, J., 和 Lee, H. (2020). 具有自主推断子任务依赖的元强化学习。发表于第八届国际学习表征会议（ICLR’20）会议论文集。OpenReview.net。
- en: Solway et al. Solway, A., Diuk, C., Córdova, N., Yee, D., Barto, A., Niv, Y.,
    and Botvinick, M. (2014). Optimal behavioral hierarchy. PLoS Comput. Biol., 10(8).
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Solway 等人 Solway, A., Diuk, C., Córdova, N., Yee, D., Barto, A., Niv, Y., 和
    Botvinick, M. (2014). 最优行为层级。PLoS 计算生物学，10(8)。
- en: 'Song et al. Song, Y., Suganthan, P., Pedrycz, W., Ou, J., He, Y., and Chen,
    Y. (2023). Ensemble reinforcement learning: A survey. CoRR, abs/2303.02618.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 Song, Y., Suganthan, P., Pedrycz, W., Ou, J., He, Y., 和 Chen, Y. (2023).
    集成强化学习：综述。CoRR, abs/2303.02618。
- en: 'Spooner et al. Spooner, T., Vadori, N., and Ganesh, S. (2021). Factored policy
    gradients: Leveraging structure for efficient learning in momdps.. In Ranzato
    et al. (?).'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spooner 等 Spooner, T., Vadori, N., 和 Ganesh, S. (2021). 分解策略梯度：利用结构进行高效学习。在
    Ranzato 等（？）。
- en: Srinivas and Abbeel Srinivas, A.,  and Abbeel, P. (2021). Unsupervised Learning
    for Reinforcement Learning..
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivas 和 Abbeel Srinivas, A., 和 Abbeel, P. (2021). 用于强化学习的无监督学习。
- en: Srouji et al. Srouji, M., Zhang, J., and Salakhutdinov, R. (2018). Structured
    control nets for deep reinforcement learning.. In Dy, and Krause (?).
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srouji 等 Srouji, M., Zhang, J., 和 Salakhutdinov, R. (2018). 用于深度强化学习的结构化控制网络。在
    Dy 和 Krause（？）。
- en: Steccanella et al. Steccanella, L., Totaro, S., and Jonsson, A. (2021). Hierarchical
    representation learning for markov decision processes. In arXiv preprint arXiv:2106.01655.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Steccanella 等 Steccanella, L., Totaro, S., 和 Jonsson, A. (2021). 马尔可夫决策过程的层次表示学习。在arXiv预印本arXiv:2106.01655。
- en: 'Sun et al. Sun, Y., Ma, S., Madaan, R., Bonatti, R., Huang, F., and Kapoor,
    A. (2023). SMART: self-supervised multi-task pretraining with control transformers.
    In Proceedings of the Eleventh International Conference on Learning Representations
    (ICLR’23).'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 Sun, Y., Ma, S., Madaan, R., Bonatti, R., Huang, F., 和 Kapoor, A. (2023).
    SMART: 自监督多任务预训练与控制变换器。在第十一届国际学习表征会议（ICLR’23）论文集。'
- en: 'Sun et al. Sun, Y., Yin, X., and Huang, F. (2021). Temple: Learning template
    of transitions for sample efficient multi-task rl.. In Yang et al. (?).'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 Sun, Y., Yin, X., 和 Huang, F. (2021). Temple: 学习过渡模板以提高样本效率的多任务rl。在 Yang
    等（？）。'
- en: Sutton Sutton, R. (1988). Learning to predict by the methods of temporal differences.
    Mach. Learn., 3, 9–44.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton Sutton, R. (1988). 通过时间差分的方法学习预测。Mach. Learn., 3, 9–44。
- en: Sutton et al. Sutton, R., McAllester, D., Singh, S., and Mansour, Y. (1999a).
    Policy gradient methods for reinforcement learning with function approximation.
    In Solla, S., Leen, T., and Müller, K. (Eds.), Proceedings of the 12th International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’99).
    The MIT Press.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等 Sutton, R., McAllester, D., Singh, S., 和 Mansour, Y. (1999a). 使用函数逼近的强化学习策略梯度方法。在
    Solla, S., Leen, T., 和 Müller, K. (编者), 第12届国际神经信息处理系统大会（NeurIPS’99）论文集。MIT出版社。
- en: 'Sutton et al. Sutton, R., Precup, D., and Singh, S. (1999b). Between mdps and
    semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial
    Intelligence, 112(1-2), 181–211.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等 Sutton, R., Precup, D., 和 Singh, S. (1999b). 在mdps和半mdps之间：强化学习中的时间抽象框架。人工智能,
    112(1-2), 181–211。
- en: 'Sutton and Barto Sutton, R. S.,  and Barto, A. G. (2018). Reinforcement learning:
    An introduction (2 edition). Adaptive computation and machine learning. MIT Press.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto Sutton, R. S., 和 Barto, A. G. (2018). 强化学习：导论（第2版）。自适应计算与机器学习。MIT出版社。
- en: Sycara et al. Sycara, K., Honavar, V., and Spaan, M. (Eds.). (2022). Proceedings
    of the Thirty-Sixth Conference on Artificial Intelligence (AAAI’22). Association
    for the Advancement of Artificial Intelligence, AAAI Press.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sycara 等 Sycara, K., Honavar, V., 和 Spaan, M. (编者). (2022). 第三十六届人工智能会议（AAAI’22）论文集。人工智能进步协会，AAAI出版社。
- en: Talele and Byl Talele, N.,  and Byl, K. (2019). Mesh-based tools to analyze
    deep reinforcement learning policies for underactuated biped locomotion. In arXiv
    preprint arXiv:1903.12311.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talele 和 Byl Talele, N., 和 Byl, K. (2019). 基于网格的工具分析针对欠驱动双足运动的深度强化学习策略。在arXiv预印本arXiv:1903.12311。
- en: Talvitie and Singh Talvitie, E.,  and Singh, S. (2008). Simple local models
    for complex dynamical systems. In Proceedings of the 21st International Conference
    on Advances in Neural Information Processing Systems (NeurIPS’08).
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talvitie 和 Singh Talvitie, E., 和 Singh, S. (2008). 复杂动态系统的简单局部模型。在第21届国际神经信息处理系统大会（NeurIPS’08）论文集。
- en: Tang et al. Tang, S., Makar, M., Sjoding, M., Doshi-Velez, F., and Wiens, J.
    (2022a). Leveraging factored action spaces for efficient offline reinforcement
    learning in healthcare. In Decision Awareness in Reinforcement Learning Workshop
    at ICML 2022.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等 Tang, S., Makar, M., Sjoding, M., Doshi-Velez, F., 和 Wiens, J. (2022a).
    利用分解动作空间进行医疗保健中的高效离线强化学习。在ICML 2022的决策意识强化学习研讨会。
- en: Tang et al. Tang, S., Makar, M., Sjoding, M., Doshi-Velez, F., and Wiens, J.
    (2022b). Leveraging factored action spaces for efficient offline reinforcement
    learning in healthcare. In Decision Awareness in Reinforcement Learning Workshop
    at ICML 2022.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 Tang, S., Makar, M., Sjoding, M., Doshi-Velez, F., 和 Wiens, J. (2022b).
    利用分解的动作空间实现医疗保健中的高效离线强化学习. 在 ICML 2022 会议的强化学习决策意识研讨会.
- en: Tavakol and Brefeld Tavakol, M.,  and Brefeld, U. (2014). Factored mdps for
    detecting topics of user sessions. In Proceedings of the 8th ACM Conference on
    Recommender Systems.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tavakol 和 Brefeld Tavakol, M., 和 Brefeld, U. (2014). 用于检测用户会话主题的分解 mdps. 在第8届
    ACM 推荐系统会议论文集.
- en: Tavakoli et al. Tavakoli, A., Pardo, F., and Kormushev, P. (2018). Action branching
    architectures for deep reinforcement learning. In McIlraith, S.,  and Weinberger,
    K. (Eds.), Proceedings of the Thirty-Second Conference on Artificial Intelligence
    (AAAI’18). AAAI Press.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tavakoli 等人 Tavakoli, A., Pardo, F., 和 Kormushev, P. (2018). 深度强化学习的动作分支架构.
    在 McIlraith, S., 和 Weinberger, K. (编辑), 第三十二届人工智能会议论文集 (AAAI’18). AAAI Press.
- en: Tennenholtz and Mannor Tennenholtz, G.,  and Mannor, S. (2019). The natural
    language of actions.. In Chaudhuri, and Salakhutdinov (?).
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tennenholtz 和 Mannor Tennenholtz, G., 和 Mannor, S. (2019). 行动的自然语言.. 在 Chaudhuri
    和 Salakhutdinov (?).
- en: Trimponias and Dietterich Trimponias, G.,  and Dietterich, T. (2023). Reinforcement
    learning with exogenous states and rewards. In arXiv preprint arXiv:2303.12957.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trimponias 和 Dietterich Trimponias, G., 和 Dietterich, T. (2023). 具有外源状态和奖励的强化学习.
    在 arXiv 预印本 arXiv:2303.12957.
- en: Tsividis et al. Tsividis, P., Loula, J., Burga, J., Foss, N., Campero, A., Pouncy,
    T., Gershman, S., and Tenenbaum, J. (2021). Human-level reinforcement learning
    through theory-based modeling, exploration, and planning. CoRR, abs/2107.12544.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsividis 等人 Tsividis, P., Loula, J., Burga, J., Foss, N., Campero, A., Pouncy,
    T., Gershman, S., 和 Tenenbaum, J. (2021). 通过基于理论的建模、探索和规划实现人类水平的强化学习. CoRR, abs/2107.12544.
- en: 'van der Pol et al. van der Pol, E., Kipf, T., Oliehoek, F., and Welling, M.
    (2020). Plannable approximations to mdp homomorphisms: Equivariance under actions.
    In Proceedings of the 19th International Conference on Autonomous Agents and Multiagent
    Systems.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'van der Pol 等人 van der Pol, E., Kipf, T., Oliehoek, F., 和 Welling, M. (2020).
    可规划的 mdp 同态近似: 动作下的等变性. 在第19届国际自主代理和多智能体系统会议论文集.'
- en: van Rossum et al. van Rossum, C., Feinberg, C., Shumays, A. A., Baxter, K.,
    and Bartha, B. (2021). A novel approach to curiosity and explainable reinforcement
    learning via interpretable sub-goals. CoRR, abs/2104.06630.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Rossum 等人 van Rossum, C., Feinberg, C., Shumays, A. A., Baxter, K., 和 Bartha,
    B. (2021). 通过可解释的子目标对好奇心和可解释的强化学习的新方法. CoRR, abs/2104.06630.
- en: Vaswani et al. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need..
    In Guyon et al. (?).
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, L., 和 Polosukhin, I. (2017). 注意力就是你需要的一切.. 在 Guyon 等人 (?).
- en: Veerapaneni et al. Veerapaneni, R., Co-Reyes, J., Chang, M., Janner, M., Finn,
    C., Wu, J., Tenenbaum, J., and Levine, S. (2020). Entity abstraction in visual
    model-based reinforcement learning. In Conference on Robot Learning. PMLR.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veerapaneni 等人 Veerapaneni, R., Co-Reyes, J., Chang, M., Janner, M., Finn, C.,
    Wu, J., Tenenbaum, J., 和 Levine, S. (2020). 视觉模型基础的强化学习中的实体抽象. 在机器人学习会议. PMLR.
- en: Verma et al. Verma, A., Murali, V., Singh, R., Kohli, P., and Chaudhuri, S.
    (2018). Programmatically interpretable reinforcement learning. In icml18.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma 等人 Verma, A., Murali, V., Singh, R., Kohli, P., 和 Chaudhuri, S. (2018).
    可程序化解释的强化学习. 在 icml18.
- en: Wallach et al. Wallach, H., Larochelle, H., Beygelzimer, A., d’Alche Buc, F.,
    Fox, E., and Garnett, R. (Eds.). (2019). Proceedings of the 32nd International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’19).
    Curran Associates.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wallach 等人 Wallach, H., Larochelle, H., Beygelzimer, A., d’Alche Buc, F., Fox,
    E., 和 Garnett, R. (编辑). (2019). 第32届国际神经信息处理系统会议论文集 (NeurIPS’19). Curran Associates.
- en: Wan et al. Wan, X., Lu, C., Parker-Holder, J., Ball, P., Nguyen, V., Ru, B.,
    and Osborne, M. (2022). Bayesian generational population-based training. In Guyon,
    I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (Eds.), Proceedings
    of the First International Conference on Automated Machine Learning. Proceedings
    of Machine Learning Research.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 Wan, X., Lu, C., Parker-Holder, J., Ball, P., Nguyen, V., Ru, B., 和 Osborne,
    M. (2022). 贝叶斯生成型种群训练. 在 Guyon, I., Lindauer, M., van der Schaar, M., Hutter,
    F., 和 Garnett, R. (编辑), 第一届自动化机器学习国际会议论文集. 机器学习研究论文集.
- en: Wang et al. Wang, G., Fang, Z., Li, B., and Li, P. (2016). Integrating symmetry
    of environment by designing special basis functions for value function approximation
    in reinforcement learning. In Fourteenth International Conference on Control,
    Automation, Robotics and Vision.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 Wang, G., Fang, Z., Li, B., 和 Li, P. (2016). 通过设计特殊基函数在强化学习中整合环境对称性。见第十四届控制、自动化、机器人和视觉国际会议论文集。
- en: Wang et al. Wang, H., Dong, S., and Shao, L. (2019). Measuring structural similarities
    in finite mdps.. In Kraus, S. (Ed.), Proceedings of the 28th International Joint
    Conference on Artificial Intelligence (IJCAI’19).
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 Wang, H., Dong, S., 和 Shao, L. (2019). 测量有限Markov决策过程中的结构相似性。见 Kraus,
    S.（编），第28届国际人工智能联合会议（IJCAI’19）论文集。
- en: 'Wang et al. Wang, J., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck,
    C., Choy, P., Cassin, M., Reynolds, M., Song, H., Buttimore, G., Reichert, D.,
    Rabinowitz, N., Matthey, L., Hassabis, D., Lerchner, A., and Botvinick, M. (2021).
    Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents..
    In Ranzato et al. (?).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, J., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck, C.,
    Choy, P., Cassin, M., Reynolds, M., Song, H., Buttimore, G., Reichert, D., Rabinowitz,
    N., Matthey, L., Hassabis, D., Lerchner, A., 和 Botvinick, M. (2021). Alchemy:
    元强化学习代理的基准和分析工具包。见 Ranzato 等（？）。'
- en: Wang et al. Wang, J., Liu, Y., and Li, B. (2020). Reinforcement learning with
    perturbed rewards.. In Rossi et al. (?).
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 Wang, J., Liu, Y., 和 Li, B. (2020). 使用扰动奖励的强化学习。见 Rossi 等（？）。
- en: Wang and van Hoof Wang, Q.,  and van Hoof, H. (2022). Model-based meta reinforcement
    learning using graph structured surrogate models and amortized policy search..
    In Chaudhuri et al. (?).
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang和van Hoof Wang, Q., 和 van Hoof, H. (2022). 基于模型的元强化学习，使用图结构代理模型和摊销策略搜索。见
    Chaudhuri 等（？）。
- en: 'Wang et al. Wang, T., Du, S., Torralba, A., Isola, P., Zhang, A., and Tian,
    Y. (2022). Denoised mdps: Learning world models better than the world itself.
    In arXiv preprint arXiv:2206.15477.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, T., Du, S., Torralba, A., Isola, P., Zhang, A., 和 Tian, Y. (2022).
    去噪Markov决策过程: 学习比世界本身更好的世界模型。见 arXiv 预印本 arXiv:2206.15477。'
- en: 'Wang et al. Wang, T., Liao, R., Ba, J., and Fidler, S. (2018). Nervenet: Learning
    structured policy with graph neural networks. In Proceedings of the Sixth International
    Conference on Learning Representations (ICLR’18).'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, T., Liao, R., Ba, J., 和 Fidler, S. (2018). Nervenet: 使用图神经网络学习结构化策略。见第六届国际学习表征会议（ICLR’18）论文集。'
- en: Wang et al. Wang, T., Torralba, A., Isola, P., and Zhang, A. (2023). Optimal
    goal-reaching reinforcement learning via quasimetric learning. CoRR, abs/2304.01203.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 Wang, T., Torralba, A., Isola, P., 和 Zhang, A. (2023). 通过准度量学习实现最优目标达成强化学习。CoRR,
    abs/2304.01203。
- en: Wen et al. Wen, Z., Precup, D., Ibrahimi, M., Barreto, A., Roy, B. V., and Singh,
    S. (2020). On efficiency in hierarchical reinforcement learning.. In Larochelle
    et al. (?).
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人 Wen, Z., Precup, D., Ibrahimi, M., Barreto, A., Roy, B. V., 和 Singh,
    S. (2020). 关于层次强化学习中的效率。见 Larochelle 等（？）。
- en: Whatley et al. Whatley, A., Luo, Z., and Tang, X. (2021). Improving RNA secondary
    structure design using deep reinforcement learning. CoRR, abs/2111.04504.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whatley 等人 Whatley, A., Luo, Z., 和 Tang, X. (2021). 使用深度强化学习改进RNA二级结构设计。CoRR,
    abs/2111.04504。
- en: Whitehead and Lin Whitehead, S.,  and Lin, L. (1995). Reinforcement learning
    of non-markov decision processes. Artif. Intell., 73(1-2), 271–306.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whitehead和Lin Whitehead, S., 和 Lin, L. (1995). 非Markov决策过程的强化学习。人工智能, 73(1-2),
    271–306。
- en: Williams Williams, R. (1992a). Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Mach. Learn., 8, 229–256.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams Williams, R. (1992a). 用于连接主义强化学习的简单统计梯度跟踪算法。机器学习, 8, 229–256。
- en: Williams Williams, R. (1992b). Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Mach. Learn., 8, 229–256.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams Williams, R. (1992b). 用于连接主义强化学习的简单统计梯度跟踪算法。机器学习, 8, 229–256。
- en: Wolf and Musolesi Wolf, L.,  and Musolesi, M. (2023). Augmented modular reinforcement
    learning based on heterogeneous knowledge. CoRR, abs/2306.01158.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf和Musolesi Wolf, L., 和 Musolesi, M. (2023). 基于异质知识的增强模块化强化学习。CoRR, abs/2306.01158。
- en: Woo et al. Woo, H., Yoo, G., and Yoo, M. (2022). Structure learning-based task
    decomposition for reinforcement learning in non-stationary environments.. In Sycara
    et al. (?).
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Woo 等人 Woo, H., Yoo, G., 和 Yoo, M. (2022). 基于结构学习的任务分解，用于非平稳环境中的强化学习。见 Sycara
    等（？）。
- en: Wu et al. Wu, B., Gupta, J., and Kochenderfer, M. (2019). Model primitive hierarchical
    lifelong reinforcement learning. In Elkind, E., Veloso, M., Agmon, N., and Taylor,
    M. (Eds.), Proceedings of the Eighteenth International Conference on Autonomous
    Agents and MultiAgent Systems (AAMAS’19), pp. 34–42\. International Foundation
    for Autonomous Agents and Multiagent Systems.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等，B., 古普塔, J., 和 科钦德费尔, M. (2019). 模型原始层次终身强化学习。发表于埃尔金德, E., 维洛索, M., 阿格蒙,
    N., 和 泰勒, M.（编辑），第十八届自主代理和多代理系统国际会议 (AAMAS’19) 论文集，第34–42页。国际自主代理和多代理系统基金会。
- en: Wu et al. Wu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A., Kakade, S.,
    Mordatch, I., and Abbeel, P. (2018). Variance reduction for policy gradient with
    action-dependent factorized baselines. In Proceedings of the Sixth International
    Conference on Learning Representations (ICLR’18).
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等，C., 拉杰斯瓦兰, A., 段, Y., 库马尔, V., 贝延, A., 卡卡德, S., 莫达奇, I., 和 阿比尔, P. (2018).
    基于动作依赖因子的方差减少用于策略梯度。发表于第六届国际学习表征会议 (ICLR’18)。
- en: Xu and Fekri Xu, D.,  and Fekri, F. (2021). Interpretable model-based hierarchical
    reinforcement learning using inductive logic programming. CoRR, abs/2106.11417.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许和费克里，D., 和 费克里, F. (2021). 使用归纳逻辑编程的可解释模型基础层次强化学习。CoRR, abs/2106.11417。
- en: 'Xu et al. Xu, K., Verma, S., Finn, C., and Levine, S. (2020). Continual learning
    of control primitives: Skill discovery via reset-games.. In Larochelle et al.
    (?).'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等，K., 瓦尔玛, S., 芬恩, C., 和 莱文, S. (2020). 控制原型的持续学习：通过重置游戏发现技能。发表于拉罗谢尔等 (?)。
- en: Yang et al. Yang, C., Hung, I., Ouyang, Y., and Chen, P. (2022). Training a
    resilient q-network against observational interference.. In Sycara et al. (?).
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等，C., 洪, I., 欧阳, Y., 和 陈, P. (2022). 训练一个对观测干扰具有弹性的 Q 网络。发表于斯卡拉等 (?)。
- en: 'Yang et al. Yang, F., Lyu, D., Liu, B., and Gustafson, S. (2018). Peorl: Integrating
    symbolic planning and hierarchical reinforcement learning for robust decision-making..
    In Lang (?).'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨等，F., 吕, D., 刘, B., 和 古斯塔夫森, S. (2018). Peorl: 将符号规划与层次强化学习整合以实现鲁棒决策制定。发表于
    Lang (?)。'
- en: Yang, Leyton-Brown, and Mausam Yang, Q., Leyton-Brown, K., and Mausam (Eds.).
    (2021). Proceedings of the Thirty-Fifth Conference on Artificial Intelligence
    (AAAI’21). Association for the Advancement of Artificial Intelligence, AAAI Press.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨、莱顿-布朗和毛萨姆杨, Q., 莱顿-布朗, K., 和 毛萨姆（编辑）。 (2021). 第三十五届人工智能会议论文集 (AAAI’21)。人工智能促进协会，AAAI出版社。
- en: Yang, Xu, Wu, and Wang Yang, R., Xu, H., Wu, Y., and Wang, X. (2020a). Multi-task
    reinforcement learning with soft modularization.. In Larochelle et al. (?).
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨、许、吴和王，R., 许, H., 吴, Y., 和 王, X. (2020a). 软模块化的多任务强化学习。发表于拉罗谢尔等 (?)。
- en: Yang et al. Yang, Y., Zhang, G., Xu, Z., and Katabi, D. (2020b). Harnessing
    structures for value-based planning and reinforcement learning. In Proceedings
    of the Eighth International Conference on Learning Representations (ICLR’20).
    OpenReview.net.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等，Y., 张, G., 许, Z., 和 卡塔比, D. (2020b). 利用结构进行基于价值的规划和强化学习。发表于第八届国际学习表征会议 (ICLR’20)
    论文集。OpenReview.net。
- en: Yarats et al. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2021). Reinforcement
    learning with prototypical representations. In Meila, M.,  and Zhang, T. (Eds.),
    icml21.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅拉茨等，D., 费格斯, R., 拉齐克, A., 和 品托, L. (2021). 使用原型表示的强化学习。发表于梅拉, M., 和 张, T.（编辑），icml21。
- en: Yin et al. Yin, D., Thiagarajan, S., Lazic, N., Rajaraman, N., Hao, B., and Szepesvári,
    C. (2023). Sample efficient deep reinforcement learning via local planning. CoRR,
    abs/2301.12579.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 银等，D., 蒂亚加拉扬, S., 拉齐克, N., 拉贾拉曼, N., 郝, B., 和 斯泽佩斯瓦里, C. (2023). 通过局部规划实现样本高效的深度强化学习。CoRR,
    abs/2301.12579。
- en: Young et al. Young, K., Ramesh, A., Kirsch, L., and Schmidhuber, J. (2022).
    The benefits of model-based generalization in reinforcement learning. In arXiv
    preprint arXiv:2211.02222.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等，K., 拉梅什, A., 基尔施, L., 和 施密德胡伯, J. (2022). 基于模型的泛化在强化学习中的好处。发表于 arXiv 预印本
    arXiv:2211.02222。
- en: Yu et al. Yu, D., Ma, H., Li, S., and Chen, J. (2022). Reachability constrained
    reinforcement learning.. In Chaudhuri et al. (?).
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 于等，D., 马, H., 李, S., 和 陈, J. (2022). 可达性约束强化学习。发表于查乌德赫里等 (?)。
- en: Zambaldi et al. Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin,
    I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston,
    V., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. (2019). Deep reinforcement
    learning with relational inductive biases. In Proceedings of the Seventh International
    Conference on Learning Representations, ICLR 2019. OpenReview.net.
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾姆巴尔迪等人，贾姆巴尔迪，V.，拉波索，D.，圣托罗，A.，巴普斯特，V.，李，Y.，巴布什金，I.，图伊尔斯，K.，赖希特，D.，利利克拉普，T.，洛克哈特，E.，香农，M.，朗斯顿，V.，帕斯卡努，R.，博特维尼克，M.，维尼亚尔斯，O.，和巴塔利亚，P.（2019）。具有关系归纳偏差的深度强化学习。发表于第七届国际学习表征会议论文集，ICLR
    2019。OpenReview.net。
- en: 'Zeng et al. Zeng, K., Zhang, Q., Chen, B., Liang, B., and Yang, J. (2022).
    APD: learning diverse behaviors for reinforcement learning through unsupervised
    active pre-training. IEEE Robotics Autom. Lett., 7(4), 12251–12258.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曾等人，曾，K.，张，Q.，陈，B.，梁，B.，和杨，J.（2022）。APD：通过无监督主动预训练学习多样行为的强化学习。IEEE机器人自动化信函，7（4），12251-12258。
- en: Zhang et al. Zhang, A., Lyle, C., Sodhani, S., Filos, A., Kwiatkowska, M., Pineau,
    J., Gal, Y., and Precup, D. (2020). Invariant causal prediction for block mdps..
    In III, and Singh (?).
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，A.，莱尔，C.，索达尼，S.，费洛斯，A.，克维亚科夫斯卡，M.，皮诺，J.，加尔，Y.，和普雷库普，D.（2020）。用于块MDP的恒定因果预测。在III和辛格（？）。
- en: Zhang et al. Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S.
    (2021). Learning invariant representations for reinforcement learning without
    reconstruction. In Proceedings of the Ninth International Conference on Learning
    Representations (ICLR’21).
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，A.，麦卡利斯特，R.，卡兰德拉，R.，加尔，Y.，和莱文，S.（2021）。学习用于强化学习的不变表征而无需重建。发表于第九届国际学习表征会议（ICLR’21）论文集。
- en: Zhang et al. Zhang, A., Sodhani, S., Khetarpal, K., and Pineau, J. (2020). Multi-task
    reinforcement learning as a hidden-parameter block mdp. In arXiv preprint arXiv:2007.07206.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，A.，索达尼，S.，凯塔尔帕尔，K.，和皮诺，J.（2020）。作为隐藏参数块MDP的多任务强化学习。发表于arXiv预印本arXiv:2007.07206。
- en: Zhang et al. Zhang, A., Sodhani, S., Khetarpal, K., and Pineau, J. (2021a).
    Learning robust state abstractions for hidden-parameter block mdps. In Proceedings
    of the Ninth International Conference on Learning Representations (ICLR’21).
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，A.，索达尼，S.，凯塔尔帕尔，K.，和皮诺，J.（2021a）。为隐藏参数块MDP学习稳健的状态抽象。发表于第九届国际学习表征会议（ICLR’21）论文集。
- en: Zhang et al. Zhang, C., Cai, Y., Huang, L., and Li, J. (2021b). Exploration
    by maximizing renyi entropy for reward-free RL framework.. In Yang et al. (?).
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，C.，蔡，Y.，黄，L.，和李，J.（2021b）。通过最大化Renyi熵进行无奖励RL框架的探索。在杨等人（？）。
- en: Zhang et al. Zhang, D., Courville, A., Bengio, Y., Zheng, Q., Zhang, A., and Chen,
    R. (2022). Latent state marginalization as a low-cost approach for improving exploration.
    CoRR, abs/2210.00999.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，D.，库维尔，A.，本吉奥，Y.，郑，Q.，张，A.，和陈，R.（2022）。作为提高探索的低成本方法的潜在状态边际化。CoRR，abs/2210.00999。
- en: Zhang et al. Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D., and Hsieh,
    C. (2020). Robust deep reinforcement learning against adversarial perturbations
    on state observations.. In Larochelle et al. (?).
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，H.，陈，H.，肖，C.，李，B.，刘，M.，博宁，D.，和谢，C.（2020）。针对状态观察的对抗性扰动的鲁棒深度强化学习。在拉罗谢尔等人（？）。
- en: Zhang et al. Zhang, H., Gao, Z., Zhou, Y., Zhang, H., Wu, K., and Lin, F. (2019a).
    Faster and safer training by embedding high-level knowledge into deep reinforcement
    learning. CoRR, abs/1910.09986.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，H.，高，Z.，周，Y.，张，H.，吴，K.，和林，F.（2019a）。通过将高级知识嵌入深度强化学习实现更快、更安全的训练。CoRR，abs/1910.09986。
- en: Zhang et al. Zhang, H., Gao, Z., Zhou, Y., Zhang, H., Wu, K., and Lin, F. (2019b).
    Faster and safer training by embedding high-level knowledge into deep reinforcement
    learning. CoRR, abs/1910.09986.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，H.，高，Z.，周，Y.，张，H.，吴，K.，和林，F.（2019b）。通过将高级知识嵌入深度强化学习实现更快、更安全的训练。CoRR，abs/1910.09986。
- en: 'Zhang et al. Zhang, S., Tong, H., Xu, J., and Maciejewski, R. (2019c). Graph
    convolutional networks: a comprehensive review. Computational Social Networks,
    6(1), 1–23.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人，张，S.，童，H.，徐，J.，和马切耶夫斯基，R.（2019c）。图卷积网络：综合综述。《计算社会网络》，6（1），1-23。
- en: Zhang and Yu Zhang, X.,  and Yu, S. Z. Y. (2021). Domain knowledge guided offline
    q learning. In Second Offline Reinforcement Learning Workshop at Neurips 2021.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张和余，张，X.，和余，S. Z. Y.（2021）。领域知识指导的离线Q学习。在Neurips 2021的第二届离线强化学习研讨会。
- en: 'Zhao et al. Zhao, T., Xie, K., and Eskénazi, M. (2019). Rethinking action spaces
    for reinforcement learning in end-to-end dialog agents with latent variable models.
    In Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人，赵婷、谢凯和埃斯克纳兹（2019）。重新思考端到端对话代理中的强化学习动作空间，使用潜变量模型。发表于2019年北美计算语言学协会会议：人类语言技术论文集。
- en: Zhou et al. Zhou, A., Kumar, V., Finn, C., and Rajeswaran, A. (2022). Policy
    architectures for compositional generalization in control. In arXiv preprint arXiv:2203.05960.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人，周安、库马尔、芬恩和拉杰斯瓦兰（2022）。控制中的组合泛化策略架构。发表于 arXiv 预印本 arXiv:2203.05960。
- en: Zhou et al. Zhou, Z., Li, X., and Zare, R. (2017). Optimizing chemical reactions
    with deep reinforcement learning. ACS central science, 3(12), 1337–1344.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人，周泽、李鑫和扎雷（2017）。使用深度强化学习优化化学反应。ACS 中央科学，3(12)，1337–1344。
- en: Zhu et al. Zhu, J., Park, T., Isola, P., and Efros, A. (2017). Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In Proceedings of the
    IEEE international conference on computer vision, pp. 2223–2232.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人，朱俊、朴泰、伊索拉和埃弗罗斯（2017）。使用循环一致对抗网络进行未配对的图像到图像翻译。发表于 IEEE 国际计算机视觉会议论文集，pp.
    2223–2232。
