- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:38:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2306.16021] Structure in Reinforcement Learning: A Survey and Open Problems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.16021](https://ar5iv.labs.arxiv.org/html/2306.16021)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \pdfcolInitStack
  prefs: []
  type: TYPE_NORMAL
- en: tcb@breakable
  prefs: []
  type: TYPE_NORMAL
- en: 'Structure in Reinforcement Learning:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey and Open Problems
  prefs: []
  type: TYPE_NORMAL
- en: \nameAditya Mohan \emaila.mohan@ai.uni-hannover.de
  prefs: []
  type: TYPE_NORMAL
- en: \addrInstitute of Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Leibniz University Hannover
  prefs: []
  type: TYPE_NORMAL
- en: \nameAmy Zhang \emailamy.zhang@austin.utexas.edu
  prefs: []
  type: TYPE_NORMAL
- en: \addrUniversity of Texas at Austin, Meta AI
  prefs: []
  type: TYPE_NORMAL
- en: \nameMarius Lindauer \emailm.lindauer@ai.uni-hannover.de
  prefs: []
  type: TYPE_NORMAL
- en: \addrInstitute of Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Leibniz University Hannover
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep
    Neural Networks (DNNs) for function approximation, has demonstrated considerable
    success in numerous applications. However, its practicality in addressing various
    real-world scenarios, characterized by diverse and unpredictable dynamics, noisy
    signals, and large state and action spaces, remains limited. This limitation stems
    from issues such as poor data efficiency, limited generalization capabilities,
    a lack of safety guarantees, and the absence of interpretability, among other
    factors. To overcome these challenges and improve performance across these crucial
    metrics, one promising avenue is to incorporate additional structural information
    about the problem into the RL learning process. Various sub-fields of RL have
    proposed methods for incorporating such inductive biases. We amalgamate these
    diverse methodologies under a unified framework, shedding light on the role of
    structure in the learning problem, and classify these methods into distinct patterns
    of incorporating structure. By leveraging this comprehensive framework, we provide
    valuable insights into the challenges of structured RL and lay the groundwork
    for a design pattern perspective on RL research. This novel perspective paves
    the way for future advancements and aids in developing more effective and efficient
    RL algorithms that can potentially handle real-world scenarios better.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) has contributed to a range of sequential decision-making
    and control problems like games (?), robotic manipulation (?), optimizing chemical
    reactions (?), and RNA folding (?). Most of the traditional research in RL focuses
    on designing agents that learn to solve a sequential decision problem induced
    by the inherent dynamics of a task, e.g., the differential equations governing
    the cart pole task in the classic control suite (?, ?). However, their performance
    significantly degrades when even small aspects of the environment change (?, ?).
    Moreover, deploying RL agents for real-world learning-based optimization has additional
    challenges, such as complicated dynamics, intractable and computationally expensive
    state and action spaces, and noisy reward signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, research in RL has started to address these issues through methods that
    can generally be categorized into two dogmas (?): (i) Generalization:Methods developed
    to solve a broader class of problems where the agent is trained on various tasks
    and environments (?, ?). (ii) Deployability:Methods that are specifically engineered
    towards concrete real-world problems by incorporating additional aspects such
    as feature engineering, computational budget optimization, safety, etc. The intersection
    of generalization and deployability is particularly interesting since it covers
    a class of problems where we require methods to handle sufficient diversity in
    the task while being deployable for specific applications. To foster research
    in this area, ? (?) argue for a design-pattern oriented approach, where methods
    can be abstracted into patterns that are specialized to specific kinds of problems.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the path to RL design patterns is hindered by gaps in our understanding
    of the relationship between the design decisions for RL methods and the properties
    of the problems they might be suited for. While decisions like using state abstractions
    for high-dimensional spaces seem obvious, decisions like using relational neural
    architectures for problems are not so obvious to a designer. One way to add principle
    to this process is to understand how to incorporate additional domain knowledge
    into the learning pipeline. A strong source of such domain knowledge is the structure
    present in the learning problem itself, including priors about the state space,
    the action space, the reward function, or the dynamics of the environment. While
    such methods have been research subjects throughout the history of RL (?), approaches
    for doing so in Deep RL are scattered across the various sub-fields in the vast
    and disparate landscape of modern RL research.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc5551af6f8b92e2a5caefd645883dba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of our framework. Domain knowledge can generally be incorporated
    into an RL pipeline as side information and can be used to achieve improved performance
    across metrics such as *Sample Efficiency*, *Generalization*, *Interpretability*,
    and *Safety*. We discuss this process in [Section 3](#S3 "3 Side Information and
    its Usage ‣ Structure in Reinforcement Learning: A Survey and Open Problems").
    A particular source of side information is decomposability in a learning problem,
    which can be categorized into four archetypes along a spectrum - *Latent*, *Factored*,
    *Relational*, and *Modular* - explained further in [Section 4.1](#S4.SS1 "4.1
    Decomposability and Structural Archetypes ‣ 4 Structure as Side Information ‣
    Structure in Reinforcement Learning: A Survey and Open Problems"). Incorporating
    side information about decomposability amounts to adding structure to a learning
    pipeline, and this process can be categorized into seven different patterns -
    *Abstraction*, *Augmentation*, *Auxiliary Optimization*, *Auxiliary Model*, *Warehouse*,
    *Environment Generation*, and *Explicitly Designed* - discussed further in [Section 5](#S5
    "5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions and Structure of the Paper. In this work, we take the first steps
    in amalgamating these approaches under our pattern-centric framework for incorporating
    structure in RL. [Figure 1](#S1.F1 "In 1 Introduction ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems") shows a general overview of three elements
    of understanding the role of incorporating structure into a learning problem that
    we cover in this work. In [Section 2](#S2 "2 Preliminaries ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems"), we describe the background and notation
    needed to formalize the relevant aspects of the RL problems. In [Section 3](#S3
    "3 Side Information and its Usage ‣ Structure in Reinforcement Learning: A Survey
    and Open Problems"), we introduce the notion of side information and define different
    additional metrics that can be addressed by incorporating side information into
    an RL pipeline. We then formulate structure as a particular kind of side information
    about decomposability in a problem in [Section 4](#S4 "4 Structure as Side Information
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems"), and categorize
    decompositions in the literature into four major archetypes. In [Section 5](#S5
    "5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems"), we formulate seven patterns of incorporating structure
    into the RL learning process and provide an overview of each pattern by connecting
    it to the relevant surveyed literature. The framework developed in this work opens
    new avenues for research while providing a common reference point for understanding
    what kind of design decisions work under which situations. We discuss these aspects
    further in [Section 6](#S6 "6 Open Problems in Structured Reinforcement Learning
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems") for more concrete
    takeaways for researchers and practitioners.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following sections summarize the main background necessary for our approach
    to studying structural decompositions and related patterns. In [Section 2.1](#S2.SS1
    "2.1 Markov Decision Processes ‣ 2 Preliminaries ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems"), we formalize the sequential decision-making
    problem as an MDP. [Section 2.2](#S2.SS2 "2.2 Reinforcement Learning ‣ 2 Preliminaries
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems") then presents
    the RL framework for solving MDPs and introduces the RL pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Markov Decision Processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sequential decision-making problems are usually formalized using the notion
    of a Markov Decision Process (MDP) (?, ?), which can be written down as a 5-tuple
    $\mathcal{M}=\langle\mathcal{S},\mathcal{A},R,P,\rho\rangle$. At any timestep,
    the environment exists in a state $s\in\mathcal{S}$, with $\rho$ being the initial
    state distribution. The agent takes an action $a\in\mathcal{A}$ which *transitions*
    the environment to a new state $s^{\prime}\in\mathcal{S}$. The stochastic transition
    function governs the dynamics of such transitions $P:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$,
    which takes the state $s$ and action $a$ as input and outputs a probability distribution
    over the next states $\Delta(.)$ from which the next state $s^{\prime}$ can be
    sampled. For each transition, the agent receives a reward $R:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$,
    with $R\in\mathcal{R}$. The sequence $(s,a,r,s^{\prime})$ is called an experience.
  prefs: []
  type: TYPE_NORMAL
- en: The agent acts according to a policy $\pi:\mathcal{S}\to\Delta(\mathcal{A})$,
    in a space of policies $\Pi$, that produces a probability distribution over actions
    given a state. This distribution is a delta distribution for deterministic policies,
    which leads to the policy outputting a single action. Using the current policy,
    an agent can repeatedly generate experiences, and a sequence of such experiences
    is also called a *trajectory* ($\tau$). In episodic RL, the trajectory consists
    of experiences collected over multiple episodes with environment resets, while
    in continual settings, the trajectory encompasses experiences collected over some
    horizon in a single episode. The rewards in $\tau$ can be accumulated into an
    expected sum called the return $G$, which can be calculated for any starting state
    $s$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $G(\pi,s)=\mathbb{E}_{(s_{0}=s,a_{1},r_{1},\dots)\sim\mathcal{M}}\bigg{[}\sum_{t=0}^{\infty}r_{t}\bigg{]}.$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'For the sum in [Equation 1](#S2.E1 "In 2.1 Markov Decision Processes ‣ 2 Preliminaries
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems") to be tractable,
    we either assume the horizon of the problem to be of a fixed length $T$ (finite-horizon
    return), i.e., the trajectory to terminate after $T$-steps, or we discount the
    future rewards by a discount factor $\gamma$ (infinite horizon return). Discounting,
    however, can also be applied to finite horizons. Solving an MDP amounts to determining
    the policy $\pi^{*}\in\Pi$ that maximizes the expectation over the returns of
    its trajectory. This expectation can be captured by the notion of the (state-action)
    value function $Q\in\mathcal{Q}$. Given a policy $\pi$, the expectation can be
    written recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{s\sim\rho}\big{[}G_{t}\mid s,a\big{]}=\mathbb{E}_{s^{\prime}\sim\tau}\big{[}R(s,a)+\gamma\mathbb{E}_{a^{\prime}\sim\pi(\cdot\mid
    s^{\prime})}[Q^{\pi}(s^{\prime},a^{\prime})]\big{]}.$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, the goal can now be formulated as the task of finding an optimal policy
    that can maximize the $Q^{\pi}(s,a)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}\in\operatorname*{arg\,max}_{\pi\in\Pi}Q^{\pi}(s,a).$ |  | (3)
    |'
  prefs: []
  type: TYPE_TB
- en: We also consider partially observable MDPs (POMDPs), which model situations
    where the state is not fully observable. A POMDP is defined as a 7-tuple $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{O},R,P,\xi,\rho\rangle$,
    where $\mathcal{S},\mathcal{A},R,P,\rho$ remain the same as defined above. Instead
    of observing the state $s\in\mathcal{S}$, the agent now has access to observation
    $o\in\mathcal{O}$ that is generated from the true state through an emission function
    $\xi:\mathcal{S}\to\mathcal{O}$. Thus, the observation takes the state’s role
    in the experience generation process, and the rest of the learning process can
    now be conditioned on $o$ instead of $s$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The task of an RL algorithm is to interact with the MDP by simulating its transition
    dynamics $P(s^{\prime}\mid s,a)$ and reward function $R(s,a)$ and learn the optimal
    policy mentioned in [Equation 3](#S2.E3 "In 2.1 Markov Decision Processes ‣ 2
    Preliminaries ‣ Structure in Reinforcement Learning: A Survey and Open Problems").
    In Deep RL, the policy is a Deep Neural Network (?) that is used to generate $\tau$.
    We can optimize such a policy by minimizing an appropriate objective $J\in\mathcal{J}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e208b8299c729ef6a86b3821f5def762.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The anatomy of an RL pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: A model of an MDP $\hat{\mathcal{M}}$ allows an agent to *plan* a trajectory
    by simulating it to generate experiences. RL methods that use such models are
    categorized into *Model-Based RL* (?). On the other hand, not having such a model
    necessitates learning the policy directly from experiences, and such methods fall
    into the category of *Model-free RL*.
  prefs: []
  type: TYPE_NORMAL
- en: RL methods can additionally be categorized based on the type of objective $J$.
    Methods that use a value function, and correspondingly the Temporal Difference
    error (?), to learn a policy fall into the category of *Value-based RL*. A key
    idea in these methods is *bootstrapping*, where they use a learned value estimate
    to improve the estimate of a state that precedes it. *On-policy* methods directly
    update the policy that generated the experiences, while *Off-policy* methods use
    a separate policy to generate experiences. *Policy-Based* Methods parameterize
    the policy directly and use the Policy Gradient (?, ?) to create $J$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A central research theme in practical RL methods focuses on approximating a
    global solution by iteratively learning one or more of the aforementioned quantities
    using supervised learning and function approximations. We use the notion of a
    pipeline to talk about different RL methods. [Figure 2](#S2.F2 "In 2.2 Reinforcement
    Learning ‣ 2 Preliminaries ‣ Structure in Reinforcement Learning: A Survey and
    Open Problems") shows the anatomy of an RL pipeline. The pipeline can be defined
    as a mathematical tuple $\Omega=\langle\mathcal{S},\mathcal{A},R,P,Q,\pi,\hat{\mathcal{M}},J,\mathcal{E}\rangle$,
    where all definitions remain the same as before. To solve an MDP, the pipeline
    operates on given an environment $\mathcal{E}$ by taking the state $s\in\mathcal{S}$
    as input and producing an action $a\in\mathcal{A}$ as an output. The environment
    operates with the dynamics $P$ and a reward function $R$. The pipeline might generate
    experiences by directly interacting with $\mathcal{E}$, i.e., *learning* from
    experiences or by simulating a learned model $\hat{\mathcal{M}}$ of the environment.
    The optimization procedure encompasses the interplay between the current policy
    $\pi$, its value function $Q$, the reward $R$, and the learning objective $J$.
    With a slight abuse of notation, we refer to any of the components of a pipeline
    as $X$ and assume the space in which it exists as $\mathcal{X}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Side Information and its Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the characterization of the problem by an MDP, there can still
    be additional information that could potentially improve performance on additional
    metrics such as *Sample Efficiency*, *Generalization*, *Interpretability*, and
    *Safety*. We call this *Side Information* (also referred to as privileged information).
    For the (semi-) supervised and unsupervised settings, side information is any
    additional information $z\in\mathcal{Z}$ that, while being neither part of the
    input nor the output space, can potentially contribute to the learning process
    (?).
  prefs: []
  type: TYPE_NORMAL
- en: 'Translated to the RL setting, this can be understood as additional information
    $z$ not provided in the original MDP definition $\mathcal{M}$. Such information
    can be incorporated into the learning process by a function $\beta:\mathcal{Z}\to\mathcal{X}$,
    where $\mathcal{X}\in\Omega$. Thus, side information can be incorporated into
    the RL pipeline by biasing one or more of the components shown in [Figure 2](#S2.F2
    "In 2.2 Reinforcement Learning ‣ 2 Preliminaries ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems").'
  prefs: []
  type: TYPE_NORMAL
- en: The natural follow-up question, then, becomes the impact of incorporating side
    information into the learning pipeline. In this work, we specifically focus on
    four ways in which side information can be used and formally define them in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Sample Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sample Efficiency is intimately tied to the idea of the Sample Complexity of
    RL. To formally define it, we use the notion of the *Sample Complexity of Exploration*
    (?): Given fixed parameters $\epsilon,\delta>0$, if the difference between the
    value functions of a learned policy $\pi$ and an optimal policy $\pi^{*}$ is $||Q^{\pi}(s,a)-Q^{\star}(s,a)||_{\infty}>\epsilon$,
    then we call this a mistake. If, with a probability of $1-\delta$, the total number
    of mistakes at a timestep $t$ is $\zeta(\epsilon,\delta)$, then we call $\zeta$
    the sample complexity of exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating side information leads to a reduction in $\zeta$, thus, improving
    the sample efficiency. Intuitively, if a pipeline demonstrates a higher reward
    than a baseline for the same number of timesteps, then we consider it more sample
    efficient. However, methods can additionally make specific claims on $\zeta$ by
    utilizing certain assumptions about the problem itself (?, ?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Exploration.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One specific way to improve the sample complexity of exploration is to impact
    the exploration mechanism using side information directly. ? (?) categorize exploration
    methods based on the type of information that an agent uses to explore the world
    into the following categories: (i) *Reward-Free Exploration*methods in which extrinsic
    rewards do not affect the choice of action. Instead, they rely on intrinsic forms
    of exploration. (ii) *Randomized Action Selection*methods use estimated value
    functions, policies, or rewards to induce exploratory behavior. (iii) *Optimism/Bonus-Based
    Exploration*methods use the *optimism in the face of uncertainty* paradigm to
    prefer actions with higher uncertain values. (iv) *Deliberate Exploration*methods
    that either use posterior distributions over dynamics (Bayesian setup) or meta-learning
    techniques to optimally solve exploration and (v) *Probability Matching*methods
    that use heuristics to select the next action. Incorporating side information
    into any of these methods generally leads to better sample efficiency, particularly
    by improving the state-space coverage of the exploration mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Transfer and Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transfer and generalization encompass performance metrics that measure how
    an RL agent performs on a set of different MDPs: Transfer evaluates how well an
    agent, trained on some MDP $\mathcal{M}_{i}$ performs on some other MDP $\mathcal{M}_{j}$.
    This can be either done in a zero-shot manner, where the agent is not fine-tuned
    on $\mathcal{M}_{j}$, or in a few-shot manner, where the agent gets to make some
    policy updates on $\mathcal{M}_{j}$ to learn as fast as possible. Generally, the
    performance gap between the two MDPs determines the transfer performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J_{\text{transfer}}(\pi):=\bm{G}(\pi,\mathcal{M}_{i})-\bm{G}(\pi,\mathcal{M}_{j}).$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Generalization extends this idea to training an agent on a set of training MDPs
    $\bm{\mathcal{M}}_{train}$, and then evaluating its performance on a separate
    set of MDPs $\bm{\mathcal{M}}_{test}$. Consequently, generalization (?) can then
    be measured by the metric.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Gen}(\pi):=\bm{G}(\pi,\bm{\mathcal{M}}_{train})-\bm{G}(\pi,\bm{\mathcal{M}}_{test}).$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: A more restrictive form of generalization can be evaluated when the training
    and testing MDPs are sampled from the same distribution, i.e., $\bm{\mathcal{M}}_{train},\bm{\mathcal{M}}_{test}\sim
    p(\bm{\mathcal{M}})$. Depending on how the transfer is done (zero-shot, few-shot,
    etc.), this notion covers any form of distribution of MDPs, including multi-task
    settings. Incorporating side information into the learning can minimize $\text{Gen}(\pi)$
    by capturing similarities between $\bm{\mathcal{M}}_{train}$ and $\bm{\mathcal{M}}_{test}$
    in many different ways.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interpretability refers to a mechanistic understanding of a system to make
    it more transparent. ? (?) enumerate three fundamental properties of model interpretability:
    (i) Simulatabilityrefers to the ability of a human to simulate the inner working
    of a system, (ii) Decomposabiltiyrefers to adding intuitive understanding to individual
    working parts of a system, (iii) Transparencyrefers to improving the understanding
    of a system’s function (e.g., quantifying its convergence properties).'
  prefs: []
  type: TYPE_NORMAL
- en: Given the coupled nature of individual parts of an RL pipeline, adding interpretability
    amounts to being able to learn a policy for the MDP that adheres to one of multiple
    such properties. Incorporating side information can help take steps towards improved
    performance on all three of these metrics, depending on the nature of side information
    and what it encompasses. We consider claims on interpretability based on whether
    a given work additionally addresses at least two of these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Safety
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Safety is the idea of learning policies that maximize the expectation of the
    return in problems in which it is important to ensure reasonable system performance
    and respect safety constraints during the learning and or deployment processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Safety in RL is a vast field in and of itself (?), we consider two specific
    categories in this work: *Safe Learning with constraints* and *Safe Exploration*.
    The former modifies the expected return account for one or more constraints $c_{i}\in
    C$, and the general form can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi\in\Pi}\mathbb{E}_{\pi}(G)\,\,\,s.t.\,\,\,c_{i}=\{h_{i}\leq\alpha\}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $h_{i}$ is a function related to the return and $\alpha$ is the threshold
    restricting the values of this function. Consequently, side information can be
    used in the formulation of such constraints.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Safe Exploration modifies the exploration process subject
    to external knowledge, which in our case translates to incorporating side information
    into the exploration process. While intuitively, this overlaps with the usage
    of side information for directed exploration, a distinguishing feature of this
    work is the final goal of this directed exploration to be safety, which might
    potentially come at the cost of sample efficiency and/or generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Structure as Side Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Structure can be considered a particular kind of side information available
    to a learning agent. To build an intuition about what we mean by this, consider
    the task of managing a large factory with many production cells ¹¹1example taken
    from ? (?). If a cell positioned early in the production line generates faulty
    parts, the whole factory may be affected. However, the quality of the parts a
    cell generates depends directly only on the state of this cell and the quality
    of the parts it receives from neighboring cells. Additionally, the cost of running
    the factory depends, among other things, on the sum of the costs of maintaining
    each local cell. Finally, while a cell responsible for anodization may receive
    parts directly from any other cell in the factory, a work order for a cylindrical
    part may restrict this dependency to cells with a lathe. In the context of producing
    cylindrical parts, the quality of the anodized parts depends directly only on
    the state of cells with a lathe. Thus, by incorporating information about the
    additive nature of production, costs, and the context of the part that needs to
    be produced, the learning pipeline can be imbued with better objectives such as
    improved sample efficiency or robustness of a learned policy to changing factory
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The vanilla MDP framework does not require incorporating such additive information
    into the learning pipeline. Thus, biasing the pipeline with this amounts to incorporating
    side information. However, this information particularly helps decompose the complicated
    learning problem into additive sub-parts that can be learned independently and,
    potentially, more efficiently. Thus, structure is a particular kind of side information
    that provides the learning pipeline with knowledge about decomposability in the
    learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we discuss the relationship between structure and decomposability.
    In [Section 4.1](#S4.SS1 "4.1 Decomposability and Structural Archetypes ‣ 4 Structure
    as Side Information ‣ Structure in Reinforcement Learning: A Survey and Open Problems"),
    we explain the impact of structural side information by explaining how it decomposes
    complex systems and categorizes such decompositions into four archetypes. In [Section 4.2](#S4.SS2
    "4.2 Latent Decomposition ‣ 4 Structure as Side Information ‣ Structure in Reinforcement
    Learning: A Survey and Open Problems") - [Section 4.5](#S4.SS5 "4.5 Modular Decomposition
    ‣ 4 Structure as Side Information ‣ Structure in Reinforcement Learning: A Survey
    and Open Problems"), we discuss these archetypes further to connect them with
    existing literature.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Decomposability and Structural Archetypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decomposability is the property of a system that allows breaking it down into
    smaller components or subsystems that can be analyzed, understood, and potentially
    learned more efficiently than the larger system, independently (?). In a decomposable
    system, the short-term behavior of each subsystem is approximately independent
    of the short-term behavior of the other subsystems. In the long run, the behavior
    of any one subsystem depends on the behavior of the other subsystems only in an
    aggregated way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning the RL pipeline in [Figure 2](#S2.F2 "In 2.2 Reinforcement Learning
    ‣ 2 Preliminaries ‣ Structure in Reinforcement Learning: A Survey and Open Problems"),
    we can see decomposability along two axes: (i) *Problem Decomposition*i.e., the
    environment parameterization, states, actions, transitions, and rewards; (ii)
    *Solution Decomposition*i.e., the learned policies, value functions, models, and
    training procedures. The spectrum of decomposability (?) provides an intuitive
    way to understand where a system lies in this regard. On one end of the spectrum,
    problems are non-decomposable, while on the other end, problems can be decomposed
    into weakly interacting sub-problems. Similarly, solutions on the former are monolithic,
    while those on the latter are modular. We capture this problem-solution interplay
    by marking four different archetypes of decomposability, as shown in [Figure 3](#S4.F3
    "In 4.1 Decomposability and Structural Archetypes ‣ 4 Structure as Side Information
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/71d01ff23ecf44acac0e39501e2c515f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Spectrum of Decomposability and Structural Archetypes. On the left
    end of the spectrum exist monolithic structural decompositions where knowledge
    about a *latent* subspace of $\mathcal{X}$ can be learned and incorporated as
    an inductive bias. Moving towards the right, we can learn multiple independent
    subspaces, albeit in a monolithic solution. These are *factored* decompositions.
    Further ahead, we see the emergence of interactionally complex decompositions,
    where knowledge about factorization and how they relate to each other can be incorporated
    into the learning process. We call these *relational decompositions*. Finally,
    we see fully distributed subsystems that can be incorporated and learned using
    individual policies. We call these *modular decompositions*.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Latent Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Latent decompositions are monolithic and can be useful in complex environments
    where the underlying structure is either unclear or non-stationary. Under this
    view, a pipeline component $\mathcal{X}$ can be approximated by a latent representation
    $\kappa$, which can then be integrated into the learning process. The quantities
    in the update that depend on $\mathcal{X}$ can now be re-conditioned on $\kappa$,
    which helps in improving performance by reducing the dimensionality of $\mathcal{X}$.
  prefs: []
  type: TYPE_NORMAL
- en: Latent States and Actions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Latent state spaces have been classically explored through the Latent MDP literature (?),
    where the aim is to discover a latent representation of the state space that is
    sufficient to learn an optimal policy. Latent states are used for tackling rich
    observation spaces, where Block MDPs (?) and Contextual MDPs (?) have shown success
    in generalization problems. Latent actions have been similarly explored in settings
    with stochastic action sets (?).
  prefs: []
  type: TYPE_NORMAL
- en: Given an encoder $\phi:\mathcal{S}\times\mathcal{A}\to\mathcal{\kappa}$ and
    a decoder $\beta:\mathcal{\kappa}\to\mathcal{S}$, the latent state-action formulation
    allows decomposing transition dynamics into a low-rank approximation,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(s^{\prime}\mid s,a)=\langle\phi(\kappa\mid s,a)\beta(s^{\prime}\mid\kappa)\rangle.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Latent Transition and Rewards.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While latent states allow decomposing transition matrices, another way to approach
    the problem directly is to decompose transition matrices into low-rank approximations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(s^{\prime}\mid s,a)=\phi(s^{\prime}\mid s,a)\beta(s^{\prime}\mid s,a).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Linear MDPs (?) and corresponding applications in Model-based RL (?, ?) have
    studied this form of direct decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: A similar decomposition can be applied to rewards as well. ? (?) have primarily
    explored this in noisy reward settings where the reward signal is assumed to be
    generated from a latent function that can be learned as an auxiliary learning
    objective.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Factored Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The factored decomposition moves slightly away from the monolithic nature by
    decomposing $\mathcal{X}$ into (latent) factors $\kappa_{1},\dots,\kappa_{n}$.
    Thus, the spaces become inner products of the individual factor spaces. A crucial
    aspect of factorization is that the factors can potentially impose conditional
    independence in their effects on the learning dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: Factored States and Actions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Factored state and action spaces have been explored in the Factored MDPs (?, ?, ?).
    Methods in this setting traditionally capture next state distribution using mechanisms
    such as Dynamic Bayesian Networks (?).
  prefs: []
  type: TYPE_NORMAL
- en: Factorization in the action space has also been used for tackling high-dimensional
    actions (?). These methods either impose a factorized structure on subsets of
    a high-dimensional action set (?) or impose this structure through the Q-values
    that lead to the final action (?). Crucially, these methods can potentially exploit
    some form of independence resulting from such factorization, either in the state
    representations or transitions.
  prefs: []
  type: TYPE_NORMAL
- en: Factored Transitions and Rewards.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Combined with factored states or modeled independently, factored rewards have
    been used to model perturbed rewards and multi-objective settings (?). While Factored
    MDPs do not naturally lead to factored policies, combining state and reward factorization
    can lead to factorization of Q-values (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Relational Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relational decompositions add a further notion of separability where in addition
    to the factored subsets, capturing immutable relations between them becomes important
    as well. Usually, these relations exist between entities in a scene and are used
    to formulate learning methods based on inductive logic (?). Traditionally, such
    relations were limited to first-order logic, but the relational structure has
    also been captured through graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The relational assumption posits that a space of predicates $\beta$ can ground
    these entities, and it can be modeled as a set of rules (such as inductive logic)
    that define how $z_{i},z_{j}\in Z$ interact with each other. An extension of this
    is by capturing $\beta$ as a graph $G=\langle V,E\rangle$ where the vertices are
    $z_{i}\in Z$ and the edges represent the relationship between them.
  prefs: []
  type: TYPE_NORMAL
- en: Using such a representation allows us to talk about generalization over the
    entities $\kappa_{i},\kappa_{j}$, and forms of $\beta$. This helps us circumvent
    the dimensionality of enumerative spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Relational States and Actions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classically, relational representations have been used to model state spaces
    in Relational MDPs (?) and Object-Oriented MDPs (?, ?). They represent factored
    state spaces using first-order representations consisting of objects, predicates,
    and functions on them to describe a set of ground MDPs. Such representations can
    capture interactionally complex structures between entities much more efficiently.
    Additionally, permutations of the interactions between the entities can help define
    new MDPs that differ in their dynamics, thus, contributing towards work in generalization.
  prefs: []
  type: TYPE_NORMAL
- en: States can also be more generally represented as graphs (?, ?), or by using
    symbolic inductive biases (?) fed to a learning module in addition to the original
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Action relations help tackle instances where the agent has multiple possible
    actions available, and the set of actions is significantly large. These methods
    capture relations using either attention mechanisms (?, ?) or graphs (?), thus
    offering scalability to high-dimensional action spaces. Additionally, relations
    between states and actions have helped define notions such as intents and affordances (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Relational Value Functions and Policies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traditional work in Relational MDPs has also explored ways to represent and
    construct first-order representations of value functions and/or policies to generalize
    to new instances. These include Regression Trees (?), Decision Lists (?), Algebraic
    Decision Diagrams (?), and Linear Basis Functions (?, ?). Recent approaches have
    started looking into DNN representations (?, ?), with extensions into modeling
    problem aspects such as morphology in Robotic tasks (?) in a relational manner,
    or using Graph-Laplacian (?) representations for intrinsic rewards (?).
  prefs: []
  type: TYPE_NORMAL
- en: Relational Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A parallel line of work looks at capturing relations in a multi-task setting,
    where task perturbations are either in the form of goals and corresponding rewards
    (?, ?, ?). Most work aims at integrating these relationships into the optimization
    procedure and/or additionally capturing them as models. We delve deeper into specifics
    in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Modular Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modular decompositions exist at the other end of the spectrum of decomposability,
    where individual value functions and/or policies can be learned for each decomposed
    entity $X$. Specifically, a task can be broken down into individual subsystems
    $\kappa_{1},\dots,\kappa_{N}$ for which models, value functions, and policies
    can be subsequently learned. Such modularity can exist along the following axes:
    (i) *Spatial Modularity*allows learning quantities specific to parts of the state
    space, thus, effectively reducing the dimensionality of the states; (ii) *Temporal
    Modularity*allows breaking down tasks into sequences over a learning horizon and,
    thus, learning modular quantities in a sequence; (iii) *Functional Modularity*allows
    decomposing the policy architecture into functionally modular parts, even if the
    problem is spatially and temporally monolithic.'
  prefs: []
  type: TYPE_NORMAL
- en: A potential consequence of such breakdown is the emergence of a hierarchy, and
    when learning problems exploit this hierarchical relationship, these problems
    come under the purview of Hierarchical RL (HRL) (?). The learned policies can
    also exhibit a hierarchy, where each policy can choose the lower-level policies
    to execute the subtasks. Each level can be treated as a planning problem (?) or
    a learning problem (?), thus, allowing solutions to combine planning and learning
    through the hierarchy. Hierarchy, however, is not a necessity for modularity.
  prefs: []
  type: TYPE_NORMAL
- en: Modularity in States and Goals
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modular decomposition of state spaces is primarily studied at high-level planning
    and state-abstractions for HRL methods (?). Additionally, the literature on skills
    has looked into the direction of training policies for individual parts of the
    state-space (?). Similarly, partial models only make predictions or specific parts
    of the observation-action spaces in Model-Based settings (?, ?). Goals have been
    specifically considered in methods that either use goals as an interface between
    levels of hierarchy (?, ?, ?), or as outputs of task specification methods (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Modularity in Actions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modularity in action spaces refers to conditioning policies on learned action
    abstractions. The classic example of such methods belongs to the realm of the
    options framework where policies are associated with temporal abstractions over
    actions (?). In HRL methods, learning and planning of the higher levels are based
    on the lower-level policies and termination conditions of their execution.
  prefs: []
  type: TYPE_NORMAL
- en: Compositional Policies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Continual settings utilize policies compositionally by treating already learned
    policies as primitives. Such methods either feed these primitives to the discrete
    optimization problems for selection mechanisms or to continuous optimization settings
    involving ensembling (?) and distillation (?). Modularity in such settings manifests
    itself by construction and is a central factor in building solutions. Even though
    the final policy in such paradigms, obtained through ensembling, selection, and/or
    distillation, can be monolithic, the method of obtaining such policies is a purely
    distributed regime.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Patterns of Incorporating Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having defined different forms of decomposability and the different objectives
    that side information can be used to accomplish, we now connect the two by understanding
    the ways of incorporating structure into a learning process. We assume that some
    form of structure exists in the problem and/or the solution space, which can be
    incorporated into the learning pipeline as an inductive bias. To understand how
    decomposability can be incorporated into the RL pipeline, we survey the literature
    with a very specific question in mind: *Do existing methods use structure in a
    repeatable manner?* The answer to this question, inspired by the categorization
    of ? (?) for the supervised learning case, brings us to *patterns of incorporating
    structure*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94b1937ff46608a3d9903621bc0847cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Abstraction
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/992ca57174930418188dbd13c9232fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Augmentation
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10d6b3519e55a3df6a51b5d26134b480.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Aux. Optimization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1dae4187bca79cdc47629da9f338b82a.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Aux. Model
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/daae5f668350bc37e3fdf580e1e37825.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Warehouse
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f02b3d10bd9ed3e30ba2d4caaaa3add5.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Environment Gen.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d62173c04948b6d31402994c52ba5cf.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) Explicitly Designed
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Patterns of incorporating structural information. We categorize the
    methods of incorporating structure as inductive biases into the learning pipeline
    into patterns that can be applied for different kinds of usages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A pattern is a principled change in the RL pipeline $\Omega$ that allows the
    pipeline to achieve one, or a combination of, the additional objectives: *Sample
    Efficiency*, *Generalization*, *Safety*, and *Interpretability*. We categorize
    the literature into seven patterns, an overview of which has been shown in [Figure 4](#S5.F4
    "In 5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems"). Our proposed patterns come from our literature survey
    and are meant to provide an initial direction for such categorization. We do not
    consider this list exhaustive but more as a starting point to build further upon.
    We present an overview of our meta-analysis on the patterns used for which of
    the four use cases in [Figure 5](#S5.F5 "In 5 Patterns of Incorporating Structure
    ‣ Structure in Reinforcement Learning: A Survey and Open Problems").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fba402d3757bcbf5be33b63280ad48e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Proclivities. A meta-analysis of the proclivities of each pattern
    to the additional objectives. On the x-axis are the patterns discussed in this
    text, while on the y-axis are the percentage of publications for each additional
    objective that address it using a particular pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we delve deeper into each pattern, explaining
    different lines of literature that apply each pattern for different use cases.
    To further provide intuition about this categorization, we will consider the running
    example of a taxi service, where the task of the RL agent (the taxi) is to pick
    up passengers from various locations and drop them at their desired destinations
    within a city grid. The agent receives a positive reward when a passenger is successfully
    dropped off at their destination and incurs a small penalty for each time step
    to encourage efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the following sections, we present a table of the surveyed methods
    that categorizes the work in the following manner: (i) The structured space, information
    about which is incorporated as side information; (ii) The type of decomposition
    exhibited for that structured space. We specifically categorize works that use
    structured task distributions through goals and/or rewards; (iii) The additional
    objectives for which the decomposition is utilized. In addition to demonstrating
    our categorization, our rationale behind the table format is to highlight the
    areas where further research might be lucrative. These are the spots in the tables
    where we could not yet find literature, and/or we believe additional work can
    be important.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Abstraction Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/2531d347621afdf37764a85c6bb7a88f.png)'
  prefs: []
  type: TYPE_IMG
- en: Abstraction pattern utilizes structural information to create abstract entities
    in the RL pipeline. For any entity, $X$, an abstraction utilizes the structural
    information to create $X_{abs}$, which takes over the role of $X$ in the learning
    procedure. In the taxi example, the state space can be abstracted to the current
    grid cell of the taxi, the destination grid cell of the current passenger, and
    whether the taxi is currently carrying a passenger. This significantly simplifies
    the state space compared to representing the full details of the city grid. The
    action space could also be abstracted to moving in the four cardinal directions,
    plus picking up and dropping off a passenger. Finding appropriate abstractions
    can be a challenging task in itself. Too much abstraction can lead to loss of
    critical information, while too little might not significantly reduce complexity (?).
    Consequently, learning-based methods that jointly learn abstractions factor this
    granularity into the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Abstractions have been thoroughly explored in the literature, with early work
    addressing a formal theory on state abstractions (?). Recent works have primarily
    used abstractions for tackling generalization. Thus, we see in [Figure 5](#S5.F5
    "In 5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems") that generalization is the most explored use case
    for abstractions. However, the aforementioned advantages of abstraction usually
    interleave these approaches with sample efficiency gains and safety as well. Given
    the widespread use of abstractions in the literature, we explore how different
    forms of abstractions impact each use case in the following paragraphs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Latent | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational |  |  | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?) | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?)
    | ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?) | ? (?), ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?),
    ? (?), ? (?), ? (?) | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Actions | Latent | ? (?), ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored |  | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Latent |  | ? (?), ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?), ? (?), ? (?), | ? (?) | ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent | ? (?) | ? (?), ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: Generalization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: State abstractions are a standard choice for improving generalization performance
    by capturing shared dynamics across MDPs into abstract state spaces using methods
    such as Invariant Causal Prediction (?, ?), similarity metrics (?, ?, ?, ?, ?, ?, ?),
    Free Energy Minimization (?), and disentanglement (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Value functions can serve as temporal abstractions for shared dynamics in Multi-task
    Settings. Successor Features (SF) (?, ?) exploit latent reward and dynamic decompositions
    by using value functions as an abstraction. Subsequent works have combined them
    with Generalized Policy Iteration (?) and Universal Value Function Approximators (?, ?).
    Factorization in value functions, other the other hand, can help improve sample
    efficiency and generalization both (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Relational abstractions contribute to generalization by incorporating symbolic
    spaces into the RL pipeline. These can help incorporate planning approaches in
    hierarchical frameworks (?, ?). Additionally, relational abstractions can help
    abstract away general aspects of a collection of MDPs, thus allowing methods to
    learn generalizable Q-values over abstract states and actions that can be transferred
    to new tasks (?) or develop methods specifically for graph-structured spaces (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Abstractions can additionally enable generalization in hierarchical settings
    by compressing state spaces (?), abstract automata (?, ?), templates of dynamics
    across tasks (?), or even be combined with options to preserve optimal values (?).
  prefs: []
  type: TYPE_NORMAL
- en: Sample Efficiency.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Latent variable models improve sample efficiency across the RL pipeline. Latent
    state abstractions can improve sample efficiency in Model-based RL (?) and also
    help improve the tractability of policy learning over options in HRL (?). In model-free
    tasks, these can also be learned as inverse models for visual features (?) or
    control in a latent space (?). Latent transition models demonstrate efficiency
    gains by capturing task-relevant information in noisy settings (?), by preserving
    bisimulation distances between original states (?), or by utilizing factorized
    abstractions (?). Learned latent abstractions (?) can also contribute to the exploration
    mechanism in the Go-Explore regime (?).
  prefs: []
  type: TYPE_NORMAL
- en: Latent action models can expedite convergence of policy gradient methods such
    as REINFORCE (?)) by shortening the learning horizon in stochastic scenarios like
    dialog generation ? (?). Action embeddings, on the other hand, can help reduce
    the dimensionality of large action spaces ? (?)
  prefs: []
  type: TYPE_NORMAL
- en: Safety and Interpretability.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Relational abstractions are a very good choice for interpretability since they
    capture interactionally complex decompositions. The combination of object-centric
    representations and learned abstractions can help add transparency (?) while symbolic
    interjections, such as tracking the relational distance between objects, can help
    improve performance  (?).
  prefs: []
  type: TYPE_NORMAL
- en: State and rewards abstractions can help with safety. Latent states can help
    to learn safe causal inference models by embedding confounders (?) On the other
    hand, meshes (?, ?) help benchmark metrics such as robustness in a learned policy.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Augmentation Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/bdb5799491df297ffdd8623d2326f85c.png)'
  prefs: []
  type: TYPE_IMG
- en: The augmentation pattern treats $X$ and $z$ as separate input entities for the
    action-selection mechanism. The combination can range from the simple concatenation
    of structural information to the state or actions to more involved methods of
    conditioning policy or value functions on additional information. Crucially, the
    structural information neither directly influences the optimization procedure
    nor changes the nature of $X$. It simply augments the already existing entities.
    In this view, abstractions that are learned in an auxiliary manner and concatenated
    to states, actions, or models can also be considered augmentations since the original
    entity remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the taxi example, one way to apply the augmentation pattern would be by
    conditioning the policy on additional information, such as the time of day or
    day of the week. This information could be useful because traffic conditions and
    passenger demands can vary depending on these factors. However, augmentations
    can increase the complexity of the policy, and care needs to be taken to ensure
    that the policy does not overfit the additional information. Due to this, this
    pattern is generally not explored to its fullest extent. While we see usages of
    augmentations equitably for most use cases in [Figure 5](#S5.F5 "In 5 Patterns
    of Incorporating Structure ‣ Structure in Reinforcement Learning: A Survey and
    Open Problems"), the number of papers utilizing this pattern still falls short
    compared to more established techniques, such as abstraction. In the next paragraphs,
    we delineate three kinds of augmentations in the surveyed work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Latent |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?) | ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Actions | Latent | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Factored | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent | ? (?) | ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Policies | Modular | ? (?), ? (?), ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: Context-based Augmentations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contextual representations of dynamics (?, ?) and goal-related information (?, ?)
    can help with generalization and sample efficiency by exposing the agent to the
    necessary information for optimality. Goal augmentations additionally allow interpretable
    mechanisms for specifying goals (?). On the other hand, augmentation of meta-learned
    latent spaces to the normal state can promote temporally coherent exploration
    across tasks (?). Action histories (?) can directly help with sample efficiency,
    and action relations (?, ?) contribute to generalization over large action sets.
  prefs: []
  type: TYPE_NORMAL
- en: Language Augmentations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Language can explicitly capture relational metadata in the world. Latent language
    interpretation models (?) can utilize the compositionality of language to achieve
    better exploration and generalization to different relational settings, as represented
    by their language descriptions. On the other hand, goal descriptions (?) can help
    hierarchical settings by exploiting semantic relationships between different subtasks
    and producing better goals for lower-level policies. Augmentations can additionally
    help make existing methods more interpretable through methods such as ? (?) by
    guiding search over approximate policies written in human-readable formats.
  prefs: []
  type: TYPE_NORMAL
- en: Control Augmentations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Augmentations can additionally help with primitive control, such as multi-level
    control in hierarchical settings. Augmenting internal latent variables conditioned
    on primitive skills (?, ?, ?) can help tackle sample efficiency in hierarchical
    settings. Augmentations can also help morphological control (?) through methods
    such as ? (?) that model the different limbs as individual agents that need to
    learn to join together into a morphology to solve a task.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Auxiliary Optimization Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This pattern uses structural side information to modify the optimization procedure.
    This includes methods involving contrastive losses, reward shaping, concurrent
    optimization, masking strategies, regularization, baselining, etc. However, given
    that the changes in the optimization can go hand-in-hand with modifications of
    other components, this pattern shares methods with many other patterns. For example,
    contrastive losses can be used to learn state abstractions. Similarly, a learned
    model can be utilized for reward shaping as well. Thus, methods that fall into
    this category simultaneously utilize both patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/9e72ee1dc309034fab511d6985c6523e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of the taxi, reward shaping could help the policy to be reused
    for slight perturbances in the city grid, where the shaped reward encourages the
    taxi to stay near areas where passengers are frequently found when it does not
    have a passenger. It is crucial to ensure that the modified optimization process
    remains aligned with the original objective, i.e., there needs to exist some form
    of regularization that controls how the modification of the optimization procedure
    respects the original objective. For reward shaping techniques, this amounts to
    the invariance of the optimal policy under the shaped reward (?). For auxiliary
    objectives, this manifests in some form of entropy (?) or divergence regularization (?).
    Constraints ensure this through recursion (?), while baselines control the variance
    of updates (?). The strongest use of constraints is in the safety literature,
    where constraints either help control the updates using some safety criterion
    or constrain the exploration. Consequently, in [Figure 5](#S5.F5 "In 5 Patterns
    of Incorporating Structure ‣ Structure in Reinforcement Learning: A Survey and
    Open Problems"), we see that the auxiliary optimization pattern peaks in its proclivity
    towards addressing safety. In the following paragraphs, we cover methods that
    optimize individual aspects of the optimization procedure, namely, rewards, learning
    objectives, constraints, and parallel optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Latent |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored |  |  | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modular | ? (?), ? (?), ? (?), ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?),
    ? (?) |  | ? (?) | ? (?), ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?), ? (?), ? (?) |  |  | ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?) |  | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Actions | Latent | ? (?), ? (?), ? (?), ? (?) | ? (?) | ? (?) | ? (?), ? (?),
    ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?) |  | ? (?) | ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Factored | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?) |  |
    ? (?), ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent | ? (?), ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?), ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Policy Space | Latent | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: Reward Modification.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reward shaping is a common way to incorporate additional information into the
    optimization procedure. Methods can gain sample efficiency by exploiting modular
    and relational decompositions through task descriptions (?), or goal information
    from a higher level policy with off-policy modification to the lower level transitions (?).
    Histories of rewards (?) can help learn symmetric relationships between states
    and, thus, improve the selection procedure for states in a mini-batch for optimization.
    Factorization of states and rewards into endogenous and exogenous factors (?),
    on the other hand, helps with safety and sample efficiency through reward corrections.
  prefs: []
  type: TYPE_NORMAL
- en: Extrinsic Rewards can also be used to guide the exploration process. Symbolic
    planning with relational representations can be used to interact with a primitive
    learning policy through extrinsic rewards in hierarchical settings, thus, adding
    interpretability while directly impacting the exploration through the extrinsic
    reward (?). Alternatively, additional reward sources can help determine the quality
    of counterfactual trajectories, which can help explain why an agent took certain
    kinds of actions (?). Additionally, running averages can rewards be used to adaptively
    tune exploration parameters for heterogeneous action spaces (?)
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, intrinsic rewards can specifically help with exploration
    in sparse reward environments. Latent decompositions can help improve such methods
    by directly impacting the exploration. Language abstractions can serve as latent
    decompositions that can be separately used for exploration (?). Alternatively,
    geometric structures can provide a way to compare state embeddings and provide
    episodic bonuses (?).
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary Learning Objectives.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Skill-based methods transfer skills between agents with different morphology
    by learning invariant subspaces and using those to create a transfer auxiliary
    objective (through a reward signal) (?), or an entropy-based term for policy regularization (?).
    In hierarchical settings, discovering appropriate sub-tasks (?) can be a highly
    sample-inefficient process. ? (?) tackle this by composing values of the sub-trajectories
    under the current policy, which they subsequently use for behavior cloning. Latent
    decompositions can help with robustness and safety when used for some form of
    policy regularization (?). Auxiliary losses, which usually help with generalization,
    can also be a very good entry point for human-like inductive biases (?). Metrics
    inspired by the geometry of latent decompositions can help learn optimal values
    in multi-task settings (?).
  prefs: []
  type: TYPE_NORMAL
- en: Constraints and Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Constrained optimization is commonplace in Safe RL, and incorporating structure
    can help improve the sample efficiency of such methods while making them more
    interpretable. Factorization of states into safe and unsafe states can help develop
    persistent safety conditions (?), or language abstractions (?). Recursive constraints (?)
    can help explicitly condition the optimization on a latent subset of safe actions
    using factored states. Restricting the exploration of options to non-risky states
    can help incorporate safety in hierarchical settings as well (?). Factorized actions
    can also help improve the sample efficiency of policy gradient methods through
    baselining (?, ?), offline methods through direct value conditioning (?), and
    value-based planning through matrix estimation (?)
  prefs: []
  type: TYPE_NORMAL
- en: Methods can also directly incorporate expert domain knowledge directly in the
    action selection mechanism for safety and interpretability (?), or for directed
    exploration to improve sample efficiency (?). Hierarchical settings can benefit
    from latent state decompositions incorporated via modification of the termination
    condition (?). Additionally, state-action equivalences can help scale Q-learning
    to large spaces through factorization (?).
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent Optimization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Parallelizing optimization using structural decompositions can specifically
    help with sample efficiency. Factored MDPs are a very good way to model factors
    that influence the content presented to users and can be used for ensembling methods
    in a parallel regime (?). Similarly, factored rewards in hierarchical settings
    can help decompose Multi-task problems into a linear combination of individual
    task MDPs (?). Alternatively, discretizing continuous sub-actions in multi-dimensional
    action spaces can help extend the MDP for each sub-action to an undiscounted lower-level
    MDP, modifying the backup for the Q values using decompositions (?). Relational
    decompositions can additionally help with masking strategies for Factored Neural
    Networks (?).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Auxiliary Model Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/d8c2f7605a30170a927e830b99f065ec.png)'
  prefs: []
  type: TYPE_IMG
- en: This pattern represents using the structural information in a model. In using
    the term model, we specifically refer to methods that utilize a model of the world
    to generate experiences, either fully or partially. This notion allows us to capture
    a range of methods, from ones using full-scale world models to generate rewards
    and next-state transitions to ones that use these methods to generate full experience
    sequences. In our categorization, we specifically look at how the structure is
    incorporated into such models to help generate some parts of learning experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our taxi agent could learn a latent model of city traffic based on past experiences.
    This model could be used to plan routes that avoid traffic and hence reach destinations
    faster. Alternatively, the agent could learn an ensembling technique to combine
    multiple models, each of which model-specific components of the traffic dynamics.
    With models, there is usually a trade-off between model complexity and accuracy,
    and it is essential to manage this carefully to avoid overfitting and maintain
    robustness. To this end, incorporating structure helps make the model-learning
    phase more efficient while allowing reuse for generalization. Hence, in [Figure 5](#S5.F5
    "In 5 Patterns of Incorporating Structure ‣ Structure in Reinforcement Learning:
    A Survey and Open Problems"), we see that the auxiliary model pattern shows a
    strong proclivity to utilizing structure for sample efficiency. In the following
    paragraphs, we explicitly discuss models that utilize decompositions and models
    used for creating decompositions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Factored |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?),
    ? (?), ? (?), ? (?), ? (?), ? (?) |  | ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?),
    ? (?), ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Actions | Latent | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?), ? (?), ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?), ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modular | ? (?), ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Latent | ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored |  | ? (?) |  | ? (?), ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent | ? (?), ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?),
    ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Relational | ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modular | ? (?), ? (?), ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: Models with structured representations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ? (?) utilize factored decomposition for state space to demonstrate the benefits
    of model-based methods in combinatorially complex environments. Similarly, the
    dreamer models (?, ?) utilize latent representations of pixel-based environments.
  prefs: []
  type: TYPE_NORMAL
- en: Object-oriented representation for states can help bypass the need to learn
    latent factors using CNNs in MBRL (?) or as random variables whose posterior can
    be refined using NNs (?). Graph (Convolutional) Networks (?) can capture rich
    higher-order interaction data, such as crowd navigation ? (?), or invariances (?).
    Action equivalences can help learn latent models (Abstract MDPs) (?) for planning
    and value iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Models for task-specific decompositions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to utilize decompositions in models is to capture task-specific
    decompositions. Models that capture some form of relevance, such as observational
    and interventional data in Causal RL (?), or task-relevant vs. irrelevant data (?)
    can help with generalization and sample efficiency gains. Latent representations
    help models capture control-relevant information (?) or subtask dependencies (?).
  prefs: []
  type: TYPE_NORMAL
- en: Models for safety usually incorporate some measure of cost to abstract safe
    states (?), or unawareness to factor states and actions (?). Alternatively, models
    can also directly guide exploration mechanisms through latent causal decompositions (?)
    and state subspaces (?) to gain sample efficiency. Generative methods such as
    CycleGAN (?) are also very good ways to use Latent models of different components
    of an MDP to generate counterfactual trajectories (?)
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Warehouse Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This pattern uses structural information to create a database of entities that
    can be combined to achieve a specific objective. These can be learned policies
    and value functions or even models. Given the online nature of such methods, they
    are often targeted toward continual and life-long learning problems. The inherent
    modularity in such methods often leads them to focus on knowledge reuse as a central
    theme.
  prefs: []
  type: TYPE_NORMAL
- en: The taxi from our running example could maintain a database of value functions
    or policies for different parts of the city or at different times of the day.
    These could be reused as the taxi navigates through the city, making learning
    more efficient. While warehousing generally can improve efficiency, it has primarily
    been explored through the skills and options framework for targeting generalization.
    An important consideration in warehousing is managing the warehouse’s size and
    diversity to avoid biasing the learning process too much toward past experiences.
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/8d8ac2908ee567ea073ef1c5320e3b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, the warehousing pattern seems to be applied to sample efficiency and
    generalization. However, warehousing also overlaps with interpretability since
    the stored data can be easily used to analyze the agent’s behavior and understand
    the policy for novel scenarios. Consequently, these objectives are equitably distributed
    in [Figure 5](#S5.F5 "In 5 Patterns of Incorporating Structure ‣ Structure in
    Reinforcement Learning: A Survey and Open Problems").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Factored |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational |  |  | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?) | ? (?), ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Actions | Latent |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?), ? (?) | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?),
    ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Factored |  | ? (?), ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modular | ? (?) | ? (?), ? (?), ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Policies | Latent |  | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?), ? (?) | ? (?),
    ? (?), ? (?), ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: Policy Warehousing.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Policy subspaces (?) is a relatively new concept that utilizes shared latent
    parameters in policies to learn a subspace that can be subsequently combined linearly
    to create new policies. Extending these subspaces by warehousing additional policies
    naturally extends them to continual settings (?).
  prefs: []
  type: TYPE_NORMAL
- en: Using goals and rewards, task factorization endows warehousing policies and
    Q values in multi-task lifelong settings. The multi-task lifelong problem can
    also be treated as a relationship graph between existing tasks generated from
    latent space (?). On the other hand, methods such as ? (?) factor MDPs into agent-specific
    and task-specific degrees of variation, for which individual modules can be trained.
    Disentanglement using variational encoder-decoder models (?) can help control
    morphologically different agents by factorizing dynamics into shared and agent-specific
    factors. Additionally, methods such as ? (?) partition the agent’s problem into
    interconnected sub-agents that learn local control policies.
  prefs: []
  type: TYPE_NORMAL
- en: Methods that utilize the skills framework effectively warehouse learned primitives,
    similar to how options warehouse associated policies in HRL. These can subsequently
    be used for maximizing mutual information in lower layers (?), sketching together
    a policy (?), diversity-seeking priors in continual settings (?), or for partitioned
    states spaces (?). Similarly, ? (?) apply the warehouse pattern on a latent embedding
    space, learned using auxiliary optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposed Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Decompositions that inherently exist in models lead to approaches that often
    ensemble multiple models that individually reflect different aspects of the problem.
    Ensemble methods such as ? (?) capture the dynamics in individual modules that
    sparsely interact and use attention mechanisms (?). Ensembling dynamics can also
    help with few-shot adaptation to unseen MDPs (?). Factored models can also be
    combined with relational decompositions to bind actions to object-centric representations ? (?).
    Latent representations in hierarchical settings (?) can additionally improve the
    sample inefficiency of Deep Option Critic (?).
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Environment Generation Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/6ce2b26e0b664f491124867ac8de0f54.png)'
  prefs: []
  type: TYPE_IMG
- en: This pattern uses structural information to create task, goal, or dynamics distributions
    from which MDPs can be sampled. This subsumes the idea of procedurally generated
    environments while additionally incorporating methods that use auxiliary models
    inducing structure in the environment generation process. The decomposition is
    reflected in the aspects of the environment generation that are impacted by the
    generative process, such as dynamics, reward structure, state space, etc. Given
    the online nature of this pattern, methods in this pattern end up addressing curriculum
    learning in one way or another.
  prefs: []
  type: TYPE_NORMAL
- en: In the taxi example, a curriculum of tasks could be generated, starting with
    simple tasks (like navigating an empty grid) and gradually introducing complexity
    (like adding traffic and passengers with different destinations). Ensuring that
    the generated MDPs provide good coverage of the problem space is crucial to avoid
    overfitting to a specific subset of tasks. This necessitates additional diversity
    constraints that must be incorporated into the environment generation process.
    Structure, crucially, provides additional interpretability and controllability
    in the environment generation process, thus, making benchmarking easier than methods
    that use unsupervised techniques (?).
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Relational | ? (?), ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?) | ? (?) | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?), ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Latent |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent |  | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?), ? (?) | ? (?), ? (?), ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?), ? (?) | ? (?), ? (?) | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular | ? (?), ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: The compositional nature of learning problems can be modeled using rule-based
    grammar. ? (?) particularly utilize this to impact the transition dynamics and
    generate environments. This allows them to train agents with an implicit compositional
    curriculum. This is further used by ? (?) in their auxiliary optimization procedure.
    Another way to capture task dependencies is through latent graphical models, which
    can be used to generate the state-space, reward functions, and transition dynamics (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Latent dynamics models allow simulating task distributions, which can help with
    generalization (?). Clustering methods such as (?), on the other hand, explore
    task similarities by meta-learning a clustering method through an exploration
    policy. In a way, they recover a factored decomposition on the task space where
    individual clusters can be further used for policy adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Explicitly Designed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This pattern encompasses all methods where the inductive biases manifest in
    specific architectures or setups that reflect the decomposability of the problem
    that they aim to utilize. Naturally, this includes highly specific Neural architectures,
    but it also easily extends to other methods like sequential architectures to capture
    hierarchies, relations, etc. Crucially, the usage of structural information is
    limited to the specificity of the architecture and not any other part of the pipeline.
    In the case of the taxi, a neural architecture could be designed to process the
    city grid as an image and output a policy. Techniques like convolutional layers
    could be used to capture the spatial structure of the city grid. Different network
    parts could be specialized for different subtasks, like identifying passenger
    locations and planning routes. However, this pattern involves a considerable amount
    of manual tuning and experimentation, and it’s critical to ensure that these designs
    generalize well across different tasks. Designing specific neural architectures
    can provide better interpretability, enabling the ability to decompose different
    components and simulate them independently. Consequently, this pattern shows the
    highest proclivity to interpretability, with Generalization being a close second
    in [Figure 5](#S5.F5 "In 5 Patterns of Incorporating Structure ‣ Structure in
    Reinforcement Learning: A Survey and Open Problems").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Space | Type | Efficiency | Generalization | Interpretabiltiy | Safety |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Goals | Factored | ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| States | Latent | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?),? (?),? (?),? (?) | ? (?),? (?),? (?),? (?),? (?), ? (?)
    | ? (?), ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modular |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Actions | Latent | ? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?) |  | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relational | ? (?) |  | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rewards | Latent |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Factored |  |  |  | ? (?) |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics | Latent | ? (?) | ? (?), ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Factored | ? (?),? (?) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Relational |  | ? (?) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Policies | Relational | ? (?), ? (?) | ? (?) | ? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modular |  | ? (?) | ? (?),? (?) |  |'
  prefs: []
  type: TYPE_TB
- en: Splitting Functionality.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to bias the architecture is to split its functionality into different
    parts. Most of the works that achieve such disambiguation are either Factored
    or Relational. Structured Control Nets (?) model linear and non-linear aspects
    of the dynamics individually and combine them additively to gain sample efficiency
    and generalization. Alternatively, Bi-linear Value Networks (?) architecturally
    decompose dynamics into state and goal-conditioned components to produce a goal-conditioned
    Q-function. Action Branching architectures (?) used a shared representation that
    is then factored into separate action branches for individual functionality. This
    approach bears similarity to capturing multi-task representations using bottlenecks (?).
  prefs: []
  type: TYPE_NORMAL
- en: Relational and Modular biases manifest in hierarchical architectures. This also
    allows them to add more interpretability to the architecture. Two-step hybrid
    policies (?), for example, demonstrate an explicit method to make policies more
    interpretable through splitting actions into pruners and selector modules. On
    the other hand, routing hierarchies explicitly capture modularity using sub-modules
    that a separate policy can use for routing them (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Capturing Invariances in Architectures.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Specialized architectures can also help capture invariance in the problem. Symbolic
    Networks (?, ?) train a set of shared parameters for Relational MDPs by first
    converting them to Graphs and then capturing node embeddings using Neural Networks.
    Homomorphic Networks (?) capture symmetry into specialized MLP and CNN architectures.
    An alternate approach to incorporating symmetry is through basis functions (?).
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms can explicitly capture entity-factored scenarios (?, ?, ?).
    Relational and Graph Networks can capture additional relational inductive biases
    explicitly. Linear Relation Networks (?) provides an architecture that scales
    linearly with the number of objects. Graph networks have also been used to model
    an agent’s morphology in embodied control explicitly (?, ?).
  prefs: []
  type: TYPE_NORMAL
- en: Specialized Modules.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A class of methods combines the best of both worlds by capturing invariance
    in additional specialized modules. Such modules can capture relational structure
    in semantic meaning (?), relational encoders for auxiliary models (?), or specialized
    architectures for incorporating domain knowledge (?).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Open Problems in Structured Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having discussed our patterns-oriented framework for understanding how to incorporate
    structure into the RL pipeline, we now turn to connect our framework with existing
    sub-fields of RL. We examine existing paradigms in these sub-fields from two major
    perspectives: Scalability and Robustness. These dimensions serve as a canvas upon
    which we can position and understand different RL paradigms in sub-fields such
    as Offline RL, Unsupervised RL, Foundation Models in RL, Partial observability,
    Big Worlds, Automated RL, and Meta-RL.'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse data scenarios require more intelligent ways to use limited experiences,
    while abundant data scenarios might suffer from data quality since they might
    be generated from noisy and often unreliable sources.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: measures how well methods scale with the increasing problem complexity in terms
    of the size of the state and action spaces, complex dynamics, noisy reward signals,
    and longer task horizons. On one hand, methods might specifically require low
    dimensional spaces and might not scale so well with increasing the size of these
    spaces, and on the other, some methods might be overkill for simple problems but
    better suited for large spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: measures the response of methods to changes in the environment. While the notion
    overlaps with generalization, robustness for our purposes more holistically looks
    at central properties of data distribution, such as initial state distributions
    and multi-modal evaluation returns. Under this notion, fundamentally different
    learning dynamics might be robust to different kinds of changes in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Structure of the Section.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following subsections, we cover sub-fields of RL that lie at different
    points of the 2D space of Scalability and Robustness. We introduce the existing
    paradigms and the current challenges for each sub-field. We then present some
    examples in which our framework can bolster further research and practice in these
    fields. Finally, we collate this discussion into takeaways that can be combined
    into specific design patterns utilizing our framework.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Offline RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Offline Reinforcement Learning (also known as batch RL) (?) involves learning
    from a fixed dataset without further interaction with the environment. This approach
    can benefit when active exploration is costly, risky, or infeasible. Consequently,
    such methods are highly data-dependent due to their reliance on the collected
    dataset, and they do not generalize well due to the limitations of the pre-collected
    data. The three dominant paradigms in Offline RL – Behavior Cloning (?), Q-Learning (?),
    and Sequence Modelling (?) – uniformly degrade in performance as the state-space
    increases (?). Offline RL also comes with its own challenges, including overcoming
    distributional shifts and exploiting the available dataset effectively. Structural
    decomposition can play a crucial role in addressing these challenges in the following
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Improved Exploitation of Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Task decomposition allows learning individual policies or value functions for
    different subtasks, which could potentially leverage the available data more effectively.
    For example, modular decomposition through warehousing separate policies for individual
    modules using the corresponding subset of the data might be more sample-efficient
    than learning a single policy for the entire task. Task decompositions, thus,
    open up new avenues for developing specialized algorithms that effectively learn
    from limited data about each subtask while balancing the effects of learning different
    subtasks. Practitioners can leverage such decompositions to maximize the utility
    of their available datasets by training models that effectively handle specific
    subtasks, potentially improving the overall system’s performance with the same
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating Distributional Shift.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The structural information could potentially help mitigate the effect of distributional
    shifts. For instance, if some factors are less prone to distributional shifts
    in a factored decomposition, we could focus more on those factors during learning.
    This opens up venues for gaining theoretical insights into the complex interplay
    of structural decompositions, task distributions, and policy performance. On the
    other hand, practical methods for environments where distributional shifts are
    common could leverage structural decomposition to create more robust RL systems.
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary Tasks for Exploration.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Structural decomposition can be used to define auxiliary tasks that facilitate
    learning from the dataset. For instance, in a relational decomposition, we could
    define auxiliary tasks that involve predicting the relationships between different
    entities, which could help in learning a useful representation of the data. Using
    the proposed framework, researchers can explore how to define meaningful auxiliary
    tasks that help the agent learn a better representation of the environment. This
    could lead to new methods that efficiently exploit the available data by learning
    about these auxiliary tasks. Practitioners can design auxiliary tasks based on
    the specific decompositions of their problem. For example, if the task has a clear
    relational structure, auxiliary tasks that predict the relations between different
    entities can potentially improve the agent’s understanding of the environment
    and its overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS1.SSS0.Px3.p2.pic1" class="ltx_picture" height="89.38" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,89.38) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 22.05 14.17)"><foreignobject width="555.91" height="61.04" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Offline RL Patterns • Use a
    Factored or Relational decomposition to create abstractions that can help with
    distribution shift and auxiliary interpretability. • Implement a Modular design
    with each module targeting a specific sub-problem, improving Scalability. • Employ
    policy reuse by warehousing policies learned for sub-problems across tasks. •
    If sufficient interaction data is available, employ data augmentation strategies
    for counterfactual scenarios using latent models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Unsupervised RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised RL (?) refers to the sub-field of behavior learning in RL, where
    an agent learns to interact with an environment without receiving explicit feedback
    or guidance in the form of rewards. Methods in this area can be characterized
    based on the nature of the metrics that are used to evaluate performance intrinsically (?).
    *Knowledge-based* methods define a self-supervised task by making predictions
    on some aspect of the environment (?, ?, ?), *Data-based methods* maximize the
    state visitation entropy for exploring the environment (?, ?, ?, ?, ?), and *Competence-based*
    methods maximize the mutual information between the trajectories and space of
    learned skills (?, ?, ?, ?, ?). The pre-training phase allows these methods to
    learn the underlying structure of data. However, this phase also requires large
    amounts of data and, thus, impacts the scalability of such methods for problems
    where the learned representations are not very useful. Consequently, such methods
    currently handle medium complexity problems, with the avenue of better scalability
    being a topic of further research.
  prefs: []
  type: TYPE_NORMAL
- en: Structural decompositions can help such methods by improving the pre-training
    phase’s tractability and the fine-tuning phase’s generality. Latent decompositions
    could help exploit structure in unlabeled data, while relational decompositions
    could add interpretability to the learned representations. Through augmentation,
    conditioning policies on specific parts of the state space can reduce the amount
    of data needed for fine-tuning. Additionally, understanding problem decomposition
    can simplify complex problems into more manageable sub-problems, effectively reducing
    the perceived problem complexity while incorporating such decomposition in external
    curricula for fine-tuning. Incorporating warehousing guided by decompositions
    for competence-based methods can boost the fine-tuning process of the learned
    skills.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS2.p3.pic1" class="ltx_picture" height="105.99" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,105.99) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject width="555.91"
    height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Unsupervised
    RL Patterns • Use latent decompositions to extract structure from unlabeled data,
    reducing Data Dependency. • Employ factored and modular decompositions and abstractions
    to manage scalability by focusing learning on different parts of the problem independently.
    • Warehouse skills across different modular sub-problems to reuse solutions and
    enhance Generality. • Manage Problem Complexity by leveraging problem decomposition
    to simplify the learning task and using decompositions for fine-tuning using curriculum
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Big Data and Foundation models in RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Foundation models (?, ?, ?) refer to a paradigm where a large model is pre-trained
    on large and heterogeneous datasets and fine-tuned for specific tasks. These models
    are “foundational” in the sense that they can serve as the basis for a wide range
    of tasks, reducing the need for training separate models for each task from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models for RL come increasingly closer to becoming a reality. Such
    RL models would follow a similar concept of training a large model on various
    tasks, environments, and behaviors to be fine-tuned for specific downstream tasks.
    SMART (?), one of the current contenders for such models, follows this paradigm
    by using a self-supervised and control-centric objective that encourages the transformer-based
    model to capture control-relevant representation and demonstrates superior performance
    when used for fine-tuning. AdA (?) trains an in-context learning agent on a vast
    distribution of tasks where the task factors are generated from a latent ruleset.
  prefs: []
  type: TYPE_NORMAL
- en: Given the pre-training paradigm, these methods are highly data-dependent in
    principle. However, incorporating large amounts of data can demonstrate scalability
    benefits by reducing fine-tuning costs for distributed applications. A natural
    question that arises is the role of Structured RL in the realm of end-end learning
    and big data. Even though such methods subscribe to an end-to-end paradigm, structural
    decompositions can benefit them differently.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability and Selection during Fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Researchers can better understand the decomposability in the pre-trained models
    by categorizing methods based on how they incorporate structure. Consequently,
    this can guide the selection processes for fine-tuning methods depending on the
    tasks at hand. Passive learning from pre-trained models can benefit from better
    explanations about what parts of a fine-tuning task space might be suited for
    what kind of warmstarting strategies. Additionally, incorporating interpretability-oriented
    decompositions such as relational representations can help design more interpretable
    fine-tuning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Task-Specific Architectures and Algorithms.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Structural information can guide the development of novel architectures. With
    a better understanding of how different architectures and algorithms incorporate
    structural information, practitioners can more effectively adapt existing methods
    or contribute to designing novel solutions tailored to their specific tasks. For
    example, Action Branching architectures might provide modular functionality in
    downstream tasks, especially suited for multi-task settings. On the other hand,
    representation bottlenecks might suit settings that deviate from each other by
    small changes in contextual features.
  prefs: []
  type: TYPE_NORMAL
- en: Improved Fine-Tuning and Transfer Learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By understanding how to decompose tasks and incorporate structural information,
    foundational models can be fine-tuned more effectively for specific tasks or transferred
    to new tasks. The understanding of decompositions could guide how to structure
    the fine-tuning process or how to adapt the foundational model to a new task.
    By understanding how to incorporate structural information during fine-tuning,
    they can potentially achieve improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking and Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By understanding the spectrum of decomposability and how various methods incorporate
    structure, we can create better benchmarks and evaluation protocols for foundational
    models. For instance, we can evaluate how well foundational models handle tasks
    of different decompositions and patterns of fine-tuning. Researchers can use this
    framework to design better evaluation protocols and benchmarks for foundational
    models. For practitioners, such benchmarks and evaluation protocols can guide
    the selection of models and algorithms for their specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS3.SSS0.Px4.p2.pic1" class="ltx_picture" height="155.8" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,155.8) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 22.05 14.17)"><foreignobject width="555.91" height="127.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Foundation Model Patterns •
    Use Factored or Relational abstractions on the pre-trained foundation model for
    state abstractions to manage high-dimensional state spaces and, thus, reduce data
    dependency. • Condition the policy on additional task-specific information, such
    as goal information, representation of specific fine-tuning instructions, or control
    priors to improve scalability. • Regularize the fine-tuning process to prevent
    catastrophic forgetting of useful features learned during pre-training. • Maintain
    a warehouse of fine-tuned policies and value functions to help reuse previously
    learned skills and adapt them to new tasks, improving learning efficiency and
    generalization. • Incorporate a curriculum of increasingly complex fine-tuning
    environments based on the agent’s performance to help the agent gradually adapt
    the foundation model’s knowledge to the specific RL task. • Use explicit architectures
    that fine-tune different RL problem aspects, such as perception, policy learning,
    and value estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Partial observability and Big Worlds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many real-world situations, the Markov property might not fully capture the
    dynamics of the environment (?, ?). This can happen in cases where the environment’s
    state or the rewards depend on more than just the most recent state and action
    or if the agent cannot fully observe the state of the environment at each time
    step. In such situations, methods must deal with non-Markovian dynamics and partial
    observability.
  prefs: []
  type: TYPE_NORMAL
- en: Abstractions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Abstractions can play a crucial role in such situations, where structural decompositions
    using abstraction patterns can make methods more sample efficient. Often used
    in options, temporal Abstractions allow the agent to make decisions over extended
    periods, thereby encapsulating potential temporal dependencies within these extended
    actions. This can effectively convert a non-Markovian problem into a Markovian
    one at the level of options. State Abstractions abstract away irrelevant aspects
    of the state and, thus, can sometimes ignore certain temporal dependencies, rendering
    the process Markovian at the level of the abstracted states. Thus, research into
    the role of decompositions in abstraction opens up possibilities to understand
    the dependencies between non-Markovian models and the kind of abstractions they
    use to solve problems with incomplete information. Abstraction can also simplify
    the observation space in POMDPs, reducing the complexity of the belief update
    process. The abstraction might involve grouping similar observations together,
    identifying higher-level features, or other simplifications. Abstractions can
    additionally allow us to break partial observability down into different types
    instead of always assuming the worst-case scenario. Utilizing such restricted
    assumptions on partial observability can help us build more specific algorithms
    and derive convergence and optimality guarantees for such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any additional information required, such as belief states or memory of past
    observations, can be used as abstractions or augmentations. This can also help
    with more efficient learning of transition models for planning. Hierarchical techniques
    that utilize optimization at different timescales can incorporate warehousing
    to reuse learned policies across various levels of abstraction. Environment generation
    patterns could also be used to generate a curriculum of increasingly complex tasks
    for the agent, starting with simpler MDPs and gradually introducing partial observability
    or other non-Markovian features.
  prefs: []
  type: TYPE_NORMAL
- en: Big worlds.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we extend the information content of the environment to its extremity, we
    delve into the realm of the big world hypothesis in RL (?), where the agent’s
    environment is multiple orders of magnitude larger than the agent, and the agent
    cannot represent the optimal value function and policy even in the limit of infinite
    data. In such scenarios, the agent must make decisions under significant uncertainty,
    which presents several challenges, including exploration, generalization, and
    efficient learning. Even though the hypothesis suggests that incorporating side
    information might not be beneficial in learning the optimal policy and value in
    such scenarios, structural decomposition of large environments in different ways
    can allow benchmarking methods along different axes while allowing a deeper study
    into the performance of algorithms on parts of the environment that the agent
    has not yet experienced.
  prefs: []
  type: TYPE_NORMAL
- en: Modular decomposition can guide the agent’s exploration process by helping the
    agent explore different parts of the environment independently. Incorporating
    modularity opens a gateway to novel methods and theoretical insights about the
    relationships between task decomposition, exploration, and learning efficiency
    in large environments. Relational decompositions can help the agent learn relationships
    between different entities, bolstering its ability to generalize to unseen parts
    of the environment. Finally, Structural information can be used to facilitate
    more efficient learning. For instance, in an auxiliary optimization pattern, the
    agent could learn faster by optimizing auxiliary tasks that are easier to learn
    or provide useful information about the environment’s structure.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS4.SSS0.Px3.p3.pic1" class="ltx_picture" height="138.9" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,138.9) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 22.05 14.17)"><foreignobject width="555.91" height="110.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Patterns for Partial Observability
    and Big Worlds • Use temporal and state abstraction to abstract away temporal
    dependencies and non-Markovian aspects of the state. Utilize Modularity to tie
    these abstractions to learned primitives such as skills or options. • Use memory
    more efficiently as an abstraction or augmentation for learned transition models.
    • Warehouse policies and utilize them for optimization across timescales, such
    as in hierarchical methods, to make them more tractable. • Utilize modular decompositions
    for guiding separate and parallel exploration mechanisms for different parts of
    the state space. Utilize relational abstractions to make this knowledge more interpretable.
    • Utilize structure for task factorization to guide benchmarking methods along
    different axes of task complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Automated RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automated RL (AutoRL) is a sub-field focused on methods to automate the process
    of designing and optimizing RL algorithms, including the agent’s architecture,
    reward function, and other hyperparameters (?). Methods in AutoRL can be placed
    on a spectrum of automation, where on one end would be methods to select pipelines
    and on the other would be methods that try to discover new algorithms ground-up
    in a data-driven manner (?). Techniques from the Automated Machine Learning literature (?)
    then transfer to the RL setting, including algorithm selection (?), hyperparameter
    optimization (?, ?, ?), dynamic configurations (?), learned optimizers (?), and
    neural architecture search (?). Similarly, techniques from the Evolutionary optimization
    and Meta-Learning literature naturally transfer to this setting with methods aiming
    to meta-learn parts of the RL pipeline such as update rules (?), loss functions (?, ?),
    symbolic representations of algorithms (?, ?, ?), or concept drift (?). However,
    there are still many open questions in AutoRL, such as properties of hyperparameter
    landscapes in RL (?), sound evaluation protocols (?), stability of training due
    to the non-stationary learning task and non-deterministic data collection on the
    fly. Consequently, most of these methods suffer from a lack of scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm Selection and Configuration.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depending on the decomposability of the problem at hand, different RL methods
    could be more appropriate. Structural decompositions can guide the selection process
    in AutoRL by suggesting appropriate types of decompositions based on the problem
    characteristics. Understanding how different decomposition types influence the
    performance of RL methods can bridge the gap between selection and configuration
    by helping researchers understand the level of abstraction needed for selection
    conditioned on the task, aiding in developing more efficient and targeted search
    algorithms. Decomposability can also guide ranking procedures, where methods that
    cater to different decomposability can be ranked differently, given a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Parameters related to structural decomposition (e.g., the number of subtasks
    in a modular decomposition) could be part of the hyperparameter optimization process
    in AutoRL. Researchers can investigate the interplay between configuration spaces
    of hyperparameters and various structural decomposition-related parameters. For
    example, high decomposability might require different exploration rates or learning
    rates than a low decomposability problem. This could lead to novel insights and
    methods for more effective hyperparameter optimization in AutoRL. Practitioners
    can use this understanding to guide the hyperparameter optimization process in
    their AutoRL system. By tuning parameters related to the decomposition, they can
    potentially improve the performance of their RL agent.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS5.SSS0.Px2.p2.pic1" class="ltx_picture" height="105.99" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,105.99) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject width="555.91"
    height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">AutoRL
    Patterns • Use methods to perform a decomposability assessment of a problem. This
    can help guide algorithm selection for problems with different types of decomposability.
    • Expedite the hyperparameter search by abstracting away task-irrelevant aspects.
    • Warehouse to reuse learned policy and value functions for landmarking performances
    of algorithms similar to the one being optimized. • Incorporate modularity information
    in the form of goals and task hierarchies in the search process. • Structure neural
    architecture search-space using decomposability in the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Meta-Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta-Reinforcement Learning, while having overlaps with AutoRL, is a field in
    and of itself (?) that focuses on training agents to adapt and learn new tasks
    or environments quickly. The general Meta-RL setup involves a bi-level optimization
    procedure where an agent learns a set of parameters by training on a distribution
    of tasks or environments that help it adapt and perform well on new, unseen tasks
    that share some form of overlap with the training tasks. ? (?) outline different
    problem settings in Meta-RL based on the kind of feedback (supervised, unsupervised,
    rewards) that is provided to the agent during the training and adaptation phases.
    We particularly refer to the standard setting where extrinsic rewards act as feedback
    during the training and adaptation phases. However, decompositions can also be
    useful for other settings similar to those discussed in other sections.
  prefs: []
  type: TYPE_NORMAL
- en: Task Decompositions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depending on the meta-task’s decomposability, different task decomposition approaches
    could be employed to guide the meta-learning process. Consequently, understanding
    how task decomposition affects Meta-RL can guide the development of more effective
    meta-learning algorithms. It might also lead to new insights on balancing learning
    between different subtasks. By identifying suitable decompositions, practitioners
    can set up their system to learn in a way that is more aligned with the structure
    of the tasks, potentially leading to improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptation Strategies.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Decompositions could inform the way a Meta-RL agent adapts to a new task. For
    instance, if the new task is highly decomposable, a modular adaptation strategy
    could be more appropriate by guiding the agent to an appropriate latent space
    of the new task. Thus, our framework of decomposability can inspire new research
    into how the task’s decomposition, f can guide adaptation strategies in Meta-RL.
    This could lead to novel methods or theories on adapting to new tasks more effectively
    based on their structure.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS6.SSS0.Px2.p2.pic1" class="ltx_picture" height="105.99" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,105.99) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 22.05 14.17)"><foreignobject width="555.91"
    height="77.64" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Meta-RL
    Patterns • Use decompositions for abstracting task distributions, which can be
    integrated into the adaptation process. • Compartmentalize the learning process
    into modules for highly decomposable problems. These modules can serve as abstract
    configurations for the meta-level and, thus, make the outer loop more tractable
    • Learn and warehouse models geared towards specific task clusters for different
    decomposability types to guide data augmentation during the adaptation phase.
    • Utilize decomposability to design learning curricula based on abstract types
    of tasks to train the warmstarting configurations
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the intricacies of Reinforcement Learning (RL) ’s complexities
    is challenging, exacerbated by the divergent methodologies employed across different
    problem domains. This fragmentation hinders the development of unifying principles
    and consistent practices in RL. To address this critical gap, we propose an innovative
    framework to understand different methods of effectively integrating the inherent
    structure of learning problems into RL algorithms. Our work serves as a pivotal
    step towards consolidating the multifaceted aspects of RL, ushering in a design
    pattern perspective for this domain.
  prefs: []
  type: TYPE_NORMAL
- en: We first conceptualized structure as side information about the decomposability
    of a learning problem and corresponding solutions. We have categorized decomposability
    into four distinct archetypes - latent, factored, relational, and modular. This
    classification delineates a spectrum that establishes insightful connections with
    existing literature, elucidating the diverse influence of structure within RL.
  prefs: []
  type: TYPE_NORMAL
- en: We then presented seven key patterns following a thorough analysis of the RL
    landscape - abstraction, augmentation, auxiliary optimization, auxiliary model,
    warehousing, environment generation, and explicitly designed patterns. These patterns
    represent strategic approaches for the incorporation of structural knowledge into
    RL. Although our framework provides a comprehensive starting point, we acknowledge
    that these patterns are not exhaustive. We envisage this as an impetus for researchers
    to refine and develop new patterns, thereby expanding the repertoire of design
    patterns in RL.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, our work offers a pattern-centric perspective on RL, underlining
    the critical role of structural decompositions in shaping both present and future
    paradigms. By promoting this perspective, we aim to stimulate a new wave of research
    in RL, enriched by a deeper and more structured understanding of the field. While
    our proposed framework is a novel contribution, it should be viewed as an initial
    step in an ongoing process. We anticipate and encourage further development and
    refinement of our framework and eagerly await the emergence of new, innovative
    patterns that will undoubtedly shape the future of RL.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs: []
  type: TYPE_NORMAL
- en: The authors thank Robert Kirk and Rohan Chitnis for their discussion and comments
    on drafts of this work. We would also like to thank Vincent François-Lavet, Khimya
    Khetrapal, and Rishabh Aggarwal for providing additional relevant references in
    the literature.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abdulhai et al. Abdulhai, M., Kim, D., Riemer, M., Liu, M., Tesauro, G., and How,
    J. (2022). Context-specific representation abstraction for deep option learning..
    In Sycara et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abel, Hershkowitz, Barth-Maron, Brawner, O’Farrell, MacGlashan, and Tellex Abel,
    D., Hershkowitz, D., Barth-Maron, G., Brawner, S., O’Farrell, K., MacGlashan,
    J., and Tellex, S. (2015). Goal-based action priors. In Proceedings of the Twenty-Fifth
    International Conference on Automated Planning and Scheduling, (ICAPS’15).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abel, Umbanhowar, Khetarpal, Arumugam, Precup, and Littman Abel, D., Umbanhowar,
    N., Khetarpal, K., Arumugam, D., Precup, D., and Littman, M. (2020). Value preserving
    state-action abstractions. In Proceedings of the 23rd International Conference
    on Artificial Intelligence and Statistics, (AISTATS’20), pp. 1639–1650.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjodah, Klinger, and Joseph Adjodah, D., Klinger, T., and Joseph, J. (2018).
    Symbolic relation networks for reinforcement learning. In Proceedings of the Workshop
    on Relational Representation Learning in Conference on Neural Information Processing
    Systems (NeurIPS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adriaensen, Biedenkapp, Shala, Awad, Eimer, Lindauer, and Hutter Adriaensen,
    S., Biedenkapp, A., Shala, G., Awad, N., Eimer, T., Lindauer, M., and Hutter,
    F. (2022). Automated dynamic algorithm configuration. Journal of Artificial Intelligence
    Research (JAIR), 75, 1633–1699.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal, Machado, Castro, and Bellemare Agarwal, R., Machado, M., Castro, P.,
    and Bellemare, M. (2021). Contrastive behavioral similarity embeddings for generalization
    in reinforcement learning. In Proceedings of the Ninth International Conference
    on Learning Representations, (ICLR’21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alabdulkarim and Riedl Alabdulkarim, A.,  and Riedl, M. (2022). Experiential
    explanations for reinforcement learning. CoRR, abs/2210.04723.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alet, Schneider, Lozano-Pérez, and Kaelbling Alet, F., Schneider, M., Lozano-Pérez,
    T., and Kaelbling, L. (2020). Meta-learning curiosity algorithms. In Proceedings
    of the Eighth International Conference on Learning Representations (ICLR’20).
    OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allen, Parikh, Gottesman, and Konidaris Allen, C., Parikh, N., Gottesman, O.,
    and Konidaris, G. (2021). Learning markov state abstractions for deep reinforcement
    learning.. In Ranzato et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amin, Gomrokchi, Aboutalebi, Satija, and Precup Amin, S., Gomrokchi, M., Aboutalebi,
    H., Satija, H., and Precup, D. (2021a). Locally persistent exploration in continuous
    control tasks with sparse rewards.. In Meila, and Zhang (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amin, Gomrokchi, Satija, van Hoof, and Precup Amin, S., Gomrokchi, M., Satija,
    H., van Hoof, H., and Precup, D. (2021b). A survey of exploration methods in reinforcement
    learning. CoRR, abs/2109.00157.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andersen and Konidaris Andersen, G.,  and Konidaris, G. (2017). Active exploration
    for learning symbolic representations.. In Guyon et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andreas, Klein, and Levine Andreas, J., Klein, D., and Levine, S. (2018). Learning
    with latent language. In Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azizzadenesheli, Lazaric, and Anandkumar Azizzadenesheli, K., Lazaric, A., and Anandkumar,
    A. (2016). Reinforcement learning in rich-observation mdps using spectral methods.
    CoRR, abs/1611.03907.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bacon, Harb, and Precup Bacon, P., Harb, J., and Precup, D. (2017). The option-critic
    architecture. In S.Singh,  and Markovitch, S. (Eds.), Proceedings of the Thirty-First
    Conference on Artificial Intelligence (AAAI’17). AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baheri Baheri, A. (2020). Safe reinforcement learning with mixture density
    network: A case study in autonomous highway driving. CoRR, abs/2007.01698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bain and Sammut Bain, M.,  and Sammut, C. (1995). A framework for behavioural
    cloning. In Furukawa, K., Michie, D., and Muggleton, S. (Eds.), Machine Intelligence
    15, Intelligent Agents [St. Catherine’s College, Oxford, UK, July 1995], pp. 103–129\.
    Oxford University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balaji, Christodoulou, Jeon, and Bell-Masterson Balaji, B., Christodoulou,
    P., Jeon, B., and Bell-Masterson, J. (2020). Factoredrl: Leveraging factored graphs
    for deep reinforcement learning. In NeurIPS Deep Reinforcement Learning Workshop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bapst, Sanchez-Gonzalez, Doersch, Stachenfeld, Kohli, Battaglia, and Hamrick
    Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K., Kohli, P., Battaglia,
    P., and Hamrick, J. (2019). Structured agents for physical construction.. In Chaudhuri, and Salakhutdinov
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barreto, Borsa, Hou, Comanici, Aygün, Hamel, Toyama, Hunt, Mourad, Silver, and Precup
    Barreto, A., Borsa, D., Hou, S., Comanici, G., Aygün, E., Hamel, P., Toyama, D.,
    Hunt, J., Mourad, S., Silver, D., and Precup, D. (2019). The option keyboard:
    Combining skills in reinforcement learning.. In Wallach et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barreto, Borsa, Quan, Schaul, Silver, Hessel, Mankowitz, Zidek, and Munos Barreto,
    A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek,
    A., and Munos, R. (2018). Transfer in deep reinforcement learning using successor
    features and generalised policy improvement.. In Dy, and Krause (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt, and Silver Barreto, A., Dabney,
    W., Munos, R., Hunt, J., Schaul, T., van Hasselt, H., and Silver, D. (2017). Successor
    features for transfer in reinforcement learning.. In Guyon et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bauer, Baumli, Baveja, Behbahani, Bhoopchand, Bradley-Schmieg, Chang, Clay,
    Collister, Dasagi, Gonzalez, Gregor, Hughes, Kashem, Loks-Thompson, Openshaw,
    Parker-Holder, Pathak, Nieves, Rakicevic, Rocktäschel, Schroecker, Sygnowski,
    Tuyls, York, Zacherl, and Zhang Bauer, J., Baumli, K., Baveja, S., Behbahani,
    F., Bhoopchand, A., Bradley-Schmieg, N., Chang, M., Clay, N., Collister, A., Dasagi,
    V., Gonzalez, L., Gregor, K., Hughes, E., Kashem, S., Loks-Thompson, M., Openshaw,
    H., Parker-Holder, J., Pathak, S., Nieves, N., Rakicevic, N., Rocktäschel, T.,
    Schroecker, Y., Sygnowski, J., Tuyls, K., York, S., Zacherl, A., and Zhang, L.
    (2023). Human-timescale adaptation in an open-ended task space. CoRR, abs/2301.07608.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baumli, Warde-Farley, Hansen, and Mnih Baumli, K., Warde-Farley, D., Hansen,
    S., and Mnih, V. (2021). Relative variational intrinsic control.. In Yang et al.
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beck, Vuorio, Liu, Xiong, Zintgraf, Finn, and Whiteson Beck, J., Vuorio, R.,
    Liu, E., Xiong, Z., Zintgraf, L., Finn, C., and Whiteson, S. (2023). A survey
    of meta-reinforcement learning. CoRR, abs/2301.08028.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellman Bellman, R. (1954). Some applications of the theory of dynamic programming
    - A review. Oper. Res., 2(3), 275–288.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belogolovsky, Korsunsky, Mannor, Tessler, and Zahavy Belogolovsky, S., Korsunsky,
    P., Mannor, S., Tessler, C., and Zahavy, T. (2021). Inverse reinforcement learning
    in contextual mdps. Mach. Learn., 110(9), 2295–2334.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio, Wallach, Larochelle, Grauman, Cesa-Bianchi, and Garnett Bengio, S.,
    Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (Eds.).
    (2018). Proceedings of the 31st International Conference on Advances in Neural
    Information Processing Systems (NeurIPS’18). Curran Associates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benjamins, Eimer, Schubert, Mohan, Döhler, Biedenkapp, Rosenhahn, Hutter, and Lindauer
    Benjamins, C., Eimer, T., Schubert, F., Mohan, A., Döhler, S., Biedenkapp, A.,
    Rosenhahn, B., Hutter, F., and Lindauer, M. (2023). Contextualize me - the case
    for context in reinforcement learning. Transactions on Machine Learning Research,
    2835-8856.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bewley and Lecune Bewley, T.,  and Lecune, F. (2022). Interpretable preference-based
    reinforcement learning with tree-structured reward functions. In 21st International
    Conference on Autonomous Agents and Multiagent Systems, AAMAS 2022, Auckland,
    New Zealand, May 9-13, 2022. International Foundation for Autonomous Agents and
    Multiagent Systems (IFAAMAS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyret, Shafti, and Faisal Beyret, B., Shafti, A., and Faisal, A. (2019). Dot-to-dot:
    Explainable hierarchical reinforcement learning for robotic manipulation. In International
    Conference on Intelligent Robots and Systems, (IROS’19), pp. 5014–5019\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhargava, Chitnis, Geramifard, Sodhani, and Zhang Bhargava, P., Chitnis, R.,
    Geramifard, A., Sodhani, S., and Zhang, A. (2023). Sequence modeling is a robust
    contender for offline reinforcement learning. CoRR, abs/2305.14550.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatt, Tjanaka, Fontaine, and Nikolaidis Bhatt, V., Tjanaka, B., Fontaine, M.,
    and Nikolaidis, S. (2022). Deep surrogate assisted generation of environments.
    In Proceedings of the 35th International Conference on Advances in Neural Information
    Processing Systems (NeurIPS’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biza, Kipf, Klee, Platt, van de Meent, and Wong Biza, O., Kipf, T., Klee, D.,
    Platt, R., van de Meent, J., and Wong, L. (2022a). Factored world models for zero-shot
    generalization in robotic manipulation. In arXiv preprint arXiv:2202.05333.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biza, Platt, van de Meent, Wong, and Kipf Biza, O., Platt, R., van de Meent,
    J., Wong, L., and Kipf, T. (2022b). Binding actions to objects in world models.
    In arXiv preprint arXiv:2204.13022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borsa, Barreto, Quan, Mankowitz, van Hasselt, Munos, Silver, and Schaul Borsa,
    D., Barreto, A., Quan, J., Mankowitz, D., van Hasselt, H., Munos, R., Silver,
    D., and Schaul, T. (2019). Universal successor features approximators. In Proceedings
    of the Seventh International Conference on Learning Representations (ICLR’19).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borsa, Graepel, and Shawe-Taylor Borsa, D., Graepel, T., and Shawe-Taylor, J.
    (2016). Learning shared representations in multi-task reinforcement learning.
    CoRR, abs/1603.02041.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boutilier, Cohen, Hassidim, Mansour, Meshi, Mladenov, and Schuurmans Boutilier,
    C., Cohen, A., Hassidim, A., Mansour, Y., Meshi, O., Mladenov, M., and Schuurmans,
    D. (2018). Planning and learning with stochastic action sets.. In Lang (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boutilier, Dearden, and Goldszmidt Boutilier, C., Dearden, R., and Goldszmidt,
    M. (2000). Stochastic dynamic programming with factored representations. Artificial
    Intelligence, 121(1-2), 49–107.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba Brockman,
    G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,
    W. (2016). OpenAI gym. In arxiv preprint arXiv:1606.01540.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell,
    Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter,
    Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford,
    Sutskever, and Amodei Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
    Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
    B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. (2020). Language models are few-shot learners.. In Larochelle et al. (?), pp. 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brunskill and Li Brunskill, E.,  and Li, L. (2013). Sample complexity of multi-task
    reinforcement learning. In Nicholson, A.,  and Smyth, P. (Eds.), Proceedings of
    the 29th conference on Uncertainty in Artificial Intelligence (UAI’13). AUAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buchholz and Scheftelowitsch Buchholz, P.,  and Scheftelowitsch, D. (2019).
    Computation of weighted sums of rewards for concurrent mdps. Math. Methods Oper.
    Res., 89(1), 1–42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buesing, Weber, Zwols, Heess, Racanière, Guez, and Lespiau Buesing, L., Weber,
    T., Zwols, Y., Heess, N., Racanière, S., Guez, A., and Lespiau, J. (2019). Woulda,
    coulda, shoulda: Counterfactually-guided policy search. In Proceesings of the
    Seventh International Conference on Learning Representations (ICLR’19). OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burgess, Matthey, Watters, Kabra, Higgins, Botvinick, and Lerchner Burgess,
    C., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., and Lerchner,
    A. (2019). Monet: Unsupervised scene decomposition and representation. CoRR, abs/1901.11390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Castro, Kastner, Panangaden, and Rowland Castro, P., Kastner, T., Panangaden,
    P., and Rowland, M. (2021). Mico: Improved representations via sampling-based
    state similarity for markov decision processes.. In Ranzato et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castro, Kastner, Panangaden, and Rowland Castro, P., Kastner, T., Panangaden,
    P., and Rowland, M. (2023). A kernel perspective on behavioural metrics for markov
    decision processes. In Transactions on Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chandak, Theocharous, Kostas, Jordan, and Thomas Chandak, Y., Theocharous, G.,
    Kostas, J., Jordan, S., and Thomas, P. (2019). Learning action representations
    for reinforcement learning.. In Chaudhuri, and Salakhutdinov (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhuri, Jegelka, Song, Szepesvári, Niu, and Sabato Chaudhuri, K., Jegelka,
    S., Song, L., Szepesvári, C., Niu, G., and Sabato, S. (Eds.). (2022). Proceedings
    of the 39th International Conference on Machine Learning (ICML’22), Vol. 162 of
    Proceedings of Machine Learning Research. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhuri and Salakhutdinov Chaudhuri, K.,  and Salakhutdinov, R. (Eds.). (2019).
    Proceedings of the 36th International Conference on Machine Learning (ICML’19),
    Vol. 97\. Proceedings of Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen, Gao, Xu, Yang, Li, Ding, Feng, and Wang Chen, C., Gao, Z., Xu, K., Yang,
    S., Li, Y., Ding, B., Feng, D., and Wang, H. (2022). Nuclear norm maximization
    based curiosity-driven learning. CoRR, abs/2205.10484.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen, Hu, Nikdel, Mori, and Savva Chen, C., Hu, S., Nikdel, P., Mori, G., and Savva,
    M. (2020). Relational graph learning for crowd navigation. In 2020 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS). IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen, Wan, Shi, Ding, Gao, and Feng Chen, C., Wan, T., Shi, P., Ding, B., Gao,
    Z., and Feng, D. (2022). Uncertainty estimation based intrinsic reward for efficient
    reinforcement learning. In 2022 IEEE International Conference on Joint Cloud Computing
    (JCC), pp. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch Chen,
    L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas,
    A., and Mordatch, I. (2021). Decision transformer: Reinforcement learning via
    sequence modeling.. In Ranzato et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheung et al. Cheung, W., Simchi-Levi, D., and Zhu, R. (2020). Reinforcement
    learning for non-stationary markov decision processes: The blessing of (more)
    optimism. In icml20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christodoulou et al. Christodoulou, P., Lange, R., Shafti, A., and Faisal, A.
    (2019). Reinforcement learning with structured hierarchical grammar representations
    of actions. CoRR, abs/1910.02876.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu and Wang Chu, Z.,  and Wang, H. (2023). Meta-reinforcement learning via
    exploratory task clustering. In arXiv preprint arXiv:2302.07958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Co-Reyes et al. Co-Reyes, J., Miao, Y., Peng, D., Real, E., Le, Q., Levine,
    S., Lee, H., and Faust, A. (2021). Evolving reinforcement learning algorithms.
    In Proceedings of the Ninth International Conference on Learning Representations
    (ICLR’20). OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dayan Dayan, P. (1993). Improving generalization for temporal difference learning:
    The successor representation. Neural Comput., 5(4), 613–624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'der Pol et al. der Pol, E. V., Worrall, D., van Hoof, H., Oliehoek, F., and Welling,
    M. (2020). Mdp homomorphic networks: Group symmetries in reinforcement learning..
    In Larochelle et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D’Eramo, Tateo, Bonarini, Restelli, and J. Peters D’Eramo, C., Tateo, D., Bonarini,
    A., Restelli, M., and J. Peters, J. (2020). Sharing knowledge in multi-task deep
    reinforcement learning. In Proceedings of the Eighth International Conference
    on Learning Representations (ICLR’20).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devin, Geng, Abbeel, Darrell, and Levine Devin, C., Geng, D., Abbeel, P., Darrell,
    T., and Levine, S. (2019). Plan arithmetic: Compositional plan vectors for multi-task
    control. CoRR, abs/1910.14033.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Devin, Gupta, Darrell, Abbeel, and Levine Devin, C., Gupta, A., Darrell, T.,
    Abbeel, P., and Levine, S. (2017). Learning modular neural network policies for
    multi-task and multi-robot transfer. In IEEE International Conference on Robotics
    and Automation (ICRA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding, Lin, Li, and Zhao Ding, W., Lin, H., Li, B., and Zhao, D. (2022). Generalizing
    goal-conditioned reinforcement learning with variational causal reasoning. In
    Proceedings of the 35th International Conference on Advances in Neural Information
    Processing Systems (NeurIPS’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diuk, Cohen, and Littman Diuk, C., Cohen, A., and Littman, M. (2008). An object-oriented
    representation for efficient reinforcement learning. In Cohen, W., McCallum, A.,
    and Roweis, S. (Eds.), Proceedings of the 25th International Conference on Machine
    Learning (ICML’08). Omnipress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du, Krishnamurthy, Jiang, Agarwal, Dudík, and Langford Du, S., Krishnamurthy,
    A., Jiang, N., Agarwal, A., Dudík, M., and Langford, J. (2019). Provably efficient
    RL with rich observations via latent state decoding.. In Chaudhuri, and Salakhutdinov
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dy and Krause Dy, J.,  and Krause, A. (Eds.). (2018). Proceedings of the 35th
    International Conference on Machine Learning (ICML’18), Vol. 80\. Proceedings
    of Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dzeroski et al. Dzeroski, S., Raedt, L. D., and Driessens, K. (2001). Relational
    reinforcement learning. Machine Learning Journal, 43(1/2), 7–52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ecoffet et al. Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K., and Clune,
    J. (2021). First return, then explore. Nature, 590(7847), 580–586.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eimer et al. Eimer, T., Lindauer, M., and Raileanu, R. (2023). Hyperparameters
    in reinforcement learning and how to tune them. In Proceedings of the International
    Conference on Machine Learning (ICML’23).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eysenbach et al. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. (2019).
    Diversity is all you need: Learning skills without a reward function. In Proceedings
    of the Seventh International Conference on Learning Representations (ICLR’19).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fern et al. Fern, A., Yoon, S., and Givan, R. (2006). Approximate policy iteration
    with a policy language bias: Solving relational markov decision processes. Journal
    of Artificial Intelligence Research, 25, 75–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Florensa et al. Florensa, C., Duan, Y., and Abbeel, P. (2017). Stochastic neural
    networks for hierarchical reinforcement learning. In Proceedings of Fifth the
    International Conference on Learning Representations (ICLR’17).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fox et al. Fox, R., Pakman, A., and Tishby, N. (2016). Taming the noise in reinforcement
    learning via soft updates. In Ihler, A.,  and Janzing, D. (Eds.), Proceedings
    of the 32nd conference on Uncertainty in Artificial Intelligence (UAI’16). AUAI
    Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. Fu, X., Yang, G., Agrawal, P., and Jaakkola, T. (2021). Learning task
    informed abstractions.. In Meila, and Zhang (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furelos-Blanco, Law, Jonsson, Broda, and Russo Furelos-Blanco, D., Law, M.,
    Jonsson, A., Broda, K., and Russo, A. (2021). Induction and exploitation of subgoal
    automata for reinforcement learning. J. Artif. Intell. Res., 70, 1031–1116.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gallouedec and Dellandrea Gallouedec, Q.,  and Dellandrea, E. (2023). Cell-free
    latent go-explore. In Proceedings of the 40th International Conference on Machine
    Learning (ICML’23).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garcia and Fernandez Garcia, J.,  and Fernandez, F. (2015). A comprehensive
    survey on safe reinforcement learning. Journal of Machine Learning Research, 16,
    1437–1480.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garg, Bajpai, and Mausam Garg, S., Bajpai, A., and Mausam (2020). Symbolic
    network: Generalized neural policies for relational mdps.. In III, and Singh (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garnelo, Arulkumaran, and Shanahan Garnelo, M., Arulkumaran, K., and Shanahan,
    M. (2016). Towards deep symbolic reinforcement learning. In arXiv preprint arXiv:1609.05518.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gasse, Grasset, Gaudron, and Oudeyer Gasse, M., Grasset, D., Gaudron, G., and Oudeyer,
    P. (2021). Causal reinforcement learning using observational and interventional
    data. In arXiv preprint arXiv:2106.14421.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaya, Doan, Caccia, Soulier, Denoyer, and Raileanu Gaya, J., Doan, T., Caccia,
    L., Soulier, L., Denoyer, L., and Raileanu, R. (2022a). Building a subspace of
    policies for scalable continual learning. In arXiv preprint arXiv:2211.10445.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaya, Soulier, and Denoyer Gaya, J., Soulier, L., and Denoyer, L. (2022b). Learning
    a subspace of policies for online adaptation in reinforcement learning. In Proceedings
    of the Tenth International Conference on Learning Representations (ICLR’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gehring, Synnaeve, Krause, and Usunier Gehring, J., Synnaeve, G., Krause, A.,
    and Usunier, N. (2021). Hierarchical skills for efficient exploration.. In Ranzato
    et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geißer et al. Geißer, F., Speck, D., and Keller, T. (2020). Trial-based heuristic
    tree search for mdps with factored action spaces. In Proceedings of the International
    Symposium on Combinatorial Search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gelada et al. Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare,
    M. (2019). Deepmdp: Learning continuous latent space models for representation
    learning.. In Chaudhuri, and Salakhutdinov (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghorbani et al. Ghorbani, M., Hosseini, R., Shariatpanahi, S., and Ahmadabadi,
    M. (2020). Reinforcement learning with subspaces using free energy paradigm. In
    arXiv preprint arXiv:2012.07091.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gillen and Byl Gillen, S.,  and Byl, K. (2021). Explicitly encouraging low fractional
    dimensional trajectories via reinforcement learning. In Conference on Robot Learning,
    pp. 2137–2147\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep
    Learning. MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal et al. Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio,
    Y., and Schölkopf, B. (2021). Recurrent independent mechanisms. In Proceedings
    of the Ninth International Conference on Learning Representations (ICLR’21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal et al. Goyal, A., Sodhani, S., Binas, J., Peng, X., Levine, S., and Bengio,
    Y. (2020). Reinforcement learning with competitive ensembles of information-constrained
    primitives. In Proceedings of the Eighth International Conference on Learning
    Representations (ICLR’20).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gregor et al. Gregor, K., Rezende, D., and Wierstra, D. (2016). Variational
    intrinsic control. CoRR, abs/1611.07507.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guestrin et al. Guestrin, C., Koller, D., Gearhart, C., and Kanodia, N. (2003a).
    Generalizing plans to new environments in relational mdps. In Gottlob, G.,  and Walsh,
    T. (Eds.), Proceedings of the 18th International Joint Conference on Artificial
    Intelligence (IJCAI’03).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guestrin et al. Guestrin, C., Koller, D., Parr, R., and Venkataraman, S. (2003b).
    Efficient solution algorithms for factored mdps. Journal of Artificial Intelligence
    Research, 19, 399–468.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. Guo, J., Gong, M., and Tao, D. (2022). A relational intervention
    approach for unsupervised dynamics generalization in model-based reinforcement
    learning. In Proceedings of the Ninth International Conference on Learning Representations
    (ICLR’21). OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. Guo, Z., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. Á.,
    Valko, M., Mesnard, T., Lattimore, T., and Munos, R. (2021). Geometric entropic
    exploration. CoRR, abs/2101.02055.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. Gupta, A., Devin, C., Liu, Y., Abbeel, P., and Levine, S. (2017).
    Learning invariant feature spaces to transfer skills with reinforcement learning.
    In Proceedings of the Fifth International Conference on Learning Representations
    (ICLR’17).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. (2018).
    Meta-reinforcement learning of structured exploration strategies.. In Bengio et al.
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gur et al. Gur, I., Jaques, N., Miao, Y., Choi, J., Tiwari, M., Lee, H., and Faust,
    A. (2021). Environment generation for zero-shot compositional reinforcement learning..
    In Ranzato et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guyon et al. Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus, R.,
    Vishwanathan, S., and Garnett, R. (Eds.). (2017). Proceedings of the 30th International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’17).
    Curran Associates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haarnoja et al. Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a).
    Latent space policies for hierarchical reinforcement learning.. In Dy, and Krause
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haarnoja, Pong, Zhou, Dalal, Abbeel, and Levine Haarnoja, T., Pong, V., Zhou,
    A., Dalal, M., Abbeel, P., and Levine, S. (2018b). Composable deep reinforcement
    learning for robotic manipulation. In 2018 IEEE International Conference on Robotics
    and Automation (ICRA’18).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hafner, Lillicrap, Ba, and Norouzi Hafner, D., Lillicrap, T., Ba, J., and Norouzi,
    M. (2020). Dream to control: Learning behaviors by latent imagination.. In III, and Singh
    (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hafner, Pasukonis, Ba, and Lillicrap Hafner, D., Pasukonis, J., Ba, J., and Lillicrap,
    T. (2023). Mastering diverse domains through world models. In arXiv preprint arXiv:2301.04104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallak, Castro, and Mannor Hallak, A., Castro, D. D., and Mannor, S. (2015).
    Contextual markov decision processes. CoRR, abs/1502.02259.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hansen-Estruch, Zhang, Nair, Yin, and Levine Hansen-Estruch, P., Zhang, A.,
    Nair, A., Yin, P., and Levine, S. (2022). Bisimulation makes analogies in goal-conditioned
    reinforcement learning.. In Chaudhuri et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harutyunyan, Dabney, Borsa, Heess, Munos, and Precup Harutyunyan, A., Dabney,
    W., Borsa, D., Heess, N., Munos, R., and Precup, D. (2019). The termination critic.
    In Chaudhuri, K.,  and Sugiyama, M. (Eds.), The 22nd International Conference
    on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha,
    Okinawa, Japan, Vol. 89 of Proceedings of Machine Learning Research, pp. 2231–2240\.
    PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hausman, Springenberg, Wang, Heess, and Riedmiller Hausman, K., Springenberg,
    J., Wang, Z., Heess, N., and Riedmiller, M. (2018). Learning an embedding space
    for transferable robot skills. In Proceedings of the Sixth International Conference
    on Learning Representations (ICLR’18).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hazan, Kakade, Singh, and van Soest Hazan, E., Kakade, S., Singh, K., and van
    Soest, A. (2019). Provably efficient maximum entropy exploration.. In Chaudhuri, and Salakhutdinov
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heess et al. Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller, M.,
    and Silver, D. (2016). Learning and transfer of modulated locomotor controllers.
    In arXiv preprint arXiv:1610.05182.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henaff et al. Henaff, M., Raileanu, R., Jiang, M., and Rocktäschel, T. (2022).
    Exploration via elliptical episodic bonuses. In neurips22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Higgins et al. Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel,
    A., Botvinick, M., Blundell, C., and Lerchner, A. (2017). Darla: Improving zero-shot
    transfer in reinforcement learning.. In Precup, and Teh (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hofer Hofer, S. (2017). On Decomposability in Robot Reinforcement Learning.
    Technische University of Berlin (Germany).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. Hong, Z., Yang, G., and Agrawal, P. (2022). Bilinear value networks.
    CoRR, abs/2204.13695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu and Montana Hu, Y.,  and Montana, G. (2019). Skill transfer in deep reinforcement
    learning under morphological heterogeneity. In arXiv preprint arXiv:1908.05265.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. Huang, W., Mordatch, I., and Pathak, D. (2020). One policy to
    control them all: Shared modular policies for agent-agnostic control.. In III, and Singh
    (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hutter et al. Hutter, F., Kotthoff, L., and Vanschoren, J. (Eds.). (2019).
    Automated Machine Learning: Methods, Systems, Challenges. Springer. Available
    for free at http://automl.org/book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Icarte et al. Icarte, R., Klassen, T., Valenzano, R., and McIlraith, S. (2022).
    Reward machines: Exploiting reward function structure in reinforcement learning.
    J. Artif. Intell. Res., 73, 173–208.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: III and Singh III, H. D.,  and Singh, A. (Eds.). (2020). Proceedings of the
    37th International Conference on Machine Learning (ICML’20), Vol. 98\. Proceedings
    of Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illanes et al. Illanes, L., Yan, X., Icarte, R., and McIlraith, S. (2020). Symbolic
    plans as high-level instructions for reinforcement learning. In Proceedings of
    the International Conference on Automated Planning and Scheduling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Innes and Lascarides Innes, C.,  and Lascarides, A. (2020). Learning factored
    markov decision processes with unawareness. In Peters, J.,  and Sontag, D. (Eds.),
    Proceedings of The 36th Uncertainty in Artificial Intelligence Conference (UAI’20).
    PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. Islam, R., Zang, H., Goyal, A., Lamb, A., Kawaguchi, K., Li, X.,
    Laroche, R., Bengio, Y., and Combes, R. (2022). Discrete factorial representations
    as an abstraction for goal conditioned reinforcement learning. In arXiv preprint
    arXiv:2211.00247.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. Jain, A., Khetarpal, K., and Precup, D. (2021a). Safe option-critic:
    Learning safety in the option-critic architecture. The Knowledge Engineering Review,
    36, e4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. Jain, A., Kosaka, N., Kim, K., and Lim, J. (2021b). Know your action
    set: Learning action relations for reinforcement learning.. In Meila, and Zhang
    (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. Jain, A., Szot, A., and Lim, J. (2020). Generalization to new actions
    in reinforcement learning.. In III, and Singh (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Janisch et al. Janisch, J., Pevnỳ, T., and Lisỳ, V. (2020). Symbolic relational
    deep reinforcement learning based on graph neural networks. In arXiv preprint
    arXiv:2009.12462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Javed Javed, K. (2023). The big world hypothesis and its ramifications on reinforcement
    learning..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang Jiang, N. (2018). Notes on state abstractions..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. Jiang, Y., Shane, S. G., Murphy, K., and Finn, C. (2019). Language
    as an abstraction for hierarchical deep reinforcement learning.. In Wallach et al.
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang, Gao, and Chen Jiang, Z., Gao, J., and Chen, J. (2022). Unsupervised skill
    discovery via recurrent skill training. In neurips22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jonschkowski, Höfer, and Brock Jonschkowski, R., Höfer, S., and Brock, O. (2015).
    Patterns for learning with side information. In arXiv preprint arXiv:1511.06429.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi and Khardon Joshi, S.,  and Khardon, R. (2011). Probabilistic relational
    planning with first order decision diagrams. Journal of Artificial Intelligence
    Research, 41, 231–266.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaiser, Otte, Runkler, and Ek Kaiser, M., Otte, C., Runkler, T., and Ek, C.
    (2019). Interpretable dynamics models for data-efficient reinforcement learning.
    In Proceedings of the 27th European Symposium on Artificial Neural Networks (ESANN’19).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kakade Kakade, S. (2003). On the Sample Complexity of Reinforcement Learning.
    University of London, University College London (United Kingdom).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplanis, Shanahan, and Clopath Kaplanis, C., Shanahan, M., and Clopath, C.
    (2019). Policy consolidation for continual reinforcement learning.. In Chaudhuri, and Salakhutdinov
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karia and Srivastava Karia, R.,  and Srivastava, S. (2022). Relational abstractions
    for generalized reinforcement learning on symbolic problems. In arXiv preprint
    arXiv:2204.12665.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kearns and Koller Kearns, M.,  and Koller, D. (1999). Efficient reinforcement
    learning in factored mdps. In Dean, T. (Ed.), Proceedings of the 16th International
    Joint Conference on Artificial Intelligence (IJCAI’99). Morgan Kaufmann Publishers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khamassi et al. Khamassi, M., Velentzas, G., Tsitsimis, T., and Tzafestas, C.
    (2017). Active exploration and parameterized reinforcement learning applied to
    a simulated human-robot interaction task. In First IEEE International Conference
    on Robotic Computing (IRC’17), pp. 28–35\. IEEE Computer Society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khetarpal et al. Khetarpal, K., Ahmed, Z., Comanici, G., Abel, D., and Precup,
    D. (2020). What can i do here? a theory of affordances in reinforcement learning..
    In III, and Singh (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khetarpal et al. Khetarpal, K., Ahmed, Z., Comanici, G., and Precup, D. (2021).
    Temporally abstract partial models.. In Ranzato et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khetarpal et al. Khetarpal, K., Klissarov, M., Chevalier-Boisvert, M., Bacon,
    P., and Precup, D. (2020). Options of interest: Temporal abstraction with interest
    functions.. In Rossi et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim and Dean Kim, K.,  and Dean, T. (2002). Solving factored mdps with large
    action space using algebraic decision diagrams. In Trends in Artificial Intelligence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf et al. Kipf, T., van der Pol, E., and Welling, M. (2020). Contrastive learning
    of structured world models. In Proceedings of the Eighth International Conference
    on Learning Representations (ICLR’20).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson,
    L., Xiao, T., Whitehead, S., Berg, A., Lo, W., Dollár, P., and Girshick, R. (2023).
    Segment anything. CoRR, abs/2304.02643.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirk et al. Kirk, R., Zhang, A., Grefenstette, E., and Rocktäschel, T. (2023).
    A survey of zero-shot generalisation in deep reinforcement learning. Journal of
    Artificial Intelligence Research, 76, 201–264.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirsch et al. Kirsch, L., van Steenkiste, S., and Schmidhuber, J. (2020). Improving
    generalization in meta reinforcement learning using learned objectives. In Proceedings
    of the Eighth International Conference on Learning Representations (ICLR’20).
    OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Klissarov and Machado Klissarov, M.,  and Machado, M. (2023). Deep laplacian-based
    options for temporally-extended exploration. In Proceedings of the 40th International
    Conference on Machine Learning (ICML’23).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kokel et al. Kokel, H., Manoharan, A., Natarajan, S., Ravindran, B., and Tadepalli,
    P. (2021). Reprel: Integrating relational planning and reinforcement learning
    for effective abstraction. In Proceedings of the International Conference on Automated
    Planning and Scheduling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koller and Parr Koller, D.,  and Parr, R. (1999). Computing factored value functions
    for policies in structured mdps. In IJCAI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kooi et al. Kooi, J., Hoogendoorn, M., and François-Lavet, V. (2022). Disentangled
    (un)controllable features. CoRR, abs/2211.00086.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. Kulkarni, T., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
    (2016). Hierarchical deep reinforcement learning: Integrating temporal abstraction
    and intrinsic motivation. In Lee, D., Sugiyama, M., von Luxburg, U., Guyon, I.,
    and Garnett, R. (Eds.), Proceedings of the 29th International Conference on Advances
    in Neural Information Processing Systems (NeurIPS’16). Curran Associates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative
    q-learning for offline reinforcement learning.. In Larochelle et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. Kumar, S., Correa, C., Dasgupta, I., Marjieh, R., Hu, M., Hawkins,
    R., Daw, N., Cohen, J., Narasimhan, K., and Griffiths, T. (2022). Using natural
    language and program abstractions to instill human inductive biases in machines.
    In Proceedings of the 35th International Conference on Advances in Neural Information
    Processing Systems (NeurIPS’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. Kumar, S., Dasgupta, I., Cohen, J., Daw, N., and Griffiths, T.
    (2021). Meta-learning of structured task distributions in humans and machines.
    In Proceedings of the Ninth International Conference on Learning Representations
    (ICLR’21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwon et al. Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S. (2021). Rl
    for latent mdps: Regret guarantees and a lower bound.. In Ranzato et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lampinen et al. Lampinen, A., Roy, N., Dasgupta, I., Chan, S., Tam, A., Mcclelland,
    J., Yan, C., Santoro, A., Rabinowitz, N., J. Wang, J., and Hill, F. (2022). Tell
    me why! explanations support learning relational and causal structure.. In Chaudhuri
    et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan and Agarwal Lan, C.,  and Agarwal, R. (2023). Revisiting bisimulation:
    A sampling-based state similarity pseudo-metric. In The First Tiny Papers Track
    at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lan, Bellemare, and Castro Lan, C., Bellemare, M., and Castro, P. (2021). Metrics
    and continuity in reinforcement learning.. In Yang et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lan, Mahmood, Yan, and Xu Lan, Q., Mahmood, A., Yan, S., and Xu, Z. (2023).
    Learning to optimize for reinforcement learning. CoRR, abs/2302.01470.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lang Lang, J. (Ed.). (2018). Proceedings of the 27th International Joint Conference
    on Artificial Intelligence (IJCAI’18).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laroche and Feraud Laroche, R.,  and Feraud, R. (2022). Reinforcement learning
    algorithm selection. In Proceedings of the Sixth International Conference on Learning
    Representations (ICLR’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larochelle, Ranzato, Hadsell, Balcan, and Lin Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M.-F., and Lin, H. (Eds.). (2020). Proceedings of the 33rd International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’20).
    Curran Associates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laskin, Yarats, Liu, Lee, Zhan, Lu, Cang, Pinto, and Abbeel Laskin, M., Yarats,
    D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. (2021).
    URLB: unsupervised reinforcement learning benchmark. In Vanschoren, J.,  and Yeung,
    S. (Eds.), Proceedings of the Neural Information Processing Systems Track on Datasets
    and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee, Nagabandi, Abbeel, and Levine Lee, A., Nagabandi, A., Abbeel, P., and Levine,
    S. (2020a). Stochastic latent actor-critic: Deep reinforcement learning with a
    latent variable model.. In Larochelle et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., and Hutter, M.
    (2020b). Learning quadrupedal locomotion over challenging terrain. Science in
    Robotics, 5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. Lee, J., Sedwards, S., and Czarnecki, K. (2022). Recursive constraints
    to prevent instability in constrained reinforcement learning. In arXiv preprint
    arXiv:2201.07958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee and Chung Lee, S.,  and Chung, S. (2021). Improving generalization in meta-rl
    with imaginary tasks from latent dynamics mixture.. In Ranzato et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, A., Spyra, O., Perel, S., Dalibard, V., Jaderberg, M., Gu, C.,
    Budden, D., Harley, T., and Gupta, P. (2019). A generalized framework for population
    based training. In Teredesai, A., Kumar, V., Li, Y., Rosales, R., Terzi, E., and Karypis,
    G. (Eds.), Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
    Discovery & Data Mining (KDD’19), p. 1791–1799\. ACM Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, L., Walsh, T., and Littman, M. (2006). Towards a unified theory
    of state abstraction for mdps.. In AI&M.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. Li, T., Pan, J., Zhu, D., and Meng, M. (2018). Learning to interrupt:
    A hierarchical deep reinforcement learning framework for efficient exploration.
    In 2018 IEEE International Conference on Robotics and Biomimetics (ROBIO), pp. 648–653\.
    IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, Y., Wu, Y., Xu, H., Wang, X., and Wu, Y. (2021). Solving compositional
    reinforcement learning problems via task reduction. In Proceedings of the Ninth
    International Conference on Learning Representations (ICLR’21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao et al. Liao, L., Fu, Z., Yang, Z., Wang, Y., Kolar, M., and Wang, Z. (2021).
    Instrumental variable value iteration for causal offline reinforcement learning.
    CoRR, abs/2102.09907.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lipton Lipton, Z. (2018). The mythos of model interpretability. Commun. ACM,
    61(10), 36–43.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. Lu, C., Kuba, J., Letcher, A., Metz, L., de Witt, C., and Foerster,
    J. (2022). Discovered policy optimisation. In neurips22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. Lu, K., Zhang, S., Stone, P., and Chen, X. (2018). Robot representation
    and reasoning with knowledge from reinforcement learning. CoRR, abs/1809.11074.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. Lu, M., Shahn, Z., Sow, D., Doshi-Velez, F., and Lehman, L. H. (2020).
    Is deep reinforcement learning ready for practical applications in healthcare?
    A sensitivity analysis of duel-ddqn for hemodynamic management in sepsis patients.
    In AMIA 2020, American Medical Informatics Association Annual Symposium, Virtual
    Event, USA, November 14-18, 2020. AMIA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luis et al. Luis, J., Miao, Y., Co-Reyes, J., Parisi, A., Tan, J., Real, E.,
    and Faust, A. (2022). Multi-objective evolution for generalizable policy gradient
    algorithms. CoRR, abs/2204.04292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu et al. Lyu, D., Yang, F., Liu, B., and Gustafson, S. (2019). Sdrl: Interpretable
    and data-efficient deep reinforcement learning leveraging symbolic planning. In
    Hentenryck, P. V.,  and Zhou, Z. (Eds.), Proceedings of the Thirty-Third Conference
    on Artificial Intelligence (AAAI’19). AAAI Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lyu et al. lyu, Y., Côme, A., Zhang, Y., and Talebi, M. (2023). Scaling up q-learning
    via exploiting state-action equivalence. Entropy, 25(4), 584.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahadevan and Maggioni Mahadevan, S.,  and Maggioni, M. (2007). Proto-value
    functions: A laplacian framework for learning representation and control in markov
    decision processes. J. Mach. Learn. Res., 8, 2169–2231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahajan et al. Mahajan, A., Samvelyan, M., Mao, L., Makoviychuk, V., Garg, A.,
    Kossaifi, J., Whiteson, S., Zhu, Y., and Anandkumar, A. (2021). Reinforcement
    learning in factored action spaces using tensor decompositions. In arXiv preprint
    arXiv:2110.14538.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahajan and Tulabandhula Mahajan, A.,  and Tulabandhula, T. (2017). Symmetry
    learning for function approximation in reinforcement learning. In arXiv preprint
    arXiv:1706.02999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mambelli et al. Mambelli, D., Träuble, F., Bauer, S., Schölkopf, B., and Locatello,
    F. (2022). Compositional multi-object reinforcement learning with linear relation
    networks. In arXiv preprint arXiv:2201.13388.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mankowitz et al. Mankowitz, D., Mann, T., and Mannor, S. (2015). Bootstrapping
    skills. In arXiv preprint arXiv:1506.03624.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mannor and Tamar Mannor, S.,  and Tamar, A. (2023). Towards deployable rl–what’s
    broken with rl research and a potential fix. In arXiv preprint arXiv:2301.01320.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martinez et al. Martinez, D., Alenya, G., and Torras, C. (2017). Relational
    reinforcement learning with guided demonstrations. Artificial Intelligence, 247,
    295–312.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marzi et al. Marzi, T., Khehra, A., Cini, A., and Alippi, C. (2023). Feudal
    graph reinforcement learning. CoRR, abs/2304.05099.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mausam and Weld Mausam, D.,  and Weld, D. (2003). Solving relational mdps with
    first-order machine learning. In Proceedings of the ICAPS workshop on planning
    under uncertainty and incomplete information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meila and Zhang Meila, M.,  and Zhang, T. (Eds.). (2021). Proceedings of the
    38th International Conference on Machine Learning (ICML’21), Vol. 139 of Proceedings
    of Machine Learning Research. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mendez et al. Mendez, J., Hussing, M., Gummadi, M., and Eaton, E. (2022a).
    Composuite: A compositional reinforcement learning benchmark. In Chandar, S.,
    Pascanu, R., and Precup, D. (Eds.), Proceedings of the First Conference on Lifelong
    Learning Agents (CoLLAs’22), Vol. 199, pp. 982–1003\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mendez et al. Mendez, J., van Seijen, H., and Eaton, E. (2022b). Modular lifelong
    reinforcement learning via neural composition. In Proceedings of the Tenth International
    Conference on Learning Representations (ICLR’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mendez et al. Mendez, J., Wang, B., and Eaton, E. (2020). Lifelong policy gradient
    learning of factored policies for faster training without forgetting.. In Larochelle
    et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng and Khushi Meng, T.,  and Khushi, M. (2019). Reinforcement learning in
    financial markets. Data, 4(3), 110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metz et al. Metz, L., Ibarz, J., Jaitly, N., and Davidson, J. (2017). Discrete
    sequential prediction of continuous actions for deep rl. In arXiv preprint arXiv:1705.05035.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mihajlovic and Petkovic Mihajlovic, V.,  and Petkovic, M. (2001). Dynamic bayesian
    networks: A state of the art. In University of Twente Document Repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirsky et al. Mirsky, R., Shperberg, S., Zhang, Y., Xu, Z., Jiang, Y., Cui,
    J., and Stone, P. (2022). Task factorization in curriculum learning. In Decision
    Awareness in Reinforcement Learning Workshop at ICML 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misra et al. Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J. (2020).
    Kinematic state abstraction and provably efficient rich-observation reinforcement
    learning.. In III, and Singh (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modi et al. Modi, A., Jiang, N., Singh, S., and Tewari, A. (2018). Markov decision
    processes with continuous side information. In Janoos, F., Mohri, M., and Sridharan,
    K. (Eds.), Algorithmic Learning Theory, ALT 2018, 7-9 April 2018, Lanzarote, Canary
    Islands, Spain, Vol. 83 of Proceedings of Machine Learning Research, pp. 597–618\.
    PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moerland et al. Moerland, T., Broekens, J., Plaat, A., and Jonker, C. (2023).
    Model-based reinforcement learning: A survey. Found. Trends Mach. Learn., 16(1),
    1–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohamed and Rezende Mohamed, S.,  and Rezende, D. (2015). Variational information
    maximisation for intrinsically motivated reinforcement learning. In Cortes, C.,
    Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (Eds.), Proceedings of the
    28th International Conference on Advances in Neural Information Processing Systems
    (NeurIPS’15). Curran Associates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohan et al. Mohan, A., Benjamins, C., Wienecke, K., Dockhorn, A., and Lindauer,
    M. (2023). Autorl hyperparameter landscapes. In Faust, A., White, C., Hutter,
    F., Garnett, R., and Gardner, J. (Eds.), Proceedings of the Second International
    Conference on Automated Machine Learning. Proceedings of Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mu et al. Mu, J., Zhong, V., Raileanu, R., Jiang, M., Goodman, N., Rocktäschel,
    T., and Grefenstette, E. (2022a). Improving intrinsic exploration with language
    abstractions. In Proceedings of the 35th International Conference on Advances
    in Neural Information Processing Systems (NeurIPS’22).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mu et al. Mu, T., Lin, K., Niu, F., and Thattai, G. (2022b). Learning two-step
    hybrid policy for graph-based interpretable reinforcement learning. Trans. Mach.
    Learn. Res., 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutti et al. Mutti, M., Mancassola, M., and Restelli, M. (2022). Unsupervised
    reinforcement learning in multiple environments.. In Sycara et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutti, Pratissoli, and Restelli Mutti, M., Pratissoli, L., and Restelli, M.
    (2021). Task-agnostic exploration via policy gradient of a non-parametric state
    entropy estimate.. In Yang et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nachum et al. Nachum, O., Shane, S. G., Lee, H., and Levine, S. (2018). Data-efficient
    hierarchical reinforcement learning.. In Bengio et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nam, Sun, Pertsch, Hwang, and Lim Nam, T., Sun, S., Pertsch, K., Hwang, S. J.,
    and Lim, J. (2022). Skill-based meta-reinforcement learning. In Proceedings of
    the Tenth International Conference on Learning Representations (ICLR’22). OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narvekar, Sinapov, Leonetti, and Stone Narvekar, S., Sinapov, J., Leonetti,
    M., and Stone, P. (2016). Source task creation for curriculum learning. In Jonker,
    C., Marsella, S., Thangarajah, J., and Tuyls, K. (Eds.), Proceedings of the International
    Conference on Autonomous Agents & Multiagent Systems (AAMAS’16), pp. 566–574\.
    ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng, Harada, and Russell Ng, A., Harada, D., and Russell, S. (1999). Policy
    invariance under reward transformations: Theory and application to reward shaping.
    In Bratko, I. (Ed.), Proceedings of the Sixteenth International Conference on
    Machine Learning (ICML’99). Morgan Kaufmann Publishers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh, Hessel, Czarnecki, Xu, van Hasselt, Singh, and Silver Oh, J., Hessel, M.,
    Czarnecki, W., Xu, Z., van Hasselt, H., Singh, S., and Silver, D. (2020). Discovering
    reinforcement learning algorithms.. In Larochelle et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ok et al. Ok, J., Proutière, A., and Tranos, D. (2018). Exploration in structured
    reinforcement learning.. In Bengio et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oliva et al. Oliva, M., Banik, S., Josifovski, J., and Knoll, A. (2022). Graph
    neural networks for relational inductive bias in vision-based deep reinforcement
    learning of robot control. In International Joint Conference on Neural Networks,
    IJCNN 2022, Padua, Italy, July 18-23, 2022, pp. 1–9\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI OpenAI (2023). GPT-4 technical report. CoRR, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papini et al. Papini, M., Tirinzoni, A., Pacchiano, A., Restelli, M., Lazaric,
    A., and Pirotta, M. (2021). Reinforcement learning in linear mdps: Constant regret
    and representation selection.. In Ranzato et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parker-Holder et al. Parker-Holder, J., Nguyen, V., and Roberts, S. J. (2020).
    Provably efficient online Hyperparameter Optimization with population-based bandits..
    In Larochelle et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parker-Holder et al. Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A.,
    Miao, Y., Eimer, T., Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F.,
    and Lindauer, M. (2022). Automated reinforcement learning (AutoRL): A survey and
    open problems. Journal of Artificial Intelligence Research (JAIR), 74, 517–568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parr and Russell Parr, R.,  and Russell, S. (1997). Reinforcement learning with
    hierarchies of machines. In Proceedings of the Tenth International Conference
    on Advances in Neural Information Processing Systems (NeurIPS’97).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pateria et al. Pateria, S., Subagdja, B., Tan, A., and Quek, C. (2022). Hierarchical
    reinforcement learning: A comprehensive survey. ACM Computing Surveys, 54(5),
    109:1–109:35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathak et al. Pathak, D., Agrawal, P., Efros, A., and Darrell, T. (2017). Curiosity-driven
    exploration by self-supervised prediction.. In Precup, and Teh (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pathak, Lu, Darrell, Isola, and Efros Pathak, D., Lu, C., Darrell, T., Isola,
    P., and Efros, A. (2019). Learning to control self-assembling morphologies: a
    study of generalization via modularity.. In Wallach et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Payani and Fekri Payani, A.,  and Fekri, F. (2020). Incorporating relational
    background knowledge into reinforcement learning via differentiable inductive
    logic programming. CoRR, abs/2003.10386.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. Peng, X., Chang, M., Zhang, G., Abbeel, P., and Levine, S. (2019).
    MCP: learning composable hierarchical control with multiplicative compositional
    policies.. In Wallach et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perez et al. Perez, C., Such, F., and Karaletsos, T. (2020). Generalized hidden
    parameter mdps transferable model-based rl in a handful of trials.. In Rossi et al.
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peters, Buhlmann, and Meinshausen Peters, J., Buhlmann, P., and Meinshausen,
    N. (2016). Causal inference by using invariant prediction: identification and
    confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical
    Methodology), 78(5), 947–1012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitis, Creager, and Garg Pitis, S., Creager, E., and Garg, A. (2020). Counterfactual
    data augmentation using locally factored dynamics.. In Larochelle et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prakash et al. Prakash, B., Waytowich, N., Ganesan, A., Oates, T., and Mohsenin,
    T. (2020). Guiding safe reinforcement learning policies using structured language
    constraints. In Espinoza, H., Hernández-Orallo, J., Chen, X. C., ÓhÉigeartaigh,
    S. S., Huang, X., Castillo-Effen, M., Mallah, R., and McDermid, J. A. (Eds.),
    Proceedings of the Workshop on Artificial Intelligence Safety, co-located with
    34th AAAI Conference on Artificial Intelligence, SafeAI@AAAI 2020, New York City,
    NY, USA, February 7, 2020, Vol. 2560 of CEUR Workshop Proceedings, pp. 153–161\.
    CEUR-WS.org.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prakash et al. Prakash, B., Waytowich, N., Oates, T., and Mohsenin, T. (2022).
    Towards an interpretable hierarchical agent framework using semantic goals. CoRR,
    abs/2210.08412.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precup and Teh Precup, D.,  and Teh, Y. (Eds.). (2017). Proceedings of the 34th
    International Conference on Machine Learning (ICML’17), Vol. 70\. Proceedings
    of Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prudencio et al. Prudencio, R., Máximo, M., and Colombini, E. (2022). A survey
    on offline reinforcement learning: Taxonomy, review, and open problems. CoRR,
    abs/2203.01387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puterman Puterman, M. (2014). Markov decision processes: discrete stochastic
    dynamic programming. John Wiley & Sons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranzato et al. Ranzato, M., Beygelzimer, A., Nguyen, K., Liang, P., Vaughan,
    J., and Dauphin, Y. (Eds.). (2021). Proceedings of the 34th International Conference
    on Advances in Neural Information Processing Systems (NeurIPS’21). Curran Associates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raza and Lin Raza, S.,  and Lin, M. (2019). Policy reuse in reinforcement learning
    for modular agents. In IEEE 2nd International Conference on Information and Computer
    Technologies (ICICT). IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross and Pineau Ross, S.,  and Pineau, J. (2008). Model-based bayesian reinforcement
    learning in large structured domains. In McAllester, D. A.,  and Myllymäki, P.
    (Eds.), UAI 2008, Proceedings of the 24th Conference in Uncertainty in Artificial
    Intelligence, Helsinki, Finland, July 9-12, 2008, pp. 476–483\. AUAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rossi et al. Rossi, F., Conitzer, V., and Sha, F. (Eds.). (2020). Proceedings
    of the Thirty-Fourth Conference on Artificial Intelligence (AAAI’20). Association
    for the Advancement of Artificial Intelligence, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. Rusu, A., Colmenarejo, S., Gülçehre, C., Desjardins, G., Kirkpatrick,
    J., Pascanu, R., Mnih, V., Kavukcuoglu, K., and Hadsell, R. (2016). Policy distillation.
    In Proceedings of Fourth International Conference on Learning Representations
    (ICLR’16).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salimans et al. Salimans, T., Ho, J., Chen, X., and Sutskever, I. (2017). Evolution
    strategies as a scalable alternative to reinforcement learning. CoRR, abs/1703.03864.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanner and Boutilier Sanner, S.,  and Boutilier, C. (2012). Approximate linear
    programming for first-order mdps. In arXiv preprint arXiv:1207.1415.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxe et al. Saxe, A., Earle, A., and Rosman, B. (2017). Hierarchy through composition
    with multitask lmdps.. In Precup, and Teh (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaul et al. Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal
    value function approximators. In Bach, F.,  and Blei, D. (Eds.), International
    conference on machine learning, Vol. 37\. Omnipress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schiewer and Wiskott Schiewer, R.,  and Wiskott, L. (2021). Modular networks
    prevent catastrophic interference in model-based multi-task reinforcement learning.
    In Proceedings of the Seventh International Conference on Machine Learning, Optimization,
    and Data Science (LOD’21), Vol. 13164 of Lecture Notes in Computer Science, pp. 299–313\.
    Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seitzer et al. Seitzer, M., Schölkopf, B., and Martius, G. (2021). Causal influence
    detection for improving efficiency in reinforcement learning.. In Ranzato et al.
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shanahan et al. Shanahan, M., Nikiforou, K., Creswell, A., Kaplanis, C., Barrett,
    D., and Garnelo, M. (2020). An explicitly relational neural network architecture.
    In icml20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. (2020).
    Dynamics-aware unsupervised discovery of skills. In Proceedings of the Eighth
    International Conference on Learning Representations (ICLR’20). OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. Sharma, V., Arora, D., Geisser, F., Mausam, A., and Singla, P.
    (2022). Symnet 2.0: Effectively handling non-fluents and actions in generalized
    neural policies for rddl relational mdps. In Uncertainty in Artificial Intelligence,
    pp. 1771–1781. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. Shu, T., Xiong, C., and Socher, R. (2018). Hierarchical and interpretable
    skill acquisition in multi-task reinforcement learning. In Proceedings of the
    Sixth International Conference on Learning Representations (ICLR’18).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shyam et al. Shyam, P., Jaskowski, W., and Gomez, F. (2019). Model-based active
    exploration.. In Chaudhuri, and Salakhutdinov (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. Silver, D., Huang, A., Maddison, C., Guez, A., Sifre, L., Driessche,
    G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,
    S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach,
    M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. (2016). Mastering the game
    of go with deep neural networks and tree search. Nature, 529(7587), 484–489.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simao et al. Simao, T., Jansen, N., and Spaan, M. (2021). Alwayssafe: Reinforcement
    learning without safety constraint violations during training. In Proceedings
    of the 20th International Conference on Autonomous Agents and MultiAgent Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. Singh, G., Peri, S. V., Kim, J., Kim, H., and Ahn, S. (2021). Structured
    world belief for reinforcement learning in POMDP.. In Meila, and Zhang (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sodhani et al. Sodhani, S., Levine, S., and Zhang, A. (2022a). Improving generalization
    with approximate factored value functions. In ICLR2022 Workshop on the Elements
    of Reasoning: Objects, Structure and Causality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sodhani et al. Sodhani, S., Meier, F., Pineau, J., and Zhang, A. (2022b). Block
    contextual mdps for continual learning. In Learning for Dynamics and Control Conference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sodhani et al. Sodhani, S., Zhang, A., and Pineau, J. (2021). Multi-task reinforcement
    learning with context-based representations.. In Meila, and Zhang (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. Sohn, S., Oh, J., and Lee, H. (2018). Hierarchical reinforcement
    learning for zero-shot generalization with subtask dependencies.. In Bengio et al.
    (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. Sohn, S., Woo, H., Choi, J., and Lee, H. (2020). Meta reinforcement
    learning with autonomous inference of subtask dependencies. In Proceedings of
    the Eighth International Conference on Learning Representations, (ICLR’20). OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solway et al. Solway, A., Diuk, C., Córdova, N., Yee, D., Barto, A., Niv, Y.,
    and Botvinick, M. (2014). Optimal behavioral hierarchy. PLoS Comput. Biol., 10(8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. Song, Y., Suganthan, P., Pedrycz, W., Ou, J., He, Y., and Chen,
    Y. (2023). Ensemble reinforcement learning: A survey. CoRR, abs/2303.02618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spooner et al. Spooner, T., Vadori, N., and Ganesh, S. (2021). Factored policy
    gradients: Leveraging structure for efficient learning in momdps.. In Ranzato
    et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas and Abbeel Srinivas, A.,  and Abbeel, P. (2021). Unsupervised Learning
    for Reinforcement Learning..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srouji et al. Srouji, M., Zhang, J., and Salakhutdinov, R. (2018). Structured
    control nets for deep reinforcement learning.. In Dy, and Krause (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steccanella et al. Steccanella, L., Totaro, S., and Jonsson, A. (2021). Hierarchical
    representation learning for markov decision processes. In arXiv preprint arXiv:2106.01655.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. Sun, Y., Ma, S., Madaan, R., Bonatti, R., Huang, F., and Kapoor,
    A. (2023). SMART: self-supervised multi-task pretraining with control transformers.
    In Proceedings of the Eleventh International Conference on Learning Representations
    (ICLR’23).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. Sun, Y., Yin, X., and Huang, F. (2021). Temple: Learning template
    of transitions for sample efficient multi-task rl.. In Yang et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton Sutton, R. (1988). Learning to predict by the methods of temporal differences.
    Mach. Learn., 3, 9–44.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton et al. Sutton, R., McAllester, D., Singh, S., and Mansour, Y. (1999a).
    Policy gradient methods for reinforcement learning with function approximation.
    In Solla, S., Leen, T., and Müller, K. (Eds.), Proceedings of the 12th International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’99).
    The MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton et al. Sutton, R., Precup, D., and Singh, S. (1999b). Between mdps and
    semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial
    Intelligence, 112(1-2), 181–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto Sutton, R. S.,  and Barto, A. G. (2018). Reinforcement learning:
    An introduction (2 edition). Adaptive computation and machine learning. MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sycara et al. Sycara, K., Honavar, V., and Spaan, M. (Eds.). (2022). Proceedings
    of the Thirty-Sixth Conference on Artificial Intelligence (AAAI’22). Association
    for the Advancement of Artificial Intelligence, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talele and Byl Talele, N.,  and Byl, K. (2019). Mesh-based tools to analyze
    deep reinforcement learning policies for underactuated biped locomotion. In arXiv
    preprint arXiv:1903.12311.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talvitie and Singh Talvitie, E.,  and Singh, S. (2008). Simple local models
    for complex dynamical systems. In Proceedings of the 21st International Conference
    on Advances in Neural Information Processing Systems (NeurIPS’08).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. Tang, S., Makar, M., Sjoding, M., Doshi-Velez, F., and Wiens, J.
    (2022a). Leveraging factored action spaces for efficient offline reinforcement
    learning in healthcare. In Decision Awareness in Reinforcement Learning Workshop
    at ICML 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. Tang, S., Makar, M., Sjoding, M., Doshi-Velez, F., and Wiens, J.
    (2022b). Leveraging factored action spaces for efficient offline reinforcement
    learning in healthcare. In Decision Awareness in Reinforcement Learning Workshop
    at ICML 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tavakol and Brefeld Tavakol, M.,  and Brefeld, U. (2014). Factored mdps for
    detecting topics of user sessions. In Proceedings of the 8th ACM Conference on
    Recommender Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tavakoli et al. Tavakoli, A., Pardo, F., and Kormushev, P. (2018). Action branching
    architectures for deep reinforcement learning. In McIlraith, S.,  and Weinberger,
    K. (Eds.), Proceedings of the Thirty-Second Conference on Artificial Intelligence
    (AAAI’18). AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tennenholtz and Mannor Tennenholtz, G.,  and Mannor, S. (2019). The natural
    language of actions.. In Chaudhuri, and Salakhutdinov (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trimponias and Dietterich Trimponias, G.,  and Dietterich, T. (2023). Reinforcement
    learning with exogenous states and rewards. In arXiv preprint arXiv:2303.12957.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsividis et al. Tsividis, P., Loula, J., Burga, J., Foss, N., Campero, A., Pouncy,
    T., Gershman, S., and Tenenbaum, J. (2021). Human-level reinforcement learning
    through theory-based modeling, exploration, and planning. CoRR, abs/2107.12544.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van der Pol et al. van der Pol, E., Kipf, T., Oliehoek, F., and Welling, M.
    (2020). Plannable approximations to mdp homomorphisms: Equivariance under actions.
    In Proceedings of the 19th International Conference on Autonomous Agents and Multiagent
    Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Rossum et al. van Rossum, C., Feinberg, C., Shumays, A. A., Baxter, K.,
    and Bartha, B. (2021). A novel approach to curiosity and explainable reinforcement
    learning via interpretable sub-goals. CoRR, abs/2104.06630.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need..
    In Guyon et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veerapaneni et al. Veerapaneni, R., Co-Reyes, J., Chang, M., Janner, M., Finn,
    C., Wu, J., Tenenbaum, J., and Levine, S. (2020). Entity abstraction in visual
    model-based reinforcement learning. In Conference on Robot Learning. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verma et al. Verma, A., Murali, V., Singh, R., Kohli, P., and Chaudhuri, S.
    (2018). Programmatically interpretable reinforcement learning. In icml18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wallach et al. Wallach, H., Larochelle, H., Beygelzimer, A., d’Alche Buc, F.,
    Fox, E., and Garnett, R. (Eds.). (2019). Proceedings of the 32nd International
    Conference on Advances in Neural Information Processing Systems (NeurIPS’19).
    Curran Associates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. Wan, X., Lu, C., Parker-Holder, J., Ball, P., Nguyen, V., Ru, B.,
    and Osborne, M. (2022). Bayesian generational population-based training. In Guyon,
    I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (Eds.), Proceedings
    of the First International Conference on Automated Machine Learning. Proceedings
    of Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. Wang, G., Fang, Z., Li, B., and Li, P. (2016). Integrating symmetry
    of environment by designing special basis functions for value function approximation
    in reinforcement learning. In Fourteenth International Conference on Control,
    Automation, Robotics and Vision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. Wang, H., Dong, S., and Shao, L. (2019). Measuring structural similarities
    in finite mdps.. In Kraus, S. (Ed.), Proceedings of the 28th International Joint
    Conference on Artificial Intelligence (IJCAI’19).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. Wang, J., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck,
    C., Choy, P., Cassin, M., Reynolds, M., Song, H., Buttimore, G., Reichert, D.,
    Rabinowitz, N., Matthey, L., Hassabis, D., Lerchner, A., and Botvinick, M. (2021).
    Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents..
    In Ranzato et al. (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. Wang, J., Liu, Y., and Li, B. (2020). Reinforcement learning with
    perturbed rewards.. In Rossi et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and van Hoof Wang, Q.,  and van Hoof, H. (2022). Model-based meta reinforcement
    learning using graph structured surrogate models and amortized policy search..
    In Chaudhuri et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. Wang, T., Du, S., Torralba, A., Isola, P., Zhang, A., and Tian,
    Y. (2022). Denoised mdps: Learning world models better than the world itself.
    In arXiv preprint arXiv:2206.15477.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. Wang, T., Liao, R., Ba, J., and Fidler, S. (2018). Nervenet: Learning
    structured policy with graph neural networks. In Proceedings of the Sixth International
    Conference on Learning Representations (ICLR’18).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. Wang, T., Torralba, A., Isola, P., and Zhang, A. (2023). Optimal
    goal-reaching reinforcement learning via quasimetric learning. CoRR, abs/2304.01203.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. Wen, Z., Precup, D., Ibrahimi, M., Barreto, A., Roy, B. V., and Singh,
    S. (2020). On efficiency in hierarchical reinforcement learning.. In Larochelle
    et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whatley et al. Whatley, A., Luo, Z., and Tang, X. (2021). Improving RNA secondary
    structure design using deep reinforcement learning. CoRR, abs/2111.04504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whitehead and Lin Whitehead, S.,  and Lin, L. (1995). Reinforcement learning
    of non-markov decision processes. Artif. Intell., 73(1-2), 271–306.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams Williams, R. (1992a). Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Mach. Learn., 8, 229–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams Williams, R. (1992b). Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Mach. Learn., 8, 229–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wolf and Musolesi Wolf, L.,  and Musolesi, M. (2023). Augmented modular reinforcement
    learning based on heterogeneous knowledge. CoRR, abs/2306.01158.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Woo et al. Woo, H., Yoo, G., and Yoo, M. (2022). Structure learning-based task
    decomposition for reinforcement learning in non-stationary environments.. In Sycara
    et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. Wu, B., Gupta, J., and Kochenderfer, M. (2019). Model primitive hierarchical
    lifelong reinforcement learning. In Elkind, E., Veloso, M., Agmon, N., and Taylor,
    M. (Eds.), Proceedings of the Eighteenth International Conference on Autonomous
    Agents and MultiAgent Systems (AAMAS’19), pp. 34–42\. International Foundation
    for Autonomous Agents and Multiagent Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. Wu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A., Kakade, S.,
    Mordatch, I., and Abbeel, P. (2018). Variance reduction for policy gradient with
    action-dependent factorized baselines. In Proceedings of the Sixth International
    Conference on Learning Representations (ICLR’18).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu and Fekri Xu, D.,  and Fekri, F. (2021). Interpretable model-based hierarchical
    reinforcement learning using inductive logic programming. CoRR, abs/2106.11417.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. Xu, K., Verma, S., Finn, C., and Levine, S. (2020). Continual learning
    of control primitives: Skill discovery via reset-games.. In Larochelle et al.
    (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. Yang, C., Hung, I., Ouyang, Y., and Chen, P. (2022). Training a
    resilient q-network against observational interference.. In Sycara et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. Yang, F., Lyu, D., Liu, B., and Gustafson, S. (2018). Peorl: Integrating
    symbolic planning and hierarchical reinforcement learning for robust decision-making..
    In Lang (?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang, Leyton-Brown, and Mausam Yang, Q., Leyton-Brown, K., and Mausam (Eds.).
    (2021). Proceedings of the Thirty-Fifth Conference on Artificial Intelligence
    (AAAI’21). Association for the Advancement of Artificial Intelligence, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang, Xu, Wu, and Wang Yang, R., Xu, H., Wu, Y., and Wang, X. (2020a). Multi-task
    reinforcement learning with soft modularization.. In Larochelle et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. Yang, Y., Zhang, G., Xu, Z., and Katabi, D. (2020b). Harnessing
    structures for value-based planning and reinforcement learning. In Proceedings
    of the Eighth International Conference on Learning Representations (ICLR’20).
    OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yarats et al. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2021). Reinforcement
    learning with prototypical representations. In Meila, M.,  and Zhang, T. (Eds.),
    icml21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. Yin, D., Thiagarajan, S., Lazic, N., Rajaraman, N., Hao, B., and Szepesvári,
    C. (2023). Sample efficient deep reinforcement learning via local planning. CoRR,
    abs/2301.12579.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. Young, K., Ramesh, A., Kirsch, L., and Schmidhuber, J. (2022).
    The benefits of model-based generalization in reinforcement learning. In arXiv
    preprint arXiv:2211.02222.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. Yu, D., Ma, H., Li, S., and Chen, J. (2022). Reachability constrained
    reinforcement learning.. In Chaudhuri et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zambaldi et al. Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin,
    I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston,
    V., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. (2019). Deep reinforcement
    learning with relational inductive biases. In Proceedings of the Seventh International
    Conference on Learning Representations, ICLR 2019. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. Zeng, K., Zhang, Q., Chen, B., Liang, B., and Yang, J. (2022).
    APD: learning diverse behaviors for reinforcement learning through unsupervised
    active pre-training. IEEE Robotics Autom. Lett., 7(4), 12251–12258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, A., Lyle, C., Sodhani, S., Filos, A., Kwiatkowska, M., Pineau,
    J., Gal, Y., and Precup, D. (2020). Invariant causal prediction for block mdps..
    In III, and Singh (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S.
    (2021). Learning invariant representations for reinforcement learning without
    reconstruction. In Proceedings of the Ninth International Conference on Learning
    Representations (ICLR’21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, A., Sodhani, S., Khetarpal, K., and Pineau, J. (2020). Multi-task
    reinforcement learning as a hidden-parameter block mdp. In arXiv preprint arXiv:2007.07206.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, A., Sodhani, S., Khetarpal, K., and Pineau, J. (2021a).
    Learning robust state abstractions for hidden-parameter block mdps. In Proceedings
    of the Ninth International Conference on Learning Representations (ICLR’21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, C., Cai, Y., Huang, L., and Li, J. (2021b). Exploration
    by maximizing renyi entropy for reward-free RL framework.. In Yang et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, D., Courville, A., Bengio, Y., Zheng, Q., Zhang, A., and Chen,
    R. (2022). Latent state marginalization as a low-cost approach for improving exploration.
    CoRR, abs/2210.00999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D., and Hsieh,
    C. (2020). Robust deep reinforcement learning against adversarial perturbations
    on state observations.. In Larochelle et al. (?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, H., Gao, Z., Zhou, Y., Zhang, H., Wu, K., and Lin, F. (2019a).
    Faster and safer training by embedding high-level knowledge into deep reinforcement
    learning. CoRR, abs/1910.09986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, H., Gao, Z., Zhou, Y., Zhang, H., Wu, K., and Lin, F. (2019b).
    Faster and safer training by embedding high-level knowledge into deep reinforcement
    learning. CoRR, abs/1910.09986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. Zhang, S., Tong, H., Xu, J., and Maciejewski, R. (2019c). Graph
    convolutional networks: a comprehensive review. Computational Social Networks,
    6(1), 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Yu Zhang, X.,  and Yu, S. Z. Y. (2021). Domain knowledge guided offline
    q learning. In Second Offline Reinforcement Learning Workshop at Neurips 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. Zhao, T., Xie, K., and Eskénazi, M. (2019). Rethinking action spaces
    for reinforcement learning in end-to-end dialog agents with latent variable models.
    In Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. Zhou, A., Kumar, V., Finn, C., and Rajeswaran, A. (2022). Policy
    architectures for compositional generalization in control. In arXiv preprint arXiv:2203.05960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. Zhou, Z., Li, X., and Zare, R. (2017). Optimizing chemical reactions
    with deep reinforcement learning. ACS central science, 3(12), 1337–1344.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. Zhu, J., Park, T., Isola, P., and Efros, A. (2017). Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In Proceedings of the
    IEEE international conference on computer vision, pp. 2223–2232.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
