- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:02:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:02:22'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2002.07995] 1 Introduction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2002.07995] 1 引言'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07995](https://ar5iv.labs.arxiv.org/html/2002.07995)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07995](https://ar5iv.labs.arxiv.org/html/2002.07995)
- en: \headevenname
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \headevenname
- en: Xiao et al.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao et al.
- en: \MakePageStyle\MakeAbstract
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \MakePageStyle\MakeAbstract
- en: Researchers have now achieved great success on dealing with 2D images using
    deep learning. In recent years, 3D computer vision and Geometry Deep Learning
    gain more and more attention. Many advanced techniques for 3D shapes have been
    proposed for different applications. Unlike 2D images, which can be uniformly
    represented by regular grids of pixels, 3D shapes have various representations,
    such as depth and multi-view images, voxel-based representation, point-based representation,
    mesh-based representation, implicit surface representation, etc. However, the
    performance for different applications largely depends on the representation used,
    and there is no unique representation that works well for all applications. Therefore,
    in this survey, we review recent development in deep learning for 3D geometry
    from a representation perspective, summarizing the advantages and disadvantages
    of different representations in different applications. We also present existing
    datasets in these representations and further discuss future research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员现在在使用深度学习处理2D图像方面取得了巨大的成功。近年来，3D计算机视觉和几何深度学习获得了越来越多的关注。许多先进的3D形状技术已被提出用于不同的应用。与可以通过规则的像素网格均匀表示的2D图像不同，3D形状有各种表示方式，如深度图和多视角图像、体素表示、点云表示、网格表示、隐式表面表示等。然而，不同应用的性能在很大程度上取决于所使用的表示，并且没有一种唯一的表示适用于所有应用。因此，在本综述中，我们从表示的角度回顾了3D几何深度学习的最新发展，总结了不同表示在不同应用中的优缺点。我们还介绍了这些表示中的现有数据集，并进一步讨论了未来的研究方向。
- en: \MakeKeywords
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \MakeKeywords
- en: 3D representation, geometry learning, neural networks, computer graphics
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 3D表示、几何学习、神经网络、计算机图形学
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent improvements in methods for acquisition and rendering of 3D models haven
    resulted in consolidated repositories containing massive amounts of 3D shapes
    on the Internet. With the increased availability of 3D models, we have been seeing
    an explosion in the demands of processing, generation and visualization of 3D
    models in a variety of disciplines, such as medicine, architecture and entertainment.
    The techniques for matching, identification and manipulation of 3D shapes have
    become fundamental building blocks in modern computer vision and computer graphics
    systems. Due to the complexity and irregularity of 3D shape data, how to effectively
    represent 3D shapes remains a challenging problem. Thus, there have been extensive
    research efforts concentrating on how to deal with and generate 3D shapes based
    on different representations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在3D模型获取和渲染方法上的改进导致了包含大量3D形状的整合库在互联网的出现。随着3D模型的可用性增加，我们在医学、建筑和娱乐等多个领域看到了对3D模型处理、生成和可视化需求的爆炸性增长。3D形状的匹配、识别和操作技术已经成为现代计算机视觉和计算机图形系统中的基础构件。由于3D形状数据的复杂性和不规则性，如何有效地表示3D形状仍然是一个具有挑战性的问题。因此，已经有大量的研究工作集中在如何基于不同的表示方法处理和生成3D形状上。
- en: In early research on 3D shape representations, 3D objects were normally modeled
    with a global approach, such as constructive solid geometry and deformed superquadrics.
    Those approaches have several drawbacks when utilized for the tasks like recognition
    and retrieval. First, when representing imperfect 3D shapes, including those with
    noise and incompleteness, which are common in practice, such representations may
    impose negative influence on matching performance. Second, the high-dimensionality
    heavily burdens the computation and tends to make models overfit. Hence, more
    sophisticated methods are designed to extract representations of 3D shapes in
    a more concise, yet discriminative and informative form.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的3D形状表示研究中，3D对象通常采用全局方法进行建模，如构造固体几何和变形超二次体。这些方法在用于识别和检索等任务时有若干缺点。首先，在表示不完美的3D形状时，包括那些常见的噪声和不完整性，这些表示可能会对匹配性能产生负面影响。其次，高维性严重负担计算，并倾向于使模型过拟合。因此，设计了更复杂的方法来提取3D形状的表示，使其更简洁，但又具有区分性和信息性。
- en: Several related surveys have been published [[9](#bib.bib9), [1](#bib.bib1),
    [35](#bib.bib35)], which focus on different aspects of deep learning for 3D geometry.
    Moreover, with rapid development of 3D shape representations and related techniques
    for deep learning, it is essential to further summarize up-to-date research works.
    In this survey, we mainly review deep learning methods on 3D shape representations
    and discuss their advantages and disadvantages considering different application
    scenarios. We now give a brief summary of different 3D shape representation categories.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 已经出版了几篇相关的调查文章 [[9](#bib.bib9), [1](#bib.bib1), [35](#bib.bib35)]，它们集中于深度学习在三维几何方面的不同方面。此外，随着三维形状表示和深度学习相关技术的快速发展，进一步总结最新的研究工作变得尤为重要。在这项调查中，我们主要回顾了三维形状表示的深度学习方法，并讨论了它们在不同应用场景下的优缺点。我们现在简要总结了不同的三维形状表示类别。
- en: Depth and multi-view images can be used to represent 3D models in the 2D field.
    The regular structure of images makes them efficient to be processed. Depending
    on whether depth maps are included, 3D shapes can be presented by RGB (color)
    or RGB-D (color and depth) images viewed from different viewpoints. Because of
    the influx of available depth data due to the popularity of 2.5D sensors, such
    as Microsoft Kinect, Intel RealSense, etc., multi-view RGB-D images are widely
    used to represent real-world 3D shapes. The large asset of image-based processing
    models can be leveraged using this representation. But it is inevitable that this
    kind of representation loses some geometry features.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图和多视角图像可以用来在二维领域中表示三维模型。图像的规则结构使得它们能够高效处理。根据是否包含深度图，三维形状可以通过不同视角下的 RGB（颜色）或
    RGB-D（颜色和深度）图像来呈现。由于 2.5D 传感器（如微软 Kinect、英特尔 RealSense 等）的普及，使得大量深度数据涌现，多视角 RGB-D
    图像被广泛用于表示现实世界的三维形状。可以利用这种表示方式来发挥图像处理模型的大量资产。但不可避免地，这种表示方式会丢失一些几何特征。
- en: A voxel is a 3D extension of the concept of pixel. Similar with pixels in 2D,
    the voxel-based representation also has a regular structure in the 3D space. The
    architectures of some neural networks which have been demonstrated useful in the
    2D image field [[48](#bib.bib48), [50](#bib.bib50)] can be easily extended to
    the voxel form. Nevertheless, adding one dimension means an exponentially increased
    data size. As the resolution increases, the amount of required memory and computational
    costs increase dramatically, which restricts the representation only to low resolutions
    when representing 3D shapes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 体素是像素概念的三维扩展。与二维中的像素类似，基于体素的表示在三维空间中也具有规则结构。已经在二维图像领域中证明有效的某些神经网络架构 [[48](#bib.bib48),
    [50](#bib.bib50)] 可以很容易地扩展到体素形式。然而，增加一个维度意味着数据量呈指数增长。随着分辨率的提高，所需的内存和计算成本急剧增加，这限制了三维形状表示仅适用于低分辨率。
- en: Surface-based representation describes 3D shapes by encoding their surfaces,
    which can also be regarded as 2-manifolds. Point clouds and meshes are both discretized
    forms of 3D shape surfaces. Point clouds use a set of sampled 3D point coordinates
    to represent the surface. It can be easily generated by scanners but difficult
    to process due to their lack of order and connectivity information. Researchers
    use order invariant operators such as the max pooling operator in deep neural
    networks [[75](#bib.bib75), [77](#bib.bib77)] to mitigate the lack of order problem.
    Meshes can depict higher quality 3D shapes with less memory and computational
    cost compared with point clouds and voxels. A mesh contains a vertex set and an
    edge set. Due to its graphical nature, researchers have made attempts to build
    graph-based convolutional neural networks for coping with meshes. Some other methods
    regard meshes as the discretization of 2-manifolds. Moreover, meshes are more
    suitable for 3D shape deformation. One can deform a mesh model by transforming
    vertices while keeping the connectivity at the same time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于表面的表示通过编码三维形状的表面来描述三维形状，这也可以被视为二维流形。点云和网格都是三维形状表面的离散化形式。点云使用一组采样的三维点坐标来表示表面。它可以通过扫描仪轻松生成，但由于缺乏顺序和连接信息，处理起来困难。研究人员使用顺序不变的运算符，例如深度神经网络中的最大池化运算符 [[75](#bib.bib75),
    [77](#bib.bib77)] 来缓解缺乏顺序的问题。与点云和体素相比，网格可以以更少的内存和计算成本描绘更高质量的三维形状。一个网格包含一个顶点集和一个边集。由于其图形特性，研究人员尝试构建基于图的卷积神经网络来处理网格。其他一些方法将网格视为二维流形的离散化。此外，网格更适合三维形状变形。可以通过变换顶点来变形一个网格模型，同时保持连接性。
- en: Implicit surface representation exploits implicit field functions, such as occupancy
    functions [[67](#bib.bib67)] and signed distance functions [[116](#bib.bib116)],
    to describe the surface of 3D shapes. The implicit functions learned by deep neural
    networks define the spatial relationship between points and surfaces. They provide
    a description with infinite resolution of 3D shapes with reasonable memory consumption,
    and are capable of representing shapes with changing topology. Nevertheless, implicit
    representations cannot reflect the geometric features of 3D shapes directly, and
    usually need to be transformed to explicit representations such as meshes. Most
    methods apply iso-surfacing, such as marching cubes [[58](#bib.bib58)], which
    is an expensive operation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式表面表示利用隐式场函数，如占用函数[[67](#bib.bib67)]和有符号距离函数[[116](#bib.bib116)]，来描述3D形状的表面。深度神经网络学习的隐式函数定义了点与表面之间的空间关系。它们在合理的内存消耗下提供了具有无限分辨率的3D形状描述，并能够表示拓扑变化的形状。然而，隐式表示无法直接反映3D形状的几何特征，通常需要转换为显式表示，如网格。大多数方法应用等值面提取，如行进立方体[[58](#bib.bib58)]，这是一项昂贵的操作。
- en: Structured representation. One way to cope with complex 3D shapes is to decompose
    them into structure and geometric details, leading to structured representations.
    Recently, increasingly more methods regard a 3D shape as a collection of parts
    and organize them linearly or hierarchically. The structure of 3D shapes is processed
    by Recurrent Neural Networks (RNNs) [[121](#bib.bib121)], Recursive Neural Networks
    (RvNNs) [[51](#bib.bib51)] or other network architectures. Each part of the shape
    can be processed by unstructured models. The structured representation focuses
    on the relations (such as symmetry, supporting, being supported, etc.) between
    different parts within a 3D shape, which provides better description capability
    than alternative representations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化表示。应对复杂的3D形状的一种方法是将其分解为结构和几何细节，从而得到结构化表示。近年来，越来越多的方法将3D形状视为部件的集合，并将其线性或层次性地组织起来。3D形状的结构由递归神经网络（RNNs）[[121](#bib.bib121)]、递归神经网络（RvNNs）[[51](#bib.bib51)]或其他网络架构处理。形状的每个部分可以由非结构化模型处理。结构化表示关注于3D形状内部不同部分之间的关系（例如对称、支撑、被支撑等），相比于其他表示方法，它提供了更好的描述能力。
- en: Deformation-based representation. Unlike rigid man-made 3D shapes such as chairs
    and tables, there are also a large number of non-rigid (e.g. articulated) 3D shapes
    such as human bodies, which also play an important role in computer animation,
    augmented reality, etc. The deformation-based representation is proposed mainly
    for describing the intrinsic deformation properties while ignoring the extrinsic
    transformation properties. Many methods use rotation-invariant local features
    for describing shape deformation to reduce the distortion and keep the geometry
    details at the same time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变形的表示。与诸如椅子和桌子等刚性人造3D形状不同，还有大量非刚性（例如关节）3D形状，如人体，这些形状在计算机动画、增强现实等领域也发挥着重要作用。基于变形的表示主要用于描述固有的变形属性，同时忽略外在的变换属性。许多方法使用旋转不变的局部特征来描述形状变形，以减少扭曲并同时保持几何细节。
- en: 'Recently, deep learning has achieved superior performance in contrast to classical
    methods in many fields, including 3D shape analysis, reconstruction, etc. A variety
    of architectures of deep networks have been designed to process or generate 3D
    shape representations, which we refer to as *geometry learning*. In the following
    sections, we focus more on most recent deep learning based methods for representing
    and processing 3D shapes in different forms. According to how the representation
    is encoded and stored, our survey is organized in the following structure: Section [2](#S2
    "2 Image-based methods") reviews image-based shape representation methods. Sections
    [3](#S3 "3 Voxel-based representations") and [4](#S4 "4 Surface-based representations")
    introduce voxel-based and surface-based representations respectively. Section
    [5](#S5 "5 Implicit representations") further introduces implicit surface representations.
    Sections [6](#S6 "6 Structure-based representations") and [7](#S7 "7 Deformation-based
    representations") review structure-based and deformation-based description methods.
    We then summarize typical datasets in Section [8](#S8 "8 Datasets") and typical
    applications for shape analysis and reconstruction in Section [9](#S9 "9 Shape
    Analysis and Reconstruction"), before concluding the paper in Section [10](#S10
    "10 Summary"). Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction") summarizes the timeline
    of representative deep learning methods based on various 3D shape representations.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习在许多领域，包括3D形状分析、重建等方面，相较于经典方法取得了卓越的性能。已经设计了多种深度网络架构来处理或生成3D形状表示，我们称之为*几何学习*。在接下来的章节中，我们将更多地关注基于深度学习的最新方法，用于以不同形式表示和处理3D形状。根据表示的编码和存储方式，我们的综述按以下结构组织：第[2](#S2
    "2 Image-based methods")节回顾了基于图像的形状表示方法。第[3](#S3 "3 Voxel-based representations")节和第[4](#S4
    "4 Surface-based representations")节分别介绍了体素基和表面基的表示方法。第[5](#S5 "5 Implicit representations")节进一步介绍了隐式表面表示方法。第[6](#S6
    "6 Structure-based representations")节和第[7](#S7 "7 Deformation-based representations")节回顾了基于结构和基于变形的描述方法。接着，在第[8](#S8
    "8 Datasets")节中总结了典型的数据集，在第[9](#S9 "9 Shape Analysis and Reconstruction")节中总结了形状分析和重建的典型应用，然后在第[10](#S10
    "10 Summary")节中总结全文。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction")总结了基于各种3D形状表示的代表性深度学习方法的时间线。
- en: <svg   height="322.23" overflow="visible" version="1.1" width="729.7"><g transform="translate(0,322.23)
    matrix(1 0 0 -1 0 0) translate(4.61,0) translate(0,5)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 25.53 168.27)" fill="#000000"
    stroke="#000000" color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 143.64 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.75 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 379.86 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g></g><g stroke="#FF0000"
    fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 303.97)" fill="#000000" stroke="#000000"><foreignobject width="58.81"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">depth map</foreignobject></g></g><g
    stroke="#FF8000" fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 292.16)" fill="#000000" stroke="#000000"><foreignobject width="98.31"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">multi-view
    images</foreignobject></g></g><g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 52.84 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="109.8" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">voxel representation</foreignobject></g></g><g
    stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 268.71)" fill="#000000" stroke="#000000"><foreignobject width="110.49"
    height="10.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">point representation</foreignobject></g></g><g
    stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 303.97)" fill="#000000" stroke="#000000"><foreignobject width="109.52"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">mesh representation</foreignobject></g></g><g
    stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 292.16)" fill="#000000" stroke="#000000"><foreignobject width="83.47"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">implicit
    surface</foreignobject></g></g><g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt"
    color="#BF8040"><g transform="matrix(1.0 0.0 0.0 1.0 214.26 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="137.96" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">structured representation</foreignobject></g></g><g
    stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 268.54)" fill="#000000" stroke="#000000"><foreignobject width="147.19"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">deformation
    representation</foreignobject></g></g> <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 0 118.58)" fill="#00FFFF"
    stroke="#00FFFF"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3D ShapeNets[[112](#bib.bib112)] <g stroke="#FF8000"
    fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><path d="M 66.93 157.48
    L 66.93 181.1" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 27.56 193.62)"
    fill="#FF8000" stroke="#FF8000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MVCNN[[93](#bib.bib93)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 66.93 200.79 L 66.93 224.41" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 27.56 236.92)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GCNN[[61](#bib.bib61)]</foreignobject></g></path></g>
    <g stroke="#FF0000" fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><path
    d="M 106.3 157.48 L 106.3 118.11" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 47.24 102.83)" fill="#FF0000" stroke="#FF0000"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Eigen et
    al.[[20](#bib.bib20)]</foreignobject></g></path></g> <g stroke="#000000" fill="#000000"
    stroke-width="1.42264pt" color="#000000"><path d="M 165.35 157.48 L 165.35 196.85"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 145.67 209.37)" fill="#000000"
    stroke="#000000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">RIMD[[24](#bib.bib24)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 188.98 157.48 L 188.98 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 149.61 118.58)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3D-R2N2[[16](#bib.bib16)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 220.47 157.48 L 220.47 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 188.98 193.62)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3D-GAN[[110](#bib.bib110)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 295.28 157.48 L 295.28 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 251.97 118.58)" fill="#00FF00" stroke="#00FF00"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointNet[[75](#bib.bib75)]
    PointOutNet[[21](#bib.bib21)]</foreignobject></g></path></g> <g stroke="#00FFFF"
    fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path d="M 295.28 102.36
    L 295.28 78.74" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 263.78
    63.46)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="118.11" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OctNet[[80](#bib.bib80)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 295.28 157.48 L 295.28 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 275.59 193.62)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O-CNN[[105](#bib.bib105)]</foreignobject></g></path></g>
    <g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt" color="#BF8040"><path
    d="M 295.28 204.72 L 295.28 228.35" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 275.59 240.86)" fill="#BF8040" stroke="#BF8040"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GRASS[[51](#bib.bib51)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 362.2 157.48 L 362.2 196.85" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 314.96 209.37)" fill="#00FF00" stroke="#00FF00"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointNet++[[77](#bib.bib77)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 433.07 157.48 L 433.07 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 393.7 118.58)" fill="#0000FF" stroke="#0000FF"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pixel2Mesh[[104](#bib.bib104)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 472.44 157.48 L 472.44 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 425.2 193.62)" fill="#00FF00" stroke="#00FF00"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointCNN[[53](#bib.bib53)]</foreignobject></g></path></g>
    <g stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><path
    d="M 519.68 157.48 L 519.68 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 480.31 118.58)" fill="#D9668C" stroke="#D9668C"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DeepSDF [[74](#bib.bib74)]
    IM-NET[[14](#bib.bib14)]</foreignobject></g></path></g> <g stroke="#BF8040" fill="#BF8040"
    stroke-width="1.42264pt" color="#BF8040"><path d="M 551.18 157.48 L 551.18 181.1"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 511.81 193.62)" fill="#BF8040"
    stroke="#BF8040"><foreignobject width="78.74" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SDM-NET[[27](#bib.bib27)] StructureNet[[68](#bib.bib68)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 551.18 216.54 L 551.18 240.16" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 511.81 248.74)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MeshCNN[[39](#bib.bib39)]</foreignobject></g></path></g>
    <g stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><path
    d="M 590.55 157.48 L 590.55 118.11" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 551.18 102.83)" fill="#D9668C" stroke="#D9668C"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Without 3D
    Supervision [[56](#bib.bib56)]</foreignobject></g></path></g> <g stroke="#BF8040"
    fill="#BF8040" stroke-width="1.42264pt" color="#BF8040"><path d="M 637.8 157.48
    L 637.8 133.86" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 602.36
    118.58)" fill="#BF8040" stroke="#BF8040"><foreignobject width="118.11" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">BSP-Net[[13](#bib.bib13)]</foreignobject></g></path></g>
    <g stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><path
    d="M 637.8 118.11 L 637.8 86.61" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 622.05 71.33)" fill="#333333" stroke="#333333"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NASA[[45](#bib.bib45)]</foreignobject></g></path></g><g
    stroke="#B3B3B3" fill="#B3B3B3" stroke-width="2.27621pt" color="#B3B3B3"><path
    d="M 7.87 157.48 L 657.3 157.48" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 657.3 157.48)"><path d="M 11.99 0 C 8.44 0.67 2.66 2.66 -1.33 5 L -1.33
    -5 C 2.66 -2.66 8.44 -0.67 11.99 0" style="stroke:none"></path></g></path></g>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The timeline of deep learning based methods for various 3D shape
    representations.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于深度学习的各种3D形状表示方法的时间线。
- en: 2 Image-based methods
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于图像的方法
- en: 2D images are the projections of 3D entities. Although the geometric information
    carried by one image is incomplete, a plausible 3D shape could be inferred from
    a set of images with different perspectives. The extra channel of depth in RGB-D
    data further enhances the capacity of image-based representations on encoding
    geometric cues. Benefiting from its image-like structure, the research using deep
    neural networks on 3D shape inferences from images started earlier than alternative
    representations that can depict the surface or geometry of 3D shapes explicitly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2D图像是3D实体的投影。虽然单张图像所携带的几何信息是不完整的，但可以从一组不同视角的图像中推断出一个合理的3D形状。RGB-D数据中的深度通道进一步增强了基于图像的表示在编码几何线索方面的能力。由于其类似图像的结构，使用深度神经网络从图像中进行3D形状推断的研究早于那些可以明确描绘3D形状表面或几何的替代表示方法。
- en: Socher et al. [[89](#bib.bib89)] proposed a convolutional and recursive neural
    network for 3D object recognition, which copes with RGB and depth images by single
    convolutional layers separately and merges the features by a recursive network.
    Eigen et al. [[20](#bib.bib20)] first proposed to reconstruct the depth map from
    a single RGB image and designed a new scale invariant loss for the training stage.
    Gupta et al. [[37](#bib.bib37)] encoded the depth map into three channels including
    disparity, height and angle. Other deep learning methods based on RGB-D images
    are designed for 3D object detection [[36](#bib.bib36), [91](#bib.bib91)], outperforming
    previous methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Socher等人[[89](#bib.bib89)]提出了一种卷积和递归神经网络用于3D对象识别，该网络通过单独的卷积层处理RGB和深度图像，并通过递归网络合并特征。Eigen等人[[20](#bib.bib20)]首次提出从单张RGB图像重建深度图，并为训练阶段设计了一种新的尺度不变损失。Gupta等人[[37](#bib.bib37)]将深度图编码为包括视差、高度和角度的三个通道。其他基于RGB-D图像的深度学习方法被设计用于3D对象检测[[36](#bib.bib36),
    [91](#bib.bib91)]，表现优于之前的方法。
- en: Images from different viewpoints can provide complementary cues to infer 3D
    objects. Thanks to the development of deep learning models in 2D fields, the learning
    methods based on multi-view image representation perform better in the 3D shape
    recognition application than those based on other 3D representations. Su et al. [[93](#bib.bib93)]
    proposed MVCNN (Multi-View Convolutional Neural Network) for 3D object recognition.
    MVCNN first processes the images in different views separately by the first part
    of CNN, then aggregates the features extracted from different views by view-pooling
    layers, and finally puts the merged feature to the remaining part of CNN. Qi et
    al. [[76](#bib.bib76)] propose to add multi-resolution into MVCNN for higher classification
    accuracy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同视角获取的图像可以提供互补线索以推断3D对象。得益于深度学习模型在2D领域的发展，基于多视图图像表示的学习方法在3D形状识别应用中表现优于基于其他3D表示的方法。Su等人[[93](#bib.bib93)]提出了MVCNN（多视图卷积神经网络）用于3D对象识别。MVCNN首先通过CNN的第一部分分别处理不同视角的图像，然后通过视图池化层聚合从不同视角提取的特征，最后将合并后的特征输入到CNN的剩余部分。Qi等人[[76](#bib.bib76)]提出在MVCNN中加入多分辨率以提高分类准确性。
- en: 3 Voxel-based representations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 体素基表示
- en: 3.1 Dense Voxel Representation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 密集体素表示
- en: The voxel-based representation is traditionally a dense representation, which
    describes 3D shape data by volumetric grids in 3D space. Each voxel in the grid
    records the status of occupancy (e.g., occupied or unoccupied) within a cuboid
    grid.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 体素基表示传统上是一种密集表示，它通过3D空间中的体积网格描述3D形状数据。网格中的每个体素记录了在立方体网格内的占用状态（例如，已占用或未占用）。
- en: 'One of the earliest methods that applies deep neural networks to volumetric
    representations was proposed by Wu et al. [[112](#bib.bib112)] in 2015, which
    is called 3D ShapeNets. Wu et al. assigned three different states to the voxels
    in the volumetric representation produced by 2.5D depth maps: observed, unobserved
    and free. 3D ShapeNets extended the deep belief network (DBN) [[41](#bib.bib41)]
    from pixel data to voxel data and replaced fully connected layers in DBN with
    convolutional layers. The model takes the aforementioned volumetric representation
    as input, and outputs category labels and predicted 3D shape by iterative computations.
    Concurrently, Maturana et al. proposed to process the volumetric representation
    with 3D Convolutional Neural Networks (3D CNNs) [[62](#bib.bib62)] and designed
    VoxNet [[63](#bib.bib63)] for object recognition. VoxNet defines several volumetric
    layers, including Input Layer, Convolutional Layers, Pooling Layers and Fully
    Connected Layers. Although these defined layers are simple extensions of traditional
    2D CNNs [[48](#bib.bib48)] to 3D, VoxNet is easy to implement and train and gets
    promising performance as the first attempt on volumetric convolutions. In addition,
    to ensure that VoxNet is invariant to orientation, Maturana et al. further augment
    the input data by rotating each shape into $n$ instances with different orientations
    in the training stage and adding a pooling operation after the output layer to
    group all the predictions from the $n$ instances in the test stage.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 应用深度神经网络到体积表示的最早方法之一是由吴等人于2015年提出的，这种方法称为3D ShapeNets。吴等人将体积表示中的体素分配为三种不同的状态：已观察、未观察和自由。3D
    ShapeNets将深度置信网络（DBN）从像素数据扩展到体素数据，并将DBN中的全连接层替换为卷积层。该模型将上述体积表示作为输入，通过迭代计算输出类别标签和预测的3D形状。与此同时，Maturana等人提出使用3D卷积神经网络（3D
    CNNs）处理体积表示，并设计了用于物体识别的VoxNet。VoxNet定义了几个体积层，包括输入层、卷积层、池化层和全连接层。虽然这些定义的层是将传统的2D
    CNNs扩展到3D的简单扩展，但VoxNet易于实现和训练，并在体积卷积的首次尝试中取得了有希望的性能。此外，为了确保VoxNet对方向的不变性，Maturana等人在训练阶段通过将每个形状旋转成$n$个不同方向的实例来扩充输入数据，并在输出层后添加一个池化操作，以在测试阶段将所有来自$n$个实例的预测结果进行分组。
- en: In addition to the development of deep belief networks and convolutional neural
    networks in shape analysis based on volumetric representation, two most successful
    generative models, namely auto-encoders and Generative Adversarial Networks (GANs) [[33](#bib.bib33)]
    are also extended to support this representation. Inspired by Denoising Auto-Encoders
    (DAEs) [[101](#bib.bib101), [102](#bib.bib102)], Sharma et al. proposed an autoencoder
    model VConv-DAE for coping with voxels [[83](#bib.bib83)]. It is one of the earliest
    unsupervised learning approaches in voxel-based shape analysis to our knowledge.
    Without object labels for training, VConv-DAE chooses mean square loss or cross
    entropy loss as the reconstruction loss function. Girdhar et al. [[32](#bib.bib32)]
    also proposed TL-embedding Network, which combine an auto-encoder for generating
    a voxel-based representation with a convolutional neural network for predicting
    the embeddings from the 2D images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于体积表示的形状分析中深度置信网络和卷积神经网络的发展外，两个最成功的生成模型，即自编码器和生成对抗网络（GANs）也被扩展以支持这种表示。受去噪自编码器（DAEs）的启发，Sharma等人提出了一种自编码器模型VConv-DAE，用于处理体素。这是我们所知的最早的体素基础形状分析的无监督学习方法之一。在没有对象标签进行训练的情况下，VConv-DAE选择均方误差或交叉熵损失作为重建损失函数。Girdhar等人还提出了TL-embedding网络，该网络结合了一个自编码器用于生成基于体素的表示和一个卷积神经网络用于从2D图像中预测嵌入。
- en: 'Choy et al. [[16](#bib.bib16)] proposed 3D-R2N2 which takes single or multiple
    images as input and reconstructs objects in occupancy grids. 3D-R2N2 regards input
    images as a sequence and designs the 3D recurrent neural network based on LSTM
    (Long Short-Term Memory) [[42](#bib.bib42)] or GRU (Gated Recurrent Unit) [[15](#bib.bib15)].
    The architecture consists of three parts: an image encoder to extract features
    from 2D images, 3D-LSTM to predict hidden states as coarse representations of
    final 3D models, and a decoder to increase the resolution and generate target
    shapes.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Choy 等人[[16](#bib.bib16)] 提出了 3D-R2N2，该方法将单张或多张图像作为输入，并在占据网格中重建物体。3D-R2N2 将输入图像视为一个序列，并基于
    LSTM（长短期记忆）[[42](#bib.bib42)] 或 GRU（门控递归单元）[[15](#bib.bib15)] 设计了3D递归神经网络。该架构包括三个部分：一个图像编码器用于从2D图像中提取特征，3D-LSTM
    用于预测隐藏状态作为最终3D模型的粗略表示，以及一个解码器用于增加分辨率并生成目标形状。
- en: Wu et al. [[110](#bib.bib110)] designed a generative model called 3D-GAN that
    applies the Generative Adversarial Network (GAN) [[33](#bib.bib33)] in voxel data.
    3D GAN learns to synthesize a 3D object from a sampled latent space vector $z$
    with the probability distribution $P(z)$. Moreover, [[110](#bib.bib110)] also
    proposed 3D-VAE-GAN inspired by VAE-GAN [[49](#bib.bib49)] for the object reconstruction
    task. 3D-VAE-GAN puts the encoder before 3D-GAN for inferring the latent vector
    $z$ from input 2D images and shares the decoder with the generator of 3D-GAN.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人[[110](#bib.bib110)] 设计了一个名为 3D-GAN 的生成模型，该模型将生成对抗网络（GAN）[[33](#bib.bib33)]
    应用于体素数据。3D GAN 学会从采样的潜在空间向量 $z$ 中合成一个3D物体，且概率分布为 $P(z)$。此外，[[110](#bib.bib110)]
    还提出了受 VAE-GAN[[49](#bib.bib49)] 启发的 3D-VAE-GAN，用于物体重建任务。3D-VAE-GAN 在 3D-GAN 之前放置了编码器，用于从输入的2D图像中推断潜在向量
    $z$，并与3D-GAN的生成器共享解码器。
- en: After the early attempts in dealing with volumetric representations by deep
    learning, researchers began to optimize the architecture of volumetric networks
    for better performance and more applications. A motivation is that the naive extension
    from traditional 2D domain networks often does not perform better than image-based
    CNNs such as MVCNN [[93](#bib.bib93)]. The main challenges affecting the performance
    include overfitting, orientation, data sparsity and low resolution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期处理体积表示的深度学习尝试之后，研究人员开始优化体积网络的架构，以提高性能和拓展应用。一个动机是，传统2D领域网络的简单扩展往往不如基于图像的CNNs（如MVCNN[[93](#bib.bib93)]）表现得更好。影响性能的主要挑战包括过拟合、方向、数据稀疏性和低分辨率。
- en: Qi et al. [[76](#bib.bib76)] proposed two new network structures aiming to improve
    the performance of volumetric CNNs. One introduces an extra task namely predicting
    class labels with subvolume space to prevent overfitting, and another utilizes
    elongated kernels to compress the 3D information into the 2D field in order to
    use 2D CNNs directly. Both of them use mlpconv layers [[55](#bib.bib55)] to replace
    traditional convolutional layers. [[76](#bib.bib76)] also augments the input data
    in different orientation and elevation to encourage the network to get more local
    features in different poses so that the results are less influenced by orientation
    changes. To further mitigate the orientation impact on recognition accuracy, instead
    of using data augmentation like [[63](#bib.bib63), [76](#bib.bib76)], [[82](#bib.bib82)]
    proposed a new model called ORION which extends VoxNet [[63](#bib.bib63)] and
    uses a fully connected layer to predict the object class label and orientation
    label simultaneously.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Qi 等人[[76](#bib.bib76)] 提出了两种新的网络结构，旨在提高体积卷积神经网络（CNNs）的性能。其中一种引入了一个额外的任务，即利用子体积空间预测类别标签，以防止过拟合；另一种则利用延长的卷积核将3D信息压缩到2D领域中，以便直接使用2D
    CNNs。这两种方法都使用了 mlpconv 层[[55](#bib.bib55)] 来替代传统的卷积层。[[76](#bib.bib76)] 还通过不同的方向和高度来增强输入数据，以鼓励网络在不同的姿态下获取更多的局部特征，从而使结果不易受到方向变化的影响。为了进一步减轻方向对识别准确性的影响，[[82](#bib.bib82)]
    提出了一个名为 ORION 的新模型，该模型扩展了 VoxNet[[63](#bib.bib63)]，并使用全连接层同时预测物体类别标签和方向标签。
- en: 3.2 Sparse Voxel Representation (Octree)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 稀疏体素表示（Octree）
- en: Voxel-based representations often lead to high computational cost because of
    the exponential increase of computations from pixels to voxels. Most of the methods
    cannot cope with or generate high-resolution models within reasonable time. For
    instance, TL-embedding Network [[32](#bib.bib32)] was designed for $20^{3}$ voxel
    grids; 3DShapeNets [[112](#bib.bib112)] and VConv-DAE [[83](#bib.bib83)] were
    designed for $24^{3}$ voxel grids with 3 voxels padding on each direction of the
    voxel grids; VoxNet [[63](#bib.bib63)], 3D-R2N2 [[16](#bib.bib16)] and ORION [[82](#bib.bib82)]
    were designed for $32^{3}$ voxel grids; 3D-GAN was designed for generating $64^{3}$
    occupancy grids as 3D shape representation. As the voxel resolution increases,
    the occupied grids become sparser in the whole 3D space, which leads to more unnecessary
    computation. To address this problem, Li et al. [[54](#bib.bib54)] designed a
    novel method called FPNN to cope with the data sparsity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于体素的表示通常会导致高计算成本，因为从像素到体素的计算量呈指数增长。大多数方法无法应对或在合理时间内生成高分辨率模型。例如，TL-embedding
    Network [[32](#bib.bib32)] 设计用于 $20^{3}$ 体素网格；3DShapeNets [[112](#bib.bib112)]
    和 VConv-DAE [[83](#bib.bib83)] 设计用于 $24^{3}$ 体素网格，每个方向上有 3 体素填充；VoxNet [[63](#bib.bib63)]、3D-R2N2 [[16](#bib.bib16)]
    和 ORION [[82](#bib.bib82)] 设计用于 $32^{3}$ 体素网格；3D-GAN 设计用于生成 $64^{3}$ 占据网格作为 3D
    形状表示。随着体素分辨率的增加，占据网格在整个 3D 空间中变得更加稀疏，从而导致更多不必要的计算。为了解决这个问题，Li 等人 [[54](#bib.bib54)]
    设计了一种新方法，称为 FPNN，以应对数据稀疏性。
- en: Some methods instead encode the voxel grids by a sparse, adaptive data structure,
    namely octree [[64](#bib.bib64)] to reduce the dimensionality of the input data.
    Häne et al. [[38](#bib.bib38)] proposed Hierarchical Surface Prediction (HSP)
    that can generate voxel grids in the form of octree from coarse to fine. Häne
    et al. observed that only the voxels near the object surface need to be predicted
    in a high resolution, so that the proposed HSP can avoid unnecessary calculation
    to ensure affordable generation of high resolution voxel grids. As introduced
    in [[38](#bib.bib38)], each node in the octree is defined as a voxel block with
    a fixed number ($16^{3}$ in the paper) of voxels in different size, and each voxel
    block is classified into occupied, boundary and free. The decoder of the model
    takes a feature vector as input, and predicts feature blocks that correspond to
    voxel blocks hierarchically. The HSP defines that the octree has 5 layers and
    each voxel blocks contains $16^{3}$ voxels, therefore, HSP can generate up to
    $256^{3}$ voxel grids. Tatarchenko et al. [[98](#bib.bib98)] also proposed a decoder
    called OGN for generating high resolution volumetric representations. In [[98](#bib.bib98)],
    nodes in the octree are separated into three categories, including “empty”, “filled”
    and “mixed”. The octree representing a 3D model and the feature map of the octree
    are stored in the form of hashing tables which are indexed by the spatial position
    and the octree level. In order to process the feature maps represented as hash
    tables, Tatarchenko et al. designed a convolutional layer named OGN-Conv, which
    converts the convolutional operation into matrix multiplication. [[98](#bib.bib98)]
    adopts the method that generates different resolution of voxel grids in each decoder
    layer by convolutional operations in feature maps, and then decides whether to
    propagate the features to the next layer by specific labels (propagating the features
    if “boundary” and skipping the feature propagation if “mixed”).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法则通过稀疏的自适应数据结构，即八叉树 [[64](#bib.bib64)]，来编码体素网格，从而减少输入数据的维度。Häne 等人 [[38](#bib.bib38)]
    提出了层次表面预测（HSP），可以从粗到细生成八叉树形式的体素网格。Häne 等人观察到，仅需对靠近物体表面的体素进行高分辨率预测，因此提出的 HSP 可以避免不必要的计算，以确保高分辨率体素网格的生成是可承受的。如
    [[38](#bib.bib38)] 中介绍的，八叉树中的每个节点定义为一个体素块，具有固定数量（论文中为 $16^{3}$）的不同大小的体素，每个体素块被分类为占用、边界和自由。模型的解码器以特征向量作为输入，并分层预测对应于体素块的特征块。HSP
    定义八叉树有 5 层，每个体素块包含 $16^{3}$ 个体素，因此，HSP 可以生成最多 $256^{3}$ 体素网格。Tatarchenko 等人 [[98](#bib.bib98)]
    还提出了一种名为 OGN 的解码器，用于生成高分辨率体积表示。在 [[98](#bib.bib98)] 中，八叉树中的节点分为三类，包括“空”、“填充”和“混合”。表示
    3D 模型的八叉树和八叉树的特征图以哈希表的形式存储，这些哈希表由空间位置和八叉树级别索引。为了处理表示为哈希表的特征图，Tatarchenko 等人设计了一种名为
    OGN-Conv 的卷积层，将卷积操作转换为矩阵乘法。[[98](#bib.bib98)] 采用的方法是在每个解码器层中通过特征图的卷积操作生成不同分辨率的体素网格，然后根据特定标签决定是否将特征传播到下一层（如果是“边界”则传播特征，如果是“混合”则跳过特征传播）。
- en: Besides the decoder model design for synthesizing voxel grids, shape analysis
    methods are also designed using octrees. However, conventional octree structure [[64](#bib.bib64)]
    has difficulty to be used in deep networks, so many researchers try to resolve
    the problem by designing new structures of octrees and special operations such
    as convolution, pooling and unpooling on octrees. Riegler et al. [[80](#bib.bib80)]
    proposed OctNet. The octree representation mentioned in [[80](#bib.bib80)] has
    a relatively regular structure than a traditional octree, which places a shallow
    octree in regular 3D grids. The shallow octree is constrained to have up to 3
    levels and is encoded in 73 bits. Each bit determines if the corresponding cell
    needs to be split. Wang et al. [[105](#bib.bib105)] also proposed a convolutional
    neural network based on octree called O-CNN, where the model also removes pointers
    like shallow octree [[80](#bib.bib80)] and stores the octree data and structure
    by a series of vectors including shuffle key vectors, labels and input signals.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于合成体素网格的解码器模型设计外，形状分析方法也使用八叉树进行设计。然而，传统的八叉树结构[[64](#bib.bib64)]在深度网络中难以使用，因此许多研究人员尝试通过设计新的八叉树结构和特殊操作（如卷积、池化和反池化）来解决这个问题。Riegler等人[[80](#bib.bib80)]提出了OctNet。[[80](#bib.bib80)]中提到的八叉树表示相比传统八叉树具有相对规则的结构，它将一个浅层八叉树放置在规则的三维网格中。该浅层八叉树限制最多有3个层级，并以73位进行编码。每一位决定相应的单元是否需要拆分。Wang等人[[105](#bib.bib105)]还提出了一种基于八叉树的卷积神经网络，称为O-CNN，其中模型也去除了类似于浅层八叉树[[80](#bib.bib80)]的指针，并通过包括洗牌键向量、标签和输入信号的一系列向量来存储八叉树数据和结构。
- en: In addition to representing voxels, octree structure can also be utilized to
    represent 3D shape surfaces with planar patches. Wang et al. [[106](#bib.bib106)]
    proposed Adaptive O-CNN, where they defined another form of octree named patch-guided
    adaptive octree, which divides a 3D shape surface into a set of planar patches
    restricted by bounding boxes corresponding to octants. They also provided an encoder
    and a decoder for the octree defined by this paper.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表示体素外，八叉树结构还可以用于表示带有平面补丁的三维形状表面。Wang等人[[106](#bib.bib106)]提出了自适应O-CNN，他们定义了另一种形式的八叉树，称为补丁引导自适应八叉树，该树将三维形状表面划分为一组由对应于八分体的边界框限制的平面补丁。他们还为本文定义的八叉树提供了一个编码器和解码器。
- en: 4 Surface-based representations
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于表面的表示
- en: 4.1 Point-based Representation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于点的表示
- en: The typical point-based representation is also referred to as point clouds or
    point sets. They can be raw data generated by 3D scanning devices. Because of
    its unordered and irregular structure, this kind of representation is relatively
    difficult to cope with by traditional deep learning methods. Therefore, most researchers
    avoided to use point clouds in a direct way at the early stage of the deep learning-based
    geometry research. One of the first models to generate point clouds by deep learning
    came out in 2017 [[21](#bib.bib21)]. They designed a neural network to learn a
    point sampler based on 3D shape point distribution. The network takes a single
    image and a random vector as input, and outputs an $N\times 3$ matrix representing
    the predicted point sets ($x$, $y$, $z$ coordinates for $N$ points). In addition,
    [[21](#bib.bib21)] proposed to use Chamfer Distance (CD) and Earth Mover’s Distance
    (EMD) [[81](#bib.bib81)] as the loss function to train the networks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的基于点的表示也被称为点云或点集。它们可以是由三维扫描设备生成的原始数据。由于其无序和不规则的结构，这种表示方法相对较难被传统的深度学习方法处理。因此，大多数研究人员在深度学习基础的几何研究初期避免直接使用点云。2017年出现了第一个通过深度学习生成点云的模型[[21](#bib.bib21)]。他们设计了一种神经网络，用于学习基于三维形状点分布的点采样器。该网络以单张图像和一个随机向量作为输入，输出一个$N\times
    3$矩阵，表示预测的点集（$N$个点的$x$、$y$、$z$坐标）。此外，[[21](#bib.bib21)]提出使用Chamfer距离（CD）和地球搬运工距离（EMD）[[81](#bib.bib81)]作为损失函数来训练网络。
- en: PointNet. At almost the same time, Qi et al. [[75](#bib.bib75)] proposed PointNet
    for shape analysis, which was the first successful deep network architecture that
    directly processes point clouds without unnecessary rendering. The pipeline of
    PointNet is illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Point-based Representation
    ‣ 4 Surface-based representations"). On account of three properties of point sets
    mentioned in [[75](#bib.bib75)], PointNet designed three components in their network,
    including using max-pooling layers as symmetry functions for dealing with the
    unordered property, concatenating global and local features together for point
    interaction, and jointly aligning the network for transformation invariance. Based
    on PointNet, Qi et al. further improved this model and proposed PointNet++ [[77](#bib.bib77)],
    in order to resolve the problem that PointNet cannot capture and deal with local
    features induced by metric well. Compared with PointNet, PointNet++ introduces
    a hierarchical structure, so that the model can capture features in different
    scales, which improves the capability of extracting 3D shape features. As PointNet
    and PointNet++ show state-of-the-art performance in shape classification and semantic
    segmentation, more and more deep learning models were proposed based on point-based
    representations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f48dfdedb86af55dc200d7232469297d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The pipeline of PointNet Ref. [[75](#bib.bib75)], ©IEEE 2017.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks for Point Clouds. Some research works focus on
    applying CNNs to the irregular and unordered form of point clouds for analysis.
    Li et al. [[53](#bib.bib53)] proposed PointCNN for point clouds and designed the
    $\mathcal{X}$-transformation to weight and permute the input point features, which
    guarantees the equivariance in different point orders. Each feature matrix needs
    to be multiplied by the $\mathcal{X}$-transformation matrix before passing through
    the convolutional operator. This process is named $\mathcal{X}$-Conv operator,
    which is the key of PointCNN. Wang et al. [[108](#bib.bib108)] proposed DGCNN,
    a dynamic graph CNN architecture for point cloud classification and segmentation.
    Instead of processing point features like PointNet [[75](#bib.bib75)], DGCNN first
    connects neighboring points in spatial or semantic space to generate a graph,
    and then captures the local geometry features by applying the EdgeConv operator
    on it. Moreover, different from other graph CNNs which process the fixed input
    graph, DGCNN changes the graph to obtain new nearest neighbors in the feature
    space in different layers, which is beneficial to get larger and sparser receptive
    fields.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Other Point Cloud Processing Techniques using Neural Networks. Klokov et al. [[47](#bib.bib47)]
    proposed Kd-Network to process point clouds based on the form of kd-trees. Yang
    et al. [[117](#bib.bib117)] proposed FoldingNet, an end-to-end auto-encoder for
    further compressing a point-based representation with unsupervised learning. Because
    point clouds can be transformed into 2D grids by folding operations, FoldingNet
    integrates folding operations in their encoder-decoder to recover input 3D shapes.
    Mehr et al. [[65](#bib.bib65)] further proposed DiscoNet for 3D model editing
    by combining multiple autoencoders which are trained for different types of 3D
    shapes specifically. The autoencoders use pre-learned mean geometry of training
    3D shapes as their templates. Meng et al. [[66](#bib.bib66)] proposed VV-Net (Voxel
    VAE Net) for point segmentation, which represents a point cloud by a structured
    voxel representation. In VV-Net, instead of containing a boolean value to represent
    occupancy status of each voxel as a normal volumetric representation, it uses
    a latent code computed by an RBF-VAE, a variational autoencoder based on a radial
    basis function (RBF) interpolation of points to describe point distribution within
    a voxel. This representation is used to extract intrinsic symmetry of point clouds
    by a group equivariant CNN, and the output is combined with PointNet [[75](#bib.bib75)]
    for better segmentation performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络的其他点云处理技术。Klokov等人 [[47](#bib.bib47)] 提出了Kd-Network，用于基于kd树的点云处理。Yang等人 [[117](#bib.bib117)]
    提出了FoldingNet，这是一种端到端的自编码器，用于通过无监督学习进一步压缩基于点的表示。由于点云可以通过折叠操作转换为2D网格，FoldingNet在其编码器-解码器中集成了折叠操作以恢复输入的3D形状。Mehr等人 [[65](#bib.bib65)]
    进一步提出了DiscoNet，通过结合多个针对不同类型3D形状专门训练的自编码器进行3D模型编辑。这些自编码器使用训练3D形状的预学习平均几何形状作为其模板。Meng等人 [[66](#bib.bib66)]
    提出了VV-Net（体素VAE网络）用于点分割，该方法通过结构化体素表示来表示点云。在VV-Net中，它使用由RBF-VAE计算的潜在代码来描述体素内的点分布，而不是像常规体积表示那样包含一个布尔值来表示每个体素的占用状态。该表示方法用于通过群等变CNN提取点云的内在对称性，并将输出与PointNet [[75](#bib.bib75)]
    结合，以获得更好的分割性能。
- en: Although the point-based representation can be more easily obtained by 3D scanners
    than other 3D representations, this raw form of 3D shapes is often unsuitable
    for 3D shape analysis, due to noise and data sparsity. Therefore, compared with
    other representations, it is essential for the point-based representation to incorporate
    an upsampling module to obtain fine-grained point clouds, such as PU-NET [[119](#bib.bib119)],
    MPU [[118](#bib.bib118)], PU-GAN [[52](#bib.bib52)], etc. Additionally, point
    cloud registration is also an essential preprocessing step, e.g. to fuse points
    from multiple scans, which aims to calculate rigid transformation parameters to
    align the point clouds. Wang et al. [[107](#bib.bib107)] proposed Deep Closest
    Point (DCP), which extends traditional Iteractive Closest Point (ICP) method [[4](#bib.bib4)]
    and uses a deep learning method to obtain the transformation parameters. Recently,
    Guo et al. [[35](#bib.bib35)] presented a survey focusing on deep learning models
    in point clouds, which provides more details in this field.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于点的表示方法比其他3D表示方法更容易通过3D扫描仪获得，但由于噪声和数据稀疏，这种原始的3D形状形式通常不适合3D形状分析。因此，与其他表示方法相比，基于点的表示方法必须结合上采样模块以获取细粒度的点云，例如PU-NET [[119](#bib.bib119)]、MPU [[118](#bib.bib118)]、PU-GAN [[52](#bib.bib52)]等。此外，点云配准也是一个重要的预处理步骤，例如，融合多个扫描点，旨在计算刚性变换参数以对齐点云。Wang等人 [[107](#bib.bib107)]
    提出了Deep Closest Point (DCP)，该方法扩展了传统的迭代最近点（ICP）方法 [[4](#bib.bib4)]，并使用深度学习方法获得变换参数。最近，Guo等人 [[35](#bib.bib35)]
    提出了一个重点关注点云中的深度学习模型的调查，提供了该领域的更多细节。
- en: 4.2 Mesh-based Representations
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于网格的表示方法
- en: Compared with point-based representations, mesh-based representations contain
    connectivity between neighboring points, so they are more suitable for describing
    local regions on surfaces. As a typical type of representation in non-Euclidean
    space, mesh-based representations can be processed by deep learning models both
    in spatial and spectral domains [[9](#bib.bib9)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于点的表示方法相比，基于网格的表示方法包含邻近点之间的连通性，因此更适合描述表面上的局部区域。作为非欧几里得空间中的典型表示类型，基于网格的表示方法可以在空间和谱域中通过深度学习模型进行处理 [[9](#bib.bib9)]。
- en: Parametric representations for meshes. Directly applying CNNs to irregular data
    structures like meshes is non-trivial, so there emerged a handful of approaches
    that map 3D shape surfaces to 2D domains such as 2D geometry images which can
    also be regarded as another 3D shape representation, and apply traditional 2D
    CNNs on them [[87](#bib.bib87), [60](#bib.bib60)]. Based on geometry images, Sinha
    et al. [[88](#bib.bib88)] proposed SurfNet for shape generation using a deep residual
    network. Similarly, Shi et al. [[84](#bib.bib84)] projected 3D models into cylinder
    panoramic images, which are processed by CNNs. Some other methods convert mesh
    models into spherical signals, and design a convolutional operator in the spherical
    domain for shape analysis. To address high-resolution signals on 3D meshes, in
    particular texture information, Huang et al. [[43](#bib.bib43)] proposed TextureNet
    to extract features in this situation, where a 4-rotational symmetric (4-RoSy)
    field is defined to parametrize surfaces. In the following, we will review deep
    learning models according to how meshes are directly treated as input, and introduce
    generative models working on meshes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 网格的参数化表示。将 CNNs 直接应用于像网格这样的不规则数据结构并不简单，因此出现了一些将 3D 形状表面映射到 2D 域的方法，例如可以视为另一种
    3D 形状表示的 2D 几何图像，并在其上应用传统的 2D CNNs[[87](#bib.bib87), [60](#bib.bib60)]。基于几何图像，Sinha
    等人[[88](#bib.bib88)] 提出了 SurfNet，用于通过深度残差网络生成形状。类似地，Shi 等人[[84](#bib.bib84)] 将
    3D 模型投影到圆柱全景图像中，由 CNNs 处理。一些其他方法将网格模型转换为球面信号，并在球面域中设计卷积操作符以进行形状分析。为了解决 3D 网格上的高分辨率信号，特别是纹理信息，Huang
    等人[[43](#bib.bib43)] 提出了 TextureNet 以在这种情况下提取特征，其中定义了一个 4-旋转对称 (4-RoSy) 场来参数化表面。接下来，我们将回顾根据网格直接作为输入处理的深度学习模型，并介绍在网格上工作的生成模型。
- en: Graphs. The mesh-based representation is constructed by sets of vertices and
    edges, which can be seen as a graph. Some models were proposed based on the graph
    spectral theorem. They generalize CNNs on graphs [[10](#bib.bib10), [40](#bib.bib40),
    [18](#bib.bib18), [46](#bib.bib46), [2](#bib.bib2)] by eigen-decomposition of
    Laplacian matrices, which is able to generalize convolutional operators to the
    spectral domain of graphs. Verma et al. [[100](#bib.bib100)] proposed another
    graph-based CNN named FeaStNet, which computes the receptive fields of convolution
    operator dynamically. Specifically, FeaStNet determines the assignment of the
    neighbor vertices by using features obtained in networks. Hanocka et al. [[39](#bib.bib39)]
    also designed operators of convolution, pooling and unpooling for triangle meshes,
    and proposed MeshCNN. Different from other graph-based methods, MeshCNN focuses
    on processing the features stored in edges, and proposes a convolution operator
    that is applied to the edges with a fixed number of neighbors and a pooling operator
    based on edge collapse. MeshCNN extracts 3D shape features with respect to specific
    tasks, and the network learns to preserve the important features and ignore the
    unimportant ones.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图形。基于网格的表示是通过顶点和边的集合构建的，可以视为一种图形。基于图谱定理提出了一些模型。这些模型通过对拉普拉斯矩阵的特征分解来推广图形上的CNNs[[10](#bib.bib10),
    [40](#bib.bib40), [18](#bib.bib18), [46](#bib.bib46), [2](#bib.bib2)]，能够将卷积操作符推广到图形的谱域。Verma
    等人[[100](#bib.bib100)] 提出了另一种基于图形的 CNN，名为 FeaStNet，该网络动态计算卷积操作符的感受野。具体来说，FeaStNet
    通过使用在网络中获得的特征来确定邻居顶点的分配。Hanocka 等人[[39](#bib.bib39)] 还为三角网格设计了卷积、池化和反池化操作符，并提出了
    MeshCNN。与其他基于图形的方法不同，MeshCNN 侧重于处理存储在边上的特征，并提出了一种应用于具有固定数量邻居的边的卷积操作符，以及一种基于边收缩的池化操作符。MeshCNN
    根据特定任务提取 3D 形状特征，并使网络学习保留重要特征而忽略不重要的特征。
- en: '2-Manifolds. The mesh-based representation can be viewed as the discretization
    of 2-manifolds. Several works are designed in 2-manifolds with a series of refined
    CNN operators to adapt to this non-Euclidean space. These methods define their
    own local patches and kernel functions for generalizing CNN models. Masci et al. [[61](#bib.bib61)]
    proposed Geodesic Convolutional Neural Networks (GCNNs) for manifolds, which extract
    and discretize local geodesic patches and apply convolutional filters on these
    patches in polar coordinates. The convolution operator is designed in the spatial
    domain and their Geodesic CNN is quite similar to conventional CNNs applied in
    Euclidean space. Localized Spectral CNNs [[6](#bib.bib6)] proposed by Boscaini
    et al. apply Windowed Fourier transform to non-Euclidean space. Anisotropic Convolutional
    Neural Networks (ACNNs) [[7](#bib.bib7)] further designed an anisotropic heat
    kernel to replace the isotropic patch operator in GCNN [[61](#bib.bib61)], which
    gives another solution to avoid ambiguity. Xu et al. [[115](#bib.bib115)] proposed
    Directionally Convolutional Networks (DCNs), which defined local patches based
    on faces of the mesh representation. In this work, researchers also designed a
    two-stream network for 3D shape segmentation, which takes local face normals and
    the global face distance histogram as input for training. Moti et al. [[70](#bib.bib70)]
    proposed MoNet to replace the weight functions in [[61](#bib.bib61), [7](#bib.bib7)]
    with Gaussian kernels with learnable parameters. Fey et al. [[22](#bib.bib22)]
    proposed SplineCNN which designed a convolutional operator based on B-splines.
    Pan et al. [[72](#bib.bib72)] designed a surface CNN for 3D irregular surface
    to preserve the standard CNN property of translation equivariance by using parallel
    translation frames and group convolutional operations. Qiao et al. [[78](#bib.bib78)]
    proposed Laplacian Pooling Network (LaplacianNet) for 3D mesh analysis. The LaplacianNet
    considers both spectral and spatial information of the mesh, and contains 3 parts:
    preprocessing features as the network input, Mesh Pooling Blocks to split surface
    and cluster patches for feature extraction, and the Correlation Network to aggregate
    global information.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 2-流形。
- en: Generative Models. There are also many generative models for the mesh-based
    representation. Wang et al. [[104](#bib.bib104)] proposed Pixel2Mesh for reconstructing
    3D shapes from single images, which generates the target triangular mesh by deforming
    an ellipsoid template. As shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Mesh-based
    Representations ‣ 4 Surface-based representations"), the Pixel2Mesh network is
    implemented based on Graph-based Convolutional Networks (GCNs) [[9](#bib.bib9)]
    and generates the target mesh from coarse to fine by an unpooling operation. Wen
    et al. [[109](#bib.bib109)] advanced Pixel2Mesh and proposed Pixel2Mesh++, which
    extends single image 3D shape reconstruction to 3D shape reconstruction from multi-view
    images. To achieve this, Pixel2Mesh++ introduces a Multi-view Deformation Network
    (MDN) to the original Pixel2Mesh, and the MDN incorporates the cross-view information
    into the process of mesh generation. Groueix et al. [[34](#bib.bib34)] proposed
    AtlasNet, which generates 3D surfaces by multiple patches. AtlasNet learns to
    convert 2D square patches into 2-manifolds to cover the surface of 3D shapes by
    MLP (Multi-Layer Perceptron). Ben-Hamu et al. [[3](#bib.bib3)] proposed a multi-chart
    generative model for 3D shape generation. The method uses a multi-chart structure
    as input and builds the network architecture based on standard image GAN [[33](#bib.bib33)].
    The transformation between 3D surface and multi-chart structure is based on  [[60](#bib.bib60)].
    However, the methods based on deforming a template mesh into the target shape
    cannot express complex topology of some 3D shapes. Pan et al. [[73](#bib.bib73)]
    proposed a new single-view reconstruction method, which combines a deformation
    network and a topology modification network to model meshes with complex topology.
    In the topology modification network, the faces with high distortion are removed.
    Tang et al. [[97](#bib.bib97)] proposed to generate complex topology meshes by
    a skeleton-bridged learning method, because skeleton can well preserve topology
    information. Instead of generating triangular meshes, Nash et al. [[71](#bib.bib71)]
    proposed PolyGen to generate the polygon mesh representation. Inspired by neural
    autoregressive models in other fields like natural language processing, researchers
    regard mesh generation as a sequence, and design a transformer-based network [[99](#bib.bib99)],
    including a vertex model and a face model. The vertex model generates a sequence
    of vertex positions and the face model generates variable-length vertex sequences
    conditioned on input vertices.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ff955df32f94d5ed156863623aa093f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The pipeline of Pixel2Mesh Ref.[[104](#bib.bib104)] ©Springer 2018.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 5 Implicit representations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to explicit representations such as point clouds and meshes, implicit
    fields have been in greater popularity in recent studies. A major reason is that
    the implicit representation is not limited by fixed topology and resolution. There
    are an increasing number of deep models, which define their own implicit representations
    and building on them further propose various methods for shape analysis and generation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The Occupancy/Indicator Function is one of the forms to represent 3D shapes
    implicitly. Occupancy Network was proposed by Mescheder et al. [[67](#bib.bib67)]
    to learn a continuous occupancy function as a new representation of 3D shapes
    by neural networks. The occupancy function reflects the 3D point status with respect
    to the 3D shape surface, where 1 means inside the surface and 0 otherwise. Researchers
    regarded this problem as a binary classification task and designed an occupancy
    network which inputs 3D point position and 3D shape observation and outputs the
    probability of occupancy. The generated implicit field is then processed by a
    Multi-resolution IsoSurface Extraction method MISE and marching cubes algorithm [[58](#bib.bib58)]
    to obtain meshes. Moreover, researchers introduce encoder networks to obtain latent
    embeddings. Similarly, Chen et al. [[14](#bib.bib14)] designed IM-NET as a decoder
    for learning generative models, which also takes an implicit function in the form
    of an indicator function.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Signed Distance Functions (SDFs) are also a form of implicit representation.
    Signed distance functions map a 3D point to a real value instead of a probability,
    which indicates the spatial relation and distance to the 3D surface. Denote $SDF(x)$
    as the signed distance value of a given 3D point $x\in\mathbb{R}^{3}$. Then $SDF(x)>0$
    if point $x$ is outside the 3D shape surface, $SDF(x)<0$ if point $x$ is inside
    the surface, and $SDF(x)=0$ means point $x$ is on the surface. The absolute value
    of $SDF(x)$ refers to the distance between point $x$ and the surface. Park et
    al. [[74](#bib.bib74)] proposed DeepSDF and introduced an auto-decoder-based DeepSDF
    as a new 3D shape representation. Wang et al. [[116](#bib.bib116)] also proposed
    Deep Implicit Surface Networks (DISNs) for single-view 3D reconstruction based
    on SDFs. Thanks to the advantages of SDF, DISN was the first to reconstruct 3D
    shapes with flexible topology and thin structure in the single-view reconstruction
    task, which is difficult for other 3D representations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Function Sets. The occupancy functions and signed distance functions represent
    the 3D shape surface by a single function learned by a deep neural network. Genova
    et al. [[31](#bib.bib31), [30](#bib.bib30)] proposed to represent the whole 3D
    shape by combining a set of shape elements. In [[31](#bib.bib31)], researchers
    proposed Structured Implicit Functions (SIFs) where each element is represented
    by a scaled axis-aligned anisotropic 3D Gaussian, and the sum of these shape elements
    represents the whole 3D shape. The parameters of Gaussians are learned by the
    CNN. [[30](#bib.bib30)] improved the SIF and proposed Deep Structured Implicit
    Functions (DSIFs) which added deep neural networks as Deep Implicit Functions
    (DIFs) to provide local geometry details. To summarize, DSIF exploits SIF to depict
    coarse information of each shape element, and applies DIF for local shape details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 函数集。占据函数和带符号距离函数通过深度神经网络学习的单个函数来表示3D形状表面。Genova等人[[31](#bib.bib31), [30](#bib.bib30)]提出通过组合一组形状元素来表示整个3D形状。在[[31](#bib.bib31)]中，研究人员提出了结构化隐式函数（SIFs），其中每个元素由缩放的轴对齐各向异性3D高斯表示，这些形状元素的总和表示整个3D形状。高斯的参数由CNN学习。[[30](#bib.bib30)]改进了SIF，并提出了深度结构化隐式函数（DSIFs），它添加了深度神经网络作为深度隐式函数（DIFs）以提供局部几何细节。总之，DSIF利用SIF描绘每个形状元素的粗略信息，并应用DIF来获取局部形状细节。
- en: Approach without 3D supervision. The above implicit representation models need
    to sample 3D points in the 3D shape bounding box as ground truth and train the
    model supervised with 3D information. But 3D ground truth may not be easy to access
    in some situations. Liu et al. [[56](#bib.bib56)] proposed a framework which learns
    implicit representations without explicit 3D supervision. The model uses a field
    probing algorithm to bridge the gap between the 3D shape and 2D images, and designs
    a silhouette loss to constrain 3D shape outline and geometry regularization to
    constrain the surface to be plausible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 无需3D监督的方法。上述隐式表示模型需要在3D形状边界框内采样3D点作为真实数据，并用3D信息对模型进行监督训练。但在某些情况下，获取3D真实数据可能并不容易。刘等人[[56](#bib.bib56)]提出了一种框架，可以在没有显式3D监督的情况下学习隐式表示。该模型使用场探测算法弥合3D形状和2D图像之间的差距，并设计了轮廓损失以约束3D形状的轮廓，同时设计了几何正则化以约束表面保持合理。
- en: 6 Structure-based representations
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基于结构的表示
- en: Recently, more and more researchers began to realize the importance of structure
    of 3D shapes and integrate structural information into deep learning models. Primitive
    representations are a typical type of structure-based representation which depict
    3D shape structure well. A primitive representation represents the 3D shape with
    primitives such as oriented 3D boxes. Instead of providing a description of geometry
    details, the primitive representation concentrates more on the overall structure
    of 3D shapes. It represents 3D shape structure as several primitives with a compact
    parameter set. More importantly, obtaining a primitive representation encourages
    to generate more detailed and plausible 3D shapes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的研究人员开始认识到3D形状结构的重要性，并将结构信息整合到深度学习模型中。原始表示是基于结构的表示的一种典型类型，它很好地描述了3D形状的结构。原始表示通过如定向3D盒子等原始元素来表示3D形状。原始表示不提供几何细节的描述，而是更关注3D形状的整体结构。它将3D形状结构表示为具有紧凑参数集的几个原始元素。更重要的是，获得原始表示有助于生成更详细和合理的3D形状。
- en: Linearly Organized. Observing that humans often regard 3D shapes as a collection
    of parts, Zou et al. [[121](#bib.bib121)] proposed 3D-PRNN, which applies LSTM
    in a primitive generator, so that 3D-PRNN can generate primitives sequentially.
    The generated primitive representations show great efficiency in depicting simple
    and regular 3D shapes. Wu et al. [[111](#bib.bib111)] further proposed an RCNN-based
    method called PQ-NET which also regards 3D shape parts as a sequence. The difference
    is that PQ-NET encodes geometry features in the network. Gao et al. [[27](#bib.bib27)]
    proposed a deep generative model named SDM-NET (Structured Deformable Mesh-Net).
    They designed a two-level VAE, containing a PartVAE for part geometry and a SP-VAE
    (Structured Parts VAE) for both structure and geometry features. In [[27](#bib.bib27)],
    each shape part is encoded in a well designed form, which records both the structure
    information (symmetry, supporting and supported) and geometry features.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组织。观察到人类常常将 3D 形状视为部件的集合，Zou 等人 [[121](#bib.bib121)] 提出了 3D-PRNN，它在原始生成器中应用
    LSTM，从而使 3D-PRNN 能够顺序生成原始体素。生成的原始体素表示在描绘简单和规则的 3D 形状时表现出极大的效率。Wu 等人 [[111](#bib.bib111)]
    进一步提出了一种基于 RCNN 的方法，称为 PQ-NET，也将 3D 形状部分视为一个序列。不同之处在于 PQ-NET 在网络中编码几何特征。Gao 等人
    [[27](#bib.bib27)] 提出了一个深度生成模型，名为 SDM-NET（结构可变形网）。他们设计了一个两级 VAE，包括用于部分几何的 PartVAE
    和用于结构与几何特征的 SP-VAE（结构部分 VAE）。在 [[27](#bib.bib27)] 中，每个形状部分都以良好设计的形式进行编码，记录了结构信息（对称、支持和被支持）和几何特征。
- en: 'Hierarchically Organized. Li et al. [[51](#bib.bib51)] proposed GRASS (Generative
    Recursive Autoencoders for Shape Structures), which is one of the first attempts
    to encode the 3D shape structure by a neural network. They describe the shape
    structure by a hierarchical binary tree, in which the child nodes are merged into
    the parent node by either adjacency or symmetry relations. Leaves in this structure
    tree represent the oriented bounding boxes (OBBs) and geometry features for each
    part, and intermediate nodes represent both the geometry feature of child nodes
    and the relations between child nodes. Inspired by recursive neural networks (RvNNs) [[90](#bib.bib90),
    [89](#bib.bib89)], GRASS also recursively merges the codes representing the OBBs
    into a root code which depicts the whole shape structure. The architecture of
    GRASS can be divided into three parts: (1) an RvNN autoencoder for encoding a
    3D shape into a fixed length code, (2) a GAN for learning the distribution of
    root codes and generating plausible structures, (3) another autoencoder for synthesizing
    geometry of each part which is inspired by [[32](#bib.bib32)]. Furthermore, to
    synthesize fine-grained geometry in voxel grids, Structure-aware recursive feature
    (SARF) is proposed, which contains both the geometry features of each part and
    global and local OBB layout.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 层次组织。Li 等人 [[51](#bib.bib51)] 提出了 GRASS（生成递归自编码器用于形状结构），这是通过神经网络对 3D 形状结构进行编码的首批尝试之一。他们通过层次二叉树描述形状结构，其中子节点通过邻接或对称关系合并到父节点。这个结构树中的叶子节点表示每个部分的定向包围盒（OBBs）和几何特征，中间节点表示子节点的几何特征以及子节点之间的关系。受到递归神经网络（RvNNs）
    [[90](#bib.bib90), [89](#bib.bib89)] 的启发，GRASS 也递归地将表示 OBB 的代码合并成一个根代码，描述整个形状结构。GRASS
    的架构可以分为三部分：（1）一个 RvNN 自编码器用于将 3D 形状编码为固定长度的代码，（2）一个 GAN 用于学习根代码的分布并生成合理的结构，（3）另一个自编码器用于合成每个部分的几何形状，这受到
    [[32](#bib.bib32)] 的启发。此外，为了在体素网格中合成细粒度几何，提出了结构感知递归特征（SARF），它包含每个部分的几何特征以及全局和局部
    OBB 布局。
- en: However, the GRASS [[51](#bib.bib51)] uses a binary tree to organize the part
    structure, which leads to ambiguity. Therefore, binary trees are not suitable
    for large scale datasets. To address the problem, Mo et al. [[68](#bib.bib68)]
    proposed StructureNet which organized the hierarchical structure in the form of
    graphs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GRASS [[51](#bib.bib51)] 使用二叉树来组织部分结构，这导致了歧义。因此，二叉树不适合大规模数据集。为了解决这个问题，Mo
    等人 [[68](#bib.bib68)] 提出了 StructureNet，它以图的形式组织层级结构。
- en: The BSP-Net (Binary Space Partitioning-Net) proposed by Chen et al. [[13](#bib.bib13)]
    is the first method to depict sharp geometry features, which constructs a 3D shape
    by convexes organized by a BSP-tree. The Binary Space Partitioning (BSP) tree
    defined in [[13](#bib.bib13)] is used to represent 3D shapes by collections of
    convexes, which includes three layers, namely hyperplane extraction, hyerplane
    grouping and shape assembly. The convexes can also be seen as a new form of primitives
    which can represent geometry details of 3D shapes rather than general structures.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Structure and Geometry. Researchers try to encode the 3D shape structure and
    geometry features separately [[51](#bib.bib51)] or jointly [[113](#bib.bib113)].
    Wang et al. [[103](#bib.bib103)] proposed Global-to-Local (G2L) generative model
    to generate man-made 3D shapes from coarse to fine. To address the problem that
    GANs cannot generate geometry details well [[110](#bib.bib110)], G2L first applies
    a GAN to generate coarse voxel grids with semantic labels that represent shape
    structure at the global level, and then puts the voxels separated by semantic
    labels into an autoencoder called Part Refiner (PR) to optimize part geometry
    details part by part at the local level. Wu et al. [[113](#bib.bib113)] proposed
    SAGNet for detailed 3D shape generation, which encodes the structure and geometry
    jointly by a GRU [[15](#bib.bib15)] architecture in order to find intra-relation
    between them. The SAGNet shows better performance in tenon-mortise joints than
    other structure-based learning methods.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 7 Deformation-based representations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deformable 3D models play an important role in computer animation. However,
    most of the methods mentioned above mainly focus on rigid 3D models, while paying
    less attention to the deformation of non-rigid models. Compared with other representations,
    deformation-based representations parameterize the deformation information and
    have better performance when used to cope with non-rigid 3D shapes, such as articulated
    models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Mesh-based Deformation Description. A mesh can be seen as a graph, which is
    convenient when manipulating the vertex positions while maintaining the connectivity
    between vertices. Therefore, a great number of methods choose meshes to represent
    deformable 3D shapes. Based on this property, some mesh-based generation methods
    generate target shapes by deforming a mesh template [[104](#bib.bib104), [109](#bib.bib109),
    [73](#bib.bib73), [27](#bib.bib27)], and these methods can also be regarded as
    deformation-based methods. The graph structure makes it easy to store deformation
    information as vertices features, which can be seen as deformation representations.
    Gao et al. [[24](#bib.bib24)] designed an efficient and rotation-invariant deformation
    representation called Rotation-Invariant Mesh Difference (RIMD), which achieves
    high performance in shape reconstruction, deformation and registration. Based
    on  [[24](#bib.bib24)], Tan et al. [[94](#bib.bib94)] proposed Mesh VAE for deformable
    shape analysis and synthesis, which takes RIMD as the feature inputs of VAE and
    uses fully connected layers for the encoder and decoder. Further, Gao et al. [[25](#bib.bib25)]
    designed an as-consistent-as-possible (ACAP) representation to constrain the rotation
    angle and rotation axes between adjacent vertices in the deformable mesh which
    the graph convolution is easily applied. Tan et al. [[95](#bib.bib95)] proposed
    the SparseAE based on the ACAP representation [[25](#bib.bib25)], which applies
    graph convolutional operators [[19](#bib.bib19)] with the ACAP [[25](#bib.bib25)]
    to analysis the mesh deformations. Gao et al. [[26](#bib.bib26)] proposed VC-GAN
    (VAE CycleGAN) for unpaired mesh deformation transfer, which is the first automatic
    work for mesh deformation transfer. This work takes the ACAP representation as
    input, and encodes the representation into latent space by a VAE, and then transfer
    deformations between source and target in the latent space domain with the cycle
    consistency and visual similarity consistency. Gao et al.  [[27](#bib.bib27)]
    firstly view the geometric details shown in Fig [5](#S7.F5 "Figure 5 ‣ 7 Deformation-based
    representations") as the deformations. Based on the previous techniques [[25](#bib.bib25),
    [26](#bib.bib26), [94](#bib.bib94), [95](#bib.bib95)], the geometric details could
    be encoded and generated. The structure in  [[27](#bib.bib27)] is also analyzed
    in the stable supportable manner [[44](#bib.bib44)]. Yuan et al.[[120](#bib.bib120)]
    apply newly designed pooling operation based on mesh simplification and graph
    convolution to VAE architecture, which also takes ACAP representation as input
    of network. Tan et al. [[96](#bib.bib96)] use ACAP representation for simulating
    thin-shell deformable materials, which apply graph-based CNN to embed high-dimensional
    features into low-dimensional features. In addition of considering a single deformable
    mesh, mesh sequences play a more important role in computer animation. And the
    deformation-based representation ACAP [[25](#bib.bib25)] is suitable for representing
    mesh sequence. Qiao et al.[[79](#bib.bib79)] also takes ACAP representation as
    input to generate mesh animation sequences by a bidirectional LSTM network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75aa016fc81f41b577401df96820dd51.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The research works on deformation-based shape representation of the
    geometrylearning group in ICT, CAS'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4d751d1d30ba7f159441944dce29ab8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example of representing a chair leg by deforming bounding box
    in SDM-NET. (a)a chair with one of its leg parts highlighted, (b)the highlighted
    part in (a) and the overlaid bounding box, (c)the bounding box used as the template,
    (d)deformed bounding box, (e)recovered shape. Ref.[[27](#bib.bib27)] ©ACM 2019'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Implicit surface based approaches. With the development of implicit surface
    representations, Jeruzalski et al. [[45](#bib.bib45)] proposed a method to represent
    articulated deformable shapes by pose parameters, called Neural Articulated Shape
    Approximation (NASA). The pose parameters mentioned in [[45](#bib.bib45)] record
    the transformation of bones defined in models. They compared three different network
    architectures, including unstructured model (U), piecewise rigid model (R) and
    piecewise deformable model (D) in the training dataset and test dataset, which
    opens another direction to represent deformable 3D shapes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 8 Datasets
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the development of 3D scanners, 3D models become easier to obtain, so there
    are more and more 3D shape datasets that have been proposed with different 3D
    representations. The larger datasets with more details bring more challenges for
    existing techniques, which further promotes the development of deep learning on
    different 3D representations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The datasets can be divided into several types in different representations
    and different applications. Choosing the appropriate dataset benefits the performance
    and generalization for learning based models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: RGB-D Images. RGB-D image datasets can be collected by depth sensors like Microsoft
    Kinect. Most of the RGB-D image datasets can be regarded as a sequence of video.
    The indoor scene RGB-D image dataset NYU Depth [[85](#bib.bib85), [86](#bib.bib86)]
    was first provided for the segmentation problem, and the v1 version [[85](#bib.bib85)]
    collects 64 categories while the v2 version [[86](#bib.bib86)] collects 464 categories.
    The KITTI [[29](#bib.bib29)] dataset provides outdoor scene images mainly for
    autonomous driving, which contains 5 categories including ‘Road’, ‘City’, ‘Residential’,
    ‘Campus’ and ‘Person’. The depth map of images can be calculated by the development
    kit provided by the KITTI dataset. And the KITTI dataset also contains 3D objects
    annotations for applications such as object detection. ScanNet [[17](#bib.bib17)]
    is a large annotated RGB-D video dataset, which includes 2.5M views in 1,513 scenes
    with 3D camera pose, surface reconstructions and semantic segmentations. Another
    dataset Human10 [[11](#bib.bib11)] is sampled from 10 human action sequences.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Man-made 3D Object Datasets. The ModelNet [[112](#bib.bib112)] is one of the
    famous CAD model datasets for 3D shape analysis, including 127,915 3D CAD Models
    in 662 categories. ModelNet provides two subsets named ModelNet10 and ModelNet40
    respectively. ModelNet10 includes 10 categories from the whole dataset, and the
    3D models in ModelNet10 are aligned manually; ModelNet40 includes 40 categories,
    and the 3D models are also aligned. ShapeNet [[12](#bib.bib12)] provides a larger
    scale dataset, containing more than 3 million models in more than 4K categories.
    ShapeNet also contains two smaller subsets: ShapeNetCore and ShapeNetSem. For
    various geometry applications, ShapeNet [[12](#bib.bib12)] provides rich annotations
    for 3D objects in the dataset, including category labels, part labels, symmetry
    information, etc. ObjectNet3D [[114](#bib.bib114)] is a large-scale dataset for
    3D object recognition from 2D images, which includes 201,888 3D objects in 90,127
    images and 44,147 different 3D shapes. The dataset is annotated with 3D pose parameters,
    which align 3D objects with 2D images. SUNCG [[92](#bib.bib92)] includes full
    room 3D models, which is suitable for 3D scene analysis and scene completion tasks.
    The 3D models in SUNCG are represented by dense voxel grids with object annotations.
    The whole dataset includes 49,884 valid floors with 404,058 rooms and 5,697,217
    object instances. PartNet provides a more detailed CAD model dataset with fine-grained,
    hierarchical part annotations, which brings more challenges and resources for
    3D object applications such as semantic segmentation, shape editing and shape
    generation. 3D-Future[[23](#bib.bib23)] provides a large-scale furniture dataset,
    which includes 20,000+ scenes in 5,000+ rooms with 10,000+ 3D instances. Each
    3D shape is of high quality with the best texture information for now.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Non-Rigid Model Datasets. TOSCA[[8](#bib.bib8)] is one of the high-resolution
    3D non-rigid model datasets, which contains 80 objects in 9 categories. The models
    are in the mesh representation, and the objects within the same category have
    the same resolution. FAUST[[5](#bib.bib5)] is a dataset of 3D human body scans
    in 10 different people with a variety of poses and the ground truth correspondences
    are also provided. Because FAUST was proposed for real-world shape registration,
    the scans provided in the dataset are noisy and incomplete, but the corresponding
    ground truth is water-tight and aligned. AMASS [[59](#bib.bib59)] provides a large
    and varied human motion dataset, which gathers previous mocap datasets with a
    consistent framework and parameterization. It contains 344 subjects, 11,265 motions
    and more than 40 hours of recordings.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '| Source | Type | Dataset | Year | Category | Size | Description |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | NYU Depth v1[[85](#bib.bib85)] | 2011 | 64 |
    - | Indoor Scene |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | NYU Depth v2[[86](#bib.bib86)] | 2012 | 464 |
    407024 | Indoor Scene |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | KITTI[[29](#bib.bib29)] | 2013 | 5 | - | Outdoor
    Scene |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | ScanNet[[17](#bib.bib17)] | 2017 | 1513 | 2.5M
    | Indoor Scene video |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | Human10[[11](#bib.bib11)] | 2018 | 10 | 9746
    | Human Action |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ModelNet[[112](#bib.bib112)] | 2015 | 662 | 127915
    | Mesh Representation |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ModelNet10[[112](#bib.bib112)] | 2015 | 10 |
    4899 | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ModelNet40[[112](#bib.bib112)] | 2015 | 40 |
    12311 | - |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ShpaeNet[[12](#bib.bib12)] | 2015 | 4K | 3millions
    | Rich Annotations |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ShapeNetCore[[12](#bib.bib12)] | 2015 | 55 |
    51300 | - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ShapeNetSem[[12](#bib.bib12)] | 2015 | 270 |
    12000 | - |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | Images and 3D Models | ObjectNet3D[[114](#bib.bib114)] | 2016
    | 100 | 44161 | 2D aligned with 3D |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | SUNCG[[92](#bib.bib92)] | 2017 | - | 49884 |
    Full Room Scene |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | PartNet[[69](#bib.bib69)] | 2019 | 24 | 26671
    | 573585 Part Instance |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | 3D-FUTURE[[23](#bib.bib23)] | 2020 | - | 10K
    | Texture Information |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | Non-Rigid Models | TOSCA[[8](#bib.bib8)] | 2008 | 9 | 80 | -
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| Real-world | Non-Rigid Models | FAUST[[5](#bib.bib5)] | 2014 | 10 | 300 |
    Human Bodies |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | Non-Rigid Models | AMASS[[59](#bib.bib59)] | 2019 | 344 | 11265
    | Human Motions |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The Overview of 3D Model Datasets'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 9 Shape Analysis and Reconstruction
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The shape representations mentioned above are fundamental for shape analysis
    and shape reconstruction. In this section, we summarize representative works in
    these two directions respectively and compare the performance of these works.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Shape Analysis
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shape analysis methods usually extract the latent codes from different 3D shape
    representations by different network architectures. The latent codes are then
    used for specific applications like shape classification, shape retrieval, shape
    segmentation, etc. And different representations are usually suitable for different
    applications. We now review the performance of different representations in different
    models and discuss suitable representations for specific applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Shape Classification and Retrieval are the basic problems of shape analysis.
    Both of them rely on the feature vectors extracted from the analysis networks.
    For shape classification, the datasets ModelNet10 and ModelNet40 [[112](#bib.bib112)]
    are widely used and Table [2](#S9.T2 "Table 2 ‣ 9.1 Shape Analysis ‣ 9 Shape Analysis
    and Reconstruction") shows the accuracy of some different methods on ModelNet10
    and ModelNet40\. For shape retrieval, given a 3D shape as a query, the target
    is to find the most similar shape(s) in the dataset to match the query. Retrieval
    methods usually learn to find a compact code to represent the object in a latent
    space, and query the closest object as the result based on Euclidean distance,
    Mahalanobis distance or other distance metrics. Different from the classification
    task, the shape retrieval task has a number of evaluation measures, including
    precision, recall, mAP (mean average precision), etc.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '| Form | Model | Accuracy(%) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| 10 | 40 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Voxel | 3DShapeNet [[112](#bib.bib112)] | 83.54 | 77.32 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Voxel | VoxNet [[63](#bib.bib63)] | 92 | 83 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| Voxel | 3D-GAN [[110](#bib.bib110)] | 91.0 | 83.3 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Voxel | Qi et al. [[76](#bib.bib76)] | - | 86 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Voxel | ORION [[82](#bib.bib82)] | 93.8 | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Point | PointNet [[75](#bib.bib75)] | - | 89.2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Multi-view | MVCNN [[93](#bib.bib93)] | - | 90.1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| Point | Kd-net[[47](#bib.bib47)] | 93.3 | 90.6 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Multi-view | Qi et al. [[76](#bib.bib76)] | - | 91.4 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Point | PointNet++ [[77](#bib.bib77)] | - | 91.9 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| Point | Point2Sequence [[57](#bib.bib57)] | 95.3 | 92.6 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Accuracy of shape classification on ModelNet10 and ModelNet40 datasets.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Shape Segmentation aims to discriminate the part categories of a 3D shape. This
    task plays an important role in understanding 3D shapes. The mean Intersection-over-Union
    (mIOU) is often used as the evaluation metric of shape segmentation. Most researchers
    choose to use the point-based representation for the segmentation task [[47](#bib.bib47),
    [75](#bib.bib75), [77](#bib.bib77), [53](#bib.bib53), [66](#bib.bib66)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Shape Symmetry Detection. Symmetry is important geometry information in 3D shapes,
    and it can be further used in many other applications such as shape alignment,
    registration, completion, etc. Gao et al. [[28](#bib.bib28)] designed the first
    unsupervised deep learning method named PRS-Net (Planar Reflective Symmetry Net)
    to detect planar reflective symmetry of 3D shapes, which designs a new symmetry
    distance loss and a regularization loss. And PRS-Net was proved to be robust in
    noisy and incomplete input and more efficient than traditional methods. As symmetry
    is largely determined by the overall shape, PRS-Net is based on a 3D voxel CNN
    and gains high performance in a low resolution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f2534f99503f704bc3853e50d1b2bb9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The pipeline of PRS-Net Ref. [[28](#bib.bib28)] ©IEEE 2020'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Shape Reconstruction
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning based generative models have been proposed for different representations,
    which is also an important field in geometry learning. The reconstruction applications
    include single-view shape reconstruction, shape generation, shape editing, etc.
    The generation methods can be summarized on the basis of representations. For
    voxel-based representations, learning based models try to predict the occupancy
    probability of each voxel in the grid. For point-based representations, learning
    based models either sample 3D points in the space or fold the 2D grids into target
    3D objects. For mesh-based representations, most of the generation methods choose
    to deform a mesh template into the final mesh. In recent study, more and more
    methods choose to use structured representation and generate coarse-to-fine 3D
    shapes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 10 Summary
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we review a series of deep learning methods based on different
    3D object representations. We first overview different 3D representation learning
    models. And the tendency of the geometry learning can be summarized to be less
    computation and memory demanding, and more detailed and structured. Then, we introduce
    3D datasets which are widely used in the research. These datasets provide rich
    resources and support evaluation for data-driven learning methods. Finally, we
    discuss 3D shape applications based on different 3D representations, including
    shape analysis and shape reconstruction. Different representations are usually
    suitable for different applications. Therefore, it is vitally important to choose
    suitable 3D representations for specific tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: \CvmAck
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: This work was supported by National Natural Science Foundation of China (No.
    61828204 and No. 61872440), Beijing Municipal Natural Science Foundation (No.
    L182016), Youth Innovation Promotion Association CAS, CCF-Tencent Open Fund, Royal
    Society-Newton Advanced Fellowship (No. NAF\R2\192151) and the Royal Society (no.
    IES\R1\180126).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] E. Ahmed, A. Saint, A. E. R. Shabayek, K. Cherenkova, R. Das, G. Gusev,
    D. Aouada, and B. Ottersten. Deep learning advances on different 3D data representations:
    A survey. arXiv preprint arXiv:1808.01462, 1, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances
    in Neural Information Processing Systems, pages 1993–2001, 2016.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] H. Ben-Hamu, H. Maron, I. Kezurer, G. Avineri, and Y. Lipman. Multi-chart
    generative surface modeling. In SIGGRAPH Asia 2018 Technical Papers, page 215\.
    ACM, 2018.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] P. J. Besl and N. D. McKay. Method for registration of 3-d shapes. In Sensor
    fusion IV: control paradigms and data structures, volume 1611, pages 586–606\.
    International Society for Optics and Photonics, 1992.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Bogo, J. Romero, M. Loper, and M. J. Black. FAUST: Dataset and evaluation
    for 3D mesh registration. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 3794–3801, 2014.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and P. Vandergheynst.
    Learning class-specific descriptors for deformable shapes using localized spectral
    convolutional networks. In Computer Graphics Forum, volume 34, pages 13–23\. Wiley
    Online Library, 2015.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Boscaini, J. Masci, E. Rodolà, and M. Bronstein. Learning shape correspondence
    with anisotropic convolutional neural networks. In Advances in Neural Information
    Processing Systems, pages 3189–3197, 2016.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Numerical geometry of
    non-rigid shapes. Springer Science & Business Media, 2008.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric
    deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18–42,
    2017.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally
    connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y.-P. Cao, Z.-N. Liu, Z.-F. Kuang, L. Kobbelt, and S.-M. Hu. Learning
    to reconstruct high-quality 3D shapes with cascaded fully convolutional networks.
    In The European Conference on Computer Vision (ECCV), September 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su, et al. ShapeNet: An information-rich 3D model repository.
    arXiv preprint arXiv:1512.03012, 2015.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Z. Chen, A. Tagliasacchi, and H. Zhang. BSP-Net: Generating compact meshes
    via binary space partitioning. arXiv preprint arXiv:1911.06971, 2019.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Chen and H. Zhang. Learning implicit fields for generative shape modeling.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5939–5948, 2019.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical
    machine translation. arXiv preprint arXiv:1406.1078, 2014.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-R2N2: A unified
    approach for single and multi-view 3D object reconstruction. In European Conference
    on Computer Vision (ECCV), pages 628–644\. Springer, 2016.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner.
    ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5828–5839,
    2017.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural
    networks on graphs with fast localized spectral filtering. In Advances in neural
    information processing systems, pages 3844–3852, 2016.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning
    molecular fingerprints. In Advances in neural information processing systems,
    pages 2224–2232, 2015.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single
    image using a multi-scale deep network. In Advances in neural information processing
    systems, pages 2366–2374, 2014.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3D
    object reconstruction from a single image. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 605–613, 2017.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Fey, J. Eric Lenssen, F. Weichert, and H. Müller. SplineCNN: Fast geometric
    deep learning with continuous b-spline kernels. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 869–877, 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future:
    3d furniture shape with texture.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Gao, Y.-K. Lai, D. Liang, S.-Y. Chen, and S. Xia. Efficient and flexible
    deformation representation for data-driven surface modeling. ACM Transactions
    on Graphics (TOG), 35(5):158, 2016.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] L. Gao, Y.-K. Lai, J. Yang, Z. Ling-Xiao, S. Xia, and L. Kobbelt. Sparse
    data driven mesh deformation. IEEE transactions on visualization and computer
    graphics, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] L. Gao, J. Yang, Y.-L. Qiao, Y.-K. Lai, P. L. Rosin, W. Xu, and S. Xia.
    Automatic unpaired shape deformation transfer. In SIGGRAPH Asia 2018 Technical
    Papers, page 237\. ACM, 2018.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] L. Gao, J. Yang, T. Wu, Y.-J. Yuan, H. Fu, Y.-K. Lai, and H. Zhang. SDM-NET:
    Deep generative network for structured deformable mesh. ACM Transactions on Graphics
    (TOG), 38(6):243, 2019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] L. Gao, L.-X. Zhang, H.-Y. Meng, Y.-H. Ren, Y.-K. Lai, and L. Kobbelt.
    PRS-Net: Planar reflective symmetry detection net for 3D models. IEEE Transactions
    on Visualization and Computer Graphics, 2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics:
    The KITTI dataset. The International Journal of Robotics Research, 32(11):1231–1237,
    2013.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] K. Genova, F. Cole, A. Sud, A. Sarna, and T. Funkhouser. Deep structured
    implicit functions. arXiv preprint arXiv:1912.06126, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] K. Genova, F. Cole, D. Vlasic, A. Sarna, W. T. Freeman, and T. Funkhouser.
    Learning shape templates with structured implicit functions. arXiv preprint arXiv:1904.06447,
    2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] R. Girdhar, D. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable
    and generative vector representation for objects. In European Conference on Computer
    Vision (ECCV), 2016.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural
    information processing systems, pages 2672–2680, 2014.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry. AtlasNet:
    A papier-mâché approach to learning 3D surface generation. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2018.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun. Deep learning
    for 3D point clouds: A survey. arXiv: 1912.12033, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Gupta, P. Arbeláez, R. Girshick, and J. Malik. Aligning 3D models to
    RGB-D images of cluttered scenes. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 4731–4740, 2015.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik. Learning rich features
    from RGB-D images for object detection and segmentation. In European conference
    on computer vision, pages 345–360. Springer, 2014.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for
    3D object reconstruction. In 2017 International Conference on 3D Vision (3DV),
    pages 412–420\. IEEE, 2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, and D. Cohen-Or.
    MeshCNN: a network with an edge. ACM Transactions on Graphics (TOG), 38(4):90,
    2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured
    data. arXiv preprint arXiv:1506.05163, 2015.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for
    deep belief nets. Neural computation, 18(7):1527–1554, 2006.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,
    9(8):1735–1780, 1997.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. Nießner, and L. J. Guibas.
    Texturenet: Consistent local parametrizations for learning from high-resolution
    signals on meshes. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 4440–4449, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S.-S. Huang, H. Fu, L.-Y. Wei, and S.-M. Hu. Support substructures: support-induced
    part-level structural representation. IEEE transactions on visualization and computer
    graphics, 22(8):2024–2036, 2015.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] T. Jeruzalski, B. Deng, M. Norouzi, J. Lewis, G. Hinton, and A. Tagliasacchi.
    NASA: Neural articulated shape approximation. arXiv preprint arXiv:1912.03207,
    2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional
    networks. arXiv preprint arXiv:1609.02907, 2016.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-networks for the
    recognition of 3D point cloud models. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 863–872, 2017.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding
    beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300,
    2015.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications
    in vision. In Proceedings of 2010 IEEE International Symposium on Circuits and
    Systems, pages 253–256\. IEEE, 2010.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas. GRASS:
    Generative recursive autoencoders for shape structures. ACM Transactions on Graphics
    (TOG), 36(4):52, 2017.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-GAN: a point cloud
    upsampling adversarial network. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 7203–7212, 2019.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. PointCNN: Convolution
    on x-transformed points. In Advances in neural information processing systems,
    pages 820–830, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: Field probing
    neural networks for 3D data. In Advances in Neural Information Processing Systems,
    pages 307–315, 2016.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400,
    2013.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Liu, S. Saito, W. Chen, and H. Li. Learning to infer implicit surfaces
    without 3D supervision. In Advances in Neural Information Processing Systems,
    pages 8293–8304, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker. Point2sequence: Learning the
    shape representation of 3D point clouds with an attention-based sequence to sequence
    network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
    pages 8778–8785, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface
    construction algorithm. In ACM siggraph computer graphics, volume 21, pages 163–169.
    ACM, 1987.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass:
    Archive of motion capture as surface shapes. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 5442–5451, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim,
    and Y. Lipman. Convolutional neural networks on surfaces via seamless toric covers.
    ACM Trans. Graph., 36(4):71–1, 2017.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional
    neural networks on Riemannian manifolds. In Proceedings of the IEEE international
    conference on computer vision workshops, pages 37–45, 2015.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. Maturana and S. Scherer. 3D convolutional neural networks for landing
    zone detection from LiDAR. In 2015 IEEE International Conference on Robotics and
    Automation (ICRA), pages 3471–3478\. IEEE, 2015.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] D. Maturana and S. Scherer. VoxNet: A 3D convolutional neural network
    for real-time object recognition. In 2015 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS), pages 922–928\. IEEE, 2015.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] D. Meagher. Geometric modeling using octree encoding. Computer graphics
    and image processing, 19(2):129–147, 1982.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] E. Mehr, A. Jourdan, N. Thome, M. Cord, and V. Guitteny. DiscoNet: Shapes
    learning on disconnected manifolds for 3D editing. In Proceedings of the IEEE
    International Conference on Computer Vision, pages 3474–3483, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha. Vv-net: Voxel vae net with
    group convolutions for point cloud segmentation. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 8500–8508, 2019.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy
    networks: Learning 3D reconstruction in function space. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4460–4470, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. J. Mitra, and L. J. Guibas.
    StructureNet: hierarchical graph networks for 3D shape generation. ACM Transactions
    on Graphics (TOG), 38(6):242, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su.
    PartNet: A large-scale benchmark for fine-grained and hierarchical part-level
    3D object understanding. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 909–918, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein.
    Geometric deep learning on graphs and manifolds using mixture model CNNs. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5115–5124,
    2017.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Nash, Y. Ganin, S. Eslami, and P. W. Battaglia. PolyGen: An autoregressive
    generative model of 3D meshes. arXiv preprint arXiv:2002.10880, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. Pan, S. Liu, Y. Liu, and X. Tong. Convolutional neural networks on
    3D surfaces using parallel frames. arXiv preprint arXiv:1808.04952, 2018.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia. Deep mesh reconstruction
    from single rgb images via topology modification networks. In Proceedings of the
    IEEE International Conference on Computer Vision, pages 9964–9973, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. DeepSDF:
    Learning continuous signed distance functions for shape representation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep learning on point
    sets for 3D classification and segmentation. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 652–660, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas. Volumetric
    and multi-view CNNs for object classification on 3D data. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 5648–5656, 2016.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++: Deep hierarchical
    feature learning on point sets in a metric space. In Advances in neural information
    processing systems, pages 5099–5108, 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y.-L. Qiao, L. Gao, J. Yang, P. L. Rosin, Y.-K. Lai, and X. Chen. LaplacianNet:
    Learning on 3D meshes with laplacian encoding and pooling. arXiv preprint arXiv:1910.14063,
    2019.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y.-L. Qiao, Y.-K. Lai, H. Fu, and L. Gao. Synthesizing mesh deformation
    sequences with bidirectional lstm. IEEE Transactions on Visualization and Computer
    Graphics, 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] G. Riegler, A. Osman Ulusoy, and A. Geiger. OctNet: Learning deep 3D representations
    at high resolutions. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 3577–3586, 2017.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover’s distance as
    a metric for image retrieval. International journal of computer vision, 40(2):99–121,
    2000.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] N. Sedaghat, M. Zolfaghari, E. Amiri, and T. Brox. Orientation-boosted
    voxel nets for 3D object recognition. In British Machine Vision Conference, 2017.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Sharma, O. Grau, and M. Fritz. Vconv-DAE: Deep volumetric shape learning
    without object labels. In European Conference on Computer Vision, pages 236–250.
    Springer, 2016.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] B. Shi, S. Bai, Z. Zhou, and X. Bai. Deeppano: Deep panoramic representation
    for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339–2343, 2015.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] N. Silberman and R. Fergus. Indoor scene segmentation using a structured
    light sensor. In Proceedings of the International Conference on Computer Vision
    - Workshop on 3D Representation and Recognition, 2011.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and
    support inference from RGBD images. In European Conference on Computer Vision,
    pages 746–760. Springer, 2012.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape surfaces using
    geometry images. In European Conference on Computer Vision, pages 223–240. Springer,
    2016.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] A. Sinha, A. Unmesh, Q. Huang, and K. Ramani. SurfNet: Generating 3D shape
    surfaces using deep residual networks. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 6040–6049, 2017.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. Socher, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng. Convolutional-recursive
    deep learning for 3D object classification. In Advances in neural information
    processing systems, pages 656–664, 2012.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes
    and natural language with recursive neural networks. In Proceedings of the 28th
    international conference on machine learning (ICML-11), pages 129–136, 2011.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S. Song and J. Xiao. Deep sliding shapes for amodal 3D object detection
    in RGB-D images. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 808–816, 2016.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic
    scene completion from a single depth image. Proceedings of 30th IEEE Conference
    on Computer Vision and Pattern Recognition, 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-view convolutional
    neural networks for 3D shape recognition. In Proceedings of the IEEE international
    conference on computer vision, pages 945–953, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Q. Tan, L. Gao, Y.-K. Lai, and S. Xia. Variational autoencoders for deforming
    3D mesh models. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5841–5850, 2018.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Q. Tan, L. Gao, Y.-K. Lai, J. Yang, and S. Xia. Mesh-based autoencoders
    for localized deformation component analysis. In Thirty-Second AAAI Conference
    on Artificial Intelligence, 2018.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Q. Tan, Z. Pan, L. Gao, and D. Manocha. Realtime simulation of thin-shell
    deformable materials using cnn-based mesh embedding. IEEE Robotics and Automation
    Letters, 5(2):2325–2332, 2020.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Tang, X. Han, J. Pan, K. Jia, and X. Tong. A skeleton-bridged deep
    learning approach for generating meshes of complex topologies from single RGB
    images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4541–4550, 2019.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks:
    Efficient convolutional architectures for high-resolution 3D outputs. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 2088–2096, 2017.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural
    information processing systems, pages 5998–6008, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Verma, E. Boyer, and J. Verbeek. FeastNet: Feature-steered graph convolutions
    for 3D shape analysis. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 2598–2606, 2018.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting
    and composing robust features with denoising autoencoders. In Proceedings of the
    25th international conference on Machine learning, pages 1096–1103\. ACM, 2008.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
    Stacked denoising autoencoders: Learning useful representations in a deep network
    with a local denoising criterion. Journal of machine learning research, 11(Dec):3371–3408,
    2010.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] H. Wang, N. Schor, R. Hu, H. Huang, D. Cohen-Or, and H. Huang. Global-to-local
    generative model for 3D shapes. In SIGGRAPH Asia 2018 Technical Papers, page 214\.
    ACM, 2018.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2Mesh:
    Generating 3D mesh models from single RGB images. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 52–67, 2018.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong. O-CNN: Octree-based
    convolutional neural networks for 3D shape analysis. ACM Transactions on Graphics
    (TOG), 36(4):72, 2017.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong. Adaptive O-CNN: a patch-based
    deep representation of 3D shapes. In SIGGRAPH Asia 2018 Technical Papers, page
    217\. ACM, 2018.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Wang and J. M. Solomon. Deep closest point: Learning representations
    for point cloud registration. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 3523–3532, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon.
    Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG),
    38(5):1–12, 2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Wen, Y. Zhang, Z. Li, and Y. Fu. Pixel2Mesh++: Multi-view 3D mesh
    generation via deformation. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1042–1051, 2019.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic
    latent space of object shapes via 3D generative-adversarial modeling. In Advances
    in neural information processing systems, pages 82–90, 2016.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] R. Wu, Y. Zhuang, K. Xu, H. Zhang, and B. Chen. PQ-NET: A generative
    part seq2seq network for 3D shapes. arXiv preprint arXiv:1911.10949, 2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3D
    ShapeNets: A deep representation for volumetric shapes. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 1912–1920, 2015.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, and H. Huang. SAGNet:
    structure-aware generative network for 3D-shape modeling. ACM Transactions on
    Graphics (TOG), 38(4):91, 2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas,
    and S. Savarese. ObjectNet3D: A large scale database for 3D object recognition.
    In European Conference on Computer Vision, pages 160–176. Springer, 2016.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] H. Xu, M. Dong, and Z. Zhong. Directionally convolutional networks for
    3D shape segmentation. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 2698–2707, 2017.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann. DISN: Deep implicit
    surface network for high-quality single-view 3D reconstruction. In NeurIPS, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Yang, C. Feng, Y. Shen, and D. Tian. FoldingNet: Point cloud auto-encoder
    via deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 206–215, 2018.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, and O. Sorkine-Hornung. Patch-based
    progressive 3D point set upsampling. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 5958–5967, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-Net: Point cloud
    upsampling network. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 2790–2799, 2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y.-J. Yuan, Y.-K. Lai, J. Yang, H. Fu, and L. Gao. Mesh variational autoencoders
    with edge contraction pooling. arXiv preprint arXiv:1908.02507, 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem. 3D-PRNN: Generating
    shape primitives with recurrent neural networks. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 900–909, 2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \Author
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: YunPengXiao.pdfYun-Peng Xiao received his bachelor’s degree in computer science
    from Nankai University, He is currently a Master Student in the Institute of Computing
    Technology, Chinese Academy of Sciences. His research interests include computer
    graphics and geometric processing.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: YuKunLai.pdfYu-Kun Lai received his bachelor’s degree and PhD degree in computer
    science from Tsinghua University in 2003 and 2008, respectively. He is currently
    a Reader in the School of Computer Science & Informatics, Cardiff University.
    His research interests include computer graphics, geometry processing, image processing
    and computer vision. He is on the editorial boards of *Computer Graphics Forum*
    and *The Visual Computer*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: FangLueZhang.pdfFang-Lue Zhang is currently a Lecturer with Victoria University
    of Wellington, New Zealand. He received the Bachelor’s degree from Zhejiang University,
    Hangzhou, China, in 2009, and the Doctoral degree from Tsinghua University, Beijing,
    China, in 2015\. His research interests include image and video editing, computer
    vision, and computer graphics. He is a member of IEEE and ACM. He received Victoria
    Early-Career Research Excellence Award in 2019.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: ChunpengLi.pdfChunpeng Li was born in 1980\. He received his PhD degree in 2008
    and now is an Associate Professor at the Institute of Computing Technology, Chinese
    Academy of Sciences. His main research interests are virtual reality, human–computer
    interaction, and computer graphics.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: GL1.pdfLin Gao received a bachelor’s degree in mathematics from Sichuan University
    and a PhD degree in computer science from Tsinghua University. He is currently
    an Associate Professor at the Institute of Computing Technology, Chinese Academy
    of Sciences. His research interests include computer graphics and geometric processing.
    He received the Newton Advanced Fellowship award from the Royal Society in 2019.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
