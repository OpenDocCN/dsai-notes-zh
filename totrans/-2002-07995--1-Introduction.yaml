- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:22'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2002.07995] 1 Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07995](https://ar5iv.labs.arxiv.org/html/2002.07995)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \headevenname
  prefs: []
  type: TYPE_NORMAL
- en: Xiao et al.
  prefs: []
  type: TYPE_NORMAL
- en: \MakePageStyle\MakeAbstract
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have now achieved great success on dealing with 2D images using
    deep learning. In recent years, 3D computer vision and Geometry Deep Learning
    gain more and more attention. Many advanced techniques for 3D shapes have been
    proposed for different applications. Unlike 2D images, which can be uniformly
    represented by regular grids of pixels, 3D shapes have various representations,
    such as depth and multi-view images, voxel-based representation, point-based representation,
    mesh-based representation, implicit surface representation, etc. However, the
    performance for different applications largely depends on the representation used,
    and there is no unique representation that works well for all applications. Therefore,
    in this survey, we review recent development in deep learning for 3D geometry
    from a representation perspective, summarizing the advantages and disadvantages
    of different representations in different applications. We also present existing
    datasets in these representations and further discuss future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: \MakeKeywords
  prefs: []
  type: TYPE_NORMAL
- en: 3D representation, geometry learning, neural networks, computer graphics
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent improvements in methods for acquisition and rendering of 3D models haven
    resulted in consolidated repositories containing massive amounts of 3D shapes
    on the Internet. With the increased availability of 3D models, we have been seeing
    an explosion in the demands of processing, generation and visualization of 3D
    models in a variety of disciplines, such as medicine, architecture and entertainment.
    The techniques for matching, identification and manipulation of 3D shapes have
    become fundamental building blocks in modern computer vision and computer graphics
    systems. Due to the complexity and irregularity of 3D shape data, how to effectively
    represent 3D shapes remains a challenging problem. Thus, there have been extensive
    research efforts concentrating on how to deal with and generate 3D shapes based
    on different representations.
  prefs: []
  type: TYPE_NORMAL
- en: In early research on 3D shape representations, 3D objects were normally modeled
    with a global approach, such as constructive solid geometry and deformed superquadrics.
    Those approaches have several drawbacks when utilized for the tasks like recognition
    and retrieval. First, when representing imperfect 3D shapes, including those with
    noise and incompleteness, which are common in practice, such representations may
    impose negative influence on matching performance. Second, the high-dimensionality
    heavily burdens the computation and tends to make models overfit. Hence, more
    sophisticated methods are designed to extract representations of 3D shapes in
    a more concise, yet discriminative and informative form.
  prefs: []
  type: TYPE_NORMAL
- en: Several related surveys have been published [[9](#bib.bib9), [1](#bib.bib1),
    [35](#bib.bib35)], which focus on different aspects of deep learning for 3D geometry.
    Moreover, with rapid development of 3D shape representations and related techniques
    for deep learning, it is essential to further summarize up-to-date research works.
    In this survey, we mainly review deep learning methods on 3D shape representations
    and discuss their advantages and disadvantages considering different application
    scenarios. We now give a brief summary of different 3D shape representation categories.
  prefs: []
  type: TYPE_NORMAL
- en: Depth and multi-view images can be used to represent 3D models in the 2D field.
    The regular structure of images makes them efficient to be processed. Depending
    on whether depth maps are included, 3D shapes can be presented by RGB (color)
    or RGB-D (color and depth) images viewed from different viewpoints. Because of
    the influx of available depth data due to the popularity of 2.5D sensors, such
    as Microsoft Kinect, Intel RealSense, etc., multi-view RGB-D images are widely
    used to represent real-world 3D shapes. The large asset of image-based processing
    models can be leveraged using this representation. But it is inevitable that this
    kind of representation loses some geometry features.
  prefs: []
  type: TYPE_NORMAL
- en: A voxel is a 3D extension of the concept of pixel. Similar with pixels in 2D,
    the voxel-based representation also has a regular structure in the 3D space. The
    architectures of some neural networks which have been demonstrated useful in the
    2D image field [[48](#bib.bib48), [50](#bib.bib50)] can be easily extended to
    the voxel form. Nevertheless, adding one dimension means an exponentially increased
    data size. As the resolution increases, the amount of required memory and computational
    costs increase dramatically, which restricts the representation only to low resolutions
    when representing 3D shapes.
  prefs: []
  type: TYPE_NORMAL
- en: Surface-based representation describes 3D shapes by encoding their surfaces,
    which can also be regarded as 2-manifolds. Point clouds and meshes are both discretized
    forms of 3D shape surfaces. Point clouds use a set of sampled 3D point coordinates
    to represent the surface. It can be easily generated by scanners but difficult
    to process due to their lack of order and connectivity information. Researchers
    use order invariant operators such as the max pooling operator in deep neural
    networks [[75](#bib.bib75), [77](#bib.bib77)] to mitigate the lack of order problem.
    Meshes can depict higher quality 3D shapes with less memory and computational
    cost compared with point clouds and voxels. A mesh contains a vertex set and an
    edge set. Due to its graphical nature, researchers have made attempts to build
    graph-based convolutional neural networks for coping with meshes. Some other methods
    regard meshes as the discretization of 2-manifolds. Moreover, meshes are more
    suitable for 3D shape deformation. One can deform a mesh model by transforming
    vertices while keeping the connectivity at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit surface representation exploits implicit field functions, such as occupancy
    functions [[67](#bib.bib67)] and signed distance functions [[116](#bib.bib116)],
    to describe the surface of 3D shapes. The implicit functions learned by deep neural
    networks define the spatial relationship between points and surfaces. They provide
    a description with infinite resolution of 3D shapes with reasonable memory consumption,
    and are capable of representing shapes with changing topology. Nevertheless, implicit
    representations cannot reflect the geometric features of 3D shapes directly, and
    usually need to be transformed to explicit representations such as meshes. Most
    methods apply iso-surfacing, such as marching cubes [[58](#bib.bib58)], which
    is an expensive operation.
  prefs: []
  type: TYPE_NORMAL
- en: Structured representation. One way to cope with complex 3D shapes is to decompose
    them into structure and geometric details, leading to structured representations.
    Recently, increasingly more methods regard a 3D shape as a collection of parts
    and organize them linearly or hierarchically. The structure of 3D shapes is processed
    by Recurrent Neural Networks (RNNs) [[121](#bib.bib121)], Recursive Neural Networks
    (RvNNs) [[51](#bib.bib51)] or other network architectures. Each part of the shape
    can be processed by unstructured models. The structured representation focuses
    on the relations (such as symmetry, supporting, being supported, etc.) between
    different parts within a 3D shape, which provides better description capability
    than alternative representations.
  prefs: []
  type: TYPE_NORMAL
- en: Deformation-based representation. Unlike rigid man-made 3D shapes such as chairs
    and tables, there are also a large number of non-rigid (e.g. articulated) 3D shapes
    such as human bodies, which also play an important role in computer animation,
    augmented reality, etc. The deformation-based representation is proposed mainly
    for describing the intrinsic deformation properties while ignoring the extrinsic
    transformation properties. Many methods use rotation-invariant local features
    for describing shape deformation to reduce the distortion and keep the geometry
    details at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, deep learning has achieved superior performance in contrast to classical
    methods in many fields, including 3D shape analysis, reconstruction, etc. A variety
    of architectures of deep networks have been designed to process or generate 3D
    shape representations, which we refer to as *geometry learning*. In the following
    sections, we focus more on most recent deep learning based methods for representing
    and processing 3D shapes in different forms. According to how the representation
    is encoded and stored, our survey is organized in the following structure: Section [2](#S2
    "2 Image-based methods") reviews image-based shape representation methods. Sections
    [3](#S3 "3 Voxel-based representations") and [4](#S4 "4 Surface-based representations")
    introduce voxel-based and surface-based representations respectively. Section
    [5](#S5 "5 Implicit representations") further introduces implicit surface representations.
    Sections [6](#S6 "6 Structure-based representations") and [7](#S7 "7 Deformation-based
    representations") review structure-based and deformation-based description methods.
    We then summarize typical datasets in Section [8](#S8 "8 Datasets") and typical
    applications for shape analysis and reconstruction in Section [9](#S9 "9 Shape
    Analysis and Reconstruction"), before concluding the paper in Section [10](#S10
    "10 Summary"). Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction") summarizes the timeline
    of representative deep learning methods based on various 3D shape representations.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S1.F1.pic1" class="ltx_picture ltx_centering" height="322.23" overflow="visible"
    version="1.1" width="729.7"><g transform="translate(0,322.23) matrix(1 0 0 -1
    0 0) translate(4.61,0) translate(0,5)"><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 25.53 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 143.64 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.75 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 379.86 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g></g><g stroke="#FF0000"
    fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 303.97)" fill="#000000" stroke="#000000"><foreignobject width="58.81"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">depth map</foreignobject></g></g><g
    stroke="#FF8000" fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 292.16)" fill="#000000" stroke="#000000"><foreignobject width="98.31"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">multi-view
    images</foreignobject></g></g><g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 52.84 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="109.8" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">voxel representation</foreignobject></g></g><g
    stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 268.71)" fill="#000000" stroke="#000000"><foreignobject width="110.49"
    height="10.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">point representation</foreignobject></g></g><g
    stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 303.97)" fill="#000000" stroke="#000000"><foreignobject width="109.52"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">mesh representation</foreignobject></g></g><g
    stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 292.16)" fill="#000000" stroke="#000000"><foreignobject width="83.47"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">implicit
    surface</foreignobject></g></g><g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt"
    color="#BF8040"><g transform="matrix(1.0 0.0 0.0 1.0 214.26 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="137.96" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">structured representation</foreignobject></g></g><g
    stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 268.54)" fill="#000000" stroke="#000000"><foreignobject width="147.19"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">deformation
    representation</foreignobject></g></g> <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 0 118.58)" fill="#00FFFF"
    stroke="#00FFFF"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3D ShapeNets[[112](#bib.bib112)] <g stroke="#FF8000"
    fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><path d="M 66.93 157.48
    L 66.93 181.1" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 27.56 193.62)"
    fill="#FF8000" stroke="#FF8000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MVCNN[[93](#bib.bib93)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 66.93 200.79 L 66.93 224.41" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 27.56 236.92)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GCNN[[61](#bib.bib61)]</foreignobject></g></path></g>
    <g stroke="#FF0000" fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><path
    d="M 106.3 157.48 L 106.3 118.11" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 47.24 102.83)" fill="#FF0000" stroke="#FF0000"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Eigen et
    al.[[20](#bib.bib20)]</foreignobject></g></path></g> <g stroke="#000000" fill="#000000"
    stroke-width="1.42264pt" color="#000000"><path d="M 165.35 157.48 L 165.35 196.85"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 145.67 209.37)" fill="#000000"
    stroke="#000000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">RIMD[[24](#bib.bib24)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 188.98 157.48 L 188.98 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 149.61 118.58)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3D-R2N2[[16](#bib.bib16)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 220.47 157.48 L 220.47 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 188.98 193.62)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3D-GAN[[110](#bib.bib110)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 295.28 157.48 L 295.28 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 251.97 118.58)" fill="#00FF00" stroke="#00FF00"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointNet[[75](#bib.bib75)]
    PointOutNet[[21](#bib.bib21)]</foreignobject></g></path></g> <g stroke="#00FFFF"
    fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path d="M 295.28 102.36
    L 295.28 78.74" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 263.78
    63.46)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="118.11" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OctNet[[80](#bib.bib80)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 295.28 157.48 L 295.28 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 275.59 193.62)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O-CNN[[105](#bib.bib105)]</foreignobject></g></path></g>
    <g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt" color="#BF8040"><path
    d="M 295.28 204.72 L 295.28 228.35" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 275.59 240.86)" fill="#BF8040" stroke="#BF8040"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GRASS[[51](#bib.bib51)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 362.2 157.48 L 362.2 196.85" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 314.96 209.37)" fill="#00FF00" stroke="#00FF00"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointNet++[[77](#bib.bib77)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 433.07 157.48 L 433.07 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 393.7 118.58)" fill="#0000FF" stroke="#0000FF"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pixel2Mesh[[104](#bib.bib104)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 472.44 157.48 L 472.44 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 425.2 193.62)" fill="#00FF00" stroke="#00FF00"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointCNN[[53](#bib.bib53)]</foreignobject></g></path></g>
    <g stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><path
    d="M 519.68 157.48 L 519.68 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 480.31 118.58)" fill="#D9668C" stroke="#D9668C"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DeepSDF [[74](#bib.bib74)]
    IM-NET[[14](#bib.bib14)]</foreignobject></g></path></g> <g stroke="#BF8040" fill="#BF8040"
    stroke-width="1.42264pt" color="#BF8040"><path d="M 551.18 157.48 L 551.18 181.1"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 511.81 193.62)" fill="#BF8040"
    stroke="#BF8040"><foreignobject width="78.74" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SDM-NET[[27](#bib.bib27)] StructureNet[[68](#bib.bib68)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 551.18 216.54 L 551.18 240.16" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 511.81 248.74)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MeshCNN[[39](#bib.bib39)]</foreignobject></g></path></g>
    <g stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><path
    d="M 590.55 157.48 L 590.55 118.11" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 551.18 102.83)" fill="#D9668C" stroke="#D9668C"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Without 3D
    Supervision [[56](#bib.bib56)]</foreignobject></g></path></g> <g stroke="#BF8040"
    fill="#BF8040" stroke-width="1.42264pt" color="#BF8040"><path d="M 637.8 157.48
    L 637.8 133.86" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 602.36
    118.58)" fill="#BF8040" stroke="#BF8040"><foreignobject width="118.11" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">BSP-Net[[13](#bib.bib13)]</foreignobject></g></path></g>
    <g stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><path
    d="M 637.8 118.11 L 637.8 86.61" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 622.05 71.33)" fill="#333333" stroke="#333333"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NASA[[45](#bib.bib45)]</foreignobject></g></path></g><g
    stroke="#B3B3B3" fill="#B3B3B3" stroke-width="2.27621pt" color="#B3B3B3"><path
    d="M 7.87 157.48 L 657.3 157.48" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 657.3 157.48)"><path d="M 11.99 0 C 8.44 0.67 2.66 2.66 -1.33 5 L -1.33
    -5 C 2.66 -2.66 8.44 -0.67 11.99 0" style="stroke:none"></path></g></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The timeline of deep learning based methods for various 3D shape
    representations.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Image-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2D images are the projections of 3D entities. Although the geometric information
    carried by one image is incomplete, a plausible 3D shape could be inferred from
    a set of images with different perspectives. The extra channel of depth in RGB-D
    data further enhances the capacity of image-based representations on encoding
    geometric cues. Benefiting from its image-like structure, the research using deep
    neural networks on 3D shape inferences from images started earlier than alternative
    representations that can depict the surface or geometry of 3D shapes explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Socher et al. [[89](#bib.bib89)] proposed a convolutional and recursive neural
    network for 3D object recognition, which copes with RGB and depth images by single
    convolutional layers separately and merges the features by a recursive network.
    Eigen et al. [[20](#bib.bib20)] first proposed to reconstruct the depth map from
    a single RGB image and designed a new scale invariant loss for the training stage.
    Gupta et al. [[37](#bib.bib37)] encoded the depth map into three channels including
    disparity, height and angle. Other deep learning methods based on RGB-D images
    are designed for 3D object detection [[36](#bib.bib36), [91](#bib.bib91)], outperforming
    previous methods.
  prefs: []
  type: TYPE_NORMAL
- en: Images from different viewpoints can provide complementary cues to infer 3D
    objects. Thanks to the development of deep learning models in 2D fields, the learning
    methods based on multi-view image representation perform better in the 3D shape
    recognition application than those based on other 3D representations. Su et al. [[93](#bib.bib93)]
    proposed MVCNN (Multi-View Convolutional Neural Network) for 3D object recognition.
    MVCNN first processes the images in different views separately by the first part
    of CNN, then aggregates the features extracted from different views by view-pooling
    layers, and finally puts the merged feature to the remaining part of CNN. Qi et
    al. [[76](#bib.bib76)] propose to add multi-resolution into MVCNN for higher classification
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Voxel-based representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Dense Voxel Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The voxel-based representation is traditionally a dense representation, which
    describes 3D shape data by volumetric grids in 3D space. Each voxel in the grid
    records the status of occupancy (e.g., occupied or unoccupied) within a cuboid
    grid.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the earliest methods that applies deep neural networks to volumetric
    representations was proposed by Wu et al. [[112](#bib.bib112)] in 2015, which
    is called 3D ShapeNets. Wu et al. assigned three different states to the voxels
    in the volumetric representation produced by 2.5D depth maps: observed, unobserved
    and free. 3D ShapeNets extended the deep belief network (DBN) [[41](#bib.bib41)]
    from pixel data to voxel data and replaced fully connected layers in DBN with
    convolutional layers. The model takes the aforementioned volumetric representation
    as input, and outputs category labels and predicted 3D shape by iterative computations.
    Concurrently, Maturana et al. proposed to process the volumetric representation
    with 3D Convolutional Neural Networks (3D CNNs) [[62](#bib.bib62)] and designed
    VoxNet [[63](#bib.bib63)] for object recognition. VoxNet defines several volumetric
    layers, including Input Layer, Convolutional Layers, Pooling Layers and Fully
    Connected Layers. Although these defined layers are simple extensions of traditional
    2D CNNs [[48](#bib.bib48)] to 3D, VoxNet is easy to implement and train and gets
    promising performance as the first attempt on volumetric convolutions. In addition,
    to ensure that VoxNet is invariant to orientation, Maturana et al. further augment
    the input data by rotating each shape into $n$ instances with different orientations
    in the training stage and adding a pooling operation after the output layer to
    group all the predictions from the $n$ instances in the test stage.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the development of deep belief networks and convolutional neural
    networks in shape analysis based on volumetric representation, two most successful
    generative models, namely auto-encoders and Generative Adversarial Networks (GANs) [[33](#bib.bib33)]
    are also extended to support this representation. Inspired by Denoising Auto-Encoders
    (DAEs) [[101](#bib.bib101), [102](#bib.bib102)], Sharma et al. proposed an autoencoder
    model VConv-DAE for coping with voxels [[83](#bib.bib83)]. It is one of the earliest
    unsupervised learning approaches in voxel-based shape analysis to our knowledge.
    Without object labels for training, VConv-DAE chooses mean square loss or cross
    entropy loss as the reconstruction loss function. Girdhar et al. [[32](#bib.bib32)]
    also proposed TL-embedding Network, which combine an auto-encoder for generating
    a voxel-based representation with a convolutional neural network for predicting
    the embeddings from the 2D images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choy et al. [[16](#bib.bib16)] proposed 3D-R2N2 which takes single or multiple
    images as input and reconstructs objects in occupancy grids. 3D-R2N2 regards input
    images as a sequence and designs the 3D recurrent neural network based on LSTM
    (Long Short-Term Memory) [[42](#bib.bib42)] or GRU (Gated Recurrent Unit) [[15](#bib.bib15)].
    The architecture consists of three parts: an image encoder to extract features
    from 2D images, 3D-LSTM to predict hidden states as coarse representations of
    final 3D models, and a decoder to increase the resolution and generate target
    shapes.'
  prefs: []
  type: TYPE_NORMAL
- en: Wu et al. [[110](#bib.bib110)] designed a generative model called 3D-GAN that
    applies the Generative Adversarial Network (GAN) [[33](#bib.bib33)] in voxel data.
    3D GAN learns to synthesize a 3D object from a sampled latent space vector $z$
    with the probability distribution $P(z)$. Moreover, [[110](#bib.bib110)] also
    proposed 3D-VAE-GAN inspired by VAE-GAN [[49](#bib.bib49)] for the object reconstruction
    task. 3D-VAE-GAN puts the encoder before 3D-GAN for inferring the latent vector
    $z$ from input 2D images and shares the decoder with the generator of 3D-GAN.
  prefs: []
  type: TYPE_NORMAL
- en: After the early attempts in dealing with volumetric representations by deep
    learning, researchers began to optimize the architecture of volumetric networks
    for better performance and more applications. A motivation is that the naive extension
    from traditional 2D domain networks often does not perform better than image-based
    CNNs such as MVCNN [[93](#bib.bib93)]. The main challenges affecting the performance
    include overfitting, orientation, data sparsity and low resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Qi et al. [[76](#bib.bib76)] proposed two new network structures aiming to improve
    the performance of volumetric CNNs. One introduces an extra task namely predicting
    class labels with subvolume space to prevent overfitting, and another utilizes
    elongated kernels to compress the 3D information into the 2D field in order to
    use 2D CNNs directly. Both of them use mlpconv layers [[55](#bib.bib55)] to replace
    traditional convolutional layers. [[76](#bib.bib76)] also augments the input data
    in different orientation and elevation to encourage the network to get more local
    features in different poses so that the results are less influenced by orientation
    changes. To further mitigate the orientation impact on recognition accuracy, instead
    of using data augmentation like [[63](#bib.bib63), [76](#bib.bib76)], [[82](#bib.bib82)]
    proposed a new model called ORION which extends VoxNet [[63](#bib.bib63)] and
    uses a fully connected layer to predict the object class label and orientation
    label simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Sparse Voxel Representation (Octree)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Voxel-based representations often lead to high computational cost because of
    the exponential increase of computations from pixels to voxels. Most of the methods
    cannot cope with or generate high-resolution models within reasonable time. For
    instance, TL-embedding Network [[32](#bib.bib32)] was designed for $20^{3}$ voxel
    grids; 3DShapeNets [[112](#bib.bib112)] and VConv-DAE [[83](#bib.bib83)] were
    designed for $24^{3}$ voxel grids with 3 voxels padding on each direction of the
    voxel grids; VoxNet [[63](#bib.bib63)], 3D-R2N2 [[16](#bib.bib16)] and ORION [[82](#bib.bib82)]
    were designed for $32^{3}$ voxel grids; 3D-GAN was designed for generating $64^{3}$
    occupancy grids as 3D shape representation. As the voxel resolution increases,
    the occupied grids become sparser in the whole 3D space, which leads to more unnecessary
    computation. To address this problem, Li et al. [[54](#bib.bib54)] designed a
    novel method called FPNN to cope with the data sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods instead encode the voxel grids by a sparse, adaptive data structure,
    namely octree [[64](#bib.bib64)] to reduce the dimensionality of the input data.
    Häne et al. [[38](#bib.bib38)] proposed Hierarchical Surface Prediction (HSP)
    that can generate voxel grids in the form of octree from coarse to fine. Häne
    et al. observed that only the voxels near the object surface need to be predicted
    in a high resolution, so that the proposed HSP can avoid unnecessary calculation
    to ensure affordable generation of high resolution voxel grids. As introduced
    in [[38](#bib.bib38)], each node in the octree is defined as a voxel block with
    a fixed number ($16^{3}$ in the paper) of voxels in different size, and each voxel
    block is classified into occupied, boundary and free. The decoder of the model
    takes a feature vector as input, and predicts feature blocks that correspond to
    voxel blocks hierarchically. The HSP defines that the octree has 5 layers and
    each voxel blocks contains $16^{3}$ voxels, therefore, HSP can generate up to
    $256^{3}$ voxel grids. Tatarchenko et al. [[98](#bib.bib98)] also proposed a decoder
    called OGN for generating high resolution volumetric representations. In [[98](#bib.bib98)],
    nodes in the octree are separated into three categories, including “empty”, “filled”
    and “mixed”. The octree representing a 3D model and the feature map of the octree
    are stored in the form of hashing tables which are indexed by the spatial position
    and the octree level. In order to process the feature maps represented as hash
    tables, Tatarchenko et al. designed a convolutional layer named OGN-Conv, which
    converts the convolutional operation into matrix multiplication. [[98](#bib.bib98)]
    adopts the method that generates different resolution of voxel grids in each decoder
    layer by convolutional operations in feature maps, and then decides whether to
    propagate the features to the next layer by specific labels (propagating the features
    if “boundary” and skipping the feature propagation if “mixed”).
  prefs: []
  type: TYPE_NORMAL
- en: Besides the decoder model design for synthesizing voxel grids, shape analysis
    methods are also designed using octrees. However, conventional octree structure [[64](#bib.bib64)]
    has difficulty to be used in deep networks, so many researchers try to resolve
    the problem by designing new structures of octrees and special operations such
    as convolution, pooling and unpooling on octrees. Riegler et al. [[80](#bib.bib80)]
    proposed OctNet. The octree representation mentioned in [[80](#bib.bib80)] has
    a relatively regular structure than a traditional octree, which places a shallow
    octree in regular 3D grids. The shallow octree is constrained to have up to 3
    levels and is encoded in 73 bits. Each bit determines if the corresponding cell
    needs to be split. Wang et al. [[105](#bib.bib105)] also proposed a convolutional
    neural network based on octree called O-CNN, where the model also removes pointers
    like shallow octree [[80](#bib.bib80)] and stores the octree data and structure
    by a series of vectors including shuffle key vectors, labels and input signals.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to representing voxels, octree structure can also be utilized to
    represent 3D shape surfaces with planar patches. Wang et al. [[106](#bib.bib106)]
    proposed Adaptive O-CNN, where they defined another form of octree named patch-guided
    adaptive octree, which divides a 3D shape surface into a set of planar patches
    restricted by bounding boxes corresponding to octants. They also provided an encoder
    and a decoder for the octree defined by this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Surface-based representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Point-based Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The typical point-based representation is also referred to as point clouds or
    point sets. They can be raw data generated by 3D scanning devices. Because of
    its unordered and irregular structure, this kind of representation is relatively
    difficult to cope with by traditional deep learning methods. Therefore, most researchers
    avoided to use point clouds in a direct way at the early stage of the deep learning-based
    geometry research. One of the first models to generate point clouds by deep learning
    came out in 2017 [[21](#bib.bib21)]. They designed a neural network to learn a
    point sampler based on 3D shape point distribution. The network takes a single
    image and a random vector as input, and outputs an $N\times 3$ matrix representing
    the predicted point sets ($x$, $y$, $z$ coordinates for $N$ points). In addition,
    [[21](#bib.bib21)] proposed to use Chamfer Distance (CD) and Earth Mover’s Distance
    (EMD) [[81](#bib.bib81)] as the loss function to train the networks.
  prefs: []
  type: TYPE_NORMAL
- en: PointNet. At almost the same time, Qi et al. [[75](#bib.bib75)] proposed PointNet
    for shape analysis, which was the first successful deep network architecture that
    directly processes point clouds without unnecessary rendering. The pipeline of
    PointNet is illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Point-based Representation
    ‣ 4 Surface-based representations"). On account of three properties of point sets
    mentioned in [[75](#bib.bib75)], PointNet designed three components in their network,
    including using max-pooling layers as symmetry functions for dealing with the
    unordered property, concatenating global and local features together for point
    interaction, and jointly aligning the network for transformation invariance. Based
    on PointNet, Qi et al. further improved this model and proposed PointNet++ [[77](#bib.bib77)],
    in order to resolve the problem that PointNet cannot capture and deal with local
    features induced by metric well. Compared with PointNet, PointNet++ introduces
    a hierarchical structure, so that the model can capture features in different
    scales, which improves the capability of extracting 3D shape features. As PointNet
    and PointNet++ show state-of-the-art performance in shape classification and semantic
    segmentation, more and more deep learning models were proposed based on point-based
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f48dfdedb86af55dc200d7232469297d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The pipeline of PointNet Ref. [[75](#bib.bib75)], ©IEEE 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks for Point Clouds. Some research works focus on
    applying CNNs to the irregular and unordered form of point clouds for analysis.
    Li et al. [[53](#bib.bib53)] proposed PointCNN for point clouds and designed the
    $\mathcal{X}$-transformation to weight and permute the input point features, which
    guarantees the equivariance in different point orders. Each feature matrix needs
    to be multiplied by the $\mathcal{X}$-transformation matrix before passing through
    the convolutional operator. This process is named $\mathcal{X}$-Conv operator,
    which is the key of PointCNN. Wang et al. [[108](#bib.bib108)] proposed DGCNN,
    a dynamic graph CNN architecture for point cloud classification and segmentation.
    Instead of processing point features like PointNet [[75](#bib.bib75)], DGCNN first
    connects neighboring points in spatial or semantic space to generate a graph,
    and then captures the local geometry features by applying the EdgeConv operator
    on it. Moreover, different from other graph CNNs which process the fixed input
    graph, DGCNN changes the graph to obtain new nearest neighbors in the feature
    space in different layers, which is beneficial to get larger and sparser receptive
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: Other Point Cloud Processing Techniques using Neural Networks. Klokov et al. [[47](#bib.bib47)]
    proposed Kd-Network to process point clouds based on the form of kd-trees. Yang
    et al. [[117](#bib.bib117)] proposed FoldingNet, an end-to-end auto-encoder for
    further compressing a point-based representation with unsupervised learning. Because
    point clouds can be transformed into 2D grids by folding operations, FoldingNet
    integrates folding operations in their encoder-decoder to recover input 3D shapes.
    Mehr et al. [[65](#bib.bib65)] further proposed DiscoNet for 3D model editing
    by combining multiple autoencoders which are trained for different types of 3D
    shapes specifically. The autoencoders use pre-learned mean geometry of training
    3D shapes as their templates. Meng et al. [[66](#bib.bib66)] proposed VV-Net (Voxel
    VAE Net) for point segmentation, which represents a point cloud by a structured
    voxel representation. In VV-Net, instead of containing a boolean value to represent
    occupancy status of each voxel as a normal volumetric representation, it uses
    a latent code computed by an RBF-VAE, a variational autoencoder based on a radial
    basis function (RBF) interpolation of points to describe point distribution within
    a voxel. This representation is used to extract intrinsic symmetry of point clouds
    by a group equivariant CNN, and the output is combined with PointNet [[75](#bib.bib75)]
    for better segmentation performance.
  prefs: []
  type: TYPE_NORMAL
- en: Although the point-based representation can be more easily obtained by 3D scanners
    than other 3D representations, this raw form of 3D shapes is often unsuitable
    for 3D shape analysis, due to noise and data sparsity. Therefore, compared with
    other representations, it is essential for the point-based representation to incorporate
    an upsampling module to obtain fine-grained point clouds, such as PU-NET [[119](#bib.bib119)],
    MPU [[118](#bib.bib118)], PU-GAN [[52](#bib.bib52)], etc. Additionally, point
    cloud registration is also an essential preprocessing step, e.g. to fuse points
    from multiple scans, which aims to calculate rigid transformation parameters to
    align the point clouds. Wang et al. [[107](#bib.bib107)] proposed Deep Closest
    Point (DCP), which extends traditional Iteractive Closest Point (ICP) method [[4](#bib.bib4)]
    and uses a deep learning method to obtain the transformation parameters. Recently,
    Guo et al. [[35](#bib.bib35)] presented a survey focusing on deep learning models
    in point clouds, which provides more details in this field.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Mesh-based Representations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared with point-based representations, mesh-based representations contain
    connectivity between neighboring points, so they are more suitable for describing
    local regions on surfaces. As a typical type of representation in non-Euclidean
    space, mesh-based representations can be processed by deep learning models both
    in spatial and spectral domains [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: Parametric representations for meshes. Directly applying CNNs to irregular data
    structures like meshes is non-trivial, so there emerged a handful of approaches
    that map 3D shape surfaces to 2D domains such as 2D geometry images which can
    also be regarded as another 3D shape representation, and apply traditional 2D
    CNNs on them [[87](#bib.bib87), [60](#bib.bib60)]. Based on geometry images, Sinha
    et al. [[88](#bib.bib88)] proposed SurfNet for shape generation using a deep residual
    network. Similarly, Shi et al. [[84](#bib.bib84)] projected 3D models into cylinder
    panoramic images, which are processed by CNNs. Some other methods convert mesh
    models into spherical signals, and design a convolutional operator in the spherical
    domain for shape analysis. To address high-resolution signals on 3D meshes, in
    particular texture information, Huang et al. [[43](#bib.bib43)] proposed TextureNet
    to extract features in this situation, where a 4-rotational symmetric (4-RoSy)
    field is defined to parametrize surfaces. In the following, we will review deep
    learning models according to how meshes are directly treated as input, and introduce
    generative models working on meshes.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs. The mesh-based representation is constructed by sets of vertices and
    edges, which can be seen as a graph. Some models were proposed based on the graph
    spectral theorem. They generalize CNNs on graphs [[10](#bib.bib10), [40](#bib.bib40),
    [18](#bib.bib18), [46](#bib.bib46), [2](#bib.bib2)] by eigen-decomposition of
    Laplacian matrices, which is able to generalize convolutional operators to the
    spectral domain of graphs. Verma et al. [[100](#bib.bib100)] proposed another
    graph-based CNN named FeaStNet, which computes the receptive fields of convolution
    operator dynamically. Specifically, FeaStNet determines the assignment of the
    neighbor vertices by using features obtained in networks. Hanocka et al. [[39](#bib.bib39)]
    also designed operators of convolution, pooling and unpooling for triangle meshes,
    and proposed MeshCNN. Different from other graph-based methods, MeshCNN focuses
    on processing the features stored in edges, and proposes a convolution operator
    that is applied to the edges with a fixed number of neighbors and a pooling operator
    based on edge collapse. MeshCNN extracts 3D shape features with respect to specific
    tasks, and the network learns to preserve the important features and ignore the
    unimportant ones.
  prefs: []
  type: TYPE_NORMAL
- en: '2-Manifolds. The mesh-based representation can be viewed as the discretization
    of 2-manifolds. Several works are designed in 2-manifolds with a series of refined
    CNN operators to adapt to this non-Euclidean space. These methods define their
    own local patches and kernel functions for generalizing CNN models. Masci et al. [[61](#bib.bib61)]
    proposed Geodesic Convolutional Neural Networks (GCNNs) for manifolds, which extract
    and discretize local geodesic patches and apply convolutional filters on these
    patches in polar coordinates. The convolution operator is designed in the spatial
    domain and their Geodesic CNN is quite similar to conventional CNNs applied in
    Euclidean space. Localized Spectral CNNs [[6](#bib.bib6)] proposed by Boscaini
    et al. apply Windowed Fourier transform to non-Euclidean space. Anisotropic Convolutional
    Neural Networks (ACNNs) [[7](#bib.bib7)] further designed an anisotropic heat
    kernel to replace the isotropic patch operator in GCNN [[61](#bib.bib61)], which
    gives another solution to avoid ambiguity. Xu et al. [[115](#bib.bib115)] proposed
    Directionally Convolutional Networks (DCNs), which defined local patches based
    on faces of the mesh representation. In this work, researchers also designed a
    two-stream network for 3D shape segmentation, which takes local face normals and
    the global face distance histogram as input for training. Moti et al. [[70](#bib.bib70)]
    proposed MoNet to replace the weight functions in [[61](#bib.bib61), [7](#bib.bib7)]
    with Gaussian kernels with learnable parameters. Fey et al. [[22](#bib.bib22)]
    proposed SplineCNN which designed a convolutional operator based on B-splines.
    Pan et al. [[72](#bib.bib72)] designed a surface CNN for 3D irregular surface
    to preserve the standard CNN property of translation equivariance by using parallel
    translation frames and group convolutional operations. Qiao et al. [[78](#bib.bib78)]
    proposed Laplacian Pooling Network (LaplacianNet) for 3D mesh analysis. The LaplacianNet
    considers both spectral and spatial information of the mesh, and contains 3 parts:
    preprocessing features as the network input, Mesh Pooling Blocks to split surface
    and cluster patches for feature extraction, and the Correlation Network to aggregate
    global information.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative Models. There are also many generative models for the mesh-based
    representation. Wang et al. [[104](#bib.bib104)] proposed Pixel2Mesh for reconstructing
    3D shapes from single images, which generates the target triangular mesh by deforming
    an ellipsoid template. As shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Mesh-based
    Representations ‣ 4 Surface-based representations"), the Pixel2Mesh network is
    implemented based on Graph-based Convolutional Networks (GCNs) [[9](#bib.bib9)]
    and generates the target mesh from coarse to fine by an unpooling operation. Wen
    et al. [[109](#bib.bib109)] advanced Pixel2Mesh and proposed Pixel2Mesh++, which
    extends single image 3D shape reconstruction to 3D shape reconstruction from multi-view
    images. To achieve this, Pixel2Mesh++ introduces a Multi-view Deformation Network
    (MDN) to the original Pixel2Mesh, and the MDN incorporates the cross-view information
    into the process of mesh generation. Groueix et al. [[34](#bib.bib34)] proposed
    AtlasNet, which generates 3D surfaces by multiple patches. AtlasNet learns to
    convert 2D square patches into 2-manifolds to cover the surface of 3D shapes by
    MLP (Multi-Layer Perceptron). Ben-Hamu et al. [[3](#bib.bib3)] proposed a multi-chart
    generative model for 3D shape generation. The method uses a multi-chart structure
    as input and builds the network architecture based on standard image GAN [[33](#bib.bib33)].
    The transformation between 3D surface and multi-chart structure is based on  [[60](#bib.bib60)].
    However, the methods based on deforming a template mesh into the target shape
    cannot express complex topology of some 3D shapes. Pan et al. [[73](#bib.bib73)]
    proposed a new single-view reconstruction method, which combines a deformation
    network and a topology modification network to model meshes with complex topology.
    In the topology modification network, the faces with high distortion are removed.
    Tang et al. [[97](#bib.bib97)] proposed to generate complex topology meshes by
    a skeleton-bridged learning method, because skeleton can well preserve topology
    information. Instead of generating triangular meshes, Nash et al. [[71](#bib.bib71)]
    proposed PolyGen to generate the polygon mesh representation. Inspired by neural
    autoregressive models in other fields like natural language processing, researchers
    regard mesh generation as a sequence, and design a transformer-based network [[99](#bib.bib99)],
    including a vertex model and a face model. The vertex model generates a sequence
    of vertex positions and the face model generates variable-length vertex sequences
    conditioned on input vertices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ff955df32f94d5ed156863623aa093f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The pipeline of Pixel2Mesh Ref.[[104](#bib.bib104)] ©Springer 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Implicit representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to explicit representations such as point clouds and meshes, implicit
    fields have been in greater popularity in recent studies. A major reason is that
    the implicit representation is not limited by fixed topology and resolution. There
    are an increasing number of deep models, which define their own implicit representations
    and building on them further propose various methods for shape analysis and generation.
  prefs: []
  type: TYPE_NORMAL
- en: The Occupancy/Indicator Function is one of the forms to represent 3D shapes
    implicitly. Occupancy Network was proposed by Mescheder et al. [[67](#bib.bib67)]
    to learn a continuous occupancy function as a new representation of 3D shapes
    by neural networks. The occupancy function reflects the 3D point status with respect
    to the 3D shape surface, where 1 means inside the surface and 0 otherwise. Researchers
    regarded this problem as a binary classification task and designed an occupancy
    network which inputs 3D point position and 3D shape observation and outputs the
    probability of occupancy. The generated implicit field is then processed by a
    Multi-resolution IsoSurface Extraction method MISE and marching cubes algorithm [[58](#bib.bib58)]
    to obtain meshes. Moreover, researchers introduce encoder networks to obtain latent
    embeddings. Similarly, Chen et al. [[14](#bib.bib14)] designed IM-NET as a decoder
    for learning generative models, which also takes an implicit function in the form
    of an indicator function.
  prefs: []
  type: TYPE_NORMAL
- en: Signed Distance Functions (SDFs) are also a form of implicit representation.
    Signed distance functions map a 3D point to a real value instead of a probability,
    which indicates the spatial relation and distance to the 3D surface. Denote $SDF(x)$
    as the signed distance value of a given 3D point $x\in\mathbb{R}^{3}$. Then $SDF(x)>0$
    if point $x$ is outside the 3D shape surface, $SDF(x)<0$ if point $x$ is inside
    the surface, and $SDF(x)=0$ means point $x$ is on the surface. The absolute value
    of $SDF(x)$ refers to the distance between point $x$ and the surface. Park et
    al. [[74](#bib.bib74)] proposed DeepSDF and introduced an auto-decoder-based DeepSDF
    as a new 3D shape representation. Wang et al. [[116](#bib.bib116)] also proposed
    Deep Implicit Surface Networks (DISNs) for single-view 3D reconstruction based
    on SDFs. Thanks to the advantages of SDF, DISN was the first to reconstruct 3D
    shapes with flexible topology and thin structure in the single-view reconstruction
    task, which is difficult for other 3D representations.
  prefs: []
  type: TYPE_NORMAL
- en: Function Sets. The occupancy functions and signed distance functions represent
    the 3D shape surface by a single function learned by a deep neural network. Genova
    et al. [[31](#bib.bib31), [30](#bib.bib30)] proposed to represent the whole 3D
    shape by combining a set of shape elements. In [[31](#bib.bib31)], researchers
    proposed Structured Implicit Functions (SIFs) where each element is represented
    by a scaled axis-aligned anisotropic 3D Gaussian, and the sum of these shape elements
    represents the whole 3D shape. The parameters of Gaussians are learned by the
    CNN. [[30](#bib.bib30)] improved the SIF and proposed Deep Structured Implicit
    Functions (DSIFs) which added deep neural networks as Deep Implicit Functions
    (DIFs) to provide local geometry details. To summarize, DSIF exploits SIF to depict
    coarse information of each shape element, and applies DIF for local shape details.
  prefs: []
  type: TYPE_NORMAL
- en: Approach without 3D supervision. The above implicit representation models need
    to sample 3D points in the 3D shape bounding box as ground truth and train the
    model supervised with 3D information. But 3D ground truth may not be easy to access
    in some situations. Liu et al. [[56](#bib.bib56)] proposed a framework which learns
    implicit representations without explicit 3D supervision. The model uses a field
    probing algorithm to bridge the gap between the 3D shape and 2D images, and designs
    a silhouette loss to constrain 3D shape outline and geometry regularization to
    constrain the surface to be plausible.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Structure-based representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, more and more researchers began to realize the importance of structure
    of 3D shapes and integrate structural information into deep learning models. Primitive
    representations are a typical type of structure-based representation which depict
    3D shape structure well. A primitive representation represents the 3D shape with
    primitives such as oriented 3D boxes. Instead of providing a description of geometry
    details, the primitive representation concentrates more on the overall structure
    of 3D shapes. It represents 3D shape structure as several primitives with a compact
    parameter set. More importantly, obtaining a primitive representation encourages
    to generate more detailed and plausible 3D shapes.
  prefs: []
  type: TYPE_NORMAL
- en: Linearly Organized. Observing that humans often regard 3D shapes as a collection
    of parts, Zou et al. [[121](#bib.bib121)] proposed 3D-PRNN, which applies LSTM
    in a primitive generator, so that 3D-PRNN can generate primitives sequentially.
    The generated primitive representations show great efficiency in depicting simple
    and regular 3D shapes. Wu et al. [[111](#bib.bib111)] further proposed an RCNN-based
    method called PQ-NET which also regards 3D shape parts as a sequence. The difference
    is that PQ-NET encodes geometry features in the network. Gao et al. [[27](#bib.bib27)]
    proposed a deep generative model named SDM-NET (Structured Deformable Mesh-Net).
    They designed a two-level VAE, containing a PartVAE for part geometry and a SP-VAE
    (Structured Parts VAE) for both structure and geometry features. In [[27](#bib.bib27)],
    each shape part is encoded in a well designed form, which records both the structure
    information (symmetry, supporting and supported) and geometry features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hierarchically Organized. Li et al. [[51](#bib.bib51)] proposed GRASS (Generative
    Recursive Autoencoders for Shape Structures), which is one of the first attempts
    to encode the 3D shape structure by a neural network. They describe the shape
    structure by a hierarchical binary tree, in which the child nodes are merged into
    the parent node by either adjacency or symmetry relations. Leaves in this structure
    tree represent the oriented bounding boxes (OBBs) and geometry features for each
    part, and intermediate nodes represent both the geometry feature of child nodes
    and the relations between child nodes. Inspired by recursive neural networks (RvNNs) [[90](#bib.bib90),
    [89](#bib.bib89)], GRASS also recursively merges the codes representing the OBBs
    into a root code which depicts the whole shape structure. The architecture of
    GRASS can be divided into three parts: (1) an RvNN autoencoder for encoding a
    3D shape into a fixed length code, (2) a GAN for learning the distribution of
    root codes and generating plausible structures, (3) another autoencoder for synthesizing
    geometry of each part which is inspired by [[32](#bib.bib32)]. Furthermore, to
    synthesize fine-grained geometry in voxel grids, Structure-aware recursive feature
    (SARF) is proposed, which contains both the geometry features of each part and
    global and local OBB layout.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the GRASS [[51](#bib.bib51)] uses a binary tree to organize the part
    structure, which leads to ambiguity. Therefore, binary trees are not suitable
    for large scale datasets. To address the problem, Mo et al. [[68](#bib.bib68)]
    proposed StructureNet which organized the hierarchical structure in the form of
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The BSP-Net (Binary Space Partitioning-Net) proposed by Chen et al. [[13](#bib.bib13)]
    is the first method to depict sharp geometry features, which constructs a 3D shape
    by convexes organized by a BSP-tree. The Binary Space Partitioning (BSP) tree
    defined in [[13](#bib.bib13)] is used to represent 3D shapes by collections of
    convexes, which includes three layers, namely hyperplane extraction, hyerplane
    grouping and shape assembly. The convexes can also be seen as a new form of primitives
    which can represent geometry details of 3D shapes rather than general structures.
  prefs: []
  type: TYPE_NORMAL
- en: Structure and Geometry. Researchers try to encode the 3D shape structure and
    geometry features separately [[51](#bib.bib51)] or jointly [[113](#bib.bib113)].
    Wang et al. [[103](#bib.bib103)] proposed Global-to-Local (G2L) generative model
    to generate man-made 3D shapes from coarse to fine. To address the problem that
    GANs cannot generate geometry details well [[110](#bib.bib110)], G2L first applies
    a GAN to generate coarse voxel grids with semantic labels that represent shape
    structure at the global level, and then puts the voxels separated by semantic
    labels into an autoencoder called Part Refiner (PR) to optimize part geometry
    details part by part at the local level. Wu et al. [[113](#bib.bib113)] proposed
    SAGNet for detailed 3D shape generation, which encodes the structure and geometry
    jointly by a GRU [[15](#bib.bib15)] architecture in order to find intra-relation
    between them. The SAGNet shows better performance in tenon-mortise joints than
    other structure-based learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Deformation-based representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deformable 3D models play an important role in computer animation. However,
    most of the methods mentioned above mainly focus on rigid 3D models, while paying
    less attention to the deformation of non-rigid models. Compared with other representations,
    deformation-based representations parameterize the deformation information and
    have better performance when used to cope with non-rigid 3D shapes, such as articulated
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Mesh-based Deformation Description. A mesh can be seen as a graph, which is
    convenient when manipulating the vertex positions while maintaining the connectivity
    between vertices. Therefore, a great number of methods choose meshes to represent
    deformable 3D shapes. Based on this property, some mesh-based generation methods
    generate target shapes by deforming a mesh template [[104](#bib.bib104), [109](#bib.bib109),
    [73](#bib.bib73), [27](#bib.bib27)], and these methods can also be regarded as
    deformation-based methods. The graph structure makes it easy to store deformation
    information as vertices features, which can be seen as deformation representations.
    Gao et al. [[24](#bib.bib24)] designed an efficient and rotation-invariant deformation
    representation called Rotation-Invariant Mesh Difference (RIMD), which achieves
    high performance in shape reconstruction, deformation and registration. Based
    on  [[24](#bib.bib24)], Tan et al. [[94](#bib.bib94)] proposed Mesh VAE for deformable
    shape analysis and synthesis, which takes RIMD as the feature inputs of VAE and
    uses fully connected layers for the encoder and decoder. Further, Gao et al. [[25](#bib.bib25)]
    designed an as-consistent-as-possible (ACAP) representation to constrain the rotation
    angle and rotation axes between adjacent vertices in the deformable mesh which
    the graph convolution is easily applied. Tan et al. [[95](#bib.bib95)] proposed
    the SparseAE based on the ACAP representation [[25](#bib.bib25)], which applies
    graph convolutional operators [[19](#bib.bib19)] with the ACAP [[25](#bib.bib25)]
    to analysis the mesh deformations. Gao et al. [[26](#bib.bib26)] proposed VC-GAN
    (VAE CycleGAN) for unpaired mesh deformation transfer, which is the first automatic
    work for mesh deformation transfer. This work takes the ACAP representation as
    input, and encodes the representation into latent space by a VAE, and then transfer
    deformations between source and target in the latent space domain with the cycle
    consistency and visual similarity consistency. Gao et al.  [[27](#bib.bib27)]
    firstly view the geometric details shown in Fig [5](#S7.F5 "Figure 5 ‣ 7 Deformation-based
    representations") as the deformations. Based on the previous techniques [[25](#bib.bib25),
    [26](#bib.bib26), [94](#bib.bib94), [95](#bib.bib95)], the geometric details could
    be encoded and generated. The structure in  [[27](#bib.bib27)] is also analyzed
    in the stable supportable manner [[44](#bib.bib44)]. Yuan et al.[[120](#bib.bib120)]
    apply newly designed pooling operation based on mesh simplification and graph
    convolution to VAE architecture, which also takes ACAP representation as input
    of network. Tan et al. [[96](#bib.bib96)] use ACAP representation for simulating
    thin-shell deformable materials, which apply graph-based CNN to embed high-dimensional
    features into low-dimensional features. In addition of considering a single deformable
    mesh, mesh sequences play a more important role in computer animation. And the
    deformation-based representation ACAP [[25](#bib.bib25)] is suitable for representing
    mesh sequence. Qiao et al.[[79](#bib.bib79)] also takes ACAP representation as
    input to generate mesh animation sequences by a bidirectional LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75aa016fc81f41b577401df96820dd51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The research works on deformation-based shape representation of the
    geometrylearning group in ICT, CAS'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4d751d1d30ba7f159441944dce29ab8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example of representing a chair leg by deforming bounding box
    in SDM-NET. (a)a chair with one of its leg parts highlighted, (b)the highlighted
    part in (a) and the overlaid bounding box, (c)the bounding box used as the template,
    (d)deformed bounding box, (e)recovered shape. Ref.[[27](#bib.bib27)] ©ACM 2019'
  prefs: []
  type: TYPE_NORMAL
- en: Implicit surface based approaches. With the development of implicit surface
    representations, Jeruzalski et al. [[45](#bib.bib45)] proposed a method to represent
    articulated deformable shapes by pose parameters, called Neural Articulated Shape
    Approximation (NASA). The pose parameters mentioned in [[45](#bib.bib45)] record
    the transformation of bones defined in models. They compared three different network
    architectures, including unstructured model (U), piecewise rigid model (R) and
    piecewise deformable model (D) in the training dataset and test dataset, which
    opens another direction to represent deformable 3D shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the development of 3D scanners, 3D models become easier to obtain, so there
    are more and more 3D shape datasets that have been proposed with different 3D
    representations. The larger datasets with more details bring more challenges for
    existing techniques, which further promotes the development of deep learning on
    different 3D representations.
  prefs: []
  type: TYPE_NORMAL
- en: The datasets can be divided into several types in different representations
    and different applications. Choosing the appropriate dataset benefits the performance
    and generalization for learning based models.
  prefs: []
  type: TYPE_NORMAL
- en: RGB-D Images. RGB-D image datasets can be collected by depth sensors like Microsoft
    Kinect. Most of the RGB-D image datasets can be regarded as a sequence of video.
    The indoor scene RGB-D image dataset NYU Depth [[85](#bib.bib85), [86](#bib.bib86)]
    was first provided for the segmentation problem, and the v1 version [[85](#bib.bib85)]
    collects 64 categories while the v2 version [[86](#bib.bib86)] collects 464 categories.
    The KITTI [[29](#bib.bib29)] dataset provides outdoor scene images mainly for
    autonomous driving, which contains 5 categories including ‘Road’, ‘City’, ‘Residential’,
    ‘Campus’ and ‘Person’. The depth map of images can be calculated by the development
    kit provided by the KITTI dataset. And the KITTI dataset also contains 3D objects
    annotations for applications such as object detection. ScanNet [[17](#bib.bib17)]
    is a large annotated RGB-D video dataset, which includes 2.5M views in 1,513 scenes
    with 3D camera pose, surface reconstructions and semantic segmentations. Another
    dataset Human10 [[11](#bib.bib11)] is sampled from 10 human action sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Man-made 3D Object Datasets. The ModelNet [[112](#bib.bib112)] is one of the
    famous CAD model datasets for 3D shape analysis, including 127,915 3D CAD Models
    in 662 categories. ModelNet provides two subsets named ModelNet10 and ModelNet40
    respectively. ModelNet10 includes 10 categories from the whole dataset, and the
    3D models in ModelNet10 are aligned manually; ModelNet40 includes 40 categories,
    and the 3D models are also aligned. ShapeNet [[12](#bib.bib12)] provides a larger
    scale dataset, containing more than 3 million models in more than 4K categories.
    ShapeNet also contains two smaller subsets: ShapeNetCore and ShapeNetSem. For
    various geometry applications, ShapeNet [[12](#bib.bib12)] provides rich annotations
    for 3D objects in the dataset, including category labels, part labels, symmetry
    information, etc. ObjectNet3D [[114](#bib.bib114)] is a large-scale dataset for
    3D object recognition from 2D images, which includes 201,888 3D objects in 90,127
    images and 44,147 different 3D shapes. The dataset is annotated with 3D pose parameters,
    which align 3D objects with 2D images. SUNCG [[92](#bib.bib92)] includes full
    room 3D models, which is suitable for 3D scene analysis and scene completion tasks.
    The 3D models in SUNCG are represented by dense voxel grids with object annotations.
    The whole dataset includes 49,884 valid floors with 404,058 rooms and 5,697,217
    object instances. PartNet provides a more detailed CAD model dataset with fine-grained,
    hierarchical part annotations, which brings more challenges and resources for
    3D object applications such as semantic segmentation, shape editing and shape
    generation. 3D-Future[[23](#bib.bib23)] provides a large-scale furniture dataset,
    which includes 20,000+ scenes in 5,000+ rooms with 10,000+ 3D instances. Each
    3D shape is of high quality with the best texture information for now.'
  prefs: []
  type: TYPE_NORMAL
- en: Non-Rigid Model Datasets. TOSCA[[8](#bib.bib8)] is one of the high-resolution
    3D non-rigid model datasets, which contains 80 objects in 9 categories. The models
    are in the mesh representation, and the objects within the same category have
    the same resolution. FAUST[[5](#bib.bib5)] is a dataset of 3D human body scans
    in 10 different people with a variety of poses and the ground truth correspondences
    are also provided. Because FAUST was proposed for real-world shape registration,
    the scans provided in the dataset are noisy and incomplete, but the corresponding
    ground truth is water-tight and aligned. AMASS [[59](#bib.bib59)] provides a large
    and varied human motion dataset, which gathers previous mocap datasets with a
    consistent framework and parameterization. It contains 344 subjects, 11,265 motions
    and more than 40 hours of recordings.
  prefs: []
  type: TYPE_NORMAL
- en: '| Source | Type | Dataset | Year | Category | Size | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | NYU Depth v1[[85](#bib.bib85)] | 2011 | 64 |
    - | Indoor Scene |'
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | NYU Depth v2[[86](#bib.bib86)] | 2012 | 464 |
    407024 | Indoor Scene |'
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | KITTI[[29](#bib.bib29)] | 2013 | 5 | - | Outdoor
    Scene |'
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | ScanNet[[17](#bib.bib17)] | 2017 | 1513 | 2.5M
    | Indoor Scene video |'
  prefs: []
  type: TYPE_TB
- en: '| Real-world | RGB-D Images | Human10[[11](#bib.bib11)] | 2018 | 10 | 9746
    | Human Action |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ModelNet[[112](#bib.bib112)] | 2015 | 662 | 127915
    | Mesh Representation |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ModelNet10[[112](#bib.bib112)] | 2015 | 10 |
    4899 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ModelNet40[[112](#bib.bib112)] | 2015 | 40 |
    12311 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ShpaeNet[[12](#bib.bib12)] | 2015 | 4K | 3millions
    | Rich Annotations |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ShapeNetCore[[12](#bib.bib12)] | 2015 | 55 |
    51300 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | ShapeNetSem[[12](#bib.bib12)] | 2015 | 270 |
    12000 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | Images and 3D Models | ObjectNet3D[[114](#bib.bib114)] | 2016
    | 100 | 44161 | 2D aligned with 3D |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | SUNCG[[92](#bib.bib92)] | 2017 | - | 49884 |
    Full Room Scene |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | PartNet[[69](#bib.bib69)] | 2019 | 24 | 26671
    | 573585 Part Instance |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | 3D CAD Models | 3D-FUTURE[[23](#bib.bib23)] | 2020 | - | 10K
    | Texture Information |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | Non-Rigid Models | TOSCA[[8](#bib.bib8)] | 2008 | 9 | 80 | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| Real-world | Non-Rigid Models | FAUST[[5](#bib.bib5)] | 2014 | 10 | 300 |
    Human Bodies |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic | Non-Rigid Models | AMASS[[59](#bib.bib59)] | 2019 | 344 | 11265
    | Human Motions |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The Overview of 3D Model Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Shape Analysis and Reconstruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The shape representations mentioned above are fundamental for shape analysis
    and shape reconstruction. In this section, we summarize representative works in
    these two directions respectively and compare the performance of these works.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Shape Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shape analysis methods usually extract the latent codes from different 3D shape
    representations by different network architectures. The latent codes are then
    used for specific applications like shape classification, shape retrieval, shape
    segmentation, etc. And different representations are usually suitable for different
    applications. We now review the performance of different representations in different
    models and discuss suitable representations for specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: Shape Classification and Retrieval are the basic problems of shape analysis.
    Both of them rely on the feature vectors extracted from the analysis networks.
    For shape classification, the datasets ModelNet10 and ModelNet40 [[112](#bib.bib112)]
    are widely used and Table [2](#S9.T2 "Table 2 ‣ 9.1 Shape Analysis ‣ 9 Shape Analysis
    and Reconstruction") shows the accuracy of some different methods on ModelNet10
    and ModelNet40\. For shape retrieval, given a 3D shape as a query, the target
    is to find the most similar shape(s) in the dataset to match the query. Retrieval
    methods usually learn to find a compact code to represent the object in a latent
    space, and query the closest object as the result based on Euclidean distance,
    Mahalanobis distance or other distance metrics. Different from the classification
    task, the shape retrieval task has a number of evaluation measures, including
    precision, recall, mAP (mean average precision), etc.
  prefs: []
  type: TYPE_NORMAL
- en: '| Form | Model | Accuracy(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel | 3DShapeNet [[112](#bib.bib112)] | 83.54 | 77.32 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel | VoxNet [[63](#bib.bib63)] | 92 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel | 3D-GAN [[110](#bib.bib110)] | 91.0 | 83.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel | Qi et al. [[76](#bib.bib76)] | - | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel | ORION [[82](#bib.bib82)] | 93.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Point | PointNet [[75](#bib.bib75)] | - | 89.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-view | MVCNN [[93](#bib.bib93)] | - | 90.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Point | Kd-net[[47](#bib.bib47)] | 93.3 | 90.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-view | Qi et al. [[76](#bib.bib76)] | - | 91.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Point | PointNet++ [[77](#bib.bib77)] | - | 91.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Point | Point2Sequence [[57](#bib.bib57)] | 95.3 | 92.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Accuracy of shape classification on ModelNet10 and ModelNet40 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Shape Segmentation aims to discriminate the part categories of a 3D shape. This
    task plays an important role in understanding 3D shapes. The mean Intersection-over-Union
    (mIOU) is often used as the evaluation metric of shape segmentation. Most researchers
    choose to use the point-based representation for the segmentation task [[47](#bib.bib47),
    [75](#bib.bib75), [77](#bib.bib77), [53](#bib.bib53), [66](#bib.bib66)].
  prefs: []
  type: TYPE_NORMAL
- en: Shape Symmetry Detection. Symmetry is important geometry information in 3D shapes,
    and it can be further used in many other applications such as shape alignment,
    registration, completion, etc. Gao et al. [[28](#bib.bib28)] designed the first
    unsupervised deep learning method named PRS-Net (Planar Reflective Symmetry Net)
    to detect planar reflective symmetry of 3D shapes, which designs a new symmetry
    distance loss and a regularization loss. And PRS-Net was proved to be robust in
    noisy and incomplete input and more efficient than traditional methods. As symmetry
    is largely determined by the overall shape, PRS-Net is based on a 3D voxel CNN
    and gains high performance in a low resolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f2534f99503f704bc3853e50d1b2bb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The pipeline of PRS-Net Ref. [[28](#bib.bib28)] ©IEEE 2020'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Shape Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning based generative models have been proposed for different representations,
    which is also an important field in geometry learning. The reconstruction applications
    include single-view shape reconstruction, shape generation, shape editing, etc.
    The generation methods can be summarized on the basis of representations. For
    voxel-based representations, learning based models try to predict the occupancy
    probability of each voxel in the grid. For point-based representations, learning
    based models either sample 3D points in the space or fold the 2D grids into target
    3D objects. For mesh-based representations, most of the generation methods choose
    to deform a mesh template into the final mesh. In recent study, more and more
    methods choose to use structured representation and generate coarse-to-fine 3D
    shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we review a series of deep learning methods based on different
    3D object representations. We first overview different 3D representation learning
    models. And the tendency of the geometry learning can be summarized to be less
    computation and memory demanding, and more detailed and structured. Then, we introduce
    3D datasets which are widely used in the research. These datasets provide rich
    resources and support evaluation for data-driven learning methods. Finally, we
    discuss 3D shape applications based on different 3D representations, including
    shape analysis and shape reconstruction. Different representations are usually
    suitable for different applications. Therefore, it is vitally important to choose
    suitable 3D representations for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: \CvmAck
  prefs: []
  type: TYPE_NORMAL
- en: This work was supported by National Natural Science Foundation of China (No.
    61828204 and No. 61872440), Beijing Municipal Natural Science Foundation (No.
    L182016), Youth Innovation Promotion Association CAS, CCF-Tencent Open Fund, Royal
    Society-Newton Advanced Fellowship (No. NAF\R2\192151) and the Royal Society (no.
    IES\R1\180126).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] E. Ahmed, A. Saint, A. E. R. Shabayek, K. Cherenkova, R. Das, G. Gusev,
    D. Aouada, and B. Ottersten. Deep learning advances on different 3D data representations:
    A survey. arXiv preprint arXiv:1808.01462, 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances
    in Neural Information Processing Systems, pages 1993–2001, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] H. Ben-Hamu, H. Maron, I. Kezurer, G. Avineri, and Y. Lipman. Multi-chart
    generative surface modeling. In SIGGRAPH Asia 2018 Technical Papers, page 215\.
    ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] P. J. Besl and N. D. McKay. Method for registration of 3-d shapes. In Sensor
    fusion IV: control paradigms and data structures, volume 1611, pages 586–606\.
    International Society for Optics and Photonics, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Bogo, J. Romero, M. Loper, and M. J. Black. FAUST: Dataset and evaluation
    for 3D mesh registration. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 3794–3801, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and P. Vandergheynst.
    Learning class-specific descriptors for deformable shapes using localized spectral
    convolutional networks. In Computer Graphics Forum, volume 34, pages 13–23\. Wiley
    Online Library, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Boscaini, J. Masci, E. Rodolà, and M. Bronstein. Learning shape correspondence
    with anisotropic convolutional neural networks. In Advances in Neural Information
    Processing Systems, pages 3189–3197, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Numerical geometry of
    non-rigid shapes. Springer Science & Business Media, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric
    deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18–42,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally
    connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y.-P. Cao, Z.-N. Liu, Z.-F. Kuang, L. Kobbelt, and S.-M. Hu. Learning
    to reconstruct high-quality 3D shapes with cascaded fully convolutional networks.
    In The European Conference on Computer Vision (ECCV), September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su, et al. ShapeNet: An information-rich 3D model repository.
    arXiv preprint arXiv:1512.03012, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Z. Chen, A. Tagliasacchi, and H. Zhang. BSP-Net: Generating compact meshes
    via binary space partitioning. arXiv preprint arXiv:1911.06971, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Chen and H. Zhang. Learning implicit fields for generative shape modeling.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5939–5948, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical
    machine translation. arXiv preprint arXiv:1406.1078, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-R2N2: A unified
    approach for single and multi-view 3D object reconstruction. In European Conference
    on Computer Vision (ECCV), pages 628–644\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner.
    ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5828–5839,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural
    networks on graphs with fast localized spectral filtering. In Advances in neural
    information processing systems, pages 3844–3852, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning
    molecular fingerprints. In Advances in neural information processing systems,
    pages 2224–2232, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single
    image using a multi-scale deep network. In Advances in neural information processing
    systems, pages 2366–2374, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3D
    object reconstruction from a single image. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 605–613, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Fey, J. Eric Lenssen, F. Weichert, and H. Müller. SplineCNN: Fast geometric
    deep learning with continuous b-spline kernels. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 869–877, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future:
    3d furniture shape with texture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Gao, Y.-K. Lai, D. Liang, S.-Y. Chen, and S. Xia. Efficient and flexible
    deformation representation for data-driven surface modeling. ACM Transactions
    on Graphics (TOG), 35(5):158, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] L. Gao, Y.-K. Lai, J. Yang, Z. Ling-Xiao, S. Xia, and L. Kobbelt. Sparse
    data driven mesh deformation. IEEE transactions on visualization and computer
    graphics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] L. Gao, J. Yang, Y.-L. Qiao, Y.-K. Lai, P. L. Rosin, W. Xu, and S. Xia.
    Automatic unpaired shape deformation transfer. In SIGGRAPH Asia 2018 Technical
    Papers, page 237\. ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] L. Gao, J. Yang, T. Wu, Y.-J. Yuan, H. Fu, Y.-K. Lai, and H. Zhang. SDM-NET:
    Deep generative network for structured deformable mesh. ACM Transactions on Graphics
    (TOG), 38(6):243, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] L. Gao, L.-X. Zhang, H.-Y. Meng, Y.-H. Ren, Y.-K. Lai, and L. Kobbelt.
    PRS-Net: Planar reflective symmetry detection net for 3D models. IEEE Transactions
    on Visualization and Computer Graphics, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics:
    The KITTI dataset. The International Journal of Robotics Research, 32(11):1231–1237,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] K. Genova, F. Cole, A. Sud, A. Sarna, and T. Funkhouser. Deep structured
    implicit functions. arXiv preprint arXiv:1912.06126, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] K. Genova, F. Cole, D. Vlasic, A. Sarna, W. T. Freeman, and T. Funkhouser.
    Learning shape templates with structured implicit functions. arXiv preprint arXiv:1904.06447,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] R. Girdhar, D. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable
    and generative vector representation for objects. In European Conference on Computer
    Vision (ECCV), 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural
    information processing systems, pages 2672–2680, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry. AtlasNet:
    A papier-mâché approach to learning 3D surface generation. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun. Deep learning
    for 3D point clouds: A survey. arXiv: 1912.12033, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Gupta, P. Arbeláez, R. Girshick, and J. Malik. Aligning 3D models to
    RGB-D images of cluttered scenes. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 4731–4740, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik. Learning rich features
    from RGB-D images for object detection and segmentation. In European conference
    on computer vision, pages 345–360. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for
    3D object reconstruction. In 2017 International Conference on 3D Vision (3DV),
    pages 412–420\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, and D. Cohen-Or.
    MeshCNN: a network with an edge. ACM Transactions on Graphics (TOG), 38(4):90,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured
    data. arXiv preprint arXiv:1506.05163, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for
    deep belief nets. Neural computation, 18(7):1527–1554, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,
    9(8):1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. Nießner, and L. J. Guibas.
    Texturenet: Consistent local parametrizations for learning from high-resolution
    signals on meshes. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 4440–4449, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S.-S. Huang, H. Fu, L.-Y. Wei, and S.-M. Hu. Support substructures: support-induced
    part-level structural representation. IEEE transactions on visualization and computer
    graphics, 22(8):2024–2036, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] T. Jeruzalski, B. Deng, M. Norouzi, J. Lewis, G. Hinton, and A. Tagliasacchi.
    NASA: Neural articulated shape approximation. arXiv preprint arXiv:1912.03207,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional
    networks. arXiv preprint arXiv:1609.02907, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-networks for the
    recognition of 3D point cloud models. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 863–872, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding
    beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications
    in vision. In Proceedings of 2010 IEEE International Symposium on Circuits and
    Systems, pages 253–256\. IEEE, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas. GRASS:
    Generative recursive autoencoders for shape structures. ACM Transactions on Graphics
    (TOG), 36(4):52, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-GAN: a point cloud
    upsampling adversarial network. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 7203–7212, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. PointCNN: Convolution
    on x-transformed points. In Advances in neural information processing systems,
    pages 820–830, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: Field probing
    neural networks for 3D data. In Advances in Neural Information Processing Systems,
    pages 307–315, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Liu, S. Saito, W. Chen, and H. Li. Learning to infer implicit surfaces
    without 3D supervision. In Advances in Neural Information Processing Systems,
    pages 8293–8304, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker. Point2sequence: Learning the
    shape representation of 3D point clouds with an attention-based sequence to sequence
    network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
    pages 8778–8785, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface
    construction algorithm. In ACM siggraph computer graphics, volume 21, pages 163–169.
    ACM, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass:
    Archive of motion capture as surface shapes. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 5442–5451, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim,
    and Y. Lipman. Convolutional neural networks on surfaces via seamless toric covers.
    ACM Trans. Graph., 36(4):71–1, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional
    neural networks on Riemannian manifolds. In Proceedings of the IEEE international
    conference on computer vision workshops, pages 37–45, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. Maturana and S. Scherer. 3D convolutional neural networks for landing
    zone detection from LiDAR. In 2015 IEEE International Conference on Robotics and
    Automation (ICRA), pages 3471–3478\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] D. Maturana and S. Scherer. VoxNet: A 3D convolutional neural network
    for real-time object recognition. In 2015 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS), pages 922–928\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] D. Meagher. Geometric modeling using octree encoding. Computer graphics
    and image processing, 19(2):129–147, 1982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] E. Mehr, A. Jourdan, N. Thome, M. Cord, and V. Guitteny. DiscoNet: Shapes
    learning on disconnected manifolds for 3D editing. In Proceedings of the IEEE
    International Conference on Computer Vision, pages 3474–3483, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha. Vv-net: Voxel vae net with
    group convolutions for point cloud segmentation. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 8500–8508, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy
    networks: Learning 3D reconstruction in function space. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4460–4470, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. J. Mitra, and L. J. Guibas.
    StructureNet: hierarchical graph networks for 3D shape generation. ACM Transactions
    on Graphics (TOG), 38(6):242, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su.
    PartNet: A large-scale benchmark for fine-grained and hierarchical part-level
    3D object understanding. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 909–918, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein.
    Geometric deep learning on graphs and manifolds using mixture model CNNs. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5115–5124,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Nash, Y. Ganin, S. Eslami, and P. W. Battaglia. PolyGen: An autoregressive
    generative model of 3D meshes. arXiv preprint arXiv:2002.10880, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. Pan, S. Liu, Y. Liu, and X. Tong. Convolutional neural networks on
    3D surfaces using parallel frames. arXiv preprint arXiv:1808.04952, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia. Deep mesh reconstruction
    from single rgb images via topology modification networks. In Proceedings of the
    IEEE International Conference on Computer Vision, pages 9964–9973, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. DeepSDF:
    Learning continuous signed distance functions for shape representation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep learning on point
    sets for 3D classification and segmentation. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 652–660, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas. Volumetric
    and multi-view CNNs for object classification on 3D data. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 5648–5656, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++: Deep hierarchical
    feature learning on point sets in a metric space. In Advances in neural information
    processing systems, pages 5099–5108, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y.-L. Qiao, L. Gao, J. Yang, P. L. Rosin, Y.-K. Lai, and X. Chen. LaplacianNet:
    Learning on 3D meshes with laplacian encoding and pooling. arXiv preprint arXiv:1910.14063,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y.-L. Qiao, Y.-K. Lai, H. Fu, and L. Gao. Synthesizing mesh deformation
    sequences with bidirectional lstm. IEEE Transactions on Visualization and Computer
    Graphics, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] G. Riegler, A. Osman Ulusoy, and A. Geiger. OctNet: Learning deep 3D representations
    at high resolutions. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 3577–3586, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover’s distance as
    a metric for image retrieval. International journal of computer vision, 40(2):99–121,
    2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] N. Sedaghat, M. Zolfaghari, E. Amiri, and T. Brox. Orientation-boosted
    voxel nets for 3D object recognition. In British Machine Vision Conference, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Sharma, O. Grau, and M. Fritz. Vconv-DAE: Deep volumetric shape learning
    without object labels. In European Conference on Computer Vision, pages 236–250.
    Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] B. Shi, S. Bai, Z. Zhou, and X. Bai. Deeppano: Deep panoramic representation
    for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339–2343, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] N. Silberman and R. Fergus. Indoor scene segmentation using a structured
    light sensor. In Proceedings of the International Conference on Computer Vision
    - Workshop on 3D Representation and Recognition, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and
    support inference from RGBD images. In European Conference on Computer Vision,
    pages 746–760. Springer, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape surfaces using
    geometry images. In European Conference on Computer Vision, pages 223–240. Springer,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] A. Sinha, A. Unmesh, Q. Huang, and K. Ramani. SurfNet: Generating 3D shape
    surfaces using deep residual networks. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 6040–6049, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. Socher, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng. Convolutional-recursive
    deep learning for 3D object classification. In Advances in neural information
    processing systems, pages 656–664, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes
    and natural language with recursive neural networks. In Proceedings of the 28th
    international conference on machine learning (ICML-11), pages 129–136, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S. Song and J. Xiao. Deep sliding shapes for amodal 3D object detection
    in RGB-D images. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 808–816, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic
    scene completion from a single depth image. Proceedings of 30th IEEE Conference
    on Computer Vision and Pattern Recognition, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-view convolutional
    neural networks for 3D shape recognition. In Proceedings of the IEEE international
    conference on computer vision, pages 945–953, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Q. Tan, L. Gao, Y.-K. Lai, and S. Xia. Variational autoencoders for deforming
    3D mesh models. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5841–5850, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Q. Tan, L. Gao, Y.-K. Lai, J. Yang, and S. Xia. Mesh-based autoencoders
    for localized deformation component analysis. In Thirty-Second AAAI Conference
    on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Q. Tan, Z. Pan, L. Gao, and D. Manocha. Realtime simulation of thin-shell
    deformable materials using cnn-based mesh embedding. IEEE Robotics and Automation
    Letters, 5(2):2325–2332, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Tang, X. Han, J. Pan, K. Jia, and X. Tong. A skeleton-bridged deep
    learning approach for generating meshes of complex topologies from single RGB
    images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4541–4550, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks:
    Efficient convolutional architectures for high-resolution 3D outputs. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 2088–2096, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural
    information processing systems, pages 5998–6008, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Verma, E. Boyer, and J. Verbeek. FeastNet: Feature-steered graph convolutions
    for 3D shape analysis. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 2598–2606, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting
    and composing robust features with denoising autoencoders. In Proceedings of the
    25th international conference on Machine learning, pages 1096–1103\. ACM, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
    Stacked denoising autoencoders: Learning useful representations in a deep network
    with a local denoising criterion. Journal of machine learning research, 11(Dec):3371–3408,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] H. Wang, N. Schor, R. Hu, H. Huang, D. Cohen-Or, and H. Huang. Global-to-local
    generative model for 3D shapes. In SIGGRAPH Asia 2018 Technical Papers, page 214\.
    ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2Mesh:
    Generating 3D mesh models from single RGB images. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 52–67, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong. O-CNN: Octree-based
    convolutional neural networks for 3D shape analysis. ACM Transactions on Graphics
    (TOG), 36(4):72, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong. Adaptive O-CNN: a patch-based
    deep representation of 3D shapes. In SIGGRAPH Asia 2018 Technical Papers, page
    217\. ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Wang and J. M. Solomon. Deep closest point: Learning representations
    for point cloud registration. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 3523–3532, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon.
    Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG),
    38(5):1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Wen, Y. Zhang, Z. Li, and Y. Fu. Pixel2Mesh++: Multi-view 3D mesh
    generation via deformation. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1042–1051, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic
    latent space of object shapes via 3D generative-adversarial modeling. In Advances
    in neural information processing systems, pages 82–90, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] R. Wu, Y. Zhuang, K. Xu, H. Zhang, and B. Chen. PQ-NET: A generative
    part seq2seq network for 3D shapes. arXiv preprint arXiv:1911.10949, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3D
    ShapeNets: A deep representation for volumetric shapes. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 1912–1920, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, and H. Huang. SAGNet:
    structure-aware generative network for 3D-shape modeling. ACM Transactions on
    Graphics (TOG), 38(4):91, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas,
    and S. Savarese. ObjectNet3D: A large scale database for 3D object recognition.
    In European Conference on Computer Vision, pages 160–176. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] H. Xu, M. Dong, and Z. Zhong. Directionally convolutional networks for
    3D shape segmentation. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 2698–2707, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann. DISN: Deep implicit
    surface network for high-quality single-view 3D reconstruction. In NeurIPS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Yang, C. Feng, Y. Shen, and D. Tian. FoldingNet: Point cloud auto-encoder
    via deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 206–215, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, and O. Sorkine-Hornung. Patch-based
    progressive 3D point set upsampling. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 5958–5967, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-Net: Point cloud
    upsampling network. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 2790–2799, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y.-J. Yuan, Y.-K. Lai, J. Yang, H. Fu, and L. Gao. Mesh variational autoencoders
    with edge contraction pooling. arXiv preprint arXiv:1908.02507, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem. 3D-PRNN: Generating
    shape primitives with recurrent neural networks. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 900–909, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \Author
  prefs: []
  type: TYPE_NORMAL
- en: YunPengXiao.pdfYun-Peng Xiao received his bachelor’s degree in computer science
    from Nankai University, He is currently a Master Student in the Institute of Computing
    Technology, Chinese Academy of Sciences. His research interests include computer
    graphics and geometric processing.
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  prefs: []
  type: TYPE_NORMAL
- en: YuKunLai.pdfYu-Kun Lai received his bachelor’s degree and PhD degree in computer
    science from Tsinghua University in 2003 and 2008, respectively. He is currently
    a Reader in the School of Computer Science & Informatics, Cardiff University.
    His research interests include computer graphics, geometry processing, image processing
    and computer vision. He is on the editorial boards of *Computer Graphics Forum*
    and *The Visual Computer*.
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  prefs: []
  type: TYPE_NORMAL
- en: FangLueZhang.pdfFang-Lue Zhang is currently a Lecturer with Victoria University
    of Wellington, New Zealand. He received the Bachelor’s degree from Zhejiang University,
    Hangzhou, China, in 2009, and the Doctoral degree from Tsinghua University, Beijing,
    China, in 2015\. His research interests include image and video editing, computer
    vision, and computer graphics. He is a member of IEEE and ACM. He received Victoria
    Early-Career Research Excellence Award in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  prefs: []
  type: TYPE_NORMAL
- en: ChunpengLi.pdfChunpeng Li was born in 1980\. He received his PhD degree in 2008
    and now is an Associate Professor at the Institute of Computing Technology, Chinese
    Academy of Sciences. His main research interests are virtual reality, human–computer
    interaction, and computer graphics.
  prefs: []
  type: TYPE_NORMAL
- en: \Author
  prefs: []
  type: TYPE_NORMAL
- en: GL1.pdfLin Gao received a bachelor’s degree in mathematics from Sichuan University
    and a PhD degree in computer science from Tsinghua University. He is currently
    an Associate Professor at the Institute of Computing Technology, Chinese Academy
    of Sciences. His research interests include computer graphics and geometric processing.
    He received the Newton Advanced Fellowship award from the Royal Society in 2019.
  prefs: []
  type: TYPE_NORMAL
