- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:00:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2006.12567] A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2006.12567] 深度学习在定位与地图构建中的调查：迈向空间机器智能时代'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2006.12567](https://ar5iv.labs.arxiv.org/html/2006.12567)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2006.12567](https://ar5iv.labs.arxiv.org/html/2006.12567)
- en: 'A Survey on Deep Learning for Localization and Mapping:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在定位与地图构建中的调查：
- en: Towards the Age of Spatial Machine Intelligence
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 迈向空间机器智能时代
- en: Changhao Chen, Bing Wang, Chris Xiaoxuan Lu, Niki Trigoni and Andrew Markham
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 陈昌浩，王兵，陆晓轩，尼基·特里戈尼，安德鲁·马克汉
- en: 'Department of Computer Science, University of Oxford Corresponding author:
    Changhao Chen (changhao.chen@cs.ox.ac.uk) A project website that updates additional
    material and extended lists of references, can be found at https://github.com/changhao-chen/deep-learning-localization-mapping.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津大学计算机科学系  通讯作者：陈昌浩 (changhao.chen@cs.ox.ac.uk)  项目网站更新了额外的资料和扩展的参考文献列表，网址为
    https://github.com/changhao-chen/deep-learning-localization-mapping。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning based localization and mapping has recently attracted significant
    attention. Instead of creating hand-designed algorithms through exploitation of
    physical models or geometric theories, deep learning based solutions provide an
    alternative to solve the problem in a data-driven way. Benefiting from ever-increasing
    volumes of data and computational power, these methods are fast evolving into
    a new area that offers accurate and robust systems to track motion and estimate
    scenes and their structure for real-world applications. In this work, we provide
    a comprehensive survey, and propose a new taxonomy for localization and mapping
    using deep learning. We also discuss the limitations of current models, and indicate
    possible future directions. A wide range of topics are covered, from learning
    odometry estimation, mapping, to global localization and simultaneous localization
    and mapping (SLAM). We revisit the problem of perceiving self-motion and scene
    understanding with on-board sensors, and show how to solve it by integrating these
    modules into a prospective spatial machine intelligence system (SMIS). It is our
    hope that this work can connect emerging works from robotics, computer vision
    and machine learning communities, and serve as a guide for future researchers
    to apply deep learning to tackle localization and mapping problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的定位和地图构建最近引起了广泛关注。与通过利用物理模型或几何理论创建手工设计的算法不同，深度学习方法提供了一种数据驱动的解决方案。得益于不断增长的数据量和计算能力，这些方法正在迅速发展成为一个新领域，提供准确且稳健的系统来跟踪运动并估计场景及其结构以应用于实际世界。在这项工作中，我们提供了一项全面的调查，并提出了基于深度学习的定位和地图构建的新分类法。我们还讨论了当前模型的局限性，并指出了可能的未来方向。覆盖了从学习里程估计、地图构建到全球定位和同时定位与地图构建（SLAM）等广泛主题。我们重新审视了通过车载传感器感知自我运动和场景理解的问题，并展示了如何通过将这些模块整合到一个前景空间机器智能系统（SMIS）中来解决这一问题。我们希望这项工作能连接机器人学、计算机视觉和机器学习领域的新兴成果，并为未来的研究人员提供指导，以应用深度学习解决定位和地图构建问题。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Learning, Localization, Mapping, SLAM, Perception, Correspondence Matching,
    Uncertainty Estimation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、定位、地图构建、SLAM、感知、匹配、 不确定性估计
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Localization and mapping is a fundamental need for human and mobile agents.
    As a motivating example, humans are able to perceive their self-motion and environment
    via multimodal sensory perception, and rely on this awareness to locate and navigate
    themselves in a complex three-dimensional space [[1](#bib.bib1)]. This ability
    is part of human spatial ability. Furthermore, the ability to perceive self-motion
    and their surroundings plays a vital role in developing cognition, and motor control
    [[2](#bib.bib2)]. In a similar vein, artificial agents or robots should also be
    able to perceive the environment and estimate their system states using on-board
    sensors. These agents could be any form of robot, e.g. self-driving vehicles,
    delivery drones or home service robots, sensing their surroundings and autonomously
    making decisions [[3](#bib.bib3)]. Equivalently, as emerging Augmented Reality
    (AR) and Virtual Reality (VR) technologies interweave cyber space and physical
    environments, the ability of machines to be perceptually aware underpins seamless
    human-machine interaction. Further applications also include mobile and wearable
    devices, such as smartphones, wristbands or Internet-of-Things (IoT) devices,
    providing users with a wide range of location-based-services, ranging from pedestrian
    navigation [[4](#bib.bib4)], to sports/activity monitoring [[5](#bib.bib5)], to
    animal tracking [[6](#bib.bib6)], or emergency response [[7](#bib.bib7)] for first-responders.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本地化和映射是人类和移动代理的基本需求。作为一个激励性的例子，人类能够通过多模态感官感知来感知自身运动和环境，并依靠这种意识在复杂的三维空间中定位和导航[[1](#bib.bib1)]。这种能力是人类空间能力的一部分。此外，感知自我运动和周围环境的能力在认知和运动控制的发展中扮演着至关重要的角色[[2](#bib.bib2)]。类似地，人工代理或机器人也应能通过车载传感器感知环境并估计其系统状态。这些代理可以是任何形式的机器人，例如自动驾驶车辆、配送无人机或家庭服务机器人，它们通过感知周围环境自主做出决策[[3](#bib.bib3)]。同样，随着新兴的增强现实（AR）和虚拟现实（VR）技术将网络空间和物理环境交织在一起，机器的感知意识能力支撑了无缝的人机交互。进一步的应用还包括移动和可穿戴设备，如智能手机、手环或物联网（IoT）设备，为用户提供广泛的基于位置的服务，从步行导航[[4](#bib.bib4)]到运动/活动监测[[5](#bib.bib5)]，再到动物追踪[[6](#bib.bib6)]，以及急救响应[[7](#bib.bib7)]。
- en: '![Refer to caption](img/ca115a799a28826386962fd622580ad1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ca115a799a28826386962fd622580ad1.png)'
- en: 'Figure 1: A spatial machine intelligence system exploits on-board sensors to
    perceive self-motion, global pose, scene geometry and semantics. (a) Conventional
    model based solutions build hand-designed algorithms to convert input sensor data
    to target values. (c) Data-driven solutions exploit learning models to construct
    this mapping function. (b) Hybrid approaches combine both hand-crafted algorithms
    and learning models. This survey discusses (b) and (c).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：空间机器智能系统利用车载传感器感知自我运动、全球姿态、场景几何和语义。(a) 传统模型基础解决方案构建手工设计的算法将输入传感器数据转换为目标值。(c)
    数据驱动解决方案利用学习模型来构建这种映射函数。(b) 混合方法结合了手工制作的算法和学习模型。本调查讨论了(b) 和(c)。
- en: '![Refer to caption](img/2c084ace53d7ada415dd4a801044aaba.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2c084ace53d7ada415dd4a801044aaba.png)'
- en: 'Figure 2: A taxonomy of existing works on deep learning for localization and
    mapping.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：关于深度学习在本地化和映射中的现有工作分类。
- en: Enabling a high level of autonomy for these and other digital agents requires
    precise and robust localization, and incrementally building and maintaining a
    world model, with the capability to continuously process new information and adapt
    to various scenarios. Such a quest is termed as ‘Spatial Machine Intelligence
    System (SMIS)’ in our work or recently as Spatial AI in [[8](#bib.bib8)]. In this
    work, broadly, localization refers to the ability to obtain the internal system
    states of robot motion, including locations, orientations and velocities, whilst
    mapping indicates the capacity to perceive external environmental states and capture
    the surroundings, including the geometry, appearance and semantics of a 2D or
    3D scene. These components can act individually to sense the internal or external
    states respectively, or jointly as in simultaneous localization and mapping (SLAM)
    to track pose and build a consistent environmental model in a global frame.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些以及其他数字代理的高水平自主性需要精确和稳健的本地化，并逐步构建和维护一个世界模型，具备持续处理新信息并适应各种场景的能力。这种追求在我们的工作中被称为“空间机器智能系统（SMIS）”，或者最近在[[8](#bib.bib8)]中称为空间人工智能。广义上说，本地化是指获取机器人运动的内部系统状态的能力，包括位置、方向和速度，而映射则是指感知外部环境状态并捕捉周围环境的能力，包括二维或三维场景的几何形状、外观和语义。这些组件可以单独作用于感知内部或外部状态，或像在同时定位与地图构建（SLAM）中一样联合作用，以在全局框架中跟踪姿态并构建一致的环境模型。
- en: 1.1 Why to Study Deep Learning for Localization and Mapping
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 为什么要研究用于本地化和映射的深度学习
- en: The problems of localization and mapping have been studied for decades, with
    a variety of intricate hand-designed models and algorithms being developed, for
    example, odometry estimation (including visual odometry [[9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)], visual-inertial odometry [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15)] and LIDAR odometry [[16](#bib.bib16)]), image-based
    localization[[17](#bib.bib17), [18](#bib.bib18)], place recognition[[19](#bib.bib19)],
    SLAM[[20](#bib.bib20), [10](#bib.bib10), [21](#bib.bib21)], and structure from
    motion (SfM)[[22](#bib.bib22), [23](#bib.bib23)]. Under ideal conditions, these
    sensors and models are capable of accurately estimating system states without
    time bound and across different environments. However, in reality, imperfect sensor
    measurements, inaccurate system modelling, complex environmental dynamics and
    unrealistic constraints impact both the accuracy and reliability of hand-crated
    systems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本地化和映射的问题已经研究了几十年，开发了各种复杂的手工设计模型和算法，例如，里程计估计（包括视觉里程计[[9](#bib.bib9)、[10](#bib.bib10)、[11](#bib.bib11)]、视觉惯性里程计[[12](#bib.bib12)、[13](#bib.bib13)、[14](#bib.bib14)、[15](#bib.bib15)]和激光雷达里程计[[16](#bib.bib16)]）、基于图像的定位[[17](#bib.bib17)、[18](#bib.bib18)]、地点识别[[19](#bib.bib19)]、SLAM[[20](#bib.bib20)、[10](#bib.bib10)、[21](#bib.bib21)]和从运动中恢复结构（SfM）[[22](#bib.bib22)、[23](#bib.bib23)]。在理想条件下，这些传感器和模型能够在不同环境下准确估计系统状态，无时间限制。然而，实际上，不完美的传感器测量、不准确的系统建模、复杂的环境动态和不切实际的约束会影响手工设计系统的准确性和可靠性。
- en: 'The limitations of model based solutions, together with recent advances in
    machine learning, especially deep learning, have motivated researchers to consider
    data-driven (learning) methods as an alternative to solve problem. Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") summarizes the relation between
    input sensor data (e.g. visual, inertial, LIDAR data or other sensors) and output
    target values (e.g. location, orientation, scene geometry or semantics) as a mapping
    function. Conventional model-based solutions are achieved by hand-designing algorithms
    and calibrating to a particular application domain, while the learning based approaches
    construct this mapping function by learned knowledge. The advantages of learning
    based methods are three-fold:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '模型基解决方案的局限性，加上最近在机器学习，特别是深度学习方面的进展，促使研究人员考虑数据驱动（学习）方法作为解决问题的替代方案。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence")总结了输入传感器数据（例如视觉、惯性、激光雷达数据或其他传感器）与输出目标值（例如位置、方向、场景几何或语义）之间的关系，作为一个映射函数。传统的模型基解决方案是通过手工设计算法并校准到特定应用领域来实现的，而基于学习的方法通过学习知识来构建这个映射函数。基于学习的方法有三方面的优势：'
- en: First of all, learning methods can leverage highly expressive deep neural network
    as an universal approximator, and automatically discover features relevant to
    task. This property enables learned models to be resilience to circumstances,
    such as featureless areas, dynamic lightning conditions, motion blur, accurate
    camera calibration, which are challenging to model by hand [[3](#bib.bib3)]. As
    a representative example, visual odometry has achieved notable improvements in
    terms of robustness by incorporating data-driven methods in its design [[24](#bib.bib24),
    [25](#bib.bib25)], outperforming the state-of-the-art conventional algorithms.
    Moreover, learning approaches are able to connect abstract elements with human
    understandable terms[[26](#bib.bib26), [27](#bib.bib27)], such as semantics labelling
    in SLAM, which is hard to describe in a formal mathematical way.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，学习方法可以利用高度表达的深度神经网络作为通用近似器，并自动发现与任务相关的特征。这种属性使得学习到的模型能够对环境具有韧性，如无特征区域、动态光照条件、运动模糊、精确相机校准，这些都是人工建模具有挑战性的[[3](#bib.bib3)]。作为一个代表性例子，视觉里程计通过在其设计中融入数据驱动的方法，在稳健性方面取得了显著的改进[[24](#bib.bib24),
    [25](#bib.bib25)]，超越了最先进的传统算法。此外，学习方法能够将抽象元素与人类可理解的术语联系起来[[26](#bib.bib26), [27](#bib.bib27)]，如SLAM中的语义标注，这在正式的数学方式中难以描述。
- en: '![Refer to caption](img/5b986e226e2bbf8d25773f2a7732fab6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b986e226e2bbf8d25773f2a7732fab6.png)'
- en: 'Figure 3: High-level conceptual illustration of a spatial machine intelligence
    system (i.e. deep learning based localization and mapping). Rounded rectangles
    represent a function module, while arrow lines connect these modules for data
    input and output. It is not necessary to include all modules to perform this system.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：空间机器智能系统（即基于深度学习的定位和地图绘制）的高级概念图。圆角矩形代表功能模块，而箭头线连接这些模块以进行数据输入和输出。执行此系统并不一定需要包含所有模块。
- en: Secondly, learning methods allow spatial machine intelligence systems to learn
    from past experience, and actively exploit new information. By building a generic
    data-driven model, it avoids human effort on specifying the full knowledge about
    mathematical and physical rules[[28](#bib.bib28)], to solve domain specific problem,
    before being deployed. This ability potentially enables learning machines to automatically
    discover new computational solutions, further develop themselves and improve their
    models, within new scenarios or confronting new circumstances. A good example
    is that by using novel view synthesis as a self-supervision signal, self-motion
    and depth can be recovered from unlabelled videos[[29](#bib.bib29), [30](#bib.bib30)].
    In addition, the learned representations can further support high-level tasks,
    such as path planning[[31](#bib.bib31)], and decision making[[32](#bib.bib32)],
    by constructing task-driven maps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，学习方法使空间机器智能系统能够从过去的经验中学习，并主动利用新信息。通过构建一个通用的数据驱动模型，它避免了在部署之前，人工指定数学和物理规则的全部知识[[28](#bib.bib28)]，以解决特定领域的问题。这种能力有可能使学习机器自动发现新的计算解决方案，进一步发展自身并改进其模型，以适应新的场景或面对新的情况。一个很好的例子是，通过使用新颖的视图合成作为自我监督信号，可以从未标记的视频中恢复自我运动和深度[[29](#bib.bib29),
    [30](#bib.bib30)]。此外，学习到的表示可以进一步支持高级任务，如路径规划[[31](#bib.bib31)]和决策制定[[32](#bib.bib32)]，通过构建任务驱动的地图。
- en: The third benefit is its capability of fully exploiting the increasing amount
    of sensor data and computational power. Deep learning or deep neural network has
    the capacity to scale to large-scale problems. The huge amount of parameters inside
    a DNN framework are automatically optimized by minimizing a loss function, by
    training on large datasets through backpropagation and gradient-descent algorithms.
    For example, the recent released GPT-3[[33](#bib.bib33)], the largest pretrained
    language model, with incredibly over 175 Billion parameters, achieves the state-of-the-art
    results on a variety of natural language processing (NLP) tasks, even without
    fine-tuning. In addition, a variety of large-scale datasets relevant to localization
    and mapping have been released, for example, in the autonomous vehicles scenarios,
    [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)] are with a collection of
    rich combinations of sensor data, and motion and semantic labels. This gives us
    an imagination that it would be possible to exploit the power of data and computation
    in solving localization and mapping.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个好处是它能够充分利用不断增加的传感器数据和计算能力。深度学习或深度神经网络具有扩展到大规模问题的能力。DNN框架内的大量参数通过最小化损失函数来自动优化，通过在大数据集上进行反向传播和梯度下降算法进行训练。例如，最近发布的GPT-3[[33](#bib.bib33)]，这是最大的预训练语言模型，具有超过1750亿个参数，在各种自然语言处理（NLP）任务中实现了最先进的结果，即使没有微调。此外，已经发布了各种与定位和制图相关的大规模数据集，例如，在自动驾驶汽车场景中，[[34](#bib.bib34)、[35](#bib.bib35)、[36](#bib.bib36)]包含了丰富的传感器数据组合，以及运动和语义标签。这让我们设想可能利用数据和计算的力量来解决定位和制图问题。
- en: However, it must also be pointed out that these learning techniques are reliant
    on massive datasets to extract statistically meaningful patterns and can struggle
    to generalize to out-of-set environments. There is lack of model interpretability.
    Additionally, although highly parallelizable, they are also typically more computationally
    costly than simpler models. Details of limitations are discussed in Section 7.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也必须指出，这些学习技术依赖于大量数据集来提取统计上有意义的模式，并且可能难以推广到数据集之外的环境中。模型的可解释性不足。此外，尽管高度可并行化，它们通常也比简单模型更具计算成本。有关限制的详细信息在第7节讨论。
- en: 1.2 Comparison with Other Surveys
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 与其他调查的比较
- en: There are several survey papers that have extensively discussed model-based
    localization and mapping approaches. The development of SLAM problem in early
    decades has been well summarized in [[37](#bib.bib37), [38](#bib.bib38)]. The
    seminal survey [[39](#bib.bib39)] provides a thorough discussion on existing SLAM
    work, reviews the history of development and charts several future directions.
    Although this paper contains a section which briefly discusses deep learning models,
    it does not overview this field comprehensively, especially due to the explosion
    of research in this area of the past five years. Other SLAM survey papers only
    focus on individual flavours of SLAM systems, including the probabilistic formulation
    of SLAM [[40](#bib.bib40)], visual odometry [[41](#bib.bib41)], pose-graph SLAM
    [[42](#bib.bib42)], and SLAM in dynamic environments [[43](#bib.bib43)]. We refer
    readers to these surveys for a better understanding of the conventional model
    based solutions. On the other hand, [[3](#bib.bib3)] has a discussion on the applications
    of deep learning to robotics research; however, its main focus is not on localization
    and mapping specifically, but a more general perspective towards the potentials
    and limits of deep learning in a broad context of robotics, including policy learning,
    reasoning and planning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有几篇综述论文广泛讨论了基于模型的定位和制图方法。SLAM问题的早期发展已在[[37](#bib.bib37)、[38](#bib.bib38)]中得到了很好的总结。开创性的综述[[39](#bib.bib39)]对现有的SLAM工作进行了彻底讨论，回顾了发展历程并规划了若干未来方向。尽管本文包含了一个简要讨论深度学习模型的章节，但并未全面概述该领域，特别是由于过去五年研究的爆炸性增长。其他SLAM综述论文仅关注SLAM系统的个别类型，包括SLAM的概率公式[[40](#bib.bib40)]、视觉里程计[[41](#bib.bib41)]、姿态图SLAM[[42](#bib.bib42)]和动态环境中的SLAM[[43](#bib.bib43)]。我们建议读者参考这些综述，以更好地理解传统的基于模型的解决方案。另一方面，[[3](#bib.bib3)]讨论了深度学习在机器人研究中的应用；然而，其主要关注点并非定位和制图，而是对深度学习在广泛机器人背景下的潜力和限制的更一般性看法，包括策略学习、推理和规划。
- en: Notably, although the problem of localization and mapping falls into the key
    notion of robotics, the incorporation of learning methods progresses in tandem
    with other research areas such as machine learning, computer vision and even natural
    language processing. This cross-disciplinary area thus imposes non-trivial difficulty
    when comprehensively summarizing related works into a survey paper. To the best
    of our knowledge, this is the first survey article that thoroughly and extensively
    covers existing work on deep learning for localization and mapping.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管定位和映射问题属于机器人技术的关键概念，但学习方法的结合与其他研究领域如机器学习、计算机视觉甚至自然语言处理同步发展。因此，这一跨学科领域在全面总结相关工作时带来了非同寻常的难度。据我们所知，这是第一篇全面而深入地涵盖深度学习在定位和映射中的现有工作的调查文章。
- en: 1.3 Survey Organization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 调查组织
- en: 'The remainder of the paper is organized as follows: Section 2 offers an overview
    and presents a taxonomy of existing deep learning based localization and mapping;
    Sections 3, 4, 5, 6 discuss the existing deep learning works on relative motion
    (odometry) estimation, mapping methods for geometric, semantic and general, global
    localization, and simultaneous localization and mapping with a focus on SLAM back-ends
    respectively; Open questions are summarized in Section 7 to discuss the limitations
    and future prospects of existing work; and finally Section 8 concludes the paper.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下：第2节提供了一个概述，并展示了现有基于深度学习的定位和映射的分类法；第3、4、5、6节分别讨论了现有的深度学习在相对运动（里程计）估计、几何、语义和一般的映射方法、全球定位以及以SLAM后端为重点的同时定位与映射方面的工作；第7节总结了未解决的问题，讨论现有工作的局限性和未来前景；最后，第8节对论文进行总结。
- en: 2 Taxonomy of Existing Approaches
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 现有方法的分类法
- en: 'We provide a new taxonomy of existing deep learning approaches, relevant to
    localization and mapping, to connect the fields of robotics, computer vision and
    machine learning. Broadly, they can be categorized into odometry estimation, mapping,
    global localization and SLAM, as illustrated by the taxonomy shown in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence"):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一种新的现有深度学习方法的分类法，相关于定位和映射，以连接机器人技术、计算机视觉和机器学习领域。大体上，它们可以被分类为里程计估计、映射、全球定位和SLAM，如图[2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ 深度学习在定位和映射中的应用调查：迈向空间机器智能时代")所示。
- en: 1) Odometry estimation concerns the calculation of the relative change in pose,
    in terms of translation and rotation, between two or more frames of sensor data.
    It continuously tracks self-motion, and is followed by a process to integrate
    these pose changes with respect to an initial state to derive global pose, in
    terms of position and orientation. This is widely known as the so-called dead
    reckoning solution. Odometry estimation can be used in providing pose information
    and as odometry motion model to assist the feedback loop of robot control. The
    key problem is to accurately estimate motion transformations from various sensor
    measurements. To this end, deep learning is applied to model the motion dynamics
    in an end-to-end fashion or extract useful features to support a pre-built system
    in a hybrid way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 里程计估计关注于计算两帧或更多传感器数据之间的相对位姿变化，包括平移和旋转。它持续追踪自我运动，接着将这些位姿变化与初始状态进行整合，以推导出全球位姿，包括位置和方向。这通常被称为所谓的航迹推算解决方案。里程计估计可以用于提供位姿信息，并作为里程计运动模型来辅助机器人控制的反馈回路。关键问题在于准确估计来自各种传感器测量的运动变换。为此，深度学习被应用于以端到端的方式建模运动动态或提取有用特征以支持预构建系统的混合方式。
- en: 2) Mapping builds and reconstructs a consistent model to describe the surrounding
    environment. Mapping can be used to provide environment information for human
    operators and high-level robot tasks, constrain the error drifts of odometry estimation,
    and retrieve the inquiry observation for global localization [[39](#bib.bib39)].
    Deep learning is leveraged as an useful tool to discover scene geometry and semantics
    from high-dimensional raw data for mapping. Deep learning based mapping methods
    are sub-divided into geometric, semantic, and general mapping, depending on whether
    the neural network learns the explicit geometry or semantics of a scene, or encodes
    the scene into an implicit neural representation respectively.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 映射构建并重建一个一致的模型来描述周围环境。映射可以用于提供人类操作员和高级机器人任务所需的环境信息，限制里程计估计的误差漂移，并检索查询观察以进行全局定位
    [[39](#bib.bib39)]。深度学习被用作发现高维原始数据中场景几何和语义的有用工具。基于深度学习的映射方法被细分为几何、语义和通用映射，这取决于神经网络是否学习场景的显式几何或语义，或者将场景编码为隐式神经表示。
- en: 3) Global localization retrieves the global pose of mobile agents in a known
    scene with prior knowledge. This is achieved by matching the inquiry input data
    with a pre-built 2D or 3D map, other spatial references, or a scene that has been
    visited before. It can be leveraged to reduce the pose drift of a dead reckoning
    system or solve the ’kidnapped robot’ problem[[40](#bib.bib40)]. Deep learning
    is used to tackle the tricky data association problem that is complicated by the
    changes in views, illumination, weather and scene dynamics, between the inquiry
    data and map.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 全局定位在已知场景中利用先验知识检索移动代理的全球姿态。这是通过将查询输入数据与预先构建的 2D 或 3D 地图、其他空间参考或先前访问过的场景进行匹配来实现的。它可以用来减少惯性导航系统的姿态漂移或解决“被绑架的机器人”问题[[40](#bib.bib40)]。深度学习用于解决因视角、照明、天气和场景动态变化而复杂的数据关联问题。
- en: '4) Simultaneous Localisation and Mapping (SLAM) integrates the aforementioned
    odometry estimation, global localization and mapping processes as front-ends,
    and jointly optimizes these modules to boost performance in both localization
    and mapping. Except these abovementioned modules, several other SLAM modules perform
    to ensure the consistency of the entire system as follows: *local optimization*
    ensures the local consistency of camera motion and scene geometry; *global optimization*
    aims to constrain the drift of global trajectories, and in a global scale; *keyframe
    detection* is used in keyframe-based SLAM to enable more efficient inference,
    while system error drifts can be mitigated by global optimization, once a loop
    closure is detected by *loop-closure detection*; *uncertainty estimation* provides
    a metric of belief in the learned poses and mapping, critical to probabilistic
    sensor fusion and back-end optimization in SLAM systems.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 同时定位与地图构建（SLAM）将上述的里程计估计、全局定位和映射过程整合为前端，并联合优化这些模块，以提高定位和映射的性能。除了这些上述模块，还有几个其他
    SLAM 模块执行以下操作，以确保整个系统的一致性：*局部优化* 确保相机运动和场景几何的一致性；*全局优化* 旨在约束全球轨迹的漂移，并在全球范围内；*关键帧检测*
    在基于关键帧的 SLAM 中用于实现更高效的推断，而系统误差漂移可以通过全局优化来缓解，一旦通过*回环检测* 检测到回环；*不确定性估计* 提供对学习姿态和映射的信任度度量，对于
    SLAM 系统中的概率传感器融合和后端优化至关重要。
- en: '![Refer to caption](img/21268a8de90a7b6d092f3fe1c6425fda.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/21268a8de90a7b6d092f3fe1c6425fda.png)'
- en: 'Figure 4: The typical structure of supervised learning of visual odometry,
    i.e. DeepVO [[24](#bib.bib24)] and unsupervised learning of visual odometry, i.e.
    SfmLearner [[29](#bib.bib29)].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：视觉里程计的监督学习的典型结构，即 DeepVO [[24](#bib.bib24)] 和视觉里程计的无监督学习，即 SfmLearner [[29](#bib.bib29)]。
- en: 'Despite the different design goals of individual components, the above components
    can be integrated into a spatial machine intelligence system (SMIS) to solve real-world
    challenges, allowing for robust operation, and long-term autonomy in the wild.
    A conceptual figure of such an integrated deep-learning based localization and
    mapping system is indicated in Figure [3](#S1.F3 "Figure 3 ‣ 1.1 Why to Study
    Deep Learning for Localization and Mapping ‣ 1 Introduction ‣ A Survey on Deep
    Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence"),
    showing the relationship of these components. In the following sections, we will
    discuss these components in details.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管各个组件的设计目标不同，但上述组件可以集成到空间机器智能系统（SMIS）中，以解决现实世界中的挑战，实现鲁棒操作和长期自主。图[3](#S1.F3
    "Figure 3 ‣ 1.1 Why to Study Deep Learning for Localization and Mapping ‣ 1 Introduction
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence")展示了这样一个集成的深度学习基础定位和映射系统的概念图，显示了这些组件之间的关系。在接下来的章节中，我们将详细讨论这些组件。'
- en: 3 Odometry Estimation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 里程计估计
- en: We begin with odometry estimation, which continuously tracks camera egomotion
    and produces relative poses. Global trajectories are reconstructed by integrating
    these relative poses, given an initial state, and thus it is critical to keep
    motion transformation estimates accurate enough to ensure high-prevision localization
    in a global scale. This section discusses deep learning approaches to achieve
    odometry estimation from various sensor data, that are fundamentally different
    in their data properties and application scenarios. The discussion mainly focuses
    on odometry estimation from visual, inertial and point-cloud data, as they are
    the common choices of sensing modalities on mobile agents.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从里程计估计开始，它持续跟踪相机自我运动并生成相对姿态。通过整合这些相对姿态，可以重建全局轨迹，给定初始状态，因此保持运动变换估计的准确性对于确保全球范围内的高精度定位至关重要。本节讨论了从各种传感器数据中实现里程计估计的深度学习方法，这些方法在数据属性和应用场景上存在根本性的不同。讨论主要集中在从视觉、惯性和点云数据中进行的里程计估计，因为这些是移动代理上的常见传感方式。
- en: 3.1 Visual Odometry
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 视觉里程计
- en: Visual odometry (VO) estimates the ego-motion of a camera, and integrates the
    relative motion between images into global poses. Deep learning methods are capable
    of extracting high-level feature representations from images, and thereby provide
    an alternative to solve the VO problem, without requiring hand-crafted feature
    extractors. Existing deep learning based VO models can be categorized into *end-to-end
    VO* and *hybrid VO*, depending on whether they are purely neural-network based
    or whether they are a combination of classical VO algorithms and deep neural networks.
    Depending on the availability of ground-truth labels in the training phase, end-to-end
    VO systems can be further classified into *supervised* VO and *unsupervised* VO.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉里程计（VO）估计相机的自我运动，并将图像之间的相对运动整合为全局姿态。深度学习方法能够从图像中提取高级特征表示，从而提供解决VO问题的替代方案，而无需手工设计特征提取器。现有的基于深度学习的VO模型可以分为*端到端VO*和*混合VO*，具体取决于它们是否完全基于神经网络，还是经典VO算法和深度神经网络的结合。根据训练阶段是否有地面真实标签，端到端VO系统可以进一步分类为*有监督*VO和*无监督*VO。
- en: 3.1.1 Supervised Learning of VO
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 有监督VO学习
- en: We start with the introduction of supervised VO, one of the most predominant
    approaches to learning-based odometry, by training a deep neural network model
    on labelled datasets to construct a mapping function from consecutive images to
    motion transformations directly, instead of exploiting the geometric structures
    of images as in conventional VO systems[[41](#bib.bib41)]. At its most basic,
    the input of deep neural network is a pair of consecutive images, and the output
    is the estimated translation and rotation between two frames of images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍有监督VO，这是学习型里程计中最主要的方法之一，通过在标记数据集上训练深度神经网络模型，直接构建从连续图像到运动变换的映射函数，而不是像传统VO系统那样利用图像的几何结构[[41](#bib.bib41)]。最基本的，深度神经网络的输入是一对连续图像，输出是两帧图像之间的估计平移和旋转。
- en: One of the first works in this area was Konda et al. [[44](#bib.bib44)]. This
    approach formulates visual odometry as a classification problem, and predicts
    the discrete changes of direction and velocity from input images using a convolutional
    neural network (ConvNet). Costante et al. [[45](#bib.bib45)] used a ConvNet to
    extract visual features from dense optical flow, and based on these visual features
    to output frame-to-frame motion estimation. Nonetheless, these two works have
    not achieved end-to-end learning from images to motion estimates, and their performance
    is still limited.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域的首批研究之一是Konda等人[[44](#bib.bib44)]。这种方法将视觉里程计公式化为一个分类问题，并使用卷积神经网络（ConvNet）从输入图像中预测方向和速度的离散变化。Costante等人[[45](#bib.bib45)]使用ConvNet从稠密光流中提取视觉特征，并基于这些视觉特征输出逐帧运动估计。然而，这两项工作尚未实现从图像到运动估计的端到端学习，它们的性能仍然有限。
- en: 'DeepVO [[24](#bib.bib24)] utilizes a combination of convolutional neural network
    (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of
    visual odometry. The framework of DeepVO becomes a typical choice in realizing
    supervised learning of VO, due to its specialization in end-to-end learning. Figure
    [4](#S2.F4 "Figure 4 ‣ 2 Taxonomy of Existing Approaches ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    (a) shows the architecture of this RNN+ConvNet based VO system, which extracts
    visual features from pairs of images via a ConvNet, and passes features through
    RNNs to model the temporal correlation of features. Its ConvNet encoder is based
    on a FlowNet structure to extract visual features suitable for optical flow and
    self-motion estimation. Using a FlowNet based encoder can be regarded as introducing
    the prior knowledge of optical flow into the learning process, and potentially
    prevents DeepVO from being overfitted to the training datasets. The reccurent
    model summarizes the history information into its hidden states, so that the output
    is inferred from both past experience and current ConvNet features from sensor
    observation. It is trained on large-scale datasets with groundtruthed camera poses
    as labels. To recover the optimal parameters $\bm{\theta}^{*}$ of framework, the
    optimization target is to minimize the Mean Square Error (MSE) of the estimated
    translations $\mathbf{\hat{p}}\in\mathbb{R}^{3}$ and euler angle based rotations
    $\hat{\bm{\varphi}}\in\mathbb{R}^{3}$:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepVO [[24](#bib.bib24)] 利用卷积神经网络（ConvNet）和递归神经网络（RNN）的组合，实现视觉里程计的端到端学习。由于其在端到端学习中的专长，DeepVO框架成为实现VO监督学习的典型选择。图[4](#S2.F4
    "Figure 4 ‣ 2 Taxonomy of Existing Approaches ‣ A Survey on Deep Learning for
    Localization and Mapping: Towards the Age of Spatial Machine Intelligence") (a)展示了基于RNN+ConvNet的VO系统的架构，该系统通过ConvNet从图像对中提取视觉特征，并通过RNN对特征的时间相关性进行建模。其ConvNet编码器基于FlowNet结构，以提取适合光流和自运动估计的视觉特征。使用基于FlowNet的编码器可以看作是将光流的先验知识引入学习过程，从而可能防止DeepVO过度拟合训练数据集。递归模型将历史信息总结到其隐藏状态中，使输出可以从过去经验和当前的ConvNet特征中推断。它在大规模数据集上进行训练，标签为地面真实的相机姿态。为了恢复框架的最优参数$\bm{\theta}^{*}$，优化目标是最小化估计平移$\mathbf{\hat{p}}\in\mathbb{R}^{3}$和基于欧拉角的旋转$\hat{\bm{\varphi}}\in\mathbb{R}^{3}$的均方误差（MSE）： '
- en: '|  | $\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{t=1}^{T}\&#124;\hat{\mathbf{p}}_{t}-\mathbf{p}_{t}\&#124;_{2}^{2}+\&#124;\hat{\bm{\varphi}}_{t}-\bm{\varphi}_{t}\&#124;_{2}^{2},$
    |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{t=1}^{T}\&#124;\hat{\mathbf{p}}_{t}-\mathbf{p}_{t}\&#124;_{2}^{2}+\&#124;\hat{\bm{\varphi}}_{t}-\bm{\varphi}_{t}\&#124;_{2}^{2},$
    |  | (1) |'
- en: where $(\hat{\mathbf{p}}_{t},\hat{\bm{\varphi}}_{t})$ are the estimates of relative
    pose at the timestep $t$, $(\mathbf{p},\bm{\varphi})$ are the corresponding groundtruth
    values, $\bm{\theta}$ are the parameters of the DNN framework, $N$ is the number
    of samples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$(\hat{\mathbf{p}}_{t},\hat{\bm{\varphi}}_{t})$是时刻$t$的相对姿态估计，$(\mathbf{p},\bm{\varphi})$是相应的真实值，$\bm{\theta}$是DNN框架的参数，$N$是样本数量。
- en: DeepVO reports impressive results on estimating the pose of driving vehicles,
    even in previously unseen scenarios. In the experiment on the KITTI odometry dataset[[46](#bib.bib46)],
    this data-driven solution outperforms conventional representative monocular VO,
    e.g. VISO2[[47](#bib.bib47)] and ORB-SLAM (without loop closure) [[21](#bib.bib21)].
    Another advantage is that supervised VO naturally produces trajectory with the
    absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous
    using only monocular information. This is because deep neural network can implicitly
    learn and maintain the global scale from large collection of images, which can
    be viewed as learning from past experience to predict current scale metric.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DeepVO在估计驾驶车辆的姿态方面报告了令人印象深刻的结果，即使在以前未见过的场景中也是如此。在KITTI测程数据集的实验中[[46](#bib.bib46)]，这种数据驱动的解决方案优于传统的代表性单目视觉里程计，例如VISO2[[47](#bib.bib47)]和ORB-SLAM（不带回环检测）[[21](#bib.bib21)]。另一个优势是有监督的视觉里程计自然会生成具有绝对尺度的轨迹，而经典的视觉里程计算法仅使用单目信息时尺度模糊。这是因为深度神经网络可以从大量图像中隐式学习和保持全局尺度，这可以被视为从过去的经验中学习以预测当前的尺度度量。
- en: Based on this typical model of supervised VO, a number of works further extended
    this approach to improve the model performance. To improve the generalization
    ability of supervised VO, [[48](#bib.bib48)] incorporates curriculum learning
    (i.e. the model is trained by increasing the data complexity) and geometric loss
    constraints. Knowledge distillation (i.e. a large model is compressed by teaching
    a smaller one) is applied into the supervised VO framework to greatly reduce the
    number of network parameters, making it more amenable for real-time operation
    on mobile devices [[49](#bib.bib49)]. Furthermore, Xue et al. [[50](#bib.bib50)]
    introduced a memory module that stores global information, and a refining module
    that improves pose estimates with the preserved contextual information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种典型的有监督视觉里程计模型，一些研究进一步扩展了这种方法以提高模型性能。为了提高有监督视觉里程计的泛化能力，[[48](#bib.bib48)]引入了课程学习（即通过增加数据复杂性训练模型）和几何损失约束。知识蒸馏（即通过教一个较小的模型来压缩一个较大的模型）被应用于有监督视觉里程计框架中，以大大减少网络参数的数量，使其更适合在移动设备上实时操作[[49](#bib.bib49)]。此外，Xue等人[[50](#bib.bib50)]引入了一个存储全局信息的记忆模块和一个利用保留的上下文信息改进姿态估计的精炼模块。
- en: In summary, these end-to-end learning methods benefit from recent advances in
    machine learning techniques and computational power, to automatically learn pose
    transformations directly from raw images that can tackle challenging real-world
    odometry estimation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些端到端学习方法利用了近期在机器学习技术和计算能力方面的进展，能够直接从原始图像中自动学习姿态变换，从而应对具有挑战性的实际世界测程估计问题。
- en: 3.1.2 Unsupervised Learning of VO
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 无监督视觉里程计学习
- en: There is growing interest in exploring unsupervised learning of VO. Unsupervised
    solutions are capable of exploiting unlabelled sensor data, and thus it saves
    human effort on labelling data, and has better adaptation and generalization ability
    in new scenarios, where no labelled data are available. This has been achieved
    in a self-supervised framework that jointly learns depth and camera ego-motion
    from video sequences, by utilizing view synthesis as a supervisory signal[[29](#bib.bib29)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对无监督视觉里程计学习的探索越来越受到关注。无监督解决方案能够利用未标记的传感器数据，从而节省了标记数据的人工工作，并且在没有标记数据的新场景中具有更好的适应性和泛化能力。这在一个自我监督的框架中实现了，该框架通过利用视图合成作为监督信号[[29](#bib.bib29)]，联合学习深度和相机自运动。
- en: 'As shown in Figure [4](#S2.F4 "Figure 4 ‣ 2 Taxonomy of Existing Approaches
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence") (b), a typical unsupervised VO solution consists of a depth
    network to predict depth maps, and a pose network to produce motion transformations
    between images. The entire framework takes consecutive images as input, and the
    supervision signal is based on novel view synthesis - given a source image $\mathbf{I}_{s}$,
    the view synthesis task is to generate a synthetic target image $\mathbf{I}_{t}$.
    A pixel of source image $\mathbf{I}_{s}(p_{s})$ is projected onto a target view
    $\mathbf{I}_{t}(p_{t})$ via:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4](#S2.F4 "图 4 ‣ 2 现有方法的分类 ‣ 深度学习在定位和映射中的应用：迈向空间机器智能时代") (b) 所示，一个典型的无监督
    VO 解决方案包括一个深度网络用于预测深度图，以及一个姿态网络用于产生图像之间的运动变换。整个框架以连续图像作为输入，监督信号基于新视图合成——给定源图像
    $\mathbf{I}_{s}$，视图合成任务是生成合成目标图像 $\mathbf{I}_{t}$。源图像 $\mathbf{I}_{s}(p_{s})$
    的一个像素通过以下方式投影到目标视图 $\mathbf{I}_{t}(p_{t})$ 上：
- en: '|  | $p_{s}\sim\mathbf{K}\mathbf{T}_{t\to s}\mathbf{D}_{t}(p_{t})\mathbf{K}^{-1}p_{t}$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{s}\sim\mathbf{K}\mathbf{T}_{t\to s}\mathbf{D}_{t}(p_{t})\mathbf{K}^{-1}p_{t}$
    |  | (2) |'
- en: 'where $\mathbf{K}$ is the camera’s intrinsic matrix, $\mathbf{T}_{t\to s}$
    denotes the camera motion matrix from target frame to source frame, and $\mathbf{D}_{t}(p_{t})$
    denotes the per-pixel depth maps in the target frame. The training objective is
    to ensure the consistency of the scene geometry by optimizing the photometric
    reconstruction loss between the real target image and the synthetic one:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{K}$ 是相机的内参矩阵，$\mathbf{T}_{t\to s}$ 表示从目标帧到源帧的相机运动矩阵，$\mathbf{D}_{t}(p_{t})$
    表示目标帧中每个像素的深度图。训练目标是通过优化真实目标图像与合成图像之间的光度重建损失来确保场景几何的一致性：
- en: '|  | $\mathcal{L}_{\text{photo}}=\sum_{<\mathbf{I}_{1},...,\mathbf{I}_{N}>\in
    S}\sum_{p}&#124;\mathbf{I}_{t}(p)-\hat{\mathbf{I}}_{s}(p)&#124;,$ |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{photo}}=\sum_{<\mathbf{I}_{1},...,\mathbf{I}_{N}>\in
    S}\sum_{p}&#124;\mathbf{I}_{t}(p)-\hat{\mathbf{I}}_{s}(p)&#124;,$ |  | (3) |'
- en: where p denotes pixel coordinates, $\mathbf{I}_{t}$ is the target image, and
    $\hat{\mathbf{I}}_{s}$ is the synthetic target image generated from the source
    image $\mathbf{I}_{s}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 p 表示像素坐标，$\mathbf{I}_{t}$ 是目标图像，$\hat{\mathbf{I}}_{s}$ 是从源图像 $\mathbf{I}_{s}$
    生成的合成目标图像。
- en: 'TABLE I: A summary of existing methods on deep learning for odometry estimation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：现有的深度学习里程估计方法的总结。
- en: '| Model | Sensor | Supervision | Scale | Performance | Contributions |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 传感器 | 监督 | 比例 | 性能 | 贡献 |'
- en: '| Seq09 | Seq10 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Seq09 | Seq10 |'
- en: '| VO | Konda et al.[[44](#bib.bib44)] | MC | Supervised | Yes | - | - | formulate
    VO as a classification problem |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| VO | Konda 等人[[44](#bib.bib44)] | MC | 有监督 | 是 | - | - | 将 VO 表述为分类问题 |'
- en: '| Costante et al.[[45](#bib.bib45)] | MC | Supervised | Yes | 6.75 | 21.23
    | extract features from optical flow for VO estimates |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Costante 等人[[45](#bib.bib45)] | MC | 有监督 | 是 | 6.75 | 21.23 | 从光流中提取特征用于
    VO 估计 |'
- en: '| Backprop KF[[51](#bib.bib51)] | MC | Hybrid | Yes | - | - | a differentiable
    Kalman filter based VO |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Backprop KF[[51](#bib.bib51)] | MC | 混合 | 是 | - | - | 基于可微分 Kalman 滤波器的 VO
    |'
- en: '| DeepVO[[24](#bib.bib24)] | MC | Supervised | Yes | - | 8.11 | combine RNN
    and ConvNet for end-to-end learning |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| DeepVO[[24](#bib.bib24)] | MC | 有监督 | 是 | - | 8.11 | 结合 RNN 和 ConvNet 进行端到端学习
    |'
- en: '| SfmLearner[[29](#bib.bib29)] | MC | Unsupervised | No | 17.84 | 37.91 | novel
    view synthesis for self-supervised learning |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| SfmLearner[[29](#bib.bib29)] | MC | 无监督 | 否 | 17.84 | 37.91 | 用于自监督学习的新视图合成
    |'
- en: '| Yin et al.[[52](#bib.bib52)] | MC | Hybrid | Yes | 4.14 | 1.70 | introduce
    learned depth to recover scale metric |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Yin 等人[[52](#bib.bib52)] | MC | 混合 | 是 | 4.14 | 1.70 | 引入学习的深度以恢复尺度度量 |'
- en: '| UnDeepVO[[53](#bib.bib53)] | SC | Unsupervised | Yes | 7.01 | 10.63 | use
    fixed stereo line to recover scale metric |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| UnDeepVO[[53](#bib.bib53)] | SC | 无监督 | 是 | 7.01 | 10.63 | 使用固定立体线来恢复尺度度量
    |'
- en: '| Barnes et al.[[54](#bib.bib54)] | MC | Hybrid | Yes | - | - | integrate learned
    depth and ephemeral masks |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Barnes 等人[[54](#bib.bib54)] | MC | 混合 | 是 | - | - | 集成了学习的深度和瞬态掩码 |'
- en: '| GeoNet[[55](#bib.bib55)] | MC | Unsupervised | No | 43.76 | 35.6 | geometric
    consistency loss and 2D flow generator |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| GeoNet[[55](#bib.bib55)] | MC | 无监督 | 否 | 43.76 | 35.6 | 几何一致性损失和 2D 流生成器
    |'
- en: '| Zhan et al.[[56](#bib.bib56)] | SC | Unsupervised | No | 11.92 | 12.45 |
    use fixed stereo line for scale recovery |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Zhan 等人[[56](#bib.bib56)] | SC | 无监督 | 否 | 11.92 | 12.45 | 使用固定立体线进行尺度恢复
    |'
- en: '| DPF[[57](#bib.bib57)] | MC | Hybrid | Yes | - | - | a differentiable particle
    filter based VO |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| DPF[[57](#bib.bib57)] | MC | 混合 | 是 | - | - | 基于可微分粒子滤波器的 VO |'
- en: '| Yang et al.[[58](#bib.bib58)] | MC | Hybrid | Yes | 0.83 | 0.74 | use learned
    depth into classical VO |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人[[58](#bib.bib58)] | MC | 混合 | 是 | 0.83 | 0.74 | 将学习的深度应用于经典VO |'
- en: '| Zhao et al.[[59](#bib.bib59)] | MC | Supervised | Yes | - | 4.38 | generate
    dense 3D flow for VO and mapping |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等人[[59](#bib.bib59)] | MC | 监督 | 是 | - | 4.38 | 生成密集的3D流用于VO和地图 |'
- en: '| Struct2Depth[[60](#bib.bib60)] | MC | Unsupervised | No | 10.2 | 28.9 | introduce
    3D geometry structure during learning |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Struct2Depth[[60](#bib.bib60)] | MC | 无监督 | 否 | 10.2 | 28.9 | 在学习过程中引入3D几何结构
    |'
- en: '| Saputra et al.[[48](#bib.bib48)] | MC | Supervised | Yes | - | 8.29 | curriculum
    learning and geometric loss constraints |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Saputra 等人[[48](#bib.bib48)] | MC | 监督 | 是 | - | 8.29 | 课程学习与几何损失约束 |'
- en: '| GANVO[[61](#bib.bib61)] | MC | Unsupervised | No | - | - | adversarial learning
    to generate depth |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GANVO[[61](#bib.bib61)] | MC | 无监督 | 否 | - | - | 对抗学习生成深度 |'
- en: '| CNN-SVO[[62](#bib.bib62)] | MC | Hybrid | Yes | 10.69 | 4.84 | use learned
    depth to initialize SVO |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| CNN-SVO[[62](#bib.bib62)] | MC | 混合 | 是 | 10.69 | 4.84 | 使用学习的深度初始化SVO |'
- en: '| Xue et al.[[50](#bib.bib50)] | MC | Supervised | Yes | - | 3.47 | memory
    and refinement module |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Xue 等人[[50](#bib.bib50)] | MC | 监督 | 是 | - | 3.47 | 记忆和优化模块 |'
- en: '| Wang et al.[[63](#bib.bib63)] | MC | Unsupervised | Yes | 9.30 | 7.21 | integrate
    RNN and flow consistency constraint |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人[[63](#bib.bib63)] | MC | 无监督 | 是 | 9.30 | 7.21 | 集成RNN和流一致性约束 |'
- en: '| Li et al.[[64](#bib.bib64)] | MC | Unsupervised | No | - | - | global optimization
    for pose graph |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人[[64](#bib.bib64)] | MC | 无监督 | 否 | - | - | 姿态图的全局优化 |'
- en: '| Saputra et al.[[49](#bib.bib49)] | MC | Supervised | Yes | - | - | knowledge
    distilling to compress deep VO model |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Saputra 等人[[49](#bib.bib49)] | MC | 监督 | 是 | - | - | 知识蒸馏以压缩深度VO模型 |'
- en: '| Gordon[[65](#bib.bib65)] | MC | Unsupervised | No | 2.7 | 6.8 | camera matrix
    learning |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Gordon[[65](#bib.bib65)] | MC | 无监督 | 否 | 2.7 | 6.8 | 相机矩阵学习 |'
- en: '| Koumis et al.[[66](#bib.bib66)] | MC | Supervised | Yes | - | - | 3D convolutional
    networks |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Koumis 等人[[66](#bib.bib66)] | MC | 监督 | 是 | - | - | 3D卷积网络 |'
- en: '| Bian et al.[[30](#bib.bib30)] | MC | Unsupervised | Yes | 11.2 | 10.1 | scale
    recovery from only monocular images |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Bian 等人[[30](#bib.bib30)] | MC | 无监督 | 是 | 11.2 | 10.1 | 从单目图像中恢复尺度 |'
- en: '| Zhan et al.[[67](#bib.bib67)] | MC | Hybrid | Yes | 2.61 | 2.29 | integrate
    learned optical flow and depth |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Zhan 等人[[67](#bib.bib67)] | MC | 混合 | 是 | 2.61 | 2.29 | 整合学习的光流和深度 |'
- en: '| D3VO[[25](#bib.bib25)] | MC | Hybrid | Yes | 0.78 | 0.62 | integrate learned
    depth, uncertainty and pose |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| D3VO[[25](#bib.bib25)] | MC | 混合 | 是 | 0.78 | 0.62 | 整合学习的深度、不确定性和姿态 |'
- en: '| VIO | VINet[[68](#bib.bib68)] | MC+I | Supervised | Yes | - | - | formulate
    VIO as a sequential learning problem |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| VIO | VINet[[68](#bib.bib68)] | MC+I | 监督 | 是 | - | - | 将VIO表述为顺序学习问题 |'
- en: '| VIOLearner[[69](#bib.bib69)] | MC+I | Unsupervised | Yes | 1.51 | 2.04 |
    online correction module |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| VIOLearner[[69](#bib.bib69)] | MC+I | 无监督 | 是 | 1.51 | 2.04 | 在线校正模块 |'
- en: '| Chen et al.[[70](#bib.bib70)] | MC+I | Supervised | Yes | - | - | feature
    selection for deep sensor fusion |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人[[70](#bib.bib70)] | MC+I | 监督 | 是 | - | - | 深度传感器融合的特征选择 |'
- en: '| DeepVIO[[71](#bib.bib71)] | SC+I | Unsupervised | Yes | 0.85 | 1.03 | learn
    VIO from stereo images and IMU |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| DeepVIO[[71](#bib.bib71)] | SC+I | 无监督 | 是 | 0.85 | 1.03 | 从立体图像和IMU中学习VIO
    |'
- en: '| LO | Velas et al.[[72](#bib.bib72)] | L | Supervised | Yes | 4.94 | 3.27
    | ConvNet to estimate odometry from point clouds |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LO | Velas 等人[[72](#bib.bib72)] | L | 监督 | 是 | 4.94 | 3.27 | ConvNet从点云中估计里程计
    |'
- en: '| LO-Net[[73](#bib.bib73)] | L | Supervised | Yes | 1.37 | 1.80 | geometric
    constraint loss |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LO-Net[[73](#bib.bib73)] | L | 监督 | 是 | 1.37 | 1.80 | 几何约束损失 |'
- en: '| DeepPCO[[74](#bib.bib74)] | L | Supervised | Yes | - | - | parallel neural
    network |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| DeepPCO[[74](#bib.bib74)] | L | 监督 | 是 | - | - | 并行神经网络 |'
- en: '| Valente et al.[[75](#bib.bib75)] | MC+L | Supervised | Yes | - | 7.60 | sensor
    fusion for LIDAR and camara |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Valente 等人[[75](#bib.bib75)] | MC+L | 监督 | 是 | - | 7.60 | LIDAR和相机的传感器融合
    |'
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model: VO, VIO and LO represent visual odometry, visual-inertial odometry and
    LIDAR odometry respectively.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型：VO、VIO 和 LO 分别代表视觉里程计、视觉惯性里程计和LIDAR里程计。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sensor: MC, SC, I and L represent monocular camera, stereo camera, inertial
    measurement unit, and LIDAR respectively.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传感器：MC、SC、I 和 L 分别代表单目相机、立体相机、惯性测量单元和LIDAR。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Supervision represents whether this work is a purely neural network based model
    trained with groundtruth labels (Supervised) or without labels (Unsupervised),
    or it is a combination of classical and deep neural network (Hybrid)
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监督表示该工作是否是一个纯粹的基于神经网络的模型，使用地面真实标签（监督）进行训练，或没有标签（无监督），或者是经典和深度神经网络的组合（混合）
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scale indicates whether a trajectory with a global scale can be produced.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规模表示是否可以生成具有全球尺度的轨迹。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance reports the localization error (a small number is better), i.e.
    the averaged translational RMSE drift (%) on lengths of 100m-800m on the KITTI
    odometry dataset[[46](#bib.bib46)]. Most works were evaluated on the Sequence
    09 and 10, and thus we took the results on these two sequences from their original
    papers for a performance comparison. Note that the training sets may be different
    in each work.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能报告定位误差（数值越小越好），即KITTI里程数据集上长度为100m-800m的平均平移RMSE漂移（%）[[46](#bib.bib46)]。大多数工作在序列09和10上进行了评估，因此我们从其原始论文中提取了这两个序列的结果进行性能比较。注意，不同的工作中的训练集可能有所不同。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献总结了每项工作相对于以往研究的主要贡献。
- en: 'However, there are basically two main problems that remained unsolved in the
    original work[[29](#bib.bib29)]: 1) this monocular image based approach is not
    able to provide pose estimates in a consistent global scale. Due to the scale
    ambiguity, no physically meaningful global trajectory can be reconstructed, limiting
    its real use. 2) The photometric loss assumes that the scene is static and without
    camera occlusions. Although the authors proposed the use of an explainability
    mask to remove scene dynamics, the influence of these environmental factors is
    still not addressed completely, which violates the assumption. To address these
    concerns, an increasing number of works [[53](#bib.bib53), [55](#bib.bib55), [56](#bib.bib56),
    [58](#bib.bib58), [59](#bib.bib59), [61](#bib.bib61), [64](#bib.bib64), [76](#bib.bib76),
    [77](#bib.bib77)] extended this unsupervised framework to achieve better performance.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，原始工作[[29](#bib.bib29)]中基本存在两个主要问题未解决：1）基于单目图像的方法无法提供一致的全球尺度姿态估计。由于尺度模糊，无法重建具有物理意义的全球轨迹，从而限制了其实际应用。2）光度损失假设场景是静态的且没有相机遮挡。虽然作者提出了使用可解释性掩膜去除场景动态，但这些环境因素的影响仍未完全解决，这违反了假设。为了解决这些问题，越来越多的工作[[53](#bib.bib53),
    [55](#bib.bib55), [56](#bib.bib56), [58](#bib.bib58), [59](#bib.bib59), [61](#bib.bib61),
    [64](#bib.bib64), [76](#bib.bib76), [77](#bib.bib77)]扩展了这一无监督框架以实现更好的性能。
- en: To solve the global scale problem, [[53](#bib.bib53), [56](#bib.bib56)] proposed
    to utilize stereo image pairs to recover the absolute scale of pose estimation.
    They introduced an additional spatial photometric loss between the left and right
    pairs of images, as the stereo baseline (i.e. motion transformation between the
    left and right images) is fixed and known throughout the dataset. Once the training
    is complete, the network produces pose predictions using only monocular images.
    Thus, although it is unsupervised in the context of not having access to ground-truth,
    the training dataset (stereo) is different to the test set (mono). [[30](#bib.bib30)]
    tackles the scale issue by introducing a geometric consistency loss, that enforces
    the consistency between predicted depth maps and reconstructed depth maps. The
    framework transforms the predicted depth maps into a 3D space, and projects them
    back to produce reconstructed depth maps. In doing so, the depth predictions are
    able to remain scale-consistent over consecutive frames, enabling pose estimates
    to be scale-consistent meanwhile.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决全球尺度问题，[[53](#bib.bib53), [56](#bib.bib56)]提出利用立体图像对来恢复姿态估计的绝对尺度。他们引入了左右图像对之间的额外空间光度损失，因为立体基线（即左右图像之间的运动变换）在整个数据集中是固定且已知的。一旦训练完成，网络仅使用单目图像进行姿态预测。因此，虽然在没有真实数据的情况下这是无监督的，但训练数据集（立体）与测试集（单目）不同。[[30](#bib.bib30)]通过引入几何一致性损失来解决尺度问题，该损失强制执行预测深度图与重建深度图之间的一致性。该框架将预测深度图转换为3D空间，并将其投影回去生成重建深度图。通过这种方式，深度预测能够在连续帧上保持尺度一致，从而使姿态估计也保持尺度一致。
- en: The photometric consistency constraint assumes that the entire scenario consists
    only of rigid static structures, e.g. buildings and lanes. However, in real-world
    applications, environmental dynamics (e.g. pedestrians and vehicles), will distort
    the photometric projection and degrade the accuracy of pose estimation. To address
    this concern, GeoNet [[55](#bib.bib55)] divides its learning process into two
    sub-tasks by estimating static scene structures and motion dynamics separately
    through a rigid structure reconstructor and a non-rigid motion localizer. In addition,
    GeoNet enforces a geometric consistency loss to mitigate the issues caused by
    camera occlusions and non-Lambertian surfaces. [[59](#bib.bib59)] adds a 2D flow
    generator along with a depth network to generate 3D flow. Benefiting from better
    3D understanding of environment, their framework is able to produce more accurate
    camera pose, along with a point cloud map. GANVO [[61](#bib.bib61)] employs a
    generative adversarial learning paradigm for depth generation, and introduces
    a temporal recurrent module for pose regression. Li et al. [[76](#bib.bib76)]
    also utilized a generative adversarial network (GAN) to generate more realistic
    depth maps and poses, and further encourage more accurate synthetic images in
    the target frame. Instead of a hand-crafted metric, a discriminator is adopted
    to evaluate the quality of synthetic images generation. In doing so, the generative
    adversarial setup facilitates the generated depth maps to be more texture-rich
    and crisper. In this way, high-level scene perception and representation are accurately
    captured and environmental dynamics are implicitly tolerated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 光度一致性约束假设整个场景仅由刚性静态结构（如建筑物和车道）构成。然而，在实际应用中，环境动态（如行人和车辆）将扭曲光度投影，降低位姿估计的准确性。为了解决这一问题，GeoNet
    [[55](#bib.bib55)] 将学习过程分为两个子任务，通过刚性结构重建器和非刚性运动定位器分别估计静态场景结构和运动动态。此外，GeoNet 强制执行几何一致性损失，以减轻由相机遮挡和非朗伯表面造成的问题。[[59](#bib.bib59)]
    添加了一个 2D 流生成器以及一个深度网络，以生成 3D 流。凭借对环境更好的 3D 理解，他们的框架能够生成更准确的相机位姿，以及一个点云地图。GANVO
    [[61](#bib.bib61)] 采用生成对抗学习范式来生成深度，并引入了一个时间递归模块来进行位姿回归。Li 等人 [[76](#bib.bib76)]
    还利用生成对抗网络（GAN）生成更逼真的深度图和位姿，并进一步鼓励目标帧中更准确的合成图像。与手工制作的度量标准不同，采用了一个判别器来评估合成图像生成的质量。这样，生成对抗设置有助于生成的深度图更富有纹理和更清晰。通过这种方式，高级场景感知和表示得以准确捕捉，环境动态也被隐式地容忍。
- en: 'Although unsupervised VO still cannot compete with supervised VO in performance,
    as illustrated in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Inertial Odometry ‣ 3 Odometry
    Estimation ‣ A Survey on Deep Learning for Localization and Mapping: Towards the
    Age of Spatial Machine Intelligence"), its concerns of scale metric and scene
    dynamics problem have been largely resolved. With the benefits of self-supervised
    learning, and ever-increasing improvement on performance, unsupervised VO would
    be a promising solution in providing pose information, and tightly coupled with
    other modules in spatial machine intelligence system.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督视觉里程计（VO）的性能仍无法与有监督视觉里程计相媲美，如图[5](#S3.F5 "图 5 ‣ 3.3 惯性里程计 ‣ 3 里程计估计 ‣ 深度学习在定位与地图构建中的应用：迈向空间机器智能时代")所示，但其在尺度度量和场景动态问题上的顾虑已在很大程度上得到解决。凭借自监督学习的优势以及性能的持续提升，无监督VO将在提供位姿信息方面成为一个有前途的解决方案，并与空间机器智能系统中的其他模块紧密结合。
- en: 3.1.3 Hybrid VO
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 混合视觉里程计
- en: Unlike end-to-end VO that only relies on a deep neural network to interpret
    pose from data, hybrid VO integrates classical geometric models with deep learning
    framework. Based on mature geometric theory, they use a deep neural network to
    expressively replace parts of a geometry model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅依赖深度神经网络从数据中解释位姿的端到端视觉里程计不同，混合视觉里程计将经典几何模型与深度学习框架相结合。基于成熟的几何理论，它们利用深度神经网络来表达性地替代几何模型的部分。
- en: A straightforward way is to incorporate the learned depth estimates into a conventional
    visual odometry algorithm to recover the absolute scale metric of poses [[52](#bib.bib52)].
    Learning depth estimation is a well-researched area in the computer vision community.
    For example, [[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]
    provide per-pixel depths in a global scale by employing a trained deep neural
    model. Thus the so-called scale problem of conventional VO is mitigated. Barnes
    et al. [[54](#bib.bib54)] utilize both the predicted depth maps and ephemeral
    masks (i.e. the area of moving objects) into a VO system to improve its robustness
    to moving objects. Zhan et al. [[67](#bib.bib67)] integrate the learned depth
    and optical flow predictions into a conventional visual odometry model, achieving
    competitive performance over other baselines. Other works combine physical motion
    models with deep neural network e.g. via a differentiable Kalman filter [[82](#bib.bib82)],
    and a particle filter [[83](#bib.bib83)]. The physical model serves as an algorithmic
    prior in the learning process. Furthermore, D3VO [[25](#bib.bib25)] incorporates
    the deep predictions of depth, pose, and uncertainty into a direct visual odometry.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接的方法是将学习到的深度估计融入传统的视觉里程计算法，以恢复姿态的绝对尺度度量[[52](#bib.bib52)]。学习深度估计是计算机视觉领域的一个广泛研究的领域。例如，[[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]通过使用训练好的深度神经网络模型提供了全局尺度的每像素深度。因此，传统视觉里程计的所谓尺度问题得到了缓解。Barnes等人[[54](#bib.bib54)]将预测的深度图和短暂的掩码（即运动物体的区域）结合到视觉里程计系统中，以提高其对运动物体的鲁棒性。Zhan等人[[67](#bib.bib67)]将学习的深度和光流预测整合到传统的视觉里程计模型中，在其他基线方法上实现了具有竞争力的性能。其他工作结合了物理运动模型和深度神经网络，例如通过可微分的卡尔曼滤波器[[82](#bib.bib82)]和粒子滤波器[[83](#bib.bib83)]。物理模型作为学习过程中的算法先验。此外，D3VO[[25](#bib.bib25)]将深度、姿态和不确定性的深度预测整合到直接视觉里程计中。
- en: Combining the benefits from both geometric theory and deep learning, hybrid
    models are normally more accurate than end-to-end VO at this stage, as shown in
    Table 1. It is notable that hybrid models even outperform the state-of-the-art
    conventional monocular VO or visual-inertial odometry (VIO) systems on common
    benchmarks, for example, D3VO[[25](#bib.bib25)] defeats several popular conventional
    VO/VIO systems, such as DSO[[84](#bib.bib84)], ORB-SLAM[[21](#bib.bib21)], VINS-Mono[[15](#bib.bib15)].
    This demonstrates the rapid rate of progress in this area.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结合几何理论和深度学习的优势，混合模型通常比端到端的视觉里程计更准确，如表1所示。值得注意的是，混合模型甚至在常见基准测试中超越了最先进的传统单目视觉里程计或视觉-惯性里程计（VIO）系统，例如，D3VO[[25](#bib.bib25)]击败了几个流行的传统视觉里程计/VIO系统，如DSO[[84](#bib.bib84)]、ORB-SLAM[[21](#bib.bib21)]、VINS-Mono[[15](#bib.bib15)]。这表明该领域的进展速度非常快。
- en: 3.2 Visual-Inertial Odometry
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 视觉-惯性里程计
- en: 'Integrating visual and inertial data as visual-inertial odometry (VIO) is a
    well-defined problem in mobile robotics. Both cameras and inertial sensors are
    relatively low-cost, power-efficient and widely deployed. These two sensors are
    complementary: monocular cameras capture the appearance and structure of a 3D
    scene, while they are scale-ambiguous, and not robust to challenging scenarios,
    e.g. strong lighting changes, lack of texture and high-speed motion; In contrast,
    IMUs are completely ego-centric, scene-independent, and can also provide absolute
    metric scale. Nevertheless, the downside is that inertial measurements, especially
    from low-cost devices, are plagued by process noise and biases. An effective fusion
    of the measurements from these two complementary sensors is of key importance
    to accurate pose estimation. Thus, according to their information fusion methods,
    conventional model based visual-inertial approaches are roughly segmented into
    three different classes : filtering approaches [[12](#bib.bib12)], fixed-lag smoothers
    [[13](#bib.bib13)] and full smoothing methods [[14](#bib.bib14)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将视觉和惯性数据整合为视觉-惯性里程计（VIO）是移动机器人中的一个明确问题。相机和惯性传感器都相对低成本、功耗低且广泛部署。这两种传感器是互补的：单目相机捕捉3D场景的外观和结构，但尺度模糊，对挑战性场景（例如强光变化、纹理缺乏和高速运动）不够鲁棒；相比之下，IMU完全是自我中心的、场景独立的，并且还可以提供绝对度量尺度。然而，缺点是惯性测量，特别是来自低成本设备的测量，常受到过程噪声和偏差的困扰。有效融合这两种互补传感器的测量对准确的姿态估计至关重要。因此，根据它们的信息融合方法，传统的基于模型的视觉-惯性方法大致分为三类：滤波方法[[12](#bib.bib12)]、固定时滞平滑器[[13](#bib.bib13)]和全平滑方法[[14](#bib.bib14)]。
- en: Data-driven approaches have emerged to consider learning 6-DoF poses directly
    from visual and inertial measurements without human intervention or calibration.
    VINet [[68](#bib.bib68)] is the first work that formulated visual-inertial odometry
    as a sequential learning problem, and proposed a deep neural network framework
    to achieve VIO in an end-to-end manner. VINet uses a ConvNet based visual encoder
    to extract visual features from two consecutive RGB images, and an inertial encoder
    to extract inertial features from a sequence of IMU data with a long short-term
    memory (LSTM) network. Here, the LSTM aims to model the temporal state evolution
    of inertial data. The visual and inertial features are concatenated together,
    and taken as the input into a further LSTM module to predict relative poses, conditioned
    on the history of system states. This learning approach has the advantage of being
    more robust to calibration and relative timing offset errors. However, VINet has
    not fully addressed the problem of learning a meaningful sensor fusion strategy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的方法已经出现，考虑直接从视觉和惯性测量中学习6自由度姿态，而无需人工干预或校准。VINet [[68](#bib.bib68)] 是首个将视觉惯性里程计表述为序列学习问题的工作，并提出了一种深度神经网络框架，以端到端的方式实现VIO。VINet
    使用基于卷积网络（ConvNet）的视觉编码器从两张连续的RGB图像中提取视觉特征，并使用惯性编码器从一系列IMU数据中提取惯性特征，后者采用长短期记忆（LSTM）网络。这里，LSTM旨在建模惯性数据的时间状态演变。视觉和惯性特征被连接在一起，并作为输入送入进一步的LSTM模块，以预测相对姿态，依赖于系统状态的历史。这种学习方法具有对校准和相对时间偏移误差更具鲁棒性的优势。然而，VINet尚未完全解决学习有意义的传感器融合策略的问题。
- en: To tackle the deep sensor fusion problem, Chen et al. [[70](#bib.bib70)] proposed
    selective sensor fusion, a framework that selectively learns context-dependent
    representations for visual inertial pose estimation. Their intuition is that the
    importance of features from different modalities should be considered according
    to the exterior (i.e., environmental) and interior (i.e., device/sensor) dynamics,
    by fully exploiting the complementary behaviors of two sensors. Their approach
    outperforms those without a fusion strategy, e.g. VINet, avoiding catastrophic
    failures.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决深度传感器融合问题，Chen等人[[70](#bib.bib70)] 提出了选择性传感器融合框架，这种框架选择性地学习依赖上下文的表示，以进行视觉惯性姿态估计。他们的直觉是，根据外部（即环境）和内部（即设备/传感器）动态，应该考虑不同模态特征的重要性，充分利用两个传感器的互补行为。他们的方法优于那些没有融合策略的方法，例如VINet，避免了灾难性的失败。
- en: 'Similar to unsupervised VO, Visual-inertial odometry can also be solved in
    a self-supervised fashion using novel view synthesis. VIOLearner [[69](#bib.bib69)]
    constructs motion transformations from raw inertial data, and converts source
    images into target images with the camera matrix and depth maps via the Equation
    [2](#S3.E2 "In 3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry
    Estimation ‣ A Survey on Deep Learning for Localization and Mapping: Towards the
    Age of Spatial Machine Intelligence") mentioned in Section [3.1.2](#S3.SS1.SSS2
    "3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry Estimation
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence"). In addition, an online error correction module corrects
    the intermediate errors of the framework. The network parameters are recovered
    by optimizing a photometric loss. Similarly, DeepVIO [[71](#bib.bib71)] incorporates
    inertial data and stereo images into this unsupervised learning framework, and
    is trained with a dedicated loss to reconstruct trajectories in a global scale.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于无监督视觉里程计（VO），视觉惯性里程计也可以通过新颖的视图合成以自监督方式解决。VIOLearner [[69](#bib.bib69)] 从原始惯性数据构建运动变换，并通过第[2](#S3.E2
    "In 3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry Estimation
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence")节提到的方程式，将源图像转换为目标图像，使用相机矩阵和深度图。此外，一个在线错误修正模块修正了框架中的中间错误。网络参数通过优化光度损失来恢复。类似地，DeepVIO
    [[71](#bib.bib71)] 将惯性数据和立体图像纳入这个无监督学习框架，并通过专用损失进行训练，以重建全球尺度的轨迹。'
- en: Learning-based VIO cannot defeat the state-of-the-art classical model based
    VIOs, but they are generally more robust to real issues[[68](#bib.bib68), [70](#bib.bib70),
    [71](#bib.bib71)] such as measurement noises, bad time synchronization, thanks
    to the impressive ability of DNNs in feature extraction and motion modelling.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的 VIO 无法击败最先进的经典模型基础 VIO，但由于深度神经网络在特征提取和运动建模中的出色能力，它们通常对实际问题如测量噪声、时间同步不良等更具鲁棒性[[68](#bib.bib68),
    [70](#bib.bib70), [71](#bib.bib71)]。
- en: 3.3 Inertial Odometry
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 惯性里程计
- en: Beyond visual odometry and visual-inertial odometry, an inertial-only solution,
    i.e. inertial odometry provides an ubiquitous alternative to solve the odometry
    estimation problem. Compared with visual methods, an inertial sensor is relatively
    low-cost, small, energy efficient and privacy preserving. It is relatively immune
    to environmental factors, such as lighting conditions or moving objects. However,
    low-cost MEMS inertial measurement units (IMU) widely found on robots and mobile
    devices are corrupted with high sensor bias and noise, leading to unbounded error
    drifts in the strapdown inertial navigation system (SINS), if inertial data are
    doubly integrated.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了视觉里程计和视觉惯性里程计外，惯性单一解决方案，即惯性里程计，为解决里程计估计问题提供了一种普遍替代方案。与视觉方法相比，惯性传感器成本相对较低，体积小，能效高且保护隐私。它对环境因素如光照条件或移动物体的免疫性相对较强。然而，广泛存在于机器人和移动设备上的低成本
    MEMS 惯性测量单元（IMU）存在较高的传感器偏差和噪声，如果惯性数据被双重积分，则会导致 strapdown 惯性导航系统（SINS）中的无界误差漂移。
- en: Chen et al. [[85](#bib.bib85)] formulated inertial odometry as a sequential
    learning problem with a key observation that 2D motion displacements in the polar
    coordinate (i.e. polar vector) can be learned from independent windows of segmented
    inertial data. The key observation is that when tracking human and wheeled configurations,
    the frequency of their vibrations is relevant to the moving speed, which is reflected
    by inertial measurements. Based on this, they proposed IONet, a LSTM based framework
    for end-to-end learning of relative poses from sequences of inertial measurements.
    Trajectories are generated by integrating motion displacements. [[86](#bib.bib86)]
    leveraged deep generative models and domain adaptation technique to improve the
    generalization ability of deep inertial odometry in new domains. [[87](#bib.bib87)]
    extends this framework by an improved triple-channel LSTM network to predict polar
    vectors for drone localization from inertial data and sampling time. RIDI [[88](#bib.bib88)]
    trains a deep neural network to regress linear velocities from inertial data,
    calibrates the collected accelerations to satisfy the constraints of the learned
    velocities, and doubly integrates the accelerations into locations with a conventional
    physical model. Similarly, [[89](#bib.bib89)] compensates the error drifts of
    the classical SINS model with the aid of learned velocities. Other works have
    also explored the usage of deep learning to detect zero-velocity phase for navigating
    pedestrians [[90](#bib.bib90)] and vehicles [[91](#bib.bib91)]. This zero-velocity
    phase provides context information to correct system error drifts via Kalman filtering.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[85](#bib.bib85)] 将惯性里程计建模为一个序列学习问题，他们的关键观察是可以从独立的分段惯性数据窗口中学习极坐标（即极向量）中的二维运动位移。关键观察是，当跟踪人类和带轮配置时，其振动频率与移动速度相关，这通过惯性测量得到体现。基于此，他们提出了
    IONet，一个基于 LSTM 的框架，用于从惯性测量序列中端到端地学习相对姿态。通过积分运动位移生成轨迹。[[86](#bib.bib86)] 利用深度生成模型和领域适应技术来提高深度惯性里程计在新领域中的泛化能力。[[87](#bib.bib87)]
    通过改进的三通道 LSTM 网络扩展了这一框架，以从惯性数据和采样时间中预测极向量以实现无人机定位。RIDI [[88](#bib.bib88)] 训练深度神经网络从惯性数据中回归线性速度，将收集的加速度校准以满足所学习速度的约束，并用传统的物理模型将加速度双重积分到位置中。类似地，[[89](#bib.bib89)]
    在学习的速度帮助下补偿了经典 SINS 模型的误差漂移。其他工作也探索了使用深度学习来检测零速度阶段，以便为行人[[90](#bib.bib90)] 和车辆[[91](#bib.bib91)]
    导航。这个零速度阶段提供了上下文信息，通过卡尔曼滤波来修正系统误差漂移。
- en: Inertial only solution can be a backup plan to offer pose information in extreme
    environments, where visual information is not available or is highly distorted.
    Deep learning has proven its capability to learn useful features from noisy IMU
    data, and compensate the error drifts of inertial dead reckoning, which is difficult
    to solve by classical algorithms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 仅依靠惯性解决方案可以作为极端环境中的备选方案，在这些环境中视觉信息不可用或高度扭曲。深度学习已经证明了其从噪声 IMU 数据中学习有用特征的能力，并能补偿惯性航迹推算的误差漂移，这是经典算法难以解决的问题。
- en: '![Refer to caption](img/ba3053551fe966de85273fb170bcdb76.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ba3053551fe966de85273fb170bcdb76.png)'
- en: 'Figure 5: A comparison of the performance of deep learning based visual odometry
    with an evaluation on the Trajectory 10 of the KITTI dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于深度学习的视觉里程估计性能比较，评估了 KITTI 数据集的轨迹 10。
- en: 3.4 LIDAR Odometry
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 LIDAR 里程估计
- en: LIDAR sensors provide high-frequency range measurements, with the benefits of
    working consistently in complex lighting conditions and optically featureless
    scenarios. Mobile robots and self-driving vehicles are normally equipped with
    LIDAR sensors to obtain relative self-motion (i.e. LIDAR odometry) and global
    pose with respect to a 3D map (LIDAR relocalization). The performance of LIDAR
    odometry is sensitive to point cloud registration errors due to non-smooth motion.
    In addition, the data quality of LIDAR measurements is also affected by extreme
    weather conditions, for example, heavy rain or fog/mist.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LIDAR 传感器提供高频率的距离测量，其优点是能够在复杂的光照条件和光学特征不足的场景中稳定工作。移动机器人和自动驾驶车辆通常配备 LIDAR 传感器，以获取相对自运动（即
    LIDAR 里程估计）和相对于 3D 地图的全球姿态（LIDAR 重新定位）。LIDAR 里程估计的性能对点云配准误差非常敏感，这种误差是由于运动不平滑造成的。此外，LIDAR
    测量的数据质量也受到极端天气条件的影响，例如大雨或雾霾。
- en: Traditionally, LIDAR odometry relies on point cloud registration to detect feature
    points, e.g. line and surface segments, and uses a matching algorithm to obtain
    the pose transformation by minimizing the distance between two consecutive point-cloud
    scans. Data-driven methods consider solving LIDAR odometry in an end-to-end fashion,
    by leveraging deep neural networks to construct a mapping function from point
    cloud scan sequences to pose estimates [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)].
    As point cloud data are challenging to be directly ingested by neural networks
    due to their sparse and irregularly sampled format, these methods typically convert
    point clouds into a regular matrix through cylindrical projection, and adopt ConvNets
    to extract features from consecutive point cloud scans. These networks regress
    relative poses and are trained via ground-truth labels. LO-Net [[73](#bib.bib73)]
    reports competitive performance over the conventional state-of-the-art algorithm,
    i.e. the LIDAR Odometry and Mapping (LOAM) algorithm [[16](#bib.bib16)].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，LIDAR 里程估计依赖于点云配准来检测特征点，例如线段和表面片段，并使用匹配算法通过最小化两个连续点云扫描之间的距离来获得姿态变换。数据驱动的方法考虑通过利用深度神经网络从点云扫描序列到姿态估计构建映射函数，以端到端的方式解决
    LIDAR 里程估计 [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)]。由于点云数据由于其稀疏和不规则采样格式难以直接被神经网络处理，这些方法通常通过圆柱投影将点云转换为规则矩阵，并采用卷积网络从连续点云扫描中提取特征。这些网络回归相对姿态，并通过地面真值标签进行训练。LO-Net
    [[73](#bib.bib73)] 报告了比传统的最先进算法，即 LIDAR 里程估计与地图（LOAM）算法 [[16](#bib.bib16)] 更具竞争力的性能。
- en: 3.5 Comparison of Odometry Estimation
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 里程估计比较
- en: 'Table [I](#S3.T1 "TABLE I ‣ 3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual
    Odometry ‣ 3 Odometry Estimation ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") compares existing
    work on odometry estimation, in terms of their sensor type, model, whether a trajectory
    with an absolute scale is produced, and their performance evaluation on the KITTI
    dataset, where available. As deep inertial odometry has not been evaluated on
    the KITTI dataset, we do not include inertial odometry in this table. The KITTI
    dataset [[46](#bib.bib46)] is a common benchmark for odometry estimation, consisting
    of a collection of sensor data from car-driving scenarios. As most data-driven
    approaches adopt the trajectory 09 and 10 of the KITTI dataset to evaluate model
    performance, we compared them according to the averaged Root Mean Square Error
    (RMSE) of the translation for all the subsequences of lengths (100, 200, .., 800)
    meters, which is provided by the official KITTI VO/SLAM evaluation metrics.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表[ I ](#S3.T1 "表 I ‣ 3.1.2 无监督学习的视觉里程计 ‣ 3.1 视觉里程计 ‣ 3 里程计估计 ‣ 关于深度学习在定位与制图中的调查：迈向空间机器智能时代")比较了现有的里程计估计工作，包括它们的传感器类型、模型、是否产生了具有绝对尺度的轨迹，以及它们在KITTI数据集上的性能评估（如果有的话）。由于深度惯性里程计未在KITTI数据集上进行评估，因此我们没有在此表中包括惯性里程计。KITTI数据集[[46](#bib.bib46)]是一个常用的里程计估计基准，由来自汽车驾驶场景的传感器数据组成。由于大多数数据驱动的方法采用KITTI数据集的09和10轨迹来评估模型性能，我们根据官方KITTI视觉里程计/SLAM评估指标提供的所有子序列的平移均方根误差（RMSE）进行比较。
- en: 'We take visual odometry as an example. Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Inertial
    Odometry ‣ 3 Odometry Estimation ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") reports the translational
    drifts of deep visual odometry models over time on the 10th trajectory of the
    KITTI dataset. Clearly, hybrid VO shows the best performance over supervised VO
    and unsupervised VO, as the hybrid model benefits from both the mature geometry
    models of traditional VO algorithms and the strong capacity for feature extraction
    of deep learning. Although supervised VO still outperforms unsupervised VO, the
    performance gap between them is diminishing as the limitations of unsupervised
    VO are gradually addressed. For example, it has been found that unsupervised VO
    now can recover global scale from monocular images [[30](#bib.bib30)]. Overall,
    data-driven visual odometry shows a remarkable increase in model performance,
    indicating the potentials of deep learning approaches in achieving more accurate
    odometry estimation in the future.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以视觉里程计为例。图[5](#S3.F5 "图 5 ‣ 3.3 惯性里程计 ‣ 3 里程计估计 ‣ 关于深度学习在定位与制图中的调查：迈向空间机器智能时代")展示了KITTI数据集第10条轨迹上深度视觉里程计模型随时间的平移漂移情况。显然，混合视觉里程计的表现优于监督式和无监督式视觉里程计，因为混合模型既受益于传统视觉里程计算法的成熟几何模型，又具有深度学习的强大特征提取能力。虽然监督式视觉里程计仍然优于无监督式视觉里程计，但随着无监督视觉里程计局限性的逐步解决，它们之间的性能差距正在缩小。例如，已经发现无监督视觉里程计现在可以从单目图像中恢复全局尺度[[30](#bib.bib30)]。总体而言，数据驱动的视觉里程计模型性能显著提升，表明深度学习方法在实现更准确的里程计估计方面具有潜力。
- en: 4 Mapping
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 制图
- en: 'Mapping refers to the ability of a mobile agent to build a consistent environmental
    model to describe the surrounding scene. Deep learning has fostered a set of tools
    for scene perception and understanding, with applications ranging from depth prediction,
    to semantic labelling, to 3D geometry reconstruction. This section provides an
    overview of existing works relevant to deep learning based mapping methods. We
    categorize them into geometric mapping, semantic mapping, and general mapping.
    Table [II](#S4.T2 "TABLE II ‣ 4 Mapping ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") summarizes the
    existing methods on deep learning based mapping.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 制图指的是移动代理构建一致的环境模型以描述周围场景的能力。深度学习促进了一套用于场景感知和理解的工具，应用范围包括深度预测、语义标注、到3D几何重建。本节提供了与基于深度学习的制图方法相关的现有工作的概述。我们将其分类为几何制图、语义制图和通用制图。表[II](#S4.T2
    "表 II ‣ 4 制图 ‣ 关于深度学习在定位与制图中的调查：迈向空间机器智能时代")总结了基于深度学习的制图方法。
- en: 'TABLE II: A summary of existing methods on deep learning for mapping.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：关于深度学习制图的现有方法总结。
- en: '|  | Output Representation | Employed by |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | 输出表示 | 使用者 |'
- en: '| Geometric Map | Depth Representation | [[78](#bib.bib78), [92](#bib.bib92),
    [79](#bib.bib79)], [[80](#bib.bib80), [81](#bib.bib81)], [[29](#bib.bib29)], [[53](#bib.bib53),
    [55](#bib.bib55), [56](#bib.bib56), [58](#bib.bib58), [59](#bib.bib59), [93](#bib.bib93),
    [61](#bib.bib61), [64](#bib.bib64)], [[76](#bib.bib76), [77](#bib.bib77)] |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 几何地图 | 深度表示 | [[78](#bib.bib78), [92](#bib.bib92), [79](#bib.bib79)], [[80](#bib.bib80),
    [81](#bib.bib81)], [[29](#bib.bib29)], [[53](#bib.bib53), [55](#bib.bib55), [56](#bib.bib56),
    [58](#bib.bib58), [59](#bib.bib59), [93](#bib.bib93), [61](#bib.bib61), [64](#bib.bib64)],
    [[76](#bib.bib76), [77](#bib.bib77)] |'
- en: '| Voxel Representation | [[94](#bib.bib94), [95](#bib.bib95)], [[96](#bib.bib96)](Object),
    [[97](#bib.bib97)], [[98](#bib.bib98)](Object), [[99](#bib.bib99)](Object), [[100](#bib.bib100)](Object)
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 体素表示 | [[94](#bib.bib94), [95](#bib.bib95)], [[96](#bib.bib96)](对象), [[97](#bib.bib97)],
    [[98](#bib.bib98)](对象), [[99](#bib.bib99)](对象), [[100](#bib.bib100)](对象) |'
- en: '| Point Representation | [[101](#bib.bib101)](Object) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 点表示 | [[101](#bib.bib101)](对象) |'
- en: '| Mesh Representation | [[102](#bib.bib102)](Object), [[103](#bib.bib103)](Object),
    [[104](#bib.bib104)](Object), [[105](#bib.bib105)](Object), [[106](#bib.bib106),
    [107](#bib.bib107)] |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 网格表示 | [[102](#bib.bib102)](对象), [[103](#bib.bib103)](对象), [[104](#bib.bib104)](对象),
    [[105](#bib.bib105)](对象), [[106](#bib.bib106), [107](#bib.bib107)] |'
- en: '| Semantic Map | Semantic Segmentation | [[26](#bib.bib26), [27](#bib.bib27),
    [108](#bib.bib108)] |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 语义地图 | 语义分割 | [[26](#bib.bib26), [27](#bib.bib27), [108](#bib.bib108)] |'
- en: '| Instance Segmentation | [[109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111)]
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 实例分割 | [[109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111)] |'
- en: '| Panoptic Segmentation | [[112](#bib.bib112)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 全景分割 | [[112](#bib.bib112)] |'
- en: '| General Map | Neural Representation | [[113](#bib.bib113)],[[114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116)], [[117](#bib.bib117), [118](#bib.bib118),
    [31](#bib.bib31), [32](#bib.bib32)] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 通用地图 | 神经表示 | [[113](#bib.bib113)],[[114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116)], [[117](#bib.bib117), [118](#bib.bib118), [31](#bib.bib31),
    [32](#bib.bib32)] |'
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Object indicates that this method is only validated on reconstructing single
    objects rather than a scene.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对象表示该方法仅在重建单个对象而非场景时经过验证。
- en: 4.1 Geometric Mapping
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 几何映射
- en: 'Broadly, geometric mapping captures the shape and structural description of
    a scene. Typical choices of the scene representations used in geometric mapping
    include depth, voxel, point and mesh. We follow this representational taxonomy
    and categorize deep learning for geometric mapping into the above four classes.
    Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Geometric Mapping ‣ 4 Mapping ‣ A Survey on
    Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine
    Intelligence") demonstrates these geometric representations on the Stanford Bunny
    benchmark.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，几何映射捕捉了场景的形状和结构描述。在几何映射中使用的典型场景表示包括深度、体素、点和网格。我们遵循这种表示法，将深度学习在几何映射中的应用分为上述四类。图
    [6](#S4.F6 "图 6 ‣ 4.1 几何映射 ‣ 4 映射 ‣ 深度学习在定位与映射中的综述：迈向空间机器智能时代") 演示了这些几何表示在斯坦福兔基准上的应用。
- en: '![Refer to caption](img/e4223f55b3e003334e300bab7d872bbe.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e4223f55b3e003334e300bab7d872bbe.png)'
- en: 'Figure 6: An illustrations of scene representations on the Stanford Bunny benchmark:
    (a) original model, (b) depth representation, (c) voxel representation (d) point
    representation (e) mesh representation.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：斯坦福兔基准上的场景表示插图：(a) 原始模型，(b) 深度表示，(c) 体素表示 (d) 点表示 (e) 网格表示。
- en: 4.1.1 Depth Representation
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 深度表示
- en: Depth maps play a pivotal role in understanding the scene geometry and structure.
    Dense scene reconstruction has been achieved by fusing depth and RGB images [[119](#bib.bib119),
    [120](#bib.bib120)]. Traditional SLAM systems represent scene geometry with dense
    depth maps (i.e. 2.5D), such as DTAM [[121](#bib.bib121)]. In addition, accurate
    depth estimation can contribute to the absolute scale recovery for visual SLAM.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图在理解场景几何和结构中发挥着关键作用。通过融合深度图和 RGB 图像 [[119](#bib.bib119), [120](#bib.bib120)]
    实现了密集场景重建。传统 SLAM 系统使用密集深度图（即 2.5D）表示场景几何，例如 DTAM [[121](#bib.bib121)]。此外，准确的深度估计有助于视觉
    SLAM 的绝对尺度恢复。
- en: Learning depth from raw images is a fast evolving area in computer vision community.
    The earliest work formulates depth estimation as a mapping function of input single
    images, constructed by a multi-scale deep neural network [[78](#bib.bib78)] to
    output the per-pixel depth maps from single images. More accurate depth prediction
    is achieved by jointly optimizing the depth and self-motion estimation [[79](#bib.bib79)].
    These supervised learning methods [[78](#bib.bib78), [92](#bib.bib92), [79](#bib.bib79)]
    can predict per-pixel depth by training deep neural networks on large data collections
    of images with corresponding depth labels. Although they are found outperforming
    the traditional structure based methods, such as [[122](#bib.bib122)], their effectiveness
    are largely reliant on model training and can be difficult to generalize to new
    scenarios in absence of labeled data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始图像中学习深度是计算机视觉领域一个快速发展的方向。最早的工作将深度估计公式化为输入单幅图像的映射函数，通过多尺度深度神经网络[[78](#bib.bib78)]构建，从单幅图像中输出每像素的深度图。通过联合优化深度和自我运动估计[[79](#bib.bib79)]可以获得更准确的深度预测。这些监督学习方法[[78](#bib.bib78),
    [92](#bib.bib92), [79](#bib.bib79)]可以通过在包含相应深度标签的大量图像数据集上训练深度神经网络来预测每像素的深度。尽管这些方法比传统的基于结构的方法如[[122](#bib.bib122)]表现更好，但它们的有效性在很大程度上依赖于模型训练，并且在缺乏标注数据的情况下，难以推广到新场景。
- en: 'On the other side, recent advances in this field focus on unsupervised solutions,
    by reformulating depth prediction as a novel view synthesis problem. [[80](#bib.bib80),
    [81](#bib.bib81)] utilized photometric consistency loss as a self-supervision
    signal for training neural models. With stereo images and a known camera baseline,
    [[80](#bib.bib80), [81](#bib.bib81)] synthesize the left view from the right image,
    and the predicted depth maps of the left view. By minimizing the distance between
    synthesized images and real images, i.e. the spatial consistency, the parameters
    of the networks can be recovered via this self-supervision in an end-to-end manner.
    Besides the spatial consistency, [[29](#bib.bib29)] proposed to apply temporal
    consistency as a self-supervised signal, by synthesizing the image in the target
    time frame from the source time frame. At the same time, egomotion is recovered
    along with the depth estimation. This framework only requires monocular images
    to learn both the depth maps and egomotion. A number of following works [[53](#bib.bib53),
    [55](#bib.bib55), [56](#bib.bib56), [58](#bib.bib58), [59](#bib.bib59), [93](#bib.bib93),
    [61](#bib.bib61), [64](#bib.bib64), [76](#bib.bib76), [77](#bib.bib77)] extended
    this framework and achieved better performance in depth and egomotion estimation.
    We refer the readers to Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Unsupervised Learning
    of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry Estimation ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence"),
    in which a variety of additional constraints haven been discussed.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，该领域的最新进展集中于无监督解决方案，通过将深度预测重新表述为新颖视图合成问题。[[80](#bib.bib80), [81](#bib.bib81)]利用光度一致性损失作为训练神经模型的自我监督信号。通过立体图像和已知的相机基线，[[80](#bib.bib80),
    [81](#bib.bib81)]从右图像和左视图的预测深度图中合成左视图。通过最小化合成图像与真实图像之间的距离，即空间一致性，网络的参数可以通过这种自我监督方式以端到端的方式恢复。除了空间一致性，[[29](#bib.bib29)]提出通过从源时间帧合成目标时间帧的图像，应用时间一致性作为自我监督信号。同时，伴随深度估计恢复自我运动。该框架仅需单目图像即可学习深度图和自我运动。许多后续工作[[53](#bib.bib53),
    [55](#bib.bib55), [56](#bib.bib56), [58](#bib.bib58), [59](#bib.bib59), [93](#bib.bib93),
    [61](#bib.bib61), [64](#bib.bib64), [76](#bib.bib76), [77](#bib.bib77)]扩展了这一框架，并在深度和自我运动估计中取得了更好的表现。我们建议读者参见第[3.1.2](#S3.SS1.SSS2
    "3.1.2 无监督学习的视觉里程计 ‣ 3.1 视觉里程计 ‣ 3 里程计估计 ‣ 深度学习在定位和地图绘制中的应用：迈向空间机器智能时代")节，其中讨论了各种额外的约束。
- en: With the depth maps predicted by ConvNets, learning based SLAM systems can integrate
    depth information to address some limitations of classical monocular solution.
    For example, CNN-SLAM [[123](#bib.bib123)] utilizes the learned depths from single
    images into a monocular SLAM framework (i.e. LSD-SLAM [[124](#bib.bib124)]). Their
    experiment shows how the learned depth maps contribute to mitigate the absolute
    scale recovery problem in pose estimates and scene reconstruction. CNN-SLAM achieves
    dense scene predictions even in texture-less areas, which is normally hard for
    a conventional SLAM system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 利用ConvNets预测的深度图，基于学习的SLAM系统可以整合深度信息来解决经典单目解决方案的一些限制。例如，CNN-SLAM [[123](#bib.bib123)]
    将从单幅图像中学习的深度信息融入单目SLAM框架（即LSD-SLAM [[124](#bib.bib124)]）。他们的实验显示，学习到的深度图如何有助于缓解位姿估计和场景重建中的绝对尺度恢复问题。CNN-SLAM
    即使在无纹理区域也能实现密集场景预测，这通常是传统SLAM系统难以做到的。
- en: 4.1.2 Voxel Representation
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 体素表示
- en: Voxel-based formulation is a natural way to represent 3D geometry. Similar to
    the usage of pixel (i.e. 2D element) in images, voxel is a volume element in a
    three-dimensional space. Previous works have explored to use multiple input views,
    to reconstruct the volumetric representation of scene [[94](#bib.bib94), [95](#bib.bib95)]
    and objects [[96](#bib.bib96)]. For example, SurfaceNet [[94](#bib.bib94)] learns
    to predict the confidence of a voxel to determine whether it is on surface or
    not, and reconstruct the 2D surface of a scene. RayNet [[95](#bib.bib95)] reconstructs
    the scene geometry by extracting view-invariant features while imposing geometric
    constraints. Recent works focus on generating high-resolution 3D volumetric models
    [[98](#bib.bib98), [97](#bib.bib97)]. For example, Tatarchenko et al. [[97](#bib.bib97)]
    designed a convolutional decoder based on octree-based formulation to enable scene
    reconstruction in much higher resolution. Other work can be found on scene completion
    from RGB-D data [[99](#bib.bib99), [100](#bib.bib100)]. One limitation of voxel
    representation is the high computational requirement, especially when attempting
    to reconstruct a scene in high resolution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于体素的表示是表示3D几何形状的一种自然方式。类似于图像中像素（即2D元素）的使用，体素是在三维空间中的体积元素。之前的研究探讨了使用多个输入视角来重建场景的体积表示[[94](#bib.bib94),
    [95](#bib.bib95)]和物体[[96](#bib.bib96)]。例如，SurfaceNet [[94](#bib.bib94)]学习预测体素的置信度，以确定它是否在表面上，并重建场景的2D表面。RayNet
    [[95](#bib.bib95)]通过提取视图不变特征，同时施加几何约束来重建场景几何。近期的研究集中在生成高分辨率的3D体积模型[[98](#bib.bib98),
    [97](#bib.bib97)]。例如，Tatarchenko等人[[97](#bib.bib97)]设计了一种基于八叉树表示的卷积解码器，以实现更高分辨率的场景重建。还可以找到基于RGB-D数据的场景补全的其他工作[[99](#bib.bib99),
    [100](#bib.bib100)]。体素表示的一个限制是高计算需求，尤其是在尝试以高分辨率重建场景时。
- en: 4.1.3 Point Representation
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 点表示
- en: Point-based formulation consists of the 3-dimensional coordinates (x, y, z)
    of points in 3D space. Point representation is easy to understand and manipulate,
    but suffers from the ambiguity problem, which means that different forms of point
    clouds can represent a same geometry. The pioneer work, PointNet [[125](#bib.bib125)],
    processes unordered point data with a single symmetric function - max pooling,
    to aggregate point features for classification and segmentation. Fan et al. [[101](#bib.bib101)]
    developed a deep generative model that generates 3D geometry in point-based formulation
    from single images. In their work, a loss function based on Earth Mover’s distance
    is introduced to tackle the problem of data ambiguity. However, their method is
    only validated on the reconstruction task of single objects. No work on point
    generation for scene reconstruction has been found yet.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点的表示由3D空间中点的三维坐标（x, y, z）组成。点表示易于理解和操作，但存在歧义问题，这意味着不同形式的点云可以表示相同的几何形状。开创性工作PointNet
    [[125](#bib.bib125)] 用一个对称函数 - 最大池化来处理无序点数据，以聚合点特征进行分类和分割。Fan等人[[101](#bib.bib101)]开发了一种深度生成模型，该模型从单幅图像中生成基于点的3D几何。在他们的工作中，引入了基于地球搬运工距离的损失函数来解决数据歧义问题。然而，他们的方法仅在单个物体的重建任务上得到验证。目前尚未发现用于场景重建的点生成工作。
- en: 4.1.4 Mesh Representation
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 网格表示
- en: Mesh-based formulation encodes the underlying structure of 3D models, such as
    edges, vertices and faces. It is a powerful representation that naturally captures
    the surface of 3D shape. Several works considered the problem of learning mesh
    generation from images [[102](#bib.bib102), [103](#bib.bib103)] or point clouds
    data [[104](#bib.bib104), [105](#bib.bib105)]. However, these approaches are only
    able to reconstruct single objects, and limited to generating models with simple
    structures or from familiar classes. To tackle the problem of scene reconstruction
    in mesh representation, [[106](#bib.bib106)] integrates the sparse features from
    monocular SLAM with dense depth maps from ConvNet for the update of 3D mesh representation.
    The depth predictions are fused into the monocular SLAM system to recover the
    absolute scale of pose and scene features estimation. To allow efficient computation
    and flexible information fusion, [[107](#bib.bib107)] utilizes 2.5D mesh to represent
    scene geometry. In their approach, the image plane coordinates of mesh vertices
    are learned by deep neural networks, while the depth maps are optimized as free
    variables.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 基于网格的表述编码了3D模型的基本结构，如边缘、顶点和面。这是一种强大的表示方式，能够自然地捕捉3D形状的表面。一些研究考虑了从图像 [[102](#bib.bib102),
    [103](#bib.bib103)] 或点云数据 [[104](#bib.bib104), [105](#bib.bib105)] 学习网格生成的问题。然而，这些方法只能重建单一对象，并且限于生成具有简单结构或来自熟悉类别的模型。为了解决网格表示中的场景重建问题，[[106](#bib.bib106)]
    将单目SLAM的稀疏特征与ConvNet的密集深度图进行集成，用于更新3D网格表示。深度预测被融合到单目SLAM系统中，以恢复姿态和场景特征估计的绝对尺度。为了实现高效计算和灵活的信息融合，[[107](#bib.bib107)]
    利用2.5D网格来表示场景几何。在他们的方法中，网格顶点的图像平面坐标由深度神经网络学习，而深度图作为自由变量进行优化。
- en: 4.2 Semantic Map
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语义地图
- en: '![Refer to caption](img/bb3b9d7b01e5d5b063558deccfacd5de.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bb3b9d7b01e5d5b063558deccfacd5de.png)'
- en: 'Figure 7: (b) semantic segmentation, (c) instance segmentation and (d) panoptic
    segmentation for semantic mapping [[126](#bib.bib126)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图7： (b) 语义分割，(c) 实例分割和 (d) 语义映射的全景分割 [[126](#bib.bib126)]。
- en: Semantic mapping connects semantic concepts (i.e. object classification, material
    composition etc) with the geometry of environments. This is treated as a data
    association problem. The advances in deep learning greatly fosters the developments
    of object recognition and semantic segmentation. Maps with semantic meanings enable
    mobile agents to have high-level understandings of their environments beyond pure
    geometry, and allow for a greater range of functionality and autonomy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 语义映射将语义概念（例如，物体分类、材料组成等）与环境的几何形状连接起来。这被视为一个数据关联问题。深度学习的进展极大地推动了物体识别和语义分割的发展。具有语义意义的地图使得移动智能体能够在超越纯几何的基础上，对其环境进行更高级的理解，并允许更广泛的功能性和自主性。
- en: SemanticFusion [[26](#bib.bib26)] is one of the early works that combined the
    semantic segmentation labels from deep ConvNet with the dense scene geometry from
    a SLAM system. It incrementally integrates per-frame semantic segmentation predictions
    into a dense 3D map by probabilistically associating the 2D frames with the 3D
    map. This combination not only generates a map with useful semantic information,
    but also shows the integration with a SLAM system helps to enhance the single
    frame segmentation. The two modules are loosely coupled in SemanticFusion. [[27](#bib.bib27)]
    proposed a self-supervised network that predicts consistent semantic labels for
    a map, by imposing constraints on the consistency of semantic predictions in multiple
    views. DA-RNN [[108](#bib.bib108)] introduces recurrent models into semantic segmentation
    framework to learn the temporal connections over multiple view frames, producing
    more accurate and consistent semantic labelling for volumetric maps from KinectFusion
    [[127](#bib.bib127)]. Yet these methods provide no information on object instances,
    which means that they are not able to distinguish among different ojects from
    the same category.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: SemanticFusion [[26](#bib.bib26)] 是早期结合了深度卷积网络的语义分割标签与来自SLAM系统的密集场景几何的工作之一。它通过概率性地将2D帧与3D地图关联，逐帧地将语义分割预测集成到密集的3D地图中。这种结合不仅生成了具有有用语义信息的地图，而且显示了与SLAM系统的集成有助于增强单帧分割。SemanticFusion中的两个模块是松散耦合的。[[27](#bib.bib27)]
    提出了一个自监督网络，通过对多个视图中的语义预测一致性施加约束，预测一致的语义标签。DA-RNN [[108](#bib.bib108)] 将递归模型引入语义分割框架，以学习多个视图帧之间的时间连接，从KinectFusion
    [[127](#bib.bib127)] 中生成更准确、一致的语义标签。然而，这些方法没有提供对象实例的信息，这意味着它们无法区分同一类别中的不同对象。
- en: 'With the advances in instance segmentation, semantic mapping evolves into the
    instance level. A good example is [[109](#bib.bib109)] that offers object-level
    semantic mapping by identifying individual objects via a bounding box detection
    module and an unsupervised geometric segmentation module. Unlike other dense semantic
    mapping approaches, Fusion++ [[110](#bib.bib110)] builds a semantic graph-based
    map, which predicts only object instances and maintains a consistent map via loop
    closure detection, pose-graph optimization and further refinement. [[111](#bib.bib111)]
    presented a framework that achieves instance-aware semantic mapping, and enables
    novel object discovery. Recently, panoptic segmentation [[126](#bib.bib126)] attracts
    a lot of attentions. PanopticFusion [[112](#bib.bib112)] advanced semantic mapping
    to the level of stuff and things level that classifies static objects, e.g. walls,
    doors, lanes as stuff classes, and other accountable objects as things classes,
    e.g. moving vehicles, human and tables. Figure [7](#S4.F7 "Figure 7 ‣ 4.2 Semantic
    Map ‣ 4 Mapping ‣ A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence") compares semantic segmentation, instance
    segmentation and panoptic segmentation.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '随着实例分割技术的进步，语义映射逐渐发展到实例级别。一个好的例子是[[109](#bib.bib109)]，它通过边界框检测模块和无监督几何分割模块识别单个对象，提供了对象级别的语义映射。与其他密集语义映射方法不同，Fusion++
    [[110](#bib.bib110)] 构建了基于语义图的地图，仅预测对象实例，并通过循环闭合检测、姿态图优化和进一步的精细化保持一致的地图。[[111](#bib.bib111)]
    提出了一个实现实例感知语义映射的框架，并支持新对象发现。最近，全景分割 [[126](#bib.bib126)] 引起了很多关注。PanopticFusion
    [[112](#bib.bib112)] 将语义映射提升到“物体与事物”级别，将静态物体（如墙壁、门、车道）分类为“物体”类别，将其他可归类对象（如移动车辆、人类和桌子）分类为“事物”类别。图
    [7](#S4.F7 "Figure 7 ‣ 4.2 Semantic Map ‣ 4 Mapping ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    比较了语义分割、实例分割和全景分割。'
- en: 4.3 General Map
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 一般地图
- en: Beyond the explicit geometric and semantic map representation, deep learning
    models are able to encode the whole scene into an implicit representation, i.e.
    a general map representation to capture the underlying scene geometry and appearance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 超越显式的几何和语义地图表示，深度学习模型能够将整个场景编码为隐式表示，即通用地图表示，以捕捉底层场景几何和外观。
- en: Utilizing deep autoencoders can automatically discover the high-level compact
    representation of high-dimensional data. A notable example is CodeSLAM [[113](#bib.bib113)]
    that encodes observed images into a compact and optimizable representation to
    contain the essential information of a dense scene. This general representation
    is further used into a keyframe-based SLAM system to infer both pose estimates
    and keyframe depth maps. Due to the reduced size of learned representations, CodeSLAM
    allows efficient optimization of tracking camera motion and scene geometry for
    a global consistency.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 利用深度自编码器可以自动发现高维数据的高层次紧凑表示。一个显著的例子是 CodeSLAM [[113](#bib.bib113)]，它将观察到的图像编码成紧凑且可优化的表示，以包含密集场景的基本信息。这个通用表示进一步用于基于关键帧的
    SLAM 系统，以推断姿态估计和关键帧深度图。由于学习表示的尺寸减少，CodeSLAM 允许高效优化跟踪相机运动和场景几何，以实现全局一致性。
- en: 'Neural rendering models are another family of works that learn to model 3D
    scene structure implicitly by exploiting view synthesis as a self-supervision
    signal. The target of neural rendering task is to reconstruct a new scene from
    an unknown viewpoint. The seminar work, Generative Query Network (GQN) [[128](#bib.bib128)]
    learns to capture representation and render a new scene. GQN consists of a representation
    network and a generation network: the representation network encodes the observations
    from reference views into a scene representation; the generation network which
    is based on recurrent model, reconstructs the scene from a new view conditioned
    on the scene representation and a stochastic latent variable. Taking inputs as
    the observed images from several viewpoints, and the camera pose of a new view,
    GQN predicts the physical scene of this new view. Intuitively, through end-to-end
    training, the representation network can capture the necessary and important factors
    of 3D environment for the scene reconstruction task via the generation network.
    GQN is extended by incorporating a geometric-aware attention mechanism to allow
    more complex environment modelling [[114](#bib.bib114)], as well as including
    multimodal data for scene inference [[115](#bib.bib115)]. Scene representation
    network (SRN) [[116](#bib.bib116)] tackles the scene rendering problem via a learned
    continuous scene representation that connects a camera pose and its corresponding
    observation. A differentiable Ray Marching algorithm is integrated into SRN to
    enforce the network to model 3D structure consistently. However, these frameworks
    can only be applied to synthetic datasets due to the complexity of real-world
    environments.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 神经渲染模型是另一类通过利用视图合成作为自我监督信号来隐式建模 3D 场景结构的工作。神经渲染任务的目标是从未知视角重建新场景。开创性的工作，生成查询网络（GQN）[[128](#bib.bib128)]
    学会捕捉表示并渲染新场景。GQN 由一个表示网络和一个生成网络组成：表示网络将来自参考视角的观察编码成场景表示；生成网络基于递归模型，从新的视角重建场景，条件是场景表示和随机潜变量。GQN
    将来自多个视角的观察图像和新视角的相机姿态作为输入，预测这个新视角的物理场景。直观地，通过端到端训练，表示网络可以通过生成网络捕捉 3D 环境中对场景重建任务必要和重要的因素。GQN
    通过结合几何感知注意机制来扩展，以允许更复杂的环境建模[[114](#bib.bib114)]，以及包括多模态数据进行场景推断[[115](#bib.bib115)]。场景表示网络（SRN）[[116](#bib.bib116)]
    通过学习的连续场景表示来解决场景渲染问题，将相机姿态与其对应的观察连接起来。一个可微分的光线行进算法被集成到 SRN 中，以强制网络一致地建模 3D 结构。然而，由于现实世界环境的复杂性，这些框架只能应用于合成数据集。
- en: Last but not least, in the quest of ‘map-less’ navigation, task-driven maps
    emerge as a novel map representation. This representation is jointly modelled
    by deep neural networks with respect to the task at hand. Generally those tasks
    leverage location information, such as navigation or path planning, requiring
    mobile agents to understand the geometry and semantics of environment. Navigation
    in unstructured environments (even in a city scale) is formulated as an policy
    learning problem in these works [[117](#bib.bib117), [118](#bib.bib118), [31](#bib.bib31),
    [32](#bib.bib32)], and solved by deep reinforcement learning. Different from traditional
    solutions that follow a procedure of building an explicit map, planning path and
    making decisions, these learning based techniques predict control signals directly
    from sensor observations in an end-to-end manner, without explicitly modelling
    the environment. The model parameters are optimized via sparse reward signals,
    for example, whenever agents reach a destination, a positive reward will be given
    to tune the neural network. Once a model is trained, the actions of agents can
    be determined conditioned on the current observations of environment, i.e. images.
    In this case, all of environmental factors, such as the geometry, appearance and
    semantics of a scene, are embedded inside the neurons of a deep neural network
    and suitable for solving the task at hand. Interestingly, the visualization of
    the neurons inside a neural model that is trained on the navigation task via reinforcement
    learning, has similar patterns as the grid and place cells inside human brain.
    This provides cognitive cues to support the effectiveness of neural map representation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，在“无地图”导航的追求中，任务驱动的地图作为一种新颖的地图表示方法出现。这种表示方法由深度神经网络根据具体任务进行联合建模。通常，这些任务利用位置信息，如导航或路径规划，要求移动代理理解环境的几何形状和语义。在这些工作中，无结构环境（即使是城市规模）的导航被表述为一个策略学习问题[[117](#bib.bib117),
    [118](#bib.bib118), [31](#bib.bib31), [32](#bib.bib32)]，并通过深度强化学习解决。与传统方法不同，传统方法遵循构建显式地图、规划路径和做出决策的过程，这些基于学习的技术直接从传感器观测中预测控制信号，采用端到端的方式，无需显式建模环境。模型参数通过稀疏奖励信号进行优化，例如，每当代理到达目的地时，将给予正奖励以调整神经网络。一旦模型训练完成，代理的动作可以根据环境的当前观测（即图像）来确定。在这种情况下，所有环境因素，如场景的几何形状、外观和语义，都嵌入在深度神经网络的神经元中，并适合于解决当前任务。有趣的是，在强化学习任务上训练的神经模型中，神经元的可视化具有与人脑中的网格细胞和位置细胞相似的模式。这为支持神经地图表示的有效性提供了认知线索。
- en: 5 Global Localization
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 全球定位
- en: Global localization concerns the retrieval of absolute pose of a mobile agent
    within a known scene. Different from odometry estimation that relies on estimating
    the internal dynamical model and can perform in an unseen scenario, in global
    localization, prior knowledge about the scene is provided and exploited, through
    a 2D or 3D scene model. Broadly, it describes the relation between the sensor
    observations and map, by matching a query image or view against a pre-built model,
    and returning an estimate of global pose.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 全球定位涉及在已知场景中检索移动代理的绝对姿态。不同于依赖于估计内部动态模型的里程计估计并且能够在未见过的场景中执行，全球定位中提供并利用了有关场景的先验知识，通过2D或3D场景模型。广义上，它描述了传感器观测与地图之间的关系，通过将查询图像或视图与预构建的模型进行匹配，并返回全球姿态的估计。
- en: 'We categorize deep learning based global localization into three categories,
    according to the types of inquiry data and map: *2D-to-2D localization* queries
    2D images against an explicit database of geo-referenced images or implicit neural
    map; *2D-to-3D localization* establishes correspondences between 2D pixels of
    images and 3D points of a scene model; and *3D-to-3D localization* matches 3D
    scans to a pre-built 3D map. Table [III](#S5.T3 "TABLE III ‣ 5.1 2D-to-2D Localization
    ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence"), [IV](#S5.T4 "TABLE IV ‣ 5.2
    2D-to-3D Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning for
    Localization and Mapping: Towards the Age of Spatial Machine Intelligence") and
    [V](#S5.T5 "TABLE V ‣ 5.2.1 Descriptor Matching Based Localization ‣ 5.2 2D-to-3D
    Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") summarize the existing
    approaches on deep learning based 2D-to-2D localization, 2D-to-3D localization
    and 3D-to-3D localization respectively.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '我们根据查询数据和地图的类型将基于深度学习的全局定位分为三类：*2D-to-2D定位* 将2D图像与显式地理参考图像数据库或隐式神经地图进行对比；*2D-to-3D定位*
    建立2D图像像素与3D场景模型中的3D点之间的对应关系；*3D-to-3D定位* 将3D扫描与预建的3D地图进行匹配。表[III](#S5.T3 "TABLE
    III ‣ 5.1 2D-to-2D Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")、[IV](#S5.T4
    "TABLE IV ‣ 5.2 2D-to-3D Localization ‣ 5 Global Localization ‣ A Survey on Deep
    Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    和 [V](#S5.T5 "TABLE V ‣ 5.2.1 Descriptor Matching Based Localization ‣ 5.2 2D-to-3D
    Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") 分别总结了基于深度学习的2D-to-2D定位、2D-to-3D定位和3D-to-3D定位的现有方法。'
- en: 5.1 2D-to-2D Localization
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 2D-to-2D 定位
- en: 2D-to-2D localization regresses the camera pose of an image against a 2D map.
    Such 2D map is explicitly built by a geo-referenced database or implicitly encoded
    in a neural network.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 2D-to-2D定位通过2D地图回归图像的相机姿态。这样的2D地图由地理参考数据库显式构建或隐式编码在神经网络中。
- en: '![Refer to caption](img/2734e61f17c483e0df298e8e18c4c17f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2734e61f17c483e0df298e8e18c4c17f.png)'
- en: 'Figure 8: The typical architectures of 2D-to-2D based localization through
    (a) explict map, i.e. RelocNet [[129](#bib.bib129)] and (b) implicit map, i.e.
    e.g. PoseNet [[130](#bib.bib130)]'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：基于2D-to-2D的定位的典型架构，包括（a）显式地图，即RelocNet [[129](#bib.bib129)] 和（b）隐式地图，即PoseNet
    [[130](#bib.bib130)]
- en: 'TABLE III: A summary on existing methods on deep learning for 2D-to-2D global
    localization'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：基于深度学习的2D-to-2D全局定位现有方法的总结
- en: '| Model | Agnostic | Performance (m/degree) | Contributions |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 不相关 | 性能（m/度） | 贡献 |'
- en: '| 7Scenes | Cambridge |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 7Scenes | 剑桥 |'
- en: '| 2D-to-2D Localization | Explicit Map | NN-Net [[131](#bib.bib131)] | Yes
    | 0.21/9.30 | - | combine retrieval and relative pose estimation |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 2D-to-2D 定位 | 显式地图 | NN-Net [[131](#bib.bib131)] | 是 | 0.21/9.30 | - | 结合检索和相对姿态估计
    |'
- en: '| DeLS-3D [[132](#bib.bib132)] | No | - | - | jointly learn with semantics
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| DeLS-3D [[132](#bib.bib132)] | 否 | - | - | 与语义共同学习 |'
- en: '| AnchorNet [[133](#bib.bib133)] | Yes | 0.09/6.74 | 0.84/2.10 | anchor point
    allocation |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| AnchorNet [[133](#bib.bib133)] | 是 | 0.09/6.74 | 0.84/2.10 | 锚点分配 |'
- en: '| RelocNet [[129](#bib.bib129)] | Yes | 0.21/6.73 | - | camera frustum overlap
    loss |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| RelocNet [[129](#bib.bib129)] | 是 | 0.21/6.73 | - | 相机视锥重叠损失 |'
- en: '| CamNet [[134](#bib.bib134)] | Yes | 0.04/1.69 | - | multi-stage image retrieval
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| CamNet [[134](#bib.bib134)] | 是 | 0.04/1.69 | - | 多阶段图像检索 |'
- en: '| Implicit Map | PoseNet [[130](#bib.bib130)] | No | 0.44/10.44 | 2.09/6.84
    | first neural network in global pose regression |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 隐式地图 | PoseNet [[130](#bib.bib130)] | 否 | 0.44/10.44 | 2.09/6.84 | 全局姿态回归中的首个神经网络
    |'
- en: '| Bayesian PoseNet [[135](#bib.bib135)] | No | 0.47/9.81 | 1.92/6.28 | estimate
    Bayesian uncertainty for global pose |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Bayesian PoseNet [[135](#bib.bib135)] | 否 | 0.47/9.81 | 1.92/6.28 | 估计全局姿态的贝叶斯不确定性
    |'
- en: '| BranchNet [[136](#bib.bib136)] | No | 0.29/8.30 | - | multi-task learning
    for orientation and translation |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| BranchNet [[136](#bib.bib136)] | 否 | 0.29/8.30 | - | 用于方向和位移的多任务学习 |'
- en: '| VidLoc [[137](#bib.bib137)] | No | 0.25/- | - | efficient localization from
    image sequences |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| VidLoc [[137](#bib.bib137)] | 否 | 0.25/- | - | 从图像序列中高效定位 |'
- en: '| Geometric PoseNet [[138](#bib.bib138)] | No | 0.23/8.12 | 1.63/2.86 | geometry-aware
    loss |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Geometric PoseNet [[138](#bib.bib138)] | 否 | 0.23/8.12 | 1.63/2.86 | 几何感知损失
    |'
- en: '| SVS-Pose [[139](#bib.bib139)] | No | - | 1.33/5.17 | data augmentation in
    3D space |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SVS-Pose [[139](#bib.bib139)] | 否 | - | 1.33/5.17 | 3D空间中的数据增强 |'
- en: '| LSTM PoseNet [[140](#bib.bib140)] | No | 0.31/9.85 | 1.30/5.52 | spatial
    correlation |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| LSTM PoseNet [[140](#bib.bib140)] | 无 | 0.31/9.85 | 1.30/5.52 | 空间相关性 |'
- en: '| Hourglass PoseNet [[141](#bib.bib141)] | No | 0.23/9.53 | - | hourglass-shaped
    architecture |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Hourglass PoseNet [[141](#bib.bib141)] | 无 | 0.23/9.53 | - | 沙漏形状的架构 |'
- en: '| VLocNet [[142](#bib.bib142)] | No | 0.05/3.80 | 0.78/2.82 | jointly learn
    global localization and odometry |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| VLocNet [[142](#bib.bib142)] | 无 | 0.05/3.80 | 0.78/2.82 | 共同学习全局定位和里程计 |'
- en: '| MapNet [[143](#bib.bib143)] | No | 0.21/7.77 | 1.63/3.64 | impose spatial
    and temporal constraints |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| MapNet [[143](#bib.bib143)] | 无 | 0.21/7.77 | 1.63/3.64 | 强加空间和时间约束 |'
- en: '| SPP-Net [[144](#bib.bib144)] | No | 0.18/6.20 | 1.24/2.68 | synthetic data
    augmentation |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| SPP-Net [[144](#bib.bib144)] | 无 | 0.18/6.20 | 1.24/2.68 | 合成数据增强 |'
- en: '| GPoseNet [[145](#bib.bib145)] | No | 0.30/9.90 | 2.00/4.60 | hybrid model
    with Gaussian Process Regressor |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| GPoseNet [[145](#bib.bib145)] | 无 | 0.30/9.90 | 2.00/4.60 | 与高斯过程回归器的混合模型
    |'
- en: '| VLocNet++ [[146](#bib.bib146)] | No | 0.02/1.39 | - | jointly learn with
    odometry and semantics |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| VLocNet++ [[146](#bib.bib146)] | 无 | 0.02/1.39 | - | 共同学习里程计和语义 |'
- en: '| LSG [[147](#bib.bib147)] | No | 0.19/7.47 | - | odometry-aided localization
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| LSG [[147](#bib.bib147)] | 无 | 0.19/7.47 | - | 里程计辅助定位 |'
- en: '| PVL [[148](#bib.bib148)] | No | - | 1.60/4.21 | prior-guided dropout mask
    to improve robustness |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PVL [[148](#bib.bib148)] | 无 | - | 1.60/4.21 | 基于先验的 dropout 掩码以提高鲁棒性 |'
- en: '| AdPR [[149](#bib.bib149)] | No | 0.22/8.8 | - | adversarial architecture
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| AdPR [[149](#bib.bib149)] | 无 | 0.22/8.8 | - | 对抗性架构 |'
- en: '| AtLoc [[150](#bib.bib150)] | No | 0.20/7.56 | - | attention-guided spatial
    correlation |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| AtLoc [[150](#bib.bib150)] | 无 | 0.20/7.56 | - | 注意力引导的空间相关性 |'
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Agnostic 表示它是否能够推广到新场景。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance reports the position (m) and orientation (degree) error (a small
    number is better) on the 7-Scenes (Indoor)[[151](#bib.bib151)] and Cambridge (Outdoor)
    dataset[[130](#bib.bib130)]. Both datasets are split into training and testing
    set. We report the averaged error on the testing set.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能报告了在 7-Scenes（室内）[[151](#bib.bib151)] 和剑桥（室外）数据集[[130](#bib.bib130)]上位置（米）和方向（度）的误差（数字越小越好）。这两个数据集都被分为训练集和测试集。我们报告了测试集上的平均误差。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献总结了每项工作的主要贡献，与以往研究相比。
- en: 5.1.1 Explicit Map Based Localization
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 显式地图基础定位
- en: 'Explicit map based 2D-to-2D localization typically represents the scene by
    a database of geo-tagged images (references) [[152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154)]. Figure [8](#S5.F8 "Figure 8 ‣ 5.1 2D-to-2D Localization ‣
    5 Global Localization ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") (a) illustrates the two stages
    of this localization with 2D references: image retrieval determines the most relevant
    part of a scene represented by reference images to the visual queries; pose regression
    obtains the relative pose of query image with respect to the reference images.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '显式地图基础的 2D 到 2D 定位通常通过地理标记图像（参考）数据库来表示场景 [[152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154)]。图 [8](#S5.F8 "Figure 8 ‣ 5.1 2D-to-2D Localization ‣ 5 Global
    Localization ‣ A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence") (a) 说明了这种基于 2D 参考的定位的两个阶段：图像检索确定与视觉查询最相关的场景部分，由参考图像表示；姿态回归获取查询图像相对于参考图像的相对姿态。'
- en: One problem here is how to find suitable image descriptors for image retrieval.
    Deep learning based approaches [[155](#bib.bib155), [156](#bib.bib156)] are based
    on a pre-trained ConvNet model to extract image-level features, and then use these
    features to evaluate the similarities against other images. In challenging situations,
    local descriptors are first extracted, followed by being aggregated to obtain
    robust global descriptors. A good example is NetVLAD [[157](#bib.bib157)] that
    designs a trainable generalized VLAD (the Vector of Locally Aggregated Descriptors)
    layer. This VLAD layer can be plugged into the off-the-shelf ConvNet architecture
    to encourage better descriptors learning for image retrieval.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题之一是如何找到合适的图像描述符进行图像检索。基于深度学习的方法 [[155](#bib.bib155), [156](#bib.bib156)]
    基于预训练的 ConvNet 模型提取图像级特征，然后使用这些特征评估与其他图像的相似性。在具有挑战性的情况下，首先提取局部描述符，然后将其聚合以获得鲁棒的全局描述符。一个好的例子是
    NetVLAD [[157](#bib.bib157)]，它设计了一个可训练的广义 VLAD（局部聚合描述符向量）层。这个 VLAD 层可以插入到现成的 ConvNet
    架构中，以鼓励更好的描述符学习用于图像检索。
- en: In order to obtain more precise poses of the queries, additional relative pose
    estimation with respect to the retrieved images is required. Traditionally, relative
    pose estimation is tackled by epipolar geometry, relying on the 2D-2D correspondences
    determined by local descriptors [[158](#bib.bib158), [159](#bib.bib159)]. In contrast,
    deep learning approaches regress the relative poses straightforwardly from pairwise
    images. For example, NN-Net [[131](#bib.bib131)] utilized neural network to estimate
    the pairwise relative poses between the query and the top N ranked references.
    A triangulation-based fusion algorithm coalesces the predicted N relative poses
    and the ground truth of 3D geometry poses, and the absolute query pose can be
    naturally calculated. Furthermore, Relocnet [[129](#bib.bib129)] introduces a
    frustum overlap loss to assist global descriptors learning that are suitable for
    camera localization. Motivated by these, CamNet [[134](#bib.bib134)] applies two
    stages retrieval, image-based coarse retrieval and pose-based fine retrieval,
    to select the most similar reference frames for finally precise pose estimation.
    Without the need of training on specific scenarios, reference-based approaches
    are naturally scalable and flexible to be utilized in new scenarios. Since reference-based
    methods need to maintain a database of geo-tagged images, they are more trivial
    to scale to large-scale scenarios, compared with the structure-based counterparts.
    Overall, these image retrieval based methods achieve a trade-off between accuracy
    and scalability.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更精确的查询姿态，需要对检索到的图像进行额外的相对姿态估计。传统上，相对姿态估计通过极线几何来处理，依赖于由局部描述符确定的2D-2D对应关系[[158](#bib.bib158),
    [159](#bib.bib159)]。相对而言，深度学习方法直接从成对图像中回归相对姿态。例如，NN-Net [[131](#bib.bib131)] 利用神经网络估计查询图像与排名前N的参考图像之间的成对相对姿态。一个基于三角测量的融合算法将预测的N个相对姿态与3D几何姿态的真实值结合，最终自然计算绝对查询姿态。此外，Relocnet
    [[129](#bib.bib129)] 引入了一个视锥体重叠损失来帮助学习适用于相机定位的全局描述符。受到这些方法的启发，CamNet [[134](#bib.bib134)]
    采用了两个阶段的检索，即基于图像的粗略检索和基于姿态的精细检索，以选择最相似的参考帧来实现最终精确的姿态估计。由于不需要在特定场景中进行训练，基于参考的方法在新场景中自然具有可扩展性和灵活性。由于基于参考的方法需要维护一个地理标记图像的数据库，相比于基于结构的方法，它们在大规模场景中的扩展更加简单。总体而言，这些基于图像检索的方法在准确性和可扩展性之间达成了一种平衡。
- en: 5.1.2 Implicit Map Based Localization
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 隐式地图基础定位
- en: 'Implicit map based localization directly regresses camera pose from single
    images, by implicitly representing the structure of entire scene inside a deep
    neural network. The common pipeline is illustrated in Figure [8](#S5.F8 "Figure
    8 ‣ 5.1 2D-to-2D Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    (b) - the input to a neural network is single images, while the output is the
    global position and orientation of query images.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '隐式地图基础定位通过在深度神经网络中隐式表示整个场景的结构，直接从单幅图像中回归相机姿态。常见的流程如图[8](#S5.F8 "Figure 8 ‣
    5.1 2D-to-2D Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")（b）所示
    - 神经网络的输入是单幅图像，而输出是查询图像的全球位置和方向。'
- en: PoseNet [[130](#bib.bib130)] is the first work to tackle camera relocalization
    problem by training a ConvNet to predict camera pose from single RGB images in
    an end-to-end manner. PoseNet is based on the main structure of GoogleNet [[160](#bib.bib160)]
    to extract visual features,, but removes the last softmax layers. Instead, a fully
    connected layer was introduced to output a 7 dimensional global pose, consisting
    of position and orientation vector in 3 and 4 dimensions respectively. However,
    PoseNet was designed with a naive regression loss function without any consideration
    for geometry, in which the hyper-parameters inside requires expensive hand-engineering
    to be tuned. Furthermore, it also suffers from the overfitting problem due to
    the high dimensionality of the feature embedding and limited training data. Thus
    various extensions enhance the original pipeline by exploiting LSTM units to reduce
    the dimensionality [[140](#bib.bib140)], applying synthetic generation to augment
    training data [[139](#bib.bib139), [136](#bib.bib136), [144](#bib.bib144)], replacing
    backbone with ResNet34 [[141](#bib.bib141)], modelling pose uncertainty [[135](#bib.bib135),
    [145](#bib.bib145)] and introducing geometry-aware loss function [[138](#bib.bib138)].
    Alternatively, Atloc [[150](#bib.bib150)] associates the features in spatial domain
    with attention mechanism, which encourages the network to focus on parts of the
    image that are temporally consistent and robust. Similarly, a prior guided dropout
    mask is additionally adopted in RVL [[148](#bib.bib148)] to further eliminate
    the uncertainty caused by dynamic objects. Different such methods only considering
    spatial connections, VidLoc [[137](#bib.bib137)] incorporates temporal constraints
    of image sequences to model the temporal connections of input images for visual
    localization. Moreover, additional motion constraints, including spatial constraints
    and other sensor constraints from GPS or SLAM systems are exploited in MapNet
    [[143](#bib.bib143)], to enforce the motion consistency between predicted poses.
    Similar motion constraints are also added by jointly optimizing a relocalization
    network and visual odometry network [[142](#bib.bib142), [147](#bib.bib147)].
    However, being application-specific, scene representations learned from localization
    tasks may ignore some useful features they are not designed for. Out of this,
    VLocNet++ [[146](#bib.bib146)] and FGSN [[161](#bib.bib161)] additionally exploits
    the inter-task relationship among learning semantics and regressing poses, achieving
    impressive results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: PoseNet [[130](#bib.bib130)] 是首个通过训练卷积网络从单张RGB图像中以端到端的方式预测相机姿态来解决相机重新定位问题的工作。PoseNet
    基于 GoogleNet [[160](#bib.bib160)] 的主要结构来提取视觉特征，但去掉了最后的 softmax 层。相反，引入了一个全连接层来输出一个
    7 维的全局姿态，其中包括 3 维和 4 维的位置信息和方向向量。然而，PoseNet 设计时使用了一个简单的回归损失函数，未考虑几何信息，因此其超参数需要昂贵的手动调整。此外，由于特征嵌入的高维度和有限的训练数据，PoseNet
    也面临过拟合问题。因此，各种扩展方法通过利用 LSTM 单元来降低维度 [[140](#bib.bib140)]、应用合成生成来增强训练数据 [[139](#bib.bib139),
    [136](#bib.bib136), [144](#bib.bib144)]、用 ResNet34 替换骨干网络 [[141](#bib.bib141)]、建模姿态不确定性
    [[135](#bib.bib135), [145](#bib.bib145)] 和引入几何感知损失函数 [[138](#bib.bib138)] 来增强原始流程。另一方面，Atloc
    [[150](#bib.bib150)] 在空间域中与注意机制关联特征，这鼓励网络关注在时间上具有一致性和鲁棒性的图像部分。同样，RVL [[148](#bib.bib148)]
    还采用了一个先验引导的 dropout 掩膜，以进一步消除动态物体引起的不确定性。不同的这种方法仅考虑空间连接，VidLoc [[137](#bib.bib137)]
    综合了图像序列的时间约束，以建模输入图像的时间连接用于视觉定位。此外，MapNet [[143](#bib.bib143)] 在其中还利用了额外的运动约束，包括空间约束和来自
    GPS 或 SLAM 系统的其他传感器约束，以强制预测姿态之间的一致性。类似的运动约束也通过联合优化重新定位网络和视觉里程计网络 [[142](#bib.bib142),
    [147](#bib.bib147)] 添加。然而，作为应用特定的，定位任务中学习到的场景表示可能忽略了一些它们未设计的有用特征。为此，VLocNet++
    [[146](#bib.bib146)] 和 FGSN [[161](#bib.bib161)] 额外利用了学习语义和回归姿态之间的任务间关系，取得了令人印象深刻的结果。
- en: Implicit map based localization approaches take the advantages of deep learning
    in automatically extracting features, that play a vital role in global localization
    in featureless environments, where conventional methods are prone to fail. However,
    the requirement of scene-specific training prohibits it from generalizing to unseen
    scenes without being retrained. Also, current implicit map based approaches have
    not shown comparable performance over other explicit map based methods [[162](#bib.bib162)].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式地图基础的定位方法利用深度学习自动提取特征，这些特征在无特征环境中的全局定位中扮演着至关重要的角色，而传统方法则容易失败。然而，场景特定的训练需求使其无法在不重新训练的情况下推广到未见过的场景。此外，目前的隐式地图基础方法在性能上未能与其他显式地图基础方法[[162](#bib.bib162)]相媲美。
- en: 5.2 2D-to-3D Localization
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 2D到3D定位
- en: '2D-to-3D localization refers to methods that recover the camera pose of a 2D
    image with respect to a 3D scene model. This 3D map is pre-built before performing
    global localization, via approaches such as structure from motion (SfM)[[43](#bib.bib43)].
    As shown in Figure [9](#S5.F9 "Figure 9 ‣ 5.2 2D-to-3D Localization ‣ 5 Global
    Localization ‣ A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence"), 2D-to-3D approaches establish 2D-3D
    correspondences between the 2D pixels of query image and the 3D points of scene
    model through local descriptor matching [[163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165)] or by regressing 3D coordinates from pixel patches [[166](#bib.bib166),
    [167](#bib.bib167), [151](#bib.bib151), [168](#bib.bib168)]. Such 2D-3D matches
    are then used to calculate camera pose by applying a Perspective-n-Point (PnP)
    solver [[169](#bib.bib169), [170](#bib.bib170)] inside a RANSAC loop [[171](#bib.bib171)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 2D到3D定位指的是恢复相对于3D场景模型的2D图像的相机姿态的方法。这个3D地图在进行全局定位之前是预先构建的，通过结构从运动（SfM）[[43](#bib.bib43)]等方法。如图[9](#S5.F9
    "图 9 ‣ 5.2 2D到3D定位 ‣ 5 全局定位 ‣ 深度学习在定位和映射中的应用：迈向空间机器智能的时代")所示，2D到3D方法通过局部描述符匹配[[163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165)]或通过从像素块回归3D坐标[[166](#bib.bib166), [167](#bib.bib167),
    [151](#bib.bib151), [168](#bib.bib168)]，在查询图像的2D像素和场景模型的3D点之间建立2D-3D对应关系。这些2D-3D匹配然后用于通过在RANSAC循环[[171](#bib.bib171)]内应用透视n点（PnP）求解器[[169](#bib.bib169),
    [170](#bib.bib170)]来计算相机姿态。
- en: '![Refer to caption](img/057688a56aed8b968d7a138a0718318c.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/057688a56aed8b968d7a138a0718318c.png)'
- en: 'Figure 9: The typical architectures of 2D-to-3D based localization through
    (a) descriptor matching, i.e. HF-Net [[172](#bib.bib172)] and (b) scene coordinate
    regression, i.e. Confidence SCR [[173](#bib.bib173)].'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：基于2D到3D的定位的典型架构，通过（a）描述符匹配，即 HF-Net [[172](#bib.bib172)] 和（b）场景坐标回归，即 Confidence
    SCR [[173](#bib.bib173)]。
- en: 'TABLE IV: A summary on existing methods on deep learning for 2D-to-3D global
    localization'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：现有的深度学习2D到3D全局定位方法的总结
- en: '| Model | Agnostic | Performance (m/degree) | Contributions |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 通用性 | 性能 (m/度) | 贡献 |'
- en: '| 7Scenes | Cambridge |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 7Scenes | 剑桥 |'
- en: '| 2D-3D Localization | Descriptor Based | NetVLAD [[157](#bib.bib157)] | Yes
    | - | - | differentiable VLAD layer |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 2D-3D定位 | 基于描述符 | NetVLAD [[157](#bib.bib157)] | 是 | - | - | 可微的VLAD层 |'
- en: '| DELF [[174](#bib.bib174)] | Yes | - | - | attentive local feature descriptor
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| DELF [[174](#bib.bib174)] | 是 | - | - | 专注的局部特征描述符 |'
- en: '| InLoc [[175](#bib.bib175)] | Yes | 0.04/1.38 | 0.31/0.73 | dense data association
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| InLoc [[175](#bib.bib175)] | 是 | 0.04/1.38 | 0.31/0.73 | 密集数据关联 |'
- en: '| SVL [[176](#bib.bib176)] | No | - | - | leverage a generative model for descriptor
    learning |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| SVL [[176](#bib.bib176)] | 否 | - | - | 利用生成模型进行描述符学习 |'
- en: '| SuperPoint [[177](#bib.bib177)] | Yes | - | - | jointly extract interest
    points and descriptors |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| SuperPoint [[177](#bib.bib177)] | 是 | - | - | 联合提取兴趣点和描述符 |'
- en: '| Sarlin et al. [[178](#bib.bib178)] | Yes | - | - | hierarchical localization
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Sarlin et al. [[178](#bib.bib178)] | 是 | - | - | 层次化定位 |'
- en: '| NC-Net [[179](#bib.bib179)] | Yes | - | - | neighbourhood consensus constraints
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| NC-Net [[179](#bib.bib179)] | 是 | - | - | 邻域共识约束 |'
- en: '| 2D3D-MatchNet [[180](#bib.bib180)] | Yes | - | - | jointly learn the descriptors
    for 2D and 3D keypoints |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 2D3D-MatchNet [[180](#bib.bib180)] | 是 | - | - | 联合学习2D和3D关键点的描述符 |'
- en: '| Unsuperpoint [[181](#bib.bib181)] | Yes | - | - | unsupervised detector and
    descriptor learning |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Unsuperpoint [[181](#bib.bib181)] | 是 | - | - | 无监督的检测器和描述符学习 |'
- en: '| HF-Net [[172](#bib.bib172)] | Yes | - | - | coarse-to-fine localization |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| HF-Net [[172](#bib.bib172)] | 是 | - | - | 粗到精的定位 |'
- en: '| D2-Net [[182](#bib.bib182)] | Yes | - | - | jointly learn keypoints and descriptors
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| D2-Net [[182](#bib.bib182)] | 是 | - | - | 联合学习关键点和描述符 |'
- en: '| Speciale et al [[183](#bib.bib183)] | No | - | - | privacy preserving localization
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Speciale等 [[183](#bib.bib183)] | 否 | - | - | 隐私保护定位 |'
- en: '| OOI-Net [[184](#bib.bib184)] | No | - | - | objects-of-interest annotations
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| OOI-Net [[184](#bib.bib184)] | 否 | - | - | 感兴趣物体的注释 |'
- en: '| Camposeco et al. [[185](#bib.bib185)] | Yes | - | 0.56/0.66 | hybrid scene
    compression for localization |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Camposeco等 [[185](#bib.bib185)] | 是 | - | 0.56/0.66 | 混合场景压缩用于定位 |'
- en: '| Cheng et al. [[186](#bib.bib186)] | Yes | - | - | cascaded parallel filtering
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Cheng等 [[186](#bib.bib186)] | 是 | - | - | 级联并行滤波 |'
- en: '| Taira et al. [[187](#bib.bib187)] | Yes | - | - | comprehensive analysis
    of pose verification |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Taira等 [[187](#bib.bib187)] | 是 | - | - | 姿态验证的综合分析 |'
- en: '| R2D2 [[188](#bib.bib188)] | Yes | - | - | learn a predictor of the descriptor
    discriminativeness |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| R2D2 [[188](#bib.bib188)] | 是 | - | - | 学习描述符的判别能力预测器 |'
- en: '| ASLFeat [[189](#bib.bib189)] | Yes | - | - | leverage deformable convolutional
    networks |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[189](#bib.bib189)] | 是 | - | - | 利用可变形卷积网络 |'
- en: '| Scene Coordinate Regression | DSAC [[190](#bib.bib190)] | No | 0.20/6.3 |
    0.32/0.78 | differentiable RANSAC |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 场景坐标回归 | DSAC [[190](#bib.bib190)] | 否 | 0.20/6.3 | 0.32/0.78 | 可微的RANSAC
    |'
- en: '| DSAC++ [[191](#bib.bib191)] | No | 0.08/2.40 | 0.19/0.50 | without using
    a 3D model of the scene |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| DSAC++ [[191](#bib.bib191)] | 否 | 0.08/2.40 | 0.19/0.50 | 不使用场景的3D模型 |'
- en: '| Angle DSAC++ [[192](#bib.bib192)] | No | 0.06/1.47 | 0.17/0.50 | angle-based
    reprojection loss |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Angle DSAC++ [[192](#bib.bib192)] | 否 | 0.06/1.47 | 0.17/0.50 | 基于角度的重投影损失
    |'
- en: '| Dense SCR [[193](#bib.bib193)] | No | 0.04/1.4 | - | full frame scene coordinate
    regression |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Dense SCR [[193](#bib.bib193)] | 否 | 0.04/1.4 | - | 全帧场景坐标回归 |'
- en: '| Confidence SCR [[173](#bib.bib173)] | No | 0.06/3.1 | - | model uncertainty
    of correspondences |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Confidence SCR [[173](#bib.bib173)] | 否 | 0.06/3.1 | - | 对应关系的模型不确定性 |'
- en: '| ESAC [[194](#bib.bib194)] | No | 0.034/1.50 | - | integrates DSAC in a Mixture
    of Experts |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| ESAC [[194](#bib.bib194)] | 否 | 0.034/1.50 | - | 将DSAC整合到专家混合中 |'
- en: '| NG-RANSAC [[195](#bib.bib195)] | No | - | 0.24/0.30 | prior-guided model
    hypothesis search |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| NG-RANSAC [[195](#bib.bib195)] | 否 | - | 0.24/0.30 | 先验引导的模型假设搜索 |'
- en: '| SANet [[196](#bib.bib196)] | Yes | 0.05/1.68 | 0.23/0.53 | scene agnostic
    architecture for camera localization |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[196](#bib.bib196)] | 是 | 0.05/1.68 | 0.23/0.53 | 针对相机定位的场景无关架构 |'
- en: '| MV-SCR [[197](#bib.bib197)] | No | 0.05/1.63 | 0.17/0.40 | multi-view constraints
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| MV-SCR [[197](#bib.bib197)] | 否 | 0.05/1.63 | 0.17/0.40 | 多视角约束 |'
- en: '| HSC-Net [[198](#bib.bib198)] | No | 0.03/0.90 | 0.13/0.30 | hierarchical
    scene coordinate network |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| HSC-Net [[198](#bib.bib198)] | 否 | 0.03/0.90 | 0.13/0.30 | 层次化场景坐标网络 |'
- en: '| KFNet [[199](#bib.bib199)] | No | 0.03/0.88 | 0.13/0.30 | extends the problem
    to the time domain |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| KFNet [[199](#bib.bib199)] | 否 | 0.03/0.88 | 0.13/0.30 | 将问题扩展到时间域 |'
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Agnostic表示它是否可以推广到新的场景。
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance reports the position (m) and orientation (degree) error (a small
    number is better) on the 7-Scenes (Indoor)[[151](#bib.bib151)] and Cambridge (Outdoor)
    dataset[[130](#bib.bib130)]. Both datasets are split into training and testing
    set. We report the averaged error on the testing set.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能报告了在7-Scenes（室内）[[151](#bib.bib151)]和Cambridge（室外）数据集[[130](#bib.bib130)]上的位置（m）和方向（degree）误差（小的数字更好）。这两个数据集都被分为训练集和测试集。我们报告了在测试集上的平均误差。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献总结了每项工作的主要贡献，与之前的研究进行比较。
- en: 5.2.1 Descriptor Matching Based Localization
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 描述符匹配基础的定位
- en: 'Descriptor matching methods mainly rely on feature detector and descriptor,
    and establish the correspondences between the features from 2D input and 3D model.
    They can be further divided into three types: detect-then-describe, detect-and-describe,
    and describe-to-detect, according to the role of detector and descriptor in the
    learning process.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符匹配方法主要依赖于特征检测器和描述符，并建立2D输入与3D模型之间的特征对应关系。根据检测器和描述符在学习过程中的角色，它们可以进一步分为三种类型：先检测再描述、检测和描述、以及描述再检测。
- en: Detect-then-describe approach first performs feature detection and then extracts
    a feature descriptor from a patch centered around each keypoint [[200](#bib.bib200),
    [201](#bib.bib201)]. The keypoint detector is typically responsible for providing
    robustness or invariance against possible real issues such as scale transformation,
    rotation, or viewpoint changes by normalizing the patch accordingly. However,
    some of these responsibilities might also be delegated to the descriptor. The
    common pipeline varies from using hand-crafted detectors [[202](#bib.bib202),
    [203](#bib.bib203)] and descriptors [[204](#bib.bib204), [205](#bib.bib205)],
    replacing either the descriptor [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [179](#bib.bib179), [209](#bib.bib209), [210](#bib.bib210)] or detector [[211](#bib.bib211),
    [212](#bib.bib212), [213](#bib.bib213)] with a learned alternative, or learning
    both the detector and descriptor [[214](#bib.bib214), [215](#bib.bib215)]. For
    efficiency, the feature detector often considers only small image regions and
    typically focuses on low-level structures such as corners or blobs [[216](#bib.bib216)].
    The descriptor then captures higher level information in a larger patch around
    the keypoint.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 检测-然后-描述方法首先执行特征检测，然后从围绕每个关键点的补丁中提取特征描述符 [[200](#bib.bib200), [201](#bib.bib201)]。关键点检测器通常负责提供对可能的实际问题（如尺度变换、旋转或视角变化）的鲁棒性或不变性，通过相应地归一化补丁。然而，这些职责中的一些也可能被委托给描述符。常见的流程包括使用手工制作的检测器
    [[202](#bib.bib202), [203](#bib.bib203)] 和描述符 [[204](#bib.bib204), [205](#bib.bib205)]，用学习的替代品替换描述符
    [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208), [179](#bib.bib179),
    [209](#bib.bib209), [210](#bib.bib210)] 或检测器 [[211](#bib.bib211), [212](#bib.bib212),
    [213](#bib.bib213)]，或同时学习检测器和描述符 [[214](#bib.bib214), [215](#bib.bib215)]。为了提高效率，特征检测器通常只考虑小的图像区域，并且通常专注于低级结构，如角点或斑点
    [[216](#bib.bib216)]。然后，描述符在关键点周围较大的补丁中捕获更高级的信息。
- en: In contrast, detect-and-describe approaches advance description stage. By sharing
    a representation from deep neural network, SuperPoint [[177](#bib.bib177)], UnSuperPoint
    [[181](#bib.bib181)] and R2D2 [[188](#bib.bib188)] attempt to learn a dense feature
    descriptor and a feature detector. However, they rely on different decoder branches
    which are trained independently with specific losses. On the contrary, D2-net
    [[182](#bib.bib182)] and ASLFeat [[189](#bib.bib189)] shares all parameters between
    detection and description and uses a joint formulation that simultaneously optimizes
    for both tasks.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，检测-描述方法推进了描述阶段。通过共享深度神经网络的表示，SuperPoint [[177](#bib.bib177)]、UnSuperPoint
    [[181](#bib.bib181)] 和 R2D2 [[188](#bib.bib188)] 尝试学习密集的特征描述符和特征检测器。然而，它们依赖于不同的解码器分支，这些分支通过特定的损失函数独立训练。相反，D2-net
    [[182](#bib.bib182)] 和 ASLFeat [[189](#bib.bib189)] 在检测和描述之间共享所有参数，并使用一种联合公式同时优化这两个任务。
- en: Similarly, the describe-to-detect approach, e.g. D2D [[217](#bib.bib217)], also
    postpones the detection to a later stage but applies such detector on pre-learned
    dense descriptors to extract a sparse set of keypoints and corresponding descriptors.
    Dense feature extraction foregoes the detection stage and performs the description
    stage densely across the whole image [[218](#bib.bib218), [219](#bib.bib219),
    [220](#bib.bib220), [176](#bib.bib176)]. In practice, this approach has shown
    to lead to better matching results than sparse feature matching, particularly
    under strong variations in illumination [[221](#bib.bib221), [222](#bib.bib222)].
    Different from these works, which purely rely on image features, 2D3D-MatchNet
    [[180](#bib.bib180)] proposed to learn local descriptors that allow direct matching
    of key points across a 2D image and 3D point cloud. Similarly, LCD [[223](#bib.bib223)]
    introduced a dual auto-encoder architecture to extract cross-domain local descriptors.
    However, they still require pre-defined 2D and 3D keypoints separately, which
    will result in poor matching results caused by inconsistent keypoint selection
    rules.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，描述-检测方法，例如 D2D [[217](#bib.bib217)]，也将检测推迟到后期，但将这种检测器应用于预学习的密集描述符上，以提取稀疏的关键点集合及其对应的描述符。密集特征提取跳过检测阶段，在整个图像上密集地进行描述
    [[218](#bib.bib218), [219](#bib.bib219), [220](#bib.bib220), [176](#bib.bib176)]。在实际应用中，这种方法已显示出比稀疏特征匹配更好的匹配结果，尤其是在光照变化较大的情况下
    [[221](#bib.bib221), [222](#bib.bib222)]。不同于这些纯粹依赖于图像特征的工作，2D3D-MatchNet [[180](#bib.bib180)]
    提出了学习局部描述符的方法，允许直接匹配 2D 图像和 3D 点云中的关键点。同样，LCD [[223](#bib.bib223)] 介绍了一种双重自编码器架构来提取跨域局部描述符。然而，它们仍然需要单独定义
    2D 和 3D 关键点，这会导致由于不一致的关键点选择规则而产生较差的匹配结果。
- en: '![Refer to caption](img/39fe3effac3a73f1698d0c8c617d6715.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/39fe3effac3a73f1698d0c8c617d6715.png)'
- en: 'Figure 10: The typical architecture of 3D-to-3D localization, e.g. L3-Net [[224](#bib.bib224)].'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 3D 到 3D 定位的典型架构，例如 L3-Net [[224](#bib.bib224)]。'
- en: 'TABLE V: A summary of existing approaches on deep learning for 3D-to-3D localization'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 深度学习在 3D 到 3D 定位上的现有方法总结'
- en: '| Models | Agnostic | Contributions |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 泛化性 | 贡献 |'
- en: '| --- | --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LocNet[[225](#bib.bib225)] | No | convert 3D points into 2D matrix, search
    in global prior map |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| LocNet[[225](#bib.bib225)] | 否 | 将 3D 点转换为 2D 矩阵，在全局先验图中搜索 |'
- en: '| PointNetVLAD[[226](#bib.bib226)] | Yes | learn global descriptor from point
    clouds |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| PointNetVLAD[[226](#bib.bib226)] | 是 | 从点云中学习全局描述符 |'
- en: '| Barsan et al.[[227](#bib.bib227)] | No | learn from LIDAR intensity maps
    and online point clouds |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Barsan et al.[[227](#bib.bib227)] | 否 | 从 LIDAR 强度图和在线点云中学习 |'
- en: '| L3-Net[[224](#bib.bib224)] | No | extract feature by PointNet |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| L3-Net[[224](#bib.bib224)] | 否 | 通过 PointNet 提取特征 |'
- en: '| PCAN[[228](#bib.bib228)] | Yes | predict the significance of each local point
    based on context |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| PCAN[[228](#bib.bib228)] | 是 | 根据上下文预测每个局部点的重要性 |'
- en: '| DeepICP[[229](#bib.bib229)] | Yes | generate matching correspondence from
    learned matching probabilities |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| DeepICP[[229](#bib.bib229)] | 是 | 从学习到的匹配概率生成匹配对应 |'
- en: '| DCP[[230](#bib.bib230)] | Yes | a learning based iterative closest point
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| DCP[[230](#bib.bib230)] | 是 | 基于学习的迭代最近点 |'
- en: '| D3Feat[[231](#bib.bib231)] | Yes | jointly learn detector and descriptors
    for 3D points |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| D3Feat[[231](#bib.bib231)] | 是 | 联合学习 3D 点的检测器和描述符 |'
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 泛化性指它是否能够推广到新场景。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献总结了每项工作相对于以往研究的主要贡献。
- en: 5.2.2 Scene Coordinate Regression Based Localization
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 场景坐标回归基础的定位
- en: Different from match-based methods that establish 2D-3D correspondences before
    calculating pose, scene coordinate regression approaches estimate the 3D coordinates
    of each pixel from the query image within the world coordinate system, i.e. the
    scene coordinates. It can be viewed as learning a transformation from the query
    image to the global coordinates of the scene. DSAC [[190](#bib.bib190)] utilizes
    a ConvNet model to regress scene coordinates, followed by a novel differentiable
    RANSAC to allow end-to-end training of the whole pipeline. Such common pipeline
    was then improved by introducing the reprojection loss [[191](#bib.bib191), [232](#bib.bib232),
    [192](#bib.bib192)] or multi-view geometric constraints [[197](#bib.bib197)] to
    enable unsupervised learning, jointly learning the observation confidences [[173](#bib.bib173),
    [195](#bib.bib195)] to enhance the sampling efficiency and accuracy, exploiting
    Mixture of Experts (MoE) strategy [[194](#bib.bib194)] or hierarchical coarse-to-fine
    [[198](#bib.bib198)] to eliminate environment ambiguities. Different from these,
    KFNet [[199](#bib.bib199)] extends the scene coordinate regression problem to
    the time domain and thus bridges the existing performance gap between temporal
    and one-shot relocalization approaches. However, they still trained for a specific
    scene and cannot be generalized to unseen scenes without retraining. To build
    a scene agnostic method, SANet [[196](#bib.bib196)] regress the scene coordinate
    map of the query by interpolating the 3D points associated with retrieved scene
    images. Unlike aforementioned methods trained in a patch-based manner, Dense SCR
    [[193](#bib.bib193)] propose to perform the scene coordinate regression in a full-frame
    manner to make the computation efficient at test time and, more importantly, to
    add more global context to the regression process to improve the robustness.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 与在计算姿态之前建立 2D-3D 对应关系的匹配方法不同，场景坐标回归方法估计来自查询图像的每个像素在世界坐标系统中的 3D 坐标，即场景坐标。这可以看作是学习从查询图像到场景的全局坐标的变换。DSAC
    [[190](#bib.bib190)] 利用 ConvNet 模型回归场景坐标，随后通过新型的可微分 RANSAC 允许整个流程的端到端训练。然后通过引入重投影损失
    [[191](#bib.bib191), [232](#bib.bib232), [192](#bib.bib192)] 或多视角几何约束 [[197](#bib.bib197)]
    来改进这种常见流程，以实现无监督学习，联合学习观测置信度 [[173](#bib.bib173), [195](#bib.bib195)] 以提高采样效率和准确性，利用专家混合（MoE）策略
    [[194](#bib.bib194)] 或层次化粗到细 [[198](#bib.bib198)] 来消除环境歧义。与这些方法不同，KFNet [[199](#bib.bib199)]
    将场景坐标回归问题扩展到时间域，从而弥合了时间性和一次性重定位方法之间的现有性能差距。然而，它们仍然针对特定场景进行训练，无法在不重新训练的情况下推广到未见过的场景。为了构建场景无关的方法，SANet
    [[196](#bib.bib196)] 通过插值与检索场景图像相关联的 3D 点来回归查询的场景坐标图。与上述以补丁为基础的训练方法不同，Dense SCR
    [[193](#bib.bib193)] 提出在全帧模式下执行场景坐标回归，以提高测试时计算效率，更重要的是，为回归过程添加更多全局上下文，以提高鲁棒性。
- en: Scene coordinate regression methods often perform better robustness and higher
    accuracy under small indoor scenarios, outperforming traditional algorithms such
    as [[18](#bib.bib18)]. But they have not yet proven their capacity in large-scale
    scenes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 场景坐标回归方法在小型室内场景中通常表现出更好的鲁棒性和更高的准确性，超越了传统算法，如[[18](#bib.bib18)]。但它们尚未证明在大规模场景中的能力。
- en: 5.3 3D-to-3D Localization
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 3D-to-3D 定位
- en: '3D-to-3D localization (or LIDAR localization) refers to methods that recover
    the global pose of 3D points (i.e. LIDAR point cloud scans) against a pre-built
    3D map by establishing a 3D-to-3D correspondence matching. Figure [10](#S5.F10
    "Figure 10 ‣ 5.2.1 Descriptor Matching Based Localization ‣ 5.2 2D-to-3D Localization
    ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") shows the pipeline of 3D-to-3D
    localization: online scans or predicted coarse poses are applied to query the
    most similar 3D map data, which are further used for precise localization by calculating
    the offset between predicted poses and ground truths or estimating the relative
    poses between online scans and queried scene.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '3D-to-3D 定位（或 LIDAR 定位）指的是通过建立 3D-to-3D 对应匹配，恢复 3D 点（即 LIDAR 点云扫描）的全局姿态的方法。图
    [10](#S5.F10 "Figure 10 ‣ 5.2.1 Descriptor Matching Based Localization ‣ 5.2 2D-to-3D
    Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") 展示了 3D-to-3D 定位的流程：在线扫描或预测的粗略姿态用于查询最相似的
    3D 地图数据，这些数据进一步用于通过计算预测姿态和实际姿态之间的偏差或估计在线扫描与查询场景之间的相对姿态进行精确定位。'
- en: By formulating LIDAR localization as a recursive Bayesian inference problem,
    [[227](#bib.bib227)] embeds both LIDAR intensity maps and online point cloud sweeps
    in a sharing space for fully differentiable pose estimation. Instead of operating
    on 3D data directly, LocNet [[225](#bib.bib225)] converts point cloud scans to
    2D rotational invariant representation for searching similar frames in the global
    prior map, and then performs the iterative closest point (ICP) methods to calculate
    global pose. Towards proposing a learning based LIDAR localization framework that
    directly processes point clouds, L3-Net [[224](#bib.bib224)] processes point cloud
    data with PointNet [[125](#bib.bib125)] to extract feature descriptors that encode
    certain useful properties, and models the temporal connections of motion dynamics
    via a recurrent neural network. It optimizes the loss between the predicted poses
    and ground truth values by minimizing the matching distance between the point
    cloud input and the 3D map. Some techniques, such as PointNetVLAD [[226](#bib.bib226)],
    PCAN [[228](#bib.bib228)] and D3Feat [[231](#bib.bib231)] explored to retrieve
    the reference scene at the beginning, while other techniques such as DeepICP [[229](#bib.bib229)]
    and DCP [[230](#bib.bib230)] allow to estimate relative motion transformations
    from 3D scans. Compared with image-based relocalization including 2D-to-3D and
    2D-to-2D localization, 3D-to-3D localization is relatively underexplored.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将LIDAR定位表述为递归贝叶斯推断问题，[[227](#bib.bib227)] 将LIDAR强度图和在线点云扫描嵌入共享空间中，以实现完全可微分的姿态估计。LocNet
    [[225](#bib.bib225)] 不直接处理3D数据，而是将点云扫描转换为2D旋转不变表示，用于在全局先验图中搜索相似帧，然后执行迭代最近点（ICP）方法来计算全局姿态。为了提出一种基于学习的LIDAR定位框架，该框架直接处理点云，L3-Net
    [[224](#bib.bib224)] 使用PointNet [[125](#bib.bib125)] 处理点云数据，以提取编码特定有用属性的特征描述符，并通过递归神经网络建模运动动态的时间连接。它通过最小化点云输入与3D地图之间的匹配距离来优化预测姿态与实际值之间的损失。一些技术，如PointNetVLAD
    [[226](#bib.bib226)]、PCAN [[228](#bib.bib228)] 和D3Feat [[231](#bib.bib231)] 探索了在开始时检索参考场景，而其他技术如DeepICP
    [[229](#bib.bib229)] 和DCP [[230](#bib.bib230)] 则允许从3D扫描中估计相对运动变换。与包括2D到3D和2D到2D的基于图像的重新定位相比，3D到3D定位相对未被充分探讨。
- en: 6 SLAM
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 SLAM
- en: 'Simultaneously tracking self-motion and estimate the structure of surroundings
    constructs a simultaneous localization and mapping (SLAM) system. The individual
    modules of localization and mapping discussed in the above sections can be viewed
    as modules of a complete SLAM systems. This section overviews the SLAM systems
    using deep learning, with the main focus on the modules that contribute to the
    integration of a SLAM system, including local/global optimization, keyframe/loop
    closure detection and uncertainty estimation. Table [VI](#S6.T6 "TABLE VI ‣ 6.1
    Local Optimization ‣ 6 SLAM ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") summarizes the existing approaches
    that employ the deep learning based SLAM modules discussed in this section.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '同时跟踪自我运动和估计周围环境的结构构建了一个同时定位与地图构建（SLAM）系统。上面部分讨论的定位和地图构建的各个模块可以视为完整SLAM系统的模块。本节概述了使用深度学习的SLAM系统，主要关注于那些有助于SLAM系统集成的模块，包括局部/全局优化、关键帧/回环闭合检测和不确定性估计。表格
    [VI](#S6.T6 "TABLE VI ‣ 6.1 Local Optimization ‣ 6 SLAM ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    总结了本节讨论的基于深度学习的SLAM模块的现有方法。'
- en: 6.1 Local Optimization
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 局部优化
- en: When jointly optimizing estimated camera motion and scene geometry, SLAM systems
    enforce them to satisfy a certain constraint. This is done by minimizing a geometric
    or photometric loss to ensure their consistency in the local area - the surroundings
    of camera poses, which can be viewed as a bundle adjustment (BA) problem[[233](#bib.bib233)].
    Learning based approaches predict depth maps and ego-motion through two individual
    networks [[29](#bib.bib29)] trained above large datasets. During the testing procedure
    when deployed online, there is a requirement that enforces the predictions to
    satisfy the local constraints. To enable local optimization, traditionally, the
    second-order solvers, e.g. Gauss-Newton (GN) method or Levenberg-Marquadt (LM)
    algorithm [[234](#bib.bib234)], are applied to optimize motion transformations
    and per-pixel depth maps
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在联合优化估计的相机运动和场景几何时，SLAM 系统强制它们满足某种约束。这是通过最小化几何或光度损失来确保它们在局部区域——相机姿态周围的区域中的一致性，这可以被视为一个捆绑调整（BA）问题[[233](#bib.bib233)]。基于学习的方法通过两个独立的网络[[29](#bib.bib29)]在大型数据集上进行训练，预测深度图和自我运动。在在线部署时的测试过程中，有一个要求强制预测满足局部约束。为了实现本地优化，传统上使用二阶求解器，如高斯-牛顿（GN）方法或列文伯格-马夸特（LM）算法
    [[234](#bib.bib234)]，来优化运动变换和每像素深度图。
- en: To this end, LS-Net [[235](#bib.bib235)] tackled this problem via a learning
    based optimizer by integrating the analytical solvers into its learning process.
    It learns a data-driven prior, followed by refining the DNN predictions with an
    analytical optimizer to ensure photometric consistency. BA-Net [[236](#bib.bib236)]
    integrates a differentiable second-order optimizer (LM algorithm) into a deep
    neural network to achieve end-to-end learning. Instead of minimizing geometric
    or photometric error, BA-Net is performed on feature space to optimize the consistency
    loss of features from multiview images extracted by ConvNets. This feature-level
    optimizer can mitigate the fundamental problems of geometric or photometric solution,
    i.e. some information may be lost in the geometric optimization, while environmental
    dynamics and lighting changes may impact the photometric optimization). These
    learning based optimizers provide an alternative to solve bundle adjustment problem.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，LS-Net [[235](#bib.bib235)] 通过将分析求解器集成到其学习过程中，采用基于学习的优化器来解决这个问题。它学习一个数据驱动的先验，然后通过分析优化器来细化
    DNN 预测，以确保光度一致性。BA-Net [[236](#bib.bib236)] 将可微分的二阶优化器（LM 算法）集成到深度神经网络中，实现端到端学习。BA-Net
    不是最小化几何或光度误差，而是在特征空间上优化从卷积网络提取的多视角图像的特征一致性损失。这种特征级优化器可以缓解几何或光度解决方案的基本问题，例如几何优化中可能会丢失一些信息，而环境动态和光照变化可能会影响光度优化。这些基于学习的优化器提供了解决捆绑调整问题的替代方案。
- en: 'TABLE VI: A summary of existing approaches on deep learning for SLAM'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：深度学习在 SLAM 中现有方法的总结
- en: '| Modules | Employed by |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 模块 | 使用者 |'
- en: '| --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Local optimization | [[235](#bib.bib235), [236](#bib.bib236)] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 本地优化 | [[235](#bib.bib235), [236](#bib.bib236)] |'
- en: '| Global optimization | [[123](#bib.bib123), [237](#bib.bib237), [64](#bib.bib64),
    [238](#bib.bib238)] |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 全局优化 | [[123](#bib.bib123), [237](#bib.bib237), [64](#bib.bib64), [238](#bib.bib238)]
    |'
- en: '| Keyframe detection | [[77](#bib.bib77)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 关键帧检测 | [[77](#bib.bib77)] |'
- en: '| Loop-closure detection | [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241),
    [242](#bib.bib242)] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 回环检测 | [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242)]
    |'
- en: '| Uncertainty Estimation | [[243](#bib.bib243), [135](#bib.bib135), [137](#bib.bib137)]
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性估计 | [[243](#bib.bib243), [135](#bib.bib135), [137](#bib.bib137)] |'
- en: 6.2 Global Optimization
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 全局优化
- en: Odometry estimation suffers from the accumulative error drifts during long-term
    operations, due to the fundamental problems of path integration, i.e. the system
    error accumulate without effective restrictions. To address this, graph-SLAM [[42](#bib.bib42)]
    constructs a topological graph to represent camera poses or scene features as
    graph nodes, which are connected by edges (measured by sensors) to constrain the
    poses. This graph-based formulation can be optimized to ensure the global consistency
    of graph nodes and edges, mitigating the possible errors on pose estimates and
    the inherent sensor measurement noise. A popular solver for global optimization
    is through Levenberg-Marquardt (LM) algorithm.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 里程计估计在长期操作过程中会受到累积误差漂移的影响，这是由于路径积分的基本问题，即系统误差在没有有效限制的情况下累积。为了解决这个问题，图-SLAM [[42](#bib.bib42)]
    构建了一个拓扑图，将相机姿态或场景特征表示为图节点，这些节点通过边（由传感器测量）连接，以约束姿态。该基于图的表述可以优化，以确保图节点和边的全局一致性，减轻姿态估计的可能误差和固有的传感器测量噪声。用于全局优化的一个流行求解器是通过
    Levenberg-Marquardt (LM) 算法。
- en: In the era of deep learning, deep neural networks excel at extracting features,
    and constructing functions from observations to poses and scene representations.
    A global optimization upon the DNN predictions is necessary to reducing the drifts
    of global trajectories and support large-scale mapping. Compared with a variety
    of well-researched solutions in classical SLAM, optimizing deep predictions globally
    is underexplored.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代，深度神经网络在特征提取以及从观测数据到姿态和场景表示的函数构建方面表现出色。对 DNN 预测进行全局优化是减少全局轨迹漂移并支持大规模映射所必需的。与经典
    SLAM 中各种经过深入研究的解决方案相比，全局优化深度预测尚未得到充分探索。
- en: Existing works explored to combine learning modules into a classical SLAM system
    at different levels - in the front-end, DNNs produce predictions as priors, followed
    by incorporating these deep predictions into the back-end for next step optimization
    and refinement. One good example is CNN-SLAM [[123](#bib.bib123)], which utilizes
    the learned per-pixel depths into LSD-SLAM [[124](#bib.bib124)], a full SLAM system
    to support loop closing and graph optimization. Camera poses and scene representations
    are jointly optimized with depth maps to produce consistent scale metrics. In
    DeepTAM [[237](#bib.bib237)], both the depth and pose predictions from deep neural
    networks are introduced into a classical DTAM system [[121](#bib.bib121)], that
    is optimized globally by the back-end to achieve more accurate scene reconstruction
    and camera motion tracking. A similar work can be found on integrating unsupervised
    VO with a graph optimization back-end [[64](#bib.bib64)]. DeepFactors [[238](#bib.bib238)]
    vice versa integrates the learned optimizable scene representation (their so-called
    code representation) into a different style of back-end - probabilistic factor
    graph for global optimization. The advantage of the factor-graph based formulation
    is its flexibility to include sensor measurements, state estimates, and constraints.
    In a factor graph bach-end, it is quite easy and convenient to add new sensor
    modalities, pairwise constraints and system states into the graph for optimization.
    However, these back-end optimizers are not yet differentiable.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的研究探索了在不同层次将学习模块与经典 SLAM 系统相结合——在前端，DNN 生成预测作为先验，然后将这些深度预测融入后端进行下一步优化和精细调整。一个很好的例子是
    CNN-SLAM [[123](#bib.bib123)]，它将学到的每像素深度应用于 LSD-SLAM [[124](#bib.bib124)]，一个完整的
    SLAM 系统，以支持回环闭合和图优化。相机姿态和场景表示与深度图共同优化，以生成一致的尺度度量。在 DeepTAM [[237](#bib.bib237)]
    中，将深度神经网络的深度和姿态预测引入经典的 DTAM 系统 [[121](#bib.bib121)]，通过后端进行全局优化，以实现更准确的场景重建和相机运动跟踪。类似的工作可以在将无监督
    VO 与图优化后端 [[64](#bib.bib64)] 集成中找到。DeepFactors [[238](#bib.bib238)] 则将可优化的场景表示（他们所谓的代码表示）集成到另一种风格的后端——用于全局优化的概率因子图。基于因子图的表述的优势在于它灵活地包含传感器测量、状态估计和约束。在因子图后端中，添加新的传感器模态、成对约束和系统状态以进行优化非常简单和方便。然而，这些后端优化器尚未具有可微分性。
- en: 6.3 Keyframe and Loop-closure Detection
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 关键帧和回环检测
- en: Detecting keyframe and loop-closing is of key importance to the back-end optimization
    of SLAM systems.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 关键帧和回环闭合对 SLAM 系统的后端优化至关重要。
- en: Keyframe selection facilitates SLAM systems to be more efficient. In the key-frame
    based SLAM systems, pose and scene estimates are only refined when a keyframe
    is detected. [[77](#bib.bib77)] provides a learning solution to detect key-frames
    together with unsupervised learning of ego-motion tracking and depths estimation
    [[29](#bib.bib29)]. Whether an image is the keyframe is determined by comparing
    its feature similarity with existing keyframes (i.e. if the similarity is below
    a threshold, this image will be treated as a new keyframe).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 关键帧选择有助于提高SLAM系统的效率。在基于关键帧的SLAM系统中，只有在检测到关键帧时，姿态和场景估计才会被细化。[[77](#bib.bib77)]
    提供了一种学习解决方案，用于检测关键帧，同时进行自我运动跟踪和深度估计的无监督学习[[29](#bib.bib29)]。一个图像是否为关键帧是通过将其特征相似性与现有关键帧进行比较来确定的（即，如果相似性低于某个阈值，该图像将被视为新的关键帧）。
- en: Loop-closure detection or place recognition is also an important module in SLAM
    back-end to reduce open-loop errors. Conventional works are based on bag-of-words
    (BoW) to store and use the visual features from the hand-crafted detectors. However,
    this problem is complicated by the changes of illumination, weather, viewpoints
    and moving objects in real-world scenarios. To solve this, previous researchers
    such as [[239](#bib.bib239)] proposed to use the ConvNet features instead, that
    are from a pre-trained model on a generic large-scale image processing dataset.
    These methods are more robust against the variance of viewpoints and conditions
    due to the high-level representations extracted by deep neural networks. Other
    representative works [[240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242)]
    are built on deep auto-encoder structure to extract a compact representation,
    that compresses scene in an unsupervised manner. Deep learning based loop closing
    contributes more robust and effective visual features, and achieves state-of-the-art
    performance in place recognition, which is suitable to be integrated in SLAM systems.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 回环检测或位置识别也是SLAM后端中的一个重要模块，以减少开环误差。传统方法基于词袋模型（BoW）来存储和使用来自手工设计探测器的视觉特征。然而，实际场景中光照、天气、视角和移动物体的变化使得这个问题变得复杂。为了解决这个问题，以前的研究者如[[239](#bib.bib239)]提出使用ConvNet特征，这些特征来自于在通用大规模图像处理数据集上预训练的模型。这些方法由于深度神经网络提取的高级表示，对视角和条件的变化具有更强的鲁棒性。其他代表性的工作[[240](#bib.bib240),
    [241](#bib.bib241), [242](#bib.bib242)]则基于深度自编码器结构提取紧凑的表示，这种表示以无监督的方式压缩场景。基于深度学习的回环闭合提供了更强大和有效的视觉特征，并在位置识别中取得了最先进的性能，适合集成到SLAM系统中。
- en: 6.4 Uncertainty Estimation
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 不确定性估计
- en: 'Safety and interpretability are an critical step towards the practical deployment
    of mobile agents in everyday life: the former enables agents to live and act with
    human reliably, while the latter allows users to have better understanding over
    the model behaviours. Although deep learning models achieve state-of-the-art performance
    in a wide range of regression and classification tasks, some corner cases should
    be given enough attention as well. In these failure cases, errors from one component
    will propagate to the other downstream modules, causing catastrophic consequences.
    To this end, there is an emerging need to estimate uncertainty for deep neural
    networks to ensure safety and provide interpretability.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和可解释性是将移动智能体实际应用到日常生活中的关键步骤：前者使智能体能够与人类可靠地共存和行动，而后者则使用户能够更好地理解模型的行为。尽管深度学习模型在广泛的回归和分类任务中实现了最先进的性能，但某些特殊情况也应给予足够的关注。在这些失败的情况下，一个组件的错误将传播到其他下游模块，导致灾难性的后果。为此，迫切需要对深度神经网络进行不确定性估计，以确保安全性并提供可解释性。
- en: 'Deep learning models usually only produce the mean values of predictions, for
    example, the output of a DNN-based visual odometry model is a 6-dimensional relative
    pose vector, i.e. the translation and rotation. In order to capture the uncertainty
    of deep models, learning models can be augmented into a Bayesian model [[244](#bib.bib244),
    [245](#bib.bib245)]. The uncertainty from Bayesian models is broadly categorized
    into Aleatoric uncertainty and epistemic uncertainty: Aleatoric uncertainty reflects
    observation noises, e.g. sensor measurement or motion noises; epistemic uncertainty
    captures the model uncertainty [[245](#bib.bib245)]. In the context of this survey,
    we focus on the work of estimating uncertainty on the specific task of localization
    and mapping, with regard to their usages, i.e. whether they capture the uncertainty
    with the purpose of motion tracking or scene understanding.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常只生成预测的均值，例如，基于 DNN 的视觉里程计模型的输出是一个 6 维相对姿态向量，即平移和旋转。为了捕捉深度模型的不确定性，学习模型可以增强为贝叶斯模型
    [[244](#bib.bib244), [245](#bib.bib245)]。贝叶斯模型中的不确定性广泛分为 Aleatoric 不确定性和 Epistemic
    不确定性：Aleatoric 不确定性反映观察噪声，例如传感器测量或运动噪声；Epistemic 不确定性捕捉模型的不确定性 [[245](#bib.bib245)]。在本次调查的背景下，我们专注于在定位和地图构建特定任务中估计不确定性的工作，涉及其用途，即它们是否以运动跟踪或场景理解为目的来捕捉不确定性。
- en: The uncertainty of DNN-based odometry estimation has been explored by [[243](#bib.bib243),
    [246](#bib.bib246)]. They adopted a common strategy to convert the target predictions
    into a Gaussian distribution, conditioned on the mean value of pose estimates
    and its covariance. The parameters inside the framework are optimized via the
    loss function with a combination of mean and covariance. By minimizing the error
    function to find the best combination, the uncertainty is automatically learned
    in an unsupervised fashion. In this way, the uncertainty of motion transformation
    is recovered. The motion uncertainty plays a vital role in probabilistic sensor
    fusion or the back-end optimization of SLAM systems. To validate the effectiveness
    of uncertainty estimation in SLAM systems, [[243](#bib.bib243)] integrated the
    learned uncertainty into a graph-SLAM as the covariances of odometry edges. Based
    on these covariances a global optimization is then performed to reduce system
    drifts. It also confirms that uncertainty estimation improves the performance
    of SLAM systems over the baseline with a fixed predefined value of covariance.
    Similar Bayesian models are applied to the global relocalization problem. As illustrated
    in [[135](#bib.bib135), [137](#bib.bib137)], the uncertainty from deep models
    are able to reflect the global location errors, in which the unreliable pose estimates
    are avoided with this belief metric.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 基于的里程计估计的不确定性已经被探索过 [[243](#bib.bib243), [246](#bib.bib246)]。他们采用了一种常见策略，将目标预测转换为高斯分布，以姿态估计的均值和协方差为条件。框架内的参数通过包含均值和协方差的损失函数进行优化。通过最小化误差函数以找到最佳组合，不确定性以无监督的方式自动学习。通过这种方式，运动变换的不确定性被恢复。运动不确定性在概率传感器融合或
    SLAM 系统的后端优化中起着至关重要的作用。为了验证 SLAM 系统中不确定性估计的有效性，[[243](#bib.bib243)] 将学习到的不确定性集成到图
    SLAM 中，作为里程计边缘的协方差。基于这些协方差，随后进行全局优化以减少系统漂移。这也确认了不确定性估计比固定预定义协方差值的基线更能提升 SLAM 系统的性能。类似的贝叶斯模型被应用于全局重新定位问题。正如
    [[135](#bib.bib135), [137](#bib.bib137)] 中所示，深度模型中的不确定性能够反映全局位置误差，通过这种信念度量避免了不可靠的姿态估计。
- en: In addition to the uncertainty for motion/relocalization, estimating the uncertainty
    for scene understanding also contributes to SLAM systems. This uncertainty offers
    a belief metric in to what extent the environmental perception and scene structure
    should be trusted. For example, in the semantic segmentation and depth estimation
    tasks, uncertainty estimation provides per-pixel uncertainties for the DNN predictions
    [[247](#bib.bib247), [245](#bib.bib245), [248](#bib.bib248), [249](#bib.bib249)].
    Further more, scene uncertainty is applicable to building hybrid SLAM systems.
    For example, photometric uncertainty can be learned to capture the variance of
    intensity on each image pixel, and hence enhances the robustness of SLAM system
    to observation noise [[25](#bib.bib25)].
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 除了运动/重定位的不确定性外，场景理解的不确定性也对SLAM系统有影响。这种不确定性提供了对环境感知和场景结构应信任程度的度量。例如，在语义分割和深度估计任务中，不确定性估计为DNN预测提供了每像素的不确定性[[247](#bib.bib247)、[245](#bib.bib245)、[248](#bib.bib248)、[249](#bib.bib249)]。此外，场景不确定性还适用于构建混合SLAM系统。例如，光度不确定性可以通过学习来捕捉每个图像像素的强度变化，从而提高SLAM系统对观测噪声的鲁棒性[[25](#bib.bib25)]。
- en: 7 Open Questions
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 个开放性问题
- en: Although deep learning has brought great success to the research in localization
    and mapping, as aforementioned, existing models are not sophisticated enough to
    completely solve the problem at hand. The current form of deep solutions is still
    in its infancy state. Towards great autonomy in the wild, there are numerous challenges
    for future researchers to investigate. Practical applications of these techniques
    should be considered a systematic research problem. We discuss several open questions
    that likely lead the further development in this area.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在定位和映射研究中取得了巨大成功，但如前所述，现有模型仍不够成熟，无法完全解决当前问题。深度解决方案的现状仍处于起步阶段。面向野外的高度自主性，未来研究者面临诸多挑战。对这些技术的实际应用应视为一个系统的研究问题。我们讨论了几个可能引领该领域进一步发展的开放性问题。
- en: 1) End-to-end model vs. hybrid model. End-to-end learning models are able to
    predict self-motion and scene directly from raw data, without any hand-engineering.
    Benefited from the advances of deep learning, end-to-end models are evolving fast
    to achieve increasing performance in accuracy, efficiency and robustness. Meanwhile,
    these models have been shown easier to be integrated with other high-level learning
    tasks, e.g. path planning and navigation[[31](#bib.bib31)]. Fundamentally, there
    exist underlying physical or geometric models to govern localization and mapping
    systems. Whether we should develop end-to-end models relying only on the power
    of data-driven approaches or integrate deep learning modules into the pre-built
    physical/geometric models as a hybrid model is a critical question for future
    research. As we can see, hybrid models already achieved the state-of-the art results
    in many tasks, e.g. visual odometry[[25](#bib.bib25)], and global localization[[191](#bib.bib191)].
    Thus, it is reasonable to investigate in the way to take better advantage of the
    prior empirical knowledge from deep learning for hybrid models. On the other side,
    pure end-to-end models are data hunger. The performance of current models can
    be limited by the size of training dataset, and it is essential to create large
    and diverse dataset to enlarge the capacity of data-driven models.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 端到端模型与混合模型。端到端学习模型能够直接从原始数据中预测自我运动和场景，无需任何手工工程。得益于深度学习的进步，端到端模型正在快速发展，以在准确性、效率和鲁棒性方面取得越来越好的表现。同时，这些模型也更容易与其他高级学习任务，如路径规划和导航[[31](#bib.bib31)]，进行集成。从根本上讲，存在潜在的物理或几何模型来控制定位和映射系统。我们是否应该仅依靠数据驱动方法的力量来开发端到端模型，还是将深度学习模块集成到预先构建的物理/几何模型中作为混合模型，这是未来研究的一个关键问题。正如我们所见，混合模型已经在许多任务中取得了最先进的结果，例如视觉里程计[[25](#bib.bib25)]和全球定位[[191](#bib.bib191)]。因此，研究如何更好地利用深度学习中的先验经验知识以改进混合模型是合理的。另一方面，纯端到端模型对数据的需求很大。目前模型的性能可能受限于训练数据集的大小，创建大规模且多样化的数据集以扩大数据驱动模型的能力至关重要。
- en: 2) Unifying evaluation benchmark and metric. Finding suitable evaluating benchmark
    and metric is always a concern for SLAM systems. This is especially the case for
    DNN based systems. The predictions from DNNs are affected by the characteristics
    of both training and test data, including the dataset size, hyperparameters (batch
    size and learning rate etc.), and the difference in testing scenarios. Therefore,
    it is hard to fairly compare them when considering dataset differences, training/testing
    configuration, or evaluation metric adopted in each work. For example, the KITTI
    dataset is a common choice to evaluate visual odometry, but previous works split
    training and testing data in different ways (e.g. [[24](#bib.bib24), [48](#bib.bib48),
    [50](#bib.bib50)] used Sequence 00, 02, 08, 09 as training set, and Sequence 03,
    04, 05, 06, 07, 10 as testing set, while [[30](#bib.bib30), [25](#bib.bib25)]
    used Sequence 00 - 08 as training, and left 09, and 10 as testing set). Some of
    them are even based on different evaluation metrics (e.g. [[24](#bib.bib24), [48](#bib.bib48),
    [50](#bib.bib50)] applied the KITTI official evaluation metric, while [[29](#bib.bib29),
    [56](#bib.bib56)] applied absolute trajectory error (ATE) as their evaluation
    metric). All these factors bring difficulties to a direct and fair comparison
    across them. Moreover, the KITTI dataset is relatively simple (the vehicle only
    moves in 2D translation) and in small size. It is not convincing if only results
    on KITTI benchmark are provided without a comprehensive evaluation in the long-term
    real-world experiment. In fact, there is a growing need in creating a benchmark
    for a though system evaluation covering various environments, self-motions and
    dynamics.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 统一的评估基准和指标。找到合适的评估基准和指标始终是 SLAM 系统关注的问题。这对于基于 DNN 的系统尤为重要。DNN 的预测受到训练和测试数据特征的影响，包括数据集大小、超参数（如批量大小和学习率等）以及测试场景的差异。因此，在考虑数据集差异、训练/测试配置或每个工作中采用的评估指标时，很难公平地进行比较。例如，KITTI
    数据集是评估视觉里程计的常见选择，但之前的工作以不同的方式划分训练和测试数据（例如，[[24](#bib.bib24), [48](#bib.bib48),
    [50](#bib.bib50)] 使用了 Sequence 00、02、08、09 作为训练集，Sequence 03、04、05、06、07、10 作为测试集，而
    [[30](#bib.bib30), [25](#bib.bib25)] 使用了 Sequence 00 - 08 作为训练集，09 和 10 作为测试集）。其中一些甚至基于不同的评估指标（例如，[[24](#bib.bib24),
    [48](#bib.bib48), [50](#bib.bib50)] 采用了 KITTI 官方评估指标，而 [[29](#bib.bib29), [56](#bib.bib56)]
    采用了绝对轨迹误差（ATE）作为评估指标）。所有这些因素使得它们之间的直接和公平比较变得困难。此外，KITTI 数据集相对简单（车辆仅在 2D 平移中移动）且规模较小。如果仅提供
    KITTI 基准上的结果，而没有在长期现实世界实验中的综合评估，那是不令人信服的。实际上，越来越需要创建一个涵盖各种环境、自我运动和动态的系统评估基准。
- en: 3) Real-world deployment. Deploying deep learning models in real-world environments
    is a systematic research problem. In the existing research, the prediction accuracy
    is always their ‘golden rule’ to follow, while other crucial issues are overlooked,
    such as whether the model structure and the parameter number of framework is optimal.
    The computational and energy consumption have to be considered on resource-constrained
    systems, such as low-cost robots or VR wearable devices. The prallerization opportunities,
    such as convolutional filters or other parallel neural network modules should
    be exploited in order to take better use of GPUs. Examples for consideration include
    in which situations the feed-back should be returned to fine-tune the systems,
    how to incorporate the self-supervised models into the systems and whether the
    systems allow the real-time online learning.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 现实世界的部署。在现实世界环境中部署深度学习模型是一个系统性的研究问题。在现有研究中，预测准确性始终是他们遵循的“金科玉律”，而其他关键问题却被忽视，例如模型结构和框架参数数量是否最优。计算和能源消耗必须考虑到资源受限的系统，例如低成本机器人或
    VR 可穿戴设备。需要利用并行化机会，例如卷积滤波器或其他并行神经网络模块，以更好地利用 GPU。需要考虑的例子包括在何种情况下应该返回反馈以微调系统，如何将自监督模型纳入系统以及系统是否允许实时在线学习。
- en: 4) Lifelong learning. Most previous works we discussed so far have only been
    validated on simple closed-form dataset, such as visual odometry and depth predictions
    are performed on the KITTI dataset. However, in an open world, the mobile agents
    will confront everchanging environmental factors, and moving dynamics. This will
    require the DNN models to continuously and coherently learn and adapt to the changes
    of the world. Moreover, new concepts and objects will appear unexpectedly, requiring
    an object discovery and new knowledge extension phase for robots.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 终身学习。我们迄今讨论的大多数工作仅在简单的封闭数据集上验证，例如视觉里程计和深度预测是在KITTI数据集上进行的。然而，在开放世界中，移动代理将面临不断变化的环境因素和移动动态。这将要求深度神经网络模型不断且连贯地学习和适应世界的变化。此外，新概念和对象将意外出现，需要一个对象发现和新知识扩展阶段来支持机器人。
- en: '5) New sensors: Beyond the common choice of on-board sensors, such as cameras,
    IMU and LIDAR, the emerging new sensors provide an alternative to construct a
    more accurate and robust multimodal system. New sensors including event camera[[250](#bib.bib250)],
    thermo camera[[251](#bib.bib251)], mm-wave device[[252](#bib.bib252)], radio signals[[253](#bib.bib253)],
    magnetic sensor[[254](#bib.bib254)], have distinct properties and data format
    compared to predominant SLAM sensors such as cameras, IMU and LIDAR. Nevertheless,
    the effective learning approaches to processing these unusual sensors are still
    underexplored.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 新传感器：除了常见的车载传感器，如摄像头、IMU和LIDAR，新兴的传感器提供了构建更准确、更强大的多模态系统的替代方案。新传感器包括事件摄像头[[250](#bib.bib250)],
    热成像摄像头[[251](#bib.bib251)], 毫米波设备[[252](#bib.bib252)], 无线电信号[[253](#bib.bib253)],
    磁传感器[[254](#bib.bib254)]，它们与主流SLAM传感器（如摄像头、IMU和LIDAR）相比具有不同的特性和数据格式。然而，有效的学习方法来处理这些非传统传感器仍然探索不足。
- en: 6) Scalability. Both the learning based localization and mapping models have
    now achieved promising results on the evaluation benchmark. However, they are
    restricted to some scenarios. For example, odometry estimation is always evaluated
    in the city area or on the roads. Whether these techniques could be applied to
    other environments, e.g. rural area or forest area is still an open question.
    Moreover, existing works on scene reconstruction are restricted on single-objects,
    synthetic data or room level. It is worthy exploring whether these learning methods
    are capable of scaling to much more complex and large-scale reconstruction problems.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 6) 可扩展性。基于学习的定位和映射模型在评估基准上已经取得了令人满意的结果。然而，它们受到某些场景的限制。例如，里程计估计总是在城市区域或道路上进行评估。这些技术是否可以应用于其他环境，例如乡村或森林区域，仍然是一个悬而未决的问题。此外，现有的场景重建工作限制于单一物体、合成数据或房间级别。值得探索这些学习方法是否能够扩展到更复杂和大规模的重建问题。
- en: 7) Safety, reliability and interpretability. Safety and reliability are critical
    to practical applications, e.g. self-driving vehicles. In these scenarios, even
    a small error of pose or scene estimates will cause disasters to the entire system.
    Deep neural networks have been long-critisized as ’black-box’, exacerbating the
    safety concerns for critical tasks. Some initial efforts explored the interpretability
    on deep models [[255](#bib.bib255)]. For example, uncertainty estimation[[244](#bib.bib244),
    [245](#bib.bib245)] can offer a belief metric, representing to what extent we
    trust our models. In this way, the unreliable predictions (with low uncertainty)
    are avoided in order to ensure the systems to stay safe and reliable.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 7) 安全性、可靠性和可解释性。安全性和可靠性对于实际应用至关重要，例如自动驾驶车辆。在这些场景中，即使是姿态或场景估计的一个小错误也会对整个系统造成灾难。深度神经网络长期以来被批评为“黑箱”，加剧了对关键任务的安全担忧。一些初步努力探索了深度模型的可解释性[[255](#bib.bib255)]。例如，不确定性估计[[244](#bib.bib244),
    [245](#bib.bib245)]可以提供一个信任度指标，表示我们对模型的信任程度。通过这种方式，避免了不可靠的预测（具有低不确定性），以确保系统保持安全和可靠。
- en: 8 Conclusions
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This work comprehensively overviews the area of deep learning for localization
    and mapping, and provides a new taxonomy to cover the relevant existing approaches
    from robotics, computer vision and machine learning communities. Learning models
    are incorporated into localization and mapping systems to connect input sensor
    data and target values, by automatically extracting useful features from raw data
    without any human effort. Deep learning based techniques have so far achieved
    the state-of-the-art performance in a variety of tasks, from visual odometry,
    global localization to dense scene reconstruction. Due to the highly expressive
    capacity of deep neural networks, these models are capable of implicitly modelling
    the factors such as environmental dynamics or sensor noises, that are hard to
    be modelled by hand, and thus are relatively more robust in real-world applications.
    In addition, high-level understanding and interaction are easy to perform for
    mobile agents with the learning based framework. The fast development of deep
    learning provides an alternative to solve classical localization and mapping problem
    in a data-driven way, and meanwhile paves the road towards a next-generation AI
    based spatial perception solution.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作全面概述了深度学习在定位和建图领域的应用，并提供了一种新的分类法，以涵盖来自机器人学、计算机视觉和机器学习领域的相关现有方法。学习模型被纳入定位和建图系统，以通过自动从原始数据中提取有用特征来连接输入传感器数据和目标值，无需人工干预。基于深度学习的技术迄今已在各种任务中取得了最先进的性能，从视觉里程计、全局定位到密集场景重建。由于深度神经网络具有高度的表达能力，这些模型能够隐式地建模一些难以手工建模的因素，如环境动态或传感器噪声，因此在实际应用中相对更具鲁棒性。此外，基于学习的框架使得移动代理能够轻松地进行高层次的理解和互动。深度学习的快速发展提供了一种数据驱动的解决经典定位和建图问题的替代方案，同时为下一代基于AI的空间感知解决方案铺平了道路。
- en: Acknowledgments
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work is supported by the EPSRC Project “ACE-OPS: From Autonomy to Cognitive
    assistance in Emergency OPerationS” (Grant Number: EP/S030832/1).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了EPSRC项目“ACE-OPS：从自主性到紧急操作中的认知辅助”（资助编号：EP/S030832/1）的支持。
- en: References
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. R. Fetsch, A. H. Turner, G. C. DeAngelis, and D. E. Angelaki, “Dynamic
    Reweighting of Visual and Vestibular Cues during Self-Motion Perception,” Journal
    of Neuroscience, vol. 29, no. 49, pp. 15601–15612, 2009.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. R. Fetsch, A. H. Turner, G. C. DeAngelis, 和 D. E. Angelaki, “自我运动感知过程中视觉与前庭线索的动态重标定”，神经科学杂志，第29卷，第49期，页码15601–15612，2009年。'
- en: '[2] K. E. Cullen, “The Vestibular System: Multimodal Integration and Encoding
    of Self-motion for Motor Control,” Trends in Neurosciences, vol. 35, no. 3, pp. 185–196,
    2012.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] K. E. Cullen, “前庭系统：多模态整合与自我运动编码用于运动控制”，神经科学趋势，第35卷，第3期，页码185–196，2012年。'
- en: '[3] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B. Upcroft,
    P. Abbeel, W. Burgard, M. Milford, and P. Corke, “The Limits and Potentials of
    Deep Learning for Robotics,” International Journal of Robotics Research, vol. 37,
    no. 4-5, pp. 405–420, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B.
    Upcroft, P. Abbeel, W. Burgard, M. Milford, 和 P. Corke, “深度学习在机器人学中的局限性与潜力”，国际机器人研究杂志，第37卷，第4-5期，页码405–420，2018年。'
- en: '[4] R. Harle, “A Survey of Indoor Inertial Positioning Systems for Pedestrians,”
    IEEE Communications Surveys and Tutorials, vol. 15, no. 3, pp. 1281–1293, 2013.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Harle, “关于行人室内惯性定位系统的调查”，IEEE通信调查与教程，第15卷，第3期，页码1281–1293，2013年。'
- en: '[5] M. Gowda, A. Dhekne, S. Shen, R. R. Choudhury, X. Yang, L. Yang, S. Golwalkar,
    and A. Essanian, “Bringing IoT to Sports Analytics,” in USENIX Symposium on Networked
    Systems Design and Implementation (NSDI), 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Gowda, A. Dhekne, S. Shen, R. R. Choudhury, X. Yang, L. Yang, S. Golwalkar,
    和 A. Essanian, “将物联网引入体育分析”，发表于USENIX网络系统设计与实现研讨会（NSDI），2017年。'
- en: '[6] M. Wijers, A. Loveridge, D. W. Macdonald, and A. Markham, “Caracal: a versatile
    passive acoustic monitoring tool for wildlife research and conservation,” Bioacoustics,
    pp. 1–17, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Wijers, A. Loveridge, D. W. Macdonald, 和 A. Markham, “Caracal: 一种多功能被动声学监测工具，用于野生动物研究和保护”，Bioacoustics，页码1–17，2019年。'
- en: '[7] A. Dhekne, A. Chakraborty, K. Sundaresan, and S. Rangarajan, “TrackIO :
    Tracking First Responders Inside-Out,” in USENIX Symposium on Networked Systems
    Design and Implementation (NSDI), 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Dhekne, A. Chakraborty, K. Sundaresan, 和 S. Rangarajan, “TrackIO：从内部追踪第一响应者”，发表于USENIX网络系统设计与实现研讨会（NSDI），2019年。'
- en: '[8] A. J. Davison, “FutureMapping: The Computational Structure of Spatial AI
    Systems,” arXiv, 2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. J. Davison, “FutureMapping: 空间AI系统的计算结构”，arXiv，2018年。'
- en: '[9] D. Nister, O. Naroditsky, and J. Bergen, “Visual odometry,” in IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), vol. 1, pp. I–652–I–659
    Vol.1, 2004.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Nister, O. Naroditsky, 和 J. Bergen，“视觉里程计，”在IEEE/CVF计算机视觉与模式识别会议（CVPR），第1卷，第I–652–I–659页，第1卷，2004年。'
- en: '[10] J. Engel, J. Sturm, and D. Cremers, “Semi-Dense Visual Odometry for a
    Monocular Camera,” in IEEE International Conference on Computer Vision (ICCV),
    pp. 1449–1456, 2013.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Engel, J. Sturm, 和 D. Cremers，“单目相机的半稠密视觉里程计，”在IEEE国际计算机视觉会议（ICCV），第1449–1456页，2013年。'
- en: '[11] C. Forster, M. Pizzoli, and D. Scaramuzza, “SVO: Fast Semi-Direct Monocular
    Visual Odometry,” in IEEE International Conference on Robotics and Automation
    (ICRA), pp. 15–22, 2014.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. Forster, M. Pizzoli, 和 D. Scaramuzza，“SVO：快速半直接单目视觉里程计，”在IEEE国际机器人与自动化会议（ICRA），第15–22页，2014年。'
- en: '[12] M. Li and A. I. Mourikis, “High-precision, Consistent EKF-based Visual-Inertial
    Odometry,” The International Journal of Robotics Research, vol. 32, no. 6, pp. 690–711,
    2013.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Li 和 A. I. Mourikis，“高精度、一致的基于EKF的视觉-惯性里程计，”《国际机器人研究杂志》，第32卷，第6期，第690–711页，2013年。'
- en: '[13] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, “Keyframe-Based
    Visual–Inertial Odometry Using Nonlinear Optimization,” The International Journal
    of Robotics Research, vol. 34, no. 3, pp. 314–334, 2015.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, 和 P. Furgale，“基于关键帧的视觉-惯性里程计使用非线性优化，”《国际机器人研究杂志》，第34卷，第3期，第314–334页，2015年。'
- en: '[14] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, “On-Manifold Preintegration
    for Real-Time Visual-Inertial Odometry,” IEEE Transactions on Robotics, vol. 33,
    no. 1, pp. 1–21, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C. Forster, L. Carlone, F. Dellaert, 和 D. Scaramuzza，“实时视觉-惯性里程计的流形预积分，”《IEEE机器人学报》，第33卷，第1期，第1–21页，2017年。'
- en: '[15] T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile Monocular
    Visual-Inertial State Estimator,” IEEE Transactions on Robotics, vol. 34, no. 4,
    pp. 1004–1020, 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Qin, P. Li, 和 S. Shen，“VINS-Mono：一种强大而多功能的单目视觉-惯性状态估计器，”《IEEE机器人学报》，第34卷，第4期，第1004–1020页，2018年。'
- en: '[16] J. Zhang and S. Singh, “LOAM: Lidar Odometry and Mapping in Real-time,”
    in Robotics: Science and Systems, 2010.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Zhang 和 S. Singh，“LOAM：实时激光雷达里程计与映射，”在机器人学：科学与系统，2010年。'
- en: '[17] W. Zhang and J. Kosecka, “Image based localization in urban environments.,”
    in International Symposium on 3D Data Processing Visualization and Transmission
    (3DPVT), vol. 6, pp. 33–40, Citeseer, 2006.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. Zhang 和 J. Kosecka，“城市环境中的基于图像的定位，”在国际三维数据处理、可视化与传输研讨会（3DPVT），第6卷，第33–40页，Citeseer，2006年。'
- en: '[18] T. Sattler, B. Leibe, and L. Kobbelt, “Fast image-based localization using
    direct 2d-to-3d matching,” in International Conference on Computer Vision (ICCV),
    pp. 667–674, IEEE, 2011.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. Sattler, B. Leibe, 和 L. Kobbelt，“使用直接2D到3D匹配的快速图像定位，”在国际计算机视觉会议（ICCV），第667–674页，IEEE，2011年。'
- en: '[19] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” IEEE Transactions on Robotics,
    vol. 32, no. 1, pp. 1–19, 2015.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, 和
    M. J. Milford，“视觉地点识别：综述，”《IEEE机器人学报》，第32卷，第1期，第1–19页，2015年。'
- en: '[20] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “MonoSLAM: Real-Time
    Single Camera SLAM,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 29, no. 6, pp. 1052–1067, 2007.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. J. Davison, I. D. Reid, N. D. Molton, 和 O. Stasse，“MonoSLAM：实时单摄像头SLAM，”《IEEE模式分析与机器智能学报》，第29卷，第6期，第1052–1067页，2007年。'
- en: '[21] R. Mur-Artal, J. Montiel, and J. D. Tardos, “ORB-SLAM : A Versatile and
    Accurate Monocular SLAM System,” IEEE Transactions on Robotics, vol. 31, no. 5,
    pp. 1147–1163, 2015.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] R. Mur-Artal, J. Montiel, 和 J. D. Tardos，“ORB-SLAM：一种多功能且准确的单目SLAM系统，”《IEEE机器人学报》，第31卷，第5期，第1147–1163页，2015年。'
- en: '[22] H. C. Longuet-Higgins, “A computer algorithm for reconstructing a scene
    from two projections,” Nature, vol. 293, no. 5828, pp. 133–135, 1981.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. C. Longuet-Higgins，“一种从两个投影重建场景的计算机算法，”《自然》，第293卷，第5828期，第133–135页，1981年。'
- en: '[23] C. Wu, “Towards linear-time incremental structure from motion,” in 2013
    International Conference on 3D Vision-3DV 2013, pp. 127–134, IEEE, 2013.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] C. Wu，“面向线性时间增量结构从运动，”在2013年国际三维视觉会议（3DV 2013），第127–134页，IEEE，2013年。'
- en: '[24] S. Wang, R. Clark, H. Wen, and N. Trigoni, “DeepVO : Towards End-to-End
    Visual Odometry with Deep Recurrent Convolutional Neural Networks,” in International
    Conference on Robotics and Automation (ICRA), 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Wang, R. Clark, H. Wen, 和 N. Trigoni，“DeepVO：基于深度递归卷积神经网络的端到端视觉里程计，”在国际机器人与自动化会议（ICRA），2017年。'
- en: '[25] N. Yang, L. von Stumberg, R. Wang, and D. Cremers, “D3vo: Deep depth,
    deep pose and deep uncertainty for monocular visual odometry,” CVPR, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Yang, L. von Stumberg, R. Wang, 和 D. Cremers, “D3vo：深度深度、深度姿态和深度不确定性的单目视觉里程计，”
    CVPR, 2020年。'
- en: '[26] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, “Semanticfusion:
    Dense 3d semantic mapping with convolutional neural networks,” in 2017 IEEE International
    Conference on Robotics and automation (ICRA), pp. 4628–4635, IEEE, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. McCormac, A. Handa, A. Davison, 和 S. Leutenegger, “Semanticfusion：利用卷积神经网络进行密集的3D语义映射，”
    在2017 IEEE国际机器人与自动化会议（ICRA），第4628–4635页，IEEE，2017年。'
- en: '[27] L. Ma, J. Stückler, C. Kerl, and D. Cremers, “Multi-view deep learning
    for consistent semantic mapping with rgb-d cameras,” in 2017 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 598–605, IEEE, 2017.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Ma, J. Stückler, C. Kerl, 和 D. Cremers, “用于一致语义映射的多视图深度学习与RGB-D相机，”
    在2017 IEEE/RSJ国际智能机器人与系统会议（IROS），第598–605页，IEEE，2017年。'
- en: '[28] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press,
    2016.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] I. Goodfellow, Y. Bengio, 和 A. Courville, 《深度学习》。MIT出版社，2016年。'
- en: '[29] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised Learning
    of Depth and Ego-Motion from Video,” in IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. Zhou, M. Brown, N. Snavely, 和 D. G. Lowe, “从视频中无监督学习深度和自我运动，” 在IEEE/CVF计算机视觉与模式识别会议（CVPR），2017年。'
- en: '[30] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, and I. Reid, “Unsupervised
    scale-consistent depth and ego-motion learning from monocular video,” in Advances
    in Neural Information Processing Systems, pp. 35–45, 2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, 和 I. Reid, “从单目视频中无监督地学习尺度一致的深度和自我运动，”
    在《神经信息处理系统进展》中，第35–45页，2019年。'
- en: '[31] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in 2017 IEEE international conference on robotics and automation (ICRA), pp. 3357–3364,
    IEEE, 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, 和 A. Farhadi,
    “在室内场景中使用深度强化学习进行目标驱动的视觉导航，” 2017 IEEE国际机器人与自动化会议（ICRA），第3357–3364页，IEEE，2017年。'
- en: '[32] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D. Teplyashin,
    K. Simonyan, A. Zisserman, R. Hadsell, et al., “Learning to navigate in cities
    without a map,” in Advances in Neural Information Processing Systems, pp. 2419–2430,
    2018.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D.
    Teplyashin, K. Simonyan, A. Zisserman, R. Hadsell, 等， “在没有地图的情况下学习在城市中导航，” 在《神经信息处理系统进展》中，第2419–2430页，2018年。'
- en: '[33] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,”
    arXiv preprint arXiv:2005.14165, 2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, 等， “语言模型是少样本学习者，” arXiv预印本 arXiv:2005.14165,
    2020年。'
- en: '[34] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” The International Journal of Robotics Research, vol. 36,
    no. 1, pp. 3–15, 2016.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W. Maddern, G. Pascoe, C. Linegar, 和 P. Newman, “1年，1000公里：牛津RobotCar数据集，”
    《国际机器人研究杂志》，第36卷，第1期，第3–15页，2016年。'
- en: '[35] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The apolloscape
    open dataset for autonomous driving and its application,” IEEE transactions on
    pattern analysis and machine intelligence, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, 和 R. Yang, “ApolloScape开放数据集及其在自动驾驶中的应用，”
    IEEE模式分析与机器智能汇刊，2019年。'
- en: '[36] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev,
    S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and
    D. Anguelov, “Scalability in perception for autonomous driving: Waymo open dataset,”
    2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A.
    Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z.
    Chen, 和 D. Anguelov, “自动驾驶中的感知可扩展性：Waymo 开放数据集，” 2019年。'
- en: '[37] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping:
    part i,” IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99–110, 2006.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] H. Durrant-Whyte 和 T. Bailey, “同时定位与地图构建：第一部分，” IEEE机器人与自动化杂志，第13卷，第2期，第99–110页，2006年。'
- en: '[38] T. Bailey and H. Durrant-Whyte, “Simultaneous localization and mapping
    (slam): Part ii,” IEEE robotics & automation magazine, vol. 13, no. 3, pp. 108–117,
    2006.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Bailey 和 H. Durrant-Whyte, “同时定位与地图构建（slam）：第二部分，” IEEE机器人与自动化杂志，第13卷，第3期，第108–117页，2006年。'
- en: '[39] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous localization
    and mapping: Toward the robust-perception age,” IEEE Transactions on robotics,
    vol. 32, no. 6, pp. 1309–1332, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, 和 J. J. Leonard，“同时定位与地图构建的过去、现在与未来：迈向稳健感知时代”，《IEEE 机器人学报》，第32卷，第6期，页1309–1332，2016年。'
- en: '[40] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT press, 2005.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Thrun, W. Burgard, 和 D. Fox，《概率机器人》。MIT出版社，2005年。'
- en: '[41] D. Scaramuzza and F. Fraundorfer, “Visual odometry [tutorial],” IEEE robotics
    & automation magazine, vol. 18, no. 4, pp. 80–92, 2011.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] D. Scaramuzza 和 F. Fraundorfer，“视觉里程计 [教程]”，《IEEE 机器人与自动化杂志》，第18卷，第4期，页80–92，2011年。'
- en: '[42] G. Grisetti, R. Kummerle, C. Stachniss, and W. Burgard, “A tutorial on
    graph-based slam,” IEEE Intelligent Transportation Systems Magazine, vol. 2, no. 4,
    pp. 31–43, 2010.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Grisetti, R. Kummerle, C. Stachniss, 和 W. Burgard，“图基SLAM教程”，《IEEE
    智能交通系统杂志》，第2卷，第4期，页31–43，2010年。'
- en: '[43] M. R. U. Saputra, A. Markham, and N. Trigoni, “Visual slam and structure
    from motion in dynamic environments: A survey,” ACM Computing Surveys (CSUR),
    vol. 51, no. 2, pp. 1–36, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. R. U. Saputra, A. Markham, 和 N. Trigoni，“动态环境中的视觉 SLAM 和运动恢复：综述”，《ACM
    计算调查（CSUR）》第51卷，第2期，页1–36，2018年。'
- en: '[44] K. Konda and R. Memisevic, “Learning Visual Odometry with a Convolutional
    Network,” in International Conference on Computer Vision Theory and Applications,
    pp. 486–490, 2015.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] K. Konda 和 R. Memisevic，“使用卷积网络学习视觉里程计”，在国际计算机视觉理论与应用会议中，页486–490，2015年。'
- en: '[45] G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia, “Exploring representation
    learning with cnns for frame-to-frame ego-motion estimation,” IEEE robotics and
    automation letters, vol. 1, no. 1, pp. 18–25, 2015.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] G. Costante, M. Mancini, P. Valigi, 和 T. A. Ciarfuglia，“探索使用 CNNs 进行帧到帧自我运动估计的表征学习”，《IEEE
    机器人与自动化快报》，第1卷，第1期，页18–25，2015年。'
- en: '[46] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The KITTI dataset,” The International Journal of Robotics Research, vol. 32, no. 11,
    pp. 1231–1237, 2013.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun，“视觉与机器人技术的结合：KITTI 数据集”，《国际机器人研究期刊》，第32卷，第11期，页1231–1237，2013年。'
- en: '[47] A. Geiger, J. Ziegler, and C. Stiller, “Stereoscan: Dense 3d reconstruction
    in real-time,” in 2011 IEEE Intelligent Vehicles Symposium (IV), pp. 963–968,
    Ieee, 2011.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. Geiger, J. Ziegler, 和 C. Stiller，“Stereoscan：实时稠密 3D 重建”，在2011年 IEEE
    智能车辆研讨会（IV）中，页963–968，IEEE，2011年。'
- en: '[48] M. R. U. Saputra, P. P. de Gusmao, S. Wang, A. Markham, and N. Trigoni,
    “Learning monocular visual odometry through geometry-aware curriculum learning,”
    in 2019 International Conference on Robotics and Automation (ICRA), pp. 3549–3555,
    IEEE, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. R. U. Saputra, P. P. de Gusmao, S. Wang, A. Markham, 和 N. Trigoni，“通过几何感知课程学习学习单目视觉里程计”，在2019年国际机器人与自动化会议（ICRA）中，页3549–3555，IEEE，2019年。'
- en: '[49] M. R. U. Saputra, P. P. de Gusmao, Y. Almalioglu, A. Markham, and N. Trigoni,
    “Distilling knowledge from a deep pose regressor network,” in Proceedings of the
    IEEE International Conference on Computer Vision (ICCV), pp. 263–272, 2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. R. U. Saputra, P. P. de Gusmao, Y. Almalioglu, A. Markham, 和 N. Trigoni，“从深度姿态回归网络中提取知识”，在《IEEE
    国际计算机视觉会议（ICCV）论文集》中，页263–272，2019年。'
- en: '[50] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, and H. Zha, “Beyond tracking:
    Selecting memory and refining poses for deep visual odometry,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8575–8583,
    2019.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, 和 H. Zha，“超越跟踪：选择记忆并精炼姿态以进行深度视觉里程计”，在《IEEE
    计算机视觉与模式识别会议（CVPR）》论文集中，页8575–8583，2019年。'
- en: '[51] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop kf: Learning
    discriminative deterministic state estimators,” in Advances in Neural Information
    Processing Systems, pp. 4376–4384, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] T. Haarnoja, A. Ajay, S. Levine, 和 P. Abbeel，“Backprop kf：学习判别性确定性状态估计器”，在《神经信息处理系统进展》中，页4376–4384，2016年。'
- en: '[52] X. Yin, X. Wang, X. Du, and Q. Chen, “Scale recovery for monocular visual
    odometry using depth estimated with deep convolutional neural fields,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 5870–5878, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] X. Yin, X. Wang, X. Du, 和 Q. Chen，“使用深度卷积神经网络估计的深度进行单目视觉里程计的尺度恢复”，在《IEEE
    国际计算机视觉会议论文集》中，页5870–5878，2017年。'
- en: '[53] R. Li, S. Wang, Z. Long, and D. Gu, “Undeepvo: Monocular visual odometry
    through unsupervised deep learning,” in 2018 IEEE international conference on
    robotics and automation (ICRA), pp. 7286–7291, IEEE, 2018.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] R. Li, S. Wang, Z. Long, 和 D. Gu， “Undeepvo：通过无监督深度学习的单目视觉里程计，” 见于2018年IEEE国际机器人与自动化会议（ICRA），第7286–7291页，IEEE，2018年。'
- en: '[54] D. Barnes, W. Maddern, G. Pascoe, and I. Posner, “Driven to distraction:
    Self-supervised distractor learning for robust monocular visual odometry in urban
    environments,” in 2018 IEEE International Conference on Robotics and Automation
    (ICRA), pp. 1894–1900, IEEE, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. Barnes, W. Maddern, G. Pascoe, 和 I. Posner， “被分心：自监督的干扰学习用于城市环境中稳健的单目视觉里程计，”
    见于2018年IEEE国际机器人与自动化会议（ICRA），第1894–1900页，IEEE，2018年。'
- en: '[55] Z. Yin and J. Shi, “GeoNet: Unsupervised Learning of Dense Depth, Optical
    Flow and Camera Pose,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. Yin 和 J. Shi， “GeoNet：无监督学习的稠密深度、光流和相机姿态，” 见于IEEE/CVF计算机视觉与模式识别会议（CVPR），2018年。'
- en: '[56] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. Reid, “Unsupervised
    Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction,”
    in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 340–349,
    2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, 和 I. Reid， “通过深度特征重建进行无监督单目深度估计和视觉里程计学习，”
    见于IEEE/CVF计算机视觉与模式识别会议（CVPR），第340–349页，2018年。'
- en: '[57] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable particle filters:
    End-to-end learning with algorithmic priors,” Robotics: Science and Systems, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] R. Jonschkowski, D. Rastogi, 和 O. Brock， “可微分粒子滤波器：具有算法先验的端到端学习，” 《机器人学：科学与系统》，2018年。'
- en: '[58] N. Yang, R. Wang, J. Stuckler, and D. Cremers, “Deep virtual stereo odometry:
    Leveraging deep depth prediction for monocular direct sparse odometry,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 817–833, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N. Yang, R. Wang, J. Stuckler, 和 D. Cremers， “深度虚拟立体视觉里程计：利用深度预测进行单目直接稀疏视觉里程计，”
    见于欧洲计算机视觉会议（ECCV），第817–833页，2018年。'
- en: '[59] C. Zhao, L. Sun, P. Purkait, T. Duckett, and R. Stolkin, “Learning monocular
    visual odometry with dense 3d mapping from dense 3d flow,” in 2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 6864–6871, IEEE, 2018.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] C. Zhao, L. Sun, P. Purkait, T. Duckett, 和 R. Stolkin， “通过稠密三维流进行稠密三维映射的单目视觉里程计学习，”
    见于2018年IEEE/RSJ国际智能机器人与系统会议（IROS），第6864–6871页，IEEE，2018年。'
- en: '[60] V. Casser, S. Pirk, R. Mahjourian, and A. Angelova, “Depth prediction
    without the sensors: Leveraging structure for unsupervised learning from monocular
    videos,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33,
    pp. 8001–8008, 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] V. Casser, S. Pirk, R. Mahjourian, 和 A. Angelova， “无需传感器的深度预测：利用结构进行无监督学习单目视频，”
    见于AAAI人工智能会议，卷33，第8001–8008页，2019年。'
- en: '[61] Y. Almalioglu, M. R. U. Saputra, P. P. de Gusmao, A. Markham, and N. Trigoni,
    “Ganvo: Unsupervised deep monocular visual odometry and depth estimation with
    generative adversarial networks,” in 2019 International Conference on Robotics
    and Automation (ICRA), pp. 5474–5480, IEEE, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Almalioglu, M. R. U. Saputra, P. P. de Gusmao, A. Markham, 和 N. Trigoni，
    “Ganvo：基于生成对抗网络的无监督深度单目视觉里程计与深度估计，” 见于2019年国际机器人与自动化会议（ICRA），第5474–5480页，IEEE，2019年。'
- en: '[62] S. Y. Loo, A. J. Amiri, S. Mashohor, S. H. Tang, and H. Zhang, “Cnn-svo:
    Improving the mapping in semi-direct visual odometry using single-image depth
    prediction,” in 2019 International Conference on Robotics and Automation (ICRA),
    pp. 5218–5223, IEEE, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Y. Loo, A. J. Amiri, S. Mashohor, S. H. Tang, 和 H. Zhang， “Cnn-svo：通过单图像深度预测改善半直接视觉里程计中的映射，”
    见于2019年国际机器人与自动化会议（ICRA），第5218–5223页，IEEE，2019年。'
- en: '[63] R. Wang, S. M. Pizer, and J.-M. Frahm, “Recurrent neural network for (un-)
    supervised learning of monocular video visual odometry and depth,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5555–5564,
    2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Wang, S. M. Pizer, 和 J.-M. Frahm， “用于（无）监督学习的单目视频视觉里程计和深度的递归神经网络，”
    见于IEEE计算机视觉与模式识别会议（CVPR），第5555–5564页，2019年。'
- en: '[64] Y. Li, Y. Ushiku, and T. Harada, “Pose graph optimization for unsupervised
    monocular visual odometry,” in 2019 International Conference on Robotics and Automation
    (ICRA), pp. 5439–5445, IEEE, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Li, Y. Ushiku, 和 T. Harada， “用于无监督单目视觉里程计的姿态图优化，” 见于2019年国际机器人与自动化会议（ICRA），第5439–5445页，IEEE，2019年。'
- en: '[65] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, “Depth from videos
    in the wild: Unsupervised monocular depth learning from unknown cameras,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 8977–8986, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Gordon, H. Li, R. Jonschkowski, 和 A. Angelova，“来自野外视频的深度：从未知相机的无监督单目深度学习”，发表于IEEE国际计算机视觉会议，第8977–8986页，2019年。'
- en: '[66] A. S. Koumis, J. A. Preiss, and G. S. Sukhatme, “Estimating metric scale
    visual odometry from videos using 3d convolutional networks,” in 2019 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS), pp. 265–272,
    IEEE, 2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. S. Koumis, J. A. Preiss, 和 G. S. Sukhatme，“使用3D卷积网络从视频中估计度量尺度视觉里程计”，发表于2019年IEEE/RSJ国际智能机器人与系统会议（IROS），第265–272页，IEEE，2019年。'
- en: '[67] H. Zhan, C. S. Weerasekera, J. Bian, and I. Reid, “Visual odometry revisited:
    What should be learnt?,” The International Conference on Robotics and Automation
    (ICRA), 2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Zhan, C. S. Weerasekera, J. Bian, 和 I. Reid，“视觉里程计再探讨：应该学习什么？”，国际机器人与自动化会议（ICRA），2020年。'
- en: '[68] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “VINet : Visual-Inertial
    Odometry as a Sequence-to-Sequence Learning Problem,” in The AAAI Conference on
    Artificial Intelligence (AAAI), pp. 3995–4001, 2017.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] R. Clark, S. Wang, H. Wen, A. Markham, 和 N. Trigoni，“VINet: 将视觉-惯性里程计视为序列到序列学习问题”，发表于AAAI人工智能会议（AAAI），第3995–4001页，2017年。'
- en: '[69] E. J. Shamwell, K. Lindgren, S. Leung, and W. D. Nothwang, “Unsupervised
    deep visual-inertial odometry with online error correction for rgb-d imagery,”
    IEEE transactions on pattern analysis and machine intelligence, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] E. J. Shamwell, K. Lindgren, S. Leung, 和 W. D. Nothwang，“用于RGB-D图像的无监督深度视觉-惯性里程计及在线误差修正”，IEEE模式分析与机器智能交易，第2019年。'
- en: '[70] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, and N. Trigoni,
    “Selective sensor fusion for neural visual-inertial odometry,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10542–10551,
    2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, 和 N. Trigoni，“用于神经视觉-惯性里程计的选择性传感器融合”，发表于IEEE计算机视觉与模式识别会议，第10542–10551页，2019年。'
- en: '[71] L. Han, Y. Lin, G. Du, and S. Lian, “Deepvio: Self-supervised deep learning
    of monocular visual inertial odometry using 3d geometric constraints,” arXiv preprint
    arXiv:1906.11435, 2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] L. Han, Y. Lin, G. Du, 和 S. Lian，“Deepvio: 使用3D几何约束的自监督深度单目视觉惯性里程计学习”，arXiv预印本arXiv:1906.11435，2019年。'
- en: '[72] M. Velas, M. Spanel, M. Hradis, and A. Herout, “Cnn for imu assisted odometry
    estimation using velodyne lidar,” in 2018 IEEE International Conference on Autonomous
    Robot Systems and Competitions (ICARSC), pp. 71–77, IEEE, 2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Velas, M. Spanel, M. Hradis, 和 A. Herout，“使用Velodyne激光雷达的IMU辅助里程计估计的CNN”，发表于2018年IEEE国际自主机器人系统与竞赛会议（ICARSC），第71–77页，IEEE，2018年。'
- en: '[73] Q. Li, S. Chen, C. Wang, X. Li, C. Wen, M. Cheng, and J. Li, “Lo-net:
    Deep real-time lidar odometry,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 8473–8482, 2019.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Q. Li, S. Chen, C. Wang, X. Li, C. Wen, M. Cheng, 和 J. Li，“Lo-net: 深度实时激光雷达里程计”，发表于IEEE计算机视觉与模式识别会议（CVPR），第8473–8482页，2019年。'
- en: '[74] W. Wang, M. R. U. Saputra, P. Zhao, P. Gusmao, B. Yang, C. Chen, A. Markham,
    and N. Trigoni, “Deeppco: End-to-end point cloud odometry through deep parallel
    neural network,” The 2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS 2019), 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] W. Wang, M. R. U. Saputra, P. Zhao, P. Gusmao, B. Yang, C. Chen, A. Markham,
    和 N. Trigoni，“Deeppco: 通过深度并行神经网络进行端到端点云里程计”，2019年IEEE/RSJ国际智能机器人与系统会议（IROS 2019），2019年。'
- en: '[75] M. Valente, C. Joly, and A. de La Fortelle, “Deep sensor fusion for real-time
    odometry estimation,” 2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Valente, C. Joly, 和 A. de La Fortelle，“用于实时里程计估计的深度传感器融合”，2019年IEEE/RSJ国际智能机器人与系统会议（IROS），2019年。'
- en: '[76] S. Li, F. Xue, X. Wang, Z. Yan, and H. Zha, “Sequential adversarial learning
    for self-supervised deep visual odometry,” in Proceedings of the IEEE International
    Conference on Computer Vision (ICCV), pp. 2851–2860, 2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Li, F. Xue, X. Wang, Z. Yan, 和 H. Zha，“自监督深度视觉里程计的序列对抗学习”，发表于IEEE国际计算机视觉会议（ICCV），第2851–2860页，2019年。'
- en: '[77] L. Sheng, D. Xu, W. Ouyang, and X. Wang, “Unsupervised collaborative learning
    of keyframe detection and visual odometry towards monocular deep slam,” in Proceedings
    of the IEEE International Conference on Computer Vision (ICCV), pp. 4302–4311,
    2019.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Sheng, D. Xu, W. Ouyang, 和 X. Wang，“无监督关键帧检测与视觉里程计协作学习，面向单目深度SLAM”，发表于IEEE国际计算机视觉会议（ICCV），第4302–4311页，2019年。'
- en: '[78] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in Advances in neural information processing
    systems, pp. 2366–2374, 2014.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] D. Eigen, C. Puhrsch, 和 R. Fergus，“利用多尺度深度网络从单张图像预测深度图”，发表于《神经信息处理系统进展》，第2366–2374页，2014年。'
- en: '[79] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5038–5047,
    2017.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, 和
    T. Brox，“Demon：深度与运动网络用于学习单目立体视觉”，发表于《IEEE计算机视觉与模式识别会议论文集》，第5038–5047页，2017年。'
- en: '[80] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised cnn for single
    view depth estimation: Geometry to the rescue,” in European Conference on Computer
    Vision, pp. 740–756, Springer, 2016.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] R. Garg, V. K. BG, G. Carneiro, 和 I. Reid，“无监督的单视图深度估计：几何学的救援”，发表于《欧洲计算机视觉会议》，第740–756页，Springer，2016年。'
- en: '[81] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 270–279, 2017.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Godard, O. Mac Aodha, 和 G. J. Brostow，“基于无监督的单目深度估计与左右一致性”，发表于《IEEE计算机视觉与模式识别会议论文集》，第270–279页，2017年。'
- en: '[82] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop KF: Learning
    Discriminative Deterministic State Estimators,” in Advances In Neural Information
    Processing Systems (NeurIPS), 2016.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] T. Haarnoja, A. Ajay, S. Levine, 和 P. Abbeel，“Backprop KF：学习判别性确定性状态估计器”，发表于《神经信息处理系统进展》（NeurIPS），2016年。'
- en: '[83] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable Particle Filters:
    End-to-End Learning with Algorithmic Priors,” in Robotics: Science and Systems,
    2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] R. Jonschkowski, D. Rastogi, 和 O. Brock，“可微分粒子滤波器：具有算法先验的端到端学习”，发表于《机器人：科学与系统》，2018年。'
- en: '[84] L. Von Stumberg, V. Usenko, and D. Cremers, “Direct sparse visual-inertial
    odometry using dynamic marginalization,” in 2018 IEEE International Conference
    on Robotics and Automation (ICRA), pp. 2510–2517, IEEE, 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] L. Von Stumberg, V. Usenko, 和 D. Cremers，“使用动态边缘化的直接稀疏视觉惯性里程计”，发表于2018
    IEEE国际机器人与自动化会议（ICRA），第2510–2517页，IEEE，2018年。'
- en: '[85] C. Chen, X. Lu, A. Markham, and N. Trigoni, “Ionet: Learning to cure the
    curse of drift in inertial odometry,” in Thirty-Second AAAI Conference on Artificial
    Intelligence, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Chen, X. Lu, A. Markham, 和 N. Trigoni，“Ionet：学习解决惯性里程计中的漂移诅咒”，发表于第32届AAAI人工智能会议，2018年。'
- en: '[86] C. Chen, Y. Miao, C. X. Lu, L. Xie, P. Blunsom, A. Markham, and N. Trigoni,
    “Motiontransformer: Transferring neural inertial tracking between domains,” in
    Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 8009–8016,
    2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] C. Chen, Y. Miao, C. X. Lu, L. Xie, P. Blunsom, A. Markham, 和 N. Trigoni，“Motiontransformer：在不同领域之间转移神经惯性跟踪”，发表于《AAAI人工智能会议论文集》，第33卷，第8009–8016页，2019年。'
- en: '[87] M. A. Esfahani, H. Wang, K. Wu, and S. Yuan, “Aboldeepio: A novel deep
    inertial odometry network for autonomous vehicles,” IEEE Transactions on Intelligent
    Transportation Systems, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. A. Esfahani, H. Wang, K. Wu, 和 S. Yuan，“Aboldeepio：一种用于自动驾驶车辆的新型深度惯性里程计网络”，《IEEE智能交通系统汇刊》，2019年。'
- en: '[88] H. Yan, Q. Shan, and Y. Furukawa, “Ridi: Robust imu double integration,”
    in Proceedings of the European Conference on Computer Vision (ECCV), pp. 621–636,
    2018.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] H. Yan, Q. Shan, 和 Y. Furukawa，“Ridi：鲁棒的IMU双重积分”，发表于《欧洲计算机视觉会议论文集》（ECCV），第621–636页，2018年。'
- en: '[89] S. Cortés, A. Solin, and J. Kannala, “Deep learning based speed estimation
    for constraining strapdown inertial navigation on smartphones,” in 2018 IEEE 28th
    International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1–6,
    IEEE, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] S. Cortés, A. Solin, 和 J. Kannala，“基于深度学习的速度估计，用于约束智能手机上的惯性导航”，发表于2018
    IEEE第28届信号处理机器学习国际研讨会（MLSP），第1–6页，IEEE，2018年。'
- en: '[90] B. Wagstaff and J. Kelly, “Lstm-based zero-velocity detection for robust
    inertial navigation,” in 2018 International Conference on Indoor Positioning and
    Indoor Navigation (IPIN), pp. 1–8, IEEE, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] B. Wagstaff 和 J. Kelly，“基于LSTM的零速度检测用于鲁棒惯性导航”，发表于2018年国际室内定位与室内导航会议（IPIN），第1–8页，IEEE，2018年。'
- en: '[91] M. Brossard, A. Barrau, and S. Bonnabel, “Rins-w: Robust inertial navigation
    system on wheels,” 2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Brossard, A. Barrau, 和 S. Bonnabel，“Rins-w：轮上的鲁棒惯性导航系统”，2019 IEEE/RSJ国际智能机器人与系统会议（IROS），2019年。'
- en: '[92] F. Liu, C. Shen, G. Lin, and I. Reid, “Learning depth from single monocular
    images using deep convolutional neural fields,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 38, no. 10, pp. 2024–2039, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] F. Liu, C. Shen, G. Lin, 和 I. Reid, “使用深度卷积神经场从单张单目图像中学习深度，” IEEE模式分析与机器智能期刊，卷.
    38, 期. 10, pp. 2024–2039, 2015。'
- en: '[93] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey, “Learning depth
    from monocular videos using direct methods,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2022–2030, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] C. Wang, J. Miguel Buenaposada, R. Zhu, 和 S. Lucey, “通过直接方法从单目视频中学习深度，”
    发表在IEEE计算机视觉与模式识别会议论文集，pp. 2022–2030, 2018。'
- en: '[94] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “Surfacenet: An end-to-end
    3d neural network for multiview stereopsis,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 2307–2315, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Ji, J. Gall, H. Zheng, Y. Liu, 和 L. Fang, “Surfacenet：一种端到端的3D神经网络用于多视角立体视觉，”
    发表在IEEE国际计算机视觉会议论文集，pp. 2307–2315, 2017。'
- en: '[95] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, and A. Geiger, “Raynet:
    Learning volumetric 3d reconstruction with ray potentials,” in Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3897–3906,
    2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, 和 A. Geiger, “Raynet：使用光线潜能学习体积3D重建，”
    发表在IEEE计算机视觉与模式识别会议论文集，pp. 3897–3906, 2018。'
- en: '[96] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in Advances in neural information processing systems, pp. 365–376, 2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Kar, C. Häne, 和 J. Malik, “学习多视角立体机器，” 发表在神经信息处理系统进展，pp. 365–376, 2017。'
- en: '[97] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Octree generating networks:
    Efficient convolutional architectures for high-resolution 3d outputs,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 2088–2096, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox, “Octree生成网络：高分辨率3D输出的高效卷积架构，”
    发表在IEEE国际计算机视觉会议论文集，pp. 2088–2096, 2017。'
- en: '[98] C. Häne, S. Tulsiani, and J. Malik, “Hierarchical surface prediction for
    3d object reconstruction,” in 2017 International Conference on 3D Vision (3DV),
    pp. 412–420, IEEE, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] C. Häne, S. Tulsiani, 和 J. Malik, “用于3D物体重建的分层表面预测，” 发表在2017年国际3D视觉会议（3DV），pp.
    412–420, IEEE, 2017。'
- en: '[99] A. Dai, C. Ruizhongtai Qi, and M. Nießner, “Shape completion using 3d-encoder-predictor
    cnns and shape synthesis,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 5868–5877, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Dai, C. Ruizhongtai Qi, 和 M. Nießner, “使用3D编码器预测CNN和形状合成的形状补全，” 发表在IEEE计算机视觉与模式识别会议论文集，pp.
    5868–5877, 2017。'
- en: '[100] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger, “Octnetfusion: Learning
    depth fusion from data,” in 2017 International Conference on 3D Vision (3DV),
    pp. 57–66, IEEE, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] G. Riegler, A. O. Ulusoy, H. Bischof, 和 A. Geiger, “Octnetfusion：从数据中学习深度融合，”
    发表在2017年国际3D视觉会议（3DV），pp. 57–66, IEEE, 2017。'
- en: '[101] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for
    3d object reconstruction from a single image,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 605–613, 2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] H. Fan, H. Su, 和 L. J. Guibas, “一种用于从单张图像重建3D物体的点集生成网络，” 发表在IEEE计算机视觉与模式识别会议论文集，pp.
    605–613, 2017。'
- en: '[102] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché
    approach to learning 3d surface generation,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 216–224, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, 和 M. Aubry, “一种纸糊法的3D表面生成学习方法，”
    发表在IEEE计算机视觉与模式识别会议论文集，pp. 216–224, 2018。'
- en: '[103] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, “Pixel2mesh:
    Generating 3d mesh models from single rgb images,” in Proceedings of the European
    Conference on Computer Vision (ECCV), pp. 52–67, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, 和 Y.-G. Jiang, “Pixel2mesh：从单张RGB图像生成3D网格模型，”
    发表在欧洲计算机视觉会议（ECCV）论文集，pp. 52–67, 2018。'
- en: '[104] L. Ladicky, O. Saurer, S. Jeong, F. Maninchedda, and M. Pollefeys, “From
    point clouds to mesh using regression,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 3893–3902, 2017.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] L. Ladicky, O. Saurer, S. Jeong, F. Maninchedda, 和 M. Pollefeys, “从点云到网格的回归方法，”
    发表在IEEE国际计算机视觉会议论文集，pp. 3893–3902, 2017。'
- en: '[105] A. Dai and M. Nießner, “Scan2mesh: From unstructured range scans to 3d
    meshes,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 5574–5583, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] A. Dai 和 M. Nießner, “Scan2mesh：从非结构化范围扫描到3D网格，” 发表在IEEE计算机视觉与模式识别会议论文集，pp.
    5574–5583, 2019。'
- en: '[106] T. Mukasa, J. Xu, and B. Stenger, “3d scene mesh from cnn depth predictions
    and sparse monocular slam,” in Proceedings of the IEEE International Conference
    on Computer Vision Workshops, pp. 921–928, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] T. Mukasa, J. Xu 和 B. Stenger，“基于CNN深度预测和稀疏单目SLAM的3D场景网格”，发表于IEEE国际计算机视觉会议工作坊，第921–928页，2017年。'
- en: '[107] M. Bloesch, T. Laidlow, R. Clark, S. Leutenegger, and A. J. Davison,
    “Learning meshes for dense visual slam,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 5855–5864, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. Bloesch, T. Laidlow, R. Clark, S. Leutenegger 和 A. J. Davison，“为密集视觉SLAM学习网格”，发表于IEEE国际计算机视觉会议论文集，第5855–5864页，2019年。'
- en: '[108] Y. Xiang and D. Fox, “Da-rnn: Semantic mapping with data associated recurrent
    neural networks,” Robotics: Science and Systems, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Xiang 和 D. Fox，“Da-rnn: 使用数据关联递归神经网络的语义映射”，《机器人学: 科学与系统》，2017年。'
- en: '[109] N. Sünderhauf, T. T. Pham, Y. Latif, M. Milford, and I. Reid, “Meaningful
    maps with object-oriented semantic mapping,” in 2017 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pp. 5079–5085, IEEE, 2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] N. Sünderhauf, T. T. Pham, Y. Latif, M. Milford 和 I. Reid，“具有面向对象语义映射的有意义地图”，发表于2017年IEEE/RSJ国际智能机器人与系统会议（IROS），第5079–5085页，IEEE，2017年。'
- en: '[110] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger, “Fusion++:
    Volumetric object-level slam,” in 2018 international conference on 3D vision (3DV),
    pp. 32–41, IEEE, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. McCormac, R. Clark, M. Bloesch, A. Davison, 和 S. Leutenegger，“Fusion++:
    体积级别对象 SLAM”，发表于2018年国际3D视觉会议（3DV），第32–41页，IEEE，2018年。'
- en: '[111] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart,
    and J. Nieto, “Volumetric instance-aware semantic mapping and 3d object discovery,”
    IEEE Robotics and Automation Letters, vol. 4, no. 3, pp. 3037–3044, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart
    和 J. Nieto，“体积实例感知语义映射与3D对象发现”，《IEEE机器人与自动化通讯》，第4卷，第3期，第3037–3044页，2019年。'
- en: '[112] G. Narita, T. Seno, T. Ishikawa, and Y. Kaji, “Panopticfusion: Online
    volumetric semantic mapping at the level of stuff and things,” IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] G. Narita, T. Seno, T. Ishikawa 和 Y. Kaji，“Panopticfusion: 在事物级别进行在线体积语义映射”，IEEE/RSJ国际智能机器人与系统会议（IROS），2019年。'
- en: '[113] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison,
    “CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM,”
    in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger 和 A. J. Davison，“CodeSLAM
    — 学习紧凑的、可优化的密集视觉SLAM表示”，发表于IEEE/CVF计算机视觉与模式识别会议（CVPR），2018年。'
- en: '[114] J. Tobin, W. Zaremba, and P. Abbeel, “Geometry-aware neural rendering,”
    in Advances in Neural Information Processing Systems, pp. 11555–11565, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Tobin, W. Zaremba 和 P. Abbeel，“几何感知神经渲染”，发表于《神经信息处理系统进展》，第11555–11565页，2019年。'
- en: '[115] J. H. Lim, P. O. Pinheiro, N. Rostamzadeh, C. Pal, and S. Ahn, “Neural
    multisensory scene inference,” in Advances in Neural Information Processing Systems,
    pp. 8994–9004, 2019.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. H. Lim, P. O. Pinheiro, N. Rostamzadeh, C. Pal 和 S. Ahn，“神经多感官场景推断”，发表于《神经信息处理系统进展》，第8994–9004页，2019年。'
- en: '[116] V. Sitzmann, M. Zollhöfer, and G. Wetzstein, “Scene representation networks:
    Continuous 3d-structure-aware neural scene representations,” in Advances in Neural
    Information Processing Systems, pp. 1119–1130, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] V. Sitzmann, M. Zollhöfer 和 G. Wetzstein，“场景表示网络：连续3D结构感知神经场景表示”，发表于《神经信息处理系统进展》，第1119–1130页，2019年。'
- en: '[117] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    and K. Kavukcuoglu, “Reinforcement learning with unsupervised auxiliary tasks,”
    ICLR, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver
    和 K. Kavukcuoglu，“具有无监督辅助任务的强化学习”，ICLR，2017年。'
- en: '[118] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al., “Learning to navigate
    in complex environments,” ICLR, 2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu 等，“学习在复杂环境中导航”，ICLR，2017年。'
- en: '[119] C. Kerl, J. Sturm, and D. Cremers, “Dense visual slam for rgb-d cameras,”
    in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2100–2106,
    IEEE, 2013.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Kerl, J. Sturm 和 D. Cremers，“针对RGB-D相机的密集视觉SLAM”，发表于2013年IEEE/RSJ国际智能机器人与系统会议，第2100–2106页，IEEE，2013年。'
- en: '[120] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and J. McDonald,
    “Real-time large-scale dense rgb-d slam with volumetric fusion,” The International
    Journal of Robotics Research, vol. 34, no. 4-5, pp. 598–626, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, 和 J. McDonald,
    “实时大规模密集rgb-d slam与体积融合，” 国际机器人研究杂志，第34卷，第4-5期，第598–626页，2015年。'
- en: '[121] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “DTAM : Dense Tracking
    and Mapping in Real-Time,” in IEEE International Conference on Computer Vision
    (ICCV), pp. 2320–2327, 2011.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] R. A. Newcombe, S. J. Lovegrove, 和 A. J. Davison, “DTAM：实时密集跟踪与地图构建，”
    在IEEE国际计算机视觉大会（ICCV）上，第2320–2327页，2011年。'
- en: '[122] K. Karsch, C. Liu, and S. B. Kang, “Depth transfer: Depth extraction
    from video using non-parametric sampling,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 36, no. 11, pp. 2144–2158, 2014.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Karsch, C. Liu, 和 S. B. Kang, “深度转移：使用非参数采样从视频中提取深度，” IEEE模式分析与机器智能汇刊，第36卷，第11期，第2144–2158页，2014年。'
- en: '[123] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 6243–6252, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] K. Tateno, F. Tombari, I. Laina, 和 N. Navab, “Cnn-slam：基于学习深度预测的实时密集单目slam，”
    在IEEE计算机视觉与模式识别会议论文集中，第6243–6252页，2017年。'
- en: '[124] J. Engel, T. Schöps, and D. Cremers, “Lsd-slam: Large-scale direct monocular
    slam,” in European conference on computer vision, pp. 834–849, Springer, 2014.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. Engel, T. Schöps, 和 D. Cremers, “Lsd-slam：大规模直接单目slam，” 在欧洲计算机视觉会议上，第834–849页，Springer，2014年。'
- en: '[125] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in Proceedings of the IEEE
    conference on computer vision and pattern recognition, pp. 652–660, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet：针对3D分类和分割的点集深度学习，”
    在IEEE计算机视觉与模式识别会议论文集中，第652–660页，2017年。'
- en: '[126] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár, “Panoptic
    segmentation,” in Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 9404–9413, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] A. Kirillov, K. He, R. Girshick, C. Rother, 和 P. Dollár, “全景分割，” 在IEEE计算机视觉与模式识别会议论文集中，第9404–9413页，2019年。'
- en: '[127] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison,
    P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon, “Kinectfusion: Real-time dense
    surface mapping and tracking,” in 2011 10th IEEE International Symposium on Mixed
    and Augmented Reality, pp. 127–136, IEEE, 2011.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison,
    P. Kohi, J. Shotton, S. Hodges, 和 A. Fitzgibbon, “Kinectfusion：实时密集表面映射与跟踪，” 在2011年第10届IEEE国际混合与增强现实研讨会上，第127–136页，IEEE，2011年。'
- en: '[128] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo,
    A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, et al., “Neural scene representation
    and rendering,” Science, vol. 360, no. 6394, pp. 1204–1210, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo,
    A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, 等, “神经场景表示与渲染，” 科学，第360卷，第6394期，第1204–1210页，2018年。'
- en: '[129] V. Balntas, S. Li, and V. Prisacariu, “Relocnet: Continuous metric learning
    relocalisation using neural nets,” in Proceedings of the European Conference on
    Computer Vision (ECCV), pp. 751–767, 2018.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] V. Balntas, S. Li, 和 V. Prisacariu, “Relocnet：使用神经网络进行连续度量学习重新定位，” 在欧洲计算机视觉会议（ECCV）论文集中，第751–767页，2018年。'
- en: '[130] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional network
    for real-time 6-dof camera relocalization,” in Proceedings of the IEEE international
    Conference on Computer Vision (ICCV), pp. 2938–2946, 2015.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Kendall, M. Grimes, 和 R. Cipolla, “Posenet：一种用于实时6自由度相机重新定位的卷积网络，”
    在IEEE国际计算机视觉会议（ICCV）论文集中，第2938–2946页，2015年。'
- en: '[131] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera relocalization
    by computing pairwise relative poses using convolutional neural network,” in Proceedings
    of the IEEE International Conference on Computer Vision Workshops, pp. 929–938,
    2017.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Z. Laskar, I. Melekhov, S. Kalia, 和 J. Kannala, “通过计算对相对姿态的卷积神经网络进行相机重新定位，”
    在IEEE国际计算机视觉会议研讨会上，第929–938页，2017年。'
- en: '[132] P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin, “Dels-3d: Deep localization
    and segmentation with a 3d semantic map,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 5860–5869, 2018.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] P. Wang, R. Yang, B. Cao, W. Xu, 和 Y. Lin, “Dels-3d：使用3D语义地图的深度定位与分割，”
    在IEEE计算机视觉与模式识别会议论文集中，第5860–5869页，2018年。'
- en: '[133] S. Saha, G. Varma, and C. Jawahar, “Improved visual relocalization by
    discovering anchor points,” arXiv preprint arXiv:1811.04370, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S. Saha, G. Varma, 和 C. Jawahar, “通过发现锚点来改进视觉重新定位，” arXiv 预印本 arXiv:1811.04370,
    2018。'
- en: '[134] M. Ding, Z. Wang, J. Sun, J. Shi, and P. Luo, “Camnet: Coarse-to-fine
    retrieval for camera re-localization,” in Proceedings of the IEEE International
    Conference on Computer Vision (ICCV), pp. 2871–2880, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Ding, Z. Wang, J. Sun, J. Shi, 和 P. Luo, “Camnet: 从粗到细的相机重新定位检索，”
    在 IEEE 国际计算机视觉会议论文集（ICCV），pp. 2871–2880, 2019。'
- en: '[135] A. Kendall and R. Cipolla, “Modelling uncertainty in deep learning for
    camera relocalization,” in 2016 IEEE international conference on Robotics and
    Automation (ICRA), pp. 4762–4769, IEEE, 2016.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Kendall 和 R. Cipolla, “在深度学习中建模相机重新定位的不确定性，” 在 2016 IEEE 国际机器人与自动化会议（ICRA），pp.
    4762–4769, IEEE, 2016。'
- en: '[136] J. Wu, L. Ma, and X. Hu, “Delving deeper into convolutional neural networks
    for camera relocalization,” in 2017 IEEE International Conference on Robotics
    and Automation (ICRA), pp. 5644–5651, IEEE, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Wu, L. Ma, 和 X. Hu, “深入探讨卷积神经网络用于相机重新定位，” 在 2017 IEEE 国际机器人与自动化会议（ICRA），pp.
    5644–5651, IEEE, 2017。'
- en: '[137] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “VidLoc: A Deep
    Spatio-Temporal Model for 6-DoF Video-Clip Relocalization,” in IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] R. Clark, S. Wang, A. Markham, N. Trigoni, 和 H. Wen, “VidLoc: 一种深度时空模型用于6自由度视频片段重新定位，”
    在 IEEE/CVF 计算机视觉与模式识别会议（CVPR），2017。'
- en: '[138] A. Kendall and R. Cipolla, “Geometric loss functions for camera pose
    regression with deep learning,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 5974–5983, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Kendall 和 R. Cipolla, “用于相机姿态回归的几何损失函数与深度学习，” 在 IEEE 计算机视觉与模式识别会议（CVPR）论文集，pp.
    5974–5983, 2017。'
- en: '[139] T. Naseer and W. Burgard, “Deep regression for monocular camera-based
    6-dof global localization in outdoor environments,” in 2017 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 1525–1530, IEEE, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] T. Naseer 和 W. Burgard, “基于单目相机的6自由度全球定位的深度回归，” 在 2017 IEEE/RSJ 国际智能机器人与系统会议（IROS），pp.
    1525–1530, IEEE, 2017。'
- en: '[140] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, and
    D. Cremers, “Image-based localization using lstms for structured feature correlation,”
    in Proceedings of the IEEE International Conference on Computer Vision (ICCV),
    pp. 627–637, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, 和 D.
    Cremers, “使用 LSTM 进行结构化特征关联的图像基础定位，” 在 IEEE 国际计算机视觉会议论文集（ICCV），pp. 627–637, 2017。'
- en: '[141] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu, “Image-based localization
    using hourglass networks,” in Proceedings of the IEEE International Conference
    on Computer Vision Workshops, pp. 879–886, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] I. Melekhov, J. Ylioinas, J. Kannala, 和 E. Rahtu, “基于图像的定位使用小时玻璃网络，”
    在 IEEE 国际计算机视觉会议论文集，pp. 879–886, 2017。'
- en: '[142] A. Valada, N. Radwan, and W. Burgard, “Deep auxiliary learning for visual
    localization and odometry,” in 2018 IEEE international conference on robotics
    and automation (ICRA), pp. 6939–6946, IEEE, 2018.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] A. Valada, N. Radwan, 和 W. Burgard, “视觉定位和里程计的深度辅助学习，” 在 2018 IEEE 国际机器人与自动化会议（ICRA），pp.
    6939–6946, IEEE, 2018。'
- en: '[143] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz, “Geometry-Aware
    Learning of Maps for Camera Localization,” in IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2616–2625, 2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, 和 J. Kautz, “基于几何感知的相机定位地图学习，”
    在 IEEE/CVF 计算机视觉与模式识别会议（CVPR），pp. 2616–2625, 2018。'
- en: '[144] P. Purkait, C. Zhao, and C. Zach, “Synthetic view generation for absolute
    pose regression and image synthesis.,” in BMVC, p. 69, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] P. Purkait, C. Zhao, 和 C. Zach, “用于绝对姿态回归和图像合成的合成视图生成，” 在 BMVC，p. 69,
    2018。'
- en: '[145] M. Cai, C. Shen, and I. D. Reid, “A hybrid probabilistic model for camera
    relocalization.,” in BMVC, vol. 1, p. 8, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Cai, C. Shen, 和 I. D. Reid, “一种用于相机重新定位的混合概率模型，” 在 BMVC，vol. 1, p.
    8, 2018。'
- en: '[146] N. Radwan, A. Valada, and W. Burgard, “Vlocnet++: Deep multitask learning
    for semantic visual localization and odometry,” IEEE Robotics and Automation Letters,
    vol. 3, no. 4, pp. 4407–4414, 2018.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] N. Radwan, A. Valada, 和 W. Burgard, “Vlocnet++: 深度多任务学习用于语义视觉定位和里程计，”
    IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4407–4414, 2018。'
- en: '[147] F. Xue, X. Wang, Z. Yan, Q. Wang, J. Wang, and H. Zha, “Local supports
    global: Deep camera relocalization with sequence enhancement,” in Proceedings
    of the IEEE International Conference on Computer Vision (ICCV), pp. 2841–2850,
    2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] F. Xue, X. Wang, Z. Yan, Q. Wang, J. Wang, 和 H. Zha, “局部支持全球：通过序列增强进行深度相机重定位,”
    见于 IEEE 国际计算机视觉会议 (ICCV), 页 2841–2850, 2019。'
- en: '[148] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, “Prior guided
    dropout for robust visual localization in dynamic environments,” in Proceedings
    of the IEEE International Conference on Computer Vision (ICCV), pp. 2791–2800,
    2019.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, 和 G. Zhang, “优先指导的 dropout
    用于动态环境中的鲁棒视觉定位,” 见于 IEEE 国际计算机视觉会议 (ICCV), 页 2791–2800, 2019。'
- en: '[149] M. Bui, C. Baur, N. Navab, S. Ilic, and S. Albarqouni, “Adversarial networks
    for camera pose regression and refinement,” in Proceedings of the IEEE International
    Conference on Computer Vision Workshops, pp. 0–0, 2019.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. Bui, C. Baur, N. Navab, S. Ilic, 和 S. Albarqouni, “用于相机姿态回归和优化的对抗网络,”
    见于 IEEE 国际计算机视觉会议研讨会, 页 0–0, 2019。'
- en: '[150] B. Wang, C. Chen, C. X. Lu, P. Zhao, N. Trigoni, and A. Markham, “Atloc:
    Attention guided camera localization,” arXiv preprint arXiv:1909.03557, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] B. Wang, C. Chen, C. X. Lu, P. Zhao, N. Trigoni, 和 A. Markham, “Atloc:
    基于注意力的相机定位,” arXiv 预印本 arXiv:1909.03557, 2019。'
- en: '[151] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon,
    “Scene coordinate regression forests for camera relocalization in rgb-d images,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2930–2937, 2013.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, 和 A. Fitzgibbon,
    “场景坐标回归森林用于 RGB-D 图像中的相机重定位,” 见于 IEEE 计算机视觉与模式识别大会, 页 2930–2937, 2013。'
- en: '[152] R. Arandjelović and A. Zisserman, “Dislocation: Scalable descriptor distinctiveness
    for location recognition,” in Asian Conference on Computer Vision, pp. 188–204,
    Springer, 2014.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] R. Arandjelović 和 A. Zisserman, “Dislocation: 可扩展的描述符特异性用于位置识别,” 见于亚洲计算机视觉大会,
    页 188–204, Springer, 2014。'
- en: '[153] D. M. Chen, G. Baatz, K. Köser, S. S. Tsai, R. Vedantham, T. Pylvänäinen,
    K. Roimela, X. Chen, J. Bach, M. Pollefeys, et al., “City-scale landmark identification
    on mobile devices,” in CVPR 2011, pp. 737–744, IEEE, 2011.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] D. M. Chen, G. Baatz, K. Köser, S. S. Tsai, R. Vedantham, T. Pylvänäinen,
    K. Roimela, X. Chen, J. Bach, M. Pollefeys, 等, “移动设备上的城市规模地标识别,” 见于 CVPR 2011,
    页 737–744, IEEE, 2011。'
- en: '[154] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla, “24/7
    place recognition by view synthesis,” in Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 1808–1817, 2015.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, 和 T. Pajdla, “24/7 场所识别通过视图合成,”
    见于 IEEE 计算机视觉与模式识别大会, 页 1808–1817, 2015。'
- en: '[155] Z. Chen, O. Lam, A. Jacobson, and M. Milford, “Convolutional neural network-based
    place recognition,” arXiv preprint arXiv:1411.1509, 2014.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Z. Chen, O. Lam, A. Jacobson, 和 M. Milford, “基于卷积神经网络的场所识别,” arXiv 预印本
    arXiv:1411.1509, 2014。'
- en: '[156] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On
    the performance of convnet features for place recognition,” in 2015 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 4297–4304, IEEE, 2015.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, 和 M. Milford, “卷积网络特征在场所识别中的性能,”
    见于 2015 IEEE/RSJ 智能机器人与系统国际会议 (IROS), 页 4297–4304, IEEE, 2015。'
- en: '[157] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “Netvlad:
    Cnn architecture for weakly supervised place recognition,” in Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp. 5297–5307, 2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, 和 J. Sivic, “Netvlad:
    用于弱监督场所识别的 CNN 架构,” 见于 IEEE 计算机视觉与模式识别大会, 页 5297–5307, 2016。'
- en: '[158] Q. Zhou, T. Sattler, M. Pollefeys, and L. Leal-Taixe, “To learn or not
    to learn: Visual localization from essential matrices,” arXiv preprint arXiv:1908.01293,
    2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Q. Zhou, T. Sattler, M. Pollefeys, 和 L. Leal-Taixe, “学习与否：从本质矩阵进行视觉定位,”
    arXiv 预印本 arXiv:1908.01293, 2019。'
- en: '[159] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, and J. Kannala,
    “Dgc-net: Dense geometric correspondence network,” in 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), pp. 1034–1042, IEEE, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, 和 J. Kannala,
    “Dgc-net: 稠密几何对应网络,” 见于 2019 IEEE 冬季计算机视觉应用会议 (WACV), 页 1034–1042, IEEE, 2019。'
- en: '[160] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in CVPR, 2015.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich, “通过卷积深入探索,” 见于 CVPR, 2015。'
- en: '[161] M. Larsson, E. Stenborg, C. Toft, L. Hammarstrand, T. Sattler, and F. Kahl,
    “Fine-grained segmentation networks: Self-supervised segmentation for improved
    long-term visual localization,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 31–41, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] M. Larsson, E. Stenborg, C. Toft, L. Hammarstrand, T. Sattler, 和 F. Kahl,
    “细粒度分割网络：自监督分割以改进长期视觉定位，” 载于IEEE国际计算机视觉大会论文集，页31–41，2019年。'
- en: '[162] T. Sattler, Q. Zhou, M. Pollefeys, and L. Leal-Taixe, “Understanding
    the limitations of cnn-based absolute camera pose regression,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3302–3312,
    2019.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] T. Sattler, Q. Zhou, M. Pollefeys, 和 L. Leal-Taixe, “理解基于CNN的绝对相机姿态回归的局限性，”
    载于IEEE计算机视觉与模式识别会议论文集，页3302–3312，2019年。'
- en: '[163] Y. Li, N. Snavely, and D. P. Huttenlocher, “Location recognition using
    prioritized feature matching,” in European conference on computer vision, pp. 791–804,
    Springer, 2010.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Y. Li, N. Snavely, 和 D. P. Huttenlocher, “使用优先特征匹配进行位置识别，” 载于欧洲计算机视觉会议论文集，页791–804，Springer，2010年。'
- en: '[164] Y. Li, N. Snavely, D. Huttenlocher, and P. Fua, “Worldwide pose estimation
    using 3d point clouds,” in European conference on computer vision, pp. 15–29,
    Springer, 2012.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Li, N. Snavely, D. Huttenlocher, 和 P. Fua, “使用3D点云进行全球姿态估计，” 载于欧洲计算机视觉会议论文集，页15–29，Springer，2012年。'
- en: '[165] B. Zeisl, T. Sattler, and M. Pollefeys, “Camera pose voting for large-scale
    image-based localization,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 2704–2712, 2015.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] B. Zeisl, T. Sattler, 和 M. Pollefeys, “大规模基于图像的定位的相机姿态投票，” 载于IEEE国际计算机视觉大会论文集，页2704–2712，2015年。'
- en: '[166] T. Cavallari, S. Golodetz, N. A. Lord, J. Valentin, L. Di Stefano, and
    P. H. Torr, “On-the-fly adaptation of regression forests for online camera relocalisation,”
    in Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 4457–4466, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] T. Cavallari, S. Golodetz, N. A. Lord, J. Valentin, L. Di Stefano, 和
    P. H. Torr, “在线相机重新定位的回归森林即时适应，” 载于IEEE计算机视觉与模式识别会议论文集，页4457–4466，2017年。'
- en: '[167] A. Guzman-Rivera, P. Kohli, B. Glocker, J. Shotton, T. Sharp, A. Fitzgibbon,
    and S. Izadi, “Multi-output learning for camera relocalization,” in Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 1114–1121,
    2014.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Guzman-Rivera, P. Kohli, B. Glocker, J. Shotton, T. Sharp, A. Fitzgibbon,
    和 S. Izadi, “用于相机重新定位的多输出学习，” 载于IEEE计算机视觉与模式识别会议论文集，页1114–1121，2014年。'
- en: '[168] D. Massiceti, A. Krull, E. Brachmann, C. Rother, and P. H. Torr, “Random
    forests versus neural networks—what’s best for camera localization?,” in 2017
    IEEE International Conference on Robotics and Automation (ICRA), pp. 5118–5125,
    IEEE, 2017.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] D. Massiceti, A. Krull, E. Brachmann, C. Rother, 和 P. H. Torr, “随机森林与神经网络——哪种更适合相机定位？”，
    2017 IEEE国际机器人与自动化大会（ICRA），页5118–5125，IEEE，2017年。'
- en: '[169] X.-S. Gao, X.-R. Hou, J. Tang, and H.-F. Cheng, “Complete solution classification
    for the perspective-three-point problem,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 25, no. 8, pp. 930–943, 2003.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] X.-S. Gao, X.-R. Hou, J. Tang, 和 H.-F. Cheng, “透视三点问题的完整解法分类，” IEEE模式分析与机器智能汇刊，第25卷，第8期，页930–943，2003年。'
- en: '[170] V. Lepetit, F. Moreno-Noguer, and P. Fua, “Epnp: An accurate o (n) solution
    to the pnp problem,” International journal of computer vision, vol. 81, no. 2,
    p. 155, 2009.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] V. Lepetit, F. Moreno-Noguer, 和 P. Fua, “Epnp：解决PNP问题的精确O(n)方案，” 国际计算机视觉杂志，第81卷，第2期，页155，2009年。'
- en: '[171] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm
    for model fitting with applications to image analysis and automated cartography,”
    Communications of the ACM, vol. 24, no. 6, pp. 381–395, 1981.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. A. Fischler 和 R. C. Bolles, “随机样本一致性：一种用于模型拟合的范式及其在图像分析和自动制图中的应用，”
    ACM通讯，第24卷，第6期，页381–395，1981年。'
- en: '[172] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, “From coarse to
    fine: Robust hierarchical localization at large scale,” in Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 12716–12725, 2019.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] P.-E. Sarlin, C. Cadena, R. Siegwart, 和 M. Dymczyk, “从粗略到精细：大规模鲁棒层次定位，”
    载于IEEE计算机视觉与模式识别会议论文集，页12716–12725，2019年。'
- en: '[173] M. Bui, S. Albarqouni, S. Ilic, and N. Navab, “Scene coordinate and correspondence
    learning for image-based localization,” arXiv preprint arXiv:1805.08443, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] M. Bui, S. Albarqouni, S. Ilic, 和 N. Navab, “基于图像的定位的场景坐标和对应学习，” arXiv预印本
    arXiv:1805.08443，2018年。'
- en: '[174] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Large-scale image
    retrieval with attentive deep local features,” in Proceedings of the IEEE international
    conference on computer vision, pp. 3456–3465, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] H. Noh, A. Araujo, J. Sim, T. Weyand, 和 B. Han, “基于注意力的深度局部特征大规模图像检索,”
    见于 IEEE 国际计算机视觉大会论文集, 页码 3456–3465, 2017。'
- en: '[175] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, and A. Torii, “Inloc: Indoor visual localization with dense matching
    and view synthesis,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 7199–7209, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, 和 A. Torii, “Inloc: 基于密集匹配和视图合成的室内视觉定位,” 见于 IEEE 计算机视觉与模式识别会议论文集, 页码 7199–7209,
    2018。'
- en: '[176] J. L. Schönberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic
    visual localization,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 6896–6906, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] J. L. Schönberger, M. Pollefeys, A. Geiger, 和 T. Sattler, “语义视觉定位,” 见于
    IEEE 计算机视觉与模式识别会议论文集, 页码 6896–6906, 2018。'
- en: '[177] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-supervised
    interest point detection and description,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pp. 224–236, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] D. DeTone, T. Malisiewicz, 和 A. Rabinovich, “Superpoint: 自监督兴趣点检测和描述,”
    见于 IEEE 计算机视觉与模式识别会议工作坊论文集, 页码 224–236, 2018。'
- en: '[178] P.-E. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, and C. Cadena, “Leveraging
    deep visual descriptors for hierarchical efficient localization,” arXiv preprint
    arXiv:1809.01019, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] P.-E. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, 和 C. Cadena, “利用深度视觉描述符进行层次化高效定位,”
    arXiv 预印本 arXiv:1809.01019, 2018。'
- en: '[179] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, and J. Sivic,
    “Neighbourhood consensus networks,” in Advances in Neural Information Processing
    Systems, pp. 1651–1662, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, 和 J. Sivic,
    “邻域共识网络,” 见于 神经信息处理系统进展, 页码 1651–1662, 2018。'
- en: '[180] M. Feng, S. Hu, M. H. Ang, and G. H. Lee, “2d3d-matchnet: learning to
    match keypoints across 2d image and 3d point cloud,” in 2019 International Conference
    on Robotics and Automation (ICRA), pp. 4790–4796, IEEE, 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] M. Feng, S. Hu, M. H. Ang, 和 G. H. Lee, “2d3d-matchnet: 学习在 2d 图像和 3d
    点云之间匹配关键点,” 见于 2019 年国际机器人与自动化大会 (ICRA), 页码 4790–4796, IEEE, 2019。'
- en: '[181] P. H. Christiansen, M. F. Kragh, Y. Brodskiy, and H. Karstoft, “Unsuperpoint:
    End-to-end unsupervised interest point detector and descriptor,” arXiv preprint
    arXiv:1907.04011, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] P. H. Christiansen, M. F. Kragh, Y. Brodskiy, 和 H. Karstoft, “Unsuperpoint:
    端到端无监督兴趣点检测器和描述符,” arXiv 预印本 arXiv:1907.04011, 2019。'
- en: '[182] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and
    T. Sattler, “D2-net: A trainable cnn for joint description and detection of local
    features,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 8092–8101, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, 和
    T. Sattler, “D2-net: 一种可训练的 cnn 用于局部特征的联合描述和检测,” 见于 IEEE 计算机视觉与模式识别会议论文集, 页码 8092–8101,
    2019。'
- en: '[183] P. Speciale, J. L. Schonberger, S. B. Kang, S. N. Sinha, and M. Pollefeys,
    “Privacy preserving image-based localization,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 5493–5503, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] P. Speciale, J. L. Schönberger, S. B. Kang, S. N. Sinha, 和 M. Pollefeys,
    “隐私保护的基于图像的定位,” 见于 IEEE 计算机视觉与模式识别会议论文集, 页码 5493–5503, 2019。'
- en: '[184] P. Weinzaepfel, G. Csurka, Y. Cabon, and M. Humenberger, “Visual localization
    by learning objects-of-interest dense match regression,” in Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 5634–5643, 2019.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] P. Weinzaepfel, G. Csurka, Y. Cabon, 和 M. Humenberger, “通过学习感兴趣对象的密集匹配回归进行视觉定位,”
    见于 IEEE 计算机视觉与模式识别会议论文集, 页码 5634–5643, 2019。'
- en: '[185] F. Camposeco, A. Cohen, M. Pollefeys, and T. Sattler, “Hybrid scene compression
    for visual localization,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 7653–7662, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] F. Camposeco, A. Cohen, M. Pollefeys, 和 T. Sattler, “用于视觉定位的混合场景压缩,”
    见于 IEEE 计算机视觉与模式识别会议论文集, 页码 7653–7662, 2019。'
- en: '[186] W. Cheng, W. Lin, K. Chen, and X. Zhang, “Cascaded parallel filtering
    for memory-efficient image-based localization,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 1032–1041, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] W. Cheng, W. Lin, K. Chen, 和 X. Zhang, “用于内存高效图像定位的级联并行过滤,” 见于 IEEE 国际计算机视觉大会论文集,
    页码 1032–1041, 2019。'
- en: '[187] H. Taira, I. Rocco, J. Sedlar, M. Okutomi, J. Sivic, T. Pajdla, T. Sattler,
    and A. Torii, “Is this the right place? geometric-semantic pose verification for
    indoor visual localization,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 4373–4383, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] H. Taira, I. Rocco, J. Sedlar, M. Okutomi, J. Sivic, T. Pajdla, T. Sattler,
    和 A. Torii，“这是正确的位置吗？室内视觉定位的几何-语义姿态验证，” 见于 IEEE 国际计算机视觉大会论文集，页码 4373–4383，2019年。'
- en: '[188] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    and M. Humenberger, “R2d2: Repeatable and reliable detector and descriptor,” arXiv
    preprint arXiv:1906.06195, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    和 M. Humenberger，“R2d2：可重复且可靠的检测器和描述符，” arXiv 预印本 arXiv:1906.06195，2019年。'
- en: '[189] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, and
    L. Quan, “Aslfeat: Learning local features of accurate shape and localization,”
    arXiv preprint arXiv:2003.10071, 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, 和
    L. Quan，“ASLFeat：学习准确形状和定位的局部特征，” arXiv 预印本 arXiv:2003.10071，2020年。'
- en: '[190] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold,
    and C. Rother, “Dsac-differentiable ransac for camera localization,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6684–6692,
    2017.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold,
    和 C. Rother，“DSAC——可微分的 RANSAC 用于相机定位，” 见于 IEEE 计算机视觉与模式识别大会论文集，页码 6684–6692，2017年。'
- en: '[191] E. Brachmann and C. Rother, “Learning less is more-6d camera localization
    via 3d surface regression,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 4654–4662, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] E. Brachmann 和 C. Rother，“学习少即是多——通过 3D 表面回归进行 6D 相机定位，” 见于 IEEE 计算机视觉与模式识别大会论文集，页码
    4654–4662，2018年。'
- en: '[192] X. Li, J. Ylioinas, J. Verbeek, and J. Kannala, “Scene coordinate regression
    with angle-based reprojection loss for camera relocalization,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 0–0, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] X. Li, J. Ylioinas, J. Verbeek, 和 J. Kannala，“带有基于角度的重投影损失的场景坐标回归用于相机重新定位，”
    见于欧洲计算机视觉大会 (ECCV) 论文集，页码 0–0，2018年。'
- en: '[193] X. Li, J. Ylioinas, and J. Kannala, “Full-frame scene coordinate regression
    for image-based localization,” arXiv preprint arXiv:1802.03237, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] X. Li, J. Ylioinas, 和 J. Kannala，“基于图像的定位的全帧场景坐标回归，” arXiv 预印本 arXiv:1802.03237，2018年。'
- en: '[194] E. Brachmann and C. Rother, “Expert sample consensus applied to camera
    re-localization,” in Proceedings of the IEEE International Conference on Computer
    Vision, pp. 7525–7534, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] E. Brachmann 和 C. Rother，“应用专家样本一致性于相机重新定位，” 见于 IEEE 国际计算机视觉大会论文集，页码
    7525–7534，2019年。'
- en: '[195] E. Brachmann and C. Rother, “Neural-guided ransac: Learning where to
    sample model hypotheses,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 4322–4331, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] E. Brachmann 和 C. Rother，“神经引导 RANSAC：学习模型假设的采样位置，” 见于 IEEE 国际计算机视觉大会论文集，页码
    4322–4331，2019年。'
- en: '[196] L. Yang, Z. Bai, C. Tang, H. Li, Y. Furukawa, and P. Tan, “Sanet: Scene
    agnostic network for camera localization,” in Proceedings of the IEEE International
    Conference on Computer Vision (ICCV), pp. 42–51, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] L. Yang, Z. Bai, C. Tang, H. Li, Y. Furukawa, 和 P. Tan，“Sanet：场景无关网络用于相机定位，”
    见于 IEEE 国际计算机视觉大会 (ICCV) 论文集，页码 42–51，2019年。'
- en: '[197] M. Cai, H. Zhan, C. Saroj Weerasekera, K. Li, and I. Reid, “Camera relocalization
    by exploiting multi-view constraints for scene coordinates regression,” in Proceedings
    of the IEEE International Conference on Computer Vision Workshops, pp. 0–0, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] M. Cai, H. Zhan, C. Saroj Weerasekera, K. Li, 和 I. Reid，“通过利用多视图约束进行场景坐标回归的相机重新定位，”
    见于 IEEE 国际计算机视觉研讨会论文集，页码 0–0，2019年。'
- en: '[198] X. Li, S. Wang, Y. Zhao, J. Verbeek, and J. Kannala, “Hierarchical scene
    coordinate classification and regression for visual localization,” in CVPR, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] X. Li, S. Wang, Y. Zhao, J. Verbeek, 和 J. Kannala，“层次化场景坐标分类与回归用于视觉定位，”
    见于 CVPR，2020年。'
- en: '[199] L. Zhou, Z. Luo, T. Shen, J. Zhang, M. Zhen, Y. Yao, T. Fang, and L. Quan,
    “Kfnet: Learning temporal camera relocalization using kalman filtering,” arXiv
    preprint arXiv:2003.10629, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] L. Zhou, Z. Luo, T. Shen, J. Zhang, M. Zhen, Y. Yao, T. Fang, 和 L. Quan，“Kfnet：使用卡尔曼滤波学习时间相机重新定位，”
    arXiv 预印本 arXiv:2003.10629，2020年。'
- en: '[200] K. Mikolajczyk and C. Schmid, “Scale & affine invariant interest point
    detectors,” International journal of computer vision, vol. 60, no. 1, pp. 63–86,
    2004.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] K. Mikolajczyk 和 C. Schmid，“尺度与仿射不变兴趣点检测器，” 国际计算机视觉期刊，第 60 卷，第 1 期，页码
    63–86，2004年。'
- en: '[201] S. Leutenegger, M. Chli, and R. Y. Siegwart, “Brisk: Binary robust invariant
    scalable keypoints,” in 2011 International conference on computer vision, pp. 2548–2555,
    Ieee, 2011.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S. Leutenegger, M. Chli, 和 R. Y. Siegwart, “Brisk: 二进制鲁棒不变可扩展关键点”，发表于2011年国际计算机视觉会议，第2548–2555页，IEEE，2011年。'
- en: '[202] H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up robust features,”
    in European conference on computer vision, pp. 404–417, Springer, 2006.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] H. Bay, T. Tuytelaars, 和 L. Van Gool, “Surf: 加速鲁棒特征”，发表于《欧洲计算机视觉会议》，第404–417页，Springer，2006年。'
- en: '[203] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    International journal of computer vision, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] D. G. Lowe, “来自尺度不变关键点的独特图像特征”，《计算机视觉国际期刊》，第60卷，第2期，第91–110页，2004年。'
- en: '[204] M. Calonder, V. Lepetit, C. Strecha, and P. Fua, “Brief: Binary robust
    independent elementary features,” in European conference on computer vision, pp. 778–792,
    Springer, 2010.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] M. Calonder, V. Lepetit, C. Strecha, 和 P. Fua, “Brief: 二进制鲁棒独立基本特征”，发表于《欧洲计算机视觉会议》，第778–792页，Springer，2010年。'
- en: '[205] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efficient
    alternative to sift or surf,” in 2011 International conference on computer vision,
    pp. 2564–2571, Ieee, 2011.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] E. Rublee, V. Rabaud, K. Konolige, 和 G. Bradski, “Orb: 作为SIFT或SURF的高效替代方案”，发表于2011年国际计算机视觉会议，第2564–2571页，IEEE，2011年。'
- en: '[206] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk, “Learning local feature
    descriptors with triplets and shallow convolutional neural networks.,” in Bmvc,
    vol. 1, p. 3, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] V. Balntas, E. Riba, D. Ponsa, 和 K. Mikolajczyk, “使用三元组和浅层卷积神经网络学习局部特征描述符”，发表于BMVC，第1卷，第3页，2016年。'
- en: '[207] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    Proceedings of the IEEE International Conference on Computer Vision, pp. 118–126,
    2015.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, 和 F. Moreno-Noguer,
    “深度卷积特征点描述符的辨别学习”，发表于《IEEE国际计算机视觉会议》，第118–126页，2015年。'
- en: '[208] K. Simonyan, A. Vedaldi, and A. Zisserman, “Learning local feature descriptors
    using convex optimisation,” IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 36, no. 8, pp. 1573–1585, 2014.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] K. Simonyan, A. Vedaldi, 和 A. Zisserman, “使用凸优化学习局部特征描述符”，《IEEE模式分析与机器智能汇刊》，第36卷，第8期，第1573–1585页，2014年。'
- en: '[209] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua, “Learning
    to find good correspondences,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 2666–2674, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, 和 P. Fua, “学习找到良好的对应关系”，发表于《IEEE计算机视觉与模式识别会议》，第2666–2674页，2018年。'
- en: '[210] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, and E. Trulls, “Beyond cartesian
    representations for local descriptors,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 253–262, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, 和 E. Trulls, “超越局部描述符的笛卡尔表示”，发表于《IEEE国际计算机视觉会议》，第253–262页，2019年。'
- en: '[211] N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys, “Quad-networks:
    unsupervised learning to rank for interest point detection,” in Proceedings of
    the IEEE conference on computer vision and pattern recognition, pp. 1822–1830,
    2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] N. Savinov, A. Seki, L. Ladicky, T. Sattler, 和 M. Pollefeys, “Quad-networks:
    无监督学习的兴趣点检测排序”，发表于《IEEE计算机视觉与模式识别会议》，第1822–1830页，2017年。'
- en: '[212] L. Zhang and S. Rusinkiewicz, “Learning to detect features in texture
    images,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 6325–6333, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] L. Zhang 和 S. Rusinkiewicz, “学习检测纹理图像中的特征”，发表于《IEEE计算机视觉与模式识别会议》，第6325–6333页，2018年。'
- en: '[213] A. B. Laguna, E. Riba, D. Ponsa, and K. Mikolajczyk, “Key. net: Keypoint
    detection by handcrafted and learned cnn filters,” arXiv preprint arXiv:1904.00889,
    2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] A. B. Laguna, E. Riba, D. Ponsa, 和 K. Mikolajczyk, “Key. net: 通过手工制作和学习的CNN滤波器进行关键点检测”，arXiv预印本
    arXiv:1904.00889，2019年。'
- en: '[214] Y. Ono, E. Trulls, P. Fua, and K. M. Yi, “Lf-net: learning local features
    from images,” in Advances in neural information processing systems, pp. 6234–6244,
    2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Y. Ono, E. Trulls, P. Fua, 和 K. M. Yi, “Lf-net: 从图像中学习局部特征”，发表于《神经信息处理系统进展》，第6234–6244页，2018年。'
- en: '[215] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua, “Lift: Learned invariant
    feature transform,” in European Conference on Computer Vision, pp. 467–483, Springer,
    2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] K. M. Yi, E. Trulls, V. Lepetit, 和 P. Fua, “Lift: 学习的不变特征变换”，发表于《欧洲计算机视觉会议》，第467–483页，Springer，2016年。'
- en: '[216] C. G. Harris, M. Stephens, et al., “A combined corner and edge detector.,”
    in Alvey vision conference, vol. 15, pp. 10–5244, Citeseer, 1988.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] C. G. Harris, M. Stephens 等，“一种结合角点和边缘检测器，”在 Alvey 视觉会议，卷 15，页码 10–5244，Citeseer，1988年。'
- en: '[217] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris, and K. Mikolajczyk,
    “D2d: Keypoint extraction with describe to detect approach,” arXiv preprint arXiv:2005.13605,
    2020.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris 和 K. Mikolajczyk，“D2D：使用描述到检测方法的关键点提取，”arXiv
    预印本 arXiv:2005.13605，2020年。'
- en: '[218] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker, “Universal correspondence
    network,” in Advances in Neural Information Processing Systems, pp. 2414–2422,
    2016.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] C. B. Choy, J. Gwak, S. Savarese 和 M. Chandraker，“通用对应网络，”在《神经信息处理系统进展》论文集中，页码
    2414–2422，2016年。'
- en: '[219] M. E. Fathy, Q.-H. Tran, M. Zeeshan Zia, P. Vernaza, and M. Chandraker,
    “Hierarchical metric learning and matching for 2d and 3d geometric correspondences,”
    in Proceedings of the European Conference on Computer Vision (ECCV), pp. 803–819,
    2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] M. E. Fathy, Q.-H. Tran, M. Zeeshan Zia, P. Vernaza 和 M. Chandraker，“用于2D和3D几何对应的分层度量学习与匹配，”在《欧洲计算机视觉会议（ECCV）论文集》中，页码
    803–819，2018年。'
- en: '[220] N. Savinov, L. Ladicky, and M. Pollefeys, “Matching neural paths: transfer
    from recognition to correspondence search,” in Advances in Neural Information
    Processing Systems, pp. 1205–1214, 2017.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] N. Savinov, L. Ladicky 和 M. Pollefeys，“匹配神经路径：从识别到对应搜索的迁移，”在《神经信息处理系统进展》论文集中，页码
    1205–1214，2017年。'
- en: '[221] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., “Benchmarking 6dof outdoor
    visual localization in changing conditions,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 8601–8610, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic 等，“在变化条件下的6DOF户外视觉定位基准测试，”在《IEEE计算机视觉与模式识别会议论文集》中，页码
    8601–8610，2018年。'
- en: '[222] H. Zhou, T. Sattler, and D. W. Jacobs, “Evaluating local features for
    day-night matching,” in European Conference on Computer Vision, pp. 724–736, Springer,
    2016.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] H. Zhou, T. Sattler 和 D. W. Jacobs，“评估日夜匹配的局部特征，”在《欧洲计算机视觉会议》中，页码 724–736，Springer，2016年。'
- en: '[223] Q.-H. Pham, M. A. Uy, B.-S. Hua, D. T. Nguyen, G. Roig, and S.-K. Yeung,
    “Lcd: Learned cross-domain descriptors for 2d-3d matching,” arXiv preprint arXiv:1911.09326,
    2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Q.-H. Pham, M. A. Uy, B.-S. Hua, D. T. Nguyen, G. Roig 和 S.-K. Yeung，“LCD：用于2D-3D匹配的学习跨域描述符，”arXiv
    预印本 arXiv:1911.09326，2019年。'
- en: '[224] W. Lu, Y. Zhou, G. Wan, S. Hou, and S. Song, “L3-net: Towards learning
    based lidar localization for autonomous driving,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 6389–6398, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] W. Lu, Y. Zhou, G. Wan, S. Hou 和 S. Song，“L3-net：面向基于学习的激光雷达定位用于自动驾驶，”在《IEEE计算机视觉与模式识别会议（CVPR）》论文集，页码
    6389–6398，2019年。'
- en: '[225] H. Yin, L. Tang, X. Ding, Y. Wang, and R. Xiong, “Locnet: Global localization
    in 3d point clouds for mobile vehicles,” in 2018 IEEE Intelligent Vehicles Symposium
    (IV), pp. 728–733, IEEE, 2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] H. Yin, L. Tang, X. Ding, Y. Wang 和 R. Xiong，“LOCNET：用于移动车辆的3D点云全局定位，”在2018年
    IEEE 智能车辆研讨会（IV）中，页码 728–733，IEEE，2018年。'
- en: '[226] M. Angelina Uy and G. Hee Lee, “Pointnetvlad: Deep point cloud based
    retrieval for large-scale place recognition,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4470–4479, 2018.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] M. Angelina Uy 和 G. Hee Lee，“PointNetVLAD：基于深度点云的大规模场所识别检索，”在《IEEE计算机视觉与模式识别会议论文集》中，页码
    4470–4479，2018年。'
- en: '[227] I. A. Barsan, S. Wang, A. Pokrovsky, and R. Urtasun, “Learning to localize
    using a lidar intensity map.,” in CoRL, pp. 605–616, 2018.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] I. A. Barsan, S. Wang, A. Pokrovsky 和 R. Urtasun，“使用激光雷达强度图学习定位，”在 CoRL
    中，页码 605–616，2018年。'
- en: '[228] W. Zhang and C. Xiao, “Pcan: 3d attention map learning using contextual
    information for point cloud based retrieval,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 12436–12445, 2019.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] W. Zhang 和 C. Xiao，“PCAN：使用上下文信息进行点云检索的3D注意力图学习，”在《IEEE计算机视觉与模式识别会议论文集》中，页码
    12436–12445，2019年。'
- en: '[229] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan, and S. Song, “Deepicp: An end-to-end
    deep neural network for 3d point cloud registration,” arXiv preprint arXiv:1905.04153,
    2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan 和 S. Song，“DeepICP：用于3D点云配准的端到端深度神经网络，”arXiv
    预印本 arXiv:1905.04153，2019年。'
- en: '[230] Y. Wang and J. M. Solomon, “Deep closest point: Learning representations
    for point cloud registration,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 3523–3532, 2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Y. Wang 和 J. M. Solomon, “深度最近点：用于点云配准的学习表示，” 发表在IEEE计算机视觉国际会议论文集，第3523–3532页,
    2019。'
- en: '[231] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3feat: Joint
    learning of dense detection and description of 3d local features,” arXiv preprint
    arXiv:2003.03164, 2020.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, 和 C.-L. Tai, “D3feat: 3D局部特征的密集检测和描述的联合学习，”
    arXiv 预印本 arXiv:2003.03164, 2020。'
- en: '[232] E. Brachmann and C. Rother, “Visual camera re-localization from rgb and
    rgb-d images using dsac,” arXiv preprint arXiv:2002.12324, 2020.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] E. Brachmann 和 C. Rother, “使用DSAC从RGB和RGB-D图像进行视觉相机重定位，” arXiv 预印本 arXiv:2002.12324,
    2020。'
- en: '[233] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle
    adjustment—a modern synthesis,” in International workshop on vision algorithms,
    pp. 298–372, Springer, 1999.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] B. Triggs, P. F. McLauchlan, R. I. Hartley, 和 A. W. Fitzgibbon, “束调整—现代综合，”
    发表在视觉算法国际研讨会论文集，第298–372页, Springer, 1999。'
- en: '[234] J. Nocedal and S. Wright, Numerical optimization. Springer Science &
    Business Media, 2006.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] J. Nocedal 和 S. Wright, 《数值优化》。Springer Science & Business Media, 2006。'
- en: '[235] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, and A. J. Davison,
    “Learning to solve nonlinear least squares for monocular stereo,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 284–299, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, 和 A. J. Davison,
    “学习解决单目立体的非线性最小二乘问题，” 发表在欧洲计算机视觉会议（ECCV）论文集，第284–299页, 2018。'
- en: '[236] C. Tang and P. Tan, “Ba-net: Dense bundle adjustment network,” International
    Conference on Learning Representations (ICLR), 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] C. Tang 和 P. Tan, “Ba-net: 密集束调整网络，” 国际学习表示会议（ICLR），2019。'
- en: '[237] H. Zhou, B. Ummenhofer, and T. Brox, “Deeptam: Deep tracking and mapping
    with convolutional neural networks,” International Journal of Computer Vision,
    vol. 128, no. 3, pp. 756–769, 2020.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] H. Zhou, B. Ummenhofer, 和 T. Brox, “Deeptam: 使用卷积神经网络进行深度跟踪和地图构建，” 《计算机视觉国际杂志》，第128卷，第3期，第756–769页,
    2020。'
- en: '[238] J. Czarnowski, T. Laidlow, R. Clark, and A. J. Davison, “Deepfactors:
    Real-time probabilistic dense monocular slam,” IEEE Robotics and Automation Letters,
    2020.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] J. Czarnowski, T. Laidlow, R. Clark, 和 A. J. Davison, “Deepfactors: 实时概率密集单目SLAM，”
    IEEE机器人与自动化通讯，2020。'
- en: '[239] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    and M. Milford, “Place recognition with convnet landmarks: Viewpoint-robust, condition-robust,
    training-free,” Proceedings of Robotics: Science and Systems XII, 2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    和 M. Milford, “使用卷积网络地标的场所识别：视角鲁棒、条件鲁棒、无需训练，” 发表在《机器人学：科学与系统XII》论文集，2015。'
- en: '[240] X. Gao and T. Zhang, “Unsupervised learning to detect loops using deep
    neural networks for visual slam system,” Autonomous robots, vol. 41, no. 1, pp. 1–18,
    2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] X. Gao 和 T. Zhang, “使用深度神经网络进行视觉SLAM系统的无监督回环检测，” 《自主机器人》，第41卷，第1期，第1–18页,
    2017。'
- en: '[241] N. Merrill and G. Huang, “Lightweight unsupervised deep loop closure,”
    Robotics: Science and Systems, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] N. Merrill 和 G. Huang, “轻量级无监督深度回环闭合，” 《机器人学：科学与系统》，2018。'
- en: '[242] A. R. Memon, H. Wang, and A. Hussain, “Loop closure detection using supervised
    and unsupervised deep neural networks for monocular slam systems,” Robotics and
    Autonomous Systems, vol. 126, p. 103470, 2020.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] A. R. Memon, H. Wang, 和 A. Hussain, “使用监督和无监督深度神经网络进行回环闭合检测以应用于单目SLAM系统，”
    《机器人与自主系统》，第126卷，第103470页, 2020。'
- en: '[243] S. Wang, R. Clark, H. Wen, and N. Trigoni, “End-to-end, sequence-to-sequence
    probabilistic visual odometry through deep neural networks,” The International
    Journal of Robotics Research, vol. 37, no. 4-5, pp. 513–542, 2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] S. Wang, R. Clark, H. Wen, 和 N. Trigoni, “端到端的序列到序列概率视觉里程计通过深度神经网络，”
    《国际机器人研究杂志》，第37卷，第4-5期，第513–542页, 2018。'
- en: '[244] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing
    model uncertainty in deep learning,” in international conference on machine learning,
    pp. 1050–1059, 2016.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Y. Gal 和 Z. Ghahramani, “Dropout作为贝叶斯近似：在深度学习中表示模型不确定性，” 发表在国际机器学习会议，
    第1050–1059页, 2016。'
- en: '[245] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?,” in Advances in neural information processing systems,
    pp. 5574–5584, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] A. Kendall 和 Y. Gal, “在计算机视觉中，贝叶斯深度学习需要什么不确定性？”， 发表在神经信息处理系统进展， 第5574–5584页,
    2017。'
- en: '[246] C. Chen, X. Lu, J. Wahlstrom, A. Markham, and N. Trigoni, “Deep neural
    network based inertial odometry using low-cost inertial measurement units,” IEEE
    Transactions on Mobile Computing, 2019.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] A. Kendall, V. Badrinarayanan, and R. Cipolla, “Bayesian segnet: Model
    uncertainty in deep convolutional encoder-decoder architectures for scene understanding,”
    in British Machine Vision Conference 2017, BMVC 2017, 2017.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] R. McAllister, Y. Gal, A. Kendall, M. Van Der Wilk, A. Shah, R. Cipolla,
    and A. Weller, “Concrete problems for autonomous vehicle safety: advantages of
    bayesian deep learning,” in Proceedings of the 26th International Joint Conference
    on Artificial Intelligence, pp. 4745–4753, AAAI Press, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] M. Klodt and A. Vedaldi, “Supervising the new with the old: learning
    sfm from sfm,” in Proceedings of the European Conference on Computer Vision (ECCV),
    pp. 698–713, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] H. Rebecq, T. Horstschäfer, G. Gallego, and D. Scaramuzza, “Evo: A geometric
    approach to event-based 6-dof parallel tracking and mapping in real time,” IEEE
    Robotics and Automation Letters, vol. 2, no. 2, pp. 593–600, 2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] M. R. U. Saputra, P. P. de Gusmao, C. X. Lu, Y. Almalioglu, S. Rosa,
    C. Chen, J. Wahlström, W. Wang, A. Markham, and N. Trigoni, “Deeptio: A deep thermal-inertial
    odometry with visual hallucination,” IEEE Robotics and Automation Letters, vol. 5,
    no. 2, pp. 1672–1679, 2020.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] C. X. Lu, S. Rosa, P. Zhao, B. Wang, C. Chen, J. A. Stankovic, N. Trigoni,
    and A. Markham, “See through smoke: Robust indoor mapping with low-cost mmwave
    radar,” in Proceedings of the 18th International Conference on Mobile Systems,
    Applications, and Services, MobiSys ’20, (New York, NY, USA), p. 14–27, Association
    for Computing Machinery, 2020.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] B. Ferris, D. Fox, and N. D. Lawrence, “Wifi-slam using gaussian process
    latent variable models.,” in IJCAI, vol. 7, pp. 2480–2485, 2007.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] C. X. Lu, Y. Li, P. Zhao, C. Chen, L. Xie, H. Wen, R. Tan, and N. Trigoni,
    “Simultaneous localization and mapping with power network electromagnetic field,”
    in Proceedings of the 24th annual international conference on mobile computing
    and networking (MobiCom), pp. 607–622, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Q.-s. Zhang and S.-C. Zhu, “Visual interpretability for deep learning:
    a survey,” Frontiers of Information Technology & Electronic Engineering, vol. 19,
    no. 1, pp. 27–39, 2018.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
