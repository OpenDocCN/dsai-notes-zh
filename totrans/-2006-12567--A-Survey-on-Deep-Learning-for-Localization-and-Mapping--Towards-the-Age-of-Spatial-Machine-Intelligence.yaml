- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:00:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2006.12567] A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2006.12567] 深度学习在定位与地图构建中的调查：迈向空间机器智能时代'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2006.12567](https://ar5iv.labs.arxiv.org/html/2006.12567)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2006.12567](https://ar5iv.labs.arxiv.org/html/2006.12567)
- en: 'A Survey on Deep Learning for Localization and Mapping:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在定位与地图构建中的调查：
- en: Towards the Age of Spatial Machine Intelligence
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 迈向空间机器智能时代
- en: Changhao Chen, Bing Wang, Chris Xiaoxuan Lu, Niki Trigoni and Andrew Markham
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 陈昌浩，王兵，陆晓轩，尼基·特里戈尼，安德鲁·马克汉
- en: 'Department of Computer Science, University of Oxford Corresponding author:
    Changhao Chen (changhao.chen@cs.ox.ac.uk) A project website that updates additional
    material and extended lists of references, can be found at https://github.com/changhao-chen/deep-learning-localization-mapping.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津大学计算机科学系  通讯作者：陈昌浩 (changhao.chen@cs.ox.ac.uk)  项目网站更新了额外的资料和扩展的参考文献列表，网址为
    https://github.com/changhao-chen/deep-learning-localization-mapping。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning based localization and mapping has recently attracted significant
    attention. Instead of creating hand-designed algorithms through exploitation of
    physical models or geometric theories, deep learning based solutions provide an
    alternative to solve the problem in a data-driven way. Benefiting from ever-increasing
    volumes of data and computational power, these methods are fast evolving into
    a new area that offers accurate and robust systems to track motion and estimate
    scenes and their structure for real-world applications. In this work, we provide
    a comprehensive survey, and propose a new taxonomy for localization and mapping
    using deep learning. We also discuss the limitations of current models, and indicate
    possible future directions. A wide range of topics are covered, from learning
    odometry estimation, mapping, to global localization and simultaneous localization
    and mapping (SLAM). We revisit the problem of perceiving self-motion and scene
    understanding with on-board sensors, and show how to solve it by integrating these
    modules into a prospective spatial machine intelligence system (SMIS). It is our
    hope that this work can connect emerging works from robotics, computer vision
    and machine learning communities, and serve as a guide for future researchers
    to apply deep learning to tackle localization and mapping problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的定位和地图构建最近引起了广泛关注。与通过利用物理模型或几何理论创建手工设计的算法不同，深度学习方法提供了一种数据驱动的解决方案。得益于不断增长的数据量和计算能力，这些方法正在迅速发展成为一个新领域，提供准确且稳健的系统来跟踪运动并估计场景及其结构以应用于实际世界。在这项工作中，我们提供了一项全面的调查，并提出了基于深度学习的定位和地图构建的新分类法。我们还讨论了当前模型的局限性，并指出了可能的未来方向。覆盖了从学习里程估计、地图构建到全球定位和同时定位与地图构建（SLAM）等广泛主题。我们重新审视了通过车载传感器感知自我运动和场景理解的问题，并展示了如何通过将这些模块整合到一个前景空间机器智能系统（SMIS）中来解决这一问题。我们希望这项工作能连接机器人学、计算机视觉和机器学习领域的新兴成果，并为未来的研究人员提供指导，以应用深度学习解决定位和地图构建问题。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Learning, Localization, Mapping, SLAM, Perception, Correspondence Matching,
    Uncertainty Estimation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、定位、地图构建、SLAM、感知、匹配、 不确定性估计
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Localization and mapping is a fundamental need for human and mobile agents.
    As a motivating example, humans are able to perceive their self-motion and environment
    via multimodal sensory perception, and rely on this awareness to locate and navigate
    themselves in a complex three-dimensional space [[1](#bib.bib1)]. This ability
    is part of human spatial ability. Furthermore, the ability to perceive self-motion
    and their surroundings plays a vital role in developing cognition, and motor control
    [[2](#bib.bib2)]. In a similar vein, artificial agents or robots should also be
    able to perceive the environment and estimate their system states using on-board
    sensors. These agents could be any form of robot, e.g. self-driving vehicles,
    delivery drones or home service robots, sensing their surroundings and autonomously
    making decisions [[3](#bib.bib3)]. Equivalently, as emerging Augmented Reality
    (AR) and Virtual Reality (VR) technologies interweave cyber space and physical
    environments, the ability of machines to be perceptually aware underpins seamless
    human-machine interaction. Further applications also include mobile and wearable
    devices, such as smartphones, wristbands or Internet-of-Things (IoT) devices,
    providing users with a wide range of location-based-services, ranging from pedestrian
    navigation [[4](#bib.bib4)], to sports/activity monitoring [[5](#bib.bib5)], to
    animal tracking [[6](#bib.bib6)], or emergency response [[7](#bib.bib7)] for first-responders.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本地化和映射是人类和移动代理的基本需求。作为一个激励性的例子，人类能够通过多模态感官感知来感知自身运动和环境，并依靠这种意识在复杂的三维空间中定位和导航[[1](#bib.bib1)]。这种能力是人类空间能力的一部分。此外，感知自我运动和周围环境的能力在认知和运动控制的发展中扮演着至关重要的角色[[2](#bib.bib2)]。类似地，人工代理或机器人也应能通过车载传感器感知环境并估计其系统状态。这些代理可以是任何形式的机器人，例如自动驾驶车辆、配送无人机或家庭服务机器人，它们通过感知周围环境自主做出决策[[3](#bib.bib3)]。同样，随着新兴的增强现实（AR）和虚拟现实（VR）技术将网络空间和物理环境交织在一起，机器的感知意识能力支撑了无缝的人机交互。进一步的应用还包括移动和可穿戴设备，如智能手机、手环或物联网（IoT）设备，为用户提供广泛的基于位置的服务，从步行导航[[4](#bib.bib4)]到运动/活动监测[[5](#bib.bib5)]，再到动物追踪[[6](#bib.bib6)]，以及急救响应[[7](#bib.bib7)]。
- en: '![Refer to caption](img/ca115a799a28826386962fd622580ad1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ca115a799a28826386962fd622580ad1.png)'
- en: 'Figure 1: A spatial machine intelligence system exploits on-board sensors to
    perceive self-motion, global pose, scene geometry and semantics. (a) Conventional
    model based solutions build hand-designed algorithms to convert input sensor data
    to target values. (c) Data-driven solutions exploit learning models to construct
    this mapping function. (b) Hybrid approaches combine both hand-crafted algorithms
    and learning models. This survey discusses (b) and (c).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：空间机器智能系统利用车载传感器感知自我运动、全球姿态、场景几何和语义。(a) 传统模型基础解决方案构建手工设计的算法将输入传感器数据转换为目标值。(c)
    数据驱动解决方案利用学习模型来构建这种映射函数。(b) 混合方法结合了手工制作的算法和学习模型。本调查讨论了(b) 和(c)。
- en: '![Refer to caption](img/2c084ace53d7ada415dd4a801044aaba.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2c084ace53d7ada415dd4a801044aaba.png)'
- en: 'Figure 2: A taxonomy of existing works on deep learning for localization and
    mapping.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：关于深度学习在本地化和映射中的现有工作分类。
- en: Enabling a high level of autonomy for these and other digital agents requires
    precise and robust localization, and incrementally building and maintaining a
    world model, with the capability to continuously process new information and adapt
    to various scenarios. Such a quest is termed as ‘Spatial Machine Intelligence
    System (SMIS)’ in our work or recently as Spatial AI in [[8](#bib.bib8)]. In this
    work, broadly, localization refers to the ability to obtain the internal system
    states of robot motion, including locations, orientations and velocities, whilst
    mapping indicates the capacity to perceive external environmental states and capture
    the surroundings, including the geometry, appearance and semantics of a 2D or
    3D scene. These components can act individually to sense the internal or external
    states respectively, or jointly as in simultaneous localization and mapping (SLAM)
    to track pose and build a consistent environmental model in a global frame.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些以及其他数字代理的高水平自主性需要精确和稳健的本地化，并逐步构建和维护一个世界模型，具备持续处理新信息并适应各种场景的能力。这种追求在我们的工作中被称为“空间机器智能系统（SMIS）”，或者最近在[[8](#bib.bib8)]中称为空间人工智能。广义上说，本地化是指获取机器人运动的内部系统状态的能力，包括位置、方向和速度，而映射则是指感知外部环境状态并捕捉周围环境的能力，包括二维或三维场景的几何形状、外观和语义。这些组件可以单独作用于感知内部或外部状态，或像在同时定位与地图构建（SLAM）中一样联合作用，以在全局框架中跟踪姿态并构建一致的环境模型。
- en: 1.1 Why to Study Deep Learning for Localization and Mapping
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 为什么要研究用于本地化和映射的深度学习
- en: The problems of localization and mapping have been studied for decades, with
    a variety of intricate hand-designed models and algorithms being developed, for
    example, odometry estimation (including visual odometry [[9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)], visual-inertial odometry [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15)] and LIDAR odometry [[16](#bib.bib16)]), image-based
    localization[[17](#bib.bib17), [18](#bib.bib18)], place recognition[[19](#bib.bib19)],
    SLAM[[20](#bib.bib20), [10](#bib.bib10), [21](#bib.bib21)], and structure from
    motion (SfM)[[22](#bib.bib22), [23](#bib.bib23)]. Under ideal conditions, these
    sensors and models are capable of accurately estimating system states without
    time bound and across different environments. However, in reality, imperfect sensor
    measurements, inaccurate system modelling, complex environmental dynamics and
    unrealistic constraints impact both the accuracy and reliability of hand-crated
    systems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本地化和映射的问题已经研究了几十年，开发了各种复杂的手工设计模型和算法，例如，里程计估计（包括视觉里程计[[9](#bib.bib9)、[10](#bib.bib10)、[11](#bib.bib11)]、视觉惯性里程计[[12](#bib.bib12)、[13](#bib.bib13)、[14](#bib.bib14)、[15](#bib.bib15)]和激光雷达里程计[[16](#bib.bib16)]）、基于图像的定位[[17](#bib.bib17)、[18](#bib.bib18)]、地点识别[[19](#bib.bib19)]、SLAM[[20](#bib.bib20)、[10](#bib.bib10)、[21](#bib.bib21)]和从运动中恢复结构（SfM）[[22](#bib.bib22)、[23](#bib.bib23)]。在理想条件下，这些传感器和模型能够在不同环境下准确估计系统状态，无时间限制。然而，实际上，不完美的传感器测量、不准确的系统建模、复杂的环境动态和不切实际的约束会影响手工设计系统的准确性和可靠性。
- en: 'The limitations of model based solutions, together with recent advances in
    machine learning, especially deep learning, have motivated researchers to consider
    data-driven (learning) methods as an alternative to solve problem. Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") summarizes the relation between
    input sensor data (e.g. visual, inertial, LIDAR data or other sensors) and output
    target values (e.g. location, orientation, scene geometry or semantics) as a mapping
    function. Conventional model-based solutions are achieved by hand-designing algorithms
    and calibrating to a particular application domain, while the learning based approaches
    construct this mapping function by learned knowledge. The advantages of learning
    based methods are three-fold:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '模型基解决方案的局限性，加上最近在机器学习，特别是深度学习方面的进展，促使研究人员考虑数据驱动（学习）方法作为解决问题的替代方案。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence")总结了输入传感器数据（例如视觉、惯性、激光雷达数据或其他传感器）与输出目标值（例如位置、方向、场景几何或语义）之间的关系，作为一个映射函数。传统的模型基解决方案是通过手工设计算法并校准到特定应用领域来实现的，而基于学习的方法通过学习知识来构建这个映射函数。基于学习的方法有三方面的优势：'
- en: First of all, learning methods can leverage highly expressive deep neural network
    as an universal approximator, and automatically discover features relevant to
    task. This property enables learned models to be resilience to circumstances,
    such as featureless areas, dynamic lightning conditions, motion blur, accurate
    camera calibration, which are challenging to model by hand [[3](#bib.bib3)]. As
    a representative example, visual odometry has achieved notable improvements in
    terms of robustness by incorporating data-driven methods in its design [[24](#bib.bib24),
    [25](#bib.bib25)], outperforming the state-of-the-art conventional algorithms.
    Moreover, learning approaches are able to connect abstract elements with human
    understandable terms[[26](#bib.bib26), [27](#bib.bib27)], such as semantics labelling
    in SLAM, which is hard to describe in a formal mathematical way.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，学习方法可以利用高度表达的深度神经网络作为通用近似器，并自动发现与任务相关的特征。这种属性使得学习到的模型能够对环境具有韧性，如无特征区域、动态光照条件、运动模糊、精确相机校准，这些都是人工建模具有挑战性的[[3](#bib.bib3)]。作为一个代表性例子，视觉里程计通过在其设计中融入数据驱动的方法，在稳健性方面取得了显著的改进[[24](#bib.bib24),
    [25](#bib.bib25)]，超越了最先进的传统算法。此外，学习方法能够将抽象元素与人类可理解的术语联系起来[[26](#bib.bib26), [27](#bib.bib27)]，如SLAM中的语义标注，这在正式的数学方式中难以描述。
- en: '![Refer to caption](img/5b986e226e2bbf8d25773f2a7732fab6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b986e226e2bbf8d25773f2a7732fab6.png)'
- en: 'Figure 3: High-level conceptual illustration of a spatial machine intelligence
    system (i.e. deep learning based localization and mapping). Rounded rectangles
    represent a function module, while arrow lines connect these modules for data
    input and output. It is not necessary to include all modules to perform this system.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：空间机器智能系统（即基于深度学习的定位和地图绘制）的高级概念图。圆角矩形代表功能模块，而箭头线连接这些模块以进行数据输入和输出。执行此系统并不一定需要包含所有模块。
- en: Secondly, learning methods allow spatial machine intelligence systems to learn
    from past experience, and actively exploit new information. By building a generic
    data-driven model, it avoids human effort on specifying the full knowledge about
    mathematical and physical rules[[28](#bib.bib28)], to solve domain specific problem,
    before being deployed. This ability potentially enables learning machines to automatically
    discover new computational solutions, further develop themselves and improve their
    models, within new scenarios or confronting new circumstances. A good example
    is that by using novel view synthesis as a self-supervision signal, self-motion
    and depth can be recovered from unlabelled videos[[29](#bib.bib29), [30](#bib.bib30)].
    In addition, the learned representations can further support high-level tasks,
    such as path planning[[31](#bib.bib31)], and decision making[[32](#bib.bib32)],
    by constructing task-driven maps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，学习方法使空间机器智能系统能够从过去的经验中学习，并主动利用新信息。通过构建一个通用的数据驱动模型，它避免了在部署之前，人工指定数学和物理规则的全部知识[[28](#bib.bib28)]，以解决特定领域的问题。这种能力有可能使学习机器自动发现新的计算解决方案，进一步发展自身并改进其模型，以适应新的场景或面对新的情况。一个很好的例子是，通过使用新颖的视图合成作为自我监督信号，可以从未标记的视频中恢复自我运动和深度[[29](#bib.bib29),
    [30](#bib.bib30)]。此外，学习到的表示可以进一步支持高级任务，如路径规划[[31](#bib.bib31)]和决策制定[[32](#bib.bib32)]，通过构建任务驱动的地图。
- en: The third benefit is its capability of fully exploiting the increasing amount
    of sensor data and computational power. Deep learning or deep neural network has
    the capacity to scale to large-scale problems. The huge amount of parameters inside
    a DNN framework are automatically optimized by minimizing a loss function, by
    training on large datasets through backpropagation and gradient-descent algorithms.
    For example, the recent released GPT-3[[33](#bib.bib33)], the largest pretrained
    language model, with incredibly over 175 Billion parameters, achieves the state-of-the-art
    results on a variety of natural language processing (NLP) tasks, even without
    fine-tuning. In addition, a variety of large-scale datasets relevant to localization
    and mapping have been released, for example, in the autonomous vehicles scenarios,
    [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)] are with a collection of
    rich combinations of sensor data, and motion and semantic labels. This gives us
    an imagination that it would be possible to exploit the power of data and computation
    in solving localization and mapping.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个好处是它能够充分利用不断增加的传感器数据和计算能力。深度学习或深度神经网络具有扩展到大规模问题的能力。DNN框架内的大量参数通过最小化损失函数来自动优化，通过在大数据集上进行反向传播和梯度下降算法进行训练。例如，最近发布的GPT-3[[33](#bib.bib33)]，这是最大的预训练语言模型，具有超过1750亿个参数，在各种自然语言处理（NLP）任务中实现了最先进的结果，即使没有微调。此外，已经发布了各种与定位和制图相关的大规模数据集，例如，在自动驾驶汽车场景中，[[34](#bib.bib34)、[35](#bib.bib35)、[36](#bib.bib36)]包含了丰富的传感器数据组合，以及运动和语义标签。这让我们设想可能利用数据和计算的力量来解决定位和制图问题。
- en: However, it must also be pointed out that these learning techniques are reliant
    on massive datasets to extract statistically meaningful patterns and can struggle
    to generalize to out-of-set environments. There is lack of model interpretability.
    Additionally, although highly parallelizable, they are also typically more computationally
    costly than simpler models. Details of limitations are discussed in Section 7.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也必须指出，这些学习技术依赖于大量数据集来提取统计上有意义的模式，并且可能难以推广到数据集之外的环境中。模型的可解释性不足。此外，尽管高度可并行化，它们通常也比简单模型更具计算成本。有关限制的详细信息在第7节讨论。
- en: 1.2 Comparison with Other Surveys
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 与其他调查的比较
- en: There are several survey papers that have extensively discussed model-based
    localization and mapping approaches. The development of SLAM problem in early
    decades has been well summarized in [[37](#bib.bib37), [38](#bib.bib38)]. The
    seminal survey [[39](#bib.bib39)] provides a thorough discussion on existing SLAM
    work, reviews the history of development and charts several future directions.
    Although this paper contains a section which briefly discusses deep learning models,
    it does not overview this field comprehensively, especially due to the explosion
    of research in this area of the past five years. Other SLAM survey papers only
    focus on individual flavours of SLAM systems, including the probabilistic formulation
    of SLAM [[40](#bib.bib40)], visual odometry [[41](#bib.bib41)], pose-graph SLAM
    [[42](#bib.bib42)], and SLAM in dynamic environments [[43](#bib.bib43)]. We refer
    readers to these surveys for a better understanding of the conventional model
    based solutions. On the other hand, [[3](#bib.bib3)] has a discussion on the applications
    of deep learning to robotics research; however, its main focus is not on localization
    and mapping specifically, but a more general perspective towards the potentials
    and limits of deep learning in a broad context of robotics, including policy learning,
    reasoning and planning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有几篇综述论文广泛讨论了基于模型的定位和制图方法。SLAM问题的早期发展已在[[37](#bib.bib37)、[38](#bib.bib38)]中得到了很好的总结。开创性的综述[[39](#bib.bib39)]对现有的SLAM工作进行了彻底讨论，回顾了发展历程并规划了若干未来方向。尽管本文包含了一个简要讨论深度学习模型的章节，但并未全面概述该领域，特别是由于过去五年研究的爆炸性增长。其他SLAM综述论文仅关注SLAM系统的个别类型，包括SLAM的概率公式[[40](#bib.bib40)]、视觉里程计[[41](#bib.bib41)]、姿态图SLAM[[42](#bib.bib42)]和动态环境中的SLAM[[43](#bib.bib43)]。我们建议读者参考这些综述，以更好地理解传统的基于模型的解决方案。另一方面，[[3](#bib.bib3)]讨论了深度学习在机器人研究中的应用；然而，其主要关注点并非定位和制图，而是对深度学习在广泛机器人背景下的潜力和限制的更一般性看法，包括策略学习、推理和规划。
- en: Notably, although the problem of localization and mapping falls into the key
    notion of robotics, the incorporation of learning methods progresses in tandem
    with other research areas such as machine learning, computer vision and even natural
    language processing. This cross-disciplinary area thus imposes non-trivial difficulty
    when comprehensively summarizing related works into a survey paper. To the best
    of our knowledge, this is the first survey article that thoroughly and extensively
    covers existing work on deep learning for localization and mapping.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Survey Organization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The remainder of the paper is organized as follows: Section 2 offers an overview
    and presents a taxonomy of existing deep learning based localization and mapping;
    Sections 3, 4, 5, 6 discuss the existing deep learning works on relative motion
    (odometry) estimation, mapping methods for geometric, semantic and general, global
    localization, and simultaneous localization and mapping with a focus on SLAM back-ends
    respectively; Open questions are summarized in Section 7 to discuss the limitations
    and future prospects of existing work; and finally Section 8 concludes the paper.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 2 Taxonomy of Existing Approaches
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We provide a new taxonomy of existing deep learning approaches, relevant to
    localization and mapping, to connect the fields of robotics, computer vision and
    machine learning. Broadly, they can be categorized into odometry estimation, mapping,
    global localization and SLAM, as illustrated by the taxonomy shown in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence"):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 1) Odometry estimation concerns the calculation of the relative change in pose,
    in terms of translation and rotation, between two or more frames of sensor data.
    It continuously tracks self-motion, and is followed by a process to integrate
    these pose changes with respect to an initial state to derive global pose, in
    terms of position and orientation. This is widely known as the so-called dead
    reckoning solution. Odometry estimation can be used in providing pose information
    and as odometry motion model to assist the feedback loop of robot control. The
    key problem is to accurately estimate motion transformations from various sensor
    measurements. To this end, deep learning is applied to model the motion dynamics
    in an end-to-end fashion or extract useful features to support a pre-built system
    in a hybrid way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 2) Mapping builds and reconstructs a consistent model to describe the surrounding
    environment. Mapping can be used to provide environment information for human
    operators and high-level robot tasks, constrain the error drifts of odometry estimation,
    and retrieve the inquiry observation for global localization [[39](#bib.bib39)].
    Deep learning is leveraged as an useful tool to discover scene geometry and semantics
    from high-dimensional raw data for mapping. Deep learning based mapping methods
    are sub-divided into geometric, semantic, and general mapping, depending on whether
    the neural network learns the explicit geometry or semantics of a scene, or encodes
    the scene into an implicit neural representation respectively.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 3) Global localization retrieves the global pose of mobile agents in a known
    scene with prior knowledge. This is achieved by matching the inquiry input data
    with a pre-built 2D or 3D map, other spatial references, or a scene that has been
    visited before. It can be leveraged to reduce the pose drift of a dead reckoning
    system or solve the ’kidnapped robot’ problem[[40](#bib.bib40)]. Deep learning
    is used to tackle the tricky data association problem that is complicated by the
    changes in views, illumination, weather and scene dynamics, between the inquiry
    data and map.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '4) Simultaneous Localisation and Mapping (SLAM) integrates the aforementioned
    odometry estimation, global localization and mapping processes as front-ends,
    and jointly optimizes these modules to boost performance in both localization
    and mapping. Except these abovementioned modules, several other SLAM modules perform
    to ensure the consistency of the entire system as follows: *local optimization*
    ensures the local consistency of camera motion and scene geometry; *global optimization*
    aims to constrain the drift of global trajectories, and in a global scale; *keyframe
    detection* is used in keyframe-based SLAM to enable more efficient inference,
    while system error drifts can be mitigated by global optimization, once a loop
    closure is detected by *loop-closure detection*; *uncertainty estimation* provides
    a metric of belief in the learned poses and mapping, critical to probabilistic
    sensor fusion and back-end optimization in SLAM systems.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21268a8de90a7b6d092f3fe1c6425fda.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The typical structure of supervised learning of visual odometry,
    i.e. DeepVO [[24](#bib.bib24)] and unsupervised learning of visual odometry, i.e.
    SfmLearner [[29](#bib.bib29)].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the different design goals of individual components, the above components
    can be integrated into a spatial machine intelligence system (SMIS) to solve real-world
    challenges, allowing for robust operation, and long-term autonomy in the wild.
    A conceptual figure of such an integrated deep-learning based localization and
    mapping system is indicated in Figure [3](#S1.F3 "Figure 3 ‣ 1.1 Why to Study
    Deep Learning for Localization and Mapping ‣ 1 Introduction ‣ A Survey on Deep
    Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence"),
    showing the relationship of these components. In the following sections, we will
    discuss these components in details.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 3 Odometry Estimation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with odometry estimation, which continuously tracks camera egomotion
    and produces relative poses. Global trajectories are reconstructed by integrating
    these relative poses, given an initial state, and thus it is critical to keep
    motion transformation estimates accurate enough to ensure high-prevision localization
    in a global scale. This section discusses deep learning approaches to achieve
    odometry estimation from various sensor data, that are fundamentally different
    in their data properties and application scenarios. The discussion mainly focuses
    on odometry estimation from visual, inertial and point-cloud data, as they are
    the common choices of sensing modalities on mobile agents.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Visual Odometry
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visual odometry (VO) estimates the ego-motion of a camera, and integrates the
    relative motion between images into global poses. Deep learning methods are capable
    of extracting high-level feature representations from images, and thereby provide
    an alternative to solve the VO problem, without requiring hand-crafted feature
    extractors. Existing deep learning based VO models can be categorized into *end-to-end
    VO* and *hybrid VO*, depending on whether they are purely neural-network based
    or whether they are a combination of classical VO algorithms and deep neural networks.
    Depending on the availability of ground-truth labels in the training phase, end-to-end
    VO systems can be further classified into *supervised* VO and *unsupervised* VO.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Supervised Learning of VO
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We start with the introduction of supervised VO, one of the most predominant
    approaches to learning-based odometry, by training a deep neural network model
    on labelled datasets to construct a mapping function from consecutive images to
    motion transformations directly, instead of exploiting the geometric structures
    of images as in conventional VO systems[[41](#bib.bib41)]. At its most basic,
    the input of deep neural network is a pair of consecutive images, and the output
    is the estimated translation and rotation between two frames of images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: One of the first works in this area was Konda et al. [[44](#bib.bib44)]. This
    approach formulates visual odometry as a classification problem, and predicts
    the discrete changes of direction and velocity from input images using a convolutional
    neural network (ConvNet). Costante et al. [[45](#bib.bib45)] used a ConvNet to
    extract visual features from dense optical flow, and based on these visual features
    to output frame-to-frame motion estimation. Nonetheless, these two works have
    not achieved end-to-end learning from images to motion estimates, and their performance
    is still limited.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepVO [[24](#bib.bib24)] utilizes a combination of convolutional neural network
    (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of
    visual odometry. The framework of DeepVO becomes a typical choice in realizing
    supervised learning of VO, due to its specialization in end-to-end learning. Figure
    [4](#S2.F4 "Figure 4 ‣ 2 Taxonomy of Existing Approaches ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    (a) shows the architecture of this RNN+ConvNet based VO system, which extracts
    visual features from pairs of images via a ConvNet, and passes features through
    RNNs to model the temporal correlation of features. Its ConvNet encoder is based
    on a FlowNet structure to extract visual features suitable for optical flow and
    self-motion estimation. Using a FlowNet based encoder can be regarded as introducing
    the prior knowledge of optical flow into the learning process, and potentially
    prevents DeepVO from being overfitted to the training datasets. The reccurent
    model summarizes the history information into its hidden states, so that the output
    is inferred from both past experience and current ConvNet features from sensor
    observation. It is trained on large-scale datasets with groundtruthed camera poses
    as labels. To recover the optimal parameters $\bm{\theta}^{*}$ of framework, the
    optimization target is to minimize the Mean Square Error (MSE) of the estimated
    translations $\mathbf{\hat{p}}\in\mathbb{R}^{3}$ and euler angle based rotations
    $\hat{\bm{\varphi}}\in\mathbb{R}^{3}$:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\theta}^{*}=\operatorname*{arg\,min}_{\bm{\theta}}\frac{1}{N}\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{t=1}^{T}\&#124;\hat{\mathbf{p}}_{t}-\mathbf{p}_{t}\&#124;_{2}^{2}+\&#124;\hat{\bm{\varphi}}_{t}-\bm{\varphi}_{t}\&#124;_{2}^{2},$
    |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: where $(\hat{\mathbf{p}}_{t},\hat{\bm{\varphi}}_{t})$ are the estimates of relative
    pose at the timestep $t$, $(\mathbf{p},\bm{\varphi})$ are the corresponding groundtruth
    values, $\bm{\theta}$ are the parameters of the DNN framework, $N$ is the number
    of samples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: DeepVO reports impressive results on estimating the pose of driving vehicles,
    even in previously unseen scenarios. In the experiment on the KITTI odometry dataset[[46](#bib.bib46)],
    this data-driven solution outperforms conventional representative monocular VO,
    e.g. VISO2[[47](#bib.bib47)] and ORB-SLAM (without loop closure) [[21](#bib.bib21)].
    Another advantage is that supervised VO naturally produces trajectory with the
    absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous
    using only monocular information. This is because deep neural network can implicitly
    learn and maintain the global scale from large collection of images, which can
    be viewed as learning from past experience to predict current scale metric.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Based on this typical model of supervised VO, a number of works further extended
    this approach to improve the model performance. To improve the generalization
    ability of supervised VO, [[48](#bib.bib48)] incorporates curriculum learning
    (i.e. the model is trained by increasing the data complexity) and geometric loss
    constraints. Knowledge distillation (i.e. a large model is compressed by teaching
    a smaller one) is applied into the supervised VO framework to greatly reduce the
    number of network parameters, making it more amenable for real-time operation
    on mobile devices [[49](#bib.bib49)]. Furthermore, Xue et al. [[50](#bib.bib50)]
    introduced a memory module that stores global information, and a refining module
    that improves pose estimates with the preserved contextual information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In summary, these end-to-end learning methods benefit from recent advances in
    machine learning techniques and computational power, to automatically learn pose
    transformations directly from raw images that can tackle challenging real-world
    odometry estimation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Unsupervised Learning of VO
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is growing interest in exploring unsupervised learning of VO. Unsupervised
    solutions are capable of exploiting unlabelled sensor data, and thus it saves
    human effort on labelling data, and has better adaptation and generalization ability
    in new scenarios, where no labelled data are available. This has been achieved
    in a self-supervised framework that jointly learns depth and camera ego-motion
    from video sequences, by utilizing view synthesis as a supervisory signal[[29](#bib.bib29)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [4](#S2.F4 "Figure 4 ‣ 2 Taxonomy of Existing Approaches
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence") (b), a typical unsupervised VO solution consists of a depth
    network to predict depth maps, and a pose network to produce motion transformations
    between images. The entire framework takes consecutive images as input, and the
    supervision signal is based on novel view synthesis - given a source image $\mathbf{I}_{s}$,
    the view synthesis task is to generate a synthetic target image $\mathbf{I}_{t}$.
    A pixel of source image $\mathbf{I}_{s}(p_{s})$ is projected onto a target view
    $\mathbf{I}_{t}(p_{t})$ via:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{s}\sim\mathbf{K}\mathbf{T}_{t\to s}\mathbf{D}_{t}(p_{t})\mathbf{K}^{-1}p_{t}$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{K}$ is the camera’s intrinsic matrix, $\mathbf{T}_{t\to s}$
    denotes the camera motion matrix from target frame to source frame, and $\mathbf{D}_{t}(p_{t})$
    denotes the per-pixel depth maps in the target frame. The training objective is
    to ensure the consistency of the scene geometry by optimizing the photometric
    reconstruction loss between the real target image and the synthetic one:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{photo}}=\sum_{<\mathbf{I}_{1},...,\mathbf{I}_{N}>\in
    S}\sum_{p}&#124;\mathbf{I}_{t}(p)-\hat{\mathbf{I}}_{s}(p)&#124;,$ |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: where p denotes pixel coordinates, $\mathbf{I}_{t}$ is the target image, and
    $\hat{\mathbf{I}}_{s}$ is the synthetic target image generated from the source
    image $\mathbf{I}_{s}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A summary of existing methods on deep learning for odometry estimation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Sensor | Supervision | Scale | Performance | Contributions |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| Seq09 | Seq10 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| VO | Konda et al.[[44](#bib.bib44)] | MC | Supervised | Yes | - | - | formulate
    VO as a classification problem |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| Costante et al.[[45](#bib.bib45)] | MC | Supervised | Yes | 6.75 | 21.23
    | extract features from optical flow for VO estimates |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| Backprop KF[[51](#bib.bib51)] | MC | Hybrid | Yes | - | - | a differentiable
    Kalman filter based VO |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| DeepVO[[24](#bib.bib24)] | MC | Supervised | Yes | - | 8.11 | combine RNN
    and ConvNet for end-to-end learning |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| SfmLearner[[29](#bib.bib29)] | MC | Unsupervised | No | 17.84 | 37.91 | novel
    view synthesis for self-supervised learning |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| Yin et al.[[52](#bib.bib52)] | MC | Hybrid | Yes | 4.14 | 1.70 | introduce
    learned depth to recover scale metric |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| UnDeepVO[[53](#bib.bib53)] | SC | Unsupervised | Yes | 7.01 | 10.63 | use
    fixed stereo line to recover scale metric |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| Barnes et al.[[54](#bib.bib54)] | MC | Hybrid | Yes | - | - | integrate learned
    depth and ephemeral masks |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| GeoNet[[55](#bib.bib55)] | MC | Unsupervised | No | 43.76 | 35.6 | geometric
    consistency loss and 2D flow generator |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| Zhan et al.[[56](#bib.bib56)] | SC | Unsupervised | No | 11.92 | 12.45 |
    use fixed stereo line for scale recovery |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| DPF[[57](#bib.bib57)] | MC | Hybrid | Yes | - | - | a differentiable particle
    filter based VO |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| Yang et al.[[58](#bib.bib58)] | MC | Hybrid | Yes | 0.83 | 0.74 | use learned
    depth into classical VO |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al.[[59](#bib.bib59)] | MC | Supervised | Yes | - | 4.38 | generate
    dense 3D flow for VO and mapping |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| Struct2Depth[[60](#bib.bib60)] | MC | Unsupervised | No | 10.2 | 28.9 | introduce
    3D geometry structure during learning |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| Saputra et al.[[48](#bib.bib48)] | MC | Supervised | Yes | - | 8.29 | curriculum
    learning and geometric loss constraints |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| GANVO[[61](#bib.bib61)] | MC | Unsupervised | No | - | - | adversarial learning
    to generate depth |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| CNN-SVO[[62](#bib.bib62)] | MC | Hybrid | Yes | 10.69 | 4.84 | use learned
    depth to initialize SVO |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| Xue et al.[[50](#bib.bib50)] | MC | Supervised | Yes | - | 3.47 | memory
    and refinement module |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| Wang et al.[[63](#bib.bib63)] | MC | Unsupervised | Yes | 9.30 | 7.21 | integrate
    RNN and flow consistency constraint |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| Li et al.[[64](#bib.bib64)] | MC | Unsupervised | No | - | - | global optimization
    for pose graph |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| Saputra et al.[[49](#bib.bib49)] | MC | Supervised | Yes | - | - | knowledge
    distilling to compress deep VO model |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| Gordon[[65](#bib.bib65)] | MC | Unsupervised | No | 2.7 | 6.8 | camera matrix
    learning |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| Koumis et al.[[66](#bib.bib66)] | MC | Supervised | Yes | - | - | 3D convolutional
    networks |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| Bian et al.[[30](#bib.bib30)] | MC | Unsupervised | Yes | 11.2 | 10.1 | scale
    recovery from only monocular images |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| Zhan et al.[[67](#bib.bib67)] | MC | Hybrid | Yes | 2.61 | 2.29 | integrate
    learned optical flow and depth |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| D3VO[[25](#bib.bib25)] | MC | Hybrid | Yes | 0.78 | 0.62 | integrate learned
    depth, uncertainty and pose |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| VIO | VINet[[68](#bib.bib68)] | MC+I | Supervised | Yes | - | - | formulate
    VIO as a sequential learning problem |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| VIOLearner[[69](#bib.bib69)] | MC+I | Unsupervised | Yes | 1.51 | 2.04 |
    online correction module |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: '| Chen et al.[[70](#bib.bib70)] | MC+I | Supervised | Yes | - | - | feature
    selection for deep sensor fusion |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| DeepVIO[[71](#bib.bib71)] | SC+I | Unsupervised | Yes | 0.85 | 1.03 | learn
    VIO from stereo images and IMU |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| LO | Velas et al.[[72](#bib.bib72)] | L | Supervised | Yes | 4.94 | 3.27
    | ConvNet to estimate odometry from point clouds |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| LO-Net[[73](#bib.bib73)] | L | Supervised | Yes | 1.37 | 1.80 | geometric
    constraint loss |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| DeepPCO[[74](#bib.bib74)] | L | Supervised | Yes | - | - | parallel neural
    network |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: '| Valente et al.[[75](#bib.bib75)] | MC+L | Supervised | Yes | - | 7.60 | sensor
    fusion for LIDAR and camara |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model: VO, VIO and LO represent visual odometry, visual-inertial odometry and
    LIDAR odometry respectively.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sensor: MC, SC, I and L represent monocular camera, stereo camera, inertial
    measurement unit, and LIDAR respectively.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervision represents whether this work is a purely neural network based model
    trained with groundtruth labels (Supervised) or without labels (Unsupervised),
    or it is a combination of classical and deep neural network (Hybrid)
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale indicates whether a trajectory with a global scale can be produced.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance reports the localization error (a small number is better), i.e.
    the averaged translational RMSE drift (%) on lengths of 100m-800m on the KITTI
    odometry dataset[[46](#bib.bib46)]. Most works were evaluated on the Sequence
    09 and 10, and thus we took the results on these two sequences from their original
    papers for a performance comparison. Note that the training sets may be different
    in each work.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'However, there are basically two main problems that remained unsolved in the
    original work[[29](#bib.bib29)]: 1) this monocular image based approach is not
    able to provide pose estimates in a consistent global scale. Due to the scale
    ambiguity, no physically meaningful global trajectory can be reconstructed, limiting
    its real use. 2) The photometric loss assumes that the scene is static and without
    camera occlusions. Although the authors proposed the use of an explainability
    mask to remove scene dynamics, the influence of these environmental factors is
    still not addressed completely, which violates the assumption. To address these
    concerns, an increasing number of works [[53](#bib.bib53), [55](#bib.bib55), [56](#bib.bib56),
    [58](#bib.bib58), [59](#bib.bib59), [61](#bib.bib61), [64](#bib.bib64), [76](#bib.bib76),
    [77](#bib.bib77)] extended this unsupervised framework to achieve better performance.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: To solve the global scale problem, [[53](#bib.bib53), [56](#bib.bib56)] proposed
    to utilize stereo image pairs to recover the absolute scale of pose estimation.
    They introduced an additional spatial photometric loss between the left and right
    pairs of images, as the stereo baseline (i.e. motion transformation between the
    left and right images) is fixed and known throughout the dataset. Once the training
    is complete, the network produces pose predictions using only monocular images.
    Thus, although it is unsupervised in the context of not having access to ground-truth,
    the training dataset (stereo) is different to the test set (mono). [[30](#bib.bib30)]
    tackles the scale issue by introducing a geometric consistency loss, that enforces
    the consistency between predicted depth maps and reconstructed depth maps. The
    framework transforms the predicted depth maps into a 3D space, and projects them
    back to produce reconstructed depth maps. In doing so, the depth predictions are
    able to remain scale-consistent over consecutive frames, enabling pose estimates
    to be scale-consistent meanwhile.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The photometric consistency constraint assumes that the entire scenario consists
    only of rigid static structures, e.g. buildings and lanes. However, in real-world
    applications, environmental dynamics (e.g. pedestrians and vehicles), will distort
    the photometric projection and degrade the accuracy of pose estimation. To address
    this concern, GeoNet [[55](#bib.bib55)] divides its learning process into two
    sub-tasks by estimating static scene structures and motion dynamics separately
    through a rigid structure reconstructor and a non-rigid motion localizer. In addition,
    GeoNet enforces a geometric consistency loss to mitigate the issues caused by
    camera occlusions and non-Lambertian surfaces. [[59](#bib.bib59)] adds a 2D flow
    generator along with a depth network to generate 3D flow. Benefiting from better
    3D understanding of environment, their framework is able to produce more accurate
    camera pose, along with a point cloud map. GANVO [[61](#bib.bib61)] employs a
    generative adversarial learning paradigm for depth generation, and introduces
    a temporal recurrent module for pose regression. Li et al. [[76](#bib.bib76)]
    also utilized a generative adversarial network (GAN) to generate more realistic
    depth maps and poses, and further encourage more accurate synthetic images in
    the target frame. Instead of a hand-crafted metric, a discriminator is adopted
    to evaluate the quality of synthetic images generation. In doing so, the generative
    adversarial setup facilitates the generated depth maps to be more texture-rich
    and crisper. In this way, high-level scene perception and representation are accurately
    captured and environmental dynamics are implicitly tolerated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 光度一致性约束假设整个场景仅由刚性静态结构（如建筑物和车道）构成。然而，在实际应用中，环境动态（如行人和车辆）将扭曲光度投影，降低位姿估计的准确性。为了解决这一问题，GeoNet
    [[55](#bib.bib55)] 将学习过程分为两个子任务，通过刚性结构重建器和非刚性运动定位器分别估计静态场景结构和运动动态。此外，GeoNet 强制执行几何一致性损失，以减轻由相机遮挡和非朗伯表面造成的问题。[[59](#bib.bib59)]
    添加了一个 2D 流生成器以及一个深度网络，以生成 3D 流。凭借对环境更好的 3D 理解，他们的框架能够生成更准确的相机位姿，以及一个点云地图。GANVO
    [[61](#bib.bib61)] 采用生成对抗学习范式来生成深度，并引入了一个时间递归模块来进行位姿回归。Li 等人 [[76](#bib.bib76)]
    还利用生成对抗网络（GAN）生成更逼真的深度图和位姿，并进一步鼓励目标帧中更准确的合成图像。与手工制作的度量标准不同，采用了一个判别器来评估合成图像生成的质量。这样，生成对抗设置有助于生成的深度图更富有纹理和更清晰。通过这种方式，高级场景感知和表示得以准确捕捉，环境动态也被隐式地容忍。
- en: 'Although unsupervised VO still cannot compete with supervised VO in performance,
    as illustrated in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Inertial Odometry ‣ 3 Odometry
    Estimation ‣ A Survey on Deep Learning for Localization and Mapping: Towards the
    Age of Spatial Machine Intelligence"), its concerns of scale metric and scene
    dynamics problem have been largely resolved. With the benefits of self-supervised
    learning, and ever-increasing improvement on performance, unsupervised VO would
    be a promising solution in providing pose information, and tightly coupled with
    other modules in spatial machine intelligence system.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督视觉里程计（VO）的性能仍无法与有监督视觉里程计相媲美，如图[5](#S3.F5 "图 5 ‣ 3.3 惯性里程计 ‣ 3 里程计估计 ‣ 深度学习在定位与地图构建中的应用：迈向空间机器智能时代")所示，但其在尺度度量和场景动态问题上的顾虑已在很大程度上得到解决。凭借自监督学习的优势以及性能的持续提升，无监督VO将在提供位姿信息方面成为一个有前途的解决方案，并与空间机器智能系统中的其他模块紧密结合。
- en: 3.1.3 Hybrid VO
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 混合视觉里程计
- en: Unlike end-to-end VO that only relies on a deep neural network to interpret
    pose from data, hybrid VO integrates classical geometric models with deep learning
    framework. Based on mature geometric theory, they use a deep neural network to
    expressively replace parts of a geometry model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅依赖深度神经网络从数据中解释位姿的端到端视觉里程计不同，混合视觉里程计将经典几何模型与深度学习框架相结合。基于成熟的几何理论，它们利用深度神经网络来表达性地替代几何模型的部分。
- en: A straightforward way is to incorporate the learned depth estimates into a conventional
    visual odometry algorithm to recover the absolute scale metric of poses [[52](#bib.bib52)].
    Learning depth estimation is a well-researched area in the computer vision community.
    For example, [[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]
    provide per-pixel depths in a global scale by employing a trained deep neural
    model. Thus the so-called scale problem of conventional VO is mitigated. Barnes
    et al. [[54](#bib.bib54)] utilize both the predicted depth maps and ephemeral
    masks (i.e. the area of moving objects) into a VO system to improve its robustness
    to moving objects. Zhan et al. [[67](#bib.bib67)] integrate the learned depth
    and optical flow predictions into a conventional visual odometry model, achieving
    competitive performance over other baselines. Other works combine physical motion
    models with deep neural network e.g. via a differentiable Kalman filter [[82](#bib.bib82)],
    and a particle filter [[83](#bib.bib83)]. The physical model serves as an algorithmic
    prior in the learning process. Furthermore, D3VO [[25](#bib.bib25)] incorporates
    the deep predictions of depth, pose, and uncertainty into a direct visual odometry.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接的方法是将学习到的深度估计融入传统的视觉里程计算法，以恢复姿态的绝对尺度度量[[52](#bib.bib52)]。学习深度估计是计算机视觉领域的一个广泛研究的领域。例如，[[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]通过使用训练好的深度神经网络模型提供了全局尺度的每像素深度。因此，传统视觉里程计的所谓尺度问题得到了缓解。Barnes等人[[54](#bib.bib54)]将预测的深度图和短暂的掩码（即运动物体的区域）结合到视觉里程计系统中，以提高其对运动物体的鲁棒性。Zhan等人[[67](#bib.bib67)]将学习的深度和光流预测整合到传统的视觉里程计模型中，在其他基线方法上实现了具有竞争力的性能。其他工作结合了物理运动模型和深度神经网络，例如通过可微分的卡尔曼滤波器[[82](#bib.bib82)]和粒子滤波器[[83](#bib.bib83)]。物理模型作为学习过程中的算法先验。此外，D3VO[[25](#bib.bib25)]将深度、姿态和不确定性的深度预测整合到直接视觉里程计中。
- en: Combining the benefits from both geometric theory and deep learning, hybrid
    models are normally more accurate than end-to-end VO at this stage, as shown in
    Table 1. It is notable that hybrid models even outperform the state-of-the-art
    conventional monocular VO or visual-inertial odometry (VIO) systems on common
    benchmarks, for example, D3VO[[25](#bib.bib25)] defeats several popular conventional
    VO/VIO systems, such as DSO[[84](#bib.bib84)], ORB-SLAM[[21](#bib.bib21)], VINS-Mono[[15](#bib.bib15)].
    This demonstrates the rapid rate of progress in this area.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结合几何理论和深度学习的优势，混合模型通常比端到端的视觉里程计更准确，如表1所示。值得注意的是，混合模型甚至在常见基准测试中超越了最先进的传统单目视觉里程计或视觉-惯性里程计（VIO）系统，例如，D3VO[[25](#bib.bib25)]击败了几个流行的传统视觉里程计/VIO系统，如DSO[[84](#bib.bib84)]、ORB-SLAM[[21](#bib.bib21)]、VINS-Mono[[15](#bib.bib15)]。这表明该领域的进展速度非常快。
- en: 3.2 Visual-Inertial Odometry
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 视觉-惯性里程计
- en: 'Integrating visual and inertial data as visual-inertial odometry (VIO) is a
    well-defined problem in mobile robotics. Both cameras and inertial sensors are
    relatively low-cost, power-efficient and widely deployed. These two sensors are
    complementary: monocular cameras capture the appearance and structure of a 3D
    scene, while they are scale-ambiguous, and not robust to challenging scenarios,
    e.g. strong lighting changes, lack of texture and high-speed motion; In contrast,
    IMUs are completely ego-centric, scene-independent, and can also provide absolute
    metric scale. Nevertheless, the downside is that inertial measurements, especially
    from low-cost devices, are plagued by process noise and biases. An effective fusion
    of the measurements from these two complementary sensors is of key importance
    to accurate pose estimation. Thus, according to their information fusion methods,
    conventional model based visual-inertial approaches are roughly segmented into
    three different classes : filtering approaches [[12](#bib.bib12)], fixed-lag smoothers
    [[13](#bib.bib13)] and full smoothing methods [[14](#bib.bib14)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将视觉和惯性数据整合为视觉-惯性里程计（VIO）是移动机器人中的一个明确问题。相机和惯性传感器都相对低成本、功耗低且广泛部署。这两种传感器是互补的：单目相机捕捉3D场景的外观和结构，但尺度模糊，对挑战性场景（例如强光变化、纹理缺乏和高速运动）不够鲁棒；相比之下，IMU完全是自我中心的、场景独立的，并且还可以提供绝对度量尺度。然而，缺点是惯性测量，特别是来自低成本设备的测量，常受到过程噪声和偏差的困扰。有效融合这两种互补传感器的测量对准确的姿态估计至关重要。因此，根据它们的信息融合方法，传统的基于模型的视觉-惯性方法大致分为三类：滤波方法[[12](#bib.bib12)]、固定时滞平滑器[[13](#bib.bib13)]和全平滑方法[[14](#bib.bib14)]。
- en: Data-driven approaches have emerged to consider learning 6-DoF poses directly
    from visual and inertial measurements without human intervention or calibration.
    VINet [[68](#bib.bib68)] is the first work that formulated visual-inertial odometry
    as a sequential learning problem, and proposed a deep neural network framework
    to achieve VIO in an end-to-end manner. VINet uses a ConvNet based visual encoder
    to extract visual features from two consecutive RGB images, and an inertial encoder
    to extract inertial features from a sequence of IMU data with a long short-term
    memory (LSTM) network. Here, the LSTM aims to model the temporal state evolution
    of inertial data. The visual and inertial features are concatenated together,
    and taken as the input into a further LSTM module to predict relative poses, conditioned
    on the history of system states. This learning approach has the advantage of being
    more robust to calibration and relative timing offset errors. However, VINet has
    not fully addressed the problem of learning a meaningful sensor fusion strategy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的方法已经出现，考虑直接从视觉和惯性测量中学习6自由度姿态，而无需人工干预或校准。VINet [[68](#bib.bib68)] 是首个将视觉惯性里程计表述为序列学习问题的工作，并提出了一种深度神经网络框架，以端到端的方式实现VIO。VINet
    使用基于卷积网络（ConvNet）的视觉编码器从两张连续的RGB图像中提取视觉特征，并使用惯性编码器从一系列IMU数据中提取惯性特征，后者采用长短期记忆（LSTM）网络。这里，LSTM旨在建模惯性数据的时间状态演变。视觉和惯性特征被连接在一起，并作为输入送入进一步的LSTM模块，以预测相对姿态，依赖于系统状态的历史。这种学习方法具有对校准和相对时间偏移误差更具鲁棒性的优势。然而，VINet尚未完全解决学习有意义的传感器融合策略的问题。
- en: To tackle the deep sensor fusion problem, Chen et al. [[70](#bib.bib70)] proposed
    selective sensor fusion, a framework that selectively learns context-dependent
    representations for visual inertial pose estimation. Their intuition is that the
    importance of features from different modalities should be considered according
    to the exterior (i.e., environmental) and interior (i.e., device/sensor) dynamics,
    by fully exploiting the complementary behaviors of two sensors. Their approach
    outperforms those without a fusion strategy, e.g. VINet, avoiding catastrophic
    failures.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决深度传感器融合问题，Chen等人[[70](#bib.bib70)] 提出了选择性传感器融合框架，这种框架选择性地学习依赖上下文的表示，以进行视觉惯性姿态估计。他们的直觉是，根据外部（即环境）和内部（即设备/传感器）动态，应该考虑不同模态特征的重要性，充分利用两个传感器的互补行为。他们的方法优于那些没有融合策略的方法，例如VINet，避免了灾难性的失败。
- en: 'Similar to unsupervised VO, Visual-inertial odometry can also be solved in
    a self-supervised fashion using novel view synthesis. VIOLearner [[69](#bib.bib69)]
    constructs motion transformations from raw inertial data, and converts source
    images into target images with the camera matrix and depth maps via the Equation
    [2](#S3.E2 "In 3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry
    Estimation ‣ A Survey on Deep Learning for Localization and Mapping: Towards the
    Age of Spatial Machine Intelligence") mentioned in Section [3.1.2](#S3.SS1.SSS2
    "3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry Estimation
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence"). In addition, an online error correction module corrects
    the intermediate errors of the framework. The network parameters are recovered
    by optimizing a photometric loss. Similarly, DeepVIO [[71](#bib.bib71)] incorporates
    inertial data and stereo images into this unsupervised learning framework, and
    is trained with a dedicated loss to reconstruct trajectories in a global scale.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于无监督视觉里程计（VO），视觉惯性里程计也可以通过新颖的视图合成以自监督方式解决。VIOLearner [[69](#bib.bib69)] 从原始惯性数据构建运动变换，并通过第[2](#S3.E2
    "In 3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry Estimation
    ‣ A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial
    Machine Intelligence")节提到的方程式，将源图像转换为目标图像，使用相机矩阵和深度图。此外，一个在线错误修正模块修正了框架中的中间错误。网络参数通过优化光度损失来恢复。类似地，DeepVIO
    [[71](#bib.bib71)] 将惯性数据和立体图像纳入这个无监督学习框架，并通过专用损失进行训练，以重建全球尺度的轨迹。'
- en: Learning-based VIO cannot defeat the state-of-the-art classical model based
    VIOs, but they are generally more robust to real issues[[68](#bib.bib68), [70](#bib.bib70),
    [71](#bib.bib71)] such as measurement noises, bad time synchronization, thanks
    to the impressive ability of DNNs in feature extraction and motion modelling.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Inertial Odometry
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond visual odometry and visual-inertial odometry, an inertial-only solution,
    i.e. inertial odometry provides an ubiquitous alternative to solve the odometry
    estimation problem. Compared with visual methods, an inertial sensor is relatively
    low-cost, small, energy efficient and privacy preserving. It is relatively immune
    to environmental factors, such as lighting conditions or moving objects. However,
    low-cost MEMS inertial measurement units (IMU) widely found on robots and mobile
    devices are corrupted with high sensor bias and noise, leading to unbounded error
    drifts in the strapdown inertial navigation system (SINS), if inertial data are
    doubly integrated.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. [[85](#bib.bib85)] formulated inertial odometry as a sequential
    learning problem with a key observation that 2D motion displacements in the polar
    coordinate (i.e. polar vector) can be learned from independent windows of segmented
    inertial data. The key observation is that when tracking human and wheeled configurations,
    the frequency of their vibrations is relevant to the moving speed, which is reflected
    by inertial measurements. Based on this, they proposed IONet, a LSTM based framework
    for end-to-end learning of relative poses from sequences of inertial measurements.
    Trajectories are generated by integrating motion displacements. [[86](#bib.bib86)]
    leveraged deep generative models and domain adaptation technique to improve the
    generalization ability of deep inertial odometry in new domains. [[87](#bib.bib87)]
    extends this framework by an improved triple-channel LSTM network to predict polar
    vectors for drone localization from inertial data and sampling time. RIDI [[88](#bib.bib88)]
    trains a deep neural network to regress linear velocities from inertial data,
    calibrates the collected accelerations to satisfy the constraints of the learned
    velocities, and doubly integrates the accelerations into locations with a conventional
    physical model. Similarly, [[89](#bib.bib89)] compensates the error drifts of
    the classical SINS model with the aid of learned velocities. Other works have
    also explored the usage of deep learning to detect zero-velocity phase for navigating
    pedestrians [[90](#bib.bib90)] and vehicles [[91](#bib.bib91)]. This zero-velocity
    phase provides context information to correct system error drifts via Kalman filtering.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Inertial only solution can be a backup plan to offer pose information in extreme
    environments, where visual information is not available or is highly distorted.
    Deep learning has proven its capability to learn useful features from noisy IMU
    data, and compensate the error drifts of inertial dead reckoning, which is difficult
    to solve by classical algorithms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba3053551fe966de85273fb170bcdb76.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A comparison of the performance of deep learning based visual odometry
    with an evaluation on the Trajectory 10 of the KITTI dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 LIDAR Odometry
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LIDAR sensors provide high-frequency range measurements, with the benefits of
    working consistently in complex lighting conditions and optically featureless
    scenarios. Mobile robots and self-driving vehicles are normally equipped with
    LIDAR sensors to obtain relative self-motion (i.e. LIDAR odometry) and global
    pose with respect to a 3D map (LIDAR relocalization). The performance of LIDAR
    odometry is sensitive to point cloud registration errors due to non-smooth motion.
    In addition, the data quality of LIDAR measurements is also affected by extreme
    weather conditions, for example, heavy rain or fog/mist.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, LIDAR odometry relies on point cloud registration to detect feature
    points, e.g. line and surface segments, and uses a matching algorithm to obtain
    the pose transformation by minimizing the distance between two consecutive point-cloud
    scans. Data-driven methods consider solving LIDAR odometry in an end-to-end fashion,
    by leveraging deep neural networks to construct a mapping function from point
    cloud scan sequences to pose estimates [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)].
    As point cloud data are challenging to be directly ingested by neural networks
    due to their sparse and irregularly sampled format, these methods typically convert
    point clouds into a regular matrix through cylindrical projection, and adopt ConvNets
    to extract features from consecutive point cloud scans. These networks regress
    relative poses and are trained via ground-truth labels. LO-Net [[73](#bib.bib73)]
    reports competitive performance over the conventional state-of-the-art algorithm,
    i.e. the LIDAR Odometry and Mapping (LOAM) algorithm [[16](#bib.bib16)].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Comparison of Odometry Estimation
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [I](#S3.T1 "TABLE I ‣ 3.1.2 Unsupervised Learning of VO ‣ 3.1 Visual
    Odometry ‣ 3 Odometry Estimation ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") compares existing
    work on odometry estimation, in terms of their sensor type, model, whether a trajectory
    with an absolute scale is produced, and their performance evaluation on the KITTI
    dataset, where available. As deep inertial odometry has not been evaluated on
    the KITTI dataset, we do not include inertial odometry in this table. The KITTI
    dataset [[46](#bib.bib46)] is a common benchmark for odometry estimation, consisting
    of a collection of sensor data from car-driving scenarios. As most data-driven
    approaches adopt the trajectory 09 and 10 of the KITTI dataset to evaluate model
    performance, we compared them according to the averaged Root Mean Square Error
    (RMSE) of the translation for all the subsequences of lengths (100, 200, .., 800)
    meters, which is provided by the official KITTI VO/SLAM evaluation metrics.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'We take visual odometry as an example. Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Inertial
    Odometry ‣ 3 Odometry Estimation ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") reports the translational
    drifts of deep visual odometry models over time on the 10th trajectory of the
    KITTI dataset. Clearly, hybrid VO shows the best performance over supervised VO
    and unsupervised VO, as the hybrid model benefits from both the mature geometry
    models of traditional VO algorithms and the strong capacity for feature extraction
    of deep learning. Although supervised VO still outperforms unsupervised VO, the
    performance gap between them is diminishing as the limitations of unsupervised
    VO are gradually addressed. For example, it has been found that unsupervised VO
    now can recover global scale from monocular images [[30](#bib.bib30)]. Overall,
    data-driven visual odometry shows a remarkable increase in model performance,
    indicating the potentials of deep learning approaches in achieving more accurate
    odometry estimation in the future.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 4 Mapping
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mapping refers to the ability of a mobile agent to build a consistent environmental
    model to describe the surrounding scene. Deep learning has fostered a set of tools
    for scene perception and understanding, with applications ranging from depth prediction,
    to semantic labelling, to 3D geometry reconstruction. This section provides an
    overview of existing works relevant to deep learning based mapping methods. We
    categorize them into geometric mapping, semantic mapping, and general mapping.
    Table [II](#S4.T2 "TABLE II ‣ 4 Mapping ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") summarizes the
    existing methods on deep learning based mapping.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: A summary of existing methods on deep learning for mapping.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Output Representation | Employed by |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Geometric Map | Depth Representation | [[78](#bib.bib78), [92](#bib.bib92),
    [79](#bib.bib79)], [[80](#bib.bib80), [81](#bib.bib81)], [[29](#bib.bib29)], [[53](#bib.bib53),
    [55](#bib.bib55), [56](#bib.bib56), [58](#bib.bib58), [59](#bib.bib59), [93](#bib.bib93),
    [61](#bib.bib61), [64](#bib.bib64)], [[76](#bib.bib76), [77](#bib.bib77)] |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Voxel Representation | [[94](#bib.bib94), [95](#bib.bib95)], [[96](#bib.bib96)](Object),
    [[97](#bib.bib97)], [[98](#bib.bib98)](Object), [[99](#bib.bib99)](Object), [[100](#bib.bib100)](Object)
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Point Representation | [[101](#bib.bib101)](Object) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Mesh Representation | [[102](#bib.bib102)](Object), [[103](#bib.bib103)](Object),
    [[104](#bib.bib104)](Object), [[105](#bib.bib105)](Object), [[106](#bib.bib106),
    [107](#bib.bib107)] |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Semantic Map | Semantic Segmentation | [[26](#bib.bib26), [27](#bib.bib27),
    [108](#bib.bib108)] |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Instance Segmentation | [[109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111)]
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Panoptic Segmentation | [[112](#bib.bib112)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| General Map | Neural Representation | [[113](#bib.bib113)],[[114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116)], [[117](#bib.bib117), [118](#bib.bib118),
    [31](#bib.bib31), [32](#bib.bib32)] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object indicates that this method is only validated on reconstructing single
    objects rather than a scene.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1 Geometric Mapping
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Broadly, geometric mapping captures the shape and structural description of
    a scene. Typical choices of the scene representations used in geometric mapping
    include depth, voxel, point and mesh. We follow this representational taxonomy
    and categorize deep learning for geometric mapping into the above four classes.
    Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Geometric Mapping ‣ 4 Mapping ‣ A Survey on
    Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine
    Intelligence") demonstrates these geometric representations on the Stanford Bunny
    benchmark.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4223f55b3e003334e300bab7d872bbe.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An illustrations of scene representations on the Stanford Bunny benchmark:
    (a) original model, (b) depth representation, (c) voxel representation (d) point
    representation (e) mesh representation.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Depth Representation
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depth maps play a pivotal role in understanding the scene geometry and structure.
    Dense scene reconstruction has been achieved by fusing depth and RGB images [[119](#bib.bib119),
    [120](#bib.bib120)]. Traditional SLAM systems represent scene geometry with dense
    depth maps (i.e. 2.5D), such as DTAM [[121](#bib.bib121)]. In addition, accurate
    depth estimation can contribute to the absolute scale recovery for visual SLAM.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Learning depth from raw images is a fast evolving area in computer vision community.
    The earliest work formulates depth estimation as a mapping function of input single
    images, constructed by a multi-scale deep neural network [[78](#bib.bib78)] to
    output the per-pixel depth maps from single images. More accurate depth prediction
    is achieved by jointly optimizing the depth and self-motion estimation [[79](#bib.bib79)].
    These supervised learning methods [[78](#bib.bib78), [92](#bib.bib92), [79](#bib.bib79)]
    can predict per-pixel depth by training deep neural networks on large data collections
    of images with corresponding depth labels. Although they are found outperforming
    the traditional structure based methods, such as [[122](#bib.bib122)], their effectiveness
    are largely reliant on model training and can be difficult to generalize to new
    scenarios in absence of labeled data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other side, recent advances in this field focus on unsupervised solutions,
    by reformulating depth prediction as a novel view synthesis problem. [[80](#bib.bib80),
    [81](#bib.bib81)] utilized photometric consistency loss as a self-supervision
    signal for training neural models. With stereo images and a known camera baseline,
    [[80](#bib.bib80), [81](#bib.bib81)] synthesize the left view from the right image,
    and the predicted depth maps of the left view. By minimizing the distance between
    synthesized images and real images, i.e. the spatial consistency, the parameters
    of the networks can be recovered via this self-supervision in an end-to-end manner.
    Besides the spatial consistency, [[29](#bib.bib29)] proposed to apply temporal
    consistency as a self-supervised signal, by synthesizing the image in the target
    time frame from the source time frame. At the same time, egomotion is recovered
    along with the depth estimation. This framework only requires monocular images
    to learn both the depth maps and egomotion. A number of following works [[53](#bib.bib53),
    [55](#bib.bib55), [56](#bib.bib56), [58](#bib.bib58), [59](#bib.bib59), [93](#bib.bib93),
    [61](#bib.bib61), [64](#bib.bib64), [76](#bib.bib76), [77](#bib.bib77)] extended
    this framework and achieved better performance in depth and egomotion estimation.
    We refer the readers to Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Unsupervised Learning
    of VO ‣ 3.1 Visual Odometry ‣ 3 Odometry Estimation ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence"),
    in which a variety of additional constraints haven been discussed.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: With the depth maps predicted by ConvNets, learning based SLAM systems can integrate
    depth information to address some limitations of classical monocular solution.
    For example, CNN-SLAM [[123](#bib.bib123)] utilizes the learned depths from single
    images into a monocular SLAM framework (i.e. LSD-SLAM [[124](#bib.bib124)]). Their
    experiment shows how the learned depth maps contribute to mitigate the absolute
    scale recovery problem in pose estimates and scene reconstruction. CNN-SLAM achieves
    dense scene predictions even in texture-less areas, which is normally hard for
    a conventional SLAM system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Voxel Representation
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Voxel-based formulation is a natural way to represent 3D geometry. Similar to
    the usage of pixel (i.e. 2D element) in images, voxel is a volume element in a
    three-dimensional space. Previous works have explored to use multiple input views,
    to reconstruct the volumetric representation of scene [[94](#bib.bib94), [95](#bib.bib95)]
    and objects [[96](#bib.bib96)]. For example, SurfaceNet [[94](#bib.bib94)] learns
    to predict the confidence of a voxel to determine whether it is on surface or
    not, and reconstruct the 2D surface of a scene. RayNet [[95](#bib.bib95)] reconstructs
    the scene geometry by extracting view-invariant features while imposing geometric
    constraints. Recent works focus on generating high-resolution 3D volumetric models
    [[98](#bib.bib98), [97](#bib.bib97)]. For example, Tatarchenko et al. [[97](#bib.bib97)]
    designed a convolutional decoder based on octree-based formulation to enable scene
    reconstruction in much higher resolution. Other work can be found on scene completion
    from RGB-D data [[99](#bib.bib99), [100](#bib.bib100)]. One limitation of voxel
    representation is the high computational requirement, especially when attempting
    to reconstruct a scene in high resolution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Point Representation
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point-based formulation consists of the 3-dimensional coordinates (x, y, z)
    of points in 3D space. Point representation is easy to understand and manipulate,
    but suffers from the ambiguity problem, which means that different forms of point
    clouds can represent a same geometry. The pioneer work, PointNet [[125](#bib.bib125)],
    processes unordered point data with a single symmetric function - max pooling,
    to aggregate point features for classification and segmentation. Fan et al. [[101](#bib.bib101)]
    developed a deep generative model that generates 3D geometry in point-based formulation
    from single images. In their work, a loss function based on Earth Mover’s distance
    is introduced to tackle the problem of data ambiguity. However, their method is
    only validated on the reconstruction task of single objects. No work on point
    generation for scene reconstruction has been found yet.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Mesh Representation
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mesh-based formulation encodes the underlying structure of 3D models, such as
    edges, vertices and faces. It is a powerful representation that naturally captures
    the surface of 3D shape. Several works considered the problem of learning mesh
    generation from images [[102](#bib.bib102), [103](#bib.bib103)] or point clouds
    data [[104](#bib.bib104), [105](#bib.bib105)]. However, these approaches are only
    able to reconstruct single objects, and limited to generating models with simple
    structures or from familiar classes. To tackle the problem of scene reconstruction
    in mesh representation, [[106](#bib.bib106)] integrates the sparse features from
    monocular SLAM with dense depth maps from ConvNet for the update of 3D mesh representation.
    The depth predictions are fused into the monocular SLAM system to recover the
    absolute scale of pose and scene features estimation. To allow efficient computation
    and flexible information fusion, [[107](#bib.bib107)] utilizes 2.5D mesh to represent
    scene geometry. In their approach, the image plane coordinates of mesh vertices
    are learned by deep neural networks, while the depth maps are optimized as free
    variables.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Semantic Map
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb3b9d7b01e5d5b063558deccfacd5de.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: (b) semantic segmentation, (c) instance segmentation and (d) panoptic
    segmentation for semantic mapping [[126](#bib.bib126)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Semantic mapping connects semantic concepts (i.e. object classification, material
    composition etc) with the geometry of environments. This is treated as a data
    association problem. The advances in deep learning greatly fosters the developments
    of object recognition and semantic segmentation. Maps with semantic meanings enable
    mobile agents to have high-level understandings of their environments beyond pure
    geometry, and allow for a greater range of functionality and autonomy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: SemanticFusion [[26](#bib.bib26)] is one of the early works that combined the
    semantic segmentation labels from deep ConvNet with the dense scene geometry from
    a SLAM system. It incrementally integrates per-frame semantic segmentation predictions
    into a dense 3D map by probabilistically associating the 2D frames with the 3D
    map. This combination not only generates a map with useful semantic information,
    but also shows the integration with a SLAM system helps to enhance the single
    frame segmentation. The two modules are loosely coupled in SemanticFusion. [[27](#bib.bib27)]
    proposed a self-supervised network that predicts consistent semantic labels for
    a map, by imposing constraints on the consistency of semantic predictions in multiple
    views. DA-RNN [[108](#bib.bib108)] introduces recurrent models into semantic segmentation
    framework to learn the temporal connections over multiple view frames, producing
    more accurate and consistent semantic labelling for volumetric maps from KinectFusion
    [[127](#bib.bib127)]. Yet these methods provide no information on object instances,
    which means that they are not able to distinguish among different ojects from
    the same category.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'With the advances in instance segmentation, semantic mapping evolves into the
    instance level. A good example is [[109](#bib.bib109)] that offers object-level
    semantic mapping by identifying individual objects via a bounding box detection
    module and an unsupervised geometric segmentation module. Unlike other dense semantic
    mapping approaches, Fusion++ [[110](#bib.bib110)] builds a semantic graph-based
    map, which predicts only object instances and maintains a consistent map via loop
    closure detection, pose-graph optimization and further refinement. [[111](#bib.bib111)]
    presented a framework that achieves instance-aware semantic mapping, and enables
    novel object discovery. Recently, panoptic segmentation [[126](#bib.bib126)] attracts
    a lot of attentions. PanopticFusion [[112](#bib.bib112)] advanced semantic mapping
    to the level of stuff and things level that classifies static objects, e.g. walls,
    doors, lanes as stuff classes, and other accountable objects as things classes,
    e.g. moving vehicles, human and tables. Figure [7](#S4.F7 "Figure 7 ‣ 4.2 Semantic
    Map ‣ 4 Mapping ‣ A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence") compares semantic segmentation, instance
    segmentation and panoptic segmentation.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 General Map
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond the explicit geometric and semantic map representation, deep learning
    models are able to encode the whole scene into an implicit representation, i.e.
    a general map representation to capture the underlying scene geometry and appearance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing deep autoencoders can automatically discover the high-level compact
    representation of high-dimensional data. A notable example is CodeSLAM [[113](#bib.bib113)]
    that encodes observed images into a compact and optimizable representation to
    contain the essential information of a dense scene. This general representation
    is further used into a keyframe-based SLAM system to infer both pose estimates
    and keyframe depth maps. Due to the reduced size of learned representations, CodeSLAM
    allows efficient optimization of tracking camera motion and scene geometry for
    a global consistency.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural rendering models are another family of works that learn to model 3D
    scene structure implicitly by exploiting view synthesis as a self-supervision
    signal. The target of neural rendering task is to reconstruct a new scene from
    an unknown viewpoint. The seminar work, Generative Query Network (GQN) [[128](#bib.bib128)]
    learns to capture representation and render a new scene. GQN consists of a representation
    network and a generation network: the representation network encodes the observations
    from reference views into a scene representation; the generation network which
    is based on recurrent model, reconstructs the scene from a new view conditioned
    on the scene representation and a stochastic latent variable. Taking inputs as
    the observed images from several viewpoints, and the camera pose of a new view,
    GQN predicts the physical scene of this new view. Intuitively, through end-to-end
    training, the representation network can capture the necessary and important factors
    of 3D environment for the scene reconstruction task via the generation network.
    GQN is extended by incorporating a geometric-aware attention mechanism to allow
    more complex environment modelling [[114](#bib.bib114)], as well as including
    multimodal data for scene inference [[115](#bib.bib115)]. Scene representation
    network (SRN) [[116](#bib.bib116)] tackles the scene rendering problem via a learned
    continuous scene representation that connects a camera pose and its corresponding
    observation. A differentiable Ray Marching algorithm is integrated into SRN to
    enforce the network to model 3D structure consistently. However, these frameworks
    can only be applied to synthetic datasets due to the complexity of real-world
    environments.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, in the quest of ‘map-less’ navigation, task-driven maps
    emerge as a novel map representation. This representation is jointly modelled
    by deep neural networks with respect to the task at hand. Generally those tasks
    leverage location information, such as navigation or path planning, requiring
    mobile agents to understand the geometry and semantics of environment. Navigation
    in unstructured environments (even in a city scale) is formulated as an policy
    learning problem in these works [[117](#bib.bib117), [118](#bib.bib118), [31](#bib.bib31),
    [32](#bib.bib32)], and solved by deep reinforcement learning. Different from traditional
    solutions that follow a procedure of building an explicit map, planning path and
    making decisions, these learning based techniques predict control signals directly
    from sensor observations in an end-to-end manner, without explicitly modelling
    the environment. The model parameters are optimized via sparse reward signals,
    for example, whenever agents reach a destination, a positive reward will be given
    to tune the neural network. Once a model is trained, the actions of agents can
    be determined conditioned on the current observations of environment, i.e. images.
    In this case, all of environmental factors, such as the geometry, appearance and
    semantics of a scene, are embedded inside the neurons of a deep neural network
    and suitable for solving the task at hand. Interestingly, the visualization of
    the neurons inside a neural model that is trained on the navigation task via reinforcement
    learning, has similar patterns as the grid and place cells inside human brain.
    This provides cognitive cues to support the effectiveness of neural map representation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 5 Global Localization
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Global localization concerns the retrieval of absolute pose of a mobile agent
    within a known scene. Different from odometry estimation that relies on estimating
    the internal dynamical model and can perform in an unseen scenario, in global
    localization, prior knowledge about the scene is provided and exploited, through
    a 2D or 3D scene model. Broadly, it describes the relation between the sensor
    observations and map, by matching a query image or view against a pre-built model,
    and returning an estimate of global pose.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'We categorize deep learning based global localization into three categories,
    according to the types of inquiry data and map: *2D-to-2D localization* queries
    2D images against an explicit database of geo-referenced images or implicit neural
    map; *2D-to-3D localization* establishes correspondences between 2D pixels of
    images and 3D points of a scene model; and *3D-to-3D localization* matches 3D
    scans to a pre-built 3D map. Table [III](#S5.T3 "TABLE III ‣ 5.1 2D-to-2D Localization
    ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence"), [IV](#S5.T4 "TABLE IV ‣ 5.2
    2D-to-3D Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning for
    Localization and Mapping: Towards the Age of Spatial Machine Intelligence") and
    [V](#S5.T5 "TABLE V ‣ 5.2.1 Descriptor Matching Based Localization ‣ 5.2 2D-to-3D
    Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization
    and Mapping: Towards the Age of Spatial Machine Intelligence") summarize the existing
    approaches on deep learning based 2D-to-2D localization, 2D-to-3D localization
    and 3D-to-3D localization respectively.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 2D-to-2D Localization
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2D-to-2D localization regresses the camera pose of an image against a 2D map.
    Such 2D map is explicitly built by a geo-referenced database or implicitly encoded
    in a neural network.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2734e61f17c483e0df298e8e18c4c17f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The typical architectures of 2D-to-2D based localization through
    (a) explict map, i.e. RelocNet [[129](#bib.bib129)] and (b) implicit map, i.e.
    e.g. PoseNet [[130](#bib.bib130)]'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: A summary on existing methods on deep learning for 2D-to-2D global
    localization'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Agnostic | Performance (m/degree) | Contributions |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| 7Scenes | Cambridge |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| 2D-to-2D Localization | Explicit Map | NN-Net [[131](#bib.bib131)] | Yes
    | 0.21/9.30 | - | combine retrieval and relative pose estimation |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| DeLS-3D [[132](#bib.bib132)] | No | - | - | jointly learn with semantics
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| AnchorNet [[133](#bib.bib133)] | Yes | 0.09/6.74 | 0.84/2.10 | anchor point
    allocation |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| RelocNet [[129](#bib.bib129)] | Yes | 0.21/6.73 | - | camera frustum overlap
    loss |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| CamNet [[134](#bib.bib134)] | Yes | 0.04/1.69 | - | multi-stage image retrieval
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| Implicit Map | PoseNet [[130](#bib.bib130)] | No | 0.44/10.44 | 2.09/6.84
    | first neural network in global pose regression |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| Bayesian PoseNet [[135](#bib.bib135)] | No | 0.47/9.81 | 1.92/6.28 | estimate
    Bayesian uncertainty for global pose |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| BranchNet [[136](#bib.bib136)] | No | 0.29/8.30 | - | multi-task learning
    for orientation and translation |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| VidLoc [[137](#bib.bib137)] | No | 0.25/- | - | efficient localization from
    image sequences |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| Geometric PoseNet [[138](#bib.bib138)] | No | 0.23/8.12 | 1.63/2.86 | geometry-aware
    loss |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| SVS-Pose [[139](#bib.bib139)] | No | - | 1.33/5.17 | data augmentation in
    3D space |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| LSTM PoseNet [[140](#bib.bib140)] | No | 0.31/9.85 | 1.30/5.52 | spatial
    correlation |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| Hourglass PoseNet [[141](#bib.bib141)] | No | 0.23/9.53 | - | hourglass-shaped
    architecture |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| VLocNet [[142](#bib.bib142)] | No | 0.05/3.80 | 0.78/2.82 | jointly learn
    global localization and odometry |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| MapNet [[143](#bib.bib143)] | No | 0.21/7.77 | 1.63/3.64 | impose spatial
    and temporal constraints |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| SPP-Net [[144](#bib.bib144)] | No | 0.18/6.20 | 1.24/2.68 | synthetic data
    augmentation |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| GPoseNet [[145](#bib.bib145)] | No | 0.30/9.90 | 2.00/4.60 | hybrid model
    with Gaussian Process Regressor |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| VLocNet++ [[146](#bib.bib146)] | No | 0.02/1.39 | - | jointly learn with
    odometry and semantics |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| LSG [[147](#bib.bib147)] | No | 0.19/7.47 | - | odometry-aided localization
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| PVL [[148](#bib.bib148)] | No | - | 1.60/4.21 | prior-guided dropout mask
    to improve robustness |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| AdPR [[149](#bib.bib149)] | No | 0.22/8.8 | - | adversarial architecture
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| AtLoc [[150](#bib.bib150)] | No | 0.20/7.56 | - | attention-guided spatial
    correlation |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance reports the position (m) and orientation (degree) error (a small
    number is better) on the 7-Scenes (Indoor)[[151](#bib.bib151)] and Cambridge (Outdoor)
    dataset[[130](#bib.bib130)]. Both datasets are split into training and testing
    set. We report the averaged error on the testing set.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.1.1 Explicit Map Based Localization
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Explicit map based 2D-to-2D localization typically represents the scene by
    a database of geo-tagged images (references) [[152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154)]. Figure [8](#S5.F8 "Figure 8 ‣ 5.1 2D-to-2D Localization ‣
    5 Global Localization ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") (a) illustrates the two stages
    of this localization with 2D references: image retrieval determines the most relevant
    part of a scene represented by reference images to the visual queries; pose regression
    obtains the relative pose of query image with respect to the reference images.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: One problem here is how to find suitable image descriptors for image retrieval.
    Deep learning based approaches [[155](#bib.bib155), [156](#bib.bib156)] are based
    on a pre-trained ConvNet model to extract image-level features, and then use these
    features to evaluate the similarities against other images. In challenging situations,
    local descriptors are first extracted, followed by being aggregated to obtain
    robust global descriptors. A good example is NetVLAD [[157](#bib.bib157)] that
    designs a trainable generalized VLAD (the Vector of Locally Aggregated Descriptors)
    layer. This VLAD layer can be plugged into the off-the-shelf ConvNet architecture
    to encourage better descriptors learning for image retrieval.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: In order to obtain more precise poses of the queries, additional relative pose
    estimation with respect to the retrieved images is required. Traditionally, relative
    pose estimation is tackled by epipolar geometry, relying on the 2D-2D correspondences
    determined by local descriptors [[158](#bib.bib158), [159](#bib.bib159)]. In contrast,
    deep learning approaches regress the relative poses straightforwardly from pairwise
    images. For example, NN-Net [[131](#bib.bib131)] utilized neural network to estimate
    the pairwise relative poses between the query and the top N ranked references.
    A triangulation-based fusion algorithm coalesces the predicted N relative poses
    and the ground truth of 3D geometry poses, and the absolute query pose can be
    naturally calculated. Furthermore, Relocnet [[129](#bib.bib129)] introduces a
    frustum overlap loss to assist global descriptors learning that are suitable for
    camera localization. Motivated by these, CamNet [[134](#bib.bib134)] applies two
    stages retrieval, image-based coarse retrieval and pose-based fine retrieval,
    to select the most similar reference frames for finally precise pose estimation.
    Without the need of training on specific scenarios, reference-based approaches
    are naturally scalable and flexible to be utilized in new scenarios. Since reference-based
    methods need to maintain a database of geo-tagged images, they are more trivial
    to scale to large-scale scenarios, compared with the structure-based counterparts.
    Overall, these image retrieval based methods achieve a trade-off between accuracy
    and scalability.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Implicit Map Based Localization
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Implicit map based localization directly regresses camera pose from single
    images, by implicitly representing the structure of entire scene inside a deep
    neural network. The common pipeline is illustrated in Figure [8](#S5.F8 "Figure
    8 ‣ 5.1 2D-to-2D Localization ‣ 5 Global Localization ‣ A Survey on Deep Learning
    for Localization and Mapping: Towards the Age of Spatial Machine Intelligence")
    (b) - the input to a neural network is single images, while the output is the
    global position and orientation of query images.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: PoseNet [[130](#bib.bib130)] is the first work to tackle camera relocalization
    problem by training a ConvNet to predict camera pose from single RGB images in
    an end-to-end manner. PoseNet is based on the main structure of GoogleNet [[160](#bib.bib160)]
    to extract visual features,, but removes the last softmax layers. Instead, a fully
    connected layer was introduced to output a 7 dimensional global pose, consisting
    of position and orientation vector in 3 and 4 dimensions respectively. However,
    PoseNet was designed with a naive regression loss function without any consideration
    for geometry, in which the hyper-parameters inside requires expensive hand-engineering
    to be tuned. Furthermore, it also suffers from the overfitting problem due to
    the high dimensionality of the feature embedding and limited training data. Thus
    various extensions enhance the original pipeline by exploiting LSTM units to reduce
    the dimensionality [[140](#bib.bib140)], applying synthetic generation to augment
    training data [[139](#bib.bib139), [136](#bib.bib136), [144](#bib.bib144)], replacing
    backbone with ResNet34 [[141](#bib.bib141)], modelling pose uncertainty [[135](#bib.bib135),
    [145](#bib.bib145)] and introducing geometry-aware loss function [[138](#bib.bib138)].
    Alternatively, Atloc [[150](#bib.bib150)] associates the features in spatial domain
    with attention mechanism, which encourages the network to focus on parts of the
    image that are temporally consistent and robust. Similarly, a prior guided dropout
    mask is additionally adopted in RVL [[148](#bib.bib148)] to further eliminate
    the uncertainty caused by dynamic objects. Different such methods only considering
    spatial connections, VidLoc [[137](#bib.bib137)] incorporates temporal constraints
    of image sequences to model the temporal connections of input images for visual
    localization. Moreover, additional motion constraints, including spatial constraints
    and other sensor constraints from GPS or SLAM systems are exploited in MapNet
    [[143](#bib.bib143)], to enforce the motion consistency between predicted poses.
    Similar motion constraints are also added by jointly optimizing a relocalization
    network and visual odometry network [[142](#bib.bib142), [147](#bib.bib147)].
    However, being application-specific, scene representations learned from localization
    tasks may ignore some useful features they are not designed for. Out of this,
    VLocNet++ [[146](#bib.bib146)] and FGSN [[161](#bib.bib161)] additionally exploits
    the inter-task relationship among learning semantics and regressing poses, achieving
    impressive results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Implicit map based localization approaches take the advantages of deep learning
    in automatically extracting features, that play a vital role in global localization
    in featureless environments, where conventional methods are prone to fail. However,
    the requirement of scene-specific training prohibits it from generalizing to unseen
    scenes without being retrained. Also, current implicit map based approaches have
    not shown comparable performance over other explicit map based methods [[162](#bib.bib162)].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 2D-to-3D Localization
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '2D-to-3D localization refers to methods that recover the camera pose of a 2D
    image with respect to a 3D scene model. This 3D map is pre-built before performing
    global localization, via approaches such as structure from motion (SfM)[[43](#bib.bib43)].
    As shown in Figure [9](#S5.F9 "Figure 9 ‣ 5.2 2D-to-3D Localization ‣ 5 Global
    Localization ‣ A Survey on Deep Learning for Localization and Mapping: Towards
    the Age of Spatial Machine Intelligence"), 2D-to-3D approaches establish 2D-3D
    correspondences between the 2D pixels of query image and the 3D points of scene
    model through local descriptor matching [[163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165)] or by regressing 3D coordinates from pixel patches [[166](#bib.bib166),
    [167](#bib.bib167), [151](#bib.bib151), [168](#bib.bib168)]. Such 2D-3D matches
    are then used to calculate camera pose by applying a Perspective-n-Point (PnP)
    solver [[169](#bib.bib169), [170](#bib.bib170)] inside a RANSAC loop [[171](#bib.bib171)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/057688a56aed8b968d7a138a0718318c.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The typical architectures of 2D-to-3D based localization through
    (a) descriptor matching, i.e. HF-Net [[172](#bib.bib172)] and (b) scene coordinate
    regression, i.e. Confidence SCR [[173](#bib.bib173)].'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: A summary on existing methods on deep learning for 2D-to-3D global
    localization'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Agnostic | Performance (m/degree) | Contributions |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| 7Scenes | Cambridge |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| 2D-3D Localization | Descriptor Based | NetVLAD [[157](#bib.bib157)] | Yes
    | - | - | differentiable VLAD layer |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| DELF [[174](#bib.bib174)] | Yes | - | - | attentive local feature descriptor
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| InLoc [[175](#bib.bib175)] | Yes | 0.04/1.38 | 0.31/0.73 | dense data association
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| SVL [[176](#bib.bib176)] | No | - | - | leverage a generative model for descriptor
    learning |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| SuperPoint [[177](#bib.bib177)] | Yes | - | - | jointly extract interest
    points and descriptors |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| Sarlin et al. [[178](#bib.bib178)] | Yes | - | - | hierarchical localization
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| NC-Net [[179](#bib.bib179)] | Yes | - | - | neighbourhood consensus constraints
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| 2D3D-MatchNet [[180](#bib.bib180)] | Yes | - | - | jointly learn the descriptors
    for 2D and 3D keypoints |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| Unsuperpoint [[181](#bib.bib181)] | Yes | - | - | unsupervised detector and
    descriptor learning |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| HF-Net [[172](#bib.bib172)] | Yes | - | - | coarse-to-fine localization |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| D2-Net [[182](#bib.bib182)] | Yes | - | - | jointly learn keypoints and descriptors
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| Speciale et al [[183](#bib.bib183)] | No | - | - | privacy preserving localization
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| OOI-Net [[184](#bib.bib184)] | No | - | - | objects-of-interest annotations
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| Camposeco et al. [[185](#bib.bib185)] | Yes | - | 0.56/0.66 | hybrid scene
    compression for localization |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| Cheng et al. [[186](#bib.bib186)] | Yes | - | - | cascaded parallel filtering
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| Taira et al. [[187](#bib.bib187)] | Yes | - | - | comprehensive analysis
    of pose verification |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| R2D2 [[188](#bib.bib188)] | Yes | - | - | learn a predictor of the descriptor
    discriminativeness |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| ASLFeat [[189](#bib.bib189)] | Yes | - | - | leverage deformable convolutional
    networks |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| Scene Coordinate Regression | DSAC [[190](#bib.bib190)] | No | 0.20/6.3 |
    0.32/0.78 | differentiable RANSAC |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| DSAC++ [[191](#bib.bib191)] | No | 0.08/2.40 | 0.19/0.50 | without using
    a 3D model of the scene |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| Angle DSAC++ [[192](#bib.bib192)] | No | 0.06/1.47 | 0.17/0.50 | angle-based
    reprojection loss |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| Dense SCR [[193](#bib.bib193)] | No | 0.04/1.4 | - | full frame scene coordinate
    regression |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| Confidence SCR [[173](#bib.bib173)] | No | 0.06/3.1 | - | model uncertainty
    of correspondences |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| ESAC [[194](#bib.bib194)] | No | 0.034/1.50 | - | integrates DSAC in a Mixture
    of Experts |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| NG-RANSAC [[195](#bib.bib195)] | No | - | 0.24/0.30 | prior-guided model
    hypothesis search |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| SANet [[196](#bib.bib196)] | Yes | 0.05/1.68 | 0.23/0.53 | scene agnostic
    architecture for camera localization |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| MV-SCR [[197](#bib.bib197)] | No | 0.05/1.63 | 0.17/0.40 | multi-view constraints
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| HSC-Net [[198](#bib.bib198)] | No | 0.03/0.90 | 0.13/0.30 | hierarchical
    scene coordinate network |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| KFNet [[199](#bib.bib199)] | No | 0.03/0.88 | 0.13/0.30 | extends the problem
    to the time domain |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance reports the position (m) and orientation (degree) error (a small
    number is better) on the 7-Scenes (Indoor)[[151](#bib.bib151)] and Cambridge (Outdoor)
    dataset[[130](#bib.bib130)]. Both datasets are split into training and testing
    set. We report the averaged error on the testing set.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2.1 Descriptor Matching Based Localization
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Descriptor matching methods mainly rely on feature detector and descriptor,
    and establish the correspondences between the features from 2D input and 3D model.
    They can be further divided into three types: detect-then-describe, detect-and-describe,
    and describe-to-detect, according to the role of detector and descriptor in the
    learning process.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Detect-then-describe approach first performs feature detection and then extracts
    a feature descriptor from a patch centered around each keypoint [[200](#bib.bib200),
    [201](#bib.bib201)]. The keypoint detector is typically responsible for providing
    robustness or invariance against possible real issues such as scale transformation,
    rotation, or viewpoint changes by normalizing the patch accordingly. However,
    some of these responsibilities might also be delegated to the descriptor. The
    common pipeline varies from using hand-crafted detectors [[202](#bib.bib202),
    [203](#bib.bib203)] and descriptors [[204](#bib.bib204), [205](#bib.bib205)],
    replacing either the descriptor [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [179](#bib.bib179), [209](#bib.bib209), [210](#bib.bib210)] or detector [[211](#bib.bib211),
    [212](#bib.bib212), [213](#bib.bib213)] with a learned alternative, or learning
    both the detector and descriptor [[214](#bib.bib214), [215](#bib.bib215)]. For
    efficiency, the feature detector often considers only small image regions and
    typically focuses on low-level structures such as corners or blobs [[216](#bib.bib216)].
    The descriptor then captures higher level information in a larger patch around
    the keypoint.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, detect-and-describe approaches advance description stage. By sharing
    a representation from deep neural network, SuperPoint [[177](#bib.bib177)], UnSuperPoint
    [[181](#bib.bib181)] and R2D2 [[188](#bib.bib188)] attempt to learn a dense feature
    descriptor and a feature detector. However, they rely on different decoder branches
    which are trained independently with specific losses. On the contrary, D2-net
    [[182](#bib.bib182)] and ASLFeat [[189](#bib.bib189)] shares all parameters between
    detection and description and uses a joint formulation that simultaneously optimizes
    for both tasks.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the describe-to-detect approach, e.g. D2D [[217](#bib.bib217)], also
    postpones the detection to a later stage but applies such detector on pre-learned
    dense descriptors to extract a sparse set of keypoints and corresponding descriptors.
    Dense feature extraction foregoes the detection stage and performs the description
    stage densely across the whole image [[218](#bib.bib218), [219](#bib.bib219),
    [220](#bib.bib220), [176](#bib.bib176)]. In practice, this approach has shown
    to lead to better matching results than sparse feature matching, particularly
    under strong variations in illumination [[221](#bib.bib221), [222](#bib.bib222)].
    Different from these works, which purely rely on image features, 2D3D-MatchNet
    [[180](#bib.bib180)] proposed to learn local descriptors that allow direct matching
    of key points across a 2D image and 3D point cloud. Similarly, LCD [[223](#bib.bib223)]
    introduced a dual auto-encoder architecture to extract cross-domain local descriptors.
    However, they still require pre-defined 2D and 3D keypoints separately, which
    will result in poor matching results caused by inconsistent keypoint selection
    rules.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39fe3effac3a73f1698d0c8c617d6715.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The typical architecture of 3D-to-3D localization, e.g. L3-Net [[224](#bib.bib224)].'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: A summary of existing approaches on deep learning for 3D-to-3D localization'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Agnostic | Contributions |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| LocNet[[225](#bib.bib225)] | No | convert 3D points into 2D matrix, search
    in global prior map |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| PointNetVLAD[[226](#bib.bib226)] | Yes | learn global descriptor from point
    clouds |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| Barsan et al.[[227](#bib.bib227)] | No | learn from LIDAR intensity maps
    and online point clouds |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| L3-Net[[224](#bib.bib224)] | No | extract feature by PointNet |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| PCAN[[228](#bib.bib228)] | Yes | predict the significance of each local point
    based on context |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| DeepICP[[229](#bib.bib229)] | Yes | generate matching correspondence from
    learned matching probabilities |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| DCP[[230](#bib.bib230)] | Yes | a learning based iterative closest point
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| D3Feat[[231](#bib.bib231)] | Yes | jointly learn detector and descriptors
    for 3D points |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agnostic indicates whether it can generalize to new scenarios.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributions summarize the main contributions of each work compared with previous
    research.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2.2 Scene Coordinate Regression Based Localization
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from match-based methods that establish 2D-3D correspondences before
    calculating pose, scene coordinate regression approaches estimate the 3D coordinates
    of each pixel from the query image within the world coordinate system, i.e. the
    scene coordinates. It can be viewed as learning a transformation from the query
    image to the global coordinates of the scene. DSAC [[190](#bib.bib190)] utilizes
    a ConvNet model to regress scene coordinates, followed by a novel differentiable
    RANSAC to allow end-to-end training of the whole pipeline. Such common pipeline
    was then improved by introducing the reprojection loss [[191](#bib.bib191), [232](#bib.bib232),
    [192](#bib.bib192)] or multi-view geometric constraints [[197](#bib.bib197)] to
    enable unsupervised learning, jointly learning the observation confidences [[173](#bib.bib173),
    [195](#bib.bib195)] to enhance the sampling efficiency and accuracy, exploiting
    Mixture of Experts (MoE) strategy [[194](#bib.bib194)] or hierarchical coarse-to-fine
    [[198](#bib.bib198)] to eliminate environment ambiguities. Different from these,
    KFNet [[199](#bib.bib199)] extends the scene coordinate regression problem to
    the time domain and thus bridges the existing performance gap between temporal
    and one-shot relocalization approaches. However, they still trained for a specific
    scene and cannot be generalized to unseen scenes without retraining. To build
    a scene agnostic method, SANet [[196](#bib.bib196)] regress the scene coordinate
    map of the query by interpolating the 3D points associated with retrieved scene
    images. Unlike aforementioned methods trained in a patch-based manner, Dense SCR
    [[193](#bib.bib193)] propose to perform the scene coordinate regression in a full-frame
    manner to make the computation efficient at test time and, more importantly, to
    add more global context to the regression process to improve the robustness.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Scene coordinate regression methods often perform better robustness and higher
    accuracy under small indoor scenarios, outperforming traditional algorithms such
    as [[18](#bib.bib18)]. But they have not yet proven their capacity in large-scale
    scenes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 3D-to-3D Localization
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '3D-to-3D localization (or LIDAR localization) refers to methods that recover
    the global pose of 3D points (i.e. LIDAR point cloud scans) against a pre-built
    3D map by establishing a 3D-to-3D correspondence matching. Figure [10](#S5.F10
    "Figure 10 ‣ 5.2.1 Descriptor Matching Based Localization ‣ 5.2 2D-to-3D Localization
    ‣ 5 Global Localization ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") shows the pipeline of 3D-to-3D
    localization: online scans or predicted coarse poses are applied to query the
    most similar 3D map data, which are further used for precise localization by calculating
    the offset between predicted poses and ground truths or estimating the relative
    poses between online scans and queried scene.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: By formulating LIDAR localization as a recursive Bayesian inference problem,
    [[227](#bib.bib227)] embeds both LIDAR intensity maps and online point cloud sweeps
    in a sharing space for fully differentiable pose estimation. Instead of operating
    on 3D data directly, LocNet [[225](#bib.bib225)] converts point cloud scans to
    2D rotational invariant representation for searching similar frames in the global
    prior map, and then performs the iterative closest point (ICP) methods to calculate
    global pose. Towards proposing a learning based LIDAR localization framework that
    directly processes point clouds, L3-Net [[224](#bib.bib224)] processes point cloud
    data with PointNet [[125](#bib.bib125)] to extract feature descriptors that encode
    certain useful properties, and models the temporal connections of motion dynamics
    via a recurrent neural network. It optimizes the loss between the predicted poses
    and ground truth values by minimizing the matching distance between the point
    cloud input and the 3D map. Some techniques, such as PointNetVLAD [[226](#bib.bib226)],
    PCAN [[228](#bib.bib228)] and D3Feat [[231](#bib.bib231)] explored to retrieve
    the reference scene at the beginning, while other techniques such as DeepICP [[229](#bib.bib229)]
    and DCP [[230](#bib.bib230)] allow to estimate relative motion transformations
    from 3D scans. Compared with image-based relocalization including 2D-to-3D and
    2D-to-2D localization, 3D-to-3D localization is relatively underexplored.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 6 SLAM
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Simultaneously tracking self-motion and estimate the structure of surroundings
    constructs a simultaneous localization and mapping (SLAM) system. The individual
    modules of localization and mapping discussed in the above sections can be viewed
    as modules of a complete SLAM systems. This section overviews the SLAM systems
    using deep learning, with the main focus on the modules that contribute to the
    integration of a SLAM system, including local/global optimization, keyframe/loop
    closure detection and uncertainty estimation. Table [VI](#S6.T6 "TABLE VI ‣ 6.1
    Local Optimization ‣ 6 SLAM ‣ A Survey on Deep Learning for Localization and Mapping:
    Towards the Age of Spatial Machine Intelligence") summarizes the existing approaches
    that employ the deep learning based SLAM modules discussed in this section.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Local Optimization
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When jointly optimizing estimated camera motion and scene geometry, SLAM systems
    enforce them to satisfy a certain constraint. This is done by minimizing a geometric
    or photometric loss to ensure their consistency in the local area - the surroundings
    of camera poses, which can be viewed as a bundle adjustment (BA) problem[[233](#bib.bib233)].
    Learning based approaches predict depth maps and ego-motion through two individual
    networks [[29](#bib.bib29)] trained above large datasets. During the testing procedure
    when deployed online, there is a requirement that enforces the predictions to
    satisfy the local constraints. To enable local optimization, traditionally, the
    second-order solvers, e.g. Gauss-Newton (GN) method or Levenberg-Marquadt (LM)
    algorithm [[234](#bib.bib234)], are applied to optimize motion transformations
    and per-pixel depth maps
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: To this end, LS-Net [[235](#bib.bib235)] tackled this problem via a learning
    based optimizer by integrating the analytical solvers into its learning process.
    It learns a data-driven prior, followed by refining the DNN predictions with an
    analytical optimizer to ensure photometric consistency. BA-Net [[236](#bib.bib236)]
    integrates a differentiable second-order optimizer (LM algorithm) into a deep
    neural network to achieve end-to-end learning. Instead of minimizing geometric
    or photometric error, BA-Net is performed on feature space to optimize the consistency
    loss of features from multiview images extracted by ConvNets. This feature-level
    optimizer can mitigate the fundamental problems of geometric or photometric solution,
    i.e. some information may be lost in the geometric optimization, while environmental
    dynamics and lighting changes may impact the photometric optimization). These
    learning based optimizers provide an alternative to solve bundle adjustment problem.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: A summary of existing approaches on deep learning for SLAM'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '| Modules | Employed by |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| Local optimization | [[235](#bib.bib235), [236](#bib.bib236)] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| Global optimization | [[123](#bib.bib123), [237](#bib.bib237), [64](#bib.bib64),
    [238](#bib.bib238)] |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| Keyframe detection | [[77](#bib.bib77)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| Loop-closure detection | [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241),
    [242](#bib.bib242)] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| Uncertainty Estimation | [[243](#bib.bib243), [135](#bib.bib135), [137](#bib.bib137)]
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: 6.2 Global Optimization
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Odometry estimation suffers from the accumulative error drifts during long-term
    operations, due to the fundamental problems of path integration, i.e. the system
    error accumulate without effective restrictions. To address this, graph-SLAM [[42](#bib.bib42)]
    constructs a topological graph to represent camera poses or scene features as
    graph nodes, which are connected by edges (measured by sensors) to constrain the
    poses. This graph-based formulation can be optimized to ensure the global consistency
    of graph nodes and edges, mitigating the possible errors on pose estimates and
    the inherent sensor measurement noise. A popular solver for global optimization
    is through Levenberg-Marquardt (LM) algorithm.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: In the era of deep learning, deep neural networks excel at extracting features,
    and constructing functions from observations to poses and scene representations.
    A global optimization upon the DNN predictions is necessary to reducing the drifts
    of global trajectories and support large-scale mapping. Compared with a variety
    of well-researched solutions in classical SLAM, optimizing deep predictions globally
    is underexplored.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Existing works explored to combine learning modules into a classical SLAM system
    at different levels - in the front-end, DNNs produce predictions as priors, followed
    by incorporating these deep predictions into the back-end for next step optimization
    and refinement. One good example is CNN-SLAM [[123](#bib.bib123)], which utilizes
    the learned per-pixel depths into LSD-SLAM [[124](#bib.bib124)], a full SLAM system
    to support loop closing and graph optimization. Camera poses and scene representations
    are jointly optimized with depth maps to produce consistent scale metrics. In
    DeepTAM [[237](#bib.bib237)], both the depth and pose predictions from deep neural
    networks are introduced into a classical DTAM system [[121](#bib.bib121)], that
    is optimized globally by the back-end to achieve more accurate scene reconstruction
    and camera motion tracking. A similar work can be found on integrating unsupervised
    VO with a graph optimization back-end [[64](#bib.bib64)]. DeepFactors [[238](#bib.bib238)]
    vice versa integrates the learned optimizable scene representation (their so-called
    code representation) into a different style of back-end - probabilistic factor
    graph for global optimization. The advantage of the factor-graph based formulation
    is its flexibility to include sensor measurements, state estimates, and constraints.
    In a factor graph bach-end, it is quite easy and convenient to add new sensor
    modalities, pairwise constraints and system states into the graph for optimization.
    However, these back-end optimizers are not yet differentiable.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Keyframe and Loop-closure Detection
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Detecting keyframe and loop-closing is of key importance to the back-end optimization
    of SLAM systems.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Keyframe selection facilitates SLAM systems to be more efficient. In the key-frame
    based SLAM systems, pose and scene estimates are only refined when a keyframe
    is detected. [[77](#bib.bib77)] provides a learning solution to detect key-frames
    together with unsupervised learning of ego-motion tracking and depths estimation
    [[29](#bib.bib29)]. Whether an image is the keyframe is determined by comparing
    its feature similarity with existing keyframes (i.e. if the similarity is below
    a threshold, this image will be treated as a new keyframe).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Loop-closure detection or place recognition is also an important module in SLAM
    back-end to reduce open-loop errors. Conventional works are based on bag-of-words
    (BoW) to store and use the visual features from the hand-crafted detectors. However,
    this problem is complicated by the changes of illumination, weather, viewpoints
    and moving objects in real-world scenarios. To solve this, previous researchers
    such as [[239](#bib.bib239)] proposed to use the ConvNet features instead, that
    are from a pre-trained model on a generic large-scale image processing dataset.
    These methods are more robust against the variance of viewpoints and conditions
    due to the high-level representations extracted by deep neural networks. Other
    representative works [[240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242)]
    are built on deep auto-encoder structure to extract a compact representation,
    that compresses scene in an unsupervised manner. Deep learning based loop closing
    contributes more robust and effective visual features, and achieves state-of-the-art
    performance in place recognition, which is suitable to be integrated in SLAM systems.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Uncertainty Estimation
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Safety and interpretability are an critical step towards the practical deployment
    of mobile agents in everyday life: the former enables agents to live and act with
    human reliably, while the latter allows users to have better understanding over
    the model behaviours. Although deep learning models achieve state-of-the-art performance
    in a wide range of regression and classification tasks, some corner cases should
    be given enough attention as well. In these failure cases, errors from one component
    will propagate to the other downstream modules, causing catastrophic consequences.
    To this end, there is an emerging need to estimate uncertainty for deep neural
    networks to ensure safety and provide interpretability.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning models usually only produce the mean values of predictions, for
    example, the output of a DNN-based visual odometry model is a 6-dimensional relative
    pose vector, i.e. the translation and rotation. In order to capture the uncertainty
    of deep models, learning models can be augmented into a Bayesian model [[244](#bib.bib244),
    [245](#bib.bib245)]. The uncertainty from Bayesian models is broadly categorized
    into Aleatoric uncertainty and epistemic uncertainty: Aleatoric uncertainty reflects
    observation noises, e.g. sensor measurement or motion noises; epistemic uncertainty
    captures the model uncertainty [[245](#bib.bib245)]. In the context of this survey,
    we focus on the work of estimating uncertainty on the specific task of localization
    and mapping, with regard to their usages, i.e. whether they capture the uncertainty
    with the purpose of motion tracking or scene understanding.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The uncertainty of DNN-based odometry estimation has been explored by [[243](#bib.bib243),
    [246](#bib.bib246)]. They adopted a common strategy to convert the target predictions
    into a Gaussian distribution, conditioned on the mean value of pose estimates
    and its covariance. The parameters inside the framework are optimized via the
    loss function with a combination of mean and covariance. By minimizing the error
    function to find the best combination, the uncertainty is automatically learned
    in an unsupervised fashion. In this way, the uncertainty of motion transformation
    is recovered. The motion uncertainty plays a vital role in probabilistic sensor
    fusion or the back-end optimization of SLAM systems. To validate the effectiveness
    of uncertainty estimation in SLAM systems, [[243](#bib.bib243)] integrated the
    learned uncertainty into a graph-SLAM as the covariances of odometry edges. Based
    on these covariances a global optimization is then performed to reduce system
    drifts. It also confirms that uncertainty estimation improves the performance
    of SLAM systems over the baseline with a fixed predefined value of covariance.
    Similar Bayesian models are applied to the global relocalization problem. As illustrated
    in [[135](#bib.bib135), [137](#bib.bib137)], the uncertainty from deep models
    are able to reflect the global location errors, in which the unreliable pose estimates
    are avoided with this belief metric.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the uncertainty for motion/relocalization, estimating the uncertainty
    for scene understanding also contributes to SLAM systems. This uncertainty offers
    a belief metric in to what extent the environmental perception and scene structure
    should be trusted. For example, in the semantic segmentation and depth estimation
    tasks, uncertainty estimation provides per-pixel uncertainties for the DNN predictions
    [[247](#bib.bib247), [245](#bib.bib245), [248](#bib.bib248), [249](#bib.bib249)].
    Further more, scene uncertainty is applicable to building hybrid SLAM systems.
    For example, photometric uncertainty can be learned to capture the variance of
    intensity on each image pixel, and hence enhances the robustness of SLAM system
    to observation noise [[25](#bib.bib25)].
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 7 Open Questions
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although deep learning has brought great success to the research in localization
    and mapping, as aforementioned, existing models are not sophisticated enough to
    completely solve the problem at hand. The current form of deep solutions is still
    in its infancy state. Towards great autonomy in the wild, there are numerous challenges
    for future researchers to investigate. Practical applications of these techniques
    should be considered a systematic research problem. We discuss several open questions
    that likely lead the further development in this area.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 1) End-to-end model vs. hybrid model. End-to-end learning models are able to
    predict self-motion and scene directly from raw data, without any hand-engineering.
    Benefited from the advances of deep learning, end-to-end models are evolving fast
    to achieve increasing performance in accuracy, efficiency and robustness. Meanwhile,
    these models have been shown easier to be integrated with other high-level learning
    tasks, e.g. path planning and navigation[[31](#bib.bib31)]. Fundamentally, there
    exist underlying physical or geometric models to govern localization and mapping
    systems. Whether we should develop end-to-end models relying only on the power
    of data-driven approaches or integrate deep learning modules into the pre-built
    physical/geometric models as a hybrid model is a critical question for future
    research. As we can see, hybrid models already achieved the state-of-the art results
    in many tasks, e.g. visual odometry[[25](#bib.bib25)], and global localization[[191](#bib.bib191)].
    Thus, it is reasonable to investigate in the way to take better advantage of the
    prior empirical knowledge from deep learning for hybrid models. On the other side,
    pure end-to-end models are data hunger. The performance of current models can
    be limited by the size of training dataset, and it is essential to create large
    and diverse dataset to enlarge the capacity of data-driven models.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 2) Unifying evaluation benchmark and metric. Finding suitable evaluating benchmark
    and metric is always a concern for SLAM systems. This is especially the case for
    DNN based systems. The predictions from DNNs are affected by the characteristics
    of both training and test data, including the dataset size, hyperparameters (batch
    size and learning rate etc.), and the difference in testing scenarios. Therefore,
    it is hard to fairly compare them when considering dataset differences, training/testing
    configuration, or evaluation metric adopted in each work. For example, the KITTI
    dataset is a common choice to evaluate visual odometry, but previous works split
    training and testing data in different ways (e.g. [[24](#bib.bib24), [48](#bib.bib48),
    [50](#bib.bib50)] used Sequence 00, 02, 08, 09 as training set, and Sequence 03,
    04, 05, 06, 07, 10 as testing set, while [[30](#bib.bib30), [25](#bib.bib25)]
    used Sequence 00 - 08 as training, and left 09, and 10 as testing set). Some of
    them are even based on different evaluation metrics (e.g. [[24](#bib.bib24), [48](#bib.bib48),
    [50](#bib.bib50)] applied the KITTI official evaluation metric, while [[29](#bib.bib29),
    [56](#bib.bib56)] applied absolute trajectory error (ATE) as their evaluation
    metric). All these factors bring difficulties to a direct and fair comparison
    across them. Moreover, the KITTI dataset is relatively simple (the vehicle only
    moves in 2D translation) and in small size. It is not convincing if only results
    on KITTI benchmark are provided without a comprehensive evaluation in the long-term
    real-world experiment. In fact, there is a growing need in creating a benchmark
    for a though system evaluation covering various environments, self-motions and
    dynamics.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 3) Real-world deployment. Deploying deep learning models in real-world environments
    is a systematic research problem. In the existing research, the prediction accuracy
    is always their ‘golden rule’ to follow, while other crucial issues are overlooked,
    such as whether the model structure and the parameter number of framework is optimal.
    The computational and energy consumption have to be considered on resource-constrained
    systems, such as low-cost robots or VR wearable devices. The prallerization opportunities,
    such as convolutional filters or other parallel neural network modules should
    be exploited in order to take better use of GPUs. Examples for consideration include
    in which situations the feed-back should be returned to fine-tune the systems,
    how to incorporate the self-supervised models into the systems and whether the
    systems allow the real-time online learning.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 4) Lifelong learning. Most previous works we discussed so far have only been
    validated on simple closed-form dataset, such as visual odometry and depth predictions
    are performed on the KITTI dataset. However, in an open world, the mobile agents
    will confront everchanging environmental factors, and moving dynamics. This will
    require the DNN models to continuously and coherently learn and adapt to the changes
    of the world. Moreover, new concepts and objects will appear unexpectedly, requiring
    an object discovery and new knowledge extension phase for robots.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '5) New sensors: Beyond the common choice of on-board sensors, such as cameras,
    IMU and LIDAR, the emerging new sensors provide an alternative to construct a
    more accurate and robust multimodal system. New sensors including event camera[[250](#bib.bib250)],
    thermo camera[[251](#bib.bib251)], mm-wave device[[252](#bib.bib252)], radio signals[[253](#bib.bib253)],
    magnetic sensor[[254](#bib.bib254)], have distinct properties and data format
    compared to predominant SLAM sensors such as cameras, IMU and LIDAR. Nevertheless,
    the effective learning approaches to processing these unusual sensors are still
    underexplored.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 6) Scalability. Both the learning based localization and mapping models have
    now achieved promising results on the evaluation benchmark. However, they are
    restricted to some scenarios. For example, odometry estimation is always evaluated
    in the city area or on the roads. Whether these techniques could be applied to
    other environments, e.g. rural area or forest area is still an open question.
    Moreover, existing works on scene reconstruction are restricted on single-objects,
    synthetic data or room level. It is worthy exploring whether these learning methods
    are capable of scaling to much more complex and large-scale reconstruction problems.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 7) Safety, reliability and interpretability. Safety and reliability are critical
    to practical applications, e.g. self-driving vehicles. In these scenarios, even
    a small error of pose or scene estimates will cause disasters to the entire system.
    Deep neural networks have been long-critisized as ’black-box’, exacerbating the
    safety concerns for critical tasks. Some initial efforts explored the interpretability
    on deep models [[255](#bib.bib255)]. For example, uncertainty estimation[[244](#bib.bib244),
    [245](#bib.bib245)] can offer a belief metric, representing to what extent we
    trust our models. In this way, the unreliable predictions (with low uncertainty)
    are avoided in order to ensure the systems to stay safe and reliable.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work comprehensively overviews the area of deep learning for localization
    and mapping, and provides a new taxonomy to cover the relevant existing approaches
    from robotics, computer vision and machine learning communities. Learning models
    are incorporated into localization and mapping systems to connect input sensor
    data and target values, by automatically extracting useful features from raw data
    without any human effort. Deep learning based techniques have so far achieved
    the state-of-the-art performance in a variety of tasks, from visual odometry,
    global localization to dense scene reconstruction. Due to the highly expressive
    capacity of deep neural networks, these models are capable of implicitly modelling
    the factors such as environmental dynamics or sensor noises, that are hard to
    be modelled by hand, and thus are relatively more robust in real-world applications.
    In addition, high-level understanding and interaction are easy to perform for
    mobile agents with the learning based framework. The fast development of deep
    learning provides an alternative to solve classical localization and mapping problem
    in a data-driven way, and meanwhile paves the road towards a next-generation AI
    based spatial perception solution.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work is supported by the EPSRC Project “ACE-OPS: From Autonomy to Cognitive
    assistance in Emergency OPerationS” (Grant Number: EP/S030832/1).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. R. Fetsch, A. H. Turner, G. C. DeAngelis, and D. E. Angelaki, “Dynamic
    Reweighting of Visual and Vestibular Cues during Self-Motion Perception,” Journal
    of Neuroscience, vol. 29, no. 49, pp. 15601–15612, 2009.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] K. E. Cullen, “The Vestibular System: Multimodal Integration and Encoding
    of Self-motion for Motor Control,” Trends in Neurosciences, vol. 35, no. 3, pp. 185–196,
    2012.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B. Upcroft,
    P. Abbeel, W. Burgard, M. Milford, and P. Corke, “The Limits and Potentials of
    Deep Learning for Robotics,” International Journal of Robotics Research, vol. 37,
    no. 4-5, pp. 405–420, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] R. Harle, “A Survey of Indoor Inertial Positioning Systems for Pedestrians,”
    IEEE Communications Surveys and Tutorials, vol. 15, no. 3, pp. 1281–1293, 2013.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Gowda, A. Dhekne, S. Shen, R. R. Choudhury, X. Yang, L. Yang, S. Golwalkar,
    and A. Essanian, “Bringing IoT to Sports Analytics,” in USENIX Symposium on Networked
    Systems Design and Implementation (NSDI), 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Wijers, A. Loveridge, D. W. Macdonald, and A. Markham, “Caracal: a versatile
    passive acoustic monitoring tool for wildlife research and conservation,” Bioacoustics,
    pp. 1–17, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Dhekne, A. Chakraborty, K. Sundaresan, and S. Rangarajan, “TrackIO :
    Tracking First Responders Inside-Out,” in USENIX Symposium on Networked Systems
    Design and Implementation (NSDI), 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. J. Davison, “FutureMapping: The Computational Structure of Spatial AI
    Systems,” arXiv, 2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] D. Nister, O. Naroditsky, and J. Bergen, “Visual odometry,” in IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), vol. 1, pp. I–652–I–659
    Vol.1, 2004.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Engel, J. Sturm, and D. Cremers, “Semi-Dense Visual Odometry for a
    Monocular Camera,” in IEEE International Conference on Computer Vision (ICCV),
    pp. 1449–1456, 2013.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] C. Forster, M. Pizzoli, and D. Scaramuzza, “SVO: Fast Semi-Direct Monocular
    Visual Odometry,” in IEEE International Conference on Robotics and Automation
    (ICRA), pp. 15–22, 2014.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Li and A. I. Mourikis, “High-precision, Consistent EKF-based Visual-Inertial
    Odometry,” The International Journal of Robotics Research, vol. 32, no. 6, pp. 690–711,
    2013.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, “Keyframe-Based
    Visual–Inertial Odometry Using Nonlinear Optimization,” The International Journal
    of Robotics Research, vol. 34, no. 3, pp. 314–334, 2015.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, “On-Manifold Preintegration
    for Real-Time Visual-Inertial Odometry,” IEEE Transactions on Robotics, vol. 33,
    no. 1, pp. 1–21, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile Monocular
    Visual-Inertial State Estimator,” IEEE Transactions on Robotics, vol. 34, no. 4,
    pp. 1004–1020, 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Zhang and S. Singh, “LOAM: Lidar Odometry and Mapping in Real-time,”
    in Robotics: Science and Systems, 2010.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Zhang and J. Kosecka, “Image based localization in urban environments.,”
    in International Symposium on 3D Data Processing Visualization and Transmission
    (3DPVT), vol. 6, pp. 33–40, Citeseer, 2006.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Sattler, B. Leibe, and L. Kobbelt, “Fast image-based localization using
    direct 2d-to-3d matching,” in International Conference on Computer Vision (ICCV),
    pp. 667–674, IEEE, 2011.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” IEEE Transactions on Robotics,
    vol. 32, no. 1, pp. 1–19, 2015.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “MonoSLAM: Real-Time
    Single Camera SLAM,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 29, no. 6, pp. 1052–1067, 2007.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. Mur-Artal, J. Montiel, and J. D. Tardos, “ORB-SLAM : A Versatile and
    Accurate Monocular SLAM System,” IEEE Transactions on Robotics, vol. 31, no. 5,
    pp. 1147–1163, 2015.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. C. Longuet-Higgins, “A computer algorithm for reconstructing a scene
    from two projections,” Nature, vol. 293, no. 5828, pp. 133–135, 1981.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] C. Wu, “Towards linear-time incremental structure from motion,” in 2013
    International Conference on 3D Vision-3DV 2013, pp. 127–134, IEEE, 2013.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Wang, R. Clark, H. Wen, and N. Trigoni, “DeepVO : Towards End-to-End
    Visual Odometry with Deep Recurrent Convolutional Neural Networks,” in International
    Conference on Robotics and Automation (ICRA), 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] N. Yang, L. von Stumberg, R. Wang, and D. Cremers, “D3vo: Deep depth,
    deep pose and deep uncertainty for monocular visual odometry,” CVPR, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, “Semanticfusion:
    Dense 3d semantic mapping with convolutional neural networks,” in 2017 IEEE International
    Conference on Robotics and automation (ICRA), pp. 4628–4635, IEEE, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] L. Ma, J. Stückler, C. Kerl, and D. Cremers, “Multi-view deep learning
    for consistent semantic mapping with rgb-d cameras,” in 2017 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 598–605, IEEE, 2017.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press,
    2016.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised Learning
    of Depth and Ego-Motion from Video,” in IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, and I. Reid, “Unsupervised
    scale-consistent depth and ego-motion learning from monocular video,” in Advances
    in Neural Information Processing Systems, pp. 35–45, 2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in 2017 IEEE international conference on robotics and automation (ICRA), pp. 3357–3364,
    IEEE, 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D. Teplyashin,
    K. Simonyan, A. Zisserman, R. Hadsell, et al., “Learning to navigate in cities
    without a map,” in Advances in Neural Information Processing Systems, pp. 2419–2430,
    2018.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,”
    arXiv preprint arXiv:2005.14165, 2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” The International Journal of Robotics Research, vol. 36,
    no. 1, pp. 3–15, 2016.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The apolloscape
    open dataset for autonomous driving and its application,” IEEE transactions on
    pattern analysis and machine intelligence, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev,
    S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and
    D. Anguelov, “Scalability in perception for autonomous driving: Waymo open dataset,”
    2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping:
    part i,” IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99–110, 2006.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] T. Bailey and H. Durrant-Whyte, “Simultaneous localization and mapping
    (slam): Part ii,” IEEE robotics & automation magazine, vol. 13, no. 3, pp. 108–117,
    2006.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous localization
    and mapping: Toward the robust-perception age,” IEEE Transactions on robotics,
    vol. 32, no. 6, pp. 1309–1332, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT press, 2005.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] D. Scaramuzza and F. Fraundorfer, “Visual odometry [tutorial],” IEEE robotics
    & automation magazine, vol. 18, no. 4, pp. 80–92, 2011.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] G. Grisetti, R. Kummerle, C. Stachniss, and W. Burgard, “A tutorial on
    graph-based slam,” IEEE Intelligent Transportation Systems Magazine, vol. 2, no. 4,
    pp. 31–43, 2010.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. R. U. Saputra, A. Markham, and N. Trigoni, “Visual slam and structure
    from motion in dynamic environments: A survey,” ACM Computing Surveys (CSUR),
    vol. 51, no. 2, pp. 1–36, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] K. Konda and R. Memisevic, “Learning Visual Odometry with a Convolutional
    Network,” in International Conference on Computer Vision Theory and Applications,
    pp. 486–490, 2015.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia, “Exploring representation
    learning with cnns for frame-to-frame ego-motion estimation,” IEEE robotics and
    automation letters, vol. 1, no. 1, pp. 18–25, 2015.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The KITTI dataset,” The International Journal of Robotics Research, vol. 32, no. 11,
    pp. 1231–1237, 2013.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Geiger, J. Ziegler, and C. Stiller, “Stereoscan: Dense 3d reconstruction
    in real-time,” in 2011 IEEE Intelligent Vehicles Symposium (IV), pp. 963–968,
    Ieee, 2011.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. R. U. Saputra, P. P. de Gusmao, S. Wang, A. Markham, and N. Trigoni,
    “Learning monocular visual odometry through geometry-aware curriculum learning,”
    in 2019 International Conference on Robotics and Automation (ICRA), pp. 3549–3555,
    IEEE, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. R. U. Saputra, P. P. de Gusmao, Y. Almalioglu, A. Markham, and N. Trigoni,
    “Distilling knowledge from a deep pose regressor network,” in Proceedings of the
    IEEE International Conference on Computer Vision (ICCV), pp. 263–272, 2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, and H. Zha, “Beyond tracking:
    Selecting memory and refining poses for deep visual odometry,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8575–8583,
    2019.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop kf: Learning
    discriminative deterministic state estimators,” in Advances in Neural Information
    Processing Systems, pp. 4376–4384, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] X. Yin, X. Wang, X. Du, and Q. Chen, “Scale recovery for monocular visual
    odometry using depth estimated with deep convolutional neural fields,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 5870–5878, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] R. Li, S. Wang, Z. Long, and D. Gu, “Undeepvo: Monocular visual odometry
    through unsupervised deep learning,” in 2018 IEEE international conference on
    robotics and automation (ICRA), pp. 7286–7291, IEEE, 2018.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] D. Barnes, W. Maddern, G. Pascoe, and I. Posner, “Driven to distraction:
    Self-supervised distractor learning for robust monocular visual odometry in urban
    environments,” in 2018 IEEE International Conference on Robotics and Automation
    (ICRA), pp. 1894–1900, IEEE, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Yin and J. Shi, “GeoNet: Unsupervised Learning of Dense Depth, Optical
    Flow and Camera Pose,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. Reid, “Unsupervised
    Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction,”
    in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 340–349,
    2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable particle filters:
    End-to-end learning with algorithmic priors,” Robotics: Science and Systems, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] N. Yang, R. Wang, J. Stuckler, and D. Cremers, “Deep virtual stereo odometry:
    Leveraging deep depth prediction for monocular direct sparse odometry,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 817–833, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Zhao, L. Sun, P. Purkait, T. Duckett, and R. Stolkin, “Learning monocular
    visual odometry with dense 3d mapping from dense 3d flow,” in 2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 6864–6871, IEEE, 2018.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] V. Casser, S. Pirk, R. Mahjourian, and A. Angelova, “Depth prediction
    without the sensors: Leveraging structure for unsupervised learning from monocular
    videos,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33,
    pp. 8001–8008, 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Almalioglu, M. R. U. Saputra, P. P. de Gusmao, A. Markham, and N. Trigoni,
    “Ganvo: Unsupervised deep monocular visual odometry and depth estimation with
    generative adversarial networks,” in 2019 International Conference on Robotics
    and Automation (ICRA), pp. 5474–5480, IEEE, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Y. Loo, A. J. Amiri, S. Mashohor, S. H. Tang, and H. Zhang, “Cnn-svo:
    Improving the mapping in semi-direct visual odometry using single-image depth
    prediction,” in 2019 International Conference on Robotics and Automation (ICRA),
    pp. 5218–5223, IEEE, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. Wang, S. M. Pizer, and J.-M. Frahm, “Recurrent neural network for (un-)
    supervised learning of monocular video visual odometry and depth,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5555–5564,
    2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Li, Y. Ushiku, and T. Harada, “Pose graph optimization for unsupervised
    monocular visual odometry,” in 2019 International Conference on Robotics and Automation
    (ICRA), pp. 5439–5445, IEEE, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, “Depth from videos
    in the wild: Unsupervised monocular depth learning from unknown cameras,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 8977–8986, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. S. Koumis, J. A. Preiss, and G. S. Sukhatme, “Estimating metric scale
    visual odometry from videos using 3d convolutional networks,” in 2019 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS), pp. 265–272,
    IEEE, 2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Zhan, C. S. Weerasekera, J. Bian, and I. Reid, “Visual odometry revisited:
    What should be learnt?,” The International Conference on Robotics and Automation
    (ICRA), 2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “VINet : Visual-Inertial
    Odometry as a Sequence-to-Sequence Learning Problem,” in The AAAI Conference on
    Artificial Intelligence (AAAI), pp. 3995–4001, 2017.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] E. J. Shamwell, K. Lindgren, S. Leung, and W. D. Nothwang, “Unsupervised
    deep visual-inertial odometry with online error correction for rgb-d imagery,”
    IEEE transactions on pattern analysis and machine intelligence, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, and N. Trigoni,
    “Selective sensor fusion for neural visual-inertial odometry,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10542–10551,
    2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] L. Han, Y. Lin, G. Du, and S. Lian, “Deepvio: Self-supervised deep learning
    of monocular visual inertial odometry using 3d geometric constraints,” arXiv preprint
    arXiv:1906.11435, 2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. Velas, M. Spanel, M. Hradis, and A. Herout, “Cnn for imu assisted odometry
    estimation using velodyne lidar,” in 2018 IEEE International Conference on Autonomous
    Robot Systems and Competitions (ICARSC), pp. 71–77, IEEE, 2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Q. Li, S. Chen, C. Wang, X. Li, C. Wen, M. Cheng, and J. Li, “Lo-net:
    Deep real-time lidar odometry,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 8473–8482, 2019.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] W. Wang, M. R. U. Saputra, P. Zhao, P. Gusmao, B. Yang, C. Chen, A. Markham,
    and N. Trigoni, “Deeppco: End-to-end point cloud odometry through deep parallel
    neural network,” The 2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS 2019), 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Valente, C. Joly, and A. de La Fortelle, “Deep sensor fusion for real-time
    odometry estimation,” 2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Li, F. Xue, X. Wang, Z. Yan, and H. Zha, “Sequential adversarial learning
    for self-supervised deep visual odometry,” in Proceedings of the IEEE International
    Conference on Computer Vision (ICCV), pp. 2851–2860, 2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] L. Sheng, D. Xu, W. Ouyang, and X. Wang, “Unsupervised collaborative learning
    of keyframe detection and visual odometry towards monocular deep slam,” in Proceedings
    of the IEEE International Conference on Computer Vision (ICCV), pp. 4302–4311,
    2019.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in Advances in neural information processing
    systems, pp. 2366–2374, 2014.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5038–5047,
    2017.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised cnn for single
    view depth estimation: Geometry to the rescue,” in European Conference on Computer
    Vision, pp. 740–756, Springer, 2016.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 270–279, 2017.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop KF: Learning
    Discriminative Deterministic State Estimators,” in Advances In Neural Information
    Processing Systems (NeurIPS), 2016.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable Particle Filters:
    End-to-End Learning with Algorithmic Priors,” in Robotics: Science and Systems,
    2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] L. Von Stumberg, V. Usenko, and D. Cremers, “Direct sparse visual-inertial
    odometry using dynamic marginalization,” in 2018 IEEE International Conference
    on Robotics and Automation (ICRA), pp. 2510–2517, IEEE, 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Chen, X. Lu, A. Markham, and N. Trigoni, “Ionet: Learning to cure the
    curse of drift in inertial odometry,” in Thirty-Second AAAI Conference on Artificial
    Intelligence, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] C. Chen, Y. Miao, C. X. Lu, L. Xie, P. Blunsom, A. Markham, and N. Trigoni,
    “Motiontransformer: Transferring neural inertial tracking between domains,” in
    Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 8009–8016,
    2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. A. Esfahani, H. Wang, K. Wu, and S. Yuan, “Aboldeepio: A novel deep
    inertial odometry network for autonomous vehicles,” IEEE Transactions on Intelligent
    Transportation Systems, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] H. Yan, Q. Shan, and Y. Furukawa, “Ridi: Robust imu double integration,”
    in Proceedings of the European Conference on Computer Vision (ECCV), pp. 621–636,
    2018.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] S. Cortés, A. Solin, and J. Kannala, “Deep learning based speed estimation
    for constraining strapdown inertial navigation on smartphones,” in 2018 IEEE 28th
    International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1–6,
    IEEE, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] B. Wagstaff and J. Kelly, “Lstm-based zero-velocity detection for robust
    inertial navigation,” in 2018 International Conference on Indoor Positioning and
    Indoor Navigation (IPIN), pp. 1–8, IEEE, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Brossard, A. Barrau, and S. Bonnabel, “Rins-w: Robust inertial navigation
    system on wheels,” 2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] F. Liu, C. Shen, G. Lin, and I. Reid, “Learning depth from single monocular
    images using deep convolutional neural fields,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 38, no. 10, pp. 2024–2039, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey, “Learning depth
    from monocular videos using direct methods,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2022–2030, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “Surfacenet: An end-to-end
    3d neural network for multiview stereopsis,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 2307–2315, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, and A. Geiger, “Raynet:
    Learning volumetric 3d reconstruction with ray potentials,” in Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3897–3906,
    2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in Advances in neural information processing systems, pp. 365–376, 2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Octree generating networks:
    Efficient convolutional architectures for high-resolution 3d outputs,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 2088–2096, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Häne, S. Tulsiani, and J. Malik, “Hierarchical surface prediction for
    3d object reconstruction,” in 2017 International Conference on 3D Vision (3DV),
    pp. 412–420, IEEE, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Dai, C. Ruizhongtai Qi, and M. Nießner, “Shape completion using 3d-encoder-predictor
    cnns and shape synthesis,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 5868–5877, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger, “Octnetfusion: Learning
    depth fusion from data,” in 2017 International Conference on 3D Vision (3DV),
    pp. 57–66, IEEE, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for
    3d object reconstruction from a single image,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 605–613, 2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché
    approach to learning 3d surface generation,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 216–224, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, “Pixel2mesh:
    Generating 3d mesh models from single rgb images,” in Proceedings of the European
    Conference on Computer Vision (ECCV), pp. 52–67, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] L. Ladicky, O. Saurer, S. Jeong, F. Maninchedda, and M. Pollefeys, “From
    point clouds to mesh using regression,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 3893–3902, 2017.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. Dai and M. Nießner, “Scan2mesh: From unstructured range scans to 3d
    meshes,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 5574–5583, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. Mukasa, J. Xu, and B. Stenger, “3d scene mesh from cnn depth predictions
    and sparse monocular slam,” in Proceedings of the IEEE International Conference
    on Computer Vision Workshops, pp. 921–928, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. Bloesch, T. Laidlow, R. Clark, S. Leutenegger, and A. J. Davison,
    “Learning meshes for dense visual slam,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 5855–5864, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Xiang and D. Fox, “Da-rnn: Semantic mapping with data associated recurrent
    neural networks,” Robotics: Science and Systems, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] N. Sünderhauf, T. T. Pham, Y. Latif, M. Milford, and I. Reid, “Meaningful
    maps with object-oriented semantic mapping,” in 2017 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pp. 5079–5085, IEEE, 2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger, “Fusion++:
    Volumetric object-level slam,” in 2018 international conference on 3D vision (3DV),
    pp. 32–41, IEEE, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart,
    and J. Nieto, “Volumetric instance-aware semantic mapping and 3d object discovery,”
    IEEE Robotics and Automation Letters, vol. 4, no. 3, pp. 3037–3044, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] G. Narita, T. Seno, T. Ishikawa, and Y. Kaji, “Panopticfusion: Online
    volumetric semantic mapping at the level of stuff and things,” IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison,
    “CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM,”
    in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] J. Tobin, W. Zaremba, and P. Abbeel, “Geometry-aware neural rendering,”
    in Advances in Neural Information Processing Systems, pp. 11555–11565, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. H. Lim, P. O. Pinheiro, N. Rostamzadeh, C. Pal, and S. Ahn, “Neural
    multisensory scene inference,” in Advances in Neural Information Processing Systems,
    pp. 8994–9004, 2019.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] V. Sitzmann, M. Zollhöfer, and G. Wetzstein, “Scene representation networks:
    Continuous 3d-structure-aware neural scene representations,” in Advances in Neural
    Information Processing Systems, pp. 1119–1130, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    and K. Kavukcuoglu, “Reinforcement learning with unsupervised auxiliary tasks,”
    ICLR, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al., “Learning to navigate
    in complex environments,” ICLR, 2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] C. Kerl, J. Sturm, and D. Cremers, “Dense visual slam for rgb-d cameras,”
    in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2100–2106,
    IEEE, 2013.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and J. McDonald,
    “Real-time large-scale dense rgb-d slam with volumetric fusion,” The International
    Journal of Robotics Research, vol. 34, no. 4-5, pp. 598–626, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “DTAM : Dense Tracking
    and Mapping in Real-Time,” in IEEE International Conference on Computer Vision
    (ICCV), pp. 2320–2327, 2011.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] K. Karsch, C. Liu, and S. B. Kang, “Depth transfer: Depth extraction
    from video using non-parametric sampling,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 36, no. 11, pp. 2144–2158, 2014.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 6243–6252, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Engel, T. Schöps, and D. Cremers, “Lsd-slam: Large-scale direct monocular
    slam,” in European conference on computer vision, pp. 834–849, Springer, 2014.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in Proceedings of the IEEE
    conference on computer vision and pattern recognition, pp. 652–660, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár, “Panoptic
    segmentation,” in Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 9404–9413, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison,
    P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon, “Kinectfusion: Real-time dense
    surface mapping and tracking,” in 2011 10th IEEE International Symposium on Mixed
    and Augmented Reality, pp. 127–136, IEEE, 2011.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo,
    A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, et al., “Neural scene representation
    and rendering,” Science, vol. 360, no. 6394, pp. 1204–1210, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] V. Balntas, S. Li, and V. Prisacariu, “Relocnet: Continuous metric learning
    relocalisation using neural nets,” in Proceedings of the European Conference on
    Computer Vision (ECCV), pp. 751–767, 2018.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional network
    for real-time 6-dof camera relocalization,” in Proceedings of the IEEE international
    Conference on Computer Vision (ICCV), pp. 2938–2946, 2015.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera relocalization
    by computing pairwise relative poses using convolutional neural network,” in Proceedings
    of the IEEE International Conference on Computer Vision Workshops, pp. 929–938,
    2017.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin, “Dels-3d: Deep localization
    and segmentation with a 3d semantic map,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 5860–5869, 2018.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. Saha, G. Varma, and C. Jawahar, “Improved visual relocalization by
    discovering anchor points,” arXiv preprint arXiv:1811.04370, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] M. Ding, Z. Wang, J. Sun, J. Shi, and P. Luo, “Camnet: Coarse-to-fine
    retrieval for camera re-localization,” in Proceedings of the IEEE International
    Conference on Computer Vision (ICCV), pp. 2871–2880, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Kendall and R. Cipolla, “Modelling uncertainty in deep learning for
    camera relocalization,” in 2016 IEEE international conference on Robotics and
    Automation (ICRA), pp. 4762–4769, IEEE, 2016.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Wu, L. Ma, and X. Hu, “Delving deeper into convolutional neural networks
    for camera relocalization,” in 2017 IEEE International Conference on Robotics
    and Automation (ICRA), pp. 5644–5651, IEEE, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “VidLoc: A Deep
    Spatio-Temporal Model for 6-DoF Video-Clip Relocalization,” in IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Kendall and R. Cipolla, “Geometric loss functions for camera pose
    regression with deep learning,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 5974–5983, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] T. Naseer and W. Burgard, “Deep regression for monocular camera-based
    6-dof global localization in outdoor environments,” in 2017 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 1525–1530, IEEE, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, and
    D. Cremers, “Image-based localization using lstms for structured feature correlation,”
    in Proceedings of the IEEE International Conference on Computer Vision (ICCV),
    pp. 627–637, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu, “Image-based localization
    using hourglass networks,” in Proceedings of the IEEE International Conference
    on Computer Vision Workshops, pp. 879–886, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. Valada, N. Radwan, and W. Burgard, “Deep auxiliary learning for visual
    localization and odometry,” in 2018 IEEE international conference on robotics
    and automation (ICRA), pp. 6939–6946, IEEE, 2018.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz, “Geometry-Aware
    Learning of Maps for Camera Localization,” in IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2616–2625, 2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] P. Purkait, C. Zhao, and C. Zach, “Synthetic view generation for absolute
    pose regression and image synthesis.,” in BMVC, p. 69, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Cai, C. Shen, and I. D. Reid, “A hybrid probabilistic model for camera
    relocalization.,” in BMVC, vol. 1, p. 8, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] N. Radwan, A. Valada, and W. Burgard, “Vlocnet++: Deep multitask learning
    for semantic visual localization and odometry,” IEEE Robotics and Automation Letters,
    vol. 3, no. 4, pp. 4407–4414, 2018.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] F. Xue, X. Wang, Z. Yan, Q. Wang, J. Wang, and H. Zha, “Local supports
    global: Deep camera relocalization with sequence enhancement,” in Proceedings
    of the IEEE International Conference on Computer Vision (ICCV), pp. 2841–2850,
    2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, “Prior guided
    dropout for robust visual localization in dynamic environments,” in Proceedings
    of the IEEE International Conference on Computer Vision (ICCV), pp. 2791–2800,
    2019.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Bui, C. Baur, N. Navab, S. Ilic, and S. Albarqouni, “Adversarial networks
    for camera pose regression and refinement,” in Proceedings of the IEEE International
    Conference on Computer Vision Workshops, pp. 0–0, 2019.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] B. Wang, C. Chen, C. X. Lu, P. Zhao, N. Trigoni, and A. Markham, “Atloc:
    Attention guided camera localization,” arXiv preprint arXiv:1909.03557, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon,
    “Scene coordinate regression forests for camera relocalization in rgb-d images,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2930–2937, 2013.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] R. Arandjelović and A. Zisserman, “Dislocation: Scalable descriptor distinctiveness
    for location recognition,” in Asian Conference on Computer Vision, pp. 188–204,
    Springer, 2014.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] D. M. Chen, G. Baatz, K. Köser, S. S. Tsai, R. Vedantham, T. Pylvänäinen,
    K. Roimela, X. Chen, J. Bach, M. Pollefeys, et al., “City-scale landmark identification
    on mobile devices,” in CVPR 2011, pp. 737–744, IEEE, 2011.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla, “24/7
    place recognition by view synthesis,” in Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 1808–1817, 2015.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Z. Chen, O. Lam, A. Jacobson, and M. Milford, “Convolutional neural network-based
    place recognition,” arXiv preprint arXiv:1411.1509, 2014.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On
    the performance of convnet features for place recognition,” in 2015 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS), pp. 4297–4304, IEEE, 2015.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “Netvlad:
    Cnn architecture for weakly supervised place recognition,” in Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp. 5297–5307, 2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Q. Zhou, T. Sattler, M. Pollefeys, and L. Leal-Taixe, “To learn or not
    to learn: Visual localization from essential matrices,” arXiv preprint arXiv:1908.01293,
    2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, and J. Kannala,
    “Dgc-net: Dense geometric correspondence network,” in 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), pp. 1034–1042, IEEE, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in CVPR, 2015.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Larsson, E. Stenborg, C. Toft, L. Hammarstrand, T. Sattler, and F. Kahl,
    “Fine-grained segmentation networks: Self-supervised segmentation for improved
    long-term visual localization,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 31–41, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] T. Sattler, Q. Zhou, M. Pollefeys, and L. Leal-Taixe, “Understanding
    the limitations of cnn-based absolute camera pose regression,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3302–3312,
    2019.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Y. Li, N. Snavely, and D. P. Huttenlocher, “Location recognition using
    prioritized feature matching,” in European conference on computer vision, pp. 791–804,
    Springer, 2010.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Li, N. Snavely, D. Huttenlocher, and P. Fua, “Worldwide pose estimation
    using 3d point clouds,” in European conference on computer vision, pp. 15–29,
    Springer, 2012.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] B. Zeisl, T. Sattler, and M. Pollefeys, “Camera pose voting for large-scale
    image-based localization,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 2704–2712, 2015.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] T. Cavallari, S. Golodetz, N. A. Lord, J. Valentin, L. Di Stefano, and
    P. H. Torr, “On-the-fly adaptation of regression forests for online camera relocalisation,”
    in Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 4457–4466, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Guzman-Rivera, P. Kohli, B. Glocker, J. Shotton, T. Sharp, A. Fitzgibbon,
    and S. Izadi, “Multi-output learning for camera relocalization,” in Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 1114–1121,
    2014.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] D. Massiceti, A. Krull, E. Brachmann, C. Rother, and P. H. Torr, “Random
    forests versus neural networks—what’s best for camera localization?,” in 2017
    IEEE International Conference on Robotics and Automation (ICRA), pp. 5118–5125,
    IEEE, 2017.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] X.-S. Gao, X.-R. Hou, J. Tang, and H.-F. Cheng, “Complete solution classification
    for the perspective-three-point problem,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 25, no. 8, pp. 930–943, 2003.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] V. Lepetit, F. Moreno-Noguer, and P. Fua, “Epnp: An accurate o (n) solution
    to the pnp problem,” International journal of computer vision, vol. 81, no. 2,
    p. 155, 2009.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm
    for model fitting with applications to image analysis and automated cartography,”
    Communications of the ACM, vol. 24, no. 6, pp. 381–395, 1981.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, “From coarse to
    fine: Robust hierarchical localization at large scale,” in Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 12716–12725, 2019.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] M. Bui, S. Albarqouni, S. Ilic, and N. Navab, “Scene coordinate and correspondence
    learning for image-based localization,” arXiv preprint arXiv:1805.08443, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Large-scale image
    retrieval with attentive deep local features,” in Proceedings of the IEEE international
    conference on computer vision, pp. 3456–3465, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, and A. Torii, “Inloc: Indoor visual localization with dense matching
    and view synthesis,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 7199–7209, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] J. L. Schönberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic
    visual localization,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 6896–6906, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-supervised
    interest point detection and description,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pp. 224–236, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] P.-E. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, and C. Cadena, “Leveraging
    deep visual descriptors for hierarchical efficient localization,” arXiv preprint
    arXiv:1809.01019, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, and J. Sivic,
    “Neighbourhood consensus networks,” in Advances in Neural Information Processing
    Systems, pp. 1651–1662, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] M. Feng, S. Hu, M. H. Ang, and G. H. Lee, “2d3d-matchnet: learning to
    match keypoints across 2d image and 3d point cloud,” in 2019 International Conference
    on Robotics and Automation (ICRA), pp. 4790–4796, IEEE, 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] P. H. Christiansen, M. F. Kragh, Y. Brodskiy, and H. Karstoft, “Unsuperpoint:
    End-to-end unsupervised interest point detector and descriptor,” arXiv preprint
    arXiv:1907.04011, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and
    T. Sattler, “D2-net: A trainable cnn for joint description and detection of local
    features,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 8092–8101, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] P. Speciale, J. L. Schonberger, S. B. Kang, S. N. Sinha, and M. Pollefeys,
    “Privacy preserving image-based localization,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 5493–5503, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] P. Weinzaepfel, G. Csurka, Y. Cabon, and M. Humenberger, “Visual localization
    by learning objects-of-interest dense match regression,” in Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 5634–5643, 2019.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] F. Camposeco, A. Cohen, M. Pollefeys, and T. Sattler, “Hybrid scene compression
    for visual localization,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 7653–7662, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] W. Cheng, W. Lin, K. Chen, and X. Zhang, “Cascaded parallel filtering
    for memory-efficient image-based localization,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 1032–1041, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Taira, I. Rocco, J. Sedlar, M. Okutomi, J. Sivic, T. Pajdla, T. Sattler,
    and A. Torii, “Is this the right place? geometric-semantic pose verification for
    indoor visual localization,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 4373–4383, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    and M. Humenberger, “R2d2: Repeatable and reliable detector and descriptor,” arXiv
    preprint arXiv:1906.06195, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, and
    L. Quan, “Aslfeat: Learning local features of accurate shape and localization,”
    arXiv preprint arXiv:2003.10071, 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold,
    and C. Rother, “Dsac-differentiable ransac for camera localization,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6684–6692,
    2017.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] E. Brachmann and C. Rother, “Learning less is more-6d camera localization
    via 3d surface regression,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 4654–4662, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] X. Li, J. Ylioinas, J. Verbeek, and J. Kannala, “Scene coordinate regression
    with angle-based reprojection loss for camera relocalization,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 0–0, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] X. Li, J. Ylioinas, and J. Kannala, “Full-frame scene coordinate regression
    for image-based localization,” arXiv preprint arXiv:1802.03237, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] E. Brachmann and C. Rother, “Expert sample consensus applied to camera
    re-localization,” in Proceedings of the IEEE International Conference on Computer
    Vision, pp. 7525–7534, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] E. Brachmann and C. Rother, “Neural-guided ransac: Learning where to
    sample model hypotheses,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 4322–4331, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] L. Yang, Z. Bai, C. Tang, H. Li, Y. Furukawa, and P. Tan, “Sanet: Scene
    agnostic network for camera localization,” in Proceedings of the IEEE International
    Conference on Computer Vision (ICCV), pp. 42–51, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] M. Cai, H. Zhan, C. Saroj Weerasekera, K. Li, and I. Reid, “Camera relocalization
    by exploiting multi-view constraints for scene coordinates regression,” in Proceedings
    of the IEEE International Conference on Computer Vision Workshops, pp. 0–0, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] X. Li, S. Wang, Y. Zhao, J. Verbeek, and J. Kannala, “Hierarchical scene
    coordinate classification and regression for visual localization,” in CVPR, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] L. Zhou, Z. Luo, T. Shen, J. Zhang, M. Zhen, Y. Yao, T. Fang, and L. Quan,
    “Kfnet: Learning temporal camera relocalization using kalman filtering,” arXiv
    preprint arXiv:2003.10629, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] K. Mikolajczyk and C. Schmid, “Scale & affine invariant interest point
    detectors,” International journal of computer vision, vol. 60, no. 1, pp. 63–86,
    2004.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] S. Leutenegger, M. Chli, and R. Y. Siegwart, “Brisk: Binary robust invariant
    scalable keypoints,” in 2011 International conference on computer vision, pp. 2548–2555,
    Ieee, 2011.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up robust features,”
    in European conference on computer vision, pp. 404–417, Springer, 2006.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    International journal of computer vision, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] M. Calonder, V. Lepetit, C. Strecha, and P. Fua, “Brief: Binary robust
    independent elementary features,” in European conference on computer vision, pp. 778–792,
    Springer, 2010.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efficient
    alternative to sift or surf,” in 2011 International conference on computer vision,
    pp. 2564–2571, Ieee, 2011.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk, “Learning local feature
    descriptors with triplets and shallow convolutional neural networks.,” in Bmvc,
    vol. 1, p. 3, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    Proceedings of the IEEE International Conference on Computer Vision, pp. 118–126,
    2015.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] K. Simonyan, A. Vedaldi, and A. Zisserman, “Learning local feature descriptors
    using convex optimisation,” IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 36, no. 8, pp. 1573–1585, 2014.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] K. Moo Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua, “Learning
    to find good correspondences,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 2666–2674, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, and E. Trulls, “Beyond cartesian
    representations for local descriptors,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 253–262, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys, “Quad-networks:
    unsupervised learning to rank for interest point detection,” in Proceedings of
    the IEEE conference on computer vision and pattern recognition, pp. 1822–1830,
    2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] L. Zhang and S. Rusinkiewicz, “Learning to detect features in texture
    images,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 6325–6333, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] A. B. Laguna, E. Riba, D. Ponsa, and K. Mikolajczyk, “Key. net: Keypoint
    detection by handcrafted and learned cnn filters,” arXiv preprint arXiv:1904.00889,
    2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Y. Ono, E. Trulls, P. Fua, and K. M. Yi, “Lf-net: learning local features
    from images,” in Advances in neural information processing systems, pp. 6234–6244,
    2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua, “Lift: Learned invariant
    feature transform,” in European Conference on Computer Vision, pp. 467–483, Springer,
    2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] C. G. Harris, M. Stephens, et al., “A combined corner and edge detector.,”
    in Alvey vision conference, vol. 15, pp. 10–5244, Citeseer, 1988.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris, and K. Mikolajczyk,
    “D2d: Keypoint extraction with describe to detect approach,” arXiv preprint arXiv:2005.13605,
    2020.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker, “Universal correspondence
    network,” in Advances in Neural Information Processing Systems, pp. 2414–2422,
    2016.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] M. E. Fathy, Q.-H. Tran, M. Zeeshan Zia, P. Vernaza, and M. Chandraker,
    “Hierarchical metric learning and matching for 2d and 3d geometric correspondences,”
    in Proceedings of the European Conference on Computer Vision (ECCV), pp. 803–819,
    2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] N. Savinov, L. Ladicky, and M. Pollefeys, “Matching neural paths: transfer
    from recognition to correspondence search,” in Advances in Neural Information
    Processing Systems, pp. 1205–1214, 2017.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., “Benchmarking 6dof outdoor
    visual localization in changing conditions,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 8601–8610, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] H. Zhou, T. Sattler, and D. W. Jacobs, “Evaluating local features for
    day-night matching,” in European Conference on Computer Vision, pp. 724–736, Springer,
    2016.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Q.-H. Pham, M. A. Uy, B.-S. Hua, D. T. Nguyen, G. Roig, and S.-K. Yeung,
    “Lcd: Learned cross-domain descriptors for 2d-3d matching,” arXiv preprint arXiv:1911.09326,
    2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] W. Lu, Y. Zhou, G. Wan, S. Hou, and S. Song, “L3-net: Towards learning
    based lidar localization for autonomous driving,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 6389–6398, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] H. Yin, L. Tang, X. Ding, Y. Wang, and R. Xiong, “Locnet: Global localization
    in 3d point clouds for mobile vehicles,” in 2018 IEEE Intelligent Vehicles Symposium
    (IV), pp. 728–733, IEEE, 2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] M. Angelina Uy and G. Hee Lee, “Pointnetvlad: Deep point cloud based
    retrieval for large-scale place recognition,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4470–4479, 2018.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] I. A. Barsan, S. Wang, A. Pokrovsky, and R. Urtasun, “Learning to localize
    using a lidar intensity map.,” in CoRL, pp. 605–616, 2018.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] W. Zhang and C. Xiao, “Pcan: 3d attention map learning using contextual
    information for point cloud based retrieval,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 12436–12445, 2019.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan, and S. Song, “Deepicp: An end-to-end
    deep neural network for 3d point cloud registration,” arXiv preprint arXiv:1905.04153,
    2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Y. Wang and J. M. Solomon, “Deep closest point: Learning representations
    for point cloud registration,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 3523–3532, 2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3feat: Joint
    learning of dense detection and description of 3d local features,” arXiv preprint
    arXiv:2003.03164, 2020.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] E. Brachmann and C. Rother, “Visual camera re-localization from rgb and
    rgb-d images using dsac,” arXiv preprint arXiv:2002.12324, 2020.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle
    adjustment—a modern synthesis,” in International workshop on vision algorithms,
    pp. 298–372, Springer, 1999.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] J. Nocedal and S. Wright, Numerical optimization. Springer Science &
    Business Media, 2006.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, and A. J. Davison,
    “Learning to solve nonlinear least squares for monocular stereo,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 284–299, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] C. Tang and P. Tan, “Ba-net: Dense bundle adjustment network,” International
    Conference on Learning Representations (ICLR), 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] H. Zhou, B. Ummenhofer, and T. Brox, “Deeptam: Deep tracking and mapping
    with convolutional neural networks,” International Journal of Computer Vision,
    vol. 128, no. 3, pp. 756–769, 2020.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] J. Czarnowski, T. Laidlow, R. Clark, and A. J. Davison, “Deepfactors:
    Real-time probabilistic dense monocular slam,” IEEE Robotics and Automation Letters,
    2020.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    and M. Milford, “Place recognition with convnet landmarks: Viewpoint-robust, condition-robust,
    training-free,” Proceedings of Robotics: Science and Systems XII, 2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] X. Gao and T. Zhang, “Unsupervised learning to detect loops using deep
    neural networks for visual slam system,” Autonomous robots, vol. 41, no. 1, pp. 1–18,
    2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] N. Merrill and G. Huang, “Lightweight unsupervised deep loop closure,”
    Robotics: Science and Systems, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] A. R. Memon, H. Wang, and A. Hussain, “Loop closure detection using supervised
    and unsupervised deep neural networks for monocular slam systems,” Robotics and
    Autonomous Systems, vol. 126, p. 103470, 2020.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] S. Wang, R. Clark, H. Wen, and N. Trigoni, “End-to-end, sequence-to-sequence
    probabilistic visual odometry through deep neural networks,” The International
    Journal of Robotics Research, vol. 37, no. 4-5, pp. 513–542, 2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing
    model uncertainty in deep learning,” in international conference on machine learning,
    pp. 1050–1059, 2016.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?,” in Advances in neural information processing systems,
    pp. 5574–5584, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] C. Chen, X. Lu, J. Wahlstrom, A. Markham, and N. Trigoni, “Deep neural
    network based inertial odometry using low-cost inertial measurement units,” IEEE
    Transactions on Mobile Computing, 2019.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] A. Kendall, V. Badrinarayanan, and R. Cipolla, “Bayesian segnet: Model
    uncertainty in deep convolutional encoder-decoder architectures for scene understanding,”
    in British Machine Vision Conference 2017, BMVC 2017, 2017.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] R. McAllister, Y. Gal, A. Kendall, M. Van Der Wilk, A. Shah, R. Cipolla,
    and A. Weller, “Concrete problems for autonomous vehicle safety: advantages of
    bayesian deep learning,” in Proceedings of the 26th International Joint Conference
    on Artificial Intelligence, pp. 4745–4753, AAAI Press, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] M. Klodt and A. Vedaldi, “Supervising the new with the old: learning
    sfm from sfm,” in Proceedings of the European Conference on Computer Vision (ECCV),
    pp. 698–713, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] H. Rebecq, T. Horstschäfer, G. Gallego, and D. Scaramuzza, “Evo: A geometric
    approach to event-based 6-dof parallel tracking and mapping in real time,” IEEE
    Robotics and Automation Letters, vol. 2, no. 2, pp. 593–600, 2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] M. R. U. Saputra, P. P. de Gusmao, C. X. Lu, Y. Almalioglu, S. Rosa,
    C. Chen, J. Wahlström, W. Wang, A. Markham, and N. Trigoni, “Deeptio: A deep thermal-inertial
    odometry with visual hallucination,” IEEE Robotics and Automation Letters, vol. 5,
    no. 2, pp. 1672–1679, 2020.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] C. X. Lu, S. Rosa, P. Zhao, B. Wang, C. Chen, J. A. Stankovic, N. Trigoni,
    and A. Markham, “See through smoke: Robust indoor mapping with low-cost mmwave
    radar,” in Proceedings of the 18th International Conference on Mobile Systems,
    Applications, and Services, MobiSys ’20, (New York, NY, USA), p. 14–27, Association
    for Computing Machinery, 2020.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] B. Ferris, D. Fox, and N. D. Lawrence, “Wifi-slam using gaussian process
    latent variable models.,” in IJCAI, vol. 7, pp. 2480–2485, 2007.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] C. X. Lu, Y. Li, P. Zhao, C. Chen, L. Xie, H. Wen, R. Tan, and N. Trigoni,
    “Simultaneous localization and mapping with power network electromagnetic field,”
    in Proceedings of the 24th annual international conference on mobile computing
    and networking (MobiCom), pp. 607–622, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Q.-s. Zhang and S.-C. Zhu, “Visual interpretability for deep learning:
    a survey,” Frontiers of Information Technology & Electronic Engineering, vol. 19,
    no. 1, pp. 27–39, 2018.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
