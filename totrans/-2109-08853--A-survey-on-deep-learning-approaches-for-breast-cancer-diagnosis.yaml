- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:51:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:51:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.08853] A survey on deep learning approaches for breast cancer diagnosis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.08853] å…³äºä¹³è…ºç™Œè¯Šæ–­çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2109.08853](https://ar5iv.labs.arxiv.org/html/2109.08853)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2109.08853](https://ar5iv.labs.arxiv.org/html/2109.08853)
- en: A survey on deep learning approaches for breast cancer diagnosis
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºä¹³è…ºç™Œè¯Šæ–­çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°
- en: Timothy KwongÂ¹, and Samaneh MazaheriÂ²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Timothy KwongÂ¹ å’Œ Samaneh MazaheriÂ²
- en: Â¹Faculty of Engineering and Applied Science,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹å·¥ç¨‹ä¸åº”ç”¨ç§‘å­¦å­¦é™¢ï¼Œ
- en: Ontario Tech University, Oshawa, ON Canada
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ æ‹¿å¤§å®‰å¤§ç•¥ç†å·¥å¤§å­¦ï¼Œå®‰å¤§ç•¥çœå¥¥æ²™ç“¦
- en: Â²Faculty of Business and Information Technology,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Â²å•†ä¸šä¸ä¿¡æ¯æŠ€æœ¯å­¦é™¢ï¼Œ
- en: Ontario Tech University, Oshawa, ON Canada
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ æ‹¿å¤§å®‰å¤§ç•¥ç†å·¥å¤§å­¦ï¼Œå®‰å¤§ç•¥çœå¥¥æ²™ç“¦
- en: timothy.kwong@ontariotechu.net, Samaneh.Mazaheri@ontariotechu.ca
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: timothy.kwong@ontariotechu.net, Samaneh.Mazaheri@ontariotechu.ca
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Deep learning has introduced several learning-based methods to recognize breast
    tumours and presents high applicability in breast cancer diagnostics. It has presented
    itself as a practical installment in Computer-Aided Diagnostic (CAD) systems to
    further assist radiologists in diagnostics for different modalities. A deep learning
    network trained on images provided by hospitals or public databases can perform
    classification, detection, and segmentation of lesion types. Significant progress
    has been made in recognizing tumours on 2D images but recognizing 3D images remains
    a frontier so far. The interconnection of deep learning networks between different
    fields of study help propels discoveries for more efficient, accurate, and robust
    networks. In this review paper, the following topics will be explored: (i) theory
    and application of deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches
    in breast tumour recognition from a performance metric perspective, and (iii)
    challenges faced in CNN approaches.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ å¼•å…¥äº†å‡ ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•æ¥è¯†åˆ«ä¹³è…ºè‚¿ç˜¤ï¼Œå¹¶åœ¨ä¹³è…ºç™Œè¯Šæ–­ä¸­å±•ç°å‡ºé«˜åº¦çš„é€‚ç”¨æ€§ã€‚å®ƒå·²ç»ä½œä¸ºä¸€ç§å®ç”¨çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿçš„ç»„æˆéƒ¨åˆ†ï¼Œè¿›ä¸€æ­¥å¸®åŠ©æ”¾å°„ç§‘åŒ»å¸ˆåœ¨ä¸åŒçš„å½±åƒæ¨¡å¼ä¸‹è¿›è¡Œè¯Šæ–­ã€‚åŸºäºåŒ»é™¢æˆ–å…¬å…±æ•°æ®åº“æä¾›çš„å›¾åƒè®­ç»ƒçš„æ·±åº¦å­¦ä¹ ç½‘ç»œå¯ä»¥è¿›è¡Œåˆ†ç±»ã€æ£€æµ‹å’Œç—…å˜ç±»å‹çš„åˆ†å‰²ã€‚è™½ç„¶åœ¨2Då›¾åƒè‚¿ç˜¤è¯†åˆ«æ–¹é¢å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†3Då›¾åƒè¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªå‰æ²¿é¢†åŸŸã€‚ä¸åŒç ”ç©¶é¢†åŸŸä¹‹é—´æ·±åº¦å­¦ä¹ ç½‘ç»œçš„äº’è”æœ‰åŠ©äºæ¨åŠ¨æ›´é«˜æ•ˆã€å‡†ç¡®å’Œç¨³å¥ç½‘ç»œçš„å‘ç°ã€‚åœ¨è¿™ç¯‡ç»¼è¿°æ–‡ç« ä¸­ï¼Œå°†æ¢è®¨ä»¥ä¸‹ä¸»é¢˜ï¼šï¼ˆiï¼‰æ·±åº¦å­¦ä¹ çš„ç†è®ºä¸åº”ç”¨ï¼Œï¼ˆiiï¼‰ä»æ€§èƒ½æŒ‡æ ‡è§’åº¦çœ‹2Dã€2.5Då’Œ3Då·ç§¯ç¥ç»ç½‘ç»œåœ¨ä¹³è…ºè‚¿ç˜¤è¯†åˆ«ä¸­çš„è¿›å±•ï¼Œä»¥åŠï¼ˆiiiï¼‰å·ç§¯ç¥ç»ç½‘ç»œæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚
- en: 'Index Terms:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å…³é”®è¯ï¼š
- en: Mammography, Digital Breast Tomosynthesis, Automatic Breast Ultrasound, MRI,
    2D Convolutional Neural Network, 3D Convolutional Neural Network, Classification,
    Detection, Segmentation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹³è…ºXçº¿æ‘„å½±ã€æ•°å­—ä¹³è…ºæ–­å±‚åˆæˆã€è‡ªåŠ¨ä¹³è…ºè¶…å£°ã€MRIã€2Då·ç§¯ç¥ç»ç½‘ç»œã€3Då·ç§¯ç¥ç»ç½‘ç»œã€åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²
- en: I Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I å¼•è¨€
- en: In 2020, female breast cancer had 2.26 million new cases making breast cancer
    the highest number of new cases out of 36 cancer sites [[1](#bib.bib1)]. Moreover,
    the number of new deaths, due to female breast cancer, was 0.684 million, ranking
    it the fourth highest of 35 other cancer sites [[1](#bib.bib1)]. Current modalities
    for breast cancer screenings include mammography, digital breast tomosynthesis,
    breast ultrasound, magnetic resonance imaging [[2](#bib.bib2)]. Mammography has
    two types, screen-film mammography and Digital Mammography (DM), where both types
    are forms of x-ray imaging that use radiation to obtain a 2D image of the breast
    tissue [[2](#bib.bib2), [3](#bib.bib3)]. In addition, mammography has facilitated
    the detection of early stage breast cancer to reduce the risk of cancer death
    [[2](#bib.bib2), [4](#bib.bib4)]. Technological advancements in image acquisition
    had brought Digital Breast Tomosynthesis (DBT). DBT addressed issues in mammography
    and delivered improved image acquisition [[5](#bib.bib5), [3](#bib.bib3), [6](#bib.bib6),
    [7](#bib.bib7)]. It captures multiple 2D images slices of the breast, which are
    then synthesized into a 3D image [[2](#bib.bib2), [6](#bib.bib6), [5](#bib.bib5)].
    However, these 3D images (volumes) are quasi-3D, due to being a reconstruction
    of multiple captured 2D images [[6](#bib.bib6), [7](#bib.bib7)]. Furthermore,
    image slices are captured using a x-ray tube that pivots parallel to the chest
    wall along a 15^âˆ˜ to 60^âˆ˜ arc [[5](#bib.bib5)]. Automatic Breast UltraSound (ABUS)
    uses high frequency to image the entire breast. These 2D images are obtained on
    the transverse plane and synthesized into a 3D volume [[8](#bib.bib8)]. Magnetic
    Resonance Imaging (MRI) uses high-powered magnets along with radio waves generated
    by a computer to image the breast [[2](#bib.bib2)]. CAD systems assist radiologists
    in making diagnostic decisions with higher confidence by providing a "second opinion"
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]. In addition, as mentioned
    by [[12](#bib.bib12)], the CAD system should improve radiologistsâ€™ performance,
    save time, seamlessly integrate with workflow, and not impose liabilities. The
    integration of deep learning algorithms into CAD systems aim to address mentioned
    objectives above, as well as reducing assessment variability from different radiologists
    [[13](#bib.bib13), [14](#bib.bib14)], reducing recall rate, and increasing cancer
    detection rate. The continuous advances in deep learning has brought upon models
    that outperform radiologists in both classification and localization of cancer
    tumors in medical images [[15](#bib.bib15), [16](#bib.bib16)]. As algorithms advance,
    computing power becomes more accessible, and expansive well-curated datasets become
    open-sourced. Machine learning techniques are able to shift towards state-of-the-art,
    and aid in tasks within the healthcare sector [[17](#bib.bib17)]. Deep learning,
    a sub-field of machine learning, conveys representations in simpler forms to solve
    the problem of learning different representations [[18](#bib.bib18)]. Moreover,
    deep learning uses multiple interconnected layers of artificial neurons to learn
    the patterns of simpler expressed forms of the actual representation [[19](#bib.bib19),
    [18](#bib.bib18)]. In 2012, a convolutional neural network architecture scored
    an error rate of 15.3%, which was 10.9% lower than the second-best entry [[20](#bib.bib20)].
    This breakthrough led to an increase in research participation in the field of
    deep learning, and the continuation of research and usage of CNN architecture
    for image recognition problems [[21](#bib.bib21)]. Convolutional Neural Network
    (CNN) are specialized networks to process data with known grid patterns, as well
    as learning spatial hierarchies of features within data [[18](#bib.bib18), [22](#bib.bib22)].
    As a result, hand-crafted features of cancer tumours are not required for CNNs,
    considering as CNNs can learn features. The applicability of deep learning in
    the medical field presents itself through classification, localization, and segmentation
    of cancer tumors in medical images from modalities such as MRI, CT, ultrasound.
    This review paper will provide insight into deep learning theory, progress of
    2D, 2.5D, and 3D CNN architectures, and challenges faced when training a network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2020å¹´ï¼Œå¥³æ€§ä¹³è…ºç™Œçš„æ–°å‘ç—…ä¾‹è¾¾åˆ°226ä¸‡ä¾‹ï¼Œä½¿ä¹³è…ºç™Œæˆä¸º36ç§ç™Œç—‡ä¸­æ–°å‘ç—…ä¾‹æœ€å¤šçš„ [[1](#bib.bib1)]ã€‚æ­¤å¤–ï¼Œç”±äºå¥³æ€§ä¹³è…ºç™Œå¯¼è‡´çš„æ–°æ­»äº¡äººæ•°ä¸º68.4ä¸‡ï¼Œæ’å35ç§å…¶ä»–ç™Œç—‡ä¸­ç¬¬å››é«˜
    [[1](#bib.bib1)]ã€‚ç›®å‰çš„ä¹³è…ºç™Œç­›æŸ¥æ–¹æ³•åŒ…æ‹¬ä¹³è…ºXçº¿æ‘„å½±ã€æ•°å­—ä¹³è…ºå±‚ææˆåƒã€ä¹³è…ºè¶…å£°å’Œç£å…±æŒ¯æˆåƒ [[2](#bib.bib2)]ã€‚ä¹³è…ºXçº¿æ‘„å½±æœ‰ä¸¤ç§ç±»å‹ï¼Œå³å±å¹•èƒ¶å·ä¹³è…ºXçº¿æ‘„å½±å’Œæ•°å­—ä¹³è…ºXçº¿æ‘„å½±ï¼ˆDMï¼‰ï¼Œè¿™ä¸¤ç§ç±»å‹éƒ½æ˜¯Xå°„çº¿æˆåƒå½¢å¼ï¼Œåˆ©ç”¨è¾å°„è·å¾—ä¹³è…ºç»„ç»‡çš„äºŒç»´å›¾åƒ
    [[2](#bib.bib2), [3](#bib.bib3)]ã€‚æ­¤å¤–ï¼Œä¹³è…ºXçº¿æ‘„å½±æœ‰åŠ©äºæ—©æœŸå‘ç°ä¹³è…ºç™Œï¼Œé™ä½ç™Œç—‡æ­»äº¡çš„é£é™© [[2](#bib.bib2),
    [4](#bib.bib4)]ã€‚å›¾åƒè·å–æŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥äº†æ•°å­—ä¹³è…ºå±‚ææˆåƒï¼ˆDBTï¼‰ã€‚DBTè§£å†³äº†ä¹³è…ºXçº¿æ‘„å½±ä¸­çš„é—®é¢˜ï¼Œå¹¶æä¾›äº†æ”¹è¿›çš„å›¾åƒè·å– [[5](#bib.bib5),
    [3](#bib.bib3), [6](#bib.bib6), [7](#bib.bib7)]ã€‚å®ƒæ•è·ä¹³è…ºçš„å¤šä¸ªäºŒç»´å›¾åƒåˆ‡ç‰‡ï¼Œç„¶åå°†è¿™äº›å›¾åƒåˆæˆæˆä¸‰ç»´å›¾åƒ
    [[2](#bib.bib2), [6](#bib.bib6), [5](#bib.bib5)]ã€‚ç„¶è€Œï¼Œè¿™äº›ä¸‰ç»´å›¾åƒï¼ˆä½“ç§¯ï¼‰æ˜¯å‡†ä¸‰ç»´çš„ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å¤šå¼ æ•è·çš„äºŒç»´å›¾åƒçš„é‡å»º
    [[6](#bib.bib6), [7](#bib.bib7)]ã€‚æ­¤å¤–ï¼Œå›¾åƒåˆ‡ç‰‡æ˜¯ä½¿ç”¨ä¸€ä¸ªXå°„çº¿ç®¡æ•è·çš„ï¼Œè¯¥Xå°„çº¿ç®¡æ²¿15^âˆ˜åˆ°60^âˆ˜çš„å¼§çº¿ç»•èƒ¸å£æ—‹è½¬ [[5](#bib.bib5)]ã€‚è‡ªåŠ¨ä¹³è…ºè¶…å£°ï¼ˆABUSï¼‰ä½¿ç”¨é«˜é¢‘ç‡å¯¹æ•´ä¸ªä¹³è…ºè¿›è¡Œæˆåƒã€‚è¿™äº›äºŒç»´å›¾åƒæ˜¯åœ¨æ¨ªæˆªé¢ä¸Šè·å¾—çš„ï¼Œå¹¶åˆæˆæˆä¸‰ç»´ä½“ç§¯
    [[8](#bib.bib8)]ã€‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä½¿ç”¨é«˜åŠŸç‡ç£é“å’Œè®¡ç®—æœºç”Ÿæˆçš„æ— çº¿ç”µæ³¢å¯¹ä¹³è…ºè¿›è¡Œæˆåƒ [[2](#bib.bib2)]ã€‚è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿé€šè¿‡æä¾›â€œç¬¬äºŒæ„è§â€æ¥å¸®åŠ©æ”¾å°„ç§‘åŒ»å¸ˆåšå‡ºæ›´æœ‰ä¿¡å¿ƒçš„è¯Šæ–­å†³ç­–
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]ã€‚æ­¤å¤–ï¼Œå¦‚[[12](#bib.bib12)]æ‰€è¿°ï¼ŒCADç³»ç»Ÿåº”æ”¹å–„æ”¾å°„ç§‘åŒ»å¸ˆçš„è¡¨ç°ï¼ŒèŠ‚çœæ—¶é—´ï¼Œä¸å·¥ä½œæµç¨‹æ— ç¼é›†æˆï¼Œå¹¶ä¸å¢åŠ è´£ä»»ã€‚å°†æ·±åº¦å­¦ä¹ ç®—æ³•é›†æˆåˆ°CADç³»ç»Ÿä¸­ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°ç›®æ ‡ï¼Œå¹¶å‡å°‘ä¸åŒæ”¾å°„ç§‘åŒ»å¸ˆçš„è¯„ä¼°å˜å¼‚æ€§
    [[13](#bib.bib13), [14](#bib.bib14)]ï¼Œé™ä½å¤æŸ¥ç‡ï¼Œæé«˜ç™Œç—‡æ£€æµ‹ç‡ã€‚æ·±åº¦å­¦ä¹ çš„æŒç»­è¿›æ­¥å¸¦æ¥äº†åœ¨åŒ»å­¦å›¾åƒä¸­åˆ†ç±»å’Œå®šä½ç™Œç—‡è‚¿ç˜¤æ–¹é¢ä¼˜äºæ”¾å°„ç§‘åŒ»å¸ˆçš„æ¨¡å‹
    [[15](#bib.bib15), [16](#bib.bib16)]ã€‚éšç€ç®—æ³•çš„è¿›æ­¥ã€è®¡ç®—èƒ½åŠ›çš„æ™®åŠå’Œå¹¿æ³›çš„ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†çš„å¼€æºï¼Œæœºå™¨å­¦ä¹ æŠ€æœ¯èƒ½å¤Ÿå‘æœ€å…ˆè¿›çš„æŠ€æœ¯è½¬å˜ï¼Œå¹¶åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„ä»»åŠ¡ä¸­æä¾›å¸®åŠ©
    [[17](#bib.bib17)]ã€‚æ·±åº¦å­¦ä¹ ä½œä¸ºæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå°†è¡¨ç¤ºä»¥æ›´ç®€å•çš„å½¢å¼ä¼ è¾¾ï¼Œä»¥è§£å†³å­¦ä¹ ä¸åŒè¡¨ç¤ºçš„é—®é¢˜ [[18](#bib.bib18)]ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šä¸ªäº’è”çš„äººå·¥ç¥ç»å…ƒå±‚æ¥å­¦ä¹ å®é™…è¡¨ç¤ºçš„ç®€å•è¡¨è¾¾å½¢å¼çš„æ¨¡å¼
    [[19](#bib.bib19), [18](#bib.bib18)]ã€‚2012å¹´ï¼Œä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œæ¶æ„çš„é”™è¯¯ç‡ä¸º15.3%ï¼Œæ¯”ç¬¬äºŒåä½10.9% [[20](#bib.bib20)]ã€‚è¿™ä¸€çªç ´å¯¼è‡´äº†å¯¹æ·±åº¦å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶å‚ä¸å¢åŠ ï¼Œä»¥åŠç»§ç»­ç ”ç©¶å’Œä½¿ç”¨CNNæ¶æ„è§£å†³å›¾åƒè¯†åˆ«é—®é¢˜
    [[21](#bib.bib21)]ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸“é—¨å¤„ç†å…·æœ‰å·²çŸ¥ç½‘æ ¼æ¨¡å¼çš„æ•°æ®çš„ç½‘ç»œï¼Œå¹¶å­¦ä¹ æ•°æ®ä¸­ç©ºé—´ç‰¹å¾çš„å±‚æ¬¡ç»“æ„ [[18](#bib.bib18),
    [22](#bib.bib22)]ã€‚å› æ­¤ï¼Œè€ƒè™‘åˆ°CNNå¯ä»¥å­¦ä¹ ç‰¹å¾ï¼Œç™Œç—‡è‚¿ç˜¤çš„æ‰‹å·¥ç‰¹å¾ä¸å†æ˜¯CNNæ‰€å¿…éœ€çš„ã€‚æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„é€‚ç”¨æ€§ä½“ç°åœ¨å¯¹åŒ»ç–—å›¾åƒä¸­ç™Œç—‡è‚¿ç˜¤çš„åˆ†ç±»ã€å®šä½å’Œåˆ†å‰²ï¼Œè¿™äº›å›¾åƒæ¥è‡ªMRIã€CTã€è¶…å£°ç­‰æ¨¡æ€ã€‚è¿™ç¯‡ç»¼è¿°æ–‡ç« å°†æ·±å…¥æ¢è®¨æ·±åº¦å­¦ä¹ ç†è®ºã€2Dã€2.5Då’Œ3D
    CNNæ¶æ„çš„å‘å±•ï¼Œä»¥åŠè®­ç»ƒç½‘ç»œæ—¶é‡åˆ°çš„æŒ‘æˆ˜ã€‚
- en: II Deep Learning Theory
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II æ·±åº¦å­¦ä¹ ç†è®º
- en: This section provides a theoretical overview on deep learning concepts, including
    data augmentation, building blocks in a typical CNN architecture, overfitting,
    and transfer learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æä¾›äº†æ·±åº¦å­¦ä¹ æ¦‚å¿µçš„ç†è®ºæ¦‚è¿°ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºã€å…¸å‹CNNæ¶æ„ä¸­çš„æ„å»ºå—ã€è¿‡æ‹Ÿåˆå’Œè¿ç§»å­¦ä¹ ã€‚
- en: II-A Data Augmentation
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A æ•°æ®å¢å¼º
- en: Data augmentation is a technique aimed at increasing the dataset size, and improving
    performance and robustness of the model [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    Data augmentation methods such as translation, rotation, reflection, blur, and
    crop, are applied directly on the original image to generate new augmented images.
    An instance of data augmentation applies each listed method to an original image
    to generate 5 new augmented image, which increases the dataset size with new unseen
    training instances. Image resizing and greyscaling are other strategies used during
    data pre-processing to reduce the computation complexity required to process these
    images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¢å¼ºæ˜¯ä¸€ç§æ—¨åœ¨å¢åŠ æ•°æ®é›†å¤§å°å¹¶æé«˜æ¨¡å‹æ€§èƒ½å’Œé²æ£’æ€§çš„æŠ€æœ¯[[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]ã€‚æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå¦‚å¹³ç§»ã€æ—‹è½¬ã€åå°„ã€æ¨¡ç³Šå’Œè£å‰ªï¼Œç›´æ¥åº”ç”¨äºåŸå§‹å›¾åƒä»¥ç”Ÿæˆæ–°çš„å¢å¼ºå›¾åƒã€‚æ•°æ®å¢å¼ºçš„ä¸€ä¸ªå®ä¾‹æ˜¯å¯¹æ¯ä¸ªåˆ—å‡ºçš„æ–¹æ³•åº”ç”¨äºåŸå§‹å›¾åƒï¼Œä»è€Œç”Ÿæˆ5ä¸ªæ–°çš„å¢å¼ºå›¾åƒï¼Œè¿™æ ·å¯ä»¥é€šè¿‡æ–°çš„æœªè§è®­ç»ƒå®ä¾‹æ¥å¢åŠ æ•°æ®é›†çš„å¤§å°ã€‚å›¾åƒçš„ç¼©æ”¾å’Œç°åº¦åŒ–æ˜¯æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„å…¶ä»–ç­–ç•¥ï¼Œä»¥å‡å°‘å¤„ç†è¿™äº›å›¾åƒæ‰€éœ€çš„è®¡ç®—å¤æ‚æ€§ã€‚
- en: '|  | $s(t)=(x\ast w)(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a)$ |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $s(t)=(x\ast w)(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a)$ |  | (1) |'
- en: '![Refer to caption](img/cb48fc9253e1673e739bef050bbd012a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/cb48fc9253e1673e739bef050bbd012a.png)'
- en: 'Figure 1: A basic Convolutional Neural Network (CNN) extracts features from
    an input to output a feature vector. This CNN contains two layers, the convolution
    and pooling layer. In convolution layer, the entire input is convolved by a kernel,
    while in the pooling layer, the input is down-sampled. The final output is a flattened
    column vector containing significant features of the input. Adapted from [[26](#bib.bib26)].'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šåŸºæœ¬çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»è¾“å…¥ä¸­æå–ç‰¹å¾å¹¶è¾“å‡ºç‰¹å¾å‘é‡ã€‚è¿™ä¸ªCNNåŒ…å«ä¸¤ä¸ªå±‚ï¼Œå·ç§¯å±‚å’Œæ± åŒ–å±‚ã€‚åœ¨å·ç§¯å±‚ä¸­ï¼Œæ•´ä¸ªè¾“å…¥è¢«ä¸€ä¸ªå·ç§¯æ ¸è¿›è¡Œå·ç§¯ï¼Œè€Œåœ¨æ± åŒ–å±‚ä¸­ï¼Œè¾“å…¥è¢«ä¸‹é‡‡æ ·ã€‚æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«è¾“å…¥é‡è¦ç‰¹å¾çš„æ‰å¹³åˆ—å‘é‡ã€‚æ”¹ç¼–è‡ª[[26](#bib.bib26)]ã€‚
- en: II-B 2D Convolutional Layer
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 2D å·ç§¯å±‚
- en: Convolutional neural networks use a mathematical operation called convolution;
    a convolution is a linear operation used, in this case, for feature extraction
    [[18](#bib.bib18), [22](#bib.bib22)]. Discrete convolution is expressed as equ.
    ([1](#S2.E1 "Equation 1 â€£ II-A Data Augmentation â€£ II Deep Learning Theory â€£ A
    survey on deep learning approaches for breast cancer diagnosis")), as seen in
    [[18](#bib.bib18), equ. (9.1)], where s(t) represents the feature map, x(a) represents
    the input, and w(t-a) represents the kernel. Moreover, equ. ([1](#S2.E1 "Equation
    1 â€£ II-A Data Augmentation â€£ II Deep Learning Theory â€£ A survey on deep learning
    approaches for breast cancer diagnosis")) illustrates an element-wise product
    between an input and a kernel to produce a feature map [[22](#bib.bib22), [27](#bib.bib27)].
    The 2D convolutional kernel is a matrix of weights that extracts meaningful features
    from the input for the network to learn and recognize different inputs. A feature
    maps can be generated through convolving a kernel with an input, then applying
    an activation function on the convolved output [[23](#bib.bib23)]. In addition,
    backpropagation is used to update the kernel weights to minimize the loss function
    [[22](#bib.bib22)]. Stride, as defined by [[22](#bib.bib22)], is "the distance
    between two successive kernel positions", which dictates the step size of the
    kernel across the input. Padding is a technique used to retain the in-plane dimensionality
    of the feature map even after a convolution further permitting more convolutional
    layers [[22](#bib.bib22), [27](#bib.bib27)]. Zero-padding aligns the border of
    the input with zeros to retain the dimensionality [[28](#bib.bib28)]. Parameter
    sharing is a mechanism used in CNN to limit the number of parameters by sharing
    the kernel weights, which ultimately reduces the model complexity [[22](#bib.bib22),
    [23](#bib.bib23), [28](#bib.bib28)]. In addition, parameters can be shared among
    more abstract features that occur within different images [[28](#bib.bib28), [29](#bib.bib29)].
    Figure [1](#S2.F1 "Figure 1 â€£ II-A Data Augmentation â€£ II Deep Learning Theory
    â€£ A survey on deep learning approaches for breast cancer diagnosis") illustrates
    the feature extraction on the input by the convolution and pooling layer to output
    a feature map.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œä½¿ç”¨ä¸€ç§å«åšå·ç§¯çš„æ•°å­¦è¿ç®—ï¼›å·ç§¯æ˜¯ä¸€ç§çº¿æ€§æ“ä½œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ç”¨äºç‰¹å¾æå– [[18](#bib.bib18), [22](#bib.bib22)]ã€‚ç¦»æ•£å·ç§¯è¡¨ç¤ºä¸ºå…¬å¼
    ([1](#S2.E1 "æ–¹ç¨‹ 1 â€£ II-A æ•°æ®å¢å¼º â€£ II æ·±åº¦å­¦ä¹ ç†è®º â€£ ä¹³è…ºç™Œè¯Šæ–­æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°"))ï¼Œå¦‚ [[18](#bib.bib18),
    å…¬å¼ (9.1)] ä¸­æ‰€ç¤ºï¼Œå…¶ä¸­ s(t) è¡¨ç¤ºç‰¹å¾å›¾ï¼Œx(a) è¡¨ç¤ºè¾“å…¥ï¼Œw(t-a) è¡¨ç¤ºå·ç§¯æ ¸ã€‚æ­¤å¤–ï¼Œå…¬å¼ ([1](#S2.E1 "æ–¹ç¨‹ 1 â€£ II-A
    æ•°æ®å¢å¼º â€£ II æ·±åº¦å­¦ä¹ ç†è®º â€£ ä¹³è…ºç™Œè¯Šæ–­æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°")) è¯´æ˜äº†è¾“å…¥ä¸å·ç§¯æ ¸ä¹‹é—´çš„é€å…ƒç´ ä¹˜ç§¯ä»¥ç”Ÿæˆç‰¹å¾å›¾ [[22](#bib.bib22),
    [27](#bib.bib27)]ã€‚2D å·ç§¯æ ¸æ˜¯ä¸€ä¸ªæƒé‡çŸ©é˜µï¼Œç”¨äºä»è¾“å…¥ä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œä»¥ä¾¿ç½‘ç»œå­¦ä¹ å’Œè¯†åˆ«ä¸åŒçš„è¾“å…¥ã€‚é€šè¿‡å°†å·ç§¯æ ¸ä¸è¾“å…¥è¿›è¡Œå·ç§¯ï¼Œç„¶ååœ¨å·ç§¯è¾“å‡ºä¸Šåº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥ç”Ÿæˆç‰¹å¾å›¾
    [[23](#bib.bib23)]ã€‚æ­¤å¤–ï¼Œåå‘ä¼ æ’­ç”¨äºæ›´æ–°å·ç§¯æ ¸æƒé‡ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•° [[22](#bib.bib22)]ã€‚æ­¥å¹…ï¼Œå¦‚ [[22](#bib.bib22)]
    æ‰€å®šä¹‰ï¼Œæ˜¯â€œä¸¤ä¸ªè¿ç»­å·ç§¯æ ¸ä½ç½®ä¹‹é—´çš„è·ç¦»â€ï¼Œå†³å®šäº†å·ç§¯æ ¸åœ¨è¾“å…¥ä¸Šçš„æ­¥é•¿ã€‚å¡«å……æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œç”¨äºåœ¨å·ç§¯åä¿æŒç‰¹å¾å›¾çš„å¹³é¢ç»´åº¦ï¼Œä»è€Œå…è®¸æ›´å¤šçš„å·ç§¯å±‚ [[22](#bib.bib22),
    [27](#bib.bib27)]ã€‚é›¶å¡«å……é€šè¿‡åœ¨è¾“å…¥è¾¹ç•Œå¯¹é½é›¶æ¥ä¿æŒç»´åº¦ [[28](#bib.bib28)]ã€‚å‚æ•°å…±äº«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œä¸­ä¸€ç§é€šè¿‡å…±äº«å·ç§¯æ ¸æƒé‡æ¥é™åˆ¶å‚æ•°æ•°é‡çš„æœºåˆ¶ï¼Œè¿™æœ€ç»ˆå‡å°‘äº†æ¨¡å‹çš„å¤æ‚æ€§
    [[22](#bib.bib22), [23](#bib.bib23), [28](#bib.bib28)]ã€‚æ­¤å¤–ï¼Œå‚æ•°å¯ä»¥åœ¨ä¸åŒå›¾åƒä¸­å‡ºç°çš„æ›´æŠ½è±¡ç‰¹å¾ä¹‹é—´å…±äº«
    [[28](#bib.bib28), [29](#bib.bib29)]ã€‚å›¾ [1](#S2.F1 "å›¾ 1 â€£ II-A æ•°æ®å¢å¼º â€£ II æ·±åº¦å­¦ä¹ ç†è®º
    â€£ ä¹³è…ºç™Œè¯Šæ–­æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°") è¯´æ˜äº†å·ç§¯å’Œæ± åŒ–å±‚å¯¹è¾“å…¥è¿›è¡Œç‰¹å¾æå–ä»¥è¾“å‡ºç‰¹å¾å›¾ã€‚
- en: II-C 3D Convolutional Layer
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 3D å·ç§¯å±‚
- en: In a 3D CNN, the kernels, stride, and pooling operation are three dimensional,
    where the third dimension represents a depth dimension [[30](#bib.bib30)]. This
    additional dimension allows 3D CNNs to extract features from an additional axis
    of information. In 3D convolutional layers, voxel represents the spatial information
    rather than pixels.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ 3D CNN ä¸­ï¼Œå·ç§¯æ ¸ã€æ­¥å¹…å’Œæ± åŒ–æ“ä½œéƒ½æ˜¯ä¸‰ç»´çš„ï¼Œå…¶ä¸­ç¬¬ä¸‰ç»´åº¦è¡¨ç¤ºæ·±åº¦ç»´åº¦ [[30](#bib.bib30)]ã€‚è¿™ä¸ªé¢å¤–çš„ç»´åº¦ä½¿å¾— 3D CNN
    èƒ½å¤Ÿä»é¢å¤–çš„ä¿¡æ¯è½´ä¸­æå–ç‰¹å¾ã€‚åœ¨ 3D å·ç§¯å±‚ä¸­ï¼Œä½“ç´ è¡¨ç¤ºç©ºé—´ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åƒç´ ã€‚
- en: '![Refer to caption](img/a6171bd3a26e3a1fd02fd96235bcd3b0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/a6171bd3a26e3a1fd02fd96235bcd3b0.png)'
- en: 'Figure 2: The Rectified Linear Unit (ReLU) function. Adapted from [[31](#bib.bib31)].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰å‡½æ•°ã€‚æ”¹ç¼–è‡ª [[31](#bib.bib31)]ã€‚
- en: II-D Activation Layer
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D æ¿€æ´»å±‚
- en: 'The Rectified Linear Unit (ReLU) is an activation function commonly used in
    neural networks [[22](#bib.bib22), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)].
    The ReLU function, as shown in the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰æ˜¯ä¸€ç§å¸¸ç”¨äºç¥ç»ç½‘ç»œçš„æ¿€æ´»å‡½æ•° [[22](#bib.bib22), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34)]ã€‚ReLU å‡½æ•°å¦‚ä¸‹é¢æ‰€ç¤ºï¼š
- en: '|  | <math   alttext="f(x)=max(0,x)=\begin{cases}x,&amp;x\geq 0\\ 0,&amp;x<0\\'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="f(x)=max(0,x)=\begin{cases}x,&amp;x\geq 0\\ 0,&amp;x<0\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mi >f</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi >x</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo  >=</mo><mrow ><mi  >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi >x</mi><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mn >0</mn><mo >,</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><mrow ><mi  >x</mi><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  >â‰¥</mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mn  >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  ><</mo><mn
    >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply  ><ci >ğ‘“</ci><ci >ğ‘¥</ci></apply><apply  ><ci >ğ‘š</ci><ci
    >ğ‘</ci><ci  >ğ‘¥</ci><interval closure="open"  ><cn type="integer" >0</cn><ci  >ğ‘¥</ci></interval></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >cases</csymbol><ci >ğ‘¥</ci><apply ><ci  >ğ‘¥</ci><cn
    type="integer"  >0</cn></apply><cn type="integer"  >0</cn><apply ><ci >ğ‘¥</ci><cn
    type="integer" >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f(x)=max(0,x)=\begin{cases}x,&x\geq 0\\ 0,&x<0\\
    \end{cases}</annotation></semantics></math> |  | (2) |
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mi >f</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi >x</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo  >=</mo><mrow ><mi  >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi >x</mi><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mn >0</mn><mo >,</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><mrow ><mi  >x</mi><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  >â‰¥</mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mn  >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  ><</mo><mn
    >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply  ><ci >ğ‘“</ci><ci >ğ‘¥</ci></apply><apply  ><ci >ğ‘š</ci><ci
    >ğ‘</ci><ci  >ğ‘¥</ci><interval closure="open"  ><cn type="integer" >0</cn><ci  >ğ‘¥</ci></interval></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >cases</csymbol><ci >ğ‘¥</ci><apply ><ci  >ğ‘¥</ci><cn
    type="integer"  >0</cn></apply><cn type="integer"  >0</cn><apply ><ci >ğ‘¥</ci><cn
    type="integer" >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f(x)=max(0,x)=\begin{cases}x,&x\geq 0\\ 0,&x<0\\
    \end{cases}</annotation></semantics></math> |  | (2) |
- en: and can be seen in [[34](#bib.bib34), equ. (1.14)], equates values less than
    zero to zero, and values greater than or equal to zero to passed in value. A plot
    of the ReLU function is depicted in Figure [2](#S2.F2 "Figure 2 â€£ II-C 3D Convolutional
    Layer â€£ II Deep Learning Theory â€£ A survey on deep learning approaches for breast
    cancer diagnosis").
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å¯ä»¥åœ¨ [[34](#bib.bib34), ç­‰å¼ (1.14)] ä¸­çœ‹åˆ°ï¼Œå°†å°äºé›¶çš„å€¼å½’é›¶ï¼Œå°†å¤§äºæˆ–ç­‰äºé›¶çš„å€¼ä¿æŒä¸å˜ã€‚ReLU å‡½æ•°çš„å›¾ç¤ºè§å›¾
    [2](#S2.F2 "å›¾ 2 â€£ II-C 3D å·ç§¯å±‚ â€£ II æ·±åº¦å­¦ä¹ ç†è®º â€£ å…³äºä¹³è…ºç™Œè¯Šæ–­çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°")ã€‚
- en: '![Refer to caption](img/6ec99838f30d83388998c7a3e8003524.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜æ–‡å­—](img/6ec99838f30d83388998c7a3e8003524.png)'
- en: 'Figure 3: Fully-Connected Neural Network (FCNN). Adapted from [[26](#bib.bib26)].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFCNNï¼‰ã€‚æ”¹ç¼–è‡ª [[26](#bib.bib26)]ã€‚
- en: II-E Pooling Layer
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E æ± åŒ–å±‚
- en: Pooling is a technique to down-sample feature maps [[22](#bib.bib22)], introduces
    invariance [[18](#bib.bib18)], and merge semantically similar features [[18](#bib.bib18)].
    Down-sampling a feature map reduces the in-plane dimensionality [[22](#bib.bib22),
    [35](#bib.bib35)], which reduces the data size without reducing key features in
    the feature map required for learning. Max pooling is a pooling operation, which
    obtains the maximum value within a square region [[18](#bib.bib18), [22](#bib.bib22),
    [35](#bib.bib35)]. Moreover, by obtaining the maximum value, this also makes a
    representation invariant to small translation or distortions [[18](#bib.bib18),
    [22](#bib.bib22)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ± åŒ–æ˜¯ä¸€ç§ä¸‹é‡‡æ ·ç‰¹å¾å›¾çš„æŠ€æœ¯ [[22](#bib.bib22)]ï¼Œå¼•å…¥ä¸å˜æ€§ [[18](#bib.bib18)]ï¼Œå¹¶åˆå¹¶è¯­ä¹‰ä¸Šç›¸ä¼¼çš„ç‰¹å¾ [[18](#bib.bib18)]ã€‚ä¸‹é‡‡æ ·ç‰¹å¾å›¾å‡å°‘äº†å¹³é¢å†…çš„ç»´åº¦
    [[22](#bib.bib22), [35](#bib.bib35)]ï¼Œä»è€Œåœ¨ä¸å‡å°‘ç‰¹å¾å›¾ä¸­ç”¨äºå­¦ä¹ çš„å…³é”®ç‰¹å¾çš„æƒ…å†µä¸‹å‡å°‘äº†æ•°æ®å¤§å°ã€‚æœ€å¤§æ± åŒ–æ˜¯ä¸€ç§æ± åŒ–æ“ä½œï¼Œåœ¨æ–¹å½¢åŒºåŸŸå†…è·å–æœ€å¤§å€¼
    [[18](#bib.bib18), [22](#bib.bib22), [35](#bib.bib35)]ã€‚æ­¤å¤–ï¼Œé€šè¿‡è·å–æœ€å¤§å€¼ï¼Œè¿™ä¹Ÿä½¿è¡¨ç¤ºå¯¹å°çš„å¹³ç§»æˆ–æ‰­æ›²å…·æœ‰ä¸å˜æ€§
    [[18](#bib.bib18), [22](#bib.bib22)]ã€‚
- en: '![Refer to caption](img/a48dd7396b3d2efdfd1b4c03cd52b894.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/a48dd7396b3d2efdfd1b4c03cd52b894.png)'
- en: 'Figure 4: The surface of this 3-dimensional graph represents the objective
    function. Gradient descent is used to traverse towards the deepest point on this
    graph. The deepest point represents the global minimum, and parameters that minimize
    the objective function. Source: Adapted using [[31](#bib.bib31)]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šè¿™ä¸ªä¸‰ç»´å›¾çš„è¡¨é¢è¡¨ç¤ºç›®æ ‡å‡½æ•°ã€‚æ¢¯åº¦ä¸‹é™ç”¨äºåœ¨æ­¤å›¾ä¸Šå‘æœ€æ·±ç‚¹ç§»åŠ¨ã€‚æœ€æ·±ç‚¹è¡¨ç¤ºå…¨å±€æœ€å°å€¼ï¼Œä»¥åŠä½¿ç›®æ ‡å‡½æ•°æœ€å°åŒ–çš„å‚æ•°ã€‚æ¥æºï¼šæ”¹ç¼–è‡ª [[31](#bib.bib31)]ã€‚
- en: II-F Fully-Connected Layer
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F å®Œå…¨è¿æ¥å±‚
- en: 'The Fully Connected (FC) layer learns a non-linear function to map all the
    features within a feature space. Figure [3](#S2.F3 "Figure 3 â€£ II-D Activation
    Layer â€£ II Deep Learning Theory â€£ A survey on deep learning approaches for breast
    cancer diagnosis") gives an illustration of the basic structure for a Fully-Connected
    Neural Network (FCNN). A CNN can have a FC layer, where the features extracted
    from the CNN are inputted into the FC layer for a decision output [[36](#bib.bib36)].
    During training, the goal is to minimize the prediction error made by the CNN,
    techniques such as back-propagation and gradient descent are used to improve prediction
    results. The objective function, also known as the loss function or cost function,
    determines the difference between the prediction and ground truth; it measures
    the network error in prediction. Binary cross-entropy is a loss function used
    in binary classification. Back-propagation is a technique used to determine the
    gradient of the objective function with respect to the weights [[32](#bib.bib32)].
    The equation for back-propagation of an objective function with respect to the
    weight can be calculated using chain rule and is given by the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨è¿æ¥ï¼ˆFCï¼‰å±‚å­¦ä¹ ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œä»¥æ˜ å°„ç‰¹å¾ç©ºé—´ä¸­çš„æ‰€æœ‰ç‰¹å¾ã€‚å›¾ [3](#S2.F3 "å›¾ 3 â€£ II-D æ¿€æ´»å±‚ â€£ II æ·±åº¦å­¦ä¹ ç†è®º â€£
    å…³äºä¹³è…ºç™Œè¯Šæ–­çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°") å±•ç¤ºäº†å®Œå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFCNNï¼‰çš„åŸºæœ¬ç»“æ„ã€‚CNN å¯ä»¥åŒ…å« FC å±‚ï¼Œå…¶ä¸­ä» CNN ä¸­æå–çš„ç‰¹å¾è¾“å…¥åˆ° FC å±‚ä»¥è¿›è¡Œå†³ç­–è¾“å‡º
    [[36](#bib.bib36)]ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ– CNN çš„é¢„æµ‹è¯¯å·®ï¼Œä½¿ç”¨åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™ç­‰æŠ€æœ¯æ¥æ”¹å–„é¢„æµ‹ç»“æœã€‚ç›®æ ‡å‡½æ•°ï¼Œä¹Ÿç§°ä¸ºæŸå¤±å‡½æ•°æˆ–ä»£ä»·å‡½æ•°ï¼Œç¡®å®šé¢„æµ‹ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼›å®ƒè¡¡é‡ç½‘ç»œåœ¨é¢„æµ‹ä¸­çš„è¯¯å·®ã€‚äºŒå…ƒäº¤å‰ç†µæ˜¯ç”¨äºäºŒåˆ†ç±»çš„æŸå¤±å‡½æ•°ã€‚åå‘ä¼ æ’­æ˜¯ä¸€ç§ç”¨äºç¡®å®šç›¸å¯¹äºæƒé‡çš„ç›®æ ‡å‡½æ•°æ¢¯åº¦çš„æŠ€æœ¯
    [[32](#bib.bib32)]ã€‚ç›¸å¯¹äºæƒé‡çš„ç›®æ ‡å‡½æ•°åå‘ä¼ æ’­çš„æ–¹ç¨‹å¯ä»¥ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
- en: '|  | $\frac{\partial{L}}{\partial{w}}=\frac{\partial{z}}{\partial{w}}\frac{\partial{a}}{\partial{z}}\frac{\partial{L}}{\partial{w}}$
    |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial{L}}{\partial{w}}=\frac{\partial{z}}{\partial{w}}\frac{\partial{a}}{\partial{z}}\frac{\partial{L}}{\partial{w}}$
    |  | (3) |'
- en: 'Gradient descent is used to minimize the objective function through iterative
    updates of the parameters, such as weights, bias, and kernels, in the negative
    direction of the gradient of the objective function [[22](#bib.bib22), [37](#bib.bib37)].
    The respected equation is given by:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ç”¨äºé€šè¿‡å¯¹å‚æ•°ï¼ˆå¦‚æƒé‡ã€åç½®å’Œæ ¸ï¼‰çš„è¿­ä»£æ›´æ–°ï¼Œåœ¨ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦è´Ÿæ–¹å‘ä¸Šæœ€å°åŒ–ç›®æ ‡å‡½æ•° [[22](#bib.bib22), [37](#bib.bib37)]ã€‚ç›¸å…³æ–¹ç¨‹ä¸ºï¼š
- en: '|  | $w:=w-\alpha\frac{\partial{L}}{\partial{w}}$ |  | (4) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $w:=w-\alpha\frac{\partial{L}}{\partial{w}}$ |  | (4) |'
- en: where w represents the weight, $\alpha$ represents the learning rate, and the
    partial derivative of the objective function with respect to the weight. Figure
    [4](#S2.F4 "Figure 4 â€£ II-E Pooling Layer â€£ II Deep Learning Theory â€£ A survey
    on deep learning approaches for breast cancer diagnosis") illustrates the 3-dimensional
    surface of an objective function, where the darkest point represents the global
    minimum of the objective function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œw ä»£è¡¨æƒé‡ï¼Œ$\alpha$ ä»£è¡¨å­¦ä¹ ç‡ï¼Œä»¥åŠç›®æ ‡å‡½æ•°ç›¸å¯¹äºæƒé‡çš„åå¯¼æ•°ã€‚å›¾ [4](#S2.F4 "Figure 4 â€£ II-E Pooling
    Layer â€£ II Deep Learning Theory â€£ A survey on deep learning approaches for breast
    cancer diagnosis") è¯´æ˜äº†ç›®æ ‡å‡½æ•°çš„ä¸‰ç»´è¡¨é¢ï¼Œå…¶ä¸­æœ€æš—çš„ç‚¹ä»£è¡¨ç›®æ ‡å‡½æ•°çš„å…¨å±€æœ€å°å€¼ã€‚
- en: '![Refer to caption](img/ed0f502fe3fa6ef1f61bd867eb07adfd.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/ed0f502fe3fa6ef1f61bd867eb07adfd.png)'
- en: 'Figure 5: A model that has overfit the training data experiences a divergence
    in training and validation loss. Adapted from [[31](#bib.bib31)].'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„æ¨¡å‹åœ¨è®­ç»ƒå’ŒéªŒè¯æŸå¤±ä¸Šå‡ºç°äº†åç¦»ã€‚æ”¹ç¼–è‡ª [[31](#bib.bib31)]ã€‚
- en: II-G Overfitting
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-G è¿‡æ‹Ÿåˆ
- en: Models that cannot generalize to new data have overfitting the training data
    as shown in Figure [5](#S2.F5 "Figure 5 â€£ II-F Fully-Connected Layer â€£ II Deep
    Learning Theory â€£ A survey on deep learning approaches for breast cancer diagnosis").
    Overfitting can be solved through early stopping, network pruning, increasing
    training data, and regularization [[38](#bib.bib38), [39](#bib.bib39)]. Early
    stopping is stopping the learning at a point where the curves for training and
    validation loss are neither overfitting or underfitting [[38](#bib.bib38), [39](#bib.bib39)].
    Network pruning involves the pruning of redundant weights, while keeping important
    weights [[40](#bib.bib40)]. An increase in training data is required to tune the
    parameters within a network, so the network can generalize to new data better.
    Training data can be increased through collection and/or data augmentation. Regularization
    aims to remove useless features and minimize the weights of less important features
    learned by the model [[38](#bib.bib38)]. However, due to the uncertainty of necessary
    features by the network, a penalty term is added to the objective function to
    minimize the number of features [[38](#bib.bib38)]. Furthermore, there are different
    types of regularization, such as L1 regularization, L2 regularization (weight
    decay), and Dropout [[38](#bib.bib38)]. Dropout was proposed by Srivastava et
    al. [[41](#bib.bib41)] and is another effective strategy in reducing overfitting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ— æ³•å¯¹æ–°æ•°æ®è¿›è¡Œæ³›åŒ–çš„æ¨¡å‹å·²ç»è¿‡æ‹Ÿåˆäº†è®­ç»ƒæ•°æ®ï¼Œå¦‚å›¾ [5](#S2.F5 "Figure 5 â€£ II-F Fully-Connected Layer
    â€£ II Deep Learning Theory â€£ A survey on deep learning approaches for breast cancer
    diagnosis") æ‰€ç¤ºã€‚è¿‡æ‹Ÿåˆå¯ä»¥é€šè¿‡æ—©åœã€ç½‘ç»œå‰ªæã€å¢åŠ è®­ç»ƒæ•°æ®å’Œæ­£åˆ™åŒ–æ¥è§£å†³[[38](#bib.bib38), [39](#bib.bib39)]ã€‚æ—©åœæ˜¯åœ¨è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿æ—¢ä¸å‡ºç°è¿‡æ‹Ÿåˆä¹Ÿä¸å‡ºç°æ¬ æ‹Ÿåˆçš„ç‚¹åœæ­¢å­¦ä¹ [[38](#bib.bib38),
    [39](#bib.bib39)]ã€‚ç½‘ç»œå‰ªææ¶‰åŠå‰ªé™¤å†—ä½™çš„æƒé‡ï¼ŒåŒæ—¶ä¿ç•™é‡è¦çš„æƒé‡[[40](#bib.bib40)]ã€‚éœ€è¦å¢åŠ è®­ç»ƒæ•°æ®ä»¥è°ƒæ•´ç½‘ç»œä¸­çš„å‚æ•°ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ–°æ•°æ®ã€‚é€šè¿‡æ”¶é›†å’Œ/æˆ–æ•°æ®å¢å¼ºå¯ä»¥å¢åŠ è®­ç»ƒæ•°æ®ã€‚æ­£åˆ™åŒ–æ—¨åœ¨å»é™¤æ— ç”¨ç‰¹å¾å¹¶æœ€å°åŒ–æ¨¡å‹å­¦ä¹ åˆ°çš„è¾ƒä¸é‡è¦ç‰¹å¾çš„æƒé‡[[38](#bib.bib38)]ã€‚ç„¶è€Œï¼Œç”±äºç½‘ç»œæ‰€éœ€ç‰¹å¾çš„ä¸ç¡®å®šæ€§ï¼Œä¼šåœ¨ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªæƒ©ç½šé¡¹ä»¥æœ€å°åŒ–ç‰¹å¾æ•°é‡[[38](#bib.bib38)]ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸åŒç±»å‹çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¦‚
    L1 æ­£åˆ™åŒ–ã€L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰å’Œ Dropout [[38](#bib.bib38)]ã€‚Dropout ç”± Srivastava ç­‰äººæå‡º [[41](#bib.bib41)]ï¼Œæ˜¯ä¸€ç§å‡å°‘è¿‡æ‹Ÿåˆçš„æœ‰æ•ˆç­–ç•¥ã€‚
- en: II-H Transfer Learning
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-H è¿ç§»å­¦ä¹ 
- en: Transfer learning is a technique used to improve a learner specialized to a
    domain through transferring knowledge from a similar domain [[42](#bib.bib42),
    [43](#bib.bib43)]. Additionally, it can be a solution to the issue of insufficient
    training data [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]. A quantity
    of quality training data suitable for effectively training a learner can be expensive,
    due to difficult collection and curation of data [[43](#bib.bib43)]. Henceforth,
    solution such as homogeneous and heterogeneous transfer learning have been proposed
    [[42](#bib.bib42)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡å°†çŸ¥è¯†ä»ç›¸ä¼¼é¢†åŸŸè½¬ç§»åˆ°ä¸€ä¸ªç‰¹å®šé¢†åŸŸæ¥æé«˜å­¦ä¹ è€…ä¸“ä¸šæ€§çš„æŠ€æœ¯[[42](#bib.bib42), [43](#bib.bib43)]ã€‚æ­¤å¤–ï¼Œå®ƒä¹Ÿå¯ä»¥è§£å†³è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜[[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)]ã€‚ç”±äºæ•°æ®æ”¶é›†å’Œæ•´ç†çš„å›°éš¾ï¼Œé€‚åˆæœ‰æ•ˆè®­ç»ƒå­¦ä¹ è€…çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®å¯èƒ½éå¸¸æ˜‚è´µ[[43](#bib.bib43)]ã€‚å› æ­¤ï¼Œæå‡ºäº†å¦‚åŒè´¨è¿ç§»å­¦ä¹ å’Œå¼‚è´¨è¿ç§»å­¦ä¹ ç­‰è§£å†³æ–¹æ¡ˆ[[42](#bib.bib42)]ã€‚
- en: III Performance Metrics
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III æ€§èƒ½æŒ‡æ ‡
- en: A confusion matrix can be used to showcase the different predictions that a
    classifier makes [[45](#bib.bib45), [46](#bib.bib46)]. A confusion matrix can
    be visualized as a 2x2 matrix with True Positive (TP), True Negative (TN), False
    Positive (FP), and False Negative (FN) in one of the grids [[46](#bib.bib46),
    [47](#bib.bib47)]. True positives are positive samples correctly predicted as
    positive, whereas false positives are negative samples incorrectly predicted as
    positive [[45](#bib.bib45), [46](#bib.bib46)]. True negatives are negative samples
    correctly predicted as negative, whereas false negatives are positive samples
    incorrectly predicted as negative [[45](#bib.bib45), [46](#bib.bib46)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ··æ·†çŸ©é˜µå¯ä»¥ç”¨æ¥å±•ç¤ºåˆ†ç±»å™¨çš„ä¸åŒé¢„æµ‹ç»“æœ [[45](#bib.bib45), [46](#bib.bib46)]ã€‚æ··æ·†çŸ©é˜µå¯ä»¥å¯è§†åŒ–ä¸ºä¸€ä¸ª2x2çš„çŸ©é˜µï¼Œå…¶ä¸­åŒ…å«çœŸæ­£ä¾‹ï¼ˆTPï¼‰ã€çœŸè´Ÿä¾‹ï¼ˆTNï¼‰ã€å‡æ­£ä¾‹ï¼ˆFPï¼‰å’Œå‡è´Ÿä¾‹ï¼ˆFNï¼‰
    [[46](#bib.bib46), [47](#bib.bib47)]ã€‚çœŸæ­£ä¾‹æ˜¯è¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±»çš„æ­£æ ·æœ¬ï¼Œè€Œå‡æ­£ä¾‹æ˜¯è¢«é”™è¯¯é¢„æµ‹ä¸ºæ­£ç±»çš„è´Ÿæ ·æœ¬ [[45](#bib.bib45),
    [46](#bib.bib46)]ã€‚çœŸè´Ÿä¾‹æ˜¯è¢«æ­£ç¡®é¢„æµ‹ä¸ºè´Ÿç±»çš„è´Ÿæ ·æœ¬ï¼Œè€Œå‡è´Ÿä¾‹æ˜¯è¢«é”™è¯¯é¢„æµ‹ä¸ºè´Ÿç±»çš„æ­£æ ·æœ¬ [[45](#bib.bib45), [46](#bib.bib46)]ã€‚
- en: 'The following equations will be used as a means of assessing the deep learning
    models shown in TABLE [I](#S3.T1 "Table I â€£ III Performance Metrics â€£ A survey
    on deep learning approaches for breast cancer diagnosis"), [II](#S4.T2 "Table
    II â€£ IV-C Status of Magnetic Resonance Imaging (MRI) â€£ IV Discussion â€£ A survey
    on deep learning approaches for breast cancer diagnosis"), and [III](#S4.T3 "Table
    III â€£ IV-D Current Approaches using 2D CNN Architecture â€£ IV Discussion â€£ A survey
    on deep learning approaches for breast cancer diagnosis"). Accuracy is a ratio
    between the correctly classified and the total samples [[46](#bib.bib46), [47](#bib.bib47)].
    The equation for accuracy is given by the following [[48](#bib.bib48), equ. (2)]:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å…¬å¼å°†ç”¨äºè¯„ä¼°è¡¨æ ¼ [I](#S3.T1 "Table I â€£ III Performance Metrics â€£ A survey on deep
    learning approaches for breast cancer diagnosis")ã€[II](#S4.T2 "Table II â€£ IV-C
    Status of Magnetic Resonance Imaging (MRI) â€£ IV Discussion â€£ A survey on deep
    learning approaches for breast cancer diagnosis") å’Œ [III](#S4.T3 "Table III â€£
    IV-D Current Approaches using 2D CNN Architecture â€£ IV Discussion â€£ A survey on
    deep learning approaches for breast cancer diagnosis") ä¸­å±•ç¤ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å‡†ç¡®åº¦æ˜¯æ­£ç¡®åˆ†ç±»æ ·æœ¬ä¸æ€»æ ·æœ¬çš„æ¯”ç‡
    [[46](#bib.bib46), [47](#bib.bib47)]ã€‚å‡†ç¡®åº¦çš„å…¬å¼ç”± [[48](#bib.bib48), equ. (2)] ç»™å‡ºï¼š
- en: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
- en: 'Sensitivity, also known as recall and true positive rate, is the fraction of
    relevant (actual true positive) instances that are retrieved [[46](#bib.bib46)].
    The equation for sensitivity is given by [[48](#bib.bib48), equ. (5)]:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ•æ„Ÿåº¦ï¼Œä¹Ÿç§°ä¸ºå¬å›ç‡å’ŒçœŸæ­£ä¾‹ç‡ï¼Œæ˜¯æ£€ç´¢åˆ°çš„ç›¸å…³ï¼ˆå®é™…çœŸæ­£ä¾‹ï¼‰å®ä¾‹çš„æ¯”ä¾‹ [[46](#bib.bib46)]ã€‚æ•æ„Ÿåº¦çš„è®¡ç®—å…¬å¼ç”± [[48](#bib.bib48),
    equ. (5)] ç»™å‡ºï¼š
- en: '|  | $Sensitivity=\frac{TP}{TP+FN}$ |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $Sensitivity=\frac{TP}{TP+FN}$ |  | (6) |'
- en: 'Specificity, also known as true negative rate, is the fraction of relevant
    (actual true negative) instance that are retrieved. The equation for specificity
    equation is given by [[48](#bib.bib48), equ. (6)]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¼‚æ€§ï¼Œä¹Ÿç§°ä¸ºçœŸè´Ÿç‡ï¼Œæ˜¯æ£€ç´¢åˆ°çš„ç›¸å…³ï¼ˆå®é™…çœŸè´Ÿä¾‹ï¼‰å®ä¾‹çš„æ¯”ä¾‹ã€‚ç‰¹å¼‚æ€§çš„å…¬å¼ç”± [[48](#bib.bib48), equ. (6)] ç»™å‡ºï¼š
- en: '|  | $Specificity=\frac{\text{TN}}{TN+FP}$ |  | (7) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $Specificity=\frac{\text{TN}}{TN+FP}$ |  | (7) |'
- en: 'Precision is the fraction of retrieved instances that are relevant (actual
    true positive) [[46](#bib.bib46), [47](#bib.bib47)]. The equation for precision
    is given by [[48](#bib.bib48), equ. (4)]:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç²¾ç¡®åº¦æ˜¯æ£€ç´¢åˆ°çš„å®ä¾‹ä¸­ç›¸å…³ï¼ˆå®é™…çœŸæ­£ä¾‹ï¼‰çš„æ¯”ä¾‹ [[46](#bib.bib46), [47](#bib.bib47)]ã€‚ç²¾ç¡®åº¦çš„å…¬å¼ç”± [[48](#bib.bib48),
    equ. (4)] ç»™å‡ºï¼š
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (8) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (8) |'
- en: 'F score is a weighted ratio measuring the average of precision and recall [[46](#bib.bib46)],
    the equation for F score is given by [[47](#bib.bib47), equ. (4)]:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: F åˆ†æ•°æ˜¯ä¸€ä¸ªåŠ æƒæ¯”ç‡ï¼Œç”¨äºæµ‹é‡ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„å¹³å‡å€¼ [[46](#bib.bib46)]ï¼ŒF åˆ†æ•°çš„å…¬å¼ç”± [[47](#bib.bib47), equ.
    (4)] ç»™å‡ºï¼š
- en: '|  | $F_{1}=2\>\frac{precision\>\cdot\>recall}{precision+recall}$ |  | (9)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{1}=2\>\frac{precision\>\cdot\>recall}{precision+recall}$ |  | (9)
    |'
- en: 'Dice Similarity Coefficient (DSC) is a spatial overlap index [[49](#bib.bib49)],
    and can be used for measuring the segmentation performance of a model. DSC is
    given by [[50](#bib.bib50), equ. (2)]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰æ˜¯ä¸€ä¸ªç©ºé—´é‡å æŒ‡æ ‡ [[49](#bib.bib49)]ï¼Œå¯ä»¥ç”¨æ¥æµ‹é‡æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ã€‚DSC ç”± [[50](#bib.bib50),
    equ. (2)] ç»™å‡ºï¼š
- en: '|  | $DSC=\frac{2\cdot TP}{2\cdot TP+FP+FN}$ |  | (10) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $DSC=\frac{2\cdot TP}{2\cdot TP+FP+FN}$ |  | (10) |'
- en: 'Matthews Correlation Coefficient (MCC) calculates for the Pearson product-moment
    correlation coefficient between the actual and predicted values [[47](#bib.bib47)].
    MCC is given by [[47](#bib.bib47), equ. (2)]:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Matthewsç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰è®¡ç®—å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„çš®å°”é€Šç§¯çŸ©ç›¸å…³ç³»æ•° [[47](#bib.bib47)]ã€‚MCC ç”± [[47](#bib.bib47),
    equ. (2)] ç»™å‡ºï¼š
- en: '|  | $MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$
    |  | (11) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$
    |  | (11) |'
- en: 'TABLE I: 2D CNN architectures for breast cancer diagnosis'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨æ ¼ I: ä¹³è…ºç™Œè¯Šæ–­çš„ 2D CNN æ¶æ„'
- en: Ref. Model Task Dataset AUROC (%) MCC (%) Dice (%) DM Dhungel et al. (2017)
    [[51](#bib.bib51)] CNN + RF + Hypothesis- Refinement Classification Detection
    Segmentation INbreast $0.76\pm 0.23$ (via RF, min. user int.) $0.69\pm 0.10$ (via
    CNN, min user int.) â€” $0.85\pm 0.02$ Al-antari et al. (2018) [[52](#bib.bib52)]
    YOLO + FrCN + CNN Classification Detection Segmentation INbreast 0.9478 (via CNN)
    0.9762 (via YOLO) 0.8593 (via FrCN) 0.8991 (via CNN) 0.9269 (via FrCN) Chougrad
    et al. (2018) [[53](#bib.bib53)] Inception v3 Classification Detection DDSM INbreast
    BCDR 0.99 (via MIAS) â€” â€” Ribli et al. (2018) [[54](#bib.bib54)] Faster-RCNN Classification
    Detection DDSM Semmelweis University 0.95 (via INbreast) â€” â€” Singh et al. (2020)
    [[55](#bib.bib55)] cGAN Classification Segmentation DDSM INbreast Hospital Sant
    Joan de Reus 0.80 â€” 0.94
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ® æ¨¡å‹ ä»»åŠ¡ æ•°æ®é›† AUROC (%) MCC (%) Dice (%) DM Dhungel et al. (2017) [[51](#bib.bib51)]
    CNN + RF + å‡è®¾-ç»†åŒ– åˆ†ç±» æ£€æµ‹ åˆ†å‰² INbreast $0.76\pm 0.23$ (é€šè¿‡ RFï¼Œæœ€å°ç”¨æˆ·äº¤äº’) $0.69\pm 0.10$
    (é€šè¿‡ CNNï¼Œæœ€å°ç”¨æˆ·äº¤äº’) â€” $0.85\pm 0.02$ Al-antari et al. (2018) [[52](#bib.bib52)] YOLO
    + FrCN + CNN åˆ†ç±» æ£€æµ‹ åˆ†å‰² INbreast 0.9478 (é€šè¿‡ CNN) 0.9762 (é€šè¿‡ YOLO) 0.8593 (é€šè¿‡ FrCN)
    0.8991 (é€šè¿‡ CNN) 0.9269 (é€šè¿‡ FrCN) Chougrad et al. (2018) [[53](#bib.bib53)] Inception
    v3 åˆ†ç±» æ£€æµ‹ DDSM INbreast BCDR 0.99 (é€šè¿‡ MIAS) â€” â€” Ribli et al. (2018) [[54](#bib.bib54)]
    Faster-RCNN åˆ†ç±» æ£€æµ‹ DDSM Semmelweis University 0.95 (é€šè¿‡ INbreast) â€” â€” Singh et al.
    (2020) [[55](#bib.bib55)] cGAN åˆ†ç±» åˆ†å‰² DDSM INbreast Hospital Sant Joan de Reus
    0.80 â€” 0.94
- en: IV Discussion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV è®¨è®º
- en: This section looks at the different deep learning architectures designed for
    classification, detection, and segmentation of breast tumours, all are shown in
    TABLE [I](#S3.T1 "Table I â€£ III Performance Metrics â€£ A survey on deep learning
    approaches for breast cancer diagnosis"), [II](#S4.T2 "Table II â€£ IV-C Status
    of Magnetic Resonance Imaging (MRI) â€£ IV Discussion â€£ A survey on deep learning
    approaches for breast cancer diagnosis"), and [III](#S4.T3 "Table III â€£ IV-D Current
    Approaches using 2D CNN Architecture â€£ IV Discussion â€£ A survey on deep learning
    approaches for breast cancer diagnosis").
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ¢è®¨äº†ç”¨äºä¹³è…ºè‚¿ç˜¤åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²çš„ä¸åŒæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œæ‰€æœ‰è¿™äº›éƒ½å±•ç¤ºåœ¨è¡¨æ ¼ [I](#S3.T1 "Table I â€£ III Performance
    Metrics â€£ A survey on deep learning approaches for breast cancer diagnosis")ã€[II](#S4.T2
    "Table II â€£ IV-C Status of Magnetic Resonance Imaging (MRI) â€£ IV Discussion â€£
    A survey on deep learning approaches for breast cancer diagnosis") å’Œ [III](#S4.T3
    "Table III â€£ IV-D Current Approaches using 2D CNN Architecture â€£ IV Discussion
    â€£ A survey on deep learning approaches for breast cancer diagnosis") ä¸­ã€‚
- en: IV-A Status of Digital Breast Tomosynthesis (DBT)
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A æ•°å­—ä¹³è…ºæ–­å±‚æ‰«æï¼ˆDBTï¼‰çš„ç°çŠ¶
- en: DBT has been gaining popularity over digital mammography for higher image quality,
    richer structural detail, and better reduction in background signal noise [[56](#bib.bib56)].
    In addition, recent studies [[57](#bib.bib57), [6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] have shown increased cancer detection rate
    with DBT in women aged 40 to 79 years with dense and non-dense breast, but results
    on reduced recall rate remains conflicted. Moreover, [[6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] have shown that DBT reduces recall rates for
    women aged 40 to 79 years with dense and non-dense breast, but [[61](#bib.bib61),
    [62](#bib.bib62)] shown recall rates of DBT plus DM similar to that of DM alone.
    DBT reduces the overlapping breast tissue that appear on 2D images as opposed
    to mammograms [[63](#bib.bib63), [6](#bib.bib6), [3](#bib.bib3)]. As a result,
    DBT images help with the detection of tumours that may appear overlapped by other
    healthy breast tissue. However, a disadvantage of DBT is being less sensitive
    to imaging malignant calcification and even groups of micro-calcification compared
    to DM [[64](#bib.bib64), [56](#bib.bib56)]. In addition, DBT systems that use
    pixel binning have increased efficiency in detector readings, but in turn reduced
    3D spatial resolution [[7](#bib.bib7)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ•°å­—ä¹³è…ºæ‘„å½±ç›¸æ¯”ï¼ŒDBT å› å…¶æ›´é«˜çš„å›¾åƒè´¨é‡ã€æ›´ä¸°å¯Œçš„ç»“æ„ç»†èŠ‚ä»¥åŠæ›´å¥½çš„èƒŒæ™¯ä¿¡å·å™ªå£°å‡å°‘è€Œé€æ¸å—åˆ°æ¬¢è¿ [[56](#bib.bib56)]ã€‚æ­¤å¤–ï¼Œè¿‘æœŸç ”ç©¶
    [[57](#bib.bib57), [6](#bib.bib6), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]
    è¡¨æ˜ï¼ŒDBT åœ¨40è‡³79å²æœ‰å¯†é›†å’Œéå¯†é›†ä¹³æˆ¿çš„å¥³æ€§ä¸­çš„ç™Œç—‡æ£€æµ‹ç‡æœ‰æ‰€æé«˜ï¼Œä½†å¯¹å‡å°‘å¬å›ç‡çš„ç»“æœä»å­˜åœ¨äº‰è®®ã€‚æ­¤å¤–ï¼Œ[[6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] è¿˜æ˜¾ç¤ºï¼ŒDBT å‡å°‘äº†40è‡³79å²æœ‰å¯†é›†å’Œéå¯†é›†ä¹³æˆ¿å¥³æ€§çš„å¬å›ç‡ï¼Œä½† [[61](#bib.bib61),
    [62](#bib.bib62)] è¡¨æ˜ DBT åŠ  DM çš„å¬å›ç‡ä¸ DM å•ç‹¬ä½¿ç”¨çš„å¬å›ç‡ç›¸ä¼¼ã€‚DBT å‡å°‘äº†åœ¨ 2D å›¾åƒä¸Šå‡ºç°çš„é‡å ä¹³è…ºç»„ç»‡ï¼Œè€Œä¸æ˜¯ä¹³è…º
    X å…‰æ£€æŸ¥ [[63](#bib.bib63), [6](#bib.bib6), [3](#bib.bib3)]ã€‚å› æ­¤ï¼ŒDBT å›¾åƒæœ‰åŠ©äºæ£€æµ‹å¯èƒ½è¢«å…¶ä»–å¥åº·ä¹³è…ºç»„ç»‡é‡å çš„è‚¿ç˜¤ã€‚ç„¶è€Œï¼ŒDBT
    çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯å¯¹æ¶æ€§é’™åŒ–å’Œå¾®é’™åŒ–ç¾¤ä½“çš„æˆåƒæ•æ„Ÿæ€§ä½äº DM [[64](#bib.bib64), [56](#bib.bib56)]ã€‚æ­¤å¤–ï¼Œä½¿ç”¨åƒç´ æ±‡èšçš„ DBT
    ç³»ç»Ÿæé«˜äº†æ¢æµ‹å™¨è¯»æ•°çš„æ•ˆç‡ï¼Œä½†åè¿‡æ¥é™ä½äº†ä¸‰ç»´ç©ºé—´åˆ†è¾¨ç‡ [[7](#bib.bib7)]ã€‚
- en: IV-B Status of Automatic Breast UltraSound (ABUS)
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B è‡ªåŠ¨ä¹³è…ºè¶…å£°ï¼ˆABUSï¼‰çš„çŠ¶æ€
- en: The ABUS consists of an ultrasound scanner and a transducer [[65](#bib.bib65),
    [66](#bib.bib66)]. The ABUS captures axial slices of the breast in different views,
    then these axial slices are used for 3D reconstruction of sagittal and coronal
    images [[65](#bib.bib65), [66](#bib.bib66)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ABUS åŒ…æ‹¬ä¸€ä¸ªè¶…å£°æ‰«æä»ªå’Œä¸€ä¸ªä¼ æ„Ÿå™¨ [[65](#bib.bib65), [66](#bib.bib66)]ã€‚ABUS æ•æ‰ä¹³æˆ¿çš„ä¸åŒè§†è§’çš„è½´å‘åˆ‡ç‰‡ï¼Œç„¶åè¿™äº›è½´å‘åˆ‡ç‰‡ç”¨äºçŸ¢çŠ¶é¢å’Œå† çŠ¶é¢å›¾åƒçš„ä¸‰ç»´é‡å»º
    [[65](#bib.bib65), [66](#bib.bib66)]ã€‚
- en: IV-C Status of Magnetic Resonance Imaging (MRI)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„çŠ¶æ€
- en: Magnetic resonance imaging (MRI) screenings are recommended to patients with
    a high risk of breast cancer, due to genetics or family history [[67](#bib.bib67),
    [2](#bib.bib2)]. Breast coils are used with an MRI to acquire the image of the
    breast; the patient lies prone with the breasts in the breast coils before entering
    the MRI [[68](#bib.bib68)]. Breast MRI has different types, such as T1-weighted
    contrast-enhanced imaging, T2-weighted, ultrafast, and diffusion-weighted imaging
    [[68](#bib.bib68)]. The dimensionality of an acquired image is dependent on the
    MRI type [[69](#bib.bib69)]. In 2D image acquisition, multiple 2D image slices
    of the object are captured, whereas in 3D image acquisition, a true 3D image can
    be captured [[69](#bib.bib69), [70](#bib.bib70)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç­›æŸ¥æ¨èç»™é‚£äº›ç”±äºé—ä¼ æˆ–å®¶æ—å²è€Œæœ‰ä¹³è…ºç™Œé«˜é£é™©çš„æ‚£è€… [[67](#bib.bib67), [2](#bib.bib2)]ã€‚ä¹³è…ºçº¿åœˆä¸
    MRI ä¸€èµ·ä½¿ç”¨ï¼Œä»¥è·å–ä¹³æˆ¿å›¾åƒï¼›æ‚£è€…ä¿¯å§åœ¨ä¹³è…ºçº¿åœˆä¸­ï¼Œç„¶åè¿›å…¥ MRI [[68](#bib.bib68)]ã€‚ä¹³è…º MRI æœ‰ä¸åŒç±»å‹ï¼Œå¦‚ T1 åŠ æƒå¯¹æ¯”å¢å¼ºæˆåƒã€T2
    åŠ æƒæˆåƒã€è¶…å¿«é€Ÿæˆåƒå’Œæ‰©æ•£åŠ æƒæˆåƒ [[68](#bib.bib68)]ã€‚è·å–çš„å›¾åƒçš„ç»´åº¦å–å†³äº MRI ç±»å‹ [[69](#bib.bib69)]ã€‚åœ¨ 2D
    å›¾åƒè·å–ä¸­ï¼Œæ•è·å¯¹è±¡çš„å¤šä¸ª 2D å›¾åƒåˆ‡ç‰‡ï¼Œè€Œåœ¨ 3D å›¾åƒè·å–ä¸­ï¼Œå¯ä»¥æ•è·çœŸå®çš„ 3D å›¾åƒ [[69](#bib.bib69), [70](#bib.bib70)]ã€‚
- en: 'TABLE II: 2.5D CNN architectures for breast cancer diagnosis'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ IIï¼šç”¨äºä¹³è…ºç™Œè¯Šæ–­çš„ 2.5D CNN æ¶æ„
- en: 'Ref. Model Task Dataset AUROC (%) Dice (%) DM-DBT Yousefi et al. (2018) [[71](#bib.bib71)]
    DCNN MI-RF- based CAD Classification Breast Imaging Research Laboratory at Massachusetts
    General Hospital (in-house): 87 DBT images (27 malignant, 60 benign) 0.87 â€” Kim
    et al. (2017) [[72](#bib.bib72)] VGG16 + LSTM Classification (in-house) 0.919
    â€” Liu et al. (2017) [[73](#bib.bib73)] 3D Anisotropic Hybrid Network (3D AH-Net)
    Segmentation (in-house): 2809 DBT volumes â€” 0.834 Zhang et al. (2020) [[74](#bib.bib74)]
    AlexNet (Late fusion + Max Pooling) Classification (in-house): 3018 negatives
    272 malignant 415 benign 0.854 â€” Liang et al. (2020) [[75](#bib.bib75)] CNN ensemble
    Classification University of Kentucky Medical Center (in-house): DBT and DM (709
    malignant, 415 benign) 0.97 â€”'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ® æ¨¡å‹ ä»»åŠ¡ æ•°æ®é›† AUROC (%) Dice (%) DM-DBT Yousefi ç­‰äººï¼ˆ2018ï¼‰ [[71](#bib.bib71)]
    DCNN MI-RF åŸºäº CAD åˆ†ç±» éº»çœæ€»åŒ»é™¢ï¼ˆå†…éƒ¨ï¼‰ï¼š87 å¼  DBT å›¾åƒï¼ˆ27 å¼ æ¶æ€§ï¼Œ60 å¼ è‰¯æ€§ï¼‰ 0.87 â€” Kim ç­‰äººï¼ˆ2017ï¼‰
    [[72](#bib.bib72)] VGG16 + LSTM åˆ†ç±»ï¼ˆå†…éƒ¨ï¼‰ 0.919 â€” Liu ç­‰äººï¼ˆ2017ï¼‰ [[73](#bib.bib73)]
    3D å„å‘å¼‚æ€§æ··åˆç½‘ç»œï¼ˆ3D AH-Netï¼‰ åˆ†å‰²ï¼ˆå†…éƒ¨ï¼‰ï¼š2809 ä¸ª DBT å· â€” 0.834 Zhang ç­‰äººï¼ˆ2020ï¼‰ [[74](#bib.bib74)]
    AlexNetï¼ˆåèåˆ + æœ€å¤§æ± åŒ–ï¼‰ åˆ†ç±»ï¼ˆå†…éƒ¨ï¼‰ï¼š3018 ä¸ªè´Ÿæ ·æœ¬ 272 ä¸ªæ¶æ€§ 415 ä¸ªè‰¯æ€§ 0.854 â€” Liang ç­‰äººï¼ˆ2020ï¼‰ [[75](#bib.bib75)]
    CNN é›†æˆ åˆ†ç±» è‚¯å¡”åŸºå¤§å­¦åŒ»å­¦ä¸­å¿ƒï¼ˆå†…éƒ¨ï¼‰ï¼šDBT å’Œ DMï¼ˆ709 ä¸ªæ¶æ€§ï¼Œ415 ä¸ªè‰¯æ€§ï¼‰ 0.97 â€”
- en: IV-D Current Approaches using 2D CNN Architecture
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D ç›®å‰ä½¿ç”¨çš„ 2D CNN æ¶æ„
- en: For 2D classification, Chougrad et al. [[53](#bib.bib53)] adopted state-of-the-art
    architectures, including ResNet50 [[76](#bib.bib76)], VGG16 [[77](#bib.bib77)],
    and Inception v3 [[78](#bib.bib78)], that were pre-trained on ImageNet, and re-purposed
    for breast cancer screening. Chougrad et al. [[53](#bib.bib53)] achieved a 0.99
    AUC for classification on the MIAS database using a pre-trained and fine-tuned
    Inception v3 model. The study concluded that fine-tuning strategy improves classification
    accuracy on state-of-the-art architecture, and Inception v3 achieved a higher
    accuracy than VGG16 and ResNet50.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº 2D åˆ†ç±»ï¼ŒChougrad ç­‰äºº [[53](#bib.bib53)] é‡‡ç”¨äº†åŒ…æ‹¬ ResNet50 [[76](#bib.bib76)]ã€VGG16
    [[77](#bib.bib77)] å’Œ Inception v3 [[78](#bib.bib78)] åœ¨å†…çš„æœ€å…ˆè¿›æ¶æ„ï¼Œè¿™äº›æ¶æ„åœ¨ ImageNet ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶é‡æ–°ç”¨äºä¹³è…ºç™Œç­›æŸ¥ã€‚Chougrad
    ç­‰äºº [[53](#bib.bib53)] ä½¿ç”¨é¢„è®­ç»ƒå¹¶å¾®è°ƒçš„ Inception v3 æ¨¡å‹åœ¨ MIAS æ•°æ®åº“ä¸Šè¾¾åˆ°äº† 0.99 çš„åˆ†ç±» AUCã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼Œå¾®è°ƒç­–ç•¥æé«˜äº†æœ€å…ˆè¿›æ¶æ„çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œè€Œ
    Inception v3 çš„å‡†ç¡®ç‡é«˜äº VGG16 å’Œ ResNet50ã€‚
- en: For 2D segmentation, Singh et al. [[55](#bib.bib55)] adapted upon a study by
    Isola et al. [[79](#bib.bib79)] to propose a conditional Generative Adversarial
    Network (cGAN) CAD framework for classification and segmentation breast tumor.
    Singh et al. [[55](#bib.bib55)] had achieved 92.11% dice coefficient score and
    84.55% IoU for a tight frame of the tumor Region Of Interest (ROI) on cGAN. Furthermore,
    the study tested Single Shot Detector (SSD), You Only Look Once (YOLO), and Faster-RCNN
    and found that SSD achieved the best results on detecting small tumor regions
    and achieved an overall accuracy of 97%. Major contributions proposed in this
    study include the first adapted cGan for breast tumor segmentation, a multi-class
    CNN for predicting four breast tumor shapes, and the proposed model outperforming
    state-of-the-art architecture.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº 2D åˆ†å‰²ï¼ŒSingh ç­‰äºº [[55](#bib.bib55)] åœ¨ Isola ç­‰äºº [[79](#bib.bib79)] çš„ç ”ç©¶åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰CAD
    æ¡†æ¶ï¼Œç”¨äºä¹³è…ºè‚¿ç˜¤çš„åˆ†ç±»å’Œåˆ†å‰²ã€‚Singh ç­‰äºº [[55](#bib.bib55)] åœ¨ cGAN ä¸Šè¾¾åˆ°äº† 92.11% çš„ Dice ç³»æ•°åˆ†æ•°å’Œ 84.55%
    çš„ IoUï¼Œç”¨äºè‚¿ç˜¤æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„ç´§å¯†æ¡†æ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶æµ‹è¯•äº† Single Shot Detector (SSD)ã€You Only Look Once
    (YOLO) å’Œ Faster-RCNNï¼Œå¹¶å‘ç° SSD åœ¨æ£€æµ‹å°è‚¿ç˜¤åŒºåŸŸæ—¶è¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°äº† 97%ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬é¦–æ¬¡é€‚åº” cGAN ç”¨äºä¹³è…ºè‚¿ç˜¤åˆ†å‰²ï¼Œæå‡ºäº†ä¸€ç§å¤šç±»åˆ«
    CNN ç”¨äºé¢„æµ‹å››ç§ä¹³è…ºè‚¿ç˜¤å½¢çŠ¶ï¼Œå¹¶ä¸”æå‡ºçš„æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„æ¶æ„ã€‚
- en: 'TABLE III: 3D CNN architectures for breast cancer diagnosis'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ IIIï¼šä¹³è…ºç™Œè¯Šæ–­çš„ 3D CNN æ¶æ„
- en: 'Ref. Model Dataset Acc(%) AUROC(%) AP(%) FNR(%) Dice(%) Fscore(%) DBT Zhang
    et al. (2018)[[80](#bib.bib80)] 3D-T2-Alex University of Kentucky (in-house) â€”
    0.6632 â€” â€” â€” â€” Fan et al. (2020)[[81](#bib.bib81)] 3D Mask-RCNN Fudan University
    Affiliated Cancer Center (in-house): 364 DBT samples (289 malignant, 75 benign)
    â€” 0.934 0.053 â€” â€” â€” Wichakam et al. (2018)[[82](#bib.bib82)] 3D ConvNet (in-house):
    115 DBT volumes (91 malignant, 24 normal) 0.72 â€” â€” â€” â€” 0.842 ABUS Lei et al. (2021)[[83](#bib.bib83)]
    Mask scoring RCNN (private) â€” â€” â€” $0.85\pm 0.104$ â€” â€” Zhou et al. (2021)[[84](#bib.bib84)]
    $\mathrm{C_{MS}}\mathrm{VNet_{Iter}}$ Peking University Peopleâ€™s Hospital (in-house):
    900 ABUS volumes â€” 0.787 â€” 0.392 $0.778\pm 0.145$ 0.811 MRI Zhou et al. (2019)[[85](#bib.bib85)]
    3D DenseNet (in-house): 720 malignant, 353 benign â€” 0.859 â€” â€” $0.501\pm 0.274$
    â€” Hu et al. (2020)[[86](#bib.bib86)] VGG19Net (in-house): 728 malignant, 199 benign
    â€” DEC: 0.85 T2w: 0.78 ImageFusion: 0.85 FeatureFusion: 0.87 ClassifierFusion:
    0.86 â€” â€” â€” â€”'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'å‚è€ƒæ¨¡å‹ æ•°æ®é›† å‡†ç¡®ç‡ï¼ˆ%ï¼‰ AUROCï¼ˆ%ï¼‰ APï¼ˆ%ï¼‰ FNRï¼ˆ%ï¼‰ Diceï¼ˆ%ï¼‰ Fscoreï¼ˆ%ï¼‰ DBT Zhang et al. (2018)[[80](#bib.bib80)]
    3D-T2-Alex è‚¯å¡”åŸºå¤§å­¦ï¼ˆå†…éƒ¨ï¼‰ â€” 0.6632 â€” â€” â€” â€” Fan et al. (2020)[[81](#bib.bib81)] 3D Mask-RCNN
    å¤æ—¦å¤§å­¦é™„å±è‚¿ç˜¤ä¸­å¿ƒï¼ˆå†…éƒ¨ï¼‰ï¼š364 DBT æ ·æœ¬ï¼ˆ289 ä¾‹æ¶æ€§ï¼Œ75 ä¾‹è‰¯æ€§ï¼‰ â€” 0.934 0.053 â€” â€” â€” Wichakam et al.
    (2018)[[82](#bib.bib82)] 3D ConvNetï¼ˆå†…éƒ¨ï¼‰ï¼š115 DBT ä½“ç§¯ï¼ˆ91 ä¾‹æ¶æ€§ï¼Œ24 ä¾‹æ­£å¸¸ï¼‰ 0.72 â€” â€” â€” â€”
    0.842 ABUS Lei et al. (2021)[[83](#bib.bib83)] Mask scoring RCNNï¼ˆç§äººï¼‰ â€” â€” â€” $0.85\pm
    0.104$ â€” â€” Zhou et al. (2021)[[84](#bib.bib84)] $\mathrm{C_{MS}}\mathrm{VNet_{Iter}}$
    åŒ—äº¬å¤§å­¦äººæ°‘åŒ»é™¢ï¼ˆå†…éƒ¨ï¼‰ï¼š900 ABUS ä½“ç§¯ â€” 0.787 â€” 0.392 $0.778\pm 0.145$ 0.811 MRI Zhou et al.
    (2019)[[85](#bib.bib85)] 3D DenseNetï¼ˆå†…éƒ¨ï¼‰ï¼š720 ä¾‹æ¶æ€§ï¼Œ353 ä¾‹è‰¯æ€§ â€” 0.859 â€” â€” $0.501\pm
    0.274$ â€” Hu et al. (2020)[[86](#bib.bib86)] VGG19Netï¼ˆå†…éƒ¨ï¼‰ï¼š728 ä¾‹æ¶æ€§ï¼Œ199 ä¾‹è‰¯æ€§ â€” DEC:
    0.85 T2w: 0.78 ImageFusion: 0.85 FeatureFusion: 0.87 ClassifierFusion: 0.86 â€”
    â€” â€” â€”'
- en: IV-E Overview on 2.5D CNN
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 2.5D CNN æ¦‚è¿°
- en: The utilization of the depth dimension and collection of images within a DBT
    volume is needed to utilize the entirety of the DBT information [[72](#bib.bib72),
    [74](#bib.bib74), [87](#bib.bib87)]. Furthermore, 2D CNN cannot preserve the between-slice
    information in DBT volumes [[88](#bib.bib88), [89](#bib.bib89), [44](#bib.bib44),
    [73](#bib.bib73)]. Moreover, the high complexity, potential overfitting, and small
    DBT dataset relative to ImageNet [[72](#bib.bib72), [74](#bib.bib74), [90](#bib.bib90)]
    can make training a 3D CNN rather infeasible, which makes approaches by [[72](#bib.bib72),
    [73](#bib.bib73), [75](#bib.bib75)] more favourable. In the following section,
    alternative methods to 2D and 3D CNN approaches for utilizing the entirety of
    information within DBT images will be discussed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ©ç”¨ DBT ä¿¡æ¯çš„å…¨éƒ¨å†…å®¹ï¼Œéœ€è¦åˆ©ç”¨æ·±åº¦ç»´åº¦å’Œ DBT ä½“ç§¯å†…çš„å›¾åƒé›†åˆ [[72](#bib.bib72), [74](#bib.bib74),
    [87](#bib.bib87)]ã€‚æ­¤å¤–ï¼Œ2D CNN æ— æ³•ä¿ç•™ DBT ä½“ç§¯ä¸­çš„åˆ‡ç‰‡é—´ä¿¡æ¯ [[88](#bib.bib88), [89](#bib.bib89),
    [44](#bib.bib44), [73](#bib.bib73)]ã€‚è€Œä¸”ï¼Œç›¸å¯¹äº ImageNet [[72](#bib.bib72), [74](#bib.bib74),
    [90](#bib.bib90)]ï¼Œé«˜å¤æ‚åº¦ã€æ½œåœ¨çš„è¿‡æ‹Ÿåˆä»¥åŠè¾ƒå°çš„ DBT æ•°æ®é›†å¯èƒ½ä½¿å¾—è®­ç»ƒ 3D CNN ç›¸å½“ä¸å¯è¡Œï¼Œè¿™ä½¿å¾— [[72](#bib.bib72),
    [73](#bib.bib73), [75](#bib.bib75)] çš„æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œå°†è®¨è®ºåˆ©ç”¨ DBT å›¾åƒå†…å…¨éƒ¨ä¿¡æ¯çš„ 2D å’Œ 3D
    CNN æ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆã€‚
- en: IV-F Current Approaches using 2.5D CNN Architecture
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F å½“å‰ä½¿ç”¨2.5D CNNæ¶æ„çš„æ–¹æ³•
- en: Kim et al. [[72](#bib.bib72)] proposed a CNN for spatial feature representation
    and depth directional long-term recurrent learning for depth feature representation.
    A VGG16 network was used as the CNN, while LSTMs are used for depth directional
    long-term recurrent learning. The model achieved an AUROC of 91.9%. However, a
    LSTM network can be difficult [[73](#bib.bib73)] and expensive [[90](#bib.bib90)]
    to train.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Kim et al. [[72](#bib.bib72)] æå‡ºäº†ç”¨äºç©ºé—´ç‰¹å¾è¡¨ç¤ºå’Œæ·±åº¦æ–¹å‘é•¿æœŸé€’å½’å­¦ä¹ çš„ CNNã€‚VGG16 ç½‘ç»œè¢«ç”¨ä½œ CNNï¼Œè€Œ
    LSTM è¢«ç”¨äºæ·±åº¦æ–¹å‘é•¿æœŸé€’å½’å­¦ä¹ ã€‚è¯¥æ¨¡å‹è¾¾åˆ°äº† 91.9% çš„ AUROCã€‚ç„¶è€Œï¼ŒLSTM ç½‘ç»œå¯èƒ½å¾ˆéš¾ [[73](#bib.bib73)] å¹¶ä¸”è®­ç»ƒæˆæœ¬é«˜
    [[90](#bib.bib90)]ã€‚
- en: Hence, a different approach for learning 3D DBT volumes was proposed by Liu
    et al. [[73](#bib.bib73)]. They proposed the 3D Anisotropic Hybrid Network (3D
    AH-Net). The 3D AH-Net achieved a global dice score of 83.4%. As mentioned by
    Liu et al., challenges with directly training a 3D CNN with DBT or CT scans include
    (1) anisotropic voxels, (2) the higher number of features needed compared to 2D
    CNN, and (3) the lack of pre-trained 3D CNN models and limited training data.
    Anisotropic voxels have uneven distribution of data that hinder the training of
    3D CNNs, such as CT and DBT volumes having within-slice resolution greater than
    between-slice resolution. This challenge of anisotropic voxels in DBT images was
    treated using anisotropic convolutions. The 3D AH-Net has a feature encoder and
    decoder. The encoder extracts deep representations from the 2D image slices. On
    the other hand, the decoder, a densely connected network of anisotropic convolutions,
    utilizes the 3D context, while keeping the between-slices consistency. The AH-ResNet,
    a 2D ResNet50 [[76](#bib.bib76)] converted into a 3D ResNet50, was used as the
    backbone and encoder of the 3D AH-Net. A 2D Multi-Channel Global Convolutional
    Network (MC-GCN) was used to train the encoder parameters used in the 3D AH-NET.
    The parameters trained on the MC-GCN were extracted and transferred into the AH-Resnet.
    The 3D AH-Net is structured in the order of AH-ResNet, decoder, then pyramid volumetric
    pooling.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåˆ˜ç­‰äººæå‡ºäº†å¦ä¸€ç§å­¦ä¹ 3D DBTä½“ç§¯çš„æ–¹æ³•ã€‚ä»–ä»¬æå‡ºäº†3Då„å‘å¼‚æ€§æ··åˆç½‘ç»œï¼ˆ3D AH-Netï¼‰ã€‚3D AH-Netå®ç°äº†83.4%çš„å…¨å±€éª°å­å¾—åˆ†ã€‚æ­£å¦‚åˆ˜ç­‰äººæ‰€æåˆ°çš„ï¼Œç›´æ¥ç”¨DBTæˆ–CTæ‰«æè®­ç»ƒ3D
    CNNé¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ï¼ˆ1ï¼‰å„å‘å¼‚æ€§ä½“ç´ ï¼Œï¼ˆ2ï¼‰ç›¸æ¯”äº2D CNNæ‰€éœ€æ›´å¤šçš„ç‰¹å¾ï¼Œä»¥åŠï¼ˆ3ï¼‰ç¼ºä¹é¢„è®­ç»ƒçš„3D CNNæ¨¡å‹å’Œæœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚å„å‘å¼‚æ€§ä½“ç´ æ•°æ®åˆ†å¸ƒä¸å‡ï¼Œè¿™ä¼šé˜»ç¢3D
    CNNçš„è®­ç»ƒï¼Œä¾‹å¦‚CTå’ŒDBTä½“ç§¯åœ¨åˆ‡ç‰‡å†…çš„åˆ†è¾¨ç‡å¤§äºåˆ‡ç‰‡ä¹‹é—´çš„åˆ†è¾¨ç‡ã€‚DBTå›¾åƒä¸­çš„å„å‘å¼‚æ€§ä½“ç´ æŒ‘æˆ˜é€šè¿‡å„å‘å¼‚æ€§å·ç§¯å¾—åˆ°å¤„ç†ã€‚3D AH-Netå…·æœ‰ç‰¹å¾ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ç¼–ç å™¨ä»2Då›¾åƒåˆ‡ç‰‡ä¸­æå–æ·±å±‚è¡¨ç¤ºã€‚è€Œè§£ç å™¨ï¼Œä½œä¸ºä¸€ä¸ªå¯†é›†è¿æ¥çš„å„å‘å¼‚æ€§å·ç§¯ç½‘ç»œï¼Œåˆ©ç”¨3Dä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿æŒåˆ‡ç‰‡é—´çš„ä¸€è‡´æ€§ã€‚AH-ResNetï¼Œä¸€ä¸ªå°†2D
    ResNet50è½¬æ¢ä¸º3D ResNet50çš„ç½‘ç»œï¼Œè¢«ç”¨ä½œ3D AH-Netçš„éª¨å¹²ç½‘å’Œç¼–ç å™¨ã€‚ä¸€ä¸ª2Då¤šé€šé“å…¨å±€å·ç§¯ç½‘ç»œï¼ˆMC-GCNï¼‰ç”¨äºè®­ç»ƒ3D AH-NETä¸­ä½¿ç”¨çš„ç¼–ç å™¨å‚æ•°ã€‚è®­ç»ƒå¥½çš„MC-GCNå‚æ•°è¢«æå–å¹¶è½¬ç§»åˆ°AH-ResNetä¸­ã€‚3D
    AH-Netçš„ç»“æ„é¡ºåºä¸ºAH-ResNetã€è§£ç å™¨ï¼Œç„¶åæ˜¯é‡‘å­—å¡”ä½“ç§¯æ± åŒ–ã€‚
- en: A key challenge mentioned by Liang et al. [[74](#bib.bib74)] is the effective
    utilization of DBT data, considering as DBT data are high in resolution and vary
    in depth. The training of a 3D CNN model with DBT data is computationally costly
    and memory intensive. As a result, Liang et al. proposed a network containing
    two types of CNNs, a CNN feature extractor and CNN classifier. The model achieved
    an AUROC of 97%. The network sequence starts with a 2D CNN feature extractor transitioning
    into an ensemble of three CNN classifiers. A classifier for DM, DBT, and DM-DBT
    feature map classification, where the DM-DBT feature map is a concatenation of
    DM and DBT feature maps.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢ç­‰äººæåˆ°çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯**æœ‰æ•ˆåˆ©ç”¨DBTæ•°æ®**ï¼Œå› ä¸ºDBTæ•°æ®å…·æœ‰é«˜åˆ†è¾¨ç‡ä¸”æ·±åº¦å„å¼‚ã€‚ç”¨DBTæ•°æ®è®­ç»ƒ3D CNNæ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜ä¸Šéƒ½éå¸¸è€—è´¹èµ„æºã€‚å› æ­¤ï¼Œæ¢ç­‰äººæå‡ºäº†ä¸€ç§åŒ…å«ä¸¤ç§CNNçš„ç½‘ç»œï¼šCNNç‰¹å¾æå–å™¨å’ŒCNNåˆ†ç±»å™¨ã€‚è¯¥æ¨¡å‹å®ç°äº†97%çš„AUROCã€‚ç½‘ç»œåºåˆ—ä»2D
    CNNç‰¹å¾æå–å™¨å¼€å§‹ï¼Œè¿‡æ¸¡åˆ°ç”±ä¸‰ç§CNNåˆ†ç±»å™¨ç»„æˆçš„é›†æˆä½“ã€‚ç”¨äºDMã€DBTå’ŒDM-DBTç‰¹å¾å›¾åˆ†ç±»çš„åˆ†ç±»å™¨ï¼Œå…¶ä¸­DM-DBTç‰¹å¾å›¾æ˜¯DMå’ŒDBTç‰¹å¾å›¾çš„æ‹¼æ¥ã€‚
- en: IV-G Overview on 3D CNN
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G 3D CNNæ¦‚è¿°
- en: A 2D CNN cannot preserve the between-slice information in DBT volumes as opposed
    to a 3D CNN [[88](#bib.bib88), [89](#bib.bib89), [44](#bib.bib44), [73](#bib.bib73)].
    A 3D CNN can learn the spatial information within a 2D image along with between-slice
    information of multiple slices. A 3D convolutional kernel enables this characteristic,
    but also generates a higher number of training parameters compared to a 2D convolutional
    kernel. As a result, the complexity of features that each kernels can extract
    increases. A challenge arises when isotropic 3D convolutional kernels are used
    to learn anisotropic DBT volumes, due to variation of resolution within each anisotropic
    voxel along each plane [[73](#bib.bib73), [88](#bib.bib88)]. Moreover, the quality
    of spatial resolution of DBT images can impact the training of 2D and 3D CNNs
    [[7](#bib.bib7), [88](#bib.bib88), [73](#bib.bib73)]. Another challenge with training
    3D CNN is the limited well-curated and publicly available DBT dataset. Buda et
    al. [[91](#bib.bib91)] addressed this issue by curating and releasing a publicly
    available dataset with 22,032 reconstructed DBT volumes from 5,060 patients.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ 2D CNN ç›¸æ¯”ï¼Œ2D CNN æ— æ³•ä¿ç•™ DBT ä½“ç§¯ä¸­çš„åˆ‡ç‰‡ä¹‹é—´ä¿¡æ¯ [[88](#bib.bib88), [89](#bib.bib89),
    [44](#bib.bib44), [73](#bib.bib73)]ã€‚3D CNN èƒ½å¤Ÿå­¦ä¹  2D å›¾åƒä¸­çš„ç©ºé—´ä¿¡æ¯ä»¥åŠå¤šä¸ªåˆ‡ç‰‡ä¹‹é—´çš„ä¿¡æ¯ã€‚3D å·ç§¯æ ¸ä½¿è¿™ä¸€ç‰¹æ€§æˆä¸ºå¯èƒ½ï¼Œä½†ä¹Ÿäº§ç”Ÿäº†æ¯”
    2D å·ç§¯æ ¸æ›´å¤šçš„è®­ç»ƒå‚æ•°ã€‚å› æ­¤ï¼Œæ¯ä¸ªå·ç§¯æ ¸èƒ½å¤Ÿæå–çš„ç‰¹å¾å¤æ‚æ€§å¢åŠ ã€‚å½“ä½¿ç”¨å„å‘åŒæ€§çš„ 3D å·ç§¯æ ¸å­¦ä¹ å„å‘å¼‚æ€§ DBT ä½“ç§¯æ—¶ï¼Œç”±äºæ¯ä¸ªå„å‘å¼‚æ€§ä½“ç´ æ²¿æ¯ä¸ªå¹³é¢çš„åˆ†è¾¨ç‡å˜åŒ–ï¼Œä¼šå‡ºç°æŒ‘æˆ˜
    [[73](#bib.bib73), [88](#bib.bib88)]ã€‚æ­¤å¤–ï¼ŒDBT å›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡è´¨é‡å¯èƒ½ä¼šå½±å“ 2D å’Œ 3D CNN çš„è®­ç»ƒ [[7](#bib.bib7),
    [88](#bib.bib88), [73](#bib.bib73)]ã€‚è®­ç»ƒ 3D CNN çš„å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ç¼ºä¹è‰¯å¥½ç­–åˆ’çš„å…¬å¼€ DBT æ•°æ®é›†ã€‚Buda ç­‰äºº
    [[91](#bib.bib91)] é€šè¿‡ç­–åˆ’å’Œå‘å¸ƒä¸€ä¸ªåŒ…å« 22,032 ä¸ªé‡å»º DBT ä½“ç§¯çš„å…¬å¼€æ•°æ®é›†æ¥è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚
- en: IV-H Current Approach using 3D CNN Architecture
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-H å½“å‰ä½¿ç”¨ 3D CNN æ¶æ„çš„æ–¹æ³•
- en: Fan et al. [[81](#bib.bib81)] showcases the superiority of 3D over 2D deep learning
    methods, such as Yousefi et al. [[71](#bib.bib71)], in learning to classify, detect,
    and segment tumours in DBT image slices. Fan et al. [[81](#bib.bib81)] proposed
    a 3D-Mask-RCNN with a ResNet50 backbone. The proposed 3D-Mask-RCNN achieved a
    sensitivity of 90% at 0.83 FPs/breast for breast-based mass detection, and an
    AP of 93.4% and FNR of 5.3% for lesion segmentation. The study concluded the proposed
    model, 3D-Mask-RCNN, outperforms the 2D counterparts, Mask-RCNN and Faster-RCNN.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Fan ç­‰äºº [[81](#bib.bib81)] å±•ç¤ºäº† 3D æ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸å¯¹äº 2D æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œä¾‹å¦‚ Yousefi ç­‰äºº [[71](#bib.bib71)]ï¼Œåœ¨å­¦ä¹ å¯¹
    DBT å›¾åƒåˆ‡ç‰‡ä¸­çš„è‚¿ç˜¤è¿›è¡Œåˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢ã€‚Fan ç­‰äºº [[81](#bib.bib81)] æå‡ºäº†ä¸€ä¸ªä»¥ ResNet50 ä¸ºéª¨å¹²ç½‘çš„ 3D-Mask-RCNNã€‚è¯¥
    3D-Mask-RCNN åœ¨ä¹³è…ºåŸºè´¨æ£€æµ‹ä¸­å®ç°äº† 90% çš„çµæ•åº¦ï¼Œåœ¨ 0.83 FPs/ä¹³è…ºçš„æ¡ä»¶ä¸‹ï¼Œå¹¶ä¸”åœ¨ç—…å˜åˆ†å‰²ä¸­è¾¾åˆ°äº† 93.4% çš„ AP å’Œ 5.3%
    çš„ FNRã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹ 3D-Mask-RCNN ä¼˜äº 2D å¯¹åº”æ¨¡å‹ Mask-RCNN å’Œ Faster-RCNNã€‚
- en: V Challenges
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V æŒ‘æˆ˜
- en: This section looks at the challenges faced in deep learning for breast tumour
    diagnosis.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚è®¨è®ºäº†åœ¨ä¹³è…ºè‚¿ç˜¤è¯Šæ–­ä¸­çš„æ·±åº¦å­¦ä¹ æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚
- en: V-A Small Dataset
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A å°æ•°æ®é›†
- en: Small datasets pose a challenge to the training of deep learning models [[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)], due to the need to familiarize the model
    with all possible cases to minimize classification errors. In addition, the lack
    of standardized datasets makes comparing and reproducing studies difficult [[94](#bib.bib94)].
    A training dataset for deep learning models should provide normal mammograms,
    mammograms with a variety of BI-RADS 1-6, mass types, calcification, asymmetries,
    architectural distortion cases, and several cases in one mammogram [[95](#bib.bib95)].
    Although techniques such as data augmentation, batch normalization, and transfer
    learning have been used to situate limited dataset sizes, large and well-curated
    datasets are still a high necessity for a well-trained model [[21](#bib.bib21),
    [96](#bib.bib96), [97](#bib.bib97)]. Yousefi et al. [[71](#bib.bib71)] used data
    augmentation techniques to increase the sample size from 5,040 to 40,320 2D slices.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å°æ•°æ®é›†å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ„æˆæŒ‘æˆ˜ [[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)]ï¼Œå› ä¸ºéœ€è¦ä½¿æ¨¡å‹ç†Ÿæ‚‰æ‰€æœ‰å¯èƒ½çš„æƒ…å†µï¼Œä»¥æœ€å°åŒ–åˆ†ç±»é”™è¯¯ã€‚æ­¤å¤–ï¼Œç¼ºä¹æ ‡å‡†åŒ–æ•°æ®é›†ä½¿å¾—æ¯”è¾ƒå’Œé‡ç°ç ”ç©¶å˜å¾—å›°éš¾
    [[94](#bib.bib94)]ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†åº”æä¾›æ­£å¸¸ä¹³è…ºXå…‰ç‰‡ã€å„ç§ BI-RADS 1-6 çš„ä¹³è…ºXå…‰ç‰‡ã€è‚¿å—ç±»å‹ã€é’™åŒ–ã€ä¸å¯¹ç§°ã€ç»“æ„ç•¸å˜æ¡ˆä¾‹ä»¥åŠä¸€å¼ ä¹³è…ºXå…‰ç‰‡ä¸­çš„å¤šä¸ªæ¡ˆä¾‹
    [[95](#bib.bib95)]ã€‚è™½ç„¶å·²ä½¿ç”¨æ•°æ®å¢å¼ºã€æ‰¹é‡å½’ä¸€åŒ–å’Œè¿ç§»å­¦ä¹ ç­‰æŠ€æœ¯æ¥åº”å¯¹æœ‰é™æ•°æ®é›†å¤§å°çš„é—®é¢˜ï¼Œä½†å¤§å‹ä¸”ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä»ç„¶æ˜¯è®­ç»ƒè‰¯å¥½æ¨¡å‹çš„é«˜åº¦éœ€æ±‚
    [[21](#bib.bib21), [96](#bib.bib96), [97](#bib.bib97)]ã€‚Yousefi ç­‰äºº [[71](#bib.bib71)]
    ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯å°†æ ·æœ¬æ•°é‡ä» 5,040 å¢åŠ åˆ° 40,320 2D åˆ‡ç‰‡ã€‚
- en: V-B Class Imbalance
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B ç±»åˆ«ä¸å¹³è¡¡
- en: Class imbalance is another challenge for deep learning, and occurs when classes
    have different ratios of training data [[98](#bib.bib98), [48](#bib.bib48), [99](#bib.bib99),
    [100](#bib.bib100), [46](#bib.bib46)]. Class imbalance can cause biases in the
    classifier, resulting in predictions skewed towards the positive or negative class
    depending on the data size ratio between classes [[96](#bib.bib96)]. In addition,
    metrics used for measuring model performance, such as accuracy [[47](#bib.bib47)],
    are susceptible to class imbalance and can affect the performance of the model.
    However, there are techniques to deal with class imbalance on both the data level
    and classifier level. Techniques for the data level are random undersampling and
    random oversampling [[101](#bib.bib101), [48](#bib.bib48), [98](#bib.bib98)],
    while the classifier level are cost-sensitive learning and thresholding [[48](#bib.bib48),
    [98](#bib.bib98)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ä¸å¹³è¡¡æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å¦ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå®ƒå‘ç”Ÿåœ¨ç±»åˆ«çš„è®­ç»ƒæ•°æ®æ¯”ä¾‹ä¸åŒçš„æƒ…å†µä¸‹[[98](#bib.bib98), [48](#bib.bib48), [99](#bib.bib99),
    [100](#bib.bib100), [46](#bib.bib46)]ã€‚ç±»åˆ«ä¸å¹³è¡¡å¯èƒ½å¯¼è‡´åˆ†ç±»å™¨äº§ç”Ÿåå·®ï¼Œä»è€Œä½¿é¢„æµ‹åå‘äºæ­£ç±»æˆ–è´Ÿç±»ï¼Œè¿™å–å†³äºç±»åˆ«ä¹‹é—´çš„æ•°æ®å¤§å°æ¯”ä¾‹[[96](#bib.bib96)]ã€‚æ­¤å¤–ï¼Œç”¨äºè¡¡é‡æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®ç‡[[47](#bib.bib47)]ï¼Œä¹Ÿå®¹æ˜“å—åˆ°ç±»åˆ«ä¸å¹³è¡¡çš„å½±å“ï¼Œä»è€Œå½±å“æ¨¡å‹çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œæœ‰æŠ€æœ¯å¯ä»¥åœ¨æ•°æ®å±‚é¢å’Œåˆ†ç±»å™¨å±‚é¢å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ã€‚æ•°æ®å±‚é¢çš„æŠ€æœ¯åŒ…æ‹¬éšæœºæ¬ é‡‡æ ·å’Œéšæœºè¿‡é‡‡æ ·[[101](#bib.bib101),
    [48](#bib.bib48), [98](#bib.bib98)]ï¼Œè€Œåˆ†ç±»å™¨å±‚é¢çš„æŠ€æœ¯åŒ…æ‹¬æˆæœ¬æ•æ„Ÿå­¦ä¹ å’Œé˜ˆå€¼è°ƒæ•´[[48](#bib.bib48), [98](#bib.bib98)]ã€‚
- en: V-C Computational Cost and Memory Constraint
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C è®¡ç®—æˆæœ¬å’Œå†…å­˜çº¦æŸ
- en: Memory constraint is an issue when dealing with training data with a large feature
    space, such as high resolution or high dimensional images [[96](#bib.bib96), [102](#bib.bib102),
    [103](#bib.bib103), [74](#bib.bib74)]. For example, when training a 3D CNN from
    scratch with DBT data with large feature spaces, computational and memory cost
    dramatically increase.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜çº¦æŸåœ¨å¤„ç†å…·æœ‰å¤§ç‰¹å¾ç©ºé—´çš„è®­ç»ƒæ•°æ®æ—¶æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚é«˜åˆ†è¾¨ç‡æˆ–é«˜ç»´å›¾åƒ[[96](#bib.bib96), [102](#bib.bib102),
    [103](#bib.bib103), [74](#bib.bib74)]ã€‚ä¾‹å¦‚ï¼Œå½“ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå…·æœ‰å¤§ç‰¹å¾ç©ºé—´çš„DBTæ•°æ®çš„3D CNNæ—¶ï¼Œè®¡ç®—å’Œå†…å­˜æˆæœ¬ä¼šæ˜¾è‘—å¢åŠ ã€‚
- en: V-D Image Quality Variability
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D å›¾åƒè´¨é‡å˜å¼‚æ€§
- en: The image quality depends on the system settings and manufacturer specification
    of the medical screening device, while the performance of models depends on the
    image quality [[56](#bib.bib56)]. Images from breast cancer screenings with poor
    resolution, sharpness, contrast, focus, or high noise can hinder the model during
    training, predictions, or localization of lesions [[17](#bib.bib17)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒè´¨é‡ä¾èµ–äºåŒ»ç–—ç­›æŸ¥è®¾å¤‡çš„ç³»ç»Ÿè®¾ç½®å’Œåˆ¶é€ å•†è§„æ ¼ï¼Œè€Œæ¨¡å‹çš„æ€§èƒ½åˆ™ä¾èµ–äºå›¾åƒè´¨é‡[[56](#bib.bib56)]ã€‚ä¹³è…ºç™Œç­›æŸ¥ä¸­å›¾åƒçš„åˆ†è¾¨ç‡ã€æ¸…æ™°åº¦ã€å¯¹æ¯”åº¦ã€ç„¦ç‚¹æˆ–å™ªå£°è¿‡é«˜ä¼šåœ¨è®­ç»ƒã€é¢„æµ‹æˆ–ç—…ç¶å®šä½è¿‡ç¨‹ä¸­å¯¹æ¨¡å‹é€ æˆé˜»ç¢[[17](#bib.bib17)]ã€‚
- en: VI Future Perspective
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI æœªæ¥å±•æœ›
- en: Neural network models discussed in this paper have achieved promising results
    in classification, detection, and segmentation tasks for breast tumours. However,
    further exploration of different architectures should be made to expand possible
    solutions to common issues, such as costly computations and memory usage, redundancies
    in learning 3D data, and robustness. In addition, there is a need for architectures
    to evaluate DBT, MRI, and ABUS images with higher confidence, efficiency, and
    speed. Furthermore, state-of-the-art models require a high level of robustness
    and confidence to display the required level of integrity to act as a "second
    opinion" for radiologist.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡è®¨è®ºçš„ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨ä¹³è…ºè‚¿ç˜¤çš„åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœã€‚ç„¶è€Œï¼Œåº”è¿›ä¸€æ­¥æ¢ç´¢ä¸åŒçš„æ¶æ„ï¼Œä»¥æ‰©å±•å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³å¸¸è§é—®é¢˜ï¼Œå¦‚è®¡ç®—æˆæœ¬å’Œå†…å­˜ä½¿ç”¨ã€3Dæ•°æ®å­¦ä¹ ä¸­çš„å†—ä½™ä»¥åŠé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæ›´é«˜æ•ˆã€æ›´å¿«é€Ÿã€æ›´è‡ªä¿¡åœ°è¯„ä¼°DBTã€MRIå’ŒABUSå›¾åƒçš„æ¶æ„ã€‚æ­¤å¤–ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹éœ€è¦å…·å¤‡é«˜åº¦çš„é²æ£’æ€§å’Œè‡ªä¿¡ï¼Œä»¥æ˜¾ç¤ºå‡ºæ‰€éœ€çš„å®Œæ•´æ€§ï¼Œä½œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿçš„â€œç¬¬äºŒæ„è§â€ã€‚
- en: VII Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII ç»“è®º
- en: Deep learning has shown significant growth in supervised-learning, and continues
    to grow towards better facilitating radiologists in workflow and decision-making.
    This paper provided an overview on deep learning theory, the effectiveness of
    different deep learning architectures for breast cancer screening, and challenges
    faced by deep learning. Moreover, this paper also aimed to establish a clear understanding
    of current progress in deep learning for breast tumour diagnosis, so future directions
    are easily discernible.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ åœ¨ç›‘ç£å­¦ä¹ é¢†åŸŸå±•ç¤ºäº†æ˜¾è‘—çš„å¢é•¿ï¼Œå¹¶ç»§ç»­æœç€æ›´å¥½åœ°æ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨å·¥ä½œæµç¨‹å’Œå†³ç­–ä¸­çš„æ–¹å‘å‘å±•ã€‚æœ¬æ–‡æ¦‚è¿°äº†æ·±åº¦å­¦ä¹ ç†è®ºã€ä¸åŒæ·±åº¦å­¦ä¹ æ¶æ„åœ¨ä¹³è…ºç™Œç­›æŸ¥ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ—¨åœ¨å»ºç«‹å¯¹ä¹³è…ºè‚¿ç˜¤è¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ å½“å‰è¿›å±•çš„æ¸…æ™°ç†è§£ï¼Œä»¥ä¾¿æœªæ¥æ–¹å‘èƒ½å¤Ÿæ¸…æ™°å¯è§ã€‚
- en: References
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] H.Â Sung, J.Â Ferlay, R.Â L. Siegel, M.Â Laversanne, I.Â Soerjomataram, A.Â Jemal,
    and F.Â Bray, â€œGlobal cancer statistics 2020: Globocan estimates of incidence and
    mortality worldwide for 36 cancers in 185 countries,â€ *CA: a cancer journal for
    clinicians*, vol.Â 71, no.Â 3, p. 212, 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Sung, J. Ferlay, R. L. Siegel, M. Laversanne, I. Soerjomataram, A. Jemal,
    å’Œ F. Brayï¼Œã€Š2020å¹´å…¨çƒç™Œç—‡ç»Ÿè®¡æ•°æ®ï¼šGlobocanå¯¹185ä¸ªå›½å®¶36ç§ç™Œç—‡å‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¼°è®¡ã€‹ï¼Œ*CAï¼šä¸´åºŠç™Œç—‡æ‚å¿—*ï¼Œç¬¬71å·ï¼Œç¬¬3æœŸï¼Œç¬¬212é¡µï¼Œ2021å¹´ã€‚'
- en: '[2] A.Â C. Society, â€œBreast cancer facts & figures 2019â€“2020,â€ *Am. Cancer Soc*,
    pp. 1â€“44, 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. C. Societyï¼Œã€Š2019â€“2020å¹´ä¹³è…ºç™Œäº‹å®ä¸æ•°æ®ã€‹ï¼Œ*ç¾å›½ç™Œç—‡å­¦ä¼š*ï¼Œç¬¬1â€“44é¡µï¼Œ2019å¹´ã€‚'
- en: '[3] C.Â P.Â A. Cancer, â€œBreast cancer screening in canada; monitoring & evaluation
    of quality indicatorsâ€“results report, january 2011 to december 2012,â€ 2017.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. P. A. Cancerï¼Œã€ŠåŠ æ‹¿å¤§ä¹³è…ºç™Œç­›æŸ¥ï¼›è´¨é‡æŒ‡æ ‡çš„ç›‘æµ‹ä¸è¯„ä¼°â€”â€”ç»“æœæŠ¥å‘Šï¼Œ2011å¹´1æœˆåˆ°2012å¹´12æœˆã€‹ï¼Œ2017å¹´ã€‚'
- en: '[4] M.Â LÃ¸berg, M.Â L. Lousdal, M.Â Bretthauer, and M.Â Kalager, â€œBenefits and
    harms of mammography screening,â€ *Breast Cancer Research*, vol.Â 17, no.Â 1, pp.
    1â€“12, 2015.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. LÃ¸berg, M. L. Lousdal, M. Bretthauer, å’Œ M. Kalagerï¼Œã€Šä¹³è…ºXå…‰æ£€æŸ¥çš„ç›Šå¤„ä¸é£é™©ã€‹ï¼Œ*ä¹³è…ºç™Œç ”ç©¶*ï¼Œç¬¬17å·ï¼Œç¬¬1æœŸï¼Œç¬¬1â€“12é¡µï¼Œ2015å¹´ã€‚'
- en: '[5] A.Â Chong, S.Â P. Weinstein, E.Â S. McDonald, and E.Â F. Conant, â€œDigital breast
    tomosynthesis: concepts and clinical practice,â€ *Radiology*, vol. 292, no.Â 1,
    pp. 1â€“14, 2019.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Chong, S. P. Weinstein, E. S. McDonald, å’Œ E. F. Conantï¼Œã€Šæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼šæ¦‚å¿µä¸ä¸´åºŠå®è·µã€‹ï¼Œ*æ”¾å°„å­¦*ï¼Œç¬¬292å·ï¼Œç¬¬1æœŸï¼Œç¬¬1â€“14é¡µï¼Œ2019å¹´ã€‚'
- en: '[6] M.Â L. Marinovich, K.Â E. Hunter, P.Â Macaskill, and N.Â Houssami, â€œBreast
    cancer screening using tomosynthesis or mammography: a meta-analysis of cancer
    detection and recall,â€ *JNCI: Journal of the National Cancer Institute*, vol.
    110, no.Â 9, pp. 942â€“949, 2018.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. L. Marinovich, K. E. Hunter, P. Macaskill, å’Œ N. Houssamiï¼Œã€Šä½¿ç”¨æ–­å±‚åˆæˆæˆ–ä¹³è…ºXå…‰æ£€æŸ¥è¿›è¡Œä¹³è…ºç™Œç­›æŸ¥ï¼šç™Œç—‡æ£€æµ‹å’Œå›å¬çš„èŸèƒåˆ†æã€‹ï¼Œ*å›½å®¶ç™Œç—‡ç ”ç©¶æ‰€æ‚å¿—*ï¼Œç¬¬110å·ï¼Œç¬¬9æœŸï¼Œç¬¬942â€“949é¡µï¼Œ2018å¹´ã€‚'
- en: '[7] D.Â B. Kopans, â€œDigital breast tomosynthesis from concept to clinical care,â€
    *American Journal of Roentgenology*, vol. 202, no.Â 2, pp. 299â€“308, 2014.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. B. Kopansï¼Œã€Šæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆä»æ¦‚å¿µåˆ°ä¸´åºŠåº”ç”¨ã€‹ï¼Œ*ç¾å›½æ”¾å°„å­¦æ‚å¿—*ï¼Œç¬¬202å·ï¼Œç¬¬2æœŸï¼Œç¬¬299â€“308é¡µï¼Œ2014å¹´ã€‚'
- en: '[8] M.Â L. Giger, M.Â F. Inciardi, A.Â Edwards, J.Â Papaioannou, K.Â Drukker, Y.Â Jiang,
    R.Â Brem, and J.Â B. Brown, â€œAutomated breast ultrasound in breast cancer screening
    of women with dense breasts: reader study of mammography-negative and mammography-positive
    cancers,â€ *American Journal of roentgenology*, vol. 206, no.Â 6, pp. 1341â€“1350,
    2016.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. L. Giger, M. F. Inciardi, A. Edwards, J. Papaioannou, K. Drukker, Y.
    Jiang, R. Brem, å’Œ J. B. Brownï¼Œã€Šè‡ªåŠ¨åŒ–ä¹³è…ºè¶…å£°åœ¨å¯†é›†ä¹³è…ºå¥³æ€§ä¹³è…ºç™Œç­›æŸ¥ä¸­çš„åº”ç”¨ï¼šå¯¹ä¹³è…ºXå…‰æ£€æŸ¥é˜´æ€§å’Œé˜³æ€§ç™Œç—‡çš„è¯»è€…ç ”ç©¶ã€‹ï¼Œ*ç¾å›½æ”¾å°„å­¦æ‚å¿—*ï¼Œç¬¬206å·ï¼Œç¬¬6æœŸï¼Œç¬¬1341â€“1350é¡µï¼Œ2016å¹´ã€‚'
- en: '[9] K.Â Doi, â€œComputer-aided diagnosis in medical imaging: historical review,
    current status and future potential,â€ *Computerized medical imaging and graphics*,
    vol.Â 31, no. 4-5, pp. 198â€“211, 2007.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] K. Doiï¼Œã€ŠåŒ»å­¦å½±åƒä¸­çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼šå†å²å›é¡¾ã€ç°çŠ¶ä¸æœªæ¥æ½œåŠ›ã€‹ï¼Œ*è®¡ç®—æœºåŒ»å­¦å½±åƒä¸å›¾å½¢*ï¼Œç¬¬31å·ï¼Œç¬¬4-5æœŸï¼Œç¬¬198â€“211é¡µï¼Œ2007å¹´ã€‚'
- en: '[10] M.Â L. Giger, N.Â Karssemeijer, and J.Â A. Schnabel, â€œBreast image analysis
    for risk assessment, detection, diagnosis, and treatment of cancer,â€ *Annual review
    of biomedical engineering*, vol.Â 15, pp. 327â€“357, 2013.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. L. Giger, N. Karssemeijer, å’Œ J. A. Schnabelï¼Œã€Šç”¨äºé£é™©è¯„ä¼°ã€æ£€æµ‹ã€è¯Šæ–­å’Œæ²»ç–—ç™Œç—‡çš„ä¹³è…ºå½±åƒåˆ†æã€‹ï¼Œ*ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å¹´åº¦ç»¼è¿°*ï¼Œç¬¬15å·ï¼Œç¬¬327â€“357é¡µï¼Œ2013å¹´ã€‚'
- en: '[11] J.-Z. Cheng, Y.-H. Chou, C.-S. Huang, Y.-C. Chang, C.-M. Tiu, K.-W. Chen,
    and C.-M. Chen, â€œComputer-aided us diagnosis of breast lesions by using cell-based
    contour grouping,â€ *Radiology*, vol. 255, no.Â 3, pp. 746â€“754, 2010.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J.-Z. Cheng, Y.-H. Chou, C.-S. Huang, Y.-C. Chang, C.-M. Tiu, K.-W. Chen,
    å’Œ C.-M. Chenï¼Œã€Šåˆ©ç”¨åŸºäºç»†èƒçš„è½®å»“åˆ†ç»„è¿›è¡Œä¹³è…ºç—…å˜çš„è®¡ç®—æœºè¾…åŠ©è¶…å£°è¯Šæ–­ã€‹ï¼Œ*æ”¾å°„å­¦*ï¼Œç¬¬255å·ï¼Œç¬¬3æœŸï¼Œç¬¬746â€“754é¡µï¼Œ2010å¹´ã€‚'
- en: '[12] B.Â VanÂ Ginneken, C.Â M. Schaefer-Prokop, and M.Â Prokop, â€œComputer-aided
    diagnosis: how to move from the laboratory to the clinic,â€ *Radiology*, vol. 261,
    no.Â 3, pp. 719â€“732, 2011.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] B. Van Ginneken, C. M. Schaefer-Prokop, å’Œ M. Prokopï¼Œã€Šè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼šå¦‚ä½•ä»å®éªŒå®¤è½¬å‘ä¸´åºŠã€‹ï¼Œ*æ”¾å°„å­¦*ï¼Œç¬¬261å·ï¼Œç¬¬3æœŸï¼Œç¬¬719â€“732é¡µï¼Œ2011å¹´ã€‚'
- en: '[13] K.Â H. Allison, L.Â M. Reisch, P.Â A. Carney, D.Â L. Weaver, S.Â J. Schnitt,
    F.Â P. Oâ€™Malley, B.Â M. Geller, and J.Â G. Elmore, â€œUnderstanding diagnostic variability
    in breast pathology: lessons learned from an expert consensus review panel,â€ *Histopathology*,
    vol.Â 65, no.Â 2, pp. 240â€“251, 2014.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K. H. Allison, L. M. Reisch, P. A. Carney, D. L. Weaver, S. J. Schnitt,
    F. P. Oâ€™Malley, B. M. Geller, å’Œ J. G. Elmoreï¼Œã€Šç†è§£ä¹³è…ºç—…ç†è¯Šæ–­çš„å˜å¼‚æ€§ï¼šä»ä¸“å®¶å…±è¯†è¯„å®¡å°ç»„ä¸­å­¦åˆ°çš„ç»éªŒæ•™è®­ã€‹ï¼Œ*ç»„ç»‡ç—…ç†å­¦*ï¼Œç¬¬65å·ï¼Œç¬¬2æœŸï¼Œç¬¬240â€“251é¡µï¼Œ2014å¹´ã€‚'
- en: '[14] L.Â J. Grimm, A.Â L. Anderson, J.Â A. Baker, K.Â S. Johnson, R.Â Walsh, S.Â C.
    Yoon, and S.Â V. Ghate, â€œInterobserver variability between breast imagers using
    the fifth edition of the bi-rads mri lexicon,â€ *American Journal of Roentgenology*,
    vol. 204, no.Â 5, pp. 1120â€“1124, 2015.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. J. Grimm, A. L. Anderson, J. A. Baker, K. S. Johnson, R. Walsh, S.
    C. Yoon, å’Œ S. V. Ghate, â€œä½¿ç”¨ç¬¬äº”ç‰ˆBI-RADS MRIè¯æ±‡è¡¨çš„ä¹³è…ºå½±åƒåŒ»å¸ˆä¹‹é—´çš„è§‚å¯Ÿè€…å˜å¼‚æ€§ï¼Œâ€ *ç¾å›½æ”¾å°„å­¦æ‚å¿—*ï¼Œç¬¬204å·ï¼Œç¬¬5æœŸï¼Œé¡µç 1120â€“1124ï¼Œ2015å¹´ã€‚'
- en: '[15] D.Â Wang, A.Â Khosla, R.Â Gargeya, H.Â Irshad, and A.Â H. Beck, â€œDeep learning
    for identifying metastatic breast cancer,â€ *arXiv preprint arXiv:1606.05718*,
    2016.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Wang, A. Khosla, R. Gargeya, H. Irshad, å’Œ A. H. Beck, â€œç”¨äºè¯†åˆ«è½¬ç§»æ€§ä¹³è…ºç™Œçš„æ·±åº¦å­¦ä¹ ï¼Œâ€
    *arXiv é¢„å°æœ¬ arXiv:1606.05718*ï¼Œ2016å¹´ã€‚'
- en: '[16] S.Â M. McKinney, M.Â Sieniek, V.Â Godbole, J.Â Godwin, N.Â Antropova, H.Â Ashrafian,
    T.Â Back, M.Â Chesus, G.Â S. Corrado, A.Â Darzi *etÂ al.*, â€œInternational evaluation
    of an ai system for breast cancer screening,â€ *Nature*, vol. 577, no. 7788, pp.
    89â€“94, 2020.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova, H. Ashrafian,
    T. Back, M. Chesus, G. S. Corrado, A. Darzi *ç­‰*ï¼Œâ€œå›½é™…è¯„ä¼°ä¹³è…ºç™Œç­›æŸ¥çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œâ€ *è‡ªç„¶*ï¼Œç¬¬577å·ï¼Œç¬¬7788æœŸï¼Œé¡µç 89â€“94ï¼Œ2020å¹´ã€‚'
- en: '[17] A.Â Ibrahim, P.Â Gamble, R.Â Jaroensri, M.Â M. Abdelsamea, C.Â H. Mermel, P.-H.Â C.
    Chen, and E.Â A. Rakha, â€œArtificial intelligence in digital breast pathology: techniques
    and applications,â€ *The Breast*, vol.Â 49, pp. 267â€“273, 2020.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Ibrahim, P. Gamble, R. Jaroensri, M. M. Abdelsamea, C. H. Mermel, P.-H.
    C. Chen, å’Œ E. A. Rakha, â€œæ•°å­—ä¹³è…ºç—…ç†ä¸­çš„äººå·¥æ™ºèƒ½ï¼šæŠ€æœ¯ä¸åº”ç”¨ï¼Œâ€ *ä¹³è…º*ï¼Œç¬¬49å·ï¼Œé¡µç 267â€“273ï¼Œ2020å¹´ã€‚'
- en: '[18] I.Â Goodfellow, Y.Â Bengio, and A.Â Courville, *Deep Learning*.Â Â Â MIT Press,
    2016, [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] I. Goodfellow, Y. Bengio, å’Œ A. Courville, *æ·±åº¦å­¦ä¹ *ã€‚MITå‡ºç‰ˆç¤¾ï¼Œ2016å¹´ï¼Œ[http://www.deeplearningbook.org](http://www.deeplearningbook.org)ã€‚'
- en: '[19] S.Â P. Singh, L.Â Wang, S.Â Gupta, H.Â Goli, P.Â Padmanabhan, and B.Â GulyÃ¡s,
    â€œ3d deep learning on medical images: a review,â€ *Sensors*, vol.Â 20, no.Â 18, p.
    5097, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. P. Singh, L. Wang, S. Gupta, H. Goli, P. Padmanabhan, å’Œ B. GulyÃ¡s,
    â€œåŒ»å­¦å½±åƒä¸Šçš„3Dæ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ï¼Œâ€ *ä¼ æ„Ÿå™¨*ï¼Œç¬¬20å·ï¼Œç¬¬18æœŸï¼Œé¡µç 5097ï¼Œ2020å¹´ã€‚'
- en: '[20] A.Â Krizhevsky, I.Â Sutskever, and G.Â E. Hinton, â€œImagenet classification
    with deep convolutional neural networks,â€ *Advances in neural information processing
    systems*, vol.Â 25, pp. 1097â€“1105, 2012.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Krizhevsky, I. Sutskever, å’Œ G. E. Hinton, â€œä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡ŒImageNetåˆ†ç±»ï¼Œâ€
    *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œç¬¬25å·ï¼Œé¡µç 1097â€“1105ï¼Œ2012å¹´ã€‚'
- en: '[21] K.Â Suzuki, â€œOverview of deep learning in medical imaging,â€ *Radiological
    physics and technology*, vol.Â 10, no.Â 3, pp. 257â€“273, 2017.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. Suzuki, â€œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒä¸­çš„æ¦‚è¿°ï¼Œâ€ *æ”¾å°„ç‰©ç†ä¸æŠ€æœ¯*ï¼Œç¬¬10å·ï¼Œç¬¬3æœŸï¼Œé¡µç 257â€“273ï¼Œ2017å¹´ã€‚'
- en: '[22] R.Â Yamashita, M.Â Nishio, R.Â K.Â G. Do, and K.Â Togashi, â€œConvolutional neural
    networks: an overview and application in radiology,â€ *Insights into imaging*,
    vol.Â 9, no.Â 4, pp. 611â€“629, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Yamashita, M. Nishio, R. K. G. Do, å’Œ K. Togashi, â€œå·ç§¯ç¥ç»ç½‘ç»œï¼šæ¦‚è¿°åŠå…¶åœ¨æ”¾å°„å­¦ä¸­çš„åº”ç”¨ï¼Œâ€
    *å½±åƒå­¦å‰æ²¿*ï¼Œç¬¬9å·ï¼Œç¬¬4æœŸï¼Œé¡µç 611â€“629ï¼Œ2018å¹´ã€‚'
- en: '[23] J.Â Gu, Z.Â Wang, J.Â Kuen, L.Â Ma, A.Â Shahroudy, B.Â Shuai, T.Â Liu, X.Â Wang,
    G.Â Wang, J.Â Cai *etÂ al.*, â€œRecent advances in convolutional neural networks,â€
    *Pattern Recognition*, vol.Â 77, pp. 354â€“377, 2018.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *ç­‰*ï¼Œâ€œå·ç§¯ç¥ç»ç½‘ç»œçš„æœ€æ–°è¿›å±•ï¼Œâ€ *æ¨¡å¼è¯†åˆ«*ï¼Œç¬¬77å·ï¼Œé¡µç 354â€“377ï¼Œ2018å¹´ã€‚'
- en: '[24] F.Â Amherd and E.Â Rodriguez, â€œHeatmap-based object detection and tracking
    with a fully convolutional neural network,â€ *arXiv preprint arXiv:2101.03541*,
    2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] F. Amherd å’Œ E. Rodriguez, â€œåŸºäºçƒ­å›¾çš„ç‰©ä½“æ£€æµ‹ä¸è·Ÿè¸ªï¼Œä½¿ç”¨å…¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€ *arXiv é¢„å°æœ¬ arXiv:2101.03541*ï¼Œ2021å¹´ã€‚'
- en: '[25] S.Â Zheng, Y.Â Song, T.Â Leung, and I.Â Goodfellow, â€œImproving the robustness
    of deep neural networks via stability training,â€ *arXiv preprint arXiv:1604.04326*,
    2016.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Zheng, Y. Song, T. Leung, å’Œ I. Goodfellow, â€œé€šè¿‡ç¨³å®šæ€§è®­ç»ƒæé«˜æ·±åº¦ç¥ç»ç½‘ç»œçš„é²æ£’æ€§ï¼Œâ€ *arXiv
    é¢„å°æœ¬ arXiv:1604.04326*ï¼Œ2016å¹´ã€‚'
- en: '[26] A.Â LeNail, â€œNn-svg: Publication-ready neural network architecture schematics,â€
    *Journal of Open Source Software*, vol.Â 4, no.Â 33, p. 747, 2019\. [Online]. Available:
    [https://doi.org/10.21105/joss.00747](https://doi.org/10.21105/joss.00747)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. LeNail, â€œNN-SVGï¼šé€‚åˆå‡ºç‰ˆçš„ç¥ç»ç½‘ç»œæ¶æ„ç¤ºæ„å›¾ï¼Œâ€ *å¼€æºè½¯ä»¶æ‚å¿—*ï¼Œç¬¬4å·ï¼Œç¬¬33æœŸï¼Œé¡µç 747ï¼Œ2019å¹´ã€‚[åœ¨çº¿].
    å¯ç”¨ï¼š[https://doi.org/10.21105/joss.00747](https://doi.org/10.21105/joss.00747)'
- en: '[27] A.Â Zhang, Z.Â C. Lipton, M.Â Li, and A.Â J. Smola, â€œDive into deep learning,â€
    *arXiv preprint arXiv:2106.11342*, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Zhang, Z. C. Lipton, M. Li, å’Œ A. J. Smola, â€œæ·±å…¥æ¢ç´¢æ·±åº¦å­¦ä¹ ï¼Œâ€ *arXiv é¢„å°æœ¬ arXiv:2106.11342*ï¼Œ2021å¹´ã€‚'
- en: '[28] K.Â Oâ€™Shea and R.Â Nash, â€œAn introduction to convolutional neural networks,â€
    *arXiv preprint arXiv:1511.08458*, 2015.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] K. Oâ€™Shea å’Œ R. Nash, â€œå·ç§¯ç¥ç»ç½‘ç»œå…¥é—¨ï¼Œâ€ *arXiv é¢„å°æœ¬ arXiv:1511.08458*ï¼Œ2015å¹´ã€‚'
- en: '[29] J.Â Wu, â€œIntroduction to convolutional neural networks,â€ *National Key
    Lab for Novel Software Technology. Nanjing University. China*, vol.Â 5, no.Â 23,
    p. 495, 2017.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Wu, â€œå·ç§¯ç¥ç»ç½‘ç»œç®€ä»‹ï¼Œâ€ *å›½å®¶æ–°å‹è½¯ä»¶æŠ€æœ¯é‡ç‚¹å®éªŒå®¤ï¼Œå—äº¬å¤§å­¦ï¼Œä¸­å›½*ï¼Œç¬¬5å·ï¼Œç¬¬23æœŸï¼Œé¡µç 495ï¼Œ2017å¹´ã€‚'
- en: '[30] G.Â Kang, K.Â Liu, B.Â Hou, and N.Â Zhang, â€œ3d multi-view convolutional neural
    networks for lung nodule classification,â€ *PloS one*, vol.Â 12, no.Â 11, p. e0188290,
    2017.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J.Â D. Hunter, â€œMatplotlib: A 2d graphics environment,â€ *Computing in Science
    & Engineering*, vol.Â 9, no.Â 3, pp. 90â€“95, 2007.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y.Â LeCun, Y.Â Bengio, and G.Â Hinton, â€œDeep learning,â€ *nature*, vol. 521,
    no. 7553, pp. 436â€“444, 2015.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P.Â Ramachandran, B.Â Zoph, and Q.Â V. Le, â€œSearching for activation functions,â€
    *arXiv preprint arXiv:1710.05941*, 2017.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C.Â Nwankpa, W.Â Ijomah, A.Â Gachagan, and S.Â Marshall, â€œActivation functions:
    Comparison of trends in practice and research for deep learning,â€ *arXiv preprint
    arXiv:1811.03378*, 2018.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M.Â Sun, Z.Â Song, X.Â Jiang, J.Â Pan, and Y.Â Pang, â€œLearning pooling for
    convolutional neural network,â€ *Neurocomputing*, vol. 224, pp. 96â€“104, 2017.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M.Â A. Mazurowski, M.Â Buda, A.Â Saha, and M.Â R. Bashir, â€œDeep learning in
    radiology: an overview of the concepts and a survey of the state of the art,â€
    *arXiv preprint arXiv:1802.08717*, 2018.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S.Â Ruder, â€œAn overview of gradient descent optimization algorithms,â€ *arXiv
    preprint arXiv:1609.04747*, 2016.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X.Â Ying, â€œAn overview of overfitting and its solutions,â€ in *Journal of
    Physics: Conference Series*, vol. 1168, no.Â 2.Â Â Â IOP Publishing, 2019, p. 022022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] W.Â S. Sarle *etÂ al.*, â€œStopped training and other remedies for overfitting,â€
    *Computing science and statistics*, pp. 352â€“360, 1996.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z.Â Liu, M.Â Sun, T.Â Zhou, G.Â Huang, and T.Â Darrell, â€œRethinking the value
    of network pruning,â€ *arXiv preprint arXiv:1810.05270*, 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] N.Â Srivastava, G.Â Hinton, A.Â Krizhevsky, I.Â Sutskever, and R.Â Salakhutdinov,
    â€œDropout: a simple way to prevent neural networks from overfitting,â€ *The journal
    of machine learning research*, vol.Â 15, no.Â 1, pp. 1929â€“1958, 2014.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K.Â Weiss, T.Â M. Khoshgoftaar, and D.Â Wang, â€œA survey of transfer learning,â€
    *Journal of Big data*, vol.Â 3, no.Â 1, pp. 1â€“40, 2016.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C.Â Tan, F.Â Sun, T.Â Kong, W.Â Zhang, C.Â Yang, and C.Â Liu, â€œA survey on deep
    transfer learning,â€ in *International conference on artificial neural networks*.Â Â Â Springer,
    2018, pp. 270â€“279.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A.Â Garcia-Garcia, S.Â Orts-Escolano, S.Â Oprea, V.Â Villena-Martinez, P.Â Martinez-Gonzalez,
    and J.Â Garcia-Rodriguez, â€œA survey on deep learning techniques for image and video
    semantic segmentation,â€ *Applied Soft Computing*, vol.Â 70, pp. 41â€“65, 2018.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J.Â Davis and M.Â Goadrich, â€œThe relationship between precision-recall and
    roc curves,â€ in *Proceedings of the 23rd international conference on Machine learning*,
    2006, pp. 233â€“240.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] L.Â A. Jeni, J.Â F. Cohn, and F.Â DeÂ LaÂ Torre, â€œFacing imbalanced dataâ€“recommendations
    for the use of performance metrics,â€ in *2013 Humaine association conference on
    affective computing and intelligent interaction*.Â Â Â IEEE, 2013, pp. 245â€“251.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D.Â Chicco and G.Â Jurman, â€œThe advantages of the matthews correlation coefficient
    (mcc) over f1 score and accuracy in binary classification evaluation,â€ *BMC genomics*,
    vol.Â 21, no.Â 1, pp. 1â€“13, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] D. Chicco å’Œ G. Jurmanï¼Œâ€œé©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰ç›¸å¯¹äºF1å¾—åˆ†å’Œå‡†ç¡®æ€§çš„ä¼˜åŠ¿ï¼Œåœ¨äºŒåˆ†ç±»è¯„ä¼°ä¸­çš„åº”ç”¨ï¼Œâ€ *BMC genomics*ï¼Œç¬¬21å·ï¼Œç¬¬1æœŸï¼Œé¡µç 1â€“13ï¼Œ2020å¹´ã€‚'
- en: '[48] J.Â M. Johnson and T.Â M. Khoshgoftaar, â€œSurvey on deep learning with class
    imbalance,â€ *Journal of Big Data*, vol.Â 6, no.Â 1, pp. 1â€“54, 2019.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. M. Johnson å’Œ T. M. Khoshgoftaarï¼Œâ€œå…³äºç±»ä¸å¹³è¡¡çš„æ·±åº¦å­¦ä¹ è°ƒæŸ¥ï¼Œâ€ *Journal of Big Data*ï¼Œç¬¬6å·ï¼Œç¬¬1æœŸï¼Œé¡µç 1â€“54ï¼Œ2019å¹´ã€‚'
- en: '[49] K.Â H. Zou, S.Â K. Warfield, A.Â Bharatha, C.Â M. Tempany, M.Â R. Kaus, S.Â J.
    Haker, W.Â M. WellsÂ III, F.Â A. Jolesz, and R.Â Kikinis, â€œStatistical validation
    of image segmentation quality based on a spatial overlap index1: scientific reports,â€
    *Academic radiology*, vol.Â 11, no.Â 2, pp. 178â€“189, 2004.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. H. Zou, S. K. Warfield, A. Bharatha, C. M. Tempany, M. R. Kaus, S.
    J. Haker, W. M. Wells III, F. A. Jolesz, å’Œ R. Kikinisï¼Œâ€œåŸºäºç©ºé—´é‡å æŒ‡æ•°çš„å›¾åƒåˆ†å‰²è´¨é‡çš„ç»Ÿè®¡éªŒè¯1ï¼šç§‘å­¦æŠ¥å‘Šï¼Œâ€
    *Academic radiology*ï¼Œç¬¬11å·ï¼Œç¬¬2æœŸï¼Œé¡µç 178â€“189ï¼Œ2004å¹´ã€‚'
- en: '[50] M.Â ElÂ Adoui, S.Â A. Mahmoudi, M.Â A. Larhmam, and M.Â Benjelloun, â€œMri breast
    tumor segmentation using different encoder and decoder cnn architectures,â€ *Computers*,
    vol.Â 8, no.Â 3, p.Â 52, 2019.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. El Adoui, S. A. Mahmoudi, M. A. Larhmam, å’Œ M. Benjellounï¼Œâ€œä½¿ç”¨ä¸åŒç¼–ç å™¨å’Œè§£ç å™¨CNNæ¶æ„è¿›è¡ŒMRIä¹³è…ºè‚¿ç˜¤åˆ†å‰²ï¼Œâ€
    *Computers*ï¼Œç¬¬8å·ï¼Œç¬¬3æœŸï¼Œé¡µç 52ï¼Œ2019å¹´ã€‚'
- en: '[51] N.Â Dhungel, G.Â Carneiro, and A.Â P. Bradley, â€œA deep learning approach
    for the analysis of masses in mammograms with minimal user intervention,â€ *Medical
    image analysis*, vol.Â 37, pp. 114â€“128, 2017.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] N. Dhungel, G. Carneiro, å’Œ A. P. Bradleyï¼Œâ€œä¸€ç§ç”¨äºåˆ†æä¹³è…ºXå…‰ç‰‡ä¸­è‚¿å—çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰æœ€å°ç”¨æˆ·å¹²é¢„ï¼Œâ€
    *Medical image analysis*ï¼Œç¬¬37å·ï¼Œé¡µç 114â€“128ï¼Œ2017å¹´ã€‚'
- en: '[52] M.Â A. Al-Antari, M.Â A. Al-Masni, M.-T. Choi, S.-M. Han, and T.-S. Kim,
    â€œA fully integrated computer-aided diagnosis system for digital x-ray mammograms
    via deep learning detection, segmentation, and classification,â€ *International
    journal of medical informatics*, vol. 117, pp. 44â€“54, 2018.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han, å’Œ T.-S. Kimï¼Œâ€œé€šè¿‡æ·±åº¦å­¦ä¹ æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»çš„å…¨åŠŸèƒ½è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿï¼Œç”¨äºæ•°å­—Xå…‰ä¹³è…ºå›¾åƒï¼Œâ€
    *International journal of medical informatics*ï¼Œç¬¬117å·ï¼Œé¡µç 44â€“54ï¼Œ2018å¹´ã€‚'
- en: '[53] H.Â Chougrad, H.Â Zouaki, and O.Â Alheyane, â€œDeep convolutional neural networks
    for breast cancer screening,â€ *Computer methods and programs in biomedicine*,
    vol. 157, pp. 19â€“30, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Chougrad, H. Zouaki, å’Œ O. Alheyaneï¼Œâ€œç”¨äºä¹³è…ºç™Œç­›æŸ¥çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€ *Computer methods
    and programs in biomedicine*ï¼Œç¬¬157å·ï¼Œé¡µç 19â€“30ï¼Œ2018å¹´ã€‚'
- en: '[54] D.Â Ribli, A.Â HorvÃ¡th, Z.Â Unger, P.Â Pollner, and I.Â Csabai, â€œDetecting
    and classifying lesions in mammograms with deep learning,â€ *Scientific reports*,
    vol.Â 8, no.Â 1, pp. 1â€“7, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. Ribli, A. HorvÃ¡th, Z. Unger, P. Pollner, å’Œ I. Csabaiï¼Œâ€œä½¿ç”¨æ·±åº¦å­¦ä¹ æ£€æµ‹å’Œåˆ†ç±»ä¹³è…ºXå…‰ç‰‡ä¸­çš„ç—…å˜ï¼Œâ€
    *Scientific reports*ï¼Œç¬¬8å·ï¼Œç¬¬1æœŸï¼Œé¡µç 1â€“7ï¼Œ2018å¹´ã€‚'
- en: '[55] V.Â K. Singh, H.Â A. Rashwan, S.Â Romani, F.Â Akram, N.Â Pandey, M.Â M.Â K. Sarker,
    A.Â Saleh, M.Â Arenas, M.Â Arquez, D.Â Puig *etÂ al.*, â€œBreast tumor segmentation and
    shape classification in mammograms using generative adversarial and convolutional
    neural network,â€ *Expert Systems with Applications*, vol. 139, p. 112855, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker,
    A. Saleh, M. Arenas, M. Arquez, D. Puig *ç­‰*ï¼Œâ€œä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œå¯¹ä¹³è…ºXå…‰ç‰‡ä¸­çš„ä¹³è…ºè‚¿ç˜¤è¿›è¡Œåˆ†å‰²å’Œå½¢çŠ¶åˆ†ç±»ï¼Œâ€
    *Expert Systems with Applications*ï¼Œç¬¬139å·ï¼Œé¡µç 112855ï¼Œ2020å¹´ã€‚'
- en: '[56] W.Â T. Tran, A.Â Sadeghi-Naini, F.-I. Lu, S.Â Gandhi, N.Â Meti, M.Â Brackstone,
    E.Â Rakovitch, and B.Â Curpen, â€œComputational radiology in breast cancer screening
    and diagnosis using artificial intelligence,â€ *Canadian Association of Radiologists
    Journal*, vol.Â 72, no.Â 1, pp. 98â€“108, 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] W. T. Tran, A. Sadeghi-Naini, F.-I. Lu, S. Gandhi, N. Meti, M. Brackstone,
    E. Rakovitch, å’Œ B. Curpenï¼Œâ€œä½¿ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œä¹³è…ºç™Œç­›æŸ¥å’Œè¯Šæ–­çš„è®¡ç®—æ”¾å°„å­¦ï¼Œâ€ *Canadian Association of Radiologists
    Journal*ï¼Œç¬¬72å·ï¼Œç¬¬1æœŸï¼Œé¡µç 98â€“108ï¼Œ2021å¹´ã€‚'
- en: '[57] E.Â F. Conant, W.Â E. Barlow, S.Â D. Herschorn, D.Â L. Weaver, E.Â F. Beaber,
    A.Â N. Tosteson, J.Â S. Haas, K.Â P. Lowry, N.Â K. Stout, A.Â Trentham-Dietz *etÂ al.*,
    â€œAssociation of digital breast tomosynthesis vs digital mammography with cancer
    detection and recall rates by age and breast density,â€ *JAMA oncology*, vol.Â 5,
    no.Â 5, pp. 635â€“642, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] E. F. Conant, W. E. Barlow, S. D. Herschorn, D. L. Weaver, E. F. Beaber,
    A. N. Tosteson, J. S. Haas, K. P. Lowry, N. K. Stout, A. Trentham-Dietz *ç­‰*ï¼Œâ€œæ•°å­—ä¹³è…ºæ–­å±‚æ‰«æä¸æ•°å­—ä¹³è…ºXå…‰ç‰‡åœ¨ç™Œç—‡æ£€æµ‹å’Œå¬å›ç‡æ–¹é¢çš„å¹´é¾„å’Œä¹³è…ºå¯†åº¦å…³è”ï¼Œâ€
    *JAMA oncology*ï¼Œç¬¬5å·ï¼Œç¬¬5æœŸï¼Œé¡µç 635â€“642ï¼Œ2019å¹´ã€‚'
- en: '[58] E.Â A. Rafferty, M.Â A. Durand, E.Â F. Conant, D.Â S. Copit, S.Â M. Friedewald,
    D.Â M. Plecha, and D.Â P. Miller, â€œBreast cancer screening using tomosynthesis and
    digital mammography in dense and nondense breasts,â€ *Jama*, vol. 315, no.Â 16,
    pp. 1784â€“1786, 2016.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] E. A. Rafferty, M. A. Durand, E. F. Conant, D. S. Copit, S. M. Friedewald,
    D. M. Plecha, å’Œ D. P. Millerï¼Œâ€œåœ¨è‡´å¯†å’Œéè‡´å¯†ä¹³è…ºä¸­ä½¿ç”¨æ–­å±‚æ‰«æå’Œæ•°å­—ä¹³è…ºXå…‰ç‰‡è¿›è¡Œä¹³è…ºç™Œç­›æŸ¥ï¼Œâ€ *Jama*ï¼Œç¬¬315å·ï¼Œç¬¬16æœŸï¼Œé¡µç 1784â€“1786ï¼Œ2016å¹´ã€‚'
- en: '[59] K.Â P. Lowry, R.Â Y. Coley, D.Â L. Miglioretti, K.Â Kerlikowske, L.Â M. Henderson,
    T.Â Onega, B.Â L. Sprague, J.Â M. Lee, S.Â Herschorn, A.Â N. Tosteson *etÂ al.*, â€œScreening
    performance of digital breast tomosynthesis vs digital mammography in community
    practice by patient age, screening round, and breast density,â€ *JAMA network open*,
    vol.Â 3, no.Â 7, pp. e2â€‰011â€‰792â€“e2â€‰011â€‰792, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] E.Â F. Conant, E.Â F. Beaber, B.Â L. Sprague, S.Â D. Herschorn, D.Â L. Weaver,
    T.Â Onega, A.Â N. Tosteson, A.Â M. McCarthy, S.Â P. Poplack, J.Â S. Haas *etÂ al.*,
    â€œBreast cancer screening using tomosynthesis in combination with digital mammography
    compared to digital mammography alone: a cohort study within the prospr consortium,â€
    *Breast cancer research and treatment*, vol. 156, no.Â 1, pp. 109â€“116, 2016.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X.-A. Phi, A.Â Tagliafico, N.Â Houssami, M.Â J. Greuter, and G.Â H. deÂ Bock,
    â€œDigital breast tomosynthesis for breast cancer screening and diagnosis in women
    with dense breastsâ€“a systematic review and meta-analysis,â€ *BMC cancer*, vol.Â 18,
    no.Â 1, pp. 1â€“9, 2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] P.Â Pattacini, A.Â Nitrosi, P.Â GiorgiÂ Rossi, V.Â Iotti, V.Â Ginocchi, S.Â Ravaioli,
    R.Â Vacondio, L.Â Braglia, S.Â Cavuto, C.Â Campari *etÂ al.*, â€œDigital mammography
    versus digital mammography plus tomosynthesis for breast cancer screening: the
    reggio emilia tomosynthesis randomized trial,â€ *Radiology*, vol. 288, no.Â 2, pp.
    375â€“385, 2018.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] J.Â A. Baker and J.Â Y. Lo, â€œBreast tomosynthesis: state-of-the-art and
    review of the literature,â€ *Academic radiology*, vol.Â 18, no.Â 10, pp. 1298â€“1310,
    2011.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J.Â V. Horvat, D.Â M. Keating, H.Â Rodrigues-Duarte, E.Â A. Morris, and V.Â L.
    Mango, â€œCalcifications at digital breast tomosynthesis: imaging features and biopsy
    techniques,â€ *Radiographics*, vol.Â 39, no.Â 2, pp. 307â€“318, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S.Â H. Kim, H.Â H. Kim, and W.Â K. Moon, â€œAutomated breast ultrasound screening
    for dense breasts,â€ *Korean journal of radiology*, vol.Â 21, no.Â 1, pp. 15â€“24,
    2020.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H.Â J. Shin, H.Â H. Kim, and J.Â H. Cha, â€œCurrent status of automated breast
    ultrasonography,â€ *Ultrasonography*, vol.Â 34, no.Â 3, p. 165, 2015.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C.Â D. Lehman, J.Â M. Lee, W.Â B. DeMartini, D.Â S. Hippe, M.Â H. Rendi, G.Â Kalish,
    P.Â Porter, J.Â Gralow, and S.Â C. Partridge, â€œScreening mri in women with a personal
    history of breast cancer,â€ *Journal of the National Cancer Institute*, vol. 108,
    no.Â 3, p. djv349, 2016.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R.Â M. Mann, N.Â Cho, and L.Â Moy, â€œBreast mri: state of the art,â€ *Radiology*,
    vol. 292, no.Â 3, pp. 520â€“536, 2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H.Â Greenspan, G.Â Oz, N.Â Kiryati, and S.Â Peled, â€œMri inter-slice reconstruction
    using super-resolution,â€ *Magnetic resonance imaging*, vol.Â 20, no.Â 5, pp. 437â€“446,
    2002.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] R.Â Z. Shilling, T.Â Q. Robbie, T.Â Bailloeul, K.Â Mewes, R.Â M. Mersereau,
    and M.Â E. Brummer, â€œA super-resolution framework for 3-d high-resolution and high-contrast
    imaging using 2-d multislice mri,â€ *IEEE transactions on medical imaging*, vol.Â 28,
    no.Â 5, pp. 633â€“644, 2008.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M.Â Yousefi, A.Â KrzyÅ¼ak, and C.Â Y. Suen, â€œMass detection in digital breast
    tomosynthesis data using convolutional neural networks and multiple instance learning,â€
    *Computers in biology and medicine*, vol.Â 96, pp. 283â€“293, 2018.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Yousefi, A. KrzyÅ¼ak, å’Œ C. Y. Suen, â€œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå’Œå¤šå®ä¾‹å­¦ä¹ è¿›è¡Œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆæ•°æ®ä¸­çš„è‚¿å—æ£€æµ‹ï¼Œâ€
    *ç”Ÿç‰©åŒ»å­¦è®¡ç®—æœº*ï¼Œç¬¬ 96 å·ï¼Œç¬¬ 283â€“293 é¡µï¼Œ2018 å¹´ã€‚'
- en: '[72] D.Â H. Kim, S.Â T. Kim, J.Â M. Chang, and Y.Â M. Ro, â€œLatent feature representation
    with depth directional long-term recurrent learning for breast masses in digital
    breast tomosynthesis,â€ *Physics in Medicine & Biology*, vol.Â 62, no.Â 3, p. 1009,
    2017.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. H. Kim, S. T. Kim, J. M. Chang, å’Œ Y. M. Ro, â€œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆä¸­ä¹³è…ºè‚¿å—çš„æ·±åº¦æ–¹å‘é•¿æœŸé€’å½’å­¦ä¹ çš„æ½œåœ¨ç‰¹å¾è¡¨ç¤ºï¼Œâ€
    *åŒ»å­¦ä¸ç”Ÿç‰©ç‰©ç†å­¦*ï¼Œç¬¬ 62 å·ï¼Œç¬¬ 3 æœŸï¼Œç¬¬ 1009 é¡µï¼Œ2017 å¹´ã€‚'
- en: '[73] S.Â Liu, D.Â Xu, S.Â K. Zhou, T.Â Mertelmeier, J.Â Wicklein, A.Â Jerebko, S.Â Grbic,
    O.Â Pauly, W.Â Cai, and D.Â Comaniciu, â€œ3d anisotropic hybrid network: Transferring
    convolutional features from 2d images to 3d anisotropic volumes,â€ *arXiv preprint
    arXiv:1711.08580*, 2017.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Liu, D. Xu, S. K. Zhou, T. Mertelmeier, J. Wicklein, A. Jerebko, S.
    Grbic, O. Pauly, W. Cai, å’Œ D. Comaniciu, â€œ3d å„å‘å¼‚æ€§æ··åˆç½‘ç»œï¼šå°†å·ç§¯ç‰¹å¾ä» 2d å›¾åƒè½¬ç§»åˆ° 3d å„å‘å¼‚æ€§ä½“ç§¯ï¼Œâ€
    *arXiv é¢„å°æœ¬ arXiv:1711.08580*ï¼Œ2017 å¹´ã€‚'
- en: '[74] Y.Â Zhang, X.Â Wang, H.Â Blanton, G.Â Liang, X.Â Xing, and N.Â Jacobs, â€œ2d convolutional
    neural networks for 3d digital breast tomosynthesis classification,â€ in *2019
    IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*.Â Â Â IEEE,
    2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Zhang, X. Wang, H. Blanton, G. Liang, X. Xing, å’Œ N. Jacobs, â€œç”¨äº 3d
    æ•°å­—ä¹³è…ºæ–­å±‚åˆæˆåˆ†ç±»çš„ 2d å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€ è§äº *2019 IEEE å›½é™…ç”Ÿç‰©ä¿¡æ¯å­¦ä¸ç”Ÿç‰©åŒ»å­¦ä¼šè®® (BIBM)*ã€‚ IEEEï¼Œ2019 å¹´ã€‚'
- en: '[75] G.Â Liang, X.Â Wang, Y.Â Zhang, X.Â Xing, H.Â Blanton, T.Â Salem, and N.Â Jacobs,
    â€œJoint 2d-3d breast cancer classification,â€ in *2019 IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*.Â Â Â IEEE, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. Liang, X. Wang, Y. Zhang, X. Xing, H. Blanton, T. Salem, å’Œ N. Jacobs,
    â€œè”åˆ 2d-3d ä¹³è…ºç™Œåˆ†ç±»ï¼Œâ€ è§äº *2019 IEEE å›½é™…ç”Ÿç‰©ä¿¡æ¯å­¦ä¸ç”Ÿç‰©åŒ»å­¦ä¼šè®® (BIBM)*ã€‚ IEEEï¼Œ2019 å¹´ã€‚'
- en: '[76] K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image
    recognition,â€ *arXiv preprint arXiv:1512.03385*, 2015.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] K. He, X. Zhang, S. Ren, å’Œ J. Sun, â€œç”¨äºå›¾åƒè¯†åˆ«çš„æ·±åº¦æ®‹å·®å­¦ä¹ ï¼Œâ€ *arXiv é¢„å°æœ¬ arXiv:1512.03385*ï¼Œ2015
    å¹´ã€‚'
- en: '[77] K.Â Simonyan and A.Â Zisserman, â€œVery deep convolutional networks for large-scale
    image recognition,â€ in *International Conference on Learning Representations*,
    2015.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] K. Simonyan å’Œ A. Zisserman, â€œç”¨äºå¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„éå¸¸æ·±å·ç§¯ç½‘ç»œï¼Œâ€ è§äº *å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®*ï¼Œ2015 å¹´ã€‚'
- en: '[78] C.Â Szegedy, V.Â Vanhoucke, S.Â Ioffe, J.Â Shlens, and Z.Â Wojna, â€œRethinking
    the inception architecture for computer vision,â€ in *Computer Vision and Pattern
    Recognition (CVPR)*, 2016.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, å’Œ Z. Wojna, â€œé‡æ–°æ€è€ƒè®¡ç®—æœºè§†è§‰ä¸­çš„
    inception æ¶æ„ï¼Œâ€ è§äº *è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2016 å¹´ã€‚'
- en: '[79] P.Â Isola, J.-Y. Zhu, T.Â Zhou, and A.Â A. Efros, â€œImage-to-image translation
    with conditional adversarial networks,â€ *CVPR*, 2017.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] P. Isola, J.-Y. Zhu, T. Zhou, å’Œ A. A. Efros, â€œä½¿ç”¨æ¡ä»¶å¯¹æŠ—ç½‘ç»œè¿›è¡Œå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ï¼Œâ€ *CVPR*ï¼Œ2017
    å¹´ã€‚'
- en: '[80] X.Â Zhang, Y.Â Zhang, E.Â Y. Han, N.Â Jacobs, Q.Â Han, X.Â Wang, and J.Â Liu,
    â€œClassification of whole mammogram and tomosynthesis images using deep convolutional
    neural networks,â€ *IEEE transactions on nanobioscience*, vol.Â 17, no.Â 3, pp. 237â€“242,
    2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] X. Zhang, Y. Zhang, E. Y. Han, N. Jacobs, Q. Han, X. Wang, å’Œ J. Liu, â€œä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œå¯¹æ•´ä¸ªä¹³è…ºæ‘„å½±å’Œæ–­å±‚åˆæˆå›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œâ€
    *IEEE çº³ç±³ç”Ÿç‰©ç§‘å­¦å­¦æŠ¥*ï¼Œç¬¬ 17 å·ï¼Œç¬¬ 3 æœŸï¼Œç¬¬ 237â€“242 é¡µï¼Œ2018 å¹´ã€‚'
- en: '[81] M.Â Fan, H.Â Zheng, S.Â Zheng, C.Â You, Y.Â Gu, X.Â Gao, W.Â Peng, and L.Â Li,
    â€œMass detection and segmentation in digital breast tomosynthesis using 3d-mask
    region-based convolutional neural network: A comparative analysis,â€ *Frontiers
    in molecular biosciences*, vol.Â 7, 2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Fan, H. Zheng, S. Zheng, C. You, Y. Gu, X. Gao, W. Peng, å’Œ L. Li, â€œä½¿ç”¨
    3d-mask åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆä¸­çš„è‚¿å—æ£€æµ‹å’Œåˆ†å‰²ï¼šæ¯”è¾ƒåˆ†æï¼Œâ€ *åˆ†å­ç”Ÿç‰©ç§‘å­¦å‰æ²¿*ï¼Œç¬¬ 7 å·ï¼Œ2020 å¹´ã€‚'
- en: '[82] I.Â Wichakam, J.Â Chayakulkheeree, and P.Â Vateekul, â€œDeep multi-label 3d
    convnet for breast cancer diagnosis in dbt with inversion augmentation,â€ in *Tenth
    International Conference on Digital Image Processing (ICDIP 2018)*.Â Â Â International
    Society for Optics and Photonics, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] I. Wichakam, J. Chayakulkheeree, å’Œ P. Vateekul, â€œç”¨äº dbt ä¹³è…ºç™Œè¯Šæ–­çš„æ·±åº¦å¤šæ ‡ç­¾ 3d
    convnet ä¸åè½¬å¢å¼ºï¼Œâ€ è§äº *ç¬¬åå±Šå›½é™…æ•°å­—å›¾åƒå¤„ç†ä¼šè®® (ICDIP 2018)*ã€‚ å›½é™…å…‰å­¦ä¸å…‰å­å­¦å­¦ä¼šï¼Œ2018 å¹´ã€‚'
- en: '[83] Y.Â Lei, X.Â He, J.Â Yao, T.Â Wang, L.Â Wang, W.Â Li, W.Â J. Curran, T.Â Liu,
    D.Â Xu, and X.Â Yang, â€œBreast tumor segmentation in 3d automatic breast ultrasound
    using mask scoring r-cnn,â€ *Medical Physics*, vol.Â 48, no.Â 1, pp. 204â€“214, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Lei, X. He, J. Yao, T. Wang, L. Wang, W. Li, W. J. Curran, T. Liu,
    D. Xu, å’Œ X. Yang, â€œä½¿ç”¨ mask scoring r-cnn è¿›è¡Œ 3d è‡ªåŠ¨ä¹³è…ºè¶…å£°ä¸­çš„ä¹³è…ºè‚¿ç˜¤åˆ†å‰²ï¼Œâ€ *åŒ»å­¦ç‰©ç†å­¦*ï¼Œç¬¬ 48 å·ï¼Œç¬¬
    1 æœŸï¼Œç¬¬ 204â€“214 é¡µï¼Œ2021 å¹´ã€‚'
- en: '[84] Y.Â Zhou, H.Â Chen, Y.Â Li, Q.Â Liu, X.Â Xu, S.Â Wang, P.-T. Yap, and D.Â Shen,
    â€œMulti-task learning for segmentation and classification of tumors in 3d automated
    breast ultrasound images,â€ *Medical Image Analysis*, vol.Â 70, p. 101918, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Zhou, H. Chen, Y. Li, Q. Liu, X. Xu, S. Wang, P.-T. Yap, å’Œ D. Shen,
    â€œç”¨äº3Dè‡ªåŠ¨åŒ–ä¹³è…ºè¶…å£°å›¾åƒä¸­è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»çš„å¤šä»»åŠ¡å­¦ä¹ ï¼Œâ€ *åŒ»å­¦å›¾åƒåˆ†æ*ï¼Œç¬¬70å·ï¼Œé¡µç 101918ï¼Œ2021ã€‚'
- en: '[85] J.Â Zhou, L.-Y. Luo, Q.Â Dou, H.Â Chen, C.Â Chen, G.-J. Li, Z.-F. Jiang, and
    P.-A. Heng, â€œWeakly supervised 3d deep learning for breast cancer classification
    and localization of the lesions in mr images,â€ *Journal of Magnetic Resonance
    Imaging*, vol.Â 50, no.Â 4, pp. 1144â€“1151, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Zhou, L.-Y. Luo, Q. Dou, H. Chen, C. Chen, G.-J. Li, Z.-F. Jiang, å’Œ
    P.-A. Heng, â€œç”¨äºä¹³è…ºç™Œåˆ†ç±»å’ŒMRå›¾åƒä¸­ç—…å˜å®šä½çš„å¼±ç›‘ç£3Dæ·±åº¦å­¦ä¹ ï¼Œâ€ *ç£å…±æŒ¯æˆåƒæ‚å¿—*ï¼Œç¬¬50å·ï¼Œç¬¬4æœŸï¼Œé¡µç 1144â€“1151ï¼Œ2019ã€‚'
- en: '[86] Q.Â Hu, H.Â M. Whitney, and M.Â L. Giger, â€œA deep learning methodology for
    improved breast cancer diagnosis using multiparametric mri,â€ *Scientific reports*,
    vol.Â 10, no.Â 1, pp. 1â€“11, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Q. Hu, H. M. Whitney, å’Œ M. L. Giger, â€œä¸€ç§æ”¹è¿›ä¹³è…ºç™Œè¯Šæ–­çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¤šå‚æ•°MRIï¼Œâ€ *ç§‘å­¦æŠ¥å‘Š*ï¼Œç¬¬10å·ï¼Œç¬¬1æœŸï¼Œé¡µç 1â€“11ï¼Œ2020ã€‚'
- en: '[87] B.Â Xiao, H.Â Sun, Y.Â Meng, Y.Â Peng, X.Â Yang, S.Â Chen, Z.Â Yan, and J.Â Zheng,
    â€œClassification of microcalcification clusters in digital breast tomosynthesis
    using ensemble convolutional neural network,â€ *BioMedical Engineering OnLine*,
    vol.Â 20, no.Â 1, pp. 1â€“20, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Xiao, H. Sun, Y. Meng, Y. Peng, X. Yang, S. Chen, Z. Yan, å’Œ J. Zheng,
    â€œåœ¨æ•°å­—ä¹³è…ºæ–­å±‚æ‰«æä¸­ä½¿ç”¨é›†æˆå·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå¾®é’™åŒ–ç°‡çš„åˆ†ç±»ï¼Œâ€ *ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹åœ¨çº¿*ï¼Œç¬¬20å·ï¼Œç¬¬1æœŸï¼Œé¡µç 1â€“20ï¼Œ2021ã€‚'
- en: '[88] S.Â Liu, D.Â Xu, S.Â K. Zhou, S.Â Grbic, W.Â Cai, and D.Â Comaniciu, â€œAnisotropic
    hybrid network for cross-dimension transferable feature learning in 3d medical
    images,â€ in *Deep Learning and Convolutional Neural Networks for Medical Imaging
    and Clinical Informatics*.Â Â Â Springer, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. Liu, D. Xu, S. K. Zhou, S. Grbic, W. Cai, å’Œ D. Comaniciu, â€œç”¨äº3DåŒ»å­¦å›¾åƒä¸­è·¨ç»´åº¦å¯è½¬ç§»ç‰¹å¾å­¦ä¹ çš„å„å‘å¼‚æ€§æ··åˆç½‘ç»œï¼Œâ€
    è§äº *æ·±åº¦å­¦ä¹ å’Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦æˆåƒå’Œä¸´åºŠä¿¡æ¯å­¦ä¸­çš„åº”ç”¨*ã€‚ Springerï¼Œ2019ã€‚'
- en: '[89] D.Â Tran, H.Â Wang, L.Â Torresani, J.Â Ray, Y.Â LeCun, and M.Â Paluri, â€œA closer
    look at spatiotemporal convolutions for action recognition,â€ in *CVPR*, 2018.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, å’Œ M. Paluri, â€œå¯¹æ—¶ç©ºå·ç§¯åœ¨åŠ¨ä½œè¯†åˆ«ä¸­çš„åº”ç”¨çš„æ›´æ·±å…¥æ¢è®¨ï¼Œâ€
    è§äº *CVPR*ï¼Œ2018ã€‚'
- en: '[90] J.Â Carreira and A.Â Zisserman, â€œQuo vadis, action recognition? a new model
    and the kinetics dataset,â€ in *Computer Vision and Pattern Recognition (CVPR)*,
    2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Carreira å’Œ A. Zisserman, â€œå‰é€”å¦‚ä½•ï¼ŒåŠ¨ä½œè¯†åˆ«ï¼Ÿä¸€ä¸ªæ–°æ¨¡å‹å’ŒKineticsæ•°æ®é›†ï¼Œâ€ è§äº *è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«
    (CVPR)*ï¼Œ2017ã€‚'
- en: '[91] M.Â Buda, A.Â Saha, R.Â Walsh, S.Â Ghate, N.Â Li, A.Â ÅšwiÄ™cicki, J.Â Y. Lo, and
    M.Â A. Mazurowski, â€œDetection of masses and architectural distortions in digital
    breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep
    learning model,â€ *arXiv preprint arXiv:2011.07995*, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Buda, A. Saha, R. Walsh, S. Ghate, N. Li, A. ÅšwiÄ™cicki, J. Y. Lo, å’Œ
    M. A. Mazurowski, â€œåœ¨æ•°å­—ä¹³è…ºæ–­å±‚æ‰«æä¸­æ£€æµ‹è‚¿å—å’Œç»“æ„æ‰­æ›²ï¼šä¸€ä¸ªåŒ…å«5,060åæ‚£è€…çš„å…¬å¼€æ•°æ®é›†å’Œä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œâ€ *arXiv é¢„å°æœ¬
    arXiv:2011.07995*ï¼Œ2020ã€‚'
- en: '[92] S.Â J.Â S. Gardezi, A.Â Elazab, B.Â Lei, and T.Â Wang, â€œBreast cancer detection
    and diagnosis using mammographic data: Systematic review,â€ *Journal of medical
    Internet research*, vol.Â 21, no.Â 7, p. e14464, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. J. S. Gardezi, A. Elazab, B. Lei, å’Œ T. Wang, â€œä½¿ç”¨ä¹³è…ºXå…‰æ•°æ®è¿›è¡Œä¹³è…ºç™Œæ£€æµ‹å’Œè¯Šæ–­ï¼šç³»ç»Ÿç»¼è¿°ï¼Œâ€
    *åŒ»å­¦äº’è”ç½‘ç ”ç©¶æ‚å¿—*ï¼Œç¬¬21å·ï¼Œç¬¬7æœŸï¼Œé¡µç e14464ï¼Œ2019ã€‚'
- en: '[93] K.Â Dembrower, P.Â Lindholm, and F.Â Strand, â€œA multi-million mammography
    image dataset and population-based screening cohort for the training and evaluation
    of deep neural networksâ€”the cohort of screen-aged women (csaw),â€ *Journal of digital
    imaging*, pp. 1â€“6, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] K. Dembrower, P. Lindholm, å’Œ F. Strand, â€œä¸€ä¸ªæ•°ç™¾ä¸‡å¼ ä¹³è…ºXå…‰å›¾åƒæ•°æ®é›†å’ŒåŸºäºäººç¾¤ç­›æŸ¥çš„é˜Ÿåˆ—ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ·±åº¦ç¥ç»ç½‘ç»œâ€”â€”ç­›æŸ¥å¹´é¾„å¥³æ€§é˜Ÿåˆ—ï¼ˆCSAWï¼‰ï¼Œâ€
    *æ•°å­—æˆåƒæ‚å¿—*ï¼Œé¡µç 1â€“6ï¼Œ2019ã€‚'
- en: '[94] R.Â S. Lee, F.Â Gimenez, A.Â Hoogi, K.Â K. Miyake, M.Â Gorovoy, and D.Â L. Rubin,
    â€œA curated mammography data set for use in computer-aided detection and diagnosis
    research,â€ *Scientific data*, vol.Â 4, no.Â 1, pp. 1â€“9, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake, M. Gorovoy, å’Œ D. L. Rubin,
    â€œä¸€ä¸ªç»è¿‡ç­–åˆ’çš„ä¹³è…ºXå…‰æ•°æ®é›†ï¼Œç”¨äºè®¡ç®—æœºè¾…åŠ©æ£€æµ‹å’Œè¯Šæ–­ç ”ç©¶ï¼Œâ€ *ç§‘å­¦æ•°æ®*ï¼Œç¬¬4å·ï¼Œç¬¬1æœŸï¼Œé¡µç 1â€“9ï¼Œ2017ã€‚'
- en: '[95] I.Â C. Moreira, I.Â Amaral, I.Â Domingues, A.Â Cardoso, M.Â J. Cardoso, and
    J.Â S. Cardoso, â€œInbreast: toward a full-field digital mammographic database,â€
    *Academic radiology*, vol.Â 19, no.Â 2, pp. 236â€“248, 2012.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] I. C. Moreira, I. Amaral, I. Domingues, A. Cardoso, M. J. Cardoso, å’Œ J.
    S. Cardoso, â€œInbreastï¼šè¿ˆå‘å…¨é¢æ•°å­—åŒ–ä¹³è…ºXå…‰æ•°æ®åº“ï¼Œâ€ *å­¦æœ¯æ”¾å°„å­¦*ï¼Œç¬¬19å·ï¼Œç¬¬2æœŸï¼Œé¡µç 236â€“248ï¼Œ2012ã€‚'
- en: '[96] D.Â Abdelhafiz, C.Â Yang, R.Â Ammar, and S.Â Nabavi, â€œDeep convolutional neural
    networks for mammography: advances, challenges and applications,â€ *BMC bioinformatics*,
    vol.Â 20, no.Â 11, pp. 1â€“20, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D. Abdelhafiz, C. Yang, R. Ammar å’Œ S. Nabaviï¼Œâ€œç”¨äºä¹³è…ºXå…‰æ£€æŸ¥çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼šè¿›å±•ã€æŒ‘æˆ˜ä¸åº”ç”¨ï¼Œâ€*BMC
    ç”Ÿç‰©ä¿¡æ¯å­¦*ï¼Œç¬¬20å·ï¼Œç¬¬11æœŸï¼Œç¬¬1â€“20é¡µï¼Œ2019å¹´ã€‚'
- en: '[97] K.-H. Yu, A.Â L. Beam, and I.Â S. Kohane, â€œArtificial intelligence in healthcare,â€
    *Nature biomedical engineering*, vol.Â 2, no.Â 10, pp. 719â€“731, 2018.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K.-H. Yu, A. L. Beam å’Œ I. S. Kohaneï¼Œâ€œåŒ»ç–—ä¿å¥ä¸­çš„äººå·¥æ™ºèƒ½ï¼Œâ€*è‡ªç„¶ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹*ï¼Œç¬¬2å·ï¼Œç¬¬10æœŸï¼Œç¬¬719â€“731é¡µï¼Œ2018å¹´ã€‚'
- en: '[98] M.Â Buda, A.Â Maki, and M.Â A. Mazurowski, â€œA systematic study of the class
    imbalance problem in convolutional neural networks,â€ *Neural Networks*, vol. 106,
    pp. 249â€“259, 2018.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Buda, A. Maki å’Œ M. A. Mazurowskiï¼Œâ€œå·ç§¯ç¥ç»ç½‘ç»œä¸­çš„ç±»ä¸å¹³è¡¡é—®é¢˜çš„ç³»ç»Ÿç ”ç©¶ï¼Œâ€*ç¥ç»ç½‘ç»œ*ï¼Œç¬¬106å·ï¼Œç¬¬249â€“259é¡µï¼Œ2018å¹´ã€‚'
- en: '[99] W.Â W. Ng, G.Â Zeng, J.Â Zhang, D.Â S. Yeung, and W.Â Pedrycz, â€œDual autoencoders
    features for imbalance classification problem,â€ *Pattern Recognition*, vol.Â 60,
    pp. 875â€“889, 2016.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] W. W. Ng, G. Zeng, J. Zhang, D. S. Yeung å’Œ W. Pedryczï¼Œâ€œç”¨äºä¸å¹³è¡¡åˆ†ç±»é—®é¢˜çš„åŒé‡è‡ªç¼–ç å™¨ç‰¹å¾ï¼Œâ€*æ¨¡å¼è¯†åˆ«*ï¼Œç¬¬60å·ï¼Œç¬¬875â€“889é¡µï¼Œ2016å¹´ã€‚'
- en: '[100] L.Â Ge, J.Â Gao, H.Â Ngo, K.Â Li, and A.Â Zhang, â€œOn handling negative transfer
    and imbalanced distributions in multiple source transfer learning,â€ *Statistical
    Analysis and Data Mining: The ASA Data Science Journal*, vol.Â 7, no.Â 4, pp. 254â€“271,
    2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] L. Ge, J. Gao, H. Ngo, K. Li å’Œ A. Zhangï¼Œâ€œå¤„ç†å¤šæºè¿ç§»å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»å’Œä¸å¹³è¡¡åˆ†å¸ƒï¼Œâ€*ç»Ÿè®¡åˆ†æä¸æ•°æ®æŒ–æ˜ï¼šASA
    æ•°æ®ç§‘å­¦æ‚å¿—*ï¼Œç¬¬7å·ï¼Œç¬¬4æœŸï¼Œç¬¬254â€“271é¡µï¼Œ2014å¹´ã€‚'
- en: '[101] S.Â M. AbdÂ Elrahman and A.Â Abraham, â€œA review of class imbalance problem,â€
    *Journal of Network and Innovative Computing*, vol.Â 1, no. 2013, pp. 332â€“340,
    2013.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. M. Abd Elrahman å’Œ A. Abrahamï¼Œâ€œç±»ä¸å¹³è¡¡é—®é¢˜çš„ç»¼è¿°ï¼Œâ€*ç½‘ç»œä¸åˆ›æ–°è®¡ç®—æ‚å¿—*ï¼Œç¬¬1å·ï¼Œ2013å¹´ï¼Œç¬¬332â€“340é¡µï¼Œ2013å¹´ã€‚'
- en: '[102] Z.Â Qiu, T.Â Yao, and T.Â Mei, â€œLearning spatio-temporal representation
    with pseudo-3d residual networks,â€ in *ICCV*, 2017.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Z. Qiu, T. Yao å’Œ T. Meiï¼Œâ€œä½¿ç”¨ä¼ª 3D æ®‹å·®ç½‘ç»œå­¦ä¹ æ—¶ç©ºè¡¨ç¤ºï¼Œâ€å‘è¡¨äº *ICCV*ï¼Œ2017å¹´ã€‚'
- en: '[103] D.Â Tran, L.Â Bourdev, R.Â Fergus, L.Â Torresani, and M.Â Paluri, â€œLearning
    spatiotemporal features with 3d convolutional networks,â€ in *International Conference
    on Computer Vision (ICCV)*, 2015.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] D. Tran, L. Bourdev, R. Fergus, L. Torresani å’Œ M. Paluriï¼Œâ€œä½¿ç”¨ 3D å·ç§¯ç½‘ç»œå­¦ä¹ æ—¶ç©ºç‰¹å¾ï¼Œâ€å‘è¡¨äº
    *å›½é™…è®¡ç®—æœºè§†è§‰å¤§ä¼šï¼ˆICCVï¼‰*ï¼Œ2015å¹´ã€‚'
