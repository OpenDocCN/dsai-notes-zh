- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:51:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2109.08853] A survey on deep learning approaches for breast cancer diagnosis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.08853](https://ar5iv.labs.arxiv.org/html/2109.08853)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A survey on deep learning approaches for breast cancer diagnosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Timothy Kwong¹, and Samaneh Mazaheri²
  prefs: []
  type: TYPE_NORMAL
- en: ¹Faculty of Engineering and Applied Science,
  prefs: []
  type: TYPE_NORMAL
- en: Ontario Tech University, Oshawa, ON Canada
  prefs: []
  type: TYPE_NORMAL
- en: ²Faculty of Business and Information Technology,
  prefs: []
  type: TYPE_NORMAL
- en: Ontario Tech University, Oshawa, ON Canada
  prefs: []
  type: TYPE_NORMAL
- en: timothy.kwong@ontariotechu.net, Samaneh.Mazaheri@ontariotechu.ca
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Deep learning has introduced several learning-based methods to recognize breast
    tumours and presents high applicability in breast cancer diagnostics. It has presented
    itself as a practical installment in Computer-Aided Diagnostic (CAD) systems to
    further assist radiologists in diagnostics for different modalities. A deep learning
    network trained on images provided by hospitals or public databases can perform
    classification, detection, and segmentation of lesion types. Significant progress
    has been made in recognizing tumours on 2D images but recognizing 3D images remains
    a frontier so far. The interconnection of deep learning networks between different
    fields of study help propels discoveries for more efficient, accurate, and robust
    networks. In this review paper, the following topics will be explored: (i) theory
    and application of deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches
    in breast tumour recognition from a performance metric perspective, and (iii)
    challenges faced in CNN approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Mammography, Digital Breast Tomosynthesis, Automatic Breast Ultrasound, MRI,
    2D Convolutional Neural Network, 3D Convolutional Neural Network, Classification,
    Detection, Segmentation
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2020, female breast cancer had 2.26 million new cases making breast cancer
    the highest number of new cases out of 36 cancer sites [[1](#bib.bib1)]. Moreover,
    the number of new deaths, due to female breast cancer, was 0.684 million, ranking
    it the fourth highest of 35 other cancer sites [[1](#bib.bib1)]. Current modalities
    for breast cancer screenings include mammography, digital breast tomosynthesis,
    breast ultrasound, magnetic resonance imaging [[2](#bib.bib2)]. Mammography has
    two types, screen-film mammography and Digital Mammography (DM), where both types
    are forms of x-ray imaging that use radiation to obtain a 2D image of the breast
    tissue [[2](#bib.bib2), [3](#bib.bib3)]. In addition, mammography has facilitated
    the detection of early stage breast cancer to reduce the risk of cancer death
    [[2](#bib.bib2), [4](#bib.bib4)]. Technological advancements in image acquisition
    had brought Digital Breast Tomosynthesis (DBT). DBT addressed issues in mammography
    and delivered improved image acquisition [[5](#bib.bib5), [3](#bib.bib3), [6](#bib.bib6),
    [7](#bib.bib7)]. It captures multiple 2D images slices of the breast, which are
    then synthesized into a 3D image [[2](#bib.bib2), [6](#bib.bib6), [5](#bib.bib5)].
    However, these 3D images (volumes) are quasi-3D, due to being a reconstruction
    of multiple captured 2D images [[6](#bib.bib6), [7](#bib.bib7)]. Furthermore,
    image slices are captured using a x-ray tube that pivots parallel to the chest
    wall along a 15^∘ to 60^∘ arc [[5](#bib.bib5)]. Automatic Breast UltraSound (ABUS)
    uses high frequency to image the entire breast. These 2D images are obtained on
    the transverse plane and synthesized into a 3D volume [[8](#bib.bib8)]. Magnetic
    Resonance Imaging (MRI) uses high-powered magnets along with radio waves generated
    by a computer to image the breast [[2](#bib.bib2)]. CAD systems assist radiologists
    in making diagnostic decisions with higher confidence by providing a "second opinion"
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]. In addition, as mentioned
    by [[12](#bib.bib12)], the CAD system should improve radiologists’ performance,
    save time, seamlessly integrate with workflow, and not impose liabilities. The
    integration of deep learning algorithms into CAD systems aim to address mentioned
    objectives above, as well as reducing assessment variability from different radiologists
    [[13](#bib.bib13), [14](#bib.bib14)], reducing recall rate, and increasing cancer
    detection rate. The continuous advances in deep learning has brought upon models
    that outperform radiologists in both classification and localization of cancer
    tumors in medical images [[15](#bib.bib15), [16](#bib.bib16)]. As algorithms advance,
    computing power becomes more accessible, and expansive well-curated datasets become
    open-sourced. Machine learning techniques are able to shift towards state-of-the-art,
    and aid in tasks within the healthcare sector [[17](#bib.bib17)]. Deep learning,
    a sub-field of machine learning, conveys representations in simpler forms to solve
    the problem of learning different representations [[18](#bib.bib18)]. Moreover,
    deep learning uses multiple interconnected layers of artificial neurons to learn
    the patterns of simpler expressed forms of the actual representation [[19](#bib.bib19),
    [18](#bib.bib18)]. In 2012, a convolutional neural network architecture scored
    an error rate of 15.3%, which was 10.9% lower than the second-best entry [[20](#bib.bib20)].
    This breakthrough led to an increase in research participation in the field of
    deep learning, and the continuation of research and usage of CNN architecture
    for image recognition problems [[21](#bib.bib21)]. Convolutional Neural Network
    (CNN) are specialized networks to process data with known grid patterns, as well
    as learning spatial hierarchies of features within data [[18](#bib.bib18), [22](#bib.bib22)].
    As a result, hand-crafted features of cancer tumours are not required for CNNs,
    considering as CNNs can learn features. The applicability of deep learning in
    the medical field presents itself through classification, localization, and segmentation
    of cancer tumors in medical images from modalities such as MRI, CT, ultrasound.
    This review paper will provide insight into deep learning theory, progress of
    2D, 2.5D, and 3D CNN architectures, and challenges faced when training a network.
  prefs: []
  type: TYPE_NORMAL
- en: II Deep Learning Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides a theoretical overview on deep learning concepts, including
    data augmentation, building blocks in a typical CNN architecture, overfitting,
    and transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation is a technique aimed at increasing the dataset size, and improving
    performance and robustness of the model [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    Data augmentation methods such as translation, rotation, reflection, blur, and
    crop, are applied directly on the original image to generate new augmented images.
    An instance of data augmentation applies each listed method to an original image
    to generate 5 new augmented image, which increases the dataset size with new unseen
    training instances. Image resizing and greyscaling are other strategies used during
    data pre-processing to reduce the computation complexity required to process these
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s(t)=(x\ast w)(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/cb48fc9253e1673e739bef050bbd012a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A basic Convolutional Neural Network (CNN) extracts features from
    an input to output a feature vector. This CNN contains two layers, the convolution
    and pooling layer. In convolution layer, the entire input is convolved by a kernel,
    while in the pooling layer, the input is down-sampled. The final output is a flattened
    column vector containing significant features of the input. Adapted from [[26](#bib.bib26)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-B 2D Convolutional Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional neural networks use a mathematical operation called convolution;
    a convolution is a linear operation used, in this case, for feature extraction
    [[18](#bib.bib18), [22](#bib.bib22)]. Discrete convolution is expressed as equ.
    ([1](#S2.E1 "Equation 1 ‣ II-A Data Augmentation ‣ II Deep Learning Theory ‣ A
    survey on deep learning approaches for breast cancer diagnosis")), as seen in
    [[18](#bib.bib18), equ. (9.1)], where s(t) represents the feature map, x(a) represents
    the input, and w(t-a) represents the kernel. Moreover, equ. ([1](#S2.E1 "Equation
    1 ‣ II-A Data Augmentation ‣ II Deep Learning Theory ‣ A survey on deep learning
    approaches for breast cancer diagnosis")) illustrates an element-wise product
    between an input and a kernel to produce a feature map [[22](#bib.bib22), [27](#bib.bib27)].
    The 2D convolutional kernel is a matrix of weights that extracts meaningful features
    from the input for the network to learn and recognize different inputs. A feature
    maps can be generated through convolving a kernel with an input, then applying
    an activation function on the convolved output [[23](#bib.bib23)]. In addition,
    backpropagation is used to update the kernel weights to minimize the loss function
    [[22](#bib.bib22)]. Stride, as defined by [[22](#bib.bib22)], is "the distance
    between two successive kernel positions", which dictates the step size of the
    kernel across the input. Padding is a technique used to retain the in-plane dimensionality
    of the feature map even after a convolution further permitting more convolutional
    layers [[22](#bib.bib22), [27](#bib.bib27)]. Zero-padding aligns the border of
    the input with zeros to retain the dimensionality [[28](#bib.bib28)]. Parameter
    sharing is a mechanism used in CNN to limit the number of parameters by sharing
    the kernel weights, which ultimately reduces the model complexity [[22](#bib.bib22),
    [23](#bib.bib23), [28](#bib.bib28)]. In addition, parameters can be shared among
    more abstract features that occur within different images [[28](#bib.bib28), [29](#bib.bib29)].
    Figure [1](#S2.F1 "Figure 1 ‣ II-A Data Augmentation ‣ II Deep Learning Theory
    ‣ A survey on deep learning approaches for breast cancer diagnosis") illustrates
    the feature extraction on the input by the convolution and pooling layer to output
    a feature map.
  prefs: []
  type: TYPE_NORMAL
- en: II-C 3D Convolutional Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a 3D CNN, the kernels, stride, and pooling operation are three dimensional,
    where the third dimension represents a depth dimension [[30](#bib.bib30)]. This
    additional dimension allows 3D CNNs to extract features from an additional axis
    of information. In 3D convolutional layers, voxel represents the spatial information
    rather than pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6171bd3a26e3a1fd02fd96235bcd3b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The Rectified Linear Unit (ReLU) function. Adapted from [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-D Activation Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Rectified Linear Unit (ReLU) is an activation function commonly used in
    neural networks [[22](#bib.bib22), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)].
    The ReLU function, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="f(x)=max(0,x)=\begin{cases}x,&amp;x\geq
    0\\ 0,&amp;x<0\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics ><mrow 
    ><mrow  ><mi
     >f</mi><mo lspace="0em" rspace="0em"
     >​</mo><mrow 
    ><mo stretchy="false"  >(</mo><mi
     >x</mi><mo stretchy="false" 
    >)</mo></mrow></mrow><mo  >=</mo><mrow
     ><mi  >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >a</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi 
    >x</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow  ><mo
    stretchy="false"  >(</mo><mn
     >0</mn><mo 
    >,</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >=</mo><mrow  ><mo
     >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mi  >x</mi><mo
     >,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mi 
    >x</mi><mo  >≥</mo><mn
     >0</mn></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mn  >0</mn><mo
     >,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mi 
    >x</mi><mo  ><</mo><mn
     >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><apply  ><ci
     >𝑓</ci><ci 
    >𝑥</ci></apply><apply  ><ci
     >𝑚</ci><ci 
    >𝑎</ci><ci  >𝑥</ci><interval
    closure="open"  ><cn type="integer"
     >0</cn><ci  >𝑥</ci></interval></apply></apply><apply
     ><apply 
    ><csymbol cd="latexml"  >cases</csymbol><ci
     >𝑥</ci><apply 
    ><ci  >𝑥</ci><cn
    type="integer"  >0</cn></apply><cn
    type="integer"  >0</cn><apply
     ><ci 
    >𝑥</ci><cn type="integer" 
    >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f(x)=max(0,x)=\begin{cases}x,&x\geq
    0\\ 0,&x<0\\ \end{cases}</annotation></semantics></math> |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: and can be seen in [[34](#bib.bib34), equ. (1.14)], equates values less than
    zero to zero, and values greater than or equal to zero to passed in value. A plot
    of the ReLU function is depicted in Figure [2](#S2.F2 "Figure 2 ‣ II-C 3D Convolutional
    Layer ‣ II Deep Learning Theory ‣ A survey on deep learning approaches for breast
    cancer diagnosis").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ec99838f30d83388998c7a3e8003524.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Fully-Connected Neural Network (FCNN). Adapted from [[26](#bib.bib26)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-E Pooling Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pooling is a technique to down-sample feature maps [[22](#bib.bib22)], introduces
    invariance [[18](#bib.bib18)], and merge semantically similar features [[18](#bib.bib18)].
    Down-sampling a feature map reduces the in-plane dimensionality [[22](#bib.bib22),
    [35](#bib.bib35)], which reduces the data size without reducing key features in
    the feature map required for learning. Max pooling is a pooling operation, which
    obtains the maximum value within a square region [[18](#bib.bib18), [22](#bib.bib22),
    [35](#bib.bib35)]. Moreover, by obtaining the maximum value, this also makes a
    representation invariant to small translation or distortions [[18](#bib.bib18),
    [22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a48dd7396b3d2efdfd1b4c03cd52b894.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The surface of this 3-dimensional graph represents the objective
    function. Gradient descent is used to traverse towards the deepest point on this
    graph. The deepest point represents the global minimum, and parameters that minimize
    the objective function. Source: Adapted using [[31](#bib.bib31)]'
  prefs: []
  type: TYPE_NORMAL
- en: II-F Fully-Connected Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Fully Connected (FC) layer learns a non-linear function to map all the
    features within a feature space. Figure [3](#S2.F3 "Figure 3 ‣ II-D Activation
    Layer ‣ II Deep Learning Theory ‣ A survey on deep learning approaches for breast
    cancer diagnosis") gives an illustration of the basic structure for a Fully-Connected
    Neural Network (FCNN). A CNN can have a FC layer, where the features extracted
    from the CNN are inputted into the FC layer for a decision output [[36](#bib.bib36)].
    During training, the goal is to minimize the prediction error made by the CNN,
    techniques such as back-propagation and gradient descent are used to improve prediction
    results. The objective function, also known as the loss function or cost function,
    determines the difference between the prediction and ground truth; it measures
    the network error in prediction. Binary cross-entropy is a loss function used
    in binary classification. Back-propagation is a technique used to determine the
    gradient of the objective function with respect to the weights [[32](#bib.bib32)].
    The equation for back-propagation of an objective function with respect to the
    weight can be calculated using chain rule and is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial{L}}{\partial{w}}=\frac{\partial{z}}{\partial{w}}\frac{\partial{a}}{\partial{z}}\frac{\partial{L}}{\partial{w}}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Gradient descent is used to minimize the objective function through iterative
    updates of the parameters, such as weights, bias, and kernels, in the negative
    direction of the gradient of the objective function [[22](#bib.bib22), [37](#bib.bib37)].
    The respected equation is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w:=w-\alpha\frac{\partial{L}}{\partial{w}}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where w represents the weight, $\alpha$ represents the learning rate, and the
    partial derivative of the objective function with respect to the weight. Figure
    [4](#S2.F4 "Figure 4 ‣ II-E Pooling Layer ‣ II Deep Learning Theory ‣ A survey
    on deep learning approaches for breast cancer diagnosis") illustrates the 3-dimensional
    surface of an objective function, where the darkest point represents the global
    minimum of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed0f502fe3fa6ef1f61bd867eb07adfd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A model that has overfit the training data experiences a divergence
    in training and validation loss. Adapted from [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-G Overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models that cannot generalize to new data have overfitting the training data
    as shown in Figure [5](#S2.F5 "Figure 5 ‣ II-F Fully-Connected Layer ‣ II Deep
    Learning Theory ‣ A survey on deep learning approaches for breast cancer diagnosis").
    Overfitting can be solved through early stopping, network pruning, increasing
    training data, and regularization [[38](#bib.bib38), [39](#bib.bib39)]. Early
    stopping is stopping the learning at a point where the curves for training and
    validation loss are neither overfitting or underfitting [[38](#bib.bib38), [39](#bib.bib39)].
    Network pruning involves the pruning of redundant weights, while keeping important
    weights [[40](#bib.bib40)]. An increase in training data is required to tune the
    parameters within a network, so the network can generalize to new data better.
    Training data can be increased through collection and/or data augmentation. Regularization
    aims to remove useless features and minimize the weights of less important features
    learned by the model [[38](#bib.bib38)]. However, due to the uncertainty of necessary
    features by the network, a penalty term is added to the objective function to
    minimize the number of features [[38](#bib.bib38)]. Furthermore, there are different
    types of regularization, such as L1 regularization, L2 regularization (weight
    decay), and Dropout [[38](#bib.bib38)]. Dropout was proposed by Srivastava et
    al. [[41](#bib.bib41)] and is another effective strategy in reducing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: II-H Transfer Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning is a technique used to improve a learner specialized to a
    domain through transferring knowledge from a similar domain [[42](#bib.bib42),
    [43](#bib.bib43)]. Additionally, it can be a solution to the issue of insufficient
    training data [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]. A quantity
    of quality training data suitable for effectively training a learner can be expensive,
    due to difficult collection and curation of data [[43](#bib.bib43)]. Henceforth,
    solution such as homogeneous and heterogeneous transfer learning have been proposed
    [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: III Performance Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A confusion matrix can be used to showcase the different predictions that a
    classifier makes [[45](#bib.bib45), [46](#bib.bib46)]. A confusion matrix can
    be visualized as a 2x2 matrix with True Positive (TP), True Negative (TN), False
    Positive (FP), and False Negative (FN) in one of the grids [[46](#bib.bib46),
    [47](#bib.bib47)]. True positives are positive samples correctly predicted as
    positive, whereas false positives are negative samples incorrectly predicted as
    positive [[45](#bib.bib45), [46](#bib.bib46)]. True negatives are negative samples
    correctly predicted as negative, whereas false negatives are positive samples
    incorrectly predicted as negative [[45](#bib.bib45), [46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equations will be used as a means of assessing the deep learning
    models shown in TABLE [I](#S3.T1 "Table I ‣ III Performance Metrics ‣ A survey
    on deep learning approaches for breast cancer diagnosis"), [II](#S4.T2 "Table
    II ‣ IV-C Status of Magnetic Resonance Imaging (MRI) ‣ IV Discussion ‣ A survey
    on deep learning approaches for breast cancer diagnosis"), and [III](#S4.T3 "Table
    III ‣ IV-D Current Approaches using 2D CNN Architecture ‣ IV Discussion ‣ A survey
    on deep learning approaches for breast cancer diagnosis"). Accuracy is a ratio
    between the correctly classified and the total samples [[46](#bib.bib46), [47](#bib.bib47)].
    The equation for accuracy is given by the following [[48](#bib.bib48), equ. (2)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Sensitivity, also known as recall and true positive rate, is the fraction of
    relevant (actual true positive) instances that are retrieved [[46](#bib.bib46)].
    The equation for sensitivity is given by [[48](#bib.bib48), equ. (5)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Sensitivity=\frac{TP}{TP+FN}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Specificity, also known as true negative rate, is the fraction of relevant
    (actual true negative) instance that are retrieved. The equation for specificity
    equation is given by [[48](#bib.bib48), equ. (6)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Specificity=\frac{\text{TN}}{TN+FP}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Precision is the fraction of retrieved instances that are relevant (actual
    true positive) [[46](#bib.bib46), [47](#bib.bib47)]. The equation for precision
    is given by [[48](#bib.bib48), equ. (4)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'F score is a weighted ratio measuring the average of precision and recall [[46](#bib.bib46)],
    the equation for F score is given by [[47](#bib.bib47), equ. (4)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F_{1}=2\>\frac{precision\>\cdot\>recall}{precision+recall}$ |  | (9)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Dice Similarity Coefficient (DSC) is a spatial overlap index [[49](#bib.bib49)],
    and can be used for measuring the segmentation performance of a model. DSC is
    given by [[50](#bib.bib50), equ. (2)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $DSC=\frac{2\cdot TP}{2\cdot TP+FP+FN}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'Matthews Correlation Coefficient (MCC) calculates for the Pearson product-moment
    correlation coefficient between the actual and predicted values [[47](#bib.bib47)].
    MCC is given by [[47](#bib.bib47), equ. (2)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: 2D CNN architectures for breast cancer diagnosis'
  prefs: []
  type: TYPE_NORMAL
- en: Ref. Model Task Dataset AUROC (%) MCC (%) Dice (%) DM Dhungel et al. (2017)
    [[51](#bib.bib51)] CNN + RF + Hypothesis- Refinement Classification Detection
    Segmentation INbreast $0.76\pm 0.23$ (via RF, min. user int.) $0.69\pm 0.10$ (via
    CNN, min user int.) — $0.85\pm 0.02$ Al-antari et al. (2018) [[52](#bib.bib52)]
    YOLO + FrCN + CNN Classification Detection Segmentation INbreast 0.9478 (via CNN)
    0.9762 (via YOLO) 0.8593 (via FrCN) 0.8991 (via CNN) 0.9269 (via FrCN) Chougrad
    et al. (2018) [[53](#bib.bib53)] Inception v3 Classification Detection DDSM INbreast
    BCDR 0.99 (via MIAS) — — Ribli et al. (2018) [[54](#bib.bib54)] Faster-RCNN Classification
    Detection DDSM Semmelweis University 0.95 (via INbreast) — — Singh et al. (2020)
    [[55](#bib.bib55)] cGAN Classification Segmentation DDSM INbreast Hospital Sant
    Joan de Reus 0.80 — 0.94
  prefs: []
  type: TYPE_NORMAL
- en: IV Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section looks at the different deep learning architectures designed for
    classification, detection, and segmentation of breast tumours, all are shown in
    TABLE [I](#S3.T1 "Table I ‣ III Performance Metrics ‣ A survey on deep learning
    approaches for breast cancer diagnosis"), [II](#S4.T2 "Table II ‣ IV-C Status
    of Magnetic Resonance Imaging (MRI) ‣ IV Discussion ‣ A survey on deep learning
    approaches for breast cancer diagnosis"), and [III](#S4.T3 "Table III ‣ IV-D Current
    Approaches using 2D CNN Architecture ‣ IV Discussion ‣ A survey on deep learning
    approaches for breast cancer diagnosis").
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Status of Digital Breast Tomosynthesis (DBT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DBT has been gaining popularity over digital mammography for higher image quality,
    richer structural detail, and better reduction in background signal noise [[56](#bib.bib56)].
    In addition, recent studies [[57](#bib.bib57), [6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] have shown increased cancer detection rate
    with DBT in women aged 40 to 79 years with dense and non-dense breast, but results
    on reduced recall rate remains conflicted. Moreover, [[6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] have shown that DBT reduces recall rates for
    women aged 40 to 79 years with dense and non-dense breast, but [[61](#bib.bib61),
    [62](#bib.bib62)] shown recall rates of DBT plus DM similar to that of DM alone.
    DBT reduces the overlapping breast tissue that appear on 2D images as opposed
    to mammograms [[63](#bib.bib63), [6](#bib.bib6), [3](#bib.bib3)]. As a result,
    DBT images help with the detection of tumours that may appear overlapped by other
    healthy breast tissue. However, a disadvantage of DBT is being less sensitive
    to imaging malignant calcification and even groups of micro-calcification compared
    to DM [[64](#bib.bib64), [56](#bib.bib56)]. In addition, DBT systems that use
    pixel binning have increased efficiency in detector readings, but in turn reduced
    3D spatial resolution [[7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Status of Automatic Breast UltraSound (ABUS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ABUS consists of an ultrasound scanner and a transducer [[65](#bib.bib65),
    [66](#bib.bib66)]. The ABUS captures axial slices of the breast in different views,
    then these axial slices are used for 3D reconstruction of sagittal and coronal
    images [[65](#bib.bib65), [66](#bib.bib66)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Status of Magnetic Resonance Imaging (MRI)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Magnetic resonance imaging (MRI) screenings are recommended to patients with
    a high risk of breast cancer, due to genetics or family history [[67](#bib.bib67),
    [2](#bib.bib2)]. Breast coils are used with an MRI to acquire the image of the
    breast; the patient lies prone with the breasts in the breast coils before entering
    the MRI [[68](#bib.bib68)]. Breast MRI has different types, such as T1-weighted
    contrast-enhanced imaging, T2-weighted, ultrafast, and diffusion-weighted imaging
    [[68](#bib.bib68)]. The dimensionality of an acquired image is dependent on the
    MRI type [[69](#bib.bib69)]. In 2D image acquisition, multiple 2D image slices
    of the object are captured, whereas in 3D image acquisition, a true 3D image can
    be captured [[69](#bib.bib69), [70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: 2.5D CNN architectures for breast cancer diagnosis'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ref. Model Task Dataset AUROC (%) Dice (%) DM-DBT Yousefi et al. (2018) [[71](#bib.bib71)]
    DCNN MI-RF- based CAD Classification Breast Imaging Research Laboratory at Massachusetts
    General Hospital (in-house): 87 DBT images (27 malignant, 60 benign) 0.87 — Kim
    et al. (2017) [[72](#bib.bib72)] VGG16 + LSTM Classification (in-house) 0.919
    — Liu et al. (2017) [[73](#bib.bib73)] 3D Anisotropic Hybrid Network (3D AH-Net)
    Segmentation (in-house): 2809 DBT volumes — 0.834 Zhang et al. (2020) [[74](#bib.bib74)]
    AlexNet (Late fusion + Max Pooling) Classification (in-house): 3018 negatives
    272 malignant 415 benign 0.854 — Liang et al. (2020) [[75](#bib.bib75)] CNN ensemble
    Classification University of Kentucky Medical Center (in-house): DBT and DM (709
    malignant, 415 benign) 0.97 —'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Current Approaches using 2D CNN Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For 2D classification, Chougrad et al. [[53](#bib.bib53)] adopted state-of-the-art
    architectures, including ResNet50 [[76](#bib.bib76)], VGG16 [[77](#bib.bib77)],
    and Inception v3 [[78](#bib.bib78)], that were pre-trained on ImageNet, and re-purposed
    for breast cancer screening. Chougrad et al. [[53](#bib.bib53)] achieved a 0.99
    AUC for classification on the MIAS database using a pre-trained and fine-tuned
    Inception v3 model. The study concluded that fine-tuning strategy improves classification
    accuracy on state-of-the-art architecture, and Inception v3 achieved a higher
    accuracy than VGG16 and ResNet50.
  prefs: []
  type: TYPE_NORMAL
- en: For 2D segmentation, Singh et al. [[55](#bib.bib55)] adapted upon a study by
    Isola et al. [[79](#bib.bib79)] to propose a conditional Generative Adversarial
    Network (cGAN) CAD framework for classification and segmentation breast tumor.
    Singh et al. [[55](#bib.bib55)] had achieved 92.11% dice coefficient score and
    84.55% IoU for a tight frame of the tumor Region Of Interest (ROI) on cGAN. Furthermore,
    the study tested Single Shot Detector (SSD), You Only Look Once (YOLO), and Faster-RCNN
    and found that SSD achieved the best results on detecting small tumor regions
    and achieved an overall accuracy of 97%. Major contributions proposed in this
    study include the first adapted cGan for breast tumor segmentation, a multi-class
    CNN for predicting four breast tumor shapes, and the proposed model outperforming
    state-of-the-art architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: 3D CNN architectures for breast cancer diagnosis'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ref. Model Dataset Acc(%) AUROC(%) AP(%) FNR(%) Dice(%) Fscore(%) DBT Zhang
    et al. (2018)[[80](#bib.bib80)] 3D-T2-Alex University of Kentucky (in-house) —
    0.6632 — — — — Fan et al. (2020)[[81](#bib.bib81)] 3D Mask-RCNN Fudan University
    Affiliated Cancer Center (in-house): 364 DBT samples (289 malignant, 75 benign)
    — 0.934 0.053 — — — Wichakam et al. (2018)[[82](#bib.bib82)] 3D ConvNet (in-house):
    115 DBT volumes (91 malignant, 24 normal) 0.72 — — — — 0.842 ABUS Lei et al. (2021)[[83](#bib.bib83)]
    Mask scoring RCNN (private) — — — $0.85\pm 0.104$ — — Zhou et al. (2021)[[84](#bib.bib84)]
    $\mathrm{C_{MS}}\mathrm{VNet_{Iter}}$ Peking University People’s Hospital (in-house):
    900 ABUS volumes — 0.787 — 0.392 $0.778\pm 0.145$ 0.811 MRI Zhou et al. (2019)[[85](#bib.bib85)]
    3D DenseNet (in-house): 720 malignant, 353 benign — 0.859 — — $0.501\pm 0.274$
    — Hu et al. (2020)[[86](#bib.bib86)] VGG19Net (in-house): 728 malignant, 199 benign
    — DEC: 0.85 T2w: 0.78 ImageFusion: 0.85 FeatureFusion: 0.87 ClassifierFusion:
    0.86 — — — —'
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Overview on 2.5D CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The utilization of the depth dimension and collection of images within a DBT
    volume is needed to utilize the entirety of the DBT information [[72](#bib.bib72),
    [74](#bib.bib74), [87](#bib.bib87)]. Furthermore, 2D CNN cannot preserve the between-slice
    information in DBT volumes [[88](#bib.bib88), [89](#bib.bib89), [44](#bib.bib44),
    [73](#bib.bib73)]. Moreover, the high complexity, potential overfitting, and small
    DBT dataset relative to ImageNet [[72](#bib.bib72), [74](#bib.bib74), [90](#bib.bib90)]
    can make training a 3D CNN rather infeasible, which makes approaches by [[72](#bib.bib72),
    [73](#bib.bib73), [75](#bib.bib75)] more favourable. In the following section,
    alternative methods to 2D and 3D CNN approaches for utilizing the entirety of
    information within DBT images will be discussed.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Current Approaches using 2.5D CNN Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kim et al. [[72](#bib.bib72)] proposed a CNN for spatial feature representation
    and depth directional long-term recurrent learning for depth feature representation.
    A VGG16 network was used as the CNN, while LSTMs are used for depth directional
    long-term recurrent learning. The model achieved an AUROC of 91.9%. However, a
    LSTM network can be difficult [[73](#bib.bib73)] and expensive [[90](#bib.bib90)]
    to train.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, a different approach for learning 3D DBT volumes was proposed by Liu
    et al. [[73](#bib.bib73)]. They proposed the 3D Anisotropic Hybrid Network (3D
    AH-Net). The 3D AH-Net achieved a global dice score of 83.4%. As mentioned by
    Liu et al., challenges with directly training a 3D CNN with DBT or CT scans include
    (1) anisotropic voxels, (2) the higher number of features needed compared to 2D
    CNN, and (3) the lack of pre-trained 3D CNN models and limited training data.
    Anisotropic voxels have uneven distribution of data that hinder the training of
    3D CNNs, such as CT and DBT volumes having within-slice resolution greater than
    between-slice resolution. This challenge of anisotropic voxels in DBT images was
    treated using anisotropic convolutions. The 3D AH-Net has a feature encoder and
    decoder. The encoder extracts deep representations from the 2D image slices. On
    the other hand, the decoder, a densely connected network of anisotropic convolutions,
    utilizes the 3D context, while keeping the between-slices consistency. The AH-ResNet,
    a 2D ResNet50 [[76](#bib.bib76)] converted into a 3D ResNet50, was used as the
    backbone and encoder of the 3D AH-Net. A 2D Multi-Channel Global Convolutional
    Network (MC-GCN) was used to train the encoder parameters used in the 3D AH-NET.
    The parameters trained on the MC-GCN were extracted and transferred into the AH-Resnet.
    The 3D AH-Net is structured in the order of AH-ResNet, decoder, then pyramid volumetric
    pooling.
  prefs: []
  type: TYPE_NORMAL
- en: A key challenge mentioned by Liang et al. [[74](#bib.bib74)] is the effective
    utilization of DBT data, considering as DBT data are high in resolution and vary
    in depth. The training of a 3D CNN model with DBT data is computationally costly
    and memory intensive. As a result, Liang et al. proposed a network containing
    two types of CNNs, a CNN feature extractor and CNN classifier. The model achieved
    an AUROC of 97%. The network sequence starts with a 2D CNN feature extractor transitioning
    into an ensemble of three CNN classifiers. A classifier for DM, DBT, and DM-DBT
    feature map classification, where the DM-DBT feature map is a concatenation of
    DM and DBT feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: IV-G Overview on 3D CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A 2D CNN cannot preserve the between-slice information in DBT volumes as opposed
    to a 3D CNN [[88](#bib.bib88), [89](#bib.bib89), [44](#bib.bib44), [73](#bib.bib73)].
    A 3D CNN can learn the spatial information within a 2D image along with between-slice
    information of multiple slices. A 3D convolutional kernel enables this characteristic,
    but also generates a higher number of training parameters compared to a 2D convolutional
    kernel. As a result, the complexity of features that each kernels can extract
    increases. A challenge arises when isotropic 3D convolutional kernels are used
    to learn anisotropic DBT volumes, due to variation of resolution within each anisotropic
    voxel along each plane [[73](#bib.bib73), [88](#bib.bib88)]. Moreover, the quality
    of spatial resolution of DBT images can impact the training of 2D and 3D CNNs
    [[7](#bib.bib7), [88](#bib.bib88), [73](#bib.bib73)]. Another challenge with training
    3D CNN is the limited well-curated and publicly available DBT dataset. Buda et
    al. [[91](#bib.bib91)] addressed this issue by curating and releasing a publicly
    available dataset with 22,032 reconstructed DBT volumes from 5,060 patients.
  prefs: []
  type: TYPE_NORMAL
- en: IV-H Current Approach using 3D CNN Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fan et al. [[81](#bib.bib81)] showcases the superiority of 3D over 2D deep learning
    methods, such as Yousefi et al. [[71](#bib.bib71)], in learning to classify, detect,
    and segment tumours in DBT image slices. Fan et al. [[81](#bib.bib81)] proposed
    a 3D-Mask-RCNN with a ResNet50 backbone. The proposed 3D-Mask-RCNN achieved a
    sensitivity of 90% at 0.83 FPs/breast for breast-based mass detection, and an
    AP of 93.4% and FNR of 5.3% for lesion segmentation. The study concluded the proposed
    model, 3D-Mask-RCNN, outperforms the 2D counterparts, Mask-RCNN and Faster-RCNN.
  prefs: []
  type: TYPE_NORMAL
- en: V Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section looks at the challenges faced in deep learning for breast tumour
    diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Small Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Small datasets pose a challenge to the training of deep learning models [[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)], due to the need to familiarize the model
    with all possible cases to minimize classification errors. In addition, the lack
    of standardized datasets makes comparing and reproducing studies difficult [[94](#bib.bib94)].
    A training dataset for deep learning models should provide normal mammograms,
    mammograms with a variety of BI-RADS 1-6, mass types, calcification, asymmetries,
    architectural distortion cases, and several cases in one mammogram [[95](#bib.bib95)].
    Although techniques such as data augmentation, batch normalization, and transfer
    learning have been used to situate limited dataset sizes, large and well-curated
    datasets are still a high necessity for a well-trained model [[21](#bib.bib21),
    [96](#bib.bib96), [97](#bib.bib97)]. Yousefi et al. [[71](#bib.bib71)] used data
    augmentation techniques to increase the sample size from 5,040 to 40,320 2D slices.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Class Imbalance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Class imbalance is another challenge for deep learning, and occurs when classes
    have different ratios of training data [[98](#bib.bib98), [48](#bib.bib48), [99](#bib.bib99),
    [100](#bib.bib100), [46](#bib.bib46)]. Class imbalance can cause biases in the
    classifier, resulting in predictions skewed towards the positive or negative class
    depending on the data size ratio between classes [[96](#bib.bib96)]. In addition,
    metrics used for measuring model performance, such as accuracy [[47](#bib.bib47)],
    are susceptible to class imbalance and can affect the performance of the model.
    However, there are techniques to deal with class imbalance on both the data level
    and classifier level. Techniques for the data level are random undersampling and
    random oversampling [[101](#bib.bib101), [48](#bib.bib48), [98](#bib.bib98)],
    while the classifier level are cost-sensitive learning and thresholding [[48](#bib.bib48),
    [98](#bib.bib98)].
  prefs: []
  type: TYPE_NORMAL
- en: V-C Computational Cost and Memory Constraint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory constraint is an issue when dealing with training data with a large feature
    space, such as high resolution or high dimensional images [[96](#bib.bib96), [102](#bib.bib102),
    [103](#bib.bib103), [74](#bib.bib74)]. For example, when training a 3D CNN from
    scratch with DBT data with large feature spaces, computational and memory cost
    dramatically increase.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Image Quality Variability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The image quality depends on the system settings and manufacturer specification
    of the medical screening device, while the performance of models depends on the
    image quality [[56](#bib.bib56)]. Images from breast cancer screenings with poor
    resolution, sharpness, contrast, focus, or high noise can hinder the model during
    training, predictions, or localization of lesions [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: VI Future Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural network models discussed in this paper have achieved promising results
    in classification, detection, and segmentation tasks for breast tumours. However,
    further exploration of different architectures should be made to expand possible
    solutions to common issues, such as costly computations and memory usage, redundancies
    in learning 3D data, and robustness. In addition, there is a need for architectures
    to evaluate DBT, MRI, and ABUS images with higher confidence, efficiency, and
    speed. Furthermore, state-of-the-art models require a high level of robustness
    and confidence to display the required level of integrity to act as a "second
    opinion" for radiologist.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has shown significant growth in supervised-learning, and continues
    to grow towards better facilitating radiologists in workflow and decision-making.
    This paper provided an overview on deep learning theory, the effectiveness of
    different deep learning architectures for breast cancer screening, and challenges
    faced by deep learning. Moreover, this paper also aimed to establish a clear understanding
    of current progress in deep learning for breast tumour diagnosis, so future directions
    are easily discernible.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Sung, J. Ferlay, R. L. Siegel, M. Laversanne, I. Soerjomataram, A. Jemal,
    and F. Bray, “Global cancer statistics 2020: Globocan estimates of incidence and
    mortality worldwide for 36 cancers in 185 countries,” *CA: a cancer journal for
    clinicians*, vol. 71, no. 3, p. 212, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. C. Society, “Breast cancer facts & figures 2019–2020,” *Am. Cancer Soc*,
    pp. 1–44, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. P. A. Cancer, “Breast cancer screening in canada; monitoring & evaluation
    of quality indicators–results report, january 2011 to december 2012,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Løberg, M. L. Lousdal, M. Bretthauer, and M. Kalager, “Benefits and
    harms of mammography screening,” *Breast Cancer Research*, vol. 17, no. 1, pp.
    1–12, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Chong, S. P. Weinstein, E. S. McDonald, and E. F. Conant, “Digital breast
    tomosynthesis: concepts and clinical practice,” *Radiology*, vol. 292, no. 1,
    pp. 1–14, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. L. Marinovich, K. E. Hunter, P. Macaskill, and N. Houssami, “Breast
    cancer screening using tomosynthesis or mammography: a meta-analysis of cancer
    detection and recall,” *JNCI: Journal of the National Cancer Institute*, vol.
    110, no. 9, pp. 942–949, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. B. Kopans, “Digital breast tomosynthesis from concept to clinical care,”
    *American Journal of Roentgenology*, vol. 202, no. 2, pp. 299–308, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. L. Giger, M. F. Inciardi, A. Edwards, J. Papaioannou, K. Drukker, Y. Jiang,
    R. Brem, and J. B. Brown, “Automated breast ultrasound in breast cancer screening
    of women with dense breasts: reader study of mammography-negative and mammography-positive
    cancers,” *American Journal of roentgenology*, vol. 206, no. 6, pp. 1341–1350,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] K. Doi, “Computer-aided diagnosis in medical imaging: historical review,
    current status and future potential,” *Computerized medical imaging and graphics*,
    vol. 31, no. 4-5, pp. 198–211, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. L. Giger, N. Karssemeijer, and J. A. Schnabel, “Breast image analysis
    for risk assessment, detection, diagnosis, and treatment of cancer,” *Annual review
    of biomedical engineering*, vol. 15, pp. 327–357, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J.-Z. Cheng, Y.-H. Chou, C.-S. Huang, Y.-C. Chang, C.-M. Tiu, K.-W. Chen,
    and C.-M. Chen, “Computer-aided us diagnosis of breast lesions by using cell-based
    contour grouping,” *Radiology*, vol. 255, no. 3, pp. 746–754, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. Van Ginneken, C. M. Schaefer-Prokop, and M. Prokop, “Computer-aided
    diagnosis: how to move from the laboratory to the clinic,” *Radiology*, vol. 261,
    no. 3, pp. 719–732, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] K. H. Allison, L. M. Reisch, P. A. Carney, D. L. Weaver, S. J. Schnitt,
    F. P. O’Malley, B. M. Geller, and J. G. Elmore, “Understanding diagnostic variability
    in breast pathology: lessons learned from an expert consensus review panel,” *Histopathology*,
    vol. 65, no. 2, pp. 240–251, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. J. Grimm, A. L. Anderson, J. A. Baker, K. S. Johnson, R. Walsh, S. C.
    Yoon, and S. V. Ghate, “Interobserver variability between breast imagers using
    the fifth edition of the bi-rads mri lexicon,” *American Journal of Roentgenology*,
    vol. 204, no. 5, pp. 1120–1124, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D. Wang, A. Khosla, R. Gargeya, H. Irshad, and A. H. Beck, “Deep learning
    for identifying metastatic breast cancer,” *arXiv preprint arXiv:1606.05718*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova, H. Ashrafian,
    T. Back, M. Chesus, G. S. Corrado, A. Darzi *et al.*, “International evaluation
    of an ai system for breast cancer screening,” *Nature*, vol. 577, no. 7788, pp.
    89–94, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Ibrahim, P. Gamble, R. Jaroensri, M. M. Abdelsamea, C. H. Mermel, P.-H. C.
    Chen, and E. A. Rakha, “Artificial intelligence in digital breast pathology: techniques
    and applications,” *The Breast*, vol. 49, pp. 267–273, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*.   MIT Press,
    2016, [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. P. Singh, L. Wang, S. Gupta, H. Goli, P. Padmanabhan, and B. Gulyás,
    “3d deep learning on medical images: a review,” *Sensors*, vol. 20, no. 18, p.
    5097, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances in neural information processing
    systems*, vol. 25, pp. 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] K. Suzuki, “Overview of deep learning in medical imaging,” *Radiological
    physics and technology*, vol. 10, no. 3, pp. 257–273, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi, “Convolutional neural
    networks: an overview and application in radiology,” *Insights into imaging*,
    vol. 9, no. 4, pp. 611–629, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern Recognition*, vol. 77, pp. 354–377, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] F. Amherd and E. Rodriguez, “Heatmap-based object detection and tracking
    with a fully convolutional neural network,” *arXiv preprint arXiv:2101.03541*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, “Improving the robustness
    of deep neural networks via stability training,” *arXiv preprint arXiv:1604.04326*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. LeNail, “Nn-svg: Publication-ready neural network architecture schematics,”
    *Journal of Open Source Software*, vol. 4, no. 33, p. 747, 2019\. [Online]. Available:
    [https://doi.org/10.21105/joss.00747](https://doi.org/10.21105/joss.00747)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, “Dive into deep learning,”
    *arXiv preprint arXiv:2106.11342*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] K. O’Shea and R. Nash, “An introduction to convolutional neural networks,”
    *arXiv preprint arXiv:1511.08458*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Wu, “Introduction to convolutional neural networks,” *National Key
    Lab for Novel Software Technology. Nanjing University. China*, vol. 5, no. 23,
    p. 495, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] G. Kang, K. Liu, B. Hou, and N. Zhang, “3d multi-view convolutional neural
    networks for lung nodule classification,” *PloS one*, vol. 12, no. 11, p. e0188290,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. D. Hunter, “Matplotlib: A 2d graphics environment,” *Computing in Science
    & Engineering*, vol. 9, no. 3, pp. 90–95, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation functions,”
    *arXiv preprint arXiv:1710.05941*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall, “Activation functions:
    Comparison of trends in practice and research for deep learning,” *arXiv preprint
    arXiv:1811.03378*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Sun, Z. Song, X. Jiang, J. Pan, and Y. Pang, “Learning pooling for
    convolutional neural network,” *Neurocomputing*, vol. 224, pp. 96–104, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. A. Mazurowski, M. Buda, A. Saha, and M. R. Bashir, “Deep learning in
    radiology: an overview of the concepts and a survey of the state of the art,”
    *arXiv preprint arXiv:1802.08717*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Ruder, “An overview of gradient descent optimization algorithms,” *arXiv
    preprint arXiv:1609.04747*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Ying, “An overview of overfitting and its solutions,” in *Journal of
    Physics: Conference Series*, vol. 1168, no. 2.   IOP Publishing, 2019, p. 022022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] W. S. Sarle *et al.*, “Stopped training and other remedies for overfitting,”
    *Computing science and statistics*, pp. 352–360, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value
    of network pruning,” *arXiv preprint arXiv:1810.05270*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The journal
    of machine learning research*, vol. 15, no. 1, pp. 1929–1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,”
    *Journal of Big data*, vol. 3, no. 1, pp. 1–40, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on deep
    transfer learning,” in *International conference on artificial neural networks*.   Springer,
    2018, pp. 270–279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P. Martinez-Gonzalez,
    and J. Garcia-Rodriguez, “A survey on deep learning techniques for image and video
    semantic segmentation,” *Applied Soft Computing*, vol. 70, pp. 41–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Davis and M. Goadrich, “The relationship between precision-recall and
    roc curves,” in *Proceedings of the 23rd international conference on Machine learning*,
    2006, pp. 233–240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] L. A. Jeni, J. F. Cohn, and F. De La Torre, “Facing imbalanced data–recommendations
    for the use of performance metrics,” in *2013 Humaine association conference on
    affective computing and intelligent interaction*.   IEEE, 2013, pp. 245–251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D. Chicco and G. Jurman, “The advantages of the matthews correlation coefficient
    (mcc) over f1 score and accuracy in binary classification evaluation,” *BMC genomics*,
    vol. 21, no. 1, pp. 1–13, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class
    imbalance,” *Journal of Big Data*, vol. 6, no. 1, pp. 1–54, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] K. H. Zou, S. K. Warfield, A. Bharatha, C. M. Tempany, M. R. Kaus, S. J.
    Haker, W. M. Wells III, F. A. Jolesz, and R. Kikinis, “Statistical validation
    of image segmentation quality based on a spatial overlap index1: scientific reports,”
    *Academic radiology*, vol. 11, no. 2, pp. 178–189, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. El Adoui, S. A. Mahmoudi, M. A. Larhmam, and M. Benjelloun, “Mri breast
    tumor segmentation using different encoder and decoder cnn architectures,” *Computers*,
    vol. 8, no. 3, p. 52, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] N. Dhungel, G. Carneiro, and A. P. Bradley, “A deep learning approach
    for the analysis of masses in mammograms with minimal user intervention,” *Medical
    image analysis*, vol. 37, pp. 114–128, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han, and T.-S. Kim,
    “A fully integrated computer-aided diagnosis system for digital x-ray mammograms
    via deep learning detection, segmentation, and classification,” *International
    journal of medical informatics*, vol. 117, pp. 44–54, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] H. Chougrad, H. Zouaki, and O. Alheyane, “Deep convolutional neural networks
    for breast cancer screening,” *Computer methods and programs in biomedicine*,
    vol. 157, pp. 19–30, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] D. Ribli, A. Horváth, Z. Unger, P. Pollner, and I. Csabai, “Detecting
    and classifying lesions in mammograms with deep learning,” *Scientific reports*,
    vol. 8, no. 1, pp. 1–7, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker,
    A. Saleh, M. Arenas, M. Arquez, D. Puig *et al.*, “Breast tumor segmentation and
    shape classification in mammograms using generative adversarial and convolutional
    neural network,” *Expert Systems with Applications*, vol. 139, p. 112855, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] W. T. Tran, A. Sadeghi-Naini, F.-I. Lu, S. Gandhi, N. Meti, M. Brackstone,
    E. Rakovitch, and B. Curpen, “Computational radiology in breast cancer screening
    and diagnosis using artificial intelligence,” *Canadian Association of Radiologists
    Journal*, vol. 72, no. 1, pp. 98–108, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] E. F. Conant, W. E. Barlow, S. D. Herschorn, D. L. Weaver, E. F. Beaber,
    A. N. Tosteson, J. S. Haas, K. P. Lowry, N. K. Stout, A. Trentham-Dietz *et al.*,
    “Association of digital breast tomosynthesis vs digital mammography with cancer
    detection and recall rates by age and breast density,” *JAMA oncology*, vol. 5,
    no. 5, pp. 635–642, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] E. A. Rafferty, M. A. Durand, E. F. Conant, D. S. Copit, S. M. Friedewald,
    D. M. Plecha, and D. P. Miller, “Breast cancer screening using tomosynthesis and
    digital mammography in dense and nondense breasts,” *Jama*, vol. 315, no. 16,
    pp. 1784–1786, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. P. Lowry, R. Y. Coley, D. L. Miglioretti, K. Kerlikowske, L. M. Henderson,
    T. Onega, B. L. Sprague, J. M. Lee, S. Herschorn, A. N. Tosteson *et al.*, “Screening
    performance of digital breast tomosynthesis vs digital mammography in community
    practice by patient age, screening round, and breast density,” *JAMA network open*,
    vol. 3, no. 7, pp. e2 011 792–e2 011 792, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] E. F. Conant, E. F. Beaber, B. L. Sprague, S. D. Herschorn, D. L. Weaver,
    T. Onega, A. N. Tosteson, A. M. McCarthy, S. P. Poplack, J. S. Haas *et al.*,
    “Breast cancer screening using tomosynthesis in combination with digital mammography
    compared to digital mammography alone: a cohort study within the prospr consortium,”
    *Breast cancer research and treatment*, vol. 156, no. 1, pp. 109–116, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X.-A. Phi, A. Tagliafico, N. Houssami, M. J. Greuter, and G. H. de Bock,
    “Digital breast tomosynthesis for breast cancer screening and diagnosis in women
    with dense breasts–a systematic review and meta-analysis,” *BMC cancer*, vol. 18,
    no. 1, pp. 1–9, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] P. Pattacini, A. Nitrosi, P. Giorgi Rossi, V. Iotti, V. Ginocchi, S. Ravaioli,
    R. Vacondio, L. Braglia, S. Cavuto, C. Campari *et al.*, “Digital mammography
    versus digital mammography plus tomosynthesis for breast cancer screening: the
    reggio emilia tomosynthesis randomized trial,” *Radiology*, vol. 288, no. 2, pp.
    375–385, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] J. A. Baker and J. Y. Lo, “Breast tomosynthesis: state-of-the-art and
    review of the literature,” *Academic radiology*, vol. 18, no. 10, pp. 1298–1310,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. V. Horvat, D. M. Keating, H. Rodrigues-Duarte, E. A. Morris, and V. L.
    Mango, “Calcifications at digital breast tomosynthesis: imaging features and biopsy
    techniques,” *Radiographics*, vol. 39, no. 2, pp. 307–318, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. H. Kim, H. H. Kim, and W. K. Moon, “Automated breast ultrasound screening
    for dense breasts,” *Korean journal of radiology*, vol. 21, no. 1, pp. 15–24,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. J. Shin, H. H. Kim, and J. H. Cha, “Current status of automated breast
    ultrasonography,” *Ultrasonography*, vol. 34, no. 3, p. 165, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. D. Lehman, J. M. Lee, W. B. DeMartini, D. S. Hippe, M. H. Rendi, G. Kalish,
    P. Porter, J. Gralow, and S. C. Partridge, “Screening mri in women with a personal
    history of breast cancer,” *Journal of the National Cancer Institute*, vol. 108,
    no. 3, p. djv349, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. M. Mann, N. Cho, and L. Moy, “Breast mri: state of the art,” *Radiology*,
    vol. 292, no. 3, pp. 520–536, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H. Greenspan, G. Oz, N. Kiryati, and S. Peled, “Mri inter-slice reconstruction
    using super-resolution,” *Magnetic resonance imaging*, vol. 20, no. 5, pp. 437–446,
    2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] R. Z. Shilling, T. Q. Robbie, T. Bailloeul, K. Mewes, R. M. Mersereau,
    and M. E. Brummer, “A super-resolution framework for 3-d high-resolution and high-contrast
    imaging using 2-d multislice mri,” *IEEE transactions on medical imaging*, vol. 28,
    no. 5, pp. 633–644, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Yousefi, A. Krzyżak, and C. Y. Suen, “Mass detection in digital breast
    tomosynthesis data using convolutional neural networks and multiple instance learning,”
    *Computers in biology and medicine*, vol. 96, pp. 283–293, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] D. H. Kim, S. T. Kim, J. M. Chang, and Y. M. Ro, “Latent feature representation
    with depth directional long-term recurrent learning for breast masses in digital
    breast tomosynthesis,” *Physics in Medicine & Biology*, vol. 62, no. 3, p. 1009,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Liu, D. Xu, S. K. Zhou, T. Mertelmeier, J. Wicklein, A. Jerebko, S. Grbic,
    O. Pauly, W. Cai, and D. Comaniciu, “3d anisotropic hybrid network: Transferring
    convolutional features from 2d images to 3d anisotropic volumes,” *arXiv preprint
    arXiv:1711.08580*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Y. Zhang, X. Wang, H. Blanton, G. Liang, X. Xing, and N. Jacobs, “2d convolutional
    neural networks for 3d digital breast tomosynthesis classification,” in *2019
    IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*.   IEEE,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] G. Liang, X. Wang, Y. Zhang, X. Xing, H. Blanton, T. Salem, and N. Jacobs,
    “Joint 2d-3d breast cancer classification,” in *2019 IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*.   IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *arXiv preprint arXiv:1512.03385*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *International Conference on Learning Representations*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *Computer Vision and Pattern
    Recognition (CVPR)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] X. Zhang, Y. Zhang, E. Y. Han, N. Jacobs, Q. Han, X. Wang, and J. Liu,
    “Classification of whole mammogram and tomosynthesis images using deep convolutional
    neural networks,” *IEEE transactions on nanobioscience*, vol. 17, no. 3, pp. 237–242,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. Fan, H. Zheng, S. Zheng, C. You, Y. Gu, X. Gao, W. Peng, and L. Li,
    “Mass detection and segmentation in digital breast tomosynthesis using 3d-mask
    region-based convolutional neural network: A comparative analysis,” *Frontiers
    in molecular biosciences*, vol. 7, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] I. Wichakam, J. Chayakulkheeree, and P. Vateekul, “Deep multi-label 3d
    convnet for breast cancer diagnosis in dbt with inversion augmentation,” in *Tenth
    International Conference on Digital Image Processing (ICDIP 2018)*.   International
    Society for Optics and Photonics, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Lei, X. He, J. Yao, T. Wang, L. Wang, W. Li, W. J. Curran, T. Liu,
    D. Xu, and X. Yang, “Breast tumor segmentation in 3d automatic breast ultrasound
    using mask scoring r-cnn,” *Medical Physics*, vol. 48, no. 1, pp. 204–214, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Zhou, H. Chen, Y. Li, Q. Liu, X. Xu, S. Wang, P.-T. Yap, and D. Shen,
    “Multi-task learning for segmentation and classification of tumors in 3d automated
    breast ultrasound images,” *Medical Image Analysis*, vol. 70, p. 101918, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Zhou, L.-Y. Luo, Q. Dou, H. Chen, C. Chen, G.-J. Li, Z.-F. Jiang, and
    P.-A. Heng, “Weakly supervised 3d deep learning for breast cancer classification
    and localization of the lesions in mr images,” *Journal of Magnetic Resonance
    Imaging*, vol. 50, no. 4, pp. 1144–1151, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Q. Hu, H. M. Whitney, and M. L. Giger, “A deep learning methodology for
    improved breast cancer diagnosis using multiparametric mri,” *Scientific reports*,
    vol. 10, no. 1, pp. 1–11, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. Xiao, H. Sun, Y. Meng, Y. Peng, X. Yang, S. Chen, Z. Yan, and J. Zheng,
    “Classification of microcalcification clusters in digital breast tomosynthesis
    using ensemble convolutional neural network,” *BioMedical Engineering OnLine*,
    vol. 20, no. 1, pp. 1–20, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] S. Liu, D. Xu, S. K. Zhou, S. Grbic, W. Cai, and D. Comaniciu, “Anisotropic
    hybrid network for cross-dimension transferable feature learning in 3d medical
    images,” in *Deep Learning and Convolutional Neural Networks for Medical Imaging
    and Clinical Informatics*.   Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A closer
    look at spatiotemporal convolutions for action recognition,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *Computer Vision and Pattern Recognition (CVPR)*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Buda, A. Saha, R. Walsh, S. Ghate, N. Li, A. Święcicki, J. Y. Lo, and
    M. A. Mazurowski, “Detection of masses and architectural distortions in digital
    breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep
    learning model,” *arXiv preprint arXiv:2011.07995*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. J. S. Gardezi, A. Elazab, B. Lei, and T. Wang, “Breast cancer detection
    and diagnosis using mammographic data: Systematic review,” *Journal of medical
    Internet research*, vol. 21, no. 7, p. e14464, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] K. Dembrower, P. Lindholm, and F. Strand, “A multi-million mammography
    image dataset and population-based screening cohort for the training and evaluation
    of deep neural networks—the cohort of screen-aged women (csaw),” *Journal of digital
    imaging*, pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake, M. Gorovoy, and D. L. Rubin,
    “A curated mammography data set for use in computer-aided detection and diagnosis
    research,” *Scientific data*, vol. 4, no. 1, pp. 1–9, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] I. C. Moreira, I. Amaral, I. Domingues, A. Cardoso, M. J. Cardoso, and
    J. S. Cardoso, “Inbreast: toward a full-field digital mammographic database,”
    *Academic radiology*, vol. 19, no. 2, pp. 236–248, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] D. Abdelhafiz, C. Yang, R. Ammar, and S. Nabavi, “Deep convolutional neural
    networks for mammography: advances, challenges and applications,” *BMC bioinformatics*,
    vol. 20, no. 11, pp. 1–20, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] K.-H. Yu, A. L. Beam, and I. S. Kohane, “Artificial intelligence in healthcare,”
    *Nature biomedical engineering*, vol. 2, no. 10, pp. 719–731, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study of the class
    imbalance problem in convolutional neural networks,” *Neural Networks*, vol. 106,
    pp. 249–259, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] W. W. Ng, G. Zeng, J. Zhang, D. S. Yeung, and W. Pedrycz, “Dual autoencoders
    features for imbalance classification problem,” *Pattern Recognition*, vol. 60,
    pp. 875–889, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] L. Ge, J. Gao, H. Ngo, K. Li, and A. Zhang, “On handling negative transfer
    and imbalanced distributions in multiple source transfer learning,” *Statistical
    Analysis and Data Mining: The ASA Data Science Journal*, vol. 7, no. 4, pp. 254–271,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] S. M. Abd Elrahman and A. Abraham, “A review of class imbalance problem,”
    *Journal of Network and Innovative Computing*, vol. 1, no. 2013, pp. 332–340,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation
    with pseudo-3d residual networks,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *International Conference
    on Computer Vision (ICCV)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
