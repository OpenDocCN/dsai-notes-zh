- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:51:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.08853] A survey on deep learning approaches for breast cancer diagnosis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.08853] 关于乳腺癌诊断的深度学习方法的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.08853](https://ar5iv.labs.arxiv.org/html/2109.08853)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2109.08853](https://ar5iv.labs.arxiv.org/html/2109.08853)
- en: A survey on deep learning approaches for breast cancer diagnosis
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于乳腺癌诊断的深度学习方法的综述
- en: Timothy Kwong¹, and Samaneh Mazaheri²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Timothy Kwong¹ 和 Samaneh Mazaheri²
- en: ¹Faculty of Engineering and Applied Science,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹工程与应用科学学院，
- en: Ontario Tech University, Oshawa, ON Canada
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大安大略理工大学，安大略省奥沙瓦
- en: ²Faculty of Business and Information Technology,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²商业与信息技术学院，
- en: Ontario Tech University, Oshawa, ON Canada
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大安大略理工大学，安大略省奥沙瓦
- en: timothy.kwong@ontariotechu.net, Samaneh.Mazaheri@ontariotechu.ca
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: timothy.kwong@ontariotechu.net, Samaneh.Mazaheri@ontariotechu.ca
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning has introduced several learning-based methods to recognize breast
    tumours and presents high applicability in breast cancer diagnostics. It has presented
    itself as a practical installment in Computer-Aided Diagnostic (CAD) systems to
    further assist radiologists in diagnostics for different modalities. A deep learning
    network trained on images provided by hospitals or public databases can perform
    classification, detection, and segmentation of lesion types. Significant progress
    has been made in recognizing tumours on 2D images but recognizing 3D images remains
    a frontier so far. The interconnection of deep learning networks between different
    fields of study help propels discoveries for more efficient, accurate, and robust
    networks. In this review paper, the following topics will be explored: (i) theory
    and application of deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches
    in breast tumour recognition from a performance metric perspective, and (iii)
    challenges faced in CNN approaches.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习引入了几种基于学习的方法来识别乳腺肿瘤，并在乳腺癌诊断中展现出高度的适用性。它已经作为一种实用的计算机辅助诊断（CAD）系统的组成部分，进一步帮助放射科医师在不同的影像模式下进行诊断。基于医院或公共数据库提供的图像训练的深度学习网络可以进行分类、检测和病变类型的分割。虽然在2D图像肿瘤识别方面已经取得了显著进展，但3D图像识别仍然是一个前沿领域。不同研究领域之间深度学习网络的互联有助于推动更高效、准确和稳健网络的发现。在这篇综述文章中，将探讨以下主题：（i）深度学习的理论与应用，（ii）从性能指标角度看2D、2.5D和3D卷积神经网络在乳腺肿瘤识别中的进展，以及（iii）卷积神经网络方法面临的挑战。
- en: 'Index Terms:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Mammography, Digital Breast Tomosynthesis, Automatic Breast Ultrasound, MRI,
    2D Convolutional Neural Network, 3D Convolutional Neural Network, Classification,
    Detection, Segmentation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺X线摄影、数字乳腺断层合成、自动乳腺超声、MRI、2D卷积神经网络、3D卷积神经网络、分类、检测、分割
- en: I Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In 2020, female breast cancer had 2.26 million new cases making breast cancer
    the highest number of new cases out of 36 cancer sites [[1](#bib.bib1)]. Moreover,
    the number of new deaths, due to female breast cancer, was 0.684 million, ranking
    it the fourth highest of 35 other cancer sites [[1](#bib.bib1)]. Current modalities
    for breast cancer screenings include mammography, digital breast tomosynthesis,
    breast ultrasound, magnetic resonance imaging [[2](#bib.bib2)]. Mammography has
    two types, screen-film mammography and Digital Mammography (DM), where both types
    are forms of x-ray imaging that use radiation to obtain a 2D image of the breast
    tissue [[2](#bib.bib2), [3](#bib.bib3)]. In addition, mammography has facilitated
    the detection of early stage breast cancer to reduce the risk of cancer death
    [[2](#bib.bib2), [4](#bib.bib4)]. Technological advancements in image acquisition
    had brought Digital Breast Tomosynthesis (DBT). DBT addressed issues in mammography
    and delivered improved image acquisition [[5](#bib.bib5), [3](#bib.bib3), [6](#bib.bib6),
    [7](#bib.bib7)]. It captures multiple 2D images slices of the breast, which are
    then synthesized into a 3D image [[2](#bib.bib2), [6](#bib.bib6), [5](#bib.bib5)].
    However, these 3D images (volumes) are quasi-3D, due to being a reconstruction
    of multiple captured 2D images [[6](#bib.bib6), [7](#bib.bib7)]. Furthermore,
    image slices are captured using a x-ray tube that pivots parallel to the chest
    wall along a 15^∘ to 60^∘ arc [[5](#bib.bib5)]. Automatic Breast UltraSound (ABUS)
    uses high frequency to image the entire breast. These 2D images are obtained on
    the transverse plane and synthesized into a 3D volume [[8](#bib.bib8)]. Magnetic
    Resonance Imaging (MRI) uses high-powered magnets along with radio waves generated
    by a computer to image the breast [[2](#bib.bib2)]. CAD systems assist radiologists
    in making diagnostic decisions with higher confidence by providing a "second opinion"
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]. In addition, as mentioned
    by [[12](#bib.bib12)], the CAD system should improve radiologists’ performance,
    save time, seamlessly integrate with workflow, and not impose liabilities. The
    integration of deep learning algorithms into CAD systems aim to address mentioned
    objectives above, as well as reducing assessment variability from different radiologists
    [[13](#bib.bib13), [14](#bib.bib14)], reducing recall rate, and increasing cancer
    detection rate. The continuous advances in deep learning has brought upon models
    that outperform radiologists in both classification and localization of cancer
    tumors in medical images [[15](#bib.bib15), [16](#bib.bib16)]. As algorithms advance,
    computing power becomes more accessible, and expansive well-curated datasets become
    open-sourced. Machine learning techniques are able to shift towards state-of-the-art,
    and aid in tasks within the healthcare sector [[17](#bib.bib17)]. Deep learning,
    a sub-field of machine learning, conveys representations in simpler forms to solve
    the problem of learning different representations [[18](#bib.bib18)]. Moreover,
    deep learning uses multiple interconnected layers of artificial neurons to learn
    the patterns of simpler expressed forms of the actual representation [[19](#bib.bib19),
    [18](#bib.bib18)]. In 2012, a convolutional neural network architecture scored
    an error rate of 15.3%, which was 10.9% lower than the second-best entry [[20](#bib.bib20)].
    This breakthrough led to an increase in research participation in the field of
    deep learning, and the continuation of research and usage of CNN architecture
    for image recognition problems [[21](#bib.bib21)]. Convolutional Neural Network
    (CNN) are specialized networks to process data with known grid patterns, as well
    as learning spatial hierarchies of features within data [[18](#bib.bib18), [22](#bib.bib22)].
    As a result, hand-crafted features of cancer tumours are not required for CNNs,
    considering as CNNs can learn features. The applicability of deep learning in
    the medical field presents itself through classification, localization, and segmentation
    of cancer tumors in medical images from modalities such as MRI, CT, ultrasound.
    This review paper will provide insight into deep learning theory, progress of
    2D, 2.5D, and 3D CNN architectures, and challenges faced when training a network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，女性乳腺癌的新发病例达到226万例，使乳腺癌成为36种癌症中新发病例最多的 [[1](#bib.bib1)]。此外，由于女性乳腺癌导致的新死亡人数为68.4万，排名35种其他癌症中第四高
    [[1](#bib.bib1)]。目前的乳腺癌筛查方法包括乳腺X线摄影、数字乳腺层析成像、乳腺超声和磁共振成像 [[2](#bib.bib2)]。乳腺X线摄影有两种类型，即屏幕胶卷乳腺X线摄影和数字乳腺X线摄影（DM），这两种类型都是X射线成像形式，利用辐射获得乳腺组织的二维图像
    [[2](#bib.bib2), [3](#bib.bib3)]。此外，乳腺X线摄影有助于早期发现乳腺癌，降低癌症死亡的风险 [[2](#bib.bib2),
    [4](#bib.bib4)]。图像获取技术的进步带来了数字乳腺层析成像（DBT）。DBT解决了乳腺X线摄影中的问题，并提供了改进的图像获取 [[5](#bib.bib5),
    [3](#bib.bib3), [6](#bib.bib6), [7](#bib.bib7)]。它捕获乳腺的多个二维图像切片，然后将这些图像合成成三维图像
    [[2](#bib.bib2), [6](#bib.bib6), [5](#bib.bib5)]。然而，这些三维图像（体积）是准三维的，因为它们是多张捕获的二维图像的重建
    [[6](#bib.bib6), [7](#bib.bib7)]。此外，图像切片是使用一个X射线管捕获的，该X射线管沿15^∘到60^∘的弧线绕胸壁旋转 [[5](#bib.bib5)]。自动乳腺超声（ABUS）使用高频率对整个乳腺进行成像。这些二维图像是在横截面上获得的，并合成成三维体积
    [[8](#bib.bib8)]。磁共振成像（MRI）使用高功率磁铁和计算机生成的无线电波对乳腺进行成像 [[2](#bib.bib2)]。计算机辅助诊断（CAD）系统通过提供“第二意见”来帮助放射科医师做出更有信心的诊断决策
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]。此外，如[[12](#bib.bib12)]所述，CAD系统应改善放射科医师的表现，节省时间，与工作流程无缝集成，并不增加责任。将深度学习算法集成到CAD系统中，旨在解决上述目标，并减少不同放射科医师的评估变异性
    [[13](#bib.bib13), [14](#bib.bib14)]，降低复查率，提高癌症检测率。深度学习的持续进步带来了在医学图像中分类和定位癌症肿瘤方面优于放射科医师的模型
    [[15](#bib.bib15), [16](#bib.bib16)]。随着算法的进步、计算能力的普及和广泛的精心策划的数据集的开源，机器学习技术能够向最先进的技术转变，并在医疗保健领域的任务中提供帮助
    [[17](#bib.bib17)]。深度学习作为机器学习的一个子领域，将表示以更简单的形式传达，以解决学习不同表示的问题 [[18](#bib.bib18)]。此外，深度学习使用多个互联的人工神经元层来学习实际表示的简单表达形式的模式
    [[19](#bib.bib19), [18](#bib.bib18)]。2012年，一个卷积神经网络架构的错误率为15.3%，比第二名低10.9% [[20](#bib.bib20)]。这一突破导致了对深度学习领域的研究参与增加，以及继续研究和使用CNN架构解决图像识别问题
    [[21](#bib.bib21)]。卷积神经网络（CNN）是专门处理具有已知网格模式的数据的网络，并学习数据中空间特征的层次结构 [[18](#bib.bib18),
    [22](#bib.bib22)]。因此，考虑到CNN可以学习特征，癌症肿瘤的手工特征不再是CNN所必需的。深度学习在医疗领域的适用性体现在对医疗图像中癌症肿瘤的分类、定位和分割，这些图像来自MRI、CT、超声等模态。这篇综述文章将深入探讨深度学习理论、2D、2.5D和3D
    CNN架构的发展，以及训练网络时遇到的挑战。
- en: II Deep Learning Theory
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度学习理论
- en: This section provides a theoretical overview on deep learning concepts, including
    data augmentation, building blocks in a typical CNN architecture, overfitting,
    and transfer learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了深度学习概念的理论概述，包括数据增强、典型CNN架构中的构建块、过拟合和迁移学习。
- en: II-A Data Augmentation
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 数据增强
- en: Data augmentation is a technique aimed at increasing the dataset size, and improving
    performance and robustness of the model [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    Data augmentation methods such as translation, rotation, reflection, blur, and
    crop, are applied directly on the original image to generate new augmented images.
    An instance of data augmentation applies each listed method to an original image
    to generate 5 new augmented image, which increases the dataset size with new unseen
    training instances. Image resizing and greyscaling are other strategies used during
    data pre-processing to reduce the computation complexity required to process these
    images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种旨在增加数据集大小并提高模型性能和鲁棒性的技术[[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。数据增强方法，如平移、旋转、反射、模糊和裁剪，直接应用于原始图像以生成新的增强图像。数据增强的一个实例是对每个列出的方法应用于原始图像，从而生成5个新的增强图像，这样可以通过新的未见训练实例来增加数据集的大小。图像的缩放和灰度化是数据预处理过程中使用的其他策略，以减少处理这些图像所需的计算复杂性。
- en: '|  | $s(t)=(x\ast w)(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a)$ |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $s(t)=(x\ast w)(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a)$ |  | (1) |'
- en: '![Refer to caption](img/cb48fc9253e1673e739bef050bbd012a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cb48fc9253e1673e739bef050bbd012a.png)'
- en: 'Figure 1: A basic Convolutional Neural Network (CNN) extracts features from
    an input to output a feature vector. This CNN contains two layers, the convolution
    and pooling layer. In convolution layer, the entire input is convolved by a kernel,
    while in the pooling layer, the input is down-sampled. The final output is a flattened
    column vector containing significant features of the input. Adapted from [[26](#bib.bib26)].'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基本的卷积神经网络（CNN）从输入中提取特征并输出特征向量。这个CNN包含两个层，卷积层和池化层。在卷积层中，整个输入被一个卷积核进行卷积，而在池化层中，输入被下采样。最终输出是一个包含输入重要特征的扁平列向量。改编自[[26](#bib.bib26)]。
- en: II-B 2D Convolutional Layer
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 2D 卷积层
- en: Convolutional neural networks use a mathematical operation called convolution;
    a convolution is a linear operation used, in this case, for feature extraction
    [[18](#bib.bib18), [22](#bib.bib22)]. Discrete convolution is expressed as equ.
    ([1](#S2.E1 "Equation 1 ‣ II-A Data Augmentation ‣ II Deep Learning Theory ‣ A
    survey on deep learning approaches for breast cancer diagnosis")), as seen in
    [[18](#bib.bib18), equ. (9.1)], where s(t) represents the feature map, x(a) represents
    the input, and w(t-a) represents the kernel. Moreover, equ. ([1](#S2.E1 "Equation
    1 ‣ II-A Data Augmentation ‣ II Deep Learning Theory ‣ A survey on deep learning
    approaches for breast cancer diagnosis")) illustrates an element-wise product
    between an input and a kernel to produce a feature map [[22](#bib.bib22), [27](#bib.bib27)].
    The 2D convolutional kernel is a matrix of weights that extracts meaningful features
    from the input for the network to learn and recognize different inputs. A feature
    maps can be generated through convolving a kernel with an input, then applying
    an activation function on the convolved output [[23](#bib.bib23)]. In addition,
    backpropagation is used to update the kernel weights to minimize the loss function
    [[22](#bib.bib22)]. Stride, as defined by [[22](#bib.bib22)], is "the distance
    between two successive kernel positions", which dictates the step size of the
    kernel across the input. Padding is a technique used to retain the in-plane dimensionality
    of the feature map even after a convolution further permitting more convolutional
    layers [[22](#bib.bib22), [27](#bib.bib27)]. Zero-padding aligns the border of
    the input with zeros to retain the dimensionality [[28](#bib.bib28)]. Parameter
    sharing is a mechanism used in CNN to limit the number of parameters by sharing
    the kernel weights, which ultimately reduces the model complexity [[22](#bib.bib22),
    [23](#bib.bib23), [28](#bib.bib28)]. In addition, parameters can be shared among
    more abstract features that occur within different images [[28](#bib.bib28), [29](#bib.bib29)].
    Figure [1](#S2.F1 "Figure 1 ‣ II-A Data Augmentation ‣ II Deep Learning Theory
    ‣ A survey on deep learning approaches for breast cancer diagnosis") illustrates
    the feature extraction on the input by the convolution and pooling layer to output
    a feature map.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络使用一种叫做卷积的数学运算；卷积是一种线性操作，在这种情况下用于特征提取 [[18](#bib.bib18), [22](#bib.bib22)]。离散卷积表示为公式
    ([1](#S2.E1 "方程 1 ‣ II-A 数据增强 ‣ II 深度学习理论 ‣ 乳腺癌诊断深度学习方法综述"))，如 [[18](#bib.bib18),
    公式 (9.1)] 中所示，其中 s(t) 表示特征图，x(a) 表示输入，w(t-a) 表示卷积核。此外，公式 ([1](#S2.E1 "方程 1 ‣ II-A
    数据增强 ‣ II 深度学习理论 ‣ 乳腺癌诊断深度学习方法综述")) 说明了输入与卷积核之间的逐元素乘积以生成特征图 [[22](#bib.bib22),
    [27](#bib.bib27)]。2D 卷积核是一个权重矩阵，用于从输入中提取有意义的特征，以便网络学习和识别不同的输入。通过将卷积核与输入进行卷积，然后在卷积输出上应用激活函数，可以生成特征图
    [[23](#bib.bib23)]。此外，反向传播用于更新卷积核权重，以最小化损失函数 [[22](#bib.bib22)]。步幅，如 [[22](#bib.bib22)]
    所定义，是“两个连续卷积核位置之间的距离”，决定了卷积核在输入上的步长。填充是一种技术，用于在卷积后保持特征图的平面维度，从而允许更多的卷积层 [[22](#bib.bib22),
    [27](#bib.bib27)]。零填充通过在输入边界对齐零来保持维度 [[28](#bib.bib28)]。参数共享是卷积神经网络中一种通过共享卷积核权重来限制参数数量的机制，这最终减少了模型的复杂性
    [[22](#bib.bib22), [23](#bib.bib23), [28](#bib.bib28)]。此外，参数可以在不同图像中出现的更抽象特征之间共享
    [[28](#bib.bib28), [29](#bib.bib29)]。图 [1](#S2.F1 "图 1 ‣ II-A 数据增强 ‣ II 深度学习理论
    ‣ 乳腺癌诊断深度学习方法综述") 说明了卷积和池化层对输入进行特征提取以输出特征图。
- en: II-C 3D Convolutional Layer
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 3D 卷积层
- en: In a 3D CNN, the kernels, stride, and pooling operation are three dimensional,
    where the third dimension represents a depth dimension [[30](#bib.bib30)]. This
    additional dimension allows 3D CNNs to extract features from an additional axis
    of information. In 3D convolutional layers, voxel represents the spatial information
    rather than pixels.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 3D CNN 中，卷积核、步幅和池化操作都是三维的，其中第三维度表示深度维度 [[30](#bib.bib30)]。这个额外的维度使得 3D CNN
    能够从额外的信息轴中提取特征。在 3D 卷积层中，体素表示空间信息，而不是像素。
- en: '![Refer to caption](img/a6171bd3a26e3a1fd02fd96235bcd3b0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6171bd3a26e3a1fd02fd96235bcd3b0.png)'
- en: 'Figure 2: The Rectified Linear Unit (ReLU) function. Adapted from [[31](#bib.bib31)].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：修正线性单元（ReLU）函数。改编自 [[31](#bib.bib31)]。
- en: II-D Activation Layer
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 激活层
- en: 'The Rectified Linear Unit (ReLU) is an activation function commonly used in
    neural networks [[22](#bib.bib22), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)].
    The ReLU function, as shown in the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）是一种常用于神经网络的激活函数 [[22](#bib.bib22), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34)]。ReLU 函数如下面所示：
- en: '|  | <math   alttext="f(x)=max(0,x)=\begin{cases}x,&amp;x\geq 0\\ 0,&amp;x<0\\'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="f(x)=max(0,x)=\begin{cases}x,&amp;x\geq 0\\ 0,&amp;x<0\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mi >f</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >x</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo  >=</mo><mrow ><mi  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow  ><mo stretchy="false"  >(</mo><mn >0</mn><mo >,</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><mrow ><mi  >x</mi><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  >≥</mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mn  >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  ><</mo><mn
    >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply  ><ci >𝑓</ci><ci >𝑥</ci></apply><apply  ><ci >𝑚</ci><ci
    >𝑎</ci><ci  >𝑥</ci><interval closure="open"  ><cn type="integer" >0</cn><ci  >𝑥</ci></interval></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >cases</csymbol><ci >𝑥</ci><apply ><ci  >𝑥</ci><cn
    type="integer"  >0</cn></apply><cn type="integer"  >0</cn><apply ><ci >𝑥</ci><cn
    type="integer" >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f(x)=max(0,x)=\begin{cases}x,&x\geq 0\\ 0,&x<0\\
    \end{cases}</annotation></semantics></math> |  | (2) |
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mi >f</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >x</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo  >=</mo><mrow ><mi  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow  ><mo stretchy="false"  >(</mo><mn >0</mn><mo >,</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><mrow ><mi  >x</mi><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  >≥</mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mn  >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mi >x</mi><mo  ><</mo><mn
    >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply  ><ci >𝑓</ci><ci >𝑥</ci></apply><apply  ><ci >𝑚</ci><ci
    >𝑎</ci><ci  >𝑥</ci><interval closure="open"  ><cn type="integer" >0</cn><ci  >𝑥</ci></interval></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >cases</csymbol><ci >𝑥</ci><apply ><ci  >𝑥</ci><cn
    type="integer"  >0</cn></apply><cn type="integer"  >0</cn><apply ><ci >𝑥</ci><cn
    type="integer" >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f(x)=max(0,x)=\begin{cases}x,&x\geq 0\\ 0,&x<0\\
    \end{cases}</annotation></semantics></math> |  | (2) |
- en: and can be seen in [[34](#bib.bib34), equ. (1.14)], equates values less than
    zero to zero, and values greater than or equal to zero to passed in value. A plot
    of the ReLU function is depicted in Figure [2](#S2.F2 "Figure 2 ‣ II-C 3D Convolutional
    Layer ‣ II Deep Learning Theory ‣ A survey on deep learning approaches for breast
    cancer diagnosis").
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以在 [[34](#bib.bib34), 等式 (1.14)] 中看到，将小于零的值归零，将大于或等于零的值保持不变。ReLU 函数的图示见图
    [2](#S2.F2 "图 2 ‣ II-C 3D 卷积层 ‣ II 深度学习理论 ‣ 关于乳腺癌诊断的深度学习方法综述")。
- en: '![Refer to caption](img/6ec99838f30d83388998c7a3e8003524.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6ec99838f30d83388998c7a3e8003524.png)'
- en: 'Figure 3: Fully-Connected Neural Network (FCNN). Adapted from [[26](#bib.bib26)].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：全连接神经网络（FCNN）。改编自 [[26](#bib.bib26)]。
- en: II-E Pooling Layer
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 池化层
- en: Pooling is a technique to down-sample feature maps [[22](#bib.bib22)], introduces
    invariance [[18](#bib.bib18)], and merge semantically similar features [[18](#bib.bib18)].
    Down-sampling a feature map reduces the in-plane dimensionality [[22](#bib.bib22),
    [35](#bib.bib35)], which reduces the data size without reducing key features in
    the feature map required for learning. Max pooling is a pooling operation, which
    obtains the maximum value within a square region [[18](#bib.bib18), [22](#bib.bib22),
    [35](#bib.bib35)]. Moreover, by obtaining the maximum value, this also makes a
    representation invariant to small translation or distortions [[18](#bib.bib18),
    [22](#bib.bib22)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 池化是一种下采样特征图的技术 [[22](#bib.bib22)]，引入不变性 [[18](#bib.bib18)]，并合并语义上相似的特征 [[18](#bib.bib18)]。下采样特征图减少了平面内的维度
    [[22](#bib.bib22), [35](#bib.bib35)]，从而在不减少特征图中用于学习的关键特征的情况下减少了数据大小。最大池化是一种池化操作，在方形区域内获取最大值
    [[18](#bib.bib18), [22](#bib.bib22), [35](#bib.bib35)]。此外，通过获取最大值，这也使表示对小的平移或扭曲具有不变性
    [[18](#bib.bib18), [22](#bib.bib22)]。
- en: '![Refer to caption](img/a48dd7396b3d2efdfd1b4c03cd52b894.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a48dd7396b3d2efdfd1b4c03cd52b894.png)'
- en: 'Figure 4: The surface of this 3-dimensional graph represents the objective
    function. Gradient descent is used to traverse towards the deepest point on this
    graph. The deepest point represents the global minimum, and parameters that minimize
    the objective function. Source: Adapted using [[31](#bib.bib31)]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：这个三维图的表面表示目标函数。梯度下降用于在此图上向最深点移动。最深点表示全局最小值，以及使目标函数最小化的参数。来源：改编自 [[31](#bib.bib31)]。
- en: II-F Fully-Connected Layer
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 完全连接层
- en: 'The Fully Connected (FC) layer learns a non-linear function to map all the
    features within a feature space. Figure [3](#S2.F3 "Figure 3 ‣ II-D Activation
    Layer ‣ II Deep Learning Theory ‣ A survey on deep learning approaches for breast
    cancer diagnosis") gives an illustration of the basic structure for a Fully-Connected
    Neural Network (FCNN). A CNN can have a FC layer, where the features extracted
    from the CNN are inputted into the FC layer for a decision output [[36](#bib.bib36)].
    During training, the goal is to minimize the prediction error made by the CNN,
    techniques such as back-propagation and gradient descent are used to improve prediction
    results. The objective function, also known as the loss function or cost function,
    determines the difference between the prediction and ground truth; it measures
    the network error in prediction. Binary cross-entropy is a loss function used
    in binary classification. Back-propagation is a technique used to determine the
    gradient of the objective function with respect to the weights [[32](#bib.bib32)].
    The equation for back-propagation of an objective function with respect to the
    weight can be calculated using chain rule and is given by the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接（FC）层学习一个非线性函数，以映射特征空间中的所有特征。图 [3](#S2.F3 "图 3 ‣ II-D 激活层 ‣ II 深度学习理论 ‣
    关于乳腺癌诊断的深度学习方法综述") 展示了完全连接神经网络（FCNN）的基本结构。CNN 可以包含 FC 层，其中从 CNN 中提取的特征输入到 FC 层以进行决策输出
    [[36](#bib.bib36)]。在训练过程中，目标是最小化 CNN 的预测误差，使用反向传播和梯度下降等技术来改善预测结果。目标函数，也称为损失函数或代价函数，确定预测与真实值之间的差异；它衡量网络在预测中的误差。二元交叉熵是用于二分类的损失函数。反向传播是一种用于确定相对于权重的目标函数梯度的技术
    [[32](#bib.bib32)]。相对于权重的目标函数反向传播的方程可以使用链式法则计算，公式如下：
- en: '|  | $\frac{\partial{L}}{\partial{w}}=\frac{\partial{z}}{\partial{w}}\frac{\partial{a}}{\partial{z}}\frac{\partial{L}}{\partial{w}}$
    |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial{L}}{\partial{w}}=\frac{\partial{z}}{\partial{w}}\frac{\partial{a}}{\partial{z}}\frac{\partial{L}}{\partial{w}}$
    |  | (3) |'
- en: 'Gradient descent is used to minimize the objective function through iterative
    updates of the parameters, such as weights, bias, and kernels, in the negative
    direction of the gradient of the objective function [[22](#bib.bib22), [37](#bib.bib37)].
    The respected equation is given by:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降用于通过对参数（如权重、偏置和核）的迭代更新，在目标函数的梯度负方向上最小化目标函数 [[22](#bib.bib22), [37](#bib.bib37)]。相关方程为：
- en: '|  | $w:=w-\alpha\frac{\partial{L}}{\partial{w}}$ |  | (4) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $w:=w-\alpha\frac{\partial{L}}{\partial{w}}$ |  | (4) |'
- en: where w represents the weight, $\alpha$ represents the learning rate, and the
    partial derivative of the objective function with respect to the weight. Figure
    [4](#S2.F4 "Figure 4 ‣ II-E Pooling Layer ‣ II Deep Learning Theory ‣ A survey
    on deep learning approaches for breast cancer diagnosis") illustrates the 3-dimensional
    surface of an objective function, where the darkest point represents the global
    minimum of the objective function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，w 代表权重，$\alpha$ 代表学习率，以及目标函数相对于权重的偏导数。图 [4](#S2.F4 "Figure 4 ‣ II-E Pooling
    Layer ‣ II Deep Learning Theory ‣ A survey on deep learning approaches for breast
    cancer diagnosis") 说明了目标函数的三维表面，其中最暗的点代表目标函数的全局最小值。
- en: '![Refer to caption](img/ed0f502fe3fa6ef1f61bd867eb07adfd.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed0f502fe3fa6ef1f61bd867eb07adfd.png)'
- en: 'Figure 5: A model that has overfit the training data experiences a divergence
    in training and validation loss. Adapted from [[31](#bib.bib31)].'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：过拟合训练数据的模型在训练和验证损失上出现了偏离。改编自 [[31](#bib.bib31)]。
- en: II-G Overfitting
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-G 过拟合
- en: Models that cannot generalize to new data have overfitting the training data
    as shown in Figure [5](#S2.F5 "Figure 5 ‣ II-F Fully-Connected Layer ‣ II Deep
    Learning Theory ‣ A survey on deep learning approaches for breast cancer diagnosis").
    Overfitting can be solved through early stopping, network pruning, increasing
    training data, and regularization [[38](#bib.bib38), [39](#bib.bib39)]. Early
    stopping is stopping the learning at a point where the curves for training and
    validation loss are neither overfitting or underfitting [[38](#bib.bib38), [39](#bib.bib39)].
    Network pruning involves the pruning of redundant weights, while keeping important
    weights [[40](#bib.bib40)]. An increase in training data is required to tune the
    parameters within a network, so the network can generalize to new data better.
    Training data can be increased through collection and/or data augmentation. Regularization
    aims to remove useless features and minimize the weights of less important features
    learned by the model [[38](#bib.bib38)]. However, due to the uncertainty of necessary
    features by the network, a penalty term is added to the objective function to
    minimize the number of features [[38](#bib.bib38)]. Furthermore, there are different
    types of regularization, such as L1 regularization, L2 regularization (weight
    decay), and Dropout [[38](#bib.bib38)]. Dropout was proposed by Srivastava et
    al. [[41](#bib.bib41)] and is another effective strategy in reducing overfitting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 无法对新数据进行泛化的模型已经过拟合了训练数据，如图 [5](#S2.F5 "Figure 5 ‣ II-F Fully-Connected Layer
    ‣ II Deep Learning Theory ‣ A survey on deep learning approaches for breast cancer
    diagnosis") 所示。过拟合可以通过早停、网络剪枝、增加训练数据和正则化来解决[[38](#bib.bib38), [39](#bib.bib39)]。早停是在训练和验证损失曲线既不出现过拟合也不出现欠拟合的点停止学习[[38](#bib.bib38),
    [39](#bib.bib39)]。网络剪枝涉及剪除冗余的权重，同时保留重要的权重[[40](#bib.bib40)]。需要增加训练数据以调整网络中的参数，使网络能够更好地泛化到新数据。通过收集和/或数据增强可以增加训练数据。正则化旨在去除无用特征并最小化模型学习到的较不重要特征的权重[[38](#bib.bib38)]。然而，由于网络所需特征的不确定性，会在目标函数中加入一个惩罚项以最小化特征数量[[38](#bib.bib38)]。此外，还有不同类型的正则化方法，如
    L1 正则化、L2 正则化（权重衰减）和 Dropout [[38](#bib.bib38)]。Dropout 由 Srivastava 等人提出 [[41](#bib.bib41)]，是一种减少过拟合的有效策略。
- en: II-H Transfer Learning
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-H 迁移学习
- en: Transfer learning is a technique used to improve a learner specialized to a
    domain through transferring knowledge from a similar domain [[42](#bib.bib42),
    [43](#bib.bib43)]. Additionally, it can be a solution to the issue of insufficient
    training data [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]. A quantity
    of quality training data suitable for effectively training a learner can be expensive,
    due to difficult collection and curation of data [[43](#bib.bib43)]. Henceforth,
    solution such as homogeneous and heterogeneous transfer learning have been proposed
    [[42](#bib.bib42)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种通过将知识从相似领域转移到一个特定领域来提高学习者专业性的技术[[42](#bib.bib42), [43](#bib.bib43)]。此外，它也可以解决训练数据不足的问题[[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)]。由于数据收集和整理的困难，适合有效训练学习者的高质量训练数据可能非常昂贵[[43](#bib.bib43)]。因此，提出了如同质迁移学习和异质迁移学习等解决方案[[42](#bib.bib42)]。
- en: III Performance Metrics
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 性能指标
- en: A confusion matrix can be used to showcase the different predictions that a
    classifier makes [[45](#bib.bib45), [46](#bib.bib46)]. A confusion matrix can
    be visualized as a 2x2 matrix with True Positive (TP), True Negative (TN), False
    Positive (FP), and False Negative (FN) in one of the grids [[46](#bib.bib46),
    [47](#bib.bib47)]. True positives are positive samples correctly predicted as
    positive, whereas false positives are negative samples incorrectly predicted as
    positive [[45](#bib.bib45), [46](#bib.bib46)]. True negatives are negative samples
    correctly predicted as negative, whereas false negatives are positive samples
    incorrectly predicted as negative [[45](#bib.bib45), [46](#bib.bib46)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以用来展示分类器的不同预测结果 [[45](#bib.bib45), [46](#bib.bib46)]。混淆矩阵可以可视化为一个2x2的矩阵，其中包含真正例（TP）、真负例（TN）、假正例（FP）和假负例（FN）
    [[46](#bib.bib46), [47](#bib.bib47)]。真正例是被正确预测为正类的正样本，而假正例是被错误预测为正类的负样本 [[45](#bib.bib45),
    [46](#bib.bib46)]。真负例是被正确预测为负类的负样本，而假负例是被错误预测为负类的正样本 [[45](#bib.bib45), [46](#bib.bib46)]。
- en: 'The following equations will be used as a means of assessing the deep learning
    models shown in TABLE [I](#S3.T1 "Table I ‣ III Performance Metrics ‣ A survey
    on deep learning approaches for breast cancer diagnosis"), [II](#S4.T2 "Table
    II ‣ IV-C Status of Magnetic Resonance Imaging (MRI) ‣ IV Discussion ‣ A survey
    on deep learning approaches for breast cancer diagnosis"), and [III](#S4.T3 "Table
    III ‣ IV-D Current Approaches using 2D CNN Architecture ‣ IV Discussion ‣ A survey
    on deep learning approaches for breast cancer diagnosis"). Accuracy is a ratio
    between the correctly classified and the total samples [[46](#bib.bib46), [47](#bib.bib47)].
    The equation for accuracy is given by the following [[48](#bib.bib48), equ. (2)]:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式将用于评估表格 [I](#S3.T1 "Table I ‣ III Performance Metrics ‣ A survey on deep
    learning approaches for breast cancer diagnosis")、[II](#S4.T2 "Table II ‣ IV-C
    Status of Magnetic Resonance Imaging (MRI) ‣ IV Discussion ‣ A survey on deep
    learning approaches for breast cancer diagnosis") 和 [III](#S4.T3 "Table III ‣
    IV-D Current Approaches using 2D CNN Architecture ‣ IV Discussion ‣ A survey on
    deep learning approaches for breast cancer diagnosis") 中展示的深度学习模型。准确度是正确分类样本与总样本的比率
    [[46](#bib.bib46), [47](#bib.bib47)]。准确度的公式由 [[48](#bib.bib48), equ. (2)] 给出：
- en: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
- en: 'Sensitivity, also known as recall and true positive rate, is the fraction of
    relevant (actual true positive) instances that are retrieved [[46](#bib.bib46)].
    The equation for sensitivity is given by [[48](#bib.bib48), equ. (5)]:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感度，也称为召回率和真正例率，是检索到的相关（实际真正例）实例的比例 [[46](#bib.bib46)]。敏感度的计算公式由 [[48](#bib.bib48),
    equ. (5)] 给出：
- en: '|  | $Sensitivity=\frac{TP}{TP+FN}$ |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $Sensitivity=\frac{TP}{TP+FN}$ |  | (6) |'
- en: 'Specificity, also known as true negative rate, is the fraction of relevant
    (actual true negative) instance that are retrieved. The equation for specificity
    equation is given by [[48](#bib.bib48), equ. (6)]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性，也称为真负率，是检索到的相关（实际真负例）实例的比例。特异性的公式由 [[48](#bib.bib48), equ. (6)] 给出：
- en: '|  | $Specificity=\frac{\text{TN}}{TN+FP}$ |  | (7) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $Specificity=\frac{\text{TN}}{TN+FP}$ |  | (7) |'
- en: 'Precision is the fraction of retrieved instances that are relevant (actual
    true positive) [[46](#bib.bib46), [47](#bib.bib47)]. The equation for precision
    is given by [[48](#bib.bib48), equ. (4)]:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是检索到的实例中相关（实际真正例）的比例 [[46](#bib.bib46), [47](#bib.bib47)]。精确度的公式由 [[48](#bib.bib48),
    equ. (4)] 给出：
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (8) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (8) |'
- en: 'F score is a weighted ratio measuring the average of precision and recall [[46](#bib.bib46)],
    the equation for F score is given by [[47](#bib.bib47), equ. (4)]:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: F 分数是一个加权比率，用于测量精确度和召回率的平均值 [[46](#bib.bib46)]，F 分数的公式由 [[47](#bib.bib47), equ.
    (4)] 给出：
- en: '|  | $F_{1}=2\>\frac{precision\>\cdot\>recall}{precision+recall}$ |  | (9)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{1}=2\>\frac{precision\>\cdot\>recall}{precision+recall}$ |  | (9)
    |'
- en: 'Dice Similarity Coefficient (DSC) is a spatial overlap index [[49](#bib.bib49)],
    and can be used for measuring the segmentation performance of a model. DSC is
    given by [[50](#bib.bib50), equ. (2)]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Dice相似系数（DSC）是一个空间重叠指标 [[49](#bib.bib49)]，可以用来测量模型的分割性能。DSC 由 [[50](#bib.bib50),
    equ. (2)] 给出：
- en: '|  | $DSC=\frac{2\cdot TP}{2\cdot TP+FP+FN}$ |  | (10) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $DSC=\frac{2\cdot TP}{2\cdot TP+FP+FN}$ |  | (10) |'
- en: 'Matthews Correlation Coefficient (MCC) calculates for the Pearson product-moment
    correlation coefficient between the actual and predicted values [[47](#bib.bib47)].
    MCC is given by [[47](#bib.bib47), equ. (2)]:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Matthews相关系数（MCC）计算实际值与预测值之间的皮尔逊积矩相关系数 [[47](#bib.bib47)]。MCC 由 [[47](#bib.bib47),
    equ. (2)] 给出：
- en: '|  | $MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$
    |  | (11) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$
    |  | (11) |'
- en: 'TABLE I: 2D CNN architectures for breast cancer diagnosis'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 I: 乳腺癌诊断的 2D CNN 架构'
- en: Ref. Model Task Dataset AUROC (%) MCC (%) Dice (%) DM Dhungel et al. (2017)
    [[51](#bib.bib51)] CNN + RF + Hypothesis- Refinement Classification Detection
    Segmentation INbreast $0.76\pm 0.23$ (via RF, min. user int.) $0.69\pm 0.10$ (via
    CNN, min user int.) — $0.85\pm 0.02$ Al-antari et al. (2018) [[52](#bib.bib52)]
    YOLO + FrCN + CNN Classification Detection Segmentation INbreast 0.9478 (via CNN)
    0.9762 (via YOLO) 0.8593 (via FrCN) 0.8991 (via CNN) 0.9269 (via FrCN) Chougrad
    et al. (2018) [[53](#bib.bib53)] Inception v3 Classification Detection DDSM INbreast
    BCDR 0.99 (via MIAS) — — Ribli et al. (2018) [[54](#bib.bib54)] Faster-RCNN Classification
    Detection DDSM Semmelweis University 0.95 (via INbreast) — — Singh et al. (2020)
    [[55](#bib.bib55)] cGAN Classification Segmentation DDSM INbreast Hospital Sant
    Joan de Reus 0.80 — 0.94
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献 模型 任务 数据集 AUROC (%) MCC (%) Dice (%) DM Dhungel et al. (2017) [[51](#bib.bib51)]
    CNN + RF + 假设-细化 分类 检测 分割 INbreast $0.76\pm 0.23$ (通过 RF，最小用户交互) $0.69\pm 0.10$
    (通过 CNN，最小用户交互) — $0.85\pm 0.02$ Al-antari et al. (2018) [[52](#bib.bib52)] YOLO
    + FrCN + CNN 分类 检测 分割 INbreast 0.9478 (通过 CNN) 0.9762 (通过 YOLO) 0.8593 (通过 FrCN)
    0.8991 (通过 CNN) 0.9269 (通过 FrCN) Chougrad et al. (2018) [[53](#bib.bib53)] Inception
    v3 分类 检测 DDSM INbreast BCDR 0.99 (通过 MIAS) — — Ribli et al. (2018) [[54](#bib.bib54)]
    Faster-RCNN 分类 检测 DDSM Semmelweis University 0.95 (通过 INbreast) — — Singh et al.
    (2020) [[55](#bib.bib55)] cGAN 分类 分割 DDSM INbreast Hospital Sant Joan de Reus
    0.80 — 0.94
- en: IV Discussion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 讨论
- en: This section looks at the different deep learning architectures designed for
    classification, detection, and segmentation of breast tumours, all are shown in
    TABLE [I](#S3.T1 "Table I ‣ III Performance Metrics ‣ A survey on deep learning
    approaches for breast cancer diagnosis"), [II](#S4.T2 "Table II ‣ IV-C Status
    of Magnetic Resonance Imaging (MRI) ‣ IV Discussion ‣ A survey on deep learning
    approaches for breast cancer diagnosis"), and [III](#S4.T3 "Table III ‣ IV-D Current
    Approaches using 2D CNN Architecture ‣ IV Discussion ‣ A survey on deep learning
    approaches for breast cancer diagnosis").
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了用于乳腺肿瘤分类、检测和分割的不同深度学习架构，所有这些都展示在表格 [I](#S3.T1 "Table I ‣ III Performance
    Metrics ‣ A survey on deep learning approaches for breast cancer diagnosis")、[II](#S4.T2
    "Table II ‣ IV-C Status of Magnetic Resonance Imaging (MRI) ‣ IV Discussion ‣
    A survey on deep learning approaches for breast cancer diagnosis") 和 [III](#S4.T3
    "Table III ‣ IV-D Current Approaches using 2D CNN Architecture ‣ IV Discussion
    ‣ A survey on deep learning approaches for breast cancer diagnosis") 中。
- en: IV-A Status of Digital Breast Tomosynthesis (DBT)
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数字乳腺断层扫描（DBT）的现状
- en: DBT has been gaining popularity over digital mammography for higher image quality,
    richer structural detail, and better reduction in background signal noise [[56](#bib.bib56)].
    In addition, recent studies [[57](#bib.bib57), [6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] have shown increased cancer detection rate
    with DBT in women aged 40 to 79 years with dense and non-dense breast, but results
    on reduced recall rate remains conflicted. Moreover, [[6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] have shown that DBT reduces recall rates for
    women aged 40 to 79 years with dense and non-dense breast, but [[61](#bib.bib61),
    [62](#bib.bib62)] shown recall rates of DBT plus DM similar to that of DM alone.
    DBT reduces the overlapping breast tissue that appear on 2D images as opposed
    to mammograms [[63](#bib.bib63), [6](#bib.bib6), [3](#bib.bib3)]. As a result,
    DBT images help with the detection of tumours that may appear overlapped by other
    healthy breast tissue. However, a disadvantage of DBT is being less sensitive
    to imaging malignant calcification and even groups of micro-calcification compared
    to DM [[64](#bib.bib64), [56](#bib.bib56)]. In addition, DBT systems that use
    pixel binning have increased efficiency in detector readings, but in turn reduced
    3D spatial resolution [[7](#bib.bib7)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与数字乳腺摄影相比，DBT 因其更高的图像质量、更丰富的结构细节以及更好的背景信号噪声减少而逐渐受到欢迎 [[56](#bib.bib56)]。此外，近期研究
    [[57](#bib.bib57), [6](#bib.bib6), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]
    表明，DBT 在40至79岁有密集和非密集乳房的女性中的癌症检测率有所提高，但对减少召回率的结果仍存在争议。此外，[[6](#bib.bib6), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60)] 还显示，DBT 减少了40至79岁有密集和非密集乳房女性的召回率，但 [[61](#bib.bib61),
    [62](#bib.bib62)] 表明 DBT 加 DM 的召回率与 DM 单独使用的召回率相似。DBT 减少了在 2D 图像上出现的重叠乳腺组织，而不是乳腺
    X 光检查 [[63](#bib.bib63), [6](#bib.bib6), [3](#bib.bib3)]。因此，DBT 图像有助于检测可能被其他健康乳腺组织重叠的肿瘤。然而，DBT
    的一个缺点是对恶性钙化和微钙化群体的成像敏感性低于 DM [[64](#bib.bib64), [56](#bib.bib56)]。此外，使用像素汇聚的 DBT
    系统提高了探测器读数的效率，但反过来降低了三维空间分辨率 [[7](#bib.bib7)]。
- en: IV-B Status of Automatic Breast UltraSound (ABUS)
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 自动乳腺超声（ABUS）的状态
- en: The ABUS consists of an ultrasound scanner and a transducer [[65](#bib.bib65),
    [66](#bib.bib66)]. The ABUS captures axial slices of the breast in different views,
    then these axial slices are used for 3D reconstruction of sagittal and coronal
    images [[65](#bib.bib65), [66](#bib.bib66)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ABUS 包括一个超声扫描仪和一个传感器 [[65](#bib.bib65), [66](#bib.bib66)]。ABUS 捕捉乳房的不同视角的轴向切片，然后这些轴向切片用于矢状面和冠状面图像的三维重建
    [[65](#bib.bib65), [66](#bib.bib66)]。
- en: IV-C Status of Magnetic Resonance Imaging (MRI)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 磁共振成像（MRI）的状态
- en: Magnetic resonance imaging (MRI) screenings are recommended to patients with
    a high risk of breast cancer, due to genetics or family history [[67](#bib.bib67),
    [2](#bib.bib2)]. Breast coils are used with an MRI to acquire the image of the
    breast; the patient lies prone with the breasts in the breast coils before entering
    the MRI [[68](#bib.bib68)]. Breast MRI has different types, such as T1-weighted
    contrast-enhanced imaging, T2-weighted, ultrafast, and diffusion-weighted imaging
    [[68](#bib.bib68)]. The dimensionality of an acquired image is dependent on the
    MRI type [[69](#bib.bib69)]. In 2D image acquisition, multiple 2D image slices
    of the object are captured, whereas in 3D image acquisition, a true 3D image can
    be captured [[69](#bib.bib69), [70](#bib.bib70)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 磁共振成像（MRI）筛查推荐给那些由于遗传或家族史而有乳腺癌高风险的患者 [[67](#bib.bib67), [2](#bib.bib2)]。乳腺线圈与
    MRI 一起使用，以获取乳房图像；患者俯卧在乳腺线圈中，然后进入 MRI [[68](#bib.bib68)]。乳腺 MRI 有不同类型，如 T1 加权对比增强成像、T2
    加权成像、超快速成像和扩散加权成像 [[68](#bib.bib68)]。获取的图像的维度取决于 MRI 类型 [[69](#bib.bib69)]。在 2D
    图像获取中，捕获对象的多个 2D 图像切片，而在 3D 图像获取中，可以捕获真实的 3D 图像 [[69](#bib.bib69), [70](#bib.bib70)]。
- en: 'TABLE II: 2.5D CNN architectures for breast cancer diagnosis'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：用于乳腺癌诊断的 2.5D CNN 架构
- en: 'Ref. Model Task Dataset AUROC (%) Dice (%) DM-DBT Yousefi et al. (2018) [[71](#bib.bib71)]
    DCNN MI-RF- based CAD Classification Breast Imaging Research Laboratory at Massachusetts
    General Hospital (in-house): 87 DBT images (27 malignant, 60 benign) 0.87 — Kim
    et al. (2017) [[72](#bib.bib72)] VGG16 + LSTM Classification (in-house) 0.919
    — Liu et al. (2017) [[73](#bib.bib73)] 3D Anisotropic Hybrid Network (3D AH-Net)
    Segmentation (in-house): 2809 DBT volumes — 0.834 Zhang et al. (2020) [[74](#bib.bib74)]
    AlexNet (Late fusion + Max Pooling) Classification (in-house): 3018 negatives
    272 malignant 415 benign 0.854 — Liang et al. (2020) [[75](#bib.bib75)] CNN ensemble
    Classification University of Kentucky Medical Center (in-house): DBT and DM (709
    malignant, 415 benign) 0.97 —'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献 模型 任务 数据集 AUROC (%) Dice (%) DM-DBT Yousefi 等人（2018） [[71](#bib.bib71)]
    DCNN MI-RF 基于 CAD 分类 麻省总医院（内部）：87 张 DBT 图像（27 张恶性，60 张良性） 0.87 — Kim 等人（2017）
    [[72](#bib.bib72)] VGG16 + LSTM 分类（内部） 0.919 — Liu 等人（2017） [[73](#bib.bib73)]
    3D 各向异性混合网络（3D AH-Net） 分割（内部）：2809 个 DBT 卷 — 0.834 Zhang 等人（2020） [[74](#bib.bib74)]
    AlexNet（后融合 + 最大池化） 分类（内部）：3018 个负样本 272 个恶性 415 个良性 0.854 — Liang 等人（2020） [[75](#bib.bib75)]
    CNN 集成 分类 肯塔基大学医学中心（内部）：DBT 和 DM（709 个恶性，415 个良性） 0.97 —
- en: IV-D Current Approaches using 2D CNN Architecture
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 目前使用的 2D CNN 架构
- en: For 2D classification, Chougrad et al. [[53](#bib.bib53)] adopted state-of-the-art
    architectures, including ResNet50 [[76](#bib.bib76)], VGG16 [[77](#bib.bib77)],
    and Inception v3 [[78](#bib.bib78)], that were pre-trained on ImageNet, and re-purposed
    for breast cancer screening. Chougrad et al. [[53](#bib.bib53)] achieved a 0.99
    AUC for classification on the MIAS database using a pre-trained and fine-tuned
    Inception v3 model. The study concluded that fine-tuning strategy improves classification
    accuracy on state-of-the-art architecture, and Inception v3 achieved a higher
    accuracy than VGG16 and ResNet50.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 2D 分类，Chougrad 等人 [[53](#bib.bib53)] 采用了包括 ResNet50 [[76](#bib.bib76)]、VGG16
    [[77](#bib.bib77)] 和 Inception v3 [[78](#bib.bib78)] 在内的最先进架构，这些架构在 ImageNet 上进行了预训练，并重新用于乳腺癌筛查。Chougrad
    等人 [[53](#bib.bib53)] 使用预训练并微调的 Inception v3 模型在 MIAS 数据库上达到了 0.99 的分类 AUC。研究结论表明，微调策略提高了最先进架构的分类准确率，而
    Inception v3 的准确率高于 VGG16 和 ResNet50。
- en: For 2D segmentation, Singh et al. [[55](#bib.bib55)] adapted upon a study by
    Isola et al. [[79](#bib.bib79)] to propose a conditional Generative Adversarial
    Network (cGAN) CAD framework for classification and segmentation breast tumor.
    Singh et al. [[55](#bib.bib55)] had achieved 92.11% dice coefficient score and
    84.55% IoU for a tight frame of the tumor Region Of Interest (ROI) on cGAN. Furthermore,
    the study tested Single Shot Detector (SSD), You Only Look Once (YOLO), and Faster-RCNN
    and found that SSD achieved the best results on detecting small tumor regions
    and achieved an overall accuracy of 97%. Major contributions proposed in this
    study include the first adapted cGan for breast tumor segmentation, a multi-class
    CNN for predicting four breast tumor shapes, and the proposed model outperforming
    state-of-the-art architecture.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 2D 分割，Singh 等人 [[55](#bib.bib55)] 在 Isola 等人 [[79](#bib.bib79)] 的研究基础上，提出了一种条件生成对抗网络（cGAN）CAD
    框架，用于乳腺肿瘤的分类和分割。Singh 等人 [[55](#bib.bib55)] 在 cGAN 上达到了 92.11% 的 Dice 系数分数和 84.55%
    的 IoU，用于肿瘤感兴趣区域（ROI）的紧密框架。此外，研究测试了 Single Shot Detector (SSD)、You Only Look Once
    (YOLO) 和 Faster-RCNN，并发现 SSD 在检测小肿瘤区域时表现最佳，总体准确率达到了 97%。该研究的主要贡献包括首次适应 cGAN 用于乳腺肿瘤分割，提出了一种多类别
    CNN 用于预测四种乳腺肿瘤形状，并且提出的模型优于最先进的架构。
- en: 'TABLE III: 3D CNN architectures for breast cancer diagnosis'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：乳腺癌诊断的 3D CNN 架构
- en: 'Ref. Model Dataset Acc(%) AUROC(%) AP(%) FNR(%) Dice(%) Fscore(%) DBT Zhang
    et al. (2018)[[80](#bib.bib80)] 3D-T2-Alex University of Kentucky (in-house) —
    0.6632 — — — — Fan et al. (2020)[[81](#bib.bib81)] 3D Mask-RCNN Fudan University
    Affiliated Cancer Center (in-house): 364 DBT samples (289 malignant, 75 benign)
    — 0.934 0.053 — — — Wichakam et al. (2018)[[82](#bib.bib82)] 3D ConvNet (in-house):
    115 DBT volumes (91 malignant, 24 normal) 0.72 — — — — 0.842 ABUS Lei et al. (2021)[[83](#bib.bib83)]
    Mask scoring RCNN (private) — — — $0.85\pm 0.104$ — — Zhou et al. (2021)[[84](#bib.bib84)]
    $\mathrm{C_{MS}}\mathrm{VNet_{Iter}}$ Peking University People’s Hospital (in-house):
    900 ABUS volumes — 0.787 — 0.392 $0.778\pm 0.145$ 0.811 MRI Zhou et al. (2019)[[85](#bib.bib85)]
    3D DenseNet (in-house): 720 malignant, 353 benign — 0.859 — — $0.501\pm 0.274$
    — Hu et al. (2020)[[86](#bib.bib86)] VGG19Net (in-house): 728 malignant, 199 benign
    — DEC: 0.85 T2w: 0.78 ImageFusion: 0.85 FeatureFusion: 0.87 ClassifierFusion:
    0.86 — — — —'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '参考模型 数据集 准确率（%） AUROC（%） AP（%） FNR（%） Dice（%） Fscore（%） DBT Zhang et al. (2018)[[80](#bib.bib80)]
    3D-T2-Alex 肯塔基大学（内部） — 0.6632 — — — — Fan et al. (2020)[[81](#bib.bib81)] 3D Mask-RCNN
    复旦大学附属肿瘤中心（内部）：364 DBT 样本（289 例恶性，75 例良性） — 0.934 0.053 — — — Wichakam et al.
    (2018)[[82](#bib.bib82)] 3D ConvNet（内部）：115 DBT 体积（91 例恶性，24 例正常） 0.72 — — — —
    0.842 ABUS Lei et al. (2021)[[83](#bib.bib83)] Mask scoring RCNN（私人） — — — $0.85\pm
    0.104$ — — Zhou et al. (2021)[[84](#bib.bib84)] $\mathrm{C_{MS}}\mathrm{VNet_{Iter}}$
    北京大学人民医院（内部）：900 ABUS 体积 — 0.787 — 0.392 $0.778\pm 0.145$ 0.811 MRI Zhou et al.
    (2019)[[85](#bib.bib85)] 3D DenseNet（内部）：720 例恶性，353 例良性 — 0.859 — — $0.501\pm
    0.274$ — Hu et al. (2020)[[86](#bib.bib86)] VGG19Net（内部）：728 例恶性，199 例良性 — DEC:
    0.85 T2w: 0.78 ImageFusion: 0.85 FeatureFusion: 0.87 ClassifierFusion: 0.86 —
    — — —'
- en: IV-E Overview on 2.5D CNN
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 2.5D CNN 概述
- en: The utilization of the depth dimension and collection of images within a DBT
    volume is needed to utilize the entirety of the DBT information [[72](#bib.bib72),
    [74](#bib.bib74), [87](#bib.bib87)]. Furthermore, 2D CNN cannot preserve the between-slice
    information in DBT volumes [[88](#bib.bib88), [89](#bib.bib89), [44](#bib.bib44),
    [73](#bib.bib73)]. Moreover, the high complexity, potential overfitting, and small
    DBT dataset relative to ImageNet [[72](#bib.bib72), [74](#bib.bib74), [90](#bib.bib90)]
    can make training a 3D CNN rather infeasible, which makes approaches by [[72](#bib.bib72),
    [73](#bib.bib73), [75](#bib.bib75)] more favourable. In the following section,
    alternative methods to 2D and 3D CNN approaches for utilizing the entirety of
    information within DBT images will be discussed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 DBT 信息的全部内容，需要利用深度维度和 DBT 体积内的图像集合 [[72](#bib.bib72), [74](#bib.bib74),
    [87](#bib.bib87)]。此外，2D CNN 无法保留 DBT 体积中的切片间信息 [[88](#bib.bib88), [89](#bib.bib89),
    [44](#bib.bib44), [73](#bib.bib73)]。而且，相对于 ImageNet [[72](#bib.bib72), [74](#bib.bib74),
    [90](#bib.bib90)]，高复杂度、潜在的过拟合以及较小的 DBT 数据集可能使得训练 3D CNN 相当不可行，这使得 [[72](#bib.bib72),
    [73](#bib.bib73), [75](#bib.bib75)] 的方法更具优势。在接下来的部分，将讨论利用 DBT 图像内全部信息的 2D 和 3D
    CNN 方法的替代方案。
- en: IV-F Current Approaches using 2.5D CNN Architecture
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 当前使用2.5D CNN架构的方法
- en: Kim et al. [[72](#bib.bib72)] proposed a CNN for spatial feature representation
    and depth directional long-term recurrent learning for depth feature representation.
    A VGG16 network was used as the CNN, while LSTMs are used for depth directional
    long-term recurrent learning. The model achieved an AUROC of 91.9%. However, a
    LSTM network can be difficult [[73](#bib.bib73)] and expensive [[90](#bib.bib90)]
    to train.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Kim et al. [[72](#bib.bib72)] 提出了用于空间特征表示和深度方向长期递归学习的 CNN。VGG16 网络被用作 CNN，而
    LSTM 被用于深度方向长期递归学习。该模型达到了 91.9% 的 AUROC。然而，LSTM 网络可能很难 [[73](#bib.bib73)] 并且训练成本高
    [[90](#bib.bib90)]。
- en: Hence, a different approach for learning 3D DBT volumes was proposed by Liu
    et al. [[73](#bib.bib73)]. They proposed the 3D Anisotropic Hybrid Network (3D
    AH-Net). The 3D AH-Net achieved a global dice score of 83.4%. As mentioned by
    Liu et al., challenges with directly training a 3D CNN with DBT or CT scans include
    (1) anisotropic voxels, (2) the higher number of features needed compared to 2D
    CNN, and (3) the lack of pre-trained 3D CNN models and limited training data.
    Anisotropic voxels have uneven distribution of data that hinder the training of
    3D CNNs, such as CT and DBT volumes having within-slice resolution greater than
    between-slice resolution. This challenge of anisotropic voxels in DBT images was
    treated using anisotropic convolutions. The 3D AH-Net has a feature encoder and
    decoder. The encoder extracts deep representations from the 2D image slices. On
    the other hand, the decoder, a densely connected network of anisotropic convolutions,
    utilizes the 3D context, while keeping the between-slices consistency. The AH-ResNet,
    a 2D ResNet50 [[76](#bib.bib76)] converted into a 3D ResNet50, was used as the
    backbone and encoder of the 3D AH-Net. A 2D Multi-Channel Global Convolutional
    Network (MC-GCN) was used to train the encoder parameters used in the 3D AH-NET.
    The parameters trained on the MC-GCN were extracted and transferred into the AH-Resnet.
    The 3D AH-Net is structured in the order of AH-ResNet, decoder, then pyramid volumetric
    pooling.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，刘等人提出了另一种学习3D DBT体积的方法。他们提出了3D各向异性混合网络（3D AH-Net）。3D AH-Net实现了83.4%的全局骰子得分。正如刘等人所提到的，直接用DBT或CT扫描训练3D
    CNN面临的挑战包括（1）各向异性体素，（2）相比于2D CNN所需更多的特征，以及（3）缺乏预训练的3D CNN模型和有限的训练数据。各向异性体素数据分布不均，这会阻碍3D
    CNN的训练，例如CT和DBT体积在切片内的分辨率大于切片之间的分辨率。DBT图像中的各向异性体素挑战通过各向异性卷积得到处理。3D AH-Net具有特征编码器和解码器。编码器从2D图像切片中提取深层表示。而解码器，作为一个密集连接的各向异性卷积网络，利用3D上下文，同时保持切片间的一致性。AH-ResNet，一个将2D
    ResNet50转换为3D ResNet50的网络，被用作3D AH-Net的骨干网和编码器。一个2D多通道全局卷积网络（MC-GCN）用于训练3D AH-NET中使用的编码器参数。训练好的MC-GCN参数被提取并转移到AH-ResNet中。3D
    AH-Net的结构顺序为AH-ResNet、解码器，然后是金字塔体积池化。
- en: A key challenge mentioned by Liang et al. [[74](#bib.bib74)] is the effective
    utilization of DBT data, considering as DBT data are high in resolution and vary
    in depth. The training of a 3D CNN model with DBT data is computationally costly
    and memory intensive. As a result, Liang et al. proposed a network containing
    two types of CNNs, a CNN feature extractor and CNN classifier. The model achieved
    an AUROC of 97%. The network sequence starts with a 2D CNN feature extractor transitioning
    into an ensemble of three CNN classifiers. A classifier for DM, DBT, and DM-DBT
    feature map classification, where the DM-DBT feature map is a concatenation of
    DM and DBT feature maps.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 梁等人提到的一个关键挑战是**有效利用DBT数据**，因为DBT数据具有高分辨率且深度各异。用DBT数据训练3D CNN模型在计算和内存上都非常耗费资源。因此，梁等人提出了一种包含两种CNN的网络：CNN特征提取器和CNN分类器。该模型实现了97%的AUROC。网络序列从2D
    CNN特征提取器开始，过渡到由三种CNN分类器组成的集成体。用于DM、DBT和DM-DBT特征图分类的分类器，其中DM-DBT特征图是DM和DBT特征图的拼接。
- en: IV-G Overview on 3D CNN
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G 3D CNN概述
- en: A 2D CNN cannot preserve the between-slice information in DBT volumes as opposed
    to a 3D CNN [[88](#bib.bib88), [89](#bib.bib89), [44](#bib.bib44), [73](#bib.bib73)].
    A 3D CNN can learn the spatial information within a 2D image along with between-slice
    information of multiple slices. A 3D convolutional kernel enables this characteristic,
    but also generates a higher number of training parameters compared to a 2D convolutional
    kernel. As a result, the complexity of features that each kernels can extract
    increases. A challenge arises when isotropic 3D convolutional kernels are used
    to learn anisotropic DBT volumes, due to variation of resolution within each anisotropic
    voxel along each plane [[73](#bib.bib73), [88](#bib.bib88)]. Moreover, the quality
    of spatial resolution of DBT images can impact the training of 2D and 3D CNNs
    [[7](#bib.bib7), [88](#bib.bib88), [73](#bib.bib73)]. Another challenge with training
    3D CNN is the limited well-curated and publicly available DBT dataset. Buda et
    al. [[91](#bib.bib91)] addressed this issue by curating and releasing a publicly
    available dataset with 22,032 reconstructed DBT volumes from 5,060 patients.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与 2D CNN 相比，2D CNN 无法保留 DBT 体积中的切片之间信息 [[88](#bib.bib88), [89](#bib.bib89),
    [44](#bib.bib44), [73](#bib.bib73)]。3D CNN 能够学习 2D 图像中的空间信息以及多个切片之间的信息。3D 卷积核使这一特性成为可能，但也产生了比
    2D 卷积核更多的训练参数。因此，每个卷积核能够提取的特征复杂性增加。当使用各向同性的 3D 卷积核学习各向异性 DBT 体积时，由于每个各向异性体素沿每个平面的分辨率变化，会出现挑战
    [[73](#bib.bib73), [88](#bib.bib88)]。此外，DBT 图像的空间分辨率质量可能会影响 2D 和 3D CNN 的训练 [[7](#bib.bib7),
    [88](#bib.bib88), [73](#bib.bib73)]。训练 3D CNN 的另一个挑战是缺乏良好策划的公开 DBT 数据集。Buda 等人
    [[91](#bib.bib91)] 通过策划和发布一个包含 22,032 个重建 DBT 体积的公开数据集来解决了这个问题。
- en: IV-H Current Approach using 3D CNN Architecture
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-H 当前使用 3D CNN 架构的方法
- en: Fan et al. [[81](#bib.bib81)] showcases the superiority of 3D over 2D deep learning
    methods, such as Yousefi et al. [[71](#bib.bib71)], in learning to classify, detect,
    and segment tumours in DBT image slices. Fan et al. [[81](#bib.bib81)] proposed
    a 3D-Mask-RCNN with a ResNet50 backbone. The proposed 3D-Mask-RCNN achieved a
    sensitivity of 90% at 0.83 FPs/breast for breast-based mass detection, and an
    AP of 93.4% and FNR of 5.3% for lesion segmentation. The study concluded the proposed
    model, 3D-Mask-RCNN, outperforms the 2D counterparts, Mask-RCNN and Faster-RCNN.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Fan 等人 [[81](#bib.bib81)] 展示了 3D 深度学习方法相对于 2D 方法的优越性，例如 Yousefi 等人 [[71](#bib.bib71)]，在学习对
    DBT 图像切片中的肿瘤进行分类、检测和分割方面。Fan 等人 [[81](#bib.bib81)] 提出了一个以 ResNet50 为骨干网的 3D-Mask-RCNN。该
    3D-Mask-RCNN 在乳腺基质检测中实现了 90% 的灵敏度，在 0.83 FPs/乳腺的条件下，并且在病变分割中达到了 93.4% 的 AP 和 5.3%
    的 FNR。研究结论表明，所提出的模型 3D-Mask-RCNN 优于 2D 对应模型 Mask-RCNN 和 Faster-RCNN。
- en: V Challenges
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 挑战
- en: This section looks at the challenges faced in deep learning for breast tumour
    diagnosis.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了在乳腺肿瘤诊断中的深度学习所面临的挑战。
- en: V-A Small Dataset
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 小数据集
- en: Small datasets pose a challenge to the training of deep learning models [[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)], due to the need to familiarize the model
    with all possible cases to minimize classification errors. In addition, the lack
    of standardized datasets makes comparing and reproducing studies difficult [[94](#bib.bib94)].
    A training dataset for deep learning models should provide normal mammograms,
    mammograms with a variety of BI-RADS 1-6, mass types, calcification, asymmetries,
    architectural distortion cases, and several cases in one mammogram [[95](#bib.bib95)].
    Although techniques such as data augmentation, batch normalization, and transfer
    learning have been used to situate limited dataset sizes, large and well-curated
    datasets are still a high necessity for a well-trained model [[21](#bib.bib21),
    [96](#bib.bib96), [97](#bib.bib97)]. Yousefi et al. [[71](#bib.bib71)] used data
    augmentation techniques to increase the sample size from 5,040 to 40,320 2D slices.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 小数据集对深度学习模型的训练构成挑战 [[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)]，因为需要使模型熟悉所有可能的情况，以最小化分类错误。此外，缺乏标准化数据集使得比较和重现研究变得困难
    [[94](#bib.bib94)]。深度学习模型的训练数据集应提供正常乳腺X光片、各种 BI-RADS 1-6 的乳腺X光片、肿块类型、钙化、不对称、结构畸变案例以及一张乳腺X光片中的多个案例
    [[95](#bib.bib95)]。虽然已使用数据增强、批量归一化和迁移学习等技术来应对有限数据集大小的问题，但大型且精心策划的数据集仍然是训练良好模型的高度需求
    [[21](#bib.bib21), [96](#bib.bib96), [97](#bib.bib97)]。Yousefi 等人 [[71](#bib.bib71)]
    使用数据增强技术将样本数量从 5,040 增加到 40,320 2D 切片。
- en: V-B Class Imbalance
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 类别不平衡
- en: Class imbalance is another challenge for deep learning, and occurs when classes
    have different ratios of training data [[98](#bib.bib98), [48](#bib.bib48), [99](#bib.bib99),
    [100](#bib.bib100), [46](#bib.bib46)]. Class imbalance can cause biases in the
    classifier, resulting in predictions skewed towards the positive or negative class
    depending on the data size ratio between classes [[96](#bib.bib96)]. In addition,
    metrics used for measuring model performance, such as accuracy [[47](#bib.bib47)],
    are susceptible to class imbalance and can affect the performance of the model.
    However, there are techniques to deal with class imbalance on both the data level
    and classifier level. Techniques for the data level are random undersampling and
    random oversampling [[101](#bib.bib101), [48](#bib.bib48), [98](#bib.bib98)],
    while the classifier level are cost-sensitive learning and thresholding [[48](#bib.bib48),
    [98](#bib.bib98)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡是深度学习中的另一个挑战，它发生在类别的训练数据比例不同的情况下[[98](#bib.bib98), [48](#bib.bib48), [99](#bib.bib99),
    [100](#bib.bib100), [46](#bib.bib46)]。类别不平衡可能导致分类器产生偏差，从而使预测偏向于正类或负类，这取决于类别之间的数据大小比例[[96](#bib.bib96)]。此外，用于衡量模型性能的指标，如准确率[[47](#bib.bib47)]，也容易受到类别不平衡的影响，从而影响模型的表现。然而，有技术可以在数据层面和分类器层面处理类别不平衡。数据层面的技术包括随机欠采样和随机过采样[[101](#bib.bib101),
    [48](#bib.bib48), [98](#bib.bib98)]，而分类器层面的技术包括成本敏感学习和阈值调整[[48](#bib.bib48), [98](#bib.bib98)]。
- en: V-C Computational Cost and Memory Constraint
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 计算成本和内存约束
- en: Memory constraint is an issue when dealing with training data with a large feature
    space, such as high resolution or high dimensional images [[96](#bib.bib96), [102](#bib.bib102),
    [103](#bib.bib103), [74](#bib.bib74)]. For example, when training a 3D CNN from
    scratch with DBT data with large feature spaces, computational and memory cost
    dramatically increase.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 内存约束在处理具有大特征空间的训练数据时是一个问题，例如高分辨率或高维图像[[96](#bib.bib96), [102](#bib.bib102),
    [103](#bib.bib103), [74](#bib.bib74)]。例如，当从头开始训练一个具有大特征空间的DBT数据的3D CNN时，计算和内存成本会显著增加。
- en: V-D Image Quality Variability
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 图像质量变异性
- en: The image quality depends on the system settings and manufacturer specification
    of the medical screening device, while the performance of models depends on the
    image quality [[56](#bib.bib56)]. Images from breast cancer screenings with poor
    resolution, sharpness, contrast, focus, or high noise can hinder the model during
    training, predictions, or localization of lesions [[17](#bib.bib17)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图像质量依赖于医疗筛查设备的系统设置和制造商规格，而模型的性能则依赖于图像质量[[56](#bib.bib56)]。乳腺癌筛查中图像的分辨率、清晰度、对比度、焦点或噪声过高会在训练、预测或病灶定位过程中对模型造成阻碍[[17](#bib.bib17)]。
- en: VI Future Perspective
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 未来展望
- en: Neural network models discussed in this paper have achieved promising results
    in classification, detection, and segmentation tasks for breast tumours. However,
    further exploration of different architectures should be made to expand possible
    solutions to common issues, such as costly computations and memory usage, redundancies
    in learning 3D data, and robustness. In addition, there is a need for architectures
    to evaluate DBT, MRI, and ABUS images with higher confidence, efficiency, and
    speed. Furthermore, state-of-the-art models require a high level of robustness
    and confidence to display the required level of integrity to act as a "second
    opinion" for radiologist.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论的神经网络模型在乳腺肿瘤的分类、检测和分割任务中取得了有希望的结果。然而，应进一步探索不同的架构，以扩展可能的解决方案，解决常见问题，如计算成本和内存使用、3D数据学习中的冗余以及鲁棒性。此外，需要开发能够更高效、更快速、更自信地评估DBT、MRI和ABUS图像的架构。此外，最先进的模型需要具备高度的鲁棒性和自信，以显示出所需的完整性，作为放射科医生的“第二意见”。
- en: VII Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: Deep learning has shown significant growth in supervised-learning, and continues
    to grow towards better facilitating radiologists in workflow and decision-making.
    This paper provided an overview on deep learning theory, the effectiveness of
    different deep learning architectures for breast cancer screening, and challenges
    faced by deep learning. Moreover, this paper also aimed to establish a clear understanding
    of current progress in deep learning for breast tumour diagnosis, so future directions
    are easily discernible.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在监督学习领域展示了显著的增长，并继续朝着更好地支持放射科医生在工作流程和决策中的方向发展。本文概述了深度学习理论、不同深度学习架构在乳腺癌筛查中的有效性，以及深度学习面临的挑战。此外，本文还旨在建立对乳腺肿瘤诊断中深度学习当前进展的清晰理解，以便未来方向能够清晰可见。
- en: References
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Sung, J. Ferlay, R. L. Siegel, M. Laversanne, I. Soerjomataram, A. Jemal,
    and F. Bray, “Global cancer statistics 2020: Globocan estimates of incidence and
    mortality worldwide for 36 cancers in 185 countries,” *CA: a cancer journal for
    clinicians*, vol. 71, no. 3, p. 212, 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Sung, J. Ferlay, R. L. Siegel, M. Laversanne, I. Soerjomataram, A. Jemal,
    和 F. Bray，《2020年全球癌症统计数据：Globocan对185个国家36种癌症发病率和死亡率的估计》，*CA：临床癌症杂志*，第71卷，第3期，第212页，2021年。'
- en: '[2] A. C. Society, “Breast cancer facts & figures 2019–2020,” *Am. Cancer Soc*,
    pp. 1–44, 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. C. Society，《2019–2020年乳腺癌事实与数据》，*美国癌症学会*，第1–44页，2019年。'
- en: '[3] C. P. A. Cancer, “Breast cancer screening in canada; monitoring & evaluation
    of quality indicators–results report, january 2011 to december 2012,” 2017.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. P. A. Cancer，《加拿大乳腺癌筛查；质量指标的监测与评估——结果报告，2011年1月到2012年12月》，2017年。'
- en: '[4] M. Løberg, M. L. Lousdal, M. Bretthauer, and M. Kalager, “Benefits and
    harms of mammography screening,” *Breast Cancer Research*, vol. 17, no. 1, pp.
    1–12, 2015.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Løberg, M. L. Lousdal, M. Bretthauer, 和 M. Kalager，《乳腺X光检查的益处与风险》，*乳腺癌研究*，第17卷，第1期，第1–12页，2015年。'
- en: '[5] A. Chong, S. P. Weinstein, E. S. McDonald, and E. F. Conant, “Digital breast
    tomosynthesis: concepts and clinical practice,” *Radiology*, vol. 292, no. 1,
    pp. 1–14, 2019.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Chong, S. P. Weinstein, E. S. McDonald, 和 E. F. Conant，《数字乳腺断层合成：概念与临床实践》，*放射学*，第292卷，第1期，第1–14页，2019年。'
- en: '[6] M. L. Marinovich, K. E. Hunter, P. Macaskill, and N. Houssami, “Breast
    cancer screening using tomosynthesis or mammography: a meta-analysis of cancer
    detection and recall,” *JNCI: Journal of the National Cancer Institute*, vol.
    110, no. 9, pp. 942–949, 2018.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. L. Marinovich, K. E. Hunter, P. Macaskill, 和 N. Houssami，《使用断层合成或乳腺X光检查进行乳腺癌筛查：癌症检测和回召的荟萃分析》，*国家癌症研究所杂志*，第110卷，第9期，第942–949页，2018年。'
- en: '[7] D. B. Kopans, “Digital breast tomosynthesis from concept to clinical care,”
    *American Journal of Roentgenology*, vol. 202, no. 2, pp. 299–308, 2014.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. B. Kopans，《数字乳腺断层合成从概念到临床应用》，*美国放射学杂志*，第202卷，第2期，第299–308页，2014年。'
- en: '[8] M. L. Giger, M. F. Inciardi, A. Edwards, J. Papaioannou, K. Drukker, Y. Jiang,
    R. Brem, and J. B. Brown, “Automated breast ultrasound in breast cancer screening
    of women with dense breasts: reader study of mammography-negative and mammography-positive
    cancers,” *American Journal of roentgenology*, vol. 206, no. 6, pp. 1341–1350,
    2016.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. L. Giger, M. F. Inciardi, A. Edwards, J. Papaioannou, K. Drukker, Y.
    Jiang, R. Brem, 和 J. B. Brown，《自动化乳腺超声在密集乳腺女性乳腺癌筛查中的应用：对乳腺X光检查阴性和阳性癌症的读者研究》，*美国放射学杂志*，第206卷，第6期，第1341–1350页，2016年。'
- en: '[9] K. Doi, “Computer-aided diagnosis in medical imaging: historical review,
    current status and future potential,” *Computerized medical imaging and graphics*,
    vol. 31, no. 4-5, pp. 198–211, 2007.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] K. Doi，《医学影像中的计算机辅助诊断：历史回顾、现状与未来潜力》，*计算机医学影像与图形*，第31卷，第4-5期，第198–211页，2007年。'
- en: '[10] M. L. Giger, N. Karssemeijer, and J. A. Schnabel, “Breast image analysis
    for risk assessment, detection, diagnosis, and treatment of cancer,” *Annual review
    of biomedical engineering*, vol. 15, pp. 327–357, 2013.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. L. Giger, N. Karssemeijer, 和 J. A. Schnabel，《用于风险评估、检测、诊断和治疗癌症的乳腺影像分析》，*生物医学工程年度综述*，第15卷，第327–357页，2013年。'
- en: '[11] J.-Z. Cheng, Y.-H. Chou, C.-S. Huang, Y.-C. Chang, C.-M. Tiu, K.-W. Chen,
    and C.-M. Chen, “Computer-aided us diagnosis of breast lesions by using cell-based
    contour grouping,” *Radiology*, vol. 255, no. 3, pp. 746–754, 2010.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J.-Z. Cheng, Y.-H. Chou, C.-S. Huang, Y.-C. Chang, C.-M. Tiu, K.-W. Chen,
    和 C.-M. Chen，《利用基于细胞的轮廓分组进行乳腺病变的计算机辅助超声诊断》，*放射学*，第255卷，第3期，第746–754页，2010年。'
- en: '[12] B. Van Ginneken, C. M. Schaefer-Prokop, and M. Prokop, “Computer-aided
    diagnosis: how to move from the laboratory to the clinic,” *Radiology*, vol. 261,
    no. 3, pp. 719–732, 2011.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] B. Van Ginneken, C. M. Schaefer-Prokop, 和 M. Prokop，《计算机辅助诊断：如何从实验室转向临床》，*放射学*，第261卷，第3期，第719–732页，2011年。'
- en: '[13] K. H. Allison, L. M. Reisch, P. A. Carney, D. L. Weaver, S. J. Schnitt,
    F. P. O’Malley, B. M. Geller, and J. G. Elmore, “Understanding diagnostic variability
    in breast pathology: lessons learned from an expert consensus review panel,” *Histopathology*,
    vol. 65, no. 2, pp. 240–251, 2014.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K. H. Allison, L. M. Reisch, P. A. Carney, D. L. Weaver, S. J. Schnitt,
    F. P. O’Malley, B. M. Geller, 和 J. G. Elmore，《理解乳腺病理诊断的变异性：从专家共识评审小组中学到的经验教训》，*组织病理学*，第65卷，第2期，第240–251页，2014年。'
- en: '[14] L. J. Grimm, A. L. Anderson, J. A. Baker, K. S. Johnson, R. Walsh, S. C.
    Yoon, and S. V. Ghate, “Interobserver variability between breast imagers using
    the fifth edition of the bi-rads mri lexicon,” *American Journal of Roentgenology*,
    vol. 204, no. 5, pp. 1120–1124, 2015.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. J. Grimm, A. L. Anderson, J. A. Baker, K. S. Johnson, R. Walsh, S.
    C. Yoon, 和 S. V. Ghate, “使用第五版BI-RADS MRI词汇表的乳腺影像医师之间的观察者变异性，” *美国放射学杂志*，第204卷，第5期，页码1120–1124，2015年。'
- en: '[15] D. Wang, A. Khosla, R. Gargeya, H. Irshad, and A. H. Beck, “Deep learning
    for identifying metastatic breast cancer,” *arXiv preprint arXiv:1606.05718*,
    2016.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Wang, A. Khosla, R. Gargeya, H. Irshad, 和 A. H. Beck, “用于识别转移性乳腺癌的深度学习，”
    *arXiv 预印本 arXiv:1606.05718*，2016年。'
- en: '[16] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova, H. Ashrafian,
    T. Back, M. Chesus, G. S. Corrado, A. Darzi *et al.*, “International evaluation
    of an ai system for breast cancer screening,” *Nature*, vol. 577, no. 7788, pp.
    89–94, 2020.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova, H. Ashrafian,
    T. Back, M. Chesus, G. S. Corrado, A. Darzi *等*，“国际评估乳腺癌筛查的人工智能系统，” *自然*，第577卷，第7788期，页码89–94，2020年。'
- en: '[17] A. Ibrahim, P. Gamble, R. Jaroensri, M. M. Abdelsamea, C. H. Mermel, P.-H. C.
    Chen, and E. A. Rakha, “Artificial intelligence in digital breast pathology: techniques
    and applications,” *The Breast*, vol. 49, pp. 267–273, 2020.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Ibrahim, P. Gamble, R. Jaroensri, M. M. Abdelsamea, C. H. Mermel, P.-H.
    C. Chen, 和 E. A. Rakha, “数字乳腺病理中的人工智能：技术与应用，” *乳腺*，第49卷，页码267–273，2020年。'
- en: '[18] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*.   MIT Press,
    2016, [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] I. Goodfellow, Y. Bengio, 和 A. Courville, *深度学习*。MIT出版社，2016年，[http://www.deeplearningbook.org](http://www.deeplearningbook.org)。'
- en: '[19] S. P. Singh, L. Wang, S. Gupta, H. Goli, P. Padmanabhan, and B. Gulyás,
    “3d deep learning on medical images: a review,” *Sensors*, vol. 20, no. 18, p.
    5097, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. P. Singh, L. Wang, S. Gupta, H. Goli, P. Padmanabhan, 和 B. Gulyás,
    “医学影像上的3D深度学习：综述，” *传感器*，第20卷，第18期，页码5097，2020年。'
- en: '[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances in neural information processing
    systems*, vol. 25, pp. 1097–1105, 2012.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行ImageNet分类，”
    *神经信息处理系统进展*，第25卷，页码1097–1105，2012年。'
- en: '[21] K. Suzuki, “Overview of deep learning in medical imaging,” *Radiological
    physics and technology*, vol. 10, no. 3, pp. 257–273, 2017.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. Suzuki, “深度学习在医学影像中的概述，” *放射物理与技术*，第10卷，第3期，页码257–273，2017年。'
- en: '[22] R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi, “Convolutional neural
    networks: an overview and application in radiology,” *Insights into imaging*,
    vol. 9, no. 4, pp. 611–629, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Yamashita, M. Nishio, R. K. G. Do, 和 K. Togashi, “卷积神经网络：概述及其在放射学中的应用，”
    *影像学前沿*，第9卷，第4期，页码611–629，2018年。'
- en: '[23] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern Recognition*, vol. 77, pp. 354–377, 2018.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *等*，“卷积神经网络的最新进展，” *模式识别*，第77卷，页码354–377，2018年。'
- en: '[24] F. Amherd and E. Rodriguez, “Heatmap-based object detection and tracking
    with a fully convolutional neural network,” *arXiv preprint arXiv:2101.03541*,
    2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] F. Amherd 和 E. Rodriguez, “基于热图的物体检测与跟踪，使用全卷积神经网络，” *arXiv 预印本 arXiv:2101.03541*，2021年。'
- en: '[25] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, “Improving the robustness
    of deep neural networks via stability training,” *arXiv preprint arXiv:1604.04326*,
    2016.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Zheng, Y. Song, T. Leung, 和 I. Goodfellow, “通过稳定性训练提高深度神经网络的鲁棒性，” *arXiv
    预印本 arXiv:1604.04326*，2016年。'
- en: '[26] A. LeNail, “Nn-svg: Publication-ready neural network architecture schematics,”
    *Journal of Open Source Software*, vol. 4, no. 33, p. 747, 2019\. [Online]. Available:
    [https://doi.org/10.21105/joss.00747](https://doi.org/10.21105/joss.00747)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. LeNail, “NN-SVG：适合出版的神经网络架构示意图，” *开源软件杂志*，第4卷，第33期，页码747，2019年。[在线].
    可用：[https://doi.org/10.21105/joss.00747](https://doi.org/10.21105/joss.00747)'
- en: '[27] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, “Dive into deep learning,”
    *arXiv preprint arXiv:2106.11342*, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Zhang, Z. C. Lipton, M. Li, 和 A. J. Smola, “深入探索深度学习，” *arXiv 预印本 arXiv:2106.11342*，2021年。'
- en: '[28] K. O’Shea and R. Nash, “An introduction to convolutional neural networks,”
    *arXiv preprint arXiv:1511.08458*, 2015.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] K. O’Shea 和 R. Nash, “卷积神经网络入门，” *arXiv 预印本 arXiv:1511.08458*，2015年。'
- en: '[29] J. Wu, “Introduction to convolutional neural networks,” *National Key
    Lab for Novel Software Technology. Nanjing University. China*, vol. 5, no. 23,
    p. 495, 2017.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Wu, “卷积神经网络简介，” *国家新型软件技术重点实验室，南京大学，中国*，第5卷，第23期，页码495，2017年。'
- en: '[30] G. Kang, K. Liu, B. Hou, and N. Zhang, “3d multi-view convolutional neural
    networks for lung nodule classification,” *PloS one*, vol. 12, no. 11, p. e0188290,
    2017.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. D. Hunter, “Matplotlib: A 2d graphics environment,” *Computing in Science
    & Engineering*, vol. 9, no. 3, pp. 90–95, 2007.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation functions,”
    *arXiv preprint arXiv:1710.05941*, 2017.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall, “Activation functions:
    Comparison of trends in practice and research for deep learning,” *arXiv preprint
    arXiv:1811.03378*, 2018.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Sun, Z. Song, X. Jiang, J. Pan, and Y. Pang, “Learning pooling for
    convolutional neural network,” *Neurocomputing*, vol. 224, pp. 96–104, 2017.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. A. Mazurowski, M. Buda, A. Saha, and M. R. Bashir, “Deep learning in
    radiology: an overview of the concepts and a survey of the state of the art,”
    *arXiv preprint arXiv:1802.08717*, 2018.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Ruder, “An overview of gradient descent optimization algorithms,” *arXiv
    preprint arXiv:1609.04747*, 2016.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Ying, “An overview of overfitting and its solutions,” in *Journal of
    Physics: Conference Series*, vol. 1168, no. 2.   IOP Publishing, 2019, p. 022022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] W. S. Sarle *et al.*, “Stopped training and other remedies for overfitting,”
    *Computing science and statistics*, pp. 352–360, 1996.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value
    of network pruning,” *arXiv preprint arXiv:1810.05270*, 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The journal
    of machine learning research*, vol. 15, no. 1, pp. 1929–1958, 2014.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,”
    *Journal of Big data*, vol. 3, no. 1, pp. 1–40, 2016.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on deep
    transfer learning,” in *International conference on artificial neural networks*.   Springer,
    2018, pp. 270–279.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P. Martinez-Gonzalez,
    and J. Garcia-Rodriguez, “A survey on deep learning techniques for image and video
    semantic segmentation,” *Applied Soft Computing*, vol. 70, pp. 41–65, 2018.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Davis and M. Goadrich, “The relationship between precision-recall and
    roc curves,” in *Proceedings of the 23rd international conference on Machine learning*,
    2006, pp. 233–240.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] L. A. Jeni, J. F. Cohn, and F. De La Torre, “Facing imbalanced data–recommendations
    for the use of performance metrics,” in *2013 Humaine association conference on
    affective computing and intelligent interaction*.   IEEE, 2013, pp. 245–251.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D. Chicco and G. Jurman, “The advantages of the matthews correlation coefficient
    (mcc) over f1 score and accuracy in binary classification evaluation,” *BMC genomics*,
    vol. 21, no. 1, pp. 1–13, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] D. Chicco 和 G. Jurman，“马修斯相关系数（MCC）相对于F1得分和准确性的优势，在二分类评估中的应用，” *BMC genomics*，第21卷，第1期，页码1–13，2020年。'
- en: '[48] J. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class
    imbalance,” *Journal of Big Data*, vol. 6, no. 1, pp. 1–54, 2019.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. M. Johnson 和 T. M. Khoshgoftaar，“关于类不平衡的深度学习调查，” *Journal of Big Data*，第6卷，第1期，页码1–54，2019年。'
- en: '[49] K. H. Zou, S. K. Warfield, A. Bharatha, C. M. Tempany, M. R. Kaus, S. J.
    Haker, W. M. Wells III, F. A. Jolesz, and R. Kikinis, “Statistical validation
    of image segmentation quality based on a spatial overlap index1: scientific reports,”
    *Academic radiology*, vol. 11, no. 2, pp. 178–189, 2004.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. H. Zou, S. K. Warfield, A. Bharatha, C. M. Tempany, M. R. Kaus, S.
    J. Haker, W. M. Wells III, F. A. Jolesz, 和 R. Kikinis，“基于空间重叠指数的图像分割质量的统计验证1：科学报告，”
    *Academic radiology*，第11卷，第2期，页码178–189，2004年。'
- en: '[50] M. El Adoui, S. A. Mahmoudi, M. A. Larhmam, and M. Benjelloun, “Mri breast
    tumor segmentation using different encoder and decoder cnn architectures,” *Computers*,
    vol. 8, no. 3, p. 52, 2019.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. El Adoui, S. A. Mahmoudi, M. A. Larhmam, 和 M. Benjelloun，“使用不同编码器和解码器CNN架构进行MRI乳腺肿瘤分割，”
    *Computers*，第8卷，第3期，页码52，2019年。'
- en: '[51] N. Dhungel, G. Carneiro, and A. P. Bradley, “A deep learning approach
    for the analysis of masses in mammograms with minimal user intervention,” *Medical
    image analysis*, vol. 37, pp. 114–128, 2017.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] N. Dhungel, G. Carneiro, 和 A. P. Bradley，“一种用于分析乳腺X光片中肿块的深度学习方法，具有最小用户干预，”
    *Medical image analysis*，第37卷，页码114–128，2017年。'
- en: '[52] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han, and T.-S. Kim,
    “A fully integrated computer-aided diagnosis system for digital x-ray mammograms
    via deep learning detection, segmentation, and classification,” *International
    journal of medical informatics*, vol. 117, pp. 44–54, 2018.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han, 和 T.-S. Kim，“通过深度学习检测、分割和分类的全功能计算机辅助诊断系统，用于数字X光乳腺图像，”
    *International journal of medical informatics*，第117卷，页码44–54，2018年。'
- en: '[53] H. Chougrad, H. Zouaki, and O. Alheyane, “Deep convolutional neural networks
    for breast cancer screening,” *Computer methods and programs in biomedicine*,
    vol. 157, pp. 19–30, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Chougrad, H. Zouaki, 和 O. Alheyane，“用于乳腺癌筛查的深度卷积神经网络，” *Computer methods
    and programs in biomedicine*，第157卷，页码19–30，2018年。'
- en: '[54] D. Ribli, A. Horváth, Z. Unger, P. Pollner, and I. Csabai, “Detecting
    and classifying lesions in mammograms with deep learning,” *Scientific reports*,
    vol. 8, no. 1, pp. 1–7, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. Ribli, A. Horváth, Z. Unger, P. Pollner, 和 I. Csabai，“使用深度学习检测和分类乳腺X光片中的病变，”
    *Scientific reports*，第8卷，第1期，页码1–7，2018年。'
- en: '[55] V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker,
    A. Saleh, M. Arenas, M. Arquez, D. Puig *et al.*, “Breast tumor segmentation and
    shape classification in mammograms using generative adversarial and convolutional
    neural network,” *Expert Systems with Applications*, vol. 139, p. 112855, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker,
    A. Saleh, M. Arenas, M. Arquez, D. Puig *等*，“使用生成对抗网络和卷积神经网络对乳腺X光片中的乳腺肿瘤进行分割和形状分类，”
    *Expert Systems with Applications*，第139卷，页码112855，2020年。'
- en: '[56] W. T. Tran, A. Sadeghi-Naini, F.-I. Lu, S. Gandhi, N. Meti, M. Brackstone,
    E. Rakovitch, and B. Curpen, “Computational radiology in breast cancer screening
    and diagnosis using artificial intelligence,” *Canadian Association of Radiologists
    Journal*, vol. 72, no. 1, pp. 98–108, 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] W. T. Tran, A. Sadeghi-Naini, F.-I. Lu, S. Gandhi, N. Meti, M. Brackstone,
    E. Rakovitch, 和 B. Curpen，“使用人工智能进行乳腺癌筛查和诊断的计算放射学，” *Canadian Association of Radiologists
    Journal*，第72卷，第1期，页码98–108，2021年。'
- en: '[57] E. F. Conant, W. E. Barlow, S. D. Herschorn, D. L. Weaver, E. F. Beaber,
    A. N. Tosteson, J. S. Haas, K. P. Lowry, N. K. Stout, A. Trentham-Dietz *et al.*,
    “Association of digital breast tomosynthesis vs digital mammography with cancer
    detection and recall rates by age and breast density,” *JAMA oncology*, vol. 5,
    no. 5, pp. 635–642, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] E. F. Conant, W. E. Barlow, S. D. Herschorn, D. L. Weaver, E. F. Beaber,
    A. N. Tosteson, J. S. Haas, K. P. Lowry, N. K. Stout, A. Trentham-Dietz *等*，“数字乳腺断层扫描与数字乳腺X光片在癌症检测和召回率方面的年龄和乳腺密度关联，”
    *JAMA oncology*，第5卷，第5期，页码635–642，2019年。'
- en: '[58] E. A. Rafferty, M. A. Durand, E. F. Conant, D. S. Copit, S. M. Friedewald,
    D. M. Plecha, and D. P. Miller, “Breast cancer screening using tomosynthesis and
    digital mammography in dense and nondense breasts,” *Jama*, vol. 315, no. 16,
    pp. 1784–1786, 2016.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] E. A. Rafferty, M. A. Durand, E. F. Conant, D. S. Copit, S. M. Friedewald,
    D. M. Plecha, 和 D. P. Miller，“在致密和非致密乳腺中使用断层扫描和数字乳腺X光片进行乳腺癌筛查，” *Jama*，第315卷，第16期，页码1784–1786，2016年。'
- en: '[59] K. P. Lowry, R. Y. Coley, D. L. Miglioretti, K. Kerlikowske, L. M. Henderson,
    T. Onega, B. L. Sprague, J. M. Lee, S. Herschorn, A. N. Tosteson *et al.*, “Screening
    performance of digital breast tomosynthesis vs digital mammography in community
    practice by patient age, screening round, and breast density,” *JAMA network open*,
    vol. 3, no. 7, pp. e2 011 792–e2 011 792, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] E. F. Conant, E. F. Beaber, B. L. Sprague, S. D. Herschorn, D. L. Weaver,
    T. Onega, A. N. Tosteson, A. M. McCarthy, S. P. Poplack, J. S. Haas *et al.*,
    “Breast cancer screening using tomosynthesis in combination with digital mammography
    compared to digital mammography alone: a cohort study within the prospr consortium,”
    *Breast cancer research and treatment*, vol. 156, no. 1, pp. 109–116, 2016.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X.-A. Phi, A. Tagliafico, N. Houssami, M. J. Greuter, and G. H. de Bock,
    “Digital breast tomosynthesis for breast cancer screening and diagnosis in women
    with dense breasts–a systematic review and meta-analysis,” *BMC cancer*, vol. 18,
    no. 1, pp. 1–9, 2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] P. Pattacini, A. Nitrosi, P. Giorgi Rossi, V. Iotti, V. Ginocchi, S. Ravaioli,
    R. Vacondio, L. Braglia, S. Cavuto, C. Campari *et al.*, “Digital mammography
    versus digital mammography plus tomosynthesis for breast cancer screening: the
    reggio emilia tomosynthesis randomized trial,” *Radiology*, vol. 288, no. 2, pp.
    375–385, 2018.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] J. A. Baker and J. Y. Lo, “Breast tomosynthesis: state-of-the-art and
    review of the literature,” *Academic radiology*, vol. 18, no. 10, pp. 1298–1310,
    2011.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. V. Horvat, D. M. Keating, H. Rodrigues-Duarte, E. A. Morris, and V. L.
    Mango, “Calcifications at digital breast tomosynthesis: imaging features and biopsy
    techniques,” *Radiographics*, vol. 39, no. 2, pp. 307–318, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. H. Kim, H. H. Kim, and W. K. Moon, “Automated breast ultrasound screening
    for dense breasts,” *Korean journal of radiology*, vol. 21, no. 1, pp. 15–24,
    2020.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. J. Shin, H. H. Kim, and J. H. Cha, “Current status of automated breast
    ultrasonography,” *Ultrasonography*, vol. 34, no. 3, p. 165, 2015.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. D. Lehman, J. M. Lee, W. B. DeMartini, D. S. Hippe, M. H. Rendi, G. Kalish,
    P. Porter, J. Gralow, and S. C. Partridge, “Screening mri in women with a personal
    history of breast cancer,” *Journal of the National Cancer Institute*, vol. 108,
    no. 3, p. djv349, 2016.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. M. Mann, N. Cho, and L. Moy, “Breast mri: state of the art,” *Radiology*,
    vol. 292, no. 3, pp. 520–536, 2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H. Greenspan, G. Oz, N. Kiryati, and S. Peled, “Mri inter-slice reconstruction
    using super-resolution,” *Magnetic resonance imaging*, vol. 20, no. 5, pp. 437–446,
    2002.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] R. Z. Shilling, T. Q. Robbie, T. Bailloeul, K. Mewes, R. M. Mersereau,
    and M. E. Brummer, “A super-resolution framework for 3-d high-resolution and high-contrast
    imaging using 2-d multislice mri,” *IEEE transactions on medical imaging*, vol. 28,
    no. 5, pp. 633–644, 2008.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Yousefi, A. Krzyżak, and C. Y. Suen, “Mass detection in digital breast
    tomosynthesis data using convolutional neural networks and multiple instance learning,”
    *Computers in biology and medicine*, vol. 96, pp. 283–293, 2018.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Yousefi, A. Krzyżak, 和 C. Y. Suen, “使用卷积神经网络和多实例学习进行数字乳腺断层合成数据中的肿块检测，”
    *生物医学计算机*，第 96 卷，第 283–293 页，2018 年。'
- en: '[72] D. H. Kim, S. T. Kim, J. M. Chang, and Y. M. Ro, “Latent feature representation
    with depth directional long-term recurrent learning for breast masses in digital
    breast tomosynthesis,” *Physics in Medicine & Biology*, vol. 62, no. 3, p. 1009,
    2017.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. H. Kim, S. T. Kim, J. M. Chang, 和 Y. M. Ro, “数字乳腺断层合成中乳腺肿块的深度方向长期递归学习的潜在特征表示，”
    *医学与生物物理学*，第 62 卷，第 3 期，第 1009 页，2017 年。'
- en: '[73] S. Liu, D. Xu, S. K. Zhou, T. Mertelmeier, J. Wicklein, A. Jerebko, S. Grbic,
    O. Pauly, W. Cai, and D. Comaniciu, “3d anisotropic hybrid network: Transferring
    convolutional features from 2d images to 3d anisotropic volumes,” *arXiv preprint
    arXiv:1711.08580*, 2017.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Liu, D. Xu, S. K. Zhou, T. Mertelmeier, J. Wicklein, A. Jerebko, S.
    Grbic, O. Pauly, W. Cai, 和 D. Comaniciu, “3d 各向异性混合网络：将卷积特征从 2d 图像转移到 3d 各向异性体积，”
    *arXiv 预印本 arXiv:1711.08580*，2017 年。'
- en: '[74] Y. Zhang, X. Wang, H. Blanton, G. Liang, X. Xing, and N. Jacobs, “2d convolutional
    neural networks for 3d digital breast tomosynthesis classification,” in *2019
    IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*.   IEEE,
    2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Zhang, X. Wang, H. Blanton, G. Liang, X. Xing, 和 N. Jacobs, “用于 3d
    数字乳腺断层合成分类的 2d 卷积神经网络，” 见于 *2019 IEEE 国际生物信息学与生物医学会议 (BIBM)*。 IEEE，2019 年。'
- en: '[75] G. Liang, X. Wang, Y. Zhang, X. Xing, H. Blanton, T. Salem, and N. Jacobs,
    “Joint 2d-3d breast cancer classification,” in *2019 IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*.   IEEE, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. Liang, X. Wang, Y. Zhang, X. Xing, H. Blanton, T. Salem, 和 N. Jacobs,
    “联合 2d-3d 乳腺癌分类，” 见于 *2019 IEEE 国际生物信息学与生物医学会议 (BIBM)*。 IEEE，2019 年。'
- en: '[76] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *arXiv preprint arXiv:1512.03385*, 2015.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，” *arXiv 预印本 arXiv:1512.03385*，2015
    年。'
- en: '[77] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *International Conference on Learning Representations*,
    2015.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深卷积网络，” 见于 *国际学习表征会议*，2015 年。'
- en: '[78] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *Computer Vision and Pattern
    Recognition (CVPR)*, 2016.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, 和 Z. Wojna, “重新思考计算机视觉中的
    inception 架构，” 见于 *计算机视觉与模式识别会议 (CVPR)*，2016 年。'
- en: '[79] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” *CVPR*, 2017.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] P. Isola, J.-Y. Zhu, T. Zhou, 和 A. A. Efros, “使用条件对抗网络进行图像到图像的转换，” *CVPR*，2017
    年。'
- en: '[80] X. Zhang, Y. Zhang, E. Y. Han, N. Jacobs, Q. Han, X. Wang, and J. Liu,
    “Classification of whole mammogram and tomosynthesis images using deep convolutional
    neural networks,” *IEEE transactions on nanobioscience*, vol. 17, no. 3, pp. 237–242,
    2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] X. Zhang, Y. Zhang, E. Y. Han, N. Jacobs, Q. Han, X. Wang, 和 J. Liu, “使用深度卷积神经网络对整个乳腺摄影和断层合成图像进行分类，”
    *IEEE 纳米生物科学学报*，第 17 卷，第 3 期，第 237–242 页，2018 年。'
- en: '[81] M. Fan, H. Zheng, S. Zheng, C. You, Y. Gu, X. Gao, W. Peng, and L. Li,
    “Mass detection and segmentation in digital breast tomosynthesis using 3d-mask
    region-based convolutional neural network: A comparative analysis,” *Frontiers
    in molecular biosciences*, vol. 7, 2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Fan, H. Zheng, S. Zheng, C. You, Y. Gu, X. Gao, W. Peng, 和 L. Li, “使用
    3d-mask 区域卷积神经网络进行数字乳腺断层合成中的肿块检测和分割：比较分析，” *分子生物科学前沿*，第 7 卷，2020 年。'
- en: '[82] I. Wichakam, J. Chayakulkheeree, and P. Vateekul, “Deep multi-label 3d
    convnet for breast cancer diagnosis in dbt with inversion augmentation,” in *Tenth
    International Conference on Digital Image Processing (ICDIP 2018)*.   International
    Society for Optics and Photonics, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] I. Wichakam, J. Chayakulkheeree, 和 P. Vateekul, “用于 dbt 乳腺癌诊断的深度多标签 3d
    convnet 与反转增强，” 见于 *第十届国际数字图像处理会议 (ICDIP 2018)*。 国际光学与光子学学会，2018 年。'
- en: '[83] Y. Lei, X. He, J. Yao, T. Wang, L. Wang, W. Li, W. J. Curran, T. Liu,
    D. Xu, and X. Yang, “Breast tumor segmentation in 3d automatic breast ultrasound
    using mask scoring r-cnn,” *Medical Physics*, vol. 48, no. 1, pp. 204–214, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Lei, X. He, J. Yao, T. Wang, L. Wang, W. Li, W. J. Curran, T. Liu,
    D. Xu, 和 X. Yang, “使用 mask scoring r-cnn 进行 3d 自动乳腺超声中的乳腺肿瘤分割，” *医学物理学*，第 48 卷，第
    1 期，第 204–214 页，2021 年。'
- en: '[84] Y. Zhou, H. Chen, Y. Li, Q. Liu, X. Xu, S. Wang, P.-T. Yap, and D. Shen,
    “Multi-task learning for segmentation and classification of tumors in 3d automated
    breast ultrasound images,” *Medical Image Analysis*, vol. 70, p. 101918, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Zhou, H. Chen, Y. Li, Q. Liu, X. Xu, S. Wang, P.-T. Yap, 和 D. Shen,
    “用于3D自动化乳腺超声图像中肿瘤分割和分类的多任务学习，” *医学图像分析*，第70卷，页码101918，2021。'
- en: '[85] J. Zhou, L.-Y. Luo, Q. Dou, H. Chen, C. Chen, G.-J. Li, Z.-F. Jiang, and
    P.-A. Heng, “Weakly supervised 3d deep learning for breast cancer classification
    and localization of the lesions in mr images,” *Journal of Magnetic Resonance
    Imaging*, vol. 50, no. 4, pp. 1144–1151, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Zhou, L.-Y. Luo, Q. Dou, H. Chen, C. Chen, G.-J. Li, Z.-F. Jiang, 和
    P.-A. Heng, “用于乳腺癌分类和MR图像中病变定位的弱监督3D深度学习，” *磁共振成像杂志*，第50卷，第4期，页码1144–1151，2019。'
- en: '[86] Q. Hu, H. M. Whitney, and M. L. Giger, “A deep learning methodology for
    improved breast cancer diagnosis using multiparametric mri,” *Scientific reports*,
    vol. 10, no. 1, pp. 1–11, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Q. Hu, H. M. Whitney, 和 M. L. Giger, “一种改进乳腺癌诊断的深度学习方法，使用多参数MRI，” *科学报告*，第10卷，第1期，页码1–11，2020。'
- en: '[87] B. Xiao, H. Sun, Y. Meng, Y. Peng, X. Yang, S. Chen, Z. Yan, and J. Zheng,
    “Classification of microcalcification clusters in digital breast tomosynthesis
    using ensemble convolutional neural network,” *BioMedical Engineering OnLine*,
    vol. 20, no. 1, pp. 1–20, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Xiao, H. Sun, Y. Meng, Y. Peng, X. Yang, S. Chen, Z. Yan, 和 J. Zheng,
    “在数字乳腺断层扫描中使用集成卷积神经网络进行微钙化簇的分类，” *生物医学工程在线*，第20卷，第1期，页码1–20，2021。'
- en: '[88] S. Liu, D. Xu, S. K. Zhou, S. Grbic, W. Cai, and D. Comaniciu, “Anisotropic
    hybrid network for cross-dimension transferable feature learning in 3d medical
    images,” in *Deep Learning and Convolutional Neural Networks for Medical Imaging
    and Clinical Informatics*.   Springer, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. Liu, D. Xu, S. K. Zhou, S. Grbic, W. Cai, 和 D. Comaniciu, “用于3D医学图像中跨维度可转移特征学习的各向异性混合网络，”
    见于 *深度学习和卷积神经网络在医学成像和临床信息学中的应用*。 Springer，2019。'
- en: '[89] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A closer
    look at spatiotemporal convolutions for action recognition,” in *CVPR*, 2018.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, 和 M. Paluri, “对时空卷积在动作识别中的应用的更深入探讨，”
    见于 *CVPR*，2018。'
- en: '[90] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *Computer Vision and Pattern Recognition (CVPR)*,
    2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Carreira 和 A. Zisserman, “前途如何，动作识别？一个新模型和Kinetics数据集，” 见于 *计算机视觉与模式识别
    (CVPR)*，2017。'
- en: '[91] M. Buda, A. Saha, R. Walsh, S. Ghate, N. Li, A. Święcicki, J. Y. Lo, and
    M. A. Mazurowski, “Detection of masses and architectural distortions in digital
    breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep
    learning model,” *arXiv preprint arXiv:2011.07995*, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Buda, A. Saha, R. Walsh, S. Ghate, N. Li, A. Święcicki, J. Y. Lo, 和
    M. A. Mazurowski, “在数字乳腺断层扫描中检测肿块和结构扭曲：一个包含5,060名患者的公开数据集和一个深度学习模型，” *arXiv 预印本
    arXiv:2011.07995*，2020。'
- en: '[92] S. J. S. Gardezi, A. Elazab, B. Lei, and T. Wang, “Breast cancer detection
    and diagnosis using mammographic data: Systematic review,” *Journal of medical
    Internet research*, vol. 21, no. 7, p. e14464, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. J. S. Gardezi, A. Elazab, B. Lei, 和 T. Wang, “使用乳腺X光数据进行乳腺癌检测和诊断：系统综述，”
    *医学互联网研究杂志*，第21卷，第7期，页码e14464，2019。'
- en: '[93] K. Dembrower, P. Lindholm, and F. Strand, “A multi-million mammography
    image dataset and population-based screening cohort for the training and evaluation
    of deep neural networks—the cohort of screen-aged women (csaw),” *Journal of digital
    imaging*, pp. 1–6, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] K. Dembrower, P. Lindholm, 和 F. Strand, “一个数百万张乳腺X光图像数据集和基于人群筛查的队列，用于训练和评估深度神经网络——筛查年龄女性队列（CSAW），”
    *数字成像杂志*，页码1–6，2019。'
- en: '[94] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake, M. Gorovoy, and D. L. Rubin,
    “A curated mammography data set for use in computer-aided detection and diagnosis
    research,” *Scientific data*, vol. 4, no. 1, pp. 1–9, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake, M. Gorovoy, 和 D. L. Rubin,
    “一个经过策划的乳腺X光数据集，用于计算机辅助检测和诊断研究，” *科学数据*，第4卷，第1期，页码1–9，2017。'
- en: '[95] I. C. Moreira, I. Amaral, I. Domingues, A. Cardoso, M. J. Cardoso, and
    J. S. Cardoso, “Inbreast: toward a full-field digital mammographic database,”
    *Academic radiology*, vol. 19, no. 2, pp. 236–248, 2012.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] I. C. Moreira, I. Amaral, I. Domingues, A. Cardoso, M. J. Cardoso, 和 J.
    S. Cardoso, “Inbreast：迈向全面数字化乳腺X光数据库，” *学术放射学*，第19卷，第2期，页码236–248，2012。'
- en: '[96] D. Abdelhafiz, C. Yang, R. Ammar, and S. Nabavi, “Deep convolutional neural
    networks for mammography: advances, challenges and applications,” *BMC bioinformatics*,
    vol. 20, no. 11, pp. 1–20, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D. Abdelhafiz, C. Yang, R. Ammar 和 S. Nabavi，“用于乳腺X光检查的深度卷积神经网络：进展、挑战与应用，”*BMC
    生物信息学*，第20卷，第11期，第1–20页，2019年。'
- en: '[97] K.-H. Yu, A. L. Beam, and I. S. Kohane, “Artificial intelligence in healthcare,”
    *Nature biomedical engineering*, vol. 2, no. 10, pp. 719–731, 2018.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K.-H. Yu, A. L. Beam 和 I. S. Kohane，“医疗保健中的人工智能，”*自然生物医学工程*，第2卷，第10期，第719–731页，2018年。'
- en: '[98] M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study of the class
    imbalance problem in convolutional neural networks,” *Neural Networks*, vol. 106,
    pp. 249–259, 2018.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Buda, A. Maki 和 M. A. Mazurowski，“卷积神经网络中的类不平衡问题的系统研究，”*神经网络*，第106卷，第249–259页，2018年。'
- en: '[99] W. W. Ng, G. Zeng, J. Zhang, D. S. Yeung, and W. Pedrycz, “Dual autoencoders
    features for imbalance classification problem,” *Pattern Recognition*, vol. 60,
    pp. 875–889, 2016.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] W. W. Ng, G. Zeng, J. Zhang, D. S. Yeung 和 W. Pedrycz，“用于不平衡分类问题的双重自编码器特征，”*模式识别*，第60卷，第875–889页，2016年。'
- en: '[100] L. Ge, J. Gao, H. Ngo, K. Li, and A. Zhang, “On handling negative transfer
    and imbalanced distributions in multiple source transfer learning,” *Statistical
    Analysis and Data Mining: The ASA Data Science Journal*, vol. 7, no. 4, pp. 254–271,
    2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] L. Ge, J. Gao, H. Ngo, K. Li 和 A. Zhang，“处理多源迁移学习中的负迁移和不平衡分布，”*统计分析与数据挖掘：ASA
    数据科学杂志*，第7卷，第4期，第254–271页，2014年。'
- en: '[101] S. M. Abd Elrahman and A. Abraham, “A review of class imbalance problem,”
    *Journal of Network and Innovative Computing*, vol. 1, no. 2013, pp. 332–340,
    2013.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. M. Abd Elrahman 和 A. Abraham，“类不平衡问题的综述，”*网络与创新计算杂志*，第1卷，2013年，第332–340页，2013年。'
- en: '[102] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation
    with pseudo-3d residual networks,” in *ICCV*, 2017.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Z. Qiu, T. Yao 和 T. Mei，“使用伪 3D 残差网络学习时空表示，”发表于 *ICCV*，2017年。'
- en: '[103] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *International Conference
    on Computer Vision (ICCV)*, 2015.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] D. Tran, L. Bourdev, R. Fergus, L. Torresani 和 M. Paluri，“使用 3D 卷积网络学习时空特征，”发表于
    *国际计算机视觉大会（ICCV）*，2015年。'
