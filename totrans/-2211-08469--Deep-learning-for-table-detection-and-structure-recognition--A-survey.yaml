- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:43:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.08469] Deep learning for table detection and structure recognition: A
    survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.08469] 深度学习在表格检测和结构识别中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.08469](https://ar5iv.labs.arxiv.org/html/2211.08469)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.08469](https://ar5iv.labs.arxiv.org/html/2211.08469)
- en: 'Deep learning for table detection and structure recognition: A survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在表格检测和结构识别中的应用：综述
- en: Mahmoud Kasem Abdelrahman Abdallah Alexander Berendeyev Ebrahem Elkady Mahmoud
    Abdalla Mohamed Mahmoud Mohamed Hamada Daniyar Nurseitov Islam Taj-Eddin
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 马哈茂德·卡塞姆·阿卜杜勒拉赫曼·阿卜杜拉、亚历山大·别连德耶夫、艾布拉欣·埃尔卡迪、马哈茂德·阿卜杜拉、穆罕默德·马哈茂德、穆罕默德·哈马达、达尼亚尔·努尔赛托夫、伊斯兰·塔吉丁
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Tables are everywhere, from scientific journals, papers, websites, and newspapers
    all the way to items we buy at the supermarket. Detecting them is thus of utmost
    importance to automatically understanding the content of a document. The performance
    of table detection has substantially increased thanks to the rapid development
    of deep learning networks. The goals of this survey are to provide a profound
    comprehension of the major developments in the field of Table Detection, offer
    insight into the different methodologies, and provide a systematic taxonomy of
    the different approaches. Furthermore, we provide an analysis of both classic
    and new applications in the field. Lastly, the datasets and source code of the
    existing models are organized to provide the reader with a compass on this vast
    literature. Finally, we go over the architecture of utilizing various object detection
    and table structure recognition methods to create an effective and efficient system,
    as well as a set of development trends to keep up with state-of-the-art algorithms
    and future research. We have also set up a public GitHub repository where we will
    be updating the most recent publications, open data, and source code. The GitHub
    repository is available at [https://github.com/abdoelsayed2016/table-detection-structure-recognition](https://github.com/abdoelsayed2016/table-detection-structure-recognition).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表格无处不在，从科学期刊、论文、网站到我们在超市购买的商品。因此，检测表格对于自动理解文档内容至关重要。由于深度学习网络的快速发展，表格检测的性能有了显著提升。本文综述的目标是深入理解表格检测领域的主要进展，提供不同方法论的见解，并对不同方法进行系统分类。此外，我们还对经典和新兴应用进行了分析。最后，我们组织了现有模型的数据集和源代码，为读者提供了一个导航，帮助他们了解这一广阔的文献领域。我们还介绍了利用各种物体检测和表格结构识别方法创建有效且高效系统的架构，并概述了跟踪最先进算法和未来研究的发展趋势。我们还建立了一个公开的GitHub仓库，用于更新最新的出版物、开放数据和源代码。GitHub仓库地址为
    [https://github.com/abdoelsayed2016/table-detection-structure-recognition](https://github.com/abdoelsayed2016/table-detection-structure-recognition)。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Convolutional neural networks , deep learning , Document processing , table
    detection , table structure recognition.\affiliation
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络，深度学习，文档处理，表格检测，表格结构识别。\affiliation
- en: '[inst1]organization=Information Technology Department, FCI,addressline=Assiut
    University, city=Assiut, postcode=71515, state=Assiut, country=Egypt'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst1] 机构=信息技术系，FCI，地址=阿斯尤特大学，城市=阿斯尤特，邮政编码=71515，州=阿斯尤特，国家=埃及'
- en: \affiliation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[inst2] organization=Department of Information System, International IT University,,city=Almaty,
    postcode=050000, country=Kazakhstan'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst2] 机构=国际IT大学信息系统系，城市=阿拉木图，邮政编码=050000，国家=哈萨克斯坦'
- en: \affiliation
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[inst3] organization=KazMunayGas Engineering LLP,city=Nur-Sultan, postcode=010000,
    country=Kazakhstan'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst3] 机构=KazMunayGas Engineering LLP，城市=努尔苏丹，邮政编码=010000，国家=哈萨克斯坦'
- en: \affiliation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[inst4] organization=Satbayev University,city=Almaty, postcode=050013, country=Kazakhstan'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst4] 机构=萨特巴耶夫大学，城市=阿拉木图，邮政编码=050013，国家=哈萨克斯坦'
- en: \affiliation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[inst5] organization=Information Technology Institute(ITI),city=Alexandria,
    postcode=5310002, country=Egypt'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst5] 机构=信息技术研究所（ITI），城市=亚历山大，邮政编码=5310002，国家=埃及'
- en: \affiliation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[inst6] organization=College of Electrical and Computer Engineering, Chungbuk
    National University,city=Cheongju, postcode=28644, country=South Korea'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst6] 机构=电气与计算机工程学院，忠北国立大学，城市=清州市，邮政编码=28644，国家=韩国'
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Textbooks, lists, formulae, graphs, tables, and other elements are common in
    documents. Most papers, in particular, contain several sorts of tables. Tables,
    as a significant part of papers, may convey more information in fewer words and
    allow readers to quickly explore, compare, and comprehend the content. Table detection
    and structure identification are crucial tasks in image analysis because they
    allow retrieving vital information from tables in a digital format. Because of
    the document’s type and the variety of document layouts, detecting and extracting
    images or document tables is tough. Researchers have previously used heuristic
    techniques to recognize tables or to break pages into many parts for table extraction.
    Few studies have focused on table structure recognition in documents following
    table detection.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 教科书、列表、公式、图表、表格和其他元素在文档中很常见。大多数文献，特别是包含多种类型的表格。作为文献的重要组成部分，表格可以用更少的文字传达更多的信息，并允许读者快速浏览、比较和理解内容。表格检测和结构识别在图像分析中至关重要，因为它们可以以数字格式检索表格中的重要信息。由于文档类型和各种文档布局的差异，检测和提取图像或文档中的表格是困难的。研究人员以前使用启发式技术来识别表格或将页面拆分为多个部分以进行表格提取。很少有研究集中在表格检测后文档中的表格结构识别。
- en: The layout and content analysis of documents are used to detect tables. Tables
    come in a number of layouts and formats. As a result, creating a general method
    for table detection and table structure recognition is quite difficult. Table
    detection is regarded as a difficult subject in scientific circles. A large number
    of studies have been conducted in this sector, although the majority of them have
    limitations. Existing commercial and open-source document analysis algorithms,
    such as Tesseract, are unable to fully detect table areas from document images.
    [[1](#bib.bib1)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的布局和内容分析用于检测表格。表格有多种布局和格式。因此，为表格检测和表格结构识别创建通用方法是相当困难的。表格检测被认为是科学界的一个难题。虽然在这个领域进行了大量研究，但大多数研究都有局限性。现有的商业和开源文档分析算法，如
    Tesseract，无法完全从文档图像中检测表格区域。[[1](#bib.bib1)]。
- en: Machine learning and deep learning have been proven to be very effective in
    computer vision research. On computer vision tasks such as picture classification,
    object detection, object position estimation, learning, and so on, deep convolutional
    neural networks (types of feed-forward artificial neural systems) have outperformed
    alternative learning models. The effectiveness of Convolutional Neural Networks
    (CNNs) in object identification is based on their ability to learn substantial
    mid-level visual properties rather than the hand-crafted low-level representations
    that are often utilized in particular approaches to image categorization. The
    object is defined by its major characteristics, which include shape, size, color,
    texture, and other characteristics. To identify such an item, a picture must clearly
    reveal the object’s presence and, moreover, its position [[2](#bib.bib2)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习在计算机视觉研究中已被证明非常有效。在计算机视觉任务如图像分类、物体检测、物体位置估计、学习等方面，深度卷积神经网络（前馈人工神经系统的一种类型）表现优于其他学习模型。卷积神经网络（CNNs）在物体识别中的有效性基于其学习重要的中级视觉特征的能力，而不是通常在特定图像分类方法中使用的手工制作的低级表示。物体由其主要特征定义，包括形状、大小、颜色、纹理等。要识别这样的物体，图像必须清楚地显示物体的存在，并且进一步显示其位置[[2](#bib.bib2)]。
- en: Object detection may thus be described as a method of locating real-world items
    in photographs. Detection is closely connected to categorization because it includes
    determining the existence and location of a certain item in an image. There are
    many items that may be identified in a picture, including automobiles, buildings,
    tables, human faces, and so on. Deep learning approaches, such as deep neural
    networks, region-based convolutional neural networks, and deeply convolutional
    neural networks, can improve object identification precision, and efficacy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测可以被描述为在照片中定位现实世界物体的方法。检测与分类密切相关，因为它涉及确定图像中特定物体的存在和位置。图像中可以识别的物体有很多，如汽车、建筑物、表格、人脸等。深度学习方法，如深度神经网络、基于区域的卷积神经网络和深度卷积神经网络，可以提高物体识别的精度和效率。
- en: In recent years, a variety of remarkable and creative strategies have been used
    to improve deep learning model detection accuracy and solve complex challenges
    encountered during the training and testing process of deep learning object recognition
    models. Modification of the activation function of deep CNNs [[3](#bib.bib3)],
    Transfer learning [[4](#bib.bib4), [5](#bib.bib5)], cancer diagnosis, detection
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)], and classification[[9](#bib.bib9)],
    and medical question answers[[10](#bib.bib10), [11](#bib.bib11)], as well as software
    engineering applications such as optimizing the time and schedule of software
    projects[[12](#bib.bib12), [13](#bib.bib13)], Intrusion Detection in IoT [[14](#bib.bib14),
    [15](#bib.bib15)] and handwritten recognition for various languages[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]., and inventive ways in
    the combined selection of the activation function and the optimization system
    for the proposed deep learning model are among these unique strategies. Among
    the various variables and initiatives that have contributed to the rapid advancement
    of table detection algorithms, the development of deep convolutional neural networks
    and GPU computational capacity should be credited. Deep learning models are now
    widely used in many aspects of computer vision, including general table detection[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]. Table
    structures, on the other hand, receive far less attention, and the table structure
    is typically characterized by the rows and columns of a table [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，为提高深度学习模型的检测精度并解决深度学习目标识别模型在训练和测试过程中遇到的复杂挑战，采用了多种显著且富有创意的策略。这些独特的策略包括对深度卷积神经网络（CNNs）激活函数的修改[[3](#bib.bib3)]、迁移学习[[4](#bib.bib4),
    [5](#bib.bib5)]、癌症诊断和检测[[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]、分类[[9](#bib.bib9)]、医学问题回答[[10](#bib.bib10),
    [11](#bib.bib11)]，以及优化软件项目时间和进度的工程应用[[12](#bib.bib12), [13](#bib.bib13)]、物联网中的入侵检测[[14](#bib.bib14),
    [15](#bib.bib15)]和多种语言的手写识别[[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)]。这些策略还包括在提议的深度学习模型中，激活函数和优化系统的组合选择方面的创新方式。在促使表格检测算法快速发展的各种变量和举措中，深度卷积神经网络和GPU计算能力的发展应当被归功于此。深度学习模型现已广泛应用于计算机视觉的许多方面，包括通用表格检测[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]。然而，表格结构却受到的关注较少，表格结构通常由表格的行和列特征[[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)]来定义。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep learning for table detection
    and structure recognition: A survey") shows a basic pipeline comparison of deep
    learning techniques and conventional approaches for the task of understanding
    tables. Traditional table recognition techniques either can’t handle varied datasets
    well enough or need extra metadata from PDF files. Extensive pre- and post-processing
    were also used in the majority of early approaches to improve the effectiveness
    of conventional table recognition systems. However, deep learning algorithms retrieve
    features using neural networks, primarily convolutional neural networks [[21](#bib.bib21)],
    instead of manually created features. Object detection or segmentation networks
    then try to differentiate the tabular portion that is further broken down and
    recognized in a document image.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep learning for table detection
    and structure recognition: A survey")展示了深度学习技术和传统方法在理解表格任务中的基本流程对比。传统的表格识别技术要么无法很好地处理多样的数据集，要么需要来自PDF文件的额外元数据。在多数早期方法中，还使用了大量的预处理和后处理来提高传统表格识别系统的效果。然而，深度学习算法通过神经网络，主要是卷积神经网络[[21](#bib.bib21)]，来提取特征，而不是手动创建特征。对象检测或分割网络随后尝试区分表格部分，这些部分在文档图像中进一步分解和识别。'
- en: This survey examines deep learning-based table detection and classification
    architectures in depth. While current evaluations are comprehensive [[28](#bib.bib28),
    [29](#bib.bib29)], the majority of them do not address recent advancements in
    the field.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述深入考察了基于深度学习的表格检测和分类架构。尽管当前的评估非常全面[[28](#bib.bib28), [29](#bib.bib29)]，但大多数评估并未涉及该领域的最新进展。
- en: 'The following are the paper’s main contributions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是论文的主要贡献：
- en: '1.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We provide a brief history of Table Datasets and the differences between them.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们简要回顾了表格数据集的历史及其之间的差异。
- en: '2.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The paper examines important table detection methods, as well as the evolution
    of these methods over time.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文考察了重要的表格检测方法以及这些方法随时间的演变。
- en: '3.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We give a thorough analysis of table structure recognition in-depth.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对表格结构识别进行了深入的详细分析。
- en: '4.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We provide Table Classification methods and compare these methods. There was
    no study that provided a broad summary of these issues that we could identify.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了表格分类方法并对这些方法进行了比较。我们未能找到提供这些问题广泛总结的研究。
- en: '5.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Experiments Result on some datasets of Table detection
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表格检测某些数据集的实验结果
- en: '![Refer to caption](img/7ee6131ec94e32cb1c96e089b2c4dbdf.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7ee6131ec94e32cb1c96e089b2c4dbdf.png)'
- en: (a) Traditional Table Detection approaches
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 传统表格检测方法
- en: '![Refer to caption](img/26efbaae1d3c56695487da691222c45b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/26efbaae1d3c56695487da691222c45b.png)'
- en: (b) Deep Learning approaches for Table Detection
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 深度学习方法用于表格检测
- en: 'Figure 1: Table analysis pipeline comparison of conventional and deep learning
    methods. While convolutional networks are used in deep learning techniques, classical
    approaches primarily perform feature extraction through image processing techniques.
    Deep learning methods for interpreting tables are more generalizable and independent
    of data than conventional approaches.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：传统方法与深度学习方法的表格分析流程比较。虽然深度学习技术中使用了卷积网络，但经典方法主要通过图像处理技术进行特征提取。深度学习方法在解释表格时比传统方法更具通用性且独立于数据。
- en: 1.1 Comparison with Previous Reviews
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 与之前评论的比较
- en: 'For many years, the issue with table analysis has been widely acknowledged.
    Figure [2](#S1.F2 "Figure 2 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction
    ‣ Deep learning for table detection and structure recognition: A survey") shows
    the upward trend in publications during the previous eight years, this analysis
    values were derived from Scopus. There have been notable table detection and table
    classification surveys published. There are outstanding studies on the subject
    of table detection in these surveys [[28](#bib.bib28), [29](#bib.bib29)]. There
    have been few recent surveys that specifically address the subject of table detection
    and classification. B. Coüasnon [[30](#bib.bib30)] released another review on
    table recognition and forms. The review gives a quick rundown of the most recent
    techniques at the time, S. Khusro [[31](#bib.bib31)] released the most recent
    review on the identification and extraction of tables in PDF documents the following
    year, according to our knowledge. Deep learning enables computational models to
    learn fantastically complex, subtle, and abstract representations, resulting in
    significant advancements in a wide range of problems such as visual recognition,
    object detection, speech recognition, natural language processing, and medical
    image analysis. In contrast, despite the fact that various deep learning-based
    algorithms for table identification have been presented, we are unaware of any
    recent thorough survey. For further advancement in table detection, a detailed
    review and explanation of prior work are required, especially for researchers
    new to the topic.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，表格分析的问题被广泛认可。图[2](#S1.F2 "图 2 ‣ 1.1 与之前评论的比较 ‣ 1 引言 ‣ 表格检测与结构识别的深度学习：综述")显示了过去八年中出版物的上升趋势，这些分析值来源于Scopus。已经出版了显著的表格检测和表格分类综述。在这些综述中，对表格检测的研究非常出色[[28](#bib.bib28),
    [29](#bib.bib29)]。近期针对表格检测和分类主题的综述很少。B. Coüasnon [[30](#bib.bib30)] 发布了另一篇关于表格识别和表单的综述。该综述快速回顾了当时最新的技术。S.
    Khusro [[31](#bib.bib31)] 发表了关于PDF文档中表格识别和提取的最新综述，据我们了解。深度学习使计算模型能够学习非常复杂、微妙和抽象的表示，导致在视觉识别、物体检测、语音识别、自然语言处理和医学图像分析等广泛问题上取得重大进展。相比之下，尽管已经提出了各种基于深度学习的表格识别算法，但我们不知道有任何近期的全面综述。为了进一步推动表格检测的发展，需要对之前的工作进行详细的回顾和解释，特别是对新研究该主题的研究人员。
- en: '![Refer to caption](img/df9565620103622b834bf451386dbba1.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/df9565620103622b834bf451386dbba1.png)'
- en: 'Figure 2: shows an illustration of an expanding trend in the area of table
    analysis. This information was gathered by looking through the annual reports
    on table detection and table identification from the years 2015 to 2022, this
    analysis values were derived from Scopus.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：展示了表格分析领域扩展趋势的示意图。这些信息是通过查阅2015至2022年的表格检测和表格识别年度报告收集的，这些分析值来源于Scopus。
- en: 1.2 Scope
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 范围
- en: The quantity of studies on deep learning-based table detection is staggering.
    They are so numerous that any complete examination of the state of the art would
    be beyond the scope of any acceptable length paper. As a result, selection criteria
    must be established, and we have narrowed our attention to the best journal and
    conference articles.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的表格检测研究数量庞大。它们如此众多，以至于对最前沿的全面审查超出了任何适当长度论文的范围。因此，必须制定选择标准，我们将注意力集中在最好的期刊和会议文章上。
- en: The main goal of this paper is to provide a comprehensive survey of deep learning-based
    table detection and classification techniques, as well as some taxonomy, a high-level
    perspective, and organization, based on popular datasets, evaluation metrics,
    context modeling, and detection proposal methods. Our goal is for our classification
    to make it easier for readers to comprehend the similarities and differences across
    a wide range of tactics. The suggested taxonomy provides a framework for researchers
    to comprehend existing research and highlight open research problems for the future.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要目标是提供对基于深度学习的表格检测和分类技术的全面调查，包括一些分类法、高层次视角和组织，基于流行的数据集、评估指标、上下文建模和检测提议方法。我们的目标是使我们的分类使读者更容易理解各种策略之间的相似性和差异。建议的分类法为研究人员提供了一个框架，以理解现有研究并突出未来的研究问题。
- en: 2 Major Challenges
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 主要挑战
- en: 2.1 Object detection Challenges
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 物体检测挑战
- en: Developing a general-purpose algorithm that fulfills two competing criteria
    of high quality/accuracy and great efficiency is ideal for object detection. High-quality
    detection must accurately localize and recognize objects in images or video frames,
    allowing for distinction of a wide range of object categories in the real world
    and localization and recognition of object instances from the same category, despite
    intra-class appearance variations, for high robustness. High efficiency necessitates
    that the full detection process is completed in real-time while maintaining reasonable
    memory and storage requirements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一个既满足高质量/准确性又具备高效率的通用算法是理想的物体检测目标。高质量检测必须准确定位和识别图像或视频帧中的物体，允许区分现实世界中的各种物体类别，并在同一类别内进行物体实例的定位和识别，尽管存在类内外观变化，以实现高鲁棒性。高效率要求在保持合理的内存和存储要求的同时，完成整个检测过程并实时进行。
- en: 2.2 Table detection Challenges
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 表格检测挑战
- en: Although a trained segmentation model can accurately locate tables, conventional
    machine learning techniques have flaws in the structural identification of tables.
    A major issue is a large number of things in such a little space. As a result,
    the network misses out on critical visual cues that may aid in the detection and
    recognition of tables [[20](#bib.bib20)]. As physical rules are available, intersections
    of horizontal and vertical lines are computed to recognize table formations. The
    Hough transform is a prominent approach in computer vision that aids in the detection
    of lines in document scans [[32](#bib.bib32)]. Length, rotation, and average darkness
    of a line are utilized to filter out false positives and determine if the line
    is, in fact, a table line [[33](#bib.bib33)]. The intersections of the remaining
    horizontal and vertical lines are computed after the Hough lines have been filtered.
    Table cells are created based on the crossings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管经过训练的分割模型可以准确定位表格，但传统的机器学习技术在表格的结构识别方面存在缺陷。一个主要问题是如此小的空间内大量的物体。因此，网络错过了可能有助于检测和识别表格的关键视觉线索[[20](#bib.bib20)]。由于存在物理规则，通过计算水平线和垂直线的交点来识别表格的形成。霍夫变换是计算机视觉中的一种重要方法，帮助检测文档扫描中的线条[[32](#bib.bib32)]。利用线条的长度、旋转和平均亮度来过滤掉假阳性，并确定该线是否实际上是表格线[[33](#bib.bib33)]。在霍夫线过滤后，计算剩余水平线和垂直线的交点。基于交点创建表格单元格。
- en: 3 A Quick Overview of Deep Learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习的快速概述
- en: From image classification and video processing to speech recognition and natural
    language understanding, deep learning has transformed a wide range of machine
    learning activities. Given the incredible rate of change[[34](#bib.bib34)], there
    is a plethora of current survey studies on deep learning [[35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45)] , medical
    image analysis applications [[38](#bib.bib38)], natural language processing [[42](#bib.bib42)],
    and speech recognition systems [[44](#bib.bib44)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像分类和视频处理到语音识别和自然语言理解，深度学习已经改变了广泛的机器学习活动。鉴于变化的惊人速度[[34](#bib.bib34)]，目前有大量关于深度学习的综述研究
    [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45)]，医疗图像分析应用 [[38](#bib.bib38)]，自然语言处理 [[42](#bib.bib42)]，以及语音识别系统
    [[44](#bib.bib44)]。
- en: 'Convolutional neural networks (CNNs), the most common deep learning model,
    can use the fundamental properties of actual signals: translation invariance,
    local connection, and compositional hierarchies. A typical CNN comprises a hierarchical
    structure and numerous layers for learning data representations at different levels
    of abstraction [[36](#bib.bib36)]. We start with a convolution'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs），作为最常见的深度学习模型，可以利用实际信号的基本特性：平移不变性、局部连接和组合层次。典型的 CNN 包含一个分层结构和多个层，用于在不同抽象级别上学习数据表示
    [[36](#bib.bib36)]。我们从卷积开始
- en: '|  | $\begin{split}x^{l-1}*w^{l}\end{split}$ |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}x^{l-1}*w^{l}\end{split}$ |  | (1) |'
- en: between a feature map from the previous layer l-1 and an input feature map $x^{l-1}$
    , convolved using a 2D convolutional kernel (or filter or weights) $w^{l}$. This
    convolution is seen as a series of layers that have been subjected to a nonlinear
    process $\sigma$, such that
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一层 l-1 的特征图和输入特征图 $x^{l-1}$ 之间，通过二维卷积核（或滤波器或权重） $w^{l}$ 进行卷积。这种卷积被视为经历了非线性过程
    $\sigma$ 的一系列层，因此
- en: '|  | $\begin{split}x^{l}_{j}=\sigma\Bigg{(}\sum_{i=1}^{N^{l-1}}x^{l-1}_{i}*w^{l}_{i,j}+b^{l}_{j}\Bigg{)}\end{split}$
    |  | (2) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}x^{l}_{j}=\sigma\Bigg{(}\sum_{i=1}^{N^{l-1}}x^{l-1}_{i}*w^{l}_{i,j}+b^{l}_{j}\Bigg{)}\end{split}$
    |  | (2) |'
- en: with a bias term $b^{l}_{j}$ and a convolution between the $N^{l-1}$ input feature
    maps $x^{l-1}_{i}$ and the matching kernel $w^{l}_{i,j}$ . For each element, the
    element-wise nonlinear function $\sigma(.)$ is commonly a rectified linear unit
    (ReLU) for each element,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 带有偏置项 $b^{l}_{j}$ 和卷积在 $N^{l-1}$ 个输入特征图 $x^{l-1}_{i}$ 与匹配的卷积核 $w^{l}_{i,j}$
    之间。对于每个元素，逐元素的非线性函数 $\sigma(.)$ 通常是每个元素的修正线性单元（ReLU），
- en: '|  | $\begin{split}\sigma(x)=max\{x,0\}\end{split}$ |  | (3) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\sigma(x)=max\{x,0\}\end{split}$ |  | (3) |'
- en: Finally, pooling is the process of downsampling and upsampling feature maps.
    Deep convolution neural networks(DCNNs) are CNNs with a large number of layers,
    often known as ”deep” networks . A CNN’s most basic layers consist of a series
    of feature maps, each of which operates as a neuron. A set of weights $w_{i,j}$
    connects each neuron in a convolutional layer to feature maps from the preceding
    layer (essentially a set of 2D filters). Whereas convolutional and pooling layers
    make up the early CNN layers, the subsequent layers are usually completely connected.
    The input picture is repeatedly convolved from earlier to later layers, and the
    receptive field or region of support grows with each layer. In general, the first
    CNN layers extract low-level characteristics (such as edges), whereas subsequent
    layers extract more generic features of increasing complexity. [[35](#bib.bib35),
    [46](#bib.bib46), [47](#bib.bib47), [36](#bib.bib36)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，池化是对特征图进行下采样和上采样的过程。深度卷积神经网络（DCNNs）是具有大量层的 CNN，通常被称为“深层”网络。CNN 的最基本层包括一系列特征图，每个特征图作为一个神经元。每个卷积层中的神经元通过一组权重
    $w_{i,j}$ 连接到前一层的特征图（本质上是一组 2D 滤波器）。尽管卷积层和池化层构成了早期的 CNN 层，但随后的层通常是完全连接的。输入图片从早期层到后期层不断进行卷积，每一层的感受野或支持区域随着层数的增加而增长。一般来说，前几层
    CNN 提取低级特征（如边缘），而后续层提取更加通用的复杂特征。[[35](#bib.bib35), [46](#bib.bib46), [47](#bib.bib47),
    [36](#bib.bib36)]
- en: DCNNs have a hierarchical structure that allows them to learn data representations
    at numerous levels of abstraction, the ability to learn highly complicated functions,
    and the ability to learn feature representations directly and automatically from
    data with minimum domain expertise. The availability of huge size labeled datasets
    and GPUs with extremely high computational capabilities is what has made DCNNs
    so successful.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DCNN（深度卷积神经网络）具有分层结构，使其能够在多个抽象层次上学习数据表示，学习高度复杂的函数，并能够直接从数据中自动学习特征表示，且对领域专业知识的需求最低。大量标注数据集和具有极高计算能力的
    GPU 的可用性是 DCNN 成功的原因之一。
- en: Despite the enormous achievements, there are still acknowledged flaws. There
    is a critical need for labeled training data as well as expensive computational
    resources, and selecting proper learning parameters and network designs still
    requires substantial expertise and experience. Trained networks are difficult
    to comprehend, lack resistance to degradations and many DCNNs have been proven
    to be vulnerable to assaults [[37](#bib.bib37)], all of which restrict their applicability
    in real-world applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了巨大的成就，但仍存在已知的缺陷。对标注训练数据和昂贵计算资源的需求非常迫切，选择合适的学习参数和网络设计仍然需要大量的专业知识和经验。训练后的网络难以理解，缺乏对退化的抵抗力，许多
    DCNN 已被证明对攻击脆弱[[37](#bib.bib37)]，这些都限制了其在实际应用中的适用性。
- en: 4 Datasets and Evaluation Metrics
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集与评估指标
- en: 4.1 Datasets
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: This section will describe datasets that are available and have been most commonly
    used for table detection, table structure recognition, and classification tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述可用的数据集及其在表格检测、表格结构识别和分类任务中最常用的数据集。
- en: 4.1.1 ICDAR 2013
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 ICDAR 2013
- en: 'ICDAR2013 dataset[[48](#bib.bib48)] referred to in served as the competition’s
    official practice dataset. Rather than focusing on a specific subset of documents,
    authors have always intended to evaluate systems as broadly as possible, and this
    dataset, as well as the actual competition dataset, were generated by systematically
    collecting PDFs from a Google search in order to make the selection as objective
    as possible. they are limited to two governmental sources with the additional
    search terms site:europa.eu and site:*.gov in order to obtain documents whose
    publications are known to be in the public domain. The ICDAR2013 dataset contains
    150 tables, 75 of which are in 27 EU excerpts and 75 of which are in 40 US Government
    excerpts. Table regions are rectangular areas of a page whose coordinates define
    them. Multiple regions can be included in the same table because a table can span
    multiple pages. Table detection or location and table structure recognition are
    the two sub-tasks of ICDAR2013\. The task of table structure recognition compares
    methods for determining table cell structure given precise location information.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.1.3 ICDAR2019 ‣ 4.1 Datasets ‣ 4 Datasets and
    Evaluation Metrics ‣ Deep learning for table detection and structure recognition:
    A survey") presents a few examples from this dataset.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'ICDAR2013 数据集[[48](#bib.bib48)]被用作竞赛的官方练习数据集。作者一直打算尽可能广泛地评估系统，而不是专注于特定子集的文档。该数据集和实际竞赛数据集都是通过系统地从
    Google 搜索中收集 PDF 文件生成的，以使选择尽可能客观。它们限于两个政府来源，附加搜索条件为 site:europa.eu 和 site:*.gov，以获取已知为公共领域的文档。ICDAR2013
    数据集包含 150 个表格，其中 75 个来自 27 个 EU 摘录，75 个来自 40 个美国政府摘录。表格区域是页面上由坐标定义的矩形区域。一个表格可以跨多个页面，因此一个表格可以包含多个区域。表格检测或定位和表格结构识别是
    ICDAR2013 的两个子任务。表格结构识别的任务是比较给定精确位置信息的情况下，确定表格单元格结构的方法。图 [3](#S4.F3 "Figure 3
    ‣ 4.1.3 ICDAR2019 ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning
    for table detection and structure recognition: A survey") 展示了该数据集的一些示例。'
- en: 4.1.2 ICDAR 2017 POD
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 ICDAR 2017 POD
- en: 'Dataset [[49](#bib.bib49)] was published for the ICDAR2017 Page Object Detection
    (POD) competition. This dataset is frequently used to test different methods of
    table detection. Compared to the ICDAR2013 table dataset, this dataset is significantly
    larger. There are 2417 total images in it, including figures, tables, and formulae.
    The dataset is commonly split into 1600 photos which have 731 tabular areas for
    training, and the remaining 817 images which have 350 tabular regions for testing.
    Figure [4](#S4.F4 "Figure 4 ‣ 4.1.3 ICDAR2019 ‣ 4.1 Datasets ‣ 4 Datasets and
    Evaluation Metrics ‣ Deep learning for table detection and structure recognition:
    A survey") illustrates two examples of this dataset.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 [[49](#bib.bib49)] 为 ICDAR2017 页面对象检测（POD）竞赛发布。该数据集经常用于测试不同的表格检测方法。与 ICDAR2013
    表格数据集相比，该数据集显著更大。它总共有 2417 张图像，包括图形、表格和公式。该数据集通常被拆分为 1600 张图像，其中有 731 个表格区域用于训练，其余
    817 张图像包含 350 个表格区域用于测试。图 [4](#S4.F4 "图 4 ‣ 4.1.3 ICDAR2019 ‣ 4.1 数据集 ‣ 4 数据集和评估指标
    ‣ 表格检测和结构识别的深度学习：综述") 展示了该数据集的两个示例。
- en: 4.1.3 ICDAR2019
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 ICDAR2019
- en: 'ICDAR2019 [[50](#bib.bib50)] proposed a dataset for table detection (TRACK
    A) and table recognition (TRACK B). The dataset is divided into two types, historical
    and modern datasets. It contains 1600 images for training and 839 images for testing.
    The historical type contains 1200 images in tracks A and B for training and 499
    images for testing. The modern type contains 600 images in tracks A and B for
    training and 340 images for testing. Document images containing one or more tables
    are provided for TRACK A. TRACK B has two sub-tracks: the first (B.1) provides
    the table region, and only the table structure recognition is required. The second
    sub-track (B.2) contains no prior knowledge. That is, both table region detection
    and table structure recognition must be performed. For the annotation of the dataset,
    a similar notation was derived from the ICDAR 2013 [[48](#bib.bib48)] Table Competition
    format, and the structures were stored in a single XML file. Each table element
    corresponds to a table with a single Coords element with a points attribute indicating
    the coordinates of the bounding polygon with N vertices. Each table element contains
    a list of cell elements as well. The attributes start-row, start-col, end-row,
    and end-col denote the position of each cell element in the table. The cell element’s
    Coords denote the coordinates of the bounding polygon of this cell box, and the
    content is the text within this cell. Figure [5](#S4.F5 "Figure 5 ‣ 4.1.3 ICDAR2019
    ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for table detection
    and structure recognition: A survey") presents a few examples from this dataset.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ICDAR2019 [[50](#bib.bib50)] 提出了一个用于表格检测（TRACK A）和表格识别（TRACK B）的数据集。该数据集分为历史数据集和现代数据集两种类型。它包含
    1600 张训练图像和 839 张测试图像。历史类型包含 1200 张 A 跟 B 路径的训练图像和 499 张测试图像。现代类型包含 600 张 A 跟
    B 路径的训练图像和 340 张测试图像。TRACK A 提供了包含一个或多个表格的文档图像。TRACK B 有两个子任务：第一个子任务（B.1）提供表格区域，仅需进行表格结构识别。第二个子任务（B.2）不包含任何先验知识，即需要进行表格区域检测和表格结构识别。数据集的标注采用了类似
    ICDAR 2013 [[48](#bib.bib48)] 表格竞赛格式的标注方式，结构存储在一个 XML 文件中。每个表格元素对应一个带有 Coords
    元素的表格，该元素的 points 属性指示具有 N 个顶点的边界多边形的坐标。每个表格元素还包含一个单元格元素列表。属性 start-row、start-col、end-row
    和 end-col 表示每个单元格元素在表格中的位置。单元格元素的 Coords 表示该单元格框的边界多边形的坐标，content 为该单元格内的文本。图
    [5](#S4.F5 "图 5 ‣ 4.1.3 ICDAR2019 ‣ 4.1 数据集 ‣ 4 数据集和评估指标 ‣ 表格检测和结构识别的深度学习：综述")
    展示了来自该数据集的一些示例。
- en: '![Refer to caption](img/7b7189a238ace57adc80e5fb158954ed.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7b7189a238ace57adc80e5fb158954ed.png)'
- en: (a)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/7676d73a8797384092d91a9945c317e7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7676d73a8797384092d91a9945c317e7.png)'
- en: (b)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 3: Examples of images in ICDAR 2013'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: ICDAR 2013 图像示例'
- en: '![Refer to caption](img/fc293e1c2155f90d8022057ec18fc712.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fc293e1c2155f90d8022057ec18fc712.png)'
- en: (a)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/8c8e6b667148daa915de451f259c2d31.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8c8e6b667148daa915de451f259c2d31.png)'
- en: (b)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 4: Examples of images in ICDAR 2017'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: ICDAR 2017 图像示例'
- en: '![Refer to caption](img/9cb3f217bcaa95833bec4f9bb9fb468f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9cb3f217bcaa95833bec4f9bb9fb468f.png)'
- en: (a)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/4973d34a56a732dd0fe67f37a528799e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4973d34a56a732dd0fe67f37a528799e.png)'
- en: (b)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 5: Examples of images in ICDAR 2019'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: ICDAR 2019 图像示例'
- en: 4.1.4 TabStructDB
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 TabStructDB
- en: 'TabStructDB is a different publicly available image-based table structure recognition
    dataset that was promoted by SA Siddiqui [[51](#bib.bib51)]. The well-known ICDAR
    2017 page object detection dataset, which contains pictures annotated with structural
    details, was used to curate this dataset. Figure [6](#S4.F6 "Figure 6 ‣ 4.1.4
    TabStructDB ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning
    for table detection and structure recognition: A survey") illustrates two examples
    of this dataset.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TabStructDB 是一个不同的公开图像基础表结构识别数据集，由 SA Siddiqui 推广[[51](#bib.bib51)]。这个著名的 ICDAR
    2017 页面对象检测数据集包含带有结构细节的标注图片，用于策划该数据集。图 [6](#S4.F6 "图 6 ‣ 4.1.4 TabStructDB ‣ 4.1
    数据集 ‣ 4 数据集与评估指标 ‣ 表格检测与结构识别的深度学习：综述") 展示了该数据集的两个示例。
- en: '![Refer to caption](img/939ed2f4fca79ac0c6c2f91940bf6b5e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/939ed2f4fca79ac0c6c2f91940bf6b5e.png)'
- en: (a)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/516c62af12d88c58aaa38bcf122790cd.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/516c62af12d88c58aaa38bcf122790cd.png)'
- en: (b)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 6: Examples of images in TabStructDB'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：TabStructDB 中的图像示例
- en: 4.1.5 TABLE2LATEX-450K
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 TABLE2LATEX-450K
- en: 'TABLE2LATEX-450K [[52](#bib.bib52)] is another sizable dataset that was released
    at the most recent ICDAR conference. The dataset includes 450,000 annotated tables
    and the associated pictures. This enormous dataset was created by crawling through
    all of the LaTeX source documents and ArXiv publications from 1991 to 2016\. The
    high-quality labeled dataset is obtained via source code extraction and further
    refining. Figure [7](#S4.F7 "Figure 7 ‣ 4.1.5 TABLE2LATEX-450K ‣ 4.1 Datasets
    ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for table detection and structure
    recognition: A survey") presents a few examples from this dataset.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: TABLE2LATEX-450K [[52](#bib.bib52)] 是另一个大规模数据集，在最近的 ICDAR 会议上发布。该数据集包括 450,000
    张标注的表格和相关图片。这个庞大的数据集是通过爬取 1991 年至 2016 年的所有 LaTeX 源文档和 ArXiv 论文创建的。高质量的标注数据集通过源代码提取和进一步的精炼获得。图
    [7](#S4.F7 "图 7 ‣ 4.1.5 TABLE2LATEX-450K ‣ 4.1 数据集 ‣ 4 数据集与评估指标 ‣ 表格检测与结构识别的深度学习：综述")
    展示了该数据集的一些示例。
- en: '![Refer to caption](img/ca6451c9460604591f7b9a1d910d0e25.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca6451c9460604591f7b9a1d910d0e25.png)'
- en: (a)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/cfddcf5c658e3ab5f3f45e79eb5133e4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cfddcf5c658e3ab5f3f45e79eb5133e4.png)'
- en: (b)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 7: Examples of images in TABLE2LATEX-450K'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：TABLE2LATEX-450K 中的图像示例
- en: 4.1.6 RVL-CDIP (SUBSET)
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 RVL-CDIP（子集）
- en: 'A well-known dataset in the world of document analysis is the RVL-CDIP (Ryerson
    Vision Lab Complex Document Information Processing) [[53](#bib.bib53)]. 400,000
    photos are included, evenly spread over 16 classifications. For the purpose of
    detecting tables, P Riba [[54](#bib.bib54)] makes subset dataset by annotating
    the 518 invoices in the RVL-CDIP dataset. The dataset has been made available
    to the general public. For testing table identification methods especially created
    for invoice document pictures, this subset of the real RVL-CDIP dataset [[53](#bib.bib53)]
    is a significant contribution. Figure [8](#S4.F8 "Figure 8 ‣ 4.1.6 RVL-CDIP (SUBSET)
    ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for table detection
    and structure recognition: A survey") presents a few examples from this dataset.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档分析领域，著名的数据集是 RVL-CDIP（Ryerson Vision Lab 复杂文档信息处理）[[53](#bib.bib53)]。该数据集包含
    400,000 张照片，均匀分布在 16 个类别中。为了检测表格，P Riba [[54](#bib.bib54)] 通过对 RVL-CDIP 数据集中的
    518 张发票进行标注，制作了子集数据集。该数据集已向公众开放。为了测试专门为发票文档图片创建的表格识别方法，这个真实的 RVL-CDIP 数据集子集[[53](#bib.bib53)]
    是一个重要贡献。图 [8](#S4.F8 "图 8 ‣ 4.1.6 RVL-CDIP（子集） ‣ 4.1 数据集 ‣ 4 数据集与评估指标 ‣ 表格检测与结构识别的深度学习：综述")
    展示了该数据集的一些示例。
- en: '![Refer to caption](img/77d65bbdd2cece5fd5e0562430b4b539.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/77d65bbdd2cece5fd5e0562430b4b539.png)'
- en: (a)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/9d2dd51a76fe4f6466b6877f38b0f639.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d2dd51a76fe4f6466b6877f38b0f639.png)'
- en: (b)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 8: Examples of images in RVL-CDIP (SUBSET)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：RVL-CDIP（子集）中的图像示例
- en: 4.1.7 IIIT-AR-13K
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.7 IIIT-AR-13K
- en: 'IIT-AR-13K is a brand-new dataset that is introduced by A Mondal [[55](#bib.bib55)].
    The yearly reports that were publicly available and written in English and other
    languages were collected to create this dataset. The biggest manually annotated
    dataset for the problem of graphical page object recognition, according to the
    authors, has been released. Annotations for figures, natural imagery, logos, and
    signatures are included in the dataset in addition to the tables. For numerous
    tasks of page object detection, the authors have included the train, validation,
    and test splits. In order to train for table detection, 11,000 samples are employed,
    while 2000 and 3000 samples are allotted for validation and testing, respectively.
    Figure [9](#S4.F9 "Figure 9 ‣ 4.1.7 IIIT-AR-13K ‣ 4.1 Datasets ‣ 4 Datasets and
    Evaluation Metrics ‣ Deep learning for table detection and structure recognition:
    A survey") illustrates two examples of this dataset.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: IIT-AR-13K是由A Mondal [[55](#bib.bib55)] 提出的全新数据集。为了创建这个数据集，收集了公开的英文及其他语言的年度报告。根据作者的说法，已发布了针对图形页面对象识别问题的最大人工标注数据集。除了表格外，数据集还包括图形、自然图像、标志和签名。为了处理多个页面对象检测任务，作者提供了训练、验证和测试数据集。为了进行表格检测训练，使用了11,000个样本，而为验证和测试分别分配了2000和3000个样本。图[9](#S4.F9
    "图9 ‣ 4.1.7 IIIT-AR-13K ‣ 4.1 数据集 ‣ 4 数据集与评估指标 ‣ 表格检测和结构识别的深度学习：综述")展示了该数据集的两个示例。
- en: '![Refer to caption](img/3c683ed6ca7f1855551573b74e500070.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3c683ed6ca7f1855551573b74e500070.png)'
- en: (a)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/71f9ce5ecfb8a39251f9128d47d4c209.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/71f9ce5ecfb8a39251f9128d47d4c209.png)'
- en: (b)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 9: Examples of images in IIIT-AR-13K'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：IIIT-AR-13K中的图像示例
- en: 4.1.8 CamCap
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.8 CamCap
- en: 'CamCap is a collection of camera-captured photos suggested by W Seo [[56](#bib.bib56)].
    Only 85 photos are present (38 tables on curved surfaces having 1295 cells and
    47 tables on planar surfaces consisting of 1162 cells). For the sake of detecting
    tables and identifying their structures, the suggested dataset is accessible to
    the general public. This dataset is a significant addition to evaluating the reliability
    of table identification techniques on camera-captured document pictures. Figure
    [10](#S4.F10 "Figure 10 ‣ 4.1.8 CamCap ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation
    Metrics ‣ Deep learning for table detection and structure recognition: A survey")
    illustrates two examples of this dataset.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: CamCap是W Seo [[56](#bib.bib56)] 推荐的相机拍摄照片集合。该数据集中仅包含85张照片（38张在曲面上的表格，共1295个单元格，以及47张在平面上的表格，共1162个单元格）。为了检测表格及其结构，该数据集对公众开放。这个数据集对评估相机拍摄文档图片的表格识别技术的可靠性是一个重要的补充。图[10](#S4.F10
    "图10 ‣ 4.1.8 CamCap ‣ 4.1 数据集 ‣ 4 数据集与评估指标 ‣ 表格检测和结构识别的深度学习：综述")展示了该数据集的两个示例。
- en: '![Refer to caption](img/196079dc43c715f154fc6802a183ebcb.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/196079dc43c715f154fc6802a183ebcb.png)'
- en: (a)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/b5f73e16dea6d341b0de30b0c08917f3.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b5f73e16dea6d341b0de30b0c08917f3.png)'
- en: (b)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 10: Examples of images in CamCap'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：CamCap中的图像示例
- en: 4.1.9 UNLV Table
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.9 UNLV 表格
- en: 'The UNLV Table dataset[[57](#bib.bib57)] includes 2889 pages of scanned document
    images gathered from various sources (magazines, newspapers, business letters,
    annual reports, etc). Scanned images are available in bitonal, grayscale, and
    fax formats, with resolutions ranging from 200 to 300 DPI. There is ground truth
    data in addition to the original dataset, which contains manually marked zones;
    zone types are provided in text format. Figure [11](#S4.F11 "Figure 11 ‣ 4.1.10
    UW-3 Table ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning
    for table detection and structure recognition: A survey") illustrates two examples
    of this dataset.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: UNLV 表格数据集[[57](#bib.bib57)] 包括来自各种来源（杂志、报纸、商业信函、年度报告等）的2889页扫描文档图像。扫描图像提供了二值图、灰度图和传真格式，分辨率从200到300
    DPI不等。数据集中除了原始图像外，还包含了人工标记的区域；区域类型以文本格式提供。图[11](#S4.F11 "图11 ‣ 4.1.10 UW-3 表格
    ‣ 4.1 数据集 ‣ 4 数据集与评估指标 ‣ 表格检测和结构识别的深度学习：综述")展示了该数据集的两个示例。
- en: 4.1.10 UW-3 Table
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.10 UW-3 表格
- en: 'The UW-3 Table dataset [[58](#bib.bib58)] contains 1600 skew-corrected English
    document images with manually edited entity bounding box ground-truth. Page frames,
    text and non-text zones, text lines, and words are all surrounded by these bounding
    boxes. Each zone’s type (text, math, table, half-tone, etc.) is also indicated.
    There are approximately 120 document images with at least one marked table zone.
    The UNLV and UW-3 Table dataset taught a user how to use the T-Truth tool and
    asked him to prepare ground truth for the target images in the above dataset.
    Each image’s ground truth is stored in an XML. Another expert manually validated
    the ground truths using the T-Truth tool’s preview edit mode, and incorrect ground
    truths were corrected. These iterations were repeated several times to ensure
    that the ground truth was correct. authors discovered that the majority of errors
    occur when labeling column spanning cells where the column boundaries coincide
    with the word boundaries. Problems can also arise when there are multiple interpretations
    of a table structure, as described by Nagy[[59](#bib.bib59)], and domain knowledge
    is required to correctly label the table structure. Figure [12](#S4.F12 "Figure
    12 ‣ 4.1.10 UW-3 Table ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep
    learning for table detection and structure recognition: A survey") illustrates
    two examples of this dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: UW-3 Table 数据集 [[58](#bib.bib58)] 包含1600张经过倾斜修正的英文文档图像，并且每张图像的实体边界框是真实的手工编辑结果。这些边界框围绕了页面框架、文本和非文本区域、文本行和单词。每个区域的类型（文本、数学、表格、半色调等）也有标注。大约有120张文档图像标记了至少一个表格区域。UNLV
    和 UW-3 Table 数据集教会用户如何使用 T-Truth 工具，并要求他为上述数据集中的目标图像准备真实的标注。每张图像的真实标注都存储在一个 XML
    文件中。另一位专家使用 T-Truth 工具的预览编辑模式手动验证了这些真实标注，并对错误的标注进行了纠正。这些迭代过程重复进行了几次，以确保真实标注的准确性。作者发现，大多数错误发生在标记列跨越单元格时，当列边界与单词边界重合时。问题还可能出现在表格结构的多种解释中，如
    Nagy[[59](#bib.bib59)] 所述，需要领域知识来正确标记表格结构。图 [12](#S4.F12 "图 12 ‣ 4.1.10 UW-3 Table
    ‣ 4.1 数据集 ‣ 4 数据集与评估指标 ‣ 表格检测与结构识别的深度学习：综述") 展示了该数据集的两个示例。
- en: '![Refer to caption](img/01e98dd57c9b790b605b59a80228fd8c.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/01e98dd57c9b790b605b59a80228fd8c.png)'
- en: (a)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/9c38335eea5e3da9edcc4cdcc3a8e709.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c38335eea5e3da9edcc4cdcc3a8e709.png)'
- en: (b)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 11: Examples of images in UNLV'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：UNLV 的图像示例
- en: '![Refer to caption](img/4353996f855894ec14580ede23c5fc2b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4353996f855894ec14580ede23c5fc2b.png)'
- en: (a)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/6bad6307022c26fb3a7127bf3146a6ab.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6bad6307022c26fb3a7127bf3146a6ab.png)'
- en: (b)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 12: Examples of images in UW3'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：UW3 的图像示例
- en: 4.1.11 Marmot
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.11 Marmot
- en: 'The Marmot dataset[[60](#bib.bib60)] is considered the first large dataset
    in table detection. it contains 2000 PDF pages with ground truth data. This labeling
    task was completed by 15 people. To reduce subjectivity, a unified labeling standard
    was established, and each ground-truth file is double-checked. The dataset’s size
    is still growing. The e-document pages in the dataset have a wide range of language
    types, page layouts, and table styles. First, it is made up of roughly equal parts
    Chinese and English pages. The Chinese pages were chosen from over 120 e-Books
    with diverse subject areas from Founder Apabi’s digital library, with no more
    than 15 pages chosen from each book. The English pages were retrieved from the
    web. Over 1500 conference and journal papers from 1970 to 2011 were crawled, covering
    a wide range of topics. The Chinese e-Book pages are mostly one column, whereas
    the English pages are printed in both one and two columns. This dataset includes
    a wide range of table types, from ruled tables to partially and non-ruled tables,
    horizontal tables to vertical tables, inside-column tables to span-column tables,
    and so on. A few samples from this dataset are shown in Figure [13](#S4.F13 "Figure
    13 ‣ 4.1.11 Marmot ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning
    for table detection and structure recognition: A survey").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'Marmot 数据集[[60](#bib.bib60)] 被认为是表格检测领域的第一个大型数据集。它包含 2000 页 PDF 页面，附有真实数据。这项标注任务由
    15 人完成。为了减少主观性，建立了统一的标注标准，并且每个真实数据文件都经过双重检查。数据集的规模仍在增长。数据集中的电子文档页面有着广泛的语言类型、页面布局和表格样式。首先，它由大致相等数量的中文和英文页面组成。中文页面从超过
    120 本来自方正 Apabi 数字图书馆的电子书中选择，每本书中选择不超过 15 页。英文页面则从网络上检索。从 1970 年到 2011 年的超过 1500
    篇会议论文和期刊论文被抓取，涵盖了广泛的主题。中文电子书页面大多为单列，而英文页面则有单列和双列两种格式。该数据集包含了各种类型的表格，从有规则的表格到部分规则和无规则的表格，从横向表格到纵向表格，从内部列表格到跨列表格等。该数据集的几个样本如图
    [13](#S4.F13 "Figure 13 ‣ 4.1.11 Marmot ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation
    Metrics ‣ Deep learning for table detection and structure recognition: A survey")
    所示。'
- en: '![Refer to caption](img/9ed05011b4c404f0a1145ed8289cb374.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ed05011b4c404f0a1145ed8289cb374.png)'
- en: (a)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/576df381cb848c2abe1fb129c9d6d16c.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/576df381cb848c2abe1fb129c9d6d16c.png)'
- en: (b)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 13: Examples of images in Marmot'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: Marmot 数据集中的图像示例'
- en: 4.1.12 TableBank
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.12 TableBank
- en: 'The TableBank [[61](#bib.bib61)] proposed a novel weak supervision approach
    for automatically creating the dataset, which is orders of magnitude larger than
    existing human-labeled datasets for table analysis. Unlike the traditional weakly
    supervised training set, this approach can generate not only large amounts of
    but also high-quality training data. There are many electronic documents available
    on the internet nowadays, such as Microsoft Word (.docx) and Latex (.tex) files.
    By definition, these online documents contain mark-up tags for tables in their
    source code. Intuitively, these source codes manipulate by adding bounding boxes
    within each document using the mark-up language. The Office XML code for Word
    documents can be modified to identify the borderline of each table. The code for
    Latex documents can also be modified to recognize table bounding boxes. This method
    generates high-quality labeled data for a wide range of domains, including business
    documents, official filings, research papers, and so on, which is extremely useful
    for large-scale table analysis tasks. The TableBank dataset is made up of 417,234
    high-quality labeled tables and their original documents from a variety of domains.
    A few samples from this dataset are shown in Figure [14](#S4.F14 "Figure 14 ‣
    4.1.12 TableBank ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning
    for table detection and structure recognition: A survey").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'TableBank [[61](#bib.bib61)] 提出了一个新颖的弱监督方法用于自动创建数据集，该数据集的规模比现有的人类标注数据集要大几个数量级。与传统的弱监督训练集不同，这种方法不仅能够生成大量数据，还能生成高质量的训练数据。如今互联网上有许多电子文档，例如
    Microsoft Word (.docx) 和 Latex (.tex) 文件。这些在线文档在其源代码中定义了表格的标记标签。直观上，这些源代码通过在每个文档中使用标记语言添加边界框来进行操作。Word
    文档的 Office XML 代码可以被修改，以识别每个表格的边界。Latex 文档的代码也可以被修改以识别表格的边界框。这种方法为各种领域生成了高质量的标注数据，包括商业文档、官方文件、研究论文等，这对大规模的表格分析任务非常有用。TableBank
    数据集由 417,234 个高质量的标注表格及其来自各种领域的原始文档组成。该数据集的几个样本如图 [14](#S4.F14 "Figure 14 ‣ 4.1.12
    TableBank ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for
    table detection and structure recognition: A survey") 所示。'
- en: '![Refer to caption](img/74bc13eafd2089fb5e9d75c25f232e7f.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/74bc13eafd2089fb5e9d75c25f232e7f.png)'
- en: (a)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/1007985785edbb3b2013c4abeb7a4277.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1007985785edbb3b2013c4abeb7a4277.png)'
- en: (b)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 14: Examples of images in TableBank'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: TableBank 中的图像示例'
- en: 4.1.13 DeepFigures
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.13 DeepFigures
- en: 'DeepFigures [[62](#bib.bib62)] uses no human assistance to generate high-quality
    training labels for the task of figure extraction in a huge number of scientific
    papers. authors do this by locating figures and captions in the rasterized PDF
    using supplementary data from two big web collections of scientific articles (PubMed
    and arXiv). authors provide the resulting dataset of approximately 5.5 million
    tables and figures induced labels to facilitate the development of modern data-driven
    approaches for this task, which is 4,000 times larger than the previous largest
    figure extraction dataset and has an average precision of 96.8%. Samples from
    this dataset are shown in Figure [15](#S4.F15 "Figure 15 ‣ 4.1.13 DeepFigures
    ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for table detection
    and structure recognition: A survey").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepFigures [[62](#bib.bib62)] 在大量科学论文中生成高质量的训练标签，无需人工辅助。作者通过利用来自两个大型科学文章网络收藏（PubMed
    和 arXiv）的补充数据，定位光栅化PDF中的图形和标题。作者提供了大约550万张表格和图形标注的结果数据集，以促进现代数据驱动方法的发展，这个数据集比之前最大的图形提取数据集大4000倍，平均精度为96.8%。图
    [15](#S4.F15 "Figure 15 ‣ 4.1.13 DeepFigures ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation
    Metrics ‣ Deep learning for table detection and structure recognition: A survey")
    显示了该数据集的一些样本。'
- en: '![Refer to caption](img/8f455e85e83257cfd95bd4565d5aefe9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f455e85e83257cfd95bd4565d5aefe9.png)'
- en: 'Figure 15: Examples of images in DeepFigures'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: DeepFigures 中的图像示例'
- en: 4.1.14 PubTables-1M
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.14 PubTables-1M
- en: 'PubTables-1M [[63](#bib.bib63)] contains nearly one million tables from scientific
    articles, supports multiple input modalities and contains detailed header and
    location information for table structures, making it useful for a wide variety
    of modeling approaches. It also addresses a significant source of ground truth
    inconsistency observed in prior datasets called over-segmentation, using a novel
    canonicalization procedure. We demonstrate that these improvements lead to a significant
    increase in training performance and a more reliable estimate of model performance
    at evaluation for table structure recognition. Further, authors show that transformer-based
    object detection models trained on PubTables-1M produce excellent results for
    all three tasks of detection, structure recognition, and functional analysis without
    the need for any special customization for these tasks. Figure [16](#S4.F16 "Figure
    16 ‣ 4.1.14 PubTables-1M ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣
    Deep learning for table detection and structure recognition: A survey") illustrates
    two examples of this dataset.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'PubTables-1M [[63](#bib.bib63)] 包含了来自科学文章的近一百万个表格，支持多种输入模式，并且包含了详细的表格结构标题和位置信息，使其适用于多种建模方法。它还解决了以前数据集中观察到的一种主要地面真实不一致来源——过度分割，采用了一种新颖的标准化程序。我们展示了这些改进显著提升了训练性能，并且在评估时提供了更可靠的模型性能估计。此外，作者表明，基于变压器的目标检测模型在
    PubTables-1M 上训练，能够在检测、结构识别和功能分析三个任务中均取得优秀的结果，无需对这些任务进行特殊定制。图 [16](#S4.F16 "Figure
    16 ‣ 4.1.14 PubTables-1M ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣
    Deep learning for table detection and structure recognition: A survey") 展示了该数据集的两个示例。'
- en: '![Refer to caption](img/697e7dcf0a96a908c89aac0bd7743323.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/697e7dcf0a96a908c89aac0bd7743323.png)'
- en: (a)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/0642efbbc8578e2e3c331e79c47f648c.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0642efbbc8578e2e3c331e79c47f648c.png)'
- en: (b)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 16: Examples of images in PubTables-1M'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: PubTables-1M 中的图像示例'
- en: 4.1.15 SciTSR
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.15 SciTSR
- en: 'SciTSR [[64](#bib.bib64)] presents a large-scale table structure recognition
    dataset derived from scientific articles that comprise 15,000 tables from PDF
    files and their related structural labels. Figure [17](#S4.F17 "Figure 17 ‣ 4.1.15
    SciTSR ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for
    table detection and structure recognition: A survey") illustrates two examples
    of this dataset.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'SciTSR [[64](#bib.bib64)] 提供了一个大规模的表格结构识别数据集，来源于科学文章，包含来自PDF文件的15,000个表格及其相关的结构标签。图
    [17](#S4.F17 "Figure 17 ‣ 4.1.15 SciTSR ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation
    Metrics ‣ Deep learning for table detection and structure recognition: A survey")
    展示了该数据集的两个示例。'
- en: '![Refer to caption](img/66b0f34695a14c1543272052f80e536b.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66b0f34695a14c1543272052f80e536b.png)'
- en: (a)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/45883727f356c716ed3ed8d8a968eeef.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/45883727f356c716ed3ed8d8a968eeef.png)'
- en: (b)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 17: Examples of images in SciTSR'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：SciTSR中的图像示例
- en: 4.1.16 FinTabNet
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.16 FinTabNet
- en: 'FinTabNet [[65](#bib.bib65)] introduces GTE, a vision-guided systematic framework
    for combined table detection and cell structured identification that can be constructed
    on top of any object detection model. Create a new penalty based on the natural
    cell containment constraint of tables with GTE-Table to train their table network
    with the help of cell location predictions. GTE-Cell is a novel hierarchical cell
    detection network that uses table layouts to detect cells. Build a technique for
    automatically labeling table and cell structures in existing texts to create a
    huge corpus of training and test data for a low cost. FinTabNet is a collection
    of real-world and complicated scientific and financial datasets with thorough
    table structure annotations to aid in structure identification training and testing.
    Figure [18](#S4.F18 "Figure 18 ‣ 4.1.16 FinTabNet ‣ 4.1 Datasets ‣ 4 Datasets
    and Evaluation Metrics ‣ Deep learning for table detection and structure recognition:
    A survey") illustrates two examples of this dataset.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: FinTabNet [[65](#bib.bib65)] 引入了GTE，这是一种基于视觉的系统框架，用于结合表格检测和单元格结构识别，可以在任何目标检测模型的基础上构建。利用GTE-Table创建一个新的惩罚机制，基于表格的自然单元格包含约束，利用单元格位置预测来训练其表格网络。GTE-Cell
    是一种新颖的分层单元格检测网络，利用表格布局来检测单元格。建立一种技术，用于在现有文本中自动标注表格和单元格结构，从而以低成本创建大量训练和测试数据集。FinTabNet
    是一个包含现实世界和复杂的科学及金融数据集的集合，具有详尽的表格结构注释，以帮助结构识别的训练和测试。图 [18](#S4.F18 "图 18 ‣ 4.1.16
    FinTabNet ‣ 4.1 数据集 ‣ 4 数据集和评估指标 ‣ 深度学习用于表格检测和结构识别：综述") 展示了该数据集的两个示例。
- en: '![Refer to caption](img/34f77bb099dd8cbb347e0d210ec966cb.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/34f77bb099dd8cbb347e0d210ec966cb.png)'
- en: (a)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/8ca1bafdd06a049a452ccc9b8de7eb33.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8ca1bafdd06a049a452ccc9b8de7eb33.png)'
- en: (b)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 18: Examples of images in FinTabNet'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：FinTabNet中的图像示例
- en: 4.1.17 PubTabNet
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.17 PubTabNet
- en: 'PubTabNet [[66](#bib.bib66)] one of the biggest openly accessible table recognition
    collection, including 568k table pictures and structured HTML representations.
    PubTabNet is built automatically by comparing the XML and PDF formats of scientific
    publications in the PubMed CentralTM Open Access Subset (PMCOA). authors also
    suggest an attention-based encoder-dual-decoder (EDD) architecture for converting
    table graphics to HTML code. A structure decoder is included in the model, which
    reconstructs the table structure and assists the cell decoder in recognizing cell
    content. Furthermore, authors also propose a new Tree-Edit-Distance-based Similarity
    (TEDS) metric for table recognition that better captures multi-hop cell misalignment
    and OCR errors than the existing metric. Figure [19](#S4.F19 "Figure 19 ‣ 4.1.17
    PubTabNet ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for
    table detection and structure recognition: A survey") illustrates two examples
    of this dataset.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: PubTabNet [[66](#bib.bib66)] 是最大的公开可用表格识别集合之一，包括568k张表格图片和结构化的HTML表示。PubTabNet
    是通过比较PubMed CentralTM开放获取子集（PMCOA）中的科学出版物的XML和PDF格式自动构建的。作者还建议了一种基于注意力的编码器-双解码器（EDD）架构，用于将表格图形转换为HTML代码。模型中包含一个结构解码器，用于重建表格结构，并帮助单元格解码器识别单元格内容。此外，作者还提出了一种基于树编辑距离的相似度（TEDS）度量，它比现有度量更好地捕捉多跳单元格对齐误差和OCR错误。图
    [19](#S4.F19 "图 19 ‣ 4.1.17 PubTabNet ‣ 4.1 数据集 ‣ 4 数据集和评估指标 ‣ 深度学习用于表格检测和结构识别：综述")
    展示了该数据集的两个示例。
- en: '![Refer to caption](img/370945a05c0ca2202c529b0a0bacae63.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/370945a05c0ca2202c529b0a0bacae63.png)'
- en: (a)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/7ffa0440d2a5e3c3498c32fc57abeda5.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7ffa0440d2a5e3c3498c32fc57abeda5.png)'
- en: (b)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 19: Examples of images in PubTabNet'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：PubTabNet中的图像示例
- en: 4.1.18 TNCR
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.18 TNCR
- en: 'TNCR [[67](#bib.bib67)] a new table collection containing images of varied
    quality gathered from free access websites The TNCR dataset may be used to recognize
    tables in scanned document pictures and classify them into five categories. TNCR
    has roughly 6621 photos and 9428 captioned tables. To build numerous robust baselines,
    this work used state-of-the-art deep learning-based approaches for table detection.
    On the TNCR dataset, Deformable DERT with Resnet-50 Backbone Network delivers
    the best results compared to other methods, with an accuracy of 86.7%, recall
    of 89.6%, and f1 score of 88.1%. A few samples from this dataset are shown in
    Figure [20](#S4.F20 "Figure 20 ‣ 4.1.18 TNCR ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation
    Metrics ‣ Deep learning for table detection and structure recognition: A survey").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 'TNCR [[67](#bib.bib67)] 是一个新的表格集合，包含从免费访问网站收集的不同质量的图像。TNCR 数据集可用于识别扫描文档图片中的表格，并将其分类为五个类别。TNCR
    约有 6621 张照片和 9428 个标注表格。为了建立多个强健的基准，这项工作使用了基于深度学习的最先进的表格检测方法。在 TNCR 数据集上，具有 Resnet-50
    Backbone 网络的可变形 DERT 相较于其他方法表现最佳，准确率为 86.7%，召回率为 89.6%，F1 分数为 88.1%。图 [20](#S4.F20
    "Figure 20 ‣ 4.1.18 TNCR ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣
    Deep learning for table detection and structure recognition: A survey") 展示了该数据集中的一些样本。'
- en: '![Refer to caption](img/98cb12d40b26442551f26522d84234f3.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/98cb12d40b26442551f26522d84234f3.png)'
- en: (a)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/698e19a44e63b8563c1dee879e4c9d49.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/698e19a44e63b8563c1dee879e4c9d49.png)'
- en: (b)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 20: Examples of images in TNCR'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：TNCR 中图像的示例
- en: 4.1.19 SynthTabNet
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.19 SynthTabNet
- en: 'To correct an imbalance in the earlier datasets, A Nassar [[68](#bib.bib68)]
    proposes SynthTabNet, a synthetically created dataset with a variety of appearance
    styles and complexity. The authors have created four synthetic datasets, each
    with 150k samples. The most common words from PubTabNet and FinTabNet as well
    as randomly produced text make up the corpora used to create the table content.
    The first two synthetic datasets have been adjusted to closely resemble the look
    of the real datasets while incorporating more intricate table structures. The
    third one adopts a colorful style with strong contrast, while the final one has
    tables with little content. Last but not least, The authors have integrated all
    synthetic datasets into a single, 600k-example synthetic dataset. A few samples
    from this dataset are shown in Figure [21](#S4.F21 "Figure 21 ‣ 4.1.19 SynthTabNet
    ‣ 4.1 Datasets ‣ 4 Datasets and Evaluation Metrics ‣ Deep learning for table detection
    and structure recognition: A survey").'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '为了纠正早期数据集中的不平衡，A Nassar [[68](#bib.bib68)] 提出了 SynthTabNet，一个具有多种外观风格和复杂性的合成数据集。作者创建了四个合成数据集，每个数据集包含
    150k 个样本。用于生成表格内容的语料库由 PubTabNet 和 FinTabNet 中最常见的词汇以及随机生成的文本构成。前两个合成数据集已被调整以尽可能接近真实数据集的外观，同时融入了更复杂的表格结构。第三个数据集采用了具有强烈对比的彩色风格，而最后一个数据集则包含内容较少的表格。最后但同样重要的是，作者将所有合成数据集合并成一个包含
    600k 个样本的合成数据集。图 [21](#S4.F21 "Figure 21 ‣ 4.1.19 SynthTabNet ‣ 4.1 Datasets ‣
    4 Datasets and Evaluation Metrics ‣ Deep learning for table detection and structure
    recognition: A survey") 展示了这个数据集中的一些样本。'
- en: '![Refer to caption](img/9aa5e1a1c520c4ac7ff93dcb5ddfa93e.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9aa5e1a1c520c4ac7ff93dcb5ddfa93e.png)'
- en: (a)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/6843e0c0db40091e2e4c17a7e0e39b70.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6843e0c0db40091e2e4c17a7e0e39b70.png)'
- en: (b)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 21: Examples of images in SynthTabNet'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：SynthTabNet 中图像的示例
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.1.19 SynthTabNet ‣ 4.1 Datasets ‣ 4 Datasets
    and Evaluation Metrics ‣ Deep learning for table detection and structure recognition:
    A survey") presents a comparison between some of the popular datasets of table
    detection and structure recognition.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S4.T1 "Table 1 ‣ 4.1.19 SynthTabNet ‣ 4.1 Datasets ‣ 4 Datasets and
    Evaluation Metrics ‣ Deep learning for table detection and structure recognition:
    A survey") 展示了流行的表格检测和结构识别数据集之间的比较。'
- en: 'Table 1: The table illustrates a quantitative comparison between some famous
    datasets in table detection.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：该表展示了一些著名表格检测数据集的定量比较。
- en: Dataset Total pages Total Tables Table detection Table Structure Classification
    Scanned ICDAR2013 462 150 ✓ ✓ ✗ ✓ ICDAR2017-POD 2,417 - ✓ ✗ ✗ ✓ TabStructDB 2.4k
    - ✗ ✓ ✗ ✓ TABLE2LATEX-450K - 450,000 ✗ ✓ ✗ ✓ RVL-CDIP (SUBSET) 518 - ✓ ✗ ✗ ✓ IIIT-AR-13K
    13K - ✓ ✗ ✗ ✓ CamCap 85 - ✓ ✓ ✗ ✗ UNLV 2889 - ✓ ✓ ✗ ✓ UW-3 dataset 1600 - ✓ ✓
    ✗ ✓ Marmot 2000 - ✓ ✗ ✗ ✓ TableBank - 417,234 ✓ ✗ ✗ ✓ ICDAR2019 - 2000 ✓ ✓ ✗ ✓
    DeepFigures - 5.5 million ✓ ✗ ✗ ✓ PubTables-1M 460,589 1 million ✓ ✓ ✗ ✓ SciTSR
    - 15,000 ✗ ✓ ✗ ✗ FinTabNet 89,646 112,887 ✓ ✓ ✗ ✗ PubTabNet - 568k ✗ ✓ ✗ ✓ TNCR
    6621 9428 ✓ ✗ ✓ ✓ SynthTabNet 600k - ✓ ✓ ✓ ✓
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 总页数 总表格 表格检测 表格结构分类 扫描的 ICDAR2013 462 150 ✓ ✓ ✗ ✓ ICDAR2017-POD 2,417 -
    ✓ ✗ ✗ ✓ TabStructDB 2.4k - ✗ ✓ ✗ ✓ TABLE2LATEX-450K - 450,000 ✗ ✓ ✗ ✓ RVL-CDIP
    (子集) 518 - ✓ ✗ ✗ ✓ IIIT-AR-13K 13K - ✓ ✗ ✗ ✓ CamCap 85 - ✓ ✓ ✗ ✗ UNLV 2889 - ✓
    ✓ ✗ ✓ UW-3 数据集 1600 - ✓ ✓ ✗ ✓ Marmot 2000 - ✓ ✗ ✗ ✓ TableBank - 417,234 ✓ ✗ ✗
    ✓ ICDAR2019 - 2000 ✓ ✓ ✗ ✓ DeepFigures - 550万 ✓ ✗ ✗ ✓ PubTables-1M 460,589 100万
    ✓ ✓ ✗ ✓ SciTSR - 15,000 ✗ ✓ ✗ ✗ FinTabNet 89,646 112,887 ✓ ✓ ✗ ✗ PubTabNet - 568k
    ✗ ✓ ✗ ✓ TNCR 6621 9428 ✓ ✗ ✓ ✓ SynthTabNet 600k - ✓ ✓ ✓ ✓
- en: 4.2 Metrics
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 指标
- en: Table detectors use multiple criteria to measure the performance of the detectors
    viz., frames per second (FPS), precision, and recall. However, mean Average Precision
    (mAP) is the most common evaluation metric. Precision is derived from Intersection
    over Union (IoU), which is the ratio of the area of overlap and the area of union
    between the ground truth and the predicted bounding box. A threshold is set to
    determine if the detection is correct. If the IoU is more than the threshold,
    it is classified as True Positive while an IoU below it is classified as False
    Positive. If the model fails to detect an object present in the ground truth,
    it is termed a False Negative. Precision measures the percentage of correct predictions
    while recall measures the correct predictions with respect to the ground truth.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表格检测器使用多种标准来衡量检测器的性能，即每秒帧数 (FPS)、精度和召回率。然而，平均精度 (mAP) 是最常用的评估指标。精度来源于交并比 (IoU)，这是地面真实值和预测边界框之间的重叠区域与联合区域的比率。设置一个阈值来确定检测是否正确。如果
    IoU 超过阈值，则分类为真正例，而低于阈值则分类为假正例。如果模型未能检测到地面真实值中存在的对象，则称为假负例。精度衡量正确预测的百分比，而召回率衡量相对于地面真实值的正确预测。
- en: '|  | Average Precision (AP) | $\displaystyle=\frac{\textrm{True Positive (TP)}}{(\textrm{True
    Positive (TP)}+\textrm{False Positive (FP)})}$ |  | (4) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均精度 (AP) | $\displaystyle=\frac{\textrm{真正例 (TP)}}{(\textrm{真正例 (TP)}+\textrm{假正例
    (FP)})}$ |  | (4) |'
- en: '|  |  | $\displaystyle=\frac{TruePositive}{AllObservations}$ |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{真正例}{所有观察}$ |  |'
- en: '|  | Average Recall (AR) | $\displaystyle=\frac{\textrm{True Positive (TP)}}{(\textrm{True
    Positive (TP)}+\textrm{False Negative (FN) })}$ |  | (5) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均召回率 (AR) | $\displaystyle=\frac{\textrm{真正例 (TP)}}{(\textrm{真正例 (TP)}+\textrm{假负例
    (FN) })}$ |  | (5) |'
- en: '|  |  | $\displaystyle=\frac{TruePositive}{AllGroundTruth}$ |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{真正例}{所有地面真实值}$ |  |'
- en: '|  | F1-score | $\displaystyle=\frac{2*(\textrm{AP}*\textrm{AR})}{(\textrm{AP}+\textrm{AR})}$
    |  | (6) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | F1分数 | $\displaystyle=\frac{2*(\textrm{AP}*\textrm{AR})}{(\textrm{AP}+\textrm{AR})}$
    |  | (6) |'
- en: Based on the above equation, average precision is computed separately for each
    class. To compare performance between the detectors, the mean of average precision
    of all classes, called mean average precision (mAP) is used, which acts as a single
    metric for final evaluation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述方程，平均精度是为每个类别单独计算的。为了比较检测器之间的性能，使用所有类别的平均精度的平均值，称为平均平均精度 (mAP)，作为最终评估的单一指标。
- en: IOU is a metric that finds the difference between ground truth annotations and
    predicted bounding boxes. This metric is used in most state of art object detection
    algorithms. In object detection, the model predicts multiple bounding boxes for
    each object and based on the confidence scores of each bounding box it removes
    unnecessary boxes based on their threshold value. We need to declare the threshold
    value based on our requirements.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: IoU 是一种度量，用于找出地面真实注释和预测边界框之间的差异。这种度量在大多数先进的物体检测算法中使用。在物体检测中，模型为每个对象预测多个边界框，并根据每个边界框的置信度分数，基于其阈值值移除不必要的框。我们需要根据需求声明阈值值。
- en: '|  | IOU | $\displaystyle=\frac{\textrm{Area of union }}{\textrm{area of intersection}}$
    |  | (7) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | IoU | $\displaystyle=\frac{\textrm{联合区域的面积 }}{\textrm{交集区域的面积}}$ |  |
    (7) |'
- en: 5 Table detection and structure recognition Models
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 表格检测和结构识别模型
- en: 'Table detection has been studied for an extended period of time. Researchers
    used different methods that can be categorized as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表格检测已经研究了很长时间。研究人员使用了不同的方法，可以归纳为以下几类：
- en: '1.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: heuristic-based methods
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于启发式的方法
- en: '2.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: machine learning-based methods
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于机器学习的方法
- en: '3.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: deep learning-based methods
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于深度学习的方法
- en: Primarily heuristic-based methods were mainly used in the 1990s, 2000s, and
    early 2010\. They employed different visual cues like lines, keywords, space features,
    etc. to detect tables.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的基于启发式的方法在1990年代、2000年代和2010年代初期被广泛使用。它们使用了不同的视觉线索，如线条、关键字、空间特征等来检测表格。
- en: P. Pyreddy et al. [[69](#bib.bib69)] proposed an approach of detecting tables
    using character alignment, holes, and gaps. Wang et al. [[70](#bib.bib70)]. used
    a statistical approach to detect table lines depending on the distance between
    consecutive words. Grouped horizontal consecutive words together with vertical
    adjacent lines were employed to propose table entity candidates. Jahan et al.
    [[71](#bib.bib71)] presented a method that uses local thresholds for word spacing
    and line height for detecting table regions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: P. Pyreddy 等人 [[69](#bib.bib69)] 提出了一种通过字符对齐、孔和间隙检测表格的方法。Wang 等人 [[70](#bib.bib70)]
    使用统计方法根据连续单词之间的距离来检测表格线。将水平连续单词与垂直相邻线进行分组，用于提出表格实体候选项。Jahan 等人 [[71](#bib.bib71)]
    提出了一种使用局部阈值检测单词间距和行高以识别表格区域的方法。
- en: Itonori [[72](#bib.bib72)] proposed a rule-based approach that led to the text-block
    arrangement and ruled line position to localize the table in the documents. Chandran
    and Kasturi [[73](#bib.bib73)] developed another table detection approach based
    on vertical and horizontal lines. Wonkyo Seo et al. [[56](#bib.bib56)] used junctions
    (intersection of the horizontal and vertical line) detection with further processing.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Itonori [[72](#bib.bib72)] 提出了基于规则的方法，该方法通过文本块排列和规则线位置来本地化文档中的表格。Chandran 和
    Kasturi [[73](#bib.bib73)] 开发了另一种基于垂直和水平线的表格检测方法。Wonkyo Seo 等人 [[56](#bib.bib56)]
    使用交点（水平和垂直线的交叉点）检测，并进行了进一步处理。
- en: Hassan et al. [[74](#bib.bib74)] locate and segment tables by analyzing spatial
    features of text blocks. Ruffolo et al. [[75](#bib.bib75)] introduced PDF-TREX,
    a heuristic bottom-up approach for table recognition in single-column PDF documents.
    It uses the spatial features of page elements to align and group them into paragraphs
    and tables. Nurminen [[76](#bib.bib76)] proposed a set of heuristics to locate
    subsequent text boxes with common alignments and assign them the probability of
    being a table.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Hassan 等人 [[74](#bib.bib74)] 通过分析文本块的空间特征来定位和分割表格。Ruffolo 等人 [[75](#bib.bib75)]
    引入了 PDF-TREX，一种用于单列 PDF 文档中表格识别的启发式自下而上的方法。它利用页面元素的空间特征对其进行对齐和分组成段落和表格。Nurminen
    [[76](#bib.bib76)] 提出了一组启发式方法，用于定位具有共同对齐的后续文本框，并赋予它们作为表格的概率。
- en: Fang et al. [[77](#bib.bib77)] used the table header as a starting point to
    detect the table region and decompose its elements. Harit et al. [[78](#bib.bib78)]
    proposed a technique for table detection based on the identification of unique
    table start and trailer patterns. Tupaj et al. [[79](#bib.bib79)] proposed an
    OCR based table detection technique. The system searches for sequences of table-like
    lines based on the keywords
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Fang 等人 [[77](#bib.bib77)] 以表格头作为起点来检测表格区域并分解其元素。Harit 等人 [[78](#bib.bib78)]
    提出了基于识别唯一表格开始和结束模式的表格检测技术。Tupaj 等人 [[79](#bib.bib79)] 提出了一种基于 OCR 的表格检测技术。该系统根据关键字搜索类似表格的行序列。
- en: The above methods work relatively well on documents with uniform layouts. However,
    heuristic rules need to be tweaked to a wider variety of tables and are not really
    suited for generic solutions. Therefore, machine learning approaches started to
    be employed to solve the table detection problem.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法在具有统一布局的文档上效果相对较好。然而，启发式规则需要针对各种表格进行调整，并且不适用于通用解决方案。因此，开始采用机器学习方法来解决表格检测问题。
- en: Machine learning-based methods were common around the 2000s and the 2010s.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的方法在2000年代和2010年代非常常见。
- en: Kieninger et al. [[80](#bib.bib80)] applied an unsupervised learning approach
    by clustering word segments. Cesarini et al. [[81](#bib.bib81)] used a modified
    XY tree supervised learning approach. Fan et al.[[82](#bib.bib82)] uses both supervised
    and unsupervised approaches to table detection in PDF documents. Wang and Hu [[83](#bib.bib83)]
    applied Decision tree and SVM classifiers to layout, content type and word group
    features. T. Kasar et al. [[84](#bib.bib84)] used the junction detection and then
    passed the information to the SVM classifier. Silva et al. [[85](#bib.bib85)]
    applied joint probability distribution over sequential observations of visual
    page elements (Hidden Markov Models) to merge potential table lines into tables.
    Klampfl et al. [[86](#bib.bib86)] compare two unsupervised table recognition methods
    from digital scientific articles. Docstrum algorithm [[87](#bib.bib87)] applies
    KNN to aggregate structures into lines and then uses perpendicular distance and
    angle between lines to combine them into text blocks. It must be noted that this
    algorithm was devised in 1993, earlier than other methods mentioned in this section.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Kieninger 等人 [[80](#bib.bib80)] 通过聚类词段应用了无监督学习方法。Cesarini 等人 [[81](#bib.bib81)]
    使用了修改过的 XY 树监督学习方法。Fan 等人 [[82](#bib.bib82)] 结合了监督学习和无监督学习的方法来检测 PDF 文档中的表格。Wang
    和 Hu [[83](#bib.bib83)] 将决策树和 SVM 分类器应用于布局、内容类型和词组特征。T. Kasar 等人 [[84](#bib.bib84)]
    使用了交点检测，然后将信息传递给 SVM 分类器。Silva 等人 [[85](#bib.bib85)] 将联合概率分布应用于视觉页面元素的序列观察（隐马尔可夫模型），以将潜在的表格线合并为表格。Klampfl
    等人 [[86](#bib.bib86)] 比较了来自数字科学文章的两种无监督表格识别方法。Docstrum 算法 [[87](#bib.bib87)] 应用了
    KNN 将结构聚合为行，然后使用行之间的垂直距离和角度将它们合并为文本块。需要注意的是，该算法是在 1993 年提出的，比本节提到的其他方法早。
- en: F Shafait [[88](#bib.bib88)] proposes a useful method for table recognition
    that performs well on documents with a range of layouts, including business reports,
    news stories, and magazine pages. The Tesseract OCR engine offers an open-source
    implementation of the algorithm.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: F Shafait [[88](#bib.bib88)] 提出了一个有效的表格识别方法，该方法在具有多种布局的文档上表现良好，包括商业报告、新闻故事和杂志页面。Tesseract
    OCR 引擎提供了该算法的开源实现。
- en: As neural networks gained interest, researchers started to apply them to document
    layout analysis tasks. Initially, they were used at simpler tasks like table detection.
    Later on, as more complex architectures were developed, more work was put into
    table columns and overall structure recognition.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络的兴趣增加，研究人员开始将其应用于文档布局分析任务。最初，它们用于简单的任务，如表格检测。后来，随着更复杂的架构的开发，更多的工作被投入到表格列和整体结构识别中。
- en: Hao et al. [[24](#bib.bib24)] employed CNN to detect whether a certain region
    proposal is a table or not. Azka Gilani et al. [[22](#bib.bib22)] proposed a Faster
    R-CNN-based model to make up for the limitations of Hao et al. [[24](#bib.bib24)]
    and other prior methodologies.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Hao 等人 [[24](#bib.bib24)] 使用 CNN 检测某个区域提议是否为表格。Azka Gilani 等人 [[22](#bib.bib22)]
    提出了基于 Faster R-CNN 的模型，以弥补 Hao 等人 [[24](#bib.bib24)] 和其他早期方法的局限性。
- en: Sebastian Schreiber et al. [[20](#bib.bib20)] were the first to perform table
    detection and structure recognition using Faster RCNN. He et al. [[89](#bib.bib89)],
    used FCN for semantic page segmentation. S. Arif et al. [[90](#bib.bib90)] attempted
    to improve the accuracy of Faster R-CNN by using semantic color-coding of text.
    Reza et al. [[91](#bib.bib91)] used a combination of GAN-based architecture for
    table detection. Agarwal et al. [[92](#bib.bib92)] used a multistage extension
    of Mask R-CNN with a dual backbone for detecting tables.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian Schreiber 等人 [[20](#bib.bib20)] 是首批使用 Faster RCNN 进行表格检测和结构识别的研究者。He
    等人 [[89](#bib.bib89)] 使用 FCN 进行语义页面分割。S. Arif 等人 [[90](#bib.bib90)] 通过使用文本的语义颜色编码来尝试提高
    Faster R-CNN 的准确性。Reza 等人 [[91](#bib.bib91)] 结合了基于 GAN 的架构进行表格检测。Agarwal 等人 [[92](#bib.bib92)]
    使用了 Mask R-CNN 的多阶段扩展和双主干进行表格检测。
- en: Recently transformer-based models were applied to document layout analysis,
    Smock, Brandon et al. [[63](#bib.bib63)] applied Carion et al.[[93](#bib.bib93)]
    DEtection TRansformer framework, a transformer encoder-decoder architecture, to
    their table dataset for both table detection and structure recognition tasks.
    Xu et al. [[94](#bib.bib94)] proposed a self-supervised pre-trained Document Image
    Transformer model using large-scale unlabeled text images for document analysis,
    including table detection
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于变换器的模型被应用于文档布局分析。Smock、Brandon 等人 [[63](#bib.bib63)] 将 Carion 等人 [[93](#bib.bib93)]
    的 DEtection TRansformer 框架（一种变换器编码器-解码器架构）应用于他们的表格数据集，用于表格检测和结构识别任务。Xu 等人 [[94](#bib.bib94)]
    提出了一个自监督预训练的文档图像变换器模型，使用大规模无标签文本图像进行文档分析，包括表格检测。
- en: 5.1 Table detection Models
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 表格检测模型
- en: 'In this section, we examine the deep learning methods used for document image
    table detection. We have divided the methods into several deep learning ideas
    for the benefit of our readers’ convenience. Table [2](#S5.T2 "Table 2 ‣ 5.1 Table
    detection Models ‣ 5 Table detection and structure recognition Models ‣ Deep learning
    for table detection and structure recognition: A survey") lists all the object
    identification-based table detection strategies. It also discusses various deep
    learning-based methods that have been used in these methods.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们审视了用于文档图像表格检测的深度学习方法。为了方便读者，我们将这些方法分为几个深度学习思路。表格 [2](#S5.T2 "表格 2 ‣ 5.1
    表格检测模型 ‣ 5 表格检测与结构识别模型 ‣ 表格检测与结构识别的深度学习：综述") 列出了所有基于对象识别的表格检测策略。它还讨论了在这些方法中使用的各种基于深度学习的方法。
- en: A Gilani [[22](#bib.bib22)] has shown how to recognize tables using deep learning.
    Document pictures are pre-processed initially in the suggested technique. These
    photos are then sent into a Region Proposal Network for table detection, which
    is followed by a fully connected neural network. The suggested approach works
    with great precision on a variety of document pictures, including documents, research
    papers, and periodicals, with various layouts.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: A Gilani [[22](#bib.bib22)] 展示了如何使用深度学习识别表格。建议的技术中，文档图片首先经过预处理。这些照片随后被送入区域提议网络进行表格检测，接着是全连接神经网络。该方法在各种文档图片上，包括文档、研究论文和期刊，具有很高的精度，适用于不同的布局。
- en: 'D Prasad [[95](#bib.bib95)] presents an automatic table detection approach
    for interpreting tabular data in document pictures, which primarily entails addressing
    two issues: table detection and table structure recognition. Using a single Convolution
    Neural Network (CNN) model, provide an enhanced deep learning-based end-to-end
    solution for handling both table detection and structure recognition challenges.
    CascadeTabNet is a Cascade mask Region-based CNN High-Resolution Network (Cascade
    mask R-CNN HRNet)-based model that simultaneously identifies table areas and recognizes
    structural body cells from those tables.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: D Prasad [[95](#bib.bib95)] 提出了一个自动表格检测方法，用于解释文档图片中的表格数据，主要涉及两个问题：表格检测和表格结构识别。使用单一的卷积神经网络（CNN）模型，提供了一种增强的基于深度学习的端到端解决方案，处理表格检测和结构识别挑战。CascadeTabNet
    是一个基于 Cascade mask Region-based CNN High-Resolution Network（Cascade mask R-CNN
    HRNet）的模型，能够同时识别表格区域和从这些表格中识别结构体单元。
- en: SS Paliwal [[96](#bib.bib96)] presents TableNet which is a new end-to-end deep
    learning model for both table detection and structure recognition. To divide the
    table and column areas, the model uses the dependency between the twin objectives
    of table detection and table structure recognition. Then, from the discovered
    tabular sub-regions, semantic rule-based row extraction is performed.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: SS Paliwal [[96](#bib.bib96)] 提出了 TableNet，这是一种新的端到端深度学习模型，用于表格检测和结构识别。该模型利用表格检测和表格结构识别这两个目标之间的依赖关系来划分表格和列区域。然后，从发现的表格子区域中执行基于语义规则的行提取。
- en: Y Huang [[97](#bib.bib97)] describes a table detecting algorithm based on the
    YOLO principle. The authors offer various adaptive improvements to YOLOv3, including
    an anchor optimization technique and two post-processing methods, to account for
    the significant differences between document objects and real objects. also employ
    k-means clustering for anchor optimization to create anchors that are more suited
    for tables than natural objects, making it easier for our model to find the exact
    placements of tables. The additional whitespaces and noisy page objects are deleted
    from the projected results during the post-processing procedure.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Y Huang [[97](#bib.bib97)] 描述了一种基于YOLO原则的表格检测算法。作者提供了对YOLOv3的各种自适应改进，包括一个锚点优化技术和两个后处理方法，以应对文档对象和真实对象之间的显著差异。此外，还采用k-means聚类进行锚点优化，以创建更适合表格而非自然对象的锚点，使我们的模型更容易找到表格的确切位置。在后处理过程中，额外的空白和嘈杂的页面对象会被删除。
- en: L Hao [[24](#bib.bib24)] offers a new method for detecting tables in PDF documents
    that are based on convolutional neural networks, one of the most widely used deep
    learning models. The suggested method begins by selecting some table-like areas
    using some vague constraints, then building and refining convolutional networks
    to identify whether the selected areas are tables or not. Furthermore, the convolutional
    networks immediately extract and use the visual aspects of table sections, while
    the non-visual information contained in original PDF documents is also taken into
    account to aid in better detection outcomes.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: L Hao [[24](#bib.bib24)] 提供了一种新的基于卷积神经网络的PDF文档表格检测方法，这是最广泛使用的深度学习模型之一。所提出的方法首先通过一些模糊约束选择一些类似表格的区域，然后构建和优化卷积网络以识别所选区域是否为表格。此外，卷积网络会直接提取和使用表格部分的视觉特征，同时还考虑原始PDF文档中的非视觉信息，以帮助获得更好的检测结果。
- en: SA Siddiqui [[98](#bib.bib98)] provide a novel strategy for detecting tables
    in documents. The approach given here takes advantage of data’s potential to recognize
    tables with any arrangement. however, the given method works directly on photos,
    making it universally applicable to any format. The proposed method uses a unique
    mix of deformable CNN and speedier R-CNN/FPN. Because tables might be present
    at variable sizes and transformations, traditional CNN has a fixed receptive field,
    which makes table recognition difficult (orientation). Deformable convolution
    bases its receptive field on the input, allowing it to shape it to match the input.
    The network can accommodate tables of any layout because of this customization
    of the receptive field.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: SA Siddiqui [[98](#bib.bib98)] 提出了一个新颖的策略来检测文档中的表格。这里的方法利用了数据识别表格的潜力，不论其排列方式如何。然而，该方法直接作用于照片，使其适用于任何格式。提出的方法使用了独特的可变形CNN和更快的R-CNN/FPN混合技术。由于表格可能存在于不同的尺寸和变换中，传统的CNN具有固定的感受野，这使得表格识别变得困难（方向）。可变形卷积基于输入来调整感受野，从而使其能够匹配输入。由于感受野的这种定制，网络能够适应任何布局的表格。
- en: N Sun [[99](#bib.bib99)] presents a corner-finding approach for faster R-CNN-based
    table detection. The Faster R-CNN network is first used to achieve coarse table
    identification and corner location. then, coordinate matching is used to group
    those corners that belong to the same table. Untrustworthy edges are filtered
    at the same time. Finally, the matching corner group fine-tunes and adjusts the
    table borders. At the pixel level, the suggested technique enhances table boundary
    finding precision.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: N Sun [[99](#bib.bib99)] 提出了一个基于Faster R-CNN的角点检测方法用于表格检测。首先使用Faster R-CNN网络进行粗略的表格识别和角点定位。然后，使用坐标匹配来将属于同一表格的角点进行分组，同时过滤掉不可靠的边缘。最后，匹配的角点组细化和调整表格边界。在像素级别，该技术提高了表格边界定位的精确度。
- en: I Kavasidis[[100](#bib.bib100)] propose a method for detecting tables and charts
    using a combination of deep CNNs, graphical models, and saliency ideas. M Holeček
    [[101](#bib.bib101)] presented the concept of table understanding utilizing graph
    convolutions in structured documents like bills, extending the applicability of
    graph neural networks. A PDF document is used in the planned research as well.
    The job of line item table detection and information extraction are combined in
    this study to tackle the problem of table detection. Any word may be quickly identified
    as a line item or not using the line item technique. Following word classification,
    the tabular region may be easily identified since, in contrast to other text sections
    on bills, table lines are able to distinguish themselves rather effectively.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: I Kavasidis [[100](#bib.bib100)] 提出了使用深度 CNN、图形模型和显著性思想相结合的方法来检测表格和图表。M Holeček
    [[101](#bib.bib101)] 提出了在结构化文档（如账单）中利用图卷积进行表格理解的概念，扩展了图神经网络的适用性。计划中的研究也使用了 PDF
    文档。此研究结合了行项表格检测和信息提取，以解决表格检测问题。行项技术可以快速识别任何单词是否为行项。经过单词分类后，由于与账单上的其他文本部分相比，表格行能够有效区分自己，因此可以轻松识别表格区域。
- en: Á Casado-García [[102](#bib.bib102)] Uses object detection techniques, The authors
    have shown that fine-tuning from a closer domain improves the performance of table
    detection after conducting a thorough examination. The authors have utilized Mask
    R-CNN, YOLO, SSD, and Retina Net in conjunction with object detection algorithms.
    Two basic datasets are chosen to be used in this investigation, TableBank and
    PascalVOC.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Á Casado-García [[102](#bib.bib102)] 使用了目标检测技术。作者在进行全面检查后证明了从更接近的领域进行微调可以提高表格检测的性能。作者利用了
    Mask R-CNN、YOLO、SSD 和 Retina Net 结合目标检测算法。在这项研究中选择了两个基本数据集，TableBank 和 PascalVOC。
- en: X Zheng [[103](#bib.bib103)] provides Global Table Extractor (GTE), a method
    for jointly detecting tables and recognizing cell structures that can be implemented
    on top of any object detection model. To train their table network with the help
    of cell placement predictions, the authors develop GTE-Table, which introduces
    a new penalty based on the inherent cell confinement limitation of tables. A novel
    hierarchical cell identification network called GTE-Cell makes use of table styles.
    Additionally, in order to quickly and inexpensively build a sizable corpus of
    training and test data, authors develop a method to automatically classify table
    and cell structures in preexisting texts.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: X Zheng [[103](#bib.bib103)] 提出了 Global Table Extractor (GTE) 方法，这是一种联合检测表格和识别单元格结构的方法，可以在任何目标检测模型之上实现。为了利用单元格位置预测训练他们的表格网络，作者开发了
    GTE-Table，该方法引入了一种新的惩罚机制，基于表格的固有单元格限制。一个名为 GTE-Cell 的新型分层单元格识别网络利用了表格样式。此外，为了快速且经济地构建大量的训练和测试数据集，作者开发了一种自动分类预存在文本中的表格和单元格结构的方法。
- en: Y Li [[104](#bib.bib104)] provides a new network to produce the layout elements
    for table text and to enhance the performance of less ruled table identification.
    The Generative Adversarial Networks(GAN) and this feature generator model are
    comparable. The authors mandate that the feature generator model extract comparable
    features for both heavily governed and loosely ruled tables.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Y Li [[104](#bib.bib104)] 提供了一种新网络，用于生成表格文本的布局元素，并提升了对规则较少的表格识别的性能。生成对抗网络（GAN）与该特征生成模型相似。作者要求特征生成模型为严格规则和宽松规则的表格提取类似的特征。
- en: DD Nguyen [[105](#bib.bib105)] introduces TableSegNet, a fully convolutional
    network with a compact design that concurrently separates and detects tables.
    TableSegNet uses a shallower path to discover table locations in high resolution
    and a deeper path to detect table areas in low resolution, splitting the found
    regions into separate tables. TableSegNet employs convolution blocks with broad
    kernel sizes throughout the feature extraction process and an additional table-border
    class in the main output to increase the detection and separation capabilities.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: DD Nguyen [[105](#bib.bib105)] 介绍了 TableSegNet，这是一种紧凑设计的全卷积网络，同时进行表格的分离和检测。TableSegNet
    使用较浅的路径在高分辨率下发现表格位置，使用较深的路径在低分辨率下检测表格区域，将发现的区域划分为不同的表格。TableSegNet 在特征提取过程中使用了具有宽核大小的卷积块，并在主输出中增加了一个表格边界类，以提高检测和分离能力。
- en: D Zhang [[106](#bib.bib106)] suggests a YOLO-table-based table detection methodology.
    To enhance the network’s capacity to learn the spatial arrangement aspects of
    tables, the authors incorporate involution into the network’s core, and the authors
    create a simple Feature Pyramid Network to increase model efficacy. This research
    also suggests a table-based enhancement technique.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: D Zhang [[106](#bib.bib106)] 提出了基于 YOLO 表格的表格检测方法。为了增强网络对表格空间布局方面的学习能力，作者将内卷技术引入网络核心，并创建了一个简单的特征金字塔网络以提高模型效能。这项研究还提出了一种基于表格的增强技术。
- en: 'Table 2: A comparison of the benefits and drawbacks of several deep learning-based
    table detection methods'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：几种基于深度学习的表格检测方法的优缺点比较
- en: Literature Method Benefits Drawbacks A Gilani[[22](#bib.bib22)] Faster R-CNN
    1) On scanned document pictures, this is the first deep learning-based table detection
    method. 2) The object detection technique is made easier by converting RGB pixels
    to distance measures. There are additional phases in the pre-processing process.
    S Schreiber[[20](#bib.bib20)] transfer learning methods + Faster R-CNN end-to-end
    strategy for detecting tables and table structures that is straightforward and
    efficient When compared to other state-of-the-art techniques, it is less accurate.
    SA Siddiqui [[98](#bib.bib98)] Deformable CNN + Faster R-CNN Deformable convolutional
    neural networks’ dynamic receptive field aids in the reconfiguration of multiple
    tabular boundries. When compared to standard convolutions, deformable convolutions
    are computationally demanding. SS Paliwal [[96](#bib.bib96)] Networks with fully
    convolutions 1) First attempt at combining a single solution to handle both the
    problem of table detection and structure recognition. 2) A comprehensive method
    for structure recognition and detection in document pictures. This approach only
    functions on column detection when used for table structure extraction. P Riba
    [[54](#bib.bib54)] OCR-based Graph NN that makes use of textual characteristics
    The suggested technique makes use of more data than only spatial attributes. 1)
    No comparisons to other state-of- the-art strategies. 2) Additional annotations
    are needed using this strategy in addition to the tabular data. N Sun [[99](#bib.bib99)]
    Faster R-CNN + Locate corners 1) Better outcomes are obtained using a novel technique.
    2) Faster R-CNN is used to identify not just tables, but also the corners of tabular
    borders 1) It is necessary to do postprocessing operations such as corner refining.
    2) Because of the additional detections, the computation is more involved. I Kavasidis
    [[100](#bib.bib100)] combination of deep CNNs, graphical models, and saliency
    1) Dilated convolutions rather than conventional convolutions are used. 2) Using
    this technique, saliency detection is performed in place of table detection. To
    provide equivalent results, many processing stages are necessary. M Holeček [[101](#bib.bib101)]
    Graph NN + line item identification Method This approach yields encouraging outcomes
    when used to layout-intensive documents like invoices and PDFs. 1) Limited baseline
    approach without comparisons to other state-of-the-art techniques 2) No publicly
    accessible table datasets are used for the evaluation of the approach. Y Huang
    [[97](#bib.bib97)] YOLO In comparison, a quicker and more effective strategy The
    suggested methodology relies on data-driven post-processing methods. Y Li [[104](#bib.bib104)]
    Generative Adversarial Networks(GAN) For ruling and less ruled tables, the GAN-based
    strategy drives the network to extract comparable characteristics. In document
    images with different tabular layouts, the generator-based model is susceptible.
    M Li [[61](#bib.bib61)] Faster R-CNN This method demonstrates how a basic Faster
    R-CNN can yield excellent results when used with a huge dataset like TableBank.
    Just a simple Faster-RCNN implementation D Prasad [[95](#bib.bib95)] Cascade mask
    Region-based CNN High-Resolution Network-based model The study shows how iterative
    transfer learning may be used to transform pictures, which can lessen the need
    on huge datasets. The same as[[22](#bib.bib22)], There are additional phases in
    the pre-processing process. Á Casado-García [[102](#bib.bib102)] Liken fine-tuning
    + Mask R-CNN, RetinaNet , SSD and YOLO Describe the advantages of using object
    detection networks in conjunction with domain-specific fine-tuning techniques
    for table detection. Closed domain fine-tuning is still insufficient to get state-of-the-art
    solutions. M Agarwal [[92](#bib.bib92)] multistage extension of Mask R-CNN with
    a dual backbone 1) A comprehensive object detection-based frame- work utilizing
    a composite backbone to deliver state- of-the-art outcomes 2) Extensive tests
    on benchmark datasets for table detection that are openly accessible. The technique
    is computationally expensive since it uses a composite backbone in addition to
    deformable convolutions. X Zheng [[103](#bib.bib103)] Global Table Extractor (GTE)
    which is general method for object detection 1) The problem of table detection
    is benefited by the extra piece-wise constraint loss introduced. 2) a complete
    method that is compatible with all object detection frameworks. Annotations for
    cellular borders are necessary since the process of table detection depends on
    cell detection.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 文献方法 优点 缺点 A Gilani[[22](#bib.bib22)] Faster R-CNN 1) 这是首个基于深度学习的表格检测方法，应用于扫描文档图片。
    2) 通过将RGB像素转换为距离测量，简化了物体检测技术。 预处理过程中有额外的阶段。 S Schreiber[[20](#bib.bib20)] 迁移学习方法
    + Faster R-CNN 端到端策略用于检测表格及表格结构，简单高效 与其他最先进技术相比，准确性较低。 SA Siddiqui [[98](#bib.bib98)]
    可变形CNN + Faster R-CNN 可变形卷积神经网络的动态感受野有助于重新配置多个表格边界。 与标准卷积相比，可变形卷积在计算上要求较高。 SS
    Paliwal [[96](#bib.bib96)] 全卷积网络 1) 首次尝试将单一解决方案用于处理表格检测和结构识别问题。 2) 提供了文档图片中结构识别和检测的全面方法。
    这种方法在用于表格结构提取时仅对列检测有效。 P Riba [[54](#bib.bib54)] 基于OCR的图形神经网络利用文本特征 所提出的方法利用了比空间属性更多的数据。
    1) 没有与其他最先进策略进行比较。 2) 除了表格数据，还需要额外的注释。 N Sun [[99](#bib.bib99)] Faster R-CNN +
    定位角点 1) 通过一种新方法获得更好的结果。 2) 使用Faster R-CNN不仅检测表格，还检测表格边界的角点 1) 需要进行后处理操作，如角点精细化。
    2) 由于额外的检测，计算更加复杂。 I Kavasidis [[100](#bib.bib100)] 深度CNN、图形模型和显著性结合 1) 使用膨胀卷积而不是传统卷积。
    2) 使用这种方法进行显著性检测以代替表格检测。 为了提供相同的结果，需要多个处理阶段。 M Holeček [[101](#bib.bib101)] 图形神经网络
    + 行项识别方法 这种方法在处理发票和PDF等布局密集型文档时产生了令人鼓舞的结果。 1) 基线方法有限，未与其他最先进技术进行比较 2) 没有使用公开的表格数据集进行方法评估。
    Y Huang [[97](#bib.bib97)] YOLO 相较而言，更快速、更有效的策略 所提出的方法依赖于数据驱动的后处理方法。 Y Li [[104](#bib.bib104)]
    生成对抗网络（GAN） 对于规则和不规则表格，基于GAN的方法驱动网络提取相似特征。 在具有不同表格布局的文档图像中，基于生成器的模型表现不稳定。 M Li
    [[61](#bib.bib61)] Faster R-CNN 该方法展示了如何使用简单的Faster R-CNN在如TableBank这样的大型数据集上取得良好结果。
    仅为简单的Faster-RCNN实现 D Prasad [[95](#bib.bib95)] 级联掩模区域CNN 高分辨率网络模型 研究展示了如何使用迭代迁移学习来转换图像，这可以减少对大型数据集的依赖。
    与[[22](#bib.bib22)]相同，预处理过程中有额外的阶段。 Á Casado-García [[102](#bib.bib102)] 微调 +
    Mask R-CNN、RetinaNet、SSD和YOLO 描述了在表格检测中结合对象检测网络与领域特定微调技术的优势。 封闭域微调仍不足以获得最先进的解决方案。
    M Agarwal [[92](#bib.bib92)] Mask R-CNN的多阶段扩展与双骨干网络 1) 一种综合对象检测框架，利用复合骨干网络实现最先进的结果
    2) 对公开可用的表格检测基准数据集进行广泛测试。 由于使用了复合骨干网络以及可变形卷积，这种技术在计算上较为昂贵。 X Zheng [[103](#bib.bib103)]
    全球表格提取器（GTE），一种通用的对象检测方法 1) 通过引入额外的分段约束损失来提升表格检测问题。 2) 一种适用于所有对象检测框架的完整方法。 由于表格检测依赖于单元格检测，因此需要对单元格边界进行注释。
- en: 5.2 Table Structure Recognition Models
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 表格结构识别模型
- en: 'In order to recognize table structures in document images, deep learning approaches
    are reviewed in this part. We divided the methods into discrete deep-learning
    principles for the benefit of our readers. Table [3](#S5.T3 "Table 3 ‣ 5.2 Table
    Structure Recognition Models ‣ 5 Table detection and structure recognition Models
    ‣ Deep learning for table detection and structure recognition: A survey"),[4](#S5.T4
    "Table 4 ‣ 5.2 Table Structure Recognition Models ‣ 5 Table detection and structure
    recognition Models ‣ Deep learning for table detection and structure recognition:
    A survey") lists all methods for recognizing table structures based on object
    detection, as well as their benefits and drawbacks. It also discusses various
    deep learning-based methods that have been used in these methods.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别文档图像中的表格结构，本部分回顾了深度学习方法。我们将这些方法划分为离散的深度学习原则，以便读者更好地理解。表[3](#S5.T3 "表 3 ‣
    5.2 表格结构识别模型 ‣ 5 表格检测和结构识别模型 ‣ 基于深度学习的表格检测和结构识别：综述")，[4](#S5.T4 "表 4 ‣ 5.2 表格结构识别模型
    ‣ 5 表格检测和结构识别模型 ‣ 基于深度学习的表格检测和结构识别：综述") 列出了所有基于目标检测的表格结构识别方法及其优缺点。还讨论了在这些方法中使用的各种基于深度学习的方法。
- en: A Zucker [[107](#bib.bib107)] presents CluSTi, a Clustering approach for recognizing
    the Structure of Tables in invoice scanned images, as an effective way. CluSTi
    makes three contributions. To begin, it uses a clustering approach to eliminate
    high noise from the table pictures. Second, it uses state-of-the-art text recognition
    to extract all text boxes. Finally, CluSTi organizes the text boxes into the correct
    rows and columns using a horizontal and vertical clustering technique with optimum
    parameters. Z Zhang [[108](#bib.bib108)] present Split, Embed, and Merge (SEM)
    is a table structure recognizer that is accurate. M Namysl [[109](#bib.bib109)]
    presents a versatile and modular table extraction approach in this research.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: A Zucker [[107](#bib.bib107)] 提出了 CluSTi，一种用于识别发票扫描图像中表格结构的聚类方法，作为一种有效的方式。CluSTi
    做出了三项贡献。首先，它使用聚类方法消除表格图像中的高噪声。其次，它使用最先进的文本识别技术提取所有文本框。最后，CluSTi 使用水平和垂直聚类技术及优化参数将文本框组织成正确的行和列。Z
    Zhang [[108](#bib.bib108)] 提出的 Split, Embed, and Merge (SEM) 是一种准确的表格结构识别器。M Namysl
    [[109](#bib.bib109)] 在这项研究中提出了一种多功能且模块化的表格提取方法。
- en: E Koci [[110](#bib.bib110)] offers a new method for identifying tables in spreadsheets
    and constructing layout areas after determining the layout role of each cell.
    Using a graph model, they express the spatial interrelationships between these
    areas. On this foundation, they present Remove and Conquer (RAC), a table recognition
    algorithm based on a set of carefully selected criteria.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: E Koci [[110](#bib.bib110)] 提供了一种新方法，用于识别电子表格中的表格，并在确定每个单元格的布局角色后构建布局区域。他们使用图模型表达这些区域之间的空间相互关系。在此基础上，他们提出了
    Remove and Conquer (RAC)，一种基于精心选择的标准的表格识别算法。
- en: Using the potential of deformable convolutional networks, SA Siddiqui [[51](#bib.bib51)]
    proposes a unique approach for analyzing tabular patterns in document pictures.
    P Riba [[54](#bib.bib54)] presents a graph-based technique for recognizing tables
    in document pictures in this paper. also employ the location, context, and content
    type instead of the raw content (recognized text), thus it’s just a structural
    perception technique that’s not reliant on the language or the quality of the
    text reading. E Koci [[111](#bib.bib111)] use genetic-based techniques for graph
    partitioning, to recognize the sections of the graph matching to tables in the
    sheet.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 利用可变形卷积网络的潜力，SA Siddiqui [[51](#bib.bib51)] 提出了分析文档图片中表格模式的独特方法。P Riba [[54](#bib.bib54)]
    在本文中提出了一种基于图的技术，用于识别文档图片中的表格。该方法还采用位置、上下文和内容类型，而不是原始内容（识别的文本），因此这只是一个不依赖于语言或文本质量的结构感知技术。E
    Koci [[111](#bib.bib111)] 使用基于遗传算法的图分区技术，识别与表格匹配的图的部分。
- en: SA Siddiqui [[112](#bib.bib112)] described the structure recognition issue as
    the semantic segmentation issue. To segment the rows and columns, the authors
    employed fully convolutional networks. The approach of prediction tiling is introduced,
    which lessens the complexity of table structural identification, assuming consistency
    in a tabular structure. The author imported pre-trained models from ImageNet and
    used the structural models of FCN’s encoder and decoder. The model creates features
    of the same size as the original input picture when given an image.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: SA Siddiqui [[112](#bib.bib112)] 将结构识别问题描述为语义分割问题。为了对行和列进行分割，作者采用了全卷积网络。引入了预测瓦片化的方法，这减少了表格结构识别的复杂性，假设表格结构的一致性。作者从
    ImageNet 导入了预训练模型，并使用了 FCN 的编码器和解码器的结构模型。给定图像时，模型创建与原始输入图像大小相同的特征。
- en: SA Khan [[113](#bib.bib113)] presents a robust deep learning-based solution
    for extracting rows and columns from a recognized table in document pictures in
    this work. The table pictures are pre-processed before being sent into a bi-directional
    Recurrent Neural Network using Gated Recurrent Units (GRU) and a fully-connected
    layer with softmax activation in the suggested solution. SF Rashid [[114](#bib.bib114)]
    provides a new learning-based approach for table content identification in diverse
    document pictures. SR Qasim [[115](#bib.bib115)] presents a graph network-based
    architecture for table recognition as a superior alternative to typical neural
    networks. S Raja [[116](#bib.bib116)] describes a method for recognizing table
    structure that combines cell detection and interaction modules to locate the cells
    and forecast their relationships with other detected cells in terms of row and
    column. Also, add structural limitations to the loss function for cell identification
    as extra differential components. The existing issues with end-to-end table identification
    were examined by Y Deng [[52](#bib.bib52)], who also highlighted the need for
    a larger dataset in this area.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: SA Khan [[113](#bib.bib113)] 在这项工作中提出了一种基于深度学习的强大解决方案，用于从文档图片中识别的表格中提取行和列。表格图片在被送入使用门控递归单元
    (GRU) 和具有 softmax 激活的全连接层的双向递归神经网络之前进行了预处理。SF Rashid [[114](#bib.bib114)] 提出了一个基于学习的新方法，用于在各种文档图片中识别表格内容。SR
    Qasim [[115](#bib.bib115)] 提出了一个基于图网络的表格识别架构，作为对典型神经网络的优越替代方案。S Raja [[116](#bib.bib116)]
    描述了一种识别表格结构的方法，结合了单元检测和交互模块，以定位单元格并预测它们与其他检测到的单元格的行列关系。此外，还在损失函数中增加了结构限制，以作为单元格识别的额外微分组件。Y
    Deng [[52](#bib.bib52)] 研究了端到端表格识别的现有问题，并强调了该领域需要更大数据集的需求。
- en: Another study by Y Zou [[117](#bib.bib117)] called for the development of an
    image-based table structure identification technique using fully convolutional
    networks. the shown work divides a table’s rows, columns, and cells. All of the
    table components’ estimated bounds are enhanced using connected component analysis.
    Based on the placement of the row and column separators, row and column numbers
    are then allocated for each cell. In addition, special algorithms are used to
    optimize cellular borders.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Y Zou [[117](#bib.bib117)] 的另一项研究呼吁开发一种基于图像的表格结构识别技术，使用全卷积网络。所展示的工作对表格的行、列和单元格进行划分。所有表格组件的估计边界通过连通组件分析得到增强。然后，根据行和列分隔符的位置，为每个单元格分配行和列编号。此外，使用特殊算法来优化单元格的边界。
- en: To identify rows and columns in tables, KA Hashmi [[118](#bib.bib118)] suggested
    a guided technique for table structure identification. The localization of rows
    and columns may be made better, according to this study, by using an anchor optimization
    approach. The boundaries of rows and columns are detected in their proposed work
    using Mask R-CNN and optimized anchors.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别表格中的行和列，KA Hashmi [[118](#bib.bib118)] 提出了一个用于表格结构识别的引导技术。根据这项研究，通过使用锚点优化方法，行和列的定位可以得到改善。他们提出的工作使用
    Mask R-CNN 和优化的锚点来检测行和列的边界。
- en: Another effort to segment tabular structures is the ReS2TIM paper by W Xue [[119](#bib.bib119)]
    which describes the reconstruction of syntactic structures from the table. Regressing
    the coordinates for each cell is this model’s main objective. A network that can
    identify the neighbors of each cell in a table is initially built using the new
    technique. In the study, a distance-based weighting system is given that will
    assist the network in overcoming the training-related class imbalance problem.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: W Xue [[119](#bib.bib119)] 的 ReS2TIM 论文提出了另一种分割表格结构的方法，描述了从表格中重建句法结构。该模型的主要目标是回归每个单元格的坐标。首先，使用新技术构建一个可以识别表格中每个单元格邻居的网络。在研究中，提供了一种基于距离的加权系统，这将帮助网络克服训练相关的类别不平衡问题。
- en: C Tensmeyer [[120](#bib.bib120)] has presented SPLERGE (Split and Merge), another
    method using dilated convolutions. Their strategy entails the use of two distinct
    deep learning models, the first of which establishes the grid-like layout of the
    table and the second of which determines if further cell spans over many rows
    or columns are possible.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: C Tensmeyer [[120](#bib.bib120)] 提出了另一种使用膨胀卷积的方法，称为 SPLERGE（Split and Merge）。他们的策略包括使用两个不同的深度学习模型，第一个模型建立表格的网格布局，第二个模型确定是否可能存在跨越多行或列的单元格。
- en: A Nassar [[68](#bib.bib68)] provide a fresh identification model for table structures.
    The latter enhances the most recent encoder-dual-decoder from PubTabNet end-to-end
    deep learning model in two important aspects. First, the authors provide a brand-new
    table-cell object detection decoder. This allows them to easily access the content
    of the table cells in programmatic PDFs without having to train any proprietary
    OCR decoders. The authors claim that this architectural improvement makes table-content
    extraction more precise and enables them to work with non-English tables. Second,
    transformer-based decoders take the place of LSTM decoders.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: A Nassar [[68](#bib.bib68)] 提供了一种全新的表格结构识别模型。该模型在两个重要方面增强了最新的 PubTabNet 端到端深度学习模型。首先，作者提供了一个全新的表格单元对象检测解码器。这使他们能够轻松访问程序化
    PDF 中的表格单元内容，而无需训练任何专有 OCR 解码器。作者声称，这一架构改进使得表格内容提取更加精确，并使他们能够处理非英语表格。其次，基于变换器的解码器取代了
    LSTM 解码器。
- en: S Raja [[121](#bib.bib121)] suggests a novel object-detection-based deep model
    that is tailored for quick optimization and captures the natural alignments of
    cells inside tables. Dense table recognition may still be problematic even with
    precise cell detection because multi-row/column spanning cells make it difficult
    to capture long-range row/column relationships. Therefore, the authors also seek
    to enhance structure recognition by determining a unique rectilinear graph-based
    formulation. The author emphasizes the relevance of empty cells in a table from
    a semantics standpoint. The authors recommend a modification to a well-liked assessment
    criterion to take these cells into consideration. To stimulate fresh perspectives
    on the issue, then provide a moderately large assessment dataset with annotations
    that are modeled after human cognition.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: S Raja [[121](#bib.bib121)] 提出了一个新颖的基于对象检测的深度模型，该模型专为快速优化而设计，并捕捉表格内单元格的自然对齐。即使在精确的单元格检测下，密集表格识别仍然可能存在问题，因为多行/列跨越的单元格使得捕捉远程行/列关系变得困难。因此，作者还试图通过确定一种独特的直线图基于公式来增强结构识别。作者从语义的角度强调了表格中空单元格的重要性。作者建议对一个受欢迎的评估标准进行修改，以考虑这些单元格。为了激发对该问题的新见解，然后提供一个带有基于人类认知建模注释的中等规模评估数据集。
- en: X Shen [[122](#bib.bib122)] suggested two modules, referred to as Rows Aggregated
    (RA) and Columns Aggregated (CA). First, to produce a rough forecast for the rows
    and columns and address the issue of high error tolerance, feature slicing and
    tiling are applied. Second, the attention maps of the channels are computed to
    further obtain the row and column information. In order to complete the rows segmentation
    and columns segmentation, the authors employ RA and CA to construct a semantic
    segmentation network termed the Rows and Columns Aggregated Network (RCANet).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: X Shen [[122](#bib.bib122)] 提出了两个模块，称为行聚合（RA）和列聚合（CA）。首先，应用特征切片和拼贴来生成行和列的粗略预测，并解决高误差容忍度的问题。其次，计算通道的注意力图以进一步获得行和列信息。为了完成行分割和列分割，作者使用
    RA 和 CA 构建了一个称为行列聚合网络（RCANet）的语义分割网络。
- en: C Ma[[123](#bib.bib123)] present RobusTabNet, a novel method for recognizing
    the structure of tables and detecting their borders from a variety of document
    pictures. The authors suggest using CornerNet as a new region proposal network
    to produce higher quality table proposals for Faster R-CNN, which has greatly
    increased the localization accuracy of Faster R-CNN for table identification.
    by utilizing only the minimal ResNet-18 backbone network. Additionally, the authors
    suggest a brand-new split-and-merge approach for recognizing table structures.
    In this method, each detected table is divided into a grid of cells using a novel
    spatial CNN separation line prediction module, and then a Grid CNN cell merging
    module is used to recover the spanning cells. Their table structure recognizer
    can accurately identify tables with significant blank areas and geometrically
    deformed (even curved) tables because the spatial CNN module can efficiently transmit
    contextual information throughout the whole table picture. B Xiao [[124](#bib.bib124)]
    postulates that a complex table structure may be represented by a graph, where
    the vertices and edges stand in for individual cells and the connections between
    them. Then, the authors design a conditional attention network and characterize
    the table structure identification issue as a cell association classification
    problem (CATT-Net).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: C Ma [[123](#bib.bib123)] 提出了RobusTabNet，这是一种新颖的方法，用于识别表格结构和从各种文档图片中检测其边界。作者建议使用CornerNet作为新的区域提议网络，以为Faster
    R-CNN生成更高质量的表格提议，这大大提高了Faster R-CNN在表格识别中的定位精度，仅利用了最小的ResNet-18主干网络。此外，作者建议了一种全新的分割和合并方法来识别表格结构。在这种方法中，每个检测到的表格使用一种新的空间CNN分割线预测模块被划分为单元网格，然后使用Grid
    CNN单元合并模块来恢复跨越的单元。由于空间CNN模块可以有效地在整个表格图片中传递上下文信息，他们的表格结构识别器可以准确识别具有显著空白区域和几何变形（甚至弯曲）的表格。B
    Xiao [[124](#bib.bib124)] 假设复杂的表格结构可以通过图形表示，其中顶点和边代表单个单元及其之间的连接。然后，作者设计了一个条件注意网络，并将表格结构识别问题描述为单元关联分类问题（CATT-Net）。
- en: 'A Jain [[125](#bib.bib125)] suggests training a deep network to recognize the
    spatial relationships between various word pairs included in the table picture
    in order to decipher the table structure. The authors offer an end-to-end pipeline
    called TSR-DSAW: TSR through Deep Spatial Association of Words, which generates
    a digital representation of a table picture in a structured format like HTML.
    The suggested technique starts by utilizing a text-detection network, such as
    CRAFT, to identify every word in the input table picture. Next, using dynamic
    programming, word pairings are created. These word pairings are underlined in
    each individual image and then given to a DenseNet-121 classifier that has been
    trained to recognize spatial correlations like same-row, same-column, same-cell,
    or none. Finally, The authors apply post-processing to the classifier output in
    order to produce the HTML table structure.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: A Jain [[125](#bib.bib125)] 建议训练一个深度网络，以识别表格图片中不同单词对之间的空间关系，从而解读表格结构。作者提出了一种称为TSR-DSAW的端到端管道：通过词语的深度空间关联来进行TSR，该方法将表格图片生成HTML等结构化格式的数字表示。建议的技术首先利用文本检测网络，如CRAFT，来识别输入表格图片中的每个单词。接着，使用动态规划创建单词配对。这些单词配对在每个单独的图像中被加下划线，然后交给一个经过训练的DenseNet-121分类器，该分类器被训练以识别空间关联，如同一行、同一列、同一单元或无。最后，作者对分类器输出应用后处理，以生成HTML表格结构。
- en: H Li [[126](#bib.bib126)] formulate the issue as a cell relation extraction
    challenge and provide T2, a cutting-edge two-phase method that successfully extracts
    table structures from digitally preserved texts. T2 offers a broad idea known
    as a prime connection that accurately represents the direct relationships between
    cells. To find complicated table structures, it also builds an alignment graph
    and uses a message-passing network.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: H Li [[126](#bib.bib126)] 将问题定义为一个单元关系提取挑战，并提供了T2，这是一种前沿的两阶段方法，成功地从数字保存的文本中提取表格结构。T2
    提供了一种广泛的思想，称为主连接，它准确地表示了单元之间的直接关系。为了寻找复杂的表格结构，它还构建了一个对齐图，并使用了消息传递网络。
- en: 'Table 3: A comparison of the benefits and drawbacks of several deep learning-based
    table Structure recognition methods'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：几种基于深度学习的表格结构识别方法的优缺点比较
- en: Literature Method Benefits Drawbacks SF Rashid [[114](#bib.bib114)] Uses the
    geometric pos- ition of words + A neu- ral network model (autoMLP) No reliance
    on complex layout analysis Mechanism. Can be used on the diverse set of documents
    with different layouts limitation is in marking columns boundaries due to variations
    in the number of words in each column E Koci [[110](#bib.bib110)] Encoding of
    spatial inter- relations between these regions using a graph rep- resentation,
    as well as rules and heuristics 1) Recognition for single-table and multi- table
    spreadsheets. 2) No reliance on any assumptions with what regards the arran- gement
    of tables Tables with few columns and empty cells are not handled well. SA Siddiqui
    [[51](#bib.bib51)] deformable CNN + Faster R-CNN 1) The use of deformable convolution
    can handle various tabular structures. 2) rel- eased a new dataset that contained
    table structure data. The tables in the proposed approach won’t operate co- rrectly
    if they have a row a- nd column span. SA Siddiqui [[112](#bib.bib112)] Fully CNNs
    The complexity of the task of identifying t- able structures is reduced by the
    proposed prediction tiling approach. 1) Additional post-processing p- rocesses
    are necessary when ro- ws or columns are excessively fragmented. 2) The technique
    is based on the tabular structures’ consistency assumption. SR Qasim [[115](#bib.bib115)]
    Graph NN + CNN 1) This paper also presents a unique, memory- efficient training
    strategy based on Monte Ca- rlo. 2) The suggested approach makes use of both textual
    and spatial characteristics. The publicly accessible table datasets are not used
    to test the system. W Xue [[119](#bib.bib119)] Graph NN + weights depending on
    distance For the cell relationship network, the class imbalance issue is solved
    using the distance- based weighting method. When dealing with sparse tab- les,
    the approach is insecure. C Tensmeyer [[120](#bib.bib120)] Dilated Convolutions
    + Fully CNN The technique is effective with both scanned and PDF document images.
    The post-processing heuristics determine how the merging p- ortion of the method
    works. SA Khan [[113](#bib.bib113)] RNN The reduced receptive field of CNNs is
    solved by the bi-directional GRU. Pre-processing procedures including binarization,
    noise reduction, and morphological modification are ne- cessary. P Riba [[54](#bib.bib54)]
    Graph Neural Networks approach 1) It is not constrained to rigid tabular lay-
    outs in terms of single rows, columns or pr- esence of rule lines. 2) The model
    is langua- ge independent 1) The method may have problems when dealing with border
    cond- itions. 2) There is a small amount of training data in the RVL-CDIP dataset
    and F1, Precision and Re- call metrics are lower than other methods. Y Deng [[52](#bib.bib52)]
    Encoder decoder net 1) In the work that is given, issues with end- to-end table
    recognition are examined. 2) Mad -e a contribution with yet another sizable data
    -set in the area of table comprehension. The other publicly accessible table recognition
    datasets are not used to assess the suggested base- line technique. E Koci [[111](#bib.bib111)]
    Graph model + Appl- ication of genetic-based approaches Requires little to no
    involvement of domain experts The accuracy of GE depends on the number of edges.
    Specifica- lly, we determined that GE ach- ieves an accuracy of only 19% for multi-table
    graphs D Prasad [[95](#bib.bib95)] Cascade mask Reg- ionbased CNN High- Resolution
    Network- based model Direct regression occurs at cellular bound- aries using an
    end-to-end method. Tables with/out ruling lin- es must undergo further post-processing.
    S Raja [[116](#bib.bib116)] Mask R-CNN + ResNet-101 based Net 1) An additional
    alignment loss is sugges- ted for precise cell detection. 2) A train- able top-down
    for cell identification and bottom-up for structure recognition coll- ection is
    proposed. When cells are empty, the strategy is weak.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 文献方法 利益 缺点 SF Rashid [[114](#bib.bib114)] 使用单词的几何位置 + 神经网络模型（autoMLP） 不依赖于复杂的布局分析机制。
    可以用于不同布局的多样化文档，但由于每列单词数量的变化，标记列边界的限制较大。 E Koci [[110](#bib.bib110)] 使用图表示法对这些区域的空间关系进行编码，以及规则和启发式方法
    1）适用于单表和多表电子表格。 2）不依赖于任何有关表格排列的假设。 列数较少和空单元格的表格处理效果较差。 SA Siddiqui [[51](#bib.bib51)]
    可变形CNN + Faster R-CNN 1）使用可变形卷积可以处理各种表格结构。 2）发布了一个包含表格结构数据的新数据集。 如果表格有行和列的跨越，提出的方法将无法正确操作。
    SA Siddiqui [[112](#bib.bib112)] 全卷积神经网络 通过提出的预测平铺方法减少了识别表格结构的任务复杂性。 1）当行或列过度碎片化时，需要额外的后处理过程。
    2）该技术基于表格结构的一致性假设。 SR Qasim [[115](#bib.bib115)] 图神经网络 + CNN 1）本文还提出了一种基于蒙特卡洛的独特、内存高效的训练策略。
    2）建议的方法利用了文本和空间特征。 没有使用公开可访问的表格数据集来测试系统。 W Xue [[119](#bib.bib119)] 图神经网络 + 基于距离的权重
    对于单元格关系网络，使用基于距离的加权方法解决了类别不平衡问题。 处理稀疏表格时，该方法不够安全。 C Tensmeyer [[120](#bib.bib120)]
    膨胀卷积 + 全卷积神经网络 该技术在扫描文档和PDF文档图像中都有效。 后处理启发式决定了合并部分的方法的工作方式。 SA Khan [[113](#bib.bib113)]
    RNN 双向GRU解决了CNN的感受野减少问题。 需要包括二值化、降噪和形态学修改在内的预处理步骤。 P Riba [[54](#bib.bib54)] 图神经网络方法
    1）在单行、列或规则线存在方面不受限制。 2）模型语言独立 1）该方法在处理边界条件时可能存在问题。 2）RVL-CDIP数据集中训练数据量较少，F1、精准度和召回率指标低于其他方法。
    Y Deng [[52](#bib.bib52)] 编码器-解码器网络 1）在给定的工作中，研究了端到端表格识别的问题。 2）在表格理解领域做出了另一个大型数据集的贡献。
    其他公开可访问的表格识别数据集未用于评估所建议的基线技术。 E Koci [[111](#bib.bib111)] 图模型 + 基于遗传算法的方法 几乎不需要领域专家的参与
    GE的准确性取决于边的数量。 具体来说，我们确定GE在多表格图上仅实现了19%的准确率。 D Prasad [[95](#bib.bib95)] 级联掩膜区域基础CNN
    高分辨率网络模型 通过端到端方法直接在单元边界处回归。 必须对有/无规则线的表格进行进一步的后处理。 S Raja [[116](#bib.bib116)]
    掩膜R-CNN + 基于ResNet-101的网络 1）建议为精确的单元格检测增加额外的对齐损失。 2）提出了一个可训练的自顶向下的单元格识别和自底向上的结构识别收集方法。
    当单元格为空时，该策略效果较差。
- en: 'Table 4: A comparison of the benefits and drawbacks of several deep learning-based
    table Structure recognition methods (continue Table [3](#S5.T3 "Table 3 ‣ 5.2
    Table Structure Recognition Models ‣ 5 Table detection and structure recognition
    Models ‣ Deep learning for table detection and structure recognition: A survey"))'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：几种基于深度学习的表格结构识别方法的优缺点比较（继续表 [3](#S5.T3 "表 3 ‣ 5.2 表格结构识别模型 ‣ 5 表格检测与结构识别模型
    ‣ 深度学习用于表格检测与结构识别：综述")）
- en: Literature Method Benefits Drawbacks B Xiao [[124](#bib.bib124)] cells’ bounding
    boxes + conditional attention network Only utilizes visual features without any
    meta- data 1) Assumes that the coordinates of cells in the table are known. 2)
    Difficulties with tables with- out borders Y Zou [[117](#bib.bib117)] Fully CNNs
    1) Using linked component analysis enhances the outcomes. 2) In a table, cells
    are segmen- ted in addition to the rows and columns. To provide comparison findings,
    a small number of post-process- ing procedures utilizing specific algorithms are
    necessary. X Zhong [[66](#bib.bib66)] Dual decoder with attention-based enc- oding
    1) To assess table recognition techniques, the methodology offers a unique evaluation
    metric called TEDS. 2) released a huge table dataset. The technique cannot be
    readily compared to other state-of-the- art techniques. KA Hashmi [[118](#bib.bib118)]
    Utilizing an optimi- zation technique for anchors+ Mask R- CNN Networks of region
    proposals converge mo- re quickly and effectively thanks to optim- ized anchoring.
    This study relies on the pre liminary pre-processing phase of clustering the ground
    truth to find appropriate anchors. A Zucker [[107](#bib.bib107)] Character Region
    Awareness for Text Detection (CRAFT) and Density-Based Spatial Clustering of Applications
    with Noise (DBSCAN) A bottom-up method, which emphasizes that the table structure
    is formed by relative pos- itions of text cells, and not by inherent bou- ndaries
    Cannot handle spreading rows or columns well X Zheng [[103](#bib.bib103)] Method
    for object detecting generally An additional innovative cluster-based tech nique
    combined with a hierarchical network to detect tabular forms. Accurately classifying
    a table is a prerequisite for final cell structure identification. Z Zhang [[108](#bib.bib108)]
    A combination of fully convo- lutional network (FCN)+ RoI- Align + the pretrained
    BERT model + Gated Recurrent Unit (GRU) decoder Directly operates on table images
    with no dependency on meta-information, can pro- cess simple and complex tables
    Oversegments tables when space between cells is large, doesn’t handle merged cells
    well M Namysl [[109](#bib.bib109)] Rule-based algorithms + graph-based table inter-
    pretation method 1) Approach allows processing images and digital documents. 2)
    Processing steps can be adapted separately 1) Support the most frequent ta- ble
    formats only. Reliance on the presence of predefined keywords. 2) Prone to the
    errors propagated from the upstream components of system. 3) Focus on the ta-
    bles with rulings A Nassar [[68](#bib.bib68)] End-to-end neural network + CNN
    Backbone + tran- sformer based layers 1) Handles different languages without being
    trained on them. 2) Predicts tables structure and bounding boxes for the table
    content Work with PDF documents A Jain [[125](#bib.bib125)] spatial associations
    + dyna- mic programming techniques Recognizing complex table structures having
    multi-span rows/columns and missing cells Uses OCR to read words from images Not
    language agnostic S Raja [[121](#bib.bib121)] object detection Better detection
    of empty cells Fails for very sparse tables wh- ere most of the cells are empty
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 文献方法 优势 缺点 B Xiao [[124](#bib.bib124)] 单元格的边界框 + 条件注意力网络 仅利用视觉特征而没有任何元数据 1)
    假设表格中单元格的坐标已知。 2) 对于没有边框的表格存在困难。 Y Zou [[117](#bib.bib117)] 完全卷积神经网络 1) 使用连接组件分析增强结果。
    2) 在表格中，单元格不仅要分割行和列。 为提供对比结果，需要少量的后处理步骤，使用特定算法。 X Zhong [[66](#bib.bib66)] 基于注意力的双解码器
    1) 为评估表格识别技术，该方法提供了一种称为TEDS的独特评估指标。 2) 发布了一个大型表格数据集。 该技术无法与其他最先进技术进行直接比较。 KA Hashmi
    [[118](#bib.bib118)] 利用锚点优化技术 + Mask R-CNN 区域提议的网络由于优化的锚定而更快有效地收敛。 本研究依赖于初步的预处理阶段，将真实数据进行聚类以找到合适的锚点。
    A Zucker [[107](#bib.bib107)] 文本检测的字符区域感知（CRAFT）和基于密度的空间聚类（DBSCAN） 一种自下而上的方法，强调表格结构是由文本单元格的相对位置形成的，而不是由固有的边界形成的。
    无法很好地处理扩展的行或列。 X Zheng [[103](#bib.bib103)] 一般物体检测方法 结合了创新的基于集群的技术与分层网络来检测表格形式。
    准确分类表格是最终单元格结构识别的前提。 Z Zhang [[108](#bib.bib108)] 全卷积网络（FCN）+ RoI-Align + 预训练BERT模型
    + 门控递归单元（GRU）解码器 直接在表格图像上操作，不依赖于元信息，可以处理简单和复杂的表格。 当单元格之间的空间较大时，会过度分割表格，不能很好地处理合并的单元格。
    M Namysl [[109](#bib.bib109)] 基于规则的算法 + 基于图的表格解释方法 1) 该方法允许处理图像和数字文档。 2) 处理步骤可以单独调整。
    1) 仅支持最常见的表格格式。 依赖于预定义关键字的存在。 2) 易受系统上游组件传播的错误影响。 3) 重点关注带有规则的表格。 A Nassar [[68](#bib.bib68)]
    端到端神经网络 + CNN骨干 + 基于变换器的层 1) 处理不同语言，而无需针对这些语言进行训练。 2) 预测表格结构和表格内容的边界框。 处理PDF文档。
    A Jain [[125](#bib.bib125)] 空间关联 + 动态编程技术 识别具有多跨度行/列和缺失单元格的复杂表格结构 使用OCR从图像中读取单词
    不是语言无关的。 S Raja [[121](#bib.bib121)] 物体检测 更好地检测空单元格 对于非常稀疏的表格，无法处理大多数单元格为空的情况。
- en: 6 Methodology
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 方法论
- en: In this section, we will extend the methodology and methods used for the TNCR
    dataset in our previous work [[67](#bib.bib67)]. in the previous work, we described
    Cascade R-CNN, Cascade Mask R-CNN, Cascade RPN, Hybrid Task Cascade (HTC), YOLO,
    and Deformable DETR. In this section, we will describe additional four methodologies
    of using object detection and classification for Faster R-CNN, Mask R-CNN, HRNets,
    Resnest, and Dynamic R-CNN.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将扩展在之前的工作中用于TNCR数据集的方法论和方法[[67](#bib.bib67)]。在之前的工作中，我们描述了Cascade R-CNN、Cascade
    Mask R-CNN、Cascade RPN、混合任务级联 (HTC)、YOLO 和 Deformable DETR。本节将描述额外的四种方法，涉及使用目标检测和分类的Faster
    R-CNN、Mask R-CNN、HRNets、Resnest和Dynamic R-CNN。
- en: 6.1 Faster R-CNN
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 Faster R-CNN
- en: 'Faster R-CNN [[127](#bib.bib127)] contains two modules: The RPN is a fully-convolutional
    network that generates region proposals, and the Fast-RCNN detector takes the
    proposal from RPN as input and generates object detection results as seen in Fig.
    [22](#S6.F22 "Figure 22 ‣ 6.1 Faster R-CNN ‣ 6 Methodology ‣ Deep learning for
    table detection and structure recognition: A survey"). A feature extraction network,
    which is often a pretrained CNN, is employed in a Faster R-CNN object detection
    network, similar to what we utilized for its predecessor. Following that, there
    are two trainable subnetworks. The first is a Region Proposal Network (RPN), which
    is used to produce object proposals as its name indicates, and the second is used
    to predict the object’s real class. The RPN that is put after the last convolutional
    layer is thus the major differentiator for Faster R-CNN. This has been taught
    to generate region proposals without the use of any external mechanisms such as
    Selective Search. Then, similar to Fast R-CNN, utilize ROI pooling, an upstream
    classifier, and a bounding box regressor.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN [[127](#bib.bib127)] 包含两个模块：RPN是一个完全卷积网络，生成区域提案，而Fast-RCNN检测器将RPN的提案作为输入，生成对象检测结果，如图
    [22](#S6.F22 "图 22 ‣ 6.1 Faster R-CNN ‣ 6 方法论 ‣ 表格检测和结构识别的深度学习：综述") 所示。Faster
    R-CNN目标检测网络中采用了一个特征提取网络，通常是一个预训练的CNN，这与我们为其前身所使用的类似。接下来，有两个可训练的子网络。第一个是区域提案网络（RPN），顾名思义，用于生成对象提案；第二个用于预测对象的实际类别。因此，放在最后一个卷积层后的RPN是Faster
    R-CNN的主要区分因素。它被教导在没有任何外部机制（如选择性搜索）的情况下生成区域提案。然后，类似于Fast R-CNN，使用ROI池化、上游分类器和边界框回归器。
- en: '![Refer to caption](img/744261d34b09af0cad671e01c48f0007.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/744261d34b09af0cad671e01c48f0007.png)'
- en: 'Figure 22: Faster R-CNN'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：Faster R-CNN
- en: 6.2 Mask R-CNN
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 Mask R-CNN
- en: 'Mask R-CNN [[128](#bib.bib128)] uses R-CNN to effectively detect objects in
    an image while also performing object segmentation tasks for each region of interest.
    As a result, segmentation runs concurrently with classification and bounding box
    regression. The high-level architecture of the Mask R-CNN is shown in Fig. [23](#S6.F23
    "Figure 23 ‣ 6.2 Mask R-CNN ‣ 6 Methodology ‣ Deep learning for table detection
    and structure recognition: A survey").'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN [[128](#bib.bib128)] 使用R-CNN有效地检测图像中的对象，同时对每个感兴趣区域执行对象分割任务。因此，分割与分类和边界框回归并行运行。Mask
    R-CNN的高级架构如图 [23](#S6.F23 "图 23 ‣ 6.2 Mask R-CNN ‣ 6 方法论 ‣ 表格检测和结构识别的深度学习：综述")
    所示。
- en: '![Refer to caption](img/92e90be23691a23544f1f4c11d914fc9.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92e90be23691a23544f1f4c11d914fc9.png)'
- en: 'Figure 23: Mask R-CNN'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：Mask R-CNN
- en: 'Mask RCNN is divided into two phases. First, it generates proposals based on
    the input image for regions where an object might be present. Second, based on
    the first stage proposals, it predicts the object’s class, refines the bounding
    box, and creates a mask at the pixel level of the object. The backbone structure
    is related to both phases. The concept of Mask R-CNN is simple: Faster R-CNN outputs
    a class label and a bounding-box offset for each candidate object; Mask R-CNN
    adds a third branch that outputs the object mask.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN分为两个阶段。首先，它根据输入图像生成可能存在对象的区域提案。其次，基于第一阶段的提案，它预测对象的类别、细化边界框，并在像素级别创建对象的掩码。主干结构与两个阶段都有关联。Mask
    R-CNN的概念很简单：Faster R-CNN为每个候选对象输出类别标签和边界框偏移量；Mask R-CNN增加了一个第三个分支，输出对象掩码。
- en: 6.3 HRNets
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 HRNets
- en: 'Ke Sun et at,[[129](#bib.bib129), [130](#bib.bib130)] present a novel architecture
    called High-Resolution Net, which is capable of maintaining high-resolution representations
    through the entire process. The first stage of the HRNet project is to build a
    high-resolution subnetwork. The next stage is to add more high-to-low resolutions
    subnetworks. The multi-scale fusions are carried out by HRNet through a parallel
    multi-resolution network as seen in Fig. [24](#S6.F24 "Figure 24 ‣ 6.3 HRNets
    ‣ 6 Methodology ‣ Deep learning for table detection and structure recognition:
    A survey").'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '孙克等人[[129](#bib.bib129), [130](#bib.bib130)]提出了一种称为High-Resolution Net的新型架构，能够在整个过程中保持高分辨率表示。HRNet项目的第一阶段是构建一个高分辨率子网络。接下来的阶段是添加更多的高到低分辨率子网络。HRNet通过并行多分辨率网络进行多尺度融合，如图[24](#S6.F24
    "Figure 24 ‣ 6.3 HRNets ‣ 6 Methodology ‣ Deep learning for table detection and
    structure recognition: A survey")所示。'
- en: '![Refer to caption](img/6ffee5c8fa57436bbc5d01599ca92f10.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6ffee5c8fa57436bbc5d01599ca92f10.png)'
- en: 'Figure 24: HRNet'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：HRNet
- en: 'Compared to existing widely-used networks [[131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134)], HRNet has two advantages: it can connect
    high-to-low-resolution subnetworks in parallel, and it can provide better pose
    estimation. Most existing fusion schemes combine low-resolution and high-resolution
    representations. Instead of doing so, HRNet performs multiscale fusions to boost
    both the high-resolution and low-resolution representations.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有广泛使用的网络[[131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134)]相比，HRNet有两个优势：可以并行连接高到低分辨率子网络，并且可以提供更好的姿态估计。大多数现有的融合方案将低分辨率和高分辨率表示结合在一起。而HRNet通过多尺度融合来提升高分辨率和低分辨率表示。
- en: 6.4 Resnest
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 Resnest
- en: 'Resnest [[135](#bib.bib135)] is a simple architecture that combines the features
    of a multipath network with a channel-wise attention strategy. It allows for the
    preservation of independent representations in the meta structure. As in a multi-path
    network, a Resnet network module performs a set of transformations on low-dimensional
    embeddings and concatenates their outputs. Each transformation is carried out
    with a different attention strategy to capture the interdependencies of a feature
    map. The key difference between the two is that the attention strategy is focused
    on the specific channel and not the whole network. The Split-Attention block is
    a computing unit that combines feature map group and split attention operations.
    A Split-Attention Block is depicted in Fig. [25](#S6.F25 "Figure 25 ‣ 6.4 Resnest
    ‣ 6 Methodology ‣ Deep learning for table detection and structure recognition:
    A survey").'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 'Resnest [[135](#bib.bib135)]是一种简单的架构，将多路径网络的特征与通道级注意力策略相结合。它允许在元结构中保留独立的表示。与多路径网络一样，Resnet网络模块对低维嵌入执行一组变换并连接其输出。每个变换都采用不同的注意力策略来捕捉特征图的相互依赖关系。两者的关键区别在于注意力策略专注于特定通道而非整个网络。Split-Attention块是一个计算单元，将特征图组和拆分注意力操作结合在一起。Split-Attention块如图[25](#S6.F25
    "Figure 25 ‣ 6.4 Resnest ‣ 6 Methodology ‣ Deep learning for table detection and
    structure recognition: A survey")所示。'
- en: '![Refer to caption](img/8c3a27199e25886696d26978b676f429.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8c3a27199e25886696d26978b676f429.png)'
- en: 'Figure 25: Resnest'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：Resnest
- en: 6.5 Dynamic R-CNN
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 Dynamic R-CNN
- en: 'Hongkai Zhang et al, [[136](#bib.bib136)] proposes Dynamic RCNN, a simple but
    effective method for maximizing the dynamic quality of object detection proposals.
    It is made up of two parts: Dynamic Label Assignment and Dynamic SmoothL1 Loss,
    which are used for classification and regression, respectively. First, adjust
    the IoU threshold for positive/negative samples based on the proposals distribution
    in the training procedure to train a better classifier that is discriminative
    for high IoU proposals. Set the threshold as the proposal’s IoU at a certain percentage
    because this can reflect the overall distribution’s quality. Change the shape
    of the regression loss function for regression to adaptively fit the regression
    label distribution change and ensure the contribution of high-quality samples
    to training.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 张宏凯等人[[136](#bib.bib136)]提出了Dynamic RCNN，这是一种简单但有效的方法，用于最大化目标检测提议的动态质量。它由两个部分组成：动态标签分配和动态SmoothL1
    Loss，分别用于分类和回归。首先，根据训练过程中的提议分布调整IoU阈值，以训练出对高IoU提议具有辨别力的更好分类器。将阈值设置为提议的IoU在某一百分比处，因为这可以反映整体分布的质量。改变回归损失函数的形状，以适应回归标签分布的变化，并确保高质量样本对训练的贡献。
- en: 7 Experiments Results
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 实验结果
- en: 7.1 Experiment Settings
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 实验设置
- en: The MMdetection [[137](#bib.bib137)] library for PyTorch has been used to implement
    each of the proposed and tested models. A vast variety of object detection and
    instance segmentation methods, as well as associated parts and modules, are included
    in the object detection toolkit known as MMDetection. It gradually transforms
    into a single platform that includes several widely used detection techniques
    and contemporary modules. Three Tesla V100-SXM GPUs with 16 GB of GPU memory,
    16 GB of RAM, two Intel Xeon E-5-2680 CPUs, and four NVIDIA Tesla k20x GPUs were
    used in the trials, which were carried out on the Google Colaboratory platform.
    With pictures scaled to a constant size of 1300 $\times$ 1500 and a batch size
    of 16, all models have been trained and evaluated. The optimizer with a momentum
    of 0.9, a weight decay of 0.0001, and a learning rate of 0.02 is known as SGD.
    The Feature Pyramid Network (FPN) neck is used by all models.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: MMdetection [[137](#bib.bib137)] 库在 PyTorch 上用于实现所提出和测试的每一个模型。对象检测工具包 MMDetection
    包含了各种对象检测和实例分割方法，以及相关的部分和模块。它逐渐演变成一个包括几种广泛使用的检测技术和现代模块的单一平台。在 Google Colaboratory
    平台上进行的试验中，使用了三台配备 16 GB GPU 内存的 Tesla V100-SXM GPU、16 GB RAM、两台 Intel Xeon E-5-2680
    CPU 和四台 NVIDIA Tesla k20x GPU。所有模型都已被训练和评估，图片缩放到 1300 $\times$ 1500 的恒定尺寸，批量大小为
    16。具有 0.9 动量、0.0001 权重衰减和 0.02 学习率的优化器称为 SGD。所有模型都使用了 Feature Pyramid Network
    (FPN) 颈部。
- en: 7.2 Results of TNCR dataset
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 TNCR 数据集的结果
- en: 'The Faster R-CNN model has achieved good performance in table detection compared
    with Cascade-RCNN and Cascade Mask-RCNN in most of the backbones. We have trained
    the Faster R-CNN model with L1 Loss [[138](#bib.bib138)] with Resnet-50 for bounding
    box regression. As shown in Table [5](#S7.T5 "Table 5 ‣ 7.2 Results of TNCR dataset
    ‣ 7 Experiments Results ‣ Deep learning for table detection and structure recognition:
    A survey"), it achieves f1-score of 0.921\. Resnet-101 backbone achieves the highest
    F1 score over 50% to 65%, ResNeXt-101-64x4d achieves the highest F1 score over
    70% to 95% and ResNeXt-101-64x4d achieves the highest F1 score over 50%:95% of
    0.786\. Resnet-50 backbone with 1$\times$ Lr schedule achieves the lowest performance
    over 50% to 60% IoUs. Also, the Resnet-50 backbone with L1 Los achieves the lowest
    performance from 65% to 95% IoUs and also achieves the lowest performance over
    50%:95%.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '在大多数骨干网络中，Faster R-CNN 模型在表格检测方面表现良好，相比于 Cascade-RCNN 和 Cascade Mask-RCNN。我们用
    L1 损失 [[138](#bib.bib138)] 和 Resnet-50 对 Faster R-CNN 模型进行了边界框回归训练。如表 [5](#S7.T5
    "Table 5 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning
    for table detection and structure recognition: A survey") 所示，它的 f1-score 为 0.921。Resnet-101
    骨干网络在 50% 到 65% 之间取得了最高的 F1 分数，ResNeXt-101-64x4d 在 70% 到 95% 之间取得了最高的 F1 分数，ResNeXt-101-64x4d
    在 50%:95% 之间取得了 0.786 的 F1 分数。Resnet-50 骨干网络在 1$\times$ Lr 调度下的表现最低，在 50% 到 60%
    IoUs 之间。同样，Resnet-50 骨干网络与 L1 损失的表现也在 65% 到 95% IoUs 之间最低，并且在 50%:95% 的范围内表现也最低。'
- en: 'Table 5: Faster R-CNN'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：Faster R-CNN
- en: Backbone Lr schd/Losses IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95%
    Resnet-50 L1Loss Precision 0.875 0.872 0.872 0.866 0.858 0.844 0.823 0.782 0.688
    0.424 0.649 Recall 0.973 0.972 0.970 0.964 0.956 0.941 0.922 0.890 0.812 0.577
    0.775 F1-Score 0.921 0.919 0.918 0.912* 0.904* 0.889* 0.869* 0.832* 0.744* 0.488*
    0.706* Resnet-50 1x Precision 0.874 0.872 0.871 0.869 0.863 0.844 0.827 0.783
    0.693 0.431 0.653 Recall 0.972 0.969 0.968 0.966 0.959 0.942 0.929 0.895 0.823
    0.587 0.779 F1-Score 0.920* 0.917* 0.916* 0.914 0.908 0.890 0.875 0.835 0.752
    0.497 0.710 Resnet-101 1x Precision 0.885 0.885 0.882 0.879 0.870 0.867 0.849
    0.820 0.763 0.555 0.720 Recall 0.973 0.973 0.971 0.969 0.961 0.956 0.943 0.920
    0.870 0.698 0.835 F1-Score 0.926 0.926 0.924 0.921 0.913 0.909 0.893 0.867 0.812
    0.618 0.773 ResNeXt-101-32x4d 1x Precision 0.880 0.879 0.877 0.875 0.872 0.866
    0.845 0.817 0.760 0.575 0.727 Recall 0.976 0.976 0.975 0.972 0.969 0.962 0.944
    0.921 0.871 0.711 0.843 F1-Score 0.925 0.924 0.923 0.920 0.917 0.911 0.891 0.865
    0.811 0.635 0.780 ResNeXt-101-64x4d 1x Precision 0.884 0.884 0.880 0.879 0.876
    0.871 0.856 0.833 0.780 0.581 0.733 Recall 0.972 0.970 0.969 0.967 0.965 0.961
    0.950 .931 0.884 0.724 0.848 F1-Score 0.925 0.925 0.922 0.920 0.918 0.913 0.900
    0.879 0.828 0.644 0.786
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Backbone Lr schd/Losses IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95%
    Resnet-50 L1Loss 精度 0.875 0.872 0.872 0.866 0.858 0.844 0.823 0.782 0.688 0.424
    0.649 召回率 0.973 0.972 0.970 0.964 0.956 0.941 0.922 0.890 0.812 0.577 0.775 F1-分数
    0.921 0.919 0.918 0.912* 0.904* 0.889* 0.869* 0.832* 0.744* 0.488* 0.706* Resnet-50
    1x 精度 0.874 0.872 0.871 0.869 0.863 0.844 0.827 0.783 0.693 0.431 0.653 召回率 0.972
    0.969 0.968 0.966 0.959 0.942 0.929 0.895 0.823 0.587 0.779 F1-分数 0.920* 0.917*
    0.916* 0.914 0.908 0.890 0.875 0.835 0.752 0.497 0.710 Resnet-101 1x 精度 0.885
    0.885 0.882 0.879 0.870 0.867 0.849 0.820 0.763 0.555 0.720 召回率 0.973 0.973 0.971
    0.969 0.961 0.956 0.943 0.920 0.870 0.698 0.835 F1-分数 0.926 0.926 0.924 0.921
    0.913 0.909 0.893 0.867 0.812 0.618 0.773 ResNeXt-101-32x4d 1x 精度 0.880 0.879
    0.877 0.875 0.872 0.866 0.845 0.817 0.760 0.575 0.727 召回率 0.976 0.976 0.975 0.972
    0.969 0.962 0.944 0.921 0.871 0.711 0.843 F1-分数 0.925 0.924 0.923 0.920 0.917
    0.911 0.891 0.865 0.811 0.635 0.780 ResNeXt-101-64x4d 1x 精度 0.884 0.884 0.880
    0.879 0.876 0.871 0.856 0.833 0.780 0.581 0.733 召回率 0.972 0.970 0.969 0.967 0.965
    0.961 0.950 0.931 0.884 0.724 0.848 F1-分数 0.925 0.925 0.922 0.920 0.918 0.913
    0.900 0.879 0.828 0.644 0.786
- en: 'We implemented Mask R-CNN [[128](#bib.bib128)] to use R-CNN for table objects
    in an image and also used for performing object segmentation for each ROI. As
    seen in Table [6](#S7.T6 "Table 6 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey"),
    Mask R-CNN shows good performance in our dataset in precision, recall, and F1
    score for all backbones. Resnet-101 backbone has achieved the highest F1 score
    of 0.774 over 50%:95% and maintains the highest F1 score at various IoUs. ResNeXt-101-32x4d
    achieves the lowest performance over 50% to 95% IoUs and also achieves an f1 score
    of 0.512 over 50%:95%. ResNeXt-101-64x4d also achieves the lowest performance
    at various IoUs except for 95% IoU.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实现了 Mask R-CNN [[128](#bib.bib128)] 用于图像中的表格对象，并且还用于对每个 ROI 进行对象分割。正如在表 [6](#S7.T6
    "Table 6 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning
    for table detection and structure recognition: A survey") 中所示，Mask R-CNN 在我们的数据集中，在所有骨干网络中显示了良好的精度、召回率和
    F1 分数。Resnet-101 骨干网络在 50%:95% 范围内达到了最高的 F1 分数 0.774，并在各种 IoU 上保持了最高的 F1 分数。ResNeXt-101-32x4d
    在 50% 到 95% IoU 范围内表现最差，并且在 50%:95% 范围内达到了 F1 分数 0.512。除了 95% IoU 外，ResNeXt-101-64x4d
    在各种 IoU 上也表现最差。'
- en: 'Table 6: Mask R-CNN'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: Mask R-CNN'
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% Resnet-50
    1x Precision 0.877 0.876 0.874 0.871 0.868 0.858 0.834 0.800 0.728 0.506 0.692
    Recall 0.973 0.972 0.970 0.967 0.963 0.952 0.932 0.903 0.840 0.651 0.812 F1-Score
    0.922 0.921 0.919 0.916 0.913 0.902 0.880 0.848 0.779 0.569 0.747 Resnet-101 1x
    Precision 0.878 0.877 0.875 0.874 0.869 0.861 0.847 0.812 0.762 0.553 0.716 Recall
    0.977 0.976 0.974 0.973 0.966 0.959 0.949 0.918 0.874 0.711 0.844 F1-Score 0.924
    0.923 0.921 0.920 0.914 0.907 0.895 0.861 0.814 0.622 0.774 ResNeXt-101-32x4d
    1x Precision 0.778 0.777 0.774 0.769 0.759 0.749 0.713 0.651 0.477 0.407 0.434
    Recall 0.975 0.974 0.968 0.964 0.952 0.941 0.913 0.856 0.725 0.695 0.626 F1-Score
    0.865* 0.864* 0.860* 0.855* 0.844* 0.834* 0.800* 0.739* 0.575* 0.513* 0.512* ResNeXt-101-64x4d
    1x Precision 0.778 0.777 0.774 0.769 0.759 0.749 0.713 0.651 0.477 0.417 0.434
    Recall 0.975 0.974 0.968 0.964 0.952 0.941 0.913 0.856 0.725 0.705 0.626 F1-Score
    0.865 0.864 0.860 0.855 0.844 0.834 0.800 0.739 0.575 0.524 0.512
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following [tables 7](#S7.T7 "In 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey"),
    [8](#S7.T8 "Table 8 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep
    learning for table detection and structure recognition: A survey"), [9](#S7.T9
    "Table 9 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning
    for table detection and structure recognition: A survey"), [10](#S7.T10 "Table
    10 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning for table
    detection and structure recognition: A survey"), [11](#S7.T11 "Table 11 ‣ 7.2
    Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning for table detection
    and structure recognition: A survey") and [12](#S7.T12 "Table 12 ‣ 7.2 Results
    of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning for table detection and
    structure recognition: A survey") show the comparative analysis we have trained
    HRNets with different methods and each method with different backbones for object
    detection and instance segmentation models. In Table [7](#S7.T7 "Table 7 ‣ 7.2
    Results of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning for table detection
    and structure recognition: A survey"), we evaluated and calculated f1-score. It
    shows that HRNetV2p-W40 achieves better performance over 50% to 65%. Also, HRNetV2p-W18
    achieves better performance over 70%, 75%, 80%, and 95%. HRNetV2p-W18 achieves
    an f1 score of 0.842 over 50%:95% IoU. HRNetV2p-W32 backbone achieves the lowest
    performance over 50% to 70%, and 90% IoUs and HRNetV2p-W40 achieve the lowest
    performance over 75% to 85% and 95%. HRNetV2p-W40 achieves f1 score of 0.841 over
    50%:95% IoU.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: HRNets - Cascade R-CNN'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    20e Precision 0.894 0.894 0.894 0.892 0.892 0.886 0.880 0.862 0.825 0.712 0.803
    Recall 0.962 0.962 0.960 0.960 0.960 0.954 0.950 0.937 0.906 0.813 0.887 F1-Score
    0.926 0.926 0.925 0.924 0.924 0.918 0.913 0.897 0.863 0.759 0.842 HRNetV2p-W32
    20e Precision 0.895 0.895 0.893 0.893 0.893 0.889 0.881 0.869 0.828 0.717 0.806
    Recall 0.955 0.955 0.954 0.954 0.953 0.949 0.943 0.933 0.900 0.810 0.882 F1-Score
    0.924* 0.924* 0.922* 0.922* 0.922* 0.918 0.910 0.899 0.862* 0.760 0.842 HRNetV2p-W40
    20e Precision 0.893 0.891 0.891 0.891 0.888 0.880 0.871 0.854 0.831 0.705 0.799
    Recall 0.967 0.965 0.965 0.964 0.961 0.956 0.948 0.935 0.914 0.811 0.889 F1-Score
    0.928 0.926 0.926 0.926 0.923 0.916* 0.907* 0.892* 0.870 0.754* 0.841*
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    20e Precision 0.894 0.894 0.894 0.892 0.892 0.886 0.880 0.862 0.825 0.712 0.803
    Recall 0.962 0.962 0.960 0.960 0.960 0.954 0.950 0.937 0.906 0.813 0.887 F1-Score
    0.926 0.926 0.925 0.924 0.924 0.918 0.913 0.897 0.863 0.759 0.842 HRNetV2p-W32
    20e Precision 0.895 0.895 0.893 0.893 0.893 0.889 0.881 0.869 0.828 0.717 0.806
    Recall 0.955 0.955 0.954 0.954 0.953 0.949 0.943 0.933 0.900 0.810 0.882 F1-Score
    0.924* 0.924* 0.922* 0.922* 0.922* 0.918 0.910 0.899 0.862* 0.760 0.842 HRNetV2p-W40
    20e Precision 0.893 0.891 0.891 0.891 0.888 0.880 0.871 0.854 0.831 0.705 0.799
    Recall 0.967 0.965 0.965 0.964 0.961 0.956 0.948 0.935 0.914 0.811 0.889 F1-Score
    0.928 0.926 0.926 0.926 0.923 0.916* 0.907* 0.892* 0.870 0.754* 0.841*
- en: 'Table [8](#S7.T8 "Table 8 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results
    ‣ Deep learning for table detection and structure recognition: A survey") shows
    the performance of the HRNets Faster R-CNN detector with various backbone structures
    with combinations of Lr Schedule. The HRNetV2p-W18 with 1$\times$ Lr Schedule
    backbone shows a low performance compared with other backbones. it achieves an
    f1 score of 0.770\. It achieves 3.2% less than HRNetV2p-W18 with 2$\times$ Lr
    Schedule. HRNetV2p-W40 with 1$\times$ Lr Schedule backbone achieves better performance
    over 50% to 85% IoUs and HRNetV2p-W40 with 2$\times$ Lr Schedule backbone achieves
    better performance over 90% and 95% IoUs. HRNetV2p-W18 with 2$\times$ Lr Schedule
    backbone achieves an f1 score of 0.802 over 50%:95%. HRNetV2p-W32 with 1$\times$
    Lr Schedule backbone share same performance over 50% to 60%.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](#S7.T8 "Table 8 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results
    ‣ Deep learning for table detection and structure recognition: A survey")展示了不同主干结构与Lr调度组合下HRNet
    Faster R-CNN检测器的性能。与其他主干结构相比，HRNetV2p-W18搭配1$\times$ Lr调度的主干表现较低。其f1得分为0.770，较HRNetV2p-W18搭配2$\times$
    Lr调度低3.2%。HRNetV2p-W40搭配1$\times$ Lr调度的主干在50%至85% IoUs范围内表现更好，而HRNetV2p-W40搭配2$\times$
    Lr调度的主干在90%和95% IoUs范围内表现更好。HRNetV2p-W18搭配2$\times$ Lr调度的主干在50%:95% IoUs范围内的f1得分为0.802。HRNetV2p-W32搭配1$\times$
    Lr调度的主干在50%到60%范围内表现相同。'
- en: 'Table 8: HRNets - Faster R-CNN'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：HRNets - Faster R-CNN
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    1x Precision 0.867 0.865 0.863 0.859 0.853 0.845 0.827 0.806 0.750 0.556 0.711
    Recall 0.972 0.970 0.968 0.964 0.959 0.952 0.940 0.915 0.869 0.711 0.842 F1-Score
    0.916* 0.914* 0.912* 0.908* 0.902* 0.895* 0.879* 0.857* 0.805* 0.624* 0.770* HRNetV2p-W18
    2x Precision 0.876 0.873 0.872 0.869 0.867 0.857 0.845 0.817 0.776 0.628 0.752
    Recall 0.962 0.960 0.958 0.955 0.953 0.946 0.937 0.910 0.874 0.759 0.860 F1-Score
    0.916 0.914 0.912 0.909 0.907 0.899 0.888 0.860 0.822 0.687 0.802 HRNetV2p-W32
    1x Precision 0.877 0.876 0.874 0.869 0.862 0.859 0.839 0.822 0.759 0.579 0.728
    Recall 0.969 0.968 0.967 0.963 0.957 0.954 0.939 0.922 0.870 0.728 0.849 F1-Score
    0.920 0.919 0.918 0.913 0.907 0.904 0.886 0.869 0.810 0.645 0.783 HRNetV2p-W32
    2x Precision 0.877 0.877 0.877 0.874 0.869 0.864 0.847 0.820 0.785 0.592 0.735
    Recall 0.964 0.964 0.963 0.960 0.956 0.951 0.939 0.918 0.886 0.734 0.849 F1-Score
    0.918 0.918 0.917 0.914 0.910 0.905 0.890 0.866 0.832 0.655 0.787 HRNetV2p-W40
    1x Precision 0.875 0.874 0.873 0.872 0.868 0.862 0.851 0.827 0.779 0.612 0.743
    Recall 0.970 0.969 0.968 0.967 0.964 0.958 0.949 0.930 0.888 0.753 0.862 F1-Score
    0.920 0.919 0.918 0.917 0.913 0.907 0.897 0.875 0.829 0.675 0.798 HRNetV2p-W40
    2x Precision 0.880 0.880 0.877 0.877 0.873 0.861 0.852 0.834 0.802 0.629 0.754
    Recall 0.957 0.957 0.954 0.954 0.951 0.943 0.935 0.918 0.890 0.755 0.856 F1-Score
    0.916 0.916 0.913 0.913 0.910 0.900 0.891 0.873 0.843 0.686 0.801
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [9](#S7.T9 "Table 9 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results
    ‣ Deep learning for table detection and structure recognition: A survey") shows
    the performance of the HRNets HTC method with the same Lr Schedule. HRNetV2p-W40
    backbone suffers from overfitting through the dataset. HRNetV2p-W18 achieves f1
    score of 0.840, precision of 0.901 and recall 0.788 over 50%:95%. HRNetV2p-W18
    achieves better performance over various IoUs. HRNetV2p-W32 shows less performance
    compare with HRNetV2p-W18 with 8.3% over 50%:95% for the f1 score.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: HRNets - HTC'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    20e Precision 0.885 0.885 0.883 0.882 0.881 0.875 0.862 0.849 0.808 0.691 0.788
    Recall 0.987 0.987 0.984 0.984 0.982 0.976 0.966 0.954 0.915 0.816 0.901 F1-Score
    0.933 0.933 0.930 0.930 0.928 0.922 0.911 0.898 0.858 0.748 0.840 HRNetV2p-W32
    20e Precision 0.851 0.851 0.849 0.846 0.843 0.834 0.816 0.792 0.737 0.516 0.684
    Recall 0.985 0.985 0.984 0.981 0.976 0.968 0.951 0.929 0.885 0.710 0.848 F1-Score
    0.913* 0.913* 0.911* 0.908* 0.904* 0.896* 0.878* 0.855* 0.804* 0.597* 0.757*
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [10](#S7.T10 "Table 10 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey")
    shows the performance of HRNets HTC method with the same Lr Schedule. HRNetV2p-W32
    has achieved the highest f1 score of 0.871 over 50%:95% and continues to achieve
    the highest F1 score at various IoUs. HRNetV2p-W32 shows good performance compare
    with HRNetV2p-W18 with 12% over 50%:95% for f1 score. HRNetV2p-W40 with 1$\times$
    and 2$\times$ Lr Schedule backbones suffer from overfitting through the dataset.
    Table [11](#S7.T11 "Table 11 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results
    ‣ Deep learning for table detection and structure recognition: A survey") shows
    the performance of HRNets Cascade Mask R-CNN method. HRNetV2p-W32 and HRNetV2p-W40
    backbones suffer from overfitting through the dataset. HRNetV2p-W18 achieve f1
    score of 0.903 over 50%:95%.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: HRNets - Mask R-CNN'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    1x Precision 0.848 0.845 0.840 0.839 0.835 0.829 0.817 0.793 0.736 0.521 0.684
    Recall 0.971 0.969 0.966 0.964 0.960 0.956 0.947 0.928 0.876 0.698 0.834 F1-Score
    0.905* 0.902* 0.898* 0.897* 0.893* 0.887* 0.877* 0.855* 0.799* 0.596* 0.751* HRNetV2p-W32
    1x Precision 0.859 0.857 0.857 0.857 0.852 0.848 0.833 0.816 0.764 0.585 0.816
    Recall 0.971 0.969 0.969 0.969 0.965 0.960 0.947 0.934 0.889 0.744 0.934 F1-Score
    0.911 0.909 0.909 0.909 0.904 0.900 0.886 0.871 0.821 0.654 0.871
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: HRNets - Cascade Mask R-CNN'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    20e Precision 0.888 0.887 0.887 0.886 0.885 0.884 0.872 0.858 0.828 0.732 0.810
    Recall 0.970 0.970 0.970 0.967 0.967 0.965 0.955 0.942 0.918 0.836 0.903 F1-Score
    0.927 0.926 0.926 0.924 0.924 0.922 0.911 0.898 0.870 0.780 0.903
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [12](#S7.T12 "Table 12 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey")
    shows the performance of HRNets with Fully Convolutional One-Stage (FCOS) Object
    Detection. HRNets FCOS achieves less performance compared with other models. HRNetV2p-W18
    with 2$\times$ Lr Schedule achieve an increment of 25% f1 score from HRNetV2p-W18
    with 1$\times$ Lr Schedule and HRNetV2p-W18 with 2$\times$ Lr Schedule achieve
    an increment of 22.5% f1 score from HRNetV2p-W32\. HRNetV2p-W18 with 2$\times$
    Lr Schedule achieve an f1 score of 0.648.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: HRNets - FCOS'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% HRNetV2p-W18
    1x Precision 0.511 0.507 0.498 0.485 0.467 0.441 0.405 0.328 0.222 0.086 0.298
    Recall 0.959 0.946 0.930 0.910 0.885 0.844 0.798 0.697 0.517 0.244 0.601 F1-Score
    0.666* 0.660* 0.648* 0.632* 0.611* 0.579* 0.537* 0.446* 0.310* 0.127* 0.398* HRNetV2p-W18
    2x Precision 0.790 0.788 0.782 0.779 0.770 0.759 0.729 0.691 0.596 0.335 0.563
    Recall 0.983 0.978 0.972 0.969 0.959 0.947 0.917 0.878 0.786 0.545 0.764 F1-Score
    0.875 0.872 0.866 0.863 0.854 0.842 0.812 0.773 0.677 0.414 0.648 HRNetV2p-W32
    1x Precision 0.566 0.561 0.555 0.539 0.528 0.504 0.469 0.400 0.275 0.086 0.326
    Recall 0.970 0.964 0.956 0.928 0.906 0.868 0.818 0.730 0.571 0.241 0.605 F1-Score
    0.714 0.709 0.702 0.681 0.667 0.637 0.596 0.516 0.371 0.126 0.423
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following [tables 13](#S7.T13 "In 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey")
    and [14](#S7.T14 "Table 14 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results
    ‣ Deep learning for table detection and structure recognition: A survey") show
    the comparative analysis, we have trained ResNeSt with Cascade R-CNN and Faster
    R-CNN methods, and each method with different backbones(Resnest-50, Resnest-101).
    For Cascade R-CNN S-101 backbone achieve an f1 score of 0.845 over 50%:95% IoU
    and also achieves the highest performance compare with the S-50 backbone and the
    Faster R-CNN method. The Faster R-CNN S-101 backbone achieves an f1 score of 0.748
    over 50%:95% IoU. Cascade R-CNN S-101 backbone has an increment of 9.2% over 50%:95%
    for the f1 score.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic RCNN proposes by [[136](#bib.bib136)], it is a simple but effective
    method for maximizing the dynamic quality of object detection proposals. Table
    [15](#S7.T15 "Table 15 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments Results ‣
    Deep learning for table detection and structure recognition: A survey") show the
    Dynamic RCNN with Resnet-50 achieves an f1 score of 0.628, the precision of 0.561,
    recall of 0.714 over 50%:95%.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: Resnest - Cascade R-CNN'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% S-50 1x
    Precision 0.895 0.894 0.891 0.885 0.881 0.875 0.870 0.854 0.808 0.659 0.777 Recall
    0.977 0.976 0.974 0.969 0.965 0.959 0.954 0.940 0.903 0.784 0.880 F1-Score 0.934
    0.933 0.930 0.925 0.921 0.915 0.910 0.894 0.852 0.716 0.825 S-101 1x Precision
    0.905 0.903 0.902 0.899 0.893 0.891 0.884 0.876 0.826 0.693 0.799 Recall 0.985
    0.984 0.983 0.979 0.976 0.972 0.965 0.958 0.917 0.811 0.898 F1-Score 0.943 0.941
    0.940 0.937 0.932 0.929 0.922 0.915 0.869 0.747 0.845
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: Resnest - Faster R-CNN'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% S-50 1x
    Precision 0.884 0.884 0.880 0.879 0.872 0.861 0.844 0.809 0.709 0.429 0.656 Recall
    0.970 0.970 0.968 0.967 0.961 0.951 0.935 0.906 0.824 0.597 0.784 F1-Score 0.925
    0.925 0.921 0.920 0.914 0.903 0.887 0.854 0.762 0.499 0.714 S-101 1x Precision
    0.893 0.893 0.890 0.888 0.879 0.876 0.862 0.823 0.747 0.495 0.694 Recall 0.981
    0.979 0.977 0.975 0.967 0.963 0.950 0.921 0.861 0.645 0.813 F1-Score 0.934 0.934
    0.931 0.929 0.920 0.917 0.903 0.869 0.799 0.560 0.748
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 15: Dynamic R-CNN'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Lr schd IoU 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95% Resnet-50
    1x Precision 0.855 0.854 0.853 0.849 0.839 0.823 0.802 0.764 0.646 0.267 0.561
    Recall 0.978 0.977 0.975 0.971 0.963 0.943 0.925 0.888 0.793 0.451 0.714 F1-Score
    0.912 0.911 0.909 0.905 0.896 0.878 0.859 0.821 0.711 0.335 0.628
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 16: Table detection'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Approach Dataset Method IoU Year 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95%
    Tesseract [[88](#bib.bib88)] UNLV Tab-stop Detection Precision - - - - - - - -
    86.00 - - 2010 Recall - - - - - - - - 79.00 - - F1-Score - - - - - - - - 82.35
    - - A Gilani[[22](#bib.bib22)] UNLV Faster R-CNN Precision - - - - - - - - 82.30
    - - 2017 Recall - - - - - - - - 90.67 - - F1-Score - - - - - - - - 86.29 - - SA
    Siddiqui[[98](#bib.bib98)] UNLV Deformable CNN + Faster R-CNN Precision 78.6 -
    - - - - - - - - - 2018 Recall 74.9 - - - - - - - - - - F1-Score 76.7 - - - - -
    - - - - - Á Casado-García[[102](#bib.bib102)] UNLV YOLO Precision - - 93.0 - 92.0
    - 83.0 - 48.0 - - 2020 Recall - - 95.0 - 94.0 - 85.0 - 49.0 - - F1-Score - - 94.0
    - 93.0 - 84.0 - 49.0 - - M Agarwal [[92](#bib.bib92)] UNLV Cascade mask R-CNN
    Precision 96.0 - 94.4 - 91.5 - 82.6 - 61.8 - - 2018 Recall 77.0 - 75.8 - 73.4
    - 66.3 - 49.6 - - F1-Score 86.5 - 85.1 - 82.5 - 74.4 - 55.7 - - S Schreiber[[20](#bib.bib20)]
    ICDAR2013 Mask R-CNN Precision 97.40 - - - - - - - - - - 2017 Recall 96.15 - -
    - - - - - - - - F1-Score 96.77 - - - - - - - - - - SA Siddiqui[[51](#bib.bib51)]
    ICDAR2013 Deformable CNN Precision 99.6 - - - - - - - - - - 2018 Recall 99.6 -
    - - - - - - - - - F1-Score 99.6 - - - - - - - - - - I Kavasidis[[100](#bib.bib100)]
    ICDAR2013 Semantic Image Segmentation Precision 97.5 - - - - - - - - - - 2019
    Recall 98.1 - - - - - - - - - - F1-Score 97.8 - - - - - - - - - - Y Huang[[97](#bib.bib97)]
    ICDAR2013 YOLO Precision 100 - 98.6 - - - 89.2 - - - - 2019 Recall 94.9 - 93.6
    - - - 84.6 - - - - F1-Score 97.3 - 96.1 - - - 86.8 - - - - SS Paliwal[[96](#bib.bib96)]
    ICDAR2013 fully convolutions Precision 96.97 - - - - - - - - - - 2019 Recall 96.28
    - - - - - - - - - - F1-Score 96.62 - - - - - - - - - - Á Casado-García[[102](#bib.bib102)]
    ICDAR2013 Mask R-CNN Precision - - 70.0 - 70.0 - 70.0 - 47.0 - - 2020 Recall -
    - 97.0 - 97.0 - 97.0 - 65.0 - - F1-Score - - 81.0 - 81.0 - 81.0 - 54.0 - - D Prasad[[95](#bib.bib95)]
    ICDAR2013 Cascade mask R-CNN HRNet Precision 100 - - - - - - - - - - 2020 Recall
    100 - - - - - - - - - - F1-Score 100 - - - - - - - - - - M Li[[61](#bib.bib61)]
    ICDAR2013 Faster R-CNN Precision 96.58 - - - - - - - - - - 2020 Recall 95.94 -
    - - - - - - - - - F1-Score 96.25 - - - - - - - - - - M Agarwal [[92](#bib.bib92)]
    ICDAR2013 Cascade mask R-CNN Precision 100.0 - 100.0 - 98.7 - 94.2 - 66.0 - -
    2021 Recall 100.0 - 100.0 - 98.7 - 94.2 - 66.0 - - F1-Score 100.0 - 100.0 - 98.7
    - 94.2 - 66.0 - - X Zheng[[103](#bib.bib103)] ICDAR2013 object detection networks
    Precision 98.97 - - - - - - - - - - 2021 Recall 99.77 - - - - - - - - - - F1-Score
    99.31 - - - - - - - - - - SA Siddiqui[[51](#bib.bib51)] ICDAR2017 Deformable CNN
    Precision - - 96.5 - - - 96.7 - - - - 2018 Recall - - 97.1 - - - 93.7 - - - -
    F1-Score - - 96.8 - - - 95.2 - - - - Y Huang[[97](#bib.bib97)] ICDAR2017 YOLO
    Precision - - 97.8 - - - 97.5 - - - - 2019 Recall - - 97.2 - - - 96.8 - - - -
    F1-Score - - 97.5 - - - 97.1 - - - -
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 17: Table detection (Continue Table [16](#S7.T16 "Table 16 ‣ 7.2 Results
    of TNCR dataset ‣ 7 Experiments Results ‣ Deep learning for table detection and
    structure recognition: A survey"))'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Approach Dataset Method IoU Year 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95%
    Y Li[[104](#bib.bib104)] ICDAR2017 Generative Adversarial Networks(GAN) Precision
    - - 94.4 - - - 90.3 - - - - 2019 Recall - - 94.4 - - - 90.3 - - - - F1-Score -
    - 94.4 - - - 90.3 - - - - N Sun [[99](#bib.bib99)] ICDAR2017 Faster R-CNN Precision
    - - - - - - 94.3 - - - - 2019 Recall - - - - - - 95.6 - - - - F1-Score - - - -
    - - 94.9 - - - - Á Casado-García[[102](#bib.bib102)] ICDAR2017 RetinaNet Precision
    - - 92.0 - 92.0 - 89.0 - 79.0 - - 2020 Recall - - 87.0 - 87.0 - 84.0 - 75.0 -
    - F1-Score - - 89.0 - 89.0 - 86.0 - 77.0 - - M Agarwal [[92](#bib.bib92)] ICDAR2017
    Cascade mask R-CNN Precision - - 96.9 - - - - - - - - 2021 Recall - - 89.9 - -
    - - - - - - F1-Score - - 93.4 - - - - - - - - D Prasad[[95](#bib.bib95)] ICDAR2019
    Cascade mask R-CNN HRNet Precision - - - - - - - - - - - 2020 Recall - - - - -
    - - - - - - F1-Score - - 94.3 - 93.4 - 92.5 - 90.1 - - M Agarwal [[92](#bib.bib92)]
    ICDAR2019 Cascade mask R-CNN Precision 98.7 - 98.0 - 97.7 - 97.1 - 93.4 - - 2021
    Recall 94.6 - 93.9 - 93.6 - 93.0 - 89.5 - - F1-Score 96.6 - 95.9 - 95.6 - 95.0
    - 91.5 - - X Zheng[[103](#bib.bib103)] ICDAR2019 object detection networks Precision
    - - - - - - 96.0 - 90.0 - - 2021 Recall - - - - - - 95.0 - 89.0 - - F1-Score -
    - - - - - 95.5 - 95.5 - - SA Siddiqui[[98](#bib.bib98)] Mormot Deformable CNN
    Precision 84.9 - - - - - - - - - - 2018 Recall 94.6 - - - - - - - - - - F1-Score
    89.5 - - - - - - - - - - M Agarwal [[92](#bib.bib92)] TableBank Cascade mask R-CNN
    Precision 93.4 - 99.5 - - - - - - - - 2021 Recall 92.4 - 97.8 - - - - - - - -
    F1-Score 92.9 - 98.6 - - - - - - - - P Riba [[54](#bib.bib54)] RVL-CDIP Graph
    NN Precision 15.2 - - - - - - - - - - 2019 Recall 36.5 - - - - - - - - - - F1-Score
    21.5 - - - - - - - - - -
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 18: Table Structure Recognition'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Approach Dataset Method IoU Year 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 50%:95%
    S Schreiber[[20](#bib.bib20)] ICDAR2013 Fully CNN Precision 95.93 - - - - - -
    - - - - 2017 Recall 87.36 - - - - - - - - - - F1-Score 91.44 - - - - - - - - -
    - SA Siddiqui[[51](#bib.bib51)] ICDAR2013 Deformable CNN Precision 93.19 - - -
    - - - - - - - 2019 Recall 93.08 - - - - - - - - - - F1-Score 92.98 - - - - - -
    - - - - W Xue[[119](#bib.bib119)] ICDAR2013 Graph NN + weights depending on distance
    Precision 92.6 - - - - - - - - - - 2019 Recall 44.7 - - - - - - - - - - F1-Score
    60.3 - - - - - - - - - - SS Paliwal[[96](#bib.bib96)] ICDAR2013 fully CNN Precision
    92.15 - - - - - - - - - - 2019 Recall 89.87 - - - - - - - - - - F1-Score 90.98
    - - - - - - - - - - SA Khan[[113](#bib.bib113)] ICDAR2013 Bi-directional RNN Precision
    96.92 - - - - - - - - - - 2019 Recall 90.12 - - - - - - - - - - F1-Score 93.39
    - - - - - - - - - - C Tensmeyer[[120](#bib.bib120)] ICDAR2013 Dilated Convolutions
    + Fully CNN Precision 95.8 - - - - - - - - - - 2019 Recall 94.6 - - - - - - -
    - - - F1-Score 95.2 - - - - - - - - - - Z Chi[[64](#bib.bib64)] ICDAR2013 Fully
    CNN Precision 88.5 - - - - - - - - - - 2019 Recall 86.0 - - - - - - - - - - F1-Score
    87.2 - - - - - - - - - - Á Casado-García[[102](#bib.bib102)] ICDAR2013 Mask R-CNN
    Precision - - 70.0 - 70.0 - 70.0 - 47.0 - - 2020 Recall - - 97.0 - 97.0 - 97.0
    - 65.0 - - F1-Score - - 81.0 - 81.0 - 81.0 - 54.0 - - S Raja[[116](#bib.bib116)]
    ICDAR2013 Object Detection Methods Precision 92.7 - - - - - - - - - - 2020 Recall
    91.1 - - - - - - - - - - F1-Score 91.9 - - - - - - - - - - KA Hashmi[[118](#bib.bib118)]
    ICDAR2013 Object Detection Methods Precision 95.37 - - - - - - - - - - 2021 Recall
    95.56 - - - - - - - - - - F1-Score 95.46 - - - - - - - - - - D Prasad[[95](#bib.bib95)]
    ICDAR2019 Object Detection Methods Precision - - - - - - - - - - - 2020 Recall
    - - - - - - - - - - - F1-Score - - 43.8 - 35.4 - 19.0 - 3.6 - - Y Zou[[117](#bib.bib117)]
    ICDAR2019 Fully CNN Precision - - 18.79 - - - 1.71 - - - - 2021 Recall - - 10.07
    - - - 0.92 - - - - F1-Score - - 13.11 - - - 1.19 - - - - X Zheng[[103](#bib.bib103)]
    ICDAR2019 Object Detection Methods Precision - - - - - - - - - - - 2021 Recall
    - - - - - - - - - - - F1-Score 54.8 - 38.5 - - - - - - - -
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 19: Open source code for most of the studies articles in Table Detection'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Article Model Year Framework Link Z Chi [[64](#bib.bib64)] SciTSR 2019 Pytorch
    [https://github.com/Academic-Hammer/SciTSR](https://github.com/Academic-Hammer/SciTSR)
    D Prasad [[95](#bib.bib95)] CascadeTabNet 2020 Pytorch [https://github.com/DevashishPrasad/CascadeTabNet](https://github.com/DevashishPrasad/CascadeTabNet)
    Á Casado-García [[102](#bib.bib102)] - 2020 mxnet [https://github.com/holms-ur/fine-tuning](https://github.com/holms-ur/fine-tuning)
    M Li [[95](#bib.bib95)] TableBank 2020 Pytorch, Detectron2 [https://github.com/doc-analysis/TableBank](https://github.com/doc-analysis/TableBank)
    S Raja S Raja [[116](#bib.bib116)] TabStructNet 2020 tensorflow [https://github.com/sachinraja13/TabStructNet.git](https://github.com/sachinraja13/TabStructNet.git)
    X Zhong [[66](#bib.bib66)] PubTabNet 2020 - [https://github.com/ibm-aur-nlp/PubTabNet](https://github.com/ibm-aur-nlp/PubTabNet)
    M Agarwal [[92](#bib.bib92)] CDeC-Net 2021 PyTorch [https://github.com/mdv3101/CDeCNet](https://github.com/mdv3101/CDeCNet)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Result of different datasets
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 7.3.1 Table Detection Evaluations
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To identify the tabular region in the document image and regress the coordinates
    of a bounding box that has been designated as a tabular region is the task of
    table detection.Tables [16](#S7.T16 "Table 16 ‣ 7.2 Results of TNCR dataset ‣
    7 Experiments Results ‣ Deep learning for table detection and structure recognition:
    A survey"),[17](#S7.T17 "Table 17 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey")
    demonstrates how several table detection techniques that have been thoroughly
    researched compare in terms of performance. The ICDAR-2013, ICDAR-2017-POD, ICDAR-2019,
    and UNLV datasets are typically used to assess the effectiveness of table detection
    algorithms.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally described in Tables [16](#S7.T16 "Table 16 ‣ 7.2 Results of TNCR
    dataset ‣ 7 Experiments Results ‣ Deep learning for table detection and structure
    recognition: A survey"),[17](#S7.T17 "Table 17 ‣ 7.2 Results of TNCR dataset ‣
    7 Experiments Results ‣ Deep learning for table detection and structure recognition:
    A survey") is the Intersection Over Union (IOU) criterion used to determine precision
    and recall. The most accurate results across all relevant datasets are underlined.
    It is important to note that several of the approaches did not specify the IOU
    threshold value but instead compared their findings to those of other approaches
    that did. So, for those procedures, we have taken into consideration the same
    threshold value.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Table Recognition Evaluations
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table recognition entails both segmenting the structure of tables (the task
    of structural segmentation is assessed based on how precisely the rows or columns
    of the tables are split) and extracting the data from the cells. We will review
    the evaluations of the few previously discussed approaches in this part.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [18](#S7.T18 "Table 18 ‣ 7.2 Results of TNCR dataset ‣ 7 Experiments
    Results ‣ Deep learning for table detection and structure recognition: A survey")
    provides a summary of the findings. It is important to note that the different
    datasets and evaluation measures used in these procedures mean that the provided
    methodologies are not directly comparable to one another.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Open source code
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several open source frameworks for creating generic deep learning models, most
    of which are written in Python, are available online, including TensorFlow, Keras,
    PyTorch, and MXNet.The open-source projects for table detection and structure
    recognition are summarized in Table [19](#S7.T19 "Table 19 ‣ 7.2 Results of TNCR
    dataset ‣ 7 Experiments Results ‣ Deep learning for table detection and structure
    recognition: A survey"). Many of the authors have also made open-source implementations
    of their proposed models available. TensorFlow and PyTorch are the most often
    utilized frameworks in these open source projects.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of document analysis, table analysis is a significant and extensively
    researched problem. The challenge of interpreting tables has been dramatically
    transformed and new standards have been set thanks to the use of deep learning
    ideas.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: As we said at the paper’s main contribution’s paragraph at the Introduction
    section, we have addressed several current processes that have advanced the process
    of information extraction from tables in document pictures by implementing deep
    learning concepts. We have discussed methods that use deep learning to detect,
    identify, and classify tables. We have also shown the most and least well-known
    techniques that have been used to detect and identify tables, respectively.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: As we did at section 7, all of the datasets that are publicly accessible and
    their access details have been compiled. On numerous datasets, we have presented
    a thorough performance comparison of the methodologies that have been addressed.
    On well-known datasets that are freely accessible to the public, state-of-the-art
    algorithms for table detection have produced results that are almost flawless.
    Once the tabular region has been identified, the work of structurally segmenting
    tables and then recognizing them follows.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: We conclude that both of these areas still have opportunities for development.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Hu, R. S. Kashi, D. Lopresti, G. T. Wilfong, Evaluating the performance
    of table processing algorithms, International Journal on Document Analysis and
    Recognition 4 (3) (2002) 140–153.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Dollár, R. Appel, S. Belongie, P. Perona, Fast feature pyramids for
    object detection, IEEE transactions on pattern analysis and machine intelligence
    36 (8) (2014) 1532–1545.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Yang, G. Yang, Modified convolutional neural network based on dropout
    and the stochastic gradient descent optimizer, Algorithms 11 (3) (2018) 28.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Li, W. Liu, G. Xiao, Detection of srew nut images based on deep transfer
    learning network, in: 2019 Chinese Automation Congress (CAC), IEEE, 2019, pp.
    951–955.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] K. L. Masita, A. N. Hasan, S. Paul, Pedestrian detection using r-cnn object
    detector, in: 2018 IEEE Latin American Conference on Computational Intelligence
    (LA-CCI), IEEE, 2018, pp. 1–6.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Z. Hu, J. Tang, Z. Wang, K. Zhang, L. Zhang, Q. Sun, Deep learning for
    image-based cancer detection and diagnosis- a survey, Pattern Recognition 83 (2018)
    134–149.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified,
    real-time object detection, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2016, pp. 779–788.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Abdallah, A. Berendeyev, I. Nuradin, D. Nurseitov, [Tncr:table net detection
    and classification dataset](https://www.sciencedirect.com/science/article/pii/S0925231221018142),
    Neurocomputing 473 (2022) 79–97. [doi:10.1016/j.neucom.2021.11.101](https://doi.org/10.1016/j.neucom.2021.11.101).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S0925231221018142](https://www.sciencedirect.com/science/article/pii/S0925231221018142)
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[9] R. Fakoor, F. Ladhak, A. Nazi, M. Huber, Using deep learning to enhance
    cancer diagnosis and classification, in: Proceedings of the international conference
    on machine learning, Vol. 28, ACM, New York, USA, 2013, pp. 3937–3949.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Minaee, Z. Liu, Automatic question-answering using a deep similarity
    neural network, in: 2017 IEEE Global Conference on Signal and Information Processing
    (GlobalSIP), IEEE, 2017, pp. 923–927.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Abdallah, M. Kasem, M. A. Hamada, S. Sdeek, Automated question-answer
    medical model based on deep learning technology, in: Proceedings of the 6th International
    Conference on Engineering & MIS 2020, 2020, pp. 1–8.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Arpteg, B. Brinne, L. Crnkovic-Friis, J. Bosch, Software engineering
    challenges of deep learning, in: 2018 44th Euromicro Conference on Software Engineering
    and Advanced Applications (SEAA), IEEE, 2018, pp. 50–59.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. A. Hamada, A. Abdallah, M. Kasem, M. Abokhalil, Neural network estimation
    model to optimize timing and schedule of software projects, in: 2021 IEEE International
    Conference on Smart Information Systems and Technologies (SIST), IEEE, 2021, pp.
    1–7.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Mahmoud, M. Kasem, A. Abdallah, H. S. Kang, Ae-lstm: Autoencoder with
    lstm-based intrusion detection in iot, in: 2022 International Telecommunications
    Conference (ITC-Egypt), IEEE, 2022, pp. 1–6.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] W. Xu, J. Jang-Jaccard, A. Singh, Y. Wei, F. Sabrina, Improving performance
    of autoencoder-based network anomaly detection on nsl-kdd dataset, IEEE Access
    9 (2021) 140136–140146.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. A. Mahmoud, I. Ahmad, W. G. Al-Khatib, M. Alshayeb, M. T. Parvez, V. Märgner,
    G. A. Fink, Khatt: An open arabic offline handwritten text database, Pattern Recognition
    47 (3) (2014) 1096–1112.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] D. Nurseitov, K. Bostanbekov, D. Kurmankhojayev, A. Alimova, A. Abdallah,
    R. Tolegenov, Handwritten kazakh and russian (hkr) database for text recognition,
    Multimedia Tools and Applications 80 (21) (2021) 33075–33097.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Toiganbayeva, M. Kasem, G. Abdimanap, K. Bostanbekov, A. Abdallah,
    A. Alimova, D. Nurseitov, Kohtd: Kazakh offline handwritten text dataset, Signal
    Processing: Image Communication 108 (2022) 116827.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Fischer, C. Y. Suen, V. Frinken, K. Riesen, H. Bunke, A fast matching
    algorithm for graph-based handwriting recognition, in: International Workshop
    on Graph-Based Representations in Pattern Recognition, Springer, 2013, pp. 194–203.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Schreiber, S. Agne, I. Wolf, A. Dengel, S. Ahmed, Deepdesrt: Deep learning
    for detection and structure recognition of tables in document images, in: 2017
    14th IAPR international conference on document analysis and recognition (ICDAR),
    Vol. 1, IEEE, 2017, pp. 1162–1167.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. Traquair, E. Kara, B. Kantarci, S. Khan, Deep learning for the detection
    of tabular information from electronic component datasheets, in: 2019 IEEE Symposium
    on Computers and Communications (ISCC), IEEE, 2019, pp. 1–6.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Gilani, S. R. Qasim, I. Malik, F. Shafait, Table detection using deep
    learning, in: 2017 14th IAPR international conference on document analysis and
    recognition (ICDAR), Vol. 1, IEEE, 2017, pp. 771–776.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] D. N. Tran, T. A. Tran, A. Oh, S. H. Kim, I. S. Na, Table detection from
    document image using vertical arrangement of text blocks, International Journal
    of Contents 11 (4) (2015) 77–85.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Hao, L. Gao, X. Yi, Z. Tang, A table detection method for pdf documents
    based on convolutional neural networks, in: 2016 12th IAPR Workshop on Document
    Analysis Systems (DAS), IEEE, 2016, pp. 287–292.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Mao, A. Rosenfeld, T. Kanungo, Document structure analysis algorithms:
    a literature survey, Document recognition and retrieval X 5010 (2003) 197–207.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] E. Kara, M. Traquair, M. Simsek, B. Kantarci, S. Khan, Holistic design
    for deep learning-based discovery of tabular structures in datasheet images, Engineering
    Applications of Artificial Intelligence 90 (2020) 103551.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Sarkar, M. Aggarwal, A. Jain, H. Gupta, B. Krishnamurthy, Document
    structure extraction using prior based high resolution hierarchical semantic segmentation,
    in: European Conference on Computer Vision, Springer, 2020, pp. 649–666.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Zanibbi, D. Blostein, J. R. Cordy, A survey of table recognition, Document
    Analysis and Recognition 7 (1) (2004) 1–16.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] D. W. Embley, M. Hurst, D. Lopresti, G. Nagy, Table-processing paradigms:
    a research survey, International Journal of Document Analysis and Recognition
    (IJDAR) 8 (2) (2006) 66–86.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] B. Coüasnon, A. Lemaitre, Recognition of tables and forms (2014).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Khusro, A. Latif, I. Ullah, On methods and tools of table detection,
    extraction and annotation in pdf documents, Journal of Information Science 41 (1)
    (2015) 41–57.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] R. Szeliski, Computer vision: algorithms and applications, Springer Science
    & Business Media, 2010.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] B. C. G. Lee, Line detection in binary document scans: a case study with
    the international tracing service archives, in: 2017 IEEE International Conference
    on Big Data (Big Data), IEEE, 2017, pp. 2256–2261.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, M. Pietikäinen,
    Deep learning for generic object detection: A survey, International journal of
    computer vision 128 (2) (2020) 261–318.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Bengio, A. Courville, P. Vincent, Representation learning: A review
    and new perspectives, IEEE transactions on pattern analysis and machine intelligence
    35 (8) (2013) 1798–1828.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. LeCun, Y. Bengio, G. Hinton, et al., Deep learning. nature, 521 (7553),
    436-444, Google Scholar Google Scholar Cross Ref Cross Ref (2015).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, A survey on deep learning
    in medical image analysis, Medical image analysis 42 (2017) 60–88.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. X. Zhu, D. Tuia, L. Mou, G.-S. Xia, L. Zhang, F. Xu, F. Fraundorfer,
    Deep learning in remote sensing: A comprehensive review and list of resources,
    IEEE Geoscience and Remote Sensing Magazine 5 (4) (2017) 8–36.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai, et al., Recent advances in convolutional neural networks, Pattern
    Recognition 77 (2018) 354–377.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, S. S. Iyengar, A survey on deep learning: Algorithms, techniques,
    and applications, ACM Computing Surveys (CSUR) 51 (5) (2018) 1–36.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] T. Young, D. Hazarika, S. Poria, E. Cambria, Recent trends in deep learning
    based natural language processing, ieee Computational intelligenCe magazine 13 (3)
    (2018) 55–75.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, M. Sun,
    Graph neural networks: A review of methods and applications, AI Open 1 (2020)
    57–81.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Zhang, J. Geiger, J. Pohjalainen, A. E.-D. Mousa, W. Jin, B. Schuller,
    Deep learning for environmentally robust speech recognition: An overview of recent
    developments, ACM Transactions on Intelligent Systems and Technology (TIST) 9 (5)
    (2018) 1–28.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, A comprehensive
    survey on graph neural networks, IEEE transactions on neural networks and learning
    systems 32 (1) (2020) 4–24.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks,
    in: European conference on computer vision, Springer, 2014, pp. 818–833.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Oquab, L. Bottou, I. Laptev, J. Sivic, Learning and transferring mid-level
    image representations using convolutional neural networks, in: Proceedings of
    the IEEE conference on computer vision and pattern recognition, 2014, pp. 1717–1724.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Göbel, T. Hassan, E. Oro, G. Orsi, Icdar 2013 table competition, in:
    2013 12th International Conference on Document Analysis and Recognition, IEEE,
    2013, pp. 1449–1453.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] L. Gao, X. Yi, Z. Jiang, L. Hao, Z. Tang, Icdar2017 competition on page
    object detection, in: 2017 14th IAPR International Conference on Document Analysis
    and Recognition (ICDAR), Vol. 1, IEEE, 2017, pp. 1417–1422.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] L. Gao, Y. Huang, H. Déjean, J.-L. Meunier, Q. Yan, Y. Fang, F. Kleber,
    E. Lang, Icdar 2019 competition on table detection and recognition (ctdar), in:
    2019 International Conference on Document Analysis and Recognition (ICDAR), IEEE,
    2019, pp. 1510–1515.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. A. Siddiqui, I. A. Fateh, S. T. R. Rizvi, A. Dengel, S. Ahmed, Deeptabstr:
    deep learning based table structure recognition, in: 2019 International Conference
    on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 1403–1409.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Deng, D. Rosenberg, G. Mann, Challenges in end-to-end neural scientific
    table recognition, in: 2019 International Conference on Document Analysis and
    Recognition (ICDAR), IEEE, 2019, pp. 894–901.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. W. Harley, A. Ufkes, K. G. Derpanis, Evaluation of deep convolutional
    nets for document image classification and retrieval, in: 2015 13th International
    Conference on Document Analysis and Recognition (ICDAR), IEEE, 2015, pp. 991–995.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] P. Riba, A. Dutta, L. Goldmann, A. Fornés, O. Ramos, J. Lladós, Table
    detection in invoice documents by graph neural networks, in: 2019 International
    Conference on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 122–127.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Mondal, P. Lipps, C. Jawahar, Iiit-ar-13k: a new dataset for graphical
    object detection in documents, in: International Workshop on Document Analysis
    Systems, Springer, 2020, pp. 216–230.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] W. Seo, H. I. Koo, N. I. Cho, Junction-based table detection in camera-captured
    document images, International Journal on Document Analysis and Recognition (IJDAR)
    18 (1) (2015) 47–57.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Shahab, F. Shafait, T. Kieninger, A. Dengel, An open approach towards
    the benchmarking of table structure recognition systems, in: Proceedings of the
    9th IAPR International Workshop on Document Analysis Systems, 2010, pp. 113–120.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] I. T. Phillips, User’s reference manual for the uw english/technical document
    image database iii, UW-III English/technical document image database manual (1996).'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. Hu, R. Kashi, D. Lopresti, G. Nagy, G. Wilfong, Why table ground-truthing
    is hard, in: Proceedings of Sixth International Conference on Document Analysis
    and Recognition, IEEE, 2001, pp. 129–133.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. Fang, X. Tao, Z. Tang, R. Qiu, Y. Liu, Dataset, ground-truth and performance
    metrics for table detection evaluation, in: 2012 10th IAPR International Workshop
    on Document Analysis Systems, IEEE, 2012, pp. 445–449.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] M. Li, L. Cui, S. Huang, F. Wei, M. Zhou, Z. Li, Tablebank: Table benchmark
    for image-based table detection and recognition, in: Proceedings of the 12th Language
    Resources and Evaluation Conference, 2020, pp. 1918–1925.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] N. Siegel, N. Lourie, R. Power, W. Ammar, Extracting scientific figures
    with distantly supervised neural networks, in: Proceedings of the 18th ACM/IEEE
    on joint conference on digital libraries, 2018, pp. 223–232.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] B. Smock, R. Pesala, R. Abraham, W. Redmond, Pubtables-1m: Towards comprehensive
    table extraction from unstructured documents, arXiv preprint arXiv:2110.00061
    (2021).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Z. Chi, H. Huang, H.-D. Xu, H. Yu, W. Yin, X.-L. Mao, Complicated table
    structure recognition, arXiv preprint arXiv:1908.04729 (2019).'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X. Zheng, D. Burdick, L. Popa, P. Zhong, N. X. R. Wang, Global table extractor
    (gte): A framework for joint table identification and cell structure recognition
    using visual context, Winter Conference for Applications in Computer Vision (WACV)
    (2021).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Zhong, E. ShafieiBavani, A. Jimeno Yepes, Image-based table recognition:
    data, model, and evaluation, in: European Conference on Computer Vision, Springer,
    2020, pp. 564–580.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. Abdallah, A. Berendeyev, I. Nuradin, D. Nurseitov, Tncr: Table net
    detection and classification dataset, Neurocomputing 473 (2022) 79–97.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Nassar, N. Livathinos, M. Lysak, P. Staar, Tableformer: Table structure
    understanding with transformers, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2022, pp. 4614–4623.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] P. Pyreddy, W. Croft, Tinti: A system for retrieval in text tables title2
    (1997).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Y. Wangt, I. T. Phillipst, R. Haralick, Automatic table ground truth generation
    and a background-analysis-based table structure extraction method, in: Proceedings
    of Sixth International Conference on Document Analysis and Recognition, IEEE,
    2001, pp. 528–532.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. A. Jahan, R. G. Ragel, Locating tables in scanned documents for reconstructing
    and republishing, in: 7th International Conference on Information and Automation
    for Sustainability, IEEE, 2014, pp. 1–6.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] K. Itonori, Table structure recognition based on textblock arrangement
    and ruled line position, in: Proceedings of 2nd International Conference on Document
    Analysis and Recognition (ICDAR’93), IEEE, 1993, pp. 765–768.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Chandran, R. Kasturi, Structural recognition of tabulated data, in:
    Proceedings of 2nd International Conference on Document Analysis and Recognition
    (ICDAR’93), IEEE, 1993, pp. 516–519.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] T. Hassan, R. Baumgartner, Table recognition and understanding from pdf
    files, in: Ninth International Conference on Document Analysis and Recognition
    (ICDAR 2007), Vol. 2, IEEE, 2007, pp. 1143–1147.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] E. Oro, M. Ruffolo, Trex: An approach for recognizing and extracting tables
    from pdf documents, in: 2009 10th International Conference on Document Analysis
    and Recognition, IEEE, 2009, pp. 906–910.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Nurminen, Algorithmic extraction of data in tables in pdf documents,
    Master’s thesis (2013).'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Fang, P. Mitra, Z. Tang, C. L. Giles, Table header detection and classification,
    in: Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] G. Harit, A. Bansal, Table detection in document images using header and
    trailer patterns, in: Proceedings of the Eighth Indian Conference on Computer
    Vision, Graphics and Image Processing, 2012, pp. 1–8.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Tupaj, Z. Shi, C. H. Chang, H. Alam, Extracting tabular information
    from text files, EECS Department, Tufts University, Medford, USA 1 (1996).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] T. Kieninger, A. Dengel, The t-recs table recognition and analysis system,
    in: International Workshop on Document Analysis Systems, Springer, 1998, pp. 255–270.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] F. Cesarini, S. Marinai, L. Sarti, G. Soda, Trainable table location in
    document images, in: Object recognition supported by user interaction for service
    robots, Vol. 3, IEEE, 2002, pp. 236–240.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Fan, D. S. Kim, Table region detection on large-scale pdf files without
    labeled data, CoRR, abs/1506.08891 (2015).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Wang, J. Hu, A machine learning based approach for table detection
    on the web, in: Proceedings of the 11th international conference on World Wide
    Web, 2002, pp. 242–250.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] T. Kasar, P. Barlas, S. Adam, C. Chatelain, T. Paquet, Learning to detect
    tables in scanned document images using line information, in: 2013 12th International
    Conference on Document Analysis and Recognition, IEEE, 2013, pp. 1185–1189.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. C. e Silva, Learning rich hidden markov models in document analysis:
    Table location, in: 2009 10th International Conference on Document Analysis and
    Recognition, IEEE, 2009, pp. 843–847.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S. Klampfl, K. Jack, R. Kern, A comparison of two unsupervised table recognition
    methods from digital scientific articles, D-Lib Magazine 20 (11) (2014) 7.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] L. O’Gorman, The document spectrum for page layout analysis, IEEE Transactions
    on pattern analysis and machine intelligence 15 (11) (1993) 1162–1173.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] F. Shafait, R. Smith, Table detection in heterogeneous documents, in:
    Proceedings of the 9th IAPR International Workshop on Document Analysis Systems,
    2010, pp. 65–72.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] D. He, S. Cohen, B. Price, D. Kifer, C. L. Giles, Multi-scale multi-task
    fcn for semantic page segmentation and table detection, in: 2017 14th IAPR International
    Conference on Document Analysis and Recognition (ICDAR), Vol. 1, IEEE, 2017, pp.
    254–261.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Arif, F. Shafait, Table detection in document images using foreground
    and background features, in: 2018 Digital Image Computing: Techniques and Applications
    (DICTA), IEEE, 2018, pp. 1–8.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. M. Reza, S. S. Bukhari, M. Jenckel, A. Dengel, Table localization and
    segmentation using gan and cnn, in: 2019 International Conference on Document
    Analysis and Recognition Workshops (ICDARW), Vol. 5, IEEE, 2019, pp. 152–157.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Agarwal, A. Mondal, C. Jawahar, Cdec-net: Composite deformable cascade
    network for table detection in document images, in: 2020 25th International Conference
    on Pattern Recognition (ICPR), IEEE, 2021, pp. 9491–9498.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko,
    End-to-end object detection with transformers, in: European conference on computer
    vision, Springer, 2020, pp. 213–229.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. Li, Y. Xu, T. Lv, L. Cui, C. Zhang, F. Wei, Dit: Self-supervised pre-training
    for document image transformer, arXiv preprint arXiv:2203.02378 (2022).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] D. Prasad, A. Gadpal, K. Kapadni, M. Visave, K. Sultanpure, Cascadetabnet:
    An approach for end to end table detection and structure recognition from image-based
    documents, in: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition workshops, 2020, pp. 572–573.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] S. S. Paliwal, D. Vishwanath, R. Rahul, M. Sharma, L. Vig, Tablenet: Deep
    learning model for end-to-end table detection and tabular data extraction from
    scanned document images, in: 2019 International Conference on Document Analysis
    and Recognition (ICDAR), IEEE, 2019, pp. 128–133.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Y. Huang, Q. Yan, Y. Li, Y. Chen, X. Wang, L. Gao, Z. Tang, A yolo-based
    table detection method, in: 2019 International Conference on Document Analysis
    and Recognition (ICDAR), IEEE, 2019, pp. 813–818.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. A. Siddiqui, M. I. Malik, S. Agne, A. Dengel, S. Ahmed, Decnt: Deep
    deformable cnn for table detection, IEEE access 6 (2018) 74151–74161.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] N. Sun, Y. Zhu, X. Hu, Faster r-cnn based table detection combining corner
    locating, in: 2019 International Conference on Document Analysis and Recognition
    (ICDAR), IEEE, 2019, pp. 1314–1319.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] I. Kavasidis, C. Pino, S. Palazzo, F. Rundo, D. Giordano, P. Messina,
    C. Spampinato, A saliency-based convolutional neural network for table and chart
    detection in digitized documents, in: International conference on image analysis
    and processing, Springer, 2019, pp. 292–302.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. Holeček, A. Hoskovec, P. Baudiš, P. Klinger, Table understanding in
    structured documents, in: 2019 International Conference on Document Analysis and
    Recognition Workshops (ICDARW), Vol. 5, IEEE, 2019, pp. 158–164.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Á. Casado-García, C. Domínguez, J. Heras, E. Mata, V. Pascual, The benefits
    of close-domain fine-tuning for table detection in document images, in: International
    workshop on document analysis systems, Springer, 2020, pp. 199–215.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] X. Zheng, D. Burdick, L. Popa, X. Zhong, N. X. R. Wang, Global table
    extractor (gte): A framework for joint table identification and cell structure
    recognition using visual context, in: Proceedings of the IEEE/CVF winter conference
    on applications of computer vision, 2021, pp. 697–706.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Li, L. Gao, Z. Tang, Q. Yan, Y. Huang, A gan-based feature generator
    for table detection, in: 2019 International Conference on Document Analysis and
    Recognition (ICDAR), IEEE, 2019, pp. 763–768.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] D.-D. Nguyen, Tablesegnet: a fully convolutional network for table detection
    and segmentation in document images, International Journal on Document Analysis
    and Recognition (IJDAR) 25 (1) (2022) 1–14.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] D. Zhang, R. Mao, R. Guo, Y. Jiang, J. Zhu, Yolo-table: disclosure document
    table detection with involution, International Journal on Document Analysis and
    Recognition (IJDAR) (2022) 1–14.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Zucker, Y. Belkada, H. Vu, V. N. Nguyen, Clusti: Clustering method
    for table structure recognition in scanned images, Mobile Networks and Applications
    26 (4) (2021) 1765–1776.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Z. Zhang, J. Zhang, J. Du, F. Wang, Split, embed and merge: An accurate
    table structure recognizer, Pattern Recognition 126 (2022) 108565.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Namysl, A. M. Esser, S. Behnke, J. Köhler, Flexible table recognition
    and semantic interpretation system., in: VISIGRAPP (4: VISAPP), 2022, pp. 27–37.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] E. Koci, M. Thiele, W. Lehner, O. Romero, Table recognition in spreadsheets
    via a graph representation, in: 2018 13th IAPR International Workshop on Document
    Analysis Systems (DAS), IEEE, 2018, pp. 139–144.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] E. Koci, M. Thiele, O. Romero, W. Lehner, A genetic-based search for
    adaptive table recognition in spreadsheets, in: 2019 International Conference
    on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 1274–1279.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. A. Siddiqui, P. I. Khan, A. Dengel, S. Ahmed, Rethinking semantic
    segmentation for table structure recognition in documents, in: 2019 International
    Conference on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 1397–1402.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] S. A. Khan, S. M. D. Khalid, M. A. Shahzad, F. Shafait, Table structure
    extraction with bi-directional gated recurrent unit networks, in: 2019 International
    Conference on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 1366–1371.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. F. Rashid, A. Akmal, M. Adnan, A. A. Aslam, A. Dengel, Table recognition
    in heterogeneous documents using machine learning, in: 2017 14th IAPR International
    conference on document analysis and recognition (ICDAR), Vol. 1, IEEE, 2017, pp.
    777–782.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. R. Qasim, H. Mahmood, F. Shafait, Rethinking table recognition using
    graph neural networks, in: 2019 International Conference on Document Analysis
    and Recognition (ICDAR), IEEE, 2019, pp. 142–147.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Raja, A. Mondal, C. Jawahar, Table structure recognition using top-down
    and bottom-up cues, in: European Conference on Computer Vision, Springer, 2020,
    pp. 70–86.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Zou, J. Ma, A deep semantic segmentation model for image-based table
    structure recognition, in: 2020 15th IEEE International Conference on Signal Processing
    (ICSP), Vol. 1, IEEE, 2020, pp. 274–280.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] K. A. Hashmi, D. Stricker, M. Liwicki, M. N. Afzal, M. Z. Afzal, Guided
    table structure recognition through anchor optimization, IEEE Access 9 (2021)
    113521–113534.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] W. Xue, Q. Li, D. Tao, Res2tim: Reconstruct syntactic structures from
    table images, in: 2019 International Conference on Document Analysis and Recognition
    (ICDAR), IEEE, 2019, pp. 749–755.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] C. Tensmeyer, V. I. Morariu, B. Price, S. Cohen, T. Martinez, Deep splitting
    and merging for table structure decomposition, in: 2019 International Conference
    on Document Analysis and Recognition (ICDAR), IEEE, 2019, pp. 114–121.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Raja, A. Mondal, C. Jawahar, Visual understanding of complex table
    structures from document images, in: Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision, 2022, pp. 2299–2308.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] X. Shen, L. Kong, Y. Bao, Y. Zhou, W. Liu, Rcanet: A rows and columns
    aggregated network for table structure recognition, in: 2022 3rd Information Communication
    Technologies Conference (ICTC), IEEE, 2022, pp. 112–116.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] C. Ma, W. Lin, L. Sun, Q. Huo, Robust table detection and structure recognition
    from heterogeneous document images, arXiv preprint arXiv:2203.09056 (2022).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] B. Xiao, M. Simsek, B. Kantarci, A. A. Alkheir, Table structure recognition
    with conditional attention, arXiv preprint arXiv:2203.03819 (2022).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] A. Jain, S. Paliwal, M. Sharma, L. Vig, Tsr-dsaw: Table structure recognition
    via deep spatial association of words, arXiv preprint arXiv:2203.06873 (2022).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] H. Li, L. Zeng, W. Zhang, J. Zhang, J. Fan, M. Zhang, A two-phase approach
    for recognizing tables with complex structures, in: International Conference on
    Database Systems for Advanced Applications, Springer, 2022, pp. 587–595.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object
    detection with region proposal networks, arXiv preprint arXiv:1506.01497 (2015).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] K. He, G. Gkioxari, P. Dollar, R. Girshick, Mask r-cnn, 2017 IEEE International
    Conference on Computer Vision (ICCV) (Oct 2017).'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] K. Sun, B. Xiao, D. Liu, J. Wang, Deep high-resolution representation
    learning for human pose estimation, in: CVPR, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] K. Sun, Y. Zhao, B. Jiang, T. Cheng, B. Xiao, D. Liu, Y. Mu, X. Wang,
    W. Liu, J. Wang, High-resolution representations for labeling pixels and regions,
    CoRR abs/1904.04514 (2019).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. Newell, K. Yang, J. Deng, Stacked hourglass networks for human pose
    estimation, in: European conference on computer vision, Springer, 2016, pp. 483–499.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, B. Schiele,
    Deepercut: A deeper, stronger, and faster multi-person pose estimation model,
    in: European Conference on Computer Vision, Springer, 2016, pp. 34–50.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] B. Xiao, H. Wu, Y. Wei, Simple baselines for human pose estimation and
    tracking, in: Proceedings of the European conference on computer vision (ECCV),
    2018, pp. 466–481.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] W. Yang, S. Li, W. Ouyang, H. Li, X. Wang, Learning feature pyramids
    for human pose estimation, in: proceedings of the IEEE international conference
    on computer vision, 2017, pp. 1281–1290.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, Z. Zhang, H. Lin, Y. Sun, T. He, J. Muller,
    R. Manmatha, M. Li, A. Smola, Resnest: Split-attention networks, arXiv preprint
    arXiv:2004.08955 (2020).'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] H. Zhang, H. Chang, B. Ma, N. Wang, X. Chen, Dynamic R-CNN: Towards high
    quality object detection via dynamic training, arXiv preprint arXiv:2004.06002
    (2020).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng,
    Z. Liu, J. Xu, et al., Mmdetection: Open mmlab detection toolbox and benchmark,
    arXiv preprint arXiv:1906.07155 (2019).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Wu, J. Yang, X. Wang, X. Li, Iou-balanced loss functions for single-stage
    object detection, arXiv preprint arXiv:1908.05641 (2019).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
