- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:15'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2310.14230] A comprehensive survey on deep active learning and its applications
    in medical image analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.14230](https://ar5iv.labs.arxiv.org/html/2310.14230)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive survey on deep active learning and its applications in medical
    image analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Haoran Wang Qiuye Jin Shiman Li Siyu Liu Manning Wang Zhijian Song Digital Medical
    Research Center, School of Basic Medical Sciences, Fudan University, Shanghai
    200032, China Shanghai Key Laboratory of Medical Image Computing and Computer
    Assisted Intervention, Shanghai 200032, China Computational Bioscience Research
    Center (CBRC), King Abdullah University of Science and Technology (KAUST), Thuwal
    23955, Saudi Arabia
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning has achieved widespread success in medical image analysis, leading
    to an increasing demand for large-scale expert-annotated medical image datasets.
    Yet, the high cost of annotating medical images severely hampers the development
    of deep learning in this field. To reduce annotation costs, active learning aims
    to select the most informative samples for annotation and train high-performance
    models with as few labeled samples as possible. In this survey, we review the
    core methods of active learning, including the evaluation of informativeness and
    sampling strategy. For the first time, we provide a detailed summary of the integration
    of active learning with other label-efficient techniques, such as semi-supervised,
    self-supervised learning, and so on. Additionally, we also highlight active learning
    works that are specifically tailored to medical image analysis. In the end, we
    offer our perspectives on the future trends and challenges of active learning
    and its applications in medical image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Active Learning, Medical Image Analysis, Survey, Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical imaging visualizes anatomical structures and pathological processes.
    It also offers crucial information in lesion detection, diagnosis, treatment planning,
    and surgical intervention. In recent years, the rise of artificial intelligence
    (AI) has led to significant success in medical image analysis. The AI-powered
    systems for medical image analysis have not only approached but even exceeded
    the performance of human experts in certain clinical tasks. Notable examples include
    skin cancer classification [Esteva et al., [2017](#bib.bib46)], lung cancer screening
    with CT scans [Ardila et al., [2019](#bib.bib6)], polyp detection during colonoscopy
    [Wang et al., [2018](#bib.bib189)], and prostate cancer tissue detection in whole-slide
    images [Tolkach et al., [2020](#bib.bib180)]. Therefore, these AI-powered systems
    can be integrated into existing clinical workflows, which helps to improve diagnostic
    accuracy for clinical experts [Sim et al., [2020](#bib.bib169)] and support less-experienced
    clinicians [Tschandl et al., [2020](#bib.bib182)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning (DL) models serve as the core of these AI-powered systems for
    learning complex patterns from raw images and generalizing them to more unseen
    cases. The success of DL often relies on large-scale human-annotated datasets.
    For example, the ImageNet dataset [Deng et al., [2009](#bib.bib41)] contains tens
    of millions of labeled images, and it’s widely used in developing DL models for
    computer vision. The size of medical image datasets keeps expanding, but it is
    still relatively smaller than that of natural image datasets. For example, the
    brain tumor segmentation dataset BraTS [Menze et al., [2014](#bib.bib126), Baid
    et al., [2021](#bib.bib11)] consists of 3D multi-sequence MRI scans. The BraTS
    dataset expanded from 65 patients in 2013 to over 1,200 in 2021. The latter is
    equivalent to more than 700,000 annotated 2D images. However, the high annotation
    cost limits the construction of large-scale medical image datasets, mainly reflected
    in the following two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/089cdc486770d5b1645061134b100e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overall framework of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Fine-grained annotation of medical images is labor-intensive and time-consuming.
    In clinical practice, automatic segmentation helps clinicians outline different
    anatomical structures and lesions more accurately. However, training such a segmentation
    model requires pixel-wise annotation, which is extremely tedious [Rajpurkar et al.,
    [2022](#bib.bib152)]. Another case is in digital pathology. Pathologists usually
    require detailed examinations and interpretations of pathological tissue slices
    under high-magnification microscopes. Due to the complex tissue structures, pathologists
    must continuously adjust the microscope’s magnification. As a result, it usually
    takes 15 to 30 minutes to examine a single slide [Qu et al., [2022](#bib.bib147)].
    Making accurate annotations is even more challenging for pathologists. In conclusion,
    the annotation process in medical image analysis demands a considerable investment
    of time and labor.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The high bar for medical image annotation leads to high costs. In computer
    vision, tasks like object detection and segmentation also require many fine-grained
    annotations. However, the widespread use of crowdsourcing platforms has significantly
    reduced the cost of obtaining high-quality annotations in these tasks [Kovashka
    et al., [2016](#bib.bib102)]. However, crowdsourcing platforms have certain limitations
    in annotating medical images. Firstly, annotating medical images demands both
    medical knowledge and clinical expertise. Some complex cases even require discussions
    among multiple senior experts. Secondly, even in some relatively simple tasks,
    crowdsourcing workers tend to provide annotations of poorer quality than professional
    annotators in medical image analysis. For example, results in Rädsch et al. [[2023](#bib.bib151)]
    supported the conclusion above in annotating the segmentation mask of surgical
    instruments. Finally, crowdsourcing platforms may also raise privacy concerns
    [Rajpurkar et al., [2022](#bib.bib152)]. In summary, high-quality annotations
    often require the involvement of experienced doctors, which inherently increases
    the annotation cost of medical images.
  prefs: []
  type: TYPE_NORMAL
- en: The high annotation cost is one of the major bottlenecks of DL in medical image
    analysis. Active learning (AL) is considered one of the most effective solutions
    for reducing annotation costs. The main idea of AL is to select the most informative
    samples for annotation and then train a model with these samples in a supervised
    way. In the general practice of AL, annotating a part of the dataset could reach
    comparable performance of annotating all samples. As a result, AL saves the annotation
    costs by querying as few informative samples for annotation as possible. Specifically,
    we refer to the AL works focusing on training a deep model as deep active learning.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing AL works in medical image analysis is essential for reducing annotation
    costs. There are already some surveys on AL in machine learning or computer vision.
    Settles [[2009](#bib.bib162)] provided a general introduction and comprehensive
    review of AL works in the machine learning era. After the advent of DL, Ren et al.
    [[2021](#bib.bib154)] reviewed the development of deep active learning and its
    applications in computer vision and natural language processing. Liu et al. [[2022b](#bib.bib113)]
    summarized the model-driven and data-driven sample selectors in deep active learning.
    Zhan et al. [[2022](#bib.bib218)] reimplemented high-impact works in deep active
    learning with fair comparisons. Takezoe et al. [[2023](#bib.bib177)] reviewed
    recent developments of deep active learning in computer vision and its industrial
    applications. Regarding related surveys in medical image analysis, Budd et al.
    [[2021](#bib.bib24)] investigated the role of humans in developing and deploying
    DL in medical image analysis, where AL is considered a vital part of this process.
    In Tajbakhsh et al. [[2020](#bib.bib176)], AL was one of the solutions for training
    high-performance medical image segmentation models with imperfect annotation.
    As one of the methods in label-efficient deep learning for medical image analysis,
    Jin et al. [[2023a](#bib.bib79)] summarized AL methods from model and data uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: However, the surveys mentioned above have certain limitations. First, new ideas
    and methods are constantly emerging with the rapid development of deep active
    learning. Thus, a more comprehensive survey of AL is needed to cover the latest
    advancements. Second, a recent trend is combining AL with other label-efficient
    techniques, which is also highlighted as a future direction by related surveys
    [Takezoe et al., [2023](#bib.bib177), Budd et al., [2021](#bib.bib24)]. However,
    existing surveys still lack summaries and discussions on this topic. Finally,
    the high annotation cost emphasizes the increased significance of AL in medical
    image analysis, yet related reviews still lack comprehensiveness in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b6681025e611c587b6401172acca334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of the process of active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey comprehensively reviews AL for medical image analysis, including
    core methods, integration with other label-efficient techniques, and AL works
    tailored to medical image analysis. We first searched relevant papers on Google
    Scholar and arXiv platforms using the keyword “Active Learning” and expanded the
    search scope through citations. It should be noted that the included papers in
    this survey mainly belong to the fields of medical image analysis and computer
    vision. AL works on language, time series, tabular data, and graphs are less emphasized.
    Additionally, most works in this survey are published in top-tier journals (including
    TPAMI, TMI, MedIA, JBHI, etc.) and conferences (including CVPR, ICCV, ECCV, ICML,
    ICLR, NeurIPS, MICCAI, ISBI, MIDL, etc.). As a result, this survey involves nearly
    164 relevant AL works with 234 references. The contributions of this paper are
    summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through an exhaustive literature search, we provide a comprehensive review and
    a novel taxonomy for AL works, especially those focusing on medical image analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While previous surveys mainly focus on evaluating informativeness, we further
    summarize different sampling strategies in deep active learning, such as diversity
    and class-balance strategies, aiming to provide references for future method improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line with current trends, this survey is the first to provide a detailed
    review of the integration of AL with other label-efficient techniques, including
    semi-supervised learning, self-supervised learning, domain adaptation, region-based
    active learning, and generative models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this survey is organized as follows: §[2](#S2 "2 Problem Settings
    and Formulations of Active Learning ‣ A comprehensive survey on deep active learning
    and its applications in medical image analysis") introduces problem settings and
    mathematical formulation of AL, §[3](#S3 "3 Core Methods of Active Learning ‣
    A comprehensive survey on deep active learning and its applications in medical
    image analysis") discusses the core methods of AL, including evaluation of informativeness
    (§[3.1](#S3.SS1 "3.1 Evaluation of Informativeness: Uncertainty ‣ 3 Core Methods
    of Active Learning ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis") & §[3.2](#S3.SS2 "3.2 Evaluation of Informativeness:
    Representativeness ‣ 3 Core Methods of Active Learning ‣ A comprehensive survey
    on deep active learning and its applications in medical image analysis")) and
    sampling strategies (§[3.3](#S3.SS3 "3.3 Sampling Strategy ‣ 3 Core Methods of
    Active Learning ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis")), §[4](#S4 "4 Integration of Active Learning and Other
    Label-Efficient Techniques ‣ A comprehensive survey on deep active learning and
    its applications in medical image analysis") reviews the integration of AL with
    other label-efficient techniques, §[5](#S5 "5 Active Learning for Medical Image
    Analysis ‣ 4.5.2 Generative Active Learning ‣ 4.5 Generative Model: Data Augmentation
    and Generative Active Learning ‣ 4.4.3 Region-based Active Domain Adaptation ‣
    4.4 Region-based Active Learning: Smaller Labeling Unit ‣ 4.3 Active Domain Adaptation:
    Tackling Distribution Shift ‣ 4.2.2 Combination of Active Learning and Self-supervised
    Learning ‣ 4.2 Self-supervised Learning: Utilizing Pre-trained Model ‣ 4.1.2 Consistency
    Regularization ‣ 4.1 Semi-supervised Learning: Utilizing Unlabeled Data ‣ 4 Integration
    of Active Learning and Other Label-Efficient Techniques ‣ A comprehensive survey
    on deep active learning and its applications in medical image analysis") summarizes
    AL works tailored to medical image analysis. We discuss existing challenges and
    future directions of AL in §[6](#S6 "6 Challenges and Future Perspectives ‣ 5.3
    Active Learning in Medical Image Reconstruction ‣ 5 Active Learning for Medical
    Image Analysis ‣ 4.5.2 Generative Active Learning ‣ 4.5 Generative Model: Data
    Augmentation and Generative Active Learning ‣ 4.4.3 Region-based Active Domain
    Adaptation ‣ 4.4 Region-based Active Learning: Smaller Labeling Unit ‣ 4.3 Active
    Domain Adaptation: Tackling Distribution Shift ‣ 4.2.2 Combination of Active Learning
    and Self-supervised Learning ‣ 4.2 Self-supervised Learning: Utilizing Pre-trained
    Model ‣ 4.1.2 Consistency Regularization ‣ 4.1 Semi-supervised Learning: Utilizing
    Unlabeled Data ‣ 4 Integration of Active Learning and Other Label-Efficient Techniques
    ‣ A comprehensive survey on deep active learning and its applications in medical
    image analysis") and conclude the whole paper in §[7](#S7 "7 Conclusion ‣ 6.5
    Towards Active Learning with Foundation Models ‣ 6 Challenges and Future Perspectives
    ‣ 5.3 Active Learning in Medical Image Reconstruction ‣ 5 Active Learning for
    Medical Image Analysis ‣ 4.5.2 Generative Active Learning ‣ 4.5 Generative Model:
    Data Augmentation and Generative Active Learning ‣ 4.4.3 Region-based Active Domain
    Adaptation ‣ 4.4 Region-based Active Learning: Smaller Labeling Unit ‣ 4.3 Active
    Domain Adaptation: Tackling Distribution Shift ‣ 4.2.2 Combination of Active Learning
    and Self-supervised Learning ‣ 4.2 Self-supervised Learning: Utilizing Pre-trained
    Model ‣ 4.1.2 Consistency Regularization ‣ 4.1 Semi-supervised Learning: Utilizing
    Unlabeled Data ‣ 4 Integration of Active Learning and Other Label-Efficient Techniques
    ‣ A comprehensive survey on deep active learning and its applications in medical
    image analysis"). The overall framework of this survey is shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A comprehensive survey on deep active learning and
    its applications in medical image analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the rapid development of AL, many related works are not covered in this
    survey. We refer readers to our constantly updated website¹¹1https://github.com/LightersWang/Awesome-Active-Learning-for-Medical-Image-Analysis
    for the latest progress of AL and its application in medical image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Settings and Formulations of Active Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AL generally involves three problem settings: membership query synthesis, stream-based
    selective sampling, and pool-based active learning [Settles, [2009](#bib.bib162)].
    In the case of membership query synthesis, we can continuously query any samples
    in the input space for annotation, including synthetic samples produced by generative
    models [Angluin, [1988](#bib.bib4), [2004](#bib.bib5)]. We also refer to this
    setting as generative active learning in this survey. Membership query synthesis
    is typically suitable for low-dimensional input spaces. However, when expanded
    to high-dimensional spaces (e.g., images), the queried samples produced by generative
    models could be unidentifiable for human labelers. The recent advances of deep
    generative models have shown great promise in synthesizing realistic medical images,
    and we further discuss its combination with AL in §[4.5](#S4.SS5 "4.5 Generative
    Model: Data Augmentation and Generative Active Learning ‣ 4.4.3 Region-based Active
    Domain Adaptation ‣ 4.4 Region-based Active Learning: Smaller Labeling Unit ‣
    4.3 Active Domain Adaptation: Tackling Distribution Shift ‣ 4.2.2 Combination
    of Active Learning and Self-supervised Learning ‣ 4.2 Self-supervised Learning:
    Utilizing Pre-trained Model ‣ 4.1.2 Consistency Regularization ‣ 4.1 Semi-supervised
    Learning: Utilizing Unlabeled Data ‣ 4 Integration of Active Learning and Other
    Label-Efficient Techniques ‣ A comprehensive survey on deep active learning and
    its applications in medical image analysis"). Stream-based selective sampling
    assumes that samples arrive one by one in a continuous stream, and we need to
    decide whether or not to request annotation for incoming samples [Cohn et al.,
    [1994](#bib.bib38)]. This setting is suitable for scenarios with limited memory,
    such as edge computing, but it neglects sample correlations.'
  prefs: []
  type: TYPE_NORMAL
- en: Most AL works follow pool-based active learning, which draw samples from a large
    pool of unlabeled data and requests oracle (e.g., doctors) for annotations. Moreover,
    if multiple samples are selected for labeling at once, we can further call this
    setting “batch-mode”. Deep active learning is in batch-mode by default since retraining
    the model every time a sample is labeled is impractical. Also, one labeled sample
    may not necessarily result in significant performance improvement. Therefore,
    unless otherwise specified, all works in this survey follow the setting of batch-mode
    pool-based active learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flowchart of active learning is illustrated in Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis"). Assuming a total of $T$ annotation rounds, active
    learning primarily consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Sample Selection: In the $t$-th round of annotation, $1\leq t\leq T$, an
    acquisition function $A$ is used to evaluate the informativeness of each sample
    in the unlabeled pool $D_{t}^{u}$. Then, a batch of samples is selected with a
    certain sampling strategy $S$. Specifically, the queried dataset of $t$-th round
    $D_{t}^{q}$ is constructed as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{t}^{q}=\underset{D_{t}^{q}\subset D_{t}^{u}}{S}\left(\underset{x\in
    D_{t}^{u}}{A}\left(x,f_{\theta_{t-1}}\right),b\right)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $x$ represents sample in the dataset, $D_{t}^{u}$ and $D_{t}^{q}$ are
    unlabeled and queried dataset in round $t$, respectively. $f_{\theta_{t-1}}$ and
    $\theta_{t-1}$ represent the deep model and its parameters from the previous round,
    respectively. The annotation budget $b$ is the number of queried samples for each
    round, far less than the total count of unlabeled samples, i.e., $b=|D_{t}^{q}|\ll|D_{t}^{u}|$.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Annotation by Oracle: After sample selection, the queried set $D_{t}^{q}$
    is sent to oracle (e.g., doctors) for annotation, and newly labeled samples are
    added into the labeled dataset $D_{t}^{l}$. The update of $D_{t}^{l}$ is as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{t}^{l}=D_{t-1}^{l}\cup\{(x,y)&#124;x\in D_{t}^{q}\}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $y$ represents the label of $x$, and $D_{t}^{l}$ and $D_{t-1}^{l}$ denote
    the labeled sets for round $t$ and the previous round, respectively. Besides,
    the queried samples should be removed from the unlabeled set $D_{t}^{u}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{t}^{u}=D_{t-1}^{u}\backslash\{(x,y)&#124;x\in D_{t}^{q}\}$ |  | (3)
    |'
  prefs: []
  type: TYPE_TB
- en: '3\. DL Model Training: After oracle annotation, we train the deep model using
    the labeled set of this round $D_{t}^{l}$ in a fully supervised manner. The deep
    model $f_{\theta_{t}}$ is trained on $D_{t}^{l}$ to obtain the optimal parameters
    $\theta_{t}$ for round $t$. The mathematical formulation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{t}=\underset{\theta}{\arg\min}\underset{(x,y)\in D_{t}^{l}}{\mathbb{E}}\left[\mathcal{L}(f_{\theta}(x),y)\right]$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}$ represents the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Repeat steps 1 to 3 until the annotation budget limit is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that the model needs proper initialization to start the
    AL process. If the initial model $f_{\theta_{0}}$ is randomly initialized, it
    could only produce meaningless informativeness. To address this issue, most AL
    works randomly choose some samples as initially labeled dataset $D_{0}^{l}$ and
    train $f_{\theta_{0}}$ upon $D_{0}^{l}$. For more details on better initialization
    of AL using pre-trained models, please refer to §[4.2](#S4.SS2 "4.2 Self-supervised
    Learning: Utilizing Pre-trained Model ‣ 4.1.2 Consistency Regularization ‣ 4.1
    Semi-supervised Learning: Utilizing Unlabeled Data ‣ 4 Integration of Active Learning
    and Other Label-Efficient Techniques ‣ A comprehensive survey on deep active learning
    and its applications in medical image analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Core Methods of Active Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we consider the evaluation of informativeness and sampling
    strategy as the core methods of AL. Informativeness represents the value of annotating
    each sample. Higher informativeness indicates a higher priority to request these
    samples for labeling. Typical metrics of informativeness include uncertainty based
    on model prediction and representativeness based on data distribution. On the
    other hand, the sampling strategy is used to select a small number of unlabeled
    samples for annotation based on their informativeness metrics. Common sampling
    strategies include top-k selection and clustering, etc. Unlike previous surveys,
    we explicitly define sampling strategies as core methods of AL for the first time.
    The rationale is that if a perfect informativeness metric existed, one could simply
    select the samples with the highest value of the informativeness metric according
    to the annotation budget. However, current informativeness metrics are more or
    less flawed to some extent. For instance, redundant and class-imbalanced queries
    are common issues in AL. We need specific sampling strategies to mitigate the
    issues arising from imperfect informativeness metrics. In this section, we reviewed
    uncertainty (§[3.1](#S3.SS1 "3.1 Evaluation of Informativeness: Uncertainty ‣
    3 Core Methods of Active Learning ‣ A comprehensive survey on deep active learning
    and its applications in medical image analysis")), informativeness (§[3.2](#S3.SS2
    "3.2 Evaluation of Informativeness: Representativeness ‣ 3 Core Methods of Active
    Learning ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis")) and sampling strategy (§[3.3](#S3.SS3 "3.3 Sampling
    Strategy ‣ 3 Core Methods of Active Learning ‣ A comprehensive survey on deep
    active learning and its applications in medical image analysis")). Additionally,
    we provide a summarization of all works in this survey. Methods and basic metrics
    of calculating uncertainty or representativeness and sampling strategies are detailed
    in Table [4](#S4 "4 Integration of Active Learning and Other Label-Efficient Techniques
    ‣ A comprehensive survey on deep active learning and its applications in medical
    image analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Evaluation of Informativeness: Uncertainty'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Uncertainty is used to assess the reliability of model predictions, with higher
    uncertainty indicating the model may be more prone to errors [Kendall and Gal,
    [2017](#bib.bib90)]. During sample selection in AL, it is challenging to determine
    the correctness of the model’s predictions. Nevertheless, we can highlight samples
    where the model is prone to making errors by uncertainty estimation. Uncertain
    samples often contain knowledge about the model that has not yet been mastered.
    Annotating them can improve the performance. Therefore, uncertainty has become
    one of the most commonly used informativeness metrics in AL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward uncertainty metrics are based on prediction probabilities,
    including least confidence [Lewis and Catlett, [1994](#bib.bib107)], entropy [Joshi
    et al., [2009](#bib.bib85)], margin [Roth and Small, [2006](#bib.bib156)], and
    mean variance [Gal et al., [2017](#bib.bib52)]. These metrics have been widely
    used in traditional AL, and their formulations are detailed in Table [1](#S3.T1
    "Table 1 ‣ 3.1 Evaluation of Informativeness: Uncertainty ‣ 3 Core Methods of
    Active Learning ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis"). Confidence is the probability of the highest predicted
    class. We often adopt the least confidence to measure uncertainty, meaning lower
    confidence indicates higher uncertainty. Entropy is the most common measure of
    uncertainty, with higher entropy indicating higher uncertainty. The margin represents
    the difference between the highest and the second-highest predicted probability,
    with a larger margin indicating greater uncertainty. Besides, a larger mean variance
    indicates higher uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Formulations of Uncertainty Metrics based on Prediction Probability.
    $x$ stands for sample, $f$ is the deep model, while $C$ is the number of classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Equations |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction Probability | $p=\text{Softmax}\left(f\left(x\right)\right)\in\mathbb{R}^{C},p=\left[p_{1},p_{2},\cdots,p_{C}\right]$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Least Confidence | $1-\underset{i}{\max}{p_{i}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | $-\sum_{i=1}^{C}{p_{i}\log{p_{i}}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | $\ \underset{i}{\max}{p_{i}}-\underset{j,j\neq k}{\max}{p_{j}},k=\underset{i}{\arg\max}{p_{i}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Variance | $-\frac{1}{C}\sum_{i=1}^{C}\left(p_{i}-\bar{p}\right)^{2},\bar{p}=-\frac{1}{C}\sum_{i=1}^{C}p_{i}$
    |'
  prefs: []
  type: TYPE_TB
- en: 'The above uncertainty metrics only require a single forward pass in deep learning.
    However, due to the notorious issue of over-confidence in deep neural networks
    [Guo et al., [2017](#bib.bib60)], they cannot be directly transferred to deep
    AL. In deep learning, over-confidence refers to the model having excessively high
    confidence in its predictions, even though they might not be accurate. Over-confidence
    results in high confidence (e.g., 0.99) of the wrong class for misclassified samples.
    For uncertain samples, it may lead to extreme confidence (e.g., 0.99 or 0.01)
    instead of normal one (e.g., 0.6 or 0.4) as it should. Over-confidence may cause
    distorted uncertainty since it affects the predicted probabilities for all classes.
    This section divides the uncertainty-based AL into multiple inference, disagreement-based
    uncertainty, uncertainty-aware models, gradient-based uncertainty, adversarial-based
    uncertainty, and performance estimation. The first three are primarily based on
    prediction probabilities, while the latter three mostly employ other statistics
    of deep models for uncertainty estimation. The taxonomy of uncertainty-based AL
    is shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Evaluation of Informativeness: Uncertainty
    ‣ 3 Core Methods of Active Learning ‣ A comprehensive survey on deep active learning
    and its applications in medical image analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddd3a59b339a0d6c0212d8d971f1b0ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The taxonomy of uncertainty-based active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Multiple Inferences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To mitigate over-confidence, a common strategy is to run the model multiple
    times with some perturbations and calculate the classic uncertainty metrics with
    the mean probability. The main idea is to reduce the bias introduced by network
    architectures or training data. These biases often contribute to the over-confidence
    issue. Three methods can be employed for multiple inferences: Monte Carlo dropout
    (MC dropout), model ensemble, and data augmentation. The first two perturb the
    model parameters, and the last perturb the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: MC dropout randomly discards some neurons in the deep model during each inference
    [Gal and Ghahramani, [2016](#bib.bib51)]. With MC dropout enabled, the model runs
    multiple times to get different predictions. Gal et al. [[2017](#bib.bib52)] was
    the pioneering work of deep AL. They were also the first to use MC dropout in
    computing uncertainty metrics like entropy, standard deviation, and Bayesian active
    learning disagreement (BALD) [Houlsby et al., [2011](#bib.bib69)]. Results showed
    that MC Dropout could significantly improve the performance of uncertainty-based
    deep AL.
  prefs: []
  type: TYPE_NORMAL
- en: Model ensemble trains multiple models to get numerous predictions during inference.
    Beluch et al. [[2018](#bib.bib16)] conducted a detailed comparison of models ensemble
    and MC dropout in uncertainty-based AL. Results demonstrated that the model ensemble
    performs better. However, model ensemble requires significant training overhead
    in DL. To reduce the computational costs, snapshot ensemble [Huang et al., [2017](#bib.bib73)]
    obtained multiple models in a single run with cyclic learning rate decay. An early
    attempt in Beluch et al. [[2018](#bib.bib16)] showed that snapshot ensemble leads
    to worse performance than model ensemble. Jung et al. [[2023](#bib.bib86)] improved
    the snapshot ensemble by maintaining the same optimization trajectory in different
    AL rounds, along with parameter regularization. Results showed that the improved
    snapshot ensemble outperforms the model ensemble. Additionally, Nath et al. [[2021](#bib.bib131)]
    employed stein variational gradient descent to train an ensemble of models, aiming
    to ensure diversity among them.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation produces different versions of input data with random transformations.
    Then, multiple predictions were obtained by running the model with different augmentations
    during inference. In point cloud semantic segmentation, Hu et al. [[2022](#bib.bib71)]
    randomly augmented the input point clouds multiple times, then calculated the
    entropy of average prediction probability for each point after registration and
    correspondence estimation. Liu et al. [[2022a](#bib.bib111)] also applied random
    augmentations to point clouds, but they used variance as the uncertainty metric.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Disagreement-based Uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Disagreements between different inferences of the same sample could also be
    a measure of uncertainty. Samples with higher disagreement indicate higher uncertainty
    and are suitable for annotation. The principle of methods in this section aligns
    with the previous section, which aims to mitigate over-confidence by introducing
    perturbations during multiple inferences. However, methods in the previous section
    focus on improving the uncertainty estimation of classical metrics, while methods
    in this section leverage the disagreements between different prediction results.
    In this line of research, we can build disagreement from the perspective of model
    and data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model disagreement: We can utilize the disagreement between the outputs of
    different models, also known as Query-by-Committee (QBC) [Seung et al., [1992](#bib.bib163)].
    Suggestive annotation (SA) [Yang et al., [2017](#bib.bib211)] trained multiple
    segmentation networks with bootstrapping. The variance among these models is used
    as the disagreement metric. Mackowiak et al. [[2018](#bib.bib120)] adopted the
    vote entropy between different MC dropout inferences as the disagreement metric.
    Peng et al. [[2021](#bib.bib141)] trained teacher and student models through knowledge
    distillation and used the L2 distance of segmentation predictions as the disagreement
    metric. In polyp segmentation of capsule colonoscopy, Bai et al. [[2022](#bib.bib10)]
    trained multiple decoders using class activation maps (CAMs) [Zhou et al., [2016](#bib.bib229)]
    generated by a classification network. They further proposed model disagreement
    and CAM disagreement for sample selection. Model disagreement included entropy
    of prediction probabilities and Dice between outputs of different decoders, while
    CAM disagreement measured the Dice between CAMs and outputs of all decoders. This
    method selected samples with high model disagreement and CAM disagreement for
    annotation. However, samples with low model disagreement but high CAM disagreement
    were treated as pseudo-labels for semi-supervised training. In rib fracture detection,
    Huang et al. [[2020](#bib.bib75)] adopted Hausdorff distance to measure the disagreements
    between different CAMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data disagreement: Since training multiple models can be computationally expensive,
    measuring the disagreements between different perturbations of input data is also
    helpful in AL. Kullback-Leibler (KL) divergence is a commonly used metric for
    quantifying disagreement. Wu et al. [[2021b](#bib.bib201)] computed KL divergence
    between different versions of augmentations as the disagreement measure. Siddiqui
    et al. [[2020](#bib.bib168)] measured the disagreement with KL divergence between
    predictions of different viewpoints in 3D scenes. In point cloud segmentation,
    Hu et al. [[2022](#bib.bib71)] employed KL divergence to measure the disagreement
    between predictions of different frames. Additionally, recent works have adopted
    alternative metrics to calculate disagreement. Lyu et al. [[2023](#bib.bib119)]
    proposed input-end committee, which constructs different predictions by randomly
    augmenting the input data. They further measured the classification and localization
    disagreements between different predictions with cross-entropy and variance, respectively.
    Parvaneh et al. [[2022](#bib.bib139)] interpolated the unlabeled samples and labeled
    prototypes in the feature space. If the prediction of the interpolated sample
    disagrees with the label of the prototype, it indicates that the unlabeled samples
    may introduce new features. Then, these unlabeled samples should be sent for annotation.
    Results showed advancements across various datasets and settings. Besides, some
    works explored the prediction disagreement within a local area inside an image.
    In object detection, Aghdam et al. [[2019](#bib.bib3)] assumed that the disagreement
    between probabilities in the neighborhood of wrongly predicted pixels should be
    high. They adopted BALD as the disagreement metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Uncertainty-aware Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main idea of uncertainty-aware models is to transform commonly used deterministic
    models into probabilistic models. In this way, the network no longer outputs a
    single point estimate but instead provides a distribution of possible predictions,
    thus mitigating over-confidence. This approach only requires a single pass of
    the deep model, significantly reducing computational and time costs during inference.
    Related works mainly fall into two categories: evidential deep learning (EDL)
    and mixture density networks (MDN).'
  prefs: []
  type: TYPE_NORMAL
- en: Evidential deep learning replaces the Softmax distribution with a Dirichlet
    distribution [Sensoy et al., [2018](#bib.bib161)]. The network’s output is interpreted
    as the parameters of a Dirichlet distribution, so the predictions followed the
    Dirichlet distribution. The Dirichlet distribution will be sharp if the model
    is confident about the predictions. Otherwise, it will be flat. To make EDL compatible
    with object detection tasks, Park et al. [[2023](#bib.bib138)] introduced a model
    evidence head to scale the parameters of the Dirichlet distribution adaptively,
    which enhanced training stability. They first calculated the epistemic uncertainty
    for each detection box. Then, the sample-level uncertainty was obtained through
    hierarchical uncertainty aggregation. Sun et al. [[2023](#bib.bib175)] also applied
    EDL in scene graph generation, achieving near full-supervision performance with
    only about 10% of the labeling cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixture density networks: Choi et al. [[2021a](#bib.bib35)] transformed the
    classification and localization heads in object detection networks to the architecture
    of MDN [Bishop, [1994](#bib.bib21)]. Besides the coordinates and class predictions
    of each bounding box, the MDN heads produced the variance of classification and
    localization. They used the variances as uncertainty metrics for sample selection.
    Results showed that this method is competitive with MC dropout and model ensemble
    while significantly reducing the inference time and model size.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Gradient-based Uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Gradient-based optimization is essential for deep learning. The gradient of
    each sample reflects its contribution to the change of model parameters. A larger
    gradient length indicates a tremendous change of parameters by the sample, thus
    implying high uncertainty. Furthermore, gradients are independent of predictive
    probabilities, making them less susceptible to over-confidence. In deep active
    learning, three metrics are frequently used as gradient-based uncertainty: gradients,
    Fisher information (FI), and influence functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient: A larger gradient norm (i.e., gradient length) denotes a greater
    influence on model parameters, indicating higher uncertainty in AL. Ash et al.
    [[2020](#bib.bib8)] proposed batch active learning by diverse gradient embeddings
    (BADGE). They calculated the gradients only for the parameters of the network’s
    final layer, with the most confident classes as pseudo labels in gradient computation.
    Then, k-Means++ is performed on gradient embeddings for sample selection. Results
    showed competitive performances of BADGE across diverse datasets, network architectures,
    and hyperparameter settings. Wang et al. [[2022b](#bib.bib192)] proved that a
    larger gradient norm corresponds to a lower upper bound of test loss. Thus, they
    employed expected empirical loss and entropy loss for gradient computation, which
    both obviate the necessity for labels. The former was a sum of the losses for
    each class, which was weighted by the corresponding probability. The latter was
    the entropy of probabilities of all classes. In MRI brain tumor segmentation,
    Dai et al. [[2020](#bib.bib40)] employed gradients for active learning. They first
    trained a variational autoencoder (VAE) [Kingma and Welling, [2013](#bib.bib93)]
    to learn the data manifold. Then, they trained a segmentation model and calculated
    gradients of Dice loss using available labeled data. The sample selection was
    guided by the gradient projected onto the data manifold. Their extended work [Dai
    et al., [2022](#bib.bib39)] further demonstrated superior performance in MRI whole
    brain segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fisher information: As the expectation of the gradient’s covariance matrix,
    FI reflects overall uncertainty according to the data distribution to model parameters.
    Annotating samples with higher FI helps the model converge faster toward optimal
    parameters. FI has already succeeded in AL of machine learning models [Chaudhuri
    et al., [2015](#bib.bib29), Sourati et al., [2017](#bib.bib171)]. However, the
    computation cost of FI grows quadratically with the increase of model parameters,
    which is unacceptable for deep active learning. Sourati et al. [[2018](#bib.bib172)]
    and their extended work [Sourati et al., [2019](#bib.bib173)] were the first to
    incorporate FI into deep active learning. They used the average gradients of each
    layer to calculate the FI matrix, thus reducing the computation cost. This method
    outperformed competitors in brain extraction across different age groups and pathological
    conditions. Additionally, Ash et al. [[2021](#bib.bib7)] only computed the FI
    matrix for the network’s last layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Influence functions: Liu et al. [[2021b](#bib.bib116)] employed influence functions
    [Koh and Liang, [2017](#bib.bib96)] to select samples that bring the most positive
    impact on model performance. The method used expected empirical loss to calculate
    the gradient since influence functions also require gradient computations.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Adversarial-based Uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Uncertainty in AL can also be estimated adversarially, including adversarial
    samples and adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial samples are created by adding carefully designed perturbations to
    normal samples by attacking the deep models [Goodfellow et al., [2014b](#bib.bib58)].
    The differences between adversarial and original samples are nearly indiscernible
    to the human eye. However, deep models would produce extremely confident wrong
    predictions for adversarial samples. The reason is that adversarial attacks push
    the original samples to the other side of the decision boundary with minimal cost,
    resulting in visually negligible changes but significantly different predictions.
    From this perspective, the strength of adversarial attacks reflects the sample’s
    distance to the decision boundary [Heo et al., [2019](#bib.bib65)]. A small perturbation
    indicates that the sample is closer to the decision boundary and, thus, is considered
    more uncertain. Ducoffe and Precioso [[2018](#bib.bib45)] adopted the DeepFool
    algorithm [Moosavi-Dezfooli et al., [2016](#bib.bib129)] for adversarial attacks.
    Samples with small adversarial perturbations are requested for labeling. Rangwani
    et al. [[2021](#bib.bib153)] attacked the deep model by maximizing the KL divergence
    between predictions of adversarial and original samples while the strength of
    perturbation is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training involves alternating training between feature extractors
    and multiple classifiers. The objectives of training feature extractors and classifiers
    are conflicting. Multi-round adversarial training increases the disagreements
    between classifiers, uncovering uncertain samples hidden by over-confidence. In
    object detection, Yuan et al. [[2021](#bib.bib217)] and their extended work [Wan
    et al., [2023](#bib.bib185)] used two classifiers for adversarial training on
    both labeled and unlabeled datasets. The first step is to fix the feature extractor
    and tune the two classifiers. The more classifiers disagreed the more uncertain
    samples were exposed. Then, they fixed the classifiers and tuned the feature extractor
    with opposite objectives, aiming to narrow the distribution gap between labeled
    and unlabeled samples. After multiple rounds of alternating training, samples
    with the highest disagreements between classifiers are sent for annotation. Fu
    et al. [[2021](#bib.bib49)] adversarially trained multiple classifiers to maximize
    their disagreement. The standard deviation between different predictions was considered
    the uncertainty metric.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Performance Estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, the uncertainty metrics are direct performance estimations
    of the current task. There are two types of such metrics: test loss or task-specific
    evaluation metrics. These metrics reflect the level of prediction error. For instance,
    a low Dice score suggests the model failed to produce accurate segmentation. Request
    annotations for these samples would be beneficial for improving the model’s performance.
    However, instead of calculating these metrics precisely, we can only estimate
    them without the ground truths. There are primarily two methods for estimating
    performance: surrogate metrics and learnable performance estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: Surrogate metrics are widely used in this line of research. For example, these
    metrics may be upper or lower bounds for loss or some evaluation metrics. Huang
    et al. [[2021](#bib.bib74)] found that within limited training iterations, the
    loss of a sample is bounded by the norm of the difference between the initial
    and final network outputs. Inspired by this, they proposed cyclic output discrepancy
    (COD) as the difference in model output between two consecutive annotation rounds.
    Results indicated that a higher COD is associated with higher loss. Therefore,
    they opted for samples with high COD. They may also demonstrate a linear correlation
    with the evaluation metrics with post-hoc validation. Shen et al. [[2020](#bib.bib165)]
    calculated the intersection over union (IoU) of all predictions by MC dropout.
    They found a strong linear correlation between this IoU and the real Dice coefficient.
    Zhao et al. [[2021](#bib.bib226)] calculated the average Dice coefficient between
    the predictions of the intermediate layers and final layer through deep supervision.
    They also found a linear correlation between this average Dice and the real Dice
    coefficient. Results showed competitive performance in skin lesion segmentation
    and X-ray hand bone segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learnable performance estimation: Additionally, we can train auxiliary neural
    network modules to predict the performance metrics. As one of the most representative
    works in this line of research, learning loss for active learning (LL4AL) [Yoo
    and Kweon, [2019](#bib.bib214)] trained an additional module to predict the loss
    value of a sample without its label. Since loss indicates the quality of network
    predictions, the predicted loss is a natural uncertainty metric for sample selection.
    Results showed that predicted and actual losses are strongly correlated. LL4AL
    also outperformed several AL baselines. In lung nodule detection with CT scans,
    Liu et al. [[2020](#bib.bib112)] built upon LL4AL to predict the loss of each
    sample and bounding box. In diagnosing COVID-19, Wu et al. [[2021b](#bib.bib201)]
    adopted both the predicted loss and the disagreements between different predictions
    for sample selection. Since AL focuses only on uncertainty ranking of the unlabeled
    samples, Kim et al. [[2021](#bib.bib92)] relaxed the loss regression to loss ranking
    prediction. Thus, they replaced the loss regressor in LL4AL with the ranker in
    RankCGAN [Saquil et al., [2018](#bib.bib159)]. Results showed that loss ranking
    prediction outperforms LL4AL.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Evaluation of Informativeness: Representativeness'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While uncertainty methods play a crucial role in deep AL, they still face certain
    challenges: 1\. Outlier selection: The goal of using uncertainty in AL is to improve
    performance by querying hard samples of the current model. However, these methods
    could also select outliers that harm the model training [Karamcheti et al., [2021](#bib.bib87)].
    This happens because uncertainty relies sorely on model predictions and ignores
    the exploration of intrinsic characteristics of the data distribution. Incorporating
    an additional informativeness measure to remove outliers would be beneficial for
    AL. 2\. Distribution misalignment: Samples selected by uncertainty methods are
    often located near the decision boundary in the feature space [Settles, [2009](#bib.bib162)].
    Therefore, the distribution of selected samples by uncertainty-based method may
    differ from the overall data distribution. This discrepancy may introduce dataset
    bias and lead to a performance drop. Therefore, the challenges above compel us
    further to discover the data distribution during the evaluation of informativeness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Representativeness-based deep AL aims to select a subset of samples that can
    represent the entire dataset. Generally, highly representative samples are located
    in dense regions of the data manifold and contain information about other nearby
    samples. Moreover, these methods require diversity in sampling result. Representative
    samples should be widely distributed across the data manifold rather than concentrated
    in a specific region. Besides, representative samples should be visually distinctive
    in properties like imaging style or visual content. Deep feature representations
    encode such information and are used to calculate the mutual relationships between
    different samples. This section introduces three formulations of representativeness-based
    AL: cover-based, discrepancy-based, and density-based representativeness AL. The
    taxonomy of these methods is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 Evaluation
    of Informativeness: Representativeness ‣ 3 Core Methods of Active Learning ‣ A
    comprehensive survey on deep active learning and its applications in medical image
    analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a85116cac04f50918a1e67a6fd982d7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The taxonomy of representativeness-based active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Cover-based Active Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can formulate representativeness-based AL as a problem of covering. A classic
    example of the covering problem is the facility location, such as covering all
    the city’s streets with some billboards [Farahani and Hekmatfar, [2009](#bib.bib47)].
    Likewise, cover-based AL uses a few samples to cover the entire dataset. Ideally,
    these samples should be representative and contain information from other samples.
    These methods usually involve two settings: set cover and maximum coverage. Both
    settings are NP-hard, meaning they cannot be optimally solved in polynomial time.
    However, near-optimal solutions could be achieved in linear time using greedy
    algorithms. Specifically, the greedy algorithms iteratively select samples that
    cover other samples the most for annotation [Feige, [1998](#bib.bib48)].'
  prefs: []
  type: TYPE_NORMAL
- en: Set cover aims to select as few samples as possible to cover the entire dataset.
    CoreSet [Sener and Savarese, [2018](#bib.bib160)] followed the setting of k-Center
    location [Hochbaum and Shmoys, [1985](#bib.bib67)], which is also a variant of
    the set cover problem. In CoreSet, the L2 distance of deep features measures the
    similarity between different samples. They employed farthest-first traversal to
    solve the k-Center problem for selecting representative samples. Agarwal et al.
    [[2020](#bib.bib2)] introduced contextual diversity for AL, a metric that fused
    uncertainty and diversity of samples spatially and semantically. They replaced
    the L2 distance with contextual diversity and used the same method in CoreSet
    for sample selection. Caramalau et al. [[2021](#bib.bib26)] adopted graph convolutional
    networks (GCN) to model the relationships between labeled and unlabeled samples.
    GCNs improved the feature representation of unlabeled samples with the labeled
    dataset. Enhanced feature representation was further used for CoreSet sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum coverage selects a given number of samples to cover the entire dataset
    as much as possible. Yehuda et al. [[2022](#bib.bib212)] found that CoreSet tends
    to select outliers, especially when the annotation budget is low. To address this
    issue, they proposed ProbCover, which changed the setting from set cover to maximum
    coverage. They employed a graph-based greedy algorithm for sample selection. With
    the help of self-supervised deep features, ProbCover effectively avoided selecting
    outlier samples. Additionally, SA [Yang et al., [2017](#bib.bib211)] provides
    another formulation of maximum coverage. SA first selected highly uncertain samples
    and then further chose representative samples for annotation. The representativeness
    was based on the cosine similarity of deep features. Specifically, sample $x$
    is represented by the most similar sample from queried dataset $D_{t}^{q}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r\left(D_{t}^{q},x\right)=\underset{x^{\prime}\in D_{t}^{q}}{\max}{sim\left(x^{\prime},x\right)}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $r$ is the representativeness of sample $x$ with respect to $D_{t}^{q}$
    and $sim(\cdot,\cdot)$ represents cosine similarity. Besides, representativeness
    $R$ between $D_{t}^{q}$ and the unlabeled set $D_{t}^{u}$ is as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R\left(D_{t}^{q},D_{t}^{u}\right)=\underset{x\in D_{t}^{u}}{\sum}r\left(D_{t}^{q},x\right)$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where a larger $R\left(D_{t}^{q},D_{t}^{u}\right)$ indicates that $D_{t}^{q}$
    better represents $D_{t}^{u}$. It should be noted that SA is a generalization
    of the maximum coverage problem since the cosine similarity ranges from 0 to 1.
    But they still employed a greedy algorithm to find sample $x$ that maximizing
    $R\left(D_{t}^{q}\cup x,D_{t}^{u}\right)-R\left(D_{t}^{q},D_{t}^{u}\right)$. SA
    has inspired many subsequent works. Xu et al. [[2018](#bib.bib209)] quantized
    the segmentation networks in SA and found that it improved the accuracy of gland
    segmentation while significantly reducing memory usage. Zheng et al. [[2019](#bib.bib227)]
    proposed representative annotation (RA), which omits the uncertainty query in
    SA. RA trained a VAE for feature extraction and partitioned the feature space
    using hierarchical clustering. They selected representative samples in each cluster
    using a similar strategy to SA. Shen et al. [[2020](#bib.bib165)] changed the
    similarity measure in SA from $sim(\cdot,\cdot)$ to $1-sim(\cdot,\cdot)$, which
    enhanced the diversity of the selected samples. In keypoint detection of medical
    images, Quan et al. [[2022](#bib.bib149)] proposed a representative method to
    select template images for few-shot learning. First, they trained a feature extractor
    using self-supervised learning and applied SIFT for initial keypoint detection.
    Next, they calculated the average cosine similarity between template images and
    the entire dataset. Finally, they picked the template combination with the highest
    similarity for annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'View of submodular functions: Both set cover and maximum coverage can be formulated
    from the perspective of submodular functions [Fujishige, [2005](#bib.bib50)].
    These functions show diminishing marginal returns, meaning each added element
    brings less gain than the previous one as the set gets larger. Generally, each
    submodular function corresponds to a particular optimization problem. If a submodular
    function is monotonic and non-negative, we can use a greedy algorithm to get near-optimal
    solutions in linear time. In cover-based AL, methods like SA and RA followed the
    setting of submodular functions, but the authors didn’t present their methods
    from this perspective. Introducing submodular functions would extend the formulation
    of AL and ensure the selected samples are both representative and diverse. Typical
    steps for this type of method involve calculating sample similarities, constructing
    a submodular optimization problem, and solving it using a greedy algorithm [Wei
    et al., [2015](#bib.bib196)]. Kothawade et al. [[2021](#bib.bib98)] introduced
    an AL framework based on submodular information measures, effectively addressing
    issues such as scarcity of rare class, redundancy, and out-of-distribution data.
    In object detection, Kothawade et al. [[2022a](#bib.bib99)] focused on samples
    of minority classes. They constructed a set of classes of interest and selected
    unlabeled samples similar to these classes for annotation through submodular mutual
    information [Kothawade et al., [2022b](#bib.bib100)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Discrepancy-based Active Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In discrepancy-based AL, unlabeled samples farthest from the labeled set are
    considered the most representative. The main idea is that if we queried such samples
    for multiple rounds, the discrepancy between the distributions of labeled and
    unlabeled sets would be significantly reduced. Therefore, a small set of samples
    could well represent the entire dataset. The key to these methods is measuring
    the discrepancy (i.e., distance) between two high-dimensional distributions. This
    section presents three metrics for measuring discrepancy: similarity-based discrepancy,
    H-divergence, and Wasserstein distance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarity-based discrepancy: As a practical and easy-to-implement metric,
    we can approximate the distance between distributions based on sample similarity.
    Caramalau et al. [[2021](#bib.bib26)] proposed UncertainGCN, which employed GCN
    to model the relationship between labeled and unlabeled samples. They selected
    the unlabeled samples with the lowest similarity to the labeled set. In gland
    and MRI infant brain segmentation, Li and Yin [[2020](#bib.bib108)] adopted the
    average cosine similarity as the distance between two datasets. They selected
    samples far from the labeled set and close to the unlabeled set. In object detection,
    Wu et al. [[2022a](#bib.bib198)] constructed prototypes with sample features and
    prediction entropy. They selected unlabeled samples that were far from the labeled
    prototype.'
  prefs: []
  type: TYPE_NORMAL
- en: H-divergence estimates the distance of distribution with the help of the discriminator
    from generative adversarial networks (GAN) [Goodfellow et al., [2014a](#bib.bib57)].
    More specifically, the discriminator tries to distinguish between labeled and
    unlabeled samples, and there is a close relationship between H-divergence and
    the discriminator’s output [Gissin and Shalev-Shwartz, [2019](#bib.bib55)]. Variational
    adversarial active learning (VAAL) [Sinha et al., [2019](#bib.bib170)] combined
    VAE with a discriminator for discrepancy-based AL. In VAAL, the VAE mapped samples
    to a latent space while the discriminator distinguished whether samples were labeled.
    These two are mutually influenced by adversarial training. VAE tried to fool the
    discriminator into judging all samples as labeled while the discriminator attempted
    to correctly differentiate between labeled and unlabeled samples. After multiple
    rounds of adversarial training, VAAL selected samples that the discriminator deemed
    most likely to be unlabeled for annotation. Unlike VAAL, Gissin and Shalev-Shwartz
    [[2019](#bib.bib55)] trained the discriminator without adversarial training. Zhang
    et al. [[2020](#bib.bib219)] replaced the discriminator’s binary label with sample
    uncertainty. They also combined features of VAE with features from the supervised
    model. Wang et al. [[2020b](#bib.bib190)] adopted a neural network module for
    sample selection. To train such a module, they added another discriminator on
    top of VAAL, which aimed to differentiate between the real and VAE-reconstructed
    features for unlabeled samples. After adversarial training of both discriminators,
    the module selected uncertain and representative samples. Kim et al. [[2021](#bib.bib92)]
    combined LL4AL with VAAL, feeding both loss ranking predictions and VAE features
    into the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein distance is widely used for computing distribution distances. Shui
    et al. [[2020](#bib.bib167)] indicated that H-divergence may compromise the diversity
    of sample selection, while Wasserstein distance ensures the queried samples are
    representative and diverse. They further proposed Wasserstein adversarial active
    learning (WAAL). Specifically, WAAL was built upon VAAL and adopted an additional
    module for sample selection. They trained this module by minimizing the Wasserstein
    distance between labeled and unlabeled sets. WAAL selected samples that are highly
    uncertain and most likely to be unlabeled for annotation. Mahmood et al. [[2022](#bib.bib124)]
    formulated AL as an optimal transport problem. They aimed at minimizing the Wasserstein
    distance between the labeled and unlabeled sets with self-supervised features.
    They further adopted mixed-integer programming that guarantees global convergence
    for diverse sample selection. Moreover, Xie et al. [[2023b](#bib.bib208)] considered
    the candidates as continuously optimizable variables based on self-supervised
    features. They randomly initialized the candidate samples at first. Then, they
    maximized the similarity between candidates and their nearest neighbors while
    minimizing the similarity between candidates and labeled samples. Finally, they
    selected the nearest neighbors of the final candidates for annotation. They proved
    the objective is equivalent to minimizing the Wasserstein distance between the
    labeled and unlabeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Density-based Active Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Density-based AL employs density estimation to characterize the data distribution
    in a high-dimensional feature space. The likelihood is the estimated density of
    the data distribution, and a more densely populated area indicates a higher likelihood.
    In this case, representative samples are samples with high likelihood. However,
    such methods can easily cause redundancy in sample selection. As a result, techniques
    like clustering are frequently used to improve diversity in sample selection.
    Density-based AL directly estimates the data distribution, which prevents the
    need to solve complex optimization problems. TypiClust [Hacohen et al., [2022](#bib.bib61)]
    projected samples to a high-dimensional feature space via a self-supervised encoder.
    The density of a sample was defined as the reciprocal of the L2 distances to its
    k-nearest neighbors. Additionally, TypiClust performed clustering beforehand to
    ensure the diversity of selected samples. Wang et al. [[2022c](#bib.bib193)] proposed
    two variants of density-based AL. The first variant fixed the feature representation.
    The process was similar to TypiClust, but they maximized the distances between
    selected samples to ensure diversity. The other variant was in an end-to-end fashion.
    Feature representation and sample selection were trained simultaneously. This
    variant used a learnable k-Means clustering to jointly optimize cluster assignment
    and feature representation with a local smoothness constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'In active domain adaptation, density estimation is also widely used to select
    representative samples in the target domain. Please refer to §[4.3](#S4.SS3 "4.3
    Active Domain Adaptation: Tackling Distribution Shift ‣ 4.2.2 Combination of Active
    Learning and Self-supervised Learning ‣ 4.2 Self-supervised Learning: Utilizing
    Pre-trained Model ‣ 4.1.2 Consistency Regularization ‣ 4.1 Semi-supervised Learning:
    Utilizing Unlabeled Data ‣ 4 Integration of Active Learning and Other Label-Efficient
    Techniques ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis") for related works.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Sampling Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most deep AL works used top-k to select samples with the highest informativeness
    for annotation. However, existing informativeness metrics face several issues,
    such as redundancy and class imbalance in selected samples. Instead of improving
    informativeness, we can introduce simple sampling strategies to resolve these
    issues effectively. Besides, specific sampling strategies can also be used for
    combining multiple informativeness metrics. Furthermore, with the recent development
    of deep AL, more studies directly employ neural networks for sample selection.
    In this context, we no longer evaluate informativeness but directly choose valuable
    samples from the unlabeled pool with neural networks. In summary, sampling strategies
    are crucial in AL, but prior surveys have seldom discussed their specific attributes.
    As one of the contributions of this survey, we systematically summarize different
    sampling strategies in AL, including diversity sampling, class-balanced sampling,
    hybrid sampling, and learnable sampling. The taxonomy of different sampling strategies
    in AL is shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.3 Sampling Strategy ‣ 3 Core Methods
    of Active Learning ‣ A comprehensive survey on deep active learning and its applications
    in medical image analysis").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9d9b65531493cf847699c3e1eea1957.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The taxonomy of different sampling strategies in active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Diversity Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diversity strategies aim to reduce redundancy in selected samples. Sampling
    redundancy is a common issue for uncertainty-based and representativeness-based
    methods, meaning some selected samples are highly similar. The lack of diversity
    leads to the waste of the annotation budget. Besides, redundancy in the training
    set may cause deep models to overfit, thus degrading performance. Therefore, many
    AL methods employ diversity sampling to mitigate the redundancy in selected samples.
    In this section, we discuss four strategies of diversity sampling, including clustering,
    farthest-first traversal, determinantal point process (DPP), and specific strategies
    tailored to certain informativeness metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is one of the most commonly used strategies of diversity sampling.
    It groups the data into several clusters and then queries samples within each
    cluster. This strategy improves the coverage of the entire feature space, thereby
    easily boosting diversity. Ash et al. [[2020](#bib.bib8)] employed k-Means++ clustering
    on gradient embeddings to select diverse uncertain samples. Citovsky et al. [[2021](#bib.bib37)]
    boosted margin-based uncertainty sampling with hierarchical clustering. They selected
    samples with the smallest margins within each cluster. When the number of queries
    exceeded the number of clusters, samples from smaller clusters were prioritized.
    This method can extend to a huge annotation budget (e.g., one million). Jin et al.
    [[2022a](#bib.bib82)] employed BIRCH clustering and chose the samples with maximum
    information density within each cluster for labeling. Compared to k-Means, BIRCH
    clustering is less sensitive to outliers and can further identify noisy samples.
    In connectomics, Lin et al. [[2020](#bib.bib110)] trained two feature extractors
    with labeled and unlabeled samples, respectively. They then selected samples for
    annotation through multiple rounds of clustering. This method achieved excellent
    performance in synapse detection and mitochondria segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Farthest-first Traversal is also a widely used strategy for diverse queries.
    The farthest-first traversal requires the distance between sampling points to
    be as large as possible in the feature space. This leads to a more uniform distribution
    of selected samples within the feature space, thus improving the diversity of
    the sampling results. This technique was first adopted by Sener and Savarese [[2018](#bib.bib160)].
    Agarwal et al. [[2020](#bib.bib2)] and Caramalau et al. [[2021](#bib.bib26)] improved
    the diversity with farthest-first traversal, leveraging their proposed contextual
    diversity and GNN-augmented features, respectively. However, when the annotation
    budget is limited, the farthest-first traversal may be biased toward outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Determinantal point process is a stochastic probability model for selecting
    subsets from a larger set. DPP reduces the probability of sampling similar elements
    to ensure diversity in the results. Bıyık et al. [[2019](#bib.bib22)] employed
    two DPPs for sample selection: Uncertainty DPP is based on uncertainty scores,
    while Exploration DPP aims to find samples near decision boundaries. Then, sampling
    results from both DPPs were sent for expert annotation. However, DPP is more computationally
    intensive compared to clustering. Ash et al. [[2020](#bib.bib8)] compared the
    performance and time cost of using k-Means++ and k-DPP. Results showed that their
    performance is similar, but the time cost for k-Means++ is significantly lower
    than that for k-DPP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specific strategies: There are also specific strategies tailored to certain
    informativeness metrics. In uncertainty-based AL, BatchBALD [Kirsch et al., [2019](#bib.bib95)]
    extended BALD-based uncertainty AL to batch mode. Results showed that BatchBALD
    improved the sampling diversity compared to [Gal et al., [2017](#bib.bib52)].
    FI-based methods formulated AL as a semi-definite programming (SDP) problem to
    improve sampling diversity. Different methods were employed for solving SDP. Sourati
    et al. [[2019](#bib.bib173)] used a commercial solver to solve SDP, while Ash
    et al. [[2021](#bib.bib7)] proposed a greedy algorithm to adapt to high-dimensional
    feature space. Moreover, diversity is an essential part of representativeness-based
    AL. Cover-based AL inherently incorporates the considerations of diversity in
    its formulations. In discrepancy-based AL, Wasserstein distance is used for diverse
    query results [Shui et al., [2020](#bib.bib167), Mahmood et al., [2022](#bib.bib124)].
    Density-based methods often employ strategies like clustering to improve diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Class-balance Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Class imbalance is a common issue for DL, where a small set of classes have
    many samples while the others only contain a few samples [Zhang et al., [2023](#bib.bib223)].
    For example, in tasks such as medical image classification, normal samples often
    outnumber abnormal ones. Training on imbalanced datasets can lead to the overfitting
    of the majority classes and underfitting of the minority classes. Apart from dealing
    with class imbalance during training, AL mitigates class imbalance by avoiding
    over-annotation of the majority classes and enhancing the annotation of the minority
    classes during dataset construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification: Choi et al. [[2021b](#bib.bib36)] directly estimated the probability
    of a classifier making a mistake for a given sample and decomposed it into three
    terms using Bayesian rules. First, they trained a VAE to estimate the likelihood
    of the data given a predicted class. Then, an additional classifier was trained
    upon VAE features to estimate class prior probabilities and the probability of
    mislabeling a specific class. By considering all three probabilities, they successfully
    mitigated class imbalance in AL. The proposed method achieved good performance
    on stepwise class-imbalanced CIFAR-10 and CIFAR-100 datasets. For uncertainty-based
    methods, Bengar et al. [[2022](#bib.bib17)] introduced an optimization framework
    to maintain class balance. They compensated the query of minority classes with
    the most confident samples of that class, leading to a more balanced class distribution
    in the queried dataset. In classification tasks, Munjal et al. [[2022](#bib.bib130)]
    tested various AL baselines on the long-tailed CIFAR-100 dataset. Results showed
    that no single method outperforms others on all budgets in the class-imbalance
    setting. However, as the number of labeled data increases, the performance gap
    between random sampling and the best AL method decreases. Hacohen et al. [[2022](#bib.bib61)]
    conducted experiments in a class-imbalanced setting similar to Munjal et al. [[2022](#bib.bib130)].
    The proposed TypiClust ensured class balance and outperformed other baseline methods.
    Jin et al. [[2022c](#bib.bib84)] assumed that samples closer to the tail of the
    distribution are more likely to belong to the minority classes. Thus, the tail
    probability is equivalent to the likelihood of minority classes. Specifically,
    they trained a VAE for feature extraction and adopted copula to estimate the tail
    probabilities upon VAE features. Finally, informative samples were selected with
    clustering and unequal probability sampling. The proposed method was validated
    on the ISIC 2020 dataset, which has a long-tailed distribution. Kothawade et al.
    [[2022c](#bib.bib101)] used submodular mutual information to focus more on samples
    of minority classes. They achieved excellent results on medical classification
    datasets in five different modalities, including X-rays, pathology, and dermoscopy.
    Besides, in blood cell detection under microscopy, Sadafi et al. [[2019](#bib.bib157)]
    requested expert annotation of a sample whenever its classification probability
    of the minority class exceeded 0.2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Segmentation: Due to some AL methods selecting regions instead of the entire
    image for annotation, there is a need to ensure that the selected regions contain
    rare or small objects (e.g., pedestrians or utility poles in autonomous driving).
    Cai et al. [[2021](#bib.bib25)] and Wu et al. [[2022b](#bib.bib199)] both proposed
    class-balanced sampling strategies for such scenarios, as detailed in §[4.4](#S4.SS4
    "4.4 Region-based Active Learning: Smaller Labeling Unit ‣ 4.3 Active Domain Adaptation:
    Tackling Distribution Shift ‣ 4.2.2 Combination of Active Learning and Self-supervised
    Learning ‣ 4.2 Self-supervised Learning: Utilizing Pre-trained Model ‣ 4.1.2 Consistency
    Regularization ‣ 4.1 Semi-supervised Learning: Utilizing Unlabeled Data ‣ 4 Integration
    of Active Learning and Other Label-Efficient Techniques ‣ A comprehensive survey
    on deep active learning and its applications in medical image analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Hybrid Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In AL, some works may use multiple informativeness metrics simultaneously. Therefore,
    the effective integration of multiple metrics remains a critical issue. This issue
    is addressed by the hybrid sampling strategy discussed in this section. Two approaches
    to hybrid sampling are primarily used, including multi-round sampling and metric
    fusion.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-round sampling first selects a subset of samples based on one particular
    informativeness metric and continues sample selection within this subset based
    on another informativeness metric. For example, SA [Yang et al., [2017](#bib.bib211)]
    performed representativeness sampling based on uncertainty to reduce redundancy
    in the sampled set. Xie et al. [[2022b](#bib.bib204)] first selected samples with
    density-based methods and selected the most uncertain samples within each cluster
    of representative samples. In another study, Xie et al. [[2022c](#bib.bib205)]
    introduced distribution and data uncertainty based on EDL, and then a two-stage
    strategy was used for sample selection. Wu et al. [[2022b](#bib.bib199)] employed
    a more complex strategy, which sets dynamic weights to adjust the budget of representativeness
    and uncertainty sampling. The weight of representativeness sampling is larger
    initially, while the situation is reversed in the latter phase. This is because
    representativeness methods can quickly spot typical data, while uncertainty methods
    continuously improve the model by querying samples with erroneous predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Metric fusion is another widely used approach of hybrid sampling. It directly
    combines different informativeness metrics. For example, one could directly sum
    up all metrics and select the samples with the highest values for annotation.
    Ranked batch-mode [Cardoso et al., [2017](#bib.bib27)] can adaptively fuse multiple
    metrics in AL.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4 Learnable Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Previously mentioned AL methods typically follow a “two-step” paradigm, which
    first involves the evaluation of informativeness and then selects samples based
    on specific heuristics (i.e., sampling strategy). However, learnable sampling
    skips the informativeness evaluation and directly uses neural networks for sample
    selection. In this context, the neural network is known as a “neural selector”.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common methods of learnable sampling is to formulate sample
    selection as a reinforcement learning (RL) problem, where the learner and the
    dataset are considered the environment, and the neural selector serves as the
    agent. The agent interacts with the environment by selecting a limited number
    of samples for annotation, and the environment returns a reward to train the neural
    selector. Haußmann et al. [[2019](#bib.bib62)] adopted a probabilistic policy
    network as the neural selector. The rewards returned by the environment encouraged
    the neural selector to choose diverse and representative samples. The neural selector
    is trained using the REINFORCE algorithm [Williams, [1992](#bib.bib197)]. In pedestrian
    re-identification, Liu et al. [[2019](#bib.bib117)] used the annotation uncertainty
    as the reward for training the neural selector. Agarwal et al. [[2020](#bib.bib2)]
    utilized the proposed contextual diversity as RL rewards and trains a bidirectional
    long short-term memory network as the neural selector. In pose estimation, Gong
    et al. [[2022](#bib.bib56)] adopted multiple agents for sample selection and directly
    used the performance improvement of the pose estimator as the reward for training
    these agents. In medical image classification, Wang et al. [[2020a](#bib.bib187)]
    employed an actor-critic framework where the critic network is used to evaluate
    the quality of the samples selected by the neural selector. This method has performed
    excellently in lung CT disease classification and diabetic retinopathy classification
    of fundus images.
  prefs: []
  type: TYPE_NORMAL
- en: For more works on learnable sampling in AL, such as formulating AL as few-shot
    learning or training neural selectors by meta-learning, please refer to the survey
    of Liu et al. [[2022b](#bib.bib113)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Integration of Active Learning and Other Label-Efficient Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Various methods have been proposed to reduce the large amount of labeled data
    required for training deep models, such as active learning, semi-supervised learning,
    self-supervised learning, etc. These methods are collectively called label-efficient
    deep learning [Jin et al., [2023a](#bib.bib79)]. Label-efficient learning is a
    broad concept that includes all related technologies designed to improve annotation
    efficiency. In §[3](#S3 "3 Core Methods of Active Learning ‣ A comprehensive survey
    on deep active learning and its applications in medical image analysis"), we summarized
    the core methods in AL, including the evaluation of informativeness and sampling
    strategies. However, there is still room for AL to further improve the label efficiency.
    For example, AL has not used unlabeled data for training, has not considered distribution
    shift, and still needs to annotate the whole image in fine-grained tasks such
    as segmentation. Integrating active learning with other label-efficient techniques
    can increase annotation efficiency. While some efforts have been made, existing
    surveys have not yet systematically organized and categorized this line of work.
    Hence, as one of the main contributions of this survey, we comprehensively reviewed
    the integration of AL with other label-efficient techniques, including semi-supervised
    learning, self-supervised learning, domain adaptation, region-based annotation,
    and generative models. Additionally, how each surveyed work integrated with other
    label-efficient techniques is summarized in Table [4](#S4 "4 Integration of Active
    Learning and Other Label-Efficient Techniques ‣ A comprehensive survey on deep
    active learning and its applications in medical image analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Methodology summarization of surveyed active learning works.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Uncertainty | Representativeness | Sampling Strategy |
    SemiSL | SelfSL | ADA | Region | Generative |'
  prefs: []
  type: TYPE_TB
- en: '| 06em. 06em. | Method | Basic Metrics | Method | Basic Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Zhu and Bento [[2017](#bib.bib234)] | 2017 | arXiv | Single Model | Distance
    to Decision Boundary | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2017](#bib.bib232)] | 2017 | CVPR | Single Model Data Disagreement
    | Entropy KL Divergence | - | - | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gal et al. [[2017](#bib.bib52)] | 2017 | ICML | Multiple Inferences - MC
    Dropout | Entropy, BALD, Least Confidence, Variance | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[2017](#bib.bib211)] | 2017 | MICCAI | Model Disagreement |
    Variance | Cover-based | Cosine Similarity | Hybrid - Multi-round |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2017](#bib.bib188)] | 2017 | TCSVT | Single Model | Least Confidence,
    Margin, Entropy | - | - | Top-k | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ducoffe and Precioso [[2018](#bib.bib45)] | 2018 | arXiv | Adversarial Samples
    | Distance to Decision Boundary | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mackowiak et al. [[2018](#bib.bib120)] | 2018 | BMVC | Model Disagreement
    | Vote Entropy | - | - | Top-k |  |  |  | Patch |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[2018](#bib.bib209)] | 2018 | CVPR | Multiple Inferences - Model
    Ensemble | Variance | Cover-based | Cosine Similarity | Hybrid - Multi-round |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Beluch et al. [[2018](#bib.bib16)] | 2018 | CVPR | Multiple Inferences -
    Model Ensemble | Entropy, BALD, Least Confidence, Variance | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sourati et al. [[2018](#bib.bib172)] | 2018 | DLMIA | Gradient-based Metrics
    | Fisher Information | - | - | Diversity - Solve Programming Problem |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sener and Savarese [[2018](#bib.bib160)] | 2018 | ICLR | - | - | Cover-based
    | L2 Distance | Diversity - Farthest-first Traversal |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kuo et al. [[2018](#bib.bib103)] | 2018 | MICCAI | Model Disagreement | JS
    Divergence | - | - | Diversity - Solve Programming Problem |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mahapatra et al. [[2018](#bib.bib121)] | 2018 | MICCAI | Multiple Inferences
    - MC Dropout | Variance | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Haußmann et al. [[2019](#bib.bib62)] | 2019 | IJCAI | - | - | - | - | Learnable
    - Reinforcement Learning |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zheng et al. [[2019](#bib.bib227)] | 2019 | AAAI | - | - | Cover-based |
    Cosine Similarity | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gissin and Shalev-Shwartz [[2019](#bib.bib55)] | 2019 | arXiv | - | - | Discrepancy-based
    | H-Divergence | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yoo and Kweon [[2019](#bib.bib214)] | 2019 | CVPR | Performance Estimation
    - Learnable | Loss | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sinha et al. [[2019](#bib.bib170)] | 2019 | ICCV | - | - | Discrepancy-based
    | H-Divergence | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2019](#bib.bib117)] | 2019 | ICCV | - | - | - | - | Learnable
    - Reinforcement Learning |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Aghdam et al. [[2019](#bib.bib3)] | 2019 | ICCV | Data Disagreement | BALD
    | - | - | Top-K |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Tran et al. [[2019](#bib.bib181)] | 2019 | ICML | Multiple Inferences - MC
    Dropout | BALD | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Qi et al. [[2019](#bib.bib145)] | 2019 | JBHI | Single Model | Entropy |
    - | - | Top-k | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sadafi et al. [[2019](#bib.bib157)] | 2019 | MICCAI | Multiple Inferences
    - MC Dropout | Average IoU, Class Frequency | - | - | Class-balance Hybrid - Fusion
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kirsch et al. [[2019](#bib.bib95)] | 2019 | NeurIPS | Multiple Inferences
    - MC Dropout | BALD | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sourati et al. [[2019](#bib.bib173)] | 2019 | TMI | Gradient-based Metrics
    | Fisher Information | - | - | Diversity - Solve Programming Problem |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kasarla et al. [[2019](#bib.bib88)] | 2019 | WACV | Single Model | Entropy
    | - | - | Top-k |  |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zheng et al. [[2020](#bib.bib228)] | 2020 | AAAI | - | - | Cover-based |
    Cosine Similarity | Diversity - Clustering | Pseudo-label |  |  | Slice |  |'
  prefs: []
  type: TYPE_TB
- en: '| Shui et al. [[2020](#bib.bib167)] | 2020 | AISTATS | Single Model | Entropy,
    Least Confidence | Discrepancy-based | Wasserstein Distance | Hybrid - Fusion
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Siddiqui et al. [[2020](#bib.bib168)] | 2020 | CVPR | Multiple Inferences
    - MC Dropout Data Disagreement | Entropy KL Divergence | - | - | Hybrid - Fusion
    |  |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2020](#bib.bib219)] | 2020 | CVPR | Single Model | Variance
    | Discrepancy-based | H-Divergence | Diversity - Farthest-first Traversal |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[2020](#bib.bib53)] | 2020 | ECCV | Data Disagreement | Variance
    | - | - | Top-k | Consistency |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2020b](#bib.bib190)] | 2020 | ECCV | - | - | Discrepancy-based
    | H-Divergence | Learnable |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Agarwal et al. [[2020](#bib.bib2)] | 2020 | ECCV | - | - | Cover-based |
    Contextual Diversity | Diversity - Farthest-first Traversal Learnable - Reinforcement
    Learning |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[2020](#bib.bib110)] | 2020 | ECCV | - | - | Clustering | L2
    Distance | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ash et al. [[2020](#bib.bib8)] | 2020 | ICLR | Gradient-based Metrics | Gradient
    | - | - | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Casanova et al. [[2020](#bib.bib28)] | 2020 | ICLR | - | - | - | - | Learnable
    - Reinforcement Learning |  |  |  | Patch |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[2020](#bib.bib40)] | 2020 | MICCAI | Gradient-based Metrics
    | Gradient | - | - | Latent Space Optimization & Nearest Neighbour Search |  |  |  |
    Slice |  |  Table 2: Methodology summarization of surveyed active learning works.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Uncertainty | Representativeness | Sampling Strategy |
    SemiSL | SelfSL | ADA | Region | Generative |'
  prefs: []
  type: TYPE_TB
- en: '| 06em. 06em. | Method | Basic Metrics | Method | Basic Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[2020](#bib.bib165)] | 2020 | MICCAI | Multiple Inferences -
    MC Dropout Performance Estimation - Surrogate | Entropy IoU of all result | Cover-based
    | Cosine Similarity | Hybrid - Multi-round |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2020](#bib.bib112)] | 2020 | MICCAI | Performance Estimation
    - Learnable | Loss | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Li and Yin [[2020](#bib.bib108)] | 2020 | MICCAI | Multiple Inferences -
    Model Ensemble | Margin | Discrepancy-based | Cosine Similarity | Hybrid - Multi-round
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2020a](#bib.bib187)] | 2020 | MICCAI | - | - | - | - | Learnable
    - Reinforcement Learning |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hiasa et al. [[2020](#bib.bib66)] | 2020 | TMI | Multiple Inferences - MC
    Dropout | Variance | Cover-based | Cosine Similarity | Hybrid - Multi-round |  |  |  |
    Slice, Pixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[2020](#bib.bib75)] | 2020 | TMI | Model Disagreement | Hausdorff
    Distance | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Su et al. [[2020](#bib.bib174)] | 2020 | WACV | Single Model | Entropy |
    Discrepancy-based | H-Divergence | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Choi et al. [[2021b](#bib.bib36)] | 2021 | CVPR | Probability of Misclassification
    | - | - | Class-balance |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. [[2021](#bib.bib49)] | 2021 | CVPR | Adversarial Training | Disagreement
    of Classifiers, margin | Discrepancy-based | H-Divergence | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hou et al. [[2021](#bib.bib68)] | 2021 | CVPR | - | - | Clustering | L2 Distance
    | Diversity - Clustering |  |  |  | Point |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. [[2021](#bib.bib92)] | 2021 | CVPR | Performance Estimation -
    Learnable | Rank of Loss | Discrepancy-based | H-Divergence | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yuan et al. [[2021](#bib.bib217)] | 2021 | CVPR | Adversarial Training |
    Disagreement of Classifiers | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[2021](#bib.bib25)] | 2021 | CVPR | Single Model | BvSB | - |
    - | Class-balance |  |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Caramalau et al. [[2021](#bib.bib26)] | 2021 | CVPR | Single Model (w/ GNN)
    | Margin | Cover-based | L2 Distance of GCN-augmented Features | Top-k Diversity
    - Farthest-first Traversal |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Prabhu et al. [[2021](#bib.bib144)] | 2021 | ICCV | Single Model | Entropy
    | - | - | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ning et al. [[2021](#bib.bib135)] | 2021 | ICCV | - | - | Discrepancy-based
    | L2 Distance | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[2021](#bib.bib74)] | 2021 | ICCV | Performance Estimation
    - Surrogate | Temporal Output Discrepancy | - | - | Top-k | Consistency |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Du et al. [[2021](#bib.bib44)] | 2021 | ICCV | - | - | Discrepancy-based
    | Semantic and distinctive scores | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Shin et al. [[2021](#bib.bib166)] | 2021 | ICCV | Model Disagreement | Inequality
    | - | - | Diversity - Clustering | Pseudo-label |  |  | Pixel, Point |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2021a](#bib.bib200)] | 2021 | ICCV | Single Model | Entropy |
    Clustering | Color Difference Surface Variation | Diversity - Clustering Hybrid
    - Fusion |  |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Rangwani et al. [[2021](#bib.bib153)] | 2021 | ICCV | Adversarial Samples
    | KL Divergence | Cover-based - Submodular | KL Divergence Bhattacharya Coefficient
    | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Choi et al. [[2021a](#bib.bib35)] | 2021 | ICCV | Uncertainty-aware Models
    - MDN | Variance | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Peng et al. [[2021](#bib.bib141)] | 2021 | ICCV | Model Disagreement | L2
    Distance | Cover-based | Cardinal of Difference Set | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021b](#bib.bib116)] | 2021 | ICCV | Gradient-based Metrics
    | Influence | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[2021](#bib.bib226)] | 2021 | JBHI | Performance Estimation
    - Surrogate | Dice | - | - | Top-k | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2021b](#bib.bib233)] | 2021 | MedIA | Single Model Data Disagreement
    | Entropy KL Divergence | - | - | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2021b](#bib.bib201)] | 2021 | MedIA | Performance Estimation
    - Learnable Data Disagreement | Loss KL Divergence | - | - | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2021a](#bib.bib230)] | 2021 | MICCAI | Performance Estimation
    - Learnable | Dice | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[2021](#bib.bib210)] | 2021 | MICCAI | Single Model | Distance
    to Mean Probability | - | - | Top-k | Consistency |  |  | Patch |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang and Yin [[2021](#bib.bib195)] | 2021 | MICCAI | Multiple Inferences
    - Model Ensemble | Variance | Discrepancy-based | Cosine Similarity | Diveristy
    - Clustering Hybrid - Multi-round | Consistency |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Nguyen et al. [[2021](#bib.bib133)] | 2021 | MIDL | Single Model | Entropy
    | Cover-based | L2 Distance | Diversity - Clustering | Pesudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ash et al. [[2021](#bib.bib7)] | 2021 | NeurIPS | Gradient-based Metrics
    | Fisher Information | - | - | Diversity - Solve Programming Problem |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kothawade et al. [[2021](#bib.bib98)] | 2021 | NeurIPS | - | - | Cover-based
    - Submodular | Gradient | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Citovsky et al. [[2021](#bib.bib37)] | 2021 | NeurIPS | Single Model | Margin
    | - | - | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Nath et al. [[2021](#bib.bib131)] | 2021 | TMI | Multiple Inferences - Model
    Ensemble | Entropy | Discrepancy-based | Mutual Information | Hybrid - Fusion
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mahapatra et al. [[2021](#bib.bib123)] | 2021 | TMI | - | - | Saliency Maps
    | Kurtosis Multivariate Radiomics Features Deep Saliency Features | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2021](#bib.bib30)] | 2021 | TPAMI | Single Model (in Feature
    Space) | Entropy | - | - | Top-k |  |  |  |  |  |  Table 2: Methodology summarization
    of surveyed active learning works.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Uncertainty | Representativeness | Sampling Strategy |
    SemiSL | SelfSL | ADA | Region | Generative |'
  prefs: []
  type: TYPE_TB
- en: '| 06em. 06em. | Method | Basic Metrics | Method | Basic Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Kothawade et al. [[2022b](#bib.bib100)] | 2022 | AAAI | - | - | Cover-based
    - Submodular | Gradient | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[2022b](#bib.bib204)] | 2022 | AAAI | Single Model | Margin |
    Density-based | Energy | Hybrid - Multi-round |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022b](#bib.bib192)] | 2022 | AAAI | Gradient-based Metrics
    | Gradient | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2022a](#bib.bib111)] | 2022 | arXiv | Multiple Inferences -
    Data Augmentations | Variance | - | - | Top-k | Pseudo-label |  |  | Superpixel
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gong et al. [[2022](#bib.bib56)] | 2022 | CVPR | - | - | Discrepancy-based
    | MMD | Learnable - Reinforcement Learning & Meta Learning |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[2022d](#bib.bib206)] | 2022 | CVPR | Single Model | Margin,
    Gradient | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2022a](#bib.bib222)] | 2022 | CVPR | Single Model Adversarial
    Samples | Entropy KL Divergence | Density-based | Mean Cosine Similarity of KNN
    | Hybrid - Equal Split | Consistency |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2022b](#bib.bib224)] | 2022 | CVPR | Single Model | Entropy
    | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Parvaneh et al. [[2022](#bib.bib139)] | 2022 | CVPR | Data Disagreement |
    Inequality | - | - | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[2022a](#bib.bib203)] | 2022 | CVPR | Single Model | Entropy
    | - | - | Top-k |  |  |  | Patch |  |'
  prefs: []
  type: TYPE_TB
- en: '| Quan et al. [[2022](#bib.bib149)] | 2022 | CVPR | - | - | Cover-based | Cosine
    Similarity | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022a](#bib.bib198)] | 2022 | CVPR | Single Model | Entropy |
    Discrepancy-based | Cosine Similarity | Class-balance |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022c](#bib.bib193)] | 2022 | ECCV | - | - | Density-based
    | KNN Density | Diversity - Clustering w/ Regularization | Consistency |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kothawade et al. [[2022a](#bib.bib99)] | 2022 | ECCV | - | - | Cover-based
    - Submodular | Cosine Similarity | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2022b](#bib.bib34)] | 2022 | ECCV | Gradient-based Metrics
    | Gradient | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. [[2022](#bib.bib71)] | 2022 | ECCV | Multiple Inferences - Data
    Augmentations Data Disagreement | Entropy KL Divergence | - | - | Hybrid - Multi-round
    | Pseudo-label |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hwang et al. [[2022](#bib.bib76)] | 2022 | ECCV | Single Model | Margin |
    Discrepancy-based | MMD | Hybrid - Multi-round | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yi et al. [[2022](#bib.bib213)] | 2022 | ECCV | Single Model | Least Confidence
    | Self-supervised Learning | Loss of Pretext Task | Hybrid - Multi-round |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022b](#bib.bib199)] | 2022 | ECCV | Single Model | Entropy |
    Density-based | GMM | Hybrid - Multi-round |  |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mahmood et al. [[2022](#bib.bib124)] | 2022 | ICLR | - | - | Discrepancy-based
    | Wasserstein Distance | Diversity - Solve Programming Problem |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hacohen et al. [[2022](#bib.bib61)] | 2022 | ICML | - | - | Density-based
    | Inverse Average Distance to KNN samples | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2022a](#bib.bib82)] | 2022 | Information Sciences | - | - |
    Clustering | Cosine Similarity | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2022c](#bib.bib84)] | 2022 | KBS | - | - | Clustering | L2 Distance
    | Class-balance |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2022b](#bib.bib83)] | 2022 | KBS | - | - | Clustering | L2 Distance
    | Diversity - Farthest-first Traversal |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[2022](#bib.bib39)] | 2022 | MedIA | Gradient-based Metrics |
    Gradient | - | - | Latent Space Optimization & Nearest Neighbour Search |  |  |  |
    Slice |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2022](#bib.bib231)] | 2022 | MedIA | Performance Estimation
    - Learnable | Dice | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Atzeni et al. [[2022](#bib.bib9)] | 2022 | MedIA | Performance Estimation
    - Surrogate | Dice | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Nath et al. [[2022](#bib.bib132)] | 2022 | MICCAI | Multiple Inferences -
    MC Dropout | Entropy | - | - | Top-k | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Balaram et al. [[2022](#bib.bib14)] | 2022 | MICCAI | Uncertainty-aware Model
    - EDL | Entropy | - | - | Top-k | Pseudo-label & Consistency |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022c](#bib.bib202)] | 2022 | MICCAI | - | - | Cover-based |
    Cosine Similarity | Diversity - Clustering |  |  |  | Slice |  |'
  prefs: []
  type: TYPE_TB
- en: '| Bai et al. [[2022](#bib.bib10)] | 2022 | MICCAI | Model Disagreement | Entropy-weighted
    Dice Distance | - | - | Hybrid - Fusion | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kothawade et al. [[2022c](#bib.bib101)] | 2022 | MICCAIW | - | - | Cover-based
    - Submodular | Gradient | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yehuda et al. [[2022](#bib.bib212)] | 2022 | NeurIPS | - | - | Cover-based
    | L2 Distance | Graph-based Algorithm |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mahapatra et al. [[2022](#bib.bib122)] | 2022 | TMI | - | - | Saliency Maps
    | Graph-based Methods | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[2022](#bib.bib109)] | 2022 | TMI | Curriculum Learning & Noisy
    Sample Detection | - | - | Top-k | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Bengar et al. [[2022](#bib.bib17)] | 2022 | WACV | Single Model | Entropy
    | - | - | Class-balance |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[2023b](#bib.bib208)] | 2023 | CVPR | - | - | Discrepancy-based
    | Wasserstein Distance | Latent Space Optimization & Nearest Neighbour Search
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Lyu et al. [[2023](#bib.bib119)] | 2023 | CVPR | Data Disagreement | Cross
    Entropy, Variance | - | - | Hybrid - Fusion | Pseudo-label |  |  | Box |  |  Table
    2: Methodology summarization of surveyed active learning works.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Uncertainty | Representativeness | Sampling Strategy |
    SemiSL | SelfSL | ADA | Region | Generative |'
  prefs: []
  type: TYPE_TB
- en: '| 06em. 06em. | Method | Basic Metrics | Method | Basic Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Yuan et al. [[2023](#bib.bib215)] | 2023 | CVPR | Single Model | Entropy
    | Discrepancy-based | H-Divergence | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[2023](#bib.bib72)] | 2023 | CVPR | Single Model | IoU Confidence
    | - | - | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Jung et al. [[2023](#bib.bib86)] | 2023 | ICLR | Multiple Inferences - Model
    Ensemble | Entropy, Variance Ratio, BALD, Margin | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[2022c](#bib.bib205)] | 2023 | ICLR | Uncertainty-aware Model
    - EDL | Mutual Information & Entropy Expectation of Dirichlet Distribution | -
    | - | Hybrid - Multi-round |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. [[2023](#bib.bib91)] | 2023 | ICCV | Single Model | BvSB | - |
    - | Class-balance |  |  |  | Superpixel |  |'
  prefs: []
  type: TYPE_TB
- en: '| Park et al. [[2023](#bib.bib138)] | 2023 | ICLR | Uncertainty-aware Model
    - EDL | Mutual Information | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sun et al. [[2023](#bib.bib175)] | 2023 | ICLR | Uncertainty-aware Model
    - EDL | Evidential Uncertainty | Density-based | Inverse Average Distance | Hybrid
    Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sadafi et al. [[2023](#bib.bib158)] | 2023 | ISBI | Multiple Inferences -
    MC Dropout Model Disagreement | Variance Inequality | - | - | Hybrid - Fusion
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2023](#bib.bib31)] | 2023 | MIDL | - | - | Loss of Self-supervised
    Pretext Tasks | Diversity - Clustering |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Lou et al. [[2023](#bib.bib118)] | 2023 | TMI | - | - | Clustering | Consistency
    | Diversity - Clustering | Pseudo-label |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Du et al. [[2022](#bib.bib43)] | 2023 | TPAMI | - | - | Discrepancy-based
    | Semantic and distinctive scores | Hybrid - Fusion |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wan et al. [[2023](#bib.bib185)] | 2023 | TPAMI | Adversarial Training |
    Disagreement of Classifiers | - | - | Top-k |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '4.1 Semi-supervised Learning: Utilizing Unlabeled Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semi-supervised learning [Chen et al., [2022a](#bib.bib33)] aims to boost performance
    by utilizing unlabeled data upon supervised training. AL and semi-supervised learning
    complements each other: AL focuses on constructing an optimal labeled dataset.
    However, massive unlabeled samples are discarded during model training. Therefore,
    we can further leverage unlabeled data to train the deep model. By integrating
    the strengths of both AL and semi-supervised learning, annotation efficiency can
    be further improved. This section will introduce the integration of AL and semi-supervised
    learning from pseudo-labeling and consistency regularization.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Pseudo-Labeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pseudo-labeling [Lee et al., [2013](#bib.bib105)] is one of the most straightforward
    methods in semi-supervised learning. It uses the model’s predictions of unlabeled
    data as pseudo-labels and combines them with labeled data for supervised training.
    Although it’s possible to assign pseudo-labels to all unlabeled samples for training,
    it may introduce noise. To mitigate this, Wang et al. [[2017](#bib.bib188)] proposed
    cost-effective active learning (CEAL), integrating pseudo-labeling with uncertainty-based
    AL. Specifically, CEAL sent the most uncertain samples for expert annotation and
    assigned pseudo-labels to the most confident samples. Many subsequent works have
    built upon the ideas of CEAL. In point cloud segmentation, both Hu et al. [[2022](#bib.bib71)]
    and Liu et al. [[2022a](#bib.bib111)] assigned pseudo-labels of the most certain
    regions. In medical image segmentation, Zhao et al. [[2021](#bib.bib226)] refined
    the pseudo-labels with dense conditional random fields. Additionally, Li et al.
    [[2022](#bib.bib109)] proposed a new approach for selecting samples for oracle
    annotation and pseudo-label. Specifically, they employed curriculum learning to
    categorize all samples into hard and easy. Hard samples were all sent for oracle
    annotation. For the easy samples, they evaluated the presence of label noise based
    on the training loss. Easy samples with low training loss were used for pseudo-labels
    to assist training, whereas easy samples with high loss were considered noisy
    and excluded from training.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Consistency Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consistency regularization is also widely applied in semi-supervised learning.
    Its basic idea is to enforce consistent outputs under perturbations of input data
    or model parameters. Maximizing consistency serves as an unsupervised loss for
    unlabeled samples. Consistency regularization helps improve the robustness and
    reduce overfitting of the model, thus enhancing model performance. Gao et al.
    [[2020](#bib.bib53)] introduced a semi-supervised active learning framework. Consistency
    here was used for both semi-supervised training and evaluating informativeness.
    In this framework, samples are fed into the model multiple times with random augmentations.
    The consistency loss of unlabeled samples was implemented by minimizing the variance
    between multiple outputs. They further selected less consistent samples for annotation.
    Results showed that combining AL with semi-supervised learning significantly improves
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, some works integrated existing consistency-based semi-supervised
    methods into the training process of AL. Huang et al. [[2021](#bib.bib74)] combined
    their proposed COD with MeanTeacher [Tarvainen and Valpola, [2017](#bib.bib179)],
    demonstrating superior performance. Both TypiClust [Hacohen et al., [2022](#bib.bib61)]
    and ProbCover Yehuda et al. [[2022](#bib.bib212)] found that their methods outperformed
    other active learning baselines in low-budget scenarios when combined with FlexMatch
    [Zhang et al., [2021](#bib.bib220)]. Wang et al. [[2022c](#bib.bib193)] combined
    density-based AL with different existing semi-supervised methods. Results showed
    that the proposed method outperforms other active learning methods and excels
    in semi-supervised learning. Zhang et al. [[2022a](#bib.bib222)] combined AL with
    both pseudo-labeling and consistency. The unlabelled images first underwent both
    strong and weak data augmentations. When the confidence level of the weakly augmented
    images exceeded a certain threshold, they used these samples for semi-supervised
    training. Specifically, predictions of the weakly augmented images were assigned
    as pseudo-labels, and the outputs of the strongly augmented images were forced
    to be consistent with the pseudo-labels. However, when the confidence level was
    lower than the threshold, they used these samples for AL. A balanced uncertainty
    selector and an adversarial instability selector were used to select samples for
    oracle annotation. They validated the effectiveness of their proposed method in
    grading metastatic epidural spinal cord compression with MRI images.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Self-supervised Learning: Utilizing Pre-trained Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last section, we discussed the integration of AL and semi-supervised
    learning, which aimed to utilize unlabeled data for better performance. However,
    the effectiveness of this approach is constrained by the size of dataset. This
    limitation is particularly evident in medical image analysis, where datasets are
    often relatively small. To further improve annotation efficiency, AL can be combined
    with self-supervised learning. Self-supervised learning [Liu et al., [2021a](#bib.bib115)]
    trained the model with the supervision from the data itself, thus allowing pre-training
    on a large dataset. After finetuning on a few randomly selected labeled samples,
    the self-supervised pre-trained models have been shown to achieve impressive performance
    [Chen et al., [2020](#bib.bib32)]. Besides, these models can also provide good
    initialization, thereby solving the cold-start problem in AL. In this section,
    we will first introduce how self-supervised models solve the cold-start problem
    in AL and then explore different ways of integrating active learning with self-supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Cold-start Problem in Active Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Current AL methods usually require several initial labeled samples to train
    an initial model and ensure reliable informativeness metrics. However, when the
    initial labeled set is small or even absent, the performance of these AL methods
    drops dramatically, sometimes even worse than random sampling [Chen et al., [2023](#bib.bib31),
    Hacohen et al., [2022](#bib.bib61), Yehuda et al., [2022](#bib.bib212)]. This
    is known as the cold-start problem in AL, which is very common in AL. In their
    semi-supervised active learning framework, Gao et al. [[2020](#bib.bib53)] found
    that performance suffered with a smaller labeling budget compared to a larger
    one when initial labels were randomly selected. Bengar et al. [[2021](#bib.bib18)]
    first pre-trained the model with self-supervised learning and then employed some
    AL baselines to select samples for labeling and finetuning. Results showed the
    performance of AL baselines tends to be worse than the random selection under
    the low-budget scenario. Additionally, Xie et al. [[2023a](#bib.bib207)] discovered
    that CoreSet based on self-supervised features is inferior to random sampling.
    Tackling the cold-start problem is vital for improving the efficacy of AL, especially
    when the annotation budget is limited. Moreover, when constructing a new dataset
    from scratch, employing cold-start AL strategies can offer a good initialization,
    thereby boosting performance.
  prefs: []
  type: TYPE_NORMAL
- en: A key solution to the cold-start problem in AL lies in selecting the optimal
    set of initial labeled samples. Since no initial labels are available, cold-start
    AL requires different strategies than existing AL methods. Self-supervised pre-trained
    models offer a good initialization for effectively tackling the cold-start problem
    in AL. In natural language processing, Yuan et al. [[2020](#bib.bib216)] was the
    first to introduce the cold-start problem in AL. They employed self-supervised
    pre-trained models to address this issue. Yi et al. [[2022](#bib.bib213)] chose
    initial samples based on the loss of self-supervised pretext tasks, showing significant
    advantages over random sampling. Pourahmadi et al. [[2021](#bib.bib143)] proposed
    a straightforward baseline for cold-start active learning. They first performed
    k-Means clustering on existing off-the-shelf self-supervised features, then selected
    cluster centers for annotation. Results indicated that this baseline is very effective
    when the annotation budget is limited. TypiClust [Hacohen et al., [2022](#bib.bib61)]
    found that when the annotation budget is low, querying typical samples is more
    beneficial, whereas when the budget is high, querying hard samples is more beneficial.
    This conclusion suggested different strategies with uncertainty methods for cold-start
    AL. Thus, based on self-supervised features, TypiClust selected samples from high-density
    areas of each k-Means cluster. Yehuda et al. [[2022](#bib.bib212)] employed a
    graph-based greedy algorithm to select the optimal initial samples based on self-supervised
    features. Chen et al. [[2023](#bib.bib31)] found that active learning also suffers
    from a cold-start problem in medical image analysis. The issues arose mainly because
    AL is often biased towards specific classes, resulting in class imbalance. Additionally,
    the models struggled to detect anomalies when only a limited number of initially
    labeled samples exist. They combined clustering and the loss of contrastive learning
    to address the cold-start problem. In CT segmentation, Nath et al. [[2022](#bib.bib132)]
    designed new pretext tasks for self-supervised pre-training. The model was trained
    to learn the threshold segmentation by an abdominal soft-tissue window. Results
    indicated that the proposed method significantly outperforms random sampling in
    selecting initial samples.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, some works have attempted to use fully supervised pre-trained
    models to address the cold-start problem. Zhou et al. [[2017](#bib.bib232)] and
    their subsequent work [Zhou et al., [2021b](#bib.bib233)] used ImageNet pre-trained
    models to select samples for labeling from completely unlabeled datasets. They
    combined entropy and disagreement as informativeness metrics, where the disagreement
    was the KL divergence of prediction probabilities between different patches of
    the same sample. They also introduced randomness to balance exploration and exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Combination of Active Learning and Self-supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Features: The simplest way to integrate AL with self-supervised learning is
    by leveraging the high-quality pre-trained features that effectively capture data
    similarities. In point cloud segmentation, Hou et al. [[2021](#bib.bib68)] performed
    k-Means clustering on the self-supervised features, then selected the points of
    cluster centers for annotation. They improved the annotation efficiency in indoor
    scene point cloud segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretext tasks: In addition, the pretext tasks in self-supervised learning can
    be used for AL. They are tasks for which the supervision comes directly from the
    data itself. Different pretext tasks correspond to different pre-training paradigms.
    Typical pretext tasks include rotation prediction [Gidaris et al., [2018](#bib.bib54)],
    colorization [Zhang et al., [2016](#bib.bib221)], jigsaw puzzles [Noroozi and
    Favaro, [2016](#bib.bib136)], contrastive learning [He et al., [2020](#bib.bib64)],
    and masked modeling [He et al., [2022](#bib.bib63)], etc. Solving these pretext
    tasks on extensive unlabeled data, the model acquires useful feature representations
    that can indirectly reflect data characteristics. Related works generally employed
    the loss of pretext task for AL. Yi et al. [[2022](#bib.bib213)] found a strong
    correlation between the loss of pretext tasks and the loss of downstream tasks.
    Thus, they initially focused on annotating samples with higher loss of pretext
    tasks and later shifted to those with lower loss. Results showed that rotation
    prediction performed the best among different pretext tasks. In Chen et al. [[2023](#bib.bib31)],
    the loss of contrastive learning was used for AL. They assumed that samples with
    higher losses are more representative of the data distribution. Specifically,
    they pre-trained on the target dataset using MoCo [He et al., [2020](#bib.bib64)]
    for contrastive learning and then used k-Means clustering to partition the unlabeled
    data into multiple clusters, selecting the samples with the highest contrastive
    loss within each cluster for annotation. They then selected samples with the highest
    contrastive loss in each cluster for annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Others: Furthermore, we can also leverage self-supervised learning in other
    ways for AL. In classification tasks, Zhang et al. [[2022b](#bib.bib224)] introduced
    one-bit annotation into AL for classification tasks. Firstly, they selected informative
    samples through uncertainty metrics. Oracles returned whether the current prediction
    was right or wrong rather than full annotation. Then, contrastive learning was
    adopted to pull the correct predictions closer to their corresponding classes
    and push away wrongly predicted samples from the predicted classes. Results indicated
    that the proposed method outperforms other AL methods regarding bit information.
    Du et al. [[2021](#bib.bib44)] integrated contrastive learning into AL to tackle
    the problem of class distribution mismatch, where unlabeled data often includes
    samples out of the class distribution of the labeled dataset. In this work, contrastive
    learning filtered samples of mismatched classes that differ from the current class
    distribution. Besides, contrastive learning highlighted samples’ informativeness
    by setting carefully designed negative samples. Their extended work Du et al.
    [[2022](#bib.bib43)] provided more theoretical analysis and experimental results
    and further integrated existing label information into the contrastive learning
    framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Active Domain Adaptation: Tackling Distribution Shift'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Domain Adaptation (DA) [Guan and Liu, [2021](#bib.bib59)] has wide applications
    in medical image analysis and computer vision. It aims to transfer knowledge from
    the source to the target domain, thus minimizing annotation costs. Currently,
    the most common setting of DA is unsupervised domain adaptation (UDA), in which
    the source domain is labeled while the target domain is unlabeled. However, the
    performance of UDA still lags behind fully supervised learning in the target domain
    [Liu et al., [2023](#bib.bib114)]. To bridge this gap, a natural idea would be
    to employ AL to select and annotate informative samples in the target domain.
    This setting is known as active domain adaptation (ADA). For better queries in
    ADA, one should consider uncertainty and whether the sample represents the target
    domain. The latter is commonly referred to as domainness or targetness in ADA.
    This section reviews the development of ADA and explores various ways of integrating
    AL with DA.
  prefs: []
  type: TYPE_NORMAL
- en: Su et al. [[2020](#bib.bib174)] was the first to introduce the concept of ADA
    and combined domain adversarial learning with AL. Through a domain discriminator
    and task model, they performed importance sampling to select target domain samples
    that are uncertain and highly different from the source domain. Fu et al. [[2021](#bib.bib49)]
    combined query-by-committee, uncertainty, and domainness for selecting the most
    informative samples under distribution shift. They adopted a domain discriminator
    to select samples with high domainness and employed Gaussian kernels to filter
    out anomalous and source-similar samples of the target domain. Random sampling
    was also used to improve diversity. Prabhu et al. [[2021](#bib.bib144)] performed
    k-Means clustering on target domain samples and selected cluster centers for annotation.
    The cluster centers were weighted by uncertainty, thus ensuring that selected
    samples were uncertain and diverse. Rangwani et al. [[2021](#bib.bib153)] formulated
    ADA as a submodular optimization problem. The sum of uncertainty, diversity, and
    representativeness was considered the gain for annotating a sample. Specifically,
    uncertainty was measured by the KL divergence between the original samples and
    their adversarial samples. Diversity was defined as the minimum KL divergence
    from a single sample to a set of samples. The Bhattacharya coefficient between
    samples was used as the representativeness score. They adopted a greedy algorithm
    to iteratively pick samples with the maximum gain. In segmentation tasks, Ning
    et al. [[2021](#bib.bib135)] introduced the idea of anchors in ADA. They concatenated
    features of different classes from the source domain images. Cluster centers of
    these concatenations were referred to as anchors. They then computed the distance
    between each target sample and its nearest anchor. Target samples with the highest
    distance were requested for annotation. Shin et al. [[2021](#bib.bib166)] proposed
    LabOR, which first used a UDA pre-trained model to generate pseudo-labels for
    target samples and trained two segmentation heads with these pseudo-labels. They
    maximized the disagreements between the two heads and annotated regions that exhibited
    the most disagreement. LabOR achieved performance close to full supervision with
    only 2.2% of target domain annotations. Hwang et al. [[2022](#bib.bib76)] first
    selected representative samples in the target domain with maximum mean discrepancy.
    Uncertain ones within these samples were sent for annotation, while the confident
    ones are used for pseudo-labels. Xie et al. [[2022b](#bib.bib204)] introduced
    the concept of energy [LeCun et al., [2006](#bib.bib104)] into ADA. The energy
    is inversely proportional to the likelihood of the data distribution. In this
    work, the model trained on the source domain was used to calculate the energy
    of target domain samples. Samples with high energy were selected for annotation,
    which suggested they are representative of the target domain and substantially
    different from the source data. Xie et al. [[2022d](#bib.bib206)] spotted the
    hard source samples by maximizing margin loss and leveraged these samples to select
    target samples close to the decision boundary. Based on EDL, Xie et al. [[2022c](#bib.bib205)]
    incorporated Dirichlet distribution to mitigate model miscalibration on the target
    domain. Distribution and data uncertainty were both used for sample selection.
    Huang et al. [[2023](#bib.bib72)] selected samples with high uncertainty and prediction
    inconsistency to their nearest prototypes. In 3D object detection, Yuan et al.
    [[2023](#bib.bib215)] adopted a diversity-based strategy to select target domain
    samples. Specifically, they first clustered samples based on similarity and selected
    prototypes of each cluster for annotation.
  prefs: []
  type: TYPE_NORMAL
- en: '4.4 Region-based Active Learning: Smaller Labeling Unit'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most AL works require the oracle to label the full image in medical image analysis
    and computer vision. However, labeling a full image can introduce redundancy in
    fine-grained tasks like segmentation and detection, resulting in an inefficient
    use of the annotation budget. For example, in segmentation tasks of autonomous
    driving, large areas in the image (e.g., roads) do not need exhaustive annotation.
    Instead, those annotation budgets would be better spent on detailed smaller areas,
    such as pedestrians or utility poles. An image can be divided into non-overlap
    regions to further improve annotation efficiency, and experts can opt to annotate
    specific regions within an image. This method is termed as “region-based active
    learning”. This section introduces region-based active learning from three perspectives:
    patches, superpixels, and region-based active domain adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Patches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Patches are most commonly used in region-based active learning, generally represented
    as square boxes. Mackowiak et al. [[2018](#bib.bib120)] combined uncertainty and
    annotation cost to select the informative patches for annotation. Casanova et al.
    [[2020](#bib.bib28)] employed deep reinforcement learning to automatically select
    informative patches for annotation. In retinal blood vessels segmentation of eye
    images, Xu et al. [[2021](#bib.bib210)] selected patches with the highest uncertainty
    for annotation. Furthermore, they utilized latent-space Mixup to encourage linearization
    between labeled and unlabeled samples, thus leveraging unlabeled data to improve
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Superpixels
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Superpixels are also widely used in region-based active learning. Superpixel
    generation algorithms [Achanta et al., [2012](#bib.bib1), Van den Bergh et al.,
    [2012](#bib.bib19)] over-segment images based on color and texture, grouping similar
    pixels into the same superpixel. Superpixel-based AL initially pre-segments the
    images and then calculates the informativeness of each superpixel. The informativeness
    metric of each superpixel is the average of its constituent pixels. Kasarla et al.
    [[2019](#bib.bib88)] introduced a baseline method for selecting superpixels based
    on uncertainty. In multi-view indoor scene segmentation, Siddiqui et al. [[2020](#bib.bib168)]
    adopted uncertainty and disagreement between different viewpoints to select informative
    superpixels for annotation. Cai et al. [[2021](#bib.bib25)] utilized uncertainty
    as the informativeness metric. They introduced a class-balanced sampling strategy
    to better select superpixels containing minority classes. Furthermore, they adopted
    a “dominant labeling” scheme. The dominant labeling is the majority class label
    of all pixels in the superpixel. They assigned the dominant labeling to every
    pixel within a superpixel, thus eliminating the need for detailed delineation.
    Results showed that, with the same number of labeling clicks, dominant labeling
    at the superpixel level significantly outperforms precise labeling at the patch
    level. As a follow-up, Kim et al. [[2023](#bib.bib91)] proposed to adaptively
    merge and split spatially adjacent, similar, and complex superpixels, respectively.
    This approach yielded better performance than [Cai et al., [2021](#bib.bib25)]
    with dominant labeling. In 3D vision, similar over-segmentation has also been
    applied to point clouds segmentation by Wu et al. [[2021a](#bib.bib200)], Hu et al.
    [[2022](#bib.bib71)] and Liu et al. [[2022a](#bib.bib111)]. The former two works
    adopted supervoxel segmentation algorithms, while the latter employed k-Means
    for over-segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Region-based Active Domain Adaptation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better utilize the annotation budget, some ADA segmentation works also employed
    patches or superpixels for annotation. In Xie et al. [[2022a](#bib.bib203)], uncertainty
    and regional impurity were used to select and annotate the most informative patches.
    Regional impurity measured the number of unique predicted classes within the neighborhood
    of a pixel, which presents the edge information. They used extremely small patches
    (e.g., size of 3x3) for annotation and achieved performance close to full supervision
    with only 5% of the annotation cost. Wu et al. [[2022b](#bib.bib199)] proposed
    a density-based method to select the most representative superpixels in the target
    domain for annotation. They employed Gaussian mixture models (GMM) as density
    estimators for superpixels in both the source and target domains, aiming to select
    those with high density in the target domain and low density in the source domain.
  prefs: []
  type: TYPE_NORMAL
- en: '4.5 Generative Model: Data Augmentation and Generative Active Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In recent years, the advancement of deep generative models enabled high-quality
    generation and flexible conditional generation. For example, a trained model could
    generate the corresponding lung X-ray scan when conditioned on a lung mask. By
    integrating generative models, we can further improve the annotation efficiency
    of AL. In this section, we discuss how AL can be combined with generative models
    from two aspects: data augmentation and generative active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Synthetic Samples as Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The simplest approach considers the synthetic sample produced by generative
    models as advanced data augmentation. These methods utilize label-conditioned
    generative models. As a result, it’s guaranteed that all synthetic samples are
    correctly labeled since specifying the labels is a prerequisite for data generation.
    This method enables us to acquire more labeled samples without any additional
    annotations. Tran et al. [[2019](#bib.bib181)] argued that most synthetic samples
    produced by generative models are not highly informative. Therefore, they first
    adopted the BALD uncertainty to select samples for annotation, then trained a
    VAE-ACGAN on these labeled data to generate more informative synthetic samples.
    Mahapatra et al. [[2018](#bib.bib121)] used conditional GANs to generate chest
    X-rays with varying diseases to augment the labeled dataset. Then, MC Dropout
    was used to select and annotate highly uncertain samples. With the help of AL
    and synthetic samples, they achieved performance near fully supervised using only
    35% of the data. Training conditional generative models require a large amount
    of labeled data, while the labeled dataset in AL is often relatively small. To
    address this issue, Lou et al. [[2023](#bib.bib118)] proposed a conditional SinGAN
    [Shaham et al., [2019](#bib.bib164)] that only requires one pair of images and
    masks for training. The SinGAN improved the annotation efficiency for nuclei segmentation.
    Additionally, Chen et al. [[2022b](#bib.bib34)] integrated implicit semantic data
    augmentation (ISDA) [Wang et al., [2021](#bib.bib194)] into AL. They initially
    used ISDA to augment unlabeled samples, then selected samples with large diversity
    between different data augmentations for annotation. The model is trained on both
    the original data and its augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Generative Active Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generative active learning selects synthetic samples produced by generative
    models for oracle annotation, thus without requiring a large unlabeled sample
    pool. The advantage of this approach lies in its ability to continuously search
    the data manifold through generative models. It’s worth noting that works in this
    section follow the setting of membership query synthesis, while works in the last
    section follow the setting of pool-based active learning. This distinction arises
    because generative models in the last section were solely utilized to augment
    existing labeled datasets. Zhu and Bento [[2017](#bib.bib234)] attempted to generate
    uncertain samples with GAN for expert annotation. Unfortunately, the quality of
    the generated samples was low and included many samples with indistinguishable
    classes. Since experts find it difficult to annotate low-quality synthetic samples,
    alternative methods are needed to annotate these samples. Chen et al. [[2021](#bib.bib30)]
    first trained a bidirectional GAN to learn the data manifold. They then selected
    uncertain areas in the feature space and generated images within these regions
    using bidirectional GAN. Finally, they used physics-based simulation to provide
    labels for the generated samples. In calcification level prediction in aortic
    stenosis of CT, they improved annotation efficiency by up to 10 times compared
    to random generation.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Active Learning for Medical Image Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to the potential of significantly reducing annotation costs, AL is receiving
    increasing attention in medical image analysis. The unique traits of medical imaging
    require us to design specialized AL methods. Building on the foundation of the
    previous two sections, this section will focus on introducing AL works tailored
    to medical image analysis across different tasks, including classification, segmentation,
    and reconstruction. Additionally, in Table [3](#S5.T3b "Table 3 ‣ 5 Active Learning
    for Medical Image Analysis ‣ 4.5.2 Generative Active Learning ‣ 4.5 Generative
    Model: Data Augmentation and Generative Active Learning ‣ 4.4.3 Region-based Active
    Domain Adaptation ‣ 4.4 Region-based Active Learning: Smaller Labeling Unit ‣
    4.3 Active Domain Adaptation: Tackling Distribution Shift ‣ 4.2.2 Combination
    of Active Learning and Self-supervised Learning ‣ 4.2 Self-supervised Learning:
    Utilizing Pre-trained Model ‣ 4.1.2 Consistency Regularization ‣ 4.1 Semi-supervised
    Learning: Utilizing Unlabeled Data ‣ 4 Integration of Active Learning and Other
    Label-Efficient Techniques ‣ A comprehensive survey on deep active learning and
    its applications in medical image analysis"), we list all the AL works related
    to medical image analysis in this survey, providing the name of the used dataset,
    its modality, ROIs, and corresponding clinical and technical tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Surveyed Works of Active Learning related to Medical Image Analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Modality | ROIs | Dataset | Clinical Task | Technical
    Task |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2017](#bib.bib232)] | 2017 | CVPR | Colonoscopy | Colon | in-house
    | Image Quality Assessment | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Colonoscopy | Colon | in-house | Polyp Detection | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Lung | in-house | Pulmonary Embolism Detection | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Gal et al. [[2017](#bib.bib52)] | 2017 | ICML | Dermscopy | Skin | ISIC 2016
    | Skin Cancer Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[2017](#bib.bib211)] | 2017 | MICCAI | Histopathology | Colon
    | Glas | Gland Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Ultrasound | Lymoh Node | in-house | Lymoh Node Segmentation | Segmentation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Beluch et al. [[2018](#bib.bib16)] | 2018 | CVPR | Fundus | Eye | EyePacs
    | Diabetic Retinopathy Detection | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[2018](#bib.bib209)] | 2018 | CVPR | Histopathology | Colon |
    Glas | Gland Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Sourati et al. [[2018](#bib.bib172)] | 2018 | DLMIA | MRI | Brain | dHCP
    & in-house | Brain Extraction | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Sourati et al. [[2019](#bib.bib173)] | 2019 | TMI |'
  prefs: []
  type: TYPE_TB
- en: '| Kuo et al. [[2018](#bib.bib103)] | 2018 | MICCAI | CT | Head | in-house |
    Intracranial Hemorrhage Detection | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Mahapatra et al. [[2018](#bib.bib121)] | 2018 | MICCAI | X-ray | Chest |
    SCR & Chestx-ray8 | Lung Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Thoracic Disease Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2019](#bib.bib80)] | 2019 | arXiv | MRI | Heart | Cardiac Atlas
    Project | MRI Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Knee | fastMRI | MRI Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Zheng et al. [[2019](#bib.bib227)] | 2019 | AAAI | Histopathology | Colon
    | Glas | Gland Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| MRI | Heart | HVSMR 2016 | Whole-heart Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Electron Microscopy | Fungus | in-house | Fungus Segmentation | Segmentation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2019](#bib.bib225)] | 2019 | CVPR | MRI | Knee | fastMRI |
    MRI Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Qi et al. [[2019](#bib.bib145)] | 2019 | JBHI | Histopathology | Breast |
    BreaKHis | Breast Cancer Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Sadafi et al. [[2019](#bib.bib157)] | 2019 | MICCAI | Mircoscopy | Blood
    | in-house | Red Blood Cell Detection | Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '| Zheng et al. [[2020](#bib.bib228)] | 2020 | AAAI | MRI | Heart | HVSMR 2016
    | Whole-heart Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Electron Microscopy | Mouse | Lee et al. [[2015](#bib.bib106)] | Neuron Boundary
    Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[2020](#bib.bib110)] | 2020 | ECCV | Electron Microscopy | Mouse
    Synapses & Mitochondria | EM-R50 | Synapse Detection & Mitochondria Segmentation
    | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[2020](#bib.bib40)] | 2020 | MICCAI | MRI | Brain | BraTS 2019
    | Brain Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Li and Yin [[2020](#bib.bib108)] | 2020 | MICCAI | Histopathology | Colon
    | Glas | Gland Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| MRI | Brain | iSeg | Infant Brain Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2020](#bib.bib112)] | 2020 | MICCA | CT | Lung | DeepLesion
    | Pulmonary Nodule Detection | Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '| Mi et al. [[2020](#bib.bib127)] | 2020 | MICCAI | Electron Microscopy | Mouse
    Cortex | SNEMI3D | Accelerated Acquisition of Electron Microscopy | Reconstruction
    |'
  prefs: []
  type: TYPE_TB
- en: '| Human Cerebrum | in-house | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Pineda et al. [[2020](#bib.bib142)] | 2020 | MICCAI | MRI | Knee | fastMRI
    | MRI Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[2020](#bib.bib165)] | 2020 | MICCAI | Immunohistochemistry
    | Breast | in-house | Breast Cancer Region Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2020a](#bib.bib187)] | 2020 | MICCAI | CT | Lung | [Tianchi](https://tianchi.aliyun.com/competition/entrance/231724/introduction)
    | Lung Diease Detection | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Fundus | Eye | EyePacs | Diabetic Retinopathy Detection | Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bakker et al. [[2020](#bib.bib13)] | 2020 | NeurIPS | MRI | Knee | fastMRI
    | MRI Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Brain | fastMRI | MRI Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Hiasa et al. [[2020](#bib.bib66)] | 2020 | TMI | CT | Hip & Thigh | TCIA
    & in-house | Muscle Segmentation | Segmentation |  Table 3: Methodology summarization
    of surveyed active learning works.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Modality | ROIs | Dataset | Clinical Task | Technical
    Task |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[2020](#bib.bib75)] | 2020 | TMI | X-ray | Chest | in-house
    | Rib Fracture Recognition | Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[2021](#bib.bib226)] | 2021 | JBHI | Dermscopy | Skin | ISIC
    2017 | Skin Lesion Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| X-ray | Hand | RSNA Bone Age Dataset | Finger Bone Segmentation | Segmentation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2021b](#bib.bib201)] | 2021 | MedIA | CT | Lung | CC-CCII | COVID-19
    Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2021b](#bib.bib233)] | 2021 | MedIA | Colonoscopy | Colon |
    in-house | Image Quality Assessment | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Colonoscopy | Colon | in-house | Polyp Detection | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Lung | in-house | Pulmonary Embolism Detection | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Wang and Yin [[2021](#bib.bib195)] | 2021 | MICCAI | Mircoscopy (Synthetic)
    | Bacterial Cells | VGG Cell | Cell Counting | Keypoint Localization |'
  prefs: []
  type: TYPE_TB
- en: '| Histopathology | Human Bone Marrow | MBM | Cell Counting | Keypoint Localization
    |'
  prefs: []
  type: TYPE_TB
- en: '| Histopathology | Human Adipocyte Cells | ADI | Cell Counting | Keypoint Localization
    |'
  prefs: []
  type: TYPE_TB
- en: '| - | Various Tissues & Species | DCC | Cell Counting | Keypoint Localization
    |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[2021](#bib.bib210)] | 2021 | MICCAI | Fundus | Eye | DRIVE |
    Retina Vessel Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| OCTA | Eye | ROSE-1 | Retina Vessel Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2021a](#bib.bib230)] | 2021 | MICCAI | CT | Lung | MSD | Tumor
    Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Colon | MSD | Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Kidney | KiTS 19 | Kidney & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Nguyen et al. [[2021](#bib.bib133)] | 2021 | MIDL | X-ray | Chest | in-house
    | Diagnosis of Airspace Opacity & Lung Lesion | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Chest | RSNA Pneumonia | Diagnosis of Pneumonia | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Chest | CheXpert | Detection of Pleural Effusion | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Mahapatra et al. [[2021](#bib.bib123)] | 2021 | TMI | X-ray | Chest | ChestX-ray8
    | Thoracic Disease Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Histopathology | Colon | Glas | Gland Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Nath et al. [[2021](#bib.bib131)] | 2021 | TMI | CT | Pancreas | MSD | Pancreas
    & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| MRI | Hippocampus | MSD | Hippocampus Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2021](#bib.bib30)] | 2021 | TPAMI | CT | Heart | in-house |
    Calcification Level Prediction in Aortic Stenosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022a](#bib.bib186)] | 2022 | arXiv | CT | Lung | AAPM | CT
    Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Spine | VerSe | CT Reconstruction | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '| Kothawade et al. [[2022b](#bib.bib100)] | 2022 | AAAI | X-ray | Chest | PneumoniaMNIST
    | Pneumonia & Normal Classification | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Quan et al. [[2022](#bib.bib149)] | 2022 | CVPR | X-ray | Head | [Kaggle](https://www.kaggle.com/datasets/jiahongqian/cephalometric-landmarks)
    | Cephalometric Landmark Detection | Keypoint Localization |'
  prefs: []
  type: TYPE_TB
- en: '| Hand | Payer et al. [[2019](#bib.bib140)] | Hand Landmark Detection | Keypoint
    Localization |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2022a](#bib.bib222)] | 2022 | CVPR | MRI | Spine | in-house
    | Diagnosis of Metastatic Epidural Spinal Cord Compression | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2022c](#bib.bib84)] | 2022 | Knowledge-based Systems | Dermscopy
    | Skin | ISIC 2020 | Skin Lesion Classification | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2022b](#bib.bib83)] | 2022 | Knowledge-based Systems | Dermscopy
    | Skin | ISIC 2018 | Skin Lesion Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| X-ray | Chest | Jaeger et al. [[2013](#bib.bib77)] | Lung Segmentation |
    Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Atzeni et al. [[2022](#bib.bib9)] | 2022 | MedIA | MRI | Brain | SATA | Brain
    Structure Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Histology | Brain | in-house | Brain Structure Segmentation | Segmentation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[2022](#bib.bib39)] | 2022 | MedIA | MRI | Brain | BraTS 2019
    | Brain Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Brain | MALC | Brain Structure Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[2022](#bib.bib231)] | 2022 | MedIA | CT | Lung | MSD | Tumor
    Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Colon | MSD | Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Kidney | KiTS 19 | Kidney & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Colonoscopy | Colon | CVC-ClinicDB | Polyp Segmentation | Segmentation |  Table
    3: Methodology summarization of surveyed active learning works.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Year | Venues | Modality | ROIs | Dataset | Clinical Task | Technical
    Task |'
  prefs: []
  type: TYPE_TB
- en: '| Nath et al. [[2022](#bib.bib132)] | 2022 | MICCAI | CT | Liver | MSD | Liver
    & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Hepatic Vessels | MSD | Hepatic Vessels & Tumor Segmentation | Segmentation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bai et al. [[2022](#bib.bib10)] | 2022 | MICCAI | Wireless Capsule Endoscopy
    | Colon | CAD-CAP | Polyp Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Balaram et al. [[2022](#bib.bib14)] | 2022 | MICCAI | X-ray | Chest | Chestx-ray8
    | Thoracic Disease Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022c](#bib.bib202)] | 2022 | MICCAI | CT | Liver | LiTS | Liver
    & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| CT, MRI | Liver | CHAOS | Liver & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Liver | Sliver07 | Liver & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Liver | MSD | Liver & Tumor Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Kothawade et al. [[2022c](#bib.bib101)] | 2022 | MICCAIW | X-ray | Chest
    | PneumoniaMNIST | Pneumonia & Normal Classification | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Histopathology | Colon | PathMNIST | Survival Prediction | Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mircoscopy | Peripheral Blood | BloodMNIST | Cell Type Classification | Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dermscopy | Skin | ISIC 2018 | Skin Lesion Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Fundus | Eye | APTOS-2019 | Diabetic Retinopathy Detection | Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[2022](#bib.bib109)] | 2022 | TMI | Histopathology | Prostate
    | PANDA | Gleason Grading of Prostate Cancer | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Mahapatra et al. [[2022](#bib.bib122)] | 2022 | TMI | X-ray | Chest | ChestXpert
    | Thoracic Disease Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2023b](#bib.bib81)] | 2023 | EAAI | Dermscopy | Skin | ISIC
    2018 | Skin Lesion Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| X-ray | Chest | Jaeger et al. [[2013](#bib.bib77)] | Lung Segmentation |
    Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Sadafi et al. [[2023](#bib.bib158)] | 2023 | ISBI | Histopathology | Breast
    | CAMELYON17 | Detection of Cancer Metastasesin Lymph Nodes | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Qu et al. [[2023](#bib.bib148)] | 2023 | MICCAI | Histopathology | Colon
    | NCT-CRC-HE-100K | Colorectal Cancer Diagnosis | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2023](#bib.bib31)] | 2023 | MIDL | Histopathology | Colon |
    PathMNIST | Survival Prediction | Classification |'
  prefs: []
  type: TYPE_TB
- en: '| CT | Abdomen | OrganAMNIST | Classification of Body Organs | Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mircoscopy | Peripheral Blood | BloodMNIST | Cell Type Classification | Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lou et al. [[2023](#bib.bib118)] | 2023 | TMI | Histopathology | Seven Organs
    | TCGA-KUMAR | Nuclei Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Breast | TNBC | Nuclei Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Seven Organs | MoNuSeg | Nuclei Segmentation | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Active Learning in Medical Image Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some common clinical tasks, such as disease diagnosis, cancer staging, and prognostic
    prediction, can be formulated as medical image classification. Most AL works in
    medical imaging classification directly employ general methods, such as using
    class-balancing sampling in §[3.3.2](#S3.SS3.SSS2 "3.3.2 Class-balance Sampling
    ‣ 3.3 Sampling Strategy ‣ 3 Core Methods of Active Learning ‣ A comprehensive
    survey on deep active learning and its applications in medical image analysis")
    to mitigate the long-tail effect of medical imaging datasets. However, specialized
    design of AL algorithms is required for certain modalities of medical image classification.
    For example, the classification of chest X-rays often involves the idea of multi-label.
    Besides, classifying pathological whole-slide images typically needs to be formulated
    as a multiple instance learning problem. This section will introduce AL works
    specifically targeted at classification problems in chest X-rays and pathological
    whole-slide images.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Chest X-ray and Multi-label Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chest X-ray examinations are crucial for screening and diagnosing lung, cardiovascular,
    skeletal, and other thoracic diseases. Computer-aided diagnosis in this domain
    has been extensively researched, including AL works aimed at reducing annotation
    costs for physicians. Mahapatra et al. [[2021](#bib.bib123)] introduced saliency
    maps to select informative samples for annotation. To aggregate the per-pixel
    saliency maps into a single scalar, they explored three different approaches,
    including computing the kurtosis of the saliency map, utilizing multivariate radiomic
    features, and combining deep features of autoencoders and clustering. Results
    demonstrated that the aggregation using deep features performs the best. Nguyen
    et al. [[2021](#bib.bib133)] introduced a gist-set to select samples near the
    decision boundary. Besides, uncertain samples with high entropy were sent for
    annotation, while the confident samples were assigned as pseudo-labels. Additionally,
    they adopted momentum updates to enhance the stability of the sample predictions.
  prefs: []
  type: TYPE_NORMAL
- en: However, multiple diseases and abnormalities often coexist simultaneously in
    diagnosing chest X-rays. Therefore, multi-label classification has been introduced,
    allowing each sample to be categorized into multiple classes [Baltruschat et al.,
    [2019](#bib.bib15)]. Consequently, AL algorithms for chest X-ray classification
    must adapt to the multi-label setting. Balaram et al. [[2022](#bib.bib14)] modified
    the EDL-based AL to accommodate the multi-label setting. Specifically, they transformed
    the Dirichlet distribution in EDL into multiple Beta distributions, each corresponding
    to one class label. They then calculated the entropy of the Beta distributions
    as the aleatoric uncertainty of the sample. Additionally, they incorporated semi-supervised
    methods like MeanTeacher [Tarvainen and Valpola, [2017](#bib.bib179)], VAT [Miyato
    et al., [2018](#bib.bib128)], and NoTeacher [Unnikrishnan et al., [2021](#bib.bib183)]
    to further reduce annotation costs. Built upon saliency maps, Mahapatra et al.
    [[2022](#bib.bib122)] further introduced GNN to model the inter-relationships
    between different labels. In this work, each class was treated as a node in a
    graph, with the relationships between classes represented as edges. They employed
    various techniques to aggregate information between different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Pathological Whole-slide Images and Multiple Instance Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to modalities like X-ray, CT, and MRI, pathological whole-slide images
    (WSIs) provide microscopic details at the cellular level, making them critically
    important for tasks such as cancer staging and prognostic prediction. However,
    WSIs are very large, with maximum resolutions reaching $100,000\times 100,000$
    pixels. To handle these large images for deep learning, WSIs are usually divided
    into many small patches. Fully supervised methods require annotations for each
    patch, resulting in high annotation costs. AL can effectively improve annotation
    efficiency. For instance, in classifying breast pathological images, Qi et al.
    [[2019](#bib.bib145)] used entropy as the uncertainty metric. Uncertain patches
    were sent for annotation, whereas those with low entropy were given pseudo-labels
    to assist training.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, pathologists might only provide WSI-level annotations in real-world
    clinical scenarios. Consequently, a prevailing direction in research is to formulate
    WSI classification as the weakly-supervised multi-instance learning (MIL) [Qu
    et al., [2022](#bib.bib147)]. In this framework, the entire WSI is viewed as a
    bag, and patches within each WSI are treated as instances within that bag. A well-trained
    MIL learner can automatically identify relevant patches based on WSI-level labels,
    thus significantly reducing annotation costs. For example, a trained MIL classifier
    can automatically spot related patches by annotating whether or not cancer metastasis
    is present in a WSI. Nonetheless, task-relevant patches are often outnumbered
    by irrelevant ones, making MIL convergence more challenging. In MIL-based pathological
    WSI classification, AL filters out irrelevant patches and selects informative
    patches for annotation. Qu et al. [[2023](#bib.bib148)] found that in addition
    to patches related to the target (e.g., tumors, lymph nodes, and normal cells),
    WSIs contain many irrelevant patches (e.g., fat, stroma, and debris). Therefore,
    they adopted the open-set AL [Ning et al., [2022](#bib.bib134)], in which the
    unlabeled pool contained both target and non-target class samples. They combined
    feature distributions with prediction uncertainty to select informative and relevant
    patches of the target class for annotation. Based on attention-based MIL, Sadafi
    et al. [[2023](#bib.bib158)] adopted MC Dropout to estimate both attention and
    classification uncertainties of each patch, then sent the most uncertain patches
    in each WSI for expert annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Active Learning in Medical Image Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Segmentation is one of the most common tasks in medical image analysis, capable
    of precisely locating anatomical structures or pathological lesions. However,
    training a segmentation model requires pixel-level annotation, which is time-consuming
    and labor-intensive for doctors. Therefore, active learning has been widely used
    in medical image segmentation and has become an important method to reduce annotation
    costs. Based on the unique traits of medical imaging, this section will focus
    on specialized designs in AL for medical image segmentation, including slice-based
    annotation, one-shot annotation, and annotation cost.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Slice-based Annotation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 3D modalities like CT and MRI, adjacent 2D slices often exhibit significant
    semantic redundancy. Consequently, annotating only the key slices of each sample
    can reduce annotation costs. Representativeness-based methods have been widely
    applied in this line of work. For instance, Zheng et al. [[2020](#bib.bib228)]
    utilized autoencoders to learn the semantic features of each slice, then selected
    and annotated key slices from axial, sagittal, and coronal planes with a strategy
    similar to RA. Specifically, they initially trained three 2D segmentation networks
    and one 3D segmentation network, where the inputs for the 2D networks are slices
    from different planes. These segmentation networks were used to generate four
    sets of pseudo-labels and subsequently to train the final 3D segmentation network.
    Results showed that this slice-based strategy outperforms uniform sampling. Building
    upon this method, Wu et al. [[2022c](#bib.bib202)] incorporated a self-attention
    module into the autoencoder to enhance slice-level feature learning. In recent
    years, uncertainty methods have been introduced for selecting key slices. In interactive
    segmentation of 3D medical images, Zhou et al. [[2021a](#bib.bib230)] and their
    subsequent work [Zhou et al., [2022](#bib.bib231)] introduced a quality assessment
    module to provide a predicted average IoU score for each slice. They chose the
    slice with the lowest IoU score in each volume for the next round of interactive
    segmentation. In muscle segmentation of CT images, Hiasa et al. [[2020](#bib.bib66)]
    selected key slices and key regions. This work adopted clustering to select key
    slices and further selected regions with high uncertainty within each key slice
    for annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 One-shot Annotation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Currently, most AL works require multiple rounds of annotation. However, this
    setting may not be practical in medical image segmentation. Multi-round annotation
    requires physicians to be readily available for each round of labeling, which
    is unrealistic in practice. If physicians cannot complete the annotations on time,
    the AL process must be suspended. In contrast, one-shot annotation eliminates
    the need for multiple interactions with physicians. It also allows for selecting
    valuable samples in a single round, thus reducing time costs. Both one-shot annotation
    and cold-start AL aim to select the most optimal initial annotations. However,
    the former allows for a higher annotation budget and strictly limits the number
    of interactions with experts to just one. Most relevant works combine self-supervised
    features and specific sampling strategies to achieve one-shot annotation. For
    example, RA [Zheng et al., [2019](#bib.bib227)] is one of the earliest works in
    one-shot AL for medical image segmentation. They applied the VAE feature and a
    representativeness strategy to select informative samples for annotation in one
    shot. RA performed excellently in segmenting gland of pathological images, whole-heart
    of MRI images, and fungal of electron microscopic images. Jin et al. [[2022b](#bib.bib83)]
    combined features of contrastive learning with farthest-first sampling to achieve
    one-shot annotation. The proposed method demonstrated effectiveness on the ISIC
    2018 and lung segmentation datasets. Additionally, Jin et al. [[2023b](#bib.bib81)]
    utilized auto-encoding transformations for self-supervised feature learning. They
    selected and annotated samples with high density based on reachable distance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Annotation Cost
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Current AL works often assume equal annotation costs for each sample. Yet, this
    is not the case in medical image segmentation, where the time to annotate different
    samples can differ greatly. AL techniques can better support physicians by considering
    annotation costs (e.g., annotation time). Nevertheless, there is still limited
    research in this specific domain. In detecting intracranial hemorrhage of CT scans,
    Kuo et al. [[2018](#bib.bib103)] combined predictive disagreement with annotation
    time to select samples for annotation. Specifically, they adopted the Jensen-Shannon
    divergence to measure the disagreement between the outputs of multiple models.
    Annotation time for each sample was estimated by the length of the segmentation
    boundary and the number of connected components. In this work, AL was framed as
    a 0-1 knapsack problem, and dynamic programming is used to solve this problem
    for selecting informative samples. In brain structure segmentation, Atzeni et al.
    [[2022](#bib.bib9)] further considered the spatial relationships between multiple
    regions of interest to more accurately estimate the annotation cost. Moreover,
    the average Dice coefficient of previous rounds was used to predict the average
    Dice for current segmentation results. They selected and annotated regions that
    can maximize the average Dice.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Active Learning in Medical Image Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AL can also be applied in medical image reconstruction. AL methods can help
    minimize the observations needed for modalities that require a long imaging time.
    This accelerates the imaging process and shortens the waiting period for patients.
    In this section, we’ll explore the application of AL in the reconstruction of
    MRI, CT, and electron microscopy.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has been applied to accelerate MRI acquisition and reconstruction.
    A common practice is to reduce k-space sampling through a fixed mask and use a
    deep model to reconstruct the undersampled MRI [Qin et al., [2018](#bib.bib146)].
    To further improve the imaging speed, learnable sampling in AL can be applied
    to select the next measurement locations in k-space. For example, Zhang et al.
    [[2019](#bib.bib225)] adopted adversarial learning to train an evaluator for selecting
    the next row in k-space. Pineda et al. [[2020](#bib.bib142)] utilized reinforcement
    learning to train a dual deep Q-network for active sampling in k-space. Bakker
    et al. [[2020](#bib.bib13)] adopted policy gradient in reinforcement learning
    to train a policy network for adaptive sampling in k-space. The reward for the
    policy network was based on the improvement in structural similarity before and
    after the acquisition. Additionally, Bakker et al. [[2022](#bib.bib12)] explored
    how to jointly optimize the reconstruction and acquisition networks.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to MRI imaging, AL has been employed in CT reconstruction as illustrated
    by Wang et al. [[2022a](#bib.bib186)]. They adaptively chose the scanning angles
    tailored to individual patients, leading to a reduction in both radiation exposure
    and scanning duration. In electron microscopy, Mi et al. [[2020](#bib.bib127)]
    initially enhanced low-resolution images to high-resolution and then predicted
    the location of region-of-interest and reconstruction error. A weighted DPP based
    on reconstruction error was applied to select pixels that needed rescanned. Results
    showed that weighted DPP maintained both low reconstruction error and spatial
    diversity.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Challenges and Future Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, annotation scarcity is a significant bottleneck hindering the development
    of medical image analysis. AL improves annotation efficiency by selectively querying
    the most informative samples for annotation. This survey reviews the recent developments
    in deep active learning, focusing on the evaluation of informativeness, sampling
    strategies, integration with other label-efficient techniques, and the application
    of AL in medical image analysis. In this section, we will discuss the existing
    challenges faced by AL in medical image analysis and its future perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Towards Active Learning with Better Uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In AL, uncertainty plays a pivotal role. However, it would be beneficial if
    the uncertainty more directly highlighted the model’s mistakes. We can enhance
    the model’s performance by querying samples with inaccurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, many works have adopted learnable performance estimation for quality
    control of deep model outputs. For instance, the recently proposed segment anything
    model (SAM) [Kirillov et al., [2023](#bib.bib94)] provides IoU estimates for each
    mask to evaluate its quality. In medical image analysis, automated quality control
    is critical to ensure the reliability and safety of the deep model outputs [Kohlberger
    et al., [2012](#bib.bib97)]. For example, Wang et al. [[2020c](#bib.bib191)] employed
    deep generative models for learnable quality control in cardiac MRI segmentation,
    where the predicted Dice scores showed a strong linear relationship with the real
    ones. Additionally, Billot et al. [[2023](#bib.bib20)] used an additional neural
    network to predict the Dice coefficient of brain tissue segmentation results.
    Overall, learnable performance estimation can accurately predict the quality of
    model outputs. Hence, delving deeper into their potential for uncertainty-based
    AL is crucial to effectively tackle the issue of over-confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, improving the probability calibration of model prediction is a promising
    way to mitigate the over-confidence issue. Calibration [Guo et al., [2017](#bib.bib60),
    Mehrtash et al., [2020](#bib.bib125)] reflects the consistency between model prediction
    probabilities and the ground truth. A well-calibrated model should display a strong
    correlation between confidence and accuracy. For instance, if a perfect-calibrated
    polyp classifier gives an average confidence score of 0.9 on a dataset, it means
    that 90% of those samples should indeed have polyps. In reality, deep models generally
    suffer from the issue of over-confidence, which essentially means that they are
    not well-calibrated. Currently, only a few uncertainty-based AL works have considered
    probability calibration. For instance, Beluch et al. [[2018](#bib.bib16)] found
    that the model ensemble has better calibration than MC Dropout. Xie et al. [[2022c](#bib.bib205)]
    mitigated miscalibration by considering all possible prediction outcomes in the
    Dirichlet distribution. However, these methods are limited to proposing a better
    uncertainty metric and validating the calibration quality post-hoc. Existing calibration
    methods [Guo et al., [2017](#bib.bib60), Ding et al., [2021](#bib.bib42)] directly
    adjusted the distribution of prediction probabilities. However, these methods
    require an additional labeled dataset, thus limiting their practical applicability.
    Therefore, integrating probability calibration into uncertainty-based AL represents
    a valuable research direction worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Towards Active Learning with Better Representativeness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Representativeness-based AL effectively utilizes feature representations and
    data distributions for sample selection. Cover-based and diversity-based AL methods
    implicitly capture the data distribution, whereas density-based AL explicitly
    estimates it. However, the latter requires supplementary strategies to ensure
    diversity. As the core of density-based AL, density estimation in high-dimensional
    spaces has always been challenging. Popular density estimation methods, such as
    kernel density estimation and GMM, may face challenges in high-dimensional spaces.
    In future research, we can consider introducing density estimators tailored to
    high-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Towards Active Learning with Weak Annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In §[4.4](#S4.SS4 "4.4 Region-based Active Learning: Smaller Labeling Unit
    ‣ 4.3 Active Domain Adaptation: Tackling Distribution Shift ‣ 4.2.2 Combination
    of Active Learning and Self-supervised Learning ‣ 4.2 Self-supervised Learning:
    Utilizing Pre-trained Model ‣ 4.1.2 Consistency Regularization ‣ 4.1 Semi-supervised
    Learning: Utilizing Unlabeled Data ‣ 4 Integration of Active Learning and Other
    Label-Efficient Techniques ‣ A comprehensive survey on deep active learning and
    its applications in medical image analysis"), we discuss region-based active learning,
    which only requires region-level annotation of a sample. However, annotating all
    pixels within the region is still needed. Some AL works have incorporated weak
    annotations to simplify the task for annotators. In object detection tasks, Vo
    et al. [[2022](#bib.bib184)] trained deep models with image-level annotation.
    They selected samples with box-in-box prediction results and annotated them with
    bounding boxes. Moreover, Lyu et al. [[2023](#bib.bib119)] adopted disagreement
    to choose which objects are worth annotating. Rather than annotating all objects
    within the image, they only required box-level annotations for a subset of objects.
    In AL of instance segmentation, Tang et al. [[2022](#bib.bib178)] only required
    annotations for each object’s class label and bounding box, without the annotation
    of fine-grained segmentation masks. In future research, AL based on weak annotations
    is a direction worthy of in-depth exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Towards Active Learning with Better Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In §[4.5](#S4.SS5 "4.5 Generative Model: Data Augmentation and Generative Active
    Learning ‣ 4.4.3 Region-based Active Domain Adaptation ‣ 4.4 Region-based Active
    Learning: Smaller Labeling Unit ‣ 4.3 Active Domain Adaptation: Tackling Distribution
    Shift ‣ 4.2.2 Combination of Active Learning and Self-supervised Learning ‣ 4.2
    Self-supervised Learning: Utilizing Pre-trained Model ‣ 4.1.2 Consistency Regularization
    ‣ 4.1 Semi-supervised Learning: Utilizing Unlabeled Data ‣ 4 Integration of Active
    Learning and Other Label-Efficient Techniques ‣ A comprehensive survey on deep
    active learning and its applications in medical image analysis"), we summarize
    the applications of generative models in AL. However, existing works have mainly
    focused on using GANs as sample generators. Recently, diffusion models [Kazerouni
    et al., [2023](#bib.bib89)] have advanced in achieving state-of-the-art generative
    quality. Furthermore, text-to-image diffusion models, represented by Stable Diffusion
    [Rombach et al., [2022](#bib.bib155)], have revolutionized the image generation
    domain. Their high-quality, text-guided generation results enable a more flexible
    image generation. Exploring the potential of diffusion models in deep AL is a
    promising avenue for future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Towards Active Learning with Foundation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the rise of visual foundational models, such as contrastive language-image
    pretraining (CLIP) [Radford et al., [2021](#bib.bib150)] and SAM [Kirillov et al.,
    [2023](#bib.bib94)], and large language models (LLMs) like GPT [OpenAI, [2023](#bib.bib137)],
    deep learning in medical image analysis and computer vision is undergoing a paradigm
    shift. These foundational models [Bommasani et al., [2021](#bib.bib23)] offer
    new opportunities for the development of AL.
  prefs: []
  type: TYPE_NORMAL
- en: AL is closely related to the training paradigms in deep learning of computer
    vision and medical image analysis. From the initial approach of train-from-scratch
    to the “pretrain-finetune” strategy using supervised or self-supervised pre-trained
    models, these paradigms usually require fine-tuning the entire network. Foundation
    models contain a wealth of knowledge. When combined with recently emerging parameter-efficient
    fine tuning (PEFT) or prompt tuning techniques [Hu et al., [2021](#bib.bib70),
    Jia et al., [2022](#bib.bib78)], we can tune only a minimal subset of model weights
    (for example, 5%) for rapid transfer to downstream tasks. As the number of fine-tuned
    parameters decreases, AL has the potential to further reduce the number of required
    annotated samples. Therefore, it is essential to investigate the applicability
    of existing AL under PEFT or prompt tuning and explore the most suitable AL strategies
    for PEFT.
  prefs: []
  type: TYPE_NORMAL
- en: In natural language processing, LLMs have already taken a dominant role. Since
    most researchers cannot tune the LLMs, they rely on in-context learning, which
    provides LLMs with limited examples to transfer to downstream tasks. We believe
    that visual in-context learning will play a vital role in future research. Therefore,
    selecting the most suitable prompts for visual in-context learning will become
    an important research direction of AL.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Active learning is important to deep learning in medical image analysis since
    it effectively reduces the annotation costs incurred by human experts. This survey
    comprehensively reviews the core methods in deep active learning, its integration
    with different label-efficient techniques, and active learning works tailored
    to medical image analysis. We further discuss its current challenges and future
    perspectives. In summary, we believe that deep active learning and its application
    in medical image analysis hold important academic value and clinical potential,
    with ample room for further development.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study was supported by the National Natural Science Foundation of China
    (Grant 82372097 and 82072021) and the Science and Technology Innovation Plan of
    Shanghai Science and Technology Commission (Grant 23S41900400).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achanta et al. [2012] Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P.,
    Süsstrunk, S., 2012. Slic superpixels compared to state-of-the-art superpixel
    methods. IEEE transactions on pattern analysis and machine intelligence 34, 2274–2282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agarwal et al. [2020] Agarwal, S., Arora, H., Anand, S., Arora, C., 2020. Contextual
    diversity for active learning, in: Computer Vision – ECCV 2020. Springer International
    Publishing, Cham. volume 12361, pp. 137–153. doi:[10.1007/978-3-030-58517-4_9](http://dx.doi.org/10.1007/978-3-030-58517-4_9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aghdam et al. [2019] Aghdam, H.H., Gonzalez-Garcia, A., Weijer, J.v.d., López,
    A.M., 2019. Active learning for deep detection neural networks, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 3672–3680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angluin [1988] Angluin, D., 1988. Queries and concept learning. Machine Learning
    2, 319–342. doi:[10.1023/A:1022821128753](http://dx.doi.org/10.1023/A:1022821128753).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angluin [2004] Angluin, D., 2004. Queries revisited. Theoretical Computer Science
    313, 175–194. doi:[10.1016/j.tcs.2003.11.004](http://dx.doi.org/10.1016/j.tcs.2003.11.004).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ardila et al. [2019] Ardila, D., Kiraly, A.P., Bharadwaj, S., Choi, B., Reicher,
    J.J., Peng, L., Tse, D., Etemadi, M., Ye, W., Corrado, G., et al., 2019. End-to-end
    lung cancer screening with three-dimensional deep learning on low-dose chest computed
    tomography. Nature medicine 25, 954–961.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ash et al. [2021] Ash, J., Goel, S., Krishnamurthy, A., Kakade, S., 2021. Gone
    fishing: Neural active learning with fisher embeddings, in: Advances in Neural
    Information Processing Systems, Curran Associates, Inc.. pp. 8927–8939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ash et al. [2020] Ash, J.T., Zhang, C., Krishnamurthy, A., Langford, J., Agarwal,
    A., 2020. Deep batch active learning by diverse, uncertain gradient lower bounds,
    in: International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atzeni et al. [2022] Atzeni, A., Peter, L., Robinson, E., Blackburn, E., Althonayan,
    J., Alexander, D.C., Iglesias, J.E., 2022. Deep active learning for suggestive
    segmentation of biomedical image stacks via optimisation of dice scores and traced
    boundary length. Medical Image Analysis 81, 102549. doi:[10.1016/j.media.2022.102549](http://dx.doi.org/10.1016/j.media.2022.102549).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. [2022] Bai, F., Xing, X., Shen, Y., Ma, H., Meng, M.Q.H., 2022.
    Discrepancy-based active learning for weakly supervised bleeding segmentation
    in wireless capsule endoscopy images, in: Wang, L., Dou, Q., Fletcher, P.T., Speidel,
    S., Li, S. (Eds.), Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2022, Springer Nature Switzerland, Cham. pp. 24–34. doi:[10.1007/978-3-031-16452-1_3](http://dx.doi.org/10.1007/978-3-031-16452-1_3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baid et al. [2021] Baid, U., Ghodasara, S., Mohan, S., Bilello, M., Calabrese,
    E., Colak, E., Farahani, K., Kalpathy-Cramer, J., Kitamura, F.C., Pati, S., et al.,
    2021. The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and
    radiogenomic classification. arXiv preprint arXiv:2107.02314 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bakker et al. [2022] Bakker, T., Muckley, M., Romero-Soriano, A., Drozdzal,
    M., Pineda, L., 2022. On learning adaptive acquisition policies for undersampled
    multi-coil mri reconstruction, in: Proceedings of The 5th International Conference
    on Medical Imaging with Deep Learning, PMLR. pp. 63–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bakker et al. [2020] Bakker, T., van Hoof, H., Welling, M., 2020. Experimental
    design for mri by greedy policy search, in: Advances in Neural Information Processing
    Systems, Curran Associates, Inc.. pp. 18954–18966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balaram et al. [2022] Balaram, S., Nguyen, C.M., Kassim, A., Krishnaswamy,
    P., 2022. Consistency-based semi-supervised evidential active learning for diagnostic
    radiograph classification, in: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S.,
    Li, S. (Eds.), Medical Image Computing and Computer Assisted Intervention – MICCAI
    2022, Springer Nature Switzerland, Cham. pp. 675–685. doi:[10.1007/978-3-031-16431-6_64](http://dx.doi.org/10.1007/978-3-031-16431-6_64).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baltruschat et al. [2019] Baltruschat, I.M., Nickisch, H., Grass, M., Knopp,
    T., Saalbach, A., 2019. Comparison of deep learning approaches for multi-label
    chest x-ray classification. Scientific reports 9, 6381.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beluch et al. [2018] Beluch, W.H., Genewein, T., Nürnberger, A., Köhler, J.M.,
    2018. The power of ensembles for active learning in image classification, in:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 9368–9377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengar et al. [2022] Bengar, J.Z., van de Weijer, J., Fuentes, L.L., Raducanu,
    B., 2022. Class-balanced active learning for image classification, in: Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1536–1545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengar et al. [2021] Bengar, J.Z., van de Weijer, J., Twardowski, B., Raducanu,
    B., 2021. Reducing label effort: Self-supervised meets active learning, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 1631–1639.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van den Bergh et al. [2012] Van den Bergh, M., Boix, X., Roig, G., De Capitani,
    B., Van Gool, L., 2012. Seeds: Superpixels extracted via energy-driven sampling,
    in: Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence,
    Italy, October 7-13, 2012, Proceedings, Part VII 12, Springer. pp. 13–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Billot et al. [2023] Billot, B., Magdamo, C., Cheng, Y., Arnold, S.E., Das,
    S., Iglesias, J.E., 2023. Robust machine learning segmentation for large-scale
    analysis of heterogeneous clinical brain mri datasets. Proceedings of the National
    Academy of Sciences 120, e2216399120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bishop [1994] Bishop, C.M., 1994. Mixture density networks .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bıyık et al. [2019] Bıyık, E., Wang, K., Anari, N., Sadigh, D., 2019. Batch
    active learning using determinantal point processes. arXiv preprint arXiv:1906.07975
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bommasani et al. [2021] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R.,
    Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E.,
    et al., 2021. On the opportunities and risks of foundation models. arXiv preprint
    arXiv:2108.07258 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budd et al. [2021] Budd, S., Robinson, E.C., Kainz, B., 2021. A survey on active
    learning and human-in-the-loop deep learning for medical image analysis. Medical
    Image Analysis 71, 102062.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2021] Cai, L., Xu, X., Liew, J.H., Foo, C.S., 2021. Revisiting
    superpixels for active learning in semantic segmentation with realistic annotation
    costs, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 10988–10997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caramalau et al. [2021] Caramalau, R., Bhattarai, B., Kim, T.K., 2021. Sequential
    graph convolutional network for active learning, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 9583–9592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cardoso et al. [2017] Cardoso, T.N.C., Silva, R.M., Canuto, S., Moro, M.M.,
    Gonçalves, M.A., 2017. Ranked batch-mode active learning. Information Sciences
    379, 313–337. doi:[10.1016/j.ins.2016.10.037](http://dx.doi.org/10.1016/j.ins.2016.10.037).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Casanova et al. [2020] Casanova, A., Pinheiro, P.O., Rostamzadeh, N., Pal,
    C.J., 2020. Reinforced active learning for image segmentation, in: International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhuri et al. [2015] Chaudhuri, K., Kakade, S.M., Netrapalli, P., Sanghavi,
    S., 2015. Convergence rates of active learning for maximum likelihood estimation,
    in: Advances in Neural Information Processing Systems, Curran Associates, Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Chen, J., Xie, Y., Wang, K., Zhang, C., Vannan, M.A., Wang,
    B., Qian, Z., 2021. Active image synthesis for efficient labeling. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 43, 3770–3781. doi:[10.1109/TPAMI.2020.2993221](http://dx.doi.org/10.1109/TPAMI.2020.2993221).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Chen, L., Bai, Y., Huang, S., Lu, Y., Wen, B., Yuille, A.L.,
    Zhou, Z., 2023. Making your first choice: To address cold start problem in vision
    active learning, in: Medical Imaging with Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., Hinton,
    G.E., 2020. Big self-supervised models are strong semi-supervised learners. Advances
    in neural information processing systems 33, 22243–22255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022a] Chen, Y., Mancini, M., Zhu, X., Akata, Z., 2022a. Semi-supervised
    and unsupervised deep visual learning: A survey. IEEE transactions on pattern
    analysis and machine intelligence .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022b] Chen, Z., Zhang, J., Wang, P., Chen, J., Li, J., 2022b.
    When active learning meets implicit semantic data augmentation, in: Avidan, S.,
    Brostow, G., Cissé, M., Farinella, G.M., Hassner, T. (Eds.), Computer Vision –
    ECCV 2022\. Springer Nature Switzerland, Cham. volume 13685, pp. 56–72. doi:[10.1007/978-3-031-19806-9_4](http://dx.doi.org/10.1007/978-3-031-19806-9_4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2021a] Choi, J., Elezi, I., Lee, H.J., Farabet, C., Alvarez, J.M.,
    2021a. Active learning for deep object detection via probabilistic modeling, in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10264–10273.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2021b] Choi, J., Yi, K.M., Kim, J., Choo, J., Kim, B., Chang,
    J., Gwon, Y., Chang, H.J., 2021b. Vab-al: Incorporating class imbalance and difficulty
    with variational bayes for active learning, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 6749–6758.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Citovsky et al. [2021] Citovsky, G., DeSalvo, G., Gentile, C., Karydas, L.,
    Rajagopalan, A., Rostamizadeh, A., Kumar, S., 2021. Batch active learning at scale,
    in: Advances in Neural Information Processing Systems, Curran Associates, Inc..
    pp. 11933–11944.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohn et al. [1994] Cohn, D., Atlas, L., Ladner, R., 1994. Improving generalization
    with active learning. Machine Learning 15, 201–221. doi:[10.1007/BF00993277](http://dx.doi.org/10.1007/BF00993277).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2022] Dai, C., Wang, S., Mo, Y., Angelini, E., Guo, Y., Bai, W.,
    2022. Suggestive annotation of brain mr images with gradient-guided sampling.
    Medical Image Analysis 77, 102373. doi:[10.1016/j.media.2022.102373](http://dx.doi.org/10.1016/j.media.2022.102373).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [2020] Dai, C., Wang, S., Mo, Y., Zhou, K., Angelini, E., Guo, Y.,
    Bai, W., 2020. Suggestive annotation of brain tumour images with gradient-guided
    sampling, in: Martel, A.L., Abolmaesumi, P., Stoyanov, D., Mateus, D., Zuluaga,
    M.A., Zhou, S.K., Racoceanu, D., Joskowicz, L. (Eds.), Medical Image Computing
    and Computer Assisted Intervention – MICCAI 2020. Springer International Publishing,
    Cham. volume 12264, pp. 156–165. doi:[10.1007/978-3-030-59719-1_16](http://dx.doi.org/10.1007/978-3-030-59719-1_16).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2009] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei,
    L., 2009. Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference
    on computer vision and pattern recognition, Ieee. pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. [2021] Ding, Z., Han, X., Liu, P., Niethammer, M., 2021. Local
    temperature scaling for probability calibration, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 6889–6899.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. [2022] Du, P., Chen, H., Zhao, S., Chai, S., Chen, H., Li, C., 2022.
    Contrastive active learning under class distribution mismatch. IEEE Transactions
    on Pattern Analysis and Machine Intelligence , 1–13doi:[10.1109/TPAMI.2022.3188807](http://dx.doi.org/10.1109/TPAMI.2022.3188807).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2021] Du, P., Zhao, S., Chen, H., Chai, S., Chen, H., Li, C., 2021.
    Contrastive coding for active learning under class distribution mismatch, in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8927–8936.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ducoffe and Precioso [2018] Ducoffe, M., Precioso, F., 2018. Adversarial active
    learning for deep networks: a margin based approach. arXiv preprint arXiv:1802.09841
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esteva et al. [2017] Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M.,
    Blau, H.M., Thrun, S., 2017. Dermatologist-level classification of skin cancer
    with deep neural networks. nature 542, 115–118.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farahani and Hekmatfar [2009] Farahani, R.Z., Hekmatfar, M., 2009. Facility
    location: concepts, models, algorithms and case studies. Springer Science & Business
    Media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feige [1998] Feige, U., 1998. A threshold of ln n for approximating set cover.
    Journal of the ACM (JACM) 45, 634–652.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2021] Fu, B., Cao, Z., Wang, J., Long, M., 2021. Transferable query
    selection for active domain adaptation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 7272–7281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujishige [2005] Fujishige, S., 2005. Submodular functions and optimization.
    Elsevier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal and Ghahramani [2016] Gal, Y., Ghahramani, Z., 2016. Dropout as a bayesian
    approximation: Representing model uncertainty in deep learning, in: international
    conference on machine learning, PMLR. pp. 1050–1059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal et al. [2017] Gal, Y., Islam, R., Ghahramani, Z., 2017. Deep bayesian active
    learning with image data, in: Proceedings of the 34th International Conference
    on Machine Learning, PMLR. pp. 1183–1192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2020] Gao, M., Zhang, Z., Yu, G., Arık, S.Ö., Davis, L.S., Pfister,
    T., 2020. Consistency-based semi-supervised active learning: Towards minimizing
    labeling cost, in: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (Eds.), Computer
    Vision – ECCV 2020\. Springer International Publishing, Cham. volume 12355, pp.
    510–526. doi:[10.1007/978-3-030-58607-2_30](http://dx.doi.org/10.1007/978-3-030-58607-2_30).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gidaris et al. [2018] Gidaris, S., Singh, P., Komodakis, N., 2018. Unsupervised
    representation learning by predicting image rotations, in: International Conference
    on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gissin and Shalev-Shwartz [2019] Gissin, D., Shalev-Shwartz, S., 2019. Discriminative
    active learning. arXiv preprint arXiv:1907.06347 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [2022] Gong, J., Fan, Z., Ke, Q., Rahmani, H., Liu, J., 2022. Meta
    agent teaming active learning for pose estimation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 11079–11089.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014a] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014a. Generative
    adversarial nets. Advances in neural information processing systems 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014b] Goodfellow, I.J., Shlens, J., Szegedy, C., 2014b.
    Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan and Liu [2021] Guan, H., Liu, M., 2021. Domain adaptation for medical
    image analysis: a survey. IEEE Transactions on Biomedical Engineering 69, 1173–1185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2017] Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q., 2017. On
    calibration of modern neural networks, in: International conference on machine
    learning, PMLR. pp. 1321–1330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hacohen et al. [2022] Hacohen, G., Dekel, A., Weinshall, D., 2022. Active learning
    on a budget: Opposite strategies suit high and low budgets, in: International
    Conference on Machine Learning, PMLR. pp. 8175–8195.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haußmann et al. [2019] Haußmann, M., Hamprecht, F., Kandemir, M., 2019. Deep
    active learning with adaptive acquisition, in: Proceedings of the 28th International
    Joint Conference on Artificial Intelligence, pp. 2470–2476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2022] He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.,
    2022. Masked autoencoders are scalable vision learners, in: Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, pp. 16000–16009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2020] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R., 2020. Momentum
    contrast for unsupervised visual representation learning, in: Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heo et al. [2019] Heo, B., Lee, M., Yun, S., Choi, J.Y., 2019. Knowledge distillation
    with adversarial samples supporting decision boundary, in: Proceedings of the
    AAAI conference on artificial intelligence, pp. 3771–3778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiasa et al. [2020] Hiasa, Y., Otake, Y., Takao, M., Ogawa, T., Sugano, N.,
    Sato, Y., 2020. Automated muscle segmentation from clinical ct using bayesian
    u-net for personalized musculoskeletal modeling. IEEE Transactions on Medical
    Imaging 39, 1030–1040. doi:[10.1109/TMI.2019.2940555](http://dx.doi.org/10.1109/TMI.2019.2940555).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochbaum and Shmoys [1985] Hochbaum, D.S., Shmoys, D.B., 1985. A best possible
    heuristic for the k-center problem. Mathematics of operations research 10, 180–184.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2021] Hou, J., Graham, B., Niessner, M., Xie, S., 2021. Exploring
    data-efficient 3d scene understanding with contrastive scene contexts, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15587–15597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. [2011] Houlsby, N., Huszár, F., Ghahramani, Z., Lengyel, M.,
    2011. Bayesian active learning for classification and preference learning. arXiv
    preprint arXiv:1112.5745 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2021] Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., et al., 2021. Lora: Low-rank adaptation of large language models,
    in: International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2022] Hu, Z., Bai, X., Zhang, R., Wang, X., Sun, G., Fu, H., Tai,
    C.L., 2022. Lidal: Inter-frame uncertainty based active learning for 3d lidar
    semantic segmentation, in: European Conference on Computer Vision, Springer. pp.
    248–265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2023] Huang, D., Li, J., Chen, W., Huang, J., Chai, Z., Li, G.,
    2023. Divide and adapt: Active domain adaptation via customized learning, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 7651–7660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2017] Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.E.,
    Weinberger, K.Q., 2017. Snapshot ensembles: Train 1, get m for free. arXiv preprint
    arXiv:1704.00109 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2021] Huang, S., Wang, T., Xiong, H., Huan, J., Dou, D., 2021.
    Semi-supervised active learning with temporal output discrepancy, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 3447–3456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2020] Huang, Y.J., Liu, W., Wang, X., Fang, Q., Wang, R., Wang,
    Y., Chen, H., Chen, H., Meng, D., Wang, L., 2020. Rectifying supporting regions
    with mixed and active supervision for rib fracture recognition. IEEE Transactions
    on Medical Imaging 39, 3843–3854. doi:[10.1109/TMI.2020.3006138](http://dx.doi.org/10.1109/TMI.2020.3006138).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hwang et al. [2022] Hwang, S., Lee, S., Kim, S., Ok, J., Kwak, S., 2022. Combating
    label distribution shift for active domain adaptation, in: European Conference
    on Computer Vision, Springer. pp. 549–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaeger et al. [2013] Jaeger, S., Karargyris, A., Candemir, S., Folio, L., Siegelman,
    J., Callaghan, F., Xue, Z., Palaniappan, K., Singh, R.K., Antani, S., et al.,
    2013. Automatic tuberculosis screening using chest radiographs. IEEE transactions
    on medical imaging 33, 233–245.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. [2022] Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S.,
    Hariharan, B., Lim, S.N., 2022. Visual prompt tuning, in: European Conference
    on Computer Vision, Springer. pp. 709–727.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2023a] Jin, C., Guo, Z., Lin, Y., Luo, L., Chen, H., 2023a. Label-efficient
    deep learning in medical image analysis: Challenges and future directions. arXiv
    preprint arXiv:2303.12484 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2019] Jin, K.H., Unser, M., Yi, K.M., 2019. Self-supervised deep
    active accelerated mri. arXiv preprint arXiv:1901.04547 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2023b] Jin, Q., Li, S., Du, X., Yuan, M., Wang, M., Song, Z., 2023b.
    Density-based one-shot active learning for image segmentation. Engineering Applications
    of Artificial Intelligence 126, 106805. doi:[10.1016/j.engappai.2023.106805](http://dx.doi.org/10.1016/j.engappai.2023.106805).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2022a] Jin, Q., Yuan, M., Li, S., Wang, H., Wang, M., Song, Z.,
    2022a. Cold-start active learning for image classification. Information Sciences
    616, 16–36. doi:[10.1016/j.ins.2022.10.066](http://dx.doi.org/10.1016/j.ins.2022.10.066).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2022b] Jin, Q., Yuan, M., Qiao, Q., Song, Z., 2022b. One-shot active
    learning for image segmentation via contrastive learning and diversity-based sampling.
    Knowledge-Based Systems 241, 108278. doi:[10.1016/j.knosys.2022.108278](http://dx.doi.org/10.1016/j.knosys.2022.108278).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2022c] Jin, Q., Yuan, M., Wang, H., Wang, M., Song, Z., 2022c. Deep
    active learning models for imbalanced image classification. Knowledge-Based Systems
    257, 109817. doi:[10.1016/j.knosys.2022.109817](http://dx.doi.org/10.1016/j.knosys.2022.109817).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. [2009] Joshi, A.J., Porikli, F., Papanikolopoulos, N., 2009. Multi-class
    active learning for image classification, in: 2009 IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 2372–2379. doi:[10.1109/CVPR.2009.5206627](http://dx.doi.org/10.1109/CVPR.2009.5206627).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jung et al. [2023] Jung, S., Kim, S., Lee, J., 2023. A simple yet powerful
    deep active learning with snapshots ensembles, in: International Conference on
    Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karamcheti et al. [2021] Karamcheti, S., Krishna, R., Fei-Fei, L., Manning,
    C., 2021. Mind your outliers! investigating the negative impact of outliers on
    active learning for visual question answering, in: Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association
    for Computational Linguistics, Online. pp. 7265–7281. doi:[10.18653/v1/2021.acl-long.564](http://dx.doi.org/10.18653/v1/2021.acl-long.564).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kasarla et al. [2019] Kasarla, T., Nagendar, G., Hegde, G.M., Balasubramanian,
    V., Jawahar, C., 2019. Region-based active learning for efficient labeling in
    semantic segmentation, in: 2019 IEEE Winter Conference on Applications of Computer
    Vision (WACV), pp. 1109–1117. doi:[10.1109/WACV.2019.00123](http://dx.doi.org/10.1109/WACV.2019.00123).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazerouni et al. [2023] Kazerouni, A., Aghdam, E.K., Heidari, M., Azad, R.,
    Fayyaz, M., Hacihaliloglu, I., Merhof, D., 2023. Diffusion models in medical imaging:
    A comprehensive survey. Medical Image Analysis , 102846.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall and Gal [2017] Kendall, A., Gal, Y., 2017. What uncertainties do we
    need in bayesian deep learning for computer vision? Advances in neural information
    processing systems 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2023] Kim, H., Oh, M., Hwang, S., Kwak, S., Ok, J., 2023. Adaptive
    superpixel for active learning in semantic segmentation, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision (ICCV), pp. 943–953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2021] Kim, K., Park, D., Kim, K.I., Chun, S.Y., 2021. Task-aware
    variational adversarial active learning, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 8166–8175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2013] Kingma, D.P., Welling, M., 2013. Auto-encoding variational
    bayes. arXiv preprint arXiv:1312.6114 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. [2023] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland,
    C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al., 2023.
    Segment anything. arXiv preprint arXiv:2304.02643 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kirsch et al. [2019] Kirsch, A., van Amersfoort, J., Gal, Y., 2019. Batchbald:
    Efficient and diverse batch acquisition for deep bayesian active learning, in:
    Advances in Neural Information Processing Systems, Curran Associates, Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koh and Liang [2017] Koh, P.W., Liang, P., 2017. Understanding black-box predictions
    via influence functions, in: International conference on machine learning, PMLR.
    pp. 1885–1894.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kohlberger et al. [2012] Kohlberger, T., Singh, V., Alvino, C., Bahlmann, C.,
    Grady, L., 2012. Evaluating segmentation error without ground truth, in: International
    Conference on Medical Image Computing and Computer-Assisted Intervention, Springer.
    pp. 528–536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothawade et al. [2021] Kothawade, S., Beck, N., Killamsetty, K., Iyer, R.,
    2021. Similar: Submodular information measures based active learning in realistic
    scenarios, in: Advances in Neural Information Processing Systems, Curran Associates,
    Inc.. pp. 18685–18697.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothawade et al. [2022a] Kothawade, S., Ghosh, S., Shekhar, S., Xiang, Y.,
    Iyer, R., 2022a. Talisman: Targeted active learning for object detection with
    rare classes and slices using submodular mutual information, in: Computer Vision
    – ECCV 2022, Springer, Cham. pp. 1–16. doi:[10.1007/978-3-031-19839-7_1](http://dx.doi.org/10.1007/978-3-031-19839-7_1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothawade et al. [2022b] Kothawade, S., Kaushal, V., Ramakrishnan, G., Bilmes,
    J., Iyer, R., 2022b. Prism: A rich class of parameterized submodular information
    measures for guided data subset selection, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, pp. 10238–10246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothawade et al. [2022c] Kothawade, S., Savarkar, A., Iyer, V., Ramakrishnan,
    G., Iyer, R., 2022c. Clinical: Targeted active learning for imbalanced medical
    image classification, in: Workshop on Medical Image Learning with Limited and
    Noisy Data, Springer. pp. 119–129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kovashka et al. [2016] Kovashka, A., Russakovsky, O., Fei-Fei, L., Grauman,
    K., et al., 2016. Crowdsourcing in computer vision. Foundations and Trends® in
    computer graphics and Vision 10, 177–243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuo et al. [2018] Kuo, W., Häne, C., Yuh, E., Mukherjee, P., Malik, J., 2018.
    Cost-sensitive active learning for intracranial hemorrhage detection, in: Frangi,
    A.F., Schnabel, J.A., Davatzikos, C., Alberola-López, C., Fichtinger, G. (Eds.),
    Medical Image Computing and Computer Assisted Intervention – MICCAI 2018\. Springer
    International Publishing, Cham. volume 11072, pp. 715–723. doi:[10.1007/978-3-030-00931-1_82](http://dx.doi.org/10.1007/978-3-030-00931-1_82).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [2006] LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang,
    F., 2006. A tutorial on energy-based learning. Predicting structured data 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2013] Lee, D.H., et al., 2013. Pseudo-label: The simple and efficient
    semi-supervised learning method for deep neural networks, in: Workshop on challenges
    in representation learning, ICML, Atlanta. p. 896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2015] Lee, K., Zlateski, A., Ashwin, V., Seung, H.S., 2015. Recursive
    training of 2d-3d convolutional networks for neuronal boundary prediction. Advances
    in Neural Information Processing Systems 28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis and Catlett [1994] Lewis, D.D., Catlett, J., 1994. Heterogeneous uncertainty
    sampling for supervised learning, in: Cohen, W.W., Hirsh, H. (Eds.), Machine Learning
    Proceedings 1994. Morgan Kaufmann, San Francisco (CA), pp. 148–156. doi:[10.1016/B978-1-55860-335-6.50026-X](http://dx.doi.org/10.1016/B978-1-55860-335-6.50026-X).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Yin [2020] Li, H., Yin, Z., 2020. Attention, suggestion and annotation:
    A deep active learning framework for biomedical image segmentation, in: Martel,
    A.L., Abolmaesumi, P., Stoyanov, D., Mateus, D., Zuluaga, M.A., Zhou, S.K., Racoceanu,
    D., Joskowicz, L. (Eds.), Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2020, Springer International Publishing, Cham. pp. 3–13. doi:[10.1007/978-3-030-59710-8_1](http://dx.doi.org/10.1007/978-3-030-59710-8_1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022] Li, W., Li, J., Wang, Z., Polson, J., Sisk, A.E., Sajed, D.P.,
    Speier, W., Arnold, C.W., 2022. Pathal: An active learning framework for histopathology
    image analysis. IEEE Transactions on Medical Imaging 41, 1176–1187. doi:[10.1109/TMI.2021.3135002](http://dx.doi.org/10.1109/TMI.2021.3135002).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2020] Lin, Z., Wei, D., Jang, W.D., Zhou, S., Chen, X., Wang, X.,
    Schalek, R., Berger, D., Matejek, B., Kamentsky, L., Peleg, A., Haehn, D., Jones,
    T., Parag, T., Lichtman, J., Pfister, H., 2020. Two stream active query suggestion
    for active learning in connectomics, in: Vedaldi, A., Bischof, H., Brox, T., Frahm,
    J.M. (Eds.), Computer Vision – ECCV 2020, Springer International Publishing, Cham.
    pp. 103–120. doi:[10.1007/978-3-030-58523-5_7](http://dx.doi.org/10.1007/978-3-030-58523-5_7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2022a] Liu, G., van Kaick, O., Huang, H., Hu, R., 2022a. Active
    self-training for weakly supervised 3d scene semantic segmentation. arXiv preprint
    arXiv:2209.07069 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2020] Liu, J., Cao, L., Tian, Y., 2020. Deep active learning for
    effective pulmonary nodule detection, in: Martel, A.L., Abolmaesumi, P., Stoyanov,
    D., Mateus, D., Zuluaga, M.A., Zhou, S.K., Racoceanu, D., Joskowicz, L. (Eds.),
    Medical Image Computing and Computer Assisted Intervention – MICCAI 2020, Springer
    International Publishing, Cham. pp. 609–618. doi:[10.1007/978-3-030-59725-2_59](http://dx.doi.org/10.1007/978-3-030-59725-2_59).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022b] Liu, P., Wang, L., Ranjan, R., He, G., Zhao, L., 2022b.
    A survey on active deep learning: from model driven to data driven. ACM Computing
    Surveys (CSUR) 54, 1–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023] Liu, S., Yin, S., Qu, L., Wang, M., Song, Z., 2023. A structure-aware
    framework of unsupervised cross-modality domain adaptation via frequency and spatial
    knowledge distillation. IEEE Transactions on Medical Imaging .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021a] Liu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang,
    J., Tang, J., 2021a. Self-supervised learning: Generative or contrastive. IEEE
    transactions on knowledge and data engineering 35, 857–876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021b] Liu, Z., Ding, H., Zhong, H., Li, W., Dai, J., He, C., 2021b.
    Influence selection for active learning, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 9274–9283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2019] Liu, Z., Wang, J., Gong, S., Lu, H., Tao, D., 2019. Deep
    reinforcement active learning for human-in-the-loop person re-identification,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
    6122–6131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lou et al. [2023] Lou, W., Li, H., Li, G., Han, X., Wan, X., 2023. Which pixel
    to annotate: A label-efficient nuclei segmentation framework. IEEE Transactions
    on Medical Imaging 42, 947–958. doi:[10.1109/TMI.2022.3221666](http://dx.doi.org/10.1109/TMI.2022.3221666).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu et al. [2023] Lyu, M., Zhou, J., Chen, H., Huang, Y., Yu, D., Li, Y., Guo,
    Y., Guo, Y., Xiang, L., Ding, G., 2023. Box-level active detection, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23766–23775.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mackowiak et al. [2018] Mackowiak, R., Lenz, P., Ghori, O., Diego, F., Lange,
    O., Rother, C., 2018. Cereals - cost-effective region-based active learning for
    semantic segmentation, in: 29th British Machine Vision Conference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahapatra et al. [2018] Mahapatra, D., Bozorgtabar, B., Thiran, J.P., Reyes,
    M., 2018. Efficient active learning for image classification and segmentation
    using a sample selection and conditional generative adversarial network, in: Frangi,
    A.F., Schnabel, J.A., Davatzikos, C., Alberola-López, C., Fichtinger, G. (Eds.),
    Medical Image Computing and Computer Assisted Intervention – MICCAI 2018\. Springer
    International Publishing, Cham. volume 11071, pp. 580–588. doi:[10.1007/978-3-030-00934-2_65](http://dx.doi.org/10.1007/978-3-030-00934-2_65).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahapatra et al. [2022] Mahapatra, D., Poellinger, A., Reyes, M., 2022. Graph
    node based interpretability guided sample selection for active learning. IEEE
    Transactions on Medical Imaging , 1–1doi:[10.1109/TMI.2022.3215017](http://dx.doi.org/10.1109/TMI.2022.3215017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahapatra et al. [2021] Mahapatra, D., Poellinger, A., Shao, L., Reyes, M.,
    2021. Interpretability-driven sample selection using self supervised learning
    for disease classification and segmentation. IEEE Transactions on Medical Imaging
    40, 2548–2562. doi:[10.1109/TMI.2021.3061724](http://dx.doi.org/10.1109/TMI.2021.3061724).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahmood et al. [2022] Mahmood, R., Fidler, S., Law, M.T., 2022. Low-budget
    active learning via wasserstein distance: An integer programming approach, in:
    International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehrtash et al. [2020] Mehrtash, A., Wells, W.M., Tempany, C.M., Abolmaesumi,
    P., Kapur, T., 2020. Confidence calibration and predictive uncertainty estimation
    for deep medical image segmentation. IEEE transactions on medical imaging 39,
    3868–3878.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menze et al. [2014] Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J.,
    Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., et al.,
    2014. The multimodal brain tumor image segmentation benchmark (brats). IEEE transactions
    on medical imaging 34, 1993–2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mi et al. [2020] Mi, L., Wang, H., Meirovitch, Y., Schalek, R., Turaga, S.C.,
    Lichtman, J.W., Samuel, A.D.T., Shavit, N., 2020. Learning guided electron microscopy
    with active acquisition, in: Martel, A.L., Abolmaesumi, P., Stoyanov, D., Mateus,
    D., Zuluaga, M.A., Zhou, S.K., Racoceanu, D., Joskowicz, L. (Eds.), Medical Image
    Computing and Computer Assisted Intervention – MICCAI 2020, Springer International
    Publishing, Cham. pp. 77–87. doi:[10.1007/978-3-030-59722-1_8](http://dx.doi.org/10.1007/978-3-030-59722-1_8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miyato et al. [2018] Miyato, T., Maeda, S.i., Koyama, M., Ishii, S., 2018.
    Virtual adversarial training: a regularization method for supervised and semi-supervised
    learning. IEEE transactions on pattern analysis and machine intelligence 41, 1979–1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi-Dezfooli et al. [2016] Moosavi-Dezfooli, S.M., Fawzi, A., Frossard,
    P., 2016. Deepfool: a simple and accurate method to fool deep neural networks,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 2574–2582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Munjal et al. [2022] Munjal, P., Hayat, N., Hayat, M., Sourati, J., Khan, S.,
    2022. Towards robust and reproducible active learning using neural networks, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 223–232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nath et al. [2021] Nath, V., Yang, D., Landman, B.A., Xu, D., Roth, H.R., 2021.
    Diminishing uncertainty within the training pool: Active learning for medical
    image segmentation. IEEE Transactions on Medical Imaging 40, 2534–2547. doi:[10.1109/TMI.2020.3048055](http://dx.doi.org/10.1109/TMI.2020.3048055).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nath et al. [2022] Nath, V., Yang, D., Roth, H.R., Xu, D., 2022. Warm start
    active learning with proxy labels and selection via semi-supervised fine-tuning,
    in: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (Eds.), Medical Image
    Computing and Computer Assisted Intervention – MICCAI 2022, Springer Nature Switzerland,
    Cham. pp. 297–308. doi:[10.1007/978-3-031-16452-1_29](http://dx.doi.org/10.1007/978-3-031-16452-1_29).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. [2021] Nguyen, C., Huynh, M.T., Tran, M.Q., Nguyen, N.H., Jain,
    M., Ngo, V.D., Vo, T.D., Bui, T., Truong, S.Q.H., 2021. Goal: Gist-set online
    active learning for efficient chest x-ray image annotation, in: Medical Imaging
    with Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2022] Ning, K.P., Zhao, X., Li, Y., Huang, S.J., 2022. Active
    learning for open-set annotation, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 41–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2021] Ning, M., Lu, D., Wei, D., Bian, C., Yuan, C., Yu, S., Ma,
    K., Zheng, Y., 2021. Multi-anchor active domain adaptation for semantic segmentation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
    9112–9122.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noroozi and Favaro [2016] Noroozi, M., Favaro, P., 2016. Unsupervised learning
    of visual representations by solving jigsaw puzzles, in: European conference on
    computer vision, Springer. pp. 69–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI, 2023. Gpt-4 technical report. [arXiv:2303.08774](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2023] Park, Y., Kim, S., Choi, W., Han, D.J., Moon, J., 2023.
    Active learning for object detection with evidential deep learning and hierarchical
    uncertainty aggregation, in: International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parvaneh et al. [2022] Parvaneh, A., Abbasnejad, E., Teney, D., Haffari, G.R.,
    van den Hengel, A., Shi, J.Q., 2022. Active learning by feature mixing, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12237--12246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Payer et al. [2019] Payer, C., Štern, D., Bischof, H., Urschler, M., 2019. Integrating
    spatial configuration into heatmap regression based cnns for landmark localization.
    Medical image analysis 54, 207--219.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. [2021] Peng, F., Wang, C., Liu, J., Yang, Z., 2021. Active learning
    for lane detection: A knowledge distillation approach, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 15152--15161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pineda et al. [2020] Pineda, L., Basu, S., Romero, A., Calandra, R., Drozdzal,
    M., 2020. Active mr k-space sampling with reinforcement learning, in: Martel,
    A.L., Abolmaesumi, P., Stoyanov, D., Mateus, D., Zuluaga, M.A., Zhou, S.K., Racoceanu,
    D., Joskowicz, L. (Eds.), Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2020, Springer International Publishing, Cham. pp. 23--33. doi:[10.1007/978-3-030-59713-9_3](http://dx.doi.org/10.1007/978-3-030-59713-9_3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pourahmadi et al. [2021] Pourahmadi, K., Nooralinejad, P., Pirsiavash, H., 2021.
    A simple baseline for low-budget active learning. arXiv preprint arXiv:2110.12033
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prabhu et al. [2021] Prabhu, V., Chandrasekaran, A., Saenko, K., Hoffman, J.,
    2021. Active domain adaptation via clustering uncertainty-weighted embeddings,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
    8505--8514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. [2019] Qi, Q., Li, Y., Wang, J., Zheng, H., Huang, Y., Ding, X., Rohde,
    G.K., 2019. Label-efficient breast cancer histopathological image classification.
    IEEE Journal of Biomedical and Health Informatics 23, 2108--2116. doi:[10.1109/JBHI.2018.2885134](http://dx.doi.org/10.1109/JBHI.2018.2885134).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. [2018] Qin, C., Schlemper, J., Caballero, J., Price, A.N., Hajnal,
    J.V., Rueckert, D., 2018. Convolutional recurrent neural networks for dynamic
    mr image reconstruction. IEEE transactions on medical imaging 38, 280--290.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qu et al. [2022] Qu, L., Liu, S., Liu, X., Wang, M., Song, Z., 2022. Towards
    label-efficient automatic diagnosis and analysis: a comprehensive survey of advanced
    deep learning-based weakly-supervised, semi-supervised and self-supervised techniques
    in histopathological image analysis. Physics in Medicine & Biology .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qu et al. [2023] Qu, L., Ma, Y., Yang, Z., Wang, M., Song, Z., 2023. Openal:
    An efficient deep active learning framework for open-set pathology image classification,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 3--13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quan et al. [2022] Quan, Q., Yao, Q., Li, J., Zhou, S.K., 2022. Which images
    to label for few-shot medical landmark detection?, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 20606--20616.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford et al. [2021] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al., 2021.
    Learning transferable visual models from natural language supervision, in: International
    conference on machine learning, PMLR. pp. 8748--8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rädsch et al. [2023] Rädsch, T., Reinke, A., Weru, V., Tizabi, M.D., Schreck,
    N., Kavur, A.E., Pekdemir, B., Roß, T., Kopp-Schneider, A., Maier-Hein, L., 2023.
    Labelling instructions matter in biomedical image analysis. Nature Machine Intelligence
    5, 273--283.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajpurkar et al. [2022] Rajpurkar, P., Chen, E., Banerjee, O., Topol, E.J.,
    2022. Ai in health and medicine. Nature medicine 28, 31--38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rangwani et al. [2021] Rangwani, H., Jain, A., Aithal, S.K., Babu, R.V., 2021.
    S3vaada: Submodular subset selection for virtual adversarial active domain adaptation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
    7516--7525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. [2021] Ren, P., Xiao, Y., Chang, X., Huang, P.Y., Li, Z., Gupta,
    B.B., Chen, X., Wang, X., 2021. A survey of deep active learning. ACM computing
    surveys (CSUR) 54, 1--40.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rombach et al. [2022] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer,
    B., 2022. High-resolution image synthesis with latent diffusion models, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684--10695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roth and Small [2006] Roth, D., Small, K., 2006. Margin-based active learning
    for structured output spaces, in: Hutchison, D., Kanade, T., Kittler, J., Kleinberg,
    J.M., Mattern, F., Mitchell, J.C., Naor, M., Nierstrasz, O., Pandu Rangan, C.,
    Steffen, B., Sudan, M., Terzopoulos, D., Tygar, D., Vardi, M.Y., Weikum, G., Fürnkranz,
    J., Scheffer, T., Spiliopoulou, M. (Eds.), Machine Learning: ECML 2006. Springer
    Berlin Heidelberg, Berlin, Heidelberg. volume 4212, pp. 413--424. doi:[10.1007/11871842_40](http://dx.doi.org/10.1007/11871842_40).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sadafi et al. [2019] Sadafi, A., Koehler, N., Makhro, A., Bogdanova, A., Navab,
    N., Marr, C., Peng, T., 2019. Multiclass deep active learning for detecting red
    blood cell subtypes in brightfield microscopy, in: Shen, D., Liu, T., Peters,
    T.M., Staib, L.H., Essert, C., Zhou, S., Yap, P.T., Khan, A. (Eds.), Medical Image
    Computing and Computer Assisted Intervention – MICCAI 2019, Springer International
    Publishing, Cham. pp. 685--693. doi:[10.1007/978-3-030-32239-7_76](http://dx.doi.org/10.1007/978-3-030-32239-7_76).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sadafi et al. [2023] Sadafi, A., Navab, N., Marr, C., 2023. Active learning
    enhances classification of histopathology whole slide images with attention-based
    multiple instance learning, in: 2023 IEEE 20th International Symposium on Biomedical
    Imaging (ISBI), pp. 1--5. doi:[10.1109/ISBI53787.2023.10230685](http://dx.doi.org/10.1109/ISBI53787.2023.10230685).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saquil et al. [2018] Saquil, Y., Kim, K.I., Hall, P., 2018. Ranking cgans:
    Subjective control over semantic image attributes. arXiv preprint arXiv:1804.04082
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener and Savarese [2018] Sener, O., Savarese, S., 2018. Active learning for
    convolutional neural networks: A core-set approach, in: International Conference
    on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensoy et al. [2018] Sensoy, M., Kaplan, L., Kandemir, M., 2018. Evidential
    deep learning to quantify classification uncertainty. Advances in neural information
    processing systems 31.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles [2009] Settles, B., 2009. Active learning literature survey .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seung et al. [1992] Seung, H.S., Opper, M., Sompolinsky, H., 1992. Query by
    committee, in: Proceedings of the Fifth Annual Workshop on Computational Learning
    Theory, Association for Computing Machinery, New York, NY, USA. pp. 287--294.
    doi:[10.1145/130385.130417](http://dx.doi.org/10.1145/130385.130417).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaham et al. [2019] Shaham, T.R., Dekel, T., Michaeli, T., 2019. Singan: Learning
    a generative model from a single natural image, in: Proceedings of the IEEE/CVF
    international conference on computer vision, pp. 4570--4580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. [2020] Shen, H., Tian, K., Dong, P., Zhang, J., Yan, K., Che, S.,
    Yao, J., Luo, P., Han, X., 2020. Deep active learning for breast cancer segmentation
    on immunohistochemistry images, in: Martel, A.L., Abolmaesumi, P., Stoyanov, D.,
    Mateus, D., Zuluaga, M.A., Zhou, S.K., Racoceanu, D., Joskowicz, L. (Eds.), Medical
    Image Computing and Computer Assisted Intervention – MICCAI 2020, Springer International
    Publishing, Cham. pp. 509--518. doi:[10.1007/978-3-030-59722-1_49](http://dx.doi.org/10.1007/978-3-030-59722-1_49).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. [2021] Shin, I., Kim, D.J., Cho, J.W., Woo, S., Park, K., Kweon,
    I.S., 2021. Labor: Labeling only if required for domain adaptive semantic segmentation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
    8588--8598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shui et al. [2020] Shui, C., Zhou, F., Gagné, C., Wang, B., 2020. Deep active
    learning: Unified and principled method for query and training, in: Proceedings
    of the Twenty Third International Conference on Artificial Intelligence and Statistics,
    PMLR. pp. 1308--1318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siddiqui et al. [2020] Siddiqui, Y., Valentin, J., Niessner, M., 2020. Viewal:
    Active learning with viewpoint entropy for semantic segmentation, in: 2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Seattle, WA,
    USA. pp. 9430--9440. doi:[10.1109/CVPR42600.2020.00945](http://dx.doi.org/10.1109/CVPR42600.2020.00945).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sim et al. [2020] Sim, Y., Chung, M.J., Kotter, E., Yune, S., Kim, M., Do, S.,
    Han, K., Kim, H., Yang, S., Lee, D.J., et al., 2020. Deep convolutional neural
    network--based software improves radiologist detection of malignant lung nodules
    on chest radiographs. Radiology 294, 199--209.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sinha et al. [2019] Sinha, S., Ebrahimi, S., Darrell, T., 2019. Variational
    adversarial active learning, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 5972--5981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sourati et al. [2017] Sourati, J., Akcakaya, M., Leen, T.K., Erdogmus, D., Dy,
    J.G., 2017. Asymptotic analysis of objectives based on fisher information in active
    learning. The Journal of Machine Learning Research 18, 1123--1163.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sourati et al. [2018] Sourati, J., Gholipour, A., Dy, J.G., Kurugol, S., Warfield,
    S.K., 2018. Active deep learning with fisher information for patch-wise semantic
    segmentation, in: Stoyanov, D., Taylor, Z., Carneiro, G., Syeda-Mahmood, T., Martel,
    A., Maier-Hein, L., Tavares, J.M.R., Bradley, A., Papa, J.P., Belagiannis, V.,
    Nascimento, J.C., Lu, Z., Conjeti, S., Moradi, M., Greenspan, H., Madabhushi,
    A. (Eds.), Deep Learning in Medical Image Analysis and Multimodal Learning for
    Clinical Decision Support. Springer International Publishing, Cham. volume 11045,
    pp. 83--91. doi:[10.1007/978-3-030-00889-5_10](http://dx.doi.org/10.1007/978-3-030-00889-5_10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sourati et al. [2019] Sourati, J., Gholipour, A., Dy, J.G., Tomas-Fernandez,
    X., Kurugol, S., Warfield, S.K., 2019. Intelligent labeling based on fisher information
    for medical image segmentation using deep learning. IEEE Transactions on Medical
    Imaging 38, 2642--2653. doi:[10.1109/TMI.2019.2907805](http://dx.doi.org/10.1109/TMI.2019.2907805).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. [2020] Su, J.C., Tsai, Y.H., Sohn, K., Liu, B., Maji, S., Chandraker,
    M., 2020. Active adversarial domain adaptation, in: Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, pp. 739--748.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2023] Sun, S., Zhi, S., Heikkilä, J., Liu, L., 2023. Evidential
    uncertainty and diversity guided active learning for scene graph generation, in:
    The Eleventh International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tajbakhsh et al. [2020] Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N.,
    Wu, Z., Ding, X., 2020. Embracing imperfect datasets: A review of deep learning
    solutions for medical image segmentation. Medical Image Analysis 63, 101693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takezoe et al. [2023] Takezoe, R., Liu, X., Mao, S., Chen, M.T., Feng, Z.,
    Zhang, S., Wang, X., et al., 2023. Deep active learning for computer vision: Past
    and future. APSIPA Transactions on Signal and Information Processing 12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2022] Tang, C., Xie, L., Zhang, G., Zhang, X., Tian, Q., Hu, X.,
    2022. Active pointly-supervised instance segmentation, in: European Conference
    on Computer Vision, Springer. pp. 606--623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarvainen and Valpola [2017] Tarvainen, A., Valpola, H., 2017. Mean teachers
    are better role models: Weight-averaged consistency targets improve semi-supervised
    deep learning results. Advances in neural information processing systems 30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolkach et al. [2020] Tolkach, Y., Dohmgörgen, T., Toma, M., Kristiansen, G.,
    2020. High-accuracy prostate cancer pathology using deep learning. Nature Machine
    Intelligence 2, 411--418.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. [2019] Tran, T., Do, T.T., Reid, I., Carneiro, G., 2019. Bayesian
    generative active deep learning, in: Proceedings of the 36th International Conference
    on Machine Learning, PMLR. pp. 6295--6304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tschandl et al. [2020] Tschandl, P., Rinner, C., Apalla, Z., Argenziano, G.,
    Codella, N., Halpern, A., Janda, M., Lallas, A., Longo, C., Malvehy, J., et al.,
    2020. Human--computer collaboration for skin cancer recognition. Nature Medicine
    26, 1229--1234.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unnikrishnan et al. [2021] Unnikrishnan, B., Nguyen, C., Balaram, S., Li, C.,
    Foo, C.S., Krishnaswamy, P., 2021. Semi-supervised classification of radiology
    images with noteacher: A teacher that is not mean. Medical Image Analysis 73,
    102148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vo et al. [2022] Vo, H.V., Siméoni, O., Gidaris, S., Bursuc, A., Pérez, P.,
    Ponce, J., 2022. Active learning strategies for weakly-supervised object detection,
    in: European Conference on Computer Vision, Springer. pp. 211--230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. [2023] Wan, F., Ye, Q., Yuan, T., Xu, S., Liu, J., Ji, X., Huang,
    Q., 2023. Multiple instance differentiation learning for active object detection.
    IEEE Transactions on Pattern Analysis and Machine Intelligence , 1--15doi:[10.1109/TPAMI.2023.3277738](http://dx.doi.org/10.1109/TPAMI.2023.3277738).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2022a] Wang, C., Shang, K., Zhang, H., Zhao, S., Liang, D., Zhou,
    S.K., 2022a. Active ct reconstruction with a learned sampling policy. arXiv preprint
    arXiv:2211.01670 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020a] Wang, J., Yan, Y., Zhang, Y., Cao, G., Yang, M., Ng, M.K.,
    2020a. Deep reinforcement active learning for medical image classification, in:
    Martel, A.L., Abolmaesumi, P., Stoyanov, D., Mateus, D., Zuluaga, M.A., Zhou,
    S.K., Racoceanu, D., Joskowicz, L. (Eds.), Medical Image Computing and Computer
    Assisted Intervention – MICCAI 2020, Springer International Publishing, Cham.
    pp. 33--42. doi:[10.1007/978-3-030-59710-8_4](http://dx.doi.org/10.1007/978-3-030-59710-8_4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2017] Wang, K., Zhang, D., Li, Y., Zhang, R., Lin, L., 2017. Cost-effective
    active learning for deep image classification. IEEE Transactions on Circuits and
    Systems for Video Technology 27, 2591--2600. doi:[10.1109/TCSVT.2016.2589879](http://dx.doi.org/10.1109/TCSVT.2016.2589879).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] Wang, P., Xiao, X., Glissen Brown, J.R., Berzin, T.M., Tu,
    M., Xiong, F., Hu, X., Liu, P., Song, Y., Zhang, D., et al., 2018. Development
    and validation of a deep-learning algorithm for the detection of polyps during
    colonoscopy. Nature biomedical engineering 2, 741--748.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020b] Wang, S., Li, Y., Ma, K., Ma, R., Guan, H., Zheng, Y.,
    2020b. Dual adversarial network for deep active learning, in: Vedaldi, A., Bischof,
    H., Brox, T., Frahm, J.M. (Eds.), Computer Vision – ECCV 2020, Springer International
    Publishing, Cham. pp. 680--696. doi:[10.1007/978-3-030-58586-0_40](http://dx.doi.org/10.1007/978-3-030-58586-0_40).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020c] Wang, S., Tarroni, G., Qin, C., Mo, Y., Dai, C., Chen,
    C., Glocker, B., Guo, Y., Rueckert, D., Bai, W., 2020c. Deep generative model-based
    quality control for cardiac mri segmentation, in: Medical Image Computing and
    Computer Assisted Intervention--MICCAI 2020: 23rd International Conference, Lima,
    Peru, October 4--8, 2020, Proceedings, Part IV 23, Springer. pp. 88--97.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2022b] Wang, T., Li, X., Yang, P., Hu, G., Zeng, X., Huang, S.,
    Xu, C.Z., Xu, M., 2022b. Boosting active learning via improving test performance.
    Proceedings of the AAAI Conference on Artificial Intelligence 36, 8566--8574.
    doi:[10.1609/aaai.v36i8.20834](http://dx.doi.org/10.1609/aaai.v36i8.20834).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022c] Wang, X., Lian, L., Yu, S.X., 2022c. Unsupervised selective
    labeling for more effective semi-supervised learning, in: European Conference
    on Computer Vision, Springer. pp. 427--445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2021] Wang, Y., Huang, G., Song, S., Pan, X., Xia, Y., Wu, C.,
    2021. Regularizing deep networks with semantic data augmentation. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 44, 3733--3748.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Yin [2021] Wang, Z., Yin, Z., 2021. Annotation-efficient cell counting,
    in: de Bruijne, M., Cattin, P.C., Cotin, S., Padoy, N., Speidel, S., Zheng, Y.,
    Essert, C. (Eds.), Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2021, Springer International Publishing, Cham. pp. 405--414. doi:[10.1007/978-3-030-87237-3_39](http://dx.doi.org/10.1007/978-3-030-87237-3_39).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2015] Wei, K., Iyer, R., Bilmes, J., 2015. Submodularity in data
    subset selection and active learning, in: Proceedings of the 32nd International
    Conference on Machine Learning, PMLR. pp. 1954--1963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams [1992] Williams, R.J., 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. Machine learning 8, 229--256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2022a] Wu, J., Chen, J., Huang, D., 2022a. Entropy-based active
    learning for object detection with progressive diversity constraint, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9397--9406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2022b] Wu, T.H., Liou, Y.S., Yuan, S.J., Lee, H.Y., Chen, T.I.,
    Huang, K.C., Hsu, W.H., 2022b. D2ada: Dynamic density-aware active domain adaptation
    for semantic segmentation, in: European Conference on Computer Vision, Springer.
    pp. 449--467.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2021a] Wu, T.H., Liu, Y.C., Huang, Y.K., Lee, H.Y., Su, H.T., Huang,
    P.C., Hsu, W.H., 2021a. Redal: Region-based and diversity-aware active learning
    for point cloud semantic segmentation, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp. 15510--15519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2021b] Wu, X., Chen, C., Zhong, M., Wang, J., Shi, J., 2021b. Covid-al:
    The diagnosis of covid-19 with deep active learning. Medical Image Analysis 68,
    101913. doi:[10.1016/j.media.2020.101913](http://dx.doi.org/10.1016/j.media.2020.101913).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2022c] Wu, Y., Zheng, B., Chen, J., Chen, D.Z., Wu, J., 2022c. Self-learning
    and one-shot learning based single-slice annotation for 3d medical image segmentation,
    in: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (Eds.), Medical Image
    Computing and Computer Assisted Intervention – MICCAI 2022, Springer Nature Switzerland,
    Cham. pp. 244--254. doi:[10.1007/978-3-031-16452-1_24](http://dx.doi.org/10.1007/978-3-031-16452-1_24).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2022a] Xie, B., Yuan, L., Li, S., Liu, C.H., Cheng, X., 2022a.
    Towards fewer annotations: Active learning via region impurity and prediction
    uncertainty for domain adaptive semantic segmentation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8068--8078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2022b] Xie, B., Yuan, L., Li, S., Liu, C.H., Cheng, X., Wang, G.,
    2022b. Active learning for domain adaptation: An energy-based approach, in: Proceedings
    of the AAAI Conference on Artificial Intelligence, pp. 8708--8716. doi:[10.1609/aaai.v36i8.20850](http://dx.doi.org/10.1609/aaai.v36i8.20850).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2022c] Xie, M., Li, S., Zhang, R., Liu, C.H., 2022c. Dirichlet-based
    uncertainty calibration for active domain adaptation, in: The Eleventh International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2022d] Xie, M., Li, Y., Wang, Y., Luo, Z., Gan, Z., Sun, Z., Chi,
    M., Wang, C., Wang, P., 2022d. Learning distinctive margin toward active domain
    adaptation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 7993--8002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2023a] Xie, Y., Ding, M., Tomizuka, M., Zhan, W., 2023a. Towards
    free data selection with general-purpose models, in: Thirty-seventh Conference
    on Neural Information Processing Systems. URL: [https://openreview.net/forum?id=KBXcDAaZE7](https://openreview.net/forum?id=KBXcDAaZE7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2023b] Xie, Y., Lu, H., Yan, J., Yang, X., Tomizuka, M., Zhan,
    W., 2023b. Active finetuning: Exploiting annotation budget in the pretraining-finetuning
    paradigm, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 23715--23724.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2018] Xu, X., Lu, Q., Yang, L., Hu, S., Chen, D., Hu, Y., Shi, Y.,
    2018. Quantization of fully convolutional networks for accurate biomedical image
    segmentation, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 8300--8308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021] Xu, Y., Xu, X., Jin, L., Gao, S., Goh, R.S.M., Ting, D.S.W.,
    Liu, Y., 2021. Partially-supervised learning for vessel segmentation in ocular
    images, in: de Bruijne, M., Cattin, P.C., Cotin, S., Padoy, N., Speidel, S., Zheng,
    Y., Essert, C. (Eds.), Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2021, Springer International Publishing, Cham. pp. 271--281. doi:[10.1007/978-3-030-87193-2_26](http://dx.doi.org/10.1007/978-3-030-87193-2_26).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2017] Yang, L., Zhang, Y., Chen, J., Zhang, S., Chen, D.Z., 2017.
    Suggestive annotation: A deep active learning framework for biomedical image segmentation,
    in: Descoteaux, M., Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne,
    S. (Eds.), Medical Image Computing and Computer Assisted Intervention - MICCAI
    2017, Springer International Publishing, Cham. pp. 399--407. doi:[10.1007/978-3-319-66179-7_46](http://dx.doi.org/10.1007/978-3-319-66179-7_46).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yehuda et al. [2022] Yehuda, O., Dekel, A., Hacohen, G., Weinshall, D., 2022.
    Active learning through a covering lens, in: Advances in Neural Information Processing
    Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. [2022] Yi, J.S.K., Seo, M., Park, J., Choi, D.G., 2022. Pt4al: Using
    self-supervised pretext tasks for active learning, in: Avidan, S., Brostow, G.,
    Cissé, M., Farinella, G.M., Hassner, T. (Eds.), Computer Vision – ECCV 2022, Springer
    Nature Switzerland, Cham. pp. 596--612. doi:[10.1007/978-3-031-19809-0_34](http://dx.doi.org/10.1007/978-3-031-19809-0_34).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yoo and Kweon [2019] Yoo, D., Kweon, I.S., 2019. Learning loss for active learning,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 93--102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2023] Yuan, J., Zhang, B., Yan, X., Chen, T., Shi, B., Li, Y.,
    Qiao, Y., 2023. Bi3d: Bi-domain active learning for cross-domain 3d object detection,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 15599--15608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2020] Yuan, M., Lin, H.T., Boyd-Graber, J., 2020. Cold-start active
    learning through self-supervised language modeling, in: Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP), Association
    for Computational Linguistics, Online. pp. 7935--7948. doi:[10.18653/v1/2020.emnlp-main.637](http://dx.doi.org/10.18653/v1/2020.emnlp-main.637).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2021] Yuan, T., Wan, F., Fu, M., Liu, J., Xu, S., Ji, X., Ye,
    Q., 2021. Multiple instance active learning for object detection, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5330--5339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhan et al. [2022] Zhan, X., Wang, Q., Huang, K.h., Xiong, H., Dou, D., Chan,
    A.B., 2022. A comparative survey of deep active learning. arXiv preprint arXiv:2203.13450
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, B., Li, L., Yang, S., Wang, S., Zha, Z.J., Huang,
    Q., 2020. State-relabeling adversarial active learning, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8756--8765.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Zhang, B., Wang, Y., Hou, W., Wu, H., Wang, J., Okumura,
    M., Shinozaki, T., 2021. Flexmatch: Boosting semi-supervised learning with curriculum
    pseudo labeling. Advances in Neural Information Processing Systems 34, 18408--18419.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2016] Zhang, R., Isola, P., Efros, A.A., 2016. Colorful image
    colorization, in: Computer Vision--ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11-14, 2016, Proceedings, Part III 14, Springer. pp.
    649--666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022a] Zhang, W., Zhu, L., Hallinan, J., Zhang, S., Makmur, A.,
    Cai, Q., Ooi, B.C., 2022a. Boostmis: Boosting medical image semi-supervised learning
    with adaptive pseudo labeling and informative active annotation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20666--20676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Zhang, Y., Kang, B., Hooi, B., Yan, S., Feng, J., 2023.
    Deep long-tailed learning: A survey. IEEE Transactions on Pattern Analysis and
    Machine Intelligence .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022b] Zhang, Y., Zhang, X., Xie, L., Li, J., Qiu, R.C., Hu,
    H., Tian, Q., 2022b. One-bit active query with contrastive pairs, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9697--9705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Zhang, Z., Romero, A., Muckley, M.J., Vincent, P., Yang,
    L., Drozdzal, M., 2019. Reducing uncertainty in undersampled mri reconstruction
    with active acquisition, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), IEEE, Long Beach, CA, USA. pp. 2049--2053. doi:[10.1109/CVPR.2019.00215](http://dx.doi.org/10.1109/CVPR.2019.00215).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2021] Zhao, Z., Zeng, Z., Xu, K., Chen, C., Guan, C., 2021. Dsal:
    Deeply supervised active learning from strong and weak labelers for biomedical
    image segmentation. IEEE Journal of Biomedical and Health Informatics 25, 3744--3751.
    doi:[10.1109/JBHI.2021.3052320](http://dx.doi.org/10.1109/JBHI.2021.3052320).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] Zheng, H., Yang, L., Chen, J., Han, J., Zhang, Y., Liang,
    P., Zhao, Z., Wang, C., Chen, D.Z., 2019. Biomedical image segmentation via representative
    annotation. Proceedings of the AAAI Conference on Artificial Intelligence 33,
    5901--5908. doi:[10.1609/aaai.v33i01.33015901](http://dx.doi.org/10.1609/aaai.v33i01.33015901).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2020] Zheng, H., Zhang, Y., Yang, L., Wang, C., Chen, D.Z., 2020.
    An annotation sparsification strategy for 3d medical image segmentation via representative
    selection and self-training. Proceedings of the AAAI Conference on Artificial
    Intelligence 34, 6925--6932. doi:[10.1609/aaai.v34i04.6175](http://dx.doi.org/10.1609/aaai.v34i04.6175).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2016] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba,
    A., 2016. Learning deep features for discriminative localization, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2921--2929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2021a] Zhou, T., Li, L., Bredell, G., Li, J., Konukoglu, E., 2021a.
    Quality-aware memory network for interactive volumetric image segmentation, in:
    de Bruijne, M., Cattin, P.C., Cotin, S., Padoy, N., Speidel, S., Zheng, Y., Essert,
    C. (Eds.), Medical Image Computing and Computer Assisted Intervention – MICCAI
    2021, Springer International Publishing, Cham. pp. 560--570. doi:[10.1007/978-3-030-87196-3_52](http://dx.doi.org/10.1007/978-3-030-87196-3_52).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2022] Zhou, T., Li, L., Bredell, G., Li, J., Konukoglu, E., 2022.
    Volumetric memory network for interactive medical image segmentation. Medical
    Image Analysis , 102599doi:[10.1016/j.media.2022.102599](http://dx.doi.org/10.1016/j.media.2022.102599).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2017] Zhou, Z., Shin, J., Zhang, L., Gurudu, S., Gotway, M., Liang,
    J., 2017. Fine-tuning convolutional neural networks for biomedical image analysis:
    Actively and incrementally, in: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 7340--7351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2021b] Zhou, Z., Shin, J.Y., Gurudu, S.R., Gotway, M.B., Liang,
    J., 2021b. Active, continual fine tuning of convolutional neural networks for
    reducing annotation efforts. Medical Image Analysis 71, 101997. doi:[10.1016/j.media.2021.101997](http://dx.doi.org/10.1016/j.media.2021.101997).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Bento [2017] Zhu, J.J., Bento, J., 2017. Generative adversarial active
    learning. arXiv preprint arXiv:1702.07956 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
