- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:42:10'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2301.12416] Deep Learning for Human Parsing: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.12416](https://ar5iv.labs.arxiv.org/html/2301.12416)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Human Parsing: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Xiaomei Zhang, Xiangyu Zhu, , Ming Tang, , and Zhen Lei, Xiaomei Zhang, Xiangyu
    Zhu and Ming Tang are with the National Laboratory of Pattern Recognition, Institute
    of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with
    the School of Artificial Intelligence, University of Chinese Academy of Sciences,
    Beijing 100049, China (e-mail: xiaomei.zhang@nlpr.ia.ac.cn; xiangyu.zhu.chen@nlpr.ia.ac.cn;
    tangm@nlpr.ia.ac.cn). Zhen Lei is with the National Laboratory of Pattern Recognition
    (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing
    100190, China, also with the School of Artificial Intelligence, University of
    Chinese Academy of Sciences (UCAS), Beijing 100190, China, and also with the Centre
    for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation,
    Chinese Academy of Sciences, Hong Kong, China (e-mail: zlei@nlpr.ia.ac.cn).Manuscript
    received Jan xx, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Human parsing is a key topic in image processing with many applications, such
    as surveillance analysis, human-robot interaction, person search, and clothing
    category classification, among many others. Recently, due to the success of deep
    learning in computer vision, there are a number of works aimed at developing human
    parsing algorithms using deep learning models. As methods have been proposed,
    a comprehensive survey of this topic is of great importance. In this survey, we
    provide an analysis of state-of-the-art human parsing methods, covering a broad
    spectrum of pioneering works for semantic human parsing. We introduce five insightful
    categories: (1) structure-driven architectures exploit the relationship of different
    human parts and the inherent hierarchical structure of a human body, (2) graph-based
    networks capture the global information to achieve an efficient and complete human
    body analysis, (3) context-aware networks explore useful contexts across all pixel
    to characterize a pixel of the corresponding class, (4) LSTM-based methods can
    combine short-distance and long-distance spatial dependencies to better exploit
    abundant local and global contexts, and (5) combined auxiliary information approaches
    use related tasks or supervision to improve network performance. We also discuss
    the advantages/disadvantages of the methods in each category and the relationships
    between methods in different categories, examine the most widely used datasets,
    report performances, and discuss promising future research directions in this
    area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Human parsing, deep learning, structure-driven architecture, graph-based network,
    context-aware network, LSTM-based method, combined auxiliary information approach.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human parsing is an essential component in many visual understanding systems.
    It aims to assign human images into multiple human parts of fine-grained semantics
    and benefits a detailed understanding of images, some examples are shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning for Human Parsing: A Survey"). Human
    parsing plays a central role in a broad range of applications, including human-friendly
    robots [[1](#bib.bib1)], person re-identification [[2](#bib.bib2)], human behavior
    analysis [[3](#bib.bib3)], clothing style recognition and retrieval [[4](#bib.bib4)],
    to name a few. Many human parsing algorithms have been developed in the literature,
    in the early stage, such as Markov random field [[5](#bib.bib5)], support vector
    machines [[6](#bib.bib6)], cascaded pictorial structures model [[7](#bib.bib7)],
    part-based tree [[8](#bib.bib8)], quadratic deformation cost [[9](#bib.bib9)],
    binary random variable [[10](#bib.bib10)]. Over the past few years, deep learning
    for human parsing achieves remarkable performance improvements, especially, FCNet [[11](#bib.bib11)]
    was proposed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ad2e44d267233d8741077d3276d21e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The goal of human parsing is to assign human images into multiple
    human parts of fine-grained semantics and benefit a detailed understanding of
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, our motivation is to focus on the recent works in human parsing
    and discuss deep learning-based parsing 2D methods proposed until 2022\. We provide
    a comprehensive review and insight on different aspects of these methods, including
    network architectures, training data, main contributions, and their limitations.
    Deep learning-based human parsing methods can be divided into the following categories
    based on their main technical contributions: (1) structure-driven architectures,
    (2) graph-based networks, (3) context-aware networks, (4) LSTM-based methods,
    (5) combined auxiliary information approaches contain pose-based auxiliary methods,
    edge-based auxiliary methods and detection-based auxiliary methods, and (6) other
    models. Additionally, we review some of the most popular human parsing datasets
    and their performance measures used for evaluating the methods. In the end, we
    discuss several potential future directions and applications for deep learning-based
    human parsing methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key contributions of our survey can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our survey focuses on the recent papers with respect to human parsing, and overviews
    deep learning-based human parsing algorithms proposed until 2022.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This survey provides insight to human parsing methods, including network architectures,
    training data, main contributions, and their limitations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We review popular human parsing datasets and provide a comparative summary of
    the properties and performance of the reviewed methods for parsing purposes, on
    popular benchmarks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss several potential future directions and applications for deep learning-based
    human parsing methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE I: Taxonomy of Head Pose Estimation Approaches'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Highlights | Representative Works |'
  prefs: []
  type: TYPE_TB
- en: '| Structure-driven Architectures | Exploiting the relationship of different
    human parts and the inherent hierarchical structure of a human body. | A-AOG [[12](#bib.bib12)],
    PCNet *et al*. [[13](#bib.bib13)] |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based Networks | Capturing the global information to achieve an efficient
    and complete human body analysis. | Graphonomy [[14](#bib.bib14), [15](#bib.bib15)],CNIF [[16](#bib.bib16)],HHP [[17](#bib.bib17)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Context-aware Networks | Exploring useful contexts across all pixel to characterize
    a pixel of the corresponding class. | ATR [[18](#bib.bib18)], M-CNN [[19](#bib.bib19)],
    Co-CNN [[20](#bib.bib20)], SCHP [[21](#bib.bib21)] |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM-based Methods | Combining short-distance and long-distance spatial dependencies
    to better exploit abundant local and global contexts. | LG-LSTM [[22](#bib.bib22)],
    Graph LSTM [[23](#bib.bib23)], structure-evolving LSTM [[24](#bib.bib24)] |'
  prefs: []
  type: TYPE_TB
- en: '| Combined Auxiliary Information Approaches | Using related tasks or supervision
    to improve network performance. | Pose-based (JMPE [[25](#bib.bib25)], SSL [[26](#bib.bib26)],
    MuLA [[27](#bib.bib27)].), Edge-based ( CorrPM [[28](#bib.bib28)], CE2P [[29](#bib.bib29)]),
    Detection-based (HAZN [[30](#bib.bib30)], Li *et al*. [[31](#bib.bib31)]) |'
  prefs: []
  type: TYPE_TB
- en: 'The remainder of the paper is organized as follows: Section 2 provides a comprehensive
    overview of deep learning-based human parsing methods according to their main
    technical contributions. We also discuss their strengths and limitation. Section
    3 introduces some of the most popular human parsing datasets and evaluation metrics.
    Section 4 discusses several applications and potential future directions. Finally,
    conclusions are given in Section 5.'
  prefs: []
  type: TYPE_NORMAL
- en: II Deep Learning-Based Human Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides a detailed review of deep learning-based human parsing
    methods proposed until 2022\. We grouped these methods into five categories based
    on their model architecture. Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Deep
    Learning for Human Parsing: A Survey") provides a list of representative systems
    for each of these categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d02dbef674aeb3487d4e2e5e66d3de58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Phrase structure grammar is based on the constituency grammar which
    defines the rule to break down a node into its constituent parts. From [[12](#bib.bib12)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Structure-Driven Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The human body is a natural hierarchy, how to use the prior knowledge to segment
    different human parts is an urgent problem to be solved. Park *et al*. [[12](#bib.bib12)]
    present an attribute and-or grammar (A-AOG) model for inferring human parts in
    the hierarchical representation, which also jointly represents the human pose
    and human attribute. A-AOG explicitly represents the decomposition and articulation
    of body parts and accounts for the correlations between parts. The network uses
    phrase structure grammars to represent the hierarchical decomposition of the human
    body from whole to parts, and employs dependency grammars to model the geometric
    articulation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4c468f0cbb05f3cfe01b904f7101b39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Dependency grammar defines adjacency relations that connect the geometry
    of a part to its dependent parts. From [[12](#bib.bib12)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The phrase structure grammar represents human parts in a coarse-to-fine method
    based on the constituency relation. The grammar is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;a\rightarrow a_{1}&#124;a_{2}&#124;a_{3},\\ \end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where a typical nonterminal node $a\in v_{n}$, $a_{i}$ is a string of nodes
    in $v_{n}\cup v_{t}$, $v_{n},v_{t}$ denotes human parts. Fig. [2](#S2.F2 "Figure
    2 ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A
    Survey") shows an example. The root node is the upper body and decomposed into
    arms, head, and torso. The arms are further decomposed into upper-arm, lower-arm,
    and hand. Dependency grammars have been widely used in natural language processing
    for syntactic parsing. Fig. [3](#S2.F3 "Figure 3 ‣ II-A Structure-Driven Architectures
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey")
    is a parsing grammar for the upper body. The root node is the torso part as it
    is the center of the body and connected to other parts. The upper arms and head
    are the child nodes of the torso.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7be8fd55da31491a17f8d548b0ef47bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The framework of PCNet. From [[13](#bib.bib13)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhu *et al*. [[13](#bib.bib13)] propose a progressive cognitive structure to
    segment human parts. In the hierarchical network, the latter layers inherit information
    from former layers and pay attention to a finer component. As shown in Fig. [4](#S2.F4
    "Figure 4 ‣ II-A Structure-Driven Architectures ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey"), the given image is sent
    into a FCN to extract original features. And then the image-level (original) features
    are decomposed into a background score map and human-level features. The human-level
    features are further decomposed into upper-body features and lower-body features,
    repeating the above steps until all are segmented.'
  prefs: []
  type: TYPE_NORMAL
- en: The structure-driven human parsing methods explore the inherent relations of
    human parts according to the prior knowledge of the human body. These methods
    can make the network pay more attention to the interested human body itself and
    reduce the interference of background and redundant information. However, these
    methods are easy to lead to error accumulation. For example, if the root node
    is wrong, the error will be passed to the subsequent nodes, resulting in error
    accumulation.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Graph-Based Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/565d6a586f3f1c1b36514d370ee61f83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The framework of Graphonomy. From [[14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph convolution [[32](#bib.bib32)] can effectively explore the semantic
    relationship among human parts, thus, some works [[14](#bib.bib14), [15](#bib.bib15)]
    introduce the graph convolution into the human parsing task. As shown in Fig. [5](#S2.F5
    "Figure 5 ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey"), Gong *et al*. [[14](#bib.bib14),
    [15](#bib.bib15)] design a graph-based human parsing network, named ”Graphonomy”,
    which employs graph convolution to capture the global information and semantic
    consistency. The image features extracted by the deep convolution network are
    projected into a high-level graph representation, where the body parts are nodes
    and the relationships between the parts are edges. Graphonomy first learns and
    propagates compact high-level graph representation among parts within one dataset
    via intra-graph reasoning, and then transfers semantic information across different
    datasets via inter-graph transfer. In this way, Graphonomy takes advantage of
    different granular annotated data. However, multiple datasets are required in
    the training process, which consumes many computing resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wang *et al*. [[16](#bib.bib16)] propose a method to explore the structural
    hierarchy of the human body by using a graph convolutional network. This method
    achieves an efficient and complete human body analysis, named an information fusion
    framework. This model models three inference processes: direct inference (directly
    predicting each part of a human body using image information), bottom-up inference
    (assembling knowledge from constituent parts), and top-down inference (leveraging
    context from parent nodes).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1cdedef156788b1f7847e50452539dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Illustration of the hierarchical graph for the human parsing task.
    From [[17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, Wang *et al*. [[17](#bib.bib17)] represent a hierarchical graph
    for the human parsing task. There are three kinds of part relations, i.e., decomposition,
    composition, and dependency, which are completely and precisely described by three
    distinct graph networks. This method represents the human semantic structure as
    a directed, hierarchical graph $g=(\upsilon,\varepsilon,y)$. As shown in Fig. [6](#S2.F6
    "Figure 6 ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey"), the node set $\upsilon=\bigcup^{3}_{l=3}\upsilon_{l}$
    denotes human parts in three different semantic levels, including the leaf nodes
    $\upsilon_{1}$, two middle-level nodes $\upsilon_{2}$ and one root $\upsilon_{3}$.
    The edge set $\varepsilon\in(^{\upsilon}_{2})$ represents the relations between
    human parts (nodes), i.e., the directed edge. $y$ represents the ground truth
    maps. Given an input image, a fully convolution network first extracts image features
    projected into node (part) features. Then node features are sent into a hierarchical
    graph to capture expressive relation information and predicted different granularity
    of results. The whole network is trained in graph learning methods and the supervision
    is human parsing datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: In graph-based human parsing networks, the semantic relationships of different
    parts are introduced into the parsing models, which can amend the accumulated
    errors by communicating with each other. In general, the body parts are the nodes
    and the relationship between the parts is the edge. Due to the diversity of the
    scale, occlusion, deformation and posture of human body parts, this kind of method
    is difficult to obtain rich contextual information, which affects the recognition
    ability of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb99379dcd0d8bbd65583f8e9f8eb80f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The framework of the active template regression model. From [[18](#bib.bib18)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Context-Aware Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Benefiting from the development of advanced CNN architectures and training
    techniques as well as the recently released human parsing datasets, context-aware
    methods attract more and more attention. As shown in Fig. [7](#S2.F7 "Figure 7
    ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning
    for Human Parsing: A Survey"), Liang *et al*. [[18](#bib.bib18)] are the first
    to apply a convolutional neural network to the human parsing task, called active
    template regression model. The method is built on the end-to-end relation between
    the input human image and the structure outputs for human parsing. The first CNN
    network (active template network) is with max-pooling and designed to predict
    the template coefficients for each label mask, and the second (active shape network)
    is without max-pooling to preserve sensitivity to label mask position and generate
    the active shape parameters. Given an image, the outputs of the two networks are
    fused to generate the probability of each category for each pixel, and superpixel
    smoothing is finally used to refine the human parsing prediction. Extensive experiments
    demonstrate the significant superiority of the ATR framework over other contemporaneous
    states of the arts for human parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, Liang *et al*. [[19](#bib.bib19)] design a quasi-parametric method,
    which takes advantage of both parametric and non-parametric approaches, namely
    supervision from annotated data and the flexibility to use newly annotated images.
    Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the
    parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict
    segment results. As shown in Fig. [8](#S2.F8 "Figure 8 ‣ II-C Context-Aware Networks
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey"),
    given a testing image, its KNN images are retrieved from the manually-annotated
    image corpus. Then the input image is paired with each semantic region of its
    KNN images and each pair is fed into the M-CNN individually. The M-CNN predicts
    the matching confidence and displacements between the input image pair. All corresponding
    label maps are combined to generate a probability map for each pixel which is
    further refined by superpixel smoothing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/792712ff49ec19a345b7fe5b7e329173.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The architecture of the quasi-parametric network. From [[19](#bib.bib19)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98a66ad801f79de47a12cd6e2cea939c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The framework of Contextualized Convolutional Neural Network. From [[20](#bib.bib20)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'To effectively integrate all kinds of contexts into a unified model in the
    human parsing task, Liang *et al*. [[20](#bib.bib20)] propose a novel Contextualized
    Convolutional Neural Network (Co-CNN). This network integrates the cross-layer
    context, global image-level context, within-super-pixel context and cross-super-pixel
    neighborhood context to obtain rich context, improving the performance of human
    parsing (Fig. [9](#S2.F9 "Figure 9 ‣ II-C Context-Aware Networks ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")). The cross-layer
    context is captured by the basic structure, which hierarchically combines the
    global semantic and local details across different layers. Then, the global image-level
    context serves as an auxiliary object in the intermediate layer and guides the
    subsequent feature learning. Finally, the local super-pixel contexts, the within-super-pixel
    context, and the cross-super-pixel neighborhood context are formulated as natural
    subcomponents to achieve local label consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8db0153e6e1c8dec5a831920d45faabf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Framework of the Self-Correction for Human Parsing. From [[21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Li *et al*. design a noise-tolerant method, named Self-Correction for Human
    Parsing (SCHP) [[21](#bib.bib21)], to progressively promote the reliability of
    the supervised labels as well as the learned models. As shown in Fig. [10](#S2.F10
    "Figure 10 ‣ II-C Context-Aware Networks ‣ II Deep Learning-Based Human Parsing
    ‣ Deep Learning for Human Parsing: A Survey"), SCHP first takes a model trained
    with inaccurate labels as initialization and then uses a cyclical learning scheduler
    to infer more reliable pseudo masks by iteratively aggregating the models in an
    online manner. Besides, the corrected labels in turn boost the model’s performance.
    In this way, the models and the labels will reciprocally become more robust and
    accurate. The human parsing methods based on contexts can obtain rich semantics
    and details, expanding the effective receptive field. However, this approach may
    result in information decay because the network is too deep.'
  prefs: []
  type: TYPE_NORMAL
- en: II-D LSTM-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d2a350760e068f6ea660c562031054e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Architecture of the LG-LSTM network. From [[22](#bib.bib22)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Long short-term memory (LSTM) [[33](#bib.bib33)] uses memory cells to independently
    read, write and forget some information, retaining the main information. It can
    mitigate the decay of information. Thus, Liang *et al*. introduce LSTM into the
    human parsing task. First, Liang *et al*. [[22](#bib.bib22)] propose a novel Local-Global
    Long Short-Term Memory (LG-LSTM) to combine short- and long-distance spatial dependencies
    (Fig. [11](#S2.F11 "Figure 11 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")). LG-LSTM layer obtains
    local guidance from neighboring positions and global guidance from the whole image
    and then imposes on each position to better exploit abundant local and global
    contexts. Given an input image, the backbone obtains its original features. Then,
    these features are sent into the transition layer and several stacked LG-LSTM
    layers to improve the ability to feature. The feed-forward convolutional layers
    are appended to generate the prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, Liang *et al*. further design Graph Long Short-Term Memory (Graph LSTM) [[23](#bib.bib23)]
    to improve LSTM, which is the generalization of LSTM from sequential data or multi-dimensional
    data to general graph-structured data. Graph LSTM takes each arbitrary-shaped
    super-pixel as a semantically consistent node, and adaptively constructs an undirected
    graph for each image, where the spatial relations of the super-pixels are naturally
    used as edges. The node updating sequence for Graph LSTM layers is determined
    by the confidence-drive scheme, and then Graph LSTM layers can sequentially update
    the hidden states of all super-pixel nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e45b351f7300e40eaa35724db4cb3e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An illustration of the structure-evolving LSTM. From [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In another work, Liang *et al*. propose to learn the intermediate interpretable
    multi-level graph structures in a progressive way, named structure-evolving LSTM [[24](#bib.bib24)]
    (Fig. [12](#S2.F12 "Figure 12 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")). In each LSTM layer,
    we estimate the compatibility of two connected nodes from their corresponding
    LSTM gate outputs, which is used to generate a merging probability. Then, the
    candidate graph structures are accordingly generated where the nodes are grouped
    into cliques with their merging probabilities. The network produces the new graph
    structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting
    stuck in local optimums by stochastic sampling with an acceptance probability.
    Once a graph structure is accepted, a higher-level graph is then constructed by
    taking the partitioned cliques as its nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37c74a716b754d588e46c2935b295378.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Pose-guided human parsing by an and-or graph using pose-context
    features for human parsing. From [[34](#bib.bib34)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-E Combined Auxiliary Information Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human parsing is a pixel-level fine classification task, which requires rich
    and effective feature expression. Combining with human body detection, human posture
    estimation and human body edge information can greatly improve the expression
    ability of features and the performance of human body analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Pose-based Auxiliary Methods. In the computer vision community, human parsing
    and human pose estimation are two complementary tasks. Joints can provide object-level
    shape information for human parsing, and pixel-level analysis can constrain the
    changes of pose position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Xia *et al*. propose a human parsing approach which uses human pose location
    as cues to provide pose-guided segment proposals for semantic parts [[34](#bib.bib34)],
    as shown in Fig. [13](#S2.F13 "Figure 13 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey"). These segment proposals
    are ranked using standard appearance cues, called pose-context. These proposals
    are selected and assembled using an and-or-graph to output a parse of the person,
    which can deal with large human appearance variability. Given a pedestrian image,
    the network first uses a pose estimation to obtain human pose joints. Then, based
    on the joints and modified RIGOR algorithm [[35](#bib.bib35)], segments aligned
    with object boundaries are generated. Human pose joints further obtain pose-context
    feature which combines segments to extract the segment features. The segment proposal
    model selects and ranks the parts. Finally, part assembling generates the paring
    results. Based on this method [[34](#bib.bib34)], Xia *et al*. further improve
    human parsing methods based on human pose estimation and design a network [[25](#bib.bib25)].
    This network trains two fully convolutional neural networks, to optimize both
    tasks simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4e3ad22114e6c3f5a07602851822e34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Framework of Self-supervised Structure-sensitive Learning (SSL)
    for human parsing. From [[26](#bib.bib26)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gong *et al*. develop a novel Self-supervised Structure-sensitive Learning
    (SSL) approach [[26](#bib.bib26)], which imposes human pose structure into parsing
    without resorting to extra supervision (Fig. [14](#S2.F14 "Figure 14 ‣ II-E Combined
    Auxiliary Information Approaches ‣ II Deep Learning-Based Human Parsing ‣ Deep
    Learning for Human Parsing: A Survey")). Note that there is no need to specifically
    label human joints in model training. This framework can be injected into any
    advanced networks to help incorporate rich high-level knowledge from joint and
    improve the parsing results. Given an image, the backbone generates parsing results.
    The generated joints and joints labels are obtained by computing the center points
    of corresponding regions in parsing maps. The structure-sensitives loss is generated
    by weighting segmentation loss with joint structure loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, Nie *et al*. present a Mutual Learning to Adapt model (MuLA) for
    joint human parsing and pose estimation [[27](#bib.bib27)]. MuLA predicts dynamic
    task-specific model parameters via recurrently leveraging guidance information
    from its parallel tasks. Thus MuLA combines the advantages of parsing and pose
    models to provide more powerful representations by incorporating information from
    their counterparts, generating more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: These two tasks can improve each other’s feature representation and enhance
    the accuracy of models. However, the two tasks have different concerns. Human
    pose estimation mainly focuses on global features and cannot provide a unique
    label for each pixel. Therefore, the help to the human parsing task is limited.
    At the same time, the two optimization methods are different, which will affect
    the improvement of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Edge-based Auxiliary Methods. In the image, the low-frequency region is the
    area with similar semantics, while the high-frequency region is usually the area
    with a larger semantic transformation ratio of area block. The edge of the human
    body is used to describe the area of human foreground and background transformation,
    which has regional differentiation. On the basis of Co-CNN [[20](#bib.bib20)],
    Liang *et al* [[36](#bib.bib36)]. add a branch to guide the separation of the
    foreground and the background regions by information on the edge of the human
    body, which improves the recognition ability of the model to different semantic
    regions. An advanced semantic boundary is used to guide pixel-level labeling and
    improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/153916626b253209e1a15c3b39cff4f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Framework of the Correlation Parsing Machine (CorrPM) framework.
    From [[28](#bib.bib28)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhang *et al*. propose a Correlation Parsing Machine (CorrPM) [[28](#bib.bib28)]
    to study how human semantic boundaries and keypoint locations can jointly improve
    human parsing (Fig. [15](#S2.F15 "Figure 15 ‣ II-E Combined Auxiliary Information
    Approaches ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing:
    A Survey")). CorrPM uses a heterogeneous non-local block to discover the spatial
    affinity among feature maps from the edge, pose and parsing. The input images
    go through the backbone to generate features in different stages. Features of
    the second and fifth stages are sent into the paring encoder to obtain coarse
    segmentation maps. Features of the second, third and fourth stages are sent into
    the edge encoder to generate semantic boundaries. Features of the five stages
    are sent into the pose encoder to predict joint location. The heterogeneous non-local
    is appended to explore the correlation among the three factors and generates fine
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: Liu *et al*. [[29](#bib.bib29)] design a simple yet effective context embedding
    with edge perceiving (CE2P) framework, which combines feature resolution, global
    context, and edge details. CE2P brings a high-performance boost to single human
    parsing task and serve as a solid baseline for future research in single/multiple
    human parsing.
  prefs: []
  type: TYPE_NORMAL
- en: The edge label of the human body can be obtained through the edge processing
    (e.g., Canny [[37](#bib.bib37)]) of the parsing label. This kind of natural information
    does not need additional labeling and is cost-effective. But the number of pixels
    occupied by the edges is small compared to the overall image. At present, the
    average intersection ratio is usually used to evaluate the performance of the
    model, and the edge accuracy has little advantage in this calculation method.
    Usually, the effect of the model edge has been greatly improved, but the analytical
    accuracy of human parsing has not been improved. Therefore, edge information cannot
    effectively improve the evaluation value of network performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d6f7f816845ab3e5c9e7d951952597b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Framework of the Hierarchical Auto-Zoom Net (HAZN) framework. From [[30](#bib.bib30)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detection-based Auxiliary Methods. Due to the interference of complex scenes
    and similar backgrounds, it is difficult to extract complete and accurate foreground.
    Detecting the human body can reduce the interference of background information,
    making the network pay more attention to the foreground, and improving the precision
    of network analysis. As shown in Fig. [16](#S2.F16 "Figure 16 ‣ II-E Combined
    Auxiliary Information Approaches ‣ II Deep Learning-Based Human Parsing ‣ Deep
    Learning for Human Parsing: A Survey"), Xia *et al*. [[30](#bib.bib30)] design
    a Hierarchical Auto-Zoom Net (HAZN) for object parsing which adapts to the local
    scales of objects and parts. HAZN is a sequence of two Auto-Zoom Nets (AZNs),
    each one has two tasks: the first is to predict the locations and scales of object
    instances (the first AZN) or their parts (the second AZN); the second is to estimate
    the part scores for predicted object instance or part regions. Given an image,
    its part scores are predicted and refined by three FCNs with three levels of granularity,
    i.e., image-, object-, and part-level. At each level, the FCN outputs the score
    maps and generates the locations and scales for the next level.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/912bd1a4f641784cf83d84c068da177d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The timeline of deep learning-based human parsing algorithms, from
    2015 to 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: Li *et al*. [[31](#bib.bib31)] address the human parsing task by segmenting
    the parts of objects at an instance level, such that each pixel in the image is
    assigned a part label, as well as the identity of the object it belongs to. An
    input image is sent into a human detection network and a body parts semantic segmentation
    network, producing $D$ detections of human and parsing results, respectively.
    These results are used to form the unary potentials of an Instance CRF which performs
    instance segmentation by associating labeled pixels with human detections.
  prefs: []
  type: TYPE_NORMAL
- en: The detection methods can reduce the interference of background information
    and make the network more focused on the human body, but this method also has
    two drawbacks. On the one hand, even if the detection method is completely correct,
    the interference of background information cannot be completely avoided, and human
    parsing methods are still needed to extract the foreground from the background.
    When analyzing the results, the closer the foreground was to the body itself,
    the more disturbing it was. On the other hand, if the detection method is incorrect,
    subsequent human parsing will inherit the error, resulting in error accumulation.
    Therefore, the human body analysis method based on the detection model has some
    drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: II-F Other Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the above methods, there are several other deep learning methods
    for human parsing, such as the following: CDGNet [[38](#bib.bib38)] simplifies
    the complex spatial human parsing problem into the horizontal and vertical positions
    of human parts individually. Accordingly, the method builds the class distribution
    labels in the horizontal and vertical directions as new supervision signals from
    the original label of human parsing. Liu *et al*. [[39](#bib.bib39)] propose a
    Hybrid Resolution Network (HyRN) by adding deconvolution and multi-scale supervision
    on top of the HRNet with minimal extra computation overhead. HyRN aggregates rich
    high-resolution representations to predict more accurate parsing results, especially
    for small components, on which the improvement to HRNet baseline exceeds 4 pp.
    Lin *et al*. [[40](#bib.bib40)] introduce an effective framework, called-domain
    complementary learning with a pose, to leverage information in both real and synthetic
    images for multi-person part segmentation. SYSU-Clothes [[41](#bib.bib41)] parses
    clothes via joint image segmentation and labels. BSANet [[42](#bib.bib42)] proposes
    to address object part parsing in the less explored multi-class setting, and designs
    a unified network architecture to solve this important problem. Grapy-ML [[43](#bib.bib43)]
    proposes a novel graph pyramid module, which enables incorporating the hierarchical
    structural prior explicitly into feature learning via self-attention-based graph
    reasoning and progressive feature refinement. PRM [[44](#bib.bib44)] is designed
    to generate features with adaptive context for various sizes and shapes of human
    parts. HSSN [[45](#bib.bib45)] aims at a structured, pixel-wise description of
    visual observation in terms of a class hierarchy. HIPN [[46](#bib.bib46)] proposes
    a new semi-supervised human parsing method for which the method only needs a small
    number of labels for training. ISNet [[47](#bib.bib47)] proposes to augment the
    pixel representations by aggregating the image-level and semantic-level contextual
    information, respectively. AFLA [[48](#bib.bib48)] proposes a novel and efficient
    cross-domain human parsing model to bridge the cross-domain differences in terms
    of visual appearance and environment conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Instance-level human parsing [[49](#bib.bib49)] is an interesting task, which
    aims to parse multiple human instances in a single pass. There are already several
    interesting works in this direction, including PNG [[49](#bib.bib49)], NAN [[50](#bib.bib50)],
    Holistic [[31](#bib.bib31)], Parsing R-CNN [[51](#bib.bib51)], AIParsing [[52](#bib.bib52)],
    M-CE2P [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: Video human parsing [[53](#bib.bib53), [54](#bib.bib54)] is also another related
    task, which parses every human body in the video data, which can be regarded as
    integrating video segmentation and instance-level human parsing. ATEN [[53](#bib.bib53)]
    first leverages convolutional gated recurrent units to encode temporal feature-level
    changes, and the optical flow of non-key frames is wrapped with the temporal memory
    to generate their features. The following works are TimeCycle [[16](#bib.bib16)],
    UVC [[55](#bib.bib55)], CRW [[56](#bib.bib56)], CLTC [[57](#bib.bib57)], LIIR [[58](#bib.bib58)]
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some milestone human parsing methods are illustrated in Fig. [17](#S2.F17 "Figure
    17 ‣ II-E Combined Auxiliary Information Approaches ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey"). Given the large number
    of works developed in the last few years, we only show some of the most representative
    ones.'
  prefs: []
  type: TYPE_NORMAL
- en: II-G Deep Learning-based Semantic Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We review some of the most prominent deep learning-based semantic segmentation
    methods. In the human parsing community, many works use a semantic segmentation
    model as the image encoder of the methods, and re-train their model from those
    initial weights. This way accelerates the convergence of the network. According
    to their architectures, we divide into four groupings, e.g., fully convolutional
    networks, encoder-decoder based models, multi-scale networks, dilated convolutional
    models and deeplab family. It is noted that there are some parts shared by many
    methods, such as the encoder and decoder process, skip connections, and dilated
    convolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc4d222fd123776abcda37683d50b35c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Fully convolutional networks, which can learn dense, per-pixel dense
    predictions for semantic segmentation. From [[11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fully Convolutional Networks. Fully convolutional networks (FCN) [[11](#bib.bib11)]
    observes that the fully connected layers lose the spatial information and removes
    them from popular architectures (e.g., AlexNet [[59](#bib.bib59)], VGG [[60](#bib.bib60)],
    GoogLeNet [[61](#bib.bib61)]). In this way, FCN includes only convolutional layers,
    which enables it to be applied to an image of any resolution, as shown in Fig. [18](#S2.F18
    "Figure 18 ‣ II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey"). The method results
    in homochronous state-of-the-art results in several image segmentation datasets
    and is considered one of the most influential works in the area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Skip connections link the outputs of nonadjacent layers by summing or concatenating,
    which combines the semantics of deep layers and details of shallow ones to obtain
    accurate segmentation results. FCN adds links that combine the final prediction
    layer with lower layers with finer strides (Fig. [19](#S2.F19 "Figure 19 ‣ II-G
    Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based Human Parsing
    ‣ Deep Learning for Human Parsing: A Survey")). The most widely used architectures
    ’FCN-32s’, ’FCN16s’, and ’FCN8s’ are obtained by the skip connection at different
    layers. More dense skip connections for the same architecture are proposed for
    various applications [[62](#bib.bib62)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/462f495888654a681bf26ff0bb90858f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Skip connections combines semantics of deep layers and details of
    shallow ones to obtain accurate predictions. From [[11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: Although FCN can be trained in an end-to-end manner on any size images, it is
    not suited to deployment onboard mobile or other portable platforms due to its
    large amount of computation. To address this misalignment with more compact and
    efficient models, ENet [[63](#bib.bib63)] is proposed, one of the earliest real-time
    semantic segmentation. Comprising a larger encoder and very simple decoder, ENet
    is built out of several variations of bottleneck residual blocks comprising a
    dimensionality reduction, convolution, and provides similar or better accuracy
    with fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: FCNs are considered revolutionary in many aspects. Since their high-performance
    and easy of deployment, they promote advances in medicine (e.g., brain tumor segmentation [[64](#bib.bib64),
    [65](#bib.bib65)]), driverless driving (e.g., ERFNet [[66](#bib.bib66)]) and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f2c21135092827090a2da132d0df350.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: U-net model (an example for $572\times 572$ resolution of input
    image). From [[67](#bib.bib67)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d70653209983f72c5ec4e5b3f1b85377.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: The SegNet architecture. The decoder upsamples its input using the
    pool indices from the encoder. From [[68](#bib.bib68)].'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-Decoder Based Models. Hierarchical features created by pooling layers
    of FCN can partially lose some localization. Thus, we discuss other popular encoder-decoder
    based models which provide finer predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder-decoder models (as known as the U-nets) consist of two components
    (i.e., the encoder network and the decoder network). The encoder gradually reduces
    the spatial dimension by pooling or convolution layers, while the decoder recovers
    the resolution by upsampling. The decoder uses pooling indices computed in the
    max-pooling step of the corresponding encoder to perform upsampling, and concatenates
    corresponding features of the decoder. U-Net [[67](#bib.bib67)] (Fig. [20](#S2.F20
    "Figure 20 ‣ II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")) and SegNet [[68](#bib.bib68)]
    (Fig. [21](#S2.F21 "Figure 21 ‣ II-G Deep Learning-based Semantic Segmentation
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey"))
    are well-known encoder-decoder algorithms. In this architecture, networks obtain
    rich details and semantics by the skip connections between the encoder and the
    decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular model in this category is a high-resolution network (HRNets) [[69](#bib.bib69),
    [70](#bib.bib70)], as shown in Fig. [22](#S2.F22 "Figure 22 ‣ II-G Deep Learning-based
    Semantic Segmentation ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for
    Human Parsing: A Survey"). Different from U-Net and SegNet, they first reduce
    the spatial dimension and then recover it. HRNet maintains high-resolution representations
    through the encoding process by connecting the high-to-low resolution convolution
    streams in parallel, and repeatedly exchanging the information across resolutions.
    Many recent works on semantic segmentation, detection and facial landmark detection
    use HRNet as the backbone by exploiting contextual models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02f91460350de5543bfe8b387245d457.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: A simple example of a high-resolution network. The main difference
    between HRNetV1 and HRNetV2 lies in the outputs of the final stages. HRNetV1 only
    generates the high-resolution features, while HRNetV2 outputs all resolution features.
    From [[70](#bib.bib70)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c976d889673d33035f6706694338a596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: FPN involves a bottom-up pathway, a top-down pathway, and lateral
    connections. From [[71](#bib.bib71)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Scale and Pyramid Network Based Networks. Multi-scale is an old method
    in image processing, which is employed in many computer vision tasks. One of the
    most prominent models of this sort is the Feature Pyramid Network (FPN) [[71](#bib.bib71)]
    (Fig. [23](#S2.F23 "Figure 23 ‣ II-G Deep Learning-based Semantic Segmentation
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey")),
    which is developed mainly for object detection and also applied to segmentation [[72](#bib.bib72),
    [65](#bib.bib65)]. Many methods use complex network structures to capture the
    inherent multi-scale context. FPN designs a simple and effective method that consists
    of a bottom-up pathway, a top-down pathway, and lateral connections. The concatenated
    feature maps are sent into a $3\times 3$ convolution to generate the output of
    each stage. Finally, each stage of the top-down pathway produces a prediction
    to detect an object.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8204c56fb035d573df9f6647a1b958dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The overview of the PSPNet. The pyramid pooling module extracts
    different sub-regions by adopting varying-size pooling kernels in different strides.
    Upsampling and concatenation are used to form the final pixel-level prediction.
    From [[73](#bib.bib73)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'PSPNet [[73](#bib.bib73)] uses the pyramid pooling module to capture the multi-scale
    global contexts for scene parsing, as shown in Fig. [24](#S2.F24 "Figure 24 ‣
    II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey"). Given an input image, PSPNet
    first uses CNN to extract features, and then sends these features to the pyramid
    pooling model to harvest different scale pattern representations. There are four
    different scales, each scale corresponds to a pyramid level and is followed by
    a $1\times 1$ convolutional layer to reduce dimensions. The outputs of the pyramid
    pooling module are upsampling and concatenating with the original features, which
    can obtain local and global context information. Finally, the representation is
    fed into a convolutional layer to generate the final pixel-level prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Other models are using multi-scale analysis for semantic segmentation, such
    as LRR (Laplacian Pyramid Reconstruction) [[74](#bib.bib74)] designs a Laplacian
    pyramid for semantic segmentation, RefineNet [[75](#bib.bib75)] proposes a multi-path
    refinement network for accurate prediction, and Multi-Scale Context Intertwining
    (MSCI) [[76](#bib.bib76)] employs multi-scale context for segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2aacdedd8f8403c48cb1b3b806a30f8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: An example of dilated convolution. (a) Sparse feature extraction
    with standard convolution. (b) Dense feature extraction with dilated convolution
    with rate 2\. From [[77](#bib.bib77)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dilated Convolutional Models and DeepLab Family. The stack pooling and striding
    at consecutive layers reduces the resolution of the features, typically by a factor
    of 32 across each direction in recent DCNNs, affecting the segmentation accuracy
    of small objects. DeepLabv1 [[78](#bib.bib78)] introduces atrous convolution (a.k.a.
    dilated convolution) which computes the responses of any layer at any desired
    resolution. Comparing the common convolution, atrous adds an extra parameter,
    rate parameter $r$, which corresponds to the stride with the input signal. It
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;y[i]=x[i+r\cdot k]w[k],\\ \end{split}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where the output $y[i]$ of atrous convolution of a 1-d input signal $x[i]$
    with a filter $w[k]$ of length $k$. See Fig. [25](#S2.F25 "Figure 25 ‣ II-G Deep
    Learning-based Semantic Segmentation ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey") for illustration, for example, a $3\times
    3$ kernel with a dilation rate of 2 will have the same size of receptive field
    as a $5\times 5$ kernel while using only 9 parameters, thus enlarging the receptive
    field with a negligible increase in computational cost.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78cf4d02977148320de7d0e161eb84a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: The DeepLabv3 model with image-level features. From [[79](#bib.bib79)].'
  prefs: []
  type: TYPE_NORMAL
- en: The DeepLab series is based on the FCN concept. Four versions of the deeplab
    series are called DeepLabv1 [[78](#bib.bib78)], DeepLabv2 [[77](#bib.bib77)],
    DeepLabv3 [[79](#bib.bib79)] and DeepLabv3+ [[80](#bib.bib80)]. DeepLabv1 lays
    the foundation for the other approaches. DeepLabv1 has two contributions. First,
    it uses atrous convolution to address the decreasing resolution in the network
    and increases the receptive field without adding computation. Second, it combines
    the responses at the final dcnn layer with a fully connected conditional random
    field (crf), which deals with the poor localization property of deep networks.
    DeepLabv2 proposes atrous spatial pyramid pooling (Aspp) to robustly segment objects
    at multiple scales. Aspp probes an incoming convolutional feature layer with filters
    at multiple sampling rates and effective fields-of-views, thus capturing objects
    and context at multiple scales.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, DeepLabv3 [[79](#bib.bib79)] combines cascaded and parallel modules
    of atrous convolutions. The parallel convolution modules are grouped in the Aspp
    with image-level features encoding global context and further boost performance,
    as shown in Fig. [26](#S2.F26 "Figure 26 ‣ II-G Deep Learning-based Semantic Segmentation
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey").
    All the outputs are concatenated and processed by a $1\times 1$ convolution to
    create the final prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: DeepLabv3+ employs an encoder-decoder architecture. It uses DeepLabv3 as the
    encoder and modifies the xception backbone with more layers. What’s more, dilated
    depthwise separable convolutions instead of max pooling and batch normalization.
    The decoder first takes the low-level features from the network backbone as inputs
    and then concatenates the outputs of the Aspp. The simple yet effective decoder
    module refines the segmentation results along object boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: III Human Parsing Datasets and Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the last decades, significant efforts have been made to develop various
    methods for human parsing. It is important to introduce some publicly available
    benchmark datasets and evaluation metrics. We also provide the quantitative performance
    of the promising models on popular datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/539c7ae7c42b98200d2a64edd87e7a72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Five sample images with their corresponding ground truth from the
    PASCAL-Person-Part, LIP, CIHP, PPSS, and ATR, respectively. From [[81](#bib.bib81),
    [26](#bib.bib26), [49](#bib.bib49), [82](#bib.bib82), [18](#bib.bib18)].'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PASCAL-Person-Part [[81](#bib.bib81)] is one of the most representative and
    widely used datasets in the human parsing task. There are multiple people appearances
    in an unconstrained environment. Its images mainly come from PASCAL VOC  [[81](#bib.bib81)]
    dataset of image semantic segmentation. This human parsing dataset has 7 classes:
    background, head, torso, upper-arm, lower-arm, upper-leg and lower-leg. This dataset
    is divided into two sets, training, and test, with 1716 and 1817 images, respectively.
    Fig. [27](#S3.F27 "Figure 27 ‣ III Human Parsing Datasets and Evaluation Metrics
    ‣ Deep Learning for Human Parsing: A Survey") shows an example image and its pixel-wise
    label.'
  prefs: []
  type: TYPE_NORMAL
- en: LIP [[26](#bib.bib26)] is one of the most popular datasets in the task, which
    is a single-person dataset. Its images are captured from a wider range of viewpoints,
    occlusions, and complex backgrounds. LIP defines 19 human parts (clothes) labels,
    including hat, hair, sunglasses, upper-clothes, dress, coat, socks, pants, gloves,
    scarf, skirt, jumpsuits, face, right-arm, left-arm, right-leg, left-leg, right-shoe
    and left-shoe, and a background class. There are 50,462 images, including 30,362
    for training, 10,000 for testing and 10,000 for validation. The testing set annotation
    is not disclosed, and the precision of the model on the testing set needs to be
    obtained through online evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: CIHP [[49](#bib.bib49)] is a new large-scale multi-person benchmark with pixel-wise
    annotations on 19 semantic part labels, each image includes about three people.
    The images are collected from real-world scenarios, containing persons appearing
    with challenging poses and viewpoints, heavy occlusions, and a wide range of resolutions.
    This benchmark is divided into three sets, 28,280 images for training, 5,000 images
    for validation, and 5,000 for testing. There is a private test set for the actual
    challenge.
  prefs: []
  type: TYPE_NORMAL
- en: PPSS [[82](#bib.bib82)] is collected from 171 surveillance videos, it can reflect
    shading and lighting changes in real scenes. There are eight categories of the
    dataset, including hair, face, top, bottom, arms, legs, and shoes. There are 3673
    annotated images in his dataset, including 1781 training images and 1892 test
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Fashionista [[83](#bib.bib83)] consists of 158,235 images collected from Chictopia.com,
    a social networking website for fashion bloggers. It selects 685 images with good
    visibility of the full body for training and evaluation and is divided into ten
    folders, where the 9 folders serve as the training set, and the remaining 1 folder
    serves as the test set. This dataset includes a total of 56 categories, of which
    53 are common clothing categories (e.g., dresses, bags, jackets, skirts, boots,
    sweaters) and the remaining three are hair, skin, and background.
  prefs: []
  type: TYPE_NORMAL
- en: Colorful Fashion Parsing Data (CFPD) [[84](#bib.bib84)] is a dataset containing
    color attributes of clothes and part categories. This dataset has 13 colors and
    23 categories of tags. To balance the labeling efficiency and accuracy, they first
    over-segment each image into about 400 patches. There are 2,682 photos to form
    the final CFPD, randomly selecting half of these images as the training set, and
    taking the other half as the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Daily Photos dataset [[85](#bib.bib85)] consists of 2,500 images, all the pixels
    in the images are thoroughly annotated with 18 types of labels. The training:
    testing ratio is 2:1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ATR [[18](#bib.bib18)] is a combined dataset from four major sources: 685 images
    from the Fashionista; 2,682 images from the CFPD; 2,500 images from the daily
    photos and 1,833 images from the Human Parsing in the Wild (HPW). There are 18
    categories in the dataset, including face, sunglasses, hat, scarf, hair, clothes,
    left arm, right arm, belt, pants, left leg, right leg, skirt, left shoe, right
    shoe, bag, dress, and other common 17 categories and 1 background. This dataset
    includes 7,700 annotated images, including 6,000 images for training, 700 images
    for validation, and 1,000 for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Fashion Clothing dataset [[86](#bib.bib86)] contains a total of 4,371 images
    from Clothing Co-Parsing (CCP) [[87](#bib.bib87)], Fashionista [[83](#bib.bib83)]
    and Colorful Fashion Parsing Data (CFPD) [[84](#bib.bib84)]. It focuses more on
    human clothing details, and it consists of a background and 17 labels representing
    jewelry, bag, coat, suit, dress, glass, hair, pants, shoes, shirt, skin, skirt,
    upper-clothes, vest, and underwear, respectively. This dataset can relatively
    reflect the network’s capability to parse object details.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Metrics for Segmentation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduce the most popular metrics for assessing the accuracy of human parsing
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel Accuracy is the ratio between the number of pixels correctly classified
    in the index dataset and the total number of pixels. The calculation formula is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PA=\frac{{\sum\limits_{i}^{C}{{t_{ii}}}}}{{\sum\limits_{i}^{C}{\sum\limits_{j}^{C}{{t_{ij}}}}}},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $C$ denotes the number of categories, $t_{ii}$ denotes pixel $i$ is accurately
    classified as $i$, $t_{ij}$ denotes the real labels for $i$ pixel discriminant
    for $j$ class. The pixel accuracy ranges from 0 to 1, the higher the value, the
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Pixel Accuracy (MPA) is the extended version of PA, which averages PA over
    all classes, obtaining the final value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intersection over Union (IoU) or the Jaccard Index is one of the most commonly
    used metrics in human parsing. It calculates the ratio of the intersection and
    union between the predicted region of the class and the region of the real label
    class in each category. For the $k$ category, IoU is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $IoU=\frac{A\cap B}{A\cup B},$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $A$ and $B$ denote the ground truth and the predicted segmentation maps,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: mean Intersection over Union (mIoU) is the extended version of IoU, which averages
    over the total number of classes. It is widely used in reporting the performance
    of modern human parsing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: F-1 score (F-1) is a common metric used to evaluate the accuracy of a model.
    It combines precision and recall to measure uniformly and is calculated by the
    harmonic average of both indicators.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F-1=2\cdot\frac{Precision\cdot Recall}{Precision+Recall}.$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Foreground Pixel Accuracy (FGAcc) only calculates the pixel accuracy of foreground
    human parts.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Quantitative Performance of Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we outline the performance of several of the previous methods
    on popular segmentation benchmarks. Since some models are not reported their performance
    on standard datasets, it is hard to make a full comparison. The following tables
    summarize the performances of several of the prominent models on different datasets.
    Table [II](#S3.T2 "TABLE II ‣ III-C Quantitative Performance of Methods ‣ III
    Human Parsing Datasets and Evaluation Metrics ‣ Deep Learning for Human Parsing:
    A Survey") focuses on the PASCAL-Person-Part test set. Since the introduction
    of FCN-based models, the performance has been greatly improved. Table [III](#S3.T3
    "TABLE III ‣ III-C Quantitative Performance of Methods ‣ III Human Parsing Datasets
    and Evaluation Metrics ‣ Deep Learning for Human Parsing: A Survey") focuses on
    the LIP validation. On this dataset, the latest model improved by 32.01% in terms
    of mIoU over the original model. Table [IV](#S3.T4 "TABLE IV ‣ III-C Quantitative
    Performance of Methods ‣ III Human Parsing Datasets and Evaluation Metrics ‣ Deep
    Learning for Human Parsing: A Survey") focuses on the CIHP test set. This dataset
    is more challenging than the PASCAL-Person-Part dataset, both two datasets are
    multi-person. Table [V](#S3.T5 "TABLE V ‣ III-C Quantitative Performance of Methods
    ‣ III Human Parsing Datasets and Evaluation Metrics ‣ Deep Learning for Human
    Parsing: A Survey") focuses on the ATR test set. The latest model achieves 87.16
    in terms of F-1\. Finally, Table [VI](#S3.T6 "TABLE VI ‣ III-C Quantitative Performance
    of Methods ‣ III Human Parsing Datasets and Evaluation Metrics ‣ Deep Learning
    for Human Parsing: A Survey") summarizes the performance of several models for
    the Fashion Clothing dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Quantitative results on PASCAL-Person-Part test in terms of mean
    pixel Intersection-over-Union (mIoU) (%)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| HAZA [[30](#bib.bib30)] | 2016 | ECCV | VGG16 | 57.54 |'
  prefs: []
  type: TYPE_TB
- en: '| LIP [[26](#bib.bib26)] | 2017 | CVPR | ResNet101 | 59.36 |'
  prefs: []
  type: TYPE_TB
- en: '| MMAN [[88](#bib.bib88)] | 2018 | ECCV | ResNet101 | 59.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Graph LSTM [[23](#bib.bib23)] | 2016 | ECCV | VGG16 | 60.61 |'
  prefs: []
  type: TYPE_TB
- en: '| SE LSTM [[24](#bib.bib24)] | 2017 | CVPR | VGG16 | 63.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Joint [[25](#bib.bib25)] | 2017 | CVPR | ResNet101 | 64.39 |'
  prefs: []
  type: TYPE_TB
- en: '| MuLA [[27](#bib.bib27)] | 2018 | ECCV | VGG16 | 65.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PCNet [[13](#bib.bib13)] | 2018 | AAAI | PSPNet101 | 65.90 |'
  prefs: []
  type: TYPE_TB
- en: '| Holistic [[31](#bib.bib31)] | 2017 | BMVC | ResNet101 | 66.3 |'
  prefs: []
  type: TYPE_TB
- en: '| WSHP [[89](#bib.bib89)] | 2018 | CVPR | VGG16 | 67.60 |'
  prefs: []
  type: TYPE_TB
- en: '| SPGNet [[90](#bib.bib90)] | 2019 | ICCV | ResNet-101 | 68.36 |'
  prefs: []
  type: TYPE_TB
- en: '| PGN [[49](#bib.bib49)] | 2018 | ECCV | ResNet101 | 68.40 |'
  prefs: []
  type: TYPE_TB
- en: '| RefineNet [[75](#bib.bib75)] | 2017 | CVPR | ResNet101 | 68.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[16](#bib.bib16)] | 2019 | ICCV | ResNet101 | 70.76 |'
  prefs: []
  type: TYPE_TB
- en: '| DTCF [[39](#bib.bib39)] | 2020 | ACMMM | ResNet101 | 70.80 |'
  prefs: []
  type: TYPE_TB
- en: '| Graphonomy [[14](#bib.bib14)] | 2019 | CVPR | DeepLab v3+ | 71.14 |'
  prefs: []
  type: TYPE_TB
- en: '| DPC [[91](#bib.bib91)] | 2018 | NeurIPS | - | 71.34 |'
  prefs: []
  type: TYPE_TB
- en: '| SNT [[92](#bib.bib92)] | 2020 | ECCV | ResNet101 | 71.59 |'
  prefs: []
  type: TYPE_TB
- en: '| NPPNet [[93](#bib.bib93)] | 2021 | ICCV | ResNet101 | 71.73 |'
  prefs: []
  type: TYPE_TB
- en: '| CDCL [[40](#bib.bib40)] | 2021 | TCSVT | ResNet101 | 72.82 |'
  prefs: []
  type: TYPE_TB
- en: '| PRHP [[94](#bib.bib94)] | 2021 | TPAMI | ResNet101 | 72.82 |'
  prefs: []
  type: TYPE_TB
- en: '| BGNet [[95](#bib.bib95)] | 2020 | ECCV | ResNet101 | 73.12 |'
  prefs: []
  type: TYPE_TB
- en: '| GWNet [[96](#bib.bib96)] | 2022 | TIP | ResNet101 | 74.67 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Quantitative results on LIP val in terms of mean pixel Intersection-over-Union
    (mIoU) (%)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| FCN-8s [[11](#bib.bib11)] | 2015 | CVPR | VGG16 | 28.29 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLabV2 [[77](#bib.bib77)] | 2017 | TPAMI | ResNet101 | 41.64 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention [[97](#bib.bib97)] | 2016 | CVPR | VGG16 | 42.92 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLab-ASPP [[77](#bib.bib77)] | 2017 | TPAMI | VGG16 | 44.03 |'
  prefs: []
  type: TYPE_TB
- en: '| LIP [[26](#bib.bib26)] | 2017 | CVPR | ResNet101 | 44.73 |'
  prefs: []
  type: TYPE_TB
- en: '| MMAN [[88](#bib.bib88)] | 2018 | ECCV | VGG16 | 46.81 |'
  prefs: []
  type: TYPE_TB
- en: '| JPPNet [[98](#bib.bib98)] | 2018 | TPAMI | PSPNet101 | 51.37 |'
  prefs: []
  type: TYPE_TB
- en: '| CE2P [[29](#bib.bib29)] | 2019 | AAAI | ResNet101 | 53.10 |'
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[99](#bib.bib99)] | 2019 | ACMMM | PSPNet | 54.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SNT [[92](#bib.bib92)] | 2020 | ECCV | ResNet101 | 54.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[28](#bib.bib28)] | 2020 | CVPR | ResNet101 | 55.33 |'
  prefs: []
  type: TYPE_TB
- en: '| SLRS [[21](#bib.bib21)] | 2020 | CVPR | ResNet101 | 56.34 |'
  prefs: []
  type: TYPE_TB
- en: '| BGNet [[95](#bib.bib95)] | 2020 | ECCV | ResNet101 | 56.82 |'
  prefs: []
  type: TYPE_TB
- en: '| GWNet [[96](#bib.bib96)] | 2022 | TIP | ResNet101 | 57.26 |'
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[16](#bib.bib16)] | 2019 | ICCV | ResNet101 | 57.74 |'
  prefs: []
  type: TYPE_TB
- en: '| DTCF [[39](#bib.bib39)] | 2020 | ACMMM | ResNet101 | 57.82 |'
  prefs: []
  type: TYPE_TB
- en: '| NPPNet [[93](#bib.bib93)] | 2021 | ICCV | ResNet101 | 58.56 |'
  prefs: []
  type: TYPE_TB
- en: '| HHP [[17](#bib.bib17)] | 2020 | CVPR | DeepLabV3 | 59.25 |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[21](#bib.bib21)] | 2020 | TPAMI | ResNet101 | 59.36 |'
  prefs: []
  type: TYPE_TB
- en: '| CDGNet [[38](#bib.bib38)] | 2022 | CVPR | ResNet101 | 60.30 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Quantitative results on CIHP test in terms of mean pixel Intersection-over-Union
    (mIoU) (%)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| PGN [[49](#bib.bib49)] | 2018 | ECCV | ResNet101 | 55.80 |'
  prefs: []
  type: TYPE_TB
- en: '| Graphonomy [[14](#bib.bib14)] | 2019 | CVPR | DeepLab v3+ | 58.58 |'
  prefs: []
  type: TYPE_TB
- en: '| M-CE2P [[29](#bib.bib29)] | 2019 | AAAI | ResNet101 | 59.50 |'
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[28](#bib.bib28)] | 2020 | CVPR | ResNet101 | 60.18 |'
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[99](#bib.bib99)] | 2019 | ACMMM | PSPNet | 60.62 |'
  prefs: []
  type: TYPE_TB
- en: '| SNT [[92](#bib.bib92)] | 2020 | ECCV | ResNet101 | 60.87 |'
  prefs: []
  type: TYPE_TB
- en: '| PCNet [[100](#bib.bib100)] | 2020 | CVPR | ResNet101 | 61.05 |'
  prefs: []
  type: TYPE_TB
- en: '| CDGNet [[38](#bib.bib38)] | 2022 | CVPR | ResNet101 | 65.56 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Quantitative results on ATR test in terms of Foreground Pixel Accuracy
    (FGAcc) and F-1 score (F-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | F.G.Acc | F-1 |'
  prefs: []
  type: TYPE_TB
- en: '| ATR [[97](#bib.bib97)] | 2015 | TPAMI | - | 71.04 | 64.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention [[97](#bib.bib97)] | 2016 | CVPR | VGG16 | 85.71 | 77.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Co-CNN [[20](#bib.bib20)] | 2015 | ICCV | VGG16 | 83.57 | 80.14 |'
  prefs: []
  type: TYPE_TB
- en: '| LG-LSTM [[22](#bib.bib22)] | 2016 | CVPR | VGG16 | 84.79 | 80.97 |'
  prefs: []
  type: TYPE_TB
- en: '| TGPNet [[86](#bib.bib86)] | 2018 | ACMMM | VGG16 | 87.91 | 81.76 |'
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[16](#bib.bib16)] | 2019 | ICCV | ResNet101 | 85.51 |  |'
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[28](#bib.bib28)] | 2020 | CVPR | ResNet101 | 90.40 | 86.12 |'
  prefs: []
  type: TYPE_TB
- en: '| HHP [[17](#bib.bib17)] | 2020 | CVPR | DeepLabV3 | 89.23 | 87.25 |'
  prefs: []
  type: TYPE_TB
- en: '| CDGNet [[38](#bib.bib38)] | 2022 | CVPR | ResNet101 | 90.19 | 87.16 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Quantitative results on Fashion Clothing test in terms of Foreground
    Pixel Accuracy (FGAcc) and F-1 score (F-1).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | F.G.Acc | F-1 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLabV2 [[77](#bib.bib77)] | 2018 | TPAMI | VGG16 | 56.08 | 37.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention [[97](#bib.bib97)] | 2016 | CVPR | VGG16 | 64.47 | 48.68 |'
  prefs: []
  type: TYPE_TB
- en: '| TGPNet [[86](#bib.bib86)] | 2018 | ACMMM | VGG16 | 66.37 | 51.92 |'
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[94](#bib.bib94)] | 2021 | TPAMI | ResNet101 | 68.59 | 58.12 |'
  prefs: []
  type: TYPE_TB
- en: '| PRHP [[94](#bib.bib94)] | 2021 | TPAMI | ResNet101 | 70.57 | 60.19 |'
  prefs: []
  type: TYPE_TB
- en: IV Opportunities and Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human parsing has made remarkable progress thanks to deep learning. We will
    introduce some of the promising research directions for further advancing human
    parsing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Weakly-Supervised and Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although supervised human parsing methods have demonstrated impressive results,
    their performance highly depends on the quantity and quality of training data.
    These data are a labor-intensive process that spends a significant amount of time
    and money. Weakly and Semi supervised [[101](#bib.bib101)] are becoming very active
    research communities. These approaches focus on knowledge transfer methods that
    migrate features with semantic labels to areas where collecting labels is difficult.
    In this way, the annotation of data can be reduced and the application scope can
    be expanded. For example, Fang *et al*. introduce pose-guide transfer [[89](#bib.bib89)]
    for human parsing. Unsupervised learning is another promising direction that is
    attracting much attraction in various fields. In the field of segmentation, it
    adaptively divides different semantic parts by semantic consistency without the
    need for annotated data. Zhang *et al* [[102](#bib.bib102)] and Liu *et al* [[103](#bib.bib103)]
    design unsupervised human parsing methods which focus on simple gestures and cannot
    handle complex gestures or missing parts. More accurate methods should be further
    discovered and applied earlier.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Total-body Human Parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Social communication is a key function of human motion [[104](#bib.bib104)].
    People communicate tremendous amounts of information with the subtlest movements [[105](#bib.bib105)],
    such as a glance and a smile. However, there are no existing systems simultaneously
    containing human parsing, face parsing, and hand parsing [[106](#bib.bib106),
    [107](#bib.bib107)] to fully understand the pixel-wise spatial attributes of human
    in the wild. In this task, the toughest challenge is the total-body human parsing
    datasets. Labeling a complete and accurate pixel-level dataset is a huge task.
    A simple method is first to generate some false face and hand labels by high accuracy
    face parsing and hand parsing methods, respectively. And then false labels and
    ground truth of human parsing form the labels of total-body. Therefore, the integrated
    dataset and total-body human parsing methods need to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Real-World Open-Set Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In real-world human parsing tasks, limited by various objective factors, it
    is difficult to collect training samples to exhaust all parts. Therefore, most
    methods cannot guarantee consistently good performance in practical scenarios.
    A more realistic scenario is open set recognition (OSR) [[108](#bib.bib108)],
    where incomplete knowledge of the world exists at training time, and unknown classes
    can be submitted to an algorithm during testing, requiring the classifiers to
    not only accurately classify the seen classes, but also effectively deal with
    unseen ones. Recently, some open-set methods are proposed in semantic segmentation.
    For example, DMLNet [[109](#bib.bib109)] detects both in-distribution and out-of-distribution
    objects with an incremental few-shot learning module to gradually incorporate
    those OOD objects into its existing knowledge base. The human body has various
    poses and many clothing types, and it is hard to define them all. Thus, real-world
    open-set methods are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Memory Efficient and Real-time Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many applications, such as mobile terminals, are critical to the parsing models
    that have a small amount of memory and/or can run in near real-time. To reduce
    memory, this can be done either by using simpler models, or by using model compression
    techniques or even training a complex model and then using knowledge distillation
    techniques to compress it into a smaller, memory efficient network that mimics
    the complex model. Real-time is associated with a small amount of memory which
    speeds up inference and allows no less than 25 frames per second to be processed,
    achieving real-time.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Interpretable Deep Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep learning-based human parsing method is a series of modeling methods
    for computers to improve prediction or behavior based on data. The current decision
    support system based on machine learning is a great improvement over the traditional
    rule-based method. Despite the advantages of machine learning models, users often
    question their decisions due to their lack of interpretability. To improve the
    transparency of machine learning models, building trust between users and machine
    learning models, and reducing potential risks in applications, such as bias in
    models, it is very necessary to provide model explanations.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Application Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human parsing has been widely used in many applications such as person re-identification,
    pose estimation, and dress people.
  prefs: []
  type: TYPE_NORMAL
- en: Person Re-identification. Person re-identification [[110](#bib.bib110)] aims
    to associate the person images captured by different cameras from various viewpoints,
    which attracts increasing attention from both the academia and the industry. However,
    part occlusions and inaccurate person detection can significantly change the visual
    appearance of a person in images and greatly increase the difficulty of this retrieval
    problem. Some methods [[111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]
    inject extra semantics in terms of the part to achieve the part alignment at the
    pixel-level. In this way, human parsing provides semantic information to help
    re-identification models perceive the position of the appearance of the human
    body.
  prefs: []
  type: TYPE_NORMAL
- en: Pose Estimation. Pose estimation and human parsing are two crucial yet challenging
    tasks for human body configuration in 2D monocular images, these two tasks are
    highly correlated and could provide beneficial information for each other. Human
    parsing can facilitate localizing body joints in difficult scenarios. For example,
    MuLA [[27](#bib.bib27)] can fast adapt parsing and pose models to provide more
    powerful representations by incorporating information from their counterparts,
    giving more robust and accurate pose results. Dense pose estimation aims at mapping
    all human pixels of an RGB image to the 3D surface of the human body [[114](#bib.bib114)].
    The mainstream dense pose estimation methods explicitly integrate human parsing
    supervision, such as DensePose R-CNN [[114](#bib.bib114)], Parsing R-CNN [[51](#bib.bib51)].
  prefs: []
  type: TYPE_NORMAL
- en: Dress People. The 3D reconstruction and modeling of humans from images is a
    central problem in computer vision [[115](#bib.bib115)]. However, many methods [[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118)] lack realism and control. Multi-Garment
    Network (MGN) [[115](#bib.bib115)] first models the inferring human body and layered
    garments on top as separate meshes from images directly. Human parsing provides
    fine-grained clothing details for dress people. Besides, Adaptive Content Generating
    and Preserving Network (ACGPN) [[119](#bib.bib119)] predicts the semantic layout
    of the reference image that will be changed after try-on, and then determines
    whether its image content needs to be generated or preserved according to the
    predicted semantic layout. Beside, Zhang *et al*. propose a Decompose-and-aggregate
    Network (DaNet) [[120](#bib.bib120)] that densely build a bridge between 2D pixels
    and 3D vertexes to facilitate the learning of 3D reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have surveyed recent human parsing algorithms based on deep learning models
    which have achieved impressive performance, and grouped them into five categories
    including structure-driven architectures, graph-based networks, context-aware
    methods, LSTM-based methods, and combined auxiliary information approaches. Then,
    we review quantitative performance analyses of these models on some popular benchmarks,
    such as PASCAL-Person-Part, LIP, CIHP, ATR, and Fashion Clothing datasets. Finally,
    we introduce some of the promising research directions for further human parsing
    algorithms and the application scenarios of the human parsing task.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] I. Ulku and E. Akagündüz, “A survey on deep learning-based architectures
    for semantic segmentation on 2d images,” *Applied Artificial Intelligence*, pp.
    1–45, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, “Person
    re-identification by symmetry-driven accumulation of local features,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010, pp. 2360–2367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Wang, T. Duan, Z. Liao, and D. Forsyth, “Discriminative hierarchical
    part-based models for human parsing and action recognition,” *J. MACH. LEARN.
    RES.*, vol. 13, no. 1, pp. 3075–3102, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K. Yamaguchi, M. H. Kiapour, and T. L. Berg, “Paper doll parsing: Retrieving
    similar styles to parse clothing items,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2013, pp. 3519–3526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint training of a
    convolutional network and a graphical model for human pose estimation,” *Advances
    in neural information processing systems*, vol. 27, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Bourdev and J. Malik, “Poselets: Body part detectors trained using 3d
    human pose annotations,” in *2009 IEEE 12th International Conference on Computer
    Vision*.   IEEE, 2009, pp. 1365–1372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] B. Sapp, A. Toshev, and B. Taskar, “Cascaded models for articulated pose
    estimation,” in *European conference on computer vision*.   Springer, 2010, pp.
    406–420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] F. Wang and Y. Li, “Beyond physical connections: Tree models in human pose
    estimation,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2013, pp. 596–603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] B. Sapp and B. Taskar, “Modec: Multimodal decomposable models for human
    pose estimation,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2013, pp. 3674–3681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Kiefel and P. V. Gehler, “Human pose estimation with fields of parts,”
    in *European conference on computer vision*.   Springer, 2014, pp. 331–346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 3431–3440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Park, B. X. Nie, and S.-C. Zhu, “Attribute and-or grammar for joint
    parsing of human pose, parts and attributes,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 40, no. 7, pp. 1555–1569, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] B. Zhu, Y. Chen, M. Tang, and J. Wang, “Progressive cognitive human parsing,”
    *In Proc. AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, and L. Lin, “Graphonomy:
    Universal human parsing via graph transfer learning,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] L. Lin, Y. Gao, K. Gong, M. Wang, and X. Liang, “Graphonomy: Universal
    image parsing via graph reasoning and transfer,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. Wang, Z. Zhang, S. Qi, J. Shen, Y. Pang, and L. Shao, “Learning compositional
    neural information fusion for human parsing,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Wang, H. Zhu, J. Dai, Y. Pang, J. Shen, and L. Shao, “Hierarchical
    human parsing with typed part-relation reasoning,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, 2020, pp. 8929–8939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] L. X, L. S, S. X, Y. J, L. L, D. J, L. L, and Y. S, “Deep human parsing
    with active template regression.” *In IEEE TPAMI*, vol. 37, no. 12, pp. 2402–2414,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, L. Lin, X. Cao, and
    S. Yan, “Matching-cnn meets knn: Quasi-parametric human parsing,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1419–1427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, and S. Yan,
    “Human parsing with contextualized convolutional neural network,” in *Proceedings
    of the IEEE international conference on computer vision*, 2015, pp. 1386–1394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. Li, Z. Liang, S. Zhao, J. Gong, and J. Shen, “Self-learning with rectification
    strategy for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 9263–9272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan, “Semantic object
    parsing with local-global long short-term memory,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2016, pp. 3185–3193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan, “Semantic object parsing
    with graph lstm,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 125–143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] X. Liang, L. Lin, X. Shen, J. Feng, S. Yan, and E. P. Xing, “Interpretable
    structure-evolving lstm,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, pp. 2175–2184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] F. Xia, P. Wang, X. Chen, and A. Yuille, “Joint multi-person pose estimation
    and semantic part segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, pp. 6080–6089.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] K. Gong, X. Liang, D. Zhang, X. Shen, and L. Lin, “Look into person: Self-supervised
    structure-sensitive learning and a new benchmark for human parsing.” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, vol. 2, no. 5, 2017, p. 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X. Nie, J. Feng, and S. Yan, “Mutual learning to adapt for joint human
    parsing and pose estimation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 502–517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Z. Zhang, C. Su, L. Zheng, and X. Xie, “Correlating edge, pose with parsing,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] T. Liu, T. Ruan, Z. Huang, Y. Wei, S. Wei, Y. Zhao, and H. Thomas, “Devil
    in the details: Towards accurate single and multiple human parsing,” *Proc. AAAI*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] F. Xia, P. Wang, L. C. Chen, and A. L. Yuille, “Zoom better to see clearer:
    Human and object parsing with hierarchical auto-zoom net,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2015, pp. 648–663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Q. Li, A. Arnab, and P. H. Torr, “Holistic, instance-level human parsing,”
    *arXiv preprint arXiv:1709.03612*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally
    connected networks on graphs,” *arXiv preprint arXiv:1312.6203*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. He, W. Huang, Y. Qiao, C. C. Loy, and X. Tang, “Reading scene text
    in deep convolutional sequences,” in *Thirtieth AAAI conference on artificial
    intelligence*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] F. Xia, J. Zhu, P. Wang, and A. Yuille, “Pose-guided human parsing by
    an and/or graph using pose-context features,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 30, no. 1, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Humayun, F. Li, and J. M. Rehg, “Rigor: Reusing inference in graph
    cuts for generating object regions,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2014, pp. 336–343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, and S. Yan,
    “Human parsing with contextualized convolutional neural network,” *In IEEE TPAMI*,
    vol. 39, no. 1, pp. 115–127, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Canny, “A computational approach to edge detection,” *IEEE Transactions
    on pattern analysis and machine intelligence*, no. 6, pp. 679–698, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. Liu, O. Choi, J. Wang, and W. Hwang, “Cdgnet: Class distribution guided
    network for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 4473–4482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Liu, L. Zhao, S. Zhang, and J. Yang, “Hybrid resolution network using
    edge guided region mutual information loss for human parsing,” in *Proceedings
    of the 28th ACM International Conference on Multimedia*, 2020, pp. 1670–1678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] K. Lin, L. Wang, K. Luo, Y. Chen, Z. Liu, and M.-T. Sun, “Cross-domain
    complementary learning using pose for multi-person part segmentation,” *IEEE Trans.
    Circuits Syst. Video Technol.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] X. Liang, L. Lin, W. Yang, P. Luo, J. Huang, and S. Yan, “Clothes co-parsing
    via joint image segmentation and labeling with application to clothing retrieval,”
    *IEEE Transactions on Multimedia*, vol. 18, no. 6, pp. 1175–1186, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Zhao, J. Li, Y. Zhang, and Y. Tian, “Multi-class part parsing with
    joint boundary-semantic awareness,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9177–9186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. He, J. Zhang, Q. Zhang, and D. Tao, “Grapy-ml: Graph pyramid mutual
    learning for cross-dataset human parsing,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 07, 2020, pp. 10 949–10 956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] X. Zhang, Y. Chen, M. Tang, J. Wang, X. Zhu, and Z. Lei, “Human parsing
    with part-aware relation modeling,” *IEEE Transactions on Multimedia*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] L. Li, T. Zhou, W. Wang, J. Li, and Y. Yang, “Deep hierarchical semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 1246–1257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Liu, S. Zhang, J. Yang, and P. Yuen, “Hierarchical information passing
    based noise-tolerant hybrid learning for semi-supervised human parsing,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 35, no. 3, 2021, pp.
    2207–2215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Z. Jin, B. Liu, Q. Chu, and N. Yu, “Isnet: Integrate image-level and semantic-level
    context for semantic segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 7189–7198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Liu, Y. Sun, D. Zhu, G. Ren, Y. Chen, J. Feng, and J. Han, “Cross-domain
    human parsing via adversarial feature and label adaptation,” in *Proceedings of
    the AAAI Conference On Artificial Intelligence*, vol. 32, no. 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] K. Gong, X. Liang, Y. Li, Y. Chen, M. Yang, and L. Lin, “Instance-level
    human parsing via part grouping network,” in *Proc. Eur. Conf. Comput. Vis.*,
    2018, pp. 770–785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Zhao, J. Li, H. Liu, S. Yan, and J. Feng, “Fine-grained multi-human
    parsing,” *International Journal of Computer Vision*, vol. 128, no. 8, pp. 2185–2203,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] L. Yang, Q. Song, Z. Wang, and M. Jiang, “Parsing r-cnn for instance-level
    human analysis,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019, pp. 364–373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S. Zhang, X. Cao, G.-J. Qi, Z. Song, and J. Zhou, “Aiparsing: Anchor-free
    instance-level human parsing,” *IEEE Transactions on Image Processing*, vol. 31,
    pp. 5599–5612, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Q. Zhou, X. Liang, K. Gong, and L. Lin, “Adaptive temporal encoding network
    for video instance-level human parsing,” in *Proceedings of the 26th ACM international
    conference on Multimedia*, 2018, pp. 1527–1535.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Liu, X. Liang, L. Liu, K. Lu, L. Lin, and S. Yan, “Fashion parsing
    with video context,” in *Proceedings of the 22nd ACM international conference
    on Multimedia*, 2014, pp. 467–476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M.-H. Yang, “Joint-task
    self-supervised learning for temporal correspondence,” *Advances in Neural Information
    Processing Systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Jabri, A. Owens, and A. Efros, “Space-time correspondence as a contrastive
    random walk,” *Advances in neural information processing systems*, vol. 33, pp.
    19 545–19 560, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Jeon, D. Min, S. Kim, and K. Sohn, “Mining better samples for contrastive
    learning of temporal correspondence,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 1034–1044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, and Y. Yang, “Locality-aware
    inter-and intra-video reconstruction for self-supervised correspondence learning,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 8719–8730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Communications of the ACM*, vol. 60,
    no. 6, pp. 84–90, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Z. Zhong, J. Li, W. Cui, and H. Jiang, “Fully convolutional networks for
    building and road extraction: Preliminary results,” in *2016 IEEE International
    Geoscience and Remote Sensing Symposium (IGARSS)*.   IEEE, 2016, pp. 1591–1594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep neural
    network architecture for real-time semantic segmentation,” *arXiv preprint arXiv:1606.02147*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] G. Wang, W. Li, S. Ourselin, and T. Vercauteren, “Automatic brain tumor
    segmentation using cascaded anisotropic convolutional neural networks,” in *International
    MICCAI brainlesion workshop*.   Springer, 2017, pp. 178–190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and
    D. Terzopoulos, “Image segmentation using deep learning: A survey,” *IEEE transactions
    on pattern analysis and machine intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] C. J. Holder and M. Shafique, “On efficient real-time semantic segmentation:
    a survey,” *arXiv preprint arXiv:2206.08605*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for scene segmentation,” *In IEEE TPAMI*, vol. PP,
    no. 99, pp. 2481–2495, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 5693–5703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] K. Sun, Y. Zhao, B. Jiang, T. Cheng, B. Xiao, D. Liu, Y. Mu, X. Wang,
    W. Liu, and J. Wang, “High-resolution representations for labeling pixels and
    regions,” *arXiv preprint arXiv:1904.04514*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang,
    P. H. Torr *et al.*, “Rethinking semantic segmentation from a sequence-to-sequence
    perspective with transformers,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2021, pp. 6881–6890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] G. Ghiasi and C. C. Fowlkes, “Laplacian pyramid reconstruction and refinement
    for semantic segmentation,” in *European conference on computer vision*.   Springer,
    2016, pp. 519–534.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] G. Lin, A. Milan, C. Shen, and I. Reid, “Refinenet: Multi-path refinement
    networks for high-resolution semantic segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 1925–1934.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang, “Multi-scale
    context intertwining for semantic segmentation,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 603–619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40,
    no. 4, pp. 834–848, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic
    image segmentation with deep convolutional nets and fully connected crfs,” *arXiv
    preprint arXiv:1412.7062*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
    with atrous separable convolution for semantic image segmentation,” in *Proc.
    Eur. Conf. Comput. Vis.*, 2018, pp. 801–818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, “Detect
    what you can: Detecting and representing objects using holistic models and body
    parts,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014, pp. 1979–1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] P. Luo, X. Wang, and X. Tang, “Pedestrian parsing via deep decompositional
    network,” in *Proceedings of the IEEE international conference on computer vision*,
    2013, pp. 2648–2655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] K. Yamaguchi, “Parsing clothing in fashion photographs,” in *CVPR*, 2012,
    pp. 3570–3577.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] S. Liu, J. Feng, C. Domokos, H. Xu, J. Huang, Z. Hu, and S. Yan, “Fashion
    parsing with weak color-category labels,” *IEEE Transactions on Multimedia*, vol. 16,
    no. 1, pp. 253–265, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Dong, Q. Chen, W. Xia, Z. Huang, and S. Yan, “A deformable mixture
    parsing model with parselets,” in *CVPR*, 2014, pp. 3408–3415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Luo, Z. Su, J. Guo, G. Zhang, and X. He, “Trusted guidance pyramid
    network for human parsing,” in *ACM MM*.   ACM, 2018, pp. 654–662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] W. Yang, P. Luo, and L. Lin, “Clothing co-parsing by joint image segmentation
    and labeling,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2014, pp. 3182–3189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Macro-micro
    adversarial network for human parsing,” in *Proc. Eur. Conf. Comput. Vis.*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] H.-S. Fang, G. Lu, X. Fang, J. Xie, Y.-W. Tai, and C. Lu, “Weakly and
    semi supervised human body part parsing via pose-guided knowledge transfer,” *arXiv
    preprint arXiv:1805.04310*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, and H. Shi, “Spgnet: Semantic prediction guidance for scene parsing,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 5218–5228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] L.-C. Chen, M. Collins, Y. Zhu, G. Papandreou, B. Zoph, F. Schroff, H. Adam,
    and J. Shlens, “Searching for efficient multi-scale architectures for dense image
    prediction,” in *Proc. Adv. Neural Inf. Process. Syst.*, 2018, pp. 8699–8710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] R. Ji, D. Du, L. Zhang, L. Wen, Y. Wu, C. Zhao, F. Huang, and S. Lyu,
    “Learning semantic neural tree for human parsing,” in *Proc. Eur. Conf. Comput.
    Vis.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] D. Zeng, Y. Huang, Q. Bao, J. Zhang, C. Su, and W. Liu, “Neural architecture
    search for joint human parsing and pose estimation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 11 385–11 394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] W. Wang, T. Zhou, S. Qi, J. Shen, and S.-C. Zhu, “Hierarchical human semantic
    parsing with comprehensive part-relation modeling,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Blended grammar network
    for human parsing,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 189–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] X. Zhang, Y. Chen, M. Tang, Z. Lei, and J. Wang, “Grammar-induced wavelet
    network for human parsing,” *IEEE Transactions on Image Processing*, vol. 31,
    pp. 4502–4514, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to scale:
    Scale-aware semantic image segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body
    parsing & pose estimation network and a new benchmark,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Liu, M. Zhang, W. Liu, J. Song, and T. Mei, “Braidnet: Braiding semantics
    and details for accurate human parsing,” in *ACM Int. Conf. Multimdia*, 2019,
    pp. 338–346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Part-aware context
    network for human parsing,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2020, pp. 8971–8980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” *National
    science review*, vol. 5, no. 1, pp. 44–53, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] X. Zhang, F. Pan, K. Xiang, X. Zhu, C. Yu, Z. Wang, and Z. Lei, “Contrastive
    and consistent learning for unsupervised human parsing,” in *Chinese Conference
    on Biometric Recognition*.   Springer, 2022, pp. 226–236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, “Unsupervised part segmentation
    through disentangling appearance and shape,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 8355–8364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. L. Birdwhistell, *Kinesics and context: Essays on body motion communication*.   University
    of Pennsylvania press, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Joo, T. Simon, and Y. Sheikh, “Total capture: A 3d deformation model
    for tracking faces, hands, and bodies,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 8320–8329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Liang, J. Yuan, and D. Thalmann, “Parsing the hand in depth images,”
    *IEEE Transactions on Multimedia*, vol. 16, no. 5, pp. 1241–1253, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Lin, H. Yang, D. Chen, M. Zeng, F. Wen, and L. Yuan, “Face parsing
    with roi tanh-warping,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 5654–5663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] C. Geng, S.-j. Huang, and S. Chen, “Recent advances in open set recognition:
    A survey,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 43,
    no. 10, pp. 3614–3631, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] J. Cen, P. Yun, J. Cai, M. Y. Wang, and M. Liu, “Deep metric learning
    for open world semantic segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 15 333–15 342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, “Identity-guided human
    semantic parsing for person re-identification,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 346–363.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. M. Kalayeh, E. Basaran, M. Gökmen, M. E. Kamasak, and M. Shah, “Human
    semantic parsing for person re-identification,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 1062–1071.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] C. Song, Y. Huang, W. Ouyang, and L. Wang, “Mask-guided contrastive attention
    model for person re-identification,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 1179–1188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Z. Li, J. Lv, Y. Chen, and J. Yuan, “Person re-identification with part
    prediction alignment,” *Computer Vision and Image Understanding*, vol. 205, p.
    103172, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] R. A. Güler, N. Neverova, and I. Kokkinos, “Densepose: Dense human pose
    estimation in the wild,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 7297–7306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] B. L. Bhatnagar, G. Tiwari, C. Theobalt, and G. Pons-Moll, “Multi-garment
    net: Learning to dress 3d people from images,” in *proceedings of the IEEE/CVF
    international conference on computer vision*, 2019, pp. 5420–5430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, and G. Pons-Moll,
    “Learning to reconstruct people in clothing from a single rgb camera,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 1175–1186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll, “Detailed
    human avatars from monocular video,” in *2018 International Conference on 3D Vision
    (3DV)*.   IEEE, 2018, pp. 98–109.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] ——, “Video based reconstruction of 3d people models,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    8387–8397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] H. Yang, R. Zhang, X. Guo, W. Liu, W. Zuo, and P. Luo, “Towards photo-realistic
    virtual try-on by adaptively generating-preserving image content,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2020,
    pp. 7850–7859.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] H. Zhang, J. Cao, G. Lu, W. Ouyang, and Z. Sun, “Learning 3d human shape
    and pose from dense body parts,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
