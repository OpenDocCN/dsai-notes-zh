- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:42:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:42:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2301.12416] Deep Learning for Human Parsing: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2301.12416] 深度学习在人体解析中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.12416](https://ar5iv.labs.arxiv.org/html/2301.12416)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2301.12416](https://ar5iv.labs.arxiv.org/html/2301.12416)
- en: 'Deep Learning for Human Parsing: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在人体解析中的应用：综述
- en: 'Xiaomei Zhang, Xiangyu Zhu, , Ming Tang, , and Zhen Lei, Xiaomei Zhang, Xiangyu
    Zhu and Ming Tang are with the National Laboratory of Pattern Recognition, Institute
    of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with
    the School of Artificial Intelligence, University of Chinese Academy of Sciences,
    Beijing 100049, China (e-mail: xiaomei.zhang@nlpr.ia.ac.cn; xiangyu.zhu.chen@nlpr.ia.ac.cn;
    tangm@nlpr.ia.ac.cn). Zhen Lei is with the National Laboratory of Pattern Recognition
    (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing
    100190, China, also with the School of Artificial Intelligence, University of
    Chinese Academy of Sciences (UCAS), Beijing 100190, China, and also with the Centre
    for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation,
    Chinese Academy of Sciences, Hong Kong, China (e-mail: zlei@nlpr.ia.ac.cn).Manuscript
    received Jan xx, 2023.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张晓梅，朱向宇，唐明，张晓梅、朱向宇和唐明均在中国科学院自动化研究所模式识别国家实验室，北京 100190，中国，同时也在中国科学院大学人工智能学院，北京
    100049，中国（电子邮件：xiaomei.zhang@nlpr.ia.ac.cn；xiangyu.zhu.chen@nlpr.ia.ac.cn；tangm@nlpr.ia.ac.cn）。雷振在中国科学院自动化研究所模式识别国家实验室（NLPR），北京
    100190，中国，同时也在中国科学院大学人工智能学院（UCAS），北京 100190，中国，以及中国科学院香港科技创新中心人工智能与机器人中心，香港，中国（电子邮件：zlei@nlpr.ia.ac.cn）。稿件收到时间：2023年xx月。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Human parsing is a key topic in image processing with many applications, such
    as surveillance analysis, human-robot interaction, person search, and clothing
    category classification, among many others. Recently, due to the success of deep
    learning in computer vision, there are a number of works aimed at developing human
    parsing algorithms using deep learning models. As methods have been proposed,
    a comprehensive survey of this topic is of great importance. In this survey, we
    provide an analysis of state-of-the-art human parsing methods, covering a broad
    spectrum of pioneering works for semantic human parsing. We introduce five insightful
    categories: (1) structure-driven architectures exploit the relationship of different
    human parts and the inherent hierarchical structure of a human body, (2) graph-based
    networks capture the global information to achieve an efficient and complete human
    body analysis, (3) context-aware networks explore useful contexts across all pixel
    to characterize a pixel of the corresponding class, (4) LSTM-based methods can
    combine short-distance and long-distance spatial dependencies to better exploit
    abundant local and global contexts, and (5) combined auxiliary information approaches
    use related tasks or supervision to improve network performance. We also discuss
    the advantages/disadvantages of the methods in each category and the relationships
    between methods in different categories, examine the most widely used datasets,
    report performances, and discuss promising future research directions in this
    area.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人体解析是图像处理中的一个关键话题，应用广泛，如监控分析、人机交互、人物搜索和服装类别分类等。近年来，由于深度学习在计算机视觉中的成功，许多工作致力于利用深度学习模型开发人体解析算法。随着方法的提出，对该主题进行全面综述显得尤为重要。在本综述中，我们对最先进的人体解析方法进行了分析，涵盖了语义人体解析的广泛开创性工作。我们介绍了五个有洞察力的类别：(1)
    结构驱动的架构利用不同人体部位之间的关系和人体的固有层次结构，(2) 基于图的网络捕捉全局信息，以实现高效而全面的人体分析，(3) 情境感知网络探索所有像素的有用上下文，以特征化相应类别的像素，(4)
    基于LSTM的方法可以结合短距离和长距离的空间依赖性，更好地利用丰富的局部和全局上下文，(5) 结合辅助信息的方法使用相关任务或监督来提高网络性能。我们还讨论了每种类别方法的优缺点以及不同类别方法之间的关系，审查了最广泛使用的数据集，报告了性能，并探讨了该领域有前景的未来研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Human parsing, deep learning, structure-driven architecture, graph-based network,
    context-aware network, LSTM-based method, combined auxiliary information approach.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人体解析，深度学习，结构驱动架构，基于图的网络，情境感知网络，基于LSTM的方法，结合辅助信息的方法。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Human parsing is an essential component in many visual understanding systems.
    It aims to assign human images into multiple human parts of fine-grained semantics
    and benefits a detailed understanding of images, some examples are shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning for Human Parsing: A Survey"). Human
    parsing plays a central role in a broad range of applications, including human-friendly
    robots [[1](#bib.bib1)], person re-identification [[2](#bib.bib2)], human behavior
    analysis [[3](#bib.bib3)], clothing style recognition and retrieval [[4](#bib.bib4)],
    to name a few. Many human parsing algorithms have been developed in the literature,
    in the early stage, such as Markov random field [[5](#bib.bib5)], support vector
    machines [[6](#bib.bib6)], cascaded pictorial structures model [[7](#bib.bib7)],
    part-based tree [[8](#bib.bib8)], quadratic deformation cost [[9](#bib.bib9)],
    binary random variable [[10](#bib.bib10)]. Over the past few years, deep learning
    for human parsing achieves remarkable performance improvements, especially, FCNet [[11](#bib.bib11)]
    was proposed.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人体解析是许多视觉理解系统中的一个重要组成部分。其目标是将人类图像划分为多个具有细粒度语义的人体部件，并有助于对图像的详细理解，如图 [1](#S1.F1
    "图 1 ‣ I 引言 ‣ 深度学习的人体解析：一项调查")所示。人体解析在广泛的应用中发挥着核心作用，包括人机友好的机器人 [[1](#bib.bib1)]、行人重识别
    [[2](#bib.bib2)]、人类行为分析 [[3](#bib.bib3)]、服装风格识别和检索 [[4](#bib.bib4)] 等。文献中已经开发了许多人类解析算法，在早期阶段，如马尔可夫随机场
    [[5](#bib.bib5)]、支持向量机 [[6](#bib.bib6)]、级联图像结构模型 [[7](#bib.bib7)]、基于部件的树 [[8](#bib.bib8)]、二次变形成本
    [[9](#bib.bib9)]、二元随机变量 [[10](#bib.bib10)]。近年来，基于深度学习的人体解析取得了显著的性能提升，尤其是 FCNet
    [[11](#bib.bib11)] 的提出。
- en: '![Refer to caption](img/8ad2e44d267233d8741077d3276d21e9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8ad2e44d267233d8741077d3276d21e9.png)'
- en: 'Figure 1: The goal of human parsing is to assign human images into multiple
    human parts of fine-grained semantics and benefit a detailed understanding of
    images.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：人体解析的目标是将人类图像划分为多个具有细粒度语义的人体部件，并有助于对图像的详细理解。
- en: 'In this paper, our motivation is to focus on the recent works in human parsing
    and discuss deep learning-based parsing 2D methods proposed until 2022\. We provide
    a comprehensive review and insight on different aspects of these methods, including
    network architectures, training data, main contributions, and their limitations.
    Deep learning-based human parsing methods can be divided into the following categories
    based on their main technical contributions: (1) structure-driven architectures,
    (2) graph-based networks, (3) context-aware networks, (4) LSTM-based methods,
    (5) combined auxiliary information approaches contain pose-based auxiliary methods,
    edge-based auxiliary methods and detection-based auxiliary methods, and (6) other
    models. Additionally, we review some of the most popular human parsing datasets
    and their performance measures used for evaluating the methods. In the end, we
    discuss several potential future directions and applications for deep learning-based
    human parsing methods.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的动机是专注于人体解析的最新研究，并讨论截至 2022 年提出的基于深度学习的 2D 解析方法。我们对这些方法的不同方面进行全面回顾和深入分析，包括网络架构、训练数据、主要贡献及其局限性。基于深度学习的人体解析方法可以根据其主要技术贡献分为以下几类：（1）结构驱动的架构，（2）基于图的网络，（3）上下文感知网络，（4）基于
    LSTM 的方法，（5）结合辅助信息的方法，包括基于姿态的辅助方法、基于边缘的辅助方法和基于检测的辅助方法，以及（6）其他模型。此外，我们回顾了一些最流行的人体解析数据集及其用于评估方法的性能指标。最后，我们讨论了深度学习人类解析方法的几个潜在未来方向和应用。
- en: 'Some key contributions of our survey can be summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查的一些关键贡献可以总结如下：
- en: '1.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Our survey focuses on the recent papers with respect to human parsing, and overviews
    deep learning-based human parsing algorithms proposed until 2022.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的调查重点关注近年来有关人体解析的论文，并概述了截至 2022 年提出的基于深度学习的人体解析算法。
- en: '2.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: This survey provides insight to human parsing methods, including network architectures,
    training data, main contributions, and their limitations.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查提供了对人体解析方法的深入了解，包括网络架构、训练数据、主要贡献及其局限性。
- en: '3.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: We review popular human parsing datasets and provide a comparative summary of
    the properties and performance of the reviewed methods for parsing purposes, on
    popular benchmarks.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了流行的人体解析数据集，并提供了对所评审方法在流行基准上的属性和性能的比较总结。
- en: '4.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: We discuss several potential future directions and applications for deep learning-based
    human parsing methods.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了深度学习基础的人体解析方法的几个潜在未来方向和应用。
- en: 'TABLE I: Taxonomy of Head Pose Estimation Approaches'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：头部姿态估计方法的分类
- en: '| Approach | Highlights | Representative Works |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 亮点 | 代表性作品 |'
- en: '| Structure-driven Architectures | Exploiting the relationship of different
    human parts and the inherent hierarchical structure of a human body. | A-AOG [[12](#bib.bib12)],
    PCNet *et al*. [[13](#bib.bib13)] |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 结构驱动的架构 | 利用不同人体部位之间的关系和人体的内在层次结构。 | A-AOG [[12](#bib.bib12)], PCNet *等*
    [[13](#bib.bib13)] |'
- en: '| Graph-based Networks | Capturing the global information to achieve an efficient
    and complete human body analysis. | Graphonomy [[14](#bib.bib14), [15](#bib.bib15)],CNIF [[16](#bib.bib16)],HHP [[17](#bib.bib17)]
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的网络 | 捕获全局信息以实现高效且完整的人体分析。 | Graphonomy [[14](#bib.bib14), [15](#bib.bib15)],
    CNIF [[16](#bib.bib16)], HHP [[17](#bib.bib17)] |'
- en: '| Context-aware Networks | Exploring useful contexts across all pixel to characterize
    a pixel of the corresponding class. | ATR [[18](#bib.bib18)], M-CNN [[19](#bib.bib19)],
    Co-CNN [[20](#bib.bib20)], SCHP [[21](#bib.bib21)] |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 上下文感知网络 | 探索所有像素的有用上下文，以表征相应类别的像素。 | ATR [[18](#bib.bib18)], M-CNN [[19](#bib.bib19)],
    Co-CNN [[20](#bib.bib20)], SCHP [[21](#bib.bib21)] |'
- en: '| LSTM-based Methods | Combining short-distance and long-distance spatial dependencies
    to better exploit abundant local and global contexts. | LG-LSTM [[22](#bib.bib22)],
    Graph LSTM [[23](#bib.bib23)], structure-evolving LSTM [[24](#bib.bib24)] |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 基于LSTM的方法 | 结合短距离和长距离空间依赖关系，以更好地利用丰富的局部和全局上下文。 | LG-LSTM [[22](#bib.bib22)],
    Graph LSTM [[23](#bib.bib23)], 结构演化LSTM [[24](#bib.bib24)] |'
- en: '| Combined Auxiliary Information Approaches | Using related tasks or supervision
    to improve network performance. | Pose-based (JMPE [[25](#bib.bib25)], SSL [[26](#bib.bib26)],
    MuLA [[27](#bib.bib27)].), Edge-based ( CorrPM [[28](#bib.bib28)], CE2P [[29](#bib.bib29)]),
    Detection-based (HAZN [[30](#bib.bib30)], Li *et al*. [[31](#bib.bib31)]) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 结合辅助信息的方法 | 利用相关任务或监督来提高网络性能。 | 基于姿态的 (JMPE [[25](#bib.bib25)], SSL [[26](#bib.bib26)],
    MuLA [[27](#bib.bib27)]), 基于边缘的 (CorrPM [[28](#bib.bib28)], CE2P [[29](#bib.bib29)]),
    基于检测的 (HAZN [[30](#bib.bib30)], Li *等* [[31](#bib.bib31)]) |'
- en: 'The remainder of the paper is organized as follows: Section 2 provides a comprehensive
    overview of deep learning-based human parsing methods according to their main
    technical contributions. We also discuss their strengths and limitation. Section
    3 introduces some of the most popular human parsing datasets and evaluation metrics.
    Section 4 discusses several applications and potential future directions. Finally,
    conclusions are given in Section 5.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下：第2节提供了基于深度学习的人体解析方法的全面概述，按照其主要技术贡献进行分类。我们还讨论了这些方法的优缺点。第3节介绍了一些最流行的人体解析数据集和评估指标。第4节讨论了几个应用和潜在的未来方向。最后，第5节给出了结论。
- en: II Deep Learning-Based Human Parsing
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度学习基础的人体解析
- en: 'This section provides a detailed review of deep learning-based human parsing
    methods proposed until 2022\. We grouped these methods into five categories based
    on their model architecture. Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Deep
    Learning for Human Parsing: A Survey") provides a list of representative systems
    for each of these categories.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '本节提供了对截至2022年提出的基于深度学习的人体解析方法的详细回顾。我们根据这些方法的模型架构将它们分为五类。表 [I](#S1.T1 "TABLE
    I ‣ I Introduction ‣ Deep Learning for Human Parsing: A Survey") 提供了每一类别的代表性系统列表。'
- en: '![Refer to caption](img/d02dbef674aeb3487d4e2e5e66d3de58.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d02dbef674aeb3487d4e2e5e66d3de58.png)'
- en: 'Figure 2: Phrase structure grammar is based on the constituency grammar which
    defines the rule to break down a node into its constituent parts. From [[12](#bib.bib12)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：短语结构语法基于成分语法，该语法定义了将节点拆分为其组成部分的规则。 来源于 [[12](#bib.bib12)]。
- en: II-A Structure-Driven Architectures
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 结构驱动的架构
- en: The human body is a natural hierarchy, how to use the prior knowledge to segment
    different human parts is an urgent problem to be solved. Park *et al*. [[12](#bib.bib12)]
    present an attribute and-or grammar (A-AOG) model for inferring human parts in
    the hierarchical representation, which also jointly represents the human pose
    and human attribute. A-AOG explicitly represents the decomposition and articulation
    of body parts and accounts for the correlations between parts. The network uses
    phrase structure grammars to represent the hierarchical decomposition of the human
    body from whole to parts, and employs dependency grammars to model the geometric
    articulation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4c468f0cbb05f3cfe01b904f7101b39.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Dependency grammar defines adjacency relations that connect the geometry
    of a part to its dependent parts. From [[12](#bib.bib12)].'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The phrase structure grammar represents human parts in a coarse-to-fine method
    based on the constituency relation. The grammar is defined as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;a\rightarrow a_{1}&#124;a_{2}&#124;a_{3},\\ \end{split}$
    |  | (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: 'where a typical nonterminal node $a\in v_{n}$, $a_{i}$ is a string of nodes
    in $v_{n}\cup v_{t}$, $v_{n},v_{t}$ denotes human parts. Fig. [2](#S2.F2 "Figure
    2 ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A
    Survey") shows an example. The root node is the upper body and decomposed into
    arms, head, and torso. The arms are further decomposed into upper-arm, lower-arm,
    and hand. Dependency grammars have been widely used in natural language processing
    for syntactic parsing. Fig. [3](#S2.F3 "Figure 3 ‣ II-A Structure-Driven Architectures
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey")
    is a parsing grammar for the upper body. The root node is the torso part as it
    is the center of the body and connected to other parts. The upper arms and head
    are the child nodes of the torso.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7be8fd55da31491a17f8d548b0ef47bf.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The framework of PCNet. From [[13](#bib.bib13)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhu *et al*. [[13](#bib.bib13)] propose a progressive cognitive structure to
    segment human parts. In the hierarchical network, the latter layers inherit information
    from former layers and pay attention to a finer component. As shown in Fig. [4](#S2.F4
    "Figure 4 ‣ II-A Structure-Driven Architectures ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey"), the given image is sent
    into a FCN to extract original features. And then the image-level (original) features
    are decomposed into a background score map and human-level features. The human-level
    features are further decomposed into upper-body features and lower-body features,
    repeating the above steps until all are segmented.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The structure-driven human parsing methods explore the inherent relations of
    human parts according to the prior knowledge of the human body. These methods
    can make the network pay more attention to the interested human body itself and
    reduce the interference of background and redundant information. However, these
    methods are easy to lead to error accumulation. For example, if the root node
    is wrong, the error will be passed to the subsequent nodes, resulting in error
    accumulation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结构驱动的人体解析方法根据人体的先验知识探讨人体部位之间的固有关系。这些方法可以使网络更加关注感兴趣的人体本身，并减少背景和冗余信息的干扰。然而，这些方法容易导致错误累积。例如，如果根节点出现错误，错误会传递到后续节点，导致错误累积。
- en: II-B Graph-Based Networks
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 基于图的网络
- en: '![Refer to caption](img/565d6a586f3f1c1b36514d370ee61f83.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/565d6a586f3f1c1b36514d370ee61f83.png)'
- en: 'Figure 5: The framework of Graphonomy. From [[14](#bib.bib14)].'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: Graphonomy的框架。来源于[[14](#bib.bib14)]。'
- en: 'The graph convolution [[32](#bib.bib32)] can effectively explore the semantic
    relationship among human parts, thus, some works [[14](#bib.bib14), [15](#bib.bib15)]
    introduce the graph convolution into the human parsing task. As shown in Fig. [5](#S2.F5
    "Figure 5 ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey"), Gong *et al*. [[14](#bib.bib14),
    [15](#bib.bib15)] design a graph-based human parsing network, named ”Graphonomy”,
    which employs graph convolution to capture the global information and semantic
    consistency. The image features extracted by the deep convolution network are
    projected into a high-level graph representation, where the body parts are nodes
    and the relationships between the parts are edges. Graphonomy first learns and
    propagates compact high-level graph representation among parts within one dataset
    via intra-graph reasoning, and then transfers semantic information across different
    datasets via inter-graph transfer. In this way, Graphonomy takes advantage of
    different granular annotated data. However, multiple datasets are required in
    the training process, which consumes many computing resources.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '图卷积[[32](#bib.bib32)]可以有效地探索人体部位之间的语义关系，因此一些研究[[14](#bib.bib14), [15](#bib.bib15)]将图卷积引入人体解析任务。如图[5](#S2.F5
    "Figure 5 ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey")所示，Gong *et al* [[14](#bib.bib14),
    [15](#bib.bib15)] 设计了一种基于图的人体解析网络，名为“Graphonomy”，它利用图卷积捕捉全局信息和语义一致性。深度卷积网络提取的图像特征被投射到高级图表示中，其中身体部位为节点，部位之间的关系为边。Graphonomy首先通过图内推理学习并传播数据集中各部位之间的紧凑高级图表示，然后通过图间传输将语义信息转移到不同的数据集中。这样，Graphonomy充分利用了不同粒度的标注数据。然而，训练过程中需要多个数据集，这消耗了大量计算资源。'
- en: 'Wang *et al*. [[16](#bib.bib16)] propose a method to explore the structural
    hierarchy of the human body by using a graph convolutional network. This method
    achieves an efficient and complete human body analysis, named an information fusion
    framework. This model models three inference processes: direct inference (directly
    predicting each part of a human body using image information), bottom-up inference
    (assembling knowledge from constituent parts), and top-down inference (leveraging
    context from parent nodes).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Wang *et al* [[16](#bib.bib16)] 提出了一种通过使用图卷积网络探索人体结构层次的方法。这种方法实现了高效且完整的人体分析，称为信息融合框架。该模型建模了三种推理过程：直接推理（直接使用图像信息预测人体的每个部位）、自下而上推理（从组成部件中汇集知识）和自上而下推理（利用父节点的上下文）。
- en: '![Refer to caption](img/1cdedef156788b1f7847e50452539dcd.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1cdedef156788b1f7847e50452539dcd.png)'
- en: 'Figure 6: Illustration of the hierarchical graph for the human parsing task.
    From [[17](#bib.bib17)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: 人体解析任务的层次图示。来源于[[17](#bib.bib17)]。'
- en: 'In addition, Wang *et al*. [[17](#bib.bib17)] represent a hierarchical graph
    for the human parsing task. There are three kinds of part relations, i.e., decomposition,
    composition, and dependency, which are completely and precisely described by three
    distinct graph networks. This method represents the human semantic structure as
    a directed, hierarchical graph $g=(\upsilon,\varepsilon,y)$. As shown in Fig. [6](#S2.F6
    "Figure 6 ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey"), the node set $\upsilon=\bigcup^{3}_{l=3}\upsilon_{l}$
    denotes human parts in three different semantic levels, including the leaf nodes
    $\upsilon_{1}$, two middle-level nodes $\upsilon_{2}$ and one root $\upsilon_{3}$.
    The edge set $\varepsilon\in(^{\upsilon}_{2})$ represents the relations between
    human parts (nodes), i.e., the directed edge. $y$ represents the ground truth
    maps. Given an input image, a fully convolution network first extracts image features
    projected into node (part) features. Then node features are sent into a hierarchical
    graph to capture expressive relation information and predicted different granularity
    of results. The whole network is trained in graph learning methods and the supervision
    is human parsing datasets.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，王*等人* [[17](#bib.bib17)] 为人体解析任务表示了一个层次化图。该方法用三种不同的图网络完全而准确地描述了三种部件关系，即分解、组合和依赖。该方法将人体语义结构表示为一个有向的层次图
    $g=(\upsilon,\varepsilon,y)$。如图 [6](#S2.F6 "Figure 6 ‣ II-B Graph-Based Networks
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey")
    所示，节点集 $\upsilon=\bigcup^{3}_{l=3}\upsilon_{l}$ 表示三个不同语义层次中的人体部件，包括叶子节点 $\upsilon_{1}$，两个中间层节点
    $\upsilon_{2}$ 和一个根节点 $\upsilon_{3}$。边集 $\varepsilon\in(^{\upsilon}_{2})$ 表示人体部件（节点）之间的关系，即有向边。$y$
    表示真实标签图。给定一张输入图像，完全卷积网络首先提取图像特征并投影到节点（部件）特征上。然后将节点特征输入到层次化图中，以捕捉表达性的关系信息并预测不同粒度的结果。整个网络在图学习方法中进行训练，监督数据为人体解析数据集。'
- en: In graph-based human parsing networks, the semantic relationships of different
    parts are introduced into the parsing models, which can amend the accumulated
    errors by communicating with each other. In general, the body parts are the nodes
    and the relationship between the parts is the edge. Due to the diversity of the
    scale, occlusion, deformation and posture of human body parts, this kind of method
    is difficult to obtain rich contextual information, which affects the recognition
    ability of the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于图的人工解析网络中，不同部件的语义关系被引入解析模型中，这可以通过相互交流来修正累计的错误。通常，身体部件是节点，部件之间的关系是边。由于人体部件的尺度、遮挡、变形和姿势的多样性，这种方法难以获取丰富的上下文信息，从而影响模型的识别能力。
- en: '![Refer to caption](img/eb99379dcd0d8bbd65583f8e9f8eb80f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb99379dcd0d8bbd65583f8e9f8eb80f.png)'
- en: 'Figure 7: The framework of the active template regression model. From [[18](#bib.bib18)].'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：活动模板回归模型的框架。来源 [[18](#bib.bib18)]。
- en: II-C Context-Aware Networks
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 上下文感知网络
- en: 'Benefiting from the development of advanced CNN architectures and training
    techniques as well as the recently released human parsing datasets, context-aware
    methods attract more and more attention. As shown in Fig. [7](#S2.F7 "Figure 7
    ‣ II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning
    for Human Parsing: A Survey"), Liang *et al*. [[18](#bib.bib18)] are the first
    to apply a convolutional neural network to the human parsing task, called active
    template regression model. The method is built on the end-to-end relation between
    the input human image and the structure outputs for human parsing. The first CNN
    network (active template network) is with max-pooling and designed to predict
    the template coefficients for each label mask, and the second (active shape network)
    is without max-pooling to preserve sensitivity to label mask position and generate
    the active shape parameters. Given an image, the outputs of the two networks are
    fused to generate the probability of each category for each pixel, and superpixel
    smoothing is finally used to refine the human parsing prediction. Extensive experiments
    demonstrate the significant superiority of the ATR framework over other contemporaneous
    states of the arts for human parsing.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '由于先进的CNN架构和训练技术的发展，以及最近发布的人体解析数据集，基于上下文的方法越来越受到关注。如图[7](#S2.F7 "Figure 7 ‣
    II-B Graph-Based Networks ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning
    for Human Parsing: A Survey")所示，梁*等人*[[18](#bib.bib18)]首次将卷积神经网络应用于人体解析任务，称为主动模板回归模型。该方法基于输入人体图像与人体解析结构输出之间的端到端关系。第一个CNN网络（主动模板网络）具有最大池化，并设计用于预测每个标签掩码的模板系数；第二个网络（主动形状网络）不使用最大池化，以保持对标签掩码位置的敏感性并生成主动形状参数。给定一张图像，两种网络的输出被融合，以生成每个像素的每个类别的概率，最后使用超像素平滑来细化人体解析预测。大量实验表明，ATR框架在人体解析方面显著优于其他同时期的技术。'
- en: 'Subsequently, Liang *et al*. [[19](#bib.bib19)] design a quasi-parametric method,
    which takes advantage of both parametric and non-parametric approaches, namely
    supervision from annotated data and the flexibility to use newly annotated images.
    Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the
    parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict
    segment results. As shown in Fig. [8](#S2.F8 "Figure 8 ‣ II-C Context-Aware Networks
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey"),
    given a testing image, its KNN images are retrieved from the manually-annotated
    image corpus. Then the input image is paired with each semantic region of its
    KNN images and each pair is fed into the M-CNN individually. The M-CNN predicts
    the matching confidence and displacements between the input image pair. All corresponding
    label maps are combined to generate a probability map for each pixel which is
    further refined by superpixel smoothing.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '随后，梁*等人*[[19](#bib.bib19)]设计了一种准参数方法，结合了参数和非参数方法，即来自标注数据的监督和使用新标注图像的灵活性。在经典的基于K最近邻（KNN）的非参数框架下，提出了参数匹配卷积神经网络（M-CNN）来预测分割结果。如图[8](#S2.F8
    "Figure 8 ‣ II-C Context-Aware Networks ‣ II Deep Learning-Based Human Parsing
    ‣ Deep Learning for Human Parsing: A Survey")所示，给定一张测试图像，从人工标注的图像库中检索其KNN图像。然后，将输入图像与每个KNN图像的语义区域配对，每对图像分别输入到M-CNN中。M-CNN预测输入图像对之间的匹配置信度和位移。所有对应的标签图被合并，以生成每个像素的概率图，并通过超像素平滑进一步细化。'
- en: '![Refer to caption](img/792712ff49ec19a345b7fe5b7e329173.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/792712ff49ec19a345b7fe5b7e329173.png)'
- en: 'Figure 8: The architecture of the quasi-parametric network. From [[19](#bib.bib19)].'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：准参数网络的架构。来源于[[19](#bib.bib19)]。
- en: '![Refer to caption](img/98a66ad801f79de47a12cd6e2cea939c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/98a66ad801f79de47a12cd6e2cea939c.png)'
- en: 'Figure 9: The framework of Contextualized Convolutional Neural Network. From [[20](#bib.bib20)].'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：上下文卷积神经网络的框架。来源于[[20](#bib.bib20)]。
- en: 'To effectively integrate all kinds of contexts into a unified model in the
    human parsing task, Liang *et al*. [[20](#bib.bib20)] propose a novel Contextualized
    Convolutional Neural Network (Co-CNN). This network integrates the cross-layer
    context, global image-level context, within-super-pixel context and cross-super-pixel
    neighborhood context to obtain rich context, improving the performance of human
    parsing (Fig. [9](#S2.F9 "Figure 9 ‣ II-C Context-Aware Networks ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")). The cross-layer
    context is captured by the basic structure, which hierarchically combines the
    global semantic and local details across different layers. Then, the global image-level
    context serves as an auxiliary object in the intermediate layer and guides the
    subsequent feature learning. Finally, the local super-pixel contexts, the within-super-pixel
    context, and the cross-super-pixel neighborhood context are formulated as natural
    subcomponents to achieve local label consistency.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在人体解析任务中有效地将各种上下文整合到一个统一的模型中，梁*等*[[20](#bib.bib20)]提出了一种新型的上下文化卷积神经网络（Co-CNN）。该网络集成了跨层上下文、全局图像级上下文、超像素内上下文和跨超像素邻域上下文，以获得丰富的上下文，提高人体解析的性能（图[9](#S2.F9
    "图 9 ‣ II-C 上下文感知网络 ‣ II 基于深度学习的人体解析 ‣ 人体解析的深度学习：综述")）。跨层上下文由基本结构捕获，该结构分层地结合了不同层次的全局语义和局部细节。然后，全局图像级上下文作为中间层的辅助对象，引导后续特征学习。最后，本地超像素上下文、超像素内上下文和跨超像素邻域上下文被构建为自然的子组件，以实现局部标签的一致性。
- en: '![Refer to caption](img/8db0153e6e1c8dec5a831920d45faabf.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8db0153e6e1c8dec5a831920d45faabf.png)'
- en: 'Figure 10: Framework of the Self-Correction for Human Parsing. From [[21](#bib.bib21)].'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：自我校正的人体解析框架。来自[[21](#bib.bib21)]。
- en: 'Li *et al*. design a noise-tolerant method, named Self-Correction for Human
    Parsing (SCHP) [[21](#bib.bib21)], to progressively promote the reliability of
    the supervised labels as well as the learned models. As shown in Fig. [10](#S2.F10
    "Figure 10 ‣ II-C Context-Aware Networks ‣ II Deep Learning-Based Human Parsing
    ‣ Deep Learning for Human Parsing: A Survey"), SCHP first takes a model trained
    with inaccurate labels as initialization and then uses a cyclical learning scheduler
    to infer more reliable pseudo masks by iteratively aggregating the models in an
    online manner. Besides, the corrected labels in turn boost the model’s performance.
    In this way, the models and the labels will reciprocally become more robust and
    accurate. The human parsing methods based on contexts can obtain rich semantics
    and details, expanding the effective receptive field. However, this approach may
    result in information decay because the network is too deep.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 李*等*设计了一种抗噪方法，名为自我校正的人体解析（SCHP）[[21](#bib.bib21)]，以逐步提升监督标签和学习模型的可靠性。如图[10](#S2.F10
    "图 10 ‣ II-C 上下文感知网络 ‣ II 基于深度学习的人体解析 ‣ 人体解析的深度学习：综述")所示，SCHP 首先以训练有误标签的模型作为初始化，然后使用周期性学习调度器通过在线方式迭代聚合模型来推断更可靠的伪掩码。此外，经过校正的标签反过来又提升了模型的性能。通过这种方式，模型和标签将相互变得更具鲁棒性和准确性。基于上下文的人体解析方法可以获得丰富的语义和细节，扩展有效的感受野。然而，这种方法可能会导致信息衰减，因为网络过于深层。
- en: II-D LSTM-based Methods
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 基于 LSTM 的方法
- en: '![Refer to caption](img/7d2a350760e068f6ea660c562031054e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7d2a350760e068f6ea660c562031054e.png)'
- en: 'Figure 11: Architecture of the LG-LSTM network. From [[22](#bib.bib22)].'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：LG-LSTM 网络的架构。来自[[22](#bib.bib22)]。
- en: 'Long short-term memory (LSTM) [[33](#bib.bib33)] uses memory cells to independently
    read, write and forget some information, retaining the main information. It can
    mitigate the decay of information. Thus, Liang *et al*. introduce LSTM into the
    human parsing task. First, Liang *et al*. [[22](#bib.bib22)] propose a novel Local-Global
    Long Short-Term Memory (LG-LSTM) to combine short- and long-distance spatial dependencies
    (Fig. [11](#S2.F11 "Figure 11 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")). LG-LSTM layer obtains
    local guidance from neighboring positions and global guidance from the whole image
    and then imposes on each position to better exploit abundant local and global
    contexts. Given an input image, the backbone obtains its original features. Then,
    these features are sent into the transition layer and several stacked LG-LSTM
    layers to improve the ability to feature. The feed-forward convolutional layers
    are appended to generate the prediction.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '长短期记忆（LSTM）[[33](#bib.bib33)]使用记忆单元独立读取、写入和遗忘一些信息，同时保留主要信息。它可以减缓信息的衰减。因此，梁*等人*将LSTM引入到人体解析任务中。首先，梁*等人*[[22](#bib.bib22)]提出了一种新型的局部-全局长短期记忆（LG-LSTM），用于结合短距离和长距离的空间依赖关系（见图[11](#S2.F11
    "Figure 11 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey")）。LG-LSTM层从邻近位置获得局部指导，从整个图像获得全局指导，然后对每个位置施加这些指导，以更好地利用丰富的局部和全局上下文。给定一张输入图像，骨干网络获取其原始特征。然后，这些特征被送入过渡层和若干堆叠的LG-LSTM层，以提高特征的能力。随后附加前馈卷积层以生成预测。'
- en: Then, Liang *et al*. further design Graph Long Short-Term Memory (Graph LSTM) [[23](#bib.bib23)]
    to improve LSTM, which is the generalization of LSTM from sequential data or multi-dimensional
    data to general graph-structured data. Graph LSTM takes each arbitrary-shaped
    super-pixel as a semantically consistent node, and adaptively constructs an undirected
    graph for each image, where the spatial relations of the super-pixels are naturally
    used as edges. The node updating sequence for Graph LSTM layers is determined
    by the confidence-drive scheme, and then Graph LSTM layers can sequentially update
    the hidden states of all super-pixel nodes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，梁*等人*进一步设计了图形长短期记忆（Graph Long Short-Term Memory，Graph LSTM）[[23](#bib.bib23)]来改进LSTM，这是一种将LSTM从序列数据或多维数据推广到一般图结构数据的技术。Graph
    LSTM将每个任意形状的超像素视为语义一致的节点，并为每张图像自适应地构建一个无向图，其中超像素的空间关系自然地作为边。Graph LSTM层的节点更新顺序由信心驱动方案确定，然后Graph
    LSTM层可以顺序更新所有超像素节点的隐藏状态。
- en: '![Refer to caption](img/0e45b351f7300e40eaa35724db4cb3e6.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0e45b351f7300e40eaa35724db4cb3e6.png)'
- en: 'Figure 12: An illustration of the structure-evolving LSTM. From [[24](#bib.bib24)].'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：结构演变LSTM的示意图。来源于[[24](#bib.bib24)]。
- en: 'In another work, Liang *et al*. propose to learn the intermediate interpretable
    multi-level graph structures in a progressive way, named structure-evolving LSTM [[24](#bib.bib24)]
    (Fig. [12](#S2.F12 "Figure 12 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")). In each LSTM layer,
    we estimate the compatibility of two connected nodes from their corresponding
    LSTM gate outputs, which is used to generate a merging probability. Then, the
    candidate graph structures are accordingly generated where the nodes are grouped
    into cliques with their merging probabilities. The network produces the new graph
    structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting
    stuck in local optimums by stochastic sampling with an acceptance probability.
    Once a graph structure is accepted, a higher-level graph is then constructed by
    taking the partitioned cliques as its nodes.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在另一项工作中，梁*等人*提出了一种以渐进方式学习中间可解释的多层图结构的方法，称为结构演变LSTM[[24](#bib.bib24)]（见图[12](#S2.F12
    "Figure 12 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey")）。在每个LSTM层中，我们从其对应的LSTM门输出中估计两个连接节点的兼容性，这用于生成合并概率。然后，相应地生成候选图结构，其中节点按其合并概率被分组为团体。网络通过Metropolis-Hasting算法生成新的图结构，该算法通过接受概率的随机采样来缓解陷入局部最优的风险。一旦图结构被接受，就通过将划分的团体作为节点来构建更高级的图。'
- en: '![Refer to caption](img/37c74a716b754d588e46c2935b295378.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37c74a716b754d588e46c2935b295378.png)'
- en: 'Figure 13: Pose-guided human parsing by an and-or graph using pose-context
    features for human parsing. From [[34](#bib.bib34)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：通过使用姿势上下文特征的与或图进行姿势引导的人体解析。来源于[[34](#bib.bib34)]。
- en: II-E Combined Auxiliary Information Approaches
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 结合辅助信息方法
- en: Human parsing is a pixel-level fine classification task, which requires rich
    and effective feature expression. Combining with human body detection, human posture
    estimation and human body edge information can greatly improve the expression
    ability of features and the performance of human body analysis.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 人体解析是一个像素级细分类任务，要求丰富而有效的特征表达。结合人体检测、人体姿态估计和人体边缘信息可以大大提高特征的表达能力和人体分析的性能。
- en: Pose-based Auxiliary Methods. In the computer vision community, human parsing
    and human pose estimation are two complementary tasks. Joints can provide object-level
    shape information for human parsing, and pixel-level analysis can constrain the
    changes of pose position.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于姿态的辅助方法。在计算机视觉领域，人类解析和人体姿态估计是两个互补的任务。关节可以为人体解析提供对象级别的形状信息，而像素级分析可以约束姿态位置的变化。
- en: 'Xia *et al*. propose a human parsing approach which uses human pose location
    as cues to provide pose-guided segment proposals for semantic parts [[34](#bib.bib34)],
    as shown in Fig. [13](#S2.F13 "Figure 13 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey"). These segment proposals
    are ranked using standard appearance cues, called pose-context. These proposals
    are selected and assembled using an and-or-graph to output a parse of the person,
    which can deal with large human appearance variability. Given a pedestrian image,
    the network first uses a pose estimation to obtain human pose joints. Then, based
    on the joints and modified RIGOR algorithm [[35](#bib.bib35)], segments aligned
    with object boundaries are generated. Human pose joints further obtain pose-context
    feature which combines segments to extract the segment features. The segment proposal
    model selects and ranks the parts. Finally, part assembling generates the paring
    results. Based on this method [[34](#bib.bib34)], Xia *et al*. further improve
    human parsing methods based on human pose estimation and design a network [[25](#bib.bib25)].
    This network trains two fully convolutional neural networks, to optimize both
    tasks simultaneously.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xia *等人* 提出了一个人类解析方法，该方法使用人体姿态位置作为线索，为语义部分提供姿态引导的分段建议[[34](#bib.bib34)]，如图
    [13](#S2.F13 "Figure 13 ‣ II-D LSTM-based Methods ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey") 所示。这些分段建议通过称为姿态上下文的标准外观线索进行排名。通过使用与或图选择并组装这些建议，输出一个处理大范围人体外观变异的解析。给定一个行人图像，网络首先使用姿态估计获取人体姿态关节。然后，基于这些关节和修改后的
    RIGOR 算法[[35](#bib.bib35)]，生成与物体边界对齐的分段。人体姿态关节进一步获得姿态上下文特征，将分段组合以提取分段特征。分段建议模型选择并排序这些部分。最后，部分组装生成配对结果。基于此方法[[34](#bib.bib34)]，Xia
    *等人* 进一步改进了基于人体姿态估计的人体解析方法，并设计了一个网络[[25](#bib.bib25)]。该网络训练两个完全卷积神经网络，以同时优化这两个任务。'
- en: '![Refer to caption](img/e4e3ad22114e6c3f5a07602851822e34.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4e3ad22114e6c3f5a07602851822e34.png)'
- en: 'Figure 14: Framework of Self-supervised Structure-sensitive Learning (SSL)
    for human parsing. From [[26](#bib.bib26)].'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 自监督结构敏感学习（SSL）的人体解析框架。来自 [[26](#bib.bib26)]。'
- en: 'Gong *et al*. develop a novel Self-supervised Structure-sensitive Learning
    (SSL) approach [[26](#bib.bib26)], which imposes human pose structure into parsing
    without resorting to extra supervision (Fig. [14](#S2.F14 "Figure 14 ‣ II-E Combined
    Auxiliary Information Approaches ‣ II Deep Learning-Based Human Parsing ‣ Deep
    Learning for Human Parsing: A Survey")). Note that there is no need to specifically
    label human joints in model training. This framework can be injected into any
    advanced networks to help incorporate rich high-level knowledge from joint and
    improve the parsing results. Given an image, the backbone generates parsing results.
    The generated joints and joints labels are obtained by computing the center points
    of corresponding regions in parsing maps. The structure-sensitives loss is generated
    by weighting segmentation loss with joint structure loss.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gong *等人* 开发了一种新颖的自监督结构敏感学习（SSL）方法[[26](#bib.bib26)]，该方法将人体姿态结构引入解析中，而无需额外监督（图
    [14](#S2.F14 "Figure 14 ‣ II-E Combined Auxiliary Information Approaches ‣ II
    Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey")）。请注意，在模型训练中无需特别标注人体关节。该框架可以注入任何先进的网络，以帮助结合来自关节的丰富高级知识并改进解析结果。给定一张图像，骨干网络生成解析结果。通过计算解析图中的相应区域的中心点来获得生成的关节和关节标签。结构敏感损失通过用关节结构损失加权分段损失生成。'
- en: Subsequently, Nie *et al*. present a Mutual Learning to Adapt model (MuLA) for
    joint human parsing and pose estimation [[27](#bib.bib27)]. MuLA predicts dynamic
    task-specific model parameters via recurrently leveraging guidance information
    from its parallel tasks. Thus MuLA combines the advantages of parsing and pose
    models to provide more powerful representations by incorporating information from
    their counterparts, generating more accurate results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，聂*等*提出了一种互学习适应模型（MuLA），用于联合人类解析和姿态估计[[27](#bib.bib27)]。MuLA通过反复利用来自并行任务的指导信息来预测动态任务特定模型参数。因此，MuLA结合了解析和姿态模型的优点，通过结合对方的信息提供更强大的表示，从而生成更准确的结果。
- en: These two tasks can improve each other’s feature representation and enhance
    the accuracy of models. However, the two tasks have different concerns. Human
    pose estimation mainly focuses on global features and cannot provide a unique
    label for each pixel. Therefore, the help to the human parsing task is limited.
    At the same time, the two optimization methods are different, which will affect
    the improvement of accuracy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个任务可以相互改善特征表示，增强模型的准确性。然而，这两个任务关注点不同。人体姿态估计主要关注全局特征，无法为每个像素提供唯一的标签。因此，对人体解析任务的帮助有限。同时，两个优化方法不同，这会影响准确性的提升。
- en: Edge-based Auxiliary Methods. In the image, the low-frequency region is the
    area with similar semantics, while the high-frequency region is usually the area
    with a larger semantic transformation ratio of area block. The edge of the human
    body is used to describe the area of human foreground and background transformation,
    which has regional differentiation. On the basis of Co-CNN [[20](#bib.bib20)],
    Liang *et al* [[36](#bib.bib36)]. add a branch to guide the separation of the
    foreground and the background regions by information on the edge of the human
    body, which improves the recognition ability of the model to different semantic
    regions. An advanced semantic boundary is used to guide pixel-level labeling and
    improve the accuracy of the model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于边缘的辅助方法。在图像中，低频区域是语义相似的区域，而高频区域通常是区域块语义变换比例较大的区域。人体的边缘用于描述人体前景和背景的转变区域，这些区域具有区域性差异。在Co-CNN[[20](#bib.bib20)]的基础上，梁*等*[[36](#bib.bib36)]增加了一个分支，通过人体边缘的信息引导前景和背景区域的分离，提高了模型对不同语义区域的识别能力。先进的语义边界被用来引导像素级标注，提高模型的准确性。
- en: '![Refer to caption](img/153916626b253209e1a15c3b39cff4f1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/153916626b253209e1a15c3b39cff4f1.png)'
- en: 'Figure 15: Framework of the Correlation Parsing Machine (CorrPM) framework.
    From [[28](#bib.bib28)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：相关性解析机器（CorrPM）框架。来源于[[28](#bib.bib28)]。
- en: 'Zhang *et al*. propose a Correlation Parsing Machine (CorrPM) [[28](#bib.bib28)]
    to study how human semantic boundaries and keypoint locations can jointly improve
    human parsing (Fig. [15](#S2.F15 "Figure 15 ‣ II-E Combined Auxiliary Information
    Approaches ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing:
    A Survey")). CorrPM uses a heterogeneous non-local block to discover the spatial
    affinity among feature maps from the edge, pose and parsing. The input images
    go through the backbone to generate features in different stages. Features of
    the second and fifth stages are sent into the paring encoder to obtain coarse
    segmentation maps. Features of the second, third and fourth stages are sent into
    the edge encoder to generate semantic boundaries. Features of the five stages
    are sent into the pose encoder to predict joint location. The heterogeneous non-local
    is appended to explore the correlation among the three factors and generates fine
    results.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '张*等*提出了一种相关性解析机器（CorrPM）[[28](#bib.bib28)]，用于研究人类语义边界和关键点位置如何共同改善人类解析（图[15](#S2.F15
    "Figure 15 ‣ II-E Combined Auxiliary Information Approaches ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")）。CorrPM 使用异质非局部块来发现来自边缘、姿态和解析的特征图之间的空间关联。输入图像经过主干网络生成不同阶段的特征。第二和第五阶段的特征被送入配对编码器以获取粗略分割图。第二、第三和第四阶段的特征被送入边缘编码器生成语义边界。五个阶段的特征被送入姿态编码器以预测关节位置。异质非局部块被附加用于探索三个因素之间的关联，并生成精细的结果。'
- en: Liu *et al*. [[29](#bib.bib29)] design a simple yet effective context embedding
    with edge perceiving (CE2P) framework, which combines feature resolution, global
    context, and edge details. CE2P brings a high-performance boost to single human
    parsing task and serve as a solid baseline for future research in single/multiple
    human parsing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 刘*等人*[[29](#bib.bib29)]设计了一个简单而有效的上下文嵌入与边缘感知（CE2P）框架，该框架结合了特征分辨率、全局上下文和边缘细节。CE2P为单个人体解析任务带来了高性能的提升，并作为未来单/多人体解析研究的坚实基线。
- en: The edge label of the human body can be obtained through the edge processing
    (e.g., Canny [[37](#bib.bib37)]) of the parsing label. This kind of natural information
    does not need additional labeling and is cost-effective. But the number of pixels
    occupied by the edges is small compared to the overall image. At present, the
    average intersection ratio is usually used to evaluate the performance of the
    model, and the edge accuracy has little advantage in this calculation method.
    Usually, the effect of the model edge has been greatly improved, but the analytical
    accuracy of human parsing has not been improved. Therefore, edge information cannot
    effectively improve the evaluation value of network performance.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 人体的边缘标签可以通过边缘处理（例如，Canny [[37](#bib.bib37)]) 从解析标签中获得。这种自然信息不需要额外的标注，且成本效益高。但边缘所占的像素数量相对于整体图像较少。目前，通常使用平均交集比来评估模型的性能，而在这种计算方法中，边缘准确性几乎没有优势。通常，模型边缘的效果已显著改善，但人体解析的分析准确性并未提升。因此，边缘信息不能有效提高网络性能的评估值。
- en: '![Refer to caption](img/8d6f7f816845ab3e5c9e7d951952597b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8d6f7f816845ab3e5c9e7d951952597b.png)'
- en: 'Figure 16: Framework of the Hierarchical Auto-Zoom Net (HAZN) framework. From [[30](#bib.bib30)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：层次化自动缩放网络（HAZN）框架的框架。来自[[30](#bib.bib30)]。
- en: 'Detection-based Auxiliary Methods. Due to the interference of complex scenes
    and similar backgrounds, it is difficult to extract complete and accurate foreground.
    Detecting the human body can reduce the interference of background information,
    making the network pay more attention to the foreground, and improving the precision
    of network analysis. As shown in Fig. [16](#S2.F16 "Figure 16 ‣ II-E Combined
    Auxiliary Information Approaches ‣ II Deep Learning-Based Human Parsing ‣ Deep
    Learning for Human Parsing: A Survey"), Xia *et al*. [[30](#bib.bib30)] design
    a Hierarchical Auto-Zoom Net (HAZN) for object parsing which adapts to the local
    scales of objects and parts. HAZN is a sequence of two Auto-Zoom Nets (AZNs),
    each one has two tasks: the first is to predict the locations and scales of object
    instances (the first AZN) or their parts (the second AZN); the second is to estimate
    the part scores for predicted object instance or part regions. Given an image,
    its part scores are predicted and refined by three FCNs with three levels of granularity,
    i.e., image-, object-, and part-level. At each level, the FCN outputs the score
    maps and generates the locations and scales for the next level.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检测的辅助方法。由于复杂场景和相似背景的干扰，提取完整准确的前景非常困难。检测人体可以减少背景信息的干扰，使网络更加关注前景，从而提高网络分析的精度。如图[16](#S2.F16
    "图16 ‣ II-E 组合辅助信息方法 ‣ II 基于深度学习的人体解析 ‣ 基于深度学习的人体解析：综述")所示，夏*等人*[[30](#bib.bib30)]设计了一种适用于目标解析的层次化自动缩放网络（HAZN），它适应于对象及其部分的局部尺度。HAZN是两个自动缩放网络（AZNs）的序列，每个网络有两个任务：第一个任务是预测对象实例（第一个AZN）或其部分（第二个AZN）的位置信息和尺度；第二个任务是估计预测的对象实例或部分区域的分数。给定一张图像，其部分分数由三个具有不同粒度的FCNs预测和细化，即图像级、对象级和部分级。在每个级别，FCN输出分数图，并生成下一层级的位置和尺度。
- en: '![Refer to caption](img/912bd1a4f641784cf83d84c068da177d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/912bd1a4f641784cf83d84c068da177d.png)'
- en: 'Figure 17: The timeline of deep learning-based human parsing algorithms, from
    2015 to 2022.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：基于深度学习的人体解析算法的时间线，从2015年到2022年。
- en: Li *et al*. [[31](#bib.bib31)] address the human parsing task by segmenting
    the parts of objects at an instance level, such that each pixel in the image is
    assigned a part label, as well as the identity of the object it belongs to. An
    input image is sent into a human detection network and a body parts semantic segmentation
    network, producing $D$ detections of human and parsing results, respectively.
    These results are used to form the unary potentials of an Instance CRF which performs
    instance segmentation by associating labeled pixels with human detections.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Li *等人* [[31](#bib.bib31)] 通过在实例级别对物体部分进行分割来解决人类解析任务，使得图像中的每一个像素都被分配一个部件标签，以及它所属的物体的身份。输入图像被送入一个人体检测网络和一个身体部位语义分割网络，分别产生
    $D$ 个人体检测和分割结果。这些结果用于形成一个实例 CRF 的一元势，该 CRF 通过将标记的像素与人体检测关联来执行实例分割。
- en: The detection methods can reduce the interference of background information
    and make the network more focused on the human body, but this method also has
    two drawbacks. On the one hand, even if the detection method is completely correct,
    the interference of background information cannot be completely avoided, and human
    parsing methods are still needed to extract the foreground from the background.
    When analyzing the results, the closer the foreground was to the body itself,
    the more disturbing it was. On the other hand, if the detection method is incorrect,
    subsequent human parsing will inherit the error, resulting in error accumulation.
    Therefore, the human body analysis method based on the detection model has some
    drawbacks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些检测方法可以减少背景信息的干扰，使网络更专注于人体，但这种方法也有两个缺点。一方面，即使检测方法完全正确，背景信息的干扰也不能完全避免，仍需要人体解析方法从背景中提取前景。在分析结果时，前景与身体本身越接近，就越容易造成干扰。另一方面，如果检测方法不正确，后续的人体解析将继承这个错误，导致错误的累积。因此，基于检测模型的人体分析方法存在一些缺陷。
- en: II-F Other Models
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 其他模型
- en: 'In addition to the above methods, there are several other deep learning methods
    for human parsing, such as the following: CDGNet [[38](#bib.bib38)] simplifies
    the complex spatial human parsing problem into the horizontal and vertical positions
    of human parts individually. Accordingly, the method builds the class distribution
    labels in the horizontal and vertical directions as new supervision signals from
    the original label of human parsing. Liu *et al*. [[39](#bib.bib39)] propose a
    Hybrid Resolution Network (HyRN) by adding deconvolution and multi-scale supervision
    on top of the HRNet with minimal extra computation overhead. HyRN aggregates rich
    high-resolution representations to predict more accurate parsing results, especially
    for small components, on which the improvement to HRNet baseline exceeds 4 pp.
    Lin *et al*. [[40](#bib.bib40)] introduce an effective framework, called-domain
    complementary learning with a pose, to leverage information in both real and synthetic
    images for multi-person part segmentation. SYSU-Clothes [[41](#bib.bib41)] parses
    clothes via joint image segmentation and labels. BSANet [[42](#bib.bib42)] proposes
    to address object part parsing in the less explored multi-class setting, and designs
    a unified network architecture to solve this important problem. Grapy-ML [[43](#bib.bib43)]
    proposes a novel graph pyramid module, which enables incorporating the hierarchical
    structural prior explicitly into feature learning via self-attention-based graph
    reasoning and progressive feature refinement. PRM [[44](#bib.bib44)] is designed
    to generate features with adaptive context for various sizes and shapes of human
    parts. HSSN [[45](#bib.bib45)] aims at a structured, pixel-wise description of
    visual observation in terms of a class hierarchy. HIPN [[46](#bib.bib46)] proposes
    a new semi-supervised human parsing method for which the method only needs a small
    number of labels for training. ISNet [[47](#bib.bib47)] proposes to augment the
    pixel representations by aggregating the image-level and semantic-level contextual
    information, respectively. AFLA [[48](#bib.bib48)] proposes a novel and efficient
    cross-domain human parsing model to bridge the cross-domain differences in terms
    of visual appearance and environment conditions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法，还有一些其他深度学习方法用于人类解析，例如：CDGNet [[38](#bib.bib38)] 将复杂的空间人类解析问题简化为人体部位的水平和垂直位置。因此，该方法在水平和垂直方向上构建了类别分布标签，作为来自原始人类解析标签的新监督信号。Liu
    *et al* [[39](#bib.bib39)] 通过在HRNet上添加反卷积和多尺度监督，提出了一个混合分辨率网络（HyRN），仅需额外的计算开销。HyRN
    汇聚丰富的高分辨率表示，以预测更准确的解析结果，特别是对小组件的改进超过了4个百分点。Lin *et al* [[40](#bib.bib40)] 引入了一个有效的框架，称为带姿态的领域互补学习，利用真实和合成图像中的信息进行多人的部件分割。SYSU-Clothes [[41](#bib.bib41)]
    通过联合图像分割和标签解析衣物。BSANet [[42](#bib.bib42)] 提出了解决在较少被探索的多类设置下的对象部件解析问题，并设计了一个统一的网络架构来解决这个重要问题。Grapy-ML [[43](#bib.bib43)]
    提出了一个新颖的图形金字塔模块，该模块通过基于自注意力的图形推理和逐步特征优化，显式地将层次结构的先验信息纳入特征学习中。PRM [[44](#bib.bib44)]
    旨在生成具有自适应上下文的特征，以适应各种尺寸和形状的人体部件。HSSN [[45](#bib.bib45)] 旨在以类别层次结构的方式对视觉观察进行结构化、逐像素的描述。HIPN [[46](#bib.bib46)]
    提出了一个新的半监督人类解析方法，该方法仅需要少量标签即可进行训练。ISNet [[47](#bib.bib47)] 提出了通过分别聚合图像级和语义级上下文信息来增强像素表示的方法。AFLA [[48](#bib.bib48)]
    提出了一个新颖而高效的跨领域人类解析模型，以桥接视觉外观和环境条件方面的跨领域差异。
- en: Instance-level human parsing [[49](#bib.bib49)] is an interesting task, which
    aims to parse multiple human instances in a single pass. There are already several
    interesting works in this direction, including PNG [[49](#bib.bib49)], NAN [[50](#bib.bib50)],
    Holistic [[31](#bib.bib31)], Parsing R-CNN [[51](#bib.bib51)], AIParsing [[52](#bib.bib52)],
    M-CE2P [[29](#bib.bib29)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 实例级人类解析 [[49](#bib.bib49)] 是一项有趣的任务，旨在一次性解析多个人体实例。在这一方向上已经有几个有趣的工作，包括PNG [[49](#bib.bib49)]、NAN [[50](#bib.bib50)]、Holistic [[31](#bib.bib31)]、Parsing
    R-CNN [[51](#bib.bib51)]、AIParsing [[52](#bib.bib52)]、M-CE2P [[29](#bib.bib29)]。
- en: Video human parsing [[53](#bib.bib53), [54](#bib.bib54)] is also another related
    task, which parses every human body in the video data, which can be regarded as
    integrating video segmentation and instance-level human parsing. ATEN [[53](#bib.bib53)]
    first leverages convolutional gated recurrent units to encode temporal feature-level
    changes, and the optical flow of non-key frames is wrapped with the temporal memory
    to generate their features. The following works are TimeCycle [[16](#bib.bib16)],
    UVC [[55](#bib.bib55)], CRW [[56](#bib.bib56)], CLTC [[57](#bib.bib57)], LIIR [[58](#bib.bib58)]
    and so on.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 视频人体解析 [[53](#bib.bib53), [54](#bib.bib54)]也是一个相关任务，它解析视频数据中的每个人体，这可以视为将视频分割与实例级人体解析整合起来。ATEN [[53](#bib.bib53)]首次利用卷积门控递归单元来编码时间特征级变化，非关键帧的光流与时间记忆一起封装以生成其特征。随后有TimeCycle [[16](#bib.bib16)]、UVC [[55](#bib.bib55)]、CRW [[56](#bib.bib56)]、CLTC [[57](#bib.bib57)]、LIIR [[58](#bib.bib58)]等。
- en: 'Some milestone human parsing methods are illustrated in Fig. [17](#S2.F17 "Figure
    17 ‣ II-E Combined Auxiliary Information Approaches ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey"). Given the large number
    of works developed in the last few years, we only show some of the most representative
    ones.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [17](#S2.F17 "图 17 ‣ II-E 组合辅助信息方法 ‣ II 基于深度学习的人体解析 ‣ 人体解析的深度学习：综述")展示了一些具有里程碑意义的人体解析方法。鉴于近年来开发了大量的工作，我们仅展示了一些最具代表性的方法。
- en: II-G Deep Learning-based Semantic Segmentation
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-G 基于深度学习的语义分割
- en: We review some of the most prominent deep learning-based semantic segmentation
    methods. In the human parsing community, many works use a semantic segmentation
    model as the image encoder of the methods, and re-train their model from those
    initial weights. This way accelerates the convergence of the network. According
    to their architectures, we divide into four groupings, e.g., fully convolutional
    networks, encoder-decoder based models, multi-scale networks, dilated convolutional
    models and deeplab family. It is noted that there are some parts shared by many
    methods, such as the encoder and decoder process, skip connections, and dilated
    convolution.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了一些最杰出的基于深度学习的语义分割方法。在人体解析社区中，许多工作将语义分割模型用作方法的图像编码器，并从这些初始权重重新训练他们的模型。这种方式加速了网络的收敛。根据它们的架构，我们将其分为四组，例如，全卷积网络、编码器-解码器模型、多尺度网络、扩张卷积模型和deeplab系列。值得注意的是，许多方法共享一些部分，例如编码器和解码器过程、跳跃连接以及扩张卷积。
- en: '![Refer to caption](img/bc4d222fd123776abcda37683d50b35c.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bc4d222fd123776abcda37683d50b35c.png)'
- en: 'Figure 18: Fully convolutional networks, which can learn dense, per-pixel dense
    predictions for semantic segmentation. From [[11](#bib.bib11)].'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：全卷积网络，它可以学习用于语义分割的密集像素级预测。来源 [[11](#bib.bib11)]。
- en: 'Fully Convolutional Networks. Fully convolutional networks (FCN) [[11](#bib.bib11)]
    observes that the fully connected layers lose the spatial information and removes
    them from popular architectures (e.g., AlexNet [[59](#bib.bib59)], VGG [[60](#bib.bib60)],
    GoogLeNet [[61](#bib.bib61)]). In this way, FCN includes only convolutional layers,
    which enables it to be applied to an image of any resolution, as shown in Fig. [18](#S2.F18
    "Figure 18 ‣ II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey"). The method results
    in homochronous state-of-the-art results in several image segmentation datasets
    and is considered one of the most influential works in the area.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 全卷积网络。全卷积网络（FCN） [[11](#bib.bib11)]观察到全连接层丧失了空间信息，并将其从流行的架构中移除（例如，AlexNet [[59](#bib.bib59)]、VGG [[60](#bib.bib60)]、GoogLeNet [[61](#bib.bib61)]）。通过这种方式，FCN仅包括卷积层，这使得它可以应用于任何分辨率的图像，如图 [18](#S2.F18
    "图 18 ‣ II-G 基于深度学习的语义分割 ‣ II 基于深度学习的人体解析 ‣ 人体解析的深度学习：综述")所示。该方法在多个图像分割数据集上取得了同时期最先进的结果，并被认为是该领域最具影响力的工作之一。
- en: 'Skip connections link the outputs of nonadjacent layers by summing or concatenating,
    which combines the semantics of deep layers and details of shallow ones to obtain
    accurate segmentation results. FCN adds links that combine the final prediction
    layer with lower layers with finer strides (Fig. [19](#S2.F19 "Figure 19 ‣ II-G
    Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based Human Parsing
    ‣ Deep Learning for Human Parsing: A Survey")). The most widely used architectures
    ’FCN-32s’, ’FCN16s’, and ’FCN8s’ are obtained by the skip connection at different
    layers. More dense skip connections for the same architecture are proposed for
    various applications [[62](#bib.bib62)].'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '跳跃连接通过求和或拼接连接非相邻层的输出，这结合了深层的语义和浅层的细节，从而获得准确的分割结果。FCN 增加了将最终预测层与步幅较小的低层结合的链接（图 [19](#S2.F19
    "Figure 19 ‣ II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")）。最广泛使用的架构 ''FCN-32s''、''FCN16s''
    和 ''FCN8s'' 是通过在不同层进行跳跃连接获得的。为各种应用提出了更密集的跳跃连接 [[62](#bib.bib62)]。'
- en: '![Refer to caption](img/462f495888654a681bf26ff0bb90858f.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/462f495888654a681bf26ff0bb90858f.png)'
- en: 'Figure 19: Skip connections combines semantics of deep layers and details of
    shallow ones to obtain accurate predictions. From [[11](#bib.bib11)].'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：跳跃连接结合了深层的语义和浅层的细节，从而获得准确的预测。来自 [[11](#bib.bib11)]。
- en: Although FCN can be trained in an end-to-end manner on any size images, it is
    not suited to deployment onboard mobile or other portable platforms due to its
    large amount of computation. To address this misalignment with more compact and
    efficient models, ENet [[63](#bib.bib63)] is proposed, one of the earliest real-time
    semantic segmentation. Comprising a larger encoder and very simple decoder, ENet
    is built out of several variations of bottleneck residual blocks comprising a
    dimensionality reduction, convolution, and provides similar or better accuracy
    with fewer parameters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 FCN 可以在任何尺寸的图像上以端到端的方式进行训练，但由于其计算量大，不适合在移动设备或其他便携平台上部署。为了解决这一问题，提出了 ENet
    [[63](#bib.bib63)]，这是最早的实时语义分割模型之一。ENet 包含一个较大的编码器和非常简单的解码器，由几个瓶颈残差块变体组成，这些块包括降维、卷积，并以更少的参数提供类似或更好的精度。
- en: FCNs are considered revolutionary in many aspects. Since their high-performance
    and easy of deployment, they promote advances in medicine (e.g., brain tumor segmentation [[64](#bib.bib64),
    [65](#bib.bib65)]), driverless driving (e.g., ERFNet [[66](#bib.bib66)]) and so
    on.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: FCNs 在许多方面被认为是革命性的。由于其高性能和易于部署，它们促进了医学（例如，脑肿瘤分割 [[64](#bib.bib64), [65](#bib.bib65)]）、无人驾驶（例如，ERFNet [[66](#bib.bib66)]）等领域的进步。
- en: '![Refer to caption](img/2f2c21135092827090a2da132d0df350.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f2c21135092827090a2da132d0df350.png)'
- en: 'Figure 20: U-net model (an example for $572\times 572$ resolution of input
    image). From [[67](#bib.bib67)].'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：U-net 模型（$572\times 572$ 分辨率的输入图像示例）。来自 [[67](#bib.bib67)]。
- en: '![Refer to caption](img/d70653209983f72c5ec4e5b3f1b85377.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d70653209983f72c5ec4e5b3f1b85377.png)'
- en: 'Figure 21: The SegNet architecture. The decoder upsamples its input using the
    pool indices from the encoder. From [[68](#bib.bib68)].'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：SegNet 架构。解码器通过使用来自编码器的池化索引对其输入进行上采样。来自 [[68](#bib.bib68)]。
- en: Encoder-Decoder Based Models. Hierarchical features created by pooling layers
    of FCN can partially lose some localization. Thus, we discuss other popular encoder-decoder
    based models which provide finer predictions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 基于编码器-解码器的模型。FCN 的池化层创建的层次特征可能会部分丢失一些定位信息。因此，我们讨论了其他流行的基于编码器-解码器的模型，它们提供了更精细的预测。
- en: 'The encoder-decoder models (as known as the U-nets) consist of two components
    (i.e., the encoder network and the decoder network). The encoder gradually reduces
    the spatial dimension by pooling or convolution layers, while the decoder recovers
    the resolution by upsampling. The decoder uses pooling indices computed in the
    max-pooling step of the corresponding encoder to perform upsampling, and concatenates
    corresponding features of the decoder. U-Net [[67](#bib.bib67)] (Fig. [20](#S2.F20
    "Figure 20 ‣ II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based
    Human Parsing ‣ Deep Learning for Human Parsing: A Survey")) and SegNet [[68](#bib.bib68)]
    (Fig. [21](#S2.F21 "Figure 21 ‣ II-G Deep Learning-based Semantic Segmentation
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey"))
    are well-known encoder-decoder algorithms. In this architecture, networks obtain
    rich details and semantics by the skip connections between the encoder and the
    decoder.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular model in this category is a high-resolution network (HRNets) [[69](#bib.bib69),
    [70](#bib.bib70)], as shown in Fig. [22](#S2.F22 "Figure 22 ‣ II-G Deep Learning-based
    Semantic Segmentation ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for
    Human Parsing: A Survey"). Different from U-Net and SegNet, they first reduce
    the spatial dimension and then recover it. HRNet maintains high-resolution representations
    through the encoding process by connecting the high-to-low resolution convolution
    streams in parallel, and repeatedly exchanging the information across resolutions.
    Many recent works on semantic segmentation, detection and facial landmark detection
    use HRNet as the backbone by exploiting contextual models.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02f91460350de5543bfe8b387245d457.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: A simple example of a high-resolution network. The main difference
    between HRNetV1 and HRNetV2 lies in the outputs of the final stages. HRNetV1 only
    generates the high-resolution features, while HRNetV2 outputs all resolution features.
    From [[70](#bib.bib70)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c976d889673d33035f6706694338a596.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: FPN involves a bottom-up pathway, a top-down pathway, and lateral
    connections. From [[71](#bib.bib71)].'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Scale and Pyramid Network Based Networks. Multi-scale is an old method
    in image processing, which is employed in many computer vision tasks. One of the
    most prominent models of this sort is the Feature Pyramid Network (FPN) [[71](#bib.bib71)]
    (Fig. [23](#S2.F23 "Figure 23 ‣ II-G Deep Learning-based Semantic Segmentation
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey")),
    which is developed mainly for object detection and also applied to segmentation [[72](#bib.bib72),
    [65](#bib.bib65)]. Many methods use complex network structures to capture the
    inherent multi-scale context. FPN designs a simple and effective method that consists
    of a bottom-up pathway, a top-down pathway, and lateral connections. The concatenated
    feature maps are sent into a $3\times 3$ convolution to generate the output of
    each stage. Finally, each stage of the top-down pathway produces a prediction
    to detect an object.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8204c56fb035d573df9f6647a1b958dd.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The overview of the PSPNet. The pyramid pooling module extracts
    different sub-regions by adopting varying-size pooling kernels in different strides.
    Upsampling and concatenation are used to form the final pixel-level prediction.
    From [[73](#bib.bib73)].'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'PSPNet [[73](#bib.bib73)] uses the pyramid pooling module to capture the multi-scale
    global contexts for scene parsing, as shown in Fig. [24](#S2.F24 "Figure 24 ‣
    II-G Deep Learning-based Semantic Segmentation ‣ II Deep Learning-Based Human
    Parsing ‣ Deep Learning for Human Parsing: A Survey"). Given an input image, PSPNet
    first uses CNN to extract features, and then sends these features to the pyramid
    pooling model to harvest different scale pattern representations. There are four
    different scales, each scale corresponds to a pyramid level and is followed by
    a $1\times 1$ convolutional layer to reduce dimensions. The outputs of the pyramid
    pooling module are upsampling and concatenating with the original features, which
    can obtain local and global context information. Finally, the representation is
    fed into a convolutional layer to generate the final pixel-level prediction.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Other models are using multi-scale analysis for semantic segmentation, such
    as LRR (Laplacian Pyramid Reconstruction) [[74](#bib.bib74)] designs a Laplacian
    pyramid for semantic segmentation, RefineNet [[75](#bib.bib75)] proposes a multi-path
    refinement network for accurate prediction, and Multi-Scale Context Intertwining
    (MSCI) [[76](#bib.bib76)] employs multi-scale context for segmentation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2aacdedd8f8403c48cb1b3b806a30f8b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: An example of dilated convolution. (a) Sparse feature extraction
    with standard convolution. (b) Dense feature extraction with dilated convolution
    with rate 2\. From [[77](#bib.bib77)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Dilated Convolutional Models and DeepLab Family. The stack pooling and striding
    at consecutive layers reduces the resolution of the features, typically by a factor
    of 32 across each direction in recent DCNNs, affecting the segmentation accuracy
    of small objects. DeepLabv1 [[78](#bib.bib78)] introduces atrous convolution (a.k.a.
    dilated convolution) which computes the responses of any layer at any desired
    resolution. Comparing the common convolution, atrous adds an extra parameter,
    rate parameter $r$, which corresponds to the stride with the input signal. It
    is defined as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;y[i]=x[i+r\cdot k]w[k],\\ \end{split}$ |  | (2) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: 'where the output $y[i]$ of atrous convolution of a 1-d input signal $x[i]$
    with a filter $w[k]$ of length $k$. See Fig. [25](#S2.F25 "Figure 25 ‣ II-G Deep
    Learning-based Semantic Segmentation ‣ II Deep Learning-Based Human Parsing ‣
    Deep Learning for Human Parsing: A Survey") for illustration, for example, a $3\times
    3$ kernel with a dilation rate of 2 will have the same size of receptive field
    as a $5\times 5$ kernel while using only 9 parameters, thus enlarging the receptive
    field with a negligible increase in computational cost.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78cf4d02977148320de7d0e161eb84a1.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: The DeepLabv3 model with image-level features. From [[79](#bib.bib79)].'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The DeepLab series is based on the FCN concept. Four versions of the deeplab
    series are called DeepLabv1 [[78](#bib.bib78)], DeepLabv2 [[77](#bib.bib77)],
    DeepLabv3 [[79](#bib.bib79)] and DeepLabv3+ [[80](#bib.bib80)]. DeepLabv1 lays
    the foundation for the other approaches. DeepLabv1 has two contributions. First,
    it uses atrous convolution to address the decreasing resolution in the network
    and increases the receptive field without adding computation. Second, it combines
    the responses at the final dcnn layer with a fully connected conditional random
    field (crf), which deals with the poor localization property of deep networks.
    DeepLabv2 proposes atrous spatial pyramid pooling (Aspp) to robustly segment objects
    at multiple scales. Aspp probes an incoming convolutional feature layer with filters
    at multiple sampling rates and effective fields-of-views, thus capturing objects
    and context at multiple scales.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, DeepLabv3 [[79](#bib.bib79)] combines cascaded and parallel modules
    of atrous convolutions. The parallel convolution modules are grouped in the Aspp
    with image-level features encoding global context and further boost performance,
    as shown in Fig. [26](#S2.F26 "Figure 26 ‣ II-G Deep Learning-based Semantic Segmentation
    ‣ II Deep Learning-Based Human Parsing ‣ Deep Learning for Human Parsing: A Survey").
    All the outputs are concatenated and processed by a $1\times 1$ convolution to
    create the final prediction.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: DeepLabv3+ employs an encoder-decoder architecture. It uses DeepLabv3 as the
    encoder and modifies the xception backbone with more layers. What’s more, dilated
    depthwise separable convolutions instead of max pooling and batch normalization.
    The decoder first takes the low-level features from the network backbone as inputs
    and then concatenates the outputs of the Aspp. The simple yet effective decoder
    module refines the segmentation results along object boundaries.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: III Human Parsing Datasets and Evaluation Metrics
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the last decades, significant efforts have been made to develop various
    methods for human parsing. It is important to introduce some publicly available
    benchmark datasets and evaluation metrics. We also provide the quantitative performance
    of the promising models on popular datasets.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/539c7ae7c42b98200d2a64edd87e7a72.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Five sample images with their corresponding ground truth from the
    PASCAL-Person-Part, LIP, CIHP, PPSS, and ATR, respectively. From [[81](#bib.bib81),
    [26](#bib.bib26), [49](#bib.bib49), [82](#bib.bib82), [18](#bib.bib18)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: III-A Datasets
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PASCAL-Person-Part [[81](#bib.bib81)] is one of the most representative and
    widely used datasets in the human parsing task. There are multiple people appearances
    in an unconstrained environment. Its images mainly come from PASCAL VOC  [[81](#bib.bib81)]
    dataset of image semantic segmentation. This human parsing dataset has 7 classes:
    background, head, torso, upper-arm, lower-arm, upper-leg and lower-leg. This dataset
    is divided into two sets, training, and test, with 1716 and 1817 images, respectively.
    Fig. [27](#S3.F27 "Figure 27 ‣ III Human Parsing Datasets and Evaluation Metrics
    ‣ Deep Learning for Human Parsing: A Survey") shows an example image and its pixel-wise
    label.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: LIP [[26](#bib.bib26)] is one of the most popular datasets in the task, which
    is a single-person dataset. Its images are captured from a wider range of viewpoints,
    occlusions, and complex backgrounds. LIP defines 19 human parts (clothes) labels,
    including hat, hair, sunglasses, upper-clothes, dress, coat, socks, pants, gloves,
    scarf, skirt, jumpsuits, face, right-arm, left-arm, right-leg, left-leg, right-shoe
    and left-shoe, and a background class. There are 50,462 images, including 30,362
    for training, 10,000 for testing and 10,000 for validation. The testing set annotation
    is not disclosed, and the precision of the model on the testing set needs to be
    obtained through online evaluation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: CIHP [[49](#bib.bib49)] is a new large-scale multi-person benchmark with pixel-wise
    annotations on 19 semantic part labels, each image includes about three people.
    The images are collected from real-world scenarios, containing persons appearing
    with challenging poses and viewpoints, heavy occlusions, and a wide range of resolutions.
    This benchmark is divided into three sets, 28,280 images for training, 5,000 images
    for validation, and 5,000 for testing. There is a private test set for the actual
    challenge.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CIHP [[49](#bib.bib49)] 是一个新的大规模多人人物基准，具有19个语义部分标签的像素级标注，每张图像包含大约三个人。图像采集自实际场景，包含具有挑战性的姿势和视角、严重遮挡以及各种分辨率。该基准分为三个集合，训练集包含28280张图像，验证集包含5000张图像，测试集包含5000张图像。实际挑战还有一个私有测试集。
- en: PPSS [[82](#bib.bib82)] is collected from 171 surveillance videos, it can reflect
    shading and lighting changes in real scenes. There are eight categories of the
    dataset, including hair, face, top, bottom, arms, legs, and shoes. There are 3673
    annotated images in his dataset, including 1781 training images and 1892 test
    images.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: PPSS [[82](#bib.bib82)] 收集自171段监控视频，它能够反映实际场景中的阴影和光照变化。数据集中包含八个类别，包括头发、脸部、上衣、下装、手臂、腿部和鞋子。该数据集中有3673张标注图像，其中包括1781张训练图像和1892张测试图像。
- en: Fashionista [[83](#bib.bib83)] consists of 158,235 images collected from Chictopia.com,
    a social networking website for fashion bloggers. It selects 685 images with good
    visibility of the full body for training and evaluation and is divided into ten
    folders, where the 9 folders serve as the training set, and the remaining 1 folder
    serves as the test set. This dataset includes a total of 56 categories, of which
    53 are common clothing categories (e.g., dresses, bags, jackets, skirts, boots,
    sweaters) and the remaining three are hair, skin, and background.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Fashionista [[83](#bib.bib83)] 由从Chictopia.com收集的158,235张图像组成，这是一个针对时尚博主的社交网络网站。选择了685张全身可见的图像用于训练和评估，并分为十个文件夹，其中9个文件夹作为训练集，其余1个文件夹作为测试集。该数据集总共有56个类别，其中53个是常见的服装类别（例如连衣裙、包、夹克、裙子、靴子、毛衣），其余三个是头发、皮肤和背景。
- en: Colorful Fashion Parsing Data (CFPD) [[84](#bib.bib84)] is a dataset containing
    color attributes of clothes and part categories. This dataset has 13 colors and
    23 categories of tags. To balance the labeling efficiency and accuracy, they first
    over-segment each image into about 400 patches. There are 2,682 photos to form
    the final CFPD, randomly selecting half of these images as the training set, and
    taking the other half as the test set.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Colorful Fashion Parsing Data (CFPD) [[84](#bib.bib84)] 是一个包含服装颜色属性和部分类别的数据集。该数据集有13种颜色和23个类别的标签。为了平衡标注效率和准确性，他们首先将每张图像过度分割为大约400个补丁。最终CFPD由2682张照片组成，随机选择一半的图像作为训练集，另一半作为测试集。
- en: 'Daily Photos dataset [[85](#bib.bib85)] consists of 2,500 images, all the pixels
    in the images are thoroughly annotated with 18 types of labels. The training:
    testing ratio is 2:1.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Daily Photos dataset [[85](#bib.bib85)] 由2500张图像组成，所有图像中的像素都被彻底标注了18种标签。训练与测试的比例为2:1。
- en: 'ATR [[18](#bib.bib18)] is a combined dataset from four major sources: 685 images
    from the Fashionista; 2,682 images from the CFPD; 2,500 images from the daily
    photos and 1,833 images from the Human Parsing in the Wild (HPW). There are 18
    categories in the dataset, including face, sunglasses, hat, scarf, hair, clothes,
    left arm, right arm, belt, pants, left leg, right leg, skirt, left shoe, right
    shoe, bag, dress, and other common 17 categories and 1 background. This dataset
    includes 7,700 annotated images, including 6,000 images for training, 700 images
    for validation, and 1,000 for testing.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ATR [[18](#bib.bib18)] 是一个来自四个主要来源的综合数据集：Fashionista的685张图像；CFPD的2682张图像；每日照片的2500张图像和来自Human
    Parsing in the Wild (HPW)的1833张图像。数据集中有18个类别，包括脸部、太阳镜、帽子、围巾、头发、衣物、左臂、右臂、腰带、裤子、左腿、右腿、裙子、左鞋、右鞋、包、连衣裙，以及其他17个常见类别和1个背景。该数据集包含7700张标注图像，其中包括6000张用于训练，700张用于验证，1000张用于测试。
- en: Fashion Clothing dataset [[86](#bib.bib86)] contains a total of 4,371 images
    from Clothing Co-Parsing (CCP) [[87](#bib.bib87)], Fashionista [[83](#bib.bib83)]
    and Colorful Fashion Parsing Data (CFPD) [[84](#bib.bib84)]. It focuses more on
    human clothing details, and it consists of a background and 17 labels representing
    jewelry, bag, coat, suit, dress, glass, hair, pants, shoes, shirt, skin, skirt,
    upper-clothes, vest, and underwear, respectively. This dataset can relatively
    reflect the network’s capability to parse object details.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion Clothing 数据集 [[86](#bib.bib86)] 包含来自 Clothing Co-Parsing (CCP) [[87](#bib.bib87)]、Fashionista
    [[83](#bib.bib83)] 和 Colorful Fashion Parsing Data (CFPD) [[84](#bib.bib84)] 的总计
    4,371 张图像。该数据集更侧重于人类服装细节，包括一个背景和 17 个标签，分别代表珠宝、包、外套、西装、连衣裙、眼镜、头发、裤子、鞋子、衬衫、皮肤、裙子、上衣、背心和内衣。这个数据集能够相对反映网络解析物体细节的能力。
- en: III-B Metrics for Segmentation Models
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 分割模型指标
- en: We introduce the most popular metrics for assessing the accuracy of human parsing
    methods.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了评估人类解析方法准确性的最常用指标。
- en: 'Pixel Accuracy is the ratio between the number of pixels correctly classified
    in the index dataset and the total number of pixels. The calculation formula is
    as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 像素准确率是指在索引数据集中正确分类的像素数量与总像素数量之间的比率。计算公式如下：
- en: '|  | $PA=\frac{{\sum\limits_{i}^{C}{{t_{ii}}}}}{{\sum\limits_{i}^{C}{\sum\limits_{j}^{C}{{t_{ij}}}}}},$
    |  | (3) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $PA=\frac{{\sum\limits_{i}^{C}{{t_{ii}}}}}{{\sum\limits_{i}^{C}{\sum\limits_{j}^{C}{{t_{ij}}}}}},$
    |  | (3) |'
- en: where $C$ denotes the number of categories, $t_{ii}$ denotes pixel $i$ is accurately
    classified as $i$, $t_{ij}$ denotes the real labels for $i$ pixel discriminant
    for $j$ class. The pixel accuracy ranges from 0 to 1, the higher the value, the
    better.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C$ 表示类别数，$t_{ii}$ 表示像素 $i$ 被准确分类为 $i$，$t_{ij}$ 表示像素 $i$ 对于 $j$ 类的真实标签。像素准确率范围从
    0 到 1，值越高，效果越好。
- en: Mean Pixel Accuracy (MPA) is the extended version of PA, which averages PA over
    all classes, obtaining the final value.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 平均像素准确率 (MPA) 是 PA 的扩展版本，它对所有类别的 PA 进行平均，从而获得最终值。
- en: 'Intersection over Union (IoU) or the Jaccard Index is one of the most commonly
    used metrics in human parsing. It calculates the ratio of the intersection and
    union between the predicted region of the class and the region of the real label
    class in each category. For the $k$ category, IoU is defined as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 交并比 (IoU) 或 Jaccard 指数是人类解析中最常用的指标之一。它计算预测类别区域与真实标签类别区域之间的交集和并集的比率。对于 $k$ 类，IoU
    定义如下：
- en: '|  | $IoU=\frac{A\cap B}{A\cup B},$ |  | (4) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $IoU=\frac{A\cap B}{A\cup B},$ |  | (4) |'
- en: where $A$ and $B$ denote the ground truth and the predicted segmentation maps,
    respectively.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 和 $B$ 分别表示真实标签和预测分割图。
- en: mean Intersection over Union (mIoU) is the extended version of IoU, which averages
    over the total number of classes. It is widely used in reporting the performance
    of modern human parsing algorithms.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 平均交并比 (mIoU) 是 IoU 的扩展版本，它对所有类别的平均值进行计算。它在报告现代人类解析算法性能时被广泛使用。
- en: F-1 score (F-1) is a common metric used to evaluate the accuracy of a model.
    It combines precision and recall to measure uniformly and is calculated by the
    harmonic average of both indicators.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: F-1 分数 (F-1) 是一种常用的度量模型准确性的指标。它结合了精确度和召回率进行统一测量，并通过两者指标的调和平均来计算。
- en: '|  | $F-1=2\cdot\frac{Precision\cdot Recall}{Precision+Recall}.$ |  | (5) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $F-1=2\cdot\frac{Precision\cdot Recall}{Precision+Recall}.$ |  | (5) |'
- en: Foreground Pixel Accuracy (FGAcc) only calculates the pixel accuracy of foreground
    human parts.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 前景像素准确率 (FGAcc) 仅计算前景人类部位的像素准确率。
- en: III-C Quantitative Performance of Methods
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 方法的定量性能
- en: 'In this section, we outline the performance of several of the previous methods
    on popular segmentation benchmarks. Since some models are not reported their performance
    on standard datasets, it is hard to make a full comparison. The following tables
    summarize the performances of several of the prominent models on different datasets.
    Table [II](#S3.T2 "TABLE II ‣ III-C Quantitative Performance of Methods ‣ III
    Human Parsing Datasets and Evaluation Metrics ‣ Deep Learning for Human Parsing:
    A Survey") focuses on the PASCAL-Person-Part test set. Since the introduction
    of FCN-based models, the performance has been greatly improved. Table [III](#S3.T3
    "TABLE III ‣ III-C Quantitative Performance of Methods ‣ III Human Parsing Datasets
    and Evaluation Metrics ‣ Deep Learning for Human Parsing: A Survey") focuses on
    the LIP validation. On this dataset, the latest model improved by 32.01% in terms
    of mIoU over the original model. Table [IV](#S3.T4 "TABLE IV ‣ III-C Quantitative
    Performance of Methods ‣ III Human Parsing Datasets and Evaluation Metrics ‣ Deep
    Learning for Human Parsing: A Survey") focuses on the CIHP test set. This dataset
    is more challenging than the PASCAL-Person-Part dataset, both two datasets are
    multi-person. Table [V](#S3.T5 "TABLE V ‣ III-C Quantitative Performance of Methods
    ‣ III Human Parsing Datasets and Evaluation Metrics ‣ Deep Learning for Human
    Parsing: A Survey") focuses on the ATR test set. The latest model achieves 87.16
    in terms of F-1\. Finally, Table [VI](#S3.T6 "TABLE VI ‣ III-C Quantitative Performance
    of Methods ‣ III Human Parsing Datasets and Evaluation Metrics ‣ Deep Learning
    for Human Parsing: A Survey") summarizes the performance of several models for
    the Fashion Clothing dataset.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Quantitative results on PASCAL-Person-Part test in terms of mean
    pixel Intersection-over-Union (mIoU) (%)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | mIoU |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| HAZA [[30](#bib.bib30)] | 2016 | ECCV | VGG16 | 57.54 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| LIP [[26](#bib.bib26)] | 2017 | CVPR | ResNet101 | 59.36 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| MMAN [[88](#bib.bib88)] | 2018 | ECCV | ResNet101 | 59.91 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| Graph LSTM [[23](#bib.bib23)] | 2016 | ECCV | VGG16 | 60.61 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| SE LSTM [[24](#bib.bib24)] | 2017 | CVPR | VGG16 | 63.57 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Joint [[25](#bib.bib25)] | 2017 | CVPR | ResNet101 | 64.39 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| MuLA [[27](#bib.bib27)] | 2018 | ECCV | VGG16 | 65.1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| PCNet [[13](#bib.bib13)] | 2018 | AAAI | PSPNet101 | 65.90 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| Holistic [[31](#bib.bib31)] | 2017 | BMVC | ResNet101 | 66.3 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| WSHP [[89](#bib.bib89)] | 2018 | CVPR | VGG16 | 67.60 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| SPGNet [[90](#bib.bib90)] | 2019 | ICCV | ResNet-101 | 68.36 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| PGN [[49](#bib.bib49)] | 2018 | ECCV | ResNet101 | 68.40 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| RefineNet [[75](#bib.bib75)] | 2017 | CVPR | ResNet101 | 68.6 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[16](#bib.bib16)] | 2019 | ICCV | ResNet101 | 70.76 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| DTCF [[39](#bib.bib39)] | 2020 | ACMMM | ResNet101 | 70.80 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| Graphonomy [[14](#bib.bib14)] | 2019 | CVPR | DeepLab v3+ | 71.14 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| DPC [[91](#bib.bib91)] | 2018 | NeurIPS | - | 71.34 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| SNT [[92](#bib.bib92)] | 2020 | ECCV | ResNet101 | 71.59 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| NPPNet [[93](#bib.bib93)] | 2021 | ICCV | ResNet101 | 71.73 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| CDCL [[40](#bib.bib40)] | 2021 | TCSVT | ResNet101 | 72.82 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| PRHP [[94](#bib.bib94)] | 2021 | TPAMI | ResNet101 | 72.82 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| BGNet [[95](#bib.bib95)] | 2020 | ECCV | ResNet101 | 73.12 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| GWNet [[96](#bib.bib96)] | 2022 | TIP | ResNet101 | 74.67 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Quantitative results on LIP val in terms of mean pixel Intersection-over-Union
    (mIoU) (%)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | mIoU |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| FCN-8s [[11](#bib.bib11)] | 2015 | CVPR | VGG16 | 28.29 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| DeepLabV2 [[77](#bib.bib77)] | 2017 | TPAMI | ResNet101 | 41.64 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| Attention [[97](#bib.bib97)] | 2016 | CVPR | VGG16 | 42.92 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| DeepLab-ASPP [[77](#bib.bib77)] | 2017 | TPAMI | VGG16 | 44.03 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| LIP [[26](#bib.bib26)] | 2017 | CVPR | ResNet101 | 44.73 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| MMAN [[88](#bib.bib88)] | 2018 | ECCV | VGG16 | 46.81 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| JPPNet [[98](#bib.bib98)] | 2018 | TPAMI | PSPNet101 | 51.37 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| CE2P [[29](#bib.bib29)] | 2019 | AAAI | ResNet101 | 53.10 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[99](#bib.bib99)] | 2019 | ACMMM | PSPNet | 54.4 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| SNT [[92](#bib.bib92)] | 2020 | ECCV | ResNet101 | 54.8 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[28](#bib.bib28)] | 2020 | CVPR | ResNet101 | 55.33 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| SLRS [[21](#bib.bib21)] | 2020 | CVPR | ResNet101 | 56.34 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| BGNet [[95](#bib.bib95)] | 2020 | ECCV | ResNet101 | 56.82 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| GWNet [[96](#bib.bib96)] | 2022 | TIP | ResNet101 | 57.26 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[16](#bib.bib16)] | 2019 | ICCV | ResNet101 | 57.74 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| DTCF [[39](#bib.bib39)] | 2020 | ACMMM | ResNet101 | 57.82 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| NPPNet [[93](#bib.bib93)] | 2021 | ICCV | ResNet101 | 58.56 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| HHP [[17](#bib.bib17)] | 2020 | CVPR | DeepLabV3 | 59.25 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[21](#bib.bib21)] | 2020 | TPAMI | ResNet101 | 59.36 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| CDGNet [[38](#bib.bib38)] | 2022 | CVPR | ResNet101 | 60.30 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Quantitative results on CIHP test in terms of mean pixel Intersection-over-Union
    (mIoU) (%)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | mIoU |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| PGN [[49](#bib.bib49)] | 2018 | ECCV | ResNet101 | 55.80 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Graphonomy [[14](#bib.bib14)] | 2019 | CVPR | DeepLab v3+ | 58.58 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| M-CE2P [[29](#bib.bib29)] | 2019 | AAAI | ResNet101 | 59.50 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[28](#bib.bib28)] | 2020 | CVPR | ResNet101 | 60.18 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[99](#bib.bib99)] | 2019 | ACMMM | PSPNet | 60.62 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| SNT [[92](#bib.bib92)] | 2020 | ECCV | ResNet101 | 60.87 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| PCNet [[100](#bib.bib100)] | 2020 | CVPR | ResNet101 | 61.05 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| CDGNet [[38](#bib.bib38)] | 2022 | CVPR | ResNet101 | 65.56 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Quantitative results on ATR test in terms of Foreground Pixel Accuracy
    (FGAcc) and F-1 score (F-1)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | F.G.Acc | F-1 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| ATR [[97](#bib.bib97)] | 2015 | TPAMI | - | 71.04 | 64.38 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| Attention [[97](#bib.bib97)] | 2016 | CVPR | VGG16 | 85.71 | 77.23 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| Co-CNN [[20](#bib.bib20)] | 2015 | ICCV | VGG16 | 83.57 | 80.14 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| LG-LSTM [[22](#bib.bib22)] | 2016 | CVPR | VGG16 | 84.79 | 80.97 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| TGPNet [[86](#bib.bib86)] | 2018 | ACMMM | VGG16 | 87.91 | 81.76 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[16](#bib.bib16)] | 2019 | ICCV | ResNet101 | 85.51 |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[28](#bib.bib28)] | 2020 | CVPR | ResNet101 | 90.40 | 86.12 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| HHP [[17](#bib.bib17)] | 2020 | CVPR | DeepLabV3 | 89.23 | 87.25 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| CDGNet [[38](#bib.bib38)] | 2022 | CVPR | ResNet101 | 90.19 | 87.16 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Quantitative results on Fashion Clothing test in terms of Foreground
    Pixel Accuracy (FGAcc) and F-1 score (F-1).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pub. | Backbone | F.G.Acc | F-1 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| DeepLabV2 [[77](#bib.bib77)] | 2018 | TPAMI | VGG16 | 56.08 | 37.09 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| Attention [[97](#bib.bib97)] | 2016 | CVPR | VGG16 | 64.47 | 48.68 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| TGPNet [[86](#bib.bib86)] | 2018 | ACMMM | VGG16 | 66.37 | 51.92 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[94](#bib.bib94)] | 2021 | TPAMI | ResNet101 | 68.59 | 58.12 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| PRHP [[94](#bib.bib94)] | 2021 | TPAMI | ResNet101 | 70.57 | 60.19 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: IV Opportunities and Application
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human parsing has made remarkable progress thanks to deep learning. We will
    introduce some of the promising research directions for further advancing human
    parsing algorithms.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Weakly-Supervised and Unsupervised Learning
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although supervised human parsing methods have demonstrated impressive results,
    their performance highly depends on the quantity and quality of training data.
    These data are a labor-intensive process that spends a significant amount of time
    and money. Weakly and Semi supervised [[101](#bib.bib101)] are becoming very active
    research communities. These approaches focus on knowledge transfer methods that
    migrate features with semantic labels to areas where collecting labels is difficult.
    In this way, the annotation of data can be reduced and the application scope can
    be expanded. For example, Fang *et al*. introduce pose-guide transfer [[89](#bib.bib89)]
    for human parsing. Unsupervised learning is another promising direction that is
    attracting much attraction in various fields. In the field of segmentation, it
    adaptively divides different semantic parts by semantic consistency without the
    need for annotated data. Zhang *et al* [[102](#bib.bib102)] and Liu *et al* [[103](#bib.bib103)]
    design unsupervised human parsing methods which focus on simple gestures and cannot
    handle complex gestures or missing parts. More accurate methods should be further
    discovered and applied earlier.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Total-body Human Parsing
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Social communication is a key function of human motion [[104](#bib.bib104)].
    People communicate tremendous amounts of information with the subtlest movements [[105](#bib.bib105)],
    such as a glance and a smile. However, there are no existing systems simultaneously
    containing human parsing, face parsing, and hand parsing [[106](#bib.bib106),
    [107](#bib.bib107)] to fully understand the pixel-wise spatial attributes of human
    in the wild. In this task, the toughest challenge is the total-body human parsing
    datasets. Labeling a complete and accurate pixel-level dataset is a huge task.
    A simple method is first to generate some false face and hand labels by high accuracy
    face parsing and hand parsing methods, respectively. And then false labels and
    ground truth of human parsing form the labels of total-body. Therefore, the integrated
    dataset and total-body human parsing methods need to be developed.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Real-World Open-Set Methods
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In real-world human parsing tasks, limited by various objective factors, it
    is difficult to collect training samples to exhaust all parts. Therefore, most
    methods cannot guarantee consistently good performance in practical scenarios.
    A more realistic scenario is open set recognition (OSR) [[108](#bib.bib108)],
    where incomplete knowledge of the world exists at training time, and unknown classes
    can be submitted to an algorithm during testing, requiring the classifiers to
    not only accurately classify the seen classes, but also effectively deal with
    unseen ones. Recently, some open-set methods are proposed in semantic segmentation.
    For example, DMLNet [[109](#bib.bib109)] detects both in-distribution and out-of-distribution
    objects with an incremental few-shot learning module to gradually incorporate
    those OOD objects into its existing knowledge base. The human body has various
    poses and many clothing types, and it is hard to define them all. Thus, real-world
    open-set methods are necessary.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Memory Efficient and Real-time Approaches
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many applications, such as mobile terminals, are critical to the parsing models
    that have a small amount of memory and/or can run in near real-time. To reduce
    memory, this can be done either by using simpler models, or by using model compression
    techniques or even training a complex model and then using knowledge distillation
    techniques to compress it into a smaller, memory efficient network that mimics
    the complex model. Real-time is associated with a small amount of memory which
    speeds up inference and allows no less than 25 frames per second to be processed,
    achieving real-time.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Interpretable Deep Models
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep learning-based human parsing method is a series of modeling methods
    for computers to improve prediction or behavior based on data. The current decision
    support system based on machine learning is a great improvement over the traditional
    rule-based method. Despite the advantages of machine learning models, users often
    question their decisions due to their lack of interpretability. To improve the
    transparency of machine learning models, building trust between users and machine
    learning models, and reducing potential risks in applications, such as bias in
    models, it is very necessary to provide model explanations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Application Scenarios
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human parsing has been widely used in many applications such as person re-identification,
    pose estimation, and dress people.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Person Re-identification. Person re-identification [[110](#bib.bib110)] aims
    to associate the person images captured by different cameras from various viewpoints,
    which attracts increasing attention from both the academia and the industry. However,
    part occlusions and inaccurate person detection can significantly change the visual
    appearance of a person in images and greatly increase the difficulty of this retrieval
    problem. Some methods [[111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]
    inject extra semantics in terms of the part to achieve the part alignment at the
    pixel-level. In this way, human parsing provides semantic information to help
    re-identification models perceive the position of the appearance of the human
    body.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Pose Estimation. Pose estimation and human parsing are two crucial yet challenging
    tasks for human body configuration in 2D monocular images, these two tasks are
    highly correlated and could provide beneficial information for each other. Human
    parsing can facilitate localizing body joints in difficult scenarios. For example,
    MuLA [[27](#bib.bib27)] can fast adapt parsing and pose models to provide more
    powerful representations by incorporating information from their counterparts,
    giving more robust and accurate pose results. Dense pose estimation aims at mapping
    all human pixels of an RGB image to the 3D surface of the human body [[114](#bib.bib114)].
    The mainstream dense pose estimation methods explicitly integrate human parsing
    supervision, such as DensePose R-CNN [[114](#bib.bib114)], Parsing R-CNN [[51](#bib.bib51)].
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Dress People. The 3D reconstruction and modeling of humans from images is a
    central problem in computer vision [[115](#bib.bib115)]. However, many methods [[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118)] lack realism and control. Multi-Garment
    Network (MGN) [[115](#bib.bib115)] first models the inferring human body and layered
    garments on top as separate meshes from images directly. Human parsing provides
    fine-grained clothing details for dress people. Besides, Adaptive Content Generating
    and Preserving Network (ACGPN) [[119](#bib.bib119)] predicts the semantic layout
    of the reference image that will be changed after try-on, and then determines
    whether its image content needs to be generated or preserved according to the
    predicted semantic layout. Beside, Zhang *et al*. propose a Decompose-and-aggregate
    Network (DaNet) [[120](#bib.bib120)] that densely build a bridge between 2D pixels
    and 3D vertexes to facilitate the learning of 3D reconstruction.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusions
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have surveyed recent human parsing algorithms based on deep learning models
    which have achieved impressive performance, and grouped them into five categories
    including structure-driven architectures, graph-based networks, context-aware
    methods, LSTM-based methods, and combined auxiliary information approaches. Then,
    we review quantitative performance analyses of these models on some popular benchmarks,
    such as PASCAL-Person-Part, LIP, CIHP, ATR, and Fashion Clothing datasets. Finally,
    we introduce some of the promising research directions for further human parsing
    algorithms and the application scenarios of the human parsing task.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] I. Ulku and E. Akagündüz, “A survey on deep learning-based architectures
    for semantic segmentation on 2d images,” *Applied Artificial Intelligence*, pp.
    1–45, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, “Person
    re-identification by symmetry-driven accumulation of local features,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010, pp. 2360–2367.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Wang, T. Duan, Z. Liao, and D. Forsyth, “Discriminative hierarchical
    part-based models for human parsing and action recognition,” *J. MACH. LEARN.
    RES.*, vol. 13, no. 1, pp. 3075–3102, 2012.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K. Yamaguchi, M. H. Kiapour, and T. L. Berg, “Paper doll parsing: Retrieving
    similar styles to parse clothing items,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2013, pp. 3519–3526.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint training of a
    convolutional network and a graphical model for human pose estimation,” *Advances
    in neural information processing systems*, vol. 27, 2014.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Bourdev and J. Malik, “Poselets: Body part detectors trained using 3d
    human pose annotations,” in *2009 IEEE 12th International Conference on Computer
    Vision*.   IEEE, 2009, pp. 1365–1372.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] B. Sapp, A. Toshev, and B. Taskar, “Cascaded models for articulated pose
    estimation,” in *European conference on computer vision*.   Springer, 2010, pp.
    406–420.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] F. Wang and Y. Li, “Beyond physical connections: Tree models in human pose
    estimation,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2013, pp. 596–603.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] B. Sapp and B. Taskar, “Modec: Multimodal decomposable models for human
    pose estimation,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2013, pp. 3674–3681.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Kiefel and P. V. Gehler, “Human pose estimation with fields of parts,”
    in *European conference on computer vision*.   Springer, 2014, pp. 331–346.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 3431–3440.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Park, B. X. Nie, and S.-C. Zhu, “Attribute and-or grammar for joint
    parsing of human pose, parts and attributes,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 40, no. 7, pp. 1555–1569, 2018.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] B. Zhu, Y. Chen, M. Tang, and J. Wang, “Progressive cognitive human parsing,”
    *In Proc. AAAI*, 2018.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, and L. Lin, “Graphonomy:
    Universal human parsing via graph transfer learning,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] L. Lin, Y. Gao, K. Gong, M. Wang, and X. Liang, “Graphonomy: Universal
    image parsing via graph reasoning and transfer,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. Wang, Z. Zhang, S. Qi, J. Shen, Y. Pang, and L. Shao, “Learning compositional
    neural information fusion for human parsing,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Wang, H. Zhu, J. Dai, Y. Pang, J. Shen, and L. Shao, “Hierarchical
    human parsing with typed part-relation reasoning,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, 2020, pp. 8929–8939.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] L. X, L. S, S. X, Y. J, L. L, D. J, L. L, and Y. S, “Deep human parsing
    with active template regression.” *In IEEE TPAMI*, vol. 37, no. 12, pp. 2402–2414,
    2015.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, L. Lin, X. Cao, and
    S. Yan, “Matching-cnn meets knn: Quasi-parametric human parsing,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1419–1427.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, and S. Yan,
    “Human parsing with contextualized convolutional neural network,” in *Proceedings
    of the IEEE international conference on computer vision*, 2015, pp. 1386–1394.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. Li, Z. Liang, S. Zhao, J. Gong, and J. Shen, “Self-learning with rectification
    strategy for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 9263–9272.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan, “Semantic object
    parsing with local-global long short-term memory,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2016, pp. 3185–3193.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan, “Semantic object parsing
    with graph lstm,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 125–143.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] X. Liang, L. Lin, X. Shen, J. Feng, S. Yan, and E. P. Xing, “Interpretable
    structure-evolving lstm,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, pp. 2175–2184.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] F. Xia, P. Wang, X. Chen, and A. Yuille, “Joint multi-person pose estimation
    and semantic part segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, pp. 6080–6089.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] K. Gong, X. Liang, D. Zhang, X. Shen, and L. Lin, “Look into person: Self-supervised
    structure-sensitive learning and a new benchmark for human parsing.” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, vol. 2, no. 5, 2017, p. 6.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X. Nie, J. Feng, and S. Yan, “Mutual learning to adapt for joint human
    parsing and pose estimation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 502–517.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Z. Zhang, C. Su, L. Zheng, and X. Xie, “Correlating edge, pose with parsing,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, June 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] T. Liu, T. Ruan, Z. Huang, Y. Wei, S. Wei, Y. Zhao, and H. Thomas, “Devil
    in the details: Towards accurate single and multiple human parsing,” *Proc. AAAI*,
    2019.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] F. Xia, P. Wang, L. C. Chen, and A. L. Yuille, “Zoom better to see clearer:
    Human and object parsing with hierarchical auto-zoom net,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2015, pp. 648–663.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Q. Li, A. Arnab, and P. H. Torr, “Holistic, instance-level human parsing,”
    *arXiv preprint arXiv:1709.03612*, 2017.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally
    connected networks on graphs,” *arXiv preprint arXiv:1312.6203*, 2013.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. He, W. Huang, Y. Qiao, C. C. Loy, and X. Tang, “Reading scene text
    in deep convolutional sequences,” in *Thirtieth AAAI conference on artificial
    intelligence*, 2016.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] F. Xia, J. Zhu, P. Wang, and A. Yuille, “Pose-guided human parsing by
    an and/or graph using pose-context features,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 30, no. 1, 2016.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Humayun, F. Li, and J. M. Rehg, “Rigor: Reusing inference in graph
    cuts for generating object regions,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2014, pp. 336–343.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, and S. Yan,
    “Human parsing with contextualized convolutional neural network,” *In IEEE TPAMI*,
    vol. 39, no. 1, pp. 115–127, 2017.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Canny, “A computational approach to edge detection,” *IEEE Transactions
    on pattern analysis and machine intelligence*, no. 6, pp. 679–698, 1986.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. Liu, O. Choi, J. Wang, and W. Hwang, “Cdgnet: Class distribution guided
    network for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 4473–4482.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Liu, L. Zhao, S. Zhang, and J. Yang, “Hybrid resolution network using
    edge guided region mutual information loss for human parsing,” in *Proceedings
    of the 28th ACM International Conference on Multimedia*, 2020, pp. 1670–1678.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] K. Lin, L. Wang, K. Luo, Y. Chen, Z. Liu, and M.-T. Sun, “Cross-domain
    complementary learning using pose for multi-person part segmentation,” *IEEE Trans.
    Circuits Syst. Video Technol.*, 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] X. Liang, L. Lin, W. Yang, P. Luo, J. Huang, and S. Yan, “Clothes co-parsing
    via joint image segmentation and labeling with application to clothing retrieval,”
    *IEEE Transactions on Multimedia*, vol. 18, no. 6, pp. 1175–1186, 2016.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Zhao, J. Li, Y. Zhang, and Y. Tian, “Multi-class part parsing with
    joint boundary-semantic awareness,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9177–9186.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. He, J. Zhang, Q. Zhang, and D. Tao, “Grapy-ml: Graph pyramid mutual
    learning for cross-dataset human parsing,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 07, 2020, pp. 10 949–10 956.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] X. Zhang, Y. Chen, M. Tang, J. Wang, X. Zhu, and Z. Lei, “Human parsing
    with part-aware relation modeling,” *IEEE Transactions on Multimedia*, 2022.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] L. Li, T. Zhou, W. Wang, J. Li, and Y. Yang, “Deep hierarchical semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 1246–1257.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Liu, S. Zhang, J. Yang, and P. Yuen, “Hierarchical information passing
    based noise-tolerant hybrid learning for semi-supervised human parsing,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 35, no. 3, 2021, pp.
    2207–2215.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Z. Jin, B. Liu, Q. Chu, and N. Yu, “Isnet: Integrate image-level and semantic-level
    context for semantic segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 7189–7198.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Liu, Y. Sun, D. Zhu, G. Ren, Y. Chen, J. Feng, and J. Han, “Cross-domain
    human parsing via adversarial feature and label adaptation,” in *Proceedings of
    the AAAI Conference On Artificial Intelligence*, vol. 32, no. 1, 2018.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] K. Gong, X. Liang, Y. Li, Y. Chen, M. Yang, and L. Lin, “Instance-level
    human parsing via part grouping network,” in *Proc. Eur. Conf. Comput. Vis.*,
    2018, pp. 770–785.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Zhao, J. Li, H. Liu, S. Yan, and J. Feng, “Fine-grained multi-human
    parsing,” *International Journal of Computer Vision*, vol. 128, no. 8, pp. 2185–2203,
    2020.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] L. Yang, Q. Song, Z. Wang, and M. Jiang, “Parsing r-cnn for instance-level
    human analysis,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019, pp. 364–373.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S. Zhang, X. Cao, G.-J. Qi, Z. Song, and J. Zhou, “Aiparsing: Anchor-free
    instance-level human parsing,” *IEEE Transactions on Image Processing*, vol. 31,
    pp. 5599–5612, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Q. Zhou, X. Liang, K. Gong, and L. Lin, “Adaptive temporal encoding network
    for video instance-level human parsing,” in *Proceedings of the 26th ACM international
    conference on Multimedia*, 2018, pp. 1527–1535.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Liu, X. Liang, L. Liu, K. Lu, L. Lin, and S. Yan, “Fashion parsing
    with video context,” in *Proceedings of the 22nd ACM international conference
    on Multimedia*, 2014, pp. 467–476.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M.-H. Yang, “Joint-task
    self-supervised learning for temporal correspondence,” *Advances in Neural Information
    Processing Systems*, vol. 32, 2019.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Jabri, A. Owens, and A. Efros, “Space-time correspondence as a contrastive
    random walk,” *Advances in neural information processing systems*, vol. 33, pp.
    19 545–19 560, 2020.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Jeon, D. Min, S. Kim, and K. Sohn, “Mining better samples for contrastive
    learning of temporal correspondence,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 1034–1044.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, and Y. Yang, “Locality-aware
    inter-and intra-video reconstruction for self-supervised correspondence learning,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 8719–8730.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Communications of the ACM*, vol. 60,
    no. 6, pp. 84–90, 2017.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1–9.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Z. Zhong, J. Li, W. Cui, and H. Jiang, “Fully convolutional networks for
    building and road extraction: Preliminary results,” in *2016 IEEE International
    Geoscience and Remote Sensing Symposium (IGARSS)*.   IEEE, 2016, pp. 1591–1594.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep neural
    network architecture for real-time semantic segmentation,” *arXiv preprint arXiv:1606.02147*,
    2016.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] G. Wang, W. Li, S. Ourselin, and T. Vercauteren, “Automatic brain tumor
    segmentation using cascaded anisotropic convolutional neural networks,” in *International
    MICCAI brainlesion workshop*.   Springer, 2017, pp. 178–190.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and
    D. Terzopoulos, “Image segmentation using deep learning: A survey,” *IEEE transactions
    on pattern analysis and machine intelligence*, 2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] C. J. Holder and M. Shafique, “On efficient real-time semantic segmentation:
    a survey,” *arXiv preprint arXiv:2206.08605*, 2022.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for scene segmentation,” *In IEEE TPAMI*, vol. PP,
    no. 99, pp. 2481–2495, 2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 5693–5703.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] K. Sun, Y. Zhao, B. Jiang, T. Cheng, B. Xiao, D. Liu, Y. Mu, X. Wang,
    W. Liu, and J. Wang, “High-resolution representations for labeling pixels and
    regions,” *arXiv preprint arXiv:1904.04514*, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2117–2125.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang,
    P. H. Torr *et al.*, “Rethinking semantic segmentation from a sequence-to-sequence
    perspective with transformers,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2021, pp. 6881–6890.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] G. Ghiasi and C. C. Fowlkes, “Laplacian pyramid reconstruction and refinement
    for semantic segmentation,” in *European conference on computer vision*.   Springer,
    2016, pp. 519–534.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] G. Lin, A. Milan, C. Shen, and I. Reid, “Refinenet: Multi-path refinement
    networks for high-resolution semantic segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 1925–1934.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang, “Multi-scale
    context intertwining for semantic segmentation,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 603–619.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40,
    no. 4, pp. 834–848, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic
    image segmentation with deep convolutional nets and fully connected crfs,” *arXiv
    preprint arXiv:1412.7062*, 2014.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
    with atrous separable convolution for semantic image segmentation,” in *Proc.
    Eur. Conf. Comput. Vis.*, 2018, pp. 801–818.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, “Detect
    what you can: Detecting and representing objects using holistic models and body
    parts,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014, pp. 1979–1986.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] P. Luo, X. Wang, and X. Tang, “Pedestrian parsing via deep decompositional
    network,” in *Proceedings of the IEEE international conference on computer vision*,
    2013, pp. 2648–2655.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] K. Yamaguchi, “Parsing clothing in fashion photographs,” in *CVPR*, 2012,
    pp. 3570–3577.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] S. Liu, J. Feng, C. Domokos, H. Xu, J. Huang, Z. Hu, and S. Yan, “Fashion
    parsing with weak color-category labels,” *IEEE Transactions on Multimedia*, vol. 16,
    no. 1, pp. 253–265, 2013.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Dong, Q. Chen, W. Xia, Z. Huang, and S. Yan, “A deformable mixture
    parsing model with parselets,” in *CVPR*, 2014, pp. 3408–3415.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Luo, Z. Su, J. Guo, G. Zhang, and X. He, “Trusted guidance pyramid
    network for human parsing,” in *ACM MM*.   ACM, 2018, pp. 654–662.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] W. Yang, P. Luo, and L. Lin, “Clothing co-parsing by joint image segmentation
    and labeling,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2014, pp. 3182–3189.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Macro-micro
    adversarial network for human parsing,” in *Proc. Eur. Conf. Comput. Vis.*, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] H.-S. Fang, G. Lu, X. Fang, J. Xie, Y.-W. Tai, and C. Lu, “Weakly and
    semi supervised human body part parsing via pose-guided knowledge transfer,” *arXiv
    preprint arXiv:1805.04310*, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, and H. Shi, “Spgnet: Semantic prediction guidance for scene parsing,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 5218–5228.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] L.-C. Chen, M. Collins, Y. Zhu, G. Papandreou, B. Zoph, F. Schroff, H. Adam,
    and J. Shlens, “Searching for efficient multi-scale architectures for dense image
    prediction,” in *Proc. Adv. Neural Inf. Process. Syst.*, 2018, pp. 8699–8710.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] R. Ji, D. Du, L. Zhang, L. Wen, Y. Wu, C. Zhao, F. Huang, and S. Lyu,
    “Learning semantic neural tree for human parsing,” in *Proc. Eur. Conf. Comput.
    Vis.*, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] D. Zeng, Y. Huang, Q. Bao, J. Zhang, C. Su, and W. Liu, “Neural architecture
    search for joint human parsing and pose estimation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 11 385–11 394.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] W. Wang, T. Zhou, S. Qi, J. Shen, and S.-C. Zhu, “Hierarchical human semantic
    parsing with comprehensive part-relation modeling,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Blended grammar network
    for human parsing,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 189–205.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] X. Zhang, Y. Chen, M. Tang, Z. Lei, and J. Wang, “Grammar-induced wavelet
    network for human parsing,” *IEEE Transactions on Image Processing*, vol. 31,
    pp. 4502–4514, 2022.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to scale:
    Scale-aware semantic image segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body
    parsing & pose estimation network and a new benchmark,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, 2018.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Liu, M. Zhang, W. Liu, J. Song, and T. Mei, “Braidnet: Braiding semantics
    and details for accurate human parsing,” in *ACM Int. Conf. Multimdia*, 2019,
    pp. 338–346.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Part-aware context
    network for human parsing,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2020, pp. 8971–8980.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” *National
    science review*, vol. 5, no. 1, pp. 44–53, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] X. Zhang, F. Pan, K. Xiang, X. Zhu, C. Yu, Z. Wang, and Z. Lei, “Contrastive
    and consistent learning for unsupervised human parsing,” in *Chinese Conference
    on Biometric Recognition*.   Springer, 2022, pp. 226–236.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, “Unsupervised part segmentation
    through disentangling appearance and shape,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 8355–8364.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. L. Birdwhistell, *Kinesics and context: Essays on body motion communication*.   University
    of Pennsylvania press, 2010.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Joo, T. Simon, and Y. Sheikh, “Total capture: A 3d deformation model
    for tracking faces, hands, and bodies,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 8320–8329.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Liang, J. Yuan, and D. Thalmann, “Parsing the hand in depth images,”
    *IEEE Transactions on Multimedia*, vol. 16, no. 5, pp. 1241–1253, 2014.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Lin, H. Yang, D. Chen, M. Zeng, F. Wen, and L. Yuan, “Face parsing
    with roi tanh-warping,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 5654–5663.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] C. Geng, S.-j. Huang, and S. Chen, “Recent advances in open set recognition:
    A survey,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 43,
    no. 10, pp. 3614–3631, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] J. Cen, P. Yun, J. Cai, M. Y. Wang, and M. Liu, “Deep metric learning
    for open world semantic segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 15 333–15 342.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, “Identity-guided human
    semantic parsing for person re-identification,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 346–363.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. M. Kalayeh, E. Basaran, M. Gökmen, M. E. Kamasak, and M. Shah, “Human
    semantic parsing for person re-identification,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 1062–1071.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] C. Song, Y. Huang, W. Ouyang, and L. Wang, “Mask-guided contrastive attention
    model for person re-identification,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 1179–1188.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Z. Li, J. Lv, Y. Chen, and J. Yuan, “Person re-identification with part
    prediction alignment,” *Computer Vision and Image Understanding*, vol. 205, p.
    103172, 2021.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] R. A. Güler, N. Neverova, and I. Kokkinos, “Densepose: Dense human pose
    estimation in the wild,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 7297–7306.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] B. L. Bhatnagar, G. Tiwari, C. Theobalt, and G. Pons-Moll, “Multi-garment
    net: Learning to dress 3d people from images,” in *proceedings of the IEEE/CVF
    international conference on computer vision*, 2019, pp. 5420–5430.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, and G. Pons-Moll,
    “Learning to reconstruct people in clothing from a single rgb camera,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 1175–1186.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll, “Detailed
    human avatars from monocular video,” in *2018 International Conference on 3D Vision
    (3DV)*.   IEEE, 2018, pp. 98–109.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] ——, “Video based reconstruction of 3d people models,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    8387–8397.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] H. Yang, R. Zhang, X. Guo, W. Liu, W. Zuo, and P. Luo, “Towards photo-realistic
    virtual try-on by adaptively generating-preserving image content,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2020,
    pp. 7850–7859.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] H. Zhang, J. Cao, G. Lu, W. Ouyang, and Z. Sun, “Learning 3d human shape
    and pose from dense body parts,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
