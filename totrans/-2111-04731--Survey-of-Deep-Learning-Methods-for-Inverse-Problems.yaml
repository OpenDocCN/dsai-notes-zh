- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:50:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:50:09'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.04731] Survey of Deep Learning Methods for Inverse Problems'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.04731] 深度学习方法在逆问题中的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.04731](https://ar5iv.labs.arxiv.org/html/2111.04731)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.04731](https://ar5iv.labs.arxiv.org/html/2111.04731)
- en: September 2020
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2020年9月
- en: Survey of Deep Learning Methods for Inverse Problems
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法在逆问题中的综述
- en: Shima Kamyab Dept. of Comp. Sci. and Eng., Shiraz University, Shiraz, Iran [sh.kamyab@cse.shirazu.ac.ir](mailto:sh.kamyab@cse.shirazu.ac.ir)
       Zohreh Azimifar Dept. of Comp. Sci. and Eng., Shiraz University, Shiraz, Iran
    [azimifar@cse.shirazu.ac.ir](mailto:azimifar@cse.shirazu.ac.ir)    Rasool Sabzi
    Dept. of Comp. Sci. and Eng., Shiraz University, Shiraz, Iran [sabzi@cse.shirazu.ac.ir](mailto:sabzi@cse.shirazu.ac.ir)
       Paul Fieguth Dept. of Systems Design Engineering, University of Waterloo, Waterloo,
    Canada [pfieguth@uwaterloo.ca](mailto:pfieguth@uwaterloo.ca)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Shima Kamyab，计算机科学与工程系，Shiraz University，伊朗 [sh.kamyab@cse.shirazu.ac.ir](mailto:sh.kamyab@cse.shirazu.ac.ir)
       Zohreh Azimifar，计算机科学与工程系，Shiraz University，伊朗 [azimifar@cse.shirazu.ac.ir](mailto:azimifar@cse.shirazu.ac.ir)
       Rasool Sabzi，计算机科学与工程系，Shiraz University，伊朗 [sabzi@cse.shirazu.ac.ir](mailto:sabzi@cse.shirazu.ac.ir)
       Paul Fieguth，系统设计工程系，滑铁卢大学，加拿大 [pfieguth@uwaterloo.ca](mailto:pfieguth@uwaterloo.ca)
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper we investigate a variety of deep learning strategies for solving
    inverse problems. We classify existing deep learning solutions for inverse problems
    into three categories of Direct Mapping, Data Consistency Optimizer, and Deep
    Regularizer. We choose a sample of each inverse problem type, so as to compare
    the robustness of the three categories, and report a statistical analysis of their
    differences. We perform extensive experiments on the classic problem of linear
    regression and three well-known inverse problems in computer vision, namely image
    denoising, 3D human face inverse rendering, and object tracking, selected as representative
    prototypes for each class of inverse problems. The overall results and the statistical
    analyses show that the solution categories have a robustness behaviour dependent
    on the type of inverse problem domain, and specifically dependent on whether or
    not the problem includes measurement outliers. Based on our experimental results,
    we conclude by proposing the most robust solution category for each inverse problem
    class.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了多种深度学习策略用于解决逆问题。我们将现有的深度学习解决方案分为直接映射、数据一致性优化器和深度正则化器三类。我们选择每种逆问题类型的样本，以比较这三类的鲁棒性，并报告它们差异的统计分析。我们在经典的线性回归问题和计算机视觉中的三个著名逆问题（即图像去噪、3D人脸逆向渲染和目标跟踪）上进行了广泛的实验，这些问题被选为每类逆问题的代表性原型。总体结果和统计分析显示，解决方案类别的鲁棒性行为取决于逆问题领域的类型，特别是是否包括测量异常值。基于我们的实验结果，我们提出了每个逆问题类别的最鲁棒解决方案。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: An inverse problem [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)] seeks to
    formulate the solution to estimating the unknown state underlying a measured system.
    Specifically, a forward function $F(\cdot)$ describes the relationship of the
    measured output
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 逆问题[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]旨在制定估计测量系统中未知状态的解决方案。具体而言，前向函数$F(\cdot)$描述了测量输出的关系
- en: '|  | $\underline{m}=F(\underline{z})+\underline{\nu}$ |  | (1) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}=F(\underline{z})+\underline{\nu}$ |  | (1) |'
- en: as a function of the system state $\underline{z}$, subject to a measurement
    noise $\underline{\nu}$. The objective of the inverse problem is to estimate $\underline{z}$
    as a function of given measurement $\underline{m}$, assuming a detailed knowledge
    of the system, $F(\cdot)$, where if $F(\cdot)$ is not known or is partially known
    the problem becomes blind or semi-blind [[4](#bib.bib4)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为系统状态$\underline{z}$的函数，受到测量噪声$\underline{\nu}$的影响。逆问题的目标是估计$\underline{z}$，作为给定测量$\underline{m}$的函数，假设对系统$F(\cdot)$有详细了解，如果$F(\cdot)$未知或部分已知，则问题变为盲目或半盲[[4](#bib.bib4)]。
- en: Different perspectives lead to different types of inverse problems. From the
    perspective of data type, two classes of inverse problems are restoration and
    reconstruction [[5](#bib.bib5)], where restoration problems have the same domain
    for measurement and state (e.g., signal or image denoising), while reconstruction
    has different domains (e.g., 3D shape inference). Next, from the perspective of
    modeling, inverse problems are classified into static and dynamic problems, where
    the static case seeks a single estimate $\underline{\hat{z}}$, consistent with
    some prior model on $\underline{z}$ and the forward model $F(\underline{z})$,
    whereas the dynamic case seeks estimates $\underline{\hat{z}}(t)$ over time, consistent
    with an initial prior and a dynamic model. In this paper we will examine each
    of these inverse problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的视角会导致不同类型的逆问题。从数据类型的角度来看，逆问题可以分为恢复和重建[[5](#bib.bib5)]，其中恢复问题在测量和状态具有相同的领域（例如，信号或图像去噪），而重建问题具有不同的领域（例如，3D形状推断）。接下来，从建模的角度来看，逆问题分为静态问题和动态问题，其中静态情况寻找一个与某个先验模型和前向模型$F(\underline{z})$一致的单一估计$\underline{\hat{z}}$，而动态情况寻找随时间变化的估计$\underline{\hat{z}}(t)$，与初始先验和动态模型一致。本文将探讨这些逆问题。
- en: 'Existing analytical methods for solving inverse problems take advantage of
    domain knowledge to regularize and constrain the problem to obtain numerically-stable
    solutions. These methods are classified into four categories [[5](#bib.bib5)]:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的求解逆问题的分析方法利用领域知识来对问题进行正则化和约束，从而获得数值稳定的解。这些方法分为四类[[5](#bib.bib5)]：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Analytic inversion, having the objective of finding a closed form, possibly
    approximate, of $F^{-1}$. This category of solutions will be highly problem dependent.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解析反演，目标是找到$F^{-1}$的闭式解，可能是近似解。此类别的解将高度依赖于具体问题。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Iterative methods, which optimize the data inconsistency term
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代方法，优化数据不一致项
- en: '|  | $\min_{\underline{z}}\,&#124;&#124;\underline{m}-F(\underline{z})&#124;&#124;.$
    |  | (2) |'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{\underline{z}}\,\|\underline{m}-F(\underline{z})\|.$ |  | (2) |'
- en: Because of the ill-posed nature of most inverse problems, the iteration tends
    to have a semi-convergent behaviour, with the reconstruction error decreasing
    until some point and then diverging, necessitating appropriate stopping criteria.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于大多数逆问题的病态特性，迭代往往具有半收敛的行为，重建误差在某个点之前减少，然后发散，需要适当的停止标准。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discretization as regularization, including projection methods searching for
    an approximate solution of an inverse problems in a predefined subspace. Choosing
    an appropriate subspace has high impact on finding stable solutions.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正则化的离散化，包括在预定义的子空间中搜索逆问题的近似解。选择适当的子空间对找到稳定解具有重要影响。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Variational methods, with the idea of minimizing data consistency penalized
    using some regularizer $R$ parameterized by $\theta$:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变分方法，旨在通过使用某些正则化器$R$参数化的最小化数据一致性：
- en: '|  | $\min_{\underline{z}}\,&#124;&#124;\underline{m}-F(\underline{z})&#124;&#124;+R(\underline{z},\theta)$
    |  | (3) |'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{\underline{z}}\,\|\underline{m}-F(\underline{z})\| + R(\underline{z},\theta)$
    |  | (3) |'
- en: This is a generic adaptable framework where $F(\cdot),R(\cdot,\cdot)$ are chosen
    to fit a specific problem, of which well-known classical examples include Tikhonov
    [[6](#bib.bib6)] and total variation [[7](#bib.bib7)] regularization.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个通用的适应框架，其中$F(\cdot),R(\cdot,\cdot)$被选择以适应特定问题，其中著名的经典例子包括Tikhonov [[6](#bib.bib6)]
    和总变差 [[7](#bib.bib7)] 正则化。
- en: These approaches have weaknesses in requiring explicitly identified prior knowledge,
    selected regularizers, some shortcomings in handling noise, computational complexity
    in inference due to the optimization-based mechanisms, and most significantly
    limited applicability, in the sense that each inverse problem needs to be solved
    one-off.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的弱点在于需要明确识别的先验知识、选择的正则化器、处理噪声时的一些缺陷、由于基于优化机制的推断计算复杂性，以及最重要的有限适用性，即每个逆问题需要单独解决。
- en: As a result, we are highly motivated to consider the roles of Deep Neural Networks
    (DNNs), which have the advantages of being generic data driven methods, are adaptable
    to a wide variety of different problems, and can learn prior models implicitly
    through examples. DNNs are currently in widespread use to solve a vast range of
    problems in machine learning [[8](#bib.bib8)], artificial intelligence [[9](#bib.bib9)],
    and computer vision [[10](#bib.bib10)]. Strong advantages of using such structures
    include their near-universal applicability, their real-time inference [[11](#bib.bib11),
    [12](#bib.bib12)], and their superiority in handling sensor and/or measurement
    noise [[13](#bib.bib13)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们非常有动力考虑深度神经网络（DNNs）的作用，它们具有通用数据驱动方法的优势，能够适应各种不同的问题，并且可以通过示例隐式学习先验模型。DNNs目前在解决广泛的机器学习[[8](#bib.bib8)]、人工智能[[9](#bib.bib9)]和计算机视觉[[10](#bib.bib10)]问题中被广泛应用。使用这些结构的显著优势包括其几乎通用的适用性、实时推理[[11](#bib.bib11),
    [12](#bib.bib12)]和处理传感器和/或测量噪声的优越性[[13](#bib.bib13)]。
- en: A variety of studies [[14](#bib.bib14), [4](#bib.bib4)] have shown that planned,
    systematic DNNs will tend to have fewer parameters and better generalization power
    compared to generic architectures, which motivates us to consider systematic strategies
    in addressing complex inverse problems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[14](#bib.bib14), [4](#bib.bib4)]表明，经过计划和系统化的DNNs往往具有比通用架构更少的参数和更好的泛化能力，这激励我们在解决复杂逆问题时考虑系统化的策略。
- en: In principle, every deep learning framework could be interpreted as solving
    some sort of inverse problem, in the sense that the network is trained to take
    measurements and to infer, from given ground truth, the desired unknown state.
    For example, for the common DNN application to image classification, the input
    is a (measured) image, and the network output is a (unknown state) label, describing
    the object or scene appearing in the image. The network parameters then implicitly
    learn the inverse of the forward model, which had been the generation of an image
    from a label.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，每个深度学习框架都可以被解读为解决某种逆问题，因为网络经过训练以获取测量数据，并从给定的真实数据中推断出所需的未知状态。例如，对于图像分类的常见DNN应用，输入是（测量的）图像，网络输出是（未知状态）标签，描述图像中出现的物体或场景。网络参数隐式地学习了正向模型的逆，正向模型是从标签生成图像。
- en: Using DNNs for solving inverse problems aims to approximate the inverse of the
    forward model [[2](#bib.bib2)]. In some cases, the forward model may be explicitly
    defined [[15](#bib.bib15), [16](#bib.bib16), [14](#bib.bib14)], whereas in other
    cases it may be implicitly defined in the form of the training data [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [15](#bib.bib15), [21](#bib.bib21),
    [22](#bib.bib22)]. In this paper our focus is on solving non-blind inverse problems,
    with the forward model known. Analytical approaches to inverse problems, whether
    deterministic or stochastic, take advantage of the explicit forward model and
    prior knowledge in formulating the solution; in contrast, DNNs cannot take advantage
    of such information, and must instead learn implicitly from large datasets of
    training data in a black-box approach.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度神经网络（DNNs）来解决逆问题的目标是近似正向模型的逆[[2](#bib.bib2)]。在某些情况下，正向模型可能是明确定义的[[15](#bib.bib15),
    [16](#bib.bib16), [14](#bib.bib14)]，而在其他情况下，它可能以训练数据的形式隐式定义[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [15](#bib.bib15), [21](#bib.bib21),
    [22](#bib.bib22)]。本文的重点是解决非盲逆问题，其中正向模型是已知的。无论是确定性的还是随机的逆问题分析方法，都利用明确的正向模型和先验知识来制定解决方案；相反，DNNs不能利用这些信息，而必须通过黑箱方法从大量的训练数据中隐式学习。
- en: Inspired by the above techniques, there are indeed a number of proposed deep
    frameworks in the literature with the aim of bringing regularization techniques
    or prior knowledge into the DNN learning process for solving inverse problems [[14](#bib.bib14),
    [16](#bib.bib16), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)].
    In this paper, we classify deep solutions for inverse problems into three categories
    based on their objective criteria, and compare them in solving different types
    of inverse problems. The focus of this paper is comparing the robustness of different
    deep learning structures based on their optimization criterion associated with
    the training scheme; that is, the main objective of this research is to provide
    insight into the choice of appropriate framework, particularly with regards to
    performance robustness. It is worth noticing here that our goal is not to outperform
    the state-of-the-art performance in different problems, rather to examine different
    frameworks with fair parameter settings and performing at least as well as existing
    analytical approaches. Using these frameworks, we select a prototype inverse problem
    from each category and evaluate the performance and the robustness of the designed
    frameworks. We believe the results obtained in this way give insight into the
    strength of each solution category in addressing different categories of inverse
    problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 受到上述技术的启发，文献中确实提出了许多深度框架，旨在将正则化技术或先验知识引入深度神经网络（DNN）学习过程，以解决反问题 [[14](#bib.bib14),
    [16](#bib.bib16), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]。本文将针对反问题的深度解决方案分为三类，基于其目标标准进行比较，并在解决不同类型的反问题时进行比较。本文的重点是比较不同深度学习结构的鲁棒性，这些结构基于与训练方案相关的优化标准；也就是说，本研究的主要目标是提供有关选择适当框架的见解，特别是在性能鲁棒性方面。值得注意的是，我们的目标不是在不同问题上超越最先进的性能，而是以公平的参数设置检查不同的框架，并至少与现有的分析方法表现相当。使用这些框架，我们从每个类别中选择一个原型反问题，评估设计框架的性能和鲁棒性。我们相信，以这种方式获得的结果能够深入了解每个解决方案类别在处理不同类别反问题中的优势。
- en: 'The rest of this paper is organized as follows: Section [2](#S2 "2 Literature
    Review ‣ Survey of Deep Learning Methods for Inverse Problems") includes a review
    of the most recent deep approaches to solving inverse problems; Section [3](#S3
    "3 Problem Definition ‣ Survey of Deep Learning Methods for Inverse Problems")
    describes the problem definition, introducing three main categories for deep solutions
    for inverse problems; Section [4](#S4 "4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems") explains the experimental results including robustness
    analysis; finally Section [6](#S6 "6 Conclusions ‣ Survey of Deep Learning Methods
    for Inverse Problems") concludes the paper, proposing the best approach based
    on our experiments.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分组织如下：第[2](#S2 "2 Literature Review ‣ Survey of Deep Learning Methods for
    Inverse Problems")节包括对解决反问题的最新深度方法的综述；第[3](#S3 "3 Problem Definition ‣ Survey
    of Deep Learning Methods for Inverse Problems")节描述了问题定义，引入了反问题的三种主要深度解决方案类别；第[4](#S4
    "4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")节解释了实验结果，包括鲁棒性分析；最后，第[6](#S6
    "6 Conclusions ‣ Survey of Deep Learning Methods for Inverse Problems")节总结了本文，提出了基于我们实验的最佳方法。
- en: 2 Literature Review
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 文献综述
- en: Inverse problems have had a long history [[27](#bib.bib27), [2](#bib.bib2),
    [3](#bib.bib3)] in a wide variety of fields. In our context, since imaging involves
    the observing of a scene or phenomenon of interest, through a lens and spatial
    sensor, where the goal is to infer some aspect of the observed scene, essentially
    all imaging is an inverse problem, widely explored in the literature [[1](#bib.bib1),
    [28](#bib.bib28), [29](#bib.bib29)]. Imaging-related inverse problems may fall
    under any of image recovery, restoration, deconvolution, pansharpening, concealment,
    inpainting, deblocking, demosaicking, super-resolution, reconstruction from projections,
    compressive sensing, among many others.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 反问题在广泛的领域中有着悠久的历史 [[27](#bib.bib27), [2](#bib.bib2), [3](#bib.bib3)]。在我们的背景下，由于成像涉及通过镜头和空间传感器观察感兴趣的场景或现象，目标是推断被观察场景的某些方面，本质上所有的成像都是一个反问题，这在文献中被广泛探索
    [[1](#bib.bib1), [28](#bib.bib28), [29](#bib.bib29)]。与成像相关的反问题可能包括图像恢复、修复、去卷积、全色融合、隐匿、修补、去块、去马赛克、超分辨率、从投影重建、压缩感知等多种类型。
- en: Inverse problems are ultimately the deducing of some function $G(\cdot)$ which
    inverts the forward problem,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 逆问题最终是推断某个函数$G(\cdot)$，它能反转正向问题。
- en: '|  | $\underline{m}=F(\underline{z})+\underline{\nu}\qquad\longrightarrow\qquad\hat{\underline{z}}=G(\underline{m})$
    |  | (4) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}=F(\underline{z})+\underline{\nu}\qquad\longrightarrow\qquad\hat{\underline{z}}=G(\underline{m})$
    |  | (4) |'
- en: where some objective criterion obviously needs to be specified in order to select
    $G(\cdot)$. Since $G(\cdot)$ is very large (an input image has many pixels), unknown,
    and frequently nonlinear, it has become increasingly attractive to consider the
    role of DNNs, in their role as universal function approximators, in deducing $G(\cdot)$,
    and a number of approaches have been recently proposed in this fashion [[4](#bib.bib4),
    [5](#bib.bib5), [30](#bib.bib30)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择$G(\cdot)$时，显然需要指定某些目标准则。由于$G(\cdot)$非常大（一个输入图像有很多像素），未知且通常是非线性的，因此考虑DNN作为通用函数逼近器的角色来推断$G(\cdot)$变得越来越有吸引力，最近也提出了多种方法[[4](#bib.bib4),
    [5](#bib.bib5), [30](#bib.bib30)]。
- en: The most common approach when using DNNs for inverse problem solving includes
    optimizing the squared-error criterion $||\underline{z}-G(\underline{m})||_{2}^{2}$,
    with $G(\cdot)$ a DNN to be learned [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20), [15](#bib.bib15), [21](#bib.bib21), [22](#bib.bib22)]. This
    strategy implicitly finds a direct mapping from $\underline{m}$ to $\hat{\underline{z}}$
    using pairs $(\underline{z},\underline{m})$ as the training data in the learning
    phase, which seeks to solve
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DNN解决逆问题的最常见方法包括优化平方误差准则$||\underline{z}-G(\underline{m})||_{2}^{2}$，其中$G(\cdot)$是待学习的DNN
    [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [15](#bib.bib15),
    [21](#bib.bib21), [22](#bib.bib22)]。该策略隐含地从$\underline{m}$到$\hat{\underline{z}}$的直接映射，使用对$(\underline{z},\underline{m})$的配对作为学习阶段的训练数据，旨在解决
- en: '|  | $\hat{W}=\arg_{W}\min\,&#124;&#124;\underline{z}-G(\underline{m},W)&#124;&#124;_{2}^{2}$
    |  | (5) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{W}=\arg_{W}\min\,&#124;&#124;\underline{z}-G(\underline{m},W)&#124;&#124;_{2}^{2}$
    |  | (5) |'
- en: for $W$ the network weights in the DNN. Such supervised training needs a large
    number of data samples, which in some cases may be generated from the forward
    function $F(\cdot)$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$W$是DNN中的网络权重。这种监督训练需要大量的数据样本，在某些情况下，这些样本可能由正向函数$F(\cdot)$生成。
- en: Recent work in direct mapping includes [[31](#bib.bib31)], in which an encoder-decoder
    structure is proposed to directly solve clinical positron emission tomography
    (PET) image reconstruction. Similarly [[32](#bib.bib32)] proposes a direct mapping
    deep learning framework to identify the impact load conditions of shell structures
    based on their final state of damage, an inverse problem of engineering failure
    analysis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在直接映射方面的研究包括[[31](#bib.bib31)]，其中提出了一种编码器-解码器结构来直接解决临床正电子发射断层扫描（PET）图像重建问题。类似地，[[32](#bib.bib32)]提出了一种直接映射深度学习框架，根据壳体结构的最终损伤状态识别其冲击载荷条件，这是工程失效分析的一个逆问题。
- en: Recent research investigates the incorporation of prior knowledge into DNN solutions
    for inverse problems. In particular, the use of intelligent initialization of
    DNN weights and analytical regularization techniques form the main classes of
    existing work in this domain [[4](#bib.bib4)]. In [[15](#bib.bib15)], an unsupervised
    deep framework is proposed for solving inverse problems using a Generative Adversarial
    Network (GAN) to learn a prior without any information about the measurement process.
    In [[33](#bib.bib33)], a variational autoencoder (VAE) is used to solve electrical
    impedance tomography (EIT), a nonlinear ill-posed inverse problem. The VAE uses
    a variety of training data sets to generate a low dimensional manifold of approximate
    solutions, which allows the ill-posed problem to be converted to a well-posed
    one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 近期研究探讨了将先验知识融入DNN解决方案以应对逆问题。特别是，DNN权重的智能初始化和解析正则化技术构成了该领域现有工作的主要类别[[4](#bib.bib4)]。在[[15](#bib.bib15)]中，提出了一种无监督深度框架，使用生成对抗网络（GAN）在没有任何测量过程信息的情况下学习先验。在[[33](#bib.bib33)]中，使用了变分自编码器（VAE）来解决电阻抗断层扫描（EIT），这是一个非线性病态逆问题。VAE使用各种训练数据集生成低维流形的近似解，从而将病态问题转化为良态问题。
- en: 'The forward model provides knowledge regarding data generation, based on the
    physics of the system. In [[16](#bib.bib16)] an iterative variational framework
    is proposed to solve linear computer vision inverse problems of denoising, impainting,
    and super-resolution. It proposes a general regularizer $R$ for linear inverse
    problems which is first learned by a huge collection of images, and which is then
    incorporated into an Alternating Direction Method of Multipliers (ADMM) algorithm
    for optimizing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模型提供了关于数据生成的知识，基于系统的物理原理。在[[16](#bib.bib16)]中，提出了一种迭代变分框架，用于解决线性计算机视觉逆问题，如去噪、修复和超分辨率。它提出了一种通用正则化器$R$，用于线性逆问题，该正则化器首先通过大量图像学习，然后将其纳入交替方向乘子法（ADMM）算法中进行优化：
- en: '|  | $min_{\underline{\hat{z}}}\;\tfrac{1}{2}&#124;&#124;\underline{m}-F\underline{\hat{z}}&#124;&#124;_{2}^{2}+\lambda
    R(\underline{\hat{z}},W)$ |  | (6) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $min_{\underline{\hat{z}}}\;\tfrac{1}{2}&#124;&#124;\underline{m}-F\underline{\hat{z}}&#124;&#124;_{2}^{2}+\lambda
    R(\underline{\hat{z}},W)$ |  | (6) |'
- en: Here regularizer $R(\cdot)$ was learned from image datasets and $W$ is the network
    weight matrix, as before. Here $F$ is a matrix, the (assumed to be) linear forward
    model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的正则化器$R(\cdot)$是从图像数据集中学习的，而$W$是网络权重矩阵，和以前一样。这里$F$是一个矩阵，即（假设为）线性前向模型。
- en: 'The equivalent approach for a non-linear forward model is considered in [[34](#bib.bib34)],
    in which a data consistency term $D(F(\underline{\hat{z}}),\underline{m})$ as
    a training objective incorporates the forward model into the problem:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[34](#bib.bib34)]中考虑了非线性前向模型的等效方法，其中将数据一致性项$D(F(\underline{\hat{z}}),\underline{m})$作为训练目标，将前向模型纳入问题中：
- en: '|  | $min_{\underline{\hat{z}}}\;\{D(F(\underline{\hat{z}}),\underline{m})+\lambda
    R(\underline{\hat{z}},W)\}$ |  | (7) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $min_{\underline{\hat{z}}}\;\{D(F(\underline{\hat{z}}),\underline{m})+\lambda
    R(\underline{\hat{z}},W)\}$ |  | (7) |'
- en: In [[35](#bib.bib35)], a self-supervised deep learning framework is proposed
    for solving inverse problems in medical imaging using only the measurements and
    forward model in training the DNN.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[35](#bib.bib35)]中，提出了一种自监督深度学习框架，用于仅利用测量值和前向模型来训练DNN以解决医学成像中的逆问题。
- en: Further DNN methods for inverse problems are explored in [[14](#bib.bib14)],
    where the forward model is explicitly used in an iterative deep learning framework,
    requiring fewer parameters compared to direct mapping approaches. In [[36](#bib.bib36)],
    an iterative deep learning framework is proposed for MRI image reconstruction.
    The work in [[37](#bib.bib37)] proposes an unsupervised framework for solving
    forward and inverse problems in EIT. In [[38](#bib.bib38)] the analytical forward
    model is directly used in determining a DNN loss function, yielding an unsupervised
    framework utilizing knowledge about data generation. Other methods optimize data
    consistency using an estimate of the forward model, learned from training data [[39](#bib.bib39)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的DNN方法在[[14](#bib.bib14)]中被探讨，其中前向模型被明确用于迭代深度学习框架，相比于直接映射方法需要更少的参数。在[[36](#bib.bib36)]中，提出了一种用于MRI图像重建的迭代深度学习框架。[[37](#bib.bib37)]中的工作提出了一种无监督框架，用于解决EIT中的前向和逆问题。在[[38](#bib.bib38)]中，分析性的前向模型被直接用于确定DNN损失函数，产生了一种利用数据生成知识的无监督框架。其他方法利用从训练数据中学习到的前向模型估计来优化数据一致性[[39](#bib.bib39)]。
- en: The approach presented in [[40](#bib.bib40)] is closely related to ours, and
    aims at analysing deep learning structures for solving inverse problems, seeking
    to understand neural networks for solving small inverse problems. Our goal in
    this paper is to categorize deep learning frameworks for different inverse problems,
    based on their objectives and training schemes, investigating the power of each
    in solving certain types of inverse problems.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[40](#bib.bib40)]中提出的方法与我们的方法密切相关，旨在分析用于解决逆问题的深度学习结构，寻求理解神经网络在解决小规模逆问题中的应用。我们论文中的目标是对不同逆问题的深度学习框架进行分类，基于它们的目标和训练方案，调查每种方法在解决特定类型逆问题中的有效性。
- en: 3 Problem Definition
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题定义
- en: Let us consider a forward model
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个前向模型
- en: '|  | $\underline{m}=F(\underline{z})+\underline{\nu}\qquad\underline{\nu}\sim
    N(0,I)$ |  | (8) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}=F(\underline{z})+\underline{\nu}\qquad\underline{\nu}\sim
    N(0,I)$ |  | (8) |'
- en: 'with given noise process $\underline{\nu}$, assumed to be white. There are
    two fundamental classes of inverse problems to solve:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的噪声过程$\underline{\nu}$被假设为白噪声。解决的逆问题有两个基本类别：
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Static Estimation Problems, in which the system state $\underline{z}$ is static,
    without any evolution over time [[2](#bib.bib2)]. We will consider the following
    static problems:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态估计问题，其中系统状态$\underline{z}$是静态的，没有随时间变化[[2](#bib.bib2)]。我们将考虑以下静态问题：
- en: –
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Image Restoration, part of a class of inverse problems in which the state and
    measurement spaces coincide (same number of pixels). Typically the measurements
    are a corrupted version of the unknown state, and the problem is to recover an
    estimate of the true signal from its corrupted version knowing the (forward) distortion
    model. Robustness and outlier detection are the main requirements for this class
    of inverse problems.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像恢复，属于一类逆问题，其中状态和测量空间相同（像素数量相同）。通常，测量是未知状态的受损版本，问题是从其受损版本中恢复真实信号的估计，同时知道（前向）失真模型。鲁棒性和异常值检测是这类逆问题的主要要求。
- en: –
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Image Reconstruction, to find a projection from some measurement space to a
    differently sized state, such as 3D shape reconstruction from 2D scenes. These
    problems need careful regularization to find feasible solutions.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像重建，旨在从某些测量空间找到与不同尺寸状态的投影，例如从二维场景重建三维形状。这些问题需要仔细的正则化来找到可行的解决方案。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dynamic Estimation Problems, in which $\underline{z}$ is subject to dynamics
    and measurements over time [[2](#bib.bib2)], such as in object tracking.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态估计问题，其中$\underline{z}$在时间上受到动态和测量的影响[[2](#bib.bib2)]，例如在物体跟踪中。
- en: 'Our focus is on DNNs as data-driven models for solving inverse problems, so
    we wish to redefine inverse problems to the context of learning from examples
    in statistical learning theory [[41](#bib.bib41)]. We need two sets of variables:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的重点是将深度神经网络（DNNs）作为解决逆问题的数据驱动模型，因此我们希望将逆问题重新定义为统计学习理论中的例子学习背景[[41](#bib.bib41)]。我们需要两组变量：
- en: '|  | $\text{Inputs~{}~{}}\underline{m}\in M\qquad\text{Outputs~{}~{}}\underline{z}\in
    Z$ |  | (9) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Inputs~{}~{}}\underline{m}\in M\qquad\text{Outputs~{}~{}}\underline{z}\in
    Z$ |  | (9) |'
- en: The relation between input and output is described by a probability distribution
    $p(\underline{m},\underline{z})\in M\times Z$, where the distribution is known
    only through a finite set of samples, the training set
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出之间的关系由概率分布$p(\underline{m},\underline{z})\in M\times Z$描述，其中分布仅通过有限的样本集，即训练集已知。
- en: '|  | $S=\{\underline{m}_{i},\underline{z}_{i}\}\qquad 1\leq i\leq N$ |  | (10)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $S=\{\underline{m}_{i},\underline{z}_{i}\}\qquad 1\leq i\leq N$ |  | (10)
    |'
- en: assumed to have been drawn independently and identically distributed (i.i.d.)
    from $p$. The learning objective is to find a function $G(\underline{m})$ to be
    an appropriate approximation of output $\underline{z}$ in the case of a given
    input $\underline{m}$. That is,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假定从$p$中独立同分布（i.i.d.）抽取。学习目标是找到一个函数$G(\underline{m})$，使其在给定输入$\underline{m}$的情况下适当地逼近输出$\underline{z}$。即，
- en: '|  | $\text{True}\;\underline{z}\;\approx\;\text{Estimated}\;\hat{\underline{z}}\;=\;G(\underline{m}&#124;S),$
    |  | (11) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{True}\;\underline{z}\;\approx\;\text{Estimated}\;\hat{\underline{z}}\;=\;G(\underline{m}&#124;S),$
    |  | (11) |'
- en: such that $G(\cdot|S)$ was learned on the basis of $S$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使得$G(\cdot|S)$是在$S$的基础上学习得到的。
- en: 'In order to measure the effectiveness of estimator function $G$ in inferring
    the desired relationship described by $p$, the expected conditional error can
    be used:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量估计函数$G$在推断由$p$描述的期望关系的有效性，可以使用期望条件误差：
- en: '|  | $I(G)=\int_{M\times Z}D\bigl{(}G(\underline{m}),\underline{z}\bigr{)}\,dp(\underline{z},\underline{m})$
    |  | (12) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(G)=\int_{M\times Z}D\bigl{(}G(\underline{m}),\underline{z}\bigr{)}\,dp(\underline{z},\underline{m})$
    |  | (12) |'
- en: where $D(G(\underline{m}),\underline{z})$ is the cost or loss function, measuring
    the cost associated with approximating true value $\underline{z}$ with an estimate
    $G(\underline{m})$. Choosing a squared loss $(G(\underline{m})-\underline{z})^{2}$
    and allows us to derive
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$D(G(\underline{m}),\underline{z})$是成本或损失函数，测量将真实值$\underline{z}$近似为估计$G(\underline{m})$的相关成本。选择平方损失$(G(\underline{m})-\underline{z})^{2}$允许我们推导出
- en: '|  | $G(\underline{m})=\int_{Z}\underline{z}\,dp(\underline{z}&#124;\underline{m})=E_{p}[\underline{z}],$
    |  | (13) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $G(\underline{m})=\int_{Z}\underline{z}\,dp(\underline{z}&#124;\underline{m})=E_{p}[\underline{z}],$
    |  | (13) |'
- en: the classic optimal Bayesian least-squares estimator [[2](#bib.bib2)]. In the
    case of learning from examples, ([13](#S3.E13 "In 3 Problem Definition ‣ Survey
    of Deep Learning Methods for Inverse Problems")) cannot be reconstructed exactly
    since only a finite set of examples $S$ is given; therefore a regularized least
    squares algorithm may be used as an alternative [[42](#bib.bib42), [43](#bib.bib43)],
    where the hypothesis space $H$ is fixed and the estimate $G_{S}^{\lambda}$ is
    obtained as
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的最优贝叶斯最小二乘估计器 [[2](#bib.bib2)]。在从示例中学习的情况下，（[13](#S3.E13 "In 3 Problem Definition
    ‣ Survey of Deep Learning Methods for Inverse Problems")）无法被精确重建，因为只给定了有限的示例集
    $S$；因此，可以使用正则化最小二乘算法作为替代 [[42](#bib.bib42), [43](#bib.bib43)]，其中假设空间 $H$ 是固定的，估计
    $G_{S}^{\lambda}$ 如下所示
- en: '|  | $G_{S}^{\lambda}=\arg_{G\in H}\min\left\{\sum_{i=1}^{N}D\bigl{(}G(\underline{m}_{i}),\underline{z}_{i}\bigr{)}+\lambda
    R\bigl{(}G(\underline{m}_{i})\bigr{)}\right\},$ |  | (14) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $G_{S}^{\lambda}=\arg_{G\in H}\min\left\{\sum_{i=1}^{N}D\bigl{(}G(\underline{m}_{i}),\underline{z}_{i}\bigr{)}+\lambda
    R\bigl{(}G(\underline{m}_{i})\bigr{)}\right\},$ |  | (14) |'
- en: where $R(\cdot)$ is a penalty term and $\lambda$ a regularization parameter.
    We may choose $\lambda$ to minimize the discrepancy
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R(\cdot)$ 是一个惩罚项，$\lambda$ 是正则化参数。我们可以选择 $\lambda$ 来最小化差异
- en: '|  | $\left&#124;I[G_{S}^{\lambda}]-\inf_{G\in H}I[G]\right&#124;,$ |  | (15)
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\|I[G_{S}^{\lambda}]-\inf_{G\in H}I[G]\right\|,$ |  | (15) |'
- en: however in general it is much simpler, and sufficient, to select $\lambda$ via
    cross-validation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常通过交叉验证选择 $\lambda$ 会更简单且足够。
- en: 'Given that $H$ is the hypothesis space of possible inverse functions, in this
    paper it is quite reasonable to understand $H$ to be the space of functions which
    can be learned by a deep neural network, on the basis of optimizing its weight
    matrix $W$. Based on the optimization criterion ([14](#S3.E14 "In 3 Problem Definition
    ‣ Survey of Deep Learning Methods for Inverse Problems")), which is actually the
    variational framework in functional analytic regularization theory [[44](#bib.bib44)],
    and which forms the basis for inverse-function DNN learning, we classify deep
    learning frameworks for solving inverse problems into three categories, based
    on optimization criteria and training schemes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 $H$ 是可能逆函数的假设空间，在本文中，理解 $H$ 为深度神经网络能够学习的函数空间是相当合理的，基于优化其权重矩阵 $W$。基于优化标准 ([14](#S3.E14
    "In 3 Problem Definition ‣ Survey of Deep Learning Methods for Inverse Problems"))，它实际上是泛函分析正则化理论中的变分框架
    [[44](#bib.bib44)]，并形成了逆函数 DNN 学习的基础，我们根据优化标准和训练方案将深度学习框架分类为三类：
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Direct Mapping
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接映射
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data Consistency Optimizer
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据一致性优化器
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Deep Regularizer
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度正则化器
- en: Each of these is developed and defined, as follows.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容都被详细开发和定义，如下所述。
- en: 3.1 Direct Mapping
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 直接映射
- en: The direct mapping category is used as the objective criterion in a large body
    of research in deep learning based inverse problems [[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [15](#bib.bib15), [21](#bib.bib21), [22](#bib.bib22)].
    These methods seek to find end-to-end solutions for
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射类别被广泛用于深度学习基础的逆问题研究的目标标准 [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20), [15](#bib.bib15), [21](#bib.bib21), [22](#bib.bib22)]。这些方法试图找到端到端的解决方案
- en: '|  | $\min_{W_{1}}\left\{\sum_{i=1}^{N}D\bigl{(}\underline{z},G(\underline{m},W_{1})\bigr{)}+\lambda
    R\bigl{(}G(\underline{m},W_{1})\bigr{)}\right\}$ |  | (16) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{W_{1}}\left\{\sum_{i=1}^{N}D\bigl{(}\underline{z},G(\underline{m},W_{1})\bigr{)}+\lambda
    R\bigl{(}G(\underline{m},W_{1})\bigr{)}\right\}$ |  | (16) |'
- en: whereby $D(\cdot,\cdot)$ is the cost function to be minimized by a DNN $G(\underline{m},W_{1})$,
    on the basis of optimizing DNN weights $W_{1}$. $R\bigl{(}G(\underline{m},W_{1})\bigr{)}$
    specifies a generic analytical regularizer, to restrict the estimator to feasible
    solutions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D(\cdot,\cdot)$ 是由 DNN $G(\underline{m},W_{1})$ 最小化的成本函数，基于优化 DNN 权重 $W_{1}$。$R\bigl{(}G(\underline{m},W_{1})\bigr{)}$
    指定了一个通用的分析正则化器，以限制估计器到可行解。
- en: The Direct Mapping category approximates an estimator $G$ as an inverse to the
    forward model $F$, requiring a dataset of pairs $\{(\underline{m}_{i},\underline{z}_{i})\}_{i}$
    of observed measurements and corresponding target system parameters, as illustrated
    in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Direct Mapping ‣ 3 Problem Definition ‣ Survey
    of Deep Learning Methods for Inverse Problems").
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射类别将估计器 $G$ 近似为正向模型 $F$ 的逆，需要一组对 $\{(\underline{m}_{i},\underline{z}_{i})\}_{i}$
    的观测测量和对应目标系统参数的数据集，如图 [1](#S3.F1 "Figure 1 ‣ 3.1 Direct Mapping ‣ 3 Problem Definition
    ‣ Survey of Deep Learning Methods for Inverse Problems") 所示。
- en: '![Refer to caption](img/f397e0b36cb87b5bcf926a95d710492c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f397e0b36cb87b5bcf926a95d710492c.png)'
- en: 'Figure 1: Direct mapping of deep learning inverse problems.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：深度学习逆问题的直接映射。
- en: This category of DNN is typically used in those cases where we have a model-based
    imaging system having a linear forward model $\underline{m}=F\underline{z}$, where
    $z$ is an image, so that convolution networks (CNNs) are nearly always used. As
    discussed earlier, for Image Restoration problems the measurements themselves
    are already images, however in more general contexts we may choose to project
    the measurements as $F^{H}\underline{m}$, back into the domain of $\underline{z}$,
    such that the CNN is trained to learn the estimator
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的DNN通常用于那些拥有线性前向模型$\underline{m}=F\underline{z}$的基于模型的成像系统的情况，其中$z$是图像，因此卷积网络（CNNs）几乎总是被使用。如前所述，对于图像恢复问题，测量本身已经是图像，然而在更一般的情况下，我们可能选择将测量投影为$F^{H}\underline{m}$，回到$\underline{z}$的领域，使CNN被训练来学习估计器。
- en: '|  | $\underline{\hat{z}}=G(F^{H}\underline{m},W_{1})$ |  | (17) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{\hat{z}}=G(F^{H}\underline{m},W_{1})$ |  | (17) |'
- en: The translation invariance of $F^{H}F$, relatively common in imaging inverse
    problems, makes the convolutional-kernel nature of CNNs particularly suitable
    for serving as the estimator for these problems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: $F^{H}F$的平移不变性在成像逆问题中相对常见，使得卷积核特性使CNNs特别适合作为这些问题的估计器。
- en: In general, the performance of direct inversion is remarkable [[4](#bib.bib4)].
    However the receptive field (i.e., the size of the field of view the unit has
    over its input layer) of the CNN should be matched to the support of the point
    spread function [[14](#bib.bib14)]. Therefore, large CNNs with many parameters
    and accordingly extensive amount of training time and data are often needed for
    the methods in this category. These DNNs are highly problem dependent and for
    different forward models (e.g., with different matrix sizes, resolutions, etc.)
    a new DNN will need to be learned.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，直接反演的性能非常出色[[4](#bib.bib4)]。然而，CNN的感受野（即单位在其输入层上具有的视场大小）应与点扩散函数的支持相匹配[[14](#bib.bib14)]。因此，这一类别的方法通常需要具有大量参数的大型CNN，并且需要相应的较长训练时间和数据。这些DNN高度依赖于问题，对于不同的前向模型（例如，不同的矩阵大小、分辨率等），需要学习新的DNN。
- en: 3.2 Data Consistency Optimizer
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据一致性优化器
- en: 'The Data Consistency Optimizer category of deep learning aims to optimize data
    consistency as an unsupervised criterion within a variational framework [[14](#bib.bib14),
    [38](#bib.bib38)]:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的数据一致性优化器类别旨在通过变分框架中的无监督标准优化数据一致性[[14](#bib.bib14), [38](#bib.bib38)]：
- en: '|  | $\min_{W_{2}}\left\{\sum_{i=1}^{N}D\Bigl{(}\underline{m},F\bigl{(}G(\underline{m},W_{2})\bigr{)}\Bigr{)}+\lambda
    R\bigl{(}G(\underline{m},W_{2})\bigr{)}\right\}$ |  | (18) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{W_{2}}\left\{\sum_{i=1}^{N}D\Bigl{(}\underline{m},F\bigl{(}G(\underline{m},W_{2})\bigr{)}\Bigr{)}+\lambda
    R\bigl{(}G(\underline{m},W_{2})\bigr{)}\right\}$ |  | (18) |'
- en: where, as in ([16](#S3.E16 "In 3.1 Direct Mapping ‣ 3 Problem Definition ‣ Survey
    of Deep Learning Methods for Inverse Problems")), $D(\cdot,\cdot)$ is the cost
    function to be minimized by DNN $G(\underline{m},W_{2})$, parameterized by weights
    $W_{2}$, subject to regularizer $R\bigl{(}G(\underline{m},W_{1})\bigr{)}$. The
    overall picture is summarized in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Data Consistency
    Optimizer ‣ 3 Problem Definition ‣ Survey of Deep Learning Methods for Inverse
    Problems").
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，如([16](#S3.E16 "在3.1直接映射 ‣ 3问题定义 ‣ 深度学习方法综述"))中所述，$D(\cdot,\cdot)$是由权重$W_{2}$参数化的DNN
    $G(\underline{m},W_{2})$需要最小化的代价函数，受到正则化器$R\bigl{(}G(\underline{m},W_{1})\bigr{)}$的约束。总体情况总结在图[2](#S3.F2
    "图2 ‣ 3.2 数据一致性优化器 ‣ 3问题定义 ‣ 深度学习方法综述")中。
- en: In contrast to ([16](#S3.E16 "In 3.1 Direct Mapping ‣ 3 Problem Definition ‣
    Survey of Deep Learning Methods for Inverse Problems")), where the network cost
    function $D$ is expressed in the space of unknowns $\underline{z}$, here ([18](#S3.E18
    "In 3.2 Data Consistency Optimizer ‣ 3 Problem Definition ‣ Survey of Deep Learning
    Methods for Inverse Problems")) expresses the cost in the space of measurements
    $\underline{m}$, based on forward model $F(\cdot)$. That is, the data consistency
    term is no longer learning from supervised examples, rather from the forward model
    we obtain an unsupervised data consistency term, not needing data labels, whereby
    the forward model provides some form of implicit supervision.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ([16](#S3.E16 "在 3.1 直接映射 ‣ 3 问题定义 ‣ 深度学习方法在逆问题中的综述")) 中网络成本函数 $D$ 表示在未知量
    $\underline{z}$ 的空间中不同，这里 ([18](#S3.E18 "在 3.2 数据一致性优化器 ‣ 3 问题定义 ‣ 深度学习方法在逆问题中的综述"))
    将成本表达在测量空间 $\underline{m}$ 中，基于前向模型 $F(\cdot)$。即，数据一致性项不再从监督示例中学习，而是从前向模型中获得无监督数据一致性项，不需要数据标签，前向模型提供某种形式的隐式监督。
- en: Compared to the direct mapping category, the use of the forward model in ([18](#S3.E18
    "In 3.2 Data Consistency Optimizer ‣ 3 Problem Definition ‣ Survey of Deep Learning
    Methods for Inverse Problems")) leads to a network with relatively few parameters,
    in part because the receptive field of the DNN need not be matched to the support
    of the point spread function. However, the ill-posedness of the inverse problem
    causes a semi-convergent behaviour [[5](#bib.bib5)] using this criterion, therefore
    an early stopping regularization needs to be adopted in the learning process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接映射类别相比，使用 ([18](#S3.E18 "在 3.2 数据一致性优化器 ‣ 3 问题定义 ‣ 深度学习方法在逆问题中的综述")) 中的前向模型导致网络参数相对较少，部分原因是
    DNN 的感受野不需要匹配点扩散函数的支持。然而，逆问题的不适定性导致使用该准则时出现半收敛行为 [[5](#bib.bib5)]，因此在学习过程中需要采用早期停止正则化。
- en: '![Refer to caption](img/43887a1d66637e1446576a0ca166c95b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/43887a1d66637e1446576a0ca166c95b.png)'
- en: 'Figure 2: Data consistency optimization, where the forward model is incorporated
    in the loss function of the DNN and is utilized during DNN training.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：数据一致性优化，其中前向模型被纳入 DNN 的损失函数中，并在 DNN 训练期间使用。
- en: 3.3 Deep Regularizer
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度正则化器
- en: 'Finally the Deep Regularizer category of deep learning methods continues to
    optimize the data consistency term, however the overall optimization process is
    undertaken in the form of an analytical variational framework and uses a DNN as
    the regularizer [[16](#bib.bib16), [34](#bib.bib34)]:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，深度正则化器类别的深度学习方法继续优化数据一致性项，但整体优化过程以分析变分框架的形式进行，并使用 DNN 作为正则化器 [[16](#bib.bib16),
    [34](#bib.bib34)]：
- en: '|  | $\min_{\underline{\hat{z}}}\left\{\sum_{i=1}^{N}D\bigl{(}\underline{m},F(\underline{\hat{z}})\bigr{)}+\lambda
    R(\underline{\hat{z}},W_{3})\right\}$ |  | (19) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\underline{\hat{z}}}\left\{\sum_{i=1}^{N}D\bigl{(}\underline{m},F(\underline{\hat{z}})\bigr{)}+\lambda
    R(\underline{\hat{z}},W_{3})\right\}$ |  | (19) |'
- en: Here $R(\underline{\hat{z}},W_{3})$ is a pre-trained deep regularizer, based
    on weight matrix $W_{3}$, usually chosen as a deep classifier [[16](#bib.bib16),
    [34](#bib.bib34)], discriminating the feasible solutions from non-feasible ones.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $R(\underline{\hat{z}},W_{3})$ 是一个基于权重矩阵 $W_{3}$ 的预训练深度正则化器，通常选择为深度分类器 [[16](#bib.bib16),
    [34](#bib.bib34)]，用于区分可行解与不可行解。
- en: This category usually includes an analytical variational framework consisting
    of a data consistency term and a learned DNN to capture the redundancy in parameter
    space (see Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Deep Regularizer ‣ 3 Problem Definition
    ‣ Survey of Deep Learning Methods for Inverse Problems")).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类别通常包括一个分析变分框架，由数据一致性项和一个学习的 DNN 组成，以捕捉参数空间中的冗余（见图 [3](#S3.F3 "图 3 ‣ 3.3 深度正则化器
    ‣ 3 问题定义 ‣ 深度学习方法在逆问题中的综述")）。
- en: '![Refer to caption](img/36246b955cffa87f1cdf3cc125a5ace0.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/36246b955cffa87f1cdf3cc125a5ace0.png)'
- en: 'Figure 3: Deep regularized category of inverse problems, in which a DNN is
    used only as the regularizer as part of an analytical variational framework.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：逆问题的深度正则化类别，其中 DNN 仅作为分析变分框架的一部分用作正则化器。
- en: For this category, an iterative algorithm (deep or analytical) is used to actually
    perform the optimization of ([19](#S3.E19 "In 3.3 Deep Regularizer ‣ 3 Problem
    Definition ‣ Survey of Deep Learning Methods for Inverse Problems")). The regularizer
    network itself is trained using the data of a specific domain. The Deep Regularizer
    category needs the fewest parameter settings, compared to the earlier categories;
    however because of the optimization based inference step it is computationally
    demanding.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个类别，实际上使用了迭代算法（深度或解析）来执行([19](#S3.E19 "In 3.3 Deep Regularizer ‣ 3 Problem
    Definition ‣ Survey of Deep Learning Methods for Inverse Problems"))的优化。正则化网络本身是使用特定领域的数据来训练的。与前面的类别相比，深度正则化类别需要最少的参数设置；然而，由于基于优化的推断步骤，它要求计算量较大。
- en: 4 Experiments
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4个实验
- en: 'Our focus in this paper is to study solution robustness in the presence of
    noise and outliers during inference. This section explores experimental results,
    for each of the the fundamental inverse-problem classes (restoration, reconstruction,
    dynamic estimation) for each of the categories of solution (direct mapping (DM),
    data consistency optimizer (DC), deep regularizer (DR)), as discussed in Section [3](#S3
    "3 Problem Definition ‣ Survey of Deep Learning Methods for Inverse Problems").
    Our study is based on a statistical analysis via the Wilcoxon signed rank test [[45](#bib.bib45)],
    a well-known tool for analysing deep learning frameworks. The null hypothesis
    is that the result of each pairwise combination of DM, DC, and DR are from the
    same distribution, i.e., that the results are not significantly different. The
    experimental results are based on the following problems:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的重点是研究推断过程中存在的噪声和异常值对解的鲁棒性的影响。本节探讨了实验结果，针对每个基本的反问题类别（恢复、重建、动态估计）和每个解的类别（直接映射（DM）、数据一致性优化器（DC）、深度正则化（DR）），如第[3](#S3
    "3 Problem Definition ‣ Survey of Deep Learning Methods for Inverse Problems")节所述。我们的研究基于Wilcoxon符号秩检验[[45](#bib.bib45)]的统计分析，这是一个用于分析深度学习框架的著名工具。零假设是DM、DC和DR的每个两两组合的结果来自相同的分布，即结果没有显著差异。实验结果基于以下问题：
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Linear Regression: a reconstruction problem, with the aim of finding line parameters
    from the noisy / outlier sample points drawn from that line.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性回归：一个重建问题，目标是从噪声/异常值样本点中找到线的参数。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Image Denoising: a restoration problem, with the objective of recovering a
    clean image from noisy observations. We use both synthetic texture images and
    real images.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像去噪：一个恢复问题，目标是从有噪声的观测中恢复出一个干净的图像。我们使用了合成纹理图像和真实图像。
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Single View 3D Shape Inverse Rendering: a reconstruction problem, for which
    the domains of the measurements and system parameters are different. The measurements
    include a limited number of 2D points (input image landmarks) with the unknown
    state, to be recovered, a 3D Morphable Model (3DMM). We use a 3D model of the
    human face, based on eigen-faces obtained from principal component analysis.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单视角3D形状反渲染：一个重建问题，其中测量和系统参数的域是不同的。测量包括一组有限的2D点（输入图像的标志点），其中未知状态需要恢复，即一个3D可塑模型（3DMM）。我们使用了一个基于主成分分析得到的人脸的3D模型。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Single Object Tracking: a dynamic estimation problem, for which the goal is
    to predict the location (system parameter) of a moving object based on its (noisy)
    locations, measured in preceding frames. While this problem seems to belong to
    the class of restoration problems, the embedded state in this problem requires
    additional assumptions regarding the time-dynamics, and thus additional search
    strategies.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单目标跟踪：一个动态估计问题，目标是根据先前帧中的（有噪声的）位置来预测移动目标的位置（系统参数）。虽然这个问题似乎属于恢复问题的类别，但这个问题中的嵌入状态需要对时间动态性做出额外的假设，因此需要额外的搜索策略。
- en: All DNNs were implemented using the KERAS library [[46](#bib.bib46)] and ADAM
    optimizer [[47](#bib.bib47)] on an NVIDIA GeForce GTX 1080 Ti. The DNN structures
    and the details of each trained DNN can be found in the corresponding subsection.
    Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ Survey of Deep Learning Methods for
    Inverse Problems") summarizes the overall experimental setup for all problems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的深度神经网络都是使用KERAS库[[46](#bib.bib46)]和ADAM优化器[[47](#bib.bib47)]在NVIDIA GeForce
    GTX 1080 Ti上实现的。各个训练的深度神经网络的结构和细节可以在相应的子节中找到。表[1](#S4.T1 "Table 1 ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems")总结了所有问题的整体实验设置。
- en: '| Inverse Problem | Measurements | Unknown parameters | Forward Model | Training
    Data |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 逆问题 | 测量数据 | 未知参数 | 正向模型 | 训练数据 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Linear Regression &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 线性回归 &#124;'
- en: '&#124; (Reconstruction) &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （重建） &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2D coordinates of &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2D 坐标 &#124;'
- en: '&#124; N drawn samples &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; N 个抽样 &#124;'
- en: '&#124; from the line &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自于直线 &#124;'
- en: '| Slope, Intercept |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 斜率，截距 |'
- en: '&#124; Straight line &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直线 &#124;'
- en: '&#124; plus noise &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加噪声 &#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Synthetic: &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合成: &#124;'
- en: '&#124; $\{(y_{i},x_{i})\}$ &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\{(y_{i},x_{i})\}$ &#124;'
- en: '&#124; including Gaussian noise &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包括高斯噪声 &#124;'
- en: '&#124; with heavy-tailed outliers &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带有重尾异常值 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image Denoising &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像去噪 &#124;'
- en: '&#124; (Restoration) &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （恢复） &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Noisy Image &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 噪声图像 &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Clean Image &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 清晰图像 &#124;'
- en: '| Image plus noise |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 图像加噪声 |'
- en: '&#124; Synthetic: &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合成: &#124;'
- en: '&#124; 5000 gray scale &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5000 灰度级 &#124;'
- en: '&#124; texture images ($64\times 64$) &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 纹理图像 ($64\times 64$) &#124;'
- en: '&#124; from stationary random process [[2](#bib.bib2)] &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自于平稳随机过程 [[2](#bib.bib2)] &#124;'
- en: '&#124; including exponential &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包括指数分布 &#124;'
- en: '&#124; number of pixel outliers &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 像素异常值的数量 &#124;'
- en: '&#124; with heavy tailed &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带有重尾 &#124;'
- en: '&#124; distribution &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布 &#124;'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3D Shape Rendering &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 形状渲染 &#124;'
- en: '&#124; (Reconstruction) &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （重建） &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Standard $2D$ landmarks &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 标准 $2D$ 标记 &#124;'
- en: '&#124; on input face image &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在输入脸部图像上 &#124;'
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Parameters of a &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 的参数 &#124;'
- en: '&#124; BFM 3D model &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BFM 3D 模型 &#124;'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Noisy projection &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 噪声投影 &#124;'
- en: '&#124; from 3D to 2D &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从 3D 到 2D &#124;'
- en: '|'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Synthetic: &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合成: &#124;'
- en: '&#124; 72 landmarks on 2D &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2D 上的 72 个标记 &#124;'
- en: '&#124; input image of a 3D human &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 人体的输入图像 &#124;'
- en: '&#124; face generated by a Besel &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由 Besel 生成的脸部 &#124;'
- en: '&#124; Face Model(BFM) [[48](#bib.bib48)] &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脸部模型（BFM） [[48](#bib.bib48)] &#124;'
- en: '&#124; including $5\%$ outliers &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包括 $5\%$ 异常值 &#124;'
- en: '&#124; in input 2D landmarks &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在输入 2D 标记中的 &#124;'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Single Object Tracking &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单一对象跟踪 &#124;'
- en: '&#124; (Dynamic Estimation) &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （动态估计） &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Noisy location of a ball &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 球的噪声位置 &#124;'
- en: '&#124; in a board &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在板上 &#124;'
- en: '&#124; from $n$ previous time step to current step &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从 $n$ 个前一时间步到当前步 &#124;'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; True Location of the ball &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 球的真实位置 &#124;'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; True object locations &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 真实对象位置 &#124;'
- en: '&#124; plus noise &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加噪声 &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Synthetic: &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合成: &#124;'
- en: '&#124; Sequences &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 序列 &#124;'
- en: '&#124; of a moving ball location &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 移动球的位置 &#124;'
- en: '&#124; with different random initial states and variable speeds &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具有不同的随机初始状态和可变速度 &#124;'
- en: '&#124; including Gaussian noise &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包括高斯噪声 &#124;'
- en: '&#124; for all measurements. &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对所有测量值。 &#124;'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 1: The four inverse problems considered in our experiments.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 我们实验中考虑的四种逆问题。'
- en: 4.1 Linear Regression
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 线性回归
- en: We begin with an exceptionally simple inverse problem. Consider a set of one
    dimensional samples $\{(x^{(i)},m_{y}^{(i)})\}_{i=1}^{N}$, subject to noise, with
    some number of the training data subject to more extreme outliers, as illustrated
    in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems").
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个极其简单的逆问题开始。考虑一组一维样本 $\{(x^{(i)},m_{y}^{(i)})\}_{i=1}^{N}$，受噪声影响，其中一些训练数据受更极端的异常值影响，如图 [4](#S4.F4
    "图 4 ‣ 4.1 线性回归 ‣ 4 实验 ‣ 逆问题深度学习方法综述")所示。
- en: '![Refer to caption](img/9e7da7e5c5be2a2158932c577398cc5a.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9e7da7e5c5be2a2158932c577398cc5a.png)'
- en: 'Figure 4: 1D sample points for linear regression, with Gaussian noise and occasional
    large outliers.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 线性回归的一维样本点，带有高斯噪声和偶尔的大异常值。'
- en: As an inverse problem, we need to define the forward model, which for linear
    regression is simply
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个逆问题，我们需要定义正向模型，对于线性回归，正向模型就是
- en: '|  | $\underline{m}_{y}=\alpha\underline{x}+\beta+\underline{\nu}.$ |  | (20)
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}_{y}=\alpha\underline{x}+\beta+\underline{\nu}.$ |  | (20)
    |'
- en: Since our interest is in assessing the robustness of the resulting inverse solver,
    the number and behaviour of outliers should be quite irregular, to make it challenging
    for a network to generalize from the training data. As a result, the noise $\underline{\nu}$
    is random variance, plus heavy-tailed (power law) outliers, where the number of
    outliers is exponentially distributed.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的兴趣在于评估结果逆解器的鲁棒性，因此异常值的数量和行为应当相当不规则，以便让网络从训练数据中进行泛化变得具有挑战性。因此，噪声$\underline{\nu}$是随机方差，加上重尾（幂律）异常值，其中异常值的数量服从指数分布。
- en: For this inverse problem, the unknown state is comprised of the system parameters
    $\underline{z}^{T}=[\alpha,\beta]$. Thus linear regression leads to a reconstruction
    problem, for which the goal is to recover the line parameters from a sample set
    including noisy and outlier data points.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个逆问题，未知状态由系统参数$\underline{z}^{T}=[\alpha,\beta]$组成。因此，线性回归变成了一个重建问题，其目标是从包含噪声和异常值数据点的样本集中恢复直线参数。
- en: With the problem defined, we next need to formulate an approach for each of
    the three solution categories. For direct mapping (DM) and data consistency (DC),
    the training data and DNN structures are the same, shown in Figure [5](#S4.F5
    "Figure 5 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep Learning Methods
    for Inverse Problems"), where the DC approach includes an additional layer which
    applies the given forward model of ([20](#S4.E20 "In 4.1 Linear Regression ‣ 4
    Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")). We used
    the KERAS library, in which a Lambda layer is designed for this forward operation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题定义完成后，我们接下来需要为每个解决方案类别制定一个方法。对于直接映射（DM）和数据一致性（DC），训练数据和DNN结构是相同的，如图 [5](#S4.F5
    "图 5 ‣ 4.1 线性回归 ‣ 4 实验 ‣ 逆问题深度学习方法调查")所示，其中DC方法包括一个额外的层，该层应用给定的前向模型（见[20](#S4.E20
    "在 4.1 线性回归 ‣ 4 实验 ‣ 逆问题深度学习方法调查")）。我们使用了KERAS库，其中设计了一个Lambda层用于此前向操作。
- en: Since the problem is one-dimensional with limited spatial structure, the network
    contains only dense feed-forward layers. Residual blocks are used in order to
    allow gradient flow through the DNN and to improve training. Network training
    was based on 1000 records, each of $N=500$ noisy sample points.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题是一维的且空间结构有限，网络仅包含密集的前馈层。使用残差块以便允许梯度在深度神经网络（DNN）中流动，并改善训练。网络训练基于1000条记录，每条记录包含$N=500$个有噪声的样本点。
- en: '![Refer to caption](img/b06ded19987ec20cb099c5696ad214a9.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b06ded19987ec20cb099c5696ad214a9.png)'
- en: 'Figure 5: DNN structure for DM and DC solutions to linear regression. The layer
    type and number of neurons are reported below each layer. Note that in the DC
    case, there is an additional Lambda layer, which computes the forward function
    from the predicted line parameters.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 线性回归的DM和DC解决方案的DNN结构。每一层的层类型和神经元数量在每层下方报告。请注意，在DC情况下，还有一个额外的Lambda层，该层从预测的直线参数计算前向函数。'
- en: The Deep Regularizer (DR) category needs a different problem modeling scheme,
    since there is not a learning phase as in DM and DC. Instead, only a DNN (usually
    a classifier) is trained to be used as the regularizer in a variational optimization
    framework. The DNN regularizer is given the system parameters $(\alpha,\beta)$
    and determines whether they account for a feasible line. Here, we define the feasible
    line as a line having a tangent in some specified range. We generate a synthetic
    set of system parameters with associated labels for training a fully connected
    DNN as the regularizer for this category. Since our interest is in the DNN solution
    of the inverse problem, and not the details of the optimization, we have chosen
    two fairly standard optimization approaches, a simplex / Nelder-Mead approach [[49](#bib.bib49)]
    and a Genetic Algorithm (GA) strategy, both based on their respective Matlab implementations.
    Because GA solutions may be different over multiple runs, we report the results
    averaged over ten independent runs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 深度正则化器（DR）类别需要不同的问题建模方案，因为没有像DM和DC那样的学习阶段。相反，只训练一个DNN（通常是分类器），以便在变分优化框架中作为正则化器。DNN正则化器接收系统参数$(\alpha,\beta)$并确定这些参数是否表示一个可行的直线。在这里，我们定义可行直线为在某个指定范围内有切线的直线。我们生成了一组合成的系统参数及其标签，用于训练一个全连接的DNN作为此类别的正则化器。由于我们关注的是逆问题的DNN解决方案，而不是优化的细节，我们选择了两种相当标准的优化方法，一种是单纯形/内尔德-米德方法[[49](#bib.bib49)]，另一种是遗传算法（GA）策略，这两者都基于各自的Matlab实现。由于GA解决方案在多次运行中可能不同，我们报告了在十次独立运行中平均的结果。
- en: Table [2](#S4.T2 "Table 2 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of
    Deep Learning Methods for Inverse Problems") shows the average solution found
    by each category over 10 independent trainings for DM and DC, and 10 independent
    inferences for DR. The table also reports Least-Squares (LS) results as a point
    of reference method, particularly to show the improvement that deep learning methods
    have to offer for robustness in solving inverse problems. Observe the significant
    difference when the DNN methods are trained with noise-free as opposed to noisy
    data, such that the noisy training data force the network to acquire a robustness
    to outliers.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S4.T2 "Table 2 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems") 显示了每个类别在DM和DC上经过10次独立训练，以及在DR上经过10次独立推断后找到的平均解决方案。该表还报告了最小二乘（LS）结果作为参考方法，特别是为了展示深度学习方法在解决逆问题时所提供的鲁棒性提升。观察到，当DNN方法在无噪声数据上进行训练时，与在有噪声数据上训练相比，显著的差异表明噪声训练数据迫使网络获得对离群值的鲁棒性。
- en: For DR we trained a 5 layer MLP with dense layers of sizes $5,4,3,2,1$, as the
    regularizer, using the generated synthetic data including feasible line parameters
    (in the specific range) as the positive training samples and invalid line parameters
    as the negative training samples. The average test accuracy of the trained regularizer
    is $95.70\%$.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DR，我们训练了一个5层的多层感知机（MLP），其密集层的大小为$5,4,3,2,1$，作为正则化器，使用生成的合成数据，其中包含可行的线参数（在特定范围内）作为正训练样本，无效的线参数作为负训练样本。训练的正则化器的平均测试准确率为$95.70\%$。
- en: 'Table 2: The error of estimated lines, with parameters averaged over 10 independent
    training / inference runs, obtained by the three DNN categories compared with
    least-squares.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：三种DNN类别估计线的误差，与最小二乘方法进行比较，参数取自10次独立训练/推断运行的平均值。
- en: '| Training Data | Measure &#124; Method | DM | DC | DR-GA | DR-NM ($z_{0}=[0,0]$)
    | LS |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据 | 测量 &#124; 方法 | DM | DC | DR-GA | DR-NM ($z_{0}=[0,0]$) | LS |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Noisy + Outlier | Error (Slope) | $\mathbf{0.23\pm 1.37}$ | $0.30\pm 1.27$
    | $0.96\pm 0.03$ | $0.90\pm 0$ | $0.61\pm 2.10$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 含噪声 + 离群值 | 误差（斜率） | $\mathbf{0.23\pm 1.37}$ | $0.30\pm 1.27$ | $0.96\pm
    0.03$ | $0.90\pm 0$ | $0.61\pm 2.10$ |'
- en: '| Error (Intercept) | $0.15\pm 1.68$ | $\mathbf{0.06\pm 1.59}$ | $1.13\pm 0.04$
    | $1.09\pm 0$ | $0.22\pm 3.00$ |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 误差（截距） | $0.15\pm 1.68$ | $\mathbf{0.06\pm 1.59}$ | $1.13\pm 0.04$ | $1.09\pm
    0$ | $0.22\pm 3.00$ |'
- en: '| Noise-Free | Error (Slope) | $1.50\pm 2.08$ | $1.26\pm 1.45$ | $0.96\pm 0.03$
    | $0.90\pm 0$ | $\mathbf{0.61\pm 2.09}$ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 无噪声 | 误差（斜率） | $1.50\pm 2.08$ | $1.26\pm 1.45$ | $0.96\pm 0.03$ | $0.90\pm
    0$ | $\mathbf{0.61\pm 2.09}$ |'
- en: '| Error (Intercept) | $0.32\pm 1.85$ | $0.32\pm 1.38$ | $1.13\pm 0.04$ | $1.09\pm
    0$ | $\mathbf{0.21\pm 3.00}$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 误差（截距） | $0.32\pm 1.85$ | $0.32\pm 1.38$ | $1.13\pm 0.04$ | $1.09\pm 0$ |
    $\mathbf{0.21\pm 3.00}$ |'
- en: We performed the Wilcoxon signed rank test, for both cases of training with
    noisy data (Table [3](#S4.T3 "Table 3 ‣ 4.1 Linear Regression ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems")) and noise-free training
    (Table  [4](#S4.T4 "Table 4 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of
    Deep Learning Methods for Inverse Problems")). The tables show the pairwise p-values
    over the 10 independent runs. A $p-value$ in excess of $0.05$ implies that the
    two methods are likely to stem from the same distribution; in particular, the
    Wilcoxon test computes the probability that the difference between the results
    of two methods are from a distribution with median equal to zero. Clearly all
    of the DNN methods are statistically significantly different from the least-squares
    (LS) results. For noisy training data, the statistical results in Table [3](#S4.T3
    "Table 3 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep Learning Methods
    for Inverse Problems") show similar performance for DM and DC, and for DR-NM and
    DR-GA, the latter similarity suggesting that the specific choice of optimization
    methodology does not significantly affect the DR performance.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了Wilcoxon符号秩检验，适用于噪声数据训练（表 [3](#S4.T3 "Table 3 ‣ 4.1 Linear Regression ‣
    4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")）和无噪声训练（表
    [4](#S4.T4 "Table 4 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems")）。这些表格显示了10次独立运行中的配对p值。$p-value$超过$0.05$表示这两种方法可能来自同一分布；特别是，Wilcoxon检验计算了两种方法结果之间差异来自中位数为零的分布的概率。显然，所有DNN方法在统计上都与最小二乘（LS）结果显著不同。对于噪声训练数据，表 [3](#S4.T3
    "Table 3 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep Learning Methods
    for Inverse Problems")中的统计结果显示DM和DC的性能相似，对于DR-NM和DR-GA，它们的相似性表明，具体的优化方法选择对DR性能没有显著影响。
- en: 'Table 3: Wilcoxon signed rank test p-values obtained for the linear regression
    problem, using noisy and outlier data for both training and testing. We used 500
    test samples to perform the statistical analysis over 10 independent training/inference
    steps of each method.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用噪声和异常值数据进行训练和测试时，线性回归问题的 Wilcoxon 符号秩检验 p 值。我们使用 500 个测试样本在每种方法的 10 次独立训练/推断步骤上进行统计分析。
- en: '|'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; p-value &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; p-value &#124;'
- en: '&#124; (Wilcoxon Test) &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Wilcoxon 测试) &#124;'
- en: '| DM | DC | DR-GA | DR-NM | LS |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| DM | DC | DR-GA | DR-NM | LS |'
- en: '| DM | - | 0.695 | 0.002 | 0.002 | 0.002 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| DM | - | 0.695 | 0.002 | 0.002 | 0.002 |'
- en: '| DC | 0.695 | - | 0.002 | 0.002 | 0.002 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| DC | 0.695 | - | 0.002 | 0.002 | 0.002 |'
- en: '| DR-GA | 0.002 | 0.002 | - | 0.781 | 0.002 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| DR-GA | 0.002 | 0.002 | - | 0.781 | 0.002 |'
- en: '| DR-NM | 0.002 | 0.002 | 0.781 | - | 0.002 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| DR-NM | 0.002 | 0.002 | 0.781 | - | 0.002 |'
- en: '| LS | 0.002 | 0.002 | 0.002 | 0.002 | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| LS | 0.002 | 0.002 | 0.002 | 0.002 | - |'
- en: The results in Table [2](#S4.T2 "Table 2 ‣ 4.1 Linear Regression ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems") show that DM and DC significantly
    improve in robustness when trained with noisy data, relative to training with
    noise-free data. The principal difference between DM/DC versus DR is the learning
    phase for DM/DC, allowing us to conclude that, at least for reconstruction problems,
    a learning phase using noisy samples in training significantly improves the robustness
    of the solution. A further observation is that whereas DM and DC achieve similar
    performance, DC is unsupervised and DM is supervised. Thus it would appear that
    the forward model knowledge and the data consistency term as objective criterion
    for DC provide an equal degree of robustness compared to the supervised learning
    in DM.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "Table 2 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems") 的结果显示，与使用无噪声数据进行训练相比，使用噪声数据训练时，DM 和 DC
    在鲁棒性方面显著提高。DM/DC 与 DR 之间的主要区别在于 DM/DC 的学习阶段，使我们可以得出结论，至少对于重建问题而言，使用噪声样本进行训练的学习阶段显著提高了解决方案的鲁棒性。进一步观察发现，虽然
    DM 和 DC 的表现相似，但 DC 是无监督的，而 DM 是有监督的。因此，前向模型知识和数据一致性项作为 DC 的目标标准提供了与 DM 中的有监督学习相等的鲁棒性。
- en: '|'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; p-value &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; p-value &#124;'
- en: '&#124; (Wilcoxon Test) &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Wilcoxon 测试) &#124;'
- en: '| DM | DC | DR-GA | DR-NM | LS |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| DM | DC | DR-GA | DR-NM | LS |'
- en: '| DM | - | 0.002 | 0.002 | 0.002 | 0.002 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| DM | - | 0.002 | 0.002 | 0.002 | 0.002 |'
- en: '| DC | 0.002 | - | 0.002 | 0.002 | 0.002 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| DC | 0.002 | - | 0.002 | 0.002 | 0.002 |'
- en: '| DR-GA | 0.002 | 0.002 | - | 0.781 | 0.002 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| DR-GA | 0.002 | 0.002 | - | 0.781 | 0.002 |'
- en: '| DR-NM | 0.002 | 0.002 | 0.781 | - | 0.002 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| DR-NM | 0.002 | 0.002 | 0.781 | - | 0.002 |'
- en: '| LS | 0.002 | 0.002 | 0.002 | 0.002 | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| LS | 0.002 | 0.002 | 0.002 | 0.002 | - |'
- en: 'Table 4: Like Table [3](#S4.T3 "Table 3 ‣ 4.1 Linear Regression ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems"), but now using noise-free
    data, i.e., without any noise or outliers, for method training. Noisy and outlier
    data remain in place for testing.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：类似于表 [3](#S4.T3 "Table 3 ‣ 4.1 Linear Regression ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems")，但现在使用的是无噪声数据，即没有任何噪声或异常值用于方法训练。噪声和异常值数据仍用于测试。
- en: For this reconstruction problem, we conclude that both DC and DM perform well,
    with the unsupervised DC showing strong performance both with noisy and noise-free
    training data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个重建问题，我们得出结论，DC 和 DM 的表现都很好，其中无监督的 DC 在噪声和无噪声训练数据上都表现出强劲的性能。
- en: 4.2 Image Denoising (Restoration)
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 图像去噪（恢复）
- en: We now consider an image denoising problem, following the steps described in
    Section [4.1](#S4.SS1 "4.1 Linear Regression ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems") for regression. We consider real and synthetic
    images, including 5 classes and 1200 training images, 400 test images per class,
    from the Linnaeus dataset [[50](#bib.bib50)] as real data, and synthesized 5000
    texture images generated by sampling from stationary periodic kernels, as synthetic
    data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们考虑一个图像去噪问题，按照第 [4.1](#S4.SS1 "4.1 Linear Regression ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems") 节中描述的回归步骤进行。我们考虑实际图像和合成图像，包括 5
    个类别和 1200 张训练图像，每个类别 400 张测试图像，来自 Linnaeus 数据集 [[50](#bib.bib50)] 作为实际数据，以及通过从平稳周期核采样生成的
    5000 张纹理图像，作为合成数据。
- en: The synthetic images are generated using an FFT method [[2](#bib.bib2)], based
    on a thin-plate second-order Gauss-Markov random field kernel
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 合成图像是使用 FFT 方法 [[2](#bib.bib2)] 生成的，基于薄板二阶 Gauss-Markov 随机场核
- en: '|  | <math   alttext="{\cal P}=\left[\begin{array}[]{ccccc}0&amp;0&amp;1&amp;0&amp;0\\
    0&amp;2&amp;-8&amp;2&amp;0\\'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="{\cal P}=\left[\begin{array}[]{ccccc}0&amp;0&amp;1&amp;0&amp;0\\
    0&amp;2&amp;-8&amp;2&amp;0\\'
- en: 1&amp;-8&amp;20+\alpha^{2}&amp;-8&amp;1\\
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 1&amp;-8&amp;20+\alpha^{2}&amp;-8&amp;1\\
- en: 0&amp;2&amp;-8&amp;2&amp;0\\
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;2&amp;-8&amp;2&amp;0\\
- en: 0&amp;0&amp;1&amp;0&amp;0\\
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;1&amp;0&amp;0\\
- en: \end{array}\right]" display="block"><semantics ><mrow ><mi >𝒫</mi><mo  >=</mo><mrow
    ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >2</mn></mtd><mtd ><mrow  ><mo >−</mo><mn >8</mn></mrow></mtd><mtd ><mn  >2</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >1</mn></mtd><mtd ><mrow  ><mo >−</mo><mn
    >8</mn></mrow></mtd><mtd ><mrow  ><mn >20</mn><mo >+</mo><msup  ><mi >α</mi><mn
    >2</mn></msup></mrow></mtd><mtd ><mrow  ><mo >−</mo><mn >8</mn></mrow></mtd><mtd
    ><mn  >1</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mn  >2</mn></mtd><mtd
    ><mrow  ><mo >−</mo><mn >8</mn></mrow></mtd><mtd ><mn  >2</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr></mtable><mo >]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >𝒫</ci><apply ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix
    ><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn type="integer"  >1</cn><cn
    type="integer"  >0</cn><cn type="integer"  >0</cn></matrixrow><matrixrow ><cn
    type="integer" >0</cn><cn type="integer" >2</cn><apply  ><cn type="integer"  >8</cn></apply><cn
    type="integer"  >2</cn><cn type="integer"  >0</cn></matrixrow><matrixrow ><cn
    type="integer" >1</cn><apply  ><cn type="integer"  >8</cn></apply><apply ><cn
    type="integer" >20</cn><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝛼</ci><cn type="integer" >2</cn></apply></apply><apply ><cn type="integer" >8</cn></apply><cn
    type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >2</cn><apply
    ><cn type="integer" >8</cn></apply><cn type="integer" >2</cn><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >1</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{\cal P}=\left[\begin{array}[]{ccccc}0&0&1&0&0\\
    0&2&-8&2&0\\ 1&-8&20+\alpha^{2}&-8&1\\ 0&2&-8&2&0\\ 0&0&1&0&0\\ \end{array}\right]</annotation></semantics></math>
    |  | (21) |
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right]" display="block"><semantics ><mrow ><mi >𝒫</mi><mo  >=</mo><mrow
    ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >2</mn></mtd><mtd ><mrow  ><mo >−</mo><mn >8</mn></mrow></mtd><mtd ><mn  >2</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >1</mn></mtd><mtd ><mrow  ><mo >−</mo><mn
    >8</mn></mrow></mtd><mtd ><mrow  ><mn >20</mn><mo >+</mo><msup  ><mi >α</mi><mn
    >2</mn></msup></mrow></mtd><mtd ><mrow  ><mo >−</mo><mn >8</mn></mrow></mtd><mtd
    ><mn  >1</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mn  >2</mn></mtd><mtd
    ><mrow  ><mo >−</mo><mn >8</mn></mrow></mtd><mtd ><mn  >2</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr></mtable><mo >]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >𝒫</ci><apply ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix
    ><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn type="integer"  >1</cn><cn
    type="integer"  >0</cn><cn type="integer"  >0</cn></matrixrow><matrixrow ><cn
    type="integer" >0</cn><cn type="integer" >2</cn><apply  ><cn type="integer"  >8</cn></apply><cn
    type="integer"  >2</cn><cn type="integer"  >0</cn></matrixrow><matrixrow ><cn
    type="integer" >1</cn><apply  ><cn type="integer"  >8</cn></apply><apply ><cn
    type="integer" >20</cn><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝛼</ci><cn type="integer" >2</cn></apply></apply><apply ><cn type="integer" >8</cn></apply><cn
    type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >2</cn><apply
    ><cn type="integer" >8</cn></apply><cn type="integer" >2</cn><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >1</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{\cal P}=\left[\begin{array}[]{ccccc}0&0&1&0&0\\
    0&2&-8&2&0\\ 1&-8&20+\alpha^{2}&-8&1\\ 0&2&-8&2&0\\ 0&0&1&0&0\\ \end{array}\right]</annotation></semantics></math>
    |  | (21) |
- en: such that a texture $T$ is found by inverting the kernel in the frequency domain,
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，通过在频域中反转核来找到纹理$T$，
- en: '|  | $T=FFT^{-1}_{2}\left(\sqrt{1\oslash FFT_{2}({\cal P})}\odot FFT_{2}(W)\right),$
    |  | (22) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $T=FFT^{-1}_{2}\left(\sqrt{1\oslash FFT_{2}({\cal P})}\odot FFT_{2}(W)\right),$
    |  | (22) |'
- en: with $\odot,\oslash$ as element-by-element multiplication and division, $W$
    as unit-variance white noise, and with the kernel ${\cal P}$ zero-padded to the
    intended size of $T$. Further details about this approach can be found in [[2](#bib.bib2)].
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用$\odot,\oslash$作为逐元素乘法和除法，$W$为单位方差的白噪声，并将核${\cal P}$零填充至目标大小$T$。关于这种方法的进一步细节可以在[[2](#bib.bib2)]中找到。
- en: Parameter $\alpha^{2}$, affecting the central element of the kernel $\cal P$,
    effectively determines the texture spatial correlation-length in $T$, as
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 $\alpha^{2}$，影响核 $\cal P$ 的中心元素，有效地决定了 $T$ 中的纹理空间相关长度，如下所示
- en: '|  | $\displaystyle\alpha^{2}=10^{4-log_{10}u}$ |  | (23) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha^{2}=10^{4-log_{10}u}$ |  | (23) |'
- en: for process correlation length, $u$, measured in pixels. We set $u$ to be a
    random integer in the range $[10,200]$ in our experiments.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于过程相关长度 $u$，以像素为单位。在我们的实验中，我们将 $u$ 设置为范围 $[10,200]$ 内的随机整数。
- en: All images are set to be $64\times 64$ in size, with pixel values normalized
    to $[0,1]$. Pixels are corrupted by additive Gaussian noise, with an exponentially
    distributed number of outliers. The inverse problem is a restoration problem,
    having the objective of restoring the original image from its noisy/outlier observation.
    The linear forward model is
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像均设置为 $64\times 64$ 的大小，像素值归一化到 $[0,1]$。像素受到加性高斯噪声的污染，外点数呈指数分布。逆问题是一个恢复问题，其目标是从噪声/外点观察中恢复原始图像。线性前向模型为
- en: '|  | $\underline{m}=\underline{z}+\underline{\nu}$ |  | (24) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}=\underline{z}+\underline{\nu}$ |  | (24) |'
- en: for measured, original, and added noise, respectively. The Gaussian noise $\nu$
    has zero mean and random variance, and an exponential number of pixels become
    outliers, their values replaced with a uniformly distributed random intensity
    value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测量的、原始的和添加的噪声，分别为。高斯噪声 $\nu$ 具有零均值和随机方差，并且一定数量的像素成为外点，其值被均匀分布的随机强度值所替代。
- en: We used 5000 training samples and 500 test samples for the learning and evaluation
    phases of the DM and DC approaches. The DNN structure for both DM and DC is the
    same and is shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Image Denoising (Restoration)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems"). In the
    case of DC, we design a DNN layer to compute the forward function. Since we are
    dealing with input images, both as measurements and system state, we design a
    fully convolutional DNN in an encoder-decoder structure, finding the main structures
    in the image through encoding and recovering the image via decoding. Since there
    may be information loss during encoding, we introduce skip connections to help
    preserve desirable information.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 5000 个训练样本和 500 个测试样本来进行 DM 和 DC 方法的学习和评估阶段。DM 和 DC 的 DNN 结构相同，如图 [6](#S4.F6
    "Figure 6 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems") 所示。在 DC 的情况下，我们设计了一个 DNN 层来计算前向函数。由于我们处理的是输入图像，无论是作为测量还是系统状态，我们设计了一个完全卷积的
    DNN，采用编码器-解码器结构，通过编码找到图像中的主要结构，并通过解码恢复图像。由于编码过程中可能会有信息丢失，我们引入了跳跃连接以帮助保留有用的信息。
- en: '![Refer to caption](img/d745db3704fbd60620ba387b4ee9ced5.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d745db3704fbd60620ba387b4ee9ced5.png)'
- en: 'Figure 6: DNN for the DM and DC solutions. We have a fully convolutional DNN
    with an encoder-decoder structure, where the values in parentheses indicate the
    stride value of the corresponding convolutional layer. The skip connection helps
    to recover desirable information which may be lost during encoding.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：用于 DM 和 DC 解决方案的 DNN。我们有一个完全卷积的 DNN，采用编码器-解码器结构，其中括号中的值表示相应卷积层的步幅值。跳跃连接有助于恢复在编码过程中可能丢失的有用信息。
- en: The DR category needs a pre-trained regularizer which determines whether the
    prediction is a feasible texture image. We trained a classifier for texture discrimination,
    generated using ([22](#S4.E22 "In 4.2 Image Denoising (Restoration) ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems")), from ordinary images
    gathered from the web, as the regularizer. Both GA and Nelder-Mead optimizers
    are used.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: DR 类别需要一个预训练的正则化器，用于确定预测是否为可行的纹理图像。我们训练了一个纹理分类器，生成方法使用了 ([22](#S4.E22 "In 4.2
    Image Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep Learning Methods
    for Inverse Problems"))，该分类器是从网络上收集的普通图像中生成的，作为正则化器。使用了 GA 和 Nelder-Mead 优化器。
- en: We use peak signal to noise ratio (PSNR) as the evaluation criterion, computed
    as
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用峰值信噪比（PSNR）作为评估标准，计算公式为
- en: '|  | $\displaystyle\text{PSNR}(I^{\text{pred}},I^{\text{GT}})=20\cdot log_{10}\max(I^{\text{pred}})-10\cdot
    log_{10}\text{MSE},$ |  | (25) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{PSNR}(I^{\text{pred}},I^{\text{GT}})=20\cdot log_{10}\max(I^{\text{pred}})-10\cdot
    log_{10}\text{MSE},$ |  | (25) |'
- en: '|  | $\displaystyle\text{MSE}=\frac{1}{n}\sum_{i,j}(I^{\text{GT}}_{i,j}-I^{\text{pred}}_{i,j})^{2}$
    |  | (26) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{MSE}=\frac{1}{n}\sum_{i,j}(I^{\text{GT}}_{i,j}-I^{\text{pred}}_{i,j})^{2}$
    |  | (26) |'
- en: where $I^{\text{GT}}_{i,j}$, $I^{\text{pred}}_{i,j}$ are the $(i,j)^{th}$ pixel
    in the ground-truth and predicted images, respectively. Note that in the DR case,
    since the input and output of the model are $64*64=4096$ images, the GA optimization
    routine was unable to find the solution in a reasonable time, therefore we do
    not avoid report any DR-GA results for this problem.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I^{\text{GT}}_{i,j}$, $I^{\text{pred}}_{i,j}$ 分别表示 Ground truth 和 predicted
    images 的 $(i,j)^{th}$ 像素。需要注意的是，在 DR 的情况下，由于模型的输入和输出是 $64*64=4096$ 张图像，GA 优化算法无法在合理的时间内找到解决方案，因此我们没有报告
    DR-GA 在这个问题上的任何结果。
- en: As a reference point, we also report results obtained by the non-local means
    (NLM) filter [[51](#bib.bib51)], to give insight into the amount of improvement
    of deep learning inverse methods over a well-established standard in image denoising.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，我们还报告了非局部均值（NLM）滤波器[[51](#bib.bib51)]得到的结果，以便了解深度学习逆问题方法相比于图像去噪领域中已经确立的标准有多大的改进。
- en: Figure [7](#S4.F7 "Figure 7 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems") shows results based on
    synthetic textures. Each row in the figure shows a sample image associated with
    a particular correlation length noise standard deviation. The DM approach offers
    by far the best reconstruction among the DNN methods, and outperforms NLM in terms
    of PSNR. The time complexity of GA in DR-GA makes it inapplicable to problems
    of significant size (even though the images were still quite modest in size).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [7](#S4.F7 "图 7 ‣ 4.2 图像去噪（修复） ‣ 4 实验 ‣ 深度学习逆问题调查")显示了基于合成纹理的结果。图中的每一行显示与特定相关长度噪声标准差相关联的样本图像。在
    DNN 方法中，DM 方法在重建方面远远优于其他方法，并在 PSNR 方面优于非局部均值滤波器。DR-GA 中 GA 的时间复杂度使得它在处理规模较大的问题时不适用（尽管图像的大小仍然相当适中）。
- en: '| u | $\sigma$ | Clean Image | Input Image (Noisy) | DM | DC | DR-NM | NLM
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| u | $\sigma$ | 清晰图像 | 输入图像（带噪） | DM | DC | DR-NM | NLM |'
- en: '| $10$ | $0.2$ | ![Refer to caption](img/b09ffa28ebcbb8460bf338aa828738a0.png)
    | ![Refer to caption](img/1d73171621a7c67304d0db1baf428208.png) | ![Refer to caption](img/cf0d53b99aa08c3afbec98ffc9b757d1.png)
    | ![Refer to caption](img/e7d763516bd31be66fec273638115eb4.png) | ![Refer to caption](img/5c584b8cf89026c9e04aab55a60b83c2.png)
    | ![Refer to caption](img/bc25dddb7f7d31572c15796350207f6f.png) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| $10$ | $0.2$ | ![请参阅题注](img/b09ffa28ebcbb8460bf338aa828738a0.png) | ![请参阅题注](img/1d73171621a7c67304d0db1baf428208.png)
    | ![请参阅题注](img/cf0d53b99aa08c3afbec98ffc9b757d1.png) | ![请参阅题注](img/e7d763516bd31be66fec273638115eb4.png)
    | ![请参阅题注](img/5c584b8cf89026c9e04aab55a60b83c2.png) | ![请参阅题注](img/bc25dddb7f7d31572c15796350207f6f.png)
    |'
- en: '| $50$ | $0.2$ | ![Refer to caption](img/2e3ce76234326a6c1307b5906ba2539c.png)
    | ![Refer to caption](img/bd32421428c106e07564ddf186a26ee0.png) | ![Refer to caption](img/b68320b6ee1ab235afd130bca744b9ce.png)
    | ![Refer to caption](img/d09c29c6ae031e392396a138172b2686.png) | ![Refer to caption](img/9317990e874a2c96f3a35460a0524f90.png)
    | ![Refer to caption](img/a8c51f16c7978aad82025107c9d890c0.png) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| $50$ | $0.2$ | ![请参阅题注](img/2e3ce76234326a6c1307b5906ba2539c.png) | ![请参阅题注](img/bd32421428c106e07564ddf186a26ee0.png)
    | ![请参阅题注](img/b68320b6ee1ab235afd130bca744b9ce.png) | ![请参阅题注](img/d09c29c6ae031e392396a138172b2686.png)
    | ![请参阅题注](img/9317990e874a2c96f3a35460a0524f90.png) | ![请参阅题注](img/a8c51f16c7978aad82025107c9d890c0.png)
    |'
- en: '| $150$ | $0.2$ | ![Refer to caption](img/fca900dd031980d095172129a70a4a38.png)
    | ![Refer to caption](img/f816e68b23286ab9a4d496d51a457496.png) | ![Refer to caption](img/ba5c54e3f58e06b1a8575777e7e1f2d4.png)
    | ![Refer to caption](img/59d62e6ecb6856f03291a4ac51839de5.png) | ![Refer to caption](img/65791a6f4e7cdf69436df3f274b4a0f0.png)
    | ![Refer to caption](img/8d93f61f7e1eda5bb45cc58e675a42ae.png) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| $150$ | $0.2$ | ![请参阅题注](img/fca900dd031980d095172129a70a4a38.png) | ![请参阅题注](img/f816e68b23286ab9a4d496d51a457496.png)
    | ![请参阅题注](img/ba5c54e3f58e06b1a8575777e7e1f2d4.png) | ![请参阅题注](img/59d62e6ecb6856f03291a4ac51839de5.png)
    | ![请参阅题注](img/65791a6f4e7cdf69436df3f274b4a0f0.png) | ![请参阅题注](img/8d93f61f7e1eda5bb45cc58e675a42ae.png)
    |'
- en: '| $200$ | $0.2$ | ![Refer to caption](img/25a67d6d302f03af83060fde22dbe873.png)
    | ![Refer to caption](img/65db94af3cdad35dd78ec41b43d4916a.png) | ![Refer to caption](img/1cb94aaaed022b0d9e390ea2bec34f77.png)
    | ![Refer to caption](img/47c51b1ba8fb31427b15d2d8402b62ce.png) | ![Refer to caption](img/34db1aa9be6df9e9cd71dda2c65d140c.png)
    | ![Refer to caption](img/beccd926b64fa88b6f2e8dd5fccfa8b1.png) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| $200$ | $0.2$ | ![请参阅题注](img/25a67d6d302f03af83060fde22dbe873.png) | ![请参阅题注](img/65db94af3cdad35dd78ec41b43d4916a.png)
    | ![请参阅题注](img/1cb94aaaed022b0d9e390ea2bec34f77.png) | ![请参阅题注](img/47c51b1ba8fb31427b15d2d8402b62ce.png)
    | ![请参阅题注](img/34db1aa9be6df9e9cd71dda2c65d140c.png) | ![请参阅题注](img/beccd926b64fa88b6f2e8dd5fccfa8b1.png)
    |'
- en: '| $100$ | $0.1$ | ![Refer to caption](img/ab71fe912d8b79c123d83e1b04fb313a.png)
    | ![Refer to caption](img/2de8c016be4592d71d5347f08eaef8c8.png) | ![Refer to caption](img/50eb1f8544cb73dd6b52647cf37ba1cf.png)
    | ![Refer to caption](img/65148f1d62e890a8272560505e8339c5.png) | ![Refer to caption](img/adc2081639a7978b7df1946ebe9b1f17.png)
    | ![Refer to caption](img/ada842c5e84d5a008c86ec1754286ab6.png) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| $100$ | $0.2$ | ![Refer to caption](img/ca2dc1cb1867d5281cf94a10317d8612.png)
    | ![Refer to caption](img/ec19e66aa6294b49945d4c854decb2ec.png) | ![Refer to caption](img/bcb8ae0c2cab427378ebbfea943c2a19.png)
    | ![Refer to caption](img/62cd1414c6fb241e3b8768a9a6168cfe.png) | ![Refer to caption](img/fd1fb9fa8eb68f7d7a4337fd5d3e316e.png)
    | ![Refer to caption](img/89d52c0a32293d952479b3fdffb1f8be.png) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| $100$ | $0.3$ | ![Refer to caption](img/5a7def1b47d4a008a3f4abaf192dbd45.png)
    | ![Refer to caption](img/0ea1d05dcc7056495997d5d2f22d9525.png) | ![Refer to caption](img/bbbdda3c95ec128340a347ea5bf23544.png)
    | ![Refer to caption](img/07bab4c0bfe358ed3c0e721ac4b38e65.png) | ![Refer to caption](img/4fbcb593e8606c5c544e6ca1ff334df5.png)
    | ![Refer to caption](img/5778aca8a03a0a55eb6fa9abaf4938ca.png) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '|  |  | Average PSNR |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '&#124; $19.75$ &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(\pm 9.52)$ &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathbf{29.24}$ &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathbf{(\pm 2.25)}$ &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $19.81$ &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(\pm 7.71)$ &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $19.75$ &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\pm 9.52$) &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $28.48$ &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\pm 5.42$) &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Image denoinsing results on synthetic textures. Only a single image
    is shown in each case, however the reported average PSNR at the bottom is computed
    over the entire test set. The given noisy image is subject to both additive noise
    and outliers. NLM, in the rightmost column, is the non-local means filter, a standard
    approach from image processing.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The Wilcoxon signed rank test was performed on the DM, DC and DR-(Nelder-Mead)
    results. The statistical analysis of the obtained results gave a $p$ value of
    0.002 for each pairwise comparison, implying a statistically significant difference,
    thus the very strong performance of DM in Figure [7](#S4.F7 "Figure 7 ‣ 4.2 Image
    Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for
    Inverse Problems") is validated.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: In the case of real images, Figure [8](#S4.F8 "Figure 8 ‣ 4.2 Image Denoising
    (Restoration) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")
    shows the visual results obtained by DM, DC and DR-NM for seven test samples.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '| Clean Image | Input Image (Noisy) | DM | DC | DR-NM | NLM |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3f75bf5b5f8435fda2d846acca52941c.png) | ![Refer to
    caption](img/be5fd40878e805776c295c0d8d433768.png) | ![Refer to caption](img/2ac71b7907acd2097052898cb9ea2cbe.png)
    | ![Refer to caption](img/4756bbf6ddea49561dac5c8c849245a6.png) | ![Refer to caption](img/83dfd7abefe3524b25477a8cdbe5e428.png)
    | ![Refer to caption](img/af735d0d0ebab87e8b336c33f3512a0f.png) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/60d9b0d277668f55e5706cf92bae6b0b.png) | ![Refer to
    caption](img/0c9eddfba201c2069e2c08406ddeb6bc.png) | ![Refer to caption](img/555e4e3a29ab3e70dd5a41a54812243a.png)
    | ![Refer to caption](img/685ba51f4b3226e29df3dbd4bff35687.png) | ![Refer to caption](img/fd9d73daa28d7cf47f9308ea779cd807.png)
    | ![Refer to caption](img/3f04207892d7712f10ad625aced66df7.png) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/5753ca9b8fc1189100492f29c89b8167.png) | ![Refer to
    caption](img/9bd2de73e8ed8fee667f15b08836094a.png) | ![Refer to caption](img/435c2654849c31b0acbf8e49699f42b1.png)
    | ![Refer to caption](img/609b7c9a29190268ffa4e1e11de69edb.png) | ![Refer to caption](img/bd6563268a2d15b238aa9fd7cb593a80.png)
    | ![Refer to caption](img/d4d2819195e5854b31dbd2ab7902a7aa.png) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/0a8e915c3963f7b70d5163ee5a1bfc10.png) | ![Refer to
    caption](img/490d998f1157642c04e4da844d5df84e.png) | ![Refer to caption](img/7c276031c733f0615061d75c31baaa3f.png)
    | ![Refer to caption](img/f7dc61fca1ff672b5aabdc86b95e596b.png) | ![Refer to caption](img/7a01ae0e321684c3f8b531ddf983edf0.png)
    | ![Refer to caption](img/52f962688f637498b5b33bd40c7ec2fd.png) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/e96d05ccaca983b7a7f06e5017d25ee5.png) | ![Refer to
    caption](img/e8295b76aee5ab184fc1e9fc753da808.png) | ![Refer to caption](img/0114ebdb07da9f162484ba460668ef82.png)
    | ![Refer to caption](img/11bda8219c6e914e1d8da33cd77e49a9.png) | ![Refer to caption](img/b8ced6579baff1f67d3d99c88865eb39.png)
    | ![Refer to caption](img/6b32b69355eee15660ba1bce28cfd63a.png) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/938676cedd23a391c600bca33114fbfb.png) | ![Refer to
    caption](img/87e85a3d488fb17bc8af0cf8039a6a1e.png) | ![Refer to caption](img/d179db6b5b4d751354039aaed95a3bb4.png)
    | ![Refer to caption](img/a11cd247db9e5ea17f78d18816d58392.png) | ![Refer to caption](img/7ad533be83f7980a6337d2b48e28aaf9.png)
    | ![Refer to caption](img/8996197c42146d9b30be235fa3ddf814.png) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/88db92d64bf2540be5b1af8cfba3ec26.png) | ![Refer to
    caption](img/f52a5d73794b64c4f8ac206d3be19694.png) | ![Refer to caption](img/50559fe5f2e58f10d516b864dab7b720.png)
    | ![Refer to caption](img/5b9a1b14bc6a0e2d1e796fd3610f8863.png) | ![Refer to caption](img/df1a6f79a81679e1986249ae9d0a405f.png)
    | ![Refer to caption](img/e0340ca642c6016772d6d08235d45e8f.png) |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| Average PSNR |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '&#124; $19.75$ &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(\pm 8.20)$ &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathbf{24.70}$ &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathbf{(\pm 2.41)}$ &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $23.86$ &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(\pm 5.30)$ &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $19.75$ &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\pm 8.20$) &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $24.38$ &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\pm 4.93$) &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: As in Figure [7](#S4.F7 "Figure 7 ‣ 4.2 Image Denoising (Restoration)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems"), but
    here for denoising results on the Linnaeus dataset. The reported average PSNR
    in the last row is computed over all test images. As in Figure [7](#S4.F7 "Figure
    7 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems"), the DM results significantly outperform other
    DNN inverse solvers and also non-local means (NLM).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：如图[7](#S4.F7 "Figure 7 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments
    ‣ Survey of Deep Learning Methods for Inverse Problems")所示，但这里是对Linnaeus数据集的去噪结果。最后一行报告的平均PSNR是对所有测试图像计算的。与图[7](#S4.F7
    "Figure 7 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems")相同，DM结果显著优于其他DNN逆解算器和非局部均值（NLM）。
- en: The statistical analysis is consistent with the results from the synthetic texture
    case, which is that all pairwise Wilcoxon tests led to a conclusion of statistically
    significant differences, with $p$ values well below $0.05$.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 统计分析与合成纹理案例中的结果一致，即所有成对Wilcoxon检验均得出统计显著差异的结论，$p$ 值远低于 $0.05$。
- en: 'From the results in Figures [7](#S4.F7 "Figure 7 ‣ 4.2 Image Denoising (Restoration)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems") and [8](#S4.F8
    "Figure 8 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems") and their respective statistical analyses,
    we conclude that:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 从图[7](#S4.F7 "Figure 7 ‣ 4.2 Image Denoising (Restoration) ‣ 4 Experiments ‣
    Survey of Deep Learning Methods for Inverse Problems")和图[8](#S4.F8 "Figure 8 ‣
    4.2 Image Denoising (Restoration) ‣ 4 Experiments ‣ Survey of Deep Learning Methods
    for Inverse Problems")及其各自的统计分析结果中，我们得出结论：
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For image denoising as a prototype for restoration problems, which have the
    same measurement and system parameter spaces, the concentration of the loss function
    on the true parameters (as in DM) provides better information and leads to a more
    effective estimator having greater robustness than the measurements themselves
    (as in DC).
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于图像去噪作为恢复问题的原型，其测量和系统参数空间相同，损失函数对真实参数的集中（如DM中所示）提供了更好的信息，并且导致比测量本身（如DC中所示）更有效的估计器，具有更大的鲁棒性。
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DR-(Nelder-Mead) performed poorly, even though it optimizes data consistency,
    like DC, however we believe that the learning phase in DC, compared to DR, provides
    knowledge for its inference and allows DC to be more robust than DR for restoration
    inverse problems.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DR-(Nelder-Mead)表现不佳，尽管它优化了数据一致性，类似于DC，但我们认为，相比于DR，DC中的学习阶段为其推断提供了知识，使DC在恢复逆问题中比DR更具鲁棒性。
- en: 4.3 3D shape Inverse Rendering (Reconstruction)
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 3D形状逆渲染（重建）
- en: We now wish to test a 3D shape inverse rendering (IR) [[48](#bib.bib48)] problem,
    for which a 3D morphable model (3DMM) [[52](#bib.bib52)] describes the 3D shape
    of a human face $\underline{s}$. This model is based on extracting eigenfaces
    $\underline{s}_{i}$, usually using PCA, from a set of 3D face shapes as the training
    data, then to obtain new faces as a weighted combination $z_{i}$ of the eigenfaces.
    The 3D shape model reconstructs a 3D face in homogeneous coordinates as
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在希望测试一个3D形状逆渲染（IR）[[48](#bib.bib48)] 问题，其中一个3D可变形模型（3DMM）[[52](#bib.bib52)]
    描述了人脸的3D形状 $\underline{s}$。该模型基于从一组3D面部形状中提取特征脸 $\underline{s}_{i}$，通常使用PCA，然后通过特征脸的加权组合
    $z_{i}$ 获得新面孔。3D形状模型在齐次坐标系下重建3D面孔为
- en: '|  | $\underline{s}=\underline{\bar{s}}+\sum_{i=1}^{n}z_{i}\underline{s}_{i},$
    |  | (27) |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{s}=\underline{\bar{s}}+\sum_{i=1}^{n}z_{i}\underline{s}_{i},$
    |  | (27) |'
- en: where $\underline{\bar{s}}$ is the mean shape of the 3DMM, and $z_{i}$ the weight
    of eigenface $\underline{s}_{i}$. We use the Besel Face Model [[48](#bib.bib48)]
    as the 3DMM in this experiment for which there are $N=54390$ 3D points in each
    face shape and 199 eigenfaces. We can therefore rewrite ([27](#S4.E27 "In 4.3
    3D shape Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems")) as
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\underline{\bar{s}}$ 是3DMM的平均形状，$z_{i}$ 是特征脸 $\underline{s}_{i}$ 的权重。我们在此实验中使用Besel面部模型[[48](#bib.bib48)]作为3DMM，每个面部形状中有
    $N=54390$ 个3D点和199个特征脸。因此，我们可以将([27](#S4.E27 "In 4.3 3D shape Inverse Rendering
    (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse
    Problems"))重写为
- en: '|  | $\underline{s}_{N}=\underline{\bar{s}}_{N}+\underline{z}^{T}*S_{N}$ |  |
    (28) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{s}_{N}=\underline{\bar{s}}_{N}+\underline{z}^{T}*S_{N}$ |  |
    (28) |'
- en: where $S$ is the tensor of $199$ eigenfaces. In our experiments each face is
    characterized by 72 standard landmarks, shown in Figure [9](#S4.F9 "Figure 9 ‣
    4.3 3D shape Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems"), which are normalized and then presented
    to the system as the measurements. Therefore we actually only care about $L=72$
    out of $N=54390$ 3D points in the 3DMM. This experiment tackles the reconstruction
    of a 3D human face by finding the weights $\underline{z}$ of the 3DMM from its
    input 2D landmarks. We generated training data from the 3DMM by assigning random
    values to the 3DMM weights, resulting in a 3D human face, and rendered the obtained
    3D shape into a 2D image using orthographic projection.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$S$是$199$个特征脸的张量。在我们的实验中，每张面孔都由72个标准地标来表征，如图[9](#S4.F9 "图9 ‣ 4.3 3D形状反向渲染（重建）
    ‣ 4 实验 ‣ 深度学习方法在逆问题中的调查")所示，这些地标被标准化后作为测量值输入系统。因此，我们实际上只关心$L=72$个地标点，而不是3DMM中的$N=54390$个3D点。这个实验通过从输入的2D地标找到3DMM的权重$\underline{z}$，来解决3D人脸的重建问题。我们通过对3DMM权重赋予随机值生成了训练数据，从而得到一个3D人脸，并使用正射投影将获得的3D形状渲染成2D图像。
- en: '![Refer to caption](img/c4136602744e5fdb88aaf05da4fce07d.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c4136602744e5fdb88aaf05da4fce07d.png)'
- en: 'Figure 9: Location and order of 72 standard landmarks on a 2D image of a sample
    human face.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：样本人脸的2D图像上的72个标准地标的位置和顺序。
- en: The measurement noise consists of small perturbations of the 2D landmarks, with
    outliers as much larger landmark perturbations. We add zero-mean Gaussian noise
    having a standard deviation of $3\times 10^{3}$ in the training data and $5\times
    10^{3}$ in the test data. Outliers are much larger, with a standard deviation
    of $5\times 10^{4}$ added to 10 of the 72 landmarks in $10\%$ of the training
    data and $20\%$ of the test data. Landmark point coordinates are in the range
    $[-8\times 10^{4},8\times 10^{4}]$, so the outlier magnitudes are very large.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 测量噪声由2D地标的微小扰动组成，其中离群值是更大规模的地标扰动。我们在训练数据中添加了标准差为$3\times 10^{3}$的零均值高斯噪声，在测试数据中则添加了标准差为$5\times
    10^{3}$的噪声。离群值要大得多，标准差为$5\times 10^{4}$，被添加到72个地标中的10个，在训练数据的$10\%$和测试数据的$20\%$中。地标点坐标的范围是$[-8\times
    10^{4},8\times 10^{4}]$，因此离群值的幅度非常大。
- en: Let subscript [L] represent the the set of landmark point indices, in which
    case the forward model is the orthographic projection
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 让下标[L]表示地标点索引的集合，在这种情况下，前向模型是正射投影
- en: '|  | <math   alttext="\underline{m}=C\underline{s}_{L}+\underline{\nu}\qquad
    C=\left[\begin{array}[]{cccc}1&amp;0&amp;0&amp;0\\ 0&amp;1&amp;0&amp;0\\'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\underline{m}=C\underline{s}_{L}+\underline{\nu}\qquad
    C=\left[\begin{array}[]{cccc}1&amp;0&amp;0&amp;0\\ 0&amp;1&amp;0&amp;0\\'
- en: 0&amp;0&amp;0&amp;0\end{array}\right]" display="block"><semantics ><mrow ><mrow
    ><munder accentunder="true" ><mi  >m</mi><mo >¯</mo></munder><mo >=</mo><mrow
    ><mrow  ><mi >C</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><munder accentunder="true"
    ><mi >s</mi><mo >¯</mo></munder><mi >L</mi></msub></mrow><mo >+</mo><munder accentunder="true"
    ><mi  >ν</mi><mo >¯</mo></munder></mrow></mrow><mrow ><mi >C</mi><mo  >=</mo><mrow
    ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mn >1</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><apply ><ci  >¯</ci><ci >𝑚</ci></apply><apply
    ><apply ><ci  >𝐶</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >¯</ci><ci >𝑠</ci></apply><ci >𝐿</ci></apply></apply><apply ><ci  >¯</ci><ci
    >𝜈</ci></apply></apply></apply><apply ><ci >𝐶</ci><apply  ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix
    ><matrixrow ><cn type="integer"  >1</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn><cn type="integer"
    >1</cn><cn type="integer" >0</cn><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\underline{m}=C\underline{s}_{L}+\underline{\nu}\qquad
    C=\left[\begin{array}[]{cccc}1&0&0&0\\ 0&1&0&0\\ 0&0&0&0\end{array}\right]</annotation></semantics></math>
    |  | (29) |
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 0&0&0&0\end{array}\right]" display="block"><semantics ><mrow ><mrow ><munder
    accentunder="true" ><mi  >m</mi><mo >¯</mo></munder><mo >=</mo><mrow ><mrow  ><mi
    >C</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><munder accentunder="true"
    ><mi >s</mi><mo >¯</mo></munder><mi >L</mi></msub></mrow><mo >+</mo><munder accentunder="true"
    ><mi  >ν</mi><mo >¯</mo></munder></mrow></mrow><mrow ><mi >C</mi><mo  >=</mo><mrow
    ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mn >1</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><apply ><ci  >¯</ci><ci >𝑚</ci></apply><apply
    ><apply ><ci  >𝐶</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >¯</ci><ci >𝑠</ci></apply><ci >𝐿</ci></apply></apply><apply ><ci  >¯</ci><ci
    >𝜈</ci></apply></apply></apply><apply ><ci >𝐶</ci><apply  ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix
    ><matrixrow ><cn type="integer"  >1</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn><cn type="integer"
    >1</cn><cn type="integer" >0</cn><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\underline{m}=C\underline{s}_{L}+\underline{\nu}\qquad
    C=\left[\begin{array}[]{cccc}1&0&0&0\\ 0&1&0&0\\ 0&0&0&0\end{array}\right]</annotation></semantics></math>
    |  | (29) |
- en: such that $C$ converts from homogeneous 3D to homogeneous 2D coordinates, and
    the measurement noise is
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 使得$C$将同质3D坐标转换为同质2D坐标，测量噪声为
- en: '|  | $\underline{\nu}\sim 0.9N(0,3\times 10^{3}I)+0.1N(0,5\times 10^{4}I)$
    |  | (30) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{\nu}\sim 0.9N(0,3\times 10^{3}I)+0.1N(0,5\times 10^{4}I)$
    |  | (30) |'
- en: as noise and outliers associated with the projection operator. Since the goal
    of this inverse problem is to estimate $\underline{z}$ in the 3DMM for a given
    3D shape, we write ([29](#S4.E29 "In 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")) as
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 作为与投影算子相关的噪声和离群值。由于此逆问题的目标是在给定的3D形状中估计$\underline{z}$，我们写作（[29](#S4.E29 "在4.3
    3D形状逆向渲染（重建） ‣ 4 实验 ‣ 逆向问题的深度学习方法概述")）为
- en: '|  | $\underline{m}=C(\underline{\bar{s}}_{L}+\underline{z}^{T}*S_{L})+\underline{\nu}$
    |  | (31) |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}=C(\underline{\bar{s}}_{L}+\underline{z}^{T}*S_{L})+\underline{\nu}$
    |  | (31) |'
- en: For the DM and DC solutions we generated 4000 sample faces as training data,
    using the Besel face model [[48](#bib.bib48)] as the 3DMM. The DR regularizer
    is a pre-trained classifier which discriminates a feasible 3D shape from random
    distorted versions of it.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DM和DC解决方案，我们生成了4000个样本面孔作为训练数据，使用Besel面孔模型[[48](#bib.bib48)]作为3DMM。DR正则化器是一个经过预训练的分类器，用于区分可行的3D形状与其随机扭曲版本。
- en: In DC we implemented the forward function layer as described in [[48](#bib.bib48)],
    with the resulting DM and DC DNN shown in Figure [10](#S4.F10 "Figure 10 ‣ 4.3
    3D shape Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems"), where we used feed-forward layers because the
    system input is the vectorized $72$ 2D homogeneous coordinates and its output
    a weight vector. We design an encoder-decoder structure for DNNs, so as to map
    the 2D coordinates to a low dimensional space and to recover the parameters from
    that low dimensional representation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在DC中，我们实现了如[[48](#bib.bib48)]所述的前向函数层，结果DM和DC DNN如图[10](#S4.F10 "Figure 10 ‣
    4.3 3D shape Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep
    Learning Methods for Inverse Problems")所示，其中我们使用了前馈层，因为系统输入是向量化的$72$个2D齐次坐标，输出是一个权重向量。我们为DNN设计了一个编码器-解码器结构，以将2D坐标映射到低维空间，并从该低维表示中恢复参数。
- en: '![Refer to caption](img/5c2e2e518c209392a2f3cbec1a0599ee.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c2e2e518c209392a2f3cbec1a0599ee.png)'
- en: 'Figure 10: DNN structure for DM and DC for 3D shape inverse rendering.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：用于3D形状逆向渲染的DM和DC DNN结构。
- en: For the DR regularizer we trained a five layer MLP classifier to discriminate
    between a 3D face shape, generated by BFM, and randomly generated 3D point clouds
    as negative examples.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DR正则化器，我们训练了一个五层的MLP分类器，以区分由BFM生成的3D人脸形状和随机生成的3D点云作为负样本。
- en: Figure [11](#S4.F11 "Figure 11 ‣ 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems") shows
    visual results obtained by each solution category, where heat maps visualize the
    point-wise error magnitude relative to the ground truth. The visual results show
    that the DM and DC methods can capture the main features in the face (including
    eye, nose, mouth) better than the DR variants, however the differences between
    DM and DC seem to be negligible.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图[11](#S4.F11 "Figure 11 ‣ 4.3 3D shape Inverse Rendering (Reconstruction) ‣
    4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")显示了各解决方案类别获得的视觉结果，其中热图可视化了相对于真实值的逐点误差大小。视觉结果表明，DM和DC方法可以比DR变体更好地捕捉面部的主要特征（包括眼睛、鼻子、嘴巴），然而DM和DC之间的差异似乎微不足道。
- en: To validate our observations, the numerical results and respective statistical
    analyses are shown in Tables [5](#S4.T5 "Table 5 ‣ 4.3 3D shape Inverse Rendering
    (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse
    Problems") and [6](#S4.T6 "Table 6 ‣ 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems"). Table [5](#S4.T5
    "Table 5 ‣ 4.3 3D shape Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems") lists the RMSE values for each
    solution category. We used 10 out of sample faces in the BFM model as test cases
    for reporting the results. In the case of DR (Nelder-Mead) we set the start point,
    i.e., $z_{0}$, as a random value and report the averaged result over 10 independent
    runs. Note that the RMSE values are expected to be relatively large, since each
    3D face shape provided by BFM is a point cloud of $53490$ 3D coordinates in the
    range $[-8\times 10^{4},8\times 10^{4}]$. As a point of comparison, we computed
    the average RMSE between a set of 500 generated 3D faces and 1000 random generated
    faces, to have a sense of RMSE normalization to random prediction. The average
    RMSE for random prediction is $1.28\times 10^{4}$, a factor of two to four times
    larger than the RMSE values reported in Table [5](#S4.T5 "Table 5 ‣ 4.3 3D shape
    Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning Methods
    for Inverse Problems").
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的观察结果，数值结果及相应的统计分析显示在表[5](#S4.T5 "Table 5 ‣ 4.3 3D shape Inverse Rendering
    (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse
    Problems")和[6](#S4.T6 "Table 6 ‣ 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")中。表[5](#S4.T5
    "Table 5 ‣ 4.3 3D shape Inverse Rendering (Reconstruction) ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems")列出了每种解决方案类别的RMSE值。我们使用BFM模型中的10张样本面孔作为测试案例来报告结果。在DR（Nelder-Mead）情况下，我们将起始点，即$z_{0}$，设置为一个随机值，并报告10次独立运行的平均结果。需要注意的是，由于BFM提供的每个3D人脸形状都是$53490$个3D坐标点云，范围为$[-8\times
    10^{4},8\times 10^{4}]$，因此RMSE值预计会相对较大。作为对比，我们计算了一组500个生成的3D人脸和1000个随机生成的人脸之间的平均RMSE，以了解RMSE相对于随机预测的标准化。随机预测的平均RMSE为$1.28\times
    10^{4}$，是表[5](#S4.T5 "Table 5 ‣ 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems")中报告的RMSE值的两到四倍。
- en: '| ![Refer to caption](img/7a6fcdf60b1fdc16ba2bea1322788baf.png) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7a6fcdf60b1fdc16ba2bea1322788baf.png) |'
- en: '| ![Refer to caption](img/611221ac4be6729937fe5a899bce2648.png) |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: 'Figure 11: Qualitative Results for 3D inverse rendering. Each result is shown
    as two faces, an upper with the actual 3D result, and a lower as a heat map showing
    the error magnitude in each point of predicted face are shown in the form of heat
    map for each prediction. For the DR method, the average error magnitude over 20
    runs is reported. We use the Besel Face Model (BFM) [[48](#bib.bib48)] which is
    based on a 3D mean face and compensates for outliers.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="10.87" width="85.56" overflow="visible"><g transform="translate(0,10.87)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,6.07) scale(1,
    -1)"><foreignobject width="42.78" height="6.07" overflow="visible">Training Data</foreignobject></g></g>
    <g  transform="translate(62.11,6.07)"><g transform="translate(0,4.8) scale(1,
    -1)"><foreignobject width="23.45" height="4.8" overflow="visible">Method</foreignobject></g></g></g></svg>
    | Noisy Test Cases ($\times 10^{3}$) | Noisy + Outlier Test Cases ($\times 10^{3}$)
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Noise-free | $\mathbf{3.8\pm 2.0}$ | $4.2\pm 1.8$ | $3.9\pm 0.7$ | $4.2\pm
    2.2$ | $5.9\pm 2.4$ | $\mathbf{5.5\pm 2.2}$ | $5.7\pm 1.2$ | $5.8\pm 3.3$ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Noisy | $\mathbf{3.5\pm 1.6}$ | $4.2\pm 2.1$ | $3.9\pm 0.7$ | $4.2\pm 2.2$
    | $\mathbf{5.4\pm 3.5}$ | $5.7\pm 3.6$ | $5.7\pm 1.2$ | $5.8\pm 3.3$ |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| Noisy + Outlier | $\mathbf{3.3\pm 1.4}$ | $3.9\pm 1.8$ | $3.9\pm 0.7$ | $4.2\pm
    2.2$ | $\mathbf{5.4\pm 2.9}$ | $\mathbf{5.4\pm 3.0}$ | $5.7\pm 1.2$ | $5.8\pm
    3.3$ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Average test RMSE with standard deviation values (over 10 out-of-sample
    faces of the BFM [[48](#bib.bib48)]) for 3D shape inverse rendering.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="10.8" width="85.56" overflow="visible"><g transform="translate(0,10.8)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,6.07) scale(1,
    -1)"><foreignobject width="42.78" height="6.07" overflow="visible">Training Data</foreignobject></g></g>
    <g  transform="translate(55.45,6.07)"><g transform="translate(0,4.73) scale(1,
    -1)"><foreignobject width="30.11" height="4.73" overflow="visible">Test Data</foreignobject></g></g></g></svg>
    | Noisy | Noisy + Outlier |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| p-value | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| Noise-free | DM | - | 0.19 | 0.43 | 0.30 | - | 0.06 | 0.06 | 0.06 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| DC | 0.19 | - | 0.43 | 0.30 | 0.06 | - | 0.06 | 0.06 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| DR-GA | 0.43 | 0.43 | - | 0.78 | 0.06 | 0.06 | - | 0.78 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '|  | DR-NM | 0.30 | 0.30 | 0.78 | - | 0.06 | 0.06 | 0.78 | - |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| Noisy | DM | - | 0.19 | 0.19 | 0.30 | - | 0.06 | 0.06 | 0.06 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| DC | 0.19 | - | 0.30 | 0.30 | 0.06 | - | 0.12 | 0.30 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| DR-GA | 0.19 | 0.30 | - | 0.78 | 0.06 | 0.12 | - | 0.78 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '|  | DR-NM | 0.30 | 0.30 | 0.78 | - | 0.06 | 0.30 | 0.78 | - |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| Noisy + Outlier | DM | - | 0.06 | 0.06 | 0.06 | - | 1 | 0.06 | 0.06 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| DC | 0.06 | - | 0.06 | 0.06 | 1 | - | 0.06 | 0.06 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| DR-GA | 0.06 | 0.06 | - | 0.78 | 0.06 | 0.06 | - | 0.78 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| DR-GA | 0.06 | 0.06 | - | 0.78 | 0.06 | 0.06 | - | 0.78 |'
- en: '|  | DR-NM | 0.06 | 0.06 | 0.78 | - | 0.06 | 0.06 | 0.78 | - |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | DR-NM | 0.06 | 0.06 | 0.78 | - | 0.06 | 0.06 | 0.78 | - |'
- en: 'Table 6: Wilcoxon signed rank test $p$ values for the 3D shape inverse rendering
    problem.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：3D形状逆渲染问题的Wilcoxon符号秩检验$p$值。
- en: Table [6](#S4.T6 "Table 6 ‣ 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems") shows
    the results of the Wilcoxon $p$ values for statistical significance in the difference
    between reported values in Table [5](#S4.T5 "Table 5 ‣ 4.3 3D shape Inverse Rendering
    (Reconstruction) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse
    Problems"), where we consider a $p$ value threshold of $0.07$.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S4.T6 "Table 6 ‣ 4.3 3D shape Inverse Rendering (Reconstruction) ‣ 4
    Experiments ‣ Survey of Deep Learning Methods for Inverse Problems") 显示了Wilcoxon
    $p$ 值的结果，用于统计显著性检测，比较表 [5](#S4.T5 "Table 5 ‣ 4.3 3D shape Inverse Rendering (Reconstruction)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems") 中报告的值，我们考虑了$p$
    值的阈值为 $0.07$。
- en: 'Based on the preceding numerical results and statistical analysis, we claim
    the following about each solution category facing with Reconstruction inverse
    problems:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前述的数值结果和统计分析，我们对每种解决方案类别在面对重建逆问题时提出以下结论：
- en: •
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Broadly, for training and test data not involving outliers, the overall performance
    of the methods is similar, with DM outperforming. This observation shows that
    the learning phase is not crucial in the presence of noise, and methods which
    concentrate on the test data can achieve equal performance compared to trainable
    frameworks.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总体来说，对于不涉及离群点的训练和测试数据，这些方法的整体表现相似，其中DM表现优于其他方法。这一观察结果表明，在存在噪声的情况下，学习阶段并不是关键因素，而那些集中于测试数据的方法能够与可训练框架相比达到相同的性能。
- en: •
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In cases involving outliers the performance of the methods is more distinct,
    but with the DM and DC methods, having a learning phase for optimizing their main
    objective term, outperforming the DR variants. We conclude that a learning phase
    is important to make methods robust to outliers.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在涉及离群点的情况下，这些方法的性能更为明显，但DM和DC方法在优化其主要目标项的学习阶段表现优于DR变体。我们得出结论，学习阶段对于使方法对离群点具有鲁棒性是重要的。
- en: •
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the case of DR, the results show similar performance of the GA and NM optimization
    schemes, with GA outperforming NM. This observation encourages the reader to use
    optimization methods with more exploration power [[53](#bib.bib53)], the ability
    of an optimization method to search broadly across the whole solution space, for
    DR solutions to reconstruction problems.
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在DR的情况下，结果显示GA和NM优化方案的性能相似，GA优于NM。这一观察结果鼓励读者使用具有更多探索能力的优化方法[[53](#bib.bib53)]，即具有在整个解空间中广泛搜索能力的优化方法，来解决DR的重建问题。
- en: •
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In all cases, we can observe that although DC is unsupervised, its performance
    when solving reconstruction inverse problems is near to that of DM, even outperforming
    DM in the case of outliers. Therefore, it is possible to solve reconstruction
    problems even without label information in the training phase.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在所有情况下，我们可以观察到，尽管DC是无监督的，但在解决重建逆问题时，其表现接近于DM，甚至在涉及离群点的情况下超越了DM。因此，即使在训练阶段没有标签信息，也有可能解决重建问题。
- en: •
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: One interesting observation is that while 3D shape inverse rendering is a complex
    reconstruction problem, the results for each solution category are qualitatively
    similar to the very different and far simpler inverse problem of linear regression,
    where DC similarly outperformed training data containing noisy and outlier samples.
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个有趣的观察是，尽管3D形状逆渲染是一个复杂的重建问题，但每种解决方案类别的结果在定性上类似于线性回归这一非常不同且远比其简单的逆问题，其中DC同样在包含噪声和离群样本的训练数据中表现优于其他方法。
- en: 4.4 Single Object Tracking (Dynamic Estimation)
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 单目标跟踪（动态估计）
- en: Up to this point we have investigated deep learning approaches applied to static
    problems. We would now like to examine a dynamic inverse problem, that of single-object
    tracking.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已研究了应用于静态问题的深度学习方法。现在，我们希望探讨一个动态逆问题，即单目标跟踪。
- en: The classical approach for tracking is the Kalman Filter (KF) [[2](#bib.bib2)]
    and its many variations, all based on a predictor-corrector framework, meaning
    that the filter alternates between prediction (asserting the time-dynamics) and
    correcting (asserting information based on the measurements). For the inverse
    problem under study, we consider the current location estimation (filtering) in
    a two dimensional environment. Synthetic object tracking problems, as considered
    here, are studied in a variety of object tracking papers [[54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)], where the specific tracking problem in this
    section is inspired from the approach of [[39](#bib.bib39), [58](#bib.bib58)]
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的跟踪方法是卡尔曼滤波器（KF）[[2](#bib.bib2)]及其多种变体，所有这些方法都基于预测-校正框架，即滤波器在预测（断言时间动态）和校正（基于测量的断言信息）之间交替进行。对于正在研究的逆问题，我们考虑在二维环境中对当前位置的估计（滤波）。在这里考虑的合成对象跟踪问题在各种对象跟踪论文中有研究[[54](#bib.bib54),
    [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57)]，本节中的具体跟踪问题受到[[39](#bib.bib39),
    [58](#bib.bib58)]的方法启发。
- en: The inverse problem task is to estimate the current ball location, given the
    noisy measurement in the corresponding time step and the previous state of the
    ball. Formally, we denote the measured ball location by $\underline{m}^{t}$, and
    the system state, the current location of the ball, as $\underline{z}^{t}$. The
    graphical model in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Single Object Tracking
    (Dynamic Estimation) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse
    Problems") illustrates the problem definition of the tracking problem, where the
    objective of the inverse problem is to address the dashed line, the inference
    of system state from corresponding measurement.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 逆问题任务是估计当前球的位置，给定相应时间步的噪声测量和球的先前状态。形式上，我们用$\underline{m}^{t}$表示测量的球位置，用系统状态，即球的当前位置，表示为$\underline{z}^{t}$。图[12](#S4.F12
    "图 12 ‣ 4.4 单对象跟踪（动态估计） ‣ 4 实验 ‣ 逆问题深度学习方法调查")中的图形模型说明了跟踪问题的定义，其中逆问题的目标是解决虚线，即从相应测量中推断系统状态。
- en: '![Refer to caption](img/4d82f2f6afb149cd0fd6b671dd72d211.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d82f2f6afb149cd0fd6b671dd72d211.png)'
- en: 'Figure 12: Graphical model for single object tracking: the goal is to estimate
    the location of a moving ball in the current frame in a bounded 2D environment.
    $\underline{m}^{t}$ denotes the current measured location and $\underline{z}^{t}$
    is the current state.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：单对象跟踪的图形模型：目标是估计在有界2D环境中的当前帧中移动球的位置。$\underline{m}^{t}$表示当前测量的位置，$\underline{z}^{t}$是当前状态。
- en: To perform the experiments, we generate the training and test sets similar to
    [[39](#bib.bib39)] except that we assume that our measurements are received from
    a detection algorithm, which detects the ball location from input images having
    a size of $32\times 32$ pixels, and that the movement of the ball is non-linear.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行实验，我们生成了与[[39](#bib.bib39)]类似的训练和测试集，只不过我们假设我们的测量是通过检测算法获得的，该算法从大小为$32\times
    32$像素的输入图像中检测球的位置，并且球的运动是非线性的。
- en: In each training and test sequence the ball starts from a random location in
    the 2D environment, with a random speed and direction, and then moving for 30
    time steps. The dynamic of the generated data includes changing the ball location
    $\underline{z}^{t}$ and its velocity $\underline{v}^{t}$ as
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练和测试序列中，球从2D环境中的随机位置开始，具有随机速度和方向，然后移动30个时间步。生成的数据动态包括改变球的位置$\underline{z}^{t}$及其速度$\underline{v}^{t}$，如下所示：
- en: '|  | $\underline{z}^{t}=\underline{v}^{(t-1)}\Delta t+\underline{z}^{(t-1)}$
    |  | (32) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{z}^{t}=\underline{v}^{(t-1)}\Delta t+\underline{z}^{(t-1)}$
    |  | (32) |'
- en: '|  | $\underline{v}^{t}=\underline{v}^{(t-1)}-(c(\underline{v}^{(t-1)})^{2}\text{sign}(\underline{v}^{t}))$
    |  | (33) |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{v}^{t}=\underline{v}^{(t-1)}-(c(\underline{v}^{(t-1)})^{2}\text{sign}(\underline{v}^{t}))$
    |  | (33) |'
- en: where $c$ is a constant and is set to $0.001$. In our data, collisions with
    walls are fully elastic and the velocity decreases exponentially over time. In
    this simulation, the training and testing data-sets contain 10000 and 3000 sequences
    of 30-time steps, respectively.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$c$是一个常数，设定为$0.001$。在我们的数据中，与墙壁的碰撞是完全弹性的，速度随着时间呈指数下降。在此模拟中，训练和测试数据集分别包含10000个和3000个30时间步的序列。
- en: The training measurement noise is
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 训练测量噪声为
- en: '|  | $\displaystyle\underline{\nu}\sim 0.95N(0,0.2I)+0.05N(0,10I),$ |  | (34)
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underline{\nu}\sim 0.95N(0,0.2I)+0.05N(0,10I),$ |  | (34)
    |'
- en: a mixture model of Gaussian noise with 5% outliers. The testing noise is similar,
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯噪声混合模型中包含5%的离群值。测试噪声类似，
- en: '|  | $\displaystyle\underline{\nu}\sim 0.85N(0,0.4I)+0.15N(0,10I)$ |  | (35)
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underline{\nu}\sim 0.85N(0,0.4I)+0.15N(0,10I)$ |  | (35)
    |'
- en: with a higher likelihood of outliers.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 离群值的可能性更高。
- en: The inverse problem is single-target tracking for which the dynamic of the model
    is unknown. The inverse problem of interest is to find $\underline{z}^{t}$ in
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 逆问题是单目标跟踪，其模型的动态未知。感兴趣的逆问题是找到$\underline{z}^{t}$，
- en: '|  | $\underline{z}^{t}=G(\underline{z}^{(t-1)},m_{t})$ |  | (36) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{z}^{t}=G(\underline{z}^{(t-1)},m_{t})$ |  | (36) |'
- en: As shown in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Single Object Tracking (Dynamic
    Estimation) ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems"),
    we can model our problem as a first order Markov model where the current measurement
    is independent of others given the current system state. The forward model is
    then defined as
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [12](#S4.F12 "Figure 12 ‣ 4.4 Single Object Tracking (Dynamic Estimation)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems") 所示，我们可以将问题建模为一阶马尔可夫模型，其中当前测量在给定当前系统状态的情况下与其他测量独立。然后，前向模型定义为
- en: '|  | $\underline{m}^{t}=F(\underline{z}^{t})=C\underline{z}^{t}+\underline{\nu},\quad
    C=I,\quad\underline{\nu}\sim N(0,\sigma)$ |  | (37) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underline{m}^{t}=F(\underline{z}^{t})=C\underline{z}^{t}+\underline{\nu},\quad
    C=I,\quad\underline{\nu}\sim N(0,\sigma)$ |  | (37) |'
- en: We can model Markov models using Recurrent Neural Networks (RNN) [[59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)]. The DNN structure for DM
    and DC solution categories is shown in Figure [13](#S4.F13 "Figure 13 ‣ 4.4 Single
    Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems"), in which the LSTM layers lead the learning process
    to capture the time state and dynamic information in the data sequences.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用递归神经网络（RNN）[[59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)]来建模马尔可夫模型。图 [13](#S4.F13
    "Figure 13 ‣ 4.4 Single Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣
    Survey of Deep Learning Methods for Inverse Problems") 展示了DM和DC解决方案类别的DNN结构，其中LSTM层引导学习过程以捕捉数据序列中的时间状态和动态信息。
- en: '![Refer to caption](img/01c96bd69e113111878506a153f57481.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/01c96bd69e113111878506a153f57481.png)'
- en: 'Figure 13: DNN structure for DM and DC solution categories in the case of single
    object tracking problem.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：单目标跟踪问题中DM和DC解决方案类别的DNN结构。
- en: We design the regularizer of the DR category as a classifier to classify location
    feasibility — those locations lying within the border of the 2D environment. Figure [14](#S4.F14
    "Figure 14 ‣ 4.4 Single Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣
    Survey of Deep Learning Methods for Inverse Problems") shows the positive and
    negative samples which we used to train the DR regularizer.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将DR类别的正则化器设计为分类器，用于分类位置可行性——那些位于2D环境边界内的位置。图 [14](#S4.F14 "Figure 14 ‣ 4.4
    Single Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems") 显示了我们用来训练DR正则化器的正负样本。
- en: '![Refer to caption](img/1d3af5df986012f2b3b179de48d17000.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d3af5df986012f2b3b179de48d17000.png)'
- en: 'Figure 14: The positive and negative samples used for training the DR regularizer,
    where the black and gray samples are in the positive and negative classes, respectively.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：用于训练DR正则化器的正负样本，其中黑色和灰色样本分别属于正类和负类。
- en: As before, we used GA (DR-GA) and Nelder-Mead (DR-NM) algorithms as optimizers
    for DR. In the case of using Nelder-Mead, the results vary as a function of starting
    point $\underline{z}_{0}$, and found that using the last sequence measurement
    as the starting point empirically gave the best result for DR-NM.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用了GA（DR-GA）和Nelder-Mead（DR-NM）算法作为DR的优化器。在使用Nelder-Mead的情况下，结果随着起始点$\underline{z}_{0}$的不同而有所变化，发现经验上使用最后一个序列测量作为起始点可以得到DR-NM的最佳结果。
- en: 4.4.1 Visual and Numerical Results and Statistical Analysis
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 视觉和数值结果及统计分析
- en: Table [7](#S4.T7 "Table 7 ‣ 4.4.1 Visual and Numerical Results and Statistical
    Analysis ‣ 4.4 Single Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems") includes the numerical results
    obtained by each method in our experiments, where we report the average RMSE between
    reference and predicted points on the test trajectory as the evaluation criterion
    for each method.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [7](#S4.T7 "Table 7 ‣ 4.4.1 Visual and Numerical Results and Statistical
    Analysis ‣ 4.4 Single Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣ Survey
    of Deep Learning Methods for Inverse Problems") 包含了我们实验中每种方法获得的数值结果，我们报告了测试轨迹中参考点和预测点之间的平均RMSE，作为每种方法的评估标准。
- en: '| Training Data |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据 |'
- en: '&#124; Noise-free &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无噪声 &#124;'
- en: '|'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Noisy &#124;'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含噪声 &#124;'
- en: '|'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Noisy + Outlier &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含噪声 + 异常值 &#124;'
- en: '|'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Method | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA
    | DR-NM |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA
    | DR-NM |'
- en: '| RMSE |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| RMSE |'
- en: '&#124; $\mathbf{1.70}$ &#124;'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{1.70}$ &#124;'
- en: '&#124; $\mathbf{\pm 0.05}$ &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{\pm 0.05}$ &#124;'
- en: '|'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $1.79$ &#124;'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $1.79$ &#124;'
- en: '&#124; $\pm 0.21$ &#124;'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.21$ &#124;'
- en: '|'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $2.05$ &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $2.05$ &#124;'
- en: '&#124; $\pm 0.00$ &#124;'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.00$ &#124;'
- en: '|'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $1.85$ &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $1.85$ &#124;'
- en: '&#124; $\pm 0.00$ &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.00$ &#124;'
- en: '|'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathbf{1.72}$ &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{1.72}$ &#124;'
- en: '&#124; $\mathbf{\pm 0.08}$ &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{\pm 0.08}$ &#124;'
- en: '|'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $2.04$ &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $2.04$ &#124;'
- en: '&#124; $\pm 0.28$ &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.28$ &#124;'
- en: '|'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $2.05$ &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $2.05$ &#124;'
- en: '&#124; $\pm 0.00$ &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.00$ &#124;'
- en: '|'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $1.85$ &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $1.85$ &#124;'
- en: '&#124; $\pm 0.00$ &#124;'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.00$ &#124;'
- en: '|'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathbf{0.39}$ &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{0.39}$ &#124;'
- en: '&#124; $\mathbf{\pm 0.03}$ &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{\pm 0.03}$ &#124;'
- en: '|'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $1.94$ &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $1.94$ &#124;'
- en: '&#124; $\pm 0.02$ &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.02$ &#124;'
- en: '|'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $2.05$ &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $2.05$ &#124;'
- en: '&#124; $\pm 0.00$ &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.00$ &#124;'
- en: '|'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $1.85$ &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $1.85$ &#124;'
- en: '&#124; $\pm 0.00$ &#124;'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\pm 0.00$ &#124;'
- en: '|'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 7: RMSE obtained by deep learning solution categories for tracking. The
    test data include both noise and outliers.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：深度学习解决方案类别在跟踪中的 RMSE。测试数据包括噪声和异常值。
- en: '| p-value (Wilcoxon Test) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| p 值（Wilcoxon 检验） |'
- en: '&#124; Training Data: &#124;'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练数据: &#124;'
- en: '&#124; Noise-free &#124;'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无噪声 &#124;'
- en: '|'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training Data: &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练数据: &#124;'
- en: '&#124; Noisy &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含噪声 &#124;'
- en: '|'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training Data: &#124;'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练数据: &#124;'
- en: '&#124; Noisy + Outlier &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含噪声 + 异常值 &#124;'
- en: '|'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM
    |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM | DM | DC | DR-GA | DR-NM
    |'
- en: '| DM | - | 0.160 | 0.002 | 0.002 | - | 0.002 | 0.002 | 0.002 | - | 0.002 |
    0.002 | 0.002 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| DM | - | 0.160 | 0.002 | 0.002 | - | 0.002 | 0.002 | 0.002 | - | 0.002 |
    0.002 | 0.002 |'
- en: '| DC | 0.160 | - | 0.013 | 0.130 | 0.002 | - | 0.322 | 0.027 | 0.002 | - |
    0.002 | 0.002 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| DC | 0.160 | - | 0.013 | 0.130 | 0.002 | - | 0.322 | 0.027 | 0.002 | - |
    0.002 | 0.002 |'
- en: '| DR-GA | 0.002 | 0.013 | - | 0.002 | 0.002 | 0.322 | - | 0.002 | 0.002 | 0.002
    | - | 0.002 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| DR-GA | 0.002 | 0.013 | - | 0.002 | 0.002 | 0.322 | - | 0.002 | 0.002 | 0.002
    | - | 0.002 |'
- en: '| DR-NM | 0.002 | 0.130 | 0.002 | - | 0.027 | 0.002 | 0.002 | - | 0.002 | 0.002
    | 0.002 | - |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| DR-NM | 0.002 | 0.130 | 0.002 | - | 0.027 | 0.002 | 0.002 | - | 0.002 | 0.002
    | 0.002 | - |'
- en: 'Table 8: Pairwise p values for tracking: the Wilcoxon signed rank test checks
    whether the obtained results are significantly different.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：跟踪的成对 p 值：Wilcoxon 符号秩检验检查获得的结果是否显著不同。
- en: The obtained results and their statistical analysis are shown in Tables [7](#S4.T7
    "Table 7 ‣ 4.4.1 Visual and Numerical Results and Statistical Analysis ‣ 4.4 Single
    Object Tracking (Dynamic Estimation) ‣ 4 Experiments ‣ Survey of Deep Learning
    Methods for Inverse Problems") and [8](#S4.T8 "Table 8 ‣ 4.4.1 Visual and Numerical
    Results and Statistical Analysis ‣ 4.4 Single Object Tracking (Dynamic Estimation)
    ‣ 4 Experiments ‣ Survey of Deep Learning Methods for Inverse Problems"), based
    on which we conclude that
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 所获得的结果及其统计分析显示在表[7](#S4.T7 "表 7 ‣ 4.4.1 可视化和数值结果及统计分析 ‣ 4.4 单目标跟踪（动态估计） ‣ 4
    实验 ‣ 逆问题的深度学习方法调查")和表[8](#S4.T8 "表 8 ‣ 4.4.1 可视化和数值结果及统计分析 ‣ 4.4 单目标跟踪（动态估计） ‣
    4 实验 ‣ 逆问题的深度学习方法调查")中，根据这些结果我们得出结论：
- en: •
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the case of single object tracking, for which system parameters are permitted
    to evolve and be measured over time [[2](#bib.bib2)], the DM category achieves
    the best performance using all types of training data. The results are improved
    when the training data contain representative noise and outliers.
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在单目标跟踪的情况下，其中系统参数可以随时间演变和测量[[2](#bib.bib2)]，DM 类别使用所有类型的训练数据实现了最佳性能。当训练数据包含具有代表性的噪声和异常值时，结果会有所改善。
- en: •
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When the training does not include outliers, the DR-NM category achieves the
    second rank after DM; note that DR-NM is an unsupervised framework without a learning
    phase, showing that a learning phase is not necessarily required, and that looking
    only into test cases can give reasonable results.
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当训练数据不包含异常值时，DR-NM 类别在 DM 之后排名第二；注意 DR-NM 是一个没有学习阶段的无监督框架，这表明学习阶段并不一定是必需的，且仅仅查看测试案例也可以得到合理的结果。
- en: •
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When the training data include noisy and outlier samples, the solutions’ behaviour
    for single object tracking is similar to that of restoration problems. In particular,
    in single object tracking the measurements and system parameters are in the same
    space, like restoration problems.
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of DR solution category for dynamic estimation problems, it is observable
    that, unlike reconstruction problems, the NM optimization scheme performs better
    than the GA approach, emphasizing the importance of exploitation power [[53](#bib.bib53),
    [63](#bib.bib63)], referring to the ability of an optimization method to concentrate
    on a specific region of the solution space.
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Discussion
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the statistical analyses adopted for robustness evaluation for each
    case, Table [9](#S5.T9 "Table 9 ‣ 5 Discussion ‣ Survey of Deep Learning Methods
    for Inverse Problems") summarizes the overall findings, for linear regression
    and 3D shape inverse rendering as reconstruction, image denoising as restoration,
    and single object tracking as dynamic estimation.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '| Inverse Problem | Problem Type | Training Data | Test Data | Score (Larger
    is better) |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| Linear Regression | Reconstruction | Noise-free | Noisy + Outlier | DC >
    (DR-GA=DR-NM) > DM |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| Noisy + Outlier | Noisy + Outlier | (DM = DC) > (DR-GA=DR-NM) |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| 3D Shape Inverse Rendering | Reconstruction | Noise-free | Noisy | DM = DC
    = DR-GA = DR-NM |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| Noise-free | Noisy + Outlier | DC > (DR-GA=DR-NM) > DM |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| Noisy | Noisy | DM = DC = DR-GA = DR-NM |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| Noisy | Noisy + Outlier | DM > (DC = DR-GA = DR-NM) |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| Noisy + Outlier | Noisy | DM > DC > (DR-GA = DR-NM) |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| Noisy + Outlier | Noisy+ Outlier | (DM = DC) > (DR-GA=DR-NM) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Denoising &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '| Restoration | Noisy + Outlier | Noisy+ Outlier | DM > DC > (DR-GA = DR-NM)
    |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
- en: '| Single Object Tracking | Dynamic Estimation | Noise-free | Noisy + Outlier
    | (DM = DC) > DR-NM > DR-GA |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| Noisy | Noisy + Outlier | DM > DR-NM > (DC = DR-GA) |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| Noisy + Outlier | Noisy + Outlier | DM > DC > DR-NM > DR-GA |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Performance comparison by solution category and inverse problem types.
    Note that $a>b$ means that method $a$ is statistically significantly better than
    method $b$.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'From Table [9](#S5.T9 "Table 9 ‣ 5 Discussion ‣ Survey of Deep Learning Methods
    for Inverse Problems") we conclude the following:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of reconstruction inverse problems, the presence of outliers in
    the training phase leads to distinct differences in robustness. Typically, DM
    will be the best method when the training data include outliers, and DC will outperform
    other methods based on having a data consistency term in its objective.
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In reconstruction problems, comparing GA and NM optimization approaches in DR
    shows that GA achieves better performance indicating the importance of exploration
    power in optimization for this class of problems.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The restoration inverse problems, which recover the system parameters from some
    measurements from the same space, need label information (as in DM) to be robust
    against noise and outliers.
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恢复逆问题，即从同一空间的某些测量中恢复系统参数，需要标签信息（如 DM 中所示）以提高对噪声和异常值的鲁棒性。
- en: •
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the case of restoration problems in static estimation, DM has the highest
    rank among tested methods. We believe this is because, in the process of finding
    a mapping from one space to itself, the exploitation of accurate solution matters
    and this property is achieved using label information in the process of training
    the framework.
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在静态估计的恢复问题中，DM 在测试方法中排名最高。我们认为这是因为，在从一个空间映射到自身的过程中，准确解决方案的利用至关重要，而这一属性在训练框架过程中通过标签信息得以实现。
- en: •
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the case of dynamic estimation problems, the DR solution performs well when
    the training data do not include outlier samples. Therefore we conclude that this
    class of problems could be solved without needing a learning phase and that solely
    the test case is sufficient to find a robust solution.
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于动态估计问题，当训练数据中不包含异常值样本时，DR 解决方案表现良好。因此我们得出结论，这类问题可以在没有学习阶段的情况下解决，仅测试案例就足以找到鲁棒的解决方案。
- en: •
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The dynamic estimation problems have additional challenges stemming from the
    time-dependent state information to be captured, an attribute which leads the
    solution to have different behavior from other problem types. We observed that
    there are similarities, based on the measurement and system parameter spaces,
    between the robustness power of the solution categories’ performance in a dynamic
    estimation problem and a static estimation problems with the same measurement
    and system parameter spaces.
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态估计问题面临来自时间依赖状态信息的额外挑战，这种属性使得解决方案与其他问题类型具有不同的行为。我们观察到，在具有相同测量和系统参数空间的动态估计问题和静态估计问题中，解决方案类别的鲁棒性表现存在相似性。
- en: 6 Conclusions
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper investigated deep learning strategies to explicitly solve inverse
    problems. The literature on deep learning methods for solving inverse problems
    was classified into three categories, each of which was evaluated on sample inverse
    problems of different types. Our focus is on the robustness of different categories,
    particularly with respect to their handling of noise and outliers. The results
    show that each solution category has different behaviours, in the sense of strengths
    and weaknesses with regards to problem assumptions, such that the problem characteristics
    need to be considered in selecting an appropriate solution mechanism for a given
    inverse problem.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了深度学习策略以显式解决逆问题。关于解决逆问题的深度学习方法的文献被分为三类，每一类都在不同类型的样本逆问题上进行了评估。我们的重点是不同类别的鲁棒性，特别是它们对噪声和异常值的处理能力。结果表明，每种解决方案类别在问题假设方面具有不同的优缺点，因此在选择适当的解决机制时需要考虑问题特征。
- en: Typically, reconstruction problems need more exploration power and the existence
    of outliers in their training data makes the DM category the most robust among
    deep learning solution categories. Otherwise, when the training data do not include
    outliers for reconstruction problems, DC achieves the best performance, although
    not using label information in their training phase. The restoration problems
    need a greater degree of exploitation power for which the DM methods are best
    suited. In the case of dynamic estimation problems, when the training data do
    not include outliers, DR achieves second rank, indicating that dynamic estimation
    problems can be solved with reasonable robustness without a need for learning
    in the presence of noise.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，重建问题需要更多的探索能力，而训练数据中的异常值使得 DM 类别在深度学习解决方案类别中最为鲁棒。否则，当训练数据中不包含异常值时，DC 达到最佳性能，尽管在训练阶段不使用标签信息。恢复问题需要更大的探索能力，而
    DM 方法最为适用。在动态估计问题的情况下，当训练数据中不包含异常值时，DR 达到第二名，这表明动态估计问题可以在存在噪声的情况下以合理的鲁棒性解决而无需学习。
- en: References
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Bertero M and Boccacci P 1998 Introduction to inverse problems in imaging
    (CRC press)'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Bertero M 和 Boccacci P 1998 《图像中的逆问题简介》（CRC press）'
- en: '[2] Fieguth P 2010 Statistical image processing and multidimensional modeling
    (Springer Science & Business Media)'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Fieguth P 2010 《统计图像处理与多维建模》（Springer Science & Business Media）'
- en: '[3] Stuart A M 2010 Inverse problems: a bayesian perspective Acta numerica  19
    451–559'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Lucas A, Iliadis M, Molina R and Katsaggelos A K 2018 Using deep neural
    networks for inverse problems in imaging: beyond analytical methods IEEE Signal
    Processing Magazine  35 20–36'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Arridge S, Maass P, Öktem O and Schönlieb C B 2019 Solving inverse problems
    using data-driven models Acta Numerica  28 1–174'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Groetsch C 1984 The theory of tikhonov regularization for fredholm equations
    104p, Boston Pitman Publication'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Makovetskii A, Voronin S and Kober V 2015 Explicit solutions of one-dimensional
    total variation problem Applications of Digital Image Processing XXXVIII vol 9599
    (International Society for Optics and Photonics) p 959926'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Balas V E, Roy S S, Sharma D and Samui P 2019 Handbook of deep learning
    applications vol 136 (Springer)'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Samek W, Wiegand T and Müller K R 2017 Explainable artificial intelligence:
    Understanding, visualizing and interpreting deep learning models arXiv preprint
    arXiv:1708.08296'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Kim H, Zollhöfer M, Tewari A, Thies J, Richardt C and Theobalt C 2018
    Inversefacenet: Deep monocular inverse face rendering Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition pp 4625–4634'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Canziani A, Paszke A and Culurciello E 2016 An analysis of deep neural
    network models for practical applications arXiv preprint arXiv:1605.07678'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Khan M, Lunnikivi H, Huttunen H and Boutellier J 2019 Comparing optimization
    methods of neural networks for real-time inference 2019 27th European Signal Processing
    Conference (EUSIPCO) (IEEE) pp 1–5'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Han B, Yao Q, Yu X, Niu G, Xu M, Hu W, Tsang I and Sugiyama M 2018 Co-teaching:
    Robust training of deep neural networks with extremely noisy labels Advances in
    Neural Information Processing Systems pp 8527–8537'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Aggarwal H K, Mani M P and Jacob M 2018 Modl: Model-based deep learning
    architecture for inverse problems IEEE Transactions on Medical Imaging  38 394–405'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Anirudh R, Thiagarajan J J, Kailkhura B and Bremer T 2018 An unsupervised
    approach to solving inverse problems using generative adversarial networks arXiv
    preprint arXiv:1805.07281'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Chang, JH Rick, Chun-Liang, Li, Barnabas, Poczos, BVK, Vijaya Kumar and
    Aswin, C Sankaranarayanan 2017 One network to solve them all–solving linear inverse
    problems using deep projection models Proceedings of the IEEE International Conference
    on Computer Vision pp 5888–5897'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Adler J and Öktem O 2017 Solving ill-posed inverse problems using iterative
    deep neural networks Inverse Problems  33 124007'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Antholzer S, Haltmeier M and Schwab J 2019 Deep learning for photoacoustic
    tomography from sparse data Inverse Problems in Science and Engineering  27 987–1005'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Jin K H, McCann M T, Froustey E and Unser M 2017 Deep convolutional neural
    network for inverse problems in imaging IEEE Transactions on Image Processing  26
    4509–4522'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Kelly B, Matthews T P and Anastasio M A 2017 Deep learning-guided image
    reconstruction from incomplete data arXiv preprint arXiv:1709.00584'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Kelly B, Matthews T P 和 Anastasio M A 2017 深度学习引导的图像重建从不完整数据中 arXiv 预印本
    arXiv:1709.00584'
- en: '[21] Zhang J and Ghanem B 2018 Ista-net: Interpretable optimization-inspired
    deep network for image compressive sensing Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition pp 1828–1837'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Zhang J 和 Ghanem B 2018 Ista-net: 可解释的优化启发式深度网络用于图像压缩感知 Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition pp 1828–1837'
- en: '[22] Fan K, Wei Q, Wang W, Chakraborty A and Heller K 2017 Inversenet: Solving
    inverse problems with splitting networks arXiv preprint arXiv:1712.00202'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Fan K, Wei Q, Wang W, Chakraborty A 和 Heller K 2017 Inversenet: 使用分裂网络解决逆问题
    arXiv 预印本 arXiv:1712.00202'
- en: '[23] Dosovitskiy A, Fischer P, Ilg E, Hausser P, Hazirbas C, Golkov V, Van
    Der Smagt P, Cremers D and Brox T 2015 Flownet: Learning optical flow with convolutional
    networks Proceedings of the IEEE International Conference on Computer Vision pp
    2758–2766'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Dosovitskiy A, Fischer P, Ilg E, Hausser P, Hazirbas C, Golkov V, Van
    Der Smagt P, Cremers D 和 Brox T 2015 Flownet: 使用卷积网络学习光流 Proceedings of the IEEE
    International Conference on Computer Vision pp 2758–2766'
- en: '[24] Wang Z, Liu D, Yang J, Han W and Huang T 2015 Deep networks for image
    super-resolution with sparse prior Proceedings of the IEEE International Conference
    on Computer Vision pp 370–378'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Wang Z, Liu D, Yang J, Han W 和 Huang T 2015 使用稀疏先验的深度网络图像超分辨率 Proceedings
    of the IEEE International Conference on Computer Vision pp 370–378'
- en: '[25] Xu L, Ren J S, Liu C and Jia J 2014 Deep convolutional neural network
    for image deconvolution Advances in neural information processing systems pp 1790–1798'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Xu L, Ren J S, Liu C 和 Jia J 2014 用于图像解卷积的深度卷积神经网络 Advances in neural
    information processing systems pp 1790–1798'
- en: '[26] Schuler C J, Hirsch M, Harmeling S and Schölkopf B 2015 Learning to deblur
    IEEE t Transactions on Pattern Analysis and Machine Intelligence  38 1439–1451'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Schuler C J, Hirsch M, Harmeling S 和 Schölkopf B 2015 学习去模糊 IEEE t Transactions
    on Pattern Analysis and Machine Intelligence 38 1439–1451'
- en: '[27] Engl H W, Hanke M and Neubauer A 1996 Regularization of inverse problems
    vol 375 (Springer Science & Business Media)'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Engl H W, Hanke M 和 Neubauer A 1996 逆问题的正则化 vol 375 (Springer Science
    & Business Media)'
- en: '[28] Mousavi A and Baraniuk R G 2017 Learning to invert: Signal recovery via
    deep convolutional networks 2017 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP) pp 2272–2276'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Mousavi A 和 Baraniuk R G 2017 学习反演：通过深度卷积网络进行信号恢复 2017 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP) pp 2272–2276'
- en: '[29] De los Reyes J C, Schönlieb C B and Valkonen T 2016 The structure of optimal
    parameters for image restoration problems Journal of Mathematical Analysis and
    Applications  434 464–500'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] De los Reyes J C, Schönlieb C B 和 Valkonen T 2016 图像恢复问题的最优参数结构 Journal
    of Mathematical Analysis and Applications 434 464–500'
- en: '[30] McCann M T and Unser M 2019 Algorithms for biomedical image reconstruction
    arXiv preprint arXiv:1901.03565'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] McCann M T 和 Unser M 2019 生物医学图像重建算法 arXiv 预印本 arXiv:1901.03565'
- en: '[31] Häggström I, Schmidtlein C R, Campanella G and Fuchs T J 2019 Deeppet:
    A deep encoder–decoder network for directly solving the pet image reconstruction
    inverse problem Medical Image Analysis  54 253–262'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Häggström I, Schmidtlein C R, Campanella G 和 Fuchs T J 2019 Deeppet: 一种深度编码器-解码器网络用于直接解决
    PET 图像重建逆问题 Medical Image Analysis 54 253–262'
- en: '[32] Chen G, Li T, Chen Q, Ren S, Wang C and Li S 2019 Application of deep
    learning neural network to identify collision load conditions based on permanent
    plastic deformation of shell structures Computational Mechanics  64 435–449'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Chen G, Li T, Chen Q, Ren S, Wang C 和 Li S 2019 深度学习神经网络在基于壳体结构的永久塑性变形识别碰撞载荷条件中的应用
    Computational Mechanics 64 435–449'
- en: '[33] Dittmer S, Kluth T, Maass P and Baguer D O 2019 Regularization by architecture:
    A deep prior approach for inverse problems Journal of Mathematical Imaging and
    Vision 1–15'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Dittmer S, Kluth T, Maass P 和 Baguer D O 2019 通过架构正则化：一种用于逆问题的深度先验方法 Journal
    of Mathematical Imaging and Vision 1–15'
- en: '[34] Li H, Schwab J, Antholzer S and Haltmeier M 2018 Nett: Solving inverse
    problems with deep neural networks arXiv preprint arXiv:1803.00092'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Li H, Schwab J, Antholzer S 和 Haltmeier M 2018 Nett: 使用深度神经网络解决逆问题 arXiv
    预印本 arXiv:1803.00092'
- en: '[35] Senouf O, Vedula S, Weiss T, Bronstein A, Michailovich O and Zibulevsky
    M 2019 Self-supervised learning of inverse problem solvers in medical imaging
    Domain Adaptation and Representation Transfer and Medical Image Learning with
    Less Labels and Imperfect Data (Springer) pp 111–119'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Senouf O, Vedula S, Weiss T, Bronstein A, Michailovich O 和 Zibulevsky
    M 2019 医学成像中逆问题求解器的自监督学习 Domain Adaptation and Representation Transfer 和 Medical
    Image Learning with Less Labels and Imperfect Data (Springer) pp 111–119'
- en: '[36] Yaman B, Hosseini S A H, Moeller S, Ellermann J, Uǧurbil K and Akçakaya
    M 2019 Self-supervised physics-based deep learning mri reconstruction without
    fully-sampled data arXiv preprint arXiv:1910.09116'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Bar L and Sochen N 2019 Unsupervised deep learning algorithm for pde-based
    forward and inverse problems arXiv preprint arXiv:1904.05417'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Cha G, Lee M and Oh S 2019 Unsupervised 3d reconstruction networks Proceedings
    of the IEEE International Conference on Computer Vision pp 3849–3858'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Fraccaro M, Kamronn S, Paquet U and Winther O 2017 A disentangled recognition
    and nonlinear dynamics model for unsupervised learning Advances in Neural Information
    Processing Systems pp 3601–3610'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Maass P 2019 Deep learning for trivial inverse problems Compressed Sensing
    and Its Applications (Springer) pp 195–209'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Vito E D, Rosasco L, Caponnetto A, Giovannini U D and Odone F 2005 Learning
    from examples as an inverse problem Journal of Machine Learning Research  6 883–904'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Poggio T and Girosi F 1989 A theory of networks for approximation and
    learning Tech. rep. Massachusetts Inst. of Tech Cambridge Artificial Intelligence
    Lab'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Cucker F and Smale S 2002 On the mathematical foundations of learning
    Bulletin of the American Mathematical Society  39 1–49'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Poggio T, Torre V and Koch C 1985 Computational vision and regularization
    theory nature  317 314–319'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Lathuilière S, Mesejo P, Alameda-Pineda X and Horaud R 2019 A comprehensive
    analysis of deep regression IEEE Transactions on Pattern Analysis and Machine
    Intelligence'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Chollet F et al. 2015 Keras [https://github.com/fchollet/keras](https://github.com/fchollet/keras)'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Kingma D P and Ba J 2014 Adam: A method for stochastic optimization arXiv
    preprint arXiv:1412.6980'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Aldrian O and Smith W A 2012 Inverse rendering of faces with a 3d morphable
    model IEEE Transactions on Pattern Analysis and Machine Intelligence  35 1080–1093'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Singer S and Nelder J 2009 Nelder-mead algorithm Scholarpedia  4 2928'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Chaladze G and Kalatozishvili L 2017 Linnaeus 5 dataset for machine learning
    Tech. rep. Tech. Rep'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Buades A, Coll B and Morel J M 2011 Non-local means denoising Image Processing
    On Line  1 208–212'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Blanz V, Vetter T et al. 1999 A morphable model for the synthesis of 3d
    faces. Siggraph vol 99 pp 187–194'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Eftimov T and Korošec P 2019 A novel statistical approach for comparing
    meta-heuristic stochastic optimization algorithms according to the distribution
    of solutions in the search space Information Sciences  489 255–273'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Kim D Y, Vo B N, Vo B T and Jeon M 2019 A labeled random finite set online
    multi-object tracker for video data Pattern Recognition  90 377–389'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Choi C and Christensen H I 2013 Rgb-d object tracking: A particle filter
    approach on gpu 2013 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IEEE) pp 1084–1091'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Black J, Ellis T, Rosin P et al. 2003 A novel method for video tracking
    performance evaluation Proceedings of the IEEE InternationalWorkshop on Visual
    Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS
    03) 125–132'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Lyons D M and Benjamin D P 2009 Locating and tracking objects by efficient
    comparison of real and predicted synthetic video imagery Intelligent Robots and
    Computer Vision XXVI: Algorithms and Techniques vol 7252 (International Society
    for Optics and Photonics) p 72520L'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Vermaak J, Lawrence N D and Pérez P 2003 Variational inference for visual
    tracking 2003 IEEE Computer Society Conference on Computer Vision and Pattern
    Recognition, 2003\. Proceedings. vol 1 (IEEE) pp I–I'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Krishnan R G, Shalit U and Sontag D 2017 Structured inference networks
    for nonlinear state space models AAAI'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Hafner D, Lillicrap T, Fischer I, Villegas R, Ha D, Lee H and Davidson
    J 2019 Learning latent dynamics for planning from pixels International Conference
    on Machine Learning (PMLR) pp 2555–2565'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Rangapuram S S, Seeger M, Gasthaus J, Stella L, Wang Y and Januschowski
    T 2018 Deep state space models for time series forecasting Proceedings of the
    32nd international conference on neural information processing systems pp 7796–7805'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Coskun H, Achilles F, DiPietro R, Navab N and Tombari F 2017 Long short-term
    memory kalman filters: Recurrent neural estimators for pose regularization Proceedings
    of the IEEE International Conference on Computer Vision pp 5524–5532'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Xu J and Zhang J 2014 Exploration-exploitation tradeoffs in metaheuristics:
    Survey and analysis Proceedings of the 33rd Chinese control conference (IEEE)
    pp 8633–8638'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
