- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2307.10275] Survey on Controlable Image Synthesis with Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.10275](https://ar5iv.labs.arxiv.org/html/2307.10275)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Survey on Controlable Image Synthesis with Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shixiong Zhang Shixiong Zhang is with the School of Automation Engineering,
    University of Electronic Science and Technology of China, Chengdu, Sichuan, China
    (email: 202221060818@std.uestc.edu.cn).    Jiao Li Jiao Li is with the College
    of Information Engineering, Sichuan Agricultural University, Chengdu, Sichuan,
    China (email: 202005852@stu.sicau.edu.cn).    Lu Yang Lu Yang is with the School
    of Automation Engineering, University of Electronic Science and Technology of
    China, Chengdu, Sichuan, China (email: yanglu@uestc.edu.cn). Corresponding Author.
    Member, IEEE.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Image synthesis has attracted emerging research interests in academic and industry
    communities. Deep learning technologies especially the generative models greatly
    inspired controllable image synthesis approaches and applications, which aim to
    generate particular visual contents with latent prompts. In order to further investigate
    low-level controllable image synthesis problem which is crucial for fine image
    rendering and editing tasks, we present a survey of some recent works on 3D controllable
    image synthesis using deep learning. We first introduce the datasets and evaluation
    indicators for 3D controllable image synthesis. Then, we review the state-of-the-art
    research for geometrically controllable image synthesis in two aspects: 1) Viewpoint/pose-controllable
    image synthesis; 2) Structure/shape-controllable image synthesis. Furthermore,
    the photometrically controllable image synthesis approaches are also reviewed
    for 3D re-lighting researches. While the emphasis is on 3D controllable image
    synthesis algorithms, the related applications, products and resources are also
    briefly summarized for practitioners.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: image synthesis, 3D controlable, NeRF, GAN, diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial Intelligence Generated Content (AIGC) is the term for digital media
    produced by machine learning methods, such as ChatGPT and stable diffusion[[1](#bib.bib1)],
    which are currently popular[[2](#bib.bib2)]. AIGC has various applications in
    domains such as entertainment, education, marketing, and research[[3](#bib.bib3)].
    Image synthesis is a subcategory of AIGC that involves generating realistic or
    stylized images from textual inputs, sketches, or other images[[4](#bib.bib4)].
    Image synthesis can also perform various tasks such as inpainting, semantic scene
    synthesis, super-resolution, and unconditional image generation[[1](#bib.bib1),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Image synthesis can be classified into two types based on controllability:
    unconditional and conditional [[8](#bib.bib8)]. Conditional image synthesis can
    be further divided into three levels of control: high, medium, and low. High-level
    control refers to the image content such as category, medium-level control refers
    to the image background and other aspects, and low-level control refers to manipulating
    the image based on the underlying principles of traditional computer vision [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conventional 3D image synthesis techniques face challenges in handling intricate
    details and patterns that vary across different objects [[12](#bib.bib12)]. Deep
    learning methods can better model the variations in shape, texture, and illumination
    of 3D objects[[13](#bib.bib13)]. The field of deep learning-based image synthesis
    has made remarkable progress in recent years, aided by the availability of more
    open source datasets [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]. Various
    image synthesis methods have emerged, such as generative adversarial network (GAN)
    [[7](#bib.bib7)], diffusion model (DM) [[6](#bib.bib6)], and neural radiance field
    (NeRF) [[17](#bib.bib17)]. These methods differ in their levels of controllability:
    GAN and DM are suitable for high-level or medium-level controllable image synthesis,
    while NeRF is suitable for low-level controllable image synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: Low-level controllable image synthesis can be categorized into geometric and
    illumination control. Geometric control involves manipulating the pose and structure
    of the scene, where the pose can refer to either the camera or the object, while
    the structure can refer to either the global shape (using depth maps, point clouds,
    or other 3D representations) or the local attributes (such as size, shape, color,
    etc.) of the object. Illumination control involves manipulating the light source
    and the material properties of the object. Refer to Fig. [2](#S2.F2 "Figure 2
    ‣ II Data Sets and Evaluation Indicators for 3D Controlable Image Synthesis ‣
    Survey on Controlable Image Synthesis with Deep Learning"), [3](#S2.F3 "Figure
    3 ‣ II-A Data Sets ‣ II Data Sets and Evaluation Indicators for 3D Controlable
    Image Synthesis ‣ Survey on Controlable Image Synthesis with Deep Learning"),
    and [4](#S2.F4 "Figure 4 ‣ II-B Evaluation Indicators ‣ II Data Sets and Evaluation
    Indicators for 3D Controlable Image Synthesis ‣ Survey on Controlable Image Synthesis
    with Deep Learning") for an example figure.
  prefs: []
  type: TYPE_NORMAL
- en: Several surveys have attempted to cover the state-of-the-art techniques and
    applications in image synthesis. However, most of these surveys have become obsolete
    due to the rapid development of the field [[8](#bib.bib8)], or have focused on
    the high-level and medium-level aspects of image synthesis, while ignoring the
    low-level aspects [[18](#bib.bib18)]. Furthermore, most of these surveys have
    adopted a methodological perspective, which is useful for researchers who want
    to understand the underlying principles and algorithms of image synthesis, but
    not for practitioners who want to apply image synthesis techniques to solve specific
    problems in various domains [[18](#bib.bib18), [19](#bib.bib19)]. This paper provides
    a task-oriented review of low-level controllable image synthesis, excluding human
    subjects[[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: This review offers a comprehensive overview of the state-of-the-art deep learning
    methods for 3D controllable image synthesis. In Section [II](#S2 "II Data Sets
    and Evaluation Indicators for 3D Controlable Image Synthesis ‣ Survey on Controlable
    Image Synthesis with Deep Learning"), we begin by introducing the common data
    sets and evaluation indicators for this task. For the data set part, we divide
    it by its content. In Section [III](#S3 "III Pose Manipulation ‣ Survey on Controlable
    Image Synthesis with Deep Learning") to [V](#S5 "V Illumination Manipulation ‣
    Survey on Controlable Image Synthesis with Deep Learning"), we survey the control
    methods based on pose, structure and illumination, and divided each part into
    global and local controls. In Section [VI](#S6 "VI Application ‣ Survey on Controlable
    Image Synthesis with Deep Learning"), we discuss some of the current applications
    of 3D controllable image synthesis based on deep learning. Finally, Section [VII](#S7
    "VII Conclusion ‣ Survey on Controlable Image Synthesis with Deep Learning") concludes
    this paper. The overview of the surveyed 3D controlable image synthesis is shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Survey on Controlable Image Synthesis
    with Deep Learning").In the following sections, we will review common data sets
    and evaluation indicators in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa275b34839dfb3a020feb4dff1fb13e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The structure diagram of this paper'
  prefs: []
  type: TYPE_NORMAL
- en: II Data Sets and Evaluation Indicators for 3D Controlable Image Synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8931656b6f720111418b05f7c9cf0a2e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Original image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23ebaae7f33e057113f1ee9caaa38714.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Image after changing viewpoint
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02da001e8c04ac473989bc27d7375bf1.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Original image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6df6db2314cd59c437252263ddaadf45.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Image after rotating object
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Pose manipulation example.Subfigure (a) and (b) correspond to the
    content of Global Pose realization in Section [III](#S3 "III Pose Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning"), which come from
    [[24](#bib.bib24)]. Subfigure (c) and (d) correspond to the content of Local Pose
    realization in Section [III](#S3 "III Pose Manipulation ‣ Survey on Controlable
    Image Synthesis with Deep Learning"), which come from [[25](#bib.bib25)]'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key challenges in 3D controllable image synthesis is to evaluate
    the quality and diversity of the generated images. Different data sets and metrics
    have been proposed to measure various aspects of 3D controllable image synthesis,
    such as realism, consistency, fidelity, and controllability. In this section,
    we will introduce some of the commonly used data sets and metrics for 3D controllable
    image synthesis, and discuss their advantages and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Data Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '3D image synthesis is the task of generating realistic images of 3D objects
    from different viewpoints. This task requires a large amount of training data
    that can capture the shape, texture, lighting and pose variations of 3D objects.
    Several datasets have been proposed for this purpose, each with its own advantages
    and limitations. Some of the data sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ABO is a synthetic data set that contains 3D shapes generated by assembling
    basic objects (ABOs) such as cubes, spheres, cylinders, and cones. It has 10 categories
    and 1000 shapes per category. ABO is useful for tasks such as shape abstraction,
    decomposition, and generation. However, ABO is also limited by its synthetic nature,
    its small number of categories and instances, and its lack of realistic lighting
    and occlusion[[24](#bib.bib24)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clevr3D is a synthetic data set that contains 3D scenes composed of simple geometric
    shapes with various attributes such as color, size, and material. It also provides
    natural language descriptions and questions for each scene. Clevr3D is useful
    for tasks such as scene understanding, reasoning, and captioning. However, Clevr3D
    is also limited by its synthetic nature, its simple scene composition, and its
    lack of realistic textures and backgrounds[[26](#bib.bib26)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ScanNet is an RGB-D video data set that contains 2.5 million views in more than
    1500 scans of indoor scenes. It provides annotations such as camera poses, surface
    reconstructions, and instance-level semantic segmentations. ScanNet is useful
    for tasks such as semantic segmentation, object detection, and pose estimation.ScanNet
    is also limited by its incomplete coverage (due to scanning difficulties), its
    inconsistent labeling (due to human errors), and its lack of fine-grained details
    (such as object parts)[[27](#bib.bib27)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RealEstate10K is a data set for view synthesis that contains camera poses corresponding
    to 10 million frames derived from about 80,000 video clips gathered from YouTube
    videos. The data set also provides links to download the original videos. RealEstate10K
    is a large-scale and diverse data set that covers various types of scenes, such
    as houses, apartments, offices, and landscapes. RealEstate10K is useful for tasks
    such as stereo magnification, light field rendering, and novel view synthesis.
    However, RealEstate10K also has some challenges, such as the low quality of the
    videos, the inconsistency of the camera poses, and the lack of depth information[[28](#bib.bib28)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04953b95d6123d887831d87c2e8a3423.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Original image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3fe22fddaf9c9598c7f3a10374aa0a2d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Image after changing the depth of car
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e3cc57d22d8c9fdeca9ed2076b18d9f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Original image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e905e7797514d873de8c898e9eada869.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Image after changing the color of car
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Structure manipulation example.Subfigure (a) and (b) correspond to
    the content of Global Structure realization in Section [IV](#S4 "IV Structure
    Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning"), which
    come from [[29](#bib.bib29)]. Subfigure (c) and (d) correspond to the content
    of Local Structure realization in Section [IV](#S4 "IV Structure Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning"), which come from
    [[30](#bib.bib30)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Point cloud data sets are collections of points that represent the shape and
    appearance of a 3D object or scene. They are often obtained from sensors such
    as lidar, radar, or cameras. Some of the data sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ShapeNet is a large-scale repository of 3D CAD models that covers 55 common
    object categories and 4 million models. It provides rich annotations such as category
    labels, part labels, alignments, and correspondences. ShapeNet is useful for tasks
    such as shape classification, segmentation, retrieval, and completion.Some of
    the limitations of ShapeNet are that it does not contain realistic textures or
    materials, it does not capture the variability and diversity of natural scenes,
    and it does not provide ground truth poses or camera parameters for rendering[[31](#bib.bib31)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KITTI is a data set for autonomous driving that contains 3D point clouds captured
    by a Velodyne HDL-64E LIDAR sensor, along with RGB images, GPS/IMU data, object
    annotations, and semantic labels. KITTI is one of the most popular and challenging
    data sets for 3D object detection and semantic segmentation, as it covers various
    scenarios, weather conditions, and occlusions. However, KITTI also has some limitations,
    such as the limited number of frames per sequence (around 200), the fixed sensor
    configuration, and the lack of dynamic objects[[32](#bib.bib32)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nuScenes is another data set for autonomous driving that contains 3D point clouds
    captured by a 32-beam LIDAR sensor, along with RGB images, radar data, GPS/IMU
    data, object annotations, and semantic labels. nuScenes is more comprehensive
    and diverse than KITTI, as it covers 1000 scenes from six cities in different
    countries, with varying traffic rules and driving behaviors. nuScenes also provides
    more temporal information, with 20 seconds of continuous data per scene. However,
    nuScenes also has some challenges, such as the lower resolution of the point clouds,
    the higher complexity of the scenes, and the need for sensor fusion[[33](#bib.bib33)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matterport3D is a data set for indoor scene understanding that contains 3D point
    clouds reconstructed from RGB-D images captured by a Matterport camera. The data
    set also provides surface reconstructions, camera poses, and 2D and 3D semantic
    segmentations. Matterport3D is a large-scale and high-quality data set that covers
    10,800 panoramic views from 194,400 RGB-D images in 90 building types. Matterport3D
    is useful for tasks such as keypoint matching, view overlap prediction, and scene
    completion. However, Matterport3D also has some limitations, such as the lack
    of dynamic objects, the dependence on RGB-D sensors, and the difficulty of obtaining
    ground truth annotations[[34](#bib.bib34)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Depth map data sets are collections of images and their corresponding depth
    values, which can be used for various computer vision tasks such as depth estimation,
    3D reconstruction, scene understanding, etc.The commonly used depth map data sets
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Middlebury Stereo is a data set of stereo images with ground truth disparity
    maps obtained using structured light or a robot arm. It contains several versions
    of data sets collected from 2001 to 2021, with different scenes, resolutions,
    and levels of difficulty. The data set is widely used for evaluating stereo matching
    algorithms and provides online benchmarks and leaderboards. The strengths of this
    data set are its high accuracy, diversity, and availability. The limitations are
    its relatively small size, indoor scenes only, and lack of semantic labels[[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NYU Depth Data set V2 is a data set of RGB-D images captured by Microsoft Kinect
    in various indoor scenes. It contains 1449 densely labeled pairs of aligned RGB
    and depth images, as well as 407024 unlabeled frames. The data set also provides
    surface normals, 3D point clouds, and semantic labels for each pixel. The data
    set is widely used for evaluating monocular depth estimation algorithms and provides
    online tools for data processing and visualization. The strengths of this data
    set are its large size, rich annotations, and realistic scenes. The limitations
    are its low resolution, noisy depth values, and indoor scenes only[[40](#bib.bib40)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KITTI also includes depth maps, but its depth maps are limited by sparse and
    noisy LiDAR depth maps. There is also a lack of real depth maps on the ground
    for certain scenes, as well as limitations on city Settings[[32](#bib.bib32)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Illumination data sets are collections of information about the intensity,
    distribution, and characteristics of artificial or natural light sources. Some
    examples of common illumination data sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-PIE is a large-scale data set that contains over 750,000 images of 337
    subjects, captured in 15 view angles and 19 illumination conditions. Each subject
    also performed different facial expressions, such as neutral, smile, surprise,
    and squint. The data set is useful for studying face recognition, face alignment,
    face synthesis, and face editing under varying conditions.However, Multi-PIE only
    contains images of Caucasian subjects, which limits its diversity and generalization[[41](#bib.bib41)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relightables is a collection of high-quality 3D scans of human subjects under
    varying lighting conditions. This data set allows for realistic rendering of human
    performances with any lighting and viewpoint, which can be integrated into any
    CG scene. Nevertheless, this data set has some drawbacks, such as the low diversity
    of subjects, poses, and expressions, and the high computational expense of processing
    the data[[42](#bib.bib42)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In conclusion, data sets are essential for 3D controllable image synthesis based
    on deep learning, as they provide the necessary information for training and evaluating
    deep generative models. These data sets provide rich annotations and variations
    for different type of control, such as viewpoint, lighting, poses, point clouds,
    and depth. However, each data set has its own strengths and weaknesses, and there
    is still room for improvement and innovation in this field.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Evaluation Indicators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/82fa2fae2bb4ef55a10c8da3cdbb5873.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Original image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3dc98e08b0f09b0ac4abc29e67d75492.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Image after changing light source
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/05eb08214b583bff8d71c49231b2c41d.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Original image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a044486e09caca4ecd595eecf7be4c73.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Image after changing roughness
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Illumination manipulation example.Subfigure (a) and (b) correspond
    to the content of Global Illumination realization in Section [V](#S5 "V Illumination
    Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning"), which
    come from [[41](#bib.bib41)]. Subfigure (c) and (d) correspond to the content
    of Local Illumination realization in Section [V](#S5 "V Illumination Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning"), which come from
    [[43](#bib.bib43)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the quality and diversity of the synthesized images, several performance
    indicators are commonly used. Some of them are:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Peak signal-to-noise ratio (PSNR)[[44](#bib.bib44)]: This measures the similarity
    between the synthesized image and a reference image in terms of pixel values.
    It is defined as the ratio of the maximum possible power of a signal to the power
    of noise that affects the fidelity of its representation. A higher PSNR indicates
    a better image quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Structural similarity index (SSIM)[[45](#bib.bib45)]: This measures the similarity
    between the synthesized image and a reference image in terms of luminance, contrast,
    and structure. It is based on the assumption that the human visual system is highly
    adapted to extract structural information from images. A higher SSIM indicates
    a better image quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Learned Perceptual Image Patch Similarity (LPIPS)[[46](#bib.bib46)]: This
    measures the similarity between the synthesized image and a reference image in
    terms of deep features. It is defined as the distance between the activations
    of two image patches for a pre-trained network. A lower LPIPS indicates a better
    image quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Inception score (IS)[[47](#bib.bib47)]: This measures the quality and diversity
    of the synthesized images using a pre-trained classifier, such as Inception-v3\.
    It is based on the idea that good images should have high class diversity (i.e.,
    they can be classified into different categories) and low class ambiguity (i.e.,
    they can be classified with high confidence). A higher IS indicates a better image
    synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Fréchet inception distance (FID)[[48](#bib.bib48)]: This measures the distance
    between the feature distributions of the synthesized images and the real images
    using a pre-trained classifier, such as Inception-v3\. It is based on the idea
    that good images should have similar feature statistics to real images. A lower
    FID indicates a better image synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Kernel Inception Distance (KID)[[49](#bib.bib49)]: This measures the squared
    maximum mean discrepancy between the feature distributions of the synthesized
    images and the real images using a pre-trained classifier, such as Inception-v3\.
    It is based on the idea that good images should have similar feature statistics
    to real images. A lower KID indicates a better image synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: III Pose Manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Global Pose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-A1 GAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/255c0b6c2d3f735527c2b032c9bde22e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Schematic of GAN, which come from [[50](#bib.bib50)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Adversarial Network (GAN)[[7](#bib.bib7)] can generate realistic
    and diverse data from a latent space. GAN consists of two neural networks: a generator
    and a discriminator. The generator tries to produce data that can fool the discriminator,
    while the discriminator tries to distinguish between real and fake data. The loss
    function of GAN measures how well the generator and the discriminator perform
    their tasks. The loss function is usually composed of two terms: one for the generator($\mathcal{L}_{G}$)
    and one for the discriminator($\mathcal{L}_{D}$). $\mathcal{L}_{G}$ is based on
    how often the discriminator classifies the generated data as real, while $\mathcal{L}_{D}$
    is based on how often it correctly classifies the real and fake data. The goal
    of GAN is to minimize $\mathcal{L}_{G}$ and maximize $\mathcal{L}_{D}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{D}=\mathbb{E}_{\mathbf{x}\sim p_{\mathbf{data}}\mathbf{(x)}}[\log(D(\mathbf{x}))]+\mathbb{E}_{\mathbf{z}\sim
    p_{\mathbf{z}}\mathbf{(z)}}[\log(1-D(G(\mathbf{z})))],$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{G}=-\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}\mathbf{(z)}}[\log(D(G(\mathbf{z})))],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{GAN}=\mathcal{L}_{D}+\mathcal{L}_{G}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'a)Crossview image synthesis. Viewpoint manipulation refers to the ability to
    manipulate the perspective or orientation of the objects or scenes in the synthetic
    images. The earliest view composites were usually only able to composite a specific
    view, such as a bird’s eye view, a frontal view of a person’s face, etc. Huang
    et al. introduced TP-GAN, a method that integrates global structure and local
    details to generate realistic frontal views of faces [[51](#bib.bib51)]. Similarly,
    Zhao et al. proposed VariGAN, which combines variational inference and generative
    adversarial networks for the progressive refinement of synthesized target images
    [[52](#bib.bib52)]. To address the challenge of generating scenes from different
    viewpoints and resolutions, Krishna and Ali developed two methods: Crossview Fork
    (X-Fork) and Crossview Sequential (X-Seq) [[53](#bib.bib53)]. These methods employ
    semantic segmentation graphs to aid conditional GANs (cGANs) in producing sharper
    images. Furthermore, Krishna and Ali utilized geometry-guided cGANs for image
    synthesis, converting ground images to aerial views [[54](#bib.bib54)].Mokhayeri
    et al. proposed a cross-domain face synthesis approach using a Controllable GAN
    (C-GAN). This method generates realistic face images under various poses by refining
    simulated images from a 3D face model through an adversarial game [[55](#bib.bib55)].
    Zhu et al. developed BridgeGAN, a technique for synthesizing bird’s eye view images
    from single frontal view images. They employed a homography view as an intermediate
    representation to accomplish this task [[56](#bib.bib56)]. Ding et al. addressed
    the problem of cross-view image synthesis by utilizing generative adversarial
    networks (GANs) based on deformable convolution and attention mechanisms [[57](#bib.bib57)].
    Lastly, Ren et al. proposed MLP-Mixer GANs for cross-view image conversion. This
    method comprises two stages to alleviate severe deformation when generating entirely
    different views [[58](#bib.bib58)].'
  prefs: []
  type: TYPE_NORMAL
- en: b)Free viewpoint image synthesis. By adding conditional inputs such as camera
    pose or camera manifold to the GAN network, it can output images from any viewpoint.
    Zhu et al. introduced CycleGAN, a method capable of recovering the front face
    from a single profile postural facial image, even when the source domain does
    not match the target domain[[59](#bib.bib59)]. This approach is based on a conditional
    variational autoencoder and generative adversarial network (cVAE-GAN) framework,
    which does not require paired data, making it a versatile method for view translation[[60](#bib.bib60)].Shen
    et al. proposed Pairwise-GAN, employing two parallel U-Nets as generators and
    PatchGAN as a discriminator to synthesize frontal face images[[61](#bib.bib61)].
    Similarly, Chan et al. presented pi-GAN, a method utilizing periodic implicit
    Generative Adversarial Networks for high-quality 3D-aware image synthesis[[62](#bib.bib62)].
    Cai et al. further extended this approach with Pix2NeRF, an unsupervised method
    leveraging pi-GAN to train on single images without relying on 3D or multi-view
    supervision[[63](#bib.bib63)].Leimkuhler et al. introduced FreeStyleGAN, which
    integrates a pre-trained StyleGAN into standard 3D rendering pipelines, enabling
    stereo rendering or consistent insertion of faces in synthetic 3D environments[[64](#bib.bib64)].
    Medin et al. proposed MOST GAN, explicitly incorporating physical facial attributes
    as prior knowledge to achieve realistic portrait image manipulation[[65](#bib.bib65)].On
    the other hand, Or-El et al. developed StyleSDF, a novel method generating images
    based on StyleGAN2 by utilizing Signed Distance Fields (SDFs) to accurately model
    3D surfaces, enabling volumetric rendering with consistent results[[66](#bib.bib66)].
    Additionally, Zheng et al. presented SDF-StyleGAN, a deep learning method for
    generating 3D shapes based on StyleGAN2, employing two new shape discriminators
    operating on global and local levels to compare real and synthetic SDF values
    and gradients, significantly enhancing shape geometry and visual quality[[67](#bib.bib67)].Moreover,
    Deng et al. proposed GRAM, a novel approach regulating point sampling and radiance
    field learning on 2D manifolds, embodied as a set of learned implicit surfaces
    in the 3D volume, leading to improved synthesis results[[68](#bib.bib68)]. Xiang
    et al. built upon this work with GRAM-HD, capable of generating high-resolution
    images with strict 3D consistency, up to a resolution of 1024x1024[[69](#bib.bib69)].In
    another line of research, Chan et al. developed an efficient framework for generating
    realistic 3D shapes from 2D images using generative adversarial networks (GANs),
    comprising a geometry-aware module predicting the 3D shape and its projection
    parameters from the input image, and a refinement module enhancing shape quality
    and details[[70](#bib.bib70)]. Similarly, Zhao et al. proposed a method for generating
    high-quality 3D images from 2D inputs using GAN, achieving consistency across
    different viewpoints and offering rendering with novel lighting effects[[71](#bib.bib71)].Lastly,
    Alhaija et al. introduced XDGAN, a method for synthesizing realistic and diverse
    3D shapes from 2D images, converting 3D shapes into compact 1-channel geometry
    images and utilizing StyleGAN3 and image-to-image translation networks to generate
    3D objects in a 2D space[[72](#bib.bib72)]. These advancements in image synthesis
    techniques have significantly enriched the field of 3D image generation from 2D
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 NeRF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c38bb91d21cb0f6aa38a9a8bf0119c16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Schematic of NeRF, which come from [[17](#bib.bib17)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Radiance Fields (NeRF)[[17](#bib.bib17)] are a novel representation
    for complex 3D scenes that can be rendered photorealistically from any viewpoint.
    NeRF models a scene as a continuous function that maps 5D coordinates (3D location
    and 2D viewing direction, expressed as ($x$, $y$, $z$, $\theta$, $\varphi$)) to
    a 4D output (RGB color and opacity). This function is learned from a set of posed
    images of the scene using a deep neural network. Before the nerf passes the ($x$,
    $y$, $z$, $\theta$, $\varphi$) input to the network, it maps the input to a higher
    dimensional space using high-frequency functions to better fit the data containing
    high-frequency variations. The high-frequency coding function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\ldots,\sin\left(2^{L-1}\pi
    p\right),\cos\left(2^{L-1}\pi p\right)\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Where $p$ is the input ($x$, $y$, $z$, $\theta$, $\varphi$).
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Enhancements to NeRF'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Publication | Image resolution | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| no camera pose | NeRF–[[73](#bib.bib73)] | arXiv2022 | 756x1008/1080x1920/520x780
    | [[74](#bib.bib74)]/[[28](#bib.bib28)]/[[73](#bib.bib73)] |'
  prefs: []
  type: TYPE_TB
- en: '| GNeRF[[75](#bib.bib75)] | ICCV2021 | 400x400/500x400 | [[17](#bib.bib17)]/[[76](#bib.bib76)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SCNeRF[[77](#bib.bib77)] | ICCV2021 |  | [[74](#bib.bib74)]/[[78](#bib.bib78)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| NoPe-NeRF[[79](#bib.bib79)] | CVPR2023 |  | [[76](#bib.bib76)]/[[74](#bib.bib74)]/[[80](#bib.bib80)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SPARF[[81](#bib.bib81)] | CVPR2023 | 960x540/648x484 | [[78](#bib.bib78)]/[[27](#bib.bib27)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| sparse data | NeRS[[82](#bib.bib82)] | NIPS2021 |  | [[82](#bib.bib82)] |'
  prefs: []
  type: TYPE_TB
- en: '| MixNeRF[[83](#bib.bib83)] | CVPR2023 |  | [[76](#bib.bib76)]/[[74](#bib.bib74)]/[[17](#bib.bib17)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SceneRF[[84](#bib.bib84)] | arXiv2023 | 1220x370 | [[85](#bib.bib85)] |'
  prefs: []
  type: TYPE_TB
- en: '| GM-NeRF[[86](#bib.bib86)] | CVPR2023 | 224x224 | [[87](#bib.bib87)]/[[88](#bib.bib88)]/[[89](#bib.bib89)]/[[90](#bib.bib90)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SPARF[[81](#bib.bib81)] | CVPR2023 | 960x540/648x484 | [[78](#bib.bib78)]/[[27](#bib.bib27)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| noisy data | RawNeRF[[91](#bib.bib91)] | CVPR2022 |  | [[91](#bib.bib91)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deblur-NeRF[[92](#bib.bib92)] | CVPR2022 |  | [[92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '| HDR-NeRF[[93](#bib.bib93)] | CVPR2022 | 400x400/804x534 | [[93](#bib.bib93)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| NAN[[94](#bib.bib94)] | CVPR2022 |  | [[94](#bib.bib94)] |'
  prefs: []
  type: TYPE_TB
- en: '| large-scale image synthesis | Mip-NeRF 360[[95](#bib.bib95)] | CVPR2022 |  |
    [[78](#bib.bib78)] |'
  prefs: []
  type: TYPE_TB
- en: '| BungeeNeRF[[96](#bib.bib96)] | ECCV2022 |  | [[97](#bib.bib97)] |'
  prefs: []
  type: TYPE_TB
- en: '| Block-NeRF[[98](#bib.bib98)] | arXiv2022 |  | [[98](#bib.bib98)] |'
  prefs: []
  type: TYPE_TB
- en: '| GridNeRF[[99](#bib.bib99)] | CVPR2023 |  | [[100](#bib.bib100)]/[[99](#bib.bib99)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| EgoNeRF[[101](#bib.bib101)] | CVPR2023 |  | [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '| image synthesis speed | PlenOctrees[[102](#bib.bib102)] | ICCV2021 | 800x800/1920x1080
    | [[17](#bib.bib17)]/[[78](#bib.bib78)] |'
  prefs: []
  type: TYPE_TB
- en: '| DirectVoxGO[[103](#bib.bib103)] | CVPR2022 | 800x800/800x800/768x576/1920x1080/512x512
    | [[17](#bib.bib17)]/[[104](#bib.bib104)]/[[105](#bib.bib105)]/[[78](#bib.bib78)]/[[106](#bib.bib106)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| R2L[[107](#bib.bib107)] | ECCV2022 |  | [[17](#bib.bib17)]/[[108](#bib.bib108)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| SqueezeNeRF[[109](#bib.bib109)] | CVPR2022 |  | [[17](#bib.bib17)]/[[74](#bib.bib74)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNeRF[[110](#bib.bib110)] | CVPR2023 | 800x800/1008x756/1256x828 | [[17](#bib.bib17)]/[[74](#bib.bib74)]/[[95](#bib.bib95)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| L2G-NeRF[[111](#bib.bib111)] | CVPR2023 |  | [[74](#bib.bib74)] |'
  prefs: []
  type: TYPE_TB
- en: 'Zhang et al. introduced NeRF++ as a framework that enhances NeRF (Neural Radiance
    Fields) through adaptive sampling, hierarchical volume rendering, and multi-scale
    feature encoding techniques [[112](#bib.bib112)]. This approach enables high-quality
    rendering for both static and dynamic scenes while improving efficiency and robustness.
    Rebain et al. proposed a method to enhance the efficiency and quality of neural
    rendering by employing spatial decomposition [[113](#bib.bib113)].Park et al.
    developed a novel technique for capturing and rendering high-quality 3D selfies
    using a single RGB camera. Their method utilizes a deformable neural radiance
    field (NeRF) model capable of representing both the geometry and appearance of
    dynamic scenes [[114](#bib.bib114)]. Li et al. introduced MINE, a method for novel
    view synthesis and depth estimation from a single image. This approach generalizes
    Multiplane Images (MPI) with continuous depth using Neural Radiance Fields (NeRF)
    [[115](#bib.bib115)].Park et al. proposed HyperNeRF, a method for representing
    and rendering complex 3D scenes with varying topology using neural radiance fields
    (NeRFs). Unlike previous NeRF-based approaches that rely on a fixed 3D coordinate
    system, HyperNeRF employs a higher-dimensional continuous embedding space to capture
    arbitrary scene changes [[116](#bib.bib116)]. Chen et al. presented Aug-NeRF,
    a novel method for training neural radiance fields (NeRFs) with physically-grounded
    augmentations at different levels: scene, camera, and pixel [[117](#bib.bib117)].Kaneko
    proposed AR-NeRF, a method for learning 3D representations of natural images without
    supervision. The approach utilizes a neural radiance field (NeRF) model to render
    images with various viewpoints and aperture sizes, capturing both depth and defocus
    effects [[118](#bib.bib118)]. Li et al. introduced SymmNeRF, a framework that
    utilizes neural radiance fields (NeRFs) to synthesize novel views of objects from
    a single image. This method leverages symmetry priors to recover fine appearance
    details, particularly in self-occluded areas [[119](#bib.bib119)].Zhou et al.
    proposed NeRFLiX, a novel framework for improving the quality of novel view synthesis
    using neural radiance fields (NeRF). This approach addresses rendering artifacts
    such as noise and blur by employing an inter-viewpoint aggregation framework that
    fuses high-quality training images to generate more realistic synthetic views
    [[120](#bib.bib120)].'
  prefs: []
  type: TYPE_NORMAL
- en: Besides, a number of researchers have proposed enhancements to the original
    NeRF model, addressing its limitations in scenarios such as no camera pose, sparse
    data, noisy data, large-scale image synthesis, and image synthesis speed.See Table
    [I](#S3.T1 "TABLE I ‣ III-A2 NeRF ‣ III-A Global Pose ‣ III Pose Manipulation
    ‣ Survey on Controlable Image Synthesis with Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Diffusion model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a1ce14797f7f3b68ca52ebed3e84c20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Schematic of diffusion model, which come from [[50](#bib.bib50)]'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most widely used models in deep learning is the diffusion model,
    which is a generative model that can produce realistic and diverse images from
    random noise. The diffusion model is based on the idea of reversing the process
    of adding Gaussian noise to an image until it becomes completely corrupted. The
    diffusion process starts from a data sample and gradually adds noise until it
    reaches a predefined noise level, i.e. $q\left(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}\right)$
    in the Fig. [7](#S3.F7 "Figure 7 ‣ III-A3 Diffusion model ‣ III-A Global Pose
    ‣ III Pose Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning").
    The generative model then learns to reverse this process by denoising the samples
    at each step i.e. $p_{\mathbf{\theta}}\left(\mathbf{x}_{t-1}\mid\mathbf{x}_{t}\right)$
    in the Fig. [7](#S3.F7 "Figure 7 ‣ III-A3 Diffusion model ‣ III-A Global Pose
    ‣ III Pose Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $q\left(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}\right)=\mathcal{N}\left(\mathbf{x}_{t};\sqrt{1-\beta_{t}}\mathbf{x}_{t-1},\beta_{t}\mathbf{I}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Sbrolli et al. introduced IC3D, a novel approach addressing various challenges
    in shape generation. This method is capable of reconstructing a 3D shape from
    a single view, synthesizing a 3D shape from multiple views, and completing a 3D
    shape from partial inputs [[121](#bib.bib121)]. Another significant contribution
    in this area is the work by Gu et al., who developed Control3Diff, a generative
    model with 3D-awareness and controllability. By combining diffusion models and
    3D GANs, Control3Diff can synthesize diverse and realistic images without relying
    on 3D ground truth data, and can be trained solely on single-view image datasets
    [[122](#bib.bib122)].Additionally, Anciukevicius et al. proposed RenderDiffusion,
    an innovative diffusion model for 3D generation and inference. Remarkably, this
    model can be trained using only monocular 2D supervision and incorporates an intermediate
    three-dimensional representation of the scene during each denoising step, effectively
    integrating a robust inductive structure into the diffusion process [[123](#bib.bib123)].Xiang
    et al. presented a novel method for generating 3D-aware images using 2D diffusion
    models. Their approach involves a sequential process of generating multiview 2D
    images from different perspectives, ultimately achieving the synthesis of 3D-aware
    images [[124](#bib.bib124)].Furthermore, Liu et al. proposed a framework for changing
    the camera viewpoint of an object using only a single RGB image. Leveraging the
    geometric priors learned by large-scale diffusion models about natural images,
    their framework employs a synthetic dataset to learn the controls for adjusting
    the relative camera viewpoint [[125](#bib.bib125)].Lastly, Chan et al. developed
    a method for generating diverse and realistic novel views of a scene based on
    a single input image. Their approach utilizes a diffusion-based model that incorporates
    3D geometry priors through a latent feature volume. This feature volume captures
    the distribution of potential scene representations and enables the rendering
    of view-consistent images [[126](#bib.bib126)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/986bf5a7e9e0f90ca4de34a6172e999a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Schematic of Transformer, which come from [[128](#bib.bib128)]'
  prefs: []
  type: TYPE_NORMAL
- en: III-A4 Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transformers are a type of neural network architecture that have been widely
    used in natural language processing. They are based on the idea of self-attention,
    which allows the network to learn the relationships between different parts of
    the input and output sequences. Transformers is introduced into the field of computer
    vision in the paper ViT[[127](#bib.bib127)]. Its core is the Attention section
    in Fig. [8](#S3.F8 "Figure 8 ‣ III-A3 Diffusion model ‣ III-A Global Pose ‣ III
    Pose Manipulation ‣ Survey on Controlable Image Synthesis with Deep Learning"),
    and its formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  |'
  prefs: []
  type: TYPE_TB
- en: Leveraging the Transformer architecture for vision applications, several studies
    have explored its potential for synthesizing 3D views. Nguyen-Ha and colleagues
    presented a pioneering approach to synthesizing new views of a scene using a given
    set of input views. Their method employs a transformer-based architecture that
    effectively captures the long-range dependencies among the input views. By using
    a sequential process, the method generates high-quality novel views. This research
    contribution is documented in [[129](#bib.bib129)]. Similarly, Yang and colleagues
    proposed an innovative method for generating viewpoint-invariant 3D shapes from
    a single image. Their approach is based on disentangling learning and parametric
    NURBS surface generation. The method employs an encoder-decoder network augmented
    with a disentangled transformer module. This configuration enables the independent
    learning of shape semantics and camera viewpoints. The output of this comprehensive
    network includes the geometric parameters of the NURBS surface representing the
    3D shape, as well as the camera-viewpoint parameters involving rotation, translation,
    and scaling. Further details of this method can be found in [[130](#bib.bib130)].
    Additionally, Kulhánek and colleagues proposed ViewFormer, an impressive neural
    rendering method that does not rely on NeRF and instead capitalizes on the power
    of transformers. ViewFormer is designed to learn a latent representation of a
    scene using only a few images, and this learned representation enables the synthesis
    of novel views. Notably, ViewFormer can handle complex scenes with varying illumination
    and geometry without requiring any 3D information or ray marching. The specific
    approach and findings of ViewFormer are detailed in [[131](#bib.bib131)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A5 Hybrid NeRF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: a)GAN-based NeRF. Neural Radiance Fields (NeRF) are a novel method for rendering
    images from arbitrary viewpoints, but they suffer from high computational cost
    due to their pixel-wise optimization. Generative Adversarial Networks (GANs) can
    synthesize realistic images in a single forward pass, but they may not preserve
    the view consistency across different viewpoints. Hence, there is a growing interest
    in exploring the integration of NeRF and GAN for efficient and consistent image
    synthesis.Meng et al. presented the GNeRF framework, which combines GANs and NeRF
    reconstruction to generate scenes with unknown or random camera poses [[75](#bib.bib75)].
    Similarly, Zhou et al. introduced CIPS-3D, a generative model that utilizes style
    transfer, shallow NeRF networks, and deep INR networks to represent 3D scenes
    and provide precise control over camera poses [[132](#bib.bib132)].Another approach
    by Xu et al. is GRAF, a generative model for radiance fields that enables high-resolution
    image synthesis while being aware of the 3D shape. GRAF disentangles camera and
    scene properties from unposed 2D images, allowing for the synthesis of novel views
    and modifications to shape and appearance [[133](#bib.bib133)]. Lan et al. proposed
    a self-supervised geometry-aware encoder for style-based 3D GAN inversion. Their
    encoder recovers the latent code of a given 3D shape and enables manipulation
    of its style and geometry attributes [[134](#bib.bib134)].Li et al. developed
    a two-step approach for 3D-aware multi-class image-to-image translation using
    NeRFs. They trained a multi-class 3D-aware GAN with a conditional architecture
    and innovative training strategy. Based on this GAN, they constructed a 3D-aware
    image-to-image translation system [[135](#bib.bib135)]. Shahbazi et al. focused
    on knowledge distillation, proposing a method to transfer the knowledge of a GAN
    trained on NeRF representation to a convolutional neural network (CNN). This enables
    efficient 3D-aware image synthesis [[136](#bib.bib136)].Kania et al. introduced
    a generative model for 3D objects based on NeRFs, which are rendered into 2D novel
    views using a hypernetwork. The model is trained adversarially with a 2D discriminator
    [[137](#bib.bib137)]. Lastly, Bhattarai et al. proposed TriPlaneNet, an encoder
    specifically designed for EG3D inversion. The task of EG3D inversion involves
    reconstructing 3D shapes from 2D edge images [[138](#bib.bib138)].
  prefs: []
  type: TYPE_NORMAL
- en: b)Diffusion model-based NeRF. Likewise, the diffusion model alone fails to produce
    images that are consistent across different viewpoints. Therefore, many researchers
    integrate it with NeRF to synthesize high-quality and view-consistent images.Muller
    et al. proposed DiffRF, which directly generates volumetric radiance fields from
    a set of posed images by using a 3D denoising model and a rendering loss [[139](#bib.bib139)].
    Similarly, Xu et al. proposed NeuralLift-360, a framework that generates a 3D
    object with 360° views from a single 2D photo using a depth-aware NeRF and a denoising
    diffusion model [[140](#bib.bib140)]. Chan et al. proposed a 3D-aware image synthesis
    framework using NeRF and diffusion models, which jointly optimizes a NeRF auto-decoder
    and a latent diffusion model to enable simultaneous 3D reconstruction and prior
    learning from multi-view images of diverse objects [[141](#bib.bib141)]. Lastly,
    Gu et al. proposed NerfDiff, a method for generating realistic and 3D-consistent
    novel views from a single input image. This method distills the knowledge of the
    conditional diffusion model (CDM) into the NeRF by synthesizing and refining a
    set of virtual views at test time, using a NeRF-guided distillation algorithm
    [[142](#bib.bib142)]. These approaches demonstrate the potential of using NeRF
    and diffusion models for 3D scene synthesis, and further research in this area
    is expected to yield even more exciting results.
  prefs: []
  type: TYPE_NORMAL
- en: c)Transformer-based NeRF. Building on the previous work of integrating generative
    adversarial networks (GANs) and neural radiance fields (NeRFs), some researchers
    have explored the possibility of using Transformer models and NeRFs to generate
    3D images that are consistent across different viewpoints. Wang et al. proposed
    a method that can handle complex scenes with dynamic objects and occlusions, and
    can generalize to unseen scenes without fine-tuning. The key idea is to use a
    transformer to learn a global latent representation of the scene, which is then
    used to condition a NeRF model that renders novel views [[143](#bib.bib143)].
    Similarly, Lin et al. proposed a method for novel view synthesis from a single
    unposed image using NeRF and vision transformer (ViT). The method leverages both
    global and local image features to form a 3D representation of the scene, which
    is then used to render novel views by a multi-layer perceptron (MLP) network [[144](#bib.bib144)].
    Finally, Liu et al. proposed a method for visual localization using a conditional
    NeRF model. The method can estimate the 6-DoF pose of a query image given a sparse
    set of reference images and their poses [[145](#bib.bib145)]. These methods demonstrate
    the potential of NeRFs and transformers in addressing challenging problems in
    computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Local Pose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-B1 GAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Liao et al. proposed a novel framework consisting of two components for learning
    generative models that can achieve this goal. The first component is a 3D generator
    that learns to reconstruct the 3D shape and appearance of an object from a single
    image, while the second component is a 2D generator that learns to render the
    3D object into a 2D image. This framework can generate high-quality images with
    controllable factors such as pose, shape, and appearance [[146](#bib.bib146)].Nguyen-Phuoc
    et al. proposed BlockGAN, a novel image generative model that can create realistic
    images of scenes composed of multiple objects. BlockGAN learns to generate 3D
    features for each object and combine them into a 3D scene representation. The
    model then renders the 3D scene into a 2D image, taking into account the occlusion
    and interaction between objects, such as shadows and lighting. BlockGAN can manipulate
    the pose and identity of each object independently while preserving image quality
    [[147](#bib.bib147)].Pan et al. proposed a novel framework that can reconstruct
    3D shapes from 2D image GANs without any supervision or prior knowledge. The method
    can generate realistic and diverse 3D shapes for various object categories, and
    the reconstructed shapes are consistent with the 2D images generated by the GANs.
    The recovered 3D shapes allow high-quality image editing such as relighting and
    object rotation [[148](#bib.bib148)].Tewari et al. proposed a novel 3D generative
    model that can learn to separate the geometry and appearance factors of objects
    from a dataset of monocular images. The model uses a non-rigid deformable scene
    formulation, where each object instance is represented by a deformed canonical
    3D volume. The model can also compute dense correspondences between images and
    embed real images into its latent space, enabling editing of real images [[149](#bib.bib149)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 NeRF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Niemeyer and Geiger introduced GIRAFFE, a deep generative model that can synthesize
    realistic and controllable images of 3D scenes. The model represents scenes as
    compositional neural feature fields that encode the shape and appearance of individual
    objects as well as the background. The model can disentangle these factors from
    unstructured and unposed image collections without any additional supervision.
    With GIRAFFE, individual objects in the scene can be manipulated by translating,
    rotating, or changing their appearance, as well as changing the camera pose[[29](#bib.bib29)].Yang
    et al. proposed a neural scene rendering system called OC-Nerf that learns an
    object-compositional neural radiance field for editable scene rendering. OC-Nerf
    consists of a scene branch and an object branch, which encode the scene and object
    geometry and appearance, respectively. The object branch is conditioned on learnable
    object activation codes that enable object-level editing such as moving, adding,
    or rotating objects[[25](#bib.bib25)].Kobayashi et al. proposed a method to enable
    semantic editing of 3D scenes represented by neural radiance fields (NeRFs). The
    authors introduced distilled feature fields (DFFs), which are 3D feature descriptors
    learned by transferring the knowledge of pre-trained 2D image feature extractors
    such as CLIP-LSeg or DINO. DFFs allow users to query and select specific regions
    or objects in the 3D space using text, image patches, or point-and-click inputs.
    The selected regions can then be edited in various ways, such as rotation, translation,
    scaling, warping, colorization, or deletion[[150](#bib.bib150)].Zhang et al. introduced
    Nerflets, a new approach to represent 3D scenes from 2D images using local radiance
    fields. Unlike prior approaches that rely on global implicit functions, Nerflets
    partition the scene into a collection of local coordinate frames that encode the
    structure and appearance of the scene. This enables efficient rendering and editing
    of complex scenes with high fidelity and detail. Nerflets can manipulate the object’s
    orientation, position, and size, among other operations[[151](#bib.bib151)].Finally,
    Zheng et al. proposed EditableNeRF, a method that allows users to edit dynamic
    scenes modeled by neural radiance fields (NeRF) with key points. The method can
    handle topological changes and generate novel views from a single camera input.
    The key points are detected and optimized automatically by the network, and users
    can drag them to modify the scene. These approaches provide various means for
    3D scene synthesis and editing, including manipulating objects, changing camera
    pose, selecting and editing specific regions or objects, and handling topological
    changes[[152](#bib.bib152)].
  prefs: []
  type: TYPE_NORMAL
- en: IV Structure Manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Global Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IV-A1 Editting point cloud
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A depth map is a representation of the distance between a scene and a reference
    point, such as a camera. It can be used to create realistic effects such as depth
    of field, occlusion, and parallax[[153](#bib.bib153)]. Chen et al. proposed a
    novel method called SPIDR for representing and manipulating 3D objects using neural
    point fields (NPFs) and signed distance functions (SDFs) [[154](#bib.bib154)].
    The method combines explicit point cloud and implicit neural representations to
    enable high-quality mesh and surface reconstruction for object deformation and
    lighting estimation. With the trained SPIDR model, various geometric edits can
    be applied to the point cloud representation, which can be used for image editing.Zhang
    et al. introduced a new method for rendering point clouds with frequency modulation,
    which enables easy editing of shape and appearance [[155](#bib.bib155)]. The method
    converts point clouds into a set of frequency-modulated signals that can be rendered
    efficiently using Fourier analysis. The signals can also be manipulated in the
    frequency domain to achieve various editing effects, such as deformation, smoothing,
    sharpening, and color adjustment.Chen et al. also proposed NeuralEditor, a novel
    method for editing neural radiance fields (NeRFs) for shape editing tasks [[156](#bib.bib156)].
    The method uses point clouds as the underlying structure to construct NeRFs and
    renders them with a new scheme based on K-D tree-guided voxels. NeuralEditor can
    perform shape deformation and scene morphing by mapping points between point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Editting depth map
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chen et al. introduced the Visual Object Networks (VON) framework, which enables
    the disentangled learning of 3D object representations from 2D images. This framework
    comprises three modules, namely a shape generator, an appearance generator, and
    a rendering network. By manipulating the generators, VON can perform a range of
    tasks, including shape manipulation, appearance transfer, and novel view synthesis
    [[157](#bib.bib157)].Mirzaei et al. proposed a reference-guided controllable inpainting
    method for neural radiance fields (NeRFs), which allows for the synthesis of novel
    views of a scene with missing regions. The method employs a reference image to
    guide the inpainting process, and a user interface that enables the user to adjust
    the degree of blending between the reference and the original NeRF [[158](#bib.bib158)].Yin
    et al. introduced OR-NeRF, a novel pipeline that can remove objects from 3D scenes
    using point or text prompts on a single view. This pipeline leverages a points
    projection strategy, a 2D segmentation model, 2D inpainting methods, and depth
    supervision and perceptual loss to achieve better editing quality and efficiency
    than previous works [[159](#bib.bib159)]. Chen et al. proposed a visual comfort
    aware-reinforcement learning (VCARL) method for depth adjustment of stereoscopic
    3D images. This method aims to improve the visual quality and comfort of 3D images
    by learning a depth adjustment policy from human feedback [[160](#bib.bib160)].
    These advancements offer various means of manipulating objects, adjusting depth,
    and generating novel views, ultimately enhancing the quality and realism of 3D
    scene synthesis and editing.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Local Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IV-B1 GAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, there have been significant advancements in the field of 3D
    scene inpainting and editing using generative adversarial networks (GANs). Jheng
    et al. proposed a dual-stream GAN for free-form 3D scene inpainting. The network
    comprises two streams, namely a depth stream and a color stream, which are jointly
    trained to inpaint the missing regions of a 3D scene. The depth stream predicts
    the depth map of the scene, while the color stream synthesizes the color image.
    This approach enables the removal of objects using existing 3D editing tools [[161](#bib.bib161)].Another
    recent development in GAN training is the introduction of LinkGAN, a regularizer
    proposed by Zhu et al. that links some latent axes to image regions or semantic
    categories. By resampling partial latent codes, this approach enables local control
    of GAN generation [[30](#bib.bib30)].Wang et al. proposed a novel method for synthesizing
    realistic images of indoor scenes with explicit camera pose control and object-level
    editing capabilities. This method builds on BlobGAN, a 2D GAN that disentangles
    individual objects in the scene using 2D blobs as latent codes. To extend this
    approach to 3D scenes, the authors introduced 3D blobs, which capture the 3D nature
    of objects and allow for flexible manipulation of their location and appearance
    [[162](#bib.bib162)]. These recent advancements in GAN-based 3D scene inpainting
    and editing have the potential to significantly improve the quality and realism
    of synthesized scenes.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 NeRF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Liu et al. [[104](#bib.bib104)] introduced Neural Sparse Voxel Fields (NSVF),
    which combines neural implicit functions with sparse voxel octrees to enable high-quality
    novel view synthesis from a sparse set of input images, without requiring explicit
    geometry reconstruction or meshing. Gu et al. [[163](#bib.bib163)] introduced
    StyleNeRF, a method that enables camera pose manipulation for synthesizing high-resolution
    images with strong multi-view coherence and photorealism. Wang et al. [[164](#bib.bib164)]
    introduced CLIP-NeRF, a method for manipulating 3D objects represented by neural
    radiance fields (NeRF) using text or image inputs. Kania et al. [[165](#bib.bib165)]
    proposed a novel method for manipulating neural 3D representations of scenes beyond
    novel view rendering by allowing the user to specify which part of the scene they
    want to control with mask annotations in the training images. Lazova et al. [[166](#bib.bib166)]
    proposed a novel method for performing flexible, 3D-aware image content manipulation
    while enabling high-quality novel view synthesis by combining scene-specific feature
    volumes with a general neural rendering network. Yuan et al. [[167](#bib.bib167)]
    proposed a method for user-controlled shape deformation of scenes represented
    by implicit neural rendering, especially Neural Radiance Field (NeRF). Sun et al.
    [[168](#bib.bib168)] proposed NeRFEditor, a learning framework for 3D scene editing
    that uses a pre-trained StyleGAN model and a NeRF model to generate stylized images
    from a 360-degree video input. Wang et al. [[169](#bib.bib169)] proposed a novel
    method for image synthesis of topology-varying objects using generative deformable
    radiance fields (GDRFs). Tertikas et al. [[170](#bib.bib170)] proposed PartNeRF,
    a novel part-aware generative model for editable 3D shape synthesis that does
    not require any explicit 3D supervision. Bao et al. [[171](#bib.bib171)] proposed
    SINE, a novel approach for editing a neural radiance field (NeRF) with a single
    image or text prompts. Cohen-Bar et al. [[172](#bib.bib172)] proposed a novel
    framework for synthesizing and manipulating 3D scenes from text prompts and object
    proxies. Finally, Mirzaei et al. [[173](#bib.bib173)] proposed a novel method
    for reconstructing 3D scenes from multiview images by leveraging neural radiance
    fields (NeRF) to model the geometry and appearance of the scene, and introducing
    a segmentation network and a perceptual inpainting network to handle occlusions
    and missing regions. These methods represent significant progress towards the
    goal of enabling high-quality, user-driven 3D scene synthesis and editing.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B3 Diffusion model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Avrahami et al. [[174](#bib.bib174)] introduced a method for local image editing
    based on natural language descriptions and region-of-interest masks. The method
    uses a pretrained language-image model (CLIP) and a denoising diffusion probabilistic
    model (DDPM) to produce realistic outcomes that conform to the text input. It
    can perform various editing tasks, such as object addition, removal, replacement,
    or modification, background replacement, and image extrapolation.Nichol et al.
    [[175](#bib.bib175)] proposed GLIDE, a diffusion-based model for text-conditional
    image synthesis and editing. This method uses a guidance technique to trade off
    diversity for fidelity and produces photorealistic images that match the text
    prompts.Couairon et al. [[176](#bib.bib176)] proposed DiffEdit, a method that
    uses text-conditioned diffusion models to edit images based on text queries. It
    can automatically generate a mask that highlights the regions of the image that
    need to be changed according to the text query. It also uses latent inference
    to preserve the content in those regions. DiffEdit can produce realistic and diverse
    semantic image edits for various text prompts and image sources.Sella et al. [[177](#bib.bib177)]
    proposed Vox-E, a novel framework that uses latent diffusion models to edit 3D
    objects based on text prompts. It takes 2D images of a 3D object as input and
    learns a voxel grid representation of it. It then optimizes a score distillation
    loss to align the voxel grid with the text prompt while regularizing it in 3D
    space to preserve the global structure of the original object. Vox-E can create
    diverse and realistic edits.Haque et al. [[178](#bib.bib178)] proposed a novel
    method for editing 3D scenes with natural language instructions. The method leverages
    a neural radiance field (NeRF) representation of the scene and a transformer-based
    model that can parse the instructions and modify the NeRF accordingly. The method
    can perform various editing tasks, such as changing the color, shape, position,
    and orientation of objects, as well as adding and removing objects, with high
    fidelity and realism.Lin et al. [[179](#bib.bib179)] proposed CompoNeRF, a novel
    method for text-guided multi-object compositional NeRF with editable 3D scene
    layout. CompoNeRF can synthesize photorealistic images of complex scenes from
    natural language descriptions and user-specified camera poses. It can also edit
    the 3D layout of the scene by manipulating the objects’ positions, orientations,
    and scales. These methods have shown promising results in advancing the field
    of image and 3D object editing using natural language descriptions, and they have
    the potential to be applied in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: V Illumination Manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: controlable image generation refers to the use of technology to generate images
    and to constrain and adjust the generation process so that the generated images
    meet specific requirements.By guiding external conditions or manipulating and
    adjusting the code, it is possible to trim a certain area or attribute of the
    image while leaving other areas or attributes unchanged.To solve the low-level
    image generation problem, we analyze the image generation for different conditions,
    lighting being one of them, and summarize the algorithms for each solution under
    different lighting conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Inverse rendering.Currently,neural rendering is applied to scene restruction.One
    approach is to capture photometric appearance variations in in-the-wild data,
    decomposing the scene into image-dependent shared components[[180](#bib.bib180)].
  prefs: []
  type: TYPE_NORMAL
- en: Another very important type of rendering is inverse rendering.The inverse rendering
    of objects under completely unknown capture conditions is a fundamental challenge
    in computer vision and graphics.This challenge is especially acute when the input
    image is captured in a complex and changing environment.Without using the nerf
    method,Mark Boss et al. proposed a join optimization framework to estimate the
    shape,BRDF,per-image camera pose and illumination[[181](#bib.bib181)].
  prefs: []
  type: TYPE_NORMAL
- en: Changwoon Choi et al. proposed IBL-NeRF also based on rendering .This method’s
    inverse rendering extends the original NeRF formulation to capture the spatial
    variation of lighting within the scene volume, in addition to surface properties.
    Specifically, the scenes of diverse materials are decomposed into intrinsic components
    for image-based ren dering, namely, albedo, roughness, surface normal, irradiance,
    and prefiltered radiance.All of the components are inferred as neural images from
    MLP, and model large-scale general scenes[[182](#bib.bib182)].
  prefs: []
  type: TYPE_NORMAL
- en: However, NeRF-based methods encode shape,reflectance and illumination implicitly
    and this makes it challenging for users to manipulate these properties in the
    rendered images explicitly.So a new hybrid SDF-based 3D neural representation
    is generated, capable of rendering scene deformations and lighting more accurately.This
    neural representation also adds a new SDF regularization.The disadvantage of this
    approach is that it sacrifices rendering quality. In reverse rendering, high render
    quality is often at odds with accurate lighting decomposition, as shadows and
    lighting can easily be misinterpreted as textures. Therefore, rendering quality
    still requires a concerted effort of surface reconstruction and reverse rendering[[154](#bib.bib154)].
    While dynamic Neural Radiation Field (NeRF) is a powerful algorithm capable of
    rendering photo-realistic novel view images from a monocular RGB video of a dynamic
    scene. But dynamic NeRF does not model the change of the reflected color during
    the warping.This is one of its drawbacks.To address this problem in rendering,
    Zhiwen Yan et al. allowed specularly reflective surfaces of different poses to
    maintain different reflective colors when mapped to the common canonical space
    by reformulating the neural radiation field function as conditional on the position
    and orientation of the surface in the observation space.This method more accurately
    reconstructs and renders dynamic specular scenes[[183](#bib.bib183)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The inverse rendering objective function of this method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathcal{L}_{\text{render }}+\mathcal{L}_{\text{pref }}+\mathcal{L}_{\text{prior
    }}+\lambda_{I,\text{ reg }}\mathcal{L}_{I,\text{ reg }}$ |  |'
  prefs: []
  type: TYPE_TB
- en: $\mathcal{L}_{\text{render }}$ and $\mathcal{L}_{\text{pref }}$ are rendering
    losses to match the rendered images with the input images.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explain each of these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{render }}=\left\&#124;L_{o}(r)-\hat{L}_{o}(r)\right\&#124;_{2}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This is for each pixel of the camera light. $r$ represents a single piexl.where
    $L_{o}$ is our nated radiance and $\hat{L}_{o}$ is ground truth radiance.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{pref}}=\sum_{j}\left\&#124;L_{\text{pref }}^{j}(r)-L_{\mathrm{G}}^{j}(r)\right\&#124;_{2}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This is the rendering loss of pre-filtered radiation.$L_{\text{pref }}^{j}(r)$
    is inferred prefiltered radiance of $j^{(}th)$ level and $L_{\mathrm{G}}^{j}(r)$
    is the radiance convolved with jth level Gaussian convolution,where $L_{\mathrm{G}}^{0}$
    = L.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{prior }}=\&#124;a(r)-\hat{a}(r)\&#124;_{2}^{2}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: The equation encourages our inferred albedo $a$ to match the pseudo albedo.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{I,\text{ reg }}=\&#124;I(r)-\mathbb{E}[\hat{I}]\&#124;_{2}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This is the irradiance regularization loss,where $\mathbb{E}[\hat{I}]$ is the
    mean of irradiance (shading) values in training set images.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Global Illumination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The absence of ideal light and the fact that the studied objects are in an unfavorable
    environment such as deflection, movement, darkness, and high interference can
    lead to under-illuminated, single irradiated light source, and complex illumination
    of the acquired images, all of which can degrade the performance of the final
    image generation.Next, we will review the various ways to deal with these aspects.
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Brightness level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The use of illumination normalization generative adversarial network - IN-GAN
    can be well generalized to images with less illumination variations.The method
    combines deep convolutional neural networks and generative adversarial networks
    to normalize the illumination of color or grayscale face images, then train feature
    extractors and classifiers, and process both frontal and non-frontal face images
    illumination.The method can be extended to other areas, not only for face image
    generation.However, it cannot preserve more texture details and has some limitations.Meanwhile,
    the training model is conducted with well-controlled illumination variations,
    which can deal with poorly controlled illumination variation to a certain extent,
    but there are still limitations to the study of other features and geometric structures
    in realistic and complex environments, etc.It can be further investigated whether
    the model can work better if the model is trained under complex lighting changes.[[184](#bib.bib184)].
  prefs: []
  type: TYPE_NORMAL
- en: When the data set is insufficient, an unsupervised approach can be used for
    this.For example, for low-light scenes, the unsupervised Aleth-NeRF method is
    used to learn directly from dark images.The algorithm is mainly a multi-view synthesis
    method that takes a low-light scene as input and renders a normally illuminated
    scene.However, a model needs to be trained specifically for different scenes,
    and also does not handle non-uniform lighting conditions well[[185](#bib.bib185)].
  prefs: []
  type: TYPE_NORMAL
- en: Also as far as the results are concerned, images taken in low-light scenes are
    affected by distracting factors such as blur and noise.For this type of problem,
    a hybrid architecture based on Retinex theory and Generative Adversarial Network
    (GAN) can be used to deal with it.For image vision tasks in the dark or under
    low light conditions, the image is first decomposed into a light image and a reflection
    image, and then the enhancement part is used to generate a high quality clear
    image, starting from minimizing the effect of blurring or noise generation.The
    method introduces Structural Similarity loss to avoid the side effect of blur.But
    real-life eligible low level and high level images may not be easily acquired
    and have the shortage of input.Also to maximize the performance of the algorithm,
    a sufficient size of data set is required.The data obtained after training also
    has the problem of real-time, which is not enough to meet real-life needs.In general,
    the algorithm is only from the perspective of solving image blurring and noise,
    making the impact of these two minimal, other aspects of the problem still exists
    more, need to further optimize the network structure.[[186](#bib.bib186)]This
    class of problems can also be explored by exploring multiple diffusion spaces
    to estimate the light component, which is used as bright pixels to enhance the
    shimmering image based on the maximum diffusion value.Generates high-fidelity
    images without significant distortion, minimizing the problem of noise amplification[[187](#bib.bib187)].Later,
    the conditional diffusion implicit model is utilized in DiFaReli’s method (DDIM)
    to decode the coding of decomposed light.Puntawat Ponglertnapakorn et al. proposed
    a novel conditioning technique that eases the modeling of the complex interaction
    between light and geometry by using a rendered shading reference to spatially
    modulate the DDIM.This method allows for single-view face re-illumination in the
    wild.However, this method has limitations in eliminating shadows cast by external
    objects and is susceptible to image ambiguity[[188](#bib.bib188)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary,the full objectives of this method are as followed:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(G,D)=$ | $\displaystyle L_{\text{adversarial }}(G,D)+\lambda_{1}\times
    L_{\text{content }}(\mathrm{G})+\lambda_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\times L_{l1}(\mathrm{G})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1},\lambda_{2}$ are weight parameters respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '$L_{\text{adversarial }}$, $L_{\text{content }}$ and $L_{l1}$ are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle L_{\text{adversarial }}(G,D)=E_{x}[\log D(x)]+E_{G(x)}[\log(1-D(G(x)))]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle L_{\text{content }}(G)=\&#124;F(y)-F(G(x))\&#124;_{1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle L_{l1}(G)=\&#124;y-G(x)\&#124;_{1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathrm{x}$ denotes input image, whereas $\mathrm{y}$ is the target image,
    $\mathrm{F}$ means feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Light source movement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A method of generating scenes with a sense of reality from captured object images
    can be used when the light is moving.On the basis of neural radiation fields (NeRFs),
    the bulk density of the scene and the radiance of the directional emission are
    simulated.A representation of each object light transmission is implicitly simulated
    using illumination and view-related neural networks.This approach can cope with
    the problem of light movement without retraining the model[[189](#bib.bib189)].
  prefs: []
  type: TYPE_NORMAL
- en: V-A3 Uneven illumination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the characteristics of light inhomogeneity in the environment, it is possible
    to use the light correction network framework, UDoc-GAN, to solve it.The main
    thing is to convert uncertain normal to abnormal image panning to deterministic
    image panning with different levels of ambient light for learning guidance.In
    contrast, Aleth-NeRF cannot handle non-uniform illumination or shadow images.Meanwhile,
    UDoc-GAN algorithm is more computationally efficient in the inference stage and
    closer to realistic requirements[[190](#bib.bib190)].
  prefs: []
  type: TYPE_NORMAL
- en: V-A4 Shadow ray
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Jingwang Ling et al. monitored the camera illumination between the scene and
    multi-view image planes and noticed shadow rays, which led to a new shadow ray
    supervision scheme. This scheme optimizes the samples and ray positions along
    the rays. By supervising the shadow rays to achieve controllable illumination,
    a neural SDF network for single-view scene reproduction under multi-illumination
    conditions is finally constructed.However,the method is only applicable to point
    and parallel light sources and has obvious requirements for the position of the
    light source. The implementation of the method is also based on a simple environment
    where the scene is not illuminate[[191](#bib.bib191)].
  prefs: []
  type: TYPE_NORMAL
- en: V-A5 Complex light variation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Also, for uncontrolled complex environment settings from which images are acquired,
    the NeRF-OSR algorithm enables the generation of new views and new illumination.This
    is a solution for image generation in complex environments.Solving some fuzzy
    performance from the perspective of optimizing this algorithm can be an interesting
    future research direction.For example, resolving inaccuracies in geometric estimation,
    incorporating more priori knowledge of the outdoor scenes, etc[[192](#bib.bib192)].Later,
    for this problem, Higuera C et al. proposed a solution to the complex problem
    of light variation by reducing the perceptual differences in vision and using
    a probabilistic diffusion model to capture light.The method is implemented based
    on simulated data and can address the limitations of large-scale data.Of course,
    the method suffers from the problem of computation time, especially in the denoising
    process which consumes more time[[193](#bib.bib193)].This is especially true for
    reflections in complex environments, for example, with glass and mirrors.YuanChen
    Guo et al. introduced NeRFReN for simulating scenes with reflections, mainly by
    dividing the scene into transmission and reflection components and modeling these
    two components with independent neural radiation fields.This approach has far-reaching
    implications for further research in scene understanding and neural editing.However,
    this method does not consider modeling curved reflective surfaces and multiple
    non-coplanar reflective surfaces[[194](#bib.bib194)].
  prefs: []
  type: TYPE_NORMAL
- en: V-B Local Illumination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-B1 Reflectance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally speaking, reflected light can be divided into three components, namely
    ambient reflection, diffuse reflection and specular reflection.The different media
    materials that cause the reflected light will show different lighting cues in
    the exposure.An omnidirectional illumination method trains deep neural networks
    on videos with automatic exposure and white balance to match real images with
    predicted illumination based on image re-illumination and then regression from
    the background[[195](#bib.bib195)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The method focuses on minimizing the reconstructed illumination loss function
    and adding an adversarial loss.And the reconstructed illumination loss and the
    adversarial loss are as followed:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{\mathrm{rec}}=\sum_{b=0}^{2}\lambda_{b}\left\&#124;\hat{M}\odot\left(\Lambda\left(\hat{I}_{b}\right)^{\frac{1}{\gamma}}-\Lambda\left(I_{b}\right)\right)\right\&#124;_{1}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In this formulation, the linear rendering of the shear is $\gamma$-encoded with
    $\gamma$ to match $I$.$\hat{M}$ represents a binary mask.$\lambda_{b}$ represents
    an optional weight.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{\mathrm{adv}}=$ | $\displaystyle\log D\left(\Lambda\left(I_{c}\right)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\log\left(1-D\left(\Lambda\left(\sum_{\theta,\phi}R(\theta,\phi)e^{G(x;\theta,\phi)}\right)^{\frac{1}{\gamma}}\right)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In this formulation, the $D$ represents an auxiliary discriminator network,the
    $G$ represents the generator,the $x$ represents input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, combining the two yields the following common objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $G^{*}=\arg\min_{G}\max_{D}\left(1-\lambda_{\mathrm{rec}}\right)\mathbf{E}\left[L_{\mathrm{adv}}\right]+\lambda_{\mathrm{rec}}\mathbf{E}\left[L_{\mathrm{rec}}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Of course, there are certainly real-life situations where the reflectance is
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: In illumination variation, there is also a cluster optimization method based
    on neural reflection field using reflection iteration to solve the problem of
    similar reflectance of different instances from the perspective of hierarchical
    clustering.However, there still exists the challenge of facing complex scenarios
    that do not conform to the unsupervised intrinsic prior, and solutions to such
    problems need to be proposed[[196](#bib.bib196)].
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 Radiance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different mediums have different radiance to light, using a web-based query
    light integration network on which reflection decomposition is performed.The algorithm
    captures changing illumination, enabling more accurate new view compositing and
    re-illumination. Finally, fast and practical distinguishable rendering areas are
    implemented.The algorithm can also estimate the shape and BRDF of the objects
    in the image, which is a point of superiority over other algorithms.However, this
    method has some limitations in the study of mutual reflection.In particular, an
    effective treatment of the interactions between all effects could be a future
    research direction[[197](#bib.bib197)].
  prefs: []
  type: TYPE_NORMAL
- en: VI Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3D controllable image synthesis has many potential applications in various domains,
    such as entertainment, industry, and security.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Entertainment Application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: a)Video games. 3D image synthesis can create immersive and interactive virtual
    worlds for gamers to explore and enjoy. It can also enhance the realism and variety
    of characters, objects and environments in the game[[198](#bib.bib198), [199](#bib.bib199)].
  prefs: []
  type: TYPE_NORMAL
- en: b)Movies and TV shows. 3D image synthesis can produce stunning visual effects
    and animations for movies and TV shows. It can also enable the creation of digital
    actors, creatures and scenarios that would be impossible or impractical to film
    in real life[[200](#bib.bib200), [201](#bib.bib201)].
  prefs: []
  type: TYPE_NORMAL
- en: c)Virtual reality and augmented reality. 3D image synthesis can generate realistic
    and immersive virtual experiences for users who wear VR or AR devices. It can
    also augment the real world with digital information and graphics that enhance
    the user’s perception and interaction[[202](#bib.bib202)].
  prefs: []
  type: TYPE_NORMAL
- en: d)Art and design. 3D image synthesis can enable artists and designers to express
    their creativity and vision in new ways. It can also facilitate the creation and
    presentation of 3D artworks, models and prototypes[[203](#bib.bib203)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Entertainment Industry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: a)Product design and prototyping. By using 3D image synthesis, designers can
    visualize and test different aspects of their products, such as shape, color,
    texture, functionality and performance, before manufacturing them. This can save
    time and money, as well as improve the quality and innovation of the products[[204](#bib.bib204)].
  prefs: []
  type: TYPE_NORMAL
- en: b)Training and simulation. By using 3D image synthesis, trainers can create
    realistic and immersive scenarios for workers to practice their skills and learn
    new procedures. For example, 3D image synthesis can be used to simulate hazardous
    environments, such as oil rigs, mines or nuclear plants, where workers can train
    safely and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: c)Inspection and quality control. By using 3D image synthesis, inspectors can
    detect and analyze defects and errors in products or processes, such as cracks,
    leaks or misalignments. For example, 3D image synthesis can be used to inspect
    complex structures, such as bridges, pipelines or aircrafts, where human inspection
    may be difficult or dangerous[[205](#bib.bib205), [206](#bib.bib206)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Entertainment Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: a)Biometric authentication. 3D image synthesis can be used to generate realistic
    face images from 3D face scans or facial landmarks, which can be used for identity
    verification or access control. For example, Face ID on iPhone uses 3D image synthesis
    to project infrared dots on the user’s face and match them with the stored 3D
    face model[[207](#bib.bib207), [208](#bib.bib208)].
  prefs: []
  type: TYPE_NORMAL
- en: b)Forensic analysis. 3D image synthesis can be used to reconstruct crime scenes
    or evidence from partial or noisy data, such as surveillance videos, witness sketches
    or DNA samples. For example, Snapshot DNA Phenotyping uses 3D image synthesis
    to predict the facial appearance of a person from their DNA[[209](#bib.bib209)].
  prefs: []
  type: TYPE_NORMAL
- en: c)Counter-terrorism. 3D image synthesis can be used to detect and prevent potential
    threats by generating realistic scenarios or simulations based on intelligence
    data or risk assessment. For example, the US Department of Defense uses 3D image
    synthesis to create virtual environments for training and testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: d)Cybersecurity. 3D image synthesis can be used to protect sensitive data or
    systems from unauthorized access or manipulation by generating fake or distorted
    images that can fool attackers or malware. For example, Adversarial Robustness
    Toolbox uses 3D image synthesis to generate adversarial examples that can evade
    or mislead deep learning models[[210](#bib.bib210)].
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper we have given a comprehensive survey of the emerging progress
    on 3D controllable image synthesis. We discussed a variety of 3D controllable
    image synthesis aspects according to their low-level vision cues. The survey reviewed
    important progress made on 3D datasets, geometrically controllable image synthesis,
    photometrically controllable image synthesis and related applications. Moreover,
    the global and local synthesis approaches are separately summarized in each controllable
    mode to further distinguish diverse synthesis tasks. Our ultimate goal is to provide
    a useful guide for the researchers and developers who would be interested to synthesizing
    and editing the image from the low-level 3D prompts. We categorize literatures
    mainly according to controllable 3D cues since they directly decide our synthesis
    tasks and abilities. However, there are still other non-rigid 3D cues such as
    body kinematic joints and elastic shape deformation which are not covered by this
    survey. In the future, we expect that more explainable controllable cues can be
    explored from current diffusion and neural radiance fields models by advanced
    latent decomposition or inverse rendering techniques. Together with the semantic-level
    controllable image synthesis, the low-level 3D controllable image synthesis and
    editing can generate more incredible and reliable images in our lives.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution
    image synthesis with latent diffusion models,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    10 684–10 695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, “A comprehensive
    survey of ai-generated content (aigc): A history of generative ai from gan to
    chatgpt,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch,
    D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel, J. Davis, D. Demszky,
    C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei,
    C. Finn, T. Gale, L. E. Gillespie, K. Goel, N. D. Goodman, S. Grossman, N. Guha,
    T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. F.
    Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani,
    O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak,
    M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D.
    Manning, S. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan,
    B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr,
    I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan,
    R. Reich, H. Ren, F. Rong, Y. H. Roohani, C. Ruiz, J. Ryan, C. R’e, D. Sadigh,
    S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, A. Tamkin, R. Taori, A. W.
    Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga,
    J. You, M. A. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou,
    and P. Liang, “On the opportunities and risks of foundation models,” *ArXiv*,
    2021\. [Online]. Available: https://crfm.stanford.edu/assets/report.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] L. Zhang and M. Agrawala, “Adding conditional control to text-to-image
    diffusion models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] X. Wang, L. Xie, C. Dong, and Y. Shan, “Real-esrgan: Training real-world
    blind super-resolution with pure synthetic data,” in *International Conference
    on Computer Vision Workshops (ICCVW)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    in *Advances in Neural Information Processing Systems*, H. Larochelle, M. Ranzato,
    R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020,
    pp. 6840–6851. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. C. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] H. Huang, P. S. Yu, and C. Wang, “An introduction to image synthesis with
    generative adversarial nets,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using convolutional
    neural networks,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016, pp. 2414–2423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Agarwal, N. Snavely, I. Simon, S. M. Seitz, and R. Szeliski, “Building
    rome in a day,” in *2009 IEEE 12th International Conference on Computer Vision*,
    2009, pp. 72–79.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] L. Yang, T. Yendo, M. Panahpour Tehrani, T. Fujii, and M. Tanimoto, “Probabilistic
    reliability based view synthesis for ftv,” in *2010 IEEE International Conference
    on Image Processing*, 2010, pp. 1785–1788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Zheng, G. Zeng, H. Li, Q. Cai, and J. Du, “Colorful 3d reconstruction
    at high resolution using multi-view representation,” *Journal of Visual Communication
    and Image Representation*, vol. 85, p. 103486, 2022\. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1047320322000402'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti,
    T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson,
    L. Schmidt, R. Kaczmarczyk, and J. Jitsev, “Laion-5b: An open large-scale dataset
    for training next generation image-text models,” *ArXiv*, vol. abs/2210.08402,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. M. Mohammad and S. Kiritchenko, “An annotated dataset of emotions evoked
    by art,” in *Proceedings of the 11th Edition of the Language Resources and Evaluation
    Conference (LREC-2018)*, Miyazaki, Japan, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Huang, Q. Li, J. Liao, L. Liu, and L. Li, “An overview of controllable
    image synthesis: Current challenges and future trends,” *SSRN Electronic Journal*,
    2022\. [Online]. Available: https://ssrn.com/abstract=4187269'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Tsirikoglou, G. Eilertsen, and J. Unger, “A survey of image synthesis
    methods for visual machine learning,” *Computer Graphics Forum*, vol. 39, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Haas, S. Graßhof, and S. S. Brandt, “Controllable gan synthesis using
    non-rigid structure-from-motion,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Zhang, A. Siarohin, Y. Liu, H. Tang, N. Sebe, and W. Wang, “Training
    and tuning generative neural radiance fields for attribute-conditional 3d-aware
    face generation,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Ko, K. Cho, D. Choi, K. Ryoo, and S. Kim, “3d gan inversion with pose
    optimization,” *WACV*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Yang, W. Wang, B. Peng, and J. Dong, “Designing a 3d-aware stylenerf
    encoder for face editing,” in *ICASSP 2023 - 2023 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang,
    T. F. Y. Vicente, T. Dideriksen, H. Arora, M. Guillaumin, and J. Malik, “Abo:
    Dataset and benchmarks for real-world 3d object understanding,” in *CVPR 2022*,
    2022\. [Online]. Available: https://www.amazon.science/publications/abo-dataset-and-benchmarks-for-real-world-3d-object-understanding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Yang, Y. Zhang, Y. Xu, Y. Li, H. Zhou, H. Bao, G. Zhang, and Z. Cui,
    “Learning object-compositional neural radiance field for editable scene rendering,”
    in *International Conference on Computer Vision (ICCV)*, October 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] X. Yan, Z. Yuan, Y. Du, Y. Liao, Y. Guo, Z. Li, and S. Cui, “Clevr3d:
    Compositional language and elementary visual reasoning for question answering
    in 3d real-world scenes,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proc. Computer
    Vision and Pattern Recognition (CVPR), IEEE*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Google, “Realestate10k: A large dataset of camera trajectories from video
    clips,” 2018\. [Online]. Available: https://google.github.io/realestate10k/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. Niemeyer and A. Geiger, “Giraffe: Representing scenes as compositional
    generative neural feature fields,” in *Proc. IEEE Conf. on Computer Vision and
    Pattern Recognition (CVPR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Zhu, C. Yang, Y. Shen, Z. Shi, D. Zhao, and Q. Chen, “Linkgan: Linking
    gan latents to pixels for controllable image synthesis,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu, “Shapenet: An
    information-rich 3d model repository,” *ArXiv*, vol. abs/1512.03012, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *2012 IEEE Conference on Computer Vision
    and Pattern Recognition*, 2012, pp. 3354–3361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M.
    Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao,
    and D. Batra, “Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments
    for embodied AI,” in *Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track (Round 2)*, 2021\. [Online]. Available:
    https://openreview.net/forum?id=-v4OuqNs5P'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame
    stereo correspondence algorithms,” *Int. J. Comput. Vision*, vol. 47, no. 1-3,
    pp. 7–42, apr 2002\. [Online]. Available: https://doi.org/10.1023/A:1014573219977'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] ——, “High-accuracy stereo depth maps using structured light,” ser. CVPR’03.   USA:
    IEEE Computer Society, 2003, pp. 195–202.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] D. Scharstein and C. Pal, “Learning conditional random fields for stereo,”
    in *2007 IEEE Conference on Computer Vision and Pattern Recognition*, 2007, pp.
    1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] H. Hirschmuller and D. Scharstein, “Evaluation of cost functions for stereo
    matching,” in *2007 IEEE Conference on Computer Vision and Pattern Recognition*,
    2007, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang,
    and P. Westling, “High-resolution stereo datasets with subpixel-accurate ground
    truth,” in *German Conference on Pattern Recognition*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” in *ECCV*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-pie,”
    in *2008 8th IEEE International Conference on Automatic Face and Gesture Recognition*,
    2008, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Guo, P. Lincoln, P. Davidson, J. Busch, X. Yu, M. Whalen, G. Harvey,
    S. O. Escolano, R. K. Pandey, J. Dourgarian, D. Tang, A. Tkach, A. Kowdle, E. Cooper,
    M. Dou, S. Fanello, G. Fyffe, C. Rhemann, J. Taylor, P. Debevec, and S. Izadi,
    “The relightables: Volumetric performance capture of humans with realistic relighting,”
    2019\. [Online]. Available: https://dl.acm.org/citation.cfm?id=3356571'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, and H. P. Lensch,
    “Nerd: Neural reflectance decomposition from image collections,” in *IEEE International
    Conference on Computer Vision (ICCV)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    metrics: Psnr vs. ssim,” in *2010 20th International Conference on Pattern Recognition*.   IEEE,
    2010, p. 2366–2369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] ——, “Image quality assessment: from error visibility to structural similarity,”
    *IEEE transactions on image processing*, vol. 13, no. 4, p. 600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2018, p. 586–595.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in *Advances in neural information processing
    systems*, 2016, p. 2234–2242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “Gans trained by a two time-scale update rule converge to a local nash equilibrium,”
    in *Proceedings of the 31st International Conference on Neural Information Processing
    Systems*, ser. NIPS’17.   Red Hook, NY, USA: Curran Associates Inc., 2017, p.
    6629–6640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Bińkowski, D. J. Sutherland, M. Arbel, and A. Gretton, “Demystifying
    MMD GANs,” in *International Conference on Learning Representations*, 2018\. [Online].
    Available: https://openreview.net/forum?id=r1lUOzWCW'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Shi, S. Peng, Y. Xu, Y. Liao, and Y. Shen, “Deep generative models
    on 3d representations: A survey,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] R. Huang, S. Zhang, T. Li, and R. He, “Beyond face rotation: Global and
    local perception gan for photorealistic and identity preserving frontal view synthesis,”
    in *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*,
    Oct 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] B. Zhao, X. Wu, Z.-Q. Cheng, H. Liu, Z. Jie, and J. Feng, “Multi-view
    image generation from a single-view,” in *Proceedings of the 26th ACM International
    Conference on Multimedia*, ser. MM ’18.   New York, NY, USA: Association for Computing
    Machinery, 2018, p. 383–391\. [Online]. Available: https://doi.org/10.1145/3240508.3240536'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] K. Regmi and A. Borji, “Cross-view image synthesis using conditional gans,”
    in *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2018,
    pp. 3501–3510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] ——, “Cross-view image synthesis using geometry-guided conditional GANs,”
    *Computer Vision and Image Understanding*, vol. 187, p. 102788, oct 2019\. [Online].
    Available: https://doi.org/10.1016%2Fj.cviu.2019.07.008'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] F. Mokhayeri, K. Kamali, and E. Granger, “Cross-domain face synthesis
    using a controllable gan,” in *2020 IEEE Winter Conference on Applications of
    Computer Vision (WACV)*, 2020, pp. 241–249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Zhu, Z. Yin, J. Shi, H. Li, and D. Lin, “Generative adversarial frontal
    view to bird view synthesis,” in *2018 International Conference on 3D Vision (3DV)*.   IEEE,
    2018, pp. 454–463.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Ding, S. Wu, H. Tang, F. Wu, G. Gao, and X.-Y. Jing, “Cross-view image
    synthesis with deformable convolution and attention mechanism,” in *Pattern Recognition
    and Computer Vision: Third Chinese Conference, PRCV 2020, Nanjing, China, October
    16–18, 2020, Proceedings, Part I*.   Berlin, Heidelberg: Springer-Verlag, 2020,
    p. 386–397\. [Online]. Available: https://doi.org/10.1007/978-3-030-60633-6_32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] B. Ren, H. Tang, and N. Sebe, “Cascaded cross mlp-mixer gans for cross-view
    image translation,” in *British Machine Vision Conference*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *2017 IEEE International
    Conference on Computer Vision (ICCV)*, 2017, pp. 2242–2251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Yin, L. Sun, and Q. Li, “Novel view synthesis on unpaired data by conditional
    deformable variational auto-encoder,” in *Computer Vision – ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII*.   Berlin,
    Heidelberg: Springer-Verlag, 2020, p. 87–103\. [Online]. Available: https://doi.org/10.1007/978-3-030-58604-1_6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Shen, J. Plested, Y. Yao, and T. Gedeon, “Pairwise-gan: Pose-based
    view synthesis through pair-wise training,” in *Neural Information Processing*,
    H. Yang, K. Pasupa, A. C.-S. Leung, J. T. Kwok, J. H. Chan, and I. King, Eds.   Cham:
    Springer International Publishing, 2020, pp. 507–515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] E. R. Chan, M. Monteiro, P. Kellnhofer, J. Wu, and G. Wetzstein, “pi-gan:
    Periodic implicit generative adversarial networks for 3d-aware image synthesis,”
    in *2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    2021, pp. 5795–5805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Cai, A. Obukhov, D. Dai, and L. Van Gool, “Pix2nerf: Unsupervised conditional
    $\pi$-gan for single image to neural radiance fields translation,” in *2022 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 3971–3980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T. Leimkühler and G. Drettakis, “FreeStyleGAN,” *ACM Transactions on Graphics*,
    vol. 40, no. 6, pp. 1–15, dec 2021\. [Online]. Available: https://doi.org/10.1145%2F3478513.3480538'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. C. Medin, B. Egger, A. Cherian, Y. Wang, J. B. Tenenbaum, X. Liu, and
    T. K. Marks, “Most-gan: 3d morphable stylegan for disentangled face image manipulation,”
    *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 2,
    pp. 1962–1971, Jun. 2022\. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/20091'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] R. Or-El, X. Luo, M. Shan, E. Shechtman, J. J. Park, and I. Kemelmacher-Shlizerman,
    “StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2022, pp. 13 503–13 513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] X.-Y. Zheng, Y. Liu, P.-S. Wang, and X. Tong, “Sdf-stylegan: Implicit
    sdf-based stylegan for 3d shape generation,” in *Comput. Graph. Forum (SGP)*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Y. Deng, J. Yang, J. Xiang, and X. Tong, “Gram: Generative radiance manifolds
    for 3d-aware image generation,” in *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Xiang, J. Yang, Y. Deng, and X. Tong, “Gram-hd: 3d-consistent image
    generation at high resolution with generative radiance manifolds,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O. Gallo,
    L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, and G. Wetzstein, “Efficient
    geometry-aware 3d generative adversarial networks,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    16 123–16 133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] X. Zhao, F. Ma, D. Güera, Z. Ren, A. G. Schwing, and A. Colburn, “Generative
    multiplane images: Making a 2d gan 3d-aware,” in *Proc. ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. A. Alhaija, A. Dirik, A. Knörig, S. Fidler, and M. Shugrina, “Xdgan:
    Multi-modal 3d shape generation in 2d space,” in *33rd British Machine Vision
    Conference 2022, BMVC 2022, London, UK, November 21-24, 2022*.   BMVA Press, 2022\.
    [Online]. Available: https://bmvc2022.mpi-inf.mpg.de/0782.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Z. Wang, S. Wu, W. Xie, M. Chen, and V. A. Prisacariu, “Nerf–: Neural
    radiance fields without known camera parameters,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari, R. Ramamoorthi,
    R. Ng, and A. Kar, “Local light field fusion: Practical view synthesis with prescriptive
    sampling guidelines,” *ACM Trans. Graph.*, vol. 38, no. 4, jul 2019\. [Online].
    Available: https://doi.org/10.1145/3306346.3322980'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Q. Meng, A. Chen, H. Luo, M. Wu, H. Su, L. Xu, X. He, and J. Yu, “GNeRF:
    GAN-based Neural Radiance Field without Posed Camera,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] R. Jensen, A. Dahl, G. Vogiatzis, E. Tola, and H. Aanæs, “Large scale
    multi-view stereopsis evaluation,” in *Proceedings of the 2014 IEEE Conference
    on Computer Vision and Pattern Recognition*, ser. CVPR ’14.   USA: IEEE Computer
    Society, 2014, pp. 406–413\. [Online]. Available: https://doi.org/10.1109/CVPR.2014.59'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] C. C. A. A. M. C. Yoonwoo Jeong, Seokjun Ahn and J. Park, “Self-calibrating
    neural radiance fields,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and temples:
    Benchmarking large-scale scene reconstruction,” *ACM Trans. Graph.*, vol. 36,
    no. 4, jul 2017\. [Online]. Available: https://doi.org/10.1145/3072959.3073599'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] W. Bian, Z. Wang, K. Li, J.-W. Bian, and V. A. Prisacariu, “Nope-nerf:
    Optimising neural radiance field with no pose prior,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    4160–4169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel,
    R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan,
    J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira,
    M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove, and
    R. A. Newcombe, “The replica dataset: A digital replica of indoor spaces,” *CoRR*,
    vol. abs/1906.05797, 2019. [Online]. Available: http://arxiv.org/abs/1906.05797'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] P. Truong, M.-J. Rakotosaona, F. Manhardt, and F. Tombari, “Sparf: Neural
    radiance fields from sparse and noisy poses.”   IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Y. Zhang, G. Yang, S. Tulsiani, and D. Ramanan, “NeRS: Neural reflectance
    surfaces for sparse-view 3d reconstruction in the wild,” in *Conference on Neural
    Information Processing Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Seo, D. Han, Y. Chang, and N. Kwak, “Mixnerf: Modeling a ray with mixture
    density for novel view synthesis from sparse inputs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    20 659–20 668.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A.-Q. Cao and R. de Charette, “Scenerf: Self-supervised monocular 3d scene
    reconstruction with radiance fields,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR
    Sequences,” in *Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV)*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Chen, W. Yi, L. Ma, X. Jia, and H. Lu, “Gm-nerf: Learning generalizable
    model-based neural radiance fields from multi-view images,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, “Function4d: Real-time
    human volumetric capture from very sparse consumer rgbd sensors,” in *IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR2021)*, June 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] B. Bhatnagar, G. Tiwari, C. Theobalt, and G. Pons-Moll, “Multi-garment
    net: Learning to dress 3d people from images,” in *2019 IEEE/CVF International
    Conference on Computer Vision (ICCV)*, 2019, pp. 5419–5429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] W. Cheng, S. Xu, J. Piao, C. Qian, W. Wu, K.-Y. Lin, and H. Li, “Generalizable
    neural performer: Learning robust radiance fields for human novel view synthesis,”
    *arXiv preprint arXiv:2204.11798*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, “Neural
    body: Implicit neural representations with structured latent codes for novel view
    synthesis of dynamic humans,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] B. Mildenhall, P. Hedman, R. Martin-Brualla, P. P. Srinivasan, and J. T.
    Barron, “NeRF in the dark: High dynamic range view synthesis from noisy raw images,”
    *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] L. Ma, X. Li, J. Liao, Q. Zhang, X. Wang, J. Wang, and P. V. Sander, “Deblur-nerf:
    Neural radiance fields from blurry images,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 12 861–12 870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] X. Huang, Q. Zhang, Y. Feng, H. Li, X. Wang, and Q. Wang, “Hdr-nerf: High
    dynamic range neural radiance fields,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 18 398–18 408.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] N. Pearl, T. Treibitz, and S. Korman, “Nan: Noise-aware nerfs for burst-denoising,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2022, pp. 12 672–12 681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman,
    “Mip-nerf 360: Unbounded anti-aliased neural radiance fields,” *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin,
    “Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering,”
    in *The European Conference on Computer Vision (ECCV)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] “Google earth studio,” https://earth.google.com/studio/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan,
    J. T. Barron, and H. Kretzschmar, “Block-nerf: Scalable large scene neural view
    synthesis,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] L. Xu, Y. Xiangli, S. Peng, X. Pan, N. Zhao, C. Theobalt, B. Dai, and
    D. Lin, “Grid-guided neural radiance fields for large urban scenes,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Turki, D. Ramanan, and M. Satyanarayanan, “Mega-nerf: Scalable construction
    of large-scale nerfs for virtual fly-throughs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    12 922–12 931.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] C. Choi, S. M. Kim, and Y. M. Kim, “Balanced spherical grid for egocentric
    view synthesis,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 16 590–16 599.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa, “PlenOctrees
    for real-time rendering of neural radiance fields,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] C. Sun, M. Sun, and H. Chen, “Direct voxel grid optimization: Super-fast
    convergence for radiance fields reconstruction,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt, “Neural sparse
    voxel fields,” *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan,
    “Blendedmvs: A large-scale dataset for generalized multi-view stereo networks,”
    *Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, and M. Zollhöfer,
    “Deepvoxels: Learning persistent 3d feature embeddings,” in *Proc. Computer Vision
    and Pattern Recognition (CVPR), IEEE*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] H. Wang, J. Ren, Z. Huang, K. Olszewski, M. Chai, Y. Fu, and S. Tulyakov,
    “R2l: Distilling neural radiance field to neural light field for efficient novel
    view synthesis,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya,
    A. Kaplanyan, and M. Steinberger, “Donerf: Towards real-time rendering of compact
    neural radiance fields using depth oracle networks,” *Computer Graphics Forum*,
    vol. 40, no. 4, pp. 45–59, 2021\. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14340'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] K. Wadhwani and T. Kojima, “SqueezeNeRF: Further factorized FastNeRF
    for memory-efficient inference,” in *2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition Workshops (CVPRW)*.   IEEE, jun 2022\. [Online]. Available:
    https://doi.org/10.1109%2Fcvprw56347.2022.00307'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Z. Chen, T. Funkhouser, P. Hedman, and A. Tagliasacchi, “Mobilenerf:
    Exploiting the polygon rasterization pipeline for efficient neural field rendering
    on mobile architectures,” in *The Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Chen, X. Chen, X. Wang, Q. Zhang, Y. Guo, Y. Shan, and F. Wang, “Local-to-global
    registration for bundle-adjusting neural radiance fields,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp.
    8264–8273.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, “Nerf++: Analyzing and
    improving neural radiance fields,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. Rebain, W. Jiang, S. Yazdani, K. Li, K. M. Yi, and A. Tagliasacchi,
    “Derf: Decomposed radiance fields,” in *2021 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2021, pp. 14 148–14 156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz,
    and R. Martin-Brualla, “Nerfies: Deformable neural radiance fields,” in *2021
    IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021, pp. 5845–5854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. Li, Z. Feng, Q. She, H. Ding, C. Wang, and G. H. Lee, “Mine: Towards
    continuous depth mpi with nerf for novel view synthesis,” in *2021 IEEE/CVF International
    Conference on Computer Vision (ICCV)*, 2021, pp. 12 558–12 568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman,
    R. Martin-Brualla, and S. M. Seitz, “Hypernerf: A higher-dimensional representation
    for topologically varying neural radiance fields,” *ACM Trans. Graph.*, vol. 40,
    no. 6, dec 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] T. Chen, P. Wang, Z. Fan, and Z. Wang, “Aug-nerf: Training stronger neural
    radiance fields with triple-level physically-grounded augmentations,” in *2022
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022,
    pp. 15 170–15 181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] T. Kaneko, “Ar-nerf: Unsupervised learning of depth and defocus effects
    from natural images with aperture rendering neural radiance fields,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2022, pp. 18 387–18 397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] X. Li, C. Hong, Y. Wang, Z. Cao, K. Xian, and G. Lin, “Symmnerf: Learning
    to explore symmetry prior for single-view view synthesis,” in *Proceedings of
    the Asian Conference on Computer Vision (ACCV)*, December 2022, pp. 1726–1742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] K. Zhou, W. Li, Y. Wang, T. Hu, N. Jiang, X. Han, and J. Lu, “Nerflix:
    High-quality neural view synthesis by learning a degradation-driven inter-viewpoint
    mixer,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2023, pp. 12 363–12 374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] C. Sbrolli, P. Cudrano, M. Frosi, and M. Matteucci, “Ic3d: Image-conditioned
    3d diffusion for shape generation,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] J. Gu, Q. Gao, S. Zhai, B. Chen, L. Liu, and J. Susskind, “Learning controllable
    3d diffusion models from single-view images,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] T. Anciukevičius, Z. Xu, M. Fisher, P. Henderson, H. Bilen, N. J. Mitra,
    and P. Guerrero, “Renderdiffusion: Image diffusion for 3d reconstruction, inpainting
    and generation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 12 608–12 618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Xiang, J. Yang, B. Huang, and X. Tong, “3d-aware image generation
    using 2d diffusion models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick,
    “Zero-1-to-3: Zero-shot one image to 3d object,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] E. R. Chan, K. Nagano, M. A. Chan, A. W. Bergman, J. J. Park, A. Levy,
    M. Aittala, S. D. Mello, T. Karras, and G. Wetzstein, “Generative novel view synthesis
    with 3d-aware diffusion models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017\. [Online]. Available:
    https://arxiv.org/pdf/1706.03762.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] P. Nguyen-Ha, L. Huynh, E. Rahtu, and J. Heikkila, “Sequential view synthesis
    with transformer,” in *Proceedings of the Asian Conference on Computer Vision
    (ACCV)*, November 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] J. Yang, Y. Li, and L. Yang, “Shape transformer nets: Generating viewpoint-invariant
    3d shapes from a single image,” *Journal of Visual Communication and Image Representation*,
    vol. 81, p. 103345, 2021\. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1047320321002285'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Kulhánek, E. Derner, T. Sattler, and R. Babuška, “Viewformer: Nerf-free
    neural rendering from few images using transformers,” in *European Conference
    on Computer Vision (ECCV)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] P. Zhou, L. Xie, B. Ni, and Q. Tian, “Cips-3d: A 3d-aware generator of
    gans based on conditionally-independent pixel synthesis,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] X. Xu, X. Pan, D. Lin, and B. Dai, “Generative occupancy fields for 3d
    surface-aware image synthesis,” in *Advances in Neural Information Processing
    Systems(NeurIPS)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Lan, X. Meng, S. Yang, C. C. Loy, and B. Dai, “Self-supervised geometry-aware
    encoder for style-based 3d gan inversion,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 20 940–20 949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Li, J. van de Weijer, Y. Wang, F. S. Khan, M. Liu, and J. Yang, “3d-aware
    multi-class image-to-image translation with nerfs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    12 652–12 662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] M. Shahbazi, E. Ntavelis, A. Tonioni, E. Collins, D. P. Paudel, M. Danelljan,
    and L. V. Gool, “Nerf-gan distillation for efficient 3d-aware generation with
    convolutions,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. Kania, A. Kasymov, M. Zięba, and P. Spurek, “Hypernerfgan: Hypernetwork
    approach to 3d nerf gan,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. R. Bhattarai, M. Nießner, and A. Sevastopolsky, “Triplanenet: An encoder
    for eg3d inversion,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] N. Müller, Y. Siddiqui, L. Porzi, S. R. Bulo, P. Kontschieder, and M. Nießner,
    “Diffrf: Rendering-guided 3d radiance field diffusion,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp. 4328–4338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] D. Xu, Y. Jiang, P. Wang, Z. Fan, Y. Wang, and Z. Wang, “Neurallift-360:
    Lifting an in-the-wild 2d photo to a 3d object with 360deg views,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 4479–4489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] H. Chen, J. Gu, A. Chen, W. Tian, Z. Tu, L. Liu, and H. Su, “Single-stage
    diffusion nerf: A unified approach to 3d generation and reconstruction,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] J. Gu, A. Trevithick, K.-E. Lin, J. Susskind, C. Theobalt, L. Liu, and
    R. Ramamoorthi, “Nerfdiff: Single-image view synthesis with nerf-guided distillation
    from 3d-aware diffusion,” in *International Conference on Machine Learning*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] D. Wang, X. Cui, S. Salcudean, and Z. J. Wang, “Generalizable neural
    radiance fields for novel view synthesis with transformer,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] K.-E. Lin, L. Yen-Chen, W.-S. Lai, T.-Y. Lin, Y.-C. Shih, and R. Ramamoorthi,
    “Vision transformer for nerf-based view synthesis from a single input image,”
    in *2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)*,
    2023, pp. 806–815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Liu, Q. Nie, Y. Liu, and C. Wang, “Nerf-loc: Visual localization with
    conditional neural radiance field,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Y. Liao, K. Schwarz, L. Mescheder, and A. Geiger, “Towards unsupervised
    learning of generative models for 3d controllable image synthesis,” in *Proceedings
    IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] T. Nguyen-Phuoc, C. Richardt, L. Mai, Y.-L. Yang, and N. Mitra, “Blockgan:
    Learning 3d object-aware scene representations from unlabelled images,” in *Advances
    in Neural Information Processing Systems 33*, Nov 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] X. Pan, B. Dai, Z. Liu, C. C. Loy, and P. Luo, “Do 2d gans know 3d shape?
    unsupervised 3d shape reconstruction from 2d image gans,” in *International Conference
    on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] A. Tewari, M. B. R, X. Pan, O. Fried, M. Agrawala, and C. Theobalt, “Disentangled3d:
    Learning a 3d generative model with disentangled geometry and appearance from
    monocular images,” in *2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, 2022, pp. 1506–1515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Kobayashi, E. Matsumoto, and V. Sitzmann, “Decomposing nerf for editing
    via feature field distillation,” in *Advances in Neural Information Processing
    Systems*, vol. 35, 2022\. [Online]. Available: https://arxiv.org/pdf/2205.15585.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] X. Zhang, A. Kundu, T. Funkhouser, L. Guibas, H. Su, and K. Genova, “Nerflets:
    Local radiance fields for efficient structure-aware 3d scene representation from
    2d supervision,” *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] C. Zheng, W. Lin, and F. Xu, “Editablenerf: Editing topologically varying
    neural radiance fields by key points,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] J. Zhang and L. Yang, “MonodepthPlus: self-supervised monocular depth
    estimation using soft-attention and learnable outlier-masking,” *Journal of Electronic
    Imaging*, vol. 30, no. 2, p. 023017, 2021. [Online]. Available: https://doi.org/10.1117/1.JEI.30.2.023017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] R. Liang, J. Zhang, H. Li, C. Yang, Y. Guan, and N. Vijaykumar, “Spidr:
    Sdf-based neural point fields for illumination and deformation,” *arXiv preprint
    arXiv:2210.08398*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Zhang, X. Huang, B. Ni, T. Li, and W. Zhang, “Frequency-modulated
    point cloud rendering with easy editing,” *arXiv preprint arXiv:2303.07596*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J.-K. Chen, J. Lyu, and Y.-X. Wang, “NeuralEditor: Editing neural radiance
    fields via manipulating point clouds,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] J.-Y. Zhu, Z. Zhang, C. Zhang, J. Wu, A. Torralba, J. B. Tenenbaum, and
    W. T. Freeman, “Visual object networks: Image generation with disentangled 3D
    representations,” in *Advances in Neural Information Processing Systems (NeurIPS)*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] A. Mirzaei, T. Aumentado-Armstrong, M. A. Brubaker, J. Kelly, A. Levinshtein,
    K. G. Derpanis, and I. Gilitschenski, “Reference-guided controllable inpainting
    of neural radiance fields,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Y. Yin, Z. Fu, F. Yang, and G. Lin, “Or-nerf: Object removing from 3d
    scenes guided by multiview segmentation with neural radiance fields,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] H. G. Kim, M. Park, S. Lee, S. Kim, and Y. M. Ro, “Visual comfort aware-reinforcement
    learning for depth adjustment of stereoscopic 3d images,” *Proceedings of the
    AAAI Conference on Artificial Intelligence*, vol. 35, no. 2, pp. 1762–1770, May
    2021\. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/16270'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] R.-F. Jheng, T.-H. Wu, J.-F. Yeh, and W. H. Hsu, “Free-form 3d scene
    inpainting with dual-stream gan,” in *British Machine Vision Conference*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Q. Wang, Y. Wang, M. Birsak, and P. Wonka, “Blobgan-3d: A spatially-disentangled
    3d-aware generative model for indoor scenes,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] J. Gu, L. Liu, P. Wang, and C. Theobalt, “Stylenerf: A style-based 3d
    aware generator for high-resolution image synthesis,” in *International Conference
    on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] C. Wang, M. Chai, M. He, D. Chen, and J. Liao, “Clip-nerf: Text-and-image
    driven manipulation of neural radiance fields,” in *2022 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 3825–3834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] K. Kania, K. M. Yi, M. Kowalski, T. Trzciński, and A. Tagliasacchi, “CoNeRF:
    Controllable Neural Radiance Fields,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] V. Lazova, V. Guzov, K. Olszewski, S. Tulyakov, and G. Pons-Moll, “Control-nerf:
    Editable feature volumes for scene rendering and manipulation,” in *2023 IEEE/CVF
    Winter Conference on Applications of Computer Vision (WACV)*, 2023, pp. 4329–4339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Y.-J. Yuan, Y.-T. Sun, Y.-K. Lai, Y. Ma, R. Jia, and L. Gao, “Nerf-editing:
    Geometry editing of neural radiance fields,” in *2022 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2022, pp. 18 332–18 343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] C. Sun, Y. Liu, J. Han, and S. Gould, “Nerfeditor: Differentiable style
    decomposition for full 3d scene editing,” *arXiv preprint arXiv:2212.03848*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Z. Wang, Y. Deng, J. Yang, J. Yu, and X. Tong, “Generative Deformable
    Radiance Fields for Disentangled Image Synthesis of Topology-Varying Objects,”
    *Computer Graphics Forum*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] K. Tertikas, D. Paschalidou, B. Pan, J. J. Park, M. A. Uy, I. Emiris,
    Y. Avrithis, and L. Guibas, “Partnerf: Generating part-aware editable 3d shapes
    without 3d supervision,” in *Proceedings IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui,
    “Sine: Semantic-driven image-based nerf editing with prior-guided editing field,”
    in *The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes, and D. Cohen-Or, “Set-the-scene:
    Global-local training for generating controllable nerf scenes,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. Kelly, M. A. Brubaker,
    I. Gilitschenski, and A. Levinshtein, “SPIn-NeRF: Multiview segmentation and perceptual
    inpainting with neural radiance fields,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] O. Avrahami, D. Lischinski, and O. Fried, “Blended diffusion for text-driven
    editing of natural images,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2022, pp. 18 208–18 218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever,
    and M. Chen, “Glide: Towards photorealistic image generation and editing with
    text-guided diffusion models,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] G. Couairon, J. Verbeek, H. Schwenk, and M. Cord, “Diffedit: Diffusion-based
    semantic image editing with mask guidance,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] E. Sella, G. Fiebelman, P. Hedman, and H. Averbuch-Elor, “Vox-e: Text-guided
    voxel editing of 3d objects,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] A. Haque, M. Tancik, A. Efros, A. Holynski, and A. Kanazawa, “Instruct-nerf2nerf:
    Editing 3d scenes with instructions,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong, and L. Wang, “Componerf:
    Text-guided multi-object compositional nerf with editable 3d scene layout,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] R. Martin-Brualla, N. Radwan, M. S. M. Sajjadi, J. T. Barron, A. Dosovitskiy,
    and D. Duckworth, “NeRF in the Wild: Neural Radiance Fields for Unconstrained
    Photo Collections,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] M. Boss, A. Engelhardt, A. Kar, Y. Li, D. Sun, J. T. Barron, H. P. Lensch,
    and V. Jampani, “SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary
    Image collections,” in *Advances in Neural Information Processing Systems (NeurIPS)*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] C. Choi, J. Kim, and Y. M. Kim, “Ibl-nerf: Image-based lighting formulation
    of neural radiance fields,” *arXiv preprint arXiv:2210.08202*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Z. Yan, C. Li, and G. H. Lee, “Nerf-ds: Neural radiance fields for dynamic
    specular objects,” *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] D. Guo, L. Zhu, S. Ling, T. Li, G. Zhang, Q. Yang, P. Wang, S. Jiang,
    S. Wu, and J. Liu, “Face illumination normalization based on generative adversarial
    network,” *Natural Computing: An International Journal*, vol. 22, no. 1, pp. 105–117,
    jul 2022\. [Online]. Available: https://doi.org/10.1007/s11047-022-09892-4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Z. Cui, L. Gu, X. Sun, Y. Qiao, and T. Harada, “Aleth-nerf: Low-light
    condition view synthesis with concealing fields,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] N. Abirami R. and D. R. Vincent P. M., “Low-light image enhancement based
    on generative adversarial network,” *Frontiers in Genetics*, vol. 12, 2021\. [Online].
    Available: https://www.frontiersin.org/articles/10.3389/fgene.2021.799777'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] W. Kim, R. Lee, M. Park, and S.-H. Lee, “Low-light image enhancement
    based on maximal diffusion values,” *IEEE Access*, vol. 7, pp. 129 150–129 163,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] P. Ponglertnapakorn, N. Tritrong, and S. Suwajanakorn, “Difareli: Diffusion
    face relighting,” *arXiv preprint arXiv:2304.09479*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] M. Guo, A. Fathi, J. Wu, and T. Funkhouser, “Object-centric neural scene
    rendering,” *arXiv preprint arXiv:2012.08503*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Y. Wang, W. gang Zhou, Z. Lu, and H. Li, “Udoc-gan: Unpaired document
    illumination correction with background light prior,” *Proceedings of the 30th
    ACM International Conference on Multimedia*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] J. Ling, Z. Wang, and F. Xu, “Shadowneus: Neural sdf reconstruction by
    shadow ray supervision,” *ArXiv*, vol. abs/2211.14086, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] V. Rudnev, M. Elgharib, W. Smith, L. Liu, V. Golyanik, and C. Theobalt,
    “Nerf for outdoor scene relighting,” in *European Conference on Computer Vision
    (ECCV)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] C. Higuera, B. Boots, and M. Mukadam, “Learning to read braille: Bridging
    the tactile reality gap with diffusion models,” *CoRR*, vol. abs/2304.01182, 2023\.
    [Online]. Available: https://doi.org/10.48550/arXiv.2304.01182'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Y.-C. Guo, D. Kang, L. Bao, Y. He, and S.-H. Zhang, “Nerfren: Neural
    radiance fields with reflections,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 18 409–18 418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] C. LeGendre, W.-C. Ma, G. Fyffe, J. Flynn, L. Charbonnel, J. Busch, and
    P. Debevec, “Deeplight: Learning illumination for unconstrained mobile mixed reality,”
    in *2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    2019, pp. 5911–5921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, “Intrinsicnerf:
    Learning intrinsic neural radiance fields for editable novel view synthesis,”
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] M. Boss, V. Jampani, R. Braun, C. Liu, J. T. Barron, and H. P. Lensch,
    “Neural-pil: Neural pre-integrated lighting for reflectance decomposition,” in
    *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] S. Saito, T. Simon, J. Saragih, and H. Joo, “Pifuhd: Multi-level pixel-aligned
    implicit function for high-resolution 3d human digitization,” in *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp. 81–90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] H. Tang, S. Bai, L. Zhang, P. H. S. Torr, and N. Sebe, “Xinggan for person
    image generation,” in *Computer Vision – ECCV 2020*, A. Vedaldi, H. Bischof, T. Brox,
    and J.-M. Frahm, Eds.   Cham: Springer International Publishing, 2020, pp. 717–734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Y. Ren, X. Yu, J. Chen, T. H. Li, and G. Li, “Deep image spatial transformation
    for person image generation,” in *2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2020, pp. 7687–7696.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Liu, Z. Qin, T. Wan, and Z. Luo, “Auto-painter: Cartoon image generation
    from sketch by using conditional wasserstein generative adversarial networks,”
    *Neurocomputing*, vol. 311, pp. 78–87, 2018\. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0925231218306209'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] H. Li, “Ai synthesis for the metaverse: From avatars to 3d scenes,” 2022.
    [Online]. Available: https://talks.stanford.edu/hao-li-pinscreen-on-ai-synthesis-for-the-metaverse-from-avatars-to-3d-scenes/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] “Mapping gothic france,” https://mcid.mcah.columbia.edu/art-atlas/mapping-gothic,
    accessed: 2023-06-03\. [Online]. Available: https://mcid.mcah.columbia.edu/art-atlas/mapping-gothic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] X. Yuejia, L. Chuanhao, L. Qingdazhu, Y. Xiaocui, L. Bo, and J. Meizhi,
    “A creative industry image generation dataset based on captions,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] C. Tatsch, J. A. B. Jnr, D. Covell, I. B. Tulu, and Y. Gu, “Rhino: An
    autonomous robot for mapping underground mine environments,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Y. Tian, L. Li, A. Fumagalli, Y. Tadesse, and B. Prabhakaran, “Haptic-enabled
    mixed reality system for mixed-initiative remote robot control,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] G. Pu, Y. Men, Y. Mao, Y. Jiang, W.-Y. Ma, and Z. Lian, “Controllable
    image synthesis with attribute-decomposed gan,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 45, no. 2, pp. 1514–1532, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] X. Wu, Y. Zhang, Q. Li, Y. Qi, J. Wang, and Y. Guo, “Face aging with
    pixel-level alignment gan,” *Applied Intelligence*, vol. 52, no. 11, p. 14665–14678,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] D. Sero, A. Zaidi, J. Li, J. D. White, T. B. Gonz’alez Zarzar, M. L.
    Marazita, S. M. Weinberg, P. Suetens, D. Vandermeulen, J. K. Wagner *et al.*,
    “Facial recognition from dna using face-to-dna classifiers,” *Nature communications*,
    vol. 10, no. 1, p. 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba,
    V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, and B. Edwards, “Adversarial
    robustness toolbox v1.0.0,” https://github.com/Trusted-AI/adversarial-robustness-toolbox,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
