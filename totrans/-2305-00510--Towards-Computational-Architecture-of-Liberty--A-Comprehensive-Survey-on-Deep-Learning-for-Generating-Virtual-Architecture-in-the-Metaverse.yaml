- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:40:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.00510] Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.00510](https://ar5iv.labs.arxiv.org/html/2305.00510)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep
    Learning for Generating Virtual Architecture in the Metaverse'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anqi Wang Emerging Interdisciplinary Areas, Hong Kong University of Science
    and TechnologyHong Kong SARChina Computational Media and Arts, Hong Kong University
    of Science and Technology (Guangzhou)GuangzhouChina ,  Jiahua Dong School of Architecture,
    The Chinese University of Hong KongHong Kong SARChina ,  Jiachuan Shen The Bartlett
    School of Architecture, University College LondonLondonUK ,  Lik-Hang Lee The
    Hong Kong Polytechnic UniversityHong Kong SARChina  and  Pan Hui Computational
    Media and Arts, Hong Kong University of Science and Technology (Guangzhou)GuangzhouChina
    Emerging Interdisciplinary Areas, Hong Kong University of Science and TechnologyHong
    Kong SARChina(2018)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 3D shape generation techniques utilizing deep learning are increasing attention
    from both computer vision and architectural design. This survey focuses on investigating
    and comparing the current latest approaches to 3D object generation with deep
    generative models (DGMs), including Generative Adversarial Networks (GANs), Variational
    Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles
    (80.7% of articles published between 2018-2022) to review the field of generated
    possibilities of architecture in virtual environments, limited to the architecture
    form. We provide an overview of architectural research, virtual environment and
    related technical approaches, followed by a review of recent trends in discrete
    voxel generation, 3D models generated from 2D images, and conditional parameters.
    We highlight under-explored issues in 3D generation and parameterized control
    that is worth further investigation. Moreover, we speculate that four research
    agendas including data limitation, editability, evaluation metrics and human-computer
    interaction are important enablers of ubiquitous interaction with immersive systems
    in architecture for computer-aided design Our work contributes to researchers’
    understanding of the current potential and future needs of deep learnings in generating
    virtual architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning, virtual environment, architectural design, computational architecture,
    3D shape generation, 3D-aware image synthesis, human-computer interaction, metaverse,
    AIGC^†^†copyright: acmcopyright^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†journal:
    JACM^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth:
    8^†^†ccs: Human-centered computing Interaction design process and methods^†^†ccs:
    Computing methodologies Machine learning^†^†ccs: Applied computing Architecture
    (buildings)^†^†ccs: Human-centered computing Virtual reality'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past decades, the study of architectural space has changed the exploration
    directions from reinforcement concrete to digital architecture, then to the information
    frameworks with virtualization beyond the physical layer. The digital innovations
    and technological advances associated with the spline, pixels, voxels, and bits
    have enabled architectural forms to be reconceptualized. The architecture has
    been not as static, permanent objects but as a larger part of a data network and
    the evolving communication between different kinds of architectural systems (Claypool,
    [2019](#bib.bib33)). For example, the scenes of video games and augmented reality
    (AR) or virtual reality (VR) cooperate in such virtual environments for architecture.
    This virtual fabrication of digital space pushes the boundaries of consideration
    in what is being produced and designed. The purpose and subject of architecture
    have radically changed in this digitalization. It is free from the constraints
    of physical construction, socioeconomic factors and environmental conditions,
    such as daylight, architectural materials or structure, budget, etc. Instead,
    the visually appealing experiences they bring enable virtual architectures to
    serve as intersections within this infinity beyond reality. It becomes a spatial
    medium full of infinite possibilities to carry society and culture.
  prefs: []
  type: TYPE_NORMAL
- en: The infinitely expanding spatial field of virtual worlds (VWs) faces many tasks
    that require efficient modeling. Creating various object models through generative
    techniques is a timely research topic (Aggarwal et al., [2021a](#bib.bib4)). Research
    on 3D model generation or 3D-aware image synthesis through deep learning (DL)
    has been booming in recent years. Generative Adversarial Networks (GANs), Variational
    Autoencoder (VAE) and the very recent diffusion model (DDPM) belong to deep learning.
    In contrast to other classic ML algorithms, Their category belongs to unsupervised
    learning (USL), which does not rely on large sets of labeled data. DL has surpassed
    human perception regarding abstraction strategies through invisible deep neural
    networks. For instance, AlphaGO can beat top board players in board games. DALLE,
    a drawing tool performing the multi-modality of text-transformed images, learns
    human intention from the natural language. DL has pushed the potential for output
    farther and farther beyond human imagination.
  prefs: []
  type: TYPE_NORMAL
- en: '1.1\. Preamble: 3D Virtual Architecture Generated by Deep Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The generative virtual architecture is a vast domain spreading over computer-aided
    design (CAD), 3D shape generation techniques, and human-computer interaction (HCI).
    On the other hand, 3D shape generation techniques by DL are a fundamental viewpoint
    in computer vision and computer graphics. Thus, we need to define these terms
    at the beginning of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c1757937607e454748646c32412e4791.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Applications of deep learning impact our lives in all aspects. (a)
    Self-drive cars; (b) AlfaGO; (c) Segmentation in the city recognition with computer
    vision; (d) Mirror World NFT, which shows AI dialogue character with personality
    and development from learning; (e) OpenCV recognizing the object types in the
    camera view; (f) Apple watch paired with deep learning detect atrial fibrillation
    with 97 % accuracy; (g) ChatGPT developed by OpenAI; (h) recommendation system
    in the Tiktok; (i) Smart agriculture implemented by deep learning with drones;
    (j) DALLE-2, one powerful painting tool empowered by machine learning; (k) D.O.U.G,
    a collaborative robotic arms interacted with human, learning human behaviors and
    gestures, performance and created by artist Soug Wen; (l) digital human body generation
    by 3D reconstruction technique; (m) An AI art movie created by GANs (Casey Reas);
    (n) BCI (Brain-computer interface).
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning. Deep learning (DL), a subclass of machine learning (ML) and
    artificial intelligence (AI), has developed rapidly with a boost in data process
    and computation. A new class of DL is deep generative models (DGMs) by combining
    generative models and deep neural networks. They rely on paradigms of unsupervised
    learning. Neural networks such as ANN, CNN and RNN, as signature deep learning
    architectures, have played essential roles in manipulating the relationship between
    the input and output data. The definition of DL signifies the system master the
    capability of self-learning and experience enhancement (Sarker, [2021b](#bib.bib150)).
    DL applications have broad applications to all aspects of life. There are plenty
    of notable examples. Such as the first fully automatic self-driving car, Navlab5
    (Fig. 1a); Alpha GO, a computer program that can beat top human Go players (Fig.
    1b); Mirror World NFT’s intelligent character ¹¹1Mirror Wolrd’s official websites:
    https://link3.to/mirrorworld that can learn and grow up from human text conversations
    (Fig. 1d); ChatGPT ²²2Introducing ChatGPT, source: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)
    developed by OpenAI  (OpenAI, [2022](#bib.bib131)), the ever first intelligent
    conversational machine model; One of the best recommendation systems in the world
    (Fig. 1h), which made TikTok stand out from 13.47 million DAUs  ³³3A report on
    TikTok, souce: https://www.statista.com/statistics/1090659/tiktok-dau-worldwide-android/;
    DALLE-2, which was commented as the ever-best AI painting tool (Fig. 1j); and
    the infinite potential BCI (Brain-computer interface) (Fig. 1n); Moreover, the
    intelligence revolution could not be ongoing without DL, for instance, smart agriculture
    (Fig. 1i), and smart transportation. Computer Vision relies on the DL closely,
    such as notable OpenCV (Fig. 1e), segmentation with vision cognition (Fig. 1c)
    and 3D scanning techniques (Fig. 1l). Additionally, loads of contemporary digital
    art were created through deep learning by inputting and processing the data of
    human gestures and bio-signals (Fig. 1k). Figure 1m represents the cutting-edge
    example of experimental AI-art films created by GAN.'
  prefs: []
  type: TYPE_NORMAL
- en: '3D Shape Generation Technique. With an increasing surge of AI-Generated Content
    (AIGC), DGMs have the capability to process 3D shape generation through various
    approaches. There are plenty of frameworks, such as GAN, VAE, Flow model, and
    so on. DGMs have the widest applications and the most prominent influence in the
    field of two-dimensional (2D) image process, such as textures, transfer style
    art, photorealistic faces and text-to-image generation (Jetchev et al., [2016](#bib.bib80);
    Ledig et al., [2017](#bib.bib98); Reed et al., [2016](#bib.bib142); Zhu et al.,
    [2017](#bib.bib188)). For innovative techniques and boosted arithmetic power,
    DGMs for 3D shape generation have burgeoned in research years. The DGMs can achieve
    this leveraging effect in the aspect of the 3D generative object by shifting from
    the outcome in the 2D image. Rapidly, a method with GANs, named 3D-GAN (Wu et al.,
    [2016](#bib.bib170)), was applicable to 3D shape generation in a probability space
    for voxel grids (See Fig. [9](#S3.F9 "Figure 9 ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")a).
    The 3D shape generation inspires some downstream operations, such as object classification
    and part segmentation, to scene semantic parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: Computer-aided Design and Deep Learning-Assisted Form Generation. Computer-aided
    design (CAD) is an extensive research field regarding digital tools-assisted creation
    and optimization in the design phase (Sarcar et al., [2008](#bib.bib147)). Especially
    in the architecture field, the design with the involvement of computational tools
    has spread over Building Information Modeling (BIM), structural performance analysis,
    robotics and digital fabrication, urban analytics, environmental performance,
    and so on (Baduge et al., [2022](#bib.bib13)). Architectural design aided by DL
    is one of the typical classifications of the CAD field by providing a wide range
    of options in design processes  (Tamke et al., [2018](#bib.bib162)). DL-assisted
    architecture generation has enlarged to the generative systems from rule-based
    topology optimization such as cellular automata ⁴⁴4A cellular automata (CA) is
    a discrete model of computation studied in automata theory and shape grammars ⁵⁵5Shape
    grammars in the computation are a specific class of production systems that generate
    geometric shapes., to neural network tools, which provide more flexibility, and
    more controllable parameters in a generation. Reviewing DL-aided form generation,
    deep neural networks in DGMs have proved useful efficiency and power in architecture
    design. In such a field, the workforce and computational power needed to coordinate
    with each other in studying form generation and future construction. Nevertheless,
    there are no such requirements in virtual environment generation. We found that
    there needs to be more knowledge in the form generation around the transformation
    between physical and virtual spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Generative 3D Virtual Architecture. Generating architectural spaces efficiently
    and applicatively from 3D representations is a popular and worthwhile research
    topic, both in the architecture and computer science domains. For architecture,
    it is crucial to clarify the rules of digital space, which aligns with functionality,
    aesthetics, and satisfaction. As Roberto Bottazzi states, as opposed to transforming
    digital architecture, urgency is how the digital space can be architecturized
    (Bottazzi, [2018](#bib.bib20)). This unveils the significance of virtual architecture.
    The increasing tendency to build a virtual world (VW) is associated with the reality
    of owning digitalized lives and produces, which refers to the metaverse.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Towards an interdisciplinary area among architecture, HCI, & AIGC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The research on virtual environments has gradually stood out at the intersection
    of human-computer interaction (HCI) and immersive techniques such as Augmented
    reality (AR) and Virtual Reality (VR) (Lee et al., [2021b](#bib.bib100)). Moreover,
    the demands in Virtual Reality (VR) or Augmented Reality (AR) environments are
    surging due to modeling productivity and efficiency. Consequently, these demands
    and developments have also raised the viewpoint for 3D generative approaches.
    The feasibility of virtual architecture clearly benefits from the CAD approach
    in terms of the modeling task load and HCI approaches. However, despite the popularity
    of exploring the possibilities of space in 3D object generation, research on architecture
    is still limited. Most studies have similarly focused on the two-dimensional (2D)
    image process. ArchiGAN (Chaillou, [2020](#bib.bib25)), studied by the MIT team,
    explores the potential of GANs in training large numbers of building floor plans
    for spatial layout and functional delineation automatically, and further advent
    applications(Chaillou, [2022](#bib.bib26)). Most other architectural research
    with GAN, such as generating some fantastic-style images by (Karras et al., [2019](#bib.bib84)),
    satisfies an imagination for designs that are either beyond the constraints of
    physical worlds, or have not been effectively proposed and illustrated before.
    For example, Özel utilizes some creative images for architecture, relying on artificial
    intelligence, to predict the future of architecture (Özel, [2020](#bib.bib134)).
    These studies consider the creativity of generating absences from an aesthetic
    perspective beyond reality. Abstract two-dimensional images are far from architecture,
    even in virtual worlds, due to the lack of methods to transfer those automatically
    generated images to a three-dimensional format. Additionally, the traditional
    method of 2D floor plans constructed in real space is unsuitable for virtual environments.
    In other words, there has been considerable divergence in the production approach
    between physical and virtual architecture. The former has to consider the external
    environment (e.g., daylight, light) and construction constraints; while the virtual
    one put more concerned with the sense of identity, definition of self, and aesthetics.
    Therefore, to close the gaps mentioned above, a novel discipline urges reconstructing
    a niche domain encompassing architectural components, constraint requirements
    in VE, and user needs. At the same time, deep learning could support generating
    computational architecture freely. It is worthwhile to mention that the gaps remain
    for most existing research.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize the problem mentioned above, first, generating the architecture
    with the design purpose for the 3D shape generation techniques is rarely considered
    since generative architecture requires sophisticated consideration and innovative
    techniques, especially for non-tech-savvy architects. Second, architectural generation
    approaches rarely regard the “virtual” and lack the usage of 3D shapes generation.
    Therefore, the design dimensions for virtual architecture generated by 3D approaches
    have not been systematically considered. Therefore, our survey addresses how to
    leverage 3D shape generation techniques to produce 3D virtual spaces from a user-centered
    perspective. We noted that this article defines the user-centered perspective
    as ‘inclusive’ to consider the needs of non-tech-savvy architects who lack technical
    (computer-science) backgrounds, and layman users who intended to create virtual
    buildings in the Metaverse.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Methodology and Related Articles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab4f8d745fe99e416d9693b8f1faef35.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) This survey investigates this intersection area.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e0fdb91ec8df06c58a7de1ebe042be9.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) A profile of the number of works cited in this paper in different categories
    and years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2. The survey’s scope and profile of related articles: a – architectural
    studies on DGMs; c – computer vision studies; v – those works on rules in VWs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey article presents findings of a systematic literature review on
    deep learning for 3D shape generation in computer vision and CAD for computational
    architecture in terms of generating virtual architecture in recent years. Since
    the problem mentioned above, the intersection of 3D generation techniques and
    virtual architecture is still nearly blank. Therefore, we anchored the three fields
    to conduct this survey in order to complement the key insight with each other:
    3D shape generation techniques, DL-assisted architectural design, and the design
    considerations in a VWs in terms of HCI (See Fig. [2(a)](#S1.F2.sf1 "In Figure
    2 ‣ 1.3\. Methodology and Related Articles ‣ 1\. Introduction ‣ Towards Computational
    Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating
    Virtual Architecture in the Metaverse")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We reviewed a sample of 187 articles and primarily focused on works published
    between 2018 and 2022 (five years, 80.7%) as follows: 2023 or later: 4 (8.7%),
    2022: 33 (17.6%), 2021: 39 (20.9%), 2020: 36 (19.3%), 2019: 30 (16%), 2018: 33
    (5.8%), before 2018: 32 (17.1%) from those three fields (See Fig.  [2(b)](#S1.F2.sf2
    "In Figure 2 ‣ 1.3\. Methodology and Related Articles ‣ 1\. Introduction ‣ Towards
    Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning
    for Generating Virtual Architecture in the Metaverse")). We found the articles
    primarily through publication databases such as ACM Digital Library, IEEE Xplore,
    ScienceDirect, Springer Link and CuminCAD. We used the following keywords Augmented
    Reality (AR), Virtual Reality (VR), deep generative models, 3D representation,
    3D model or shape or geometry, object generation, 3D-aware image, shape synthesis,
    point cloud, voxel grid, mesh, implicit neural field, virtual architecture, virtual
    environment, deep learning design, generative design, Generative Adversarial Network
    (GAN), 3D GAN, VAE (Variational Autoencoder), diffusion model (DDPM), text to
    3D, image to 3D, zero-shot, computational architecture, spatial objects, flexible
    spaces, virtual rules, design discipline, human-computer-interaction (HCI), evaluation
    metric, human perception, emotion, simulation, participatory design, aesthetics,
    real-time interaction, and combinations of these keywords. Additionally, we count
    on some latest or high-influential research on Computer Vision (CV) only published
    in arXiv. We screened through the titles and the keywords to ensure they only
    included full papers and extended abstracts. Short papers and abstracts were excluded
    from this scope. When the keywords and abstracts do not appear as the key information
    or elements in our investigating scope, we read the whole publication to check
    whether it is included or not. After the screening, we got 147 articles and 19
    CV research published on arXiv to review, i.e., 166 authoritative articles. Additionally,
    online resources were directly searched through the Google search engine, we mainly
    conclude 19 articles and 2 relevant architectural projects from the perspective
    of architecture design, categorized by the virtual world, computational architecture,
    architectural theory and so on. Eventually, a total of 187 articles and 2 architectural
    projects are included in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various other surveys further locate this scope, as follows: Category 1 (machine
    learning  (Kreuzberger et al., [2023](#bib.bib93); Penney and Chen, [2019](#bib.bib138))
    or deep learning (Hatcher and Yu, [2018](#bib.bib64); Kreuzberger et al., [2023](#bib.bib93);
    Alom et al., [2019](#bib.bib7); Akinosho et al., [2020](#bib.bib6); Khan et al.,
    [2020](#bib.bib87); Alom et al., [2019](#bib.bib7); Sarker, [2021a](#bib.bib149);
    Pouyanfar et al., [2018](#bib.bib140); Abdar et al., [2021](#bib.bib2))): 3D shape
    generation (Shi et al., [2022](#bib.bib157); Oussidi and Elhassouny, [2018](#bib.bib133);
    Cao et al., [2020](#bib.bib24)), scene synthesis  (Xia and Xue, [2022](#bib.bib173);
    Zhang et al., [2019](#bib.bib185)), applications (Dong et al., [2021](#bib.bib48);
    Dargan et al., [2020](#bib.bib36)), 3D representation (Guo et al., [2020](#bib.bib59)),
    3D reconstruction from 2D  (Yuniarti and Suciati, [2019](#bib.bib177)), and generative
    models (Harshvardhan et al., [2020](#bib.bib63); Aggarwal et al., [2021a](#bib.bib4),
    [b](#bib.bib5); Wang et al., [2021](#bib.bib168); Jabbar et al., [2021](#bib.bib74);
    Pavan Kumar and Jayagopal, [2021](#bib.bib135); Croitoru et al., [2023](#bib.bib35);
    Creswell et al., [2018](#bib.bib34)). Category 2 (DL-assisted architectural design
     (Penney and Chen, [2019](#bib.bib138); Newton, [2019](#bib.bib124))): infrastructure
     (Tamke et al., [2018](#bib.bib162)), intelligent construction  (Baduge et al.,
    [2022](#bib.bib13); Akinosho et al., [2020](#bib.bib6)), life cycle  (Hong et al.,
    [2020](#bib.bib69)), or other design  (Regenwetter et al., [2022](#bib.bib143)).
    And Category 3 (architecture in a virtual environment or virtual worlds): design
    disciplinary in virtual reality (theories and applications)  (Bartle, [2004](#bib.bib16)),
    HCI in the virtual architecture (human senses and emotions)  (Lee et al., [2021b](#bib.bib100)),
    metaverse or virtual worlds  (Bartle, [2010](#bib.bib17); Dionisio et al., [2013](#bib.bib47);
    Lee et al., [2021a](#bib.bib99)). In contrast, this article reviews the approaches
    to 3D shape generation and the factors of virtualization in architecture in recent
    years, especially in the last five years (2018-2022), regarding virtual rules,
    design principles, social parameters, and HCI methods for CAD design. We argue
    to combine these research areas and consider this an interdisciplinary problem.
    Finally, the research outlines the crucial challenges of HCI in virtual architectural
    model generation tasks. Our survey article uniquely considers the prominent features
    of the above categories and further paves a path towards the computational architecture
    of liberty, with the below contributions.'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We provide a comprehensive investigation for the inclusion of DL-assisted architectural
    design and deep generative models, dedicated to developing a critical lens for
    computational architecture in virtual environments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We highlight an opportunity to address the academic gap between the two existing
    areas of research, attempting to respond algorithmically to social factors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose research topics for the future of virtual architecture towards liberty,
    considering disciplinary beyond reality such as humanism and spirituality.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.4\. Scope and Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f5a4fc67586e80a207f45b59dbd31ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3. The survey structure (Sections 2 – 4).
  prefs: []
  type: TYPE_NORMAL
- en: Although the intersection of DL and architecture is in all aspects, we only
    investigate articles where DL considers the generative deep learning of 3D virtual
    architecture, especially for the 3D DGMs. We only include the research limit to
    the 2D style imaginary drawings coupled with providing innovative approaches to
    the 3D transition. We also excluded the articles that only consider the real problem
    such as BIM rather than implementing it in a purely virtual environment. This
    scope reflects the automatic generation of timely design issues in the virtual
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper reviews the current problem space in this field consisting of rules
    of the virtual world, social parameters and civilization of formal liberty starting
    from Section 2. Section 3 covers the innovative form generation in architecture
    under generation approaches in terms of 3D form transposition and 3D solid form
    generation. Four topics are covered in this section, including GAN ed into specific
    training, VAE for the specific information extraction, 3D-aware image synthesis
    and diffusion model based on the conditional text (See Fig.  [3](#S1.F3 "Figure
    3 ‣ 1.4\. Scope and Structure ‣ 1\. Introduction ‣ Towards Computational Architecture
    of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture
    in the Metaverse")). Subsequently, we revisit the field from the perspective of
    HCI, formulating research agendas in four grand challenges, ranging from data
    limitation, editability, and evaluation metrics to HCI design, collecting user
    information, operation and perception. We indicate that can explore new possibilities
    for optimizing and inventing innovative methods regarding automatically generating
    virtual architecture with human and social group-centric considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Overview of Generated 3D Virtual Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the last century, discussing methods and rules for computer-aided design
    has never stopped. However, initially, the digitized models simply worked to simulate
    the physical environment or document the design process. The discussion around
    virtual architecture started when web-based social media or games were invented.
    After the emergence of the metaverse concept, enthusiasm for virtual architecture
    research has intensified, indicating that we entered a new era of existence with
    a proliferation of different kinds of virtual environments (VEs). On the one hand,
    VE is built on a liminal reproduction to reality, which has no legitimacy and
    produces no consequences (Nazmeeva, [2019](#bib.bib122)). On the other hand, the
    VE is built on potential interactions of humans as social attributes in virtual
    spaces. Lee et al. state that digital natives are essential for developing the
    ultimate form of the metaverse (Lee et al., [2021a](#bib.bib99)). Those digital
    ones enable boosting their impacts on all craft as well as user-generated content
    (UGC) through social interaction with avatars. As virtual worlds evolve, social
    attributes are expanded, such as poverty rights, identity, roles, and group differentiation
     (Schroeder et al., [2001](#bib.bib151)). As Roberto Bottazzi argues, the roadmap
    for our modern society should be the architecturalization of unregulated digital
    spaces(Bottazzi, [2018](#bib.bib20)). Increasingly, people are integrating virtual
    spaces into a coexistence of living spaces. In general, the research from the
    primary “virtual worlds” until the explosion of the metaverse is rather diverse.
    However, most studies have focused on the layer of virtual reality or interactive
    games, but not specifically on the subject of space itself.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. The Rules of Virtual Worlds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The discussion of virtual worlds, which are constructed and internally coherent,
    has always ceased. There have three primary layers stacked in the concept of the
    virtual worlds, semantics, and virtual environment to the architecture  (Nevelsteen,
    [2018](#bib.bib123)). Consequently, these virtual worlds are everywhere, from
    politics to video games to token economies. Immersion, presence, and interactivity
    intertwine as the three pillars of virtual worlds  (Mütterlein, [2018](#bib.bib118)).
    Gilbert identified five essential characteristics of virtual worlds  (Gilbert,
    [2011](#bib.bib55)). They are embodied in every aspect, such as spatial perception,
    public or private activities, social experience and emotional expression. Furthermore,
    many scholars have tried to define and classify the virtual world in terms of
    layers or development stages. The confused definition of VW has been mitigated
    by the exuberance of the metaverse. Dionisio divided the virtual world into 5
    developing stages, ranging from text and 2D graphical interfaces to UGC and then
    to a complete decentralized economic system (Dionisio et al., [2013](#bib.bib47)).
    In the latest research on metaverse, Lee et al. state that there are three stages
    toward the co-existence of physical and virtual space - digital twins, digital
    natives, and surreality (Lee et al., [2021a](#bib.bib99)). The digital twin is
    a reproduced version of the physical world, depending on the development of CAD
    for both industry and architecture. The surging numbers of digital natives enable
    boosting their positive impacts on the interaction with avatars and all craft
    as well as user-generated content. Surreality is the ultimate ideal world that
    the metaverse aims for, supporting heterogeneous activities with interoperability
    in real-time between the physical and virtual worlds.
  prefs: []
  type: TYPE_NORMAL
- en: From the technology perspective, software and hardware architecture defines
    spatial functionality, constrain and social interaction. These architectures form
    the politics of VW (Lessig, [2009](#bib.bib101)), while code forms the laws of
    the graphic VE. Every law invented by the human has Intrinsic value with specific
    intention and elaborate design. Therefore, the design discipline consists of codes
    and computing ought to satisfy the complex parameters, including computing capability,
    cost-benefit ratio and user preference. Despite the codes and programs providing
    the rules and laws in the VW, it is undeniable that unique expertise is required
    to handle the design and organization of VWs. This is not enough by the ability
    given by the codability in computers solely. This composite capability, as a special
    case of visual, analogically integrated reasoning, is fully capable of being a
    key expertise. It can operate at multiple scales and in multiple contexts to map,
    analyze, and organize VWs, while being able to introduce new systems, rules, and
    forms into them  (Jovanovic, [2022](#bib.bib82)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Virtuality in the Architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Virtuality is a fundamental characteristic specific to architecture in a VE
    and encompasses the following three elements: immersion, presence, and interactivity.
    Stem from three pillars in the virtual world  (Mütterlein, [2018](#bib.bib118)),
    the virtual architecture shares the same interpretation but more specific deployment.
    In other words, since the virtual building is a specific type of mediated matter
    that has a 3D representation in the VW, we can regard these buildings as a subtype
    of the virtual environment. Various components and frameworks of the virtual world
    are assumed to be applied within it. Therefore, the theory of the VW is equally
    applicable to virtual architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The design discipline of virtual architecture had to regard these three pillars
    as essential. Some research on architectural design has conducted these rules,
    VRoamer  (Cheng et al., [2019](#bib.bib32)) reports an interactive VE through
    the releasing users’ attention to achieve immersion. Not only for the research,
    the architecture projects also tend to integrity with immersive technology. Zaha
    Hadid Architects and JOURNEE have jointly developed a virtual NFT gallery ”NFTism”,
    which is one of a handful of virtual buildings with interactivity. This gallery
    inherits Zaha’s representative fluidic form, supporting MMO (massively multiplayer
    online) technology and integrating audio-video interaction  ⁶⁶6A report by Archdaily:
    [https://www.archdaily.com/972886/zaha-hadid-architects-presents-virtual-gallery-exploring-architecture-nfts-and-the-metaverse](https://www.archdaily.com/972886/zaha-hadid-architects-presents-virtual-gallery-exploring-architecture-nfts-and-the-metaverse).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Design Discipline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.2.1\. The Absence of Reality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To better clarify the design principles of virtual architecture, we compare
    them to real-world architecture. First, real factors had to be absent in building
    a virtual architecture, encompassing design consideration, construction structure,
    and economic cost. Precisely, a kind of emerging factor running on the immersive
    technologies and fitting the virtual logic replaces the original position occupied
    by the real ones. The following elaborate exact three aspects: First, from environmental
    factors to social factors: environmental factors such as the direction of wind
    and light affect the spatial layout of the architecture, while they are not effective
    for a virtual building. What is replaced in the virtual world is a more neoliberal
    virtual logic, since humans gather socially unrestricted by time and space by
    emphasizing social activities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, from Building structure to unrestricted form: the structure of a virtual
    building is more toward a more accessible and open form. Many architectural structures
    that existing technology could not implement have been consecrated as paper architecture
     ⁷⁷7Visionary architecture that couldn’t build in reality, only as drawings, collages,
    or models. with cutting-edge conception. For example, Zaha’s early works were
    not structurally possible with the technology and construction of the time. There
    are plenty of schools of thought in this regard, such as bionic architecture ⁸⁸8Bionic
    architecture is usually computationally adapted to the structure or form of organic
    matter in nature. Design considerations for biomimetic architecture include the
    physiological, behavioral, and structural adaptations of living organisms. and
    responsive architecture ⁹⁹9Responsive architecture refers to the ability of an
    architecture or building to exhibit the ability to change its form to constantly
    reflect the conditions of its surroundings. It reflects the idea of interactive
    architecture..'
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, from construction costs to the cost of scene and computation: It is
    taken for granted that real construction costs, such as material and labor costs,
    are transferred to the costs of modeling, lighting and rendering, while the complex
    data computation for such tasks as rendering consumes computer memory. The cost
    of building and constructing complex and fantastical real scenes is exponentially
    higher, and they are achieved through special effects and modeling rendering techniques
    that are impossible to achieve in the real world.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the above, virtual buildings and real buildings are built with a
    transforming logic, from construction to unrestrained form. Real-world architecture
    needs to start from the spatial planning and functional layout of the floor plan,
    so as to deduce and complete the architectural design. But in the virtual environment,
    the alternative solution is to start directly from the functional layout on the
    3D space, mostly modeling through the game engine or 3D modeling software. This
    is not only the production method of virtual assets, but even some advanced real
    buildings are starting to do so because of the efficiency and higher accuracy
    of spatial perception.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Social Factors As Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0996adbf613b196b1ac61de48f26dd4a.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Algorithmic Social Sciences ResearchUnit (ASSRU)  ^(11)^(11)11Source: [http://www.assru.org/index.html](http://www.assru.org/index.html)
    (ASSRU, [1999](#bib.bib11)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba1671d43a420f8f432bd336c167291b.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Parametric Semiology ^(13)^(13)13Source: [https://www.patrikschumacher.com/Texts/Design%20of%20Information%20Rich%20Environments.html](https://www.patrikschumacher.com/Texts/Design%20of%20Information%20Rich%20Environments.html):
    Semio-field, differentiation of public vs private as a parametric range.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4. Social factors as parameters in different theories.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to highlight that the discipline of virtual spaces cannot abandon
    the social impacts. The built environment is a vast, navigable, and information-rich
    communication interface, especially in the virtual world. It provides potential
    social participants with information about the communicative interactions expected
    within its scope (Schumacher, [2013](#bib.bib152)) (Fig.  [11](#footnote11 "footnote
    11In Figure 4 ‣ 2.2.2\. Social Factors As Parameters ‣ 2.2\. Design Discipline
    ‣ 2\. Overview of Generated 3D Virtual Architecture ‣ Towards Computational Architecture
    of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture
    in the Metaverse")). Although virtuality has unique attributes beyond reality,
    people are active in a virtual environment with social abilities. In other words,
    virtual technology supports social activities and goals in an immersive environment.
    Additionally, from the psychology perspective, familiarity with the realistic
    scene facilitates the boosting of presence and self-awareness, due to the preference
    given by the exposure rate  (Myers and Twenge, [2012](#bib.bib119)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Driven by the information society and virtual world, socialization is a medium
    for communication that is increasingly complex, which conveys a rich diversity
    of social systems and sophisticated information in multiple scenarios. For example,
    Lam et al. have developed a context-aware, contextually interactive AR urban interface
    enabling users to locate websites intuitively with minimal modifications  (Lam
    et al., [2019](#bib.bib96)). Architecture signifies a spatial place containing
    activities, where the study on the semiotics of spatial forms has always revolved
    around the topic of simulating or restoring social scenarios, including public
    spaces, semi-public, and private spaces (See Fig.  [13](#footnote13 "footnote
    13In Figure 4 ‣ 2.2.2\. Social Factors As Parameters ‣ 2.2\. Design Discipline
    ‣ 2\. Overview of Generated 3D Virtual Architecture ‣ Towards Computational Architecture
    of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture
    in the Metaverse")). Space conveys an invitation to participate in framing social
    situations  (Schumacher, [2013](#bib.bib152)). For example, there are a lot of
    studies discussing the human perception of spaces in urban design studies in history.
    Jacobs (Jacobs, [2016](#bib.bib75)) introduced walkable streets as a concept in
    the forming of neighborhoods, which considers visual qualities, connectivity of
    circulations and other indicators. Following the tendency of human-centric design,
    a lot of researchers explored the making of desirable streets and the making of
    places on different scales. Appleton  (Appleton, [1996](#bib.bib8)) introduced
    the prospect-refuge theory to address the safety sense of humans in placemaking,
    which significantly influences socializing. Hall (Hall et al., [1968](#bib.bib61))
    introduced proxemic zones to represent different types of social distances. These
    theories regarding the human sense of space are still widely used in nowadays
    design discipline. All those are from the significance of the social parameters
    in terms of the architectural discipline.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. The Goal of Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The design principles of virtual architecture serve the purpose of constructing
    buildings in VEs. With the boom in virtual technology and the rise of social platforms
    for 3D virtual worlds, the production demand for infinite and sprawling virtual
    environments has surged rapidly. The main task confronts with building rapid,
    large-scale architectural environments. These construction tasks are mostly done
    collaboratively by 3D modeling software and game engines such as Unity. 3D building
    models are 3D spatial representations of artificial spatial elements  (Yao et al.,
    [2018](#bib.bib175)). Its most relevant quality criteria are completeness, the
    spatial accuracy of location and the level of detail  (Keil et al., [2021](#bib.bib86)).
    In addition, realistic simulations regarding scale and size are also important,
    including granularity and simulation as well. All in all, those are very significant
    to bring the experience for users. The non-uniform approach causes various problems,
    such as inconsistent buildings from the manual and automatic operation  (Keil
    et al., [2021](#bib.bib86)), as well as the expensive cost of human resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'One reliable solution is facing many efficient and automatic construction tasks
    in a virtual environment. The solution based on the computation approach is relatively
    consistent since the same automation frameworks are applied for all spatial objects.
    All these approaches are across various scales including urban and architecture.
    MineDojo uses autonomous agents that utilize large pre-trained video language
    models for automation to generate 3D scenes of VWs  (Fan et al., [2022](#bib.bib49)).
    From the recently released by Tencent, the proposed solution for the automatic
    generation of 3D virtual scenes contains 3 modules ranging from city layout generation,
    building exterior generation and interior mapping generation  (Lab, [2023](#bib.bib95))
     ^(14)^(14)14Source: [https://gdcvault.com/play/1028921/Recorded-AI-Enhanced-Procedural-City](https://gdcvault.com/play/1028921/Recorded-AI-Enhanced-Procedural-City).
    It is a new paradigm that combines design perspectives using multiple CV techniques.
    Similarly, there are several studies on computational generation in architecture.
    Most of them are generated by extracting the logic of urban planning  (Ingram
    et al., [1996](#bib.bib73); Veselỳ, [2022](#bib.bib166)), i.e., designing 3D layouts
    and functional divisions from urban plan layouts. In another approach (Veselỳ,
    [2022](#bib.bib166)), three methods with different specific objectives are integrated.
    It generates building massing configurations by autonomously inferring the composition
    rules of existing urban areas.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4\. The Problem on the Collective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'However, there is still a mismatch between that new paradigm of mechanism run
    on the virtuality and the purposes of the emerging virtual environment: collective,
    presence, and unencumbered (Nazmeeva, [2019](#bib.bib122)). The collective pertains
    to the notion that virtual spaces are communal environments wherein individuals
    from diverse backgrounds and cultures can converge and engage with one another.
    The nature of the collective refers to the highly multi-social experience. We
    only proliferate the products that manifest our unique identities and personal
    needs (Nazmeeva, [2019](#bib.bib122)). That conflicts with the collective and
    the products for everyday use, especially the space or architecture. As we live
    in a world with a seamless fusion of reality and the virtual, such as the exquisite
    information and goods powered by a recommendation system on social media, creation
    or live space beyond reality, virtual economic mechanisms, ownership, identity,
    and so on. All of these exhibit the precision of individual values. Apparently,
    the collective and congregate have become blind here. In other words, virtual
    technologies should support collective social activities and goals cued by individual
    experiences in immersive environments. Many theories of virtual worlds emphasize
    this point. Activity Theory argues that virtual worlds should be designed to support
    users in achieving their goals  (Jonassen and Rohrer-Murphy, [1999](#bib.bib81)).
    Bartle’s Four Keys to Virtual World Design states that providing players with
    a sense of purpose is one of the key metrics for designing virtual worlds  (Bartle,
    [2004](#bib.bib16)). The purpose enhances user engagement by providing them with
    a sense of progress and accomplishment, thereby creating a sense of immersion.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Deep Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We briefly overview the progression of deep generative models for 3D representation,
    including 3D shape generation and 3D aware image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c9dda7fba238a71a1ad5b3c12c4794a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5. The frameworks of GAN, VAE and Diffusion Models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1\. 3D Shape Generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'is contributed by traditional deep generative models, in addition to the well-known
    Generative Adversarial Networks (GANs) and variational autoencoders (VAEs), normalizing
    flows (N-Flows), very recent diffusion probabilistic models (DDPMs) as well as
    energy-based models (EBMs), which learn by maximizing from the similarity of the
    given data. These deep generative models generate a tangible 3D object that is
    ready for rendering. It conveys a latent variable to a high-quality image. Although
    every model has its own benefits and great progress in recent years, the domain
    in architecture relies on the GAN mostly, while VAE and the very latest diffusion
    models are designed for a few research. Considering the relevance, we thus introduce
    the GAN, VAE and diffusion models in detail rather than an exhaustive list of
    models included in other CV survey articles (Fig.  [5](#S2.F5 "Figure 5 ‣ 2.3\.
    Deep Generative Models ‣ 2\. Overview of Generated 3D Virtual Architecture ‣ Towards
    Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning
    for Generating Virtual Architecture in the Metaverse")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs. GANs are a type of semi-supervised learning relying on the noise value.
    The GANs rely on machine learning algorithms to construct two neural networks:
    one is a generator, and another is a discriminator. It trains a large database
    by means of a zero-sum game between two of these neural networks to generate agnostic
    creative results.'
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders. Variational autoencoders are probabilistic generative
    models that use neural networks partially. Along with inputs and outputs, neural
    networks need encoders and decoders. The latent space refers to the process of
    learning data features and simplifying data representations to facilitate model
    training for a specific purpose. To guarantee that the latent space of a Variational
    Autoencoder has acceptable qualities and can be used to create fresh data, the
    distribution of its encodings is regularised during training (Kingma et al., [2019](#bib.bib90)).
    Furthermore, the name ”variational” originates from the tight connection between
    regularisation and the variational inference technique used in statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Models. Through modeling the dispersion of data points in latent space,
    we discover the underlying structure of a dataset of images or volumetric, e.g.,
    Denoising Diffusion Probabilistic Models (Ho et al., [2020a](#bib.bib66)). This
    entails teaching a neural network to remove the blurring effect of Gaussian noise
    on an image. It has the prominent advantage of generating sharp and detailed features.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. 3D-Aware Image Synthesis.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This approaches extract latent vectors from the latent space and decode them
    into a target representation by using GAN. Generally, the generation pipeline
    is for an image with 3D awareness as a result and it also starts with an image
    as a generative source.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. 3D Representations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69ca4fe41c95f9ea8147fb6b8e77fc55.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. The 3D representations for (a) Voxel grids, (b) Meshes, (c) Point
    cloud, (d) Neural fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two types of 3D generation develop diverse representations of 3D scenes
    in computer vision and computer graphics. The 3D representation in 3D shape generally
    includes explicit representations, such as voxel grids, point clouds, meshes,
    and implicit neural fields. A 3D-aware image includes depth or normal maps, voxel
    grids, neural fields and hybrid representations. The integration between them
    and the architecture generation is also different. For example, a point cloud
    is often considered when 3D serves as an input source to train the generative
    model. The 3D representation is articulated in existing survey research as a classification
    (Fig.  [6](#S2.F6 "Figure 6 ‣ 2.4\. 3D Representations ‣ 2\. Overview of Generated
    3D Virtual Architecture ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    Below are the brief descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: Architectural design has a preference for explicit representations due to the
    controllability, familiarity, visualization, and availability regarding modifying
    in 3D modeling software. Explicit geometric representations are easier to visualize
    and interpret as they directly represent 3D space. The designers can precisely
    position and adjust each point or voxel, allowing for more accurate control over
    the shape and form of the generated geometry. Nevertheless, implicit representations
    (neural fields) have huge possibilities in architectural research regarding their
    benefits to offer more flexible, continuous, and efficient representations of
    geometry.
  prefs: []
  type: TYPE_NORMAL
- en: Voxel grids. It refers to a three-dimensional grid of values organised into
    rows and columns. The grid contains rows, columns, and layer intersections, referred
    to as a voxel, i.e., a miniature 3D cube (Deng et al., [2020](#bib.bib41)).
  prefs: []
  type: TYPE_NORMAL
- en: Point clouds. A point cloud (Rusu and Cousins, [2011](#bib.bib145)) is a distinct
    collection of data points in space, which might indicate a three-dimensional form
    or item through Cartesian coordinates (X, Y, Z) assigned to each point location.
  prefs: []
  type: TYPE_NORMAL
- en: Meshes. A 3D mesh is the polygonal framework upon which a 3D object is built
    (Nichol et al., [2021](#bib.bib127)). Reference points along the X, Y, and Z axes
    describe the height, breadth, and depth of a 3D mesh’s constituent forms. It is
    important to note that creating a photorealistic 3D model sometimes requires many
    polygons.
  prefs: []
  type: TYPE_NORMAL
- en: Neural fields. It creates images by using traditional volume rendering methods
    to query 5D coordinates along camera rays and projects the resulting colours and
    densities onto a 2D plane. Despite its use of depth data, the scene geometry is
    rendered in exquisite detail, complete with intricate occlusions (Mildenhall et al.,
    [2020](#bib.bib113)).
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid representation. This refers to a hybrid pipeline of 3D representation
    for the pre-training in a 3D feature space embedded in both the virtual and actual
    worlds. The hybrid pipeline can include multitudinous data sources and image frame
    features (Shen et al., [2021](#bib.bib156)), depending on the generation purposes
    of the 3D volumetry.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5\. The Design Factors in DL-Aided Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last subsection, we summarize the design principles for virtual architecture,
    first adapting to the essential characteristics of virtuality as a guideline,
    using computational generation massively and efficiently as a method, meanwhile
    emphasizing the social factors as parameters. On this foundation, we identify
    the mismatch between the architectural collectives and the logic for private production.
    In this section, we explain how to design a virtual building with the above framework
    using a specific automated algorithmic framework.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1\. Interpretability and Input Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Relying on the interpretability of the input data is crucial as a first step
    in generating virtual buildings aided by ML algorithms. Generally, interpretability
    requires the valid illustration of dataset input itself in the field of architecture (Koh,
    [2022](#bib.bib92); Çakmak, [2022](#bib.bib23)). The generated results that meet
    this goal have the ability to support participators for a variety of design purposes.
    Bridging the gap between data and purpose is the massive human and computational
    exertions that drive interpretability in design goals. For example, BAŞAK ÇAKMAK
    explored extended design cognition with GANs and an encoder-decoder  (Çakmak,
    [2022](#bib.bib23)). This methodology conducts the partitioned 3D point clouds
    captured by lidar according to the type of components as input. The research implements
    the extension for those models manually and automatically to promote the DL framework
    to learn spatial organization.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of matching datasets with required parameters associated with spatial
    design goals has been widely used in solutions for architectural design. Such
    applications with DL frameworks are often capable of designing solutions with
    explicit design goals. For instance, Adaptive Acoustic implements a methodology
    with CGN to generate a computational 3D concert hall. The designers trained meshes
    of concert hall interiors as well as the space and acoustic parameters as two
    datasets to pursue an architecture fitting into acoustics requirements. Manually,
    the latter were refined into quantifiable information as parameters containing
    seats, volume, reverberation time, acoustical absorption area, absorption coefficient
    and so on, resulting in AI-generated concert hall forms with acoustic features.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2\. The Algorithmic Form
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The algorithmic form is “the relationship between computation and information
    about computationally generated objects (such as strings or any other data structures)”
    (Chaitin, [1975b](#bib.bib28)). A growing number of social algorithm proposals
    promise that neural networks and machine learning algorithms are research areas
    that can take social factors into account. For example, with different data input
    to the housing generation algorithms, various master plans can be generated with
    varying perceptions of privacy or construction price choices  (Koh, [2022](#bib.bib92)).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there is a growing emphasis on social parameters, ranging from
    the data-driven in algorithmic social sciences to agent-based parametric semiotics
    in the architectural form (Schumacher, [2013](#bib.bib152); ASSRU, [1999](#bib.bib11)).
    Algorithm form implicates the relationship between the computed objects such as
    String and any data structure and the information(Chaitin, [1975a](#bib.bib27)).
    The growing number of social algorithms proposed promises that neural networks
    and machine learning algorithms are areas of research that can take social factors
    into account. In this regard, the social goal stands in the middle of computationally
    generated forms and architectural designs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3\. The Liberty of Form
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad77c057f37386eb7189d0ed848ea818.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) George Guida, 2022\. Multimodal Architecture: Applications of Language
    in a Machine Learning Aided Design Process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cf7aa6a1f2a8e47e8694f38b28f8d9c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Joris Putteneers, 2016\. Synesthesia.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c58824cfe7a03991fd94f5778beff464.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The generating process in Synesthesia.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d1e788eaa73deb677000abf660584dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '(d) A data-driven architectural project named E-motion: a digital interface
    allows users to capture real-time data to interact with for a rethinking of co-living
    among various species.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08c198e0ea17785436cb3b0fb3a949b2.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Viviane toraci fiorella, Taza celilia, Prandini Alvaro Campo. ”ISOS” in
    “Volumeric Cinema” workshop by Current.CAM, 2022
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24064f774b7ef3a2cbabfc9ac0d9ef17.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Tane Moleta and Mizuho Nishioka, the co-constructive project, ”Populating
    Virtual Worlds Together”, 2021  (MOLETA and NISHIOKA, [2021](#bib.bib116)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/816400057745c0218f0b9e8bbacd65e7.png)'
  prefs: []
  type: TYPE_IMG
- en: (g)  (Barsan-Pipu et al., [2020](#bib.bib15)) generates a 3D volumetric architecture
    for virtual environments by utilizing BCI to capture affective-driven dynamic
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8e9bd56078a2ad9d32082a0acfa84bf8.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) Current.CAM, VR gallery, 2021
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7. Some architecture projects with the liberty of form.
  prefs: []
  type: TYPE_NORMAL
- en: 'The virtual architectural form is more flexible than ever before, and the boundaries
    of the definition are more indistinct and inclusive (See Fig.  [7](#S2.F7 "Figure
    7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The Design Factors in DL-Aided Architecture
    ‣ 2\. Overview of Generated 3D Virtual Architecture ‣ Towards Computational Architecture
    of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture
    in the Metaverse")). The generative logic of forms has met transformation, where
    the geometry of space has been expanded to the intelligence of space. The intelligence
    of space represents a multisensory approach where we are free to generate form
    embedded as assistance. Joris Putteneers’ project creates a surreal and complex
    architectural construction by simulating the particle motions in Houdini  ^(15)^(15)15Source:
    [https://putteneersjoris.xyz/projects/synesthesia/synesthesia.html](https://putteneersjoris.xyz/projects/synesthesia/synesthesia.html)
    (Fig.  [7(b)](#S2.F7.sf2 "In Figure 7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The
    Design Factors in DL-Aided Architecture ‣ 2\. Overview of Generated 3D Virtual
    Architecture ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")
    and [7(c)](#S2.F7.sf3 "In Figure 7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The Design
    Factors in DL-Aided Architecture ‣ 2\. Overview of Generated 3D Virtual Architecture
    ‣ Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep
    Learning for Generating Virtual Architecture in the Metaverse")). This is a figurative
    abstraction of the algorithm in 3D space. The form for virtual architecture can
    also be a visualization of data in 3D space. For example, an architectural project,
    namely E-motion, designs an interface for data visualization driven by the redistribution
    and simulation of animal and human movement habits, thus linking human and non-human
    intelligence  ^(16)^(16)16Fei Chen, Mochen Jiang, Haojun Cui, and Yuankai Wang,
    E-motion, 2020\. Source: [https://bproautumn2020.bartlettarchucl.com/rc18/e-motion](https://bproautumn2020.bartlettarchucl.com/rc18/e-motion)
    (Fig.  [7(d)](#S2.F7.sf4 "In Figure 7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The
    Design Factors in DL-Aided Architecture ‣ 2\. Overview of Generated 3D Virtual
    Architecture ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    While George Guida visualizes the influence of intelligent algorithms and multimodality
    for the architecture in another project  ^(17)^(17)17Source: [https://www.gsd.harvard.edu/project/2022-digital-design-prize-george-guidas-multimodal-architecture-applications-of-language-in-a-machine-learning-aided-design-process/](https://www.gsd.harvard.edu/project/2022-digital-design-prize-george-guidas-multimodal-architecture-applications-of-language-in-a-machine-learning-aided-design-process/)
    (Fig. [7(a)](#S2.F7.sf1 "In Figure 7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The
    Design Factors in DL-Aided Architecture ‣ 2\. Overview of Generated 3D Virtual
    Architecture ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    In the Fourth Virtual Dimension, the authors propose a redefinition of the dimensionality
    of thermoception in VR to understand and engage with the spatial and directional
    aspects of virtual scenes  (Sheehan et al., [2021](#bib.bib155)) (Fig. [7(f)](#S2.F7.sf6
    "In Figure 7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The Design Factors in DL-Aided
    Architecture ‣ 2\. Overview of Generated 3D Virtual Architecture ‣ Towards Computational
    Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating
    Virtual Architecture in the Metaverse")). The form of virtual architecture is
    even a kind of co-construction. For instance, a project, namely Populating Virtual
    Worlds Together, encourages artists with no experience in 3D modeling to create
    using a participatory design approach  (MOLETA and NISHIOKA, [2021](#bib.bib116)).
    It leads to an autonomous virtual world consisting of cubes and corresponding
    columns of varying heights and forests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, in the generation of virtual architecture, VWs generally take into
    account more aesthetic, cultural, and human-centered intentions.  (Barsan-Pipu
    et al., [2020](#bib.bib15)) (Fig.  [7(g)](#S2.F7.sf7 "In Figure 7 ‣ 2.5.3\. The
    Liberty of Form ‣ 2.5\. The Design Factors in DL-Aided Architecture ‣ 2\. Overview
    of Generated 3D Virtual Architecture ‣ Towards Computational Architecture of Liberty:
    A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in
    the Metaverse")). The definition of ”good” architecture has been practically the
    core of architectural discourse  (Bava, [2020](#bib.bib18)). Discussions around
    digital architecture often address this question by escaping into the realm of
    taste or artistic judgment. Aesthetics is criticized as using digital and data
    as a supportive tool homogeneously as a solution in digital architecture. In contrast,
    aesthetics in virtual architecture can justify the grand plan. For instance, Current.
    CAM’s VR exhibition is formed by continuous partitioned spaces with purely fluid
    blue, reinforcing the digital interface’s shaping on the human senses (Fig.  [7(h)](#S2.F7.sf8
    "In Figure 7 ‣ 2.5.3\. The Liberty of Form ‣ 2.5\. The Design Factors in DL-Aided
    Architecture ‣ 2\. Overview of Generated 3D Virtual Architecture ‣ Towards Computational
    Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating
    Virtual Architecture in the Metaverse")). In a workshop, they organised the interaction
    between virtual avatars and fantastically dramatic environments to explore human
    perception of space (Fig.  [7(e)](#S2.F7.sf5 "In Figure 7 ‣ 2.5.3\. The Liberty
    of Form ‣ 2.5\. The Design Factors in DL-Aided Architecture ‣ 2\. Overview of
    Generated 3D Virtual Architecture ‣ Towards Computational Architecture of Liberty:
    A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in
    the Metaverse")). This transcendent novelty of virtual architecture encourages
    the user’s quest for novel audiovisual sensations.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Generated 3D Architecture: A PARADIGM SHIFT'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before the 3D algorithms that can automatically store and process the 3D data,
    the 3D generation methods were mostly developed based on 2D images. The remarkable
    growth of the 3D generation in recent years has revealed the tremendous power
    of this field. Compared to 2D image generation, 3D generation is a daunting task
    regarding the aspects of the 3D dataset, computational consumption, feature learning,
    and probability distribution in 3D space. We investigate the virtual architecture
    generation based on various DGMs both in methods of CV (Table  [1](#S3.T1 "Table
    1 ‣ 3.2\. 3D Solid Form Generation with the GANs ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse"),
    Table  [3.4](#S3.SS4 "3.4\. 3D-Aware Image Synthesis ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse"))
    and architecture (Table  [2](#S3.T2 "Table 2 ‣ 3.2.2\. 3D Solid Form Generation
    ‣ 3.2\. 3D Solid Form Generation with the GANs ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    In the first part, we introduce some research on architectural designs concentrating
    on the 2D deep generative models aiming for 3D transposition, especially GANs.
    Then, we divide deep learning generation approaches for 3D representations into
    four categories based on DGMs: (See Fig.  [8](#S3.F8 "Figure 8 ‣ 3\. Generated
    3D Architecture: A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty:
    A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in
    the Metaverse")):'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D form generation from probabilistic spaces or 2D image sets with GANs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D information extraction from latent space with VAEs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent advances in 3D-aware image synthesis and the possibilities of incorporating
    with architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latest research on diffusion models based on conditional text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b76e0ce1a601151a066688c26b263112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8. A systematic taxonomy for a review of generation approaches on virtual
    architecture design with DGMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d77e4c04a32f5b67679a110df1dc613f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9. The examples of generated objects in the field of computer. (a) 3D
    GAN  (Wu et al., [2016](#bib.bib170)), (b)PointFlow  (Yang et al., [2019](#bib.bib174)),
    (c) HoloGAN  (Nguyen-Phuoc et al., [2019](#bib.bib126)), (d)StyleSDF  (Or-El et al.,
    [2022](#bib.bib132)) (e)Point-E  (Nichol et al., [2022](#bib.bib128)) (f)DreamFusion
     (Poole et al., [2022](#bib.bib139)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. 3D Form Transposition with Constrained Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the past few years, 2D image generation by deep generative models has been
    rapidly developed. Most DL-assisted architecture with deep generative models relies
    on dealing with 2D drawings (Jaminet et al., [2021](#bib.bib78); Mohammad, [2019](#bib.bib115);
    Srivastava et al., [2021](#bib.bib159)), such as composition by overlapping the
    section or plan. It has resulted in the opinion of equaling deep learning in DL-assisted
    architectural design to pix2pix. Significant progress in this methodology is post-processing
    those generated images for targeting 3D models. Those methods intuitively consider
    a post-process through heuristic algorithms or human labor rather than scientific
    methods, aiming for critical ideas and innovative concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. 3D Form Transposition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/44e8a3192f31db024f10175712e9aad1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A pipeline in described work  (Zhang and Blasetti, [2020](#bib.bib183)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d37086d50c463d6e44b5fd483ce27117.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) An process signifies pixels filter from 2D to 3D lattice in  (Ren and Zheng,
    [2020](#bib.bib144)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10. The architectural projects through 3D form transposition with 2D
    deep generative models.
  prefs: []
  type: TYPE_NORMAL
- en: '3D transposition indicates a methodology commences with segmenting a 3D model
    into discrete images, such as sections, plans, and projections from multiple viewpoints.
    It transforms resulting abstractions into 3D representations with using tedious
    computational methods or intuitive manual manipulation (See Fig.  [10(a)](#S3.F10.sf1
    "In Figure 10 ‣ 3.1.1\. 3D Form Transposition ‣ 3.1\. 3D Form Transposition with
    Constrained Approaches ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards
    Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning
    for Generating Virtual Architecture in the Metaverse")) (See Fig.  [10(b)](#S3.F10.sf2
    "In Figure 10 ‣ 3.1.1\. 3D Form Transposition ‣ 3.1\. 3D Form Transposition with
    Constrained Approaches ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards
    Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning
    for Generating Virtual Architecture in the Metaverse")). As an illustration, Zhang
    and Blasetti employed section transformation between two models to manipulate
    the form from 2D to 3D (Zhang and Blasetti, [2020](#bib.bib183)). Inherited from
    2D design thinking, the 3D form transposition experiments are likewise mainly
    conducted based on 2D to 3D composition using the image-to-image translation networks
    such as Style Transfer (Özel, [2020](#bib.bib134); Ren and Zheng, [2020](#bib.bib144);
    Liu et al., [2021](#bib.bib107); Zhang and Blasetti, [2020](#bib.bib183)), StyleGAN
    (Del Campo et al., [2019](#bib.bib39); Zhang, [2019](#bib.bib181); Zhang and Huang,
    [2021](#bib.bib184)) and pix2pixGAN (Yu, [2020](#bib.bib176); Di Carlo et al.,
    [2022](#bib.bib43)). They act as 2D-based form finding tools that support the
    decision-making process for designers in transforming 3D models into different
    formats. The representation of the 2D images was developed further from pixels
    or voxels to lateral thinking. Data can be compressed to a high dimensional latent
    space with enhanced connections. EI Asmar and Sareen employ vector arithmetic
    and interpolations to navigate in the latent space to generate various images
    as options for 3D voxelization (Asmar and Sareen, [2020](#bib.bib10)). Bank et
    al. developed an interactive tool that can manipulate data in latent spaces, where
    the generated stylized images were represented as point clouds and can be assembled
    as spectral entities in adjustable resolution (Bank et al., [2022](#bib.bib14)).
    Similarly, in the latent space, the continuous sequence of images can be generated
    by feature interpolation. The project ‘Generali Center’ developed by Del Campo
    et al. also utilized StyleGAN to present latent walks(del Campo et al., [2022](#bib.bib40)).
    Using pixel projection to convert the values in the pixels into a 3D model is
    a significant advancement in their research.'
  prefs: []
  type: TYPE_NORMAL
- en: Huang et al. (Huang et al., [2021](#bib.bib70)) employs latent space to encode
    the 3D information from images of GANs. The generated image is technically a set
    of points mapping from latent space to a 2D graph, then it conduct an interpolation
    containing a sequence of perspectives. The perspectival GAN (Kim and Huang, [2022](#bib.bib89))
    is an extended research comprising latent space rotation to learn 3D information
    in 2D images.
  prefs: []
  type: TYPE_NORMAL
- en: The 2D image-to-image translation algorithms utilized in 3D form generation
    are evidence of computational freedom in the innovative 3D generation since the
    future direction of virtual spaces lies in complex variations. This generation
    method allows for the preservation of high resolution in both input and output,
    as 2D images are lightweight and simple to process. Furthermore, training algorithms
    for 2D-based GANs networks have been well-developed, thereby providing a wide
    range of possibilities for human interaction with algorithms, as well as various
    adjustable output options through contouring 2D patterns into 3D forms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. 3D Solid Form Generation with the GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Method Names | Publication& Year | 3D Representations | Models |'
  prefs: []
  type: TYPE_TB
- en: '| 3D GAN (Wu et al., [2016](#bib.bib170)) | NIPS 2016 | Voxel grid | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| Text2Shape (Chen et al., [2018](#bib.bib30)) | ACCV 2018 | Voxel grid | GAN
    |'
  prefs: []
  type: TYPE_TB
- en: '| PLATONICGAN (Henzler et al., [2019](#bib.bib65)) | CVPR 2019 | Voxel grid
    | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| IG GAN  (Lunz et al., [2020](#bib.bib110)) | arXiv 2020 | Voxel grid | GAN
    |'
  prefs: []
  type: TYPE_TB
- en: '| Achlioptas et al.  (Achlioptas et al., [2018](#bib.bib3)) | ICML 2018 | Point
    cloud | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| Shu et al.  (Shu et al., [2019](#bib.bib158)) | CVPR 2019 | Point cloud |
    GAN |'
  prefs: []
  type: TYPE_TB
- en: '| Get3d  (Gao et al., [2022](#bib.bib51)) | NeurIPS 2022 | Mesh | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| IM-Net  (Chen and Zhang, [2019](#bib.bib31)) | CVPR 2019 | Neural field |
    GAN |'
  prefs: []
  type: TYPE_TB
- en: '| Kleineberg et al.  (Kleineberg et al., [2020](#bib.bib91)) | arXiv 2020 |
    Neural field | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| Brock et al.  (Brock et al., [2016](#bib.bib21)) | arXiv 2016 | Voxel grid
    | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| Autosdf  (Mittal et al., [2022](#bib.bib114)) | CVPR 2022 | Voxel gird |
    VAE |'
  prefs: []
  type: TYPE_TB
- en: '| Sagnet  (Wu et al., [2019](#bib.bib172)) | TOG 2019 | Voxel gird | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al.  (Li et al., [2020](#bib.bib102)) | AAAI 2020 | Voxel gird | VAE
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pq-net  (Wu et al., [2020](#bib.bib171)) | CVPR 2020 | Voxel gird | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| AdversarialAE  (Zamorski et al., [2020](#bib.bib178)) | CVPR 2020 | Voxel
    gird | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Chart  (Ben-Hamu et al., [2018](#bib.bib19)) | TOG 2018 | Mesh | VAE
    |'
  prefs: []
  type: TYPE_TB
- en: '| SDM-NET  (Gao et al., [2019](#bib.bib52)) | TOG 2019 | Mesh | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| Tm-net  (Gao et al., [2021](#bib.bib53)) | TOG 2021 | Mesh | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| Polygen  (Nash et al., [2020](#bib.bib121)) | ICML 2020 | Mesh | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| PointFLow  (Yang et al., [2019](#bib.bib174)) | CVPR 2019 | Point cloud |
    Normaliz. flow model |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP-Forge  (Sanghi et al., [2022](#bib.bib146)) | CVPR 2022 | Voxel grid
    | Normaliz. flow model |'
  prefs: []
  type: TYPE_TB
- en: '| PDV  (Zhou et al., [2021](#bib.bib187)) | CVPR 2021 | Hybrid: point-voxel
    | Diffusion model |'
  prefs: []
  type: TYPE_TB
- en: '| Magic3D  (Lin et al., [2022](#bib.bib105)) | arXiv 2022 | Neural field-Mesh
    | Diffusion model |'
  prefs: []
  type: TYPE_TB
- en: '| LION  (Zeng et al., [2022](#bib.bib180)) | arXiv 2022 | Mesh | Diffusion
    model |'
  prefs: []
  type: TYPE_TB
- en: '| Point-E  (Nichol et al., [2022](#bib.bib128)) | arXiv 2022 | Neural field-Point
    cloud | Diffusion model |'
  prefs: []
  type: TYPE_TB
- en: Table 1. An overview of 3D generative approaches of 3D shape generation. For
    3D shape generation, each method allows generating editable models for explicit
    representations. Models indicates the DGM types inlcuding GAN, VAE, normalizing
    flow model, and diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. 3D Shape Generation with GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GANs have been developed as a controlled 3D generation method from image data
    that can generate different explicit representations, including point clouds (Cai
    et al., [2020](#bib.bib22)) or voxel grids (Michalkiewicz et al., [2019](#bib.bib112);
    Mescheder et al., [2019](#bib.bib111); Dinh et al., [2014](#bib.bib45); Gulrajani
    et al., [2017](#bib.bib58); Li et al., [2017](#bib.bib103); Litany et al., [2018](#bib.bib106);
    Lorensen and Cline, [1987](#bib.bib109); Nijkamp et al., [2020](#bib.bib130);
    LeCun et al., [2010](#bib.bib97); Genova et al., [2019](#bib.bib54); Lunz et al.,
    [2020](#bib.bib110)), and implicit neural functions, such as occupancy field and
    signed distance function (SDF). Wu et al. adopted the architecture of a generative
    adversarial network to generate the 3D voxel grids relying on capturing the probability
    distribution of 3D shapes  (Wu et al., [2016](#bib.bib170)) (Fig.  [9](#S3.F9
    "Figure 9 ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards Computational
    Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating
    Virtual Architecture in the Metaverse")a). Many approaches already achieve the
    outstanding outcome in a more fine-grained shape  (Wu et al., [2019](#bib.bib172);
    Li et al., [2020](#bib.bib102); Wu et al., [2020](#bib.bib171)). However, the
    general disadvantage of this approach for voxel grids is that fine-grained voxels
    cannot be accomplished due to the cubic increase in computational cost. PLATONICGAN
    and IG GAN  (Henzler et al., [2019](#bib.bib65); Lunz et al., [2020](#bib.bib110))
    also generate the 3D voxel grids models from the unstructured 2D image data with
    GAN. While another 3D representation, point cloud, is the output as raw data through
    depth scanning. For the various problems in generating point clouds with GAN,
    numbers of researchers introduce different approaches, ranging from the converge
     (Achlioptas et al., [2018](#bib.bib3)), utilizing the local contexts  (Shu et al.,
    [2019](#bib.bib158); Valsesia et al., [2019](#bib.bib165); Hui et al., [2020](#bib.bib71);
    Arshad and Beksi, [2020](#bib.bib9)), as well as the high memory consumption  (Ramasinghe
    et al., [2020](#bib.bib141)). The mesh representation is usually utilized as the
    target object in the 3D modeling software. Nevertheless the popularity in the
    design discipline and traditional computer graphics, the difficulties lie in applying
    the deep generation models to the mesh. There are two main reasons. Firstly, non-Euclidean
    data could not directly apply to the convolutional neural networks (CNN). Secondly,
    the difficulty of connecting the mesh vertices to composite the shape is high
     (Shi et al., [2022](#bib.bib157)). Get3D  (Gao et al., [2022](#bib.bib51)) enables
    the high-quality geometry and texture from the 2D image collections by incorporating
    the differentiable surface modeling and differentiable rendering to GANs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. 3D Solid Form Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Reference | Category | Obejctive | Methodology | 3D Representations | Generative
    Models |'
  prefs: []
  type: TYPE_TB
- en: '| (Del Campo et al., [2019](#bib.bib39)) | 2D to 3D | Test AI agency in design
    | Utilizing Style Transfer to train two datasets Baroque and Modern images as
    a basis to form a 3D model | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Özel, [2020](#bib.bib134)) | 2D to 3D | HCI in urban design | Utilizing
    Style Transfer to generate different stylized images and generate 3D geometry
    through procedural modeling | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Ren and Zheng, [2020](#bib.bib144)) | 2D to 3D | Test AI agency in design
    | Utilizing Style Transfer to replace pixels with voxelization units to generate
    3D forms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Liu et al., [2021](#bib.bib107)) | 2D to 3D | Toolkits for 3D generation
    | Utilizing Style Transfer to assist the generation of 3D structure from 2D images
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Yu, [2020](#bib.bib176)) | 2D to 3D | Generate building massing | Utilizing
    pix2pixGAN to generate plan pattern and section pattern, then converted to 3D
    massing | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Di Carlo et al., [2022](#bib.bib43)) | 2D to 3D | Generate building massing
    | Utilizing pix2pixGAN to generate urban morphology to create building massing
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang and Blasetti, [2020](#bib.bib183)) | 2D to 3D | Form finding to assist
    design | Slicing a 3D model and trained with different combinations of 2D styleGAN
    networks, and finally stitching into a 3D model | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang, [2019](#bib.bib181)) | 2D to 3D | Form finding to assist design |
    3D model generation based on 2D plan and section using Style Transfer | - | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang and Huang, [2021](#bib.bib184)) | 2D to 3D | Form finding to assist
    design | Combining the spatial sequence information to generate 3D form from 2D
    images through multi-level deep generative networks such as styleGAN | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Bank et al., [2022](#bib.bib14)) | 2D to 3D | Human and neural network interface
    | Utilizing 3D solid for training to map spatial semantics to a latent space assembled
    using point cloud representations | Point cloud | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| (Asmar and Sareen, [2020](#bib.bib10)) | 2D to 3D | Integrate latent space
    in design | GAN allows for navigation in the latent space to create digital designs
    using vector arithmetic and interpolation techniques, then converting resulting
    images to 3D voxel structures | Voxel grid | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| (Huang et al., [2021](#bib.bib70)) | 2D to 3D | Recognize 2D pattern to 3D
    form | Utilizing Latent space rotation and perspective projection to generate
    3D model | Voxel grid | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| (Kim and Huang, [2022](#bib.bib89)) | 2D to 3D | Recognize 2D pattern to
    3D form | Utilizing Latent space rotation and perspective projection to generate
    3D model | Voxel grid | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| (Çakmak, [2022](#bib.bib23)) | 3D Solid | Extend design cognition | Utilizing
    a GAN Model with a pair of encoder-decoder to process datasets and generate new
    3D models, resulting in different 3D representations like point cloud and mesh
    | Point cloud/Mesh | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| (Veselỳ, [2022](#bib.bib166)) | 3D Solid | Generate building massing | Utilizing
    3D BAG dataset as a basis to train urban morphology data to generate 3D building
    massing | - | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| (de Miguel et al., [2019](#bib.bib37)) | 3D Solid | Generation, manipulation
    and form finding of structural typologies | Utilizing VAE to learn continuous
    latent space to generate new geometries | Voxel grid | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| (Kahraman et al., [2021](#bib.bib83)) | 3D Solid | Solve design problems
    incorporating deep learning | Utilizing VAE to manipulate objects according to
    the different criteria selected | Voxel grid | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| (Sebestyen et al., [2021](#bib.bib154)) | 3D Solid | New way to design parametric
    models | Utilizing VAE to encode and decode geometries through dimensionality
    manipulation | Voxel grid | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang, [2020](#bib.bib182)) | NLP-3D Solid | Language assisted design |
    Utilizing language model to predict housing plan by training large dataset relating
    texts to forms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Guida, [2023](#bib.bib57)) | NLP-3D Solid | Language assisted design in
    HCI | Utilizing diffusion models to generate 3D forms from text input | - | Diffusion
    model |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2. The related works in the architecture fields. The applications in
    the architectural field are sorted into different categories in this table, including
    2D to 3D transposition, 3D solid generation and NLP based 3D form generation.
    The category means the generation methodology: ‘2D to 3D’ means the 3D form generation
    is based on 2D images; ‘3D Solid’ means generating 3D form directly with DGMs
    including GAN, VAE, and diffusion model. ‘NLP’ means the 3D form generation process
    includes text input to assist the output control. ‘3D representations’ in architecture
    encompass explicit point cloud, voxel grid, and mesh. The Objective column explains
    the directions and objectives the research aims to address. The methodology column
    gives an overview to what kind of workflow the generation process proposed. This
    table compares the research in the architectural field.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1d7167dd8f01e6bd24b573071e6e655.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Results for voxel grids from  (Veselỳ, [2022](#bib.bib166)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3485afb5b8e15909790ed942c84e8f93.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) 3D GAN housing utilize 3D GAN to generate housing with 3D assets as input
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2d2c881bc5cd58528fd5f4fede6503b.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The overview methodology of  (Çakmak, [2022](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11. Three examples of applying GANs for architectural designs in 3D solod
    form generation.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the architectural design utilizing GANs in 3D solid form generation
    is all based on explicit representations including voxel grids, point clouds,
    and meshes. 3D solid form generation refers to a direct 3D data acquisition, evaluation,
    transformation, and rearrangement using deep generative models  (Steinfeld et al.,
    [2019](#bib.bib160)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, with improved algorithms, GANs can recognize 3D representations
    such as mesh and point cloud, which shifts the paradigm of generation from 2D
    to 3D by a direct route using 3D point cloud semantic segmentation in 3D spaces.
    Immanuel Koh uses a 3D GAN network to train a large dataset of both exterior and
    interior Singapore high-rise buildings to generate innovative housing typologies
    automatically  (Koh, [2022](#bib.bib92)) (See Fig.  [11(b)](#S3.F11.sf2 "In Figure
    11 ‣ 3.2.2\. 3D Solid Form Generation ‣ 3.2\. 3D Solid Form Generation with the
    GANs ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards Computational
    Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating
    Virtual Architecture in the Metaverse")). It tested the agencies of generative
    spaces using deep neural networks, which inherit the configurations of architectural
    forms by extracting building block arrangements. Moreover, the connection of 3D
    GANs with Houdini can expand the algorithm to integrate with the 3D form generation.
    For instance, Joris Puteneers uses 3D GAN as a form-finding tool in a project
    named ugly & stupid ^(18)^(18)18Source: [https://putteneersjoris.xyz/projects/Ugly%20Stupid%20Honest/ugly_stupid_honest.html](https://putteneersjoris.xyz/projects/Ugly%20Stupid%20Honest/ugly_stupid_honest.html),
    which tested the agency of algorithms in creating artifacts based on image recognition.
    Besides, Cakmak added an encoder-decoder network in GAN to process the datasets
    and generate new 3D models, which are then represented in different alternative
    formats like point cloud and mesh (Çakmak, [2022](#bib.bib23)) (See Fig.  [11(c)](#S3.F11.sf3
    "In Figure 11 ‣ 3.2.2\. 3D Solid Form Generation ‣ 3.2\. 3D Solid Form Generation
    with the GANs ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards Computational
    Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating
    Virtual Architecture in the Metaverse")). This also meant extending design cognition
    by adding AI as an agent in the design thinking process. A noteworthy study has
    been conducted on geometry extraction within urban environments using 3D GANs
     (Veselỳ, [2022](#bib.bib166)) (See Fig.  [11(a)](#S3.F11.sf1 "In Figure 11 ‣
    3.2.2\. 3D Solid Form Generation ‣ 3.2\. 3D Solid Form Generation with the GANs
    ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards Computational Architecture
    of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture
    in the Metaverse")). This enables generated 3D dataset automatically through 3D
    BAG  ^(19)^(19)19An overview of 3D BAG, source:  [https://docs.3dbag.nl/en/](https://docs.3dbag.nl/en/).
    The 3D BAG dataset includes different levels of 3D details, which can be read
    and manipulated by GAN. The model was trained based on three layers of information:
    building geometry, site context, and area of interest. The information is stored
    in raster data for 3D representation of voxel grids in deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: This method of matching training data sets with parameters associated with spatial
    design goals has been widely used in architectural design solutions. Such applications,
    in which GANs as the generation framework, can often design solutions with explicit
    design goals.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Such methods discussed have several limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Fewer variations in style. The performance of GANs frameworks heavily depends
    on the quality and nature of the input data, resulting in limited variations in
    style. In the context of 2D to 3D form finding, a significant proportion of studies
    have relied on styleGAN as the basis for form generation, which tends to produce
    outputs that mimic the style of the “style image” without adjustable options.
  prefs: []
  type: TYPE_NORMAL
- en: Singleness of category. Similarly, since one training process can only process
    one single category of the dataset, the output is constrained to the category
    for design purpose. For example, the 3D-GAN-Housing project has a certain degree
    of repetition in high-rise building design due to the limitation of the trained
    structure(Koh, [2022](#bib.bib92)). Since the dataset is limited, the blocks always
    follow the same evolutionary rule, which lowers the variation in style and is
    limited to specific categories. This can be adapted to modular housing design,
    however, not suitable for virtual space generation.
  prefs: []
  type: TYPE_NORMAL
- en: Unpredictability in design. The image-to-image translation of GANs is characterized
    by unpredictability. The training process of these algorithms is time-consuming
    and requires substantial computing resources. Also, the generative logic underlying
    these processes is that designers can only evaluate their effects once they observe
    the final output. The latent vector undergoes arbitrary modifications in different
    epochs, adding to the complexity and unpredictability of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Topological inconsistency. Firstly, when the 3D forms are constructed solely
    from 2D images, these forms will inevitably carry traces of the slicing process
    leading to a loss of interior details and the overall consistency of the structure.
    Secondly, using a constrained 3D segmentation algorithm poses a significant challenge
    in generating consistent forms, leading to topological inconsistencies in the
    form of gaps and defects in the final output. For instance, applying this method
    to the reconstruction of furniture reveals inconsistencies in the generated 3D
    shapes, as the algorithm needed to be pre-trained on the specific type of objects(Wu
    et al., [2016](#bib.bib170)).
  prefs: []
  type: TYPE_NORMAL
- en: Computing requirements of 3D data. Compared to other models, GAN models can
    typically produce 3D structures that are more detailed and realistic, but they
    are also more unstable and challenging to train. However, converting data from
    2D to 3D usually takes a long time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Architectural Form from Latent Space with Variational Autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.3.1\. 3D Shape Generation with VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consequently, aforementioned GAN approaches, to improve the instability in
    the GAN, Brock et al. introduced a variational auto-encoder to process 3D voxel
    grids  (Brock et al., [2016](#bib.bib21)). It utilizes a pair of encoder-decoder:
    the encoder consists of four 3D convolutional layers to map the information to
    the latent vectors, and the decoder transforms the latent vectors into the 3D
    voxel. As aforementioned, the later work proposes improvements in blurry voxels
    for smooth rounded edges (Mittal et al., [2022](#bib.bib114)). For the point clouds
    as representation, although the research progress is overcoming the difficulties,
    the instability of GAN has derived the invention of the other types of generative
    model based on the encoder in the 3D generation, the VAE and adversarial auto-encoder
    model (AAE)  (Zamorski et al., [2020](#bib.bib178)). The difficulties in generating
    meshes with VAEs are similar to GANs. For the complexity of processing topology,
    The parameterization of mesh called multi-chart approaches  (Ben-Hamu et al.,
    [2018](#bib.bib19)) can handle this irregular structure of meshes. Many approaches
    work on simplifying the process using this method (Gao et al., [2019](#bib.bib52),
    [2021](#bib.bib53); Nash et al., [2020](#bib.bib121)). TM-Net proposes an improved
    approach that defines a textured space on the template cube mesh based on the
    SDM-Net (Gao et al., [2021](#bib.bib53)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Latent Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For research on architectural generation, VAEs extract information through the
    latent space with a pair of encoder-decoder. As aforementioned, the limitations
    in producing scientific and accurate design results with GANs derive from the
    framework itself. Furthermore, most existing generated architecture with DL aided
    have focused on 2D drawings. Consequently, there is a gap in these approaches
    regarding their ability to extract and utilize essential low-level spatial semantic
    and structural features to understand design intent and factors. Azizi et al.
    (Azizi et al., [2020](#bib.bib12)) proposed VAE to enable encoding and decoding
    information about the spatial utilization of people’s movements and activities
    in space to generate reliable and plausible architectural compositions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Architectural information extraction from latent space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85d6ebd2884081ad888b8efa1652c024.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The framework of VAE processing 3D-structure data in Deep Form Finding  (de Miguel
    et al., [2019](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13a4648050b0a1f0ac7975c9f0e032a5.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) An overview methodology in  (Azizi et al., [2020](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12. Two pioneering architectural designs utilized VAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'In pioneering research of VAE integrated structural generation project Deep
    Form Finding (de Miguel et al., [2019](#bib.bib37)), the researchers used labeled
    connectivity vectors extracted from ”3D-canvas” as data representation in rectangular
    3D cubes since the cubes are convenient to be used to illustrate the 3D structure
    information of any forms (See Fig.  [12(a)](#S3.F12.sf1 "In Figure 12 ‣ 3.3.3\.
    Architectural information extraction from latent space ‣ 3.3\. Architectural Form
    from Latent Space with Variational Autoencoder ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    The outcome achieved 3D voxelized wireframes of architectural forms through VAE
    models, where the encoder processes the input data and maps it to a lower-dimensional
    latent space, while the decoder takes the latent representation and maps it back
    to the original input space. The VAE model can learn continuous latent distributions
    of the input data and output hybrids of different forms with different style strengths.
    Since researchers found that the 3D GAN is hard to learn 3D information, VAE was
    considered to have higher potential and has been used to test the capabilities
    of deep neural networks in manipulating 3D geometries in the architectural field.
    Another proof of concept application is the design of a 3D voxel chair using multi-object
    VAE (Kahraman et al., [2021](#bib.bib83)). This application aims to generate different
    types of chairs based on pre-defined criteria, ranging from leisure to work. VAE
    has also been used to morph multiple simple objects such as cylinders, cubes,
    and spheres into new shapes within a given composition range (Sebestyen et al.,
    [2021](#bib.bib154)). From the above application, we can see that although VAE
    is a well-developed neural network, the usage of complex space generation in architecture
    is still very limited. While another approach has targeted training by examining
    the floor plan of the building (Azizi et al., [2020](#bib.bib12)), which is not
    associated with the construction logic of the virtual space, the approach contains
    the consideration of human factors involved in the HCI methodology. The floor
    plan is a potential representation that encodes multiple features. Autoencoders
    represent the graph as a vector in continuous space. The attributed graph as the
    intermediate representation encodes spatial semantics, structural information,
    and crowd behavioral features (See Fig.  [12(b)](#S3.F12.sf2 "In Figure 12 ‣ 3.3.3\.
    Architectural information extraction from latent space ‣ 3.3\. Architectural Form
    from Latent Space with Variational Autoencoder ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).'
  prefs: []
  type: TYPE_NORMAL
- en: VAEs utilize pointwise loss to find a probability density by explicit representations
    to obtain an optimal solution by minimizing a lower bound on the log-likelihood
    function, which results in accurate generation results but lower resolution. GANs
    learn to generate from training distributions through playing zero-sum-game, resulting
    in uncertain generation results but can ensure high-quality data input. This results
    in different applications in the architectural field. For example, the applications
    in GANs are typically used for testing the agencies of AI, providing conceptual
    design options and approaching the democratization of design. While VAEs are always
    being tested in the form-finding process, to generate different design options
    available for different criteria and scenarios. However, most research incorporating
    either GANs or VAEs in design only provides a general approach to visual aesthetics
    instead of the design solutions on spatial functions and structures (Regenwetter
    et al., [2022](#bib.bib143)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. 3D-Aware Image Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Method Names | Publication & Year | 3D Repre- sentations | Single/multiple-view
    | Geometry | Editability | Controllability | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '| Camera | Object |'
  prefs: []
  type: TYPE_TB
- en: '| NeRF  (Gu et al., [2021](#bib.bib56)) | CVPR 2019 | Neural field | multiple
    | ✓ | - | ✓ | ✓relighting | 3D Novel View Synthesis |'
  prefs: []
  type: TYPE_TB
- en: '| HoloGAN  (Nguyen-Phuoc et al., [2019](#bib.bib126)) | CVPR 2019 | Voxel grid
    | single | - | - | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Pi-GAN  (Chan et al., [2021](#bib.bib29)) | CVPR 2021 | Neural field | single
    | ✓ | - | position | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Giraffe  (Niemeyer and Geiger, [2021](#bib.bib129)) | CVPR 2021 | Neural
    field | single | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| StyleSDF  (Or-El et al., [2022](#bib.bib132)) | CVPR 2022 | Neural field
    | single | ✓ | - | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| StyleNeRF  (Gu et al., [2021](#bib.bib56)) | arXiv 2021 | Neural field |
    single | ✓ | - | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DreamField  (Jain et al., [2022](#bib.bib77)) | CVPR 2022 | Neural field
    | multiple | ✓ |  | ✓ | - | CLIP: Text input |'
  prefs: []
  type: TYPE_TB
- en: '| DreamFusion  (Poole et al., [2022](#bib.bib139)) | arXiv 2022 | Neural field
    | single | ✓ | ✓ | ✓ | ✓ | Text input |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP-NeRF  (Wang et al., [2022](#bib.bib167)) | CVPR 2022 | Neural field
    | single | - | - | ✓ | - | CLIP: Text input |'
  prefs: []
  type: TYPE_TB
- en: Table 3. An overview of 3D generative approaches of 3D-Aware Image Synthesis.
    Single/multiple represents the result generated by a single image adopting a sample
    of single-view or multiples image adopting multiple-view images. Geometry indicates
    whether this method allow to export to mesh. Editability indicates whether this
    generation process enable to edit, such as composing objects in scene. 3D-aware
    image synthesis perform by controllability including camera pose, position or
    object pose, location, relighting, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The 3D-aware image synthesis introduces expressive and efficient neural scene
    representations inspired by the 3D view synthesis like NeRF  (Xia and Xue, [2022](#bib.bib173)).
    It exhibits its capability of 3D view-consistent rendering and efficient and expressive
    presentation, as well as interactive edibility. It is super appropriate for the
    field of architecture to adopt 3D-aware synthesis since this method enables filling
    the gap lacking large-scale and high-quality 3D datasets in the field of DL-assisted
    architecture. 3D-aware synthesis only relies on supervising 2D images, which adopt
    differentiable neural rendering. This process involves the use of sophisticated
    techniques such as depth estimation and multi-view stereo by generating a 3D-aware
    image from 2D images. It exhibits its capability of 3D view-consistent rendering.
    Since without 3D representations for VAE-based models to render, most 3D-aware
    image syntheses utilize a GAN-based model sampling the latent vectors and decoder
    it to target a 3D representation. Although some methods implement the export of
    mesh models (Chan et al., [2021](#bib.bib29); Or-El et al., [2022](#bib.bib132);
    Gu et al., [2021](#bib.bib56)), however, according to this survey, existing architectural
    studies have not adopted this novel approach. We provide proof of its potential
    in virtual building generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1\. 3D-aware image synthesis and its editablity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '3D-aware image synthesis has achieved tremendous progress made in the implicit
    representation of 3D models  (Chan et al., [2021](#bib.bib29); Jang and Agapito,
    [2021](#bib.bib79); Niemeyer and Geiger, [2021](#bib.bib129); Schwarz et al.,
    [2022](#bib.bib153)), in terms of two mainstream problems, resolution, and multi-view
    consistency. It utilizes image synthesis in a more controllable way to generate
    synthetic 3D scene representations by incorporating generative models. Later research
    has focused on generating 3D-aware images with the integration of GAN-based model
     (Nguyen-Phuoc et al., [2019](#bib.bib126); Chan et al., [2021](#bib.bib29)) (Fig.
     [9](#S3.F9 "Figure 9 ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards
    Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning
    for Generating Virtual Architecture in the Metaverse")c d f). For instance, HoloGAN
    (Fig.  [9](#S3.F9 "Figure 9 ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT
    ‣ Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep
    Learning for Generating Virtual Architecture in the Metaverse")c) can be trained
    end-to-end from unlabeled 2D images without pose labeling, 3D shape, or the same
    view(Nguyen-Phuoc et al., [2019](#bib.bib126)). It is the first unsupervised model
    for learning from natural images. Some latest studies  (Chan et al., [2021](#bib.bib29);
    Or-El et al., [2022](#bib.bib132); Gu et al., [2021](#bib.bib56)) prove their
    framework could predominantly improve two dominant problems for 3D-aware synthesis,
    high resolution and consistency of multiple views of synthetic images. The SDF-based
    method defines detailed 3D surfaces, leading to consistent body drawing. For instance,
    StyleSDF shows higher quality results in terms of visual and geometric quality
     (Or-El et al., [2022](#bib.bib132)) (See Fig. [9](#S3.F9 "Figure 9 ‣ 3\. Generated
    3D Architecture: A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty:
    A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in
    the Metaverse")d). Moreover, cutting-edge methods demonstrate that integrating
    3D-aware images with CLIP model(Jain et al., [2022](#bib.bib77); Wang et al.,
    [2022](#bib.bib167)) enables 3D geometry generation from natural language descriptions.
    While DreamFusion conducts a loss derived from the distillation of a 2D diffusion
    model instead of CLIP  (Poole et al., [2022](#bib.bib139)). The originality of
    Dream Field contains a pre-training process of 2D image-to-text models to optimize
    the underlying 3D representations. On the other hand, some progress in advanced
    leaps out of the solid box of pre-training 2D image-to-text models. DreamFusion
    incorporates the diffusion model as a strong image prior to this text-to-image
    pre-training, improving the efficiency of the generation. In addition, 3D-aware
    synthesis is applicable to incorporating other deep generative models, such as
    the very recent diffusion models. This advanced diffusion model will be specifically
    elucidated in the following subsection. The main goal of 3D-aware image synthesis
    is gaining the explicit camera pose in the task (Shi et al., [2022](#bib.bib157)).
    The controllability takes users to enter a more engaging and interactive environment.
    Some approaches also support the editability of the object pose. For instance,
    GIRAFFE allows to pan and rotate the obtained 3D objects in the scene  (Niemeyer
    and Geiger, [2021](#bib.bib129)). StyleNeRF also allows altering style attributes
    while supporting style blending, inversion, and semantic editing of the generated
    results  (Gu et al., [2021](#bib.bib56)). This editability provides various solutions
    from the perspective of the subdivision of generated target.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2\. 3D-aware image utilized in architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Chan et al., [2021](#bib.bib29)) conducts an integrated method by transforming
    implicit neural representation into mesh representation, which performs an ability
    to editability in 3D space for architecture. StyleSDF  (Or-El et al., [2022](#bib.bib132))
    and StyleNeRF  (Gu et al., [2021](#bib.bib56)) also implement methods converting
    to geometry. Meng et al. as pioneering architects have launched a configurative
    Colab with user-friendly interaction for the creators supporting conditional text
    input and some parameters including style attributes based on DreamField  (Jain
    et al., [2022](#bib.bib77))  ^(20)^(20)20Source:[https://github.com/shengyu-meng/dreamfields-3D](https://github.com/shengyu-meng/dreamfields-3D).
    However, the concern is that single effectiveness for simple objects. These methods
    have plenty of limitations on the high resolution, which has the incapability
    of generating 3D precise structures with internal spaces. Efficiency is reduced
    hugely for the task of generating complex architectural structures. As a result,
    it is difficult to obtain a valid building with internal structure and functional
    space from image synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: Although 3D-aware synthesis is relatively premature for virtual architecture,
    its ability to convert to mesh, controllability, and multi-modality with linguistic
    descriptions have demonstrated its potential for generating complex and unique
    architectural forms. In contrast to generation for explicit representations, it
    offers more flexible, continuous, and efficient representations of geometry, as
    well as the capability of integration with other deep learning generation techniques.
    As the field of 3D DGMs and 3D-aware image synthesis evolves, architects may increasingly
    explore the potential of this technique as a common toolkit for virtual architectural
    design.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Emerging Generation Based on Diffusion Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, diffusion as one of the deep generative models has gained a growing
    interest in generating 3D shapes due to its high quality with fine details and
    controllable attributes. It outperforms Generative Adversarial Networks (GANs)
    in fidelity due to intricate details and sharp edges while maintaining stability
    during training and reducing the risk of mode collapse. This superiority stems
    from their ability to enable fine-grained control over the generation process
    with specific attributes or interpolation between shapes smoothly and continuously.
    In contrast, a less controlled approach to GANs dictates its difficulty in specifying
    the desired properties of the output.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1\. 3D Diffusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DreamFusion adopts diffusion models to denoising images for a high-quality image
    for 3D-aware image synthesis  (Poole et al., [2022](#bib.bib139)). Despite the
    flexibility of conditional diffusion sampling, as revealed by studies of GANs,
    traditional diffusion as a DGM only samples pixels. Ben et al. abandoned processing
    large amounts of data from 2D images to 3D while generating a 3D model directly.
    In DreamFusion, a parameter of 3D volume, instead of images’ indicators, $\theta$,
    and g is a volumetric renderer. It yields a sample through an optimization performed
    by minimizing a loss function. Two limitations exist, DreamFusion was improved
    in the latest research, known as Magic3D  (Lin et al., [2022](#bib.bib105)), which
    are the low resolution of geometry and textures and the expensive computation
    as well as intensive memory. LION  (Zeng et al., [2022](#bib.bib180)) has a higher
    quality performance by utilizing the diffusion models combined with a hierarchy
    VAE. Its flexibility of operation and application has also increased compared
    to previous models 3D DDMs  (Ho et al., [2020b](#bib.bib67)) due to conditional
    synthesis and shape interpolation. Unlike most existing DDPMs, PVD  (Zhou et al.,
    [2021](#bib.bib187)) employs a unified probabilistic formula to generate high-fidelity
    3D shapes with multiple results from a single-view depth scan of a real object.
    Moreover, diffusion models allow for the generation of 3D shapes with controllable
    attributes such as shape and texture, which can be modified by conditioning the
    generation process on specific attributes. These findings suggest that diffusion
    models may offer a more robust and controlled approach to 3D shape generation,
    particularly regarding complex shapes with intricate details and specific attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2\. 3D Diffusion Applications in Architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d71ddad5eeac1ea307a3e8b66e78e962.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Selected results in (Zhang, [2020](#bib.bib182)) of the linguistics-based
    architectural form DGMs that make 3D form predictions based on the text descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88fccc05581399eeb2bd5cca2935c576.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Methodology incorporating stable diffusion (SD) with Lora by AIG; Source:
    [https://www.bilibili.com/video/BV1Qb411Z7UP/](https://www.bilibili.com/video/BV1Qb411Z7UP/).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0a8bf54c6e7b4c57bb11bdbdb685a191.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) A plugin utilized diffusion model in 3D modeling software Rhino for  (Guida,
    [2023](#bib.bib57)).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13. Architectural designs utilized diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of diffusion models in architecture is an emerging and promising field
    for development. Integrating an application programming interface (API) directly
    into the diffusion model in Rhino’s visual programming environment, Grasshopper
    has the potential to usher in a paradigm shift in the generation of architectural
    3D forms. For example, morphological heatmap images transforming from 3D architecture
    models can be trained using Lora models, in which stable diffusion can further
    edit (See Fig.  [13(b)](#S3.F13.sf2 "In Figure 13 ‣ 3.5.2\. 3D Diffusion Applications
    in Architecture ‣ 3.5\. Emerging Generation Based on Diffusion Model ‣ 3.4.2\.
    3D-aware image utilized in architecture ‣ 3.4.1\. 3D-aware image synthesis and
    its editablity ‣ 3.4\. 3D-Aware Image Synthesis ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    The generated grayscale images processed by stable diffusion include height information,
    which can be easily transformed into a mesh model in the same modeling environment
     ^(21)^(21)21Source: [https://www.bilibili.com/video/BV1Qb411Z7UP/](https://www.bilibili.com/video/BV1Qb411Z7UP/).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3\. Controllability and Generative Models Conditioned on Text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Text-to-3D models as a featured method have surged their development in generative
    3D shapes in recent two years (Chen et al., [2018](#bib.bib30); Poole et al.,
    [2022](#bib.bib139); Liu et al., [2022](#bib.bib108); Lin et al., [2022](#bib.bib105);
    Nichol et al., [2022](#bib.bib128)). The earliest research we tracked is text2shape (Chen
    et al., [2018](#bib.bib30)), in which 3D models with color and shape paired with
    natural language formed datasets to build implicit semantic links. Recent research
    has made a remarkable breakthrough in associating text and 3D models with unsupervised
    learning. Similar to 3D-aware synthesis, most methods utilize CLIP with unsupervised
    learning (Lin et al., [2022](#bib.bib105); Nichol et al., [2022](#bib.bib128)).
  prefs: []
  type: TYPE_NORMAL
- en: The text-to-3D approach demonstrates superior controllability compared to other
    methods for generating 3D models, along with customized style attributes. This
    approach interprets textual prompts, resulting in an intuitive visual representation
    catering to design intention. For instance, Magic 3D  (Lin et al., [2022](#bib.bib105))
    has developed a toolkit that offers advanced control over 3D-generated styles
    and content through various image conditioning and prompt-based editing to achieve
    the desired result. As a result, the text-to-3D approach democratizes 3D geometry
    generation, providing access for individuals with varying levels of expertise
    to produce creatively. As aforementioned, integrating Colab with DL algorithms
    also serves as a gateway for designers, artists, and amateurs to participate in
    the burgeoning field of content production.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4\. 3D Form Driven by Text in Architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are some initial applications using languages as starting points to utilize
    design. However, the generative model is constrained in such applications. For
    example, Del Campo used attentional GAN (AttnGAN) to assist the brainstorming
    process for transforming written ideas of multipurpose spaces to visual outputs
    (del Campo, [2021](#bib.bib38)). Then, the final outcome was based on the previously
    demonstrated visuals. Zhang developed a machine-learning framework capable of
    encoding the input geometry into a new geometry by using text to form a prediction
    (Zhang, [2020](#bib.bib182)) (See Fig.  [13(a)](#S3.F13.sf1 "In Figure 13 ‣ 3.5.2\.
    3D Diffusion Applications in Architecture ‣ 3.5\. Emerging Generation Based on
    Diffusion Model ‣ 3.4.2\. 3D-aware image utilized in architecture ‣ 3.4.1\. 3D-aware
    image synthesis and its editablity ‣ 3.4\. 3D-Aware Image Synthesis ‣ 3\. Generated
    3D Architecture: A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty:
    A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in
    the Metaverse")). In this framework, different usage of spaces has been trained
    with adjacent matrices to understand the linguistic instructions. With the integration
    of natural language supervision, the diffusion models exhibit high-quality performance
    in form generation and have great potential to become HCI tools. George Guida
    explored the user interface integration in the 3D form generation process for
    designers to embrace more design opportunities in a multi-modal loop by combining
    these user-friendly language models (Guida, [2023](#bib.bib57)) (See Fig.  [13(c)](#S3.F13.sf3
    "In Figure 13 ‣ 3.5.2\. 3D Diffusion Applications in Architecture ‣ 3.5\. Emerging
    Generation Based on Diffusion Model ‣ 3.4.2\. 3D-aware image utilized in architecture
    ‣ 3.4.1\. 3D-aware image synthesis and its editablity ‣ 3.4\. 3D-Aware Image Synthesis
    ‣ 3\. Generated 3D Architecture: A PARADIGM SHIFT ‣ Towards Computational Architecture
    of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture
    in the Metaverse")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Research Agenda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Sections 2 and 3, we investigate that building generation in virtual worlds
    urgently requires HCI and CAD 3D construction methods correlated with human needs
    to achieve novel and liberal building forms that are efficient and intimate to
    humans. The 3D building generation methods that we investigated rely on an artificial
    intelligence framework, among which HCI methodology takes the responsibility of
    assistance for generating and constructing in the virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/356a154b31a04c1e645d735e431653b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14. Research agendas in a full process of DGMs-assisted architectural
    design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although many studies have pointed to a wide variety of applications in the
    intersection of VR and architecture, there is still a lack of systematic evaluation
    of these interactions in such conditions. What’s more, it is crucial to use interactive
    techniques to quantify the generation of virtual buildings systematically and
    to assess whether their needs are being met. Additionally, further exploration
    of interactive methods in generative approaches is lacking (Lin and Lo, [2021](#bib.bib104))
    (See Fig.  [14](#S4.F14 "Figure 14 ‣ 4\. Research Agenda ‣ 3.5.4\. 3D Form Driven
    by Text in Architecture ‣ 3.5\. Emerging Generation Based on Diffusion Model ‣
    3.4.2\. 3D-aware image utilized in architecture ‣ 3.4.1\. 3D-aware image synthesis
    and its editablity ‣ 3.4\. 3D-Aware Image Synthesis ‣ 3\. Generated 3D Architecture:
    A PARADIGM SHIFT ‣ Towards Computational Architecture of Liberty: A Comprehensive
    Survey on Deep Learning for Generating Virtual Architecture in the Metaverse")).
    In this article, the application of virtual reality technology in the architectural
    design process is inefficient due to interaction limitations  (Lin and Lo, [2021](#bib.bib104)).
    Therefore, in order to encourage this research topic to conduct, we advocate the
    following research topics to enhance the ease of implementation in the production
    pipeline. This section reveals several research agendas in the generative approach
    for virtual building by focusing on this HCI methodology and human factors.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Data Limitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data Limitation is one of the greatest challenges in both virtual architectures
    and computational architectures. Regenwetter et al. announced three main issues
    lying on this challenge. First, the lack of available and public datasets of 3D
    datasets becomes a hurdle to design industries. The second is insufficient data
    size in those datasets. Third, data sparsity and bias in datasets exist. Our review
    reveals that data limitation on 3D datasets is vital for virtual architecture.
    It led to a restriction on the massive computation from 3D solid generation whenever
    in GANs or VAEs, since those designers could not find appropriate datasets of
    3D buildings.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Editability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1\. Editing in 3D Shape Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned in Section 3, we described the significance of editability for
    design industries that rely on 3D model editing. This significance lies in the
    timeliness of feedback and adjustments and in the perception of the 3D space.
    Some work has demonstrated the autonomy of editing in 3D shape generation techniques
    available to users. For example, Liu et al. proposed a method for user-centric
    3D shapes generation assisted by a 3D GAN by drawing the target model as 3D voxel
    grids. While more approaches demonstrate methods that indirectly edit 3D models
    through implicit representations (Hao et al., [2020](#bib.bib62); Ibing et al.,
    [2021](#bib.bib72); Deng et al., [2021](#bib.bib42); Zheng et al., [2021](#bib.bib186)).
    Compared to the explicit representations, their more compact approaches includes
    sparse 3D points or bounding boxes  (Shi et al., [2022](#bib.bib157)). The former
    approach is widely used in virtual building designs as a user-centered participatory
    design experience. While a gap exists in the latter approach for producers to
    meet specific design goals, we need more user-friendly development tools, software
    or online platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Editing in 3D-aware Synthesis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For 3D-aware image synthesis methods, there is also still a lot of room for
    improvement in editability. All 3D-aware controllers support the camera pose.
    While some methods also accomplish the insertion of new objects and pose control.
    In contrast to 3D shape generation, 3D-aware image synthesis precludes direct
    user manipulation in 3D space and requires latent vector editing to control the
    composition, shape, and appearance. These latent vectors model all other variables
    that are not captured by physical factors, while being able to control small changes
    in the scene, such as lighting, coloring and so on  (Xia and Xue, [2022](#bib.bib173)).
    Furthermore, there are several approaches that allow additional inputs to alter
    the editing of the scene, such as textual descriptions, semantic labels (Liu et al.,
    [2022](#bib.bib108); Jahan et al., [2021](#bib.bib76); Fu et al., [2022](#bib.bib50)),
    images (Sanghi et al., [2022](#bib.bib146)), and parameter controls. This approach
    is analogous to the parametric design for architecture and is more closely aligned
    with the classical design process, controlling the final form from a user-adjustable
    panel. Therefore, this edibility is one of the reasons for its rapid popularity
    in a large number of AIGC-driven 3D designs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.3.1\. The Efficiency of Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The fidelity, photorealistic, and geometric quality are important elements of
    the evaluation metrics. However, existing evaluation methods lack metrics to evaluate
    these. Intuitively, these generated models do not meet the normative conditions
    for use. In addition, Regenwetter et al. points out that due to the inadequacy
    of current 3D databases and the lack of real references (Regenwetter et al., [2022](#bib.bib143)).
    The evaluation criteria are clustered on the stability of generative models rather
    than reflecting the distance from the real sample. In general, there is a great
    lack of reliable methods to assess the difference between the generated real results
    and the target ones.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. Qualitative Human Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Human perception, albeit primarily qualitative, becomes essential feedback for
    designers as an indicator of understanding user-design fitness, hence improving
    design solutions. One of the typical methods of user evaluation concerns the user’s
    behavior, such as movement path, attention, and so on. Ding et al. proposed that
    these spatial perceptions are related to building structural features, building
    spatial geometric features, and building spatial functional attributes (Ding et al.,
    [2022](#bib.bib44)). From both qualitative and quantitative perspectives, it is
    difficult to assess the practical impact of collecting and processing these biological
    and perceptual data into associations with spatial elements in complex design
    decisions. Nevertheless, eyesight as a human sense has been used to build the
    relationship to the qualitative human evaluation. Some research focused on eyesight
    to visualize the attention evaluation to better understand the users’ behaviors
    and make the evaluation  (Narahara, [2022](#bib.bib120); Wells et al., [2021](#bib.bib169);
    Pei et al., [2021](#bib.bib136)). An Eye-Tracking Voxel Environment Sculptor (EVES)
    was developed, in which eye-tracking data obtained from the designer is used directly
    as input in the modeling environment to manipulate and sculpt voxels  (Wells et al.,
    [2021](#bib.bib169)). These methods of qualitative human perception provide a
    number of viable evaluation metrics for deep generative models to determine the
    spatial perceptual and emotional impact.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Aesthetics Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Aesthetic evaluation enables us to operate automated computer methods to evaluate
    results and optimize spaces based on human positions and understanding of the
    environment, while freeing up human resources. For example, virtual spaces can
    be Interactive and variable spontaneously based on the data obtained from the
    assessment. This integration of the criteria of quantitative aesthetic evaluation
    and optimized architectural forms have the potential to be embedded in the pipeline
    of generative forms. Various research on the generative shape or space by CAD
    mentioned the aesthetics evaluation. It mainly concentrates on feature extraction
    and computational evaluation of visual features. Integrating visual aesthetic
    criteria is challenging because it applies the quantitative approach to assess
    qualitative formal features. The evaluation of spatial and structural designs
    in computer vision involves visual feature analysis (VCA) as well as structural
    analysis (SA) and geometric analysis (GA)  (Narahara, [2022](#bib.bib120)).
  prefs: []
  type: TYPE_NORMAL
- en: One type represents the evaluation of the form of the building itself by standardizing
    the criteria line of a series of visual features. Such as intricacy, heterogeneity,
    continuity, and surface recesses based on a design project for an exhibition are
    evaluated  (STUART-SMITH and DANAHY, [2022](#bib.bib161)). These results demonstrate
    that a limited set of aesthetic design criteria can be correlated with structural
    and geometric data in quantitative indicators.In addition, some studies give insight
    into the evaluation of visual features for specific geometric models (Kavakoglu,
    [2021](#bib.bib85)). The other type represents an example of quantitative aesthetic
    evaluation embedded in a parametric model (Sardenberg, [2019](#bib.bib148)). Nonetheless,
    the proposed visual features and corresponding analytical criteria for 3D deep
    generative models provide an extremely limited form of aesthetic assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Human-Computer Interactive Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.4.1\. User-centered Adaptive Design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing research that exhibits the real-time form interaction with the behavior
    or design purpose still focuses mainly on the interaction between parametric design
    and building form. The parameter design and the black box of algorithms in the
    DL approach are two distinct ways. So far, how to design and generate forms in
    real-time in the DL approach is still an unknown problem. Ubiquitous computing
    can be more useful in a virtual environment through smart wearable devices. Simulation
    of atmospheric qualities in VR explores space and directionality in virtual scenarios
    with thermal perception as feedback  (Sheehan et al., [2021](#bib.bib155)). Another
    project explores the thoughts and emotions of the presence of scenarios in virtual
    space, translating physiological data into digital space  (Tosello, [2003](#bib.bib164)).
    Both of them are only toward artistic expression. On the other hand, the existing
    real-time interactions, such as perceptions of data, impact architectural forms
    and focus on the study of parametricism architecture (Guo et al., [2021](#bib.bib60);
    Zarei et al., [2021](#bib.bib179)). This parametricism is closer to the classical
    design process and is not identical to the fully automatized generation through
    DGMs. We believe virtual architecture’s future is toward interactive self-response
    forms based on real-time data by DL-generated approaches. That will lead to the
    liberty of virtual architecture. We believe there is still a long journey to go
    through before such implemented interactions in the development, integrated with
    a DL-generated approach.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2\. Human Perception
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The collection of biosignals considering the five senses in HCI is one typical
    research focus, as well as considering how to process the context-aware interaction.
    At the same time, perception is the organization, recognition and interpretation
    of sensory information to understand the presented information and the environment.
    In addition, the method of user evaluation often facilitates HCI as a subsequent
    phase after biosignal collection, context awareness, and understanding of the
    information and environment.
  prefs: []
  type: TYPE_NORMAL
- en: User evaluation often takes the role of an indicator for testing and iterative
    purposes of design results. It usually considers the user’s performance in completing
    a specific task, a process that requires quantitative and qualitative assessments,
    including biodata collection, and group interviews, among other methods. Since
    the evaluation comes from the biosignals and perceptions generated by the human
    experience in the virtual environment, we elaborate on them in this section. Space
    could make the users produce a particular emotional condition, such as relaxed
    or nervous, leveraging the space attribution in terms of forms, perspectives,
    lights, coloring (Tonn, [2017](#bib.bib163)) and materials (Barsan-Pipu et al.,
    [2020](#bib.bib15)). The method of user evaluation has frequently been employed
    in the field of HCI as a test of design results and as an indicator of the iterative
    purpose. It usually takes into account the user’s performance in completing a
    specific task. This process entails the use of quantitative and qualitative questionnaire
    assessments, biological data, and group interviews, among other methods.
  prefs: []
  type: TYPE_NORMAL
- en: The information-physical design of emotional computing systems is important
    in 3D generation methods. The research in HCI has been applied extensively in
    the investigation of spatially perceived data  (Ding et al., [2022](#bib.bib44)),
    neuroscientific cognitive biosignatures  (MUN et al., [2019](#bib.bib117); Ding
    et al., [2022](#bib.bib44)), eye-tracking  (Barsan-Pipu et al., [2020](#bib.bib15);
    Pei et al., [2020](#bib.bib137)) and biosignature-based emotion metrics  (Nguyen
    et al., [2019](#bib.bib125); Homolja et al., [2020](#bib.bib68); Pei et al., [2020](#bib.bib137)).
    The continued surge in recent years towards wearable devices with embedded sensors
    and actuators has encouraged the exploration of virtual spaces  (Diniz et al.,
    [2019](#bib.bib46)) within a design framework that facilitates enhanced human
    interaction. The field relies heavily on interdisciplinary collaboration at the
    intersection of 3D modeling, visualization in virtual reality, sensing technologies,
    and smart wearables to evolve human-machine-environment interactions and create
    a heightened awareness of what constitutes our spatial experience.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3\. Participatory Design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This refers to the involvement of stakeholders in the design process and decision-making,
    ranging from similative visualization to data collection, to evaluation. Virtual
    environments can provide co-collaborative environments and reality-based simulations
    for participatory design.It is an increasingly important research area in the
    study of architecture  (Kwiecinski et al., [2017](#bib.bib94)). One key feature
    of participatory research is inclusiveness, including adapting the research environment,
    methodology and dissemination routes to permit the widest and most accessible
    engagement. Kim et al. deployed pix2pix and CycleGAN into real-time collective
    design toolkits for streets to enable citizens to stylize their own urban streets (KIM
    et al., [2022](#bib.bib88)). The benefit allows non-professionals and professional
    designers to engage in collaborative design decisions at the same level. The critical
    input of non-professionals plays an indispensable role in the design, e.g., government
    personnel, and community members.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is a comprehensive investigation reflecting and providing a rethinking
    of the relationship among the 3D shape or image generation, social happenings
    and the scale of computational architecture. It is a synthesis of the “social”
    and the “object”. The approaches of generative approaches by CAD are very common
    and mostly already regard technical principles and design discipline in terms
    of regulations, economic and social constraints, and the efficiency of the space.
    In the survey, we investigated the related works that indicate the approaches
    for deep neural networks to produce virtual architectures automatically. We are
    concerned with both 3D-generation approaches and design disciplines, aiming to
    fill the current research gap from an interdisciplinary point of view. Three categories
    including GANs, VAEs, and DDPMs are used in the architectural design. However,
    due to the technical barrier and limited datasets, the architects only use cumbersome
    approaches to generate the computational architecture. Our survey unveils that
    research is not systematic until now, especially for virtual architecture. We
    call for further work on the 3D datasets, edibility, evaluation metrics and human-computer
    interactive design.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdar et al. (2021) Moloud Abdar et al. 2021. A review of uncertainty quantification
    in deep learning: Techniques, applications and challenges. *Information Fusion*
    76 (2021), 243–297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achlioptas et al. (2018) Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas,
    and Leonidas Guibas. 2018. Learning representations and generative models for
    3d point clouds. In *International conference on machine learning*. PMLR, 40–49.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggarwal et al. (2021a) Alankrita Aggarwal et al. 2021a. Generative adversarial
    network: An overview of theory and applications. *International Journal of Information
    Management Data Insights* 1, 1 (2021), 100004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggarwal et al. (2021b) Alankrita Aggarwal, Mamta Mittal, and Gopi Battineni.
    2021b. Generative adversarial network: An overview of theory and applications.
    *International Journal of Information Management Data Insights* 1, 1 (2021), 100004.
    [https://doi.org/10.1016/j.jjimei.2020.100004](https://doi.org/10.1016/j.jjimei.2020.100004)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akinosho et al. (2020) Taofeek D Akinosho et al. 2020. Deep learning in the
    construction industry: A review of present status and future innovations. *Journal
    of Building Engineering* 32 (2020), 101827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alom et al. (2019) Md Zahangir Alom et al. 2019. A state-of-the-art survey on
    deep learning theory and architectures. *electronics* 8, 3 (2019), 292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appleton (1996) Jay Appleton. 1996. *The experience of landscape*. Wiley Chichester.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arshad and Beksi (2020) Mohammad Samiul Arshad and William J Beksi. 2020. A
    progressive conditional generative adversarial network for generating dense and
    colored 3D point clouds. In *2020 International Conference on 3D Vision (3DV)*.
    IEEE, 712–722.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Asmar and Sareen (2020) Karen El Asmar and Harpreet Sareen. 2020. Machinic
    Interpolations: A GAN Pipeline for Integrating Lateral Thinking in Computational
    Tools of Architecture. (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ASSRU (1999) ASSRU. 1999. Algorithmic Social Sciences Research Unit (ASSRU).
    [http://www.assru.org/index.html](http://www.assru.org/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azizi et al. (2020) Vahid Azizi et al. 2020. Floorplan embedding with latent
    semantics and human behavior annotations. In *Proc. of the 11th Annual Symp. on
    Simulation for Architecture and Urban Design*. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baduge et al. (2022) Shanaka Kristombu Baduge et al. 2022. Artificial intelligence
    and smart vision for building and construction 4.0: Machine and deep learning
    methods and applications. *Automation in Construction* 141 (2022), 104440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bank et al. (2022) Mathias Bank, Viktoria Sandor, Kristina Schinegger, and Stefan
    Rutzinger. 2022. Learning Spatiality-A GAN method for designing architectural
    models through labelled sections. (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barsan-Pipu et al. (2020) Claudiu Barsan-Pipu, Nathalie Sleiman, and Theodor
    Moldovan. 2020. Affective Computing for Generating Virtual Procedural Environments
    Using Game Technologies. (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bartle (2004) Richard A Bartle. 2004. *Designing virtual worlds*. New Riders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bartle (2010) Richard A Bartle. 2010. From MUDs to MMORPGs: The history of
    virtual worlds. *International handbook of internet research* (2010), 23–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bava (2020) Alessandro Bava. 2020. Computational Tendencies. [https://www.e-flux.com/architecture/intelligence/310405/computational-tendencies/](https://www.e-flux.com/architecture/intelligence/310405/computational-tendencies/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ben-Hamu et al. (2018) Heli Ben-Hamu, Haggai Maron, Itay Kezurer, Gal Avineri,
    and Yaron Lipman. 2018. Multi-chart generative surface modeling. *ACM Transactions
    on Graphics (TOG)* 37, 6 (2018), 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bottazzi (2018) Roberto Bottazzi. 2018. *Digital architecture beyond computers:
    Fragments of a cultural history of computational design*. Bloomsbury Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brock et al. (2016) Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.
    2016. Generative and discriminative voxel modeling with convolutional neural networks.
    *arXiv preprint arXiv:1608.04236* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2020) Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao,
    Serge Belongie, Noah Snavely, and Bharath Hariharan. 2020. Learning gradient fields
    for shape generation. In *European Conference on Computer Vision*. Springer, 364–381.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Çakmak (2022) Başak Çakmak. 2022. *Extending design cognition with computer
    vision and generative deep learning*. Master’s thesis. Middle East Technical University.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2020) Wenming Cao, Zhiyue Yan, Zhiquan He, and Zhihai He. 2020.
    A comprehensive survey on geometric deep learning. *IEEE Access* 8 (2020), 35929–35949.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaillou (2020) Stanislas Chaillou. 2020. Archigan: Artificial intelligence
    x architecture. In *Architectural intelligence*. Springer, 117–127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaillou (2022) Stanislas Chaillou. 2022. The advent of architectural AI. In
    *Artificial Intelligence and Architecture*. Birkhäuser, 32–61.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaitin (1975a) Gregory J Chaitin. 1975a. Randomness and mathematical proof.
    *Scientific American* 232, 5 (1975), 47–53.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaitin (1975b) Gregory J Chaitin. 1975b. A theory of program size formally
    identical to information theory. *Journal of the ACM (JACM)* 22, 3 (1975), 329–340.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021) Eric R Chan et al. 2021. pi-gan: Periodic implicit generative
    adversarial networks for 3d-aware image synthesis. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*. 5799–5809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Kevin Chen, Christopher B Choy, Manolis Savva, Angel X Chang,
    Thomas Funkhouser, and Silvio Savarese. 2018. Text2shape: Generating shapes from
    natural language by learning joint embeddings. In *Asian conference on computer
    vision*. Springer, 100–116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Zhang (2019) Zhiqin Chen and Hao Zhang. 2019. Learning implicit fields
    for generative shape modeling. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 5939–5948.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2019) Lung-Pan Cheng, Eyal Ofek, Christian Holz, and Andrew D.
    Wilson. 2019. VRoamer: Generating On-The-Fly VR Experiences While Walking inside
    Large, Unknown Real-World Building Environments. In *2019 IEEE Conference on Virtual
    Reality and 3D User Interfaces (VR)*. 359–366. [https://doi.org/10.1109/VR.2019.8798074](https://doi.org/10.1109/VR.2019.8798074)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claypool (2019) Mollie Claypool. 2019. Discrete automation - architecture -
    e-flux. [https://www.e-flux.com/architecture/becoming-digital/248060/discrete-automation/](https://www.e-flux.com/architecture/becoming-digital/248060/discrete-automation/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creswell et al. (2018) Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran,
    Biswa Sengupta, and Anil A Bharath. 2018. Generative adversarial networks: An
    overview. *IEEE signal processing magazine* 35, 1 (2018), 53–65.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Croitoru et al. (2023) Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
    and Mubarak Shah. 2023. Diffusion models in vision: A survey. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dargan et al. (2020) Shaveta Dargan et al. 2020. A survey of deep learning
    and its applications: a new paradigm to machine learning. *Archives of Computational
    Methods in Engineering* 27 (2020), 1071–1092.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Miguel et al. (2019) Jaime de Miguel et al. 2019. Deep Form Finding Using
    Variational Autoencoders for deep form finding of structural typologies. In *37th
    Conference on Education and Research in Computer Aided Architectural Design in
    Europe (eCAADe) & 23rd Conference of the Iberoamerican Society Digital Graphics
    (SIGraDi)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: del Campo (2021) Matias del Campo. 2021. Architecture, language and AI-language,
    attentional generative adversarial networks (AttnGAN) and architecture design.
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Del Campo et al. (2019) Matias Del Campo, Sandra Manninger, M Sanche, and L
    Wang. 2019. The Church of AI—An examination of architecture in a posthuman design
    ecology. In *Intelligent & Informed-Proceedings of the 24th CAADRIA Conference,
    Victoria University of Wellington, Wellington, New Zealand*. 15–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: del Campo et al. (2022) Matias del Campo, Sandra Manninger, and Yining Yuan.
    2022. Generali Center vienna austria. [https://caadria2022.org/projects/generali-center-vienna-austria/](https://caadria2022.org/projects/generali-center-vienna-austria/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2020) Jiajun Deng, Shaoshuai Shi, Pei-Cian Li, Wen gang Zhou,
    Yanyong Zhang, and Houqiang Li. 2020. Voxel R-CNN: Towards High Performance Voxel-based
    3D Object Detection. *ArXiv* abs/2012.15712 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2021) Yu Deng, Jiaolong Yang, and Xin Tong. 2021. Deformed implicit
    field: Modeling 3d shapes with learned dense correspondence. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 10286–10296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Di Carlo et al. (2022) Raffaele Di Carlo, Divyae Mittal, and Ondrej Veselỳ.
    2022. Generating 3D Building Volumes for a Given Urban Context using Pix2Pix GAN.
    *Legal Depot D/2022/14982/02* (2022), 287.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2022) Xinyue Ding, Xiangmin Guo, Tian Tian Lo, and Ke Wang. 2022.
    The Spatial Environment Affects Human Emotion Perception-Using Physiological Signal
    Modes. (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dinh et al. (2014) Laurent Dinh, David Krueger, and Yoshua Bengio. 2014. Nice:
    Non-linear independent components estimation. *arXiv preprint arXiv:1410.8516*
    (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diniz et al. (2019) Nancy Diniz, Frank Melendez, Woraya Boonyapanachoti, and
    Sebastian Morales. 2019. Body Architectures-Real time data visualization and responsive
    immersive environments. (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dionisio et al. (2013) John David N Dionisio, William G Burns III, and Richard
    Gilbert. 2013. 3D virtual worlds and the metaverse: Current status and future
    possibilities. *ACM Computing Surveys (CSUR)* 45, 3 (2013), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2021) Shi Dong, Ping Wang, and Khushnood Abbas. 2021. A survey
    on deep learning and its applications. *Computer Science Review* 40 (2021), 100379.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    Minedojo: Building open-ended embodied agents with internet-scale knowledge. *arXiv
    preprint arXiv:2206.08853* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2022) Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath
    Sridhar. 2022. Shapecrafter: A recursive text-conditioned 3d shape generation
    model. *arXiv preprint arXiv:2207.09446* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2022) Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue
    Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. 2022. Get3d: A generative
    model of high quality 3d textured shapes learned from images. *Advances In Neural
    Information Processing Systems* 35 (2022), 31841–31854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Lin Gao et al. 2019. SDM-NET: Deep generative network for
    structured deformable mesh. *ACM Transactions on Graphics (TOG)* 38, 6 (2019),
    1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai,
    and Hao Zhang. 2021. Tm-net: Deep generative networks for textured meshes. *ACM
    Transactions on Graphics (TOG)* 40, 6 (2021), 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genova et al. (2019) Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna,
    William T Freeman, and Thomas Funkhouser. 2019. Learning shape templates with
    structured implicit functions. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 7154–7164.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gilbert (2011) RL Gilbert. 2011. The PROSE Project: A program of in-world behavioral
    research on the Metaverse. *Journal of Virtual Worlds Research* 4, 1 (2011), 3–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021) Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.
    2021. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis.
    *arXiv preprint arXiv:2110.08985* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guida (2023) George Guida. 2023. Multimodal Architecture: Applications of language
    in a machine learning aided design process. (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulrajani et al. (2017) Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
    Dumoulin, and Aaron C Courville. 2017. Improved training of wasserstein gans.
    *Advances in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and
    Mohammed Bennamoun. 2020. Deep learning for 3d point clouds: A survey. *IEEE transactions
    on pattern analysis and machine intelligence* 43, 12 (2020), 4338–4364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2021) Zhe Guo et al. 2021. The method of responsive shape design
    based on real-time interaction process. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hall et al. (1968) Edward T Hall, Ray L Birdwhistell, Bernhard Bock, Paul Bohannan,
    A Richard Diebold Jr, Marshall Durbin, Munro S Edmonson, JL Fischer, Dell Hymes,
    Solon T Kimball, et al. 1968. Proxemics [and comments and replies]. *Current anthropology*
    9, 2/3 (1968), 83–108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2020) Zekun Hao et al. 2020. Dualsdf: Semantic shape manipulation
    using a two-level representation. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*. 7631–7641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harshvardhan et al. (2020) GM Harshvardhan et al. 2020. A comprehensive survey
    and analysis of generative models in machine learning. *Computer Science Review*
    38 (2020), 100285.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hatcher and Yu (2018) William Grant Hatcher and Wei Yu. 2018. A survey of deep
    learning: Platforms, applications and emerging research trends. *IEEE Access*
    6 (2018), 24411–24432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henzler et al. (2019) Philipp Henzler, Niloy J Mitra, and Tobias Ritschel.
    2019. Escaping plato’s cave: 3d shape from adversarial rendering. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*. 9984–9993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020a) Jonathan Ho, Ajay Jain, and P. Abbeel. 2020a. Denoising Diffusion
    Probabilistic Models. *ArXiv* abs/2006.11239 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020b) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020b. Denoising
    diffusion probabilistic models. *Advances in Neural Information Processing Systems*
    33 (2020), 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Homolja et al. (2020) Mitra Homolja, Sayyed Amir Hossain Maghool, and Marc Aurel
    Schnabel. 2020. The Impact of Moving through the Built Environment on Emotional
    and Neurophysiological State-A Systematic Literature Review. (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2020) Tianzhen Hong, Zhe Wang, Xuan Luo, and Wanni Zhang. 2020.
    State-of-the-art on research and applications of machine learning in the building
    life cycle. *Energy and Buildings* 212 (2020), 109831.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2021) Jeffrey Huang, Mikhael Johanes, Frederick Chando Kim, Christina
    Doumpioti, and Georg-Christoph Holz. 2021. On gans, nlp and architecture: Combining
    human and machine intelligences for the generation and evaluation of meaningful
    designs. *Technology— Architecture+ Design* 5, 2 (2021), 207–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hui et al. (2020) Le Hui, Rui Xu, Jin Xie, Jianjun Qian, and Jian Yang. 2020.
    Progressive point cloud deconvolution generation network. In *Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XV 16*. Springer, 397–413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ibing et al. (2021) Moritz Ibing, Isaak Lim, and Leif Kobbelt. 2021. 3D shape
    generation with grid-based implicit functions. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 13559–13568.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ingram et al. (1996) Rob Ingram et al. 1996. Building Virtual Cities: applying
    urban planning principles to the design of virtual environments. In *Proceedings
    of the ACM Symposium on Virtual Reality Software and Technology*. 83–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jabbar et al. (2021) Abdul Jabbar, Xi Li, and Bourahla Omar. 2021. A survey
    on generative adversarial networks: Variants, applications, and training. *ACM
    Computing Surveys (CSUR)* 54, 8 (2021), 1–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobs (2016) Jane Jacobs. 2016. *The death and life of great American cities*.
    Vintage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jahan et al. (2021) Tansin Jahan, Yanran Guan, and Oliver Van Kaick. 2021. Semantics-Guided
    Latent Space Exploration for Shape Generation. In *Computer Graphics Forum*, Vol. 40\.
    Wiley Online Library, 115–126.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2022) Ajay Jain et al. 2022. Zero-shot text-guided object generation
    with dream fields. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 867–876.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaminet et al. (2021) Jean Jaminet et al. 2021. Serlio and Artificial Intelligence:
    Problematizing the Image-to-Object Workflow. In *The International Conference
    on Computational Design and Robotic Fabrication*. Springer, 3–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jang and Agapito (2021) Wonbong Jang and Lourdes Agapito. 2021. Codenerf: Disentangled
    neural radiance fields for object categories. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*. 12949–12958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jetchev et al. (2016) Nikolay Jetchev, Urs Bergmann, and Roland Vollgraf. 2016.
    Texture synthesis with spatial generative adversarial networks. *arXiv preprint
    arXiv:1611.08207* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jonassen and Rohrer-Murphy (1999) David H Jonassen and Lucia Rohrer-Murphy.
    1999. Activity theory as a framework for designing constructivist learning environments.
    *Educational technology research and development* 47, 1 (1999), 61–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jovanovic (2022) Damjan Jovanovic. 2022. Games and Worldmaking. [https://journal.b-pro.org/article/p3-games-and-worldmaking/](https://journal.b-pro.org/article/p3-games-and-worldmaking/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahraman et al. (2021) Ridvan Kahraman et al. 2021. Augmenting Design. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*. 4401–4410.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kavakoglu (2021) Aysegul Akcay Kavakoglu. 2021. Computational Aesthetics of
    Low Poly: [Re]Configuration of Form. *Blucher Design Proceedings* 9, 6 (2021),
    17–28. [https://doi.org/10.5151/sigradi2021-235](https://doi.org/10.5151/sigradi2021-235)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keil et al. (2021) Julian Keil et al. 2021. Creating immersive virtual environments
    based on open geospatial data and game engines. *KN-Journal of Cartography and
    Geographic Information* 71, 1 (2021), 53–65.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan et al. (2020) Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed
    Qureshi. 2020. A survey of the recent architectures of deep convolutional neural
    networks. *Artificial intelligence review* 53 (2020), 5455–5516.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KIM et al. (2022) DONGYUN KIM, GEORGE GUIDA, JOSE LUIS GARCÍA, and DEL CASTILLO Y
    LÓPEZ. 2022. PARTICIPATORY URBAN DESIGN WITH GENERATIVE ADVERSARIAL NETWORKS.
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim and Huang (2022) Frederick Chando Kim and Jeffrey Huang. 2022. Perspectival
    GAN-Architectural form-making through dimensional transformation. (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma et al. (2019) Diederik P. Kingma et al. 2019. An Introduction to Variational
    Autoencoders. *ArXiv* abs/1906.02691 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kleineberg et al. (2020) Marian Kleineberg, Matthias Fey, and Frank Weichert.
    2020. Adversarial generation of continuous implicit shape representations. *arXiv
    preprint arXiv:2002.00349* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koh (2022) Immanuel Koh. 2022. 3D-Gan-Housing (neural sampling series). [https://caadria2022.org/projects/3d-gan-housing-neural-sampling-series/](https://caadria2022.org/projects/3d-gan-housing-neural-sampling-series/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kreuzberger et al. (2023) Dominik Kreuzberger, Niklas Kühl, and Sebastian Hirschl.
    2023. Machine learning operations (mlops): Overview, definition, and architecture.
    *IEEE Access* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwiecinski et al. (2017) Krystian Kwiecinski, Jacek Markusiewicz, and Agata
    Pasternak. 2017. Participatory Design Supported with Design System and Augmented
    Reality. (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lab (2023) Tencent AI Lab. 2023. AI enhanced procedural city generation. [https://gdcvault.com/play/1028921/Recorded-AI-Enhanced-Procedural-City](https://gdcvault.com/play/1028921/Recorded-AI-Enhanced-Procedural-City)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lam et al. (2019) Kit Yung Lam et al. 2019. M2a: A framework for visualizing
    information from mobile web to mobile augmented reality. In *2019 IEEE International
    Conference on Pervasive Computing and Communications (PerCom*. IEEE, 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2010) Yann LeCun, Corinna Cortes, and Chris Burges. 2010. MNIST
    handwritten digit database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ledig et al. (2017) Christian Ledig et al. 2017. Photo-realistic single image
    super-resolution using a generative adversarial network. In *Proceedings of the
    IEEE conference on computer vision and pattern recognition*. 4681–4690.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2021a) Lik-Hang Lee et al. 2021a. All one needs to know about metaverse:
    A complete survey on technological singularity, virtual ecosystem, and research
    agenda. *arXiv preprint arXiv:2110.05352* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2021b) Lik-Hang Lee et al. 2021b. Towards augmented reality driven
    human-city interaction: Current research on mobile headsets and future challenges.
    *ACM Computing Surveys (CSUR)* 54, 8 (2021), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lessig (2009) Lawrence Lessig. 2009. *Code: And other laws of cyberspace*.
    ReadHowYouWant. com.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Jun Li, Chengjie Niu, and Kai Xu. 2020. Learning part generation
    and assembly for structure-aware shape synthesis. In *Proceedings of the AAAI
    conference on artificial intelligence*, Vol. 34. 11362–11369.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017) Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang,
    and Leonidas Guibas. 2017. Grass: Generative recursive autoencoders for shape
    structures. *ACM Transactions on Graphics (TOG)* 36, 4 (2017), 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin and Lo (2021) Chaohe Lin and Tian Tian Lo. 2021. Expanding the Methods of
    Human-VR Interaction (HVRI) for Architectural Design Process. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2022) Chen-Hsuan Lin et al. 2022. Magic3D: High-Resolution Text-to-3D
    Content Creation. *arXiv preprint arXiv:2211.10440* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litany et al. (2018) Or Litany et al. 2018. Deformable shape completion with
    graph convolutional autoencoders. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*. 1886–1895.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Chuan Liu, Jiaqi Shen, Yue Ren, and Hao Zheng. 2021. Pipes
    of AI–Machine Learning Assisted 3D Modeling Design. In *Proceedings of the 2020
    DigitalFUTURES: The 2nd International Conference on Computational Design and Robotic
    Fabrication (CDRF 2020)*. Springer, 17–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Zhengzhe Liu et al. 2022. Towards Implicit Text-Guided 3D
    Shape Generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 17896–17906.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lorensen and Cline (1987) William E Lorensen and Harvey E Cline. 1987. Marching
    cubes: A high resolution 3D surface construction algorithm. *ACM siggraph computer
    graphics* 21, 4 (1987), 163–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lunz et al. (2020) Sebastian Lunz, Yingzhen Li, Andrew Fitzgibbon, and Nate
    Kushman. 2020. Inverse graphics gan: Learning to generate 3d shapes from unstructured
    2d data. *arXiv preprint arXiv:2002.12674* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mescheder et al. (2019) Lars Mescheder et al. 2019. Occupancy networks: Learning
    3d reconstruction in function space. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 4460–4470.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michalkiewicz et al. (2019) Mateusz Michalkiewicz, Jhony K Pontes, Dominic
    Jack, Mahsa Baktashmotlagh, and Anders Eriksson. 2019. Deep level sets: Implicit
    surface representations for 3d shape inference. *arXiv preprint arXiv:1901.06802*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mildenhall et al. (2020) Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
    Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes
    as Neural Radiance Fields for View Synthesis. *ArXiv* abs/2003.08934 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mittal et al. (2022) Paritosh Mittal et al. 2022. Autosdf: Shape priors for
    3d completion, reconstruction and generation. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 306–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohammad (2019) Ali SAQ Mohammad. 2019. *Hybrid elevations using GAN Networks*.
    Ph. D. Dissertation. The University of North Carolina at Charlotte.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MOLETA and NISHIOKA (2021) TANE MOLETA and MIZUHO NISHIOKA. 2021. Populating
    Virtual Worlds: Architecture, Photography, Sonic Art, Creative Writing Collide
    at “in the Forest with the Trees We Made”. (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MUN et al. (2019) KRISTINE MUN, DANE CLEMENSON, and BIAYNA BOGOSIAN. 2019. THE
    WELL TEMPERED ENVIRONMENT OF EXPERIENCE. *INTELLIGENT & INFORMED* 15 (2019), 573.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mütterlein (2018) Joschka Mütterlein. 2018. The three pillars of virtual reality?
    Investigating the roles of immersion, presence, and interactivity. (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Myers and Twenge (2012) David G Myers and Jean M Twenge. 2012. *Exploring social
    psychology*. McGraw-Hill New York.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narahara (2022) Taro Narahara. 2022. Kurashiki Viewer: Qualitative Evaluations
    of Architectural Spaces inside Virtual Reality. (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nash et al. (2020) Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia.
    2020. Polygen: An autoregressive generative model of 3d meshes. In *International
    conference on machine learning*. PMLR, 7220–7229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nazmeeva (2019) Alina Nazmeeva. 2019. *Constructing the virtual as a social
    form*. Ph. D. Dissertation. Massachusetts Institute of Technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nevelsteen (2018) Kim JL Nevelsteen. 2018. Virtual world, defined from a technological
    perspective and applied to video games, mixed reality, and the Metaverse. *Computer
    animation and virtual worlds* 29, 1 (2018), e1752.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton (2019) David Newton. 2019. Generative deep learning in architectural
    design. *Technology— Architecture+ Design* 3, 2 (2019), 176–189.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2019) Binh Vinh Duc Nguyen, Peng Chengzhi, and Wang Tsung-Hsien.
    2019. KOALA-Developing a generative house design system with agent-based modelling
    of social spatial processes. In *Intelligent & Informed-Proceedings of the 24th
    CAADRIA Conference*, Vol. 1\. CAADRIA, 235–244.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen-Phuoc et al. (2019) Thu Nguyen-Phuoc et al. 2019. Hologan: Unsupervised
    learning of 3d representations from natural images. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*. 7588–7597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
    Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide:
    Towards photorealistic image generation and editing with text-guided diffusion
    models. *arXiv preprint arXiv:2112.10741* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nichol et al. (2022) Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin,
    and Mark Chen. 2022. Point-E: A System for Generating 3D Point Clouds from Complex
    Prompts. arXiv:2212.08751 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niemeyer and Geiger (2021) Michael Niemeyer and Andreas Geiger. 2021. Giraffe:
    Representing scenes as compositional generative neural feature fields. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 11453–11464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nijkamp et al. (2020) Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and
    Ying Nian Wu. 2020. On the anatomy of mcmc-based maximum likelihood learning of
    energy-based models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 34. 5272–5280.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) OpenAI. 2022. CHATGPT: Optimizing language models for dialogue.
    [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Or-El et al. (2022) Roy Or-El et al. 2022. Stylesdf: High-resolution 3d-consistent
    image and geometry generation. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 13503–13513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oussidi and Elhassouny (2018) Achraf Oussidi and Azeddine Elhassouny. 2018.
    Deep generative models: Survey. In *2018 International conference on intelligent
    systems and computer vision (ISCV)*. IEEE, 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Özel (2020) Güvenç Özel. 2020. Interdisciplinary AI: A Machine Learning System
    for Streamlining External Aesthetic and Cultural Influences in Architecture. In
    *Architectural Intelligence: Selected Papers from the 1st International Conference
    on Computational Design and Robotic Fabrication (CDRF 2019)*. Springer, 103–116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pavan Kumar and Jayagopal (2021) MR Pavan Kumar and Prabhu Jayagopal. 2021.
    Generative adversarial networks: a survey on applications and challenges. *International
    Journal of Multimedia Information Retrieval* 10, 1 (2021), 1–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pei et al. (2021) Wanyu Pei, Xiangmin Guo, and TianTian Lo. 2021. Detecting
    Virtual Perception Based on Multi-Dimensional Biofeedback-A Method to Pre-Evaluate
    Architectural Design Objectives. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pei et al. (2020) Wanyu Pei, TianTian LO, and Xiangmin Guo. 2020. A Biofeedback
    Process: Detecting Architectural Space with the Integration of Emotion Recognition
    and Eye-tracking Technology. (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Penney and Chen (2019) Drew D Penney and Lizhong Chen. 2019. A survey of machine
    learning applied to computer architecture design. *arXiv preprint arXiv:1909.12373*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poole et al. (2022) Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.
    2022. Dreamfusion: Text-to-3d using 2d diffusion. *arXiv preprint arXiv:2209.14988*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pouyanfar et al. (2018) Samira Pouyanfar et al. 2018. A survey on deep learning:
    Algorithms, techniques, and applications. *ACM Computing Surveys (CSUR)* 51, 5
    (2018), 1–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramasinghe et al. (2020) Sameera Ramasinghe et al. 2020. Spectral-GANs for high-resolution
    3D point-cloud generation. In *2020 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*. IEEE, 8169–8176.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis.
    In *International conference on machine learning*. PMLR, 1060–1069.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regenwetter et al. (2022) Lyle Regenwetter, Amin Heyrani Nobari, and Faez Ahmed.
    2022. Deep generative models in engineering design: A review. *Journal of Mechanical
    Design* 144, 7 (2022), 071704.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren and Zheng (2020) Yue Ren and Hao Zheng. 2020. The Spire of AI-Voxel-based
    3D neural style transfer. In *Proceedings of the 25th International Conference
    on Computer-Aided Architectural Design Research in Asia (CAADRIA)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rusu and Cousins (2011) Radu Bogdan Rusu and Steve B. Cousins. 2011. 3D is
    here: Point Cloud Library (PCL). *2011 IEEE International Conference on Robotics
    and Automation* (2011), 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanghi et al. (2022) Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,
    Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. 2022. Clip-forge: Towards
    zero-shot text-to-shape generation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 18603–18613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarcar et al. (2008) MMM Sarcar et al. 2008. *Computer aided design and manufacturing*.
    PHI Learning Pvt. Ltd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sardenberg (2019) Victor Sardenberg. 2019. Aesthetic Quantification as Search
    Criteria in Architectural Design. *Blucher Design Proceedings* 7, 1 (2019), 17–24.
    [https://doi.org/10.5151/proceedings-ecaadesigradi2019_088](https://doi.org/10.5151/proceedings-ecaadesigradi2019_088)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarker (2021a) Iqbal H Sarker. 2021a. Deep learning: a comprehensive overview
    on techniques, taxonomy, applications and research directions. *SN Computer Science*
    2, 6 (2021), 420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarker (2021b) Iqbal H Sarker. 2021b. Machine learning: Algorithms, real-world
    applications and research directions. *SN Computer Science* 2, 3 (2021), 1–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroeder et al. (2001) Ralph Schroeder, Avon Huxor, and Andy Smith. 2001.
    Activeworlds: geography and social interaction in virtual reality. *Futures* 33,
    7 (2001), 569–587. [https://doi.org/10.1016/S0016-3287(01)00002-7](https://doi.org/10.1016/S0016-3287(01)00002-7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schumacher (2013) Patrik Schumacher. 2013. Parametric Semiology – The Design
    of Information Rich Environments. [https://www.patrikschumacher.com/Texts/Design%20of%20Information%20Rich%20Environments.html](https://www.patrikschumacher.com/Texts/Design%20of%20Information%20Rich%20Environments.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schwarz et al. (2022) Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,
    and Andreas Geiger. 2022. Voxgraf: Fast 3d-aware image synthesis with sparse voxel
    grids. *arXiv preprint arXiv:2206.07695* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sebestyen et al. (2021) Adam Sebestyen, Johanna Rock, and Urs Leonhard Hirschberg.
    2021. Towards Abductive Reasoning-Based ComputationalDesign Tools: Using Machine
    Learning as a way to explore the combined design spaces of multiple parametric
    models. In *39th eCAADe Conference: Education and Research in Computer Aided Architectural
    Design in Europe: Towards a new, configurable architecture: eCAADe 2021*. 141–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheehan et al. (2021) Liam Jordan Sheehan, Andre Brown, Marc Aurel Schnabel,
    and Tane Moleta. 2021. The Fourth Virtual Dimension-Stimulating the Human Senses
    to Create Virtual Atmospheric Qualities. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2021) Tianchang Shen et al. 2021. Deep Marching Tetrahedra: a
    Hybrid Representation for High-Resolution 3D Shape Synthesis. In *Advances in
    Neural Information Processing Systems (NeurIPS)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2022) Zifan Shi, Sida Peng, Yinghao Xu, Yiyi Liao, and Yujun Shen.
    2022. Deep generative models on 3d representations: A survey. *arXiv preprint
    arXiv:2210.15663* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. (2019) Dong Wook Shu et al. 2019. 3d point cloud generative adversarial
    network based on tree structured graph convolutions. In *Proceedings of the IEEE/CVF
    international conference on computer vision*. 3859–3868.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srivastava et al. (2021) Akshay Srivastava, Longtai Liao, and Henan Liu. 2021.
    An anonymous composition. *The Routledge Companion to Artificial Intelligence
    in Architecture* (2021), 442.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Steinfeld et al. (2019) Kyle Steinfeld et al. 2019. Fresh eyes: a framework
    for the application of machine learning to generative architectural design, and
    a report of activities at smartgeometry 2018\. In *Computer-Aided Architectural
    Design.” Hello, Culture” 18th International Conf., CAAD Futures 2019, Daejeon,
    S. Korea, June 26–28, 2019*. Springer, 32–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STUART-SMITH and DANAHY (2022) ROBERT STUART-SMITH and PATRICK DANAHY. 2022.
    Visual Character Analysis within Algorithmic Design. *POST-CARBON, Proceedings
    of the 27th CAADRIA* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tamke et al. (2018) Martin Tamke, Paul Nicholas, and Mateusz Zwierzycki. 2018.
    Machine learning for architectural design: Practices and infrastructure. *International
    Journal of Architectural Computing* 16, 2 (2018), 123–143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonn (2017) Christian Tonn. 2017. Designing Colour in Virtual Reality-Comparing
    a Virtual Reality based and a Screen based Colour Design Method. (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tosello (2003) Maria E Tosello. 2003. Performing Cyberspace: Dance, Technology
    and Virtual Architecture. *International Journal of Architectural Computing* 1,
    3 (2003), 393–413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valsesia et al. (2019) Diego Valsesia, Giulia Fracastoro, and Enrico Magli.
    2019. Learning localized generative models for 3d point clouds via graph convolution.
    In *International conference on learning representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veselỳ (2022) Ondrej Veselỳ. 2022. Building massing generation using GAN trained
    on Dutch 3D city models. (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Can Wang et al. 2022. Clip-nerf: Text-and-image driven manipulation
    of neural radiance fields. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 3835–3844.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Zhengwei Wang, Qi She, and Tomas E Ward. 2021. Generative
    adversarial networks in computer vision: A survey and taxonomy. *ACM Computing
    Surveys (CSUR)* 54, 2 (2021), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wells et al. (2021) Cameron Wells et al. 2021. Beauty is in the Eye of the Beholder-Improving
    the Human-Computer Interface within VRAD by the active and two-way employment
    of our visual senses. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2016) Jiajun Wu et al. 2016. Learning a probabilistic latent space
    of object shapes via 3d generative-adversarial modeling. *Advances in neural information
    processing systems* 29 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Baoquan Chen.
    2020. Pq-net: A generative part seq2seq network for 3d shapes. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 829–838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Zhijie Wu, Xiang Wang, Di Lin, Dani Lischinski, Daniel Cohen-Or,
    and Hui Huang. 2019. Sagnet: Structure-aware generative network for 3d-shape modeling.
    *ACM Transactions on Graphics (TOG)* 38, 4 (2019), 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia and Xue (2022) Weihao Xia and Jing-Hao Xue. 2022. A Survey on 3D-aware Image
    Synthesis. *arXiv preprint arXiv:2210.14267* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Guandao Yang et al. 2019. Pointflow: 3d point cloud generation
    with continuous normalizing flows. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 4541–4550.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2018) Zhihang Yao, Claus Nagel, Felix Kunde, György Hudra, Philipp
    Willkomm, Andreas Donaubauer, Thomas Adolphi, and Thomas H Kolbe. 2018. 3DCityDB-a
    3D geodatabase solution for the management, analysis, and visualization of semantic
    3D city models based on CityGML. *Open Geospatial Data, Software and Standards*
    3, 1 (2018), 1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu (2020) De Yu. 2020. Reprogramming Urban Block by Machine Creativity-How to
    use neural networks as generative tools to design space. (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuniarti and Suciati (2019) Anny Yuniarti and Nanik Suciati. 2019. A review
    of deep learning techniques for 3D reconstruction of 2D images. In *2019 12th
    International Conference on Information & Communication Technology and System
    (ICTS)*. IEEE, 327–331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zamorski et al. (2020) Maciej Zamorski et al. 2020. Adversarial autoencoders
    for compact representations of 3D point clouds. *Computer Vision and Image Understanding*
    193 (2020), 102921.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zarei et al. (2021) Maryam Zarei, Halil Erhan, Ahmed M Abuzuraiq, Osama Alsalman,
    and Alyssa Haas. 2021. Design and development of interactive systems for integration
    of comparative visual analytics in design workflow. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2022) Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
    Or Litany, Sanja Fidler, and Karsten Kreis. 2022. LION: Latent Point Diffusion
    Models for 3D Shape Generation. *arXiv preprint arXiv:2210.06978* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang (2019) Hang Zhang. 2019. 3D model generation on architectural plan and
    section training through machine learning. *Technologies* 7, 4 (2019), 82.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang (2020) Hang Zhang. 2020. Text-to-Form: 3D Prediction by Linguistic Description.
    In *ACADIA 20: Distributed Proximities / Volume I: Technical Papers [Proceedings
    of the 40th Annual Conference of the Association for Computer Aided Design in
    Architecture (ACADIA) 978-0-578-95213-0](Online and Global. 24-30 October 2020.)*.
    238–247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Blasetti (2020) Hang Zhang and Ezio Blasetti. 2020. 3D architectural
    form style transfer through machine learning. (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Huang (2021) Hang Zhang and Ye Huang. 2021. Machine learning aided
    2D-3D architectural form finding at high resolution. In *Proceedings of the 2020
    DigitalFUTURES: The 2nd International Conference on Computational Design and Robotic
    Fabrication (CDRF 2020)*. Springer, 159–168.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Song-Hai Zhang, Shao-Kui Zhang, Yuan Liang, and Peter Hall.
    2019. A survey of 3D indoor scene synthesis. *Journal of Computer Science and
    Technology* 34 (2019), 594–608.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2021) Zerong Zheng, Tao Yu, Qionghai Dai, and Yebin Liu. 2021.
    Deep implicit templates for 3d shape representation. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 1429–1439.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021) Linqi Zhou, Yilun Du, and Jiajun Wu. 2021. 3d shape generation
    and completion through point-voxel diffusion. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*. 5826–5835.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*. 2223–2232.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
