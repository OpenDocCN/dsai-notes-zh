- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:59:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2008.05221] Compression of Deep Learning Models for Text: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2008.05221] 深度学习模型的文本压缩：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.05221](https://ar5iv.labs.arxiv.org/html/2008.05221)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2008.05221](https://ar5iv.labs.arxiv.org/html/2008.05221)
- en: 'Compression of Deep Learning Models for Text: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型的文本压缩：一项调查
- en: Manish Gupta [gmanish@microsoft.com](mailto:gmanish@microsoft.com) [0002-2843-3110](https://orcid.org/0002-2843-3110
    "ORCID identifier")  and  Puneet Agrawal [punagr@microsoft.com](mailto:punagr@microsoft.com)
    Microsoft, India(2020)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Manish Gupta [gmanish@microsoft.com](mailto:gmanish@microsoft.com) [0002-2843-3110](https://orcid.org/0002-2843-3110
    "ORCID identifier") 和 Puneet Agrawal [punagr@microsoft.com](mailto:punagr@microsoft.com)
    微软，印度（2020）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In recent years, the fields of natural language processing (NLP) and information
    retrieval (IR) have made tremendous progress thanks to deep learning models like
    Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term
    Memory (LSTMs) networks, and Transformer (Vaswani et al., [2017](#bib.bib122))
    based models like Bidirectional Encoder Representations from Transformers (BERT) (Devlin
    et al., [2018](#bib.bib25)), Generative Pre-training Transformer (GPT-2) (Radford
    et al., [2019](#bib.bib96)), Multi-task Deep Neural Network (MT-DNN) (Liu et al.,
    [2019b](#bib.bib75)), Extra-Long Network (XLNet) (Yang et al., [2019](#bib.bib136)),
    Text-to-text transfer transformer (T5) (Raffel et al., [2019](#bib.bib97)), T-NLG (Rosset,
    [2019](#bib.bib100)) and GShard (Lepikhin et al., [2020](#bib.bib65)). But these
    models are humongous in size. On the other hand, real world applications demand
    small model size, low response times and low computational power wattage. In this
    survey, we discuss six different types of methods (Pruning, Quantization, Knowledge
    Distillation, Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer
    based methods) for compression of such models to enable their deployment in real
    industry NLP projects. Given the critical need of building applications with efficient
    and small models, and the large amount of recently published work in this area,
    we believe that this survey organizes the plethora of work done by the ‘deep learning
    for NLP’ community in the past few years and presents it as a coherent story.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，自然语言处理（NLP）和信息检索（IR）领域由于深度学习模型如递归神经网络（RNNs）、门控递归单元（GRUs）和长短期记忆（LSTMs）网络，以及基于Transformer（Vaswani等，[2017](#bib.bib122)）的模型如双向编码器表示（BERT）（Devlin等，[2018](#bib.bib25)）、生成预训练变换器（GPT-2）（Radford等，[2019](#bib.bib96)）、多任务深度神经网络（MT-DNN）（Liu等，[2019b](#bib.bib75)）、超长网络（XLNet）（Yang等，[2019](#bib.bib136)）、文本到文本转移变换器（T5）（Raffel等，[2019](#bib.bib97)）、T-NLG（Rosset，[2019](#bib.bib100)）和GShard（Lepikhin等，[2020](#bib.bib65)）取得了巨大的进展。但这些模型的体积巨大。另一方面，现实世界应用需要小模型尺寸、低响应时间和低计算功耗。在本次调查中，我们探讨了六种不同类型的方法（剪枝、量化、知识蒸馏、参数共享、张量分解和亚二次Transformer方法）来压缩这些模型，以便在实际工业NLP项目中部署。鉴于构建高效和小型模型的迫切需求，以及近期在这一领域发布的大量工作，我们相信这次调查整理了‘深度学习用于NLP’社区在过去几年中的大量工作，并将其呈现为一个连贯的故事。
- en: 'Model compression, Deep Learning, Pruning, Quantization, Knowledge Distillation,
    Parameter Sharing, Tensor Factorization, Sub-Quadratic Transformers^†^†copyright:
    acmcopyright^†^†journalyear: 2020^†^†doi: 10.1145/1122445.1122456^†^†conference:
    TKDD; Aug 2020; TKDD^†^†booktitle: TKDD^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Neural networks^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: General and reference Surveys and overviews'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩，深度学习，剪枝，量化，知识蒸馏，参数共享，张量分解，亚二次变换器^†^†版权：acmcopyright^†^†期刊年份：2020^†^†doi：10.1145/1122445.1122456^†^†会议：TKDD；2020年8月；TKDD^†^†书名：TKDD^†^†价格：15.00^†^†isbn：978-1-4503-XXXX-X/18/06^†^†ccs：计算方法
    神经网络^†^†ccs：计算方法 机器学习^†^†ccs：一般和参考 调查与概述
- en: Deep learning models have revolutionized multiple fields of information systems
    including natural language processing (NLP), computer vision, and speech analysis.
    While they have enabled multiple tasks to attain very high accuracy values, model
    sizes and prediction latencies have increased significantly as well. Specific
    to text, Recurrent neural networks (RNNs), Gated Recurrent Units (GRUs) and long
    short term memory (LSTM) networks have been used for quite some time for various
    natural language processing (NLP) tasks. These models are large especially because
    of the input and output embedding parameters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型已经彻底改变了包括自然语言处理（NLP）、计算机视觉和语音分析在内的信息系统多个领域。虽然它们使得多个任务能够达到非常高的准确度，但模型的体积和预测延迟也显著增加。具体到文本，递归神经网络（RNNs）、门控递归单元（GRUs）和长短期记忆（LSTM）网络已经被使用了相当长时间，用于各种自然语言处理（NLP）任务。这些模型特别庞大，主要由于输入和输出嵌入参数的原因。
- en: 'In the past three years, the field of NLP has made significant progress as
    is evident from the GLUE (Wang et al., [2019b](#bib.bib126)) and SuperGLUE (Wang
    et al., [2019a](#bib.bib125)) leaderboards¹¹1[https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)^,²²2[https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard).
    Transformer (Vaswani et al., [2017](#bib.bib122)) based models like Bidirectional
    Encoder Representations from Transformers (BERT) (Devlin et al., [2018](#bib.bib25)),
    Generative Pre-training Transformer (GPT-2) (Radford et al., [2019](#bib.bib96)),
    Multi-task Deep Neural Network (MT-DNN) (Liu et al., [2019b](#bib.bib75)), Extra-Long
    Network (XLNet) (Yang et al., [2019](#bib.bib136)), MegatronLM (Shoeybi et al.,
    [2019](#bib.bib107)), Text-to-text transfer transformer (T5) (Raffel et al., [2019](#bib.bib97)),
    T-NLG (Rosset, [2019](#bib.bib100)) and GShard (Lepikhin et al., [2020](#bib.bib65))
    have been major contributors to this success. But these models are humongous in
    size: BERT (340M parameters), GPT-2 (1.5B parameters), MegatronLM (8.3B parameters),
    T5 (11B parameters), T-NLG (17B parameters) and GShard (600B parameters). Bianco
    et al. (Bianco et al., [2018](#bib.bib9)) and Sanh et al. (Sanh et al., [2019](#bib.bib102))
    provide a great overview of the sizes of recent deep learning models in computer
    vision and NLP respectively.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去三年中，自然语言处理（NLP）领域取得了显著进展，这从GLUE（Wang et al., [2019b](#bib.bib126)）和SuperGLUE（Wang
    et al., [2019a](#bib.bib125)）排行榜中可以明显看出¹¹1[https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)^,²²2[https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard)。基于Transformer（Vaswani
    et al., [2017](#bib.bib122)）的模型，如双向编码器表示（BERT）（Devlin et al., [2018](#bib.bib25)）、生成预训练Transformer（GPT-2）（Radford
    et al., [2019](#bib.bib96)）、多任务深度神经网络（MT-DNN）（Liu et al., [2019b](#bib.bib75)）、超长网络（XLNet）（Yang
    et al., [2019](#bib.bib136)）、MegatronLM（Shoeybi et al., [2019](#bib.bib107)）、文本到文本转移Transformer（T5）（Raffel
    et al., [2019](#bib.bib97)）、T-NLG（Rosset, [2019](#bib.bib100)）和GShard（Lepikhin
    et al., [2020](#bib.bib65)）在这一成功中发挥了重要作用。但这些模型的体积庞大：BERT（340M参数）、GPT-2（1.5B参数）、MegatronLM（8.3B参数）、T5（11B参数）、T-NLG（17B参数）和GShard（600B参数）。Bianco
    et al.（Bianco et al., [2018](#bib.bib9)）和Sanh et al.（Sanh et al., [2019](#bib.bib102)）提供了对近期计算机视觉和自然语言处理领域深度学习模型规模的全面概述。
- en: Training these large models needs a lot of GPU infrastructure, and leads to
    a large power consumption. Training a BERT-base model on GPUs is roughly equivalent
    to a trans-American flight in terms of power and carbon footprint (Strubell et al.,
    [2019](#bib.bib110)). Deployment of such gigantic models is difficult even on
    high-end servers. Indeed a large number of real world applications run on machines
    with resource constrained environments, for example, edge devices, sensors and
    mobile phones. Edge devices could include offline medical equipment, and modules
    on drones, robots, glasses, etc. Often times, besides desiring a small model size,
    low response times are critical. For example, applications like driverless cars
    or apps to aid the blind require processing speeds of around 30 frames per second.
    Similarly, search engines need to be able to process billions of queries per day.
    Overall, the following factors motivate us to study compression of deep learning
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这些大型模型需要大量的 GPU 基础设施，并且会导致高额的电力消耗。在 GPU 上训练一个 BERT-base 模型的电力和碳足迹大致相当于一次横跨美洲的飞行（Strubell
    等， [2019](#bib.bib110)）。即便在高端服务器上，部署如此庞大的模型也很困难。实际上，许多实际应用运行在资源受限的环境中，例如边缘设备、传感器和手机。边缘设备可能包括离线医疗设备，以及无人机、机器人、眼镜等上的模块。通常，除了希望模型体积较小之外，低响应时间也至关重要。例如，像无人驾驶汽车或辅助盲人应用这样的应用程序需要处理速度约为每秒
    30 帧。类似地，搜索引擎需要能够处理每天数十亿次查询。总体而言，以下因素促使我们研究深度学习模型的压缩。
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memory (RAM) usage
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存（RAM）使用
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prediction latency
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测延迟
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Power dissipation
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 电力消耗
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inference on resource constrained devices
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在资源受限的设备上的推断
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ease of training/finetuning
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练/微调的便利性
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ease of deployment and update
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署和更新的便利性
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ease of distributed training
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布式训练的便利性
- en: '![Refer to caption](img/e6a2d1fdcb7453301d50fff20ce52403.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e6a2d1fdcb7453301d50fff20ce52403.png)'
- en: Figure 1\. Overview of Model Compression Methods for Text
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 文本模型压缩方法概述
- en: Large networks do not fit in on-chip storage and hence require the more costly
    external DRAM accesses. Running a 1 billion connection neural network, for example,
    at 30 frames per second would require (30Hz)(1G)(640pJ) = 19.2W just for DRAM
    access – well beyond the power envelope of a typical mobile device. This implies
    that a mobile phone running such an app could suffer from fast draining of the
    battery, leading to overheating of the phone. Han et al. (Han et al., [2016a](#bib.bib36))
    discuss details of power dissipation for deep learning models. Another option
    to avoid large RAM, high prediction times and high power dissipation, is to run
    such massive deep learning models on cloud servers. But for many real world applications,
    it is desirable to run them on local client devices to avoid network delay, to
    guard user privacy and to avoid power dissipation in terms of input/output data
    communication.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型网络无法适应片上存储，因此需要更昂贵的外部 DRAM 访问。例如，运行一个拥有 10 亿连接的神经网络以每秒 30 帧的速度将需要 (30Hz)(1G)(640pJ)
    = 19.2W 的 DRAM 访问功耗，这远远超出了典型移动设备的功率范围。这意味着运行此类应用程序的手机可能会面临快速耗电的问题，从而导致手机过热。Han
    等（Han 等，[2016a](#bib.bib36)）讨论了深度学习模型的电力消耗的详细信息。另一种避免大型 RAM、高预测时间和高电力消耗的选项是将如此庞大的深度学习模型运行在云服务器上。但对于许多实际应用，最好将它们运行在本地客户端设备上，以避免网络延迟、保护用户隐私，并避免输入/输出数据通信中的电力消耗。
- en: Small models can indeed also lead to low prediction latencies. For example,
    Diamos et al. (Diamos et al., [2016](#bib.bib26)) showed that for small models,
    one can cache the RNN weights in on-chip memory such as caches, block RAM, or
    register files across multiple timesteps. This could lead to as much as 146x speedup
    if the entire RNN layer can be stored in registers rather than the GPU DRAM of
    an NVIDIA TitanX GPU.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 小型模型确实也可以降低预测延迟。例如，Diamos 等（Diamos 等，[2016](#bib.bib26)）展示了对于小型模型，可以将 RNN 权重缓存到片上内存中，例如缓存、块
    RAM 或寄存器文件，这可以在整个 RNN 层可以存储在寄存器中而不是 NVIDIA TitanX GPU 的 GPU DRAM 时实现高达 146 倍的加速。
- en: Finally, it is easier to perform software development, deployment and updates
    with smaller models. Large models are difficult to handle. For example, it is
    impossible to fine-tune pretrained BERT-large model on a GPU with 12-16 GB RAM.
    This poses a large barrier of entry for communities without the resources to purchase
    several large Graphic Processing Units (GPUs). For large models, tuning various
    configuration parameters needs lots of resources. Smaller models lead to improved
    speed of learning and allow for more hyper-parameter configurations to be evaluated.
    Mobile-first companies dislike large apps. App stores are very sensitive to the
    size of the binary files. For example, iPhone App Store has the restriction “apps
    above 150 MB will not download until you connect to Wi-Fi”. Smaller models are
    easier to use and deploy in real world systems. Large models need multiple server
    nodes. On the other hand, multiple instances of smaller models can be run simultaneously
    on the same machine leading to higher QPS (queries per second). Lastly, smaller
    models also decrease the communication overhead of distributed training of the
    models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，较小的模型使得软件开发、部署和更新变得更为简单。大型模型难以处理。例如，不可能在拥有12-16 GB RAM的GPU上对预训练的BERT-large模型进行微调。这对没有资源购买多个大型图形处理单元（GPU）的社区构成了巨大的门槛。对于大型模型，调整各种配置参数需要大量资源。较小的模型可以提高学习速度，并允许评估更多的超参数配置。以移动优先的公司不喜欢大型应用程序。应用商店对二进制文件的大小非常敏感。例如，iPhone
    App Store有“超过150 MB的应用程序将不会下载，直到你连接到Wi-Fi”的限制。较小的模型在实际系统中更容易使用和部署。大型模型需要多个服务器节点。另一方面，多个较小模型的实例可以在同一台机器上同时运行，从而提高QPS（每秒查询次数）。最后，较小的模型还可以减少分布式训练的通信开销。
- en: 'Fortunately, there is a large amount of redundancy among the weights of these
    large neural networks. A small subset of the weights are sufficient to reconstruct
    the entire network. Denil et al. (Denil et al., [2013](#bib.bib24)) showed that
    by simply using $\sim$5% of the weights, it is possible to predict the remaining
    weights without any drop in accuracy. This observation led to a large number of
    research efforts across multiple communities on compression of large deep learning
    models. In this survey, we aim to systematically explore this large body of literature
    in the NLP community by organizing it into various categories. Figure [1](#S0.F1
    "Figure 1 ‣ Compression of Deep Learning Models for Text: A Survey") shows a broad
    organization of model compression methods for text. In this survey we do not focus
    on specific methods proposed in other communities like vision and speech only
    and which make use of image/audio specific architectures and hence cannot be applied
    to text. Also, we do not discuss methods on hardware optimizations to reduce latency.
    While there are other surveys in the broad area of model compression (Cheng et al.,
    [2017](#bib.bib15); Deng et al., [2020](#bib.bib23)) also specifically on knowledge
    distillation (Wang and Yoon, [2020](#bib.bib127)), they are either old or focus
    on computer vision related problems.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这些大型神经网络的权重中存在大量的冗余。一个小的权重子集足以重建整个网络。Denil等（Denil et al., [2013](#bib.bib24)）显示，仅使用约5%的权重，就能预测剩余的权重而不会降低准确性。这一观察引发了多个社区在压缩大型深度学习模型方面的大量研究工作。在这项调查中，我们旨在通过将其组织成各种类别，系统地探索NLP社区中大量的文献。图[1](#S0.F1
    "图 1 ‣ 文本深度学习模型压缩：调查")显示了文本模型压缩方法的广泛组织。在这项调查中，我们不专注于其他社区（如视觉和语音）中提出的具体方法，这些方法利用了图像/音频特定的架构，因此不能应用于文本。此外，我们不讨论减少延迟的硬件优化方法。虽然在模型压缩的广泛领域（Cheng
    et al., [2017](#bib.bib15); Deng et al., [2020](#bib.bib23)）以及专门针对知识蒸馏的方法（Wang
    and Yoon, [2020](#bib.bib127)）中有其他调查，它们要么过时，要么集中在计算机视觉相关的问题上。
- en: '1\. Model Compression Methods: Overview'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 模型压缩方法概述
- en: In this survey, we discuss compression methods using pruning, quantization,
    knowledge distillation, parameter sharing, tensor decomposition and sub-quadratic
    Transformers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们讨论了使用修剪、量化、知识蒸馏、参数共享、张量分解和子二次Transformer的压缩方法。
- en: The most obvious way to reduce model size is to sparsify weight matrices. Pruning
    methods differ based on what is pruned and the actual logic used to prune. Given
    a matrix, one can prune
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型大小最明显的方法是对权重矩阵进行稀疏化。修剪方法基于被修剪的内容和实际用于修剪的逻辑有所不同。给定一个矩阵，可以进行修剪。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some weight entries
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些权重条目
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rows or columns (i.e., neurons)
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行或列（即，神经元）
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Weight blocks
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重块
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Attention heads (in case of Transformer based methods)
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力头（在基于Transformer的方法中）
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Layers or a combination of the structures.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层或结构的组合。
- en: Other interesting aspects of pruning methods include the following.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝方法的其他有趣方面包括以下几点。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How to decide which weights/neurons/blocks/heads to prune?
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何决定剪枝哪些权重/神经元/块/头？
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Should you prune large networks or build small networks?
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该剪枝大型网络还是构建小型网络？
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Should you do one-shot pruning versus iterative/gradual pruning?
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该选择一次性剪枝还是迭代/渐进式剪枝？
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How does regularization help when pruning?
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝时正则化如何帮助？
- en: 'We discuss these aspects of pruning based methods in Section [2](#S2 "2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第[2](#S2 "2\. Pruning ‣ Compression of Deep Learning Models for Text: A
    Survey")节讨论了这些剪枝方法的方面。'
- en: 'Besides removing the weights themselves, another way to reduce the size of
    weight matrices is to reduce the number of bits needed to represent each weight.
    Typically weights are stored as 32-bit double values. In an extreme case, weights
    can be quantized to two values (binary 1 bit). But other popular ways include
    quantization to three values (ternary) or multiple bits. Quantization can be uniform
    or non-uniform. Quantization methods can be deterministic or stochastic. Quantization
    can be performed in a loss-aware or unaware manner. Quantization bin ranges can
    be trained versus tuned. Finally, the level of quantization needs to be different
    across layers of RNNs, LSTMs or Transformers to attain a favorable model size
    versus accuracy tradeoff. We discuss these aspects of quantization based methods
    in Section [3](#S3 "3\. Quantization ‣ Compression of Deep Learning Models for
    Text: A Survey").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '除了去除权重本身，减少权重矩阵大小的另一种方法是减少表示每个权重所需的位数。通常权重以32位双精度值存储。在极端情况下，权重可以量化为两个值（二进制1位）。但其他流行的方法包括量化为三个值（三进制）或多个位。量化可以是均匀的或非均匀的。量化方法可以是确定性的或随机的。量化可以在有损或无损的方式下进行。量化箱范围可以是训练的或调整的。最后，为了获得有利的模型大小与准确性的权衡，RNN、LSTM或Transformer的不同层的量化级别需要不同。我们在第[3](#S3
    "3\. Quantization ‣ Compression of Deep Learning Models for Text: A Survey")节讨论了这些量化方法的方面。'
- en: A third way of doing model compression is using knowledge distillation (also
    broadly known as student teacher networks). In such methods, the idea is to first
    train a deep teacher model using labeled data, and then transfer “dark knowledge”
    from teacher to train a shallow student network. Thus, the student model is trained
    to mimic a pre-trained, larger teacher. After the student is trained, it is deployed
    while the teacher network can be thrown. Distillation methods vary based on the
    following design choices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩的第三种方法是使用知识蒸馏（也被广泛称为学生教师网络）。在这种方法中，首先使用标记数据训练一个深度教师模型，然后将“隐性知识”从教师转移到训练一个浅层学生网络。因此，学生模型被训练成模仿一个预训练的、更大的教师模型。学生模型训练完成后，可以进行部署，而教师网络则可以丢弃。蒸馏方法的差异取决于以下设计选择。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different types of teacher model
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教师模型的不同类型
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different types of loss function like squared error between the logits of the
    models, KL divergence between the predictive distributions, or some other measure
    of agreement between the model predictions
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同类型的损失函数，如模型的对数值之间的平方误差、预测分布之间的KL散度或模型预测之间的其他一致性度量。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different choices for what dataset the student model trains on (a large unlabeled
    dataset, a held-out data set, or the original training set)
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学生模型训练的不同数据集选择（大型未标记数据集、保留数据集或原始训练集）
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different choices for what to mimic from the teacher – teacher’s class probabilities,
    teacher’s feature representation.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从教师那里模仿什么的不同选择——教师的类别概率、教师的特征表示。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learn from whom – teacher, teacher assistant, or other fellow students.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从谁那里学习——教师、教师助理还是其他同学。
- en: 'We discuss these aspects of knowledge distillation based methods in Section [4](#S4
    "4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第[4](#S4 "4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning
    Models for Text: A Survey")节讨论了这些知识蒸馏方法的方面。'
- en: Another way that reduces overall weights is to have parameters shared across
    multiple weight structures. Broadly, the method is called parameter sharing. Methods
    differ depending on the following aspects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少总体权重的方法是让参数在多个权重结构之间共享。一般来说，这种方法称为参数共享。方法的差异取决于以下方面。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Which parameters are shared
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哪些参数是共享的
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Technique used to share parameters
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 共享参数使用的技术
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The level at which sharing is performed
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行共享的层级
- en: 'We discuss these aspects of parameter sharing based methods in Section [5](#S5
    "5\. Parameter sharing ‣ Compression of Deep Learning Models for Text: A Survey").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[5](#S5 "5\. 参数共享 ‣ 文本深度学习模型压缩：一项综述")节中讨论了基于参数共享的方法的这些方面。
- en: Yet another way to avoid large matrices is to approximate them using a combination
    of smaller matrices. Such tensor decomposition methods for model compression factorize
    large tensors into multiple smaller components. Methods differ based on the following
    aspects.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个避免大型矩阵的方法是使用多个较小矩阵的组合进行近似。这种用于模型压缩的张量分解方法将大型张量分解为多个较小的组件。方法根据以下方面有所不同。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Type of factorization technique
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分解技术的类型
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Matrices being factorized
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 被分解的矩阵
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The property of weight matrix being exploited
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重矩阵的利用特性
- en: 'We discuss these aspects of tensor decomposition methods in Section [6](#S6
    "6\. Tensor decomposition ‣ Compression of Deep Learning Models for Text: A Survey").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[6](#S6 "6\. 张量分解 ‣ 文本深度学习模型压缩：一项综述")节中讨论了张量分解方法的这些方面。
- en: 'In Transformer based models, besides the model size, latency is a concern because
    of quadratic complexity in terms of the input sequence size. Also, RAM needed
    for Transformer models is quadratic in nature. Hence, very recently, there have
    been several efforts on designing Transformers with sub-quadratic complexity.
    Some of these methods are super-linear while many are linear. Linear complexity
    methods use various techniques for enforcing linearity – the broad idea is to
    compute a transformed representation for every token using attention over a fixed
    small number of other tokens. Methods differ in terms of defining the set of other
    tokens to be used to compute a transformed representation for the current token.
    We discuss such methods in Section [7](#S7 "7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 Transformer 的模型中，除了模型大小，延迟也是一个关注点，因为输入序列大小的平方复杂度。除此之外，Transformer 模型所需的内存也是平方级的。因此，最近有几项努力在设计具有亚平方复杂度的
    Transformer。这些方法中有些是超线性的，而许多是线性的。线性复杂度方法使用各种技术来强制执行线性——其广泛的想法是使用对固定少量其他标记的注意力来计算每个标记的变换表示。方法在定义用于计算当前标记变换表示的其他标记集合方面有所不同。我们在第[7](#S7
    "7\. 亚平方复杂度的 Transformer ‣ 文本深度学习模型压缩：一项综述")节中讨论了这些方法。
- en: 2\. Pruning
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 剪枝
- en: 'The first proposed methods for model compression were based on pruning. Pruning
    can be done in two ways: structured versus unstructured. In unstructured pruning,
    individual weight connections are removed from a network by setting them to 0\.
    One can prune away weights from a weight matrix depending on various criteria
    (e.g., prune away low magnitude weights). Such unstructured pruning methods lead
    to sparse matrices and need special sparse matrix manipulation libraries at inference
    time. Hence, various structured pruning methods have also been proposed which
    aim to prune away structures like neurons, weight matrix blocks, attention heads
    or layers. Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning
    Models for Text: A Survey") provides a broad overview of various pruning styles.
    Fig [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey")(B) depicts unstructured pruning. Fig [2](#S2.F2 "Figure 2 ‣ 2\.
    Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(C)-(G) shows
    various structured pruning methods. In this section, we discuss unstructured weight
    pruning methods in Section [2.1](#S2.SS1 "2.1\. Pruning Weights ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey") and structured pruning
    methods in Sections [2.2](#S2.SS2 "2.2\. Pruning Neurons ‣ 2\. Pruning ‣ Compression
    of Deep Learning Models for Text: A Survey")-[2.4](#S2.SS4 "2.4\. Pruning Heads
    and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最早提出的模型压缩方法基于剪枝。剪枝可以通过两种方式进行：结构化剪枝和非结构化剪枝。在非结构化剪枝中，通过将网络中的单个权重连接设置为 0 来移除这些连接。可以根据各种标准（例如，剪除低幅度权重）从权重矩阵中剪除权重。这种非结构化剪枝方法会导致稀疏矩阵，并且在推理时需要特殊的稀疏矩阵操作库。因此，还提出了各种结构化剪枝方法，这些方法旨在剪除神经元、权重矩阵块、注意力头或层等结构。图 [2](#S2.F2
    "图 2 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述") 提供了各种剪枝风格的广泛概述。图 [2](#S2.F2 "图 2 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述")(B)
    描述了非结构化剪枝。图 [2](#S2.F2 "图 2 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述")(C)-(G) 展示了各种结构化剪枝方法。在本节中，我们在第
    [2.1](#S2.SS1 "2.1\. 剪枝权重 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述") 节讨论了非结构化权重剪枝方法，在第 [2.2](#S2.SS2
    "2.2\. 剪枝神经元 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述") 至 [2.4](#S2.SS4 "2.4\. 剪枝头部和层 ‣ 2\. 剪枝
    ‣ 文本深度学习模型的压缩：综述") 节讨论了结构化剪枝方法。
- en: '![Refer to caption](img/4959ac34477ed48696e7936d054d55b4.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4959ac34477ed48696e7936d054d55b4.png)'
- en: 'Figure 2\. Different Types of Pruning: (A) represents no pruning. Filled cells
    represent pruned entries. (B), (H) and (I) are unstructured pruning methods discussed
    in Section [2.1](#S2.SS1 "2.1\. Pruning Weights ‣ 2\. Pruning ‣ Compression of
    Deep Learning Models for Text: A Survey"). Remaining are structured pruning methods:
    (C) is discussed in Section [2.2](#S2.SS2 "2.2\. Pruning Neurons ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey"), (D) in Section [2.3](#S2.SS3
    "2.3\. Pruning Blocks ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey"), (E), (F) and (G) in Section [2.4](#S2.SS4 "2.4\. Pruning Heads
    and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 剪枝的不同类型：（A）表示没有剪枝。填充的单元格表示已剪枝的条目。（B）、（H）和（I）是第 [2.1](#S2.SS1 "2.1\. 剪枝权重
    ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述") 节讨论的非结构化剪枝方法。其余的是结构化剪枝方法：（C）在第 [2.2](#S2.SS2 "2.2\.
    剪枝神经元 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述") 节讨论，（D）在第 [2.3](#S2.SS3 "2.3\. 剪枝块 ‣ 2\. 剪枝
    ‣ 文本深度学习模型的压缩：综述") 节讨论，（E）、（F）和（G）在第 [2.4](#S2.SS4 "2.4\. 剪枝头部和层 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：综述")
    节讨论。
- en: In pruning, the main idea is to grow a large model and then prune away weights
    to end up with a much smaller but effective model. This is inspired by the following
    biological observation. Trillions of synapses are generated in the human brain
    during the first few months of birth. At one year old, synapse count peaks at
    1000 trillion. And then natural pruning begins to occur. A ten year old child
    has nearly 500 trillion synapses. This ‘pruning’ mechanism removes redundant connections
    in the brain (Walsh, [2013](#bib.bib124)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在剪枝中，主要思想是先训练一个大型模型，然后剪除权重，得到一个更小但有效的模型。这一过程受到以下生物学观察的启发。人类大脑在出生后的头几个月生成了数万亿个突触。在一岁时，突触数量达到
    1000 万亿的峰值。然后，自然剪枝开始发生。一个十岁的孩子大约有 500 万亿个突触。这种‘剪枝’机制移除大脑中的冗余连接（Walsh, [2013](#bib.bib124)）。
- en: One natural question is should you prune large networks or build small dense
    networks? Pruning involves extra processing plus sparse matrices need special
    handling. Can we avoid it by training smaller models? Zhu et al. (Zhu and Gupta,
    [2017](#bib.bib146)) experimented with models of various sizes with/ without pruning
    of stacked LSTMs models for language modeling, and seq2seq models for Neural Machine
    Translation (NMT). They found that large-sparse models consistently outperform
    small-dense models and achieve up to 10x reduction in number of non-zero parameters
    with minimal loss in accuracy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是，应该剪枝大规模网络还是构建小规模密集网络？剪枝涉及额外的处理，并且稀疏矩阵需要特殊处理。我们是否可以通过训练较小的模型来避免这一点？Zhu
    等人（Zhu 和 Gupta，[2017](#bib.bib146)）在语言建模的堆叠 LSTM 模型和神经机器翻译（NMT）的 seq2seq 模型上实验了不同规模的模型，有无剪枝。他们发现，大规模稀疏模型始终优于小规模密集模型，并且在非零参数数量上可减少多达
    10 倍，准确性损失极小。
- en: 2.1\. Pruning Weights
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 剪枝权重
- en: 'The weights to be pruned can be chosen using two heuristics: (1) Using Hessian
    matrix computation, or (2) using magnitude of the weights. Further, magnitude
    based pruning methods could do pruning in one shot (typically statically after
    training is done), or iteratively (also called dynamic or gradual pruning), or
    iteratively with pruning and densification. Accordingly, there are four main ways
    of performing unstructured weight pruning: (1) Hessian based methods, (2) magnitude
    pruning, (3) iterative magnitude pruning, and (4) iterative magnitude pruning
    and densification. In this subsection, we discuss these methods in detail.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要剪枝的权重可以通过两种启发式方法选择：（1）使用 Hessian 矩阵计算，或（2）使用权重的幅度。此外，基于幅度的剪枝方法可以一次性剪枝（通常在训练完成后静态进行），也可以迭代进行（也称为动态或渐进剪枝），或迭代剪枝和稠密化。因此，进行无结构权重剪枝的主要方法有四种：（1）基于
    Hessian 的方法，（2）幅度剪枝，（3）迭代幅度剪枝，以及（4）迭代幅度剪枝和稠密化。在本小节中，我们详细讨论这些方法。
- en: 2.1.1\. Hessian based Methods
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 基于 Hessian 的方法
- en: In their seminal work (Optimal Brain Damage or OBD) on proposing weight pruning
    as a method for model compression, LeCun et al. (LeCun et al., [1990](#bib.bib63))
    defined saliency of a weight as change in the objective function $E$ caused by
    deleting that parameter. Using Taylor series and making multiple assumptions,
    they propose to use the following as a measure of saliency of weight $u_{i}$.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的开创性工作（最优脑损伤或 OBD）中，LeCun 等人（LeCun 等，[1990](#bib.bib63)）提出了将权重剪枝作为模型压缩的一种方法，并将权重的显著性定义为删除该参数引起的目标函数
    $E$ 的变化。利用泰勒级数并做出多个假设，他们建议使用以下作为权重 $u_{i}$ 的显著性度量。
- en: '| (1) |  | $\displaystyle s(u_{i})=\frac{1}{2}\sum_{i}h_{ii}\delta u_{i}^{2}$
    |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle s(u_{i})=\frac{1}{2}\sum_{i}h_{ii}\delta u_{i}^{2}$
    |  |'
- en: where $h_{ii}=\frac{\partial^{2}E}{\partial u_{i}\partial u_{j}}$ is the $i^{th}$
    element on the diagonal of the Hessian matrix. Weights with low saliency can be
    pruned and the pruned network can be retrained to adjust the remaining weights.
    The procedure for computation of the diagonal of the Hessian is very similar to
    the back-propagation algorithm used for computing the first derivatives. Hence,
    computing the diagonal of the Hessian is of the same order of complexity as computing
    the gradient.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{ii}=\frac{\partial^{2}E}{\partial u_{i}\partial u_{j}}$ 是 Hessian 矩阵的第
    $i^{th}$ 个对角元素。显著性较低的权重可以被剪枝，剪枝后的网络可以重新训练以调整剩余的权重。计算 Hessian 对角线的过程与用于计算一阶导数的反向传播算法非常相似。因此，计算
    Hessian 对角线的复杂度与计算梯度的复杂度相同。
- en: OBD ignores cross terms in the Hessian matrix. But on most real datasets, Hessian
    is strongly non-diagonal. Hence, to avoid pruning of important weights, Hassibi
    et al. (Hassibi and Stork, [1993](#bib.bib40)) proposed a method called Optimal
    Brain Surgeon (OBS) which considers cross terms as well. Using a similar derivative
    of $E$ wrt weight $w_{i}$, saliency of the weight is computed as follows.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: OBD 忽略了 Hessian 矩阵中的交叉项。但在大多数真实数据集中，Hessian 矩阵的非对角性很强。因此，为了避免剪枝重要的权重，Hassibi
    等人（Hassibi 和 Stork，[1993](#bib.bib40)）提出了一种称为最优脑外科医生（OBS）的方法，该方法也考虑了交叉项。使用类似于权重
    $w_{i}$ 的 $E$ 导数的方式，计算权重的显著性如下。
- en: '| (2) |  | $\displaystyle L_{i}=\frac{1}{2}\frac{w_{i}^{2}}{[H^{-1}]_{ii}}$
    |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle L_{i}=\frac{1}{2}\frac{w_{i}^{2}}{[H^{-1}]_{ii}}$
    |  |'
- en: Computing $H^{-1}$ is difficult. Hence, they provide a faster recursion relation
    for calculating $H^{-1}$ from training data and structural information of the
    network. Also, unlike other methods (like OBD or magnitude pruning), OBS does
    not demand (typically slow) retraining after the pruning of a weight. In every
    step, we delete $w_{i}$ with min $L_{i}$ and update all weights as follows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 $H^{-1}$ 很困难。因此，他们提供了一种更快的递归关系来从训练数据和网络的结构信息中计算 $H^{-1}$。此外，与其他方法（如 OBD 或大小修剪）不同，OBS
    不要求（通常较慢的）在修剪权重后重新训练。在每一步中，我们删除最小 $L_{i}$ 的 $w_{i}$ 并更新所有权重。
- en: '| (3) |  | $\displaystyle\delta w=-\frac{w_{i}}{[H^{-1}]_{ii}}H^{-1}e_{i}$
    |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\delta w=-\frac{w_{i}}{[H^{-1}]_{ii}}H^{-1}e_{i}$
    |  |'
- en: where $e_{i}$ is the unit vector in weight space corresponding to (scalar) weight
    $w_{i}$. Unfortunately, these methods (OBD and OBS) are computationally prohibitive
    as second derivative (i.e. Hessian) computations are expensive.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $e_{i}$ 是与（标量）权重 $w_{i}$ 相对应的权重空间中的单位向量。不幸的是，这些方法（OBD 和 OBS）的计算开销很大，因为二阶导数（即
    Hessian）的计算非常昂贵。
- en: 2.1.2\. Magnitude Pruning Methods
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 大小修剪方法
- en: A more computationally feasible method for pruning connections and relearning
    weights based solely on the magnitude of the original weights is to simply prune
    away weights with small magnitudes. The idea was first proposed by Han et al. (Han
    et al., [2015a](#bib.bib37)). Pruning away low magnitude weights makes the matrix
    sparse. Sparse matrices can be stored in Compressed Sparse Row/Column (CSR/CSC)
    formats. Further space can be saved by storing the index difference instead of
    the absolute position, and encoding this difference using small fixed number of
    bits. See et al. (See et al., [2016](#bib.bib104)) experimented with these pruning
    methods on encoder-decoder LSTM NMT (neural machine translation) models. They
    perform magnitude pruning on all weight matrices of a 4-layer LSTM. They find
    that higher layers, attention and softmax weights are the most important, while
    lower layers and the embedding weights hold a lot of redundancy. At the lower
    layers the parameters for the input are most crucial, but at higher layers the
    parameters for the gates also become important. These methods typically have a
    target pruning percent as a hyper-parameter and pruning is either performed statically
    (after training the full model) or dynamically (while training itself). Retraining
    the sparse pruned network helps in improving accuracy.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一种计算上更可行的修剪连接和仅基于原始权重大小重新学习权重的方法是简单地修剪掉小幅度的权重。这个想法最早由 Han 等人提出（Han et al., [2015a](#bib.bib37)）。修剪掉低幅度的权重使得矩阵变得稀疏。稀疏矩阵可以以压缩稀疏行/列（CSR/CSC）格式存储。通过存储索引差值而不是绝对位置，并使用小固定数目的位来编码这个差值，可以进一步节省空间。See
    等人（See et al., [2016](#bib.bib104)）在编码器-解码器 LSTM 神经机器翻译（NMT）模型上实验了这些修剪方法。他们对 4
    层 LSTM 的所有权重矩阵进行了大小修剪。他们发现，高层、注意力和 softmax 权重是最重要的，而低层和嵌入权重包含了很多冗余。在低层，输入参数最为关键，而在高层，门控参数也变得重要。这些方法通常将目标修剪百分比作为超参数，修剪要么在静态（在训练完整模型后）进行，要么在动态（在训练过程中）进行。重新训练稀疏修剪后的网络有助于提高准确性。
- en: 'In a typical encoder-decoder LSTM model, there are these weight classes: source
    embedding weights, target embedding weights, source layer weights, target layer
    weights, attention weights and softmax weights. An important consideration related
    to magnitude pruning is how do we distribute the pruning over these different
    weight classes of a model, given a target $x$% pruning? Three ways suggested by
    See et al. (See et al., [2016](#bib.bib104)) include class-blind, class-uniform
    and class-distribution. In the class-blind way, we take all parameters, sort them
    by magnitude and prune the $x$% with smallest magnitude, regardless of the weight
    class. So some classes are pruned proportionally more than others. In the class-uniform
    way, Within each class, we sort the weights by magnitude and prune the x% with
    smallest magnitude. So all classes have exactly x% of their parameters pruned.
    In the class-distribution scheme, for each class $c$, weights with magnitude less
    than $\lambda\sigma_{c}$ are pruned. Here, $\sigma_{c}$ is the standard deviation
    of that class and $\lambda$ is a universal parameter chosen such that in total,
    $x$% of all parameters are pruned. Class-blind pruning is the simplest and adheres
    to the principle that pruning weights (or equivalently, setting them to zero)
    is least damaging when those weights are small, regardless of their locations
    in the architecture. Class-uniform pruning and class-distribution pruning both
    seek to prune proportionally within each weight class, either absolutely, or relative
    to the standard deviation of that class. They observe that class-blind pruning
    outperforms both other schemes.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的编码器-解码器 LSTM 模型中，有以下几种权重类别：源嵌入权重、目标嵌入权重、源层权重、目标层权重、注意力权重和 softmax 权重。与幅度剪枝相关的重要考虑是如何在模型的这些不同权重类别中分配剪枝，以实现目标
    $x$% 剪枝？See 等人（See et al., [2016](#bib.bib104)）提出了三种方法：类盲剪枝、类均匀剪枝和类分布剪枝。在类盲剪枝中，我们将所有参数按幅度排序，并剪枝幅度最小的
    $x$% 参数，而不考虑权重类别。因此，一些类别的剪枝比例会比其他
- en: 2.1.3\. Iterative Magnitude Pruning Methods
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 迭代幅度剪枝方法
- en: 'Typically, it has been seen that rather than pruning in one-shot, it is a good
    idea to prune gradually over epochs. This way of pruning is called iterative or
    gradual pruning (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of
    Deep Learning Models for Text: A Survey")(H)). For starting proportion $x$% and
    ending proportion $y$%, iterative magnitude pruning procedure prunes $x$% of each
    of the weights, does re-training, and then prunes $(y-x)/T$% of the weights every
    $K$ iterations. $T$ is the number of times, pruning is done. Sometimes, pruning
    is started after few warmup iterations have already been performed. Magnitude
    pruning has been seen to be very effective with regularization ($L_{1}/L_{2}$)
    while training. Dropouts also help in effective pruning. In some pruning methods,
    a weight once pruned cannot be a part of the network in future iterations. On
    the other hand, other methods do not modify the gradients of a pruned weight in
    the back-propagation step. In that case, it is possible for the updates of a pruned
    weight to be larger than the threshold of that layer, and then the weight will
    be involved in the forward pass again. Also, in every pruning iteration, we could
    either use a fixed threshold (Han et al., [2015b](#bib.bib39)) or monotonically
    increase it (Narang et al., [2017a](#bib.bib87)).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '通常发现，与其一次性剪枝，不如在多个周期内逐步剪枝。这样的剪枝方式被称为迭代或逐步剪枝（见图[2](#S2.F2 "Figure 2 ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey")(H)）。对于起始比例$x$%和结束比例$y$%，迭代幅度剪枝过程剪去$x$%的权重，进行重新训练，然后每$K$次迭代剪去$(y-x)/T$%的权重。$T$是剪枝的次数。有时，剪枝在执行了几次热身迭代后开始。幅度剪枝在训练时结合正则化（$L_{1}/L_{2}$）非常有效。Dropouts也有助于有效剪枝。在一些剪枝方法中，一旦剪枝的权重不能在未来迭代中成为网络的一部分。另一方面，其他方法在反向传播步骤中不会修改剪枝权重的梯度。在这种情况下，剪枝权重的更新可能大于该层的阈值，权重将再次参与前向传递。此外，在每次剪枝迭代中，我们可以使用固定的阈值（Han
    等，[2015b](#bib.bib39)）或单调增加它（Narang 等，[2017a](#bib.bib87)）。'
- en: In case of gradual pruning (Narang et al., [2017a](#bib.bib87)), where pruning
    threshold $\epsilon$ is monotonically increased, $\epsilon$ is determined as follows
    in every iteration $i$. Let $f$ be the number of iterations after which $\epsilon$
    is updated. Also, after a few warmup iterations, weights are sorted using absolute
    values and we pick the weight corresponding to the $90^{th}$ percentile as $q$.
    Pruning threshold $\epsilon$ is increased in two stages. In the first stage (which
    starts at start iteration $s$ and continues until ramp iteration $r$, we use $\theta$
    as the initial slope to prune weights. In the second stage (which starts at ramp
    iteration $r$ and continues until end iteration $e$), we use $\phi$ as the ramp
    slope to change the rate of pruning. Typically, $\phi$ is set to 1.5$\theta$ where
    $\theta$ is calculated as follows.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在逐步剪枝的情况下（Narang 等，[2017a](#bib.bib87)），其中剪枝阈值$\epsilon$单调增加，$\epsilon$在每次迭代$i$中确定。设$f$为$\epsilon$更新后的迭代次数。此外，在经过几次热身迭代后，使用绝对值对权重进行排序，并选择对应于第$90^{th}$百分位的权重作为$q$。剪枝阈值$\epsilon$分为两个阶段增加。在第一个阶段（从开始迭代$s$开始，到斜坡迭代$r$结束），我们使用$\theta$作为初始斜率来剪枝权重。在第二个阶段（从斜坡迭代$r$开始，到结束迭代$e$），我们使用$\phi$作为斜坡斜率来改变剪枝速度。通常，$\phi$设置为1.5$\theta$，其中$\theta$的计算方法如下。
- en: '| (4) |  | $\displaystyle\theta=\frac{2qf}{2(r-s)+3(e-r)}$ |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\theta=\frac{2qf}{2(r-s)+3(e-r)}$ |  |'
- en: Thus, from iteration $s$ to $r$, we set the pruning threshold as follows.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从迭代$s$到$r$，我们设置剪枝阈值如下。
- en: '| (5) |  | $\displaystyle\epsilon=\frac{\theta(i-s+1)}{f}$ |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle\epsilon=\frac{\theta(i-s+1)}{f}$ |  |'
- en: From iterations $r+1$ to $e$, we set the pruning threshold as follows.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从迭代$r+1$到$e$，我们设置剪枝阈值如下。
- en: '| (6) |  | $\displaystyle\epsilon=\frac{\theta(r-s+1)+\phi(i-r+1)}{f}$ |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\epsilon=\frac{\theta(r-s+1)+\phi(i-r+1)}{f}$ |  |'
- en: Typically when pruning, biases are not pruned since they are much fewer in number.
    Overall, RNN/LSTM model size can be reduced by 90% and speed-up is around 2x to
    7x using gradual pruning with no deterioration in accuracy. Also, layers closer
    to input are pruned more aggressively compared to the final layers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在剪枝时，不剪枝偏置，因为它们的数量相对较少。总体而言，使用逐步剪枝可以将RNN/LSTM模型的大小减少90%，速度提升约2倍至7倍，且精度没有降低。此外，与最终层相比，更靠近输入的层剪枝更为激进。
- en: Another way of performing iterative pruning is to set a pruning target per iteration (Zhu
    and Gupta, [2017](#bib.bib146)). In this scheme, we start with an initial sparsity
    value $s_{0}$. To achieve a final sparsity value of $s_{f}$ after $n$ pruning
    steps with pruning frequency $f$, pruning target in iteration $i$ can be computed
    as follows.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种执行迭代修剪的方法是为每次迭代设定修剪目标（Zhu 和 Gupta，[2017](#bib.bib146)）。在这种方案中，我们从初始稀疏值 $s_{0}$
    开始。为了在 $n$ 次修剪步骤后达到最终稀疏值 $s_{f}$，修剪目标在第 $i$ 次迭代中可以如下计算。
- en: '| (7) |  | $\displaystyle s_{i}=s_{f}+(s_{0}-s_{f})\left(1-\frac{i}{nf}\right)^{3}$
    |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\displaystyle s_{i}=s_{f}+(s_{0}-s_{f})\left(1-\frac{i}{nf}\right)^{3}$
    |  |'
- en: Thus, the sparsity of the network is gradually increased while allowing the
    network training steps to recover from any pruning-induced loss in accuracy. We
    prune the network rapidly in the initial phase when the redundant connections
    are abundant and gradually reduce the number of weights being pruned each time
    as there are fewer and fewer weights remaining in the network.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络的稀疏度会逐渐增加，同时允许网络训练步骤从任何修剪引起的准确度损失中恢复。在初始阶段，当冗余连接较多时，我们迅速修剪网络，并随着网络中剩余权重的逐渐减少，每次修剪的权重数量逐渐减少。
- en: Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) found that iterative pruning
    leads to poor results when pruning Transformer models like BERT. Guo et al. (Guo
    et al., [2019a](#bib.bib33)) found that there are two problems with pruning especially
    when done with regularization.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Cheong 等人（Cheong 和 Daniel，[2019](#bib.bib16)）发现，当修剪像 BERT 这样的 Transformer 模型时，迭代修剪会导致较差的结果。Guo
    等人（Guo 等人，[2019a](#bib.bib33)）发现修剪，特别是带有正则化时，会有两个问题。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The larger weights $w_{j}$ are penalized more heavily than smaller weights $w_{i}$
    in $L_{1}$ regularization, which violates the original intention of weight pruning,
    “removing the unimportant connections”.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 $L_{1}$ 正则化中，大的权重 $w_{j}$ 被比小的权重 $w_{i}$ 更加严厉地惩罚，这违背了权重修剪的原始意图，即“去除不重要的连接”。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Direct optimization of a regularization penalty term causes divergence from
    the original loss function and has negative effect on the effectiveness of gradient-based
    update.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接优化正则化惩罚项会导致偏离原始损失函数，并对基于梯度的更新效果产生负面影响。
- en: They propose to perform reweighted $L_{1}$ minimization where $\alpha_{i}>0$
    are inversely proportional to magnitude of corresponding weights $|w_{i}|$. Thus,
    they solve the following optimization problem
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 他们建议执行重新加权的 $L_{1}$ 最小化，其中 $\alpha_{i}>0$ 与相应权重 $|w_{i}|$ 的大小成反比。因此，他们解决了以下优化问题
- en: '| (8) |  | $\displaystyle\min_{w}f(w)+\gamma\sum_{i}\alpha_{i}&#124;w_{i}&#124;$
    |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle\min_{w}f(w)+\gamma\sum_{i}\alpha_{i}|w_{i}|$ |  |'
- en: where $f(w)$ is the original loss function for the network. This optimization
    is solved using a reweighted proximal pruning (RPP) method (which depends on proximal
    operators). RPP decouples the goals of high sparsity from minimizing loss, and
    hence leads to improved accuracy even with high levels of pruning for BERT.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(w)$ 是网络的原始损失函数。这个优化问题通过重新加权的近端修剪（RPP）方法（依赖于近端算子）来解决。RPP 将高稀疏度的目标与最小化损失解耦，因此即使在对
    BERT 进行高水平修剪时，也能提高准确性。
- en: 2.1.4\. Iterative Magnitude Pruning and Densification
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 迭代幅度修剪与稠密化
- en: 'Further, the effectiveness of pruning can be improved by performing pruning
    and densification (Han et al., [2016b](#bib.bib38); Dai et al., [2018](#bib.bib20))
    alternately across multiple iterations (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey")(I)). There are two
    ways of doing this. In the first method (Han et al., [2016b](#bib.bib38)), in
    each iteration, either pruning is performed or densification. The sparse training
    regularizes the model, and the dense training restores the pruned weights, increasing
    the model capacity without overfitting. Sparsification helps the optimizer escape
    saddle points, and leads to regularized training which converges to a significantly
    better minima. In the second method (Dai et al., [2018](#bib.bib20)), in every
    iteration some dormant weights can reappear in the network while other active
    ones can get pruned out. A dormant $w\in W$ is activated iff $|w.grad|$ is larger
    than the $(100\alpha)^{th}$ percentile of all elements in $|W.grad|$. A $w\in
    W$ is removed iff $|w|$ is smaller than the $(100\beta)^{th}$ percentile of all
    elements in $|W|$. $\alpha$ and $\beta$ refer to growth ratio, and pruning ratio,
    respectively.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，通过在多个迭代中交替进行剪枝和稠密化（Han et al., [2016b](#bib.bib38); Dai et al., [2018](#bib.bib20)），可以提高剪枝的效果（见图
    [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text:
    A Survey")(I)）。有两种方法可以做到这一点。第一种方法（Han et al., [2016b](#bib.bib38)）是在每次迭代中，要么进行剪枝，要么进行稠密化。稀疏训练对模型进行正则化，而稠密训练恢复了剪枝的权重，增加了模型的容量而不会过拟合。稀疏化帮助优化器摆脱鞍点，并导致正则化训练收敛到更好的极小值。第二种方法（Dai
    et al., [2018](#bib.bib20)）是在每次迭代中，一些沉睡的权重可以重新出现在网络中，而其他活跃的权重则可以被剪枝。一个沉睡的 $w\in
    W$ 当且仅当 $|w.grad|$ 大于所有 $|W.grad|$ 元素的 $(100\alpha)^{th}$ 百分位时被激活。一个 $w\in W$
    当且仅当 $|w|$ 小于所有 $|W|$ 元素的 $(100\beta)^{th}$ 百分位时被移除。$\alpha$ 和 $\beta$ 分别指增长率和剪枝率。'
- en: 2.2\. Pruning Neurons
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 剪枝神经元
- en: 'It is difficult to implement unstructured pruning practically since, at inference
    time, special support is needed for matrix multiplication in the sparse space.
    Pruning away neurons leads to removal of a row or a column from a weight matrix,
    thereby avoiding sparse matrix handling (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey")(C)). However, compared
    to pruning weights, pruning neurons is less flexible since we need to find entire
    rows/columns for deletion. In this section, we discuss ways of determining neurons
    that can be pruned.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上实施无结构剪枝很困难，因为在推理时，需要对稀疏空间中的矩阵乘法提供特殊支持。剪除神经元会导致从权重矩阵中删除一行或一列，从而避免了稀疏矩阵处理（见图
    [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text:
    A Survey")(C)）。然而，与剪枝权重相比，剪枝神经元的灵活性较低，因为我们需要找到整个行/列进行删除。本节讨论了确定可以剪枝的神经元的方法。'
- en: 2.2.1\. Removing Low Importance Neurons
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 去除低重要性神经元
- en: He et al. (He et al., [2014](#bib.bib42)) proposed three node importance functions
    to determine importance score for neurons.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: He 等人（He et al., [2014](#bib.bib42)）提出了三种节点重要性函数来确定神经元的重要性评分。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Entropy: For a neuron $i$, let $a_{i}$ ($d_{i}$) be the #instances with node
    output $>(or\leq)$ 0.5 for binary classification with a sigmoid activation. Then
    entropy for node $i$ can be written as follows.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 熵：对于神经元 $i$，设 $a_{i}$ ($d_{i}$) 为节点输出 $>(或\leq)$ 0.5 的实例数，适用于具有 sigmoid 激活的二分类。则节点
    $i$ 的熵可以写作如下。
- en: '| (9) |  | $\displaystyle\text{Entropy}(i)=\frac{d_{i}}{a_{i}+d_{i}}\log_{2}\frac{d_{i}}{a_{i}+d_{i}}+\frac{a_{i}}{a_{i}+d_{i}}\log_{2}\frac{a_{i}}{a_{i}+d_{i}}$
    |  |'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle\text{熵}(i)=\frac{d_{i}}{a_{i}+d_{i}}\log_{2}\frac{d_{i}}{a_{i}+d_{i}}+\frac{a_{i}}{a_{i}+d_{i}}\log_{2}\frac{a_{i}}{a_{i}+d_{i}}$
    |  |'
- en: The intuition is that if one node’s outputs are almost identical on all training
    data, these outputs do not generate variations to later layers and consequently
    the node may not be useful.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直观上，如果一个节点的输出在所有训练数据上几乎相同，这些输出不会对后续层产生变化，因此该节点可能没有用处。
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Output-weights Norm (onorm): average $L_{1}$-norm of the weights of its outgoing
    links.'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出权重范数（onorm）：其输出链接权重的平均 $L_{1}$-范数。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input-weights norm (inorm): average $L_{1}$-norm of the weights of its incoming
    links.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入权重范数（inorm）：其输入链接权重的平均 $L_{1}$-范数。
- en: All the neurons are sorted by their scores and nodes with less importance values
    are removed. In most cases, onorm has been found to be the best among these importance
    functions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所有神经元按其评分排序，重要性值较低的节点被删除。在大多数情况下，onorm 被发现是这些重要性函数中最好的。
- en: Special regularizers have also been proposed to force neurons to push either
    all incoming or outgoing connection weights towards zero (Murray and Chiang, [2015](#bib.bib86);
    Pan et al., [2016](#bib.bib91)). Specifically, for handling incoming connections,
    the following two regularizers are popular.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 也有专门的正则化器被提出，以强制神经元将所有输入或输出连接权重推向零（Murray和Chiang，[2015](#bib.bib86)；Pan等人，[2016](#bib.bib91)）。具体而言，处理输入连接的以下两个正则化器是流行的。
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $L_{2}$ norm on weight matrix $W$ defined as follows.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重矩阵$W$上的$L_{2}$范数定义如下。
- en: '| (10) |  | $\displaystyle\sum_{i}&#124;&#124;W_{i:}&#124;&#124;_{2}=\sum_{i}\left(\sum_{j}W^{2}_{ij}\right)^{1/2}$
    |  |'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle\sum_{i}&#124;&#124;W_{i:}&#124;&#124;_{2}=\sum_{i}\left(\sum_{j}W^{2}_{ij}\right)^{1/2}$
    |  |'
- en: This puts equal pressure on each row, but within each row, the larger values
    contribute more, and therefore there is more pressure on larger values towards
    zero.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对每一行施加相等的压力，但在每一行中，较大的值贡献更多，因此对较大值的压力更大。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $L_{\infty}$ norm on weight matrix $W$ defined as follows.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重矩阵$W$上的$L_{\infty}$范数定义如下。
- en: '| (11) |  | $\displaystyle\sum_{i}&#124;&#124;W_{i:}&#124;&#124;_{\infty}=\sum_{i}\max_{j}&#124;W_{ij}&#124;$
    |  |'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle\sum_{i}&#124;&#124;W_{i:}&#124;&#124;_{\infty}=\sum_{i}\max_{j}&#124;W_{ij}&#124;$
    |  |'
- en: This puts equal pressure on each row, but within each row, only the maximum
    value (or values) matter, and therefore the pressure towards zero is entirely
    on the maximum value(s).
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对每一行施加相等的压力，但在每一行中，只有最大值（或值）才重要，因此压力完全作用于最大值（或值）。
- en: Similar regularizers can easily be defined for outgoing connections as well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的正则化器也可以很容易地定义为出站连接。
- en: 2.2.2\. Removing Redundant Neurons
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2. 删除冗余神经元
- en: Consider a simple network with one hidden layer with $n$ neurons. Thus, the
    output can be computed as follows.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有$n$个神经元的简单网络。因此，输出可以按如下方式计算。
- en: '| (12) |  | $\displaystyle z=a_{1}h(W_{1}^{T}X)+a_{2}h(W_{2}^{T}X)+...+a_{n}h(W_{n}^{T}X)$
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\displaystyle z=a_{1}h(W_{1}^{T}X)+a_{2}h(W_{2}^{T}X)+...+a_{n}h(W_{n}^{T}X)$
    |  |'
- en: where $a_{i}$ and $W_{i}$ indicate weights. In case $W_{1}==W_{2}$, $h(w_{1}^{T}X)=h(w_{2}^{T}X)$.
    Thus, we can compute output as follows.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$a_{i}$和$W_{i}$表示权重。如果$W_{1}==W_{2}$，则$h(w_{1}^{T}X)=h(w_{2}^{T}X)$。因此，我们可以按如下方式计算输出。
- en: '| (13) |  | $\displaystyle z=(a_{1}+a_{2})h(W_{1}^{T}X)+0.h(W_{2}^{T}X)+...+a_{n}h(W_{n}^{T}X)$
    |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle z=(a_{1}+a_{2})h(W_{1}^{T}X)+0.h(W_{2}^{T}X)+...+a_{n}h(W_{n}^{T}X)$
    |  |'
- en: In general, whenever two weight sets ($W_{1}$, $W_{2}$) are equal, one of them
    can effectively be removed. This should be done with a surgery step, i.e., we
    need to alter the co-efficient $a_{1}$ to $a_{1}+a_{2}$. Of course, for many pairs
    of weight sets (i.e., neurons), $W_{1}$ and $W_{2}$ are not exactly the same.
    Hence, Srinivas et al. (Srinivas and Babu, [2015](#bib.bib109)) proposed this
    3 step method for redundant neuron identification and removal.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，只要两个权重集（$W_{1}$, $W_{2}$）相等，就可以有效地删除其中一个。这应该通过手术步骤完成，即需要将系数$a_{1}$更改为$a_{1}+a_{2}$。当然，对于许多权重集对（即神经元），$W_{1}$和$W_{2}$并不完全相同。因此，Srinivas等人（Srinivas和Babu，[2015](#bib.bib109)）提出了这种3步法用于冗余神经元的识别和移除。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compute saliency $s_{ij}$ for all possible neuron pairs (i, j) as follows.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算所有可能神经元对（i, j）的显著性$s_{ij}$，方法如下。
- en: '| (14) |  | $\displaystyle s_{ij}=\langle a_{j}^{2}\rangle&#124;&#124;\epsilon_{ij}&#124;&#124;_{2}^{2}$
    |  |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (14) |  | $\displaystyle s_{ij}=\langle a_{j}^{2}\rangle&#124;&#124;\epsilon_{ij}&#124;&#124;_{2}^{2}$
    |  |'
- en: where $\langle a_{j}^{2}\rangle$ denotes the average of the quantity over all
    output neurons. Let $S$ be the matrix with all $s_{ij}$ values.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$\langle a_{j}^{2}\rangle$表示所有输出神经元上量的平均值。设$S$为包含所有$s_{ij}$值的矩阵。
- en: •
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pick the indices $(i^{\prime},j^{\prime})$ corresponding to the minimum $s_{ij}$.
    Delete the $j^{\prime}$ neuron, and update $a_{i}^{\prime}$ as follows.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择对应于最小$s_{ij}$的索引$(i^{\prime},j^{\prime})$。删除$j^{\prime}$神经元，并按如下方式更新$a_{i}^{\prime}$。
- en: '| (15) |  | $\displaystyle a_{i}^{\prime}\leftarrow a_{i}^{\prime}+a_{j}^{\prime}$
    |  |'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle a_{i}^{\prime}\leftarrow a_{i}^{\prime}+a_{j}^{\prime}$
    |  |'
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Update $S$ by removing the $j^{\prime th}$ column and row, and updating the
    $i^{\prime th}$ column (to account for the updated $a_{i}^{\prime}$).
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过删除$j^{\prime}$列和行并更新$i^{\prime}$列（以考虑更新后的$a_{i}^{\prime}$）来更新$S$。
- en: 2.3\. Pruning Blocks
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 剪枝块
- en: 'In weight pruning, irregularity of sparse matrices limits the maximum performance
    and energy efficiency achievable on hardware accelerators. Pruning neurons avoids
    sparse matrix issues but is limited in term of overall pruning possible. Hence,
    block based pruning methods were introduced (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\.
    Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(D)).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '在权重剪枝中，稀疏矩阵的不规则性限制了在硬件加速器上可以实现的最大性能和能效。剪枝神经元可以避免稀疏矩阵问题，但整体剪枝的范围受到限制。因此，引入了基于块的剪枝方法（见图[2](#S2.F2
    "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(D)）。'
- en: 'Block-sparse formats store blocks contiguously in memory reducing irregular
    memory accesses. If the maximum magnitude weight of a block is below the current
    threshold, we set all the weights in that block to zeros. Similar to iterative
    weight pruning, block pruning (Narang et al., [2017b](#bib.bib88)) can also be
    done iteratively using a monotonically growing threshold. Any blocks that had
    been zeroed out are held at zero even after pruning has ended resulting in a sparse
    model at the end of training. Just like weight pruning (as discussed in Section [2.1](#S2.SS1
    "2.1\. Pruning Weights ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey")), the start slope $\theta$ and ramp slope $\phi$ determine the
    rate at which the threshold increases. For block pruning, we need to modify the
    start slope to account for the number of elements in a block ($N_{b}$). Thus,
    the start slope for block pruning is typically set as follows.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '块稀疏格式将块连续地存储在内存中，从而减少了不规则的内存访问。如果一个块的最大权重幅度低于当前阈值，我们将该块中的所有权重设置为零。类似于迭代权重剪枝，块剪枝（Narang
    et al., [2017b](#bib.bib88)）也可以使用单调增长的阈值进行迭代。任何已经被置零的块在剪枝结束后仍保持为零，从而在训练结束时形成一个稀疏模型。就像权重剪枝（如在第[2.1](#S2.SS1
    "2.1\. Pruning Weights ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey")节讨论的那样），起始斜率$\theta$和斜坡斜率$\phi$决定了阈值增长的速率。对于块剪枝，我们需要调整起始斜率以考虑块中的元素数量($N_{b}$)。因此，块剪枝的起始斜率通常设定如下。'
- en: '| (16) |  | $\displaystyle\theta_{b}=\theta\times\sqrt[4]{N_{b}}$ |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\displaystyle\theta_{b}=\theta\times\sqrt[4]{N_{b}}$ |  |'
- en: Further, to enable effective removal of blocks, Narang et al. (Narang et al.,
    [2017b](#bib.bib88)) propose group Lasso regularization method. Group lasso is
    a type of weight regularization that works on groups of weights and can zero out
    all the weights in a group. For each block, we add a loss term proportional to
    the $L_{2}$ norm of the block. Thus, we optimize for the following.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了有效去除块，Narang et al.（Narang et al., [2017b](#bib.bib88)）提出了组Lasso正则化方法。组Lasso是一种作用于权重组的权重正则化方法，可以将组中的所有权重置零。对于每个块，我们添加一个与块的$L_{2}$范数成比例的损失项。因此，我们优化如下。
- en: '| (17) |  | $\displaystyle\min_{w}f(w)+\lambda_{g}\sum_{g=1}^{G}&#124;&#124;w^{(g)}&#124;&#124;_{2}$
    |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\displaystyle\min_{w}f(w)+\lambda_{g}\sum_{g=1}^{G}&#124;&#124;w^{(g)}&#124;&#124;_{2}$
    |  |'
- en: When we combine group lasso with block pruning, group lasso guides the selection
    of blocks to prune. Group lasso regularization is applied to coincide with the
    pruning schedule, i.e., we turn off regularization when the pruning schedule ends.
    Typically, inducing block sparsity with 4x4 blocks in vanilla RNNs and GRUs works
    well, compared to larger block sizes. Larger blocks require lower sparsity to
    maintain similar accuracy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将组Lasso与块剪枝结合时，组Lasso引导选择需要剪枝的块。组Lasso正则化与剪枝计划同步应用，即当剪枝计划结束时，我们关闭正则化。通常，在普通RNNs和GRUs中使用4x4的块诱导块稀疏性效果较好，相比于更大的块大小。更大的块需要较低的稀疏性以维持相似的准确性。
- en: Unfortunately, it becomes challenging to maintain the same model accuracy when
    block sparsity is applied. Also, block sizes (i.e., pruning granularity) are application-sensitive,
    making it another hyper-parameter to tune. To avoid these problems, Cao et al. (Cao
    et al., [2019](#bib.bib10)) proposed a new method called Bank-Balanced Sparsity
    (BBS). BBS splits each weight matrix row into multiple equal-sized banks, and
    adopts fine-grained pruning to each bank independently to obtain identical sparsity
    among banks. Each bank has the same number of non-zero values. For example, retaining
    top two weights in each bank of size 4 implies a sparsity of 50%. We apply the
    BBS pruning method iteratively to a pre-trained network, and fine-tune the network
    after each pruning iteration to restore the model accuracy. BBS achieves almost
    the same model accuracy as unstructured sparsity and significantly outperforms
    block sparsity when pruning weights at the same sparsity level. BBS is also amenable
    to FPGA (Field Programmable Gate Arrays) acceleration because it inherently provides
    a balanced matrix partitioning for parallel computing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当应用块稀疏性时，保持相同的模型准确性变得具有挑战性。此外，块的大小（即剪枝粒度）对应用敏感，使其成为另一个需要调整的超参数。为了避免这些问题，曹等人（Cao
    et al.，[2019](#bib.bib10)）提出了一种新的方法，称为银行平衡稀疏性（BBS）。BBS 将每个权重矩阵的行分成多个相等大小的银行，并对每个银行进行细粒度剪枝，以在银行之间获得相同的稀疏性。每个银行具有相同数量的非零值。例如，在每个大小为
    4 的银行中保留前两个权重意味着 50% 的稀疏性。我们将 BBS 剪枝方法迭代地应用于预训练网络，并在每次剪枝迭代后微调网络以恢复模型准确性。BBS 实现了与非结构化稀疏性几乎相同的模型准确性，并且在相同稀疏性水平下显著优于块稀疏性。BBS
    还适用于 FPGA（现场可编程门阵列）加速，因为它本质上提供了平衡的矩阵分区以进行并行计算。
- en: 2.4\. Pruning Heads and Layers
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 剪枝头部和层
- en: Besides neurons and blocks, for Transformer based models, structured pruning
    can also be applied to attention heads and entire layers. In this section, we
    discuss such methods.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了神经元和块，对于基于 Transformer 的模型，结构化剪枝还可以应用于注意力头和整个层。在这一部分，我们讨论了这些方法。
- en: 2.4.1\. Pruning Attention Heads
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 剪枝注意力头
- en: 'BERT-base model consists of 12 layers each with 12 attention heads. Similarly,
    a typical NMT encoder-decoder Transformer with 6 layers each for encoder as well
    as decoder contains 16 attention heads per layer. Michel et al. (Michel et al.,
    [2019](#bib.bib80)) found that majority of attention heads can be removed without
    deviating too much from the original score. Surprisingly, in some cases removing
    an attention head (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of
    Deep Learning Models for Text: A Survey")(E)) results in an increase in accuracy.
    When these heads are removed individually, only 8 (out of 96) heads in 6-layer
    WMT NMT Transformer (16 heads/layer) cause a statistically significant change
    in performance when they are removed from the model, half of which actually result
    in a higher BLEU score. For most layers, one head is indeed sufficient at test
    time, even though the network was trained with 12 (BERT) or 16 (WMT Transformer)
    attention heads. One can also do iterative pruning of multiple heads (rather than
    just one at a time) across layers. For iterative pruning, head importance score
    is defined using the expected sensitivity of the model to the mask variables $\xi_{h}$
    as follows.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: BERT-base 模型包含 12 层，每层有 12 个注意力头。类似地，一个典型的 NMT 编码器-解码器 Transformer 具有 6 层编码器和
    6 层解码器，每层包含 16 个注意力头。Michel 等人（Michel et al.，[2019](#bib.bib80)）发现，大多数注意力头可以在不偏离原始评分太多的情况下被移除。令人惊讶的是，在某些情况下，移除一个注意力头（见图
    [2](#S2.F2 "图 2 ‣ 2\. 剪枝 ‣ 深度学习模型的压缩：综述")(E)）会导致准确性提高。当这些头部被单独移除时，只有 6 层 WMT NMT
    Transformer（每层 16 个头，共 96 个头）中的 8 个头部在从模型中移除时会引起统计上显著的性能变化，其中一半实际上会导致 BLEU 分数提高。对于大多数层，尽管网络是用
    12 个（BERT）或 16 个（WMT Transformer）注意力头训练的，但测试时实际上一个头部是足够的。也可以在多个层中进行迭代剪枝（而不仅仅是一次剪枝一个头）。对于迭代剪枝，头部重要性评分使用模型对掩码变量
    $\xi_{h}$ 的期望敏感性定义，如下所示。
- en: '| (18) |  | $I_{h}=E_{x\sim X}\left&#124;\frac{\partial L(x)}{\partial\xi_{h}}\right&#124;=E_{x\sim
    X}\left&#124;\text{Att}_{h}(x)^{T}\frac{\partial L(x)}{\partial\text{Att}_{h}(x)}\right&#124;$
    |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $I_{h}=E_{x\sim X}\left\lvert\frac{\partial L(x)}{\partial\xi_{h}}\right\rvert=E_{x\sim
    X}\left\lvert\text{Att}_{h}(x)^{T}\frac{\partial L(x)}{\partial\text{Att}_{h}(x)}\right\rvert$
    |  |'
- en: where $X$ is the data distribution, $L(x)$ is the loss on sample $x$, and $Att_{h}(x)$
    is the output of the attention head $h$ for instance $x$, . Intuitively, if $I_{h}$
    has a high value then changing $\xi_{h}$ is liable to have a large effect on the
    model. Hence, in every iteration heads with low $I_{h}$ values are pruned out.
    Michel et al. (Michel et al., [2019](#bib.bib80)) observed that pruning up to
    20% and 40% of heads from NMT and BERT models respectively, did not lead to any
    noticeable negative impact on accuracy.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$X$是数据分布，$L(x)$是样本$x$上的损失，$Att_{h}(x)$是实例$x$的注意力头$h$的输出。直观上，如果$I_{h}$的值很高，那么改变$\xi_{h}$可能会对模型产生较大的影响。因此，在每次迭代中，低$I_{h}$值的头部被剪枝。Michel等人（Michel等人，[2019](#bib.bib80)）观察到，从NMT和BERT模型中剪去最多20%和40%的头部分别没有对准确率产生明显的负面影响。
- en: 'Voita et al. (Voita et al., [2019](#bib.bib123)) used two other head importance
    scores to prune attention heads from the NMT model. The two scoring methods were:
    (1) Layer-wise relevance propagation (LRP) (Ding et al., [2017](#bib.bib27)).
    LRP is a method for computing the relative contribution of neurons at one point
    in a network to neurons at another. (2) “confidence” of a head which is computed
    as the average of its maximum attention weight excluding the end of sentence symbol,
    where the average is taken over tokens in a set of sentences used for evaluation.
    For pruning the heads, they propose a method based on stochastic gates and a differentiable
    relaxation of the $L_{0}$ penalty. $L_{0}$ norm equals the number of non-zero
    components and pushes the model to switch off less important heads. They find
    that only a small subset of heads are important for translation. On the English-Russian
    WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15
    BLEU.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Voita等人（Voita等人，[2019](#bib.bib123)）使用了另外两种头部重要性评分来剪枝NMT模型中的注意力头。两种评分方法是：（1）逐层相关传播（LRP）（Ding等人，[2017](#bib.bib27)）。LRP是一种计算网络中一个点的神经元对另一个点的神经元的相对贡献的方法。（2）计算为“头部的信心”，该信心为其最大注意力权重的平均值，排除句子结束符，其中平均值是对用于评估的一组句子中的令牌进行的。对于剪枝头部，他们提出了一种基于随机门和$L_{0}$惩罚的可微放松方法。$L_{0}$范数等于非零分量的数量，并推动模型关闭不重要的头部。他们发现只有一小部分头部对翻译很重要。在英语-俄语WMT数据集中，剪去48个编码器头中的38个，仅导致0.15
    BLEU的下降。
- en: 2.4.2\. Pruning Layers
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 剪枝层
- en: 'Note that dropping attention heads does not reduce runtime as they are usually
    computed in parallel. While one can prune weights, neurons or attention heads,
    how can we design a scheme to prune away layers (see Fig. [2](#S2.F2 "Figure 2
    ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(G))?
    The LayerDrop idea proposed in (Fan et al., [2019](#bib.bib28)) is inspired by
    DropConnect. DropConnect randomly drops weights while training on a batch. LayerDrop
    does structured dropout: it drops groups of weights, heads, feed forward network
    (FFN) matrices, or layers. The layers to be pruned can be decided using one of
    these ways:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，丢弃注意力头并不会减少运行时间，因为它们通常是并行计算的。虽然可以剪枝权重、神经元或注意力头，但我们如何设计一个方案来剪枝层（见图[2](#S2.F2
    "图 2 ‣ 2\. 剪枝 ‣ 文本深度学习模型压缩：综述")(G)）？LayerDrop思想（Fan等人，[2019](#bib.bib28)）的灵感来自DropConnect。DropConnect在批次训练时随机丢弃权重。LayerDrop进行结构化丢弃：它丢弃权重组、头部、前馈网络（FFN）矩阵或层。要剪枝的层可以使用以下这些方法之一来决定：
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Every Other: Prune every other layer (with rate $p$), e.g., every $3^{rd}$
    layer in a 12-layer BERT model.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每隔一个：剪枝每隔一个层（丢弃率$p$），例如，在一个12层的BERT模型中每隔$3^{rd}$层。
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Search on Validation: Search for a set of layers to be pruned by checking their
    impact on a validation set. This entails trying various combinations.'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在验证集上搜索：通过检查层对验证集的影响来搜索要剪枝的层集合。这涉及尝试各种组合。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Driven Pruning: Learn the drop rate $p_{d}$ of each layer in a data driven
    manner.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据驱动剪枝：以数据驱动的方式学习每层的丢弃率$p_{d}$。
- en: Given a target drop rate $p$, we learn an individual drop rate $p_{d}$ for the
    layer at depth $d$ such that the average rate over layers is equal to $p$. At
    inference time, we forward only the fixed top-$k$ highest scoring layers based
    on the softmax output. Across the three methods, “Every Other” strategy works
    surprisingly well across many tasks and configurations. “Search on Validation”
    and “Data Driven Pruning” only offer marginal gains.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个目标丢弃率$p$，我们为深度$d$处的层学习一个单独的丢弃率$p_{d}$，使得各层的平均丢弃率等于$p$。在推理时，我们仅根据softmax输出前向传递固定的top-$k$最高评分层。在这三种方法中，“每隔一个”策略在许多任务和配置中表现得非常好。“在验证集上搜索”和“数据驱动剪枝”仅提供了边际增益。
- en: 2.4.3\. Pruning General Structures
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 剪枝通用结构
- en: 'Lastly, Prasanna et al. (Prasanna et al., [2020](#bib.bib94)) experiment with
    pruning both the FFN layers (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression
    of Deep Learning Models for Text: A Survey")(F)) as well as attention heads (see
    Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models
    for Text: A Survey")(E)) in a BERT network. Just like (Michel et al., [2019](#bib.bib80)),
    they assign a mask variable to each of these structures. To decide which structures
    to prune, we look at the expected sensitivity of the model to the mask variables.
    High sensitivity implies large impact on the model output and hence corresponding
    structures should be retained. They find that it is possible to find a subnetwork
    of elements that achieves performance comparable with that of the full model,
    and similarly-sized subnetworks sampled from the rest of the model perform worse.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，Prasanna 等（Prasanna 等，[2020](#bib.bib94)）实验了在 BERT 网络中同时剪枝 FFN 层（见图 [2](#S2.F2
    "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(F)）以及注意力头（见图 [2](#S2.F2
    "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(E)）。与（Michel
    等，[2019](#bib.bib80)）类似，他们为这些结构分配了一个掩码变量。为了决定剪枝哪些结构，我们查看模型对掩码变量的预期敏感度。高敏感度意味着对模型输出的影响较大，因此应保留对应结构。他们发现可以找到一个子网络，其性能与完整模型相当，而从其余模型中抽取的相似大小子网络表现较差。'
- en: '| Task | Dataset | Model | Method | Size (Pruned; Orig) | Eval. (Pruned; Orig)
    | Metric |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 模型 | 方法 | 大小（剪枝; 原始） | 评估（剪枝; 原始） | 指标 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Language modeling | Europarl v7 English | 2-layer MLP | Prune Neurons (Murray
    and Chiang, [2015](#bib.bib86)) | 5.06M; 5.1M | 57; 100 | Perplexity (L) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Europarl v7 英文 | 2层 MLP | 剪枝神经元 （Murray 和 Chiang，[2015](#bib.bib86)）
    | 5.06M; 5.1M | 57; 100 | 困惑度 (L) |'
- en: '| Language modeling | PTB | 2-layer LSTM | Iter. Mag. (Zhu and Gupta, [2017](#bib.bib146))
    | 6.6M; 66M | 80.24; 78.45 | Perplexity (L) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | 2层 LSTM | Iter. Mag. （Zhu 和 Gupta，[2017](#bib.bib146)） | 6.6M;
    66M | 80.24; 78.45 | 困惑度 (L) |'
- en: '| Language modeling | PTB | LSTM | Iter. Mag. (Cao et al., [2019](#bib.bib10))
    | 20M; 66M | 78.5; 78.8 | Perplexity (L) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | Iter. Mag. （Cao 等，[2019](#bib.bib10)） | 20M; 66M | 78.5;
    78.8 | 困惑度 (L) |'
- en: '| Language modeling | PTB | LSTM | Block Sparsity (Cao et al., [2019](#bib.bib10))
    | 20M; 66M | 83; 78.8 | Perplexity (L) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | Block Sparsity （Cao 等，[2019](#bib.bib10)） | 20M; 66M
    | 83; 78.8 | 困惑度 (L) |'
- en: '| Language modeling | PTB | LSTM | BBS (Cao et al., [2019](#bib.bib10)) | 20M;
    66M | 78.5; 78.8 | Perplexity (L) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | BBS （Cao 等，[2019](#bib.bib10)） | 20M; 66M | 78.5; 78.8
    | 困惑度 (L) |'
- en: '| Language modeling | Wikitext-103 | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 19.5; 18.2 | Perplexity (L) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikitext-103 | Transformer | LayerDrop （Fan 等，[2019](#bib.bib28)）
    | 22M; 44M | 19.5; 18.2 | 困惑度 (L) |'
- en: '| Language modeling | AFP from English Gigaword | 2-layer MLP | Prune Neurons (Murray
    and Chiang, [2015](#bib.bib86)) | 5.07M; 5.1M | 107; 100 | Perplexity (L) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 英语 Gigaword 的 AFP | 2层 MLP | 剪枝神经元 （Murray 和 Chiang，[2015](#bib.bib86)）
    | 5.07M; 5.1M | 107; 100 | 困惑度 (L) |'
- en: '| Linguistic acceptability | CoLA | BERT-large | RPP/Iter. Mag. (Guo et al.,
    [2019a](#bib.bib33)) | 138M/170M; 340M | 82.8/76.3; 81.5 | Matthews (H) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 语言接受度 | CoLA | BERT-large | RPP/Iter. Mag. （Guo 等，[2019a](#bib.bib33)） |
    138M/170M; 340M | 82.8/76.3; 81.5 | Matthews (H) |'
- en: '| Machine reading comp. | MRPC | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 88.1/83.5; 89.3 | Acc (H) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读理解 | MRPC | BERT-large | RPP/Iter. Mag. （Guo 等，[2019a](#bib.bib33)） |
    138M/170M; 340M | 88.1/83.5; 89.3 | 准确率 (H) |'
- en: '| Machine reading comp. | MRPC | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 85.3; 88.9 | Acc (H) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读理解 | MRPC | BERT-base | LayerDrop （Fan 等，[2019](#bib.bib28)） | 66M; 110M
    | 85.3; 88.9 | 准确率 (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | Multi-layer LSTM | Mag. (See et al., [2016](#bib.bib104))
    | 43M; 216M | 20.91; 20.5 | BLEU (H) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT14 | 多层 LSTM | Mag. （See 等，[2016](#bib.bib104)）
    | 43M; 216M | 20.91; 20.5 | BLEU (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT16 | 4-layer LSTM | Iter. Mag. (Zhu and Gupta,
    [2017](#bib.bib146)) | 23M; 211M | 26.19; 26.77 | BLEU (H) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT16 | 4层 LSTM | Iter. Mag. （Zhu 和 Gupta，[2017](#bib.bib146)）
    | 23M; 211M | 26.19; 26.77 | BLEU (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT16 | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 29; 29 | BLEU (H) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT16 | Transformer | LayerDrop （Fan 等，[2019](#bib.bib28)）
    | 22M; 44M | 29; 29 | BLEU (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | Iter. Mag. (Cheong and Daniel,
    [2019](#bib.bib16)) | 22M; 44M | 26.4; 28.09 | BLEU (H) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | Iter. Mag. （Cheong 和 Daniel，[2019](#bib.bib16)）
    | 22M; 44M | 26.4; 28.09 | BLEU (H) |'
- en: '| Paraphrasing | QQP | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 91.2/85.1; 91.2 | Acc (H) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 释义 | QQP | BERT-large | RPP/Iter. Mag. (郭等，[2019a](#bib.bib33)) | 138M/170M;
    340M | 91.2/85.1; 91.2 | Acc (H) |'
- en: '| Question answering | SQuAD 1.1 | BERT-large | RPP/Iter. Mag. (Guo et al.,
    [2019a](#bib.bib33)) | 138M/170M; 340M | 90.23/85.3; 90.9 | Acc (H) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD 1.1 | BERT-large | RPP/Iter. Mag. (郭等，[2019a](#bib.bib33)) |
    138M/170M; 340M | 90.23/85.3; 90.9 | Acc (H) |'
- en: '| Question answering | SQuAD 2.0 | BERT-large | RPP/Iter. Mag. (Guo et al.,
    [2019a](#bib.bib33)) | 138M/170M; 340M | 81.3/75.3; 81.9 | Acc (H) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD 2.0 | BERT-large | RPP/Iter. Mag. (郭等，[2019a](#bib.bib33)) |
    138M/170M; 340M | 81.3/75.3; 81.9 | Acc (H) |'
- en: '| Sentiment analysis | SST-2 | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 92.4/91.3; 93.2 | Acc (H) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST-2 | BERT-large | RPP/Iter. Mag. (郭等，[2019a](#bib.bib33)) | 138M/170M;
    340M | 92.4/91.3; 93.2 | Acc (H) |'
- en: '| Sentiment analysis | SST-2 | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 92.5; 93.5 | Acc (H) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST-2 | BERT-base | LayerDrop (范等，[2019](#bib.bib28)) | 66M; 110M
    | 92.5; 93.5 | Acc (H) |'
- en: '| Speech recognition | 2100 hours English Speech | 2 CONV+7 BiRNNs+CTC | Iter.
    Mag. (Narang et al., [2017a](#bib.bib87)) | 11.1M; 67M | 10.59; 10.67 | CER (L)
    on dev |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英语语音 | 2 CONV+7 BiRNNs+CTC | Iter. Mag. (纳朗等，[2017a](#bib.bib87))
    | 11.1M; 67M | 10.59; 10.67 | CER (L) on dev |'
- en: '| Speech recognition | 2100 hours English Speech | 2 CONV+7 BiGRUs+CTC | Iter.
    Mag. (Narang et al., [2017a](#bib.bib87)) | 17.8M; 115M | 9.76; 9.55 | CER (L)
    on dev |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英语语音 | 2 CONV+7 BiGRUs+CTC | Iter. Mag. (纳朗等，[2017a](#bib.bib87))
    | 17.8M; 115M | 9.76; 9.55 | CER (L) on dev |'
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+7 BiRNNs+CTC | Block
    Sparsity (Narang et al., [2017b](#bib.bib88)) | 25.8M; 67M | 15.66; 15.36 | CER
    (L) on test |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英语语音 | 2 CONV+7 BiRNNs+CTC | Block Sparsity (纳朗等，[2017b](#bib.bib88))
    | 25.8M; 67M | 15.66; 15.36 | CER (L) on test |'
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+7 BiRNNs+CTC | Block
    Sparsity+Group Lasso (Narang et al., [2017b](#bib.bib88)) | 12.9M; 67M | 15.89;
    15.36 | CER (L) on test |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英语语音 | 2 CONV+7 BiRNNs+CTC | Block Sparsity+Group Lasso (纳朗等，[2017b](#bib.bib88))
    | 12.9M; 67M | 15.89; 15.36 | CER (L) on test |'
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+3 BiGRUs+CTC | Block
    Sparsity (Narang et al., [2017b](#bib.bib88)) | 25.6M; 115M | 16.23; 15.42 | CER
    (L) on test |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英语语音 | 2 CONV+3 BiGRUs+CTC | Block Sparsity (纳朗等，[2017b](#bib.bib88))
    | 25.6M; 115M | 16.23; 15.42 | CER (L) on test |'
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+3 BiGRUs+CTC | Block
    Sparsity+Group Lasso (Narang et al., [2017b](#bib.bib88)) | 10.8M; 115M | 16.78;
    15.42 | CER (L) on test |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英语语音 | 2 CONV+3 BiGRUs+CTC | Block Sparsity+Group Lasso (纳朗等，[2017b](#bib.bib88))
    | 10.8M; 115M | 16.78; 15.42 | CER (L) on test |'
- en: '| Speech recognition | AN4 | 2 CONV+3 HLSTMs+CTC | Grow and Prune (Dai et al.,
    [2018](#bib.bib20)) | 2.6M; 44.72M | 10.37; 8.92 | WER (L) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | AN4 | 2 CONV+3 HLSTMs+CTC | Grow and Prune (戴等，[2018](#bib.bib20))
    | 2.6M; 44.72M | 10.37; 8.92 | WER (L) |'
- en: '| Speech recognition | Switchboard (swb/fsh) | 7-layer MLP | Prune Neurons (He
    et al., [2014](#bib.bib42)) | 12.2M; 32.19M | 25.5/28.8; 25.7/28.8 | WER (L) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | Switchboard (swb/fsh) | 7层MLP | Prune Neurons (何等，[2014](#bib.bib42))
    | 12.2M; 32.19M | 25.5/28.8; 25.7/28.8 | WER (L) |'
- en: '| Speech recognition | TIMIT | 5-layer MLP | Prune Neurons (He et al., [2014](#bib.bib42))
    | 3.5M; 5.76M | 20.7; 20.79 | PER (L) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | TIMIT | 5层MLP | Prune Neurons (何等，[2014](#bib.bib42)) | 3.5M; 5.76M
    | 20.7; 20.79 | PER (L) |'
- en: '| Speech recognition | TIMIT | LSTM | Iter. Mag. (Cao et al., [2019](#bib.bib10))
    | 0.32M; 3.2M | 23.5; 23.5 | PER (L) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | TIMIT | LSTM | Iter. Mag. (曹等，[2019](#bib.bib10)) | 0.32M; 3.2M |
    23.5; 23.5 | PER (L) |'
- en: '| Speech recognition | TIMIT | LSTM | Block Sparsity (Cao et al., [2019](#bib.bib10))
    | 0.32M; 3.2M | 26.5; 23.5 | PER (L) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | TIMIT | LSTM | Block Sparsity (曹等，[2019](#bib.bib10)) | 0.32M; 3.2M
    | 26.5; 23.5 | PER (L) |'
- en: '| Speech recognition | TIMIT | LSTM | BBS (Cao et al., [2019](#bib.bib10))
    | 0.32M; 3.2M | 23.5; 23.5 | PER (L) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | TIMIT | LSTM | BBS (曹等，[2019](#bib.bib10)) | 0.32M; 3.2M | 23.5; 23.5
    | PER (L) |'
- en: '| Speech recognition | WSJ 92 | 1 CONV+3 FC+1 BiRNN | DSD (Han et al., [2016b](#bib.bib38))
    | 4.07M; 8.14M | 27.9; 27.45 | WER (L) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | WSJ 92 | 1 CONV+3 FC+1 BiRNN | DSD (韩等，[2016b](#bib.bib38)) | 4.07M;
    8.14M | 27.9; 27.45 | WER (L) |'
- en: '| Speech recognition | WSJ 92 | 2 CONV+7 BiRNNs+CTC | DSD (Han et al., [2016b](#bib.bib38))
    | 33.5M; 67M | 10.65; 9.02 | WER (L) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | WSJ 92 | 2 CONV+7 BiRNNs+CTC | DSD (韩等，[2016b](#bib.bib38)) | 33.5M;
    67M | 10.65; 9.02 | WER (L) |'
- en: '| Speech recognition | WSJ 93 | 1 CONV+3 FC+1 BiRNN | DSD (Han et al., [2016b](#bib.bib38))
    | 4.07M; 8.14M | 32.99; 31.6 | WER (L) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | WSJ 93 | 1 CONV+3 FC+1 BiRNN | DSD (韩等，[2016b](#bib.bib38)) | 4.07M;
    8.14M | 32.99; 31.6 | WER (L) |'
- en: '| Speech recognition | WSJ 93 | 2 CONV+7 BiRNNs+CTC | DSD (Han et al., [2016b](#bib.bib38))
    | 33.5M; 67M | 14.84; 13.44 | WER (L) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | WSJ 93 | 2 CONV+7 BiRNNs+CTC | DSD (Han et al., [2016b](#bib.bib38))
    | 33.5M; 67M | 14.84; 13.44 | WER (L) |'
- en: '| Summarization | CNN-Dailymail | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 39; 40 | ROUGE (H) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 摘要生成 | CNN-Dailymail | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 39; 40 | ROUGE (H) |'
- en: '| Textual entailment | MNLI | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 86.1/77; 86.1 | Acc (H) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 86.1/77; 86.1 | 准确率 (H) |'
- en: '| Textual entailment | MNLI-m | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 85.7/82.5; 85.9 | Acc (H) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI-m | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 85.7/82.5; 85.9 | 准确率 (H) |'
- en: '| Textual entailment | MNLI-m | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 82.9; 84.6 | Acc (H) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI-m | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28)) |
    66M; 110M | 82.9; 84.6 | 准确率 (H) |'
- en: '| Textual entailment | QNLI | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 92.3/90.2; 91.3 | Acc (H) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | QNLI | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 92.3/90.2; 91.3 | 准确率 (H) |'
- en: '| Textual entailment | QNLI | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 89.4; 90.5 | Acc (H) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | QNLI | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28)) | 66M;
    110M | 89.4; 90.5 | 准确率 (H) |'
- en: '| Textual entailment | RTE | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 70.1/68.6; 70.1 | Acc (H) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | RTE | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 70.1/68.6; 70.1 | 准确率 (H) |'
- en: Table 1\. Comparison of various pruning methods (sorted by Task and then Dataset).
    CONV=Convolution. CTC=Connectionist temporal classification. FC=Fully connected.
    HLSTM=hidden-layer LSTM (Dai et al., [2018](#bib.bib20)). In the metric column,
    H means high is better while L means low is better. PER/CER/WER=Phone/Character/Word
    error rate. For (Murray and Chiang, [2015](#bib.bib86)), embedding weights have
    not been considered when computing model size in the table. Block sparsity methods
    use block size of 4x4\. BBS uses 64 banks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 各种剪枝方法的比较（按任务和数据集排序）。CONV=卷积。CTC=连接时序分类。FC=全连接。HLSTM=隐藏层LSTM (Dai et al.,
    [2018](#bib.bib20))。在指标列中，H表示值越高越好，而L表示值越低越好。PER/CER/WER=电话/字符/词误差率。对于 (Murray
    and Chiang, [2015](#bib.bib86))，在计算表中模型大小时未考虑嵌入权重。块稀疏方法使用4x4的块大小。BBS使用64个库。
- en: 2.5\. Summary
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 总结
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2.4.3\. Pruning General Structures ‣ 2.4\. Pruning
    Heads and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text:
    A Survey") compares various pruning methods across different tasks and datasets.
    Size and accuracy of both the original and the pruned model are shown. The papers
    report multiple (model size, model accuracy) pairs but we carefully chose the
    pair such that accuracy is typically within 10% of the original or the best pruned
    model accuracy reported. For language modeling, most popular datasets include
    PTB, Europarl v7 English, Wikitext-103 and AFP from English Gigaword. On PTB using
    LSTMs, we observe that Bank-Balanced Sparsity method (Cao et al., [2019](#bib.bib10))
    leads to lowest perplexity. For the Linguistic Acceptability CoLA task, RPP (Guo
    et al., [2019a](#bib.bib33)) resulted into a smaller and a more accurate model
    compared to iterative magnitude pruning. As expected in some senses, the pruned
    model provides better accuracy than the unpruned one since pruning causes regularization.
    For the machine reading comprehension, question answering and paraphrasing tasks
    also, RPP seems to work better than iterative magnitude pruning. For the machine
    translation (NMT) task, on English-German WMT datasets, pruned Transformer models
    provide better accuracy than pruned LSTM with comparable number of parameters.
    For the sentiment analysis task, on SST-2, although RPP leads to a better pruned
    model compared to iterative pruning, LayerDrop (Fan et al., [2019](#bib.bib28))
    improves further on it with a model less than half of the RPP-pruned model. For
    the speech recognition task, experiments have been reported on 2100 hours English
    speech data, TIMIT, WSJ, Switchboard and AN4 datasets. On 2100 hours English speech
    data, Block Sparsity+Group Lasso is better than Block sparsity without regularization.
    Also, it is better than plain iterative magnitude pruning. On TIMIT, block sparsity (Narang
    et al., [2017b](#bib.bib88)) leads to a more accurate 90% pruned LSTM model compared
    to the unpruned one. For the summarization task, LayerDrop (Fan et al., [2019](#bib.bib28))
    can compress the model to half without any noticeable accuracy change. Finally,
    for the textual entailment task, experiments have been done on GLUE (Wang et al.,
    [2019b](#bib.bib126)) datasets: MNLI, MNLI-m, QNLI and RTE. Models pruned from
    BERT-large perform better than models pruned from BERT-base; RPP performs better
    than iterative magnitude pruning.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S2.T1 "表格 1 ‣ 2.4.3\. 修剪通用结构 ‣ 2.4\. 修剪头部和层 ‣ 2\. 修剪 ‣ 文本深度学习模型的压缩：综述")
    比较了在不同任务和数据集上各种修剪方法。展示了原始模型和修剪模型的大小和准确性。论文报告了多个（模型大小，模型准确性）对，但我们仔细选择了一个对，使得准确性通常在原始模型或报告的最佳修剪模型准确性的
    10% 以内。对于语言建模，大多数流行的数据集包括 PTB、Europarl v7 英文、Wikitext-103 和来自 English Gigaword
    的 AFP。在使用 LSTM 的 PTB 上，我们观察到 Bank-Balanced Sparsity 方法（Cao 等，[2019](#bib.bib10)）导致最低的困惑度。对于语言接受性
    CoLA 任务，RPP（Guo 等，[2019a](#bib.bib33)）相比于迭代幅度修剪，得到了一个更小且更准确的模型。正如预期的那样，修剪模型提供了比未修剪模型更好的准确性，因为修剪引起了正则化。对于机器阅读理解、问答和释义任务，RPP
    似乎也比迭代幅度修剪效果更好。对于机器翻译（NMT）任务，在英德 WMT 数据集上，修剪后的 Transformer 模型比修剪后的 LSTM 在参数数量相当的情况下提供了更好的准确性。对于情感分析任务，在
    SST-2 上，虽然 RPP 相比于迭代修剪得到了更好的修剪模型，但 LayerDrop（Fan 等，[2019](#bib.bib28)）进一步改进了它，模型大小不到
    RPP 修剪模型的一半。对于语音识别任务，已经在 2100 小时的英文语音数据、TIMIT、WSJ、Switchboard 和 AN4 数据集上进行了实验。在
    2100 小时的英文语音数据上，Block Sparsity+Group Lasso 比没有正则化的 Block sparsity 更好。此外，它也比普通的迭代幅度修剪更好。在
    TIMIT 上，block sparsity（Narang 等，[2017b](#bib.bib88)）相比于未修剪模型，导致了一个更准确的 90% 修剪
    LSTM 模型。对于总结任务，LayerDrop（Fan 等，[2019](#bib.bib28)）可以将模型压缩到原来的一半，而没有明显的准确性变化。最后，对于文本蕴涵任务，已经在
    GLUE（Wang 等，[2019b](#bib.bib126)）数据集：MNLI、MNLI-m、QNLI 和 RTE 上进行了实验。从 BERT-large
    修剪的模型比从 BERT-base 修剪的模型表现更好；RPP 比迭代幅度修剪表现更好。
- en: 'While older methods (LeCun et al., [1990](#bib.bib63); Hassibi and Stork, [1993](#bib.bib40))
    claimed that Hessian based methods were more effective than magnitude based pruning,
    almost all recent methods have been based on magnitude based pruning. See et al. (See
    et al., [2016](#bib.bib104)) proposed three pruning schemes. They make the following
    observations: (1) Class-blind pruning outperforms both other schemes. Further,
    the overall performance loss is caused disproportionately by a few classes: softmax
    weights, source and target embedding weights. (2) It seems that higher layers
    are more important than lower layers, and that attention and softmax weights are
    crucial in LSTMs. (3) After retraining the pruned NMT models, baseline performance
    (20.48 BLEU) is both recovered and improved upon, up to 80% pruning (20.91 BLEU),
    with only a small performance loss at 90% pruning (20.13 BLEU). (4) In LSTMs,
    the parameters corresponding to the less common words are more dispensable. Weights
    connecting to the input are most crucial, followed by the input gate, then the
    output gate, then the forget gate. This is particularly true of the lower layers,
    which focus primarily on the input. However for higher layers, especially on the
    target side, weights connecting to the gates are as important as those connecting
    to the input.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然较早的方法（LeCun et al., [1990](#bib.bib63); Hassibi and Stork, [1993](#bib.bib40)）声称基于
    Hessian 的方法比基于大小的修剪更有效，但几乎所有最近的方法都基于大小修剪。See 等人（See et al., [2016](#bib.bib104)）提出了三种修剪方案。他们做出了以下观察：（1）类盲修剪优于其他两种方案。此外，整体性能损失主要由少数几个类别造成：softmax
    权重、源嵌入和目标嵌入权重。（2）似乎高层比低层更重要，注意力和 softmax 权重在 LSTMs 中至关重要。（3）在对修剪后的 NMT 模型重新训练后，基准性能（20.48
    BLEU）得到了恢复和改进，达到 80% 修剪（20.91 BLEU），在 90% 修剪（20.13 BLEU）时仅有小幅性能损失。（4）在 LSTMs 中，对不常见单词的参数更为可有可无。与输入连接的权重最为关键，其次是输入门，然后是输出门，再然后是遗忘门。这在主要关注输入的低层特别如此。然而对于高层，特别是在目标端，与门连接的权重与与输入连接的权重同样重要。
- en: Narang et al. (Narang et al., [2017a](#bib.bib87)) observe that for approximately
    same number of parameters, gradual/iterative pruning is 7% to 9% better than hard
    pruning. They also conclude that the initial layers are pruned more aggressively
    compared to the final layers. Zhu et al. (Zhu and Gupta, [2017](#bib.bib146))
    advise that in order to get the best-performing sparse model of a certain size,
    we should train a dense model that is 5x-10x larger and then prune to the desired
    number of parameters rather than taking the largest and best-performing dense
    model and pruning this model by 20x or more to the desired number of parameters.
    Guo et al. (Guo et al., [2019a](#bib.bib33)) find that RPP is much better than
    typical iterative pruning. In their experiments with BERT they find that for both
    original BERT and BERT pruned with RPP, the low-dimensional manifolds of the language
    representations are similar, showing the similar projection. This implies that
    the BERT applied with RPP keeps most of the language representation information
    similar to that from the original BERT.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Narang 等人（Narang et al., [2017a](#bib.bib87)）观察到，在参数数量大致相同的情况下，渐进/迭代修剪比硬修剪效果好
    7% 到 9%。他们还得出结论，初始层的修剪比最终层更为激进。Zhu 等人（Zhu and Gupta, [2017](#bib.bib146)）建议，为了获得最佳性能的稀疏模型，我们应训练一个大
    5 倍到 10 倍的密集模型，然后修剪到所需的参数数量，而不是直接修剪最大且性能最佳的密集模型，以达到所需的参数数量。Guo 等人（Guo et al.,
    [2019a](#bib.bib33)）发现 RPP 比典型的迭代修剪效果好得多。他们在 BERT 上的实验表明，无论是原始 BERT 还是用 RPP 修剪后的
    BERT，语言表示的低维流形是相似的，显示出类似的投影。这意味着应用 RPP 的 BERT 保留了大部分与原始 BERT 相似的语言表示信息。
- en: 'For block pruning, Narang et al. (Narang et al., [2017b](#bib.bib88)) make
    the following observations: (1) We can create block-sparse RNNs with sparsity
    ranging from 80% to 90% with small loss in accuracy. This allows us to reduce
    the model size by roughly 10×. Block sparsity works with a variety of block sizes
    up to 32×32\. (2) For block size 4×4, models with sparsity greater 90% yield a
    relative accuracy loss of 30% or higher. Similarly, for blocks of 16×16, models
    with sparsity greater than 86% have 30% or more accuracy loss. A similar trend
    is observed for block size 32×32\. This indicates that there is a tradeoff between
    sparsity, block size and accuracy of the model. (3) For both block pruning and
    weight pruning, we see that the initial layers are pruned more aggressively compared
    to the final layers. Increasing sparsity in the layers closer to the output results
    in poor accuracy. Additionally, the variance in sparsity across the layers increases
    with the block size. Further, Cao et al. (Cao et al., [2019](#bib.bib10)) make
    the following observations comparing block sparsity with BBS: (1) BBS achieves
    almost the same model accuracy regardless of the change of bank size. For block
    sparsity, however, increasing the block size adversely affects model accuracy.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于块剪枝，Narang 等人（Narang 等人，[2017b](#bib.bib88)）做出了以下观察：（1）我们可以创建块稀疏的 RNN，其稀疏度范围从
    80% 到 90%，并且准确度损失较小。这使得我们可以将模型大小减少大约 10×。块稀疏适用于多种块大小，最大可达 32×32。 （2）对于块大小 4×4，稀疏度大于
    90% 的模型会导致 30% 或更高的相对准确度损失。类似地，对于 16×16 的块，稀疏度大于 86% 的模型也会有 30% 或更多的准确度损失。对于块大小
    32×32 也观察到类似的趋势。这表明稀疏度、块大小和模型准确度之间存在权衡。（3）无论是块剪枝还是权重剪枝，我们发现相对于最终层，初始层的剪枝更加激进。在接近输出的层中增加稀疏度会导致准确度下降。此外，随着块大小的增加，各层间稀疏度的方差也会增加。此外，Cao
    等人（Cao 等人，[2019](#bib.bib10)）在将块稀疏与 BBS 进行比较时做出了以下观察：（1）无论银行大小的变化如何，BBS 几乎可以实现相同的模型准确度。然而，对于块稀疏，增加块大小会对模型准确度产生不利影响。
- en: For pruning of attention heads, Michel et al. (Michel et al., [2019](#bib.bib80))
    observe that one can prune up to 20% and 40% of heads from 6-layer NMT Transformer
    and BERT resp., without incurring any noticeable negative impact. When trying
    to remove a head at a time, only 8 (out of 96) heads in 6-layer NMT Transformer
    (16 heads/layer) cause a statistically significant change in performance when
    they are removed from the model, half of which actually result in a higher BLEU
    score. Further Voita et al. (Voita et al., [2019](#bib.bib123)) find that on the
    English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop
    of only 0.15 BLEU.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于注意力头的剪枝，Michel 等人（Michel 等人，[2019](#bib.bib80)）观察到，从 6 层 NMT Transformer 和
    BERT 中剪除 20% 和 40% 的头部不会带来明显的负面影响。当尝试一次移除一个头部时，在 6 层 NMT Transformer（每层 16 个头部）中，只有
    8 个（总共 96 个）头部的移除会对模型性能产生统计学上显著的变化，其中一半实际上会导致更高的 BLEU 分数。此外，Voita 等人（Voita 等人，[2019](#bib.bib123)）发现，在英语-俄语
    WMT 数据集上，剪除 48 个编码器头部中的 38 个只会导致 0.15 的 BLEU 分数下降。
- en: Overall, to summarize, pruning has been the most popular method for model compression.
    Pruning methods can be unstructured (prune weights) or structured (prune neurons,
    blocks, attention heads, layers). While weight pruning theoretically leads to
    pruning to a large extent, practical implementation of sparse data structures
    is difficult. Pruning and regularization need to be done together carefully. Also,
    it is critical to define the importance functions for various structures carefully.
    Among weight pruning methods, while iterative magnitude pruning with regularization
    works well for RNNs and LSTMs, RPP performs better for Transformer based models.
    Pruning blocks using BBS is better than pruning neurons. For Transformer models,
    pruning just the heads do not provide much model compression, but dropping a combination
    of attention heads and layers is better.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，总结起来，剪枝一直是模型压缩最流行的方法。剪枝方法可以是无结构的（剪枝权重）或结构化的（剪枝神经元、块、注意力头、层）。虽然权重剪枝理论上可以大范围剪枝，但稀疏数据结构的实际实现是困难的。剪枝和正则化需要一起谨慎进行。此外，准确地定义各种结构的重要性函数至关重要。在权重剪枝方法中，虽然带有正则化的迭代幅度剪枝对
    RNN 和 LSTM 有效，但 RPP 对基于 Transformer 的模型表现更好。使用 BBS 剪枝块优于剪枝神经元。对于 Transformer 模型，仅剪枝头部不会提供太多的模型压缩，但丢弃注意力头和层的组合效果更好。
- en: 3\. Quantization
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 量化
- en: 'While pruning saves on the model size by removing weights, quantization aims
    to reduce the number of bits needed to store weights. Most computer architectures
    use 32 bits to represent weights. However, estimated precision of the brain (hippocampal
    spine) synapses is around 4.6 bits (Bartol et al., [2015](#bib.bib6)). Empirical
    evidence suggests that most quantities in the nervous system (for instance, firing
    of the neurons) have variability of a few percent due to biological noise, or
    a precision of 1 in 100 at best (Linden, [2018](#bib.bib71)). Thus, each decision
    could depend on $\log_{2}(100)$=6.64 bits. Thus, we should be able to store weights
    in our artificial neural networks on average in a space of 4–7 bits. Given this
    motivation, various methods have been proposed which perform 1-bit (or binary
    quantization), ternary quantization, and general quantization exploring the spectrum
    between 3 and 32 bits. We discuss such methods in this section. Fig. [3](#S3.F3
    "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models for Text: A
    Survey") provides a broad overview of various quantization styles.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然剪枝通过移除权重来节省模型大小，但量化的目标是减少存储权重所需的位数。大多数计算机架构使用 32 位来表示权重。然而，大脑（海马脊突）的估计精度约为
    4.6 位（Bartol et al., [2015](#bib.bib6)）。实证证据表明，神经系统中的大多数量（例如，神经元的放电）由于生物噪声具有几个百分点的变异性，或者最好有
    1/100 的精度（Linden, [2018](#bib.bib71)）。因此，每个决策可能依赖于 $\log_{2}(100)$=6.64 位。因此，我们应该能够在我们的人工神经网络中平均以
    4–7 位的空间存储权重。基于这一动机，提出了各种方法来执行 1 位（或二值化）、三值化以及在 3 位和 32 位之间探索的通用量化方法。我们在本节中讨论这些方法。图
    [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey") 提供了各种量化风格的广泛概述。'
- en: '![Refer to caption](img/0bad909a793e8a1f59531e71a8b3bb59.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0bad909a793e8a1f59531e71a8b3bb59.png)'
- en: 'Figure 3\. Different Types of Quantization: Binary (A), Ternary (B) and General
    Quantized (C and D). Note that X axis denotes the weight value while the Y axis
    denotes frequency.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 不同类型的量化：二值化（A）、三值化（B）和一般量化（C 和 D）。注意，X 轴表示权重值，而 Y 轴表示频率。
- en: 3.1\. Binarized Networks
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 二值化网络
- en: 'Binarized networks use binary quantization (see Fig. [3](#S3.F3 "Figure 3 ‣
    3\. Quantization ‣ Compression of Deep Learning Models for Text: A Survey")(A)),
    which quantizes weights using 1 bit. Quantizing weights to 1 bit provides a compression
    of 32x but leads to a significant drop in accuracy across many tasks. However,
    in a hybrid quantization scheme, such binary quantization can be very helpful
    for some layers in a network. Binarization can be done using deterministic methods
    or could be stochastic in nature. Also, while naïve binarization has a very simple
    way of fixing the binary boundary threshold, one could perform a complex loss
    aware binarization as well. We discuss these variants of binarization in this
    section.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '二值化网络使用二值量化（见图 [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep
    Learning Models for Text: A Survey")(A)），它使用 1 位来量化权重。将权重量化为 1 位可以实现 32 倍的压缩，但在许多任务中会导致准确率显著下降。然而，在混合量化方案中，这种二值化可以对网络中的某些层非常有用。二值化可以使用确定性方法完成，也可以是随机的。此外，虽然简单的二值化有一种非常简单的方法来修正二值边界阈值，但也可以执行复杂的损失感知二值化。我们在本节中讨论这些二值化的变体。'
- en: 3.1.1\. Deterministic Binarization
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 确定性二值化
- en: Simplest way of binary quantization is to set the weight as 1 for non-negative
    weights, and to -1 for negative weights. This leads to 32x compression. Also,
    the matrix multiplication for binary matrices is $\sim$7x faster (Hubara et al.,
    [2017](#bib.bib48)) leading to faster model inference. In the forward pass, binary
    networks drastically reduce memory size and accesses, and replace most arithmetic
    operations with bit-wise operations, which leads to great increases of power efficiency.
    Also, in the simplest version, binarization can be performed in a static manner,
    i.e., after the training is done. However, this method leads to large loss in
    accuracy.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的二值化方法是将非负权重设置为 1，将负权重设置为 -1。这会导致 32 倍的压缩。此外，二值矩阵的矩阵乘法比一般矩阵快 $\sim$7 倍（Hubara
    et al., [2017](#bib.bib48)），从而加快了模型推理速度。在前向传播中，二值网络显著减少了内存大小和访问次数，并将大多数算术操作替换为按位操作，从而显著提高了功耗效率。此外，在最简单的版本中，二值化可以以静态方式进行，即在训练完成后。然而，这种方法会导致准确率的大幅下降。
- en: A variant of this simple method is to set the weight to a constant $c_{1}$ for
    non-negative weights, and to another constant $c_{2}$ for negative weights. Binary
    Scheme (BS)-Fixed method (Lam, [2018](#bib.bib60)) stores the original weights
    and during the forward pass replaces the values with a masked value of $c_{1}$
    or $c_{2}$, where $c_{1}$ and $c_{2}$ are fixed and chosen with hyperparameter
    tuning. Full precision weights are used during training. At the end of training,
    the weights are replaced with the index of its masked value. Choosing the values
    of $c_{1}$ and $c_{2}$ can be difficult and time-consuming in BS-Fixed. Thus,
    in the BS-flexible method (Cheong and Daniel, [2019](#bib.bib16)), we initialize
    $c_{1}$ and $c_{2}$ using KMeans with two centroids over the weights, and then
    update $c_{1}$ and $c_{2}$ using back-propagation. Also, in the BS-Flexible method,
    weights are quantized as follows.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单方法的一种变体是将非负权重的权重设置为常量 $c_{1}$，将负权重的权重设置为另一个常量 $c_{2}$。二进制方案（BS）-固定方法（Lam，[2018](#bib.bib60)）存储原始权重，并在前向传递期间用掩码值
    $c_{1}$ 或 $c_{2}$ 替换这些值，其中 $c_{1}$ 和 $c_{2}$ 是固定的，并通过超参数调整选择。在训练期间使用全精度权重。在训练结束时，权重被替换为其掩码值的索引。在
    BS-Fixed 方法中，选择 $c_{1}$ 和 $c_{2}$ 的值可能困难且耗时。因此，在 BS-Flexible 方法（Cheong 和 Daniel，[2019](#bib.bib16)）中，我们使用
    KMeans 初始化 $c_{1}$ 和 $c_{2}$，通过对权重进行两个质心的聚类，然后使用反向传播更新 $c_{1}$ 和 $c_{2}$。此外，在
    BS-Flexible 方法中，权重按如下方式量化。
- en: '| (19) |  | $w_{b}=\begin{cases}c_{1}&amp;\text{if }w\geq(c_{1}+c_{2})/2\\
    c_{2}&amp;\text{if }w<(c_{1}+c_{2})/2\end{cases}$ |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $w_{b}=\begin{cases}c_{1}&\text{如果 }w\geq(c_{1}+c_{2})/2\\ c_{2}&\text{如果
    }w<(c_{1}+c_{2})/2\end{cases}$ |  |'
- en: Note that $w$ is the original weight value while $w_{b}$ is the binarized weight
    value. These changes eliminate the need for hyper-parameter tuning.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，$w$ 是原始权重值，而 $w_{b}$ 是二值化的权重值。这些更改消除了对超参数调整的需求。
- en: 3.1.2\. Stochastic Binarization
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 随机二值化
- en: Stochastic (Courbariaux et al., [2015](#bib.bib18)) binarization is performed
    as follows.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 随机（Courbariaux 等，[2015](#bib.bib18)）二值化如下进行。
- en: '| (20) |  | $w_{b}=\begin{cases}+1&amp;\text{with probability }p=\sigma(w)\\
    -1&amp;\text{with probability }1-p\end{cases}$ |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $w_{b}=\begin{cases}+1&\text{当概率为 }p=\sigma(w)\\ -1&\text{当概率为
    }1-p\end{cases}$ |  |'
- en: where
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里
- en: '| (21) |  | $\displaystyle\sigma(w)=\text{clip}\left(\frac{w+1}{2},0,1\right)=\max\left(0,\min\left(1,\frac{w+1}{2}\right)\right)$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\displaystyle\sigma(w)=\text{clip}\left(\frac{w+1}{2},0,1\right)=\max\left(0,\min\left(1,\frac{w+1}{2}\right)\right)$
    |  |'
- en: 'We only binarize the weights during the forward and backward propagations but
    not during the parameter update. Keeping good precision weights during the updates
    is necessary for Stochastic Gradient Descent (SGD). This is possible using something
    called as “Straight Through Estimator (STE) trick” (Bengio et al., [2013](#bib.bib8)).
    As per STE, as the quantized value is an approximation of the original value,
    we can substitute the gradient with respect to the quantized value for the gradient
    of original value. The trick allows the inclusion of quantization into the computation
    graph of back-propagation and allows QNNs to represent parameters, activations
    and gradients with low bitwidth numbers. For test-time inference, there are three
    options using such a quantization method:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在前向和反向传播期间对权重进行二值化，而在参数更新期间不进行二值化。保持更新期间的良好精度权重对于随机梯度下降（SGD）是必要的。这可以通过一种称为“直通估计器（STE）技巧”（Bengio
    等，[2013](#bib.bib8)）来实现。根据 STE，由于量化值是原始值的近似值，我们可以用量化值的梯度替代原始值的梯度。这一技巧允许将量化纳入反向传播的计算图中，并允许
    QNN 使用低位宽数值表示参数、激活和梯度。在测试时推理中，使用这种量化方法有三种选择：
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Use the resulting binary weights $w_{b}$ (this makes most sense with the deterministic
    binarization).
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用得到的二进制权重 $w_{b}$（这在确定性二值化中最有意义）。
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the stochastic case, many different networks can be sampled by sampling a
    $w_{b}$ for each weight. The ensemble output of these networks can then be obtained
    by averaging the outputs from individual networks.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在随机情况下，可以通过为每个权重采样 $w_{b}$ 来采样许多不同的网络。然后可以通过对单个网络的输出进行平均来获得这些网络的集成输出。
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Use original weights. But this does not reduce model size.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用原始权重。但这不会减少模型的大小。
- en: Besides this, there have been further efforts that make train/test faster but
    do not reduce model size. For example, Lin et al. (Lin et al., [2015](#bib.bib70))
    convert multiplications in the backward pass into bit-shifts by restricting the
    activations to be power-of-two integers. Hubara et al. (Hubara et al., [2016](#bib.bib47))
    binarize weights and activations, at the inference phase and the entire training
    phase of a deep network.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还有进一步的努力旨在加快训练/测试速度，但不减少模型大小。例如，Lin 等人 (Lin et al., [2015](#bib.bib70))
    通过将激活限制为二的整数幂，将反向传播中的乘法操作转换为位移操作。Hubara 等人 (Hubara et al., [2016](#bib.bib47))
    在深度网络的推理阶段和整个训练阶段对权重和激活进行二值化。
- en: 3.1.3\. Loss Aware Binarization (LAB)
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 损失感知二值化（LAB）
- en: The naïve binary quantization methods divide the real number line into two parts
    and each part was mapped to a quantized weight value. Can we decide per weight
    value which of the two weights it should be quantized to? Thus the idea behind
    Binary Weight Networks (BWN) (Rastegari et al., [2016](#bib.bib98)) is to approximate
    the weight vector $W\in R^{n}$ using a binary vector $B\in\{+1,-1\}^{n}$ and a
    scaling factor $\alpha\in R^{+}$ such that $W\approx\alpha B$. This can be expressed
    as an optimization problem as follows.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 幼稚的二值量化方法将实数线分为两部分，每部分被映射到一个量化的权重值。我们能否为每个权重值决定它应该被量化为哪一个权重？因此，Binary Weight
    Networks（BWN） (Rastegari et al., [2016](#bib.bib98)) 的思想是使用二进制向量 $B\in\{+1,-1\}^{n}$
    和缩放因子 $\alpha\in R^{+}$ 来近似权重向量 $W\in R^{n}$，使得 $W\approx\alpha B$。这可以表达为如下优化问题。
- en: '| (22) |  | $\displaystyle\alpha^{*},B^{*}=\operatorname*{argmin}_{\alpha,B}&#124;&#124;W-\alpha
    B&#124;&#124;^{2}$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\displaystyle\alpha^{*},B^{*}=\operatorname*{argmin}_{\alpha,B}\left\|W-\alpha
    B\right\|^{2}$ |  |'
- en: We can expand and write this as follows.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展开并将其写为如下形式。
- en: '| (23) |  | $\displaystyle&#124;&#124;W-\alpha B&#124;&#124;^{2}=\alpha^{2}B^{T}B-2\alpha
    W^{T}B+W^{T}W$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\displaystyle\text{ } \left\|W-\alpha B\right\|^{2}=\alpha^{2}B^{T}B-2\alpha
    W^{T}B+W^{T}W$ |  |'
- en: Since $B\in{+1,-1}^{n}$, $B^{T}B=n$. Also $W^{T}W$ is a constant. Thus $B^{*}=\operatorname*{argmax}_{B}W^{B}$
    such that $B\in{+1,-1}^{n}$. This optimization can be solved by simply assigning
    $B_{i}=+1$ when $W_{i}\geq 0$, and $B_{i}=-1$ otherwise. To compute $\alpha^{*}$,
    we set the derivative of $||W-\alpha B||^{2}$ wrt $\alpha$ to 0 and get the solution
    as follows.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $B\in\{+1,-1\}^{n}$，$B^{T}B=n$。此外，$W^{T}W$ 是一个常数。因此，$B^{*}=\operatorname*{argmax}_{B}W^{B}$
    使得 $B\in\{+1,-1\}^{n}$。这个优化可以通过简单地将 $B_{i}=+1$ 当 $W_{i}\geq 0$，否则 $B_{i}=-1$ 来解决。为了计算
    $\alpha^{*}$，我们将 $||W-\alpha B||^{2}$ 关于 $\alpha$ 的导数设置为 0，并得到如下解。
- en: '| (24) |  | $\displaystyle\alpha^{*}=\frac{\sum&#124;W_{i}&#124;}{n}$ |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\displaystyle\alpha^{*}=\frac{\sum\left|W_{i}\right|}{n}$ |  |'
- en: Thus, besides the binarized weight matrix, a scaling parameter is also learned
    in BWN.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了二值化的权重矩阵外，BWN 还学习一个缩放参数。
- en: To take this idea further, can we learn $\alpha$ and $B$ to minimize the overall
    network’s loss function? Thus, now, the Weight binarization can be formulated
    as the following optimization problem.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨这个想法，我们能否学习 $\alpha$ 和 $B$ 以最小化整个网络的损失函数？因此，现在，权重二值化可以被公式化为以下优化问题。
- en: '| (25) |  | $\displaystyle\min_{\hat{w}}\text{loss}(\hat{w})$ |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $\displaystyle\min_{\hat{w}}\text{loss}(\hat{w})$ |  |'
- en: '| (26) |  | $\displaystyle\text{such that }\hat{w}_{l}=\alpha_{l}b_{l};\alpha_{l}>0;b_{l}\in\{+1,-1\}^{n_{l}};l=1,...,L$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\displaystyle\text{使得 }\hat{w}_{l}=\alpha_{l}b_{l};\alpha_{l}>0;b_{l}\in\{+1,-1\}^{n_{l}};l=1,...,L$
    |  |'
- en: where $L$ is the number of layers, $n_{l}$ is the number of weights in layer
    $l$. This loss aware binarization (Hou et al., [2016](#bib.bib46)) problem can
    be solved using proximal Newton algorithm (Lee et al., [2014](#bib.bib64)) to
    find the best $\alpha_{l}$ and $B_{l}$.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是层数，$n_{l}$ 是层 $l$ 中的权重数量。这个损失感知二值化 (Hou et al., [2016](#bib.bib46))
    问题可以使用近端牛顿算法 (Lee et al., [2014](#bib.bib64)) 来找到最佳的 $\alpha_{l}$ 和 $B_{l}$。
- en: 3.2\. Ternarized Networks
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 三值化网络
- en: 'Unfortunately, binary quantization of the recurrent weights in RNNs/LSTMs never
    worked (Ott et al., [2016](#bib.bib90)). When the true value of a weight is near
    zero, its quantized value is either set to -1 or 1\. This results into an artificial
    increase in the magnitude of the weights and the vanishing/exploding gradients
    problem becomes more severe. Hence, another popular form of quantization is ternary
    quantization (see Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of
    Deep Learning Models for Text: A Survey")(B)). Ternary quantization can help achieve
    a min of 16x compression (up to 32x compression if hardware allows to avoid storing
    zeros). In this section, we discuss different variants of ternary quantization
    from the simplest ternary connect networks to hybrid ternary networks like HitNets.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '不幸的是，RNNs/LSTMs中递归权重的二值量化从未奏效（Ott等， [2016](#bib.bib90)）。当权重的真实值接近零时，其量化值要么设置为-1，要么设置为1。这导致了权重幅度的人工增加，并且梯度消失/爆炸的问题变得更加严重。因此，另一种流行的量化形式是三值量化（见图
    [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey")(B)）。三值量化可以帮助实现最小16倍的压缩（如果硬件允许避免存储零，则可以达到32倍压缩）。在本节中，我们将讨论从最简单的三值连接网络到像HitNets这样的混合三值网络的不同三值量化变体。'
- en: 3.2.1\. Ternary Weight Networks
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 三值权重网络
- en: The simplest method for ternary quantization is ternary connect (Lin et al.,
    [2015](#bib.bib70)) whose deterministic form is as follows.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 三值量化的最简单方法是三值连接（Lin等，[2015](#bib.bib70)），其确定性形式如下。
- en: '| (27) |  | <math   alttext="w_{t}=\begin{cases}+1&amp;\text{if }w>0.5\\ 0&amp;\text{if
    }-0.5<w\leq 0.5\\'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '| (27) |  | <math   alttext="w_{t}=\begin{cases}+1&amp;\text{如果 }w>0.5\\ 0&amp;\text{如果
    }-0.5<w\leq 0.5\\'
- en: -1&amp;\text{if }w\leq-0.5\\
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: -1 &amp; \text{如果 } w \leq -0.5\\
- en: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >w</mi></mrow><mo >></mo><mn >0.5</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mn  >0</mn></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo
    >−</mo><mn >0.5</mn></mrow><mo ><</mo><mi >w</mi><mo  >≤</mo><mn >0.5</mn></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo  >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo
    >≤</mo><mrow ><mo  >−</mo><mn >0.5</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >if </mtext></ci><ci >𝑤</ci></apply><cn
    type="float"  >0.5</cn></apply><cn type="integer"  >0</cn><apply ><apply ><apply  ><ci
    ><mtext >if </mtext></ci><cn type="float" >0.5</cn></apply><ci >𝑤</ci></apply><apply
    ><cn type="float" >0.5</cn></apply></apply><apply ><cn type="integer" >1</cn></apply><apply
    ><apply  ><ci ><mtext >if </mtext></ci><ci >𝑤</ci></apply><apply ><cn type="float"
    >0.5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >w_{t}=\begin{cases}+1&\text{if }w>0.5\\ 0&\text{if }-0.5<w\leq 0.5\\ -1&\text{if
    }w\leq-0.5\\ \end{cases}</annotation></semantics></math> |  |
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >w</mi></mrow><mo >></mo><mn >0.5</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mn  >0</mn></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext  >如果 </mtext><mo
    >−</mo><mn >0.5</mn></mrow><mo ><</mo><mi >w</mi><mo  >≤</mo><mn >0.5</mn></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo  >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext  >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo
    >≤</mo><mrow ><mo  >−</mo><mn >0.5</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >如果 </mtext></ci><ci >𝑤</ci></apply><cn
    type="float"  >0.5</cn></apply><cn type="integer"  >0</cn><apply ><apply ><apply  ><ci
    ><mtext >如果 </mtext></ci><cn type="float" >0.5</cn></apply><ci >𝑤</ci></apply><apply
    ><cn type="float" >0.5</cn></apply></apply><apply ><cn type="integer" >1</cn></apply><apply
    ><apply  ><ci ><mtext >如果 </mtext></ci><ci >𝑤</ci></apply><apply ><cn type="float"
    >0.5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >w_{t}=\begin{cases}+1&\text{如果 }w>0.5\\ 0&\text{如果 }-0.5<w\leq 0.5\\ -1&\text{如果
    }w\leq-0.5\\ \end{cases}</annotation></semantics></math> |  |
- en: Note that $w$ is the original weight value while $w_{t}$ is the ternarized weight
    value. Like binary connect, ternary connect also eliminates all multiplications
    in the forward pass. In the stochastic form, assuming original weights have been
    normalized to be in the range [-1,1], ternary quantization is done as follows.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，$w$ 是原始权重值，而 $w_{t}$ 是三值化的权重值。与二值连接类似，三值连接也消除了前向传播中的所有乘法。在随机形式下，假设原始权重已经归一化到范围[-1,1]，三值量化的处理方法如下。
- en: '| (28) |  | <math   alttext="w_{t}=\begin{cases}+1&amp;\text{with prob }w\text{
    if }w\in(0,1]\\ 0&amp;\text{with prob }1-w\text{ if }w\in(0,1]\\'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '| (28) |  | <math alttext="w_{t}=\begin{cases}+1&amp;\text{以概率 }w\text{ 当 }w\in(0,1]\\
    0&amp;\text{以概率 }1-w\text{ 当 }w\in(0,1]\\'
- en: 0&amp;\text{with prob }1+w\text{ if }w\in[-1,0]\\
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;\text{以概率 }1+w\text{ 当 }w\in[-1,0]\\
- en: -1&amp;\text{with prob }-w\text{ if }w\in[-1,0]\\
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: -1&amp;\text{以概率 }-w\text{ 当 }w\in[-1,0]\\
- en: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >with prob </mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mi >w</mi><mo lspace="0em" rspace="0em"  >​</mo><mtext > if </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"
    >(</mo><mn  >0</mn><mo >,</mo><mn >1</mn><mo stretchy="false" >]</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd  columnalign="left" ><mrow ><mrow  ><mrow
    ><mtext >with prob </mtext><mo lspace="0em" rspace="0em" >​</mo><mn >1</mn></mrow><mo
    >−</mo><mrow ><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mtext > if </mtext><mo
    lspace="0em" rspace="0em" >​</mo><mi >w</mi></mrow></mrow><mo >∈</mo><mrow ><mo
    stretchy="false" >(</mo><mn  >0</mn><mo >,</mo><mn >1</mn><mo stretchy="false"
    >]</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd  columnalign="left"
    ><mrow ><mrow  ><mrow ><mtext >with prob </mtext><mo lspace="0em" rspace="0em"
    >​</mo><mn >1</mn></mrow><mo >+</mo><mrow ><mi >w</mi><mo lspace="0em" rspace="0em"
    >​</mo><mtext > if </mtext><mo lspace="0em" rspace="0em" >​</mo><mi >w</mi></mrow></mrow><mo
    >∈</mo><mrow ><mo stretchy="false" >[</mo><mrow ><mo >−</mo><mn >1</mn></mrow><mo
    >,</mo><mn  >0</mn><mo stretchy="false"  >]</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo  >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext  >with prob </mtext><mo  >−</mo><mrow ><mi >w</mi><mo lspace="0em"
    rspace="0em" >​</mo><mtext > if </mtext><mo lspace="0em" rspace="0em" >​</mo><mi
    >w</mi></mrow></mrow><mo >∈</mo><mrow ><mo stretchy="false" >[</mo><mrow ><mo
    >−</mo><mn >1</mn></mrow><mo >,</mo><mn  >0</mn><mo stretchy="false"  >]</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >with prob </mtext></ci><ci >𝑤</ci><ci
    ><mtext > if </mtext></ci><ci >𝑤</ci></apply><interval closure="open-closed" ><cn
    type="integer" >0</cn><cn type="integer" >1</cn></interval></apply><cn type="integer"  >0</cn><apply
    ><apply ><apply  ><ci ><mtext >with prob </mtext></ci><cn type="integer"  >1</cn></apply><apply
    ><ci >𝑤</ci><ci ><mtext > if </mtext></ci><ci >𝑤</ci></apply></apply><interval
    closure="open-closed" ><cn type="integer" >0</cn><cn type="integer" >1</cn></interval></apply><cn
    type="integer"  >0</cn><apply ><apply ><apply  ><ci ><mtext >with prob </mtext></ci><cn
    type="integer"  >1</cn></apply><apply ><ci >𝑤</ci><ci ><mtext > if </mtext></ci><ci
    >𝑤</ci></apply></apply><interval closure="closed" ><apply ><cn type="integer"
    >1</cn></apply><cn type="integer" >0</cn></interval></apply><apply ><cn type="integer"
    >1</cn></apply><apply ><apply  ><ci ><mtext >with prob </mtext></ci><apply ><ci  >𝑤</ci><ci
    ><mtext > if </mtext></ci><ci >𝑤</ci></apply></apply><interval closure="closed"  ><apply
    ><cn type="integer" >1</cn></apply><cn type="integer"  >0</cn></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >w_{t}=\begin{cases}+1&\text{with prob }w\text{ if
    }w\in(0,1]\\ 0&\text{with prob }1-w\text{ if }w\in(0,1]\\ 0&\text{with prob }1+w\text{
    if }w\in[-1,0]\\ -1&\text{with prob }-w\text{ if }w\in[-1,0]\\ \end{cases}</annotation></semantics></math>
    |  |
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><msub ><mi >w</mi><mi >t</mi></msub><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >概率为</mtext><mo lspace="0em" rspace="0em"
    >​</mo><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mtext >如果</mtext><mo lspace="0em"
    rspace="0em" >​</mo><mi >w</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false" >(</mo><mn
    >0</mn><mo >,</mo><mn >1</mn><mo stretchy="false" >]</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd  columnalign="left" ><mrow ><mrow  ><mrow
    ><mtext >概率为</mtext><mo lspace="0em" rspace="0em" >​</mo><mn >1</mn></mrow><mo
    >−</mo><mrow ><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mtext >如果</mtext><mo
    lspace="0em" rspace="0em" >​</mo><mi >w</mi></mrow></mrow><mo >∈</mo><mrow ><mo
    stretchy="false" >(</mo><mn >0</mn><mo >,</mo><mn >1</mn><mo stretchy="false"
    >]</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd  columnalign="left"
    ><mrow ><mrow  ><mrow ><mtext >概率为</mtext><mo lspace="0em" rspace="0em" >​</mo><mn
    >1</mn></mrow><mo >+</mo><mrow ><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mtext
    >如果</mtext><mo lspace="0em" rspace="0em" >​</mo><mi >w</mi></mrow></mrow><mo >∈</mo><mrow
    ><mo stretchy="false" >[</mo><mrow ><mo >−</mo><mn >1</mn></mrow><mo >,</mo><mn
    >0</mn><mo stretchy="false"  >]</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mo >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext  >概率为</mtext><mo  >−</mo><mrow
    ><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mtext >如果</mtext><mo lspace="0em"
    rspace="0em" >​</mo><mi >w</mi></mrow></mrow><mo >∈</mo><mrow ><mo stretchy="false"
    >[</mo><mrow ><mo >−</mo><mn >1</mn></mrow><mo >,</mo><mn >0</mn><mo stretchy="false"  >]</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >概率为</mtext></ci><ci >𝑤</ci><ci ><mtext
    >如果</mtext></ci><ci >𝑤</ci></apply><interval closure="open-closed" ><cn type="integer"
    >0</cn><cn type="integer" >1</cn></interval></apply><cn type="integer"  >0</cn><apply
    ><apply ><apply  ><ci ><mtext >概率为</mtext></ci><cn type="integer"  >1</cn></apply><apply
    ><ci >𝑤</ci><ci ><mtext >如果</mtext></ci><ci >𝑤</ci></apply></apply><interval closure="open-closed"
    ><cn type="integer" >0</cn><cn type="integer" >1</cn></interval></apply><cn type="integer"  >0</cn><apply
    ><apply ><apply  ><ci ><mtext >概率为</mtext></ci><cn type="integer"  >1</cn></apply><apply
    ><ci >𝑤</ci><ci ><mtext >如果</mtext></ci><ci >𝑤</ci></apply></apply><interval closure="closed"
    ><apply ><cn type="integer" >1</cn></apply><cn type="integer" >0</cn></interval></apply><apply
    ><cn type="integer" >1</cn></apply><apply ><apply  ><ci ><mtext >概率为</mtext></ci><apply
    ><ci  >𝑤</ci><ci ><mtext >如果</mtext></ci><ci >𝑤</ci></apply></apply><interval
    closure="closed"  ><apply ><cn type="integer" >1</cn></apply><cn type="integer"  >0</cn></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >w_{t}=\begin{cases}+1&\text{with prob }w\text{ if
    }w\in(0,1]\\ 0&\text{with prob }1-w\text{ if }w\in(0,1]\\ 0&\text{with prob }1+w\text{
    if }w\in[-1,0]\\ -1&\text{with prob }-w\text{ if }w\in[-1,0]\\ \end{cases}</annotation></semantics></math>
    |  |
- en: A slightly related way called as Bernoulli Ternary Quantization where $w_{t}$
    is set to +1 (or -1) with prob $p$ if $w>0$ (or) $<0$, and set to 0 with prob
    1-p where $p\sim$Bernoulli($|x|$). Yet another way to set the boundaries for the
    three ranges is to use Gaussian based ternary weights (Alom et al., [2018](#bib.bib2))
    as follows.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一种略相关的方法称为伯努利三值量化，其中 $w_{t}$ 以概率 $p$ 设为 +1（或 -1），如果 $w>0$（或）$<0$，以概率 1-p 设为
    0，其中 $p\sim$伯努利($|x|$)。另一种设定三种范围边界的方法是使用基于高斯的三值权重（Alom 等，[2018](#bib.bib2)），如下所示。
- en: '| (29) |  | <math   alttext="w_{t}=\begin{cases}+1&amp;\text{if }w>-(\mu+\sigma/2)\\
    0&amp;\text{if }-(\mu+\sigma/2)<w\leq(\mu+\sigma/2)\\'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '| (29) |  | <math alttext="w_{t}=\begin{cases}+1&amp;\text{如果 }w>-(\mu+\sigma/2)\\
    0&amp;\text{如果 }-(\mu+\sigma/2)<w\leq(\mu+\sigma/2)\\'
- en: -1&amp;\text{if }w\leq-(\mu+\sigma/2)\\
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: -1&amp;\text{如果 }w\leq-(\mu+\sigma/2)\\
- en: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >w</mi></mrow><mo >></mo><mrow ><mo  >−</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><mi >μ</mi><mo >+</mo><mrow ><mi >σ</mi><mo >/</mo><mn >2</mn></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mn
    >0</mn></mtd><mtd  columnalign="left" ><mrow ><mrow  ><mtext >if </mtext><mo >−</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >μ</mi><mo >+</mo><mrow ><mi >σ</mi><mo
    >/</mo><mn >2</mn></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    ><</mo><mi >w</mi><mo  >≤</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi >μ</mi><mo
    >+</mo><mrow ><mi >σ</mi><mo >/</mo><mn >2</mn></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mo  >−</mo><mn
    >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo >≤</mo><mrow ><mo  >−</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >μ</mi><mo >+</mo><mrow ><mi >σ</mi><mo
    >/</mo><mn >2</mn></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >if </mtext></ci><ci >𝑤</ci></apply><apply
    ><apply ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer" >2</cn></apply></apply></apply></apply><cn
    type="integer"  >0</cn><apply ><apply ><apply  ><ci ><mtext >if </mtext></ci><apply
    ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer" >2</cn></apply></apply></apply><ci
    >𝑤</ci></apply><apply ><apply  ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer"  >2</cn></apply></apply></apply></apply><apply
    ><cn type="integer" >1</cn></apply><apply ><apply ><ci  ><mtext >if </mtext></ci><ci
    >𝑤</ci></apply><apply ><apply ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer"
    >2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >w_{t}=\begin{cases}+1&\text{if }w>-(\mu+\sigma/2)\\
    0&\text{if }-(\mu+\sigma/2)<w\leq(\mu+\sigma/2)\\ -1&\text{if }w\leq-(\mu+\sigma/2)\\
    \end{cases}</annotation></semantics></math> |  |
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >w</mi></mrow><mo >></mo><mrow ><mo  >−</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><mi >μ</mi><mo >+</mo><mrow ><mi >σ</mi><mo >/</mo><mn >2</mn></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mn
    >0</mn></mtd><mtd  columnalign="left" ><mrow ><mrow  ><mtext >如果 </mtext><mo >−</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >μ</mi><mo >+</mo><mrow ><mi >σ</mi><mo
    >/</mo><mn >2</mn></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    ><</mo><mi >w</mi><mo  >≤</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi >μ</mi><mo
    >+</mo><mrow ><mi >σ</mi><mo >/</mo><mn >2</mn></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mo  >−</mo><mn
    >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext  >如果 </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo >≤</mo><mrow ><mo  >−</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >μ</mi><mo >+</mo><mrow ><mi >σ</mi><mo
    >/</mo><mn >2</mn></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >如果 </mtext></ci><ci >𝑤</ci></apply><apply
    ><apply ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer" >2</cn></apply></apply></apply></apply><cn
    type="integer"  >0</cn><apply ><apply ><apply  ><ci ><mtext >如果 </mtext></ci><apply
    ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer" >2</cn></apply></apply></apply><ci
    >𝑤</ci></apply><apply ><apply  ><ci >𝜇</ci><apply ><ci >𝜎</ci><cn type="integer"  >2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >w_{t}=\begin{cases}+1&\text{如果 }w>-(\mu+\sigma/2)\\
    0&\text{如果 }-(\mu+\sigma/2)<w\leq(\mu+\sigma/2)\\ -1&\text{如果 }w\leq-(\mu+\sigma/2)\\
    \end{cases}</annotation></semantics></math> |  |
- en: where $\mu$ and $\sigma$ are the mean and standard deviation of the weight matrix
    being quantized.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu$ 和 $\sigma$ 是被量化的权重矩阵的均值和标准差。
- en: 3.2.2\. Trained Ternary Quantization
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 训练的三元量化
- en: Rather than using the rules for ternary quantization as mentioned above, one
    can learn the boundary ranges or the quantized values for individual weights.
    One way of learning the right ternary representation per weight value is to minimize
    the Euclidean distance between full precision weights $W$ and the ternary weights
    $T$ along with a scaling factor (Li et al., [2016b](#bib.bib66)). This can be
    expressed as the following optimization problem.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述提到的三元量化规则相比，可以学习个别权重的边界范围或量化值。学习每个权重值的正确三元表示的一种方法是最小化全精度权重 $W$ 和三元权重 $T$
    之间的欧氏距离，并加上一个缩放因子（Li et al., [2016b](#bib.bib66)）。这可以表示为以下优化问题。
- en: '| (30) |  | $\displaystyle\alpha^{*},T^{*}=\operatorname*{argmin}_{\alpha,T}&#124;&#124;W-\alpha
    T&#124;&#124;_{2}^{2}$ |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\displaystyle\alpha^{*},T^{*}=\operatorname*{argmin}_{\alpha,T}&#124;&#124;W-\alpha
    T&#124;&#124;_{2}^{2}$ |  |'
- en: '| (31) |  | $\displaystyle\text{such that }\alpha\geq 0;T_{i}\in\{-1,0,1\};i=1,2,...,n$
    |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\displaystyle\text{使得 }\alpha\geq 0;T_{i}\in\{-1,0,1\};i=1,2,...,n$
    |  |'
- en: Note that this is equivalent to the BWN method (Rastegari et al., [2016](#bib.bib98)).
    This does not lead to a closed form solution. Hence, we approximate the solution
    with threshold-based ternary function.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这等同于 BWN 方法（Rastegari et al., [2016](#bib.bib98)）。这并不会导致封闭形式的解。因此，我们用基于阈值的三元函数来近似解。
- en: '| (32) |  | <math   alttext="w_{t}=\begin{cases}+1&amp;\text{if }w>\Delta\\
    0&amp;\text{if }-\Delta<w\leq\Delta\\'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '| (32) |  | <math   alttext="w_{t}=\begin{cases}+1&amp;\text{如果 }w>\Delta\\
    0&amp;\text{如果 }-\Delta<w\leq\Delta\\'
- en: -1&amp;\text{if }w\leq-\Delta\\
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: -1&amp;\text{如果 }w\leq-\Delta\\
- en: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >w</mi></mrow><mo >></mo><mi mathvariant="normal" >Δ</mi></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd  columnalign="left" ><mrow ><mrow  ><mtext
    >if </mtext><mo >−</mo><mi mathvariant="normal"  >Δ</mi></mrow><mo ><</mo><mi
    >w</mi><mo  >≤</mo><mi mathvariant="normal"  >Δ</mi></mrow></mtd></mtr><mtr ><mtd
    columnalign="left"  ><mrow ><mo  >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo
    >≤</mo><mrow ><mo  >−</mo><mi mathvariant="normal"  >Δ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >if </mtext></ci><ci >𝑤</ci></apply><ci
    >Δ</ci></apply><cn type="integer"  >0</cn><apply ><apply ><apply  ><ci ><mtext
    >if </mtext></ci><ci >Δ</ci></apply><ci >𝑤</ci></apply><apply ><ci  >Δ</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply><apply ><apply ><ci  ><mtext >if </mtext></ci><ci
    >𝑤</ci></apply><apply ><ci >Δ</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >w_{t}=\begin{cases}+1&\text{if }w>\Delta\\ 0&\text{if
    }-\Delta<w\leq\Delta\\ -1&\text{if }w\leq-\Delta\\ \end{cases}</annotation></semantics></math>
    |  |
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >w</mi><mi >t</mi></msub><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mo >+</mo><mn  >1</mn></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >w</mi></mrow><mo >></mo><mi mathvariant="normal" >Δ</mi></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd  columnalign="left" ><mrow ><mrow  ><mtext
    >如果 </mtext><mo >−</mo><mi mathvariant="normal"  >Δ</mi></mrow><mo ><</mo><mi
    >w</mi><mo  >≤</mo><mi mathvariant="normal"  >Δ</mi></mrow></mtd></mtr><mtr ><mtd
    columnalign="left"  ><mrow ><mo  >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext  >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >w</mi></mrow><mo
    >≤</mo><mrow ><mo  >−</mo><mi mathvariant="normal"  >Δ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑤</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><cn type="integer"
    >1</cn></apply><apply ><apply ><ci  ><mtext >如果 </mtext></ci><ci >𝑤</ci></apply><ci
    >Δ</ci></apply><cn type="integer"  >0</cn><apply ><apply ><apply  ><ci ><mtext
    >如果 </mtext></ci><ci >Δ</ci></apply><ci >𝑤</ci></apply><apply ><ci  >Δ</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply><apply ><apply ><ci  ><mtext >如果 </mtext></ci><ci
    >𝑤</ci></apply><apply ><ci >Δ</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >w_{t}=\begin{cases}+1&\text{如果 }w>\Delta\\ 0&\text{如果
    }-\Delta<w\leq\Delta\\ -1&\text{如果 }w\leq-\Delta\\ \end{cases}</annotation></semantics></math>
    |  |
- en: The approximation works when we set $\Delta$ as follows.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 该近似在我们将 $\Delta$ 设定如下时有效。
- en: '| (33) |  | $\displaystyle\Delta^{*}=\operatorname*{argmax}_{\Delta>0}\frac{1}{&#124;I_{\Delta}&#124;}\left(\sum_{i\in
    I_{\Delta}}&#124;W_{i}&#124;\right)^{2}$ |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $\displaystyle\Delta^{*}=\operatorname*{argmax}_{\Delta>0}\frac{1}{|I_{\Delta}|}\left(\sum_{i\in
    I_{\Delta}}|W_{i}|\right)^{2}$ |  |'
- en: where $I_{\Delta}$ is the number of weights with magnitude$>\Delta$. Again,
    this has no straightforward solution, unless we assume that original weights $W_{i}$’s
    are generated from uniform or normal distribution. When $W_{i}$’s are uniformly
    distributed in $[-a,a]$ and $\Delta$ lies in $(0,a]$, the approximated $\Delta^{*}$
    is $a/3$, which equals to $\frac{2}{3}E(W)$. When $W_{i}$’s are generated from
    normal distributions $N(0,\sigma^{2})$, the approximated $\Delta^{*}$ is 0.6$\sigma$
    which equals to 0.75$E(|W|)$. Thus, we can use the following rule of thumb for
    fast and easy computation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$I_{\Delta}$是具有大于$\Delta$的权重数量。再次强调，除非我们假设原始权重$W_{i}$是从均匀分布或正态分布生成的，否则没有直接的解决方案。当$W_{i}$在$[-a,a]$中均匀分布且$\Delta$在$(0,a]$中时，近似的$\Delta^{*}$是$a/3$，这等于$\frac{2}{3}E(W)$。当$W_{i}$从正态分布$N(0,\sigma^{2})$中生成时，近似的$\Delta^{*}$是0.6$\sigma$，这等于0.75$E(|W|)$。因此，我们可以使用以下经验法则进行快速和简便的计算。
- en: '| (34) |  | $\displaystyle\Delta^{*}\approx 0.7E(W)=\frac{0.7}{n}\sum_{i=1}^{n}&#124;W_{i}&#124;$
    |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\displaystyle\Delta^{*}\approx 0.7E(W)=\frac{0.7}{n}\sum_{i=1}^{n}|W_{i}|$
    |  |'
- en: 'Another way to learn the quantization step size $\Delta$ in Eq. [32](#S3.E32
    "In 3.2.2\. Trained Ternary Quantization ‣ 3.2\. Ternarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey") is to learn in a loss-aware
    manner (Hwang and Sung, [2014](#bib.bib49)), i.e., tuning it to minimize the overall
    network loss. Given a multi-layered network, we need to perform such quantization
    layer by layer in a greedy manner. We first train the network with full precision
    weights. We quantize all input data and signals of hidden layers. Next, we start
    with the weight quantizer between the input layer and the first hidden layer,
    try several step sizes around the initial step size and measure the output error
    of the network with the training set. The initial step size is determined using
    Lloyd-Max algorithm (Lloyd, [1982](#bib.bib76)). Choose the step size that minimizes
    the output error and quantize the weights. Further, we perform these steps for
    the next few layers until the output layer. Finally, the quantized neural network
    is retrained.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种学习公式中量化步长$\Delta$的方法是以损失感知的方式进行学习（Hwang 和 Sung，[2014](#bib.bib49)），即调整它以最小化整体网络损失。给定一个多层网络，我们需要逐层执行这种量化，采取贪婪的方式。我们首先使用全精度权重训练网络。然后，我们量化所有输入数据和隐藏层的信号。接下来，我们从输入层和第一个隐藏层之间的权重量化器开始，尝试几个接近初始步长的步长，并测量训练集上的网络输出误差。初始步长是使用Lloyd-Max算法（Lloyd，[1982](#bib.bib76)）确定的。选择使输出误差最小的步长并量化权重。进一步地，我们对接下来的几层执行这些步骤，直到输出层。最后，对量化后的神经网络进行再训练。
- en: 'Yet another way of training ternary quantization (Zhu et al., [2016](#bib.bib145))
    is to quantize weights to one of $-W_{l}^{n}$, 0, $W_{l}^{p}$ for each layer $l$,
    where $W_{l}^{n}$ and $W_{l}^{p}$ are trainable parameters, learned using back-propagation.
    First, we normalize the full-precision weights to the range [-1, +1] by dividing
    each weight by the maximum weight. During SGD, we back propagate the gradient
    to both $W_{l}^{n}$ and $W_{l}^{p}$ and to the latent full-precision weights.
    This makes it possible to adjust the ternary assignment (i.e. which of the three
    values a weight is assigned). To decide the quantization step size $\Delta_{l}$
    for a layer $l$, two heuristics can be used: (1) set $\Delta_{l}=t\times\max(|w_{l}|)$
    where $t$ is a constant and $w_{l}$ are the full precision weights in layer $l$.
    (2) maintain a constant sparsity $r$ for all layers throughout training. By adjusting
    the hyper-parameter $r$ we can obtain ternary weight networks with various sparsities.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种训练三值量化的方法（Zhu 等，[2016](#bib.bib145)）是将权重量化为每层$l$的$-W_{l}^{n}$、0、$W_{l}^{p}$之一，其中$W_{l}^{n}$和$W_{l}^{p}$是可训练的参数，通过反向传播学习得到。首先，我们通过将每个权重除以最大权重，将全精度权重规范化到范围[-1,
    +1]。在SGD过程中，我们将梯度反向传播到$W_{l}^{n}$和$W_{l}^{p}$以及潜在的全精度权重。这使得调整三值分配（即权重被分配的三种值中的哪一个）成为可能。为了确定层$l$的量化步长$\Delta_{l}$，可以使用两种启发式方法：（1）设置$\Delta_{l}=t\times\max(|w_{l}|)$，其中$t$是一个常数，$w_{l}$是层$l$中的全精度权重。（2）在训练过程中保持所有层的恒定稀疏性$r$。通过调整超参数$r$，我们可以获得具有不同稀疏性的三值权重网络。
- en: 3.2.3\. Hybrid Ternary Quantization
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 混合三值量化
- en: 'Given various ternary quantization methods proposed so far, one can combine
    them and use different methods for different layers. Wang et al. (Wang et al.,
    [2018](#bib.bib128)) found that threshold ternary quantization (TTQ) (Eq. [32](#S3.E32
    "In 3.2.2\. Trained Ternary Quantization ‣ 3.2\. Ternarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey")) is preferable for
    weights in an RNN while Bernoulli Ternary Quantization (BTQ) is preferable for
    activations. This is based on the observation that in an RNN, the distribution
    of weights follows normal distribution (with different ranges across different
    weight matrices), while for activations, the range is [0,1] and most of the values
    are located near to the two poles instead of the middle of the range. In the training
    phase (where we need to store the full precision weights), ternary quantization
    of weights only saves 1.4x memory consumption but quantizing both weights and
    activations can achieve up to 16x memory savings.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于目前提出的各种三值量化方法，可以将它们结合起来，并对不同的层使用不同的方法。Wang等（Wang等，[2018](#bib.bib128)）发现阈值三值量化（TTQ）（Eq.[32](#S3.E32
    "In 3.2.2\. Trained Ternary Quantization ‣ 3.2\. Ternarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey")）对于RNN中的权重更为合适，而伯努利三值量化（BTQ）则更适合激活。这是基于观察到在RNN中，权重的分布遵循正态分布（不同权重矩阵的范围不同），而激活的范围是[0,1]，大多数值靠近两个极端而不是范围的中间。在训练阶段（我们需要存储全精度权重），三值量化权重仅节省1.4倍的内存消耗，但量化权重和激活可以实现高达16倍的内存节省。'
- en: The HitNet architecture (Wang et al., [2018](#bib.bib128)) with this hybrid
    ternary quantization can be defined using these equations, where $i_{t},f_{t},o_{t}$
    are the input, forget and output gates; $x_{t}$ is input at time $t$; $c_{t}$
    is the cell output; and $h_{t}$ is the hidden layer output; $W_{x}$, $W_{h}$,
    $b_{x}$, $b_{h}$ are weights.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: HitNet架构（Wang等，[2018](#bib.bib128)）使用这种混合三值量化可以通过以下方程定义，其中$i_{t},f_{t},o_{t}$是输入门、遗忘门和输出门；$x_{t}$是在时间$t$的输入；$c_{t}$是单元输出；$h_{t}$是隐藏层输出；$W_{x}$，$W_{h}$，$b_{x}$，$b_{h}$是权重。
- en: '|  | $\displaystyle i_{t},f_{t},g_{t},o_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(\text{TTQ}(W_{x})x_{t}+\text{TTQ}(b_{x})$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle i_{t},f_{t},g_{t},o_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(\text{TTQ}(W_{x})x_{t}+\text{TTQ}(b_{x})$
    |  |'
- en: '|  |  | $\displaystyle+$ | $\displaystyle\text{TTQ}(W_{h})h_{t-1}+\text{TTQ}(b_{h}))$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+$ | $\displaystyle\text{TTQ}(W_{h})h_{t-1}+\text{TTQ}(b_{h}))$
    |  |'
- en: '|  | $\displaystyle c_{t}$ | $\displaystyle=$ | $\displaystyle f_{t}\times
    c_{t-1}+i_{t}\times g_{t}$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c_{t}$ | $\displaystyle=$ | $\displaystyle f_{t}\times
    c_{t-1}+i_{t}\times g_{t}$ |  |'
- en: '| (35) |  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\text{BTQ}(o_{t}\times\sigma(c_{t}))$
    |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\text{BTQ}(o_{t}\times\sigma(c_{t}))$
    |  |'
- en: 3.3\. General Quantized Networks
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 一般量化网络
- en: So far we discussed methods designed specifically for binary and ternary quantization.
    Now, we discuss general $k$-bit quantization methods. We will discuss (1) uniform
    quantization methods which perform equal width binning, (2) non-uniform methods
    which are closer to equal frequency binning, (3) loss-aware quantization methods,
    and (4) methods specifically designed for Transformer models.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了专门针对二值和三值量化设计的方法。现在，我们讨论一般的$k$-位量化方法。我们将讨论（1）执行等宽分箱的均匀量化方法，（2）接近等频分箱的非均匀方法，（3）损失感知量化方法，以及（4）专门为Transformer模型设计的方法。
- en: 3.3.1\. Uniform Quantization
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 均匀量化
- en: 'Uniform $k$-bit Quantization simply splits the range of original weights into
    $2^{k}-1$ equal size intervals (Rastegari et al., [2016](#bib.bib98); Hubara et al.,
    [2017](#bib.bib48); He et al., [2016](#bib.bib41)). Refer Fig. [3](#S3.F3 "Figure
    3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models for Text: A Survey")(C).
    If original weights are in range [-1,1], they can be quantized as follows.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '均匀$k$-位量化将原始权重的范围分成$2^{k}-1$个等大小的区间（Rastegari等，[2016](#bib.bib98)；Hubara等，[2017](#bib.bib48)；He等，[2016](#bib.bib41)）。参见图[3](#S3.F3
    "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models for Text: A
    Survey")(C)。如果原始权重在范围[-1,1]内，它们可以按照以下方式进行量化。'
- en: '| (36) |  | $q_{k}(x)=2\left(\frac{\text{round}[(2^{k}-1)\left(\frac{x+1}{2}\right)]}{2^{k}-1}-\frac{1}{2}\right)$
    |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $q_{k}(x)=2\left(\frac{\text{round}[(2^{k}-1)\left(\frac{x+1}{2}\right)]}{2^{k}-1}-\frac{1}{2}\right)$
    |  |'
- en: Similarly, if entries are in range [0,1], we could use the following formula.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果条目在范围[0,1]内，我们可以使用以下公式。
- en: '| (37) |  | $\displaystyle q_{k}(x)=\frac{1}{2^{k}-1}\lfloor(2^{k}-1)x+\frac{1}{2}\rfloor$
    |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $\displaystyle q_{k}(x)=\frac{1}{2^{k}-1}\lfloor(2^{k}-1)x+\frac{1}{2}\rfloor$
    |  |'
- en: When the weights in matrix $X$ are not in the range [0,1], we can first scale
    weights as follows.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 当矩阵 $X$ 中的权重不在 [0,1] 范围内时，我们可以首先按如下方式对权重进行缩放。
- en: '| (38) |  | $\displaystyle\tilde{X}=\frac{X-\beta}{\alpha}$ |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $\displaystyle\tilde{X}=\frac{X-\beta}{\alpha}$ |  |'
- en: where $\alpha=\max(X)-\min(X)$ and $\beta=\min(X)$. After quantization, we can
    apply a reverse transform to approximate the original values. Overall, the quantized
    result can be written as follows.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha=\max(X)-\min(X)$ 和 $\beta=\min(X)$。量化后，我们可以应用反变换来逼近原始值。总体而言，量化结果可以写作如下。
- en: '| (39) |  | $\displaystyle Q(X)=\alpha q_{k}(\tilde{X})+\beta$ |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $\displaystyle Q(X)=\alpha q_{k}(\tilde{X})+\beta$ |  |'
- en: Given any quantization function $q_{k}(x)$, one can use it for quantizing weight
    matrices of various recurrent models like RNNs, GRUs and LSTMs (Ott et al., [2016](#bib.bib90)).
    Typical inference equations for a GRU can be written as follows.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任何量化函数 $q_{k}(x)$，可以用它来量化各种递归模型的权重矩阵，如 RNN、GRU 和 LSTM（Ott 等，[2016](#bib.bib90)）。GRU
    的典型推断方程可以写作如下。
- en: '| (40) |  | $\displaystyle z_{t}=\sigma(W_{z}.[h_{t-1},x_{t}]);r_{t}=\sigma(W_{r}.[h_{t-1},x_{t}])$
    |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| (40) |  | $\displaystyle z_{t}=\sigma(W_{z}.[h_{t-1},x_{t}]);r_{t}=\sigma(W_{r}.[h_{t-1},x_{t}])$
    |  |'
- en: '| (41) |  | $\displaystyle\tilde{h_{t}}=\text{tanh}(W.[r_{t}\times h_{t-1},x_{t}]);h_{t}=(1-z_{t})h_{t-1}+z_{t}\tilde{h_{t}}$
    |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| (41) |  | $\displaystyle\tilde{h_{t}}=\text{tanh}(W.[r_{t}\times h_{t-1},x_{t}]);h_{t}=(1-z_{t})h_{t-1}+z_{t}\tilde{h_{t}}$
    |  |'
- en: Besides the matrix multiplications needed to compute $z_{t}$, $r_{t}$ and $\tilde{h_{t}}$,
    the gate structure of $\tilde{h_{t}}$ and $h_{t}$ brings in the need for element-wise
    multiplication. As $\tilde{h_{t}}$ and $h_{t}$ are also the inputs to computations
    at the next timestamp, and noting that a quantized value multiplied by a quantized
    value will have a larger bit-width, we need to insert additional quantization
    steps after element-wise multiplications. Another problem with quantization of
    GRU structure lies in the different value range of gates. The range of tanh is
    [-1, 1], which is different from the value range [0, 1] of $z_{t}$ and $r_{t}$.
    Keeping in mind these observations, the equations for a quantized GRU can be written
    as follows, after the weights $W_{z}$, $W_{r}$ and $W$ and input $x_{t}$ have
    already been quantized to [-1,1].
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算 $z_{t}$、$r_{t}$ 和 $\tilde{h_{t}}$ 所需的矩阵乘法外，$\tilde{h_{t}}$ 和 $h_{t}$ 的门结构还引入了逐元素乘法的需要。由于
    $\tilde{h_{t}}$ 和 $h_{t}$ 也是下一个时间戳计算的输入，并且注意到量化值与量化值相乘将具有更大的位宽，我们需要在逐元素乘法之后插入额外的量化步骤。GRU
    结构量化的另一个问题在于门的不同值范围。tanh 的范围是 [-1, 1]，这与 $z_{t}$ 和 $r_{t}$ 的值范围 [0, 1] 不同。考虑到这些观察，量化
    GRU 的方程可以写作如下，在权重 $W_{z}$、$W_{r}$ 和 $W$ 以及输入 $x_{t}$ 已经量化到 [-1,1] 之后。
- en: '| (42) |  | $\displaystyle z_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W_{z}.[h_{t-1},x_{t}])$
    |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $\displaystyle z_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W_{z}.[h_{t-1},x_{t}])$
    |  |'
- en: '| (43) |  | $\displaystyle r_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W_{r}.[h_{t-1},x_{t}])$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| (43) |  | $\displaystyle r_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W_{r}.[h_{t-1},x_{t}])$
    |  |'
- en: '| (44) |  | $\displaystyle\tilde{h_{t}}$ | $\displaystyle=$ | $\displaystyle
    tanh\left(W.\left[2q_{k}\left(\frac{1}{2}(r_{t}h_{t-1})+\frac{1}{2}\right)-1,x_{t}\right]\right)$
    |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| (44) |  | $\displaystyle\tilde{h_{t}}$ | $\displaystyle=$ | $\displaystyle
    tanh\left(W.\left[2q_{k}\left(\frac{1}{2}(r_{t}h_{t-1})+\frac{1}{2}\right)-1,x_{t}\right]\right)$
    |  |'
- en: '| (45) |  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle 2q_{k}\left(\frac{1}{2}((1-z_{t})h_{t-1}+z_{t}\tilde{h_{t}})+\frac{1}{2}\right)-1$
    |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| (45) |  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle 2q_{k}\left(\frac{1}{2}((1-z_{t})h_{t-1}+z_{t}\tilde{h_{t}})+\frac{1}{2}\right)-1$
    |  |'
- en: Following a similar method, we can also quantize LSTM networks.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 采用类似的方法，我们也可以对 LSTM 网络进行量化。
- en: 3.3.2\. Balanced Quantization
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2. 平衡量化
- en: Uniform quantization is easy to implement but far from optimum when quantizing
    non-uniform data, which is believed to be the trained weights and activations
    of deep neural network. One way of performing non-uniform quantization is exponential
    quantization (Ott et al., [2016](#bib.bib90)). It quantizes the weight values
    to an integer power of 2\. If we let
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀量化容易实现，但在量化非均匀数据时远未达到最佳，这些数据被认为是深度神经网络的训练权重和激活值。执行非均匀量化的一种方法是指数量化（Ott 等，[2016](#bib.bib90)）。它将权重值量化为
    2 的整数次方。如果我们设定
- en: '| (46) |  | $\displaystyle p=\frac{&#124;W&#124;}{2^{\lfloor\log_{2}&#124;W&#124;\rfloor}}-1$
    |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| (46) |  | $\displaystyle p=\frac{&#124;W&#124;}{2^{\lfloor\log_{2}&#124;W&#124;\rfloor}}-1$
    |  |'
- en: deterministic exponential quantization can be written as follows.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性指数量化可以写作如下。
- en: '| (47) |  | $\log_{2}W_{q}=\begin{cases}\lceil\log_{2}&#124;W&#124;\rceil&amp;\text{if
    }p>0.5\\ \lfloor\log_{2}&#124;W&#124;\rfloor&amp;\text{otherwise }\end{cases}$
    |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| (47) |  | $\log_{2}W_{q}=\begin{cases}\lceil\log_{2}&#124;W&#124;\rceil&amp;\text{如果
    }p>0.5\\ \lfloor\log_{2}&#124;W&#124;\rfloor&amp;\text{否则 }\end{cases}$ |  |'
- en: Similarly, stochastic exponential quantization can be written as follows.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，随机指数量化可以写作如下。
- en: '| (48) |  | $\log_{2}W_{q}=\begin{cases}\lceil\log_{2}&#124;W&#124;\rceil&amp;\text{with
    prob }p\\ \lfloor\log_{2}&#124;W&#124;\rfloor&amp;\text{with prob }1-p\end{cases}$
    |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| (48) |  | $\log_{2}W_{q}=\begin{cases}\lceil\log_{2}&#124;W&#124;\rceil&amp;\text{概率为
    }p\\ \lfloor\log_{2}&#124;W&#124;\rfloor&amp;\text{概率为 }1-p\end{cases}$ |  |'
- en: 'Exponential quantization enables storing weights in low precision and eliminating
    multiplications. However, it still does not perform quantization in a way which
    is sensitive to the distribution of the weights. Distributions of parameters in
    neural networks are often imbalanced, such that the uniform quantization determined
    from extremal values may under utilize available bitwidth. When we quantize values,
    it may be desirable to make the quantized values have balanced distributions,
    to take full advantage of the available parameter space. Balanced quantization
    method (Zhou et al., [2017](#bib.bib144)) starts by partitioning numbers into
    $2^{k}$ bins containing roughly the same number of entries (percentiles). Refer
    Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey")(D). Each partition is then mapped to an evenly-divided interval
    in the closed interval [0, 1]. Finally, the quantization step maps intervals into
    discrete values using Eq. [36](#S3.E36 "In 3.3.1\. Uniform Quantization ‣ 3.3\.
    General Quantized Networks ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey") and transforms the value range to be approximately the same
    as input.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '指数量化使得以低精度存储权重并消除乘法成为可能。然而，它仍然没有以对权重分布敏感的方式进行量化。神经网络中参数的分布通常是不平衡的，因此从极值确定的均匀量化可能会未充分利用可用的位宽。当我们量化值时，可能希望使量化后的值具有平衡的分布，以充分利用可用的参数空间。平衡量化方法（Zhou
    et al., [2017](#bib.bib144)）首先将数字划分为包含大致相同数量条目的$2^{k}$个箱体（百分位数）。参考图[3](#S3.F3
    "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models for Text: A
    Survey")(D)。然后，将每个分区映射到闭区间[0, 1]中的均匀划分区间。最后，量化步骤使用公式[36](#S3.E36 "In 3.3.1\. Uniform
    Quantization ‣ 3.3\. General Quantized Networks ‣ 3\. Quantization ‣ Compression
    of Deep Learning Models for Text: A Survey")将区间映射到离散值，并将值范围转换为大致与输入相同。'
- en: A naïve implementation using percentiles as thresholds would require sorting
    of weight values during each forward operation in back-propagation, which may
    slow down the training process. The $2^{k}$ evenly spaced percentiles required
    in histogram equalization can be computed from the recursive application of partitioning
    of numbers by medians. Further, the mean $\mu$ can be used to approximate the
    median $m$. Thus, we can perform approximate histogram equalization without doing
    sorting.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 使用百分位数作为阈值的原始实现将需要在每次反向传播中的前向操作期间对权重值进行排序，这可能会减慢训练过程。在直方图均衡化中需要的$2^{k}$个均匀间隔的百分位数可以通过递归应用中位数划分数字来计算。此外，均值$\mu$可以用来近似中位数$m$。因此，我们可以在不进行排序的情况下执行近似直方图均衡化。
- en: 3.3.3\. KMeans based Quantization Schemes
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 基于KMeans的量化方案
- en: Yet another way of performing non-uniform quantization is to decide bin boundaries
    using clustering in a static manner. In this static-KMeans method (Muller and
    Indiveri, [2015](#bib.bib85)), We first train the neural network with full-precision
    parameters. Then apply KMeans to the weights. After clustering, the value of each
    pixel is set to the value of the center of the cluster it belongs to. We also
    need to store mapping from integers to cluster centers. Given $k$ clusters, we
    only need $\log(k)$ bits to code the clusters.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种执行非均匀量化的方法是通过静态方式使用聚类来决定箱体边界。在这种静态-KMeans方法中（Muller 和 Indiveri，[2015](#bib.bib85)），我们首先用全精度参数训练神经网络。然后对权重应用KMeans。聚类后，每个像素的值被设置为它所属聚类的中心值。我们还需要存储整数到聚类中心的映射。给定$k$个聚类，我们只需$\log(k)$位来编码这些聚类。
- en: A better approach is to perform KMeans clustering during training. In this method (Han
    et al., [2015a](#bib.bib37)), multiple connections (belonging to the same cluster)
    share the same weight, and we fine-tune those shared weights. For the forward
    pass, the cluster index stored for each connection is mapped to a centroid which
    is then used as the weight. For back-propagation, during update, all the gradients
    are grouped by the cluster index and summed together, multiplied by the learning
    rate and subtracted from the shared centroids from last iteration. We use KMeans
    clustering to identify the shared weights for each layer of a trained network,
    so that all the weights that fall into the same cluster will share the same weight.
    Weights are not shared across layers. To calculate the compression rate, given
    $k$ clusters, we only need $\log_{2}k$ bits to encode the index. In general, for
    a network with $n$ connections and each connection is represented with $b$ bits,
    constraining the connections to have only $k$ shared weights will result in a
    compression rate of
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是在训练过程中执行KMeans聚类。在这种方法中（Han et al., [2015a](#bib.bib37)），多个连接（属于同一簇）共享相同的权重，我们对这些共享权重进行微调。在前向传播中，每个连接存储的簇索引被映射到一个质心，然后作为权重使用。在反向传播中，在更新过程中，所有梯度按簇索引分组并求和，乘以学习率，然后从上一轮的共享质心中减去。我们使用KMeans聚类来识别训练网络中每一层的共享权重，以便所有属于同一簇的权重将共享相同的权重。权重在层之间不共享。要计算压缩率，给定$k$个簇，我们只需$\log_{2}k$位来编码索引。一般而言，对于一个具有$n$个连接的网络，每个连接用$b$位表示，将连接限制为仅有$k$个共享权重将导致以下压缩率：
- en: '| (49) |  | $\displaystyle r=\frac{nb}{n\log_{2}k+kb}$ |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| (49) |  | $\displaystyle r=\frac{nb}{n\log_{2}k+kb}$ |  |'
- en: 'There are two other ways of using KMeans for non-uniform quantization: Product
    Quantization (PQ) and Residual Quantization (RQ) (Gong et al., [2014](#bib.bib31)).
    In product quantization (PQ), we partition the vector space into many disjoint
    subspaces, and perform quantization (KMeans) in each subspace. Weight matrix $W$
    is partitioned columnwise: $W=[W^{1},W^{2},...,W^{s}]$ where $W^{i}\in R^{m\times
    n/s}$ assuming $n$ is divisible by $s$. Then we perform KMeans on each submatrix
    $W^{i}$ to obtain clusters $c^{i}_{1},...,c^{i}_{k}$. Thus, we get $s$ codebooks.
    The reconstructed matrix is $\hat{W}=[\hat{W}^{1},\hat{W}^{2},...,\hat{W}^{s}]$
    where $\hat{W}^{i}_{j}$ is the closest centroid $c^{i}_{j}$. PQ can be applied
    to either the x-axis or the y-axis of the matrix. We need to store the cluster
    indexes and codebooks for each subvector. The compression rate for this method
    can be written as follows.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KMeans进行非均匀量化的另外两种方法是：产品量化（PQ）和残差量化（RQ）（Gong et al., [2014](#bib.bib31)）。在产品量化（PQ）中，我们将向量空间划分为多个不相交的子空间，并在每个子空间内执行量化（KMeans）。权重矩阵$W$按列划分：$W=[W^{1},W^{2},...,W^{s}]$，其中$W^{i}\in
    R^{m\times n/s}$，假设$n$可以被$s$整除。然后我们对每个子矩阵$W^{i}$执行KMeans，以获得簇$c^{i}_{1},...,c^{i}_{k}$。因此，我们得到$s$个码本。重构矩阵为$\hat{W}=[\hat{W}^{1},\hat{W}^{2},...,\hat{W}^{s}]$，其中$\hat{W}^{i}_{j}$是最近的质心$c^{i}_{j}$。PQ可以应用于矩阵的x轴或y轴。我们需要存储每个子向量的簇索引和码本。该方法的压缩率可以表示为：
- en: '| (50) |  | $\displaystyle r=\frac{32mn}{32kn+log_{2}(k)ms}$ |  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| (50) |  | $\displaystyle r=\frac{32mn}{32kn+log_{2}(k)ms}$ |  |'
- en: Residual quantization (RQ) is similar. In RQ, we first quantize the vectors
    into k-centers. Next we find out the residuals for each data point ($w-c$) and
    perform KMeans on the residuals. Do it recursively $t$ times. Then the resultant
    weight vectors are calculated as $\hat{W}_{z}=c^{1}_{j}+c^{2}_{j}+...+c^{t}_{j}$
    given we have recursively performed $t$ iterations. We need to store all the codebooks
    for each iteration, which potentially needs large amount of memory. The compression
    rate for this method can be written as follows.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 残差量化（RQ）也类似。在RQ中，我们首先将向量量化为k个中心。接下来，我们计算每个数据点的残差（$w-c$），并对残差执行KMeans。递归进行$t$次。然后，结果权重向量计算为$\hat{W}_{z}=c^{1}_{j}+c^{2}_{j}+...+c^{t}_{j}$，前提是我们已经递归执行了$t$次迭代。我们需要存储每次迭代的所有码本，这可能需要大量内存。该方法的压缩率可以表示为：
- en: '| (51) |  | $\displaystyle r=\frac{m}{tk+log_{2}(k)tn}$ |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| (51) |  | $\displaystyle r=\frac{m}{tk+log_{2}(k)tn}$ |  |'
- en: 3.3.4\. Loss Aware Quantization (LAQ)
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. 失真感知量化（LAQ）
- en: 'Generalizing the loss aware binarization approach (Sec. [3.1.3](#S3.SS1.SSS3
    "3.1.3\. Loss Aware Binarization (LAB) ‣ 3.1\. Binarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey")) (Rastegari et al.,
    [2016](#bib.bib98)), we can perform $k$-bit quantization (Guo et al., [2017](#bib.bib35))
    by attempting to solve the following problem.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '推广损失感知二值化方法（Sec. [3.1.3](#S3.SS1.SSS3 "3.1.3\. Loss Aware Binarization (LAB)
    ‣ 3.1\. Binarized Networks ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey")）（Rastegari et al., [2016](#bib.bib98)），我们可以通过尝试解决以下问题来执行
    $k$-位量化（Guo et al., [2017](#bib.bib35)）。'
- en: '| (52) |  | $\displaystyle\min_{\{\alpha_{i},b_{i}\}_{i=1}^{k}}\left&#124;\left&#124;w-\sum_{i=1}^{k}\alpha_{i}b_{i}\right&#124;\right&#124;^{2}$
    |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| (52) |  | $\displaystyle\min_{\{\alpha_{i},b_{i}\}_{i=1}^{k}}\left&#124;\left&#124;w-\sum_{i=1}^{k}\alpha_{i}b_{i}\right&#124;\right&#124;^{2}$
    |  |'
- en: where $w\in R^{n}$ is the original weight vector, $\alpha_{i}\in R$ and $b_{i}\in\{-1,+1\}^{n}$
    are variables to be learned. This NP-hard problem can be solved using an iterative
    greedy approximation which sequentially minimizes the residue. In each iteration,
    first the residue is computed as
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w\in R^{n}$ 是原始权重向量，$\alpha_{i}\in R$ 和 $b_{i}\in\{-1,+1\}^{n}$ 是待学习的变量。这个
    NP-困难问题可以通过迭代贪婪近似方法来解决，该方法逐步最小化残差。在每次迭代中，首先计算残差为
- en: '| (53) |  | $\displaystyle r_{i-1}=w-\sum_{j=1}^{i-1}\alpha_{j}b_{j},$ |  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| (53) |  | $\displaystyle r_{i-1}=w-\sum_{j=1}^{i-1}\alpha_{j}b_{j},$ |  |'
- en: and then $\alpha_{i}$ and $b_{i}$ are computed as $\alpha_{i}=\frac{1}{n}||r_{i-1}||_{1}$
    and $b_{i}=\text{sign}(r_{i-1})$. Further, refined greedy approximation (Guo et al.,
    [2017](#bib.bib35)) extends this to further decrease the quantization error. In
    the $j^{th}$ iteration after $\alpha_{j}$ and $b_{j}$ have been updated, the method
    adds one extra step to refine all computed $\{\alpha_{i}\}_{i=1}^{j}$ with the
    least squares solution as follows.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算 $\alpha_{i}$ 和 $b_{i}$ 为 $\alpha_{i}=\frac{1}{n}||r_{i-1}||_{1}$ 和 $b_{i}=\text{sign}(r_{i-1})$。进一步的精细化贪婪近似（Guo
    et al., [2017](#bib.bib35)）扩展了这一方法，以进一步减少量化误差。在第 $j^{th}$ 次迭代中，在 $\alpha_{j}$
    和 $b_{j}$ 更新后，该方法增加了一个额外步骤，通过以下最小二乘解来精细化所有计算的 $\{\alpha_{i}\}_{i=1}^{j}$。
- en: '| (54) |  | $\displaystyle[\alpha_{1},...,\alpha_{j}]=((B_{j}^{T}B_{j})^{-1}B_{j}^{T}w)^{T}$
    |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| (54) |  | $\displaystyle[\alpha_{1},...,\alpha_{j}]=((B_{j}^{T}B_{j})^{-1}B_{j}^{T}w)^{T}$
    |  |'
- en: where $B_{j}=[b_{1},...,b_{j}]$. Typically refined greedy is more accurate than
    the greedy approach. In refined greedy approximation, after modification on the
    computed $\alpha$’s, $b$’s are no longer optimal while the method keeps all of
    them fixed. To improve the refined greedy approximation, alternating minimizing
    $\alpha$’s and $b$’s becomes a natural choice. Xu et al. (Xu et al., [2018](#bib.bib133))
    find that only two alternating cycles is good enough to find high precision quantization.
    Further, similar to (Rastegari et al., [2016](#bib.bib98)), for an LSTM, we can
    combine overall network loss minimization with the multi-bit quantization loss
    minimization using this bi-level optimization.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $B_{j}=[b_{1},...,b_{j}]$。通常情况下，精细化贪婪方法比贪婪方法更准确。在精细化贪婪近似中，在对计算出的 $\alpha$
    进行修改后，$b$ 不再是最优的，而该方法保持所有 $b$ 不变。为了改进精细化贪婪近似，交替最小化 $\alpha$ 和 $b$ 成为一种自然选择。Xu
    et al.（Xu et al., [2018](#bib.bib133)）发现，仅两个交替周期足以找到高精度量化。此外，与（Rastegari et al.,
    [2016](#bib.bib98)）类似，对于 LSTM，我们可以通过这种双层优化结合整体网络损失最小化与多位量化损失最小化。
- en: '| (55) |  | $\displaystyle\min_{w,\{\alpha_{i},b_{i}\}_{i=1}^{k}}\text{LSTM}\left(\sum_{i=1}^{k}\alpha_{i}b_{i}\right)$
    |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| (55) |  | $\displaystyle\min_{w,\{\alpha_{i},b_{i}\}_{i=1}^{k}}\text{LSTM}\left(\sum_{i=1}^{k}\alpha_{i}b_{i}\right)$
    |  |'
- en: '| (56) |  | $\displaystyle\text{such that }\{\alpha_{i},b_{i}\}_{i=1}^{k}=\operatorname*{argmin}_{\{\alpha_{i}^{\prime},b_{i}^{\prime}\}_{i=1}^{k}}\left&#124;\left&#124;w-\sum_{i=1}^{k}\alpha_{i}^{\prime}b_{i}^{\prime}\right&#124;\right&#124;^{2}$
    |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| (56) |  | $\displaystyle\text{such that }\{\alpha_{i},b_{i}\}_{i=1}^{k}=\operatorname*{argmin}_{\{\alpha_{i}^{\prime},b_{i}^{\prime}\}_{i=1}^{k}}\left&#124;\left&#124;w-\sum_{i=1}^{k}\alpha_{i}^{\prime}b_{i}^{\prime}\right&#124;\right&#124;^{2}$
    |  |'
- en: 3.3.5\. Quantization for Word Embeddings and Transformers
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5\. 单词嵌入和变换器的量化
- en: Each word vector is typically represented as a 300–500 dimensional vector, with
    each parameter being 32 bits. As there are millions of words, word vectors may
    take up to 3–6 GB of memory/storage. Can we quantize word vectors? We can clearly
    quantize them after training. But, we could also quantize when learning word embeddings.
    For example, Lam et al. (Lam, [2018](#bib.bib60)) perform 1-bit and 2-bit quantization
    while performing word2vec (Mikolov et al., [2013](#bib.bib81)) training using
    the Continuous Bag of Words (CBOW) method. They observe that quantization while
    training leads to better results compared to quantization after training.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词向量通常表示为一个300–500维的向量，每个参数为32位。由于词汇量庞大，词向量可能占用高达3–6 GB的内存/存储。我们能否对词向量进行量化？我们可以在训练后明确地进行量化。但我们也可以在学习词嵌入时进行量化。例如，Lam
    et al. (Lam, [2018](#bib.bib60)) 在使用连续词袋模型 (CBOW) 方法进行word2vec (Mikolov et al.,
    [2013](#bib.bib81)) 训练时执行了1位和2位量化。他们观察到，在训练过程中进行量化比在训练后进行量化会得到更好的结果。
- en: 'Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) applied BS-Fixed and
    BS-Flexible binary quantization to Transformer models. They observed that the
    Transformer architecture is highly resistant to quantization, and is able to match
    the original model up to a 4-bit representation. Simple iterative pruning is much
    worse compared to quantization. Lastly, Shen et al. (Shen et al., [2019](#bib.bib105))
    propose mixed-precision quantization for BERT based on the observation that different
    encoder layers should use different number of bits for quantization. Layers that
    exhibit flatter curvature of the loss gradient surface can be quantized to lower
    bit precision. Thus, they use different number of bits at different levels of
    granularity: layers, attention heads and groups of neurons. They observe that
    quantizing embedding layers with 8 bits and other weight matrices with 2–4 bits
    leads to results comparable with full-precision BERT.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) 将BS-Fixed和BS-Flexible二进制量化应用于Transformer模型。他们观察到Transformer架构对量化具有很高的抵抗力，并且能够匹配原始模型高达4位表示。与量化相比，简单的迭代修剪效果要差得多。最后，Shen
    et al. (Shen et al., [2019](#bib.bib105)) 提出了基于不同编码器层应使用不同位数进行量化的观察的BERT混合精度量化方法。表现出损失梯度面较平坦的层可以量化为较低的位数。因此，他们在不同的粒度级别：层、注意力头和神经元组上使用不同的位数。他们观察到，用8位量化嵌入层和用2–4位量化其他权重矩阵的结果与全精度BERT相当。
- en: 3.4\. Summary
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 总结
- en: '| Task | Dataset | Model | Method | Eval | Bits (weights) | Bits (activation)
    | Metric |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 模型 | 方法 | 评估 | 权重（位数） | 激活（位数） | 指标 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Language modeling | IMDB | GRU | Uniform Q. (He et al., [2016](#bib.bib41))
    | 0.882; 0.905 | 4 | 4 | Acc (H) |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | IMDB | GRU | 均匀量化 (He et al., [2016](#bib.bib41)) | 0.882; 0.905 |
    4 | 4 | Acc (H) |'
- en: '| Language modeling | Linux Kernel | LSTM | BinaryConnect (Courbariaux et al.,
    [2015](#bib.bib18)) | 3.532; 1.329 | 1 | FP | CE (L) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Linux 内核 | LSTM | BinaryConnect (Courbariaux et al., [2015](#bib.bib18))
    | 3.532; 1.329 | 1 | FP | CE (L) |'
- en: '| Language modeling | Linux Kernel | LSTM | Loss Aware B. (Hou et al., [2016](#bib.bib46))
    | 1.305/1.409; 1.329 | 1 | FP/1 | CE (L) |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Linux 内核 | LSTM | 损失感知 B. (Hou et al., [2016](#bib.bib46)) | 1.305/1.409;
    1.329 | 1 | FP/1 | CE (L) |'
- en: '| Language modeling | Linux Kernel | LSTM | BNN (Hubara et al., [2017](#bib.bib48))
    | 3.624; 1.329 | 1 | 1 | CE (L) |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Linux 内核 | LSTM | BNN (Hubara et al., [2017](#bib.bib48)) | 3.624;
    1.329 | 1 | 1 | CE (L) |'
- en: '| Language modeling | Linux Kernel | LSTM | BWN (Rastegari et al., [2016](#bib.bib98))
    | 1.307; 1.329 | 1 | FP | CE (L) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Linux 内核 | LSTM | BWN (Rastegari et al., [2016](#bib.bib98)) | 1.307;
    1.329 | 1 | FP | CE (L) |'
- en: '| Language modeling | PTB | GRU | Uniform Q. (He et al., [2016](#bib.bib41))
    | 102; 100 | 4 | 4 | PPW (L) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | GRU | 均匀量化 (He et al., [2016](#bib.bib41)) | 102; 100 | 4 |
    4 | PPW (L) |'
- en: '| Language modeling | PTB | GRU | Balanced Q. (Zhou et al., [2017](#bib.bib144))
    | 116; 100 | 4 | 4 | PPW (L) |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | GRU | 平衡量化 (Zhou et al., [2017](#bib.bib144)) | 116; 100 | 4
    | 4 | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | Greedy (Guo et al., [2017](#bib.bib35))
    | 118.9; 89.8 | 2 | FP | PPW (L) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | 贪心算法 (Guo et al., [2017](#bib.bib35)) | 118.9; 89.8 |
    2 | FP | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | Refined Loss Aware (Guo et al., [2017](#bib.bib35))
    | 95.6; 89.8 | 2 | 3 | PPW (L) |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | 精炼的损失感知 (Guo et al., [2017](#bib.bib35)) | 95.6; 89.8
    | 2 | 3 | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | Uniform Q. (He et al., [2016](#bib.bib41))
    | 152/114; 109 | 2/4 | 2/4 | PPW (L) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | 均匀量化 (He et al., [2016](#bib.bib41)) | 152/114; 109 |
    2/4 | 2/4 | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | BNN (Hubara et al., [2017](#bib.bib48))
    | 100; 97 | 4 | 4 | PPW (L) |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | BNN （Hubara 等，[2017](#bib.bib48)） | 100; 97 | 4 | 4 |
    PPW (L) |'
- en: '| Language modeling | PTB | LSTM | HitNet (Wang et al., [2018](#bib.bib128))
    | 110.3; 97.2 | 2 | 2 | PPW (L) |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | HitNet （Wang 等，[2018](#bib.bib128)） | 110.3; 97.2 | 2
    | 2 | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 103.1/91.4; 89.8 | 2/4 | FP | PPW (L) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | Alternating LAQ （Xu 等，[2018](#bib.bib133)） | 103.1/91.4;
    89.8 | 2/4 | FP | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 91.9; 89.8 | 2 | 3 | PPW (L) |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | Alternating LAQ （Xu 等，[2018](#bib.bib133)） | 91.9; 89.8
    | 2 | 3 | PPW (L) |'
- en: '| Language modeling | PTB | LSTM | Balanced Q. (Zhou et al., [2017](#bib.bib144))
    | 126/123; 106 | 2 | 2/3 | PPW (L) |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | LSTM | Balanced Q. （Zhou 等，[2017](#bib.bib144)） | 126/123; 106
    | 2 | 2/3 | PPW (L) |'
- en: '| Language modeling | Text-8 | LSTM | Refined Loss Aware (Guo et al., [2017](#bib.bib35))
    | 122.3; 101.1 | 2 | 3 | PPW (L) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Text-8 | LSTM | Refined Loss Aware （Guo 等，[2017](#bib.bib35)） | 122.3;
    101.1 | 2 | 3 | PPW (L) |'
- en: '| Language modeling | Text-8 | LSTM | HitNet (Wang et al., [2018](#bib.bib128))
    | 169.1; 151.4 | 2 | 2 | PPW (L) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Text-8 | LSTM | HitNet （Wang 等，[2018](#bib.bib128)） | 169.1; 151.4
    | 2 | 2 | PPW (L) |'
- en: '| Language modeling | Text-8 | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 105.1; 101.1 | 2 | 3 | PPW (L) |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Text-8 | LSTM | Alternating LAQ （Xu 等，[2018](#bib.bib133)） | 105.1;
    101.1 | 2 | 3 | PPW (L) |'
- en: '| Language modeling | Text-8 | RNN | Exponential Q. (Ott et al., [2016](#bib.bib90))
    | 1.639; 1.588 | 2 | FP | BPC (L) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Text-8 | RNN | Exponential Q. （Ott 等，[2016](#bib.bib90)） | 1.639;
    1.588 | 2 | FP | BPC (L) |'
- en: '| Language modeling | War and Peace | LSTM | BinaryConnect (Courbariaux et al.,
    [2015](#bib.bib18)) | 2.942; 1.268 | 1 | FP | CE (L) |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 战争与和平 | LSTM | BinaryConnect （Courbariaux 等，[2015](#bib.bib18)） |
    2.942; 1.268 | 1 | FP | CE (L) |'
- en: '| Language modeling | War and Peace | LSTM | Loss Aware B. (Hou et al., [2016](#bib.bib46))
    | 1.291/1.376; 1.268 | 1 | FP/1 | CE (L) |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 战争与和平 | LSTM | Loss Aware B. （Hou 等，[2016](#bib.bib46)） | 1.291/1.376;
    1.268 | 1 | FP/1 | CE (L) |'
- en: '| Language modeling | War and Peace | LSTM | BNN (Hubara et al., [2017](#bib.bib48))
    | 3.05; 1.268 | 1 | 1 | CE (L) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 战争与和平 | LSTM | BNN （Hubara 等，[2017](#bib.bib48)） | 3.05; 1.268 | 1
    | 1 | CE (L) |'
- en: '| Language modeling | War and Peace | LSTM | BWN (Rastegari et al., [2016](#bib.bib98))
    | 1.313; 1.268 | 1 | FP | CE (L) |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 战争与和平 | LSTM | BWN （Rastegari 等，[2016](#bib.bib98)） | 1.313; 1.268
    | 1 | FP | CE (L) |'
- en: '| Language modeling | Wikidata-2 | LSTM | HitNet (Wang et al., [2018](#bib.bib128))
    | 126.72; 114.37 | 2 | 2 | PPW (L) |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikidata-2 | LSTM | HitNet （Wang 等，[2018](#bib.bib128)） | 126.72;
    114.37 | 2 | 2 | PPW (L) |'
- en: '| Language modeling | WikiText-2 | LSTM | Refined Loss Aware (Guo et al., [2017](#bib.bib35))
    | 105.8; 114.37 | 2 | 3 | PPW (L) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | WikiText-2 | LSTM | Refined Loss Aware （Guo 等，[2017](#bib.bib35)）
    | 105.8; 114.37 | 2 | 3 | PPW (L) |'
- en: '| Language modeling | WikiText-2 | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 102.7; 100.1 | 2 | 3 | PPW (L) |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | WikiText-2 | LSTM | Alternating LAQ （Xu 等，[2018](#bib.bib133)） | 102.7;
    100.1 | 2 | 3 | PPW (L) |'
- en: '| Named Entity Recognition | CoNLL-03 | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 91.06; 95 | 2w8e | 8 | F1 (H) |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别 | CoNLL-03 | BERT-base | QBERT （Shen 等，[2019](#bib.bib105)） | 91.06;
    95 | 2w8e | 8 | F1 (H) |'
- en: '| Named Entity Recognition | CoNLL-03 | BERT-base | Mixed-precision Q. (Shen
    et al., [2019](#bib.bib105)) | 94.37; 95 | 2-3w8e | 8 | F1 (H) |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别 | CoNLL-03 | BERT-base | Mixed-precision Q. （Shen 等，[2019](#bib.bib105)）
    | 94.37; 95 | 2-3w8e | 8 | F1 (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | BS-Fixed/BS-Flexible (Cheong
    and Daniel, [2019](#bib.bib16)) | 11.61/12.11; 28.09 | 1 | FP | BLEU (H) |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | BS-Fixed/BS-Flexible （Cheong
    和 Daniel，[2019](#bib.bib16)） | 11.61/12.11; 28.09 | 1 | FP | BLEU (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | K-Means 1/4-bit Q. (Cheong
    and Daniel, [2019](#bib.bib16)) | 12.07/27.65; 28.09 | 1 | FP | BLEU (H) |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | K-Means 1/4-bit Q. （Cheong
    和 Daniel，[2019](#bib.bib16)） | 12.07/27.65; 28.09 | 1 | FP | BLEU (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | K-Means 1-bit att-Q. (Cheong
    and Daniel, [2019](#bib.bib16)) | 24.96; 28.09 | 1 | FP | BLEU (H) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | K-Means 1-bit att-Q. （Cheong
    和 Daniel，[2019](#bib.bib16)） | 24.96; 28.09 | 1 | FP | BLEU (H) |'
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | BS-Flexible 1-bit att-Q. (Cheong
    and Daniel, [2019](#bib.bib16)) | 25.54; 28.09 | 1 | FP | BLEU (H) |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | BS-Flexible 1-bit att-Q. （Cheong
    和 Daniel，[2019](#bib.bib16)） | 25.54; 28.09 | 1 | FP | BLEU (H) |'
- en: '| Question answering | SQuAD | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 79.6; 88.69 | 2w8e | 8 | F1 (H) |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD | BERT-base | QBERT （Shen 等，[2019](#bib.bib105)） | 79.6; 88.69
    | 2w8e | 8 | F1 (H) |'
- en: '| Question answering | SQuAD | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 86.95; 88.69 | 2-3w8e | 8 | F1 (H) |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD | BERT-base | Mixed-precision Q. (Shen et al., [2019](#bib.bib105))
    | 86.95; 88.69 | 2-3w8e | 8 | F1 (H) |'
- en: '| Question answering | SQuAD | Facebook’s DrQA | BS-Fixed (Lam, [2018](#bib.bib60))
    | 77.04; 75.28 | 2 | FP | F1 (H) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD | Facebook’s DrQA | BS-Fixed (Lam, [2018](#bib.bib60)) | 77.04;
    75.28 | 2 | FP | F1 (H) |'
- en: '| Sentiment analysis | IMDB | LSTM | Gaussian Q. (Alom et al., [2018](#bib.bib2))
    | 79.64; 82.87 | 1 | FP | Acc (H) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | LSTM | Gaussian Q. (Alom et al., [2018](#bib.bib2)) | 79.64;
    82.87 | 1 | FP | Acc (H) |'
- en: '| Sentiment analysis | IMDB | LSTM | Gaussian T./B. (Alom et al., [2018](#bib.bib2))
    | 76.86/76.25; 82.87 | 2 | FP | Acc (H) |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | LSTM | Gaussian T./B. (Alom et al., [2018](#bib.bib2)) | 76.86/76.25;
    82.87 | 2 | FP | Acc (H) |'
- en: '| Sentiment analysis | SST-2 | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 84.63; 93 | 2w8e | 8 | Acc (H) |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST-2 | BERT-base | QBERT (Shen et al., [2019](#bib.bib105)) | 84.63;
    93 | 2w8e | 8 | Acc (H) |'
- en: '| Sentiment analysis | SST-2 | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 92.08; 93 | 2-3w8e | 8 | Acc (H) |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST-2 | BERT-base | Mixed-precision Q. (Shen et al., [2019](#bib.bib105))
    | 92.08; 93 | 2-3w8e | 8 | Acc (H) |'
- en: '| Speech recognition | TIDIGITS | GRU | Pow2 T. (Ott et al., [2016](#bib.bib90))
    | 99.18; 99.1 | 2 | FP | Acc (H) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | TIDIGITS | GRU | Pow2 T. (Ott et al., [2016](#bib.bib90)) | 99.18;
    99.1 | 2 | FP | Acc (H) |'
- en: '| Speech recognition | TIMIT | 4-layer MLP | Loss Aware T. (Hwang and Sung,
    [2014](#bib.bib49)) | 29.97/28.35; 26.24 | 1/2 | 1 | FER (L) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | TIMIT | 4层 MLP | Loss Aware T. (Hwang and Sung, [2014](#bib.bib49))
    | 29.97/28.35; 26.24 | 1/2 | 1 | FER (L) |'
- en: '| Speech recognition | WSJ | 4-layer BiLSTM | Pow2 T. (Ott et al., [2016](#bib.bib90))
    | 10.49; 11.16 | 2 | FP | WER (L) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | WSJ | 4层 BiLSTM | Pow2 T. (Ott et al., [2016](#bib.bib90)) | 10.49;
    11.16 | 2 | FP | WER (L) |'
- en: '| Textual entailment | MNLI | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 77.02; 84.4 | 2w8e | 8 | Acc (H) |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI | BERT-base | QBERT (Shen et al., [2019](#bib.bib105)) | 77.02;
    84.4 | 2w8e | 8 | Acc (H) |'
- en: '| Textual entailment | MNLI | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 82.29; 84.4 | 2-3w8e | 8 | Acc (H) |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI | BERT-base | Mixed-precision Q. (Shen et al., [2019](#bib.bib105))
    | 82.29; 84.4 | 2-3w8e | 8 | Acc (H) |'
- en: '| Textual entailment | MNLI-m | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 76.56; 84 | 2w8e | 8 | Acc (H) |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI-m | BERT-base | QBERT (Shen et al., [2019](#bib.bib105)) | 76.56;
    84 | 2w8e | 8 | Acc (H) |'
- en: '| Textual entailment | MNLI-m | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 81.75; 84 | 2-3w8e | 8 | Acc (H) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | MNLI-m | BERT-base | Mixed-precision Q. (Shen et al., [2019](#bib.bib105))
    | 81.75; 84 | 2-3w8e | 8 | Acc (H) |'
- en: '| Word similarity | M. Turk | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.602; 0.617 | 2 | FP | CHR (H) |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似度 | M. Turk | 词嵌入 | BS-Fixed (Lam, [2018](#bib.bib60)) | 0.602; 0.617
    | 2 | FP | CHR (H) |'
- en: '| Word similarity | MEN | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.764; 0.745 | 2 | FP | CHR (H) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似度 | MEN | 词嵌入 | BS-Fixed (Lam, [2018](#bib.bib60)) | 0.764; 0.745 | 2
    | FP | CHR (H) |'
- en: '| Word similarity | Rare Words | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.362; 0.4 | 2 | FP | CHR (H) |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似度 | 稀有词汇 | 词嵌入 | BS-Fixed (Lam, [2018](#bib.bib60)) | 0.362; 0.4 | 2
    | FP | CHR (H) |'
- en: '| Word similarity | SimLex | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.387; 0.358 | 2 | FP | CHR (H) |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似度 | SimLex | 词嵌入 | BS-Fixed (Lam, [2018](#bib.bib60)) | 0.387; 0.358
    | 2 | FP | CHR (H) |'
- en: '| Word similarity | WordSim Relatedness | Word embeddings | BS-Fixed (Lam,
    [2018](#bib.bib60)) | 0.594; 0.529 | 2 | FP | CHR (H) |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似度 | WordSim 相关性 | 词嵌入 | BS-Fixed (Lam, [2018](#bib.bib60)) | 0.594; 0.529
    | 2 | FP | CHR (H) |'
- en: '| Word similarity | WordSim Similarity | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.752; 0.741 | 2 | FP | CHR (H) |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似度 | WordSim 相似度 | 词嵌入 | BS-Fixed (Lam, [2018](#bib.bib60)) | 0.752; 0.741
    | 2 | FP | CHR (H) |'
- en: Table 2\. Comparison of various quantization methods (sorted by Task and then
    Dataset). Q.=Quantization, B.=Binarization, T.=Ternarization, PPW=Perplexity per
    word, BPC=Bits per character, CE=cross-entropy, FER=frame error rate, CHR=correlation
    with human rankings. FP=full precision (32 bits). For (Lam, [2018](#bib.bib60)),
    we report results with word embedding dimensions set to 1000\. In the metric column,
    H means high is better while L means low is better. For quantization of the BERT-base
    model (Shen et al., [2019](#bib.bib105)), we report number of bits used for encoders
    as well as for embeddings. ‘2-3w8e’ means 2 or 3 bits were used for encoder weights
    while 8 bits were used for embeddings. For NMT results by Cheong et al. (Cheong
    and Daniel, [2019](#bib.bib16)), “att-Q” means only attention layers were quantized.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 各种量化方法的比较（按任务和数据集排序）。Q.=量化，B.=二值化，T.=三值化，PPW=每词困惑度，BPC=每字符比特，CE=交叉熵，FER=帧错误率，CHR=与人工排名的相关性。FP=全精度（32位）。对于（Lam,
    [2018](#bib.bib60)），我们报告了词嵌入维度设置为1000的结果。在指标列中，H表示高更好，而L表示低更好。对于BERT-base模型（Shen
    et al., [2019](#bib.bib105)）的量化，我们报告了编码器和嵌入所用的比特数。‘2-3w8e’表示编码器权重使用了2或3位，而嵌入使用了8位。对于Cheong等人（Cheong
    and Daniel, [2019](#bib.bib16)）的NMT结果，“att-Q”表示仅对注意力层进行了量化。
- en: Ott et al. (Ott et al., [2016](#bib.bib90)) observed that the weight binarization
    methods do not work with RNNs. Hubara et al. (Hubara et al., [2017](#bib.bib48))
    were the first to attempt to quantize both weights and activations by trying to
    evaluate the accuracy of quantized recurrent models trained on the Penn Treebank
    dataset. Similar to (Ott et al., [2016](#bib.bib90)), Hubara et al. (Hubara et al.,
    [2017](#bib.bib48)) found that binarization of weight matrices lead to large accuracy
    degradation. Later techniques like the one by Xu et al. (Xu et al., [2018](#bib.bib133))
    with 2 bits for weights and 3 bits for activations showed better results.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Ott等人（Ott et al., [2016](#bib.bib90)）观察到权重二值化方法在RNN中不起作用。Hubara等人（Hubara et
    al., [2017](#bib.bib48)）首次尝试通过评估在Penn Treebank数据集上训练的量化递归模型的准确性来量化权重和激活。与（Ott
    et al., [2016](#bib.bib90)）类似，Hubara等人（Hubara et al., [2017](#bib.bib48)）发现权重矩阵的二值化会导致准确性的大幅下降。后来如Xu等人（Xu
    et al., [2018](#bib.bib133)）的技术使用2位权重和3位激活显示了更好的结果。
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3.4\. Summary ‣ 3\. Quantization ‣ Compression
    of Deep Learning Models for Text: A Survey") compares various quantization methods
    across different tasks and datasets. Accuracy of both the original and the quantized
    model are shown. Also, we report number of bits used for weights (which indicate
    the model size) as well as activations. For the same task, dataset and model combination,
    different papers report different accuracy of the full precision model because
    of slight changes in training hyper-parameters; hence we report accuracy of full
    precision model for each row.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S3.T2 "Table 2 ‣ 3.4\. Summary ‣ 3\. Quantization ‣ Compression of Deep
    Learning Models for Text: A Survey")比较了不同任务和数据集中的各种量化方法。显示了原始模型和量化模型的准确性。此外，我们还报告了权重（这表明模型大小）和激活的比特数。对于相同的任务、数据集和模型组合，不同的论文由于训练超参数的微小变化报告了不同的全精度模型准确性，因此我们报告了每一行的全精度模型准确性。'
- en: For language modeling, PTB, Text-8, WikiText-2, Linux Kernel, IMDB and “War
    and Peace” are the popular datasets. Across all the datasets, loss aware binarization
    outperforms other weight binarization schemes. On the Linux Kernel dataset, it
    is even better than the full-precision network. BinaryConnect does not work well
    here because of the problem of exploding gradients. On PTB, Xu et al.’s Alternating
    LAQ (Xu et al., [2018](#bib.bib133)) with 2 bits for weights and 3 bits for activations
    leads to an LSTM which is just 2.1 points worse in terms of perplexity per word.
    By 3-bit quantization, Alternating LAQ can achieve $\sim$10.5x memory saving and
    $\sim$3× real inference acceleration. Uniform and Balanced quantization are rule-based
    and not aim at minimizing the error. Balanced quantization proposed by Zhou et
    al. (Zhou et al., [2017](#bib.bib144)) performs better than HitNet (Wang et al.,
    [2018](#bib.bib128)) and uniform quantization (He et al., [2016](#bib.bib41)).
    Balanced quantization leads to better results compared to unbalanced counterparts,
    especially when quantizing to 2-bit weights. However, for 4-bit weights, there
    is no clear gap between scaling by mean and scaling by max (i.e. balanced and
    unbalanced quantization).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言建模，PTB、Text-8、WikiText-2、Linux Kernel、IMDB 和《战争与和平》是流行的数据集。在所有数据集中，损失感知二值化优于其他权重二值化方案。在
    Linux Kernel 数据集中，它甚至比全精度网络更好。BinaryConnect 在这里效果不佳，因为梯度爆炸的问题。在 PTB 数据集上，Xu 等人的交替
    LAQ（Xu 等， [2018](#bib.bib133)）使用 2 位权重和 3 位激活，从而得到的 LSTM 在每个单词的困惑度上仅差 2.1 分。通过
    3 位量化，交替 LAQ 可以实现 $\sim$10.5 倍的内存节省和 $\sim$3× 的实际推理加速。均匀量化和均衡量化是基于规则的，并不旨在最小化误差。由
    Zhou 等人提出的均衡量化（Zhou 等， [2017](#bib.bib144)）表现优于 HitNet（Wang 等， [2018](#bib.bib128)）和均匀量化（He
    等， [2016](#bib.bib41)）。与不均衡的量化方案相比，均衡量化在量化到 2 位权重时表现更好。然而，对于 4 位权重，按均值缩放和按最大值缩放（即均衡和不均衡量化）之间没有明显差距。
- en: Across multiple tasks like named entity recognition with CoNLL-03, question
    answering with SQuAD, sentiment analysis with SST-2, textual entailment using
    MNLI, Shen et al. (Shen et al., [2019](#bib.bib105)) show that mixed precision
    quantization (where different number of bits are used for different groups of
    neurons – 128 groups in each layer) of BERT is better than QBERT. The reason behind
    this is the discrepancy (in QBERT) that not all the layers have the same sensitivity
    to quantization. For more sensitive layers, higher bit precision needs to be set,
    while for layers that are less sensitive, 2-bits setting is already sufficient.
    With only additional 5MB memory storage, 2/3-bits mixed-precision Q-BERT is able
    to retain the performance drop within 2.3% for MNLI, SQuAD and 1.1% for SST-2,
    CoNLL-03, with up to 13x compression ratio in weights. For machine translation,
    Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) observe that Transformer
    architecture is highly resistant to quantization (unlike to pruning), and are,
    in essence, able to match the original model up to a 4-bit representation. Binary
    Scheme Flexible outperforms 1-bit k-means in both pure 1-bit compression and quantizing
    only the attention layers, suggesting not tying the weights to particular centroids
    improves performance, and outperform Binary Scheme Fixed, indicating learning
    the values to be a superior method. After binarizing only the attention layers,
    we are still able to recover 90% of the model’s performance. Lam et al. (Lam,
    [2018](#bib.bib60)) experimented with quantization of word embeddings and showed
    that 2-bit quantized word vectors outperform full precision vectors on word similarity
    tasks, but do worse on word analogy tasks. Intuitively, they reason that full
    precision Word2Vec is prone to overfitting with increased epochs of training;
    quantized training does not seem to suffer as much from this. For sentiment analysis
    on IMDB, Alom et al. (Alom et al., [2018](#bib.bib2)) show that quantization to
    4 bits is better than to 3 or 2 bits, which is expected. They also show that the
    normal distribution shows better performance against uniform distribution with
    quantized weights. For speech recognition, Ott et al. (Ott et al., [2016](#bib.bib90))
    show that pow-2 ternarization is the best.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个任务中，如使用 CoNLL-03 进行命名实体识别、使用 SQuAD 进行问答、使用 SST-2 进行情感分析、使用 MNLI 进行文本蕴含，Shen
    等人（Shen et al., [2019](#bib.bib105)）展示了 BERT 的混合精度量化（在每层中对不同组的神经元使用不同的位数—每层 128
    组）优于 QBERT。其原因在于 QBERT 中的差异，即并非所有层对量化的敏感度相同。对于更敏感的层，需要设置更高的位精度，而对于不太敏感的层，2 位设置已经足够。仅需额外
    5MB 的内存存储，2/3 位混合精度 Q-BERT 能将 MNLI、SQuAD 的性能下降保持在 2.3% 以内，以及 SST-2、CoNLL-03 的
    1.1% 以内，同时权重压缩比高达 13 倍。对于机器翻译，Cheong 等人（Cheong and Daniel, [2019](#bib.bib16)）观察到
    Transformer 架构对量化具有较高的抗性（不同于剪枝），本质上能够匹配原始模型的 4 位表示。Binary Scheme Flexible 在纯 1
    位压缩和仅量化注意力层中均优于 1 位 k-means，表明不将权重绑定到特定的质心上可以提高性能，且优于 Binary Scheme Fixed，说明学习这些值是一种更优的方法。在仅对注意力层进行二值化后，我们仍能恢复模型
    90% 的性能。Lam 等人（Lam, [2018](#bib.bib60)）对词嵌入的量化进行了实验，显示 2 位量化的词向量在词相似性任务中优于全精度向量，但在词类比任务中表现较差。他们直观地推测，全精度
    Word2Vec 随训练轮次的增加容易过拟合，而量化训练似乎不容易出现这种情况。对于 IMDB 上的情感分析，Alom 等人（Alom et al., [2018](#bib.bib2)）显示，量化到
    4 位比量化到 3 位或 2 位效果更好，这符合预期。他们还表明，相比于均匀分布，量化权重的正态分布表现更佳。对于语音识别，Ott 等人（Ott et al.,
    [2016](#bib.bib90)）显示，pow-2 三值化是最好的。
- en: Cheong et al (Cheong and Daniel, [2019](#bib.bib16)) were the first to quantize
    Transformers. They observed that in the last attention layer of the decoder over
    the encoder hidden states, the attention distribution of the original and 4-bit
    model are highly similar, indicating 4 bit weights, i.e weights that take on one
    of 16 values, is enough to get the full effects of attention. Attention distributions
    in the encoder layers of the Transformer for the original and 4-bit models are
    almost indistinguishable from one another. This again highlights the idea that
    self-attention is highly resistant to quantization and could be heavily compressed.
    Later Shen et al. (Shen et al., [2019](#bib.bib105)) showed that comparable performance
    to full precision BERT can be achieved with at most 2.3% performance degradation
    across many tasks, even with ultra-low precision quantization down to 2 bits,
    corresponding up to 13x compression of the model parameters, and up to 4x compression
    of the embedding table as well as activations.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Cheong 等人（Cheong 和 Daniel，[2019](#bib.bib16)）是首次对 Transformers 进行量化的。他们观察到，在解码器的最后一个注意力层中，关于编码器隐藏状态的注意力分布在原始模型和
    4 位模型之间高度相似，表明 4 位权重，即取 16 个值之一的权重，足以获得注意力的全部效果。Transformer 的编码器层中的注意力分布在原始模型和
    4 位模型之间几乎无法区分。这再次强调了自注意力对量化具有高度抗性，并且可以被大量压缩。后来 Shen 等人（Shen 等人，[2019](#bib.bib105)）显示，与全精度
    BERT 相当的性能可以在许多任务中实现，最多仅有 2.3% 的性能下降，即使是超低精度量化到 2 位，也相当于模型参数压缩高达 13 倍，以及嵌入表和激活的压缩高达
    4 倍。
- en: Overall, quantization performs model compression by reducing the number of bits
    per weight value. Binary quantization does not work well by itself for text based
    neural models. But ternary and higher-bit quantization lead to significant model
    size reduction without loss in accuracy across tasks. One consideration for quantization
    is that 3-bit quantized execution is typically not supported in hardware. It is
    however possible to load 3-bit quantized values and cast them to higher bit precision
    such as 4 or 8 bits in the execution units. This would still have the benefit
    of reduced memory volume to/from DRAM. Non-uniform quantization methods like balanced
    quantization or KMeans based quantization methods are better than uniform quantization
    methods. Loss aware quantization done while training is better than static loss-unaware
    quantization. Mixed-precision quantization combined with pruning is highly effective
    for Transformer based models.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，量化通过减少每个权重值的位数来执行模型压缩。二值量化对于基于文本的神经模型本身效果不好。但三值量化和更高位数的量化可以显著减少模型大小，而不会在任务中丧失准确性。量化的一个考虑因素是，3
    位量化执行通常在硬件中不被支持。然而，可以加载 3 位量化值并将其转换为更高的位精度，如 4 位或 8 位。这仍然具有减少 DRAM 内存量的好处。像平衡量化或基于
    KMeans 的量化方法这样的非均匀量化方法优于均匀量化方法。在训练时进行的损失感知量化优于静态无损失感知量化。混合精度量化结合剪枝对基于 Transformer
    的模型非常有效。
- en: 4\. Knowledge Distillation (KD)
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 知识蒸馏 (KD)
- en: KD methods are the most popular model compression methods for Transformer networks.
    Also called student-teacher networks, the main idea is to first train a deep teacher
    network, and then learn a shallow student network that mimics the teacher. After
    training, the student model is deployed. What information (“dark knowledge”) from
    the teacher can be used to train the student? What loss functions can be used
    to ensure right flow of information from teacher to student? Can we have an ensemble
    of teachers, or teacher assistants or rather fellow students who can train the
    student? We discuss these aspects in this section.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: KD 方法是 Transformer 网络中最流行的模型压缩方法。也称为学生-教师网络，其主要思想是首先训练一个深层的教师网络，然后学习一个模仿教师的浅层学生网络。训练完成后，学生模型被部署。那么，从教师那里可以用来训练学生的是什么信息（“黑暗知识”）？可以使用哪些损失函数来确保从教师到学生的信息流动正确？我们可以拥有一组教师、教师助手，还是其他能够训练学生的同学？我们在本节中讨论了这些方面。
- en: 4.1\. Various Distillation Architectures
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 各种蒸馏架构
- en: 'Ba and Caruna (Ba and Caruana, [2014](#bib.bib4)) proposed Student Teacher
    networks (or mimic models) where the student uses the logits before softmax from
    the teacher network for training (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various
    Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of
    Deep Learning Models for Text: A Survey")(A)). The student model is not trained
    on the original labels; it is trained to learn the function that was learned by
    the teacher model. Thus, the student model is optimized to minimize the L2 loss
    between the teacher logits and the student logits across all training instances.
    Such distilled student models are more accurate than the same shallow student
    trained directly on the original labeled training data mainly because: (1) Teacher
    removes noisy labels, if any. (2) The uncertainty from the teacher is more informative
    to the student than the original 0/1 labels. (3) The original targets may depend
    in part on features not available as inputs for learning, but the student sees
    targets that depend only on the input features. The dependence on unavailable
    features has been eliminated by filtering targets through the teacher.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ba 和 Caruna（Ba 和 Caruana，[2014](#bib.bib4)）提出了学生教师网络（或模拟模型），其中学生使用来自教师网络的 logits
    进行训练（见图 [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\.
    Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text: A
    Survey")(A)）。学生模型不是基于原始标签进行训练的，而是训练去学习教师模型所学的函数。因此，学生模型被优化以最小化教师 logits 和学生 logits
    在所有训练实例上的 L2 损失。这种蒸馏后的学生模型比直接在原始标记训练数据上训练的同样浅层学生更准确，主要原因有：（1）教师去除了噪声标签（如果有的话）。
    （2）来自教师的 uncertainty 对学生的帮助大于原始的 0/1 标签。 （3）原始目标可能部分依赖于不可用的特征，但学生看到的目标仅依赖于输入特征。通过教师过滤，依赖于不可用特征的问题被消除了。'
- en: Yet another way of utilizing logits is to have the student learn from noisy
    teacher logits (Sau and Balasubramanian, [2016](#bib.bib103)). After obtaining
    logits from the teacher, Gaussian noise with mean 0 and standard deviation $\sigma$
    is added to teacher’s logits. This perturbation can be applied to samples selected
    with probability $\alpha$. The perturbed outputs produce the effect of a regularizer.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种利用 logits 的方式是让学生从噪声教师 logits 中学习（Sau 和 Balasubramanian，[2016](#bib.bib103)）。在从教师处获取
    logits 后，向教师的 logits 中添加均值为 0 和标准差为 $\sigma$ 的高斯噪声。这种扰动可以应用于以概率 $\alpha$ 选择的样本。扰动后的输出产生了正则化的效果。
- en: '![Refer to caption](img/b5f5552f20332eafbca8058c6457e48d.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b5f5552f20332eafbca8058c6457e48d.png)'
- en: Figure 4\. Different Types of Knowledge Distillation methods.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 不同类型的知识蒸馏方法。
- en: 'While Ba and Caruna (Ba and Caruana, [2014](#bib.bib4)) suggested using only
    the logits, Hinton et al. (Hinton et al., [2015](#bib.bib44)) suggested training
    the student by minimizing the cross entropy loss between the teacher softmax output
    and the student softmax output, besides minimizing the cross entropy between student
    prediction and actual label. The first part is called the soft loss and the second
    one is called the hard loss. Typically hard loss is given much lower weight compared
    to the soft loss term. To make the softmax output non-peaked and thereby transfer
    more useful information from teacher to student, softmax with temperature $>$1
    should be used. The same temperature should be used for training both the teacher
    and the student, but after the student has been trained the temperature can be
    set to 1 at test time. Besides logits and softmax output, Sobolev training for
    neural networks is a method for incorporating target derivatives in addition to
    the target values while training student network (see Fig. [4](#S4.F4 "Figure
    4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey")(C)). Czarnecki et al. (Czarnecki
    et al., [2017](#bib.bib19)) experiment with first two derivatives of the targets.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管 Ba 和 Caruna（Ba 和 Caruana，[2014](#bib.bib4)）建议只使用 logits，Hinton 等人（Hinton
    et al., [2015](#bib.bib44)）建议通过最小化教师 softmax 输出与学生 softmax 输出之间的交叉熵损失来训练学生，同时还要最小化学生预测与实际标签之间的交叉熵。第一部分称为
    soft loss，第二部分称为 hard loss。通常，hard loss 的权重远低于 soft loss。为了使 softmax 输出不那么尖锐，从而将更多有用的信息从教师转移到学生，应使用温度
    $>$1 的 softmax。训练教师和学生时应使用相同的温度，但在学生训练完成后，测试时温度可以设置为 1。除了 logits 和 softmax 输出，Sobolev
    神经网络训练是一种在训练学生网络时除目标值外还结合目标导数的方法（见图 [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation
    Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning
    Models for Text: A Survey")(C)）。Czarnecki 等人（Czarnecki et al., [2017](#bib.bib19)）在目标的前两阶导数上进行了实验。'
- en: KD has also been used along with quantization for better model compression (Bengio
    et al., [2013](#bib.bib8); Mishra and Marr, [2017](#bib.bib83); Polino et al.,
    [2018](#bib.bib92)). We start with a trained full-precision large teacher network
    and an apprentice (student) network that has been initialised with full-precision
    weights. The apprentice network’s precision is lowered and is fine-tuned using
    KD.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: KD 也与量化一起使用以实现更好的模型压缩（Bengio et al., [2013](#bib.bib8); Mishra and Marr, [2017](#bib.bib83);
    Polino et al., [2018](#bib.bib92)）。我们从一个经过训练的全精度大型教师网络和一个已初始化为全精度权重的学徒（学生）网络开始。将学徒网络的精度降低，并使用
    KD 进行微调。
- en: 'Why just use the output from the last layer of the teacher for training the
    student? In FitNets (Romero et al., [2014](#bib.bib99)), the student performs
    hint-based training, i.e., the student is trained using not only the outputs but
    also the intermediate representations learned by the teacher as hints to improve
    the training process and final performance of the student (see Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation
    (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(B)). we choose
    a hidden layer of the FitNet, the guided layer, to learn from the teacher’s hint
    layer. Because the student intermediate hidden layer will generally be smaller
    than the teacher’s intermediate hidden layer, additional parameters are introduced
    to map the student hidden layer to the prediction of the teacher hidden layer.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么仅使用教师的最后一层输出进行学生的训练？在 FitNets（Romero et al., [2014](#bib.bib99)）中，学生进行基于提示的训练，即学生不仅使用输出，还使用教师学习的中间表示作为提示，以改善训练过程和最终性能（见图
    [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge
    Distillation (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(B)）。我们选择
    FitNet 的隐藏层，即引导层，从教师的提示层中学习。由于学生的中间隐藏层通常比教师的中间隐藏层要小，因此引入额外的参数来将学生的隐藏层映射到教师隐藏层的预测。'
- en: 'While the methods discussed so far use logits, softmax output or their derivatives
    to transfer knowledge, Yim et al. (Yim et al., [2017](#bib.bib138)) proposed a
    “flow of solution procedure (FSP)” method where the distilled knowledge is transferred
    in terms of flow between layers, which is calculated by computing the inner product
    between features from two layers (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various
    Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of
    Deep Learning Models for Text: A Survey")(D)). What does this “flow” capture intuitively?
    If we view the input of a deep network as the question and the output as the answer,
    we can think of the generated features at the middle of the network as the intermediate
    result in the solution process. There are many ways to solve the problem of generating
    the output from the input. Hence, mimicking the generated features of the teacher
    can be a hard constraint for the student. Learning the solution process from teacher
    is important. More concretely, the student is trained to minimize the L2 difference
    between the teacher and student FSP matrices computed across various pairs of
    layers and across multiple training instances. A similar method called Representational
    distance learning (RDL) has also been proposed in (McClure and Kriegeskorte, [2016](#bib.bib79)).'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管迄今讨论的方法使用logits、softmax输出或其导数来传递知识，但Yim等（Yim et al., [2017](#bib.bib138)）提出了一种“解决方案过程流（FSP）”方法，其中蒸馏的知识以层间流动的形式进行转移，这通过计算来自两个层的特征之间的内积来实现（见图[4](#S4.F4
    "图 4 ‣ 4.1\. 各种蒸馏架构 ‣ 4\. 知识蒸馏 (KD) ‣ 深度学习模型压缩：综述")(D)）。这个“流”直观上捕获了什么？如果我们将深度网络的输入视为问题，将输出视为答案，我们可以认为生成的特征在网络中间是解决过程中的中间结果。有很多方法可以从输入生成输出。因此，模仿教师生成的特征对学生来说可以是一个硬约束。向学生学习解决过程是重要的。更具体地说，学生被训练以最小化教师和学生FSP矩阵之间在各种层对和多个训练实例中计算的L2差异。类似的方法叫做表示距离学习（RDL），也在（McClure
    and Kriegeskorte, [2016](#bib.bib79)）中提出。
- en: Lastly, multiple KD variants have been proposed for sequence-level predictions (Kim
    and Rush, [2016](#bib.bib57); Freitag et al., [2017](#bib.bib30)), e.g., for neural
    machine translation (NMT). In word-level KD, cross-entropy is minimized between
    the student/teacher distributions for each word in the actual target sequence,
    as well as between the student distribution and the degenerate data distribution,
    which has all of its probability mass on one word. In sequence-level KD (Seq-KD)
    the student network is trained on the output from beam search of the teacher network
    that had the highest score. In sequence-level interpolation (Seq-Inter) the student
    is trained on the output from beam search of the teacher network that had the
    highest similarity (say using BLEU score) with the target sequence.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，已经提出了多种KD变体用于序列级预测（Kim and Rush, [2016](#bib.bib57); Freitag et al., [2017](#bib.bib30)），例如用于神经机器翻译（NMT）。在词级KD中，最小化学生/教师分布之间的交叉熵，对于实际目标序列中的每个词，以及学生分布和退化数据分布之间的交叉熵，后者将所有的概率质量集中在一个词上。在序列级KD（Seq-KD）中，学生网络在教师网络的beam
    search输出上进行训练，该输出具有最高的得分。在序列级插值（Seq-Inter）中，学生在教师网络的beam search输出上进行训练，该输出与目标序列具有最高的相似度（例如使用BLEU评分）。
- en: 4.2\. Collaborative Learning
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 协作学习
- en: 'Can multiple students learn from each other? Is a powerful teacher really required?
    In the deep mutual learning (DML) method (Zhang et al., [2018](#bib.bib142)),
    different from the one-way transfer between a static pre-defined teacher and a
    student in model distillation, with DML, an ensemble of students learn collaboratively
    and teach each other throughout the training process. Surprisingly, no prior powerful
    teacher network is necessary – mutual learning of a collection of simple student
    networks works, and moreover outperforms distillation from a more powerful yet
    static teacher. Specifically, each student is trained with two losses: a conventional
    supervised learning loss, and a mimicry loss that aligns each student’s class
    posterior with the class probabilities of other students.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 多个学生可以相互学习吗？是否真的需要一个强大的教师？在深度互学（DML）方法中（Zhang et al., [2018](#bib.bib142)），与模型蒸馏中静态预定义教师和学生之间的一对一传递不同，DML中，学生们在整个训练过程中通过合作学习和相互教学。令人惊讶的是，不需要先验的强大教师网络——一个简单学生网络的互学效果良好，而且超过了从一个更强大但静态的教师处进行的蒸馏。具体来说，每个学生都通过两个损失进行训练：一个传统的监督学习损失和一个模仿损失，该损失使每个学生的类别后验与其他学生的类别概率对齐。
- en: 'Anil et al. (Anil et al., [2018](#bib.bib3)) propose a similar method but suggest
    letting the students learn independently just using the conventional supervised
    learning (hard) loss at least for a few burn in iterations (see Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation
    (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(E)). After this,
    the mutual learning can be done as in DML. They also propose a variant of their
    Co-Distillation method to perform this training in a distributed scenario where
    communication efficiency is also important. To update the parameters of one network
    using co-distillation one only needs the predictions of the other networks, which
    can be computed locally from copies of the other networks’ weights. Empirically,
    using stale predictions instead of up-to-date predictions for the other neural
    networks has little to no adverse effect on the quality of the final trained model
    produced by co-distillation.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 'Anil 等人（Anil et al., [2018](#bib.bib3)）提出了一种类似的方法，但建议让学生独立学习，至少在几个烧入迭代中仅使用传统的监督学习（硬）损失（见图
    [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge
    Distillation (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(E)）。之后可以像
    DML 中一样进行相互学习。他们还提出了一种 Co-Distillation 方法的变体，以在分布式场景中执行这种训练，其中通信效率也很重要。为了使用共提炼更新一个网络的参数，只需其他网络的预测，这些预测可以通过其他网络权重的副本在本地计算得到。通过实证观察，使用过时的预测而不是最新的预测对共提炼产生的最终训练模型的质量几乎没有不利影响。'
- en: 4.3\. Multiple Teachers
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 多教师
- en: So far we have talked about a student mimicing a single teacher. However, it
    is interesting to explore if the student can learn better in presence of multiple
    teachers or from a teacher assistant.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了学生模仿单一教师的情况。然而，探讨学生在多个教师或教师助理存在时是否能更好地学习是很有趣的。
- en: 'Intuitively and also observed empirically, student network performance degrades
    when the gap between student and teacher is large. Given a fixed student network,
    one cannot employ an arbitrarily large teacher, or in other words, a teacher can
    effectively transfer its knowledge to students up to a certain size, not smaller.
    To alleviate this shortcoming, Mirzadeh et al. (Mirzadeh et al., [2019](#bib.bib82))
    introduced multi-step KD, which employs an intermediate-sized network (teacher
    assistant) to bridge the gap between the student and the teacher (see Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation
    (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(F)). The teacher
    assistant (TA) models are distilled from the teacher, and the student is then
    only distilled from the TAs. One could also perform multi-step TA distillation,
    for example, distillation path could be $10\rightarrow 6\rightarrow 4\rightarrow
    2$.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '直观上以及通过实证观察发现，当学生网络与教师之间的差距较大时，学生网络的性能会下降。给定一个固定的学生网络，不能使用任意大的教师，换句话说，教师只能有效地将知识传递给一定规模的学生，不能更小。为了缓解这一缺陷，Mirzadeh
    等人（Mirzadeh et al., [2019](#bib.bib82)）引入了多步 KD 方法，它使用一个中等规模的网络（教师助理）来弥合学生与教师之间的差距（见图
    [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge
    Distillation (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(F)）。教师助理（TA）模型是从教师中提炼出来的，然后学生仅从
    TA 中提炼。也可以执行多步 TA 提炼，例如，提炼路径可以是 $10\rightarrow 6\rightarrow 4\rightarrow 2$。'
- en: 'A simple way to do KD with multiple teachers is to train student with cross
    entropy loss between student predictions and average prediction from multiple
    teachers (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(G)). A more effective method is to augment this with a relative dissimilarity
    (RD) loss (You et al., [2017](#bib.bib139)) defined over intermediate layer outputs
    generated for a triplet of instances between the student and an ensemble of teachers.
    For the student, the middle layer is selected. For each teacher, we select the
    layer such that most teachers are consistent with the resulting order relationships
    under the voting strategy. We discuss the RD loss given a student and a teacher.
    Consider a triplet of instances ($x_{i}$, $x_{i}^{+}$, $x_{i}^{-}$) such that
    at an intermediate layer of the teacher network, distance between activations
    for $x_{i}^{+}$ and $x_{i}$ is smaller than the distance between activations for
    $x_{i}^{-}$ and $x_{i}$. Let $p_{i}$ be the intermediate output from student for
    example $x_{i}$. Then the RD loss for the triplet ($x_{i}$, $x_{i}^{+}$, $x_{i}^{-}$)
    can be written as follows.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '使用多个教师进行知识蒸馏的一种简单方法是通过学生预测与多个教师的平均预测之间的交叉熵损失进行训练（见图[4](#S4.F4 "Figure 4 ‣ 4.1\.
    Various Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression
    of Deep Learning Models for Text: A Survey")(G)）。一种更有效的方法是将其与相对不相似度（RD）损失相结合（You等，[2017](#bib.bib139)），该损失在学生和教师集的实例三元组生成的中间层输出上定义。对于学生，选择中间层。对于每个教师，我们选择一层，使得在投票策略下，大多数教师对结果的顺序关系保持一致。我们讨论给定学生和教师的RD损失。考虑一个实例三元组（$x_{i}$，$x_{i}^{+}$，$x_{i}^{-}$），在教师网络的中间层上，$x_{i}^{+}$和$x_{i}$的激活之间的距离小于$x_{i}^{-}$和$x_{i}$的激活之间的距离。设$p_{i}$为学生对实例$x_{i}$的中间输出。那么该三元组（$x_{i}$，$x_{i}^{+}$，$x_{i}^{-}$）的RD损失可以写作如下。'
- en: '| (57) |  | $\displaystyle\text{Loss}=\max(0,d(p_{i},p_{i}^{+})-d(p_{i},p_{i}^{-})+\delta)$
    |  |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| (57) |  | $\displaystyle\text{Loss}=\max(0,d(p_{i},p_{i}^{+})-d(p_{i},p_{i}^{-})+\delta)$
    |  |'
- en: where $d$ is the distance function, and $\delta$ is a small constant to prevent
    the trivial solution. To extend this loss function definition to multiple teachers,
    the order between the instances $x_{i}^{+}$ and $x_{i}^{-}$ given $x_{i}$ is decided
    based on majority voting between the teachers.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$d$是距离函数，$\delta$是一个小常数，用以防止平凡解。为了将这种损失函数定义扩展到多个教师的情况，给定$x_{i}$时，$x_{i}^{+}$和$x_{i}^{-}$之间的顺序是通过教师之间的多数投票决定的。
- en: There are also specific settings when distilling from multiple teachers becomes
    natural, e.g., when the number of classes is large (Hinton et al., [2015](#bib.bib44))
    or in multi-lingual settings (Tan et al., [2019](#bib.bib114)). When the number
    of classes is very large, the teacher model could be an ensemble that contains
    one generalist model trained on all the data and many “specialist” models, each
    of which is trained on data that is highly enriched in examples from a very confusable
    subset of the classes (like different types of mushroom). Softmax distribution
    vector of this type of specialist can be made much smaller by combining all of
    the classes it does not care about into a single dustbin class. Each specialist
    model is initialized with the weights of the generalist model. These weights are
    then slightly modified by training the specialist with half its examples coming
    from its special subset and half sampled at random from the remainder of the training
    set. To derive groupings of object categories for the specialists, we focus on
    categories that the full generalist network often confuses. When training the
    student, for each instance, we first find the $set$k$ofn$ most probable classes
    according to the generalist model. Then, we take all the specialist models, $m$,
    whose special subset of confusable classes has a non-empty intersection with $k$
    and call this the active set of specialists $A_{k}$. Given student’s full probability
    distribution $q$ over all the classes, we minimize the following.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在蒸馏多个教师模型时，也有一些特定的设置，例如，当类别数量很大时（Hinton et al., [2015](#bib.bib44)）或在多语言设置中（Tan
    et al., [2019](#bib.bib114)）。当类别数量非常大时，教师模型可以是一个包含所有数据的通用模型和许多“专家”模型的集群，每个专家模型都在高度丰富的非常容易混淆的类别子集数据上进行训练（如不同类型的蘑菇）。这类专家的Softmax分布向量可以通过将它不关心的所有类别合并为一个单一的“尘埃桶”类别来大大缩小。每个专家模型都以通用模型的权重初始化，然后通过用一半来自其特殊子集的样本和一半随机抽取的样本来微调这些权重。为了为专家推导对象类别分组，我们关注通用网络经常混淆的类别。在训练学生模型时，对于每个实例，我们首先根据通用模型找到最可能的
    $k$ 类。然后，我们取所有那些其混淆类别的特殊子集与 $k$ 有非空交集的专家模型 $m$，并称其为专家的活动集 $A_{k}$。给定学生模型对所有类别的完整概率分布
    $q$，我们最小化以下内容。
- en: '| (58) |  | $\displaystyle\text{Loss}=KL(p^{g},q)+\sum_{m\in A_{k}}KL(p^{m},q)$
    |  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| (58) |  | $\displaystyle\text{Loss}=KL(p^{g},q)+\sum_{m\in A_{k}}KL(p^{m},q)$
    |  |'
- en: where $p^{g}$ is output distribution from the generalist model, and $p^{m}$
    is the output distribution from the $m^{th}$ specialist model.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p^{g}$ 是来自通用模型的输出分布，而 $p^{m}$ 是来自第 $m^{th}$ 个专家模型的输出分布。
- en: An ensemble of teachers is also very useful in a multi-lingual NMT setting (Tan
    et al., [2019](#bib.bib114)). Individual models for each language pair are first
    trained and regarded as teachers, and then the multilingual model is trained to
    fit the training data and match the outputs of individual models simultaneously
    through KD. When the accuracy of multilingual model surpasses the individual model
    for the accuracy threshold $\tau$ on a certain language pair, we remove the distillation
    loss and just train the model with original negative log-likelihood loss for this
    pair. Lastly, when learning from a teacher ensemble, it is burdensome to load
    all the teacher models in the GPU memory for distillation. Alternatively, we first
    generate the output probability distribution of each teacher model for each instance
    offline, and then just load the top-$K$ probabilities of the distribution into
    memory and normalize them so that they sum to 1 for distillation. This reduces
    the memory cost from the scale of $|V|$ (the vocabulary size) to $K$.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在多语言NMT设置中，教师集群也非常有用（Tan et al., [2019](#bib.bib114)）。首先训练每个语言对的单独模型，将其视为教师，然后训练多语言模型以适应训练数据，并通过KD同时匹配各个模型的输出。当多语言模型的准确性超过某一语言对的准确性阈值
    $\tau$ 的单独模型时，我们会移除蒸馏损失，仅用原始的负对数似然损失训练该语言对的模型。最后，当从教师集群中学习时，将所有教师模型加载到GPU内存中进行蒸馏是繁重的。替代方案是，我们首先离线生成每个教师模型对于每个实例的输出概率分布，然后仅将前
    $K$ 个概率加载到内存中，并对其进行归一化，使它们的和为1以进行蒸馏。这将内存成本从 $|V|$（词汇表大小）的规模降低到 $K$。
- en: 4.4\. Distilling Transformers
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 蒸馏变换器
- en: Recently, there has been a lot of work around distilling Transformers to smaller
    Transformers with less number of layers or to Bidirectional LSTMs. Some of these
    methods aim at improving the accuracy versus model size tradeoff, while others
    focus on complex settings like mismatching student-teacher vocabulary (Zhao et al.,
    [2019](#bib.bib143)) or mismatch number of attention heads.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，围绕将 Transformers 蒸馏成更小的 Transformers 或双向 LSTM 的研究有很多。这些方法中的一些旨在改善准确性与模型大小之间的权衡，而另一些则关注于复杂设置，例如学生-教师词汇表不匹配（Zhao
    et al., [2019](#bib.bib143)）或注意力头数量不匹配。
- en: Zhao et al. (Zhao et al., [2019](#bib.bib143)) learn a student with small vocabulary
    compared to the teacher using a dual training method. During distillation, for
    a given training sequence input to the teacher model, they mix the teacher and
    student vocabularies by randomly selecting tokens from the sequence to segment
    using the student vocabulary, with the other tokens segmented using the teacher
    vocabulary. As part of the masked language model (MLM) task, the model now needs
    to learn to predict words from the student vocabulary using context words segmented
    using the teacher vocabulary, and vice versa. The expectation is that the student
    embeddings can be learned effectively this way from the teacher embeddings as
    well as teacher model parameters. We perform dual training only for the teacher
    model inputs. The student model receives words segmented exclusively using the
    student vocabulary. Also, during MLM, the model uses different softmax layers
    for the teacher and the student vocabularies depending on which one was used to
    segment the word in question. Instead of distilling solely on the teacher model’s
    final-layer outputs, layer-wise teacher model parameters can also be leveraged
    to directly optimize parameters of corresponding layers in the student model.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人（Zhao et al., [2019](#bib.bib143)）采用双重训练方法训练一个词汇量小于教师的学生模型。在蒸馏过程中，对于输入教师模型的给定训练序列，他们通过从序列中随机选择令牌以使用学生词汇表进行分割，同时使用教师词汇表分割其他令牌，从而混合教师和学生词汇表。作为掩蔽语言模型（MLM）任务的一部分，模型现在需要学习从学生词汇表中预测单词，同时使用教师词汇表分割的上下文单词，反之亦然。期望通过这种方式，学生嵌入可以有效地从教师嵌入以及教师模型参数中学习。我们仅对教师模型输入执行双重训练。学生模型接收完全使用学生词汇表分割的单词。此外，在
    MLM 过程中，模型根据用于分割问题单词的词汇表不同，使用不同的 softmax 层。除了仅在教师模型的最终层输出上进行蒸馏外，还可以利用逐层的教师模型参数来直接优化学生模型中对应层的参数。
- en: 'In Patient KD (PKD) (Sun et al., [2019](#bib.bib111)), the student learns from
    the teacher’s output after every $k$ layers (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\.
    Various Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression
    of Deep Learning Models for Text: A Survey")(H)) or the output from the last few
    layers of the teacher (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation
    Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning
    Models for Text: A Survey")(I)). The student BERT is initialized using some layers
    of the pre-trained teacher BERT. TinyBERT (Jiao et al., [2019](#bib.bib51)) further
    extends this idea by using extensive knowledge from embedding layer, and attention
    and hidden sub-layers of multiple teacher layers, and also the overall teacher
    output (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(J)). Each student layer is first mapped to a teacher layer before the
    student training. Liu et al. (Liu et al., [2019a](#bib.bib74)) distill a multi-task
    student from a multi-task teacher, given the soft targets of the training data
    across multiple tasks. If task $t$ has a teacher, the task-specific loss is the
    average of two objective functions, one for the correct targets and the other
    for the soft targets assigned by the teacher. In MiniLM (Wang et al., [2020b](#bib.bib130)),
    the student is trained by deeply mimicking the self-attention behavior of the
    last Transformer layer of the teacher (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various
    Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of
    Deep Learning Models for Text: A Survey")(K)). Besides self-attention distributions,
    MiniLM introduces the self-attention value-relation transfer to help the student
    achieve a deeper mimicry. The value-relation is computed as pairwise correlation
    between different components of the value matrix across various attention heads
    of the final layer. Pretrained Distillation (Turc et al., [2019](#bib.bib120))
    pretrains the student model with a self-supervised masked LM objective on a large
    corpus first, then performs a standard KD on supervised tasks.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Patient KD (PKD)（Sun et al., [2019](#bib.bib111)）中，学生在每 $k$ 层之后从教师的输出中学习（见图
    [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge
    Distillation (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(H)），或从教师最后几层的输出中学习（见图
    [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge
    Distillation (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(I)）。学生
    BERT 是使用预训练教师 BERT 的一些层进行初始化的。TinyBERT（Jiao et al., [2019](#bib.bib51)）进一步扩展了这一思想，通过使用来自多个教师层的嵌入层、注意力和隐藏子层的广泛知识，以及整体教师输出（见图
    [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge
    Distillation (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(J)）。每个学生层在学生训练之前首先映射到一个教师层。Liu
    等人（Liu et al., [2019a](#bib.bib74)）从多任务教师中蒸馏出一个多任务学生，给定跨多个任务的训练数据的软目标。如果任务 $t$
    有一个教师，则任务特定的损失是两个目标函数的平均值，一个是正确目标，另一个是教师分配的软目标。在 MiniLM（Wang et al., [2020b](#bib.bib130)）中，学生通过深度模仿教师最后一个
    Transformer 层的自注意力行为进行训练（见图 [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation
    Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning
    Models for Text: A Survey")(K)）。除了自注意力分布，MiniLM 引入了自注意力值关系转移，以帮助学生实现更深层次的模仿。值关系计算为最终层的各个注意力头的值矩阵不同组件之间的成对相关性。预训练蒸馏（Turc
    et al., [2019](#bib.bib120)）首先在大语料库上使用自监督的掩码语言模型目标对学生模型进行预训练，然后在有监督任务上执行标准的 KD。'
- en: Most of these models learn one-to-one layer mapping, where each student layer
    is guided by only one specific teacher layer. Li et al. (Li et al., [2020](#bib.bib67))
    propose a method where each student intermediate layer learns from every teacher
    intermediate layer with learnable attention weights. Both the embedding-layer
    distillation and the prediction-layer distillation employ the one-to-one layer
    mapping as in TinyBERT and BERT-PKD.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型大多采用一对一层映射的学习方式，每个学生层仅由一个特定的教师层指导。Li 等人（Li et al., [2020](#bib.bib67)）提出了一种方法，其中每个学生中间层从每个教师中间层学习，采用可学习的注意力权重。嵌入层蒸馏和预测层蒸馏都采用与
    TinyBERT 和 BERT-PKD 相同的一对一层映射。
- en: 'Tang et al. (Tang et al., [2019](#bib.bib115)) propose distillation of a BERT
    model to a single layer BiLSTM using KL divergence between student and teacher
    logits (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(L)). Mukherjee et al. (Mukherjee and Awadallah, [2020](#bib.bib84))
    also distill a multi-lingual BERT (mBERT) model to a BiLSTM. Representation transfer
    is done from Transformer-based teacher model to BiLSTM-based student model with
    different embedding dimensions and disparate output spaces. Distillation features
    include teacher logits and internal teacher representations for one teacher layer.
    To make all output spaces compatible, a non-linear projection of the parameters
    in student representation is done to have same shape as teacher representation
    for each token. The projection parameters are learned by minimizing the KL-divergence
    (KLD) between the representations of the student and the chosen layer from the
    teacher. Overall there are multiple loss functions for the student training: supervised
    hard loss, soft loss wrt output logits, and soft loss wrt internal teacher layer.
    Rather than optimizing for all loss functions jointly, stage-wise training is
    performed where each loss function is sequentially used for optimization.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tang 等人（Tang et al., [2019](#bib.bib115)）提出将 BERT 模型蒸馏为单层 BiLSTM，使用学生和教师 logits
    之间的 KL 散度（见图 [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣
    4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(L)）。Mukherjee 等人（Mukherjee and Awadallah, [2020](#bib.bib84)）也将多语言
    BERT（mBERT）模型蒸馏为 BiLSTM。表示转移是从基于 Transformer 的教师模型到基于 BiLSTM 的学生模型，具有不同的嵌入维度和不同的输出空间。蒸馏特征包括教师
    logits 和一个教师层的内部表示。为了使所有输出空间兼容，对学生表示中的参数进行非线性投影，以便每个标记的形状与教师表示相同。投影参数通过最小化学生表示与教师选择层之间的
    KL 散度（KLD）来学习。总的来说，学生训练有多个损失函数：监督硬损失、与输出 logits 相关的软损失和与内部教师层相关的软损失。与其联合优化所有损失函数，不如进行分阶段训练，其中每个损失函数按顺序用于优化。'
- en: Lastly, there have been recent efforts (Sun et al., [2020](#bib.bib112); Iandola
    et al., [2020](#bib.bib50)) to distill Transformer models to slightly modified
    Transformer architectures. MobileBERT (Sun et al., [2020](#bib.bib112)) is a thin
    version of BERT-large, while equipped with bottleneck structures and a carefully
    designed balance between self-attentions and feed-forward networks (FFN). To train
    MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck
    incorporated BERT-large model (IB-BERT). The IB-BERT uses the inverted-bottleneck
    structures to adjust its feature map size to 512\. Thus, in the bottleneck structure,
    the inputs to the multi-head attention (MHA) module are from wider feature maps
    (of inter-block size), while the inputs to the FFN are from narrower bottlenecks
    (of intra-block size). To fix this issue, MobileBERT uses stacked feed-forward
    networks to re-balance the relative size between MHA and FFN. Each MobileBERT
    layer contains one MHA but 4 stacked FFN after each MHA. Then, we conduct knowledge
    transfer from this teacher to MobileBERT using feature map transfer and attention
    transfer across all layers. Also, while distilling they perform progressive knowledge
    transfer, i.e., they progressively train each layer in $L$ stages, where $L$ is
    the number of layers. When the $l$-th layer is trained, all the trainable parameters
    in the layers below are frozen. Another difference in the MobileBERT student architecture
    is usage of high information flow residual connections between the high-channel-count
    layers. MobileBERT is 4.3x smaller and 5.5x faster than BERT-base while achieving
    competitive results on well-known benchmarks. Iandola et al. (Iandola et al.,
    [2020](#bib.bib50)) propose a new Transformer model architecture called SqueezeBERT
    which is much like BERT-base, but with the position-wise fully connected layers
    implemented as convolutions, and grouped convolutions for many of the layers.
    Distillation for SqueezeBERT is rather straightforward. Distillation is applied
    only to the final layer, and only during finetuning using soft cross entropy loss
    with respect to a weighted sum of the teacher’s logits and a one-hot encoding
    of the ground-truth. Teacher model is a BERT-base model finetuned independently
    on each GLUE task, and these task-specific teacher weights are used for distillation.
    Xu et al. (Xu et al., [2020](#bib.bib134)) propose a method for progressive module
    replacement for compressing BERT. Their approach first divides the original BERT
    into several modules and builds their compact substitutes. Then, the original
    modules are randomly replaced with their substitutes to train the compact modules
    to mimic the behavior of the original modules. We progressively increase the probability
    of replacement through the training. In this way, their approach brings a deeper
    level of interaction between the original and compact models.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最近有一些努力（Sun et al., [2020](#bib.bib112); Iandola et al., [2020](#bib.bib50)）致力于将
    Transformer 模型提炼为略微修改的 Transformer 架构。MobileBERT（Sun et al., [2020](#bib.bib112)）是
    BERT-large 的一个简化版本，同时配备了瓶颈结构和精心设计的自注意力与前馈网络（FFN）之间的平衡。为了训练 MobileBERT，我们首先训练一个特别设计的教师模型，即集成了倒瓶颈的
    BERT-large 模型（IB-BERT）。IB-BERT 使用倒瓶颈结构将其特征图大小调整为 512\。因此，在瓶颈结构中，多头注意力（MHA）模块的输入来自更宽的特征图（跨块大小），而
    FFN 的输入来自更窄的瓶颈（块内大小）。为了解决这个问题，MobileBERT 使用堆叠的前馈网络来重新平衡 MHA 和 FFN 之间的相对大小。每个 MobileBERT
    层包含一个 MHA，但在每个 MHA 后有 4 个堆叠的 FFN。然后，我们通过特征图转移和跨层注意力转移，将知识从这个教师模型传递到 MobileBERT。同时，在提炼过程中，他们执行渐进式知识转移，即他们在
    $L$ 个阶段逐步训练每一层，其中 $L$ 是层数。当第 $l$ 层被训练时，所有下层的可训练参数都被冻结。MobileBERT 学生架构的另一个不同之处是使用了高信息流残差连接，连接了高通道层。MobileBERT
    比 BERT-base 小 4.3 倍，速度快 5.5 倍，同时在知名基准测试上取得了具有竞争力的结果。Iandola et al.（Iandola et
    al., [2020](#bib.bib50)）提出了一种新的 Transformer 模型架构，称为 SqueezeBERT，这与 BERT-base 非常相似，但位置-wise
    全连接层被实现为卷积，并且许多层使用了分组卷积。SqueezeBERT 的提炼相对简单。提炼仅应用于最终层，并且仅在使用针对教师模型的 logits 的加权和和真实标签的一热编码的软交叉熵损失进行微调时进行。教师模型是独立在每个
    GLUE 任务上微调的 BERT-base 模型，这些任务特定的教师权重用于提炼。Xu et al.（Xu et al., [2020](#bib.bib134)）提出了一种渐进模块替换的方法来压缩
    BERT。他们的方法首先将原始 BERT 分为几个模块，并构建它们的紧凑替代品。然后，原始模块被随机替换为其替代品，以训练紧凑模块模仿原始模块的行为。我们通过训练逐步增加替换的概率。通过这种方式，他们的方法在原始模型和紧凑模型之间带来了更深层次的交互。
- en: 4.5\. Summary
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 总结
- en: '| Task | Dataset | Teacher/Student models | Method | Metric | Size (Distilled;
    Orig) | Eval. (Distilled; Orig) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 教师/学生模型 | 方法 | 指标 | 大小 (蒸馏; 原始) | 评估 (蒸馏; 原始) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Abs. summarization | CNN/DailyMail | UniLM-large/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | ROUGE-L (H) | 33M; 340M | 39.73; 40.34 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 摘要总结 | CNN/DailyMail | UniLM-large/12-layer BERT | MiniLM (Wang et al., [2020b](#bib.bib130))
    | ROUGE-L (H) | 33M; 340M | 39.73; 40.34 |'
- en: '| Ad CTR prediction | Criteo | No teacher/DNN | CoDistillation (Anil et al.,
    [2018](#bib.bib3)) | MAE (L) | - | 0.019; 0.022 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 广告点击率预测 | Criteo | 无教师/DNN | CoDistillation (Anil et al., [2018](#bib.bib3))
    | MAE (L) | - | 0.019; 0.022 |'
- en: '| Cross-lingual NLI | XNLI (15 langs.) | XLM-R-base/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | Acc (H) | 21M; 85M | 71.1; 74.5 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 跨语言自然语言推理 | XNLI (15 种语言) | XLM-R-base/12-layer BERT | MiniLM (Wang et al.,
    [2020b](#bib.bib130)) | 准确率 (H) | 21M; 85M | 71.1; 74.5 |'
- en: '| Cross-lingual QA | MLQA (7 langs.) | XLM-R-base/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | F1 (H) | 21M; 85M | 63.2; 64.9 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 跨语言问答 | MLQA (7 种语言) | XLM-R-base/12-layer BERT | MiniLM (Wang et al., [2020b](#bib.bib130))
    | F1 (H) | 21M; 85M | 63.2; 64.9 |'
- en: '| Intent detection | SNIPS (7-class) | BERT-base/6-layer BERT | Mixed-vocabulary
    training (Zhao et al., [2019](#bib.bib143)) | Acc (H) | 6.2M; 109M | 98.7; 98.6
    |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 意图检测 | SNIPS (7-class) | BERT-base/6-layer BERT | 混合词汇训练 (Zhao et al., [2019](#bib.bib143))
    | 准确率 (H) | 6.2M; 109M | 98.7; 98.6 |'
- en: '| Language modeling | Common Crawl | No teacher/2-layer LSTM | CoDistillation (Anil
    et al., [2018](#bib.bib3)) | CE (L) | - | 3.91; 3.92 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Common Crawl | 无教师/2-layer LSTM | CoDistillation (Anil et al., [2018](#bib.bib3))
    | 交叉熵 (L) | - | 3.91; 3.92 |'
- en: '| Machine reading comp. | RACE | BERT-base/6-layer BERT | Patient KD (Sun et al.,
    [2019](#bib.bib111)) | Acc (H) | 67M; 109M | 60.34; 65.30 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读理解 | RACE | BERT-base/6-layer BERT | Patient KD (Sun et al., [2019](#bib.bib111))
    | 准确率 (H) | 67M; 109M | 60.34; 65.30 |'
- en: '| Machine reading comp. | RACE | BERT-base/6-layer BERT | Vanilla KD (Sun et al.,
    [2019](#bib.bib111)) | Acc (H) | 67M; 109M | 58.74; 65.30 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读理解 | RACE | BERT-base/6-layer BERT | 原始 KD (Sun et al., [2019](#bib.bib111))
    | 准确率 (H) | 67M; 109M | 58.74; 65.30 |'
- en: '| NER (41 langs.) | Wikiann-41 | Multiple mBERT/BiLSTM | XtremeDistill (Mukherjee
    and Awadallah, [2020](#bib.bib84)) | F1 (H) | 31.8M; 179M*41 | 88.64; 90.76 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别 (41 种语言) | Wikiann-41 | 多个 mBERT/BiLSTM | XtremeDistill (Mukherjee
    and Awadallah, [2020](#bib.bib84)) | F1 (H) | 31.8M; 179M*41 | 88.64; 90.76 |'
- en: '| NMT (de$\rightarrow$en) | OpenNMT | 4-layer LSTM/2-layer LSTM | Vanilla KD (Polino
    et al., [2018](#bib.bib92)) | BLEU (H) | 64.8M; 84.8M | 15.48; 15.88 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (de$\rightarrow$en) | OpenNMT | 4-layer LSTM/2-layer LSTM | 原始 KD
    (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 64.8M; 84.8M | 15.48; 15.88 |'
- en: '| NMT (de$\rightarrow$en) | OpenNMT | 4-layer LSTM/2-layer LSTM | Quantized
    Distillation (4 bits) (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 64.8M;
    84.8M | 15.26; 15.88 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (de$\rightarrow$en) | OpenNMT | 4-layer LSTM/2-layer LSTM | 量化蒸馏 (4
    位) (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 64.8M; 84.8M | 15.26; 15.88
    |'
- en: '| NMT (de$\rightarrow$en) | WMT13 | 4-layer LSTM/2-layer LSTM | Quantized Distillation
    (4 bits) (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 81.6M; 84.8M | 35.32;
    34.7 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (de$\rightarrow$en) | WMT13 | 4-layer LSTM/2-layer LSTM | 量化蒸馏 (4
    位) (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 81.6M; 84.8M | 35.32; 34.7
    |'
- en: '| NMT (de$\rightarrow$en) | WMT13 | 4-layer LSTM/2-layer LSTM | Vanilla KD (Polino
    et al., [2018](#bib.bib92)) | BLEU (H) | 81.6M; 84.8M | 30.21; 34.7 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (de$\rightarrow$en) | WMT13 | 4-layer LSTM/2-layer LSTM | 原始 KD (Polino
    et al., [2018](#bib.bib92)) | BLEU (H) | 81.6M; 84.8M | 30.21; 34.7 |'
- en: '| NMT (en$\rightarrow$Others) | IWSLT (13 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*12
    | 22.96; 22.72 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$其他) | IWSLT (13 种语言) | 多个 Transformers/Transformer
    | 多教师 (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*12 | 22.96; 22.72
    |'
- en: '| NMT (en$\rightarrow$Others) | IWSLT (13 langs.) | Multiple Transformers/Transformer
    | Seq-KD with Multiple teachers (Kim and Rush, [2016](#bib.bib57)) | BLEU (H)
    | 44M; 44M*12 | 21.98; 22.72 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$其他) | IWSLT (13 种语言) | 多个 Transformers/Transformer
    | 多教师 Seq-KD (Kim and Rush, [2016](#bib.bib57)) | BLEU (H) | 44M; 44M*12 | 21.98;
    22.72 |'
- en: '| NMT (en$\rightarrow$Others) | WMT (7 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*6
    | 24.47; 24.50 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$其他) | WMT (7 种语言) | 多个 Transformers/Transformer |
    多教师 (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*6 | 24.47; 24.50 |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Word-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 14.9; 17.7 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Word-KD
    (Kim and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 14.9; 17.7 |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Seq-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 18.1; 17.7 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Seq-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 18.1; 17.7 |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Seq-KD + Seq-Inter
    + Word-KD (Kim and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 18.5; 17.7
    |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT14 | 4层 LSTM/2层 LSTM | Seq-KD + Seq-Inter +
    Word-KD (Kim 和 Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 18.5; 17.7 |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Pruned Seq-KD
    + Seq-Inter (Kim and Rush, [2016](#bib.bib57)) | BLEU@5 (H) | 8M/17M; 221M | 18.5/19.1;
    19.5 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$de) | WMT14 | 4层 LSTM/2层 LSTM | 剪枝 Seq-KD + Seq-Inter
    (Kim 和 Rush, [2016](#bib.bib57)) | BLEU@5 (H) | 8M/17M; 221M | 18.5/19.1; 19.5
    |'
- en: '| NMT (Others$\rightarrow$en) | IWSLT (13 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*12
    | 30.34; 29.7 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| NMT (其他$\rightarrow$en) | IWSLT (13 种语言) | 多个 Transformer/Transformer | 多位教师
    (Tan 等人, [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*12 | 30.34; 29.7 |'
- en: '| NMT (Others$\rightarrow$en) | Ted Talk (45 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*43
    | 28.95; 25.17 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| NMT (其他$\rightarrow$en) | Ted Talk (45 种语言) | 多个 Transformer/Transformer
    | 多位教师 (Tan 等人, [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*43 | 28.95; 25.17 |'
- en: '| NMT (Others$\rightarrow$en) | WMT (7 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*6
    | 28.61; 27.07 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| NMT (其他$\rightarrow$en) | WMT (7 种语言) | 多个 Transformer/Transformer | 多位教师
    (Tan 等人, [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*6 | 28.61; 27.07 |'
- en: '| NMT (th$\rightarrow$en) | IWSLT15 | 4-layer LSTM/2-layer LSTM | Word-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 11.8; 14.3 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| NMT (th$\rightarrow$en) | IWSLT15 | 4层 LSTM/2层 LSTM | Word-KD (Kim 和 Rush,
    [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 11.8; 14.3 |'
- en: '| NMT (th$\rightarrow$en) | IWSLT15 | 4-layer LSTM/2-layer LSTM | Seq-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 12.8; 14.3 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| NMT (th$\rightarrow$en) | IWSLT15 | 4层 LSTM/2层 LSTM | Seq-KD (Kim 和 Rush,
    [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 12.8; 14.3 |'
- en: '| NMT (th$\rightarrow$en) | IWSLT15 | 4-layer LSTM/2-layer LSTM | Seq-KD +
    Seq-Inter + Word-KD (Kim and Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M |
    14.2; 14.3 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| NMT (th$\rightarrow$en) | IWSLT15 | 4层 LSTM/2层 LSTM | Seq-KD + Seq-Inter
    + Word-KD (Kim 和 Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 14.2; 14.3 |'
- en: '| Question generation | SQuAD 1.1 | UniLM-large/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | BLEU@4 (H) | 33M; 340M | 23.27; 24.32 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 问题生成 | SQuAD 1.1 | UniLM-large/12层 BERT | MiniLM (Wang 等人, [2020b](#bib.bib130))
    | BLEU@4 (H) | 33M; 340M | 23.27; 24.32 |'
- en: '| Slot filling | SNIPS (39 slots) | BERT-base/6-layer BERT | Mixed-vocabulary
    training (Zhao et al., [2019](#bib.bib143)) | F1 (H) | 6.2M; 109M | 95.0; 97.0
    |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 插槽填充 | SNIPS (39 个插槽) | BERT-base/6层 BERT | 混合词汇训练 (Zhao 等人, [2019](#bib.bib143))
    | F1 (H) | 6.2M; 109M | 95.0; 97.0 |'
- en: Table 3\. Comparison of various knowledge distillation methods (sorted by Task
    and then Dataset). CE=cross entropy, MAE=mean absolute error. en=English, th=Thai.
    MRC=Machine Reading Comprehension. NLI=Natural Language Inference. QA=Question
    Answering. NER=Named Entity Recognition. In the metric column, H means high is
    better while L means low is better.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 各种知识蒸馏方法的比较（按任务和数据集排序）。CE=交叉熵，MAE=均方绝对误差。en=英语，th=泰语。MRC=机器阅读理解。NLI=自然语言推理。QA=问答。NER=命名实体识别。在度量列中，H
    代表高值更好，而 L 代表低值更好。
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey") compares various knowledge
    distillation methods across different tasks and datasets. Accuracy of both the
    original and the distilled model are shown in the Eval column. Also, we report
    model size for both the student as well as the teacher models. Note that sometimes
    smaller student models perform better than the teacher models. This could be because
    (1) for some (task, dataset) pairs, the smaller models are a better regularized
    fit compared to potentially overfitted teacher models, and (2) student models
    can be rigorously trained using additional semi-supervision while teacher models
    depend on limited labeled training data.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "表 3 ‣ 4.5\. 总结 ‣ 4\. 知识蒸馏 (KD) ‣ 文本深度学习模型压缩：综述") 比较了不同任务和数据集上的各种知识蒸馏方法。Eval
    列显示了原始模型和蒸馏模型的准确率。我们还报告了学生模型和教师模型的大小。注意，有时较小的学生模型表现优于教师模型。这可能是因为（1）对于某些 (任务, 数据集)
    配对，较小的模型相比于可能过拟合的教师模型，能够更好地进行正则化拟合，（2）学生模型可以通过额外的半监督训练进行严格训练，而教师模型依赖于有限的标注训练数据。
- en: While knowledge distillation has been used for distilling NLP models across
    many applications, NMT is the most popular one. For abstractive summarization,
    MiniLM (Wang et al., [2020b](#bib.bib130)) leads to a student models which is
    less than one tenth of the teacher without much loss in ROUGE-L. Similarly, MiniLM
    shows good results for cross-lingual NLI and multi-lingual QA as well. For Ad
    click through rate (CTR) prediction and language modeling, Anil et al. (Anil et al.,
    [2018](#bib.bib3)) show that co-distillation leads to lower MAE and cross entropy
    respectively compared to the individually trained models. Zhao et al. (Zhao et al.,
    [2019](#bib.bib143))’s mixed-vocab training leads to 6-layer model that retains
    over 95% of the BERT-base model’s slot filling F1 score while being 30x smaller
    ($<$10 MB without quantization) and 57x faster on a mobile device, yet task-agnostic.
    For Named Entity Recognition (NER), Mukherjee et al. (Mukherjee and Awadallah,
    [2020](#bib.bib84)) show that XtremeDistil leads to massive compression of teacher
    models like mBERT by upto 35x in terms of parameters and 51x in terms of latency
    for batch inference while retaining 95% of its F1-score for NER over 41 languages.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管知识蒸馏已经在许多应用中用于蒸馏自然语言处理（NLP）模型，但神经机器翻译（NMT）是最受欢迎的应用之一。对于抽象总结，MiniLM (Wang et
    al., [2020b](#bib.bib130)) 生成的学生模型不到教师模型的十分之一，且ROUGE-L损失不大。同样，MiniLM 在跨语言自然语言推理（NLI）和多语言问答（QA）中也表现良好。在广告点击率（CTR）预测和语言建模中，Anil
    et al. (Anil et al., [2018](#bib.bib3)) 表明，联合蒸馏相比于单独训练的模型在平均绝对误差（MAE）和交叉熵上分别有较低的表现。Zhao
    et al. (Zhao et al., [2019](#bib.bib143)) 的混合词汇训练生成了一个6层的模型，该模型保留了BERT-base模型超过95%的槽填充F1分数，同时体积缩小了30倍（$<$10
    MB且未量化）并且在移动设备上速度提高了57倍，但任务无关。对于命名实体识别（NER），Mukherjee et al. (Mukherjee and Awadallah,
    [2020](#bib.bib84)) 显示，XtremeDistil能将像mBERT这样的教师模型压缩至原始模型的35倍，并在批量推断中延迟减少51倍，同时保持在41种语言中NER的F1分数的95%。
- en: 'For NMT, experiments have been done on OpenNMT, WMT, IWSLT and TedTalk datasets.
    Kim et al. (Kim and Rush, [2016](#bib.bib57)) make the following observations:
    (1) Sequence-level knowledge distillation (Seq-KD) does better than word-level
    knowledge distillation (Word-KD) on English $\rightarrow$ German and performs
    similarly on Thai $\rightarrow$ English. (2) Combining them (Seq-KD + Word-KD)
    results in further gains, indicating that these methods provide orthogonal means
    of transferring knowledge from the teacher to the student: Word-KD is transferring
    knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge
    at the global (i.e. sequence) level. (3) Applying weight pruning on top of knowledge
    distillation results in a student model that has 13x fewer parameters than the
    original teacher model, with a decrease of 0.4 BLEU. Tan et al. (Tan et al., [2019](#bib.bib114))
    show that one model is enough to handle multiple languages (up to 44 languages),
    with comparable or even better accuracy than individual models. Their method achieves
    larger improvements on some languages, such as Da, Et, Fi, Hi and Hy, than others.
    This is correlated with the data size of the languages. When a language is of
    smaller data size, it may get more improvement due to the benefit of multilingual
    training.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NMT，已在OpenNMT、WMT、IWSLT和TedTalk数据集上进行了实验。Kim et al. (Kim and Rush, [2016](#bib.bib57))
    做出了以下观察：（1）序列级知识蒸馏（Seq-KD）比词级知识蒸馏（Word-KD）在英语 $\rightarrow$ 德语上效果更好，并且在泰语 $\rightarrow$
    英语上表现相似。（2）将它们结合起来（Seq-KD + Word-KD）会进一步提升性能，表明这些方法提供了从教师到学生传递知识的不同方式：Word-KD
    在局部（即词）级别传递知识，而 Seq-KD 在全局（即序列）级别传递知识。（3）在知识蒸馏的基础上应用权重剪枝会生成一个参数比原始教师模型少13倍的学生模型，并且BLEU得分降低了0.4。Tan
    et al. (Tan et al., [2019](#bib.bib114)) 显示，一个模型足以处理多种语言（最多44种语言），其准确性与单独模型相当甚至更高。他们的方法在一些语言上，如Da、Et、Fi、Hi和Hy，取得了更大的改善。这与语言的数据量有关。当语言的数据量较小时，可能因多语言训练的好处而获得更多提升。
- en: '| Method | Method | Teacher/Student | Size | MRPC | MNLI | MNLI-m | SST-2 |
    QQP | QNLI | RTE | CoLA | STS-B | SQuAD | SQuAD |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 方法 | 教师/学生 | 规模 | MRPC | MNLI | MNLI-m | SST-2 | QQP | QNLI | RTE |
    CoLA | STS-B | SQuAD | SQuAD |'
- en: '| Category |  | models |  | F1 | Acc | Acc | Acc | F1 | Acc | Acc | MCC | Spearman
    | 1.1 F1 | 2.0 F1 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 类别 |  | 模型 |  | F1 | 准确率 | 准确率 | 准确率 | F1 | 准确率 | 准确率 | MCC | Spearman |
    1.1 F1 | 2.0 F1 |'
- en: '| Original Models | BERT-B (Devlin et al., [2018](#bib.bib25)) | - | 109M |
    88.9 | 83.4 | 84.6 | 93.5 | 71.2 | 90.5 | 66.4 | 52.1 | 85.8 | 88.4 | 77.7 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 原始模型 | BERT-B (Devlin et al., [2018](#bib.bib25)) | - | 109M | 88.9 | 83.4
    | 84.6 | 93.5 | 71.2 | 90.5 | 66.4 | 52.1 | 85.8 | 88.4 | 77.7 |'
- en: '| BERT-L (Devlin et al., [2018](#bib.bib25)) | - | 340M | 89.3 | 85.9 | 86.7
    | 94.9 | 72.1 | 92.7 | 70.1 | 60.5 | 86.5 | 90.9 | 81.9 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| BERT-L （Devlin等，[2018](#bib.bib25)） | - | 340M | 89.3 | 85.9 | 86.7 | 94.9
    | 72.1 | 92.7 | 70.1 | 60.5 | 86.5 | 90.9 | 81.9 |'
- en: '| MTDNN (ensemble) (Liu et al., [2019b](#bib.bib75)) | - | 340*4M | 90.0 |
    87.2 | 86.7 | 95.6 | 72.4 | 93.9 | 80.9 | 61.5 | 88.3 | - | - |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| MTDNN (集成) （刘等，[2019b](#bib.bib75)） | - | 340*4M | 90.0 | 87.2 | 86.7 | 95.6
    | 72.4 | 93.9 | 80.9 | 61.5 | 88.3 | - | - |'
- en: '| Knowledge Distillation Methods | Distilled-BiLSTM (Tang et al., [2019](#bib.bib115))
    | BERT-L/BiLSTM | 0.96M | - | 72.6 | 73.0 | 90.7 | 68.2 | - | - | - | - | - |
    - |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏方法 | 蒸馏-BiLSTM （Tang等，[2019](#bib.bib115)） | BERT-L/BiLSTM | 0.96M |
    - | 72.6 | 73.0 | 90.7 | 68.2 | - | - | - | - | - | - |'
- en: '| Mixed-vocab. training (Zhao et al., [2019](#bib.bib143)) | BERT-L/BERT-12
    | 10.9M | 87.2 | 80.5 | 80.7 | 90.6 | - | - | - | - | - | - | - |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 混合词汇训练 （赵等，[2019](#bib.bib143)） | BERT-L/BERT-12 | 10.9M | 87.2 | 80.5 |
    80.7 | 90.6 | - | - | - | - | - | - | - |'
- en: '| TinyBERT (Jiao et al., [2019](#bib.bib51)) | BERT-B/BERT-4 | 14.5M | 86.4
    | 81.8 | 82.5 | 92.6 | 71.3 | 87.7 | 66.6 | 44.1 | 80.4 | 82.1 | 71.8 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| TinyBERT （焦等，[2019](#bib.bib51)） | BERT-B/BERT-4 | 14.5M | 86.4 | 81.8 |
    82.5 | 92.6 | 71.3 | 87.7 | 66.6 | 44.1 | 80.4 | 82.1 | 71.8 |'
- en: '| BERT-EMD (Li et al., [2020](#bib.bib67)) | BERT-B/BERT-4 | 14.5M | 87.6 |
    80.6 | 82.1 | 91.0 | 69.3 | 87.2 | 66.2 | 25.6 | 82.3 | - | - |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| BERT-EMD （李等，[2020](#bib.bib67)） | BERT-B/BERT-4 | 14.5M | 87.6 | 80.6 |
    82.1 | 91.0 | 69.3 | 87.2 | 66.2 | 25.6 | 82.3 | - | - |'
- en: '| MobileBERT (Sun et al., [2020](#bib.bib112)) | BERT-L/BERT-6 | 25.3M | 88.8
    | 82.6 | 83.3 | 92.8 | 70.2 | 90.6 | 66.2 | 50.5 | 84.4 | 90.0 | 79.2 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| MobileBERT （孙等，[2020](#bib.bib112)） | BERT-L/BERT-6 | 25.3M | 88.8 | 82.6
    | 83.3 | 92.8 | 70.2 | 90.6 | 66.2 | 50.5 | 84.4 | 90.0 | 79.2 |'
- en: '| MobileBERT+Quantization (Sun et al., [2020](#bib.bib112)) | BERT-L/BERT-6
    | 25.3M | 87.0 | - | 83.9 | 91.9 | - | 90.8 | - | - | - | 90.0 | - |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| MobileBERT+量化 （孙等，[2020](#bib.bib112)） | BERT-L/BERT-6 | 25.3M | 87.0 | -
    | 83.9 | 91.9 | - | 90.8 | - | - | - | 90.0 | - |'
- en: '| MiniLM (Wang et al., [2020b](#bib.bib130)) | BERT-B/BERT-12 | 33M | 89.5
    | - | 85.7 | 93.0 | 91.3 | 91.5 | 73.3 | 58.5 | - | - | 81.7 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| MiniLM （王等，[2020b](#bib.bib130)） | BERT-B/BERT-12 | 33M | 89.5 | - | 85.7
    | 93.0 | 91.3 | 91.5 | 73.3 | 58.5 | - | - | 81.7 |'
- en: '| SqueezeBERT (Iandola et al., [2020](#bib.bib50)) | BERT-B/SqueezeBERT | 51.1M
    | 87.8 | 81.1 | 82.0 | 91.4 | 80.3 | 90.1 | 73.2 | 46.5 | 86.7 | - | - |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeBERT （Iandola等，[2020](#bib.bib50)） | BERT-B/SqueezeBERT | 51.1M |
    87.8 | 81.1 | 82.0 | 91.4 | 80.3 | 90.1 | 73.2 | 46.5 | 86.7 | - | - |'
- en: '| DistilBERT (Sanh et al., [2019](#bib.bib102)) | BERT-B/BERT-4 | 52.2M | 82.4
    | 78.0 | 78.9 | 91.4 | 68.5 | 85.2 | 54.1 | 32.8 | 76.1 | 81.2 | 64.1 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| DistilBERT （Sanh等，[2019](#bib.bib102)） | BERT-B/BERT-4 | 52.2M | 82.4 | 78.0
    | 78.9 | 91.4 | 68.5 | 85.2 | 54.1 | 32.8 | 76.1 | 81.2 | 64.1 |'
- en: '| Patient KD (Sun et al., [2019](#bib.bib111)) | BERT-B/BERT-4 | 52.2M | 82.6
    | 79.3 | 79.9 | 89.4 | 70.2 | 85.1 | 62.3 | 24.8 | 79.8 | 79.5 | 64.6 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| Patient KD （孙等，[2019](#bib.bib111)） | BERT-B/BERT-4 | 52.2M | 82.6 | 79.3
    | 79.9 | 89.4 | 70.2 | 85.1 | 62.3 | 24.8 | 79.8 | 79.5 | 64.6 |'
- en: '| BERT-of-Theseus (Xu et al., [2020](#bib.bib134)) | BERT-B/BERT-6 | 66M |
    87.6 | 82.1 | 82.4 | 92.2 | 71.6 | 89.6 | 66.2 | 47.8 | 84.1 | - | - |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| BERT-of-Theseus （徐等，[2020](#bib.bib134)） | BERT-B/BERT-6 | 66M | 87.6 | 82.1
    | 82.4 | 92.2 | 71.6 | 89.6 | 66.2 | 47.8 | 84.1 | - | - |'
- en: '| MiniLM (Wang et al., [2020b](#bib.bib130)) | BERT-B/BERT-6 | 66M | 88.4 |
    - | 84.0 | 92.0 | 91.0 | 91.0 | 71.5 | 49.2 | - | - | 76.4 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| MiniLM （王等，[2020b](#bib.bib130)） | BERT-B/BERT-6 | 66M | 88.4 | - | 84.0
    | 92.0 | 91.0 | 91.0 | 71.5 | 49.2 | - | - | 76.4 |'
- en: '| BERT-EMD (Li et al., [2020](#bib.bib67)) | BERT-B/BERT-6 | 66M | 89.8 | 83.5
    | 84.7 | 93.3 | 72.0 | 90.7 | 71.7 | 47.5 | 86.8 | - | - |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| BERT-EMD （李等，[2020](#bib.bib67)） | BERT-B/BERT-6 | 66M | 89.8 | 83.5 | 84.7
    | 93.3 | 72.0 | 90.7 | 71.7 | 47.5 | 86.8 | - | - |'
- en: '| Patient KD (Sun et al., [2019](#bib.bib111)) | BERT-B/BERT-6 | 67M | 85.0
    | 81.0 | 81.5 | 92.0 | 70.7 | 89.0 | 65.5 | 43.5 | 81.6 | 85.3 | 69.8 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| Patient KD （孙等，[2019](#bib.bib111)） | BERT-B/BERT-6 | 67M | 85.0 | 81.0 |
    81.5 | 92.0 | 70.7 | 89.0 | 65.5 | 43.5 | 81.6 | 85.3 | 69.8 |'
- en: '| Vanilla KD (Hinton et al., [2015](#bib.bib44)) | BERT-B/BERT-6 | 67M | 86.2
    | 79.8 | 80.2 | 91.5 | 70.1 | 88.3 | 64.7 | - | - | - | - |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| Vanilla KD （辛顿等，[2015](#bib.bib44)） | BERT-B/BERT-6 | 67M | 86.2 | 79.8 |
    80.2 | 91.5 | 70.1 | 88.3 | 64.7 | - | - | - | - |'
- en: '| DistilBERT (Sanh et al., [2019](#bib.bib102)) | BERT-B/BERT-6 | 67M | 86.9
    | 81.3 | 82.6 | 92.5 | 70.1 | 88.9 | 58.4 | 49.0 | 81.3 | 86.2 | 69.5 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| DistilBERT （Sanh等，[2019](#bib.bib102)） | BERT-B/BERT-6 | 67M | 86.9 | 81.3
    | 82.6 | 92.5 | 70.1 | 88.9 | 58.4 | 49.0 | 81.3 | 86.2 | 69.5 |'
- en: '| TinyBERT (Jiao et al., [2019](#bib.bib51)) | BERT-B/BERT-6 | 67M | 87.3 |
    83.2 | 84.6 | 93.1 | 71.6 | 90.4 | 70.0 | 51.1 | 83.7 | 87.5 | 77.7 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| TinyBERT （焦等，[2019](#bib.bib51)） | BERT-B/BERT-6 | 67M | 87.3 | 83.2 | 84.6
    | 93.1 | 71.6 | 90.4 | 70.0 | 51.1 | 83.7 | 87.5 | 77.7 |'
- en: '| Pretrained Distillation (Turc et al., [2019](#bib.bib120)) | BERT-B/BERT-6
    | 67M | 86.8 | 82.2 | 82.8 | 91.8 | 70.4 | 88.9 | 65.3 | - | - | - | - |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 预训练蒸馏 （Turc等，[2019](#bib.bib120)） | BERT-B/BERT-6 | 67M | 86.8 | 82.2 | 82.8
    | 91.8 | 70.4 | 88.9 | 65.3 | - | - | - | - |'
- en: '| MTDNN-KD (Liu et al., [2019a](#bib.bib74)) | MTDNN/MTDNN-KD | 340M | 91.1
    | 86.7 | 87.5 | 95.6 | 72.7 | 96.0 | 85.1 | 65.4 | 89.6 | - | - |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| MTDNN-KD (Liu et al., [2019a](#bib.bib74)) | MTDNN/MTDNN-KD | 340M | 91.1
    | 86.7 | 87.5 | 95.6 | 72.7 | 96.0 | 85.1 | 65.4 | 89.6 | - | - |'
- en: '| Param. | ALBERT-B (Lan et al., [2019](#bib.bib61)) (dev) | - | 12M | - |
    - | 81.6 | 90.3 | - | - | - | - | - | 89.3 | 80 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | ALBERT-B (Lan et al., [2019](#bib.bib61)) (dev) | - | 12M | - | - |
    81.6 | 90.3 | - | - | - | - | - | 89.3 | 80 |'
- en: '| Sharing | ALBERT-L (Lan et al., [2019](#bib.bib61)) (dev) | - | 18M | - |
    - | 83.5 | 91.7 | - | - | - | - | - | 90.6 | 82.3 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 共享 | ALBERT-L (Lan et al., [2019](#bib.bib61)) (dev) | - | 18M | - | - |
    83.5 | 91.7 | - | - | - | - | - | 90.6 | 82.3 |'
- en: '| Tensor Decomp. | FLOP (Wang et al., [2019c](#bib.bib131)) | - | 80M | 88.61
    | - | - | 92.09 | - | 89.05 | - | - | 88.18 | - | - |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 张量分解 | FLOP (Wang et al., [2019c](#bib.bib131)) | - | 80M | 88.61 | - | -
    | 92.09 | - | 89.05 | - | - | 88.18 | - | - |'
- en: '| Pruning | RPP Iterative Magnitude Pruning (Guo et al., [2019a](#bib.bib33))
    | - | 138M | 88.1 | 86.1 | 85.7 | 92.4 | 91.2 | 92.3 | 70.1 | 82.8 | - | 90.23
    | 75.3 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝 | RPP 迭代幅度剪枝 (Guo et al., [2019a](#bib.bib33)) | - | 138M | 88.1 | 86.1
    | 85.7 | 92.4 | 91.2 | 92.3 | 70.1 | 82.8 | - | 90.23 | 75.3 |'
- en: '| Iterative Magnitude Pruning (Guo et al., [2019a](#bib.bib33)) | - | 170M
    | 83.5 | 77 | 82.5 | 91.3 | 85.1 | 90.2 | 68.6 | 76.3 | - | 85.3 | - |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 迭代幅度剪枝 (Guo et al., [2019a](#bib.bib33)) | - | 170M | 83.5 | 77 | 82.5 |
    91.3 | 85.1 | 90.2 | 68.6 | 76.3 | - | 85.3 | - |'
- en: '| LayerDrop (Fan et al., [2019](#bib.bib28)) | - | 66M | 85.3 | - | 82.9 |
    92.5 | - | 89.4 | - | - | - | - | - |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| LayerDrop (Fan et al., [2019](#bib.bib28)) | - | 66M | 85.3 | - | 82.9 |
    92.5 | - | 89.4 | - | - | - | - | - |'
- en: '| Quant. | Mixed-precision quant. (QBERTMP) (Shen et al., [2019](#bib.bib105))
    | - | - | - | 82.29 | 81.75 | 92.08 | - | - | - | - | - | 86.95 | - |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 混合精度量化 (QBERTMP) (Shen et al., [2019](#bib.bib105)) | - | - | - | 82.29
    | 81.75 | 92.08 | - | - | - | - | - | 86.95 | - |'
- en: '| SubQuad Trans. | Linformer (Wang et al., [2020a](#bib.bib129)) | - | - |
    - | - | - | 93.1 | 90.8 | 91.2 | - | - | - | - | - |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| SubQuad Trans. | Linformer (Wang et al., [2020a](#bib.bib129)) | - | - |
    - | - | - | 93.1 | 90.8 | 91.2 | - | - | - | - | - |'
- en: Table 4\. Comparison of various methods across various GLUE (Wang et al., [2019b](#bib.bib126))
    and SQuAD tasks. Please refer (Wang et al., [2019b](#bib.bib126)) for detailed
    description of tasks. Top part (first three rows) shows results for basic Transformer
    methods (teacher models). Middle part shows results for knowledge distillation
    methods. Bottom part shows results for a mix of other methods across categories.
    BERT-L=BERT-large, BERT-B=BERT-base, BERT-$i$=$i$-layer BERT. MCC refers to Matthews
    correlation. Results for SQuAD are on dev set. Empty entries indicate that the
    papers do not report those results, or NA entries.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 比较了各种方法在不同 GLUE (Wang et al., [2019b](#bib.bib126)) 和 SQuAD 任务上的表现。有关任务的详细描述，请参见 (Wang
    et al., [2019b](#bib.bib126))。顶部部分（前三行）显示了基本 Transformer 方法（教师模型）的结果。中间部分显示了知识蒸馏方法的结果。底部部分显示了各种类别的其他方法的结果。BERT-L=BERT-large，BERT-B=BERT-base，BERT-$i$=$i$-层
    BERT。MCC 指的是 Matthews 相关系数。SQuAD 的结果来自开发集。空白项表示论文未报告这些结果，或标记为 NA。
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey") compares various knowledge
    distillation methods (besides other model compression methods) across various
    GLUE (Wang et al., [2019b](#bib.bib126)) and SQuAD tasks. Also, we report model
    size for both the student as well as the teacher models. Different distillation
    methods use one of these as the teacher: BERT-large, BERT-base or MTDNN. PKD (Sun
    et al., [2019](#bib.bib111)) outperforms the Vanilla KD (Hinton et al., [2015](#bib.bib44))
    on almost all the datasets except for MRPC. TinyBERT-4 significantly outperforms
    the 4-layer BERT-PKD and DistilBERT by a margin of at least 4.4%, with  28% parameters
    and 3.1x inference speedup. Compared with the teacher BERT-base, 4-layer TinyBERT
    is 7.5x smaller and 9.4x faster in the model efficiency, while maintaining competitive
    performances. The 6-layer TinyBERT achieves comparable results with the teacher.
    Overall, TinyBERT consistently outperforms both the 4-layer and 6-layer baselines
    like PKD, DistilBERT and MiniLM.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S4.T4 "Table 4 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD) ‣ Compression
    of Deep Learning Models for Text: A Survey") 比较了各种知识蒸馏方法（以及其他模型压缩方法）在不同 GLUE (Wang
    et al., [2019b](#bib.bib126)) 和 SQuAD 任务上的表现。此外，我们还报告了学生模型和教师模型的大小。不同的蒸馏方法使用 BERT-large、BERT-base
    或 MTDNN 作为教师模型。PKD (Sun et al., [2019](#bib.bib111)) 在几乎所有数据集上的表现都优于 Vanilla KD (Hinton
    et al., [2015](#bib.bib44))，唯独在 MRPC 数据集上除外。TinyBERT-4 显著优于 4 层 BERT-PKD 和 DistilBERT，性能提升至少为
    4.4%，参数减少 28%，推理速度提升 3.1 倍。与教师 BERT-base 相比，4 层 TinyBERT 在模型效率上小 7.5 倍，快 9.4 倍，同时保持竞争力的表现。6
    层 TinyBERT 达到了与教师相当的结果。总体而言，TinyBERT 一直优于 PKD、DistilBERT 和 MiniLM 这类的 4 层和 6 层基线模型。'
- en: Turc et al. (Turc et al., [2019](#bib.bib120)) show how appropriate pretraining
    can improve the quality of distillation. MiniLM outperforms DistilBERT and TinyBERT
    across most tasks. The 6-layer MiniLM is 2.0x faster than original BERTBASE, while
    retaining more than 99% performance on a variety of tasks, such as SQuAD 2.0 and
    MNLI. Distilled MT-DNN significantly outperforms the original MT-DNN on 7 out
    of 9 GLUE tasks. 4-layer BERT-EMD outperforms 4-layer DistilBERT and BERT-PKD
    by a substantial margin, even with only 30% parameters and inference time. Furthermore,
    it exceeds the TinyBERT model by 2.3% accuracy on RTE, 2.2% F1 on MRPC, and 1.9%
    Spearman correlation on STS-B. 6-layer BERT-EMD performs better than the 12-layer
    BERT-base model on 7 out of 9 tasks, with only about 50% parameters and inference
    time of the original BERT-base model. Tang et al. (Tang et al., [2019](#bib.bib115))
    distill BERT to BiLSTMs. They observe that the distilled BiLSTM model uses 349
    times fewer parameters than BERT-large and is 434 times faster. Also, mixed vocab
    training by Zhao et al. (Zhao et al., [2019](#bib.bib143)) produces a small 12-layer
    model that performs competitively with 6-layer PKD and 4-layer DistilBERT while
    being $\sim$5-6x smaller.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: Turc 等人（Turc et al., [2019](#bib.bib120)）展示了适当的预训练如何提高蒸馏的质量。MiniLM 在大多数任务中优于
    DistilBERT 和 TinyBERT。6 层 MiniLM 比原始 BERTBASE 快 2.0 倍，同时在各种任务中保持了 99% 以上的性能，如
    SQuAD 2.0 和 MNLI。蒸馏的 MT-DNN 在 9 个 GLUE 任务中的 7 个上显著优于原始的 MT-DNN。4 层 BERT-EMD 在参数和推理时间仅为
    30% 的情况下，比 4 层 DistilBERT 和 BERT-PKD 高出不少。此外，它在 RTE 上的准确率比 TinyBERT 高 2.3%，在 MRPC
    上的 F1 比 TinyBERT 高 2.2%，在 STS-B 上的 Spearman 相关性比 TinyBERT 高 1.9%。6 层 BERT-EMD
    在 9 个任务中的 7 个上表现优于 12 层 BERT-base 模型，同时其参数和推理时间仅为原始 BERT-base 模型的约 50%。Tang 等人（Tang
    et al., [2019](#bib.bib115)）将 BERT 蒸馏到 BiLSTMs。他们观察到，蒸馏后的 BiLSTM 模型使用的参数比 BERT-large
    少 349 倍，速度快 434 倍。此外，Zhao 等人（Zhao et al., [2019](#bib.bib143)）通过混合词汇训练生产了一个小型的
    12 层模型，其性能与 6 层 PKD 和 4 层 DistilBERT 相竞争，同时其体积约为 5-6 倍小。
- en: MobileBERT is 4.3x smaller and 5.5x faster than BERT-base. On the SQuAD v1.1/v2.0
    question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1
    higher than BERT-base). On the natural language inference tasks of GLUE, MobileBERT
    can achieve a GLUE score of 77.7, which is only 0.6 lower than BERT-base, with
    a latency of 62 ms on a Pixel 4 phone. While quantization can further compress
    MobileBERT by 4x, there is nearly no performance degradation from it. SqueezeBERT
    is approximately half the size of BERT-base. MobileBERT is half the size of SqueezeBERT.
    SqueezeBERT is 4.3x faster than BERT-base, while MobileBERT is 3.0x faster than
    BERT-base. On 4 of GLUE tasks SqueezeBERT outperforms the accuracy of MobileBERT,
    while on the other 4 of the GLUE tasks MobileBERT outperforms SqueezeBERT. MobileBERT
    and SqueezeBERT outperform BERT-base significantly across all tasks.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: MobileBERT 比 BERT-base 小 4.3 倍，速度快 5.5 倍。在 SQuAD v1.1/v2.0 问答任务中，MobileBERT
    实现了 90.0/79.2 的开发 F1 分数（比 BERT-base 高 1.5/2.1）。在 GLUE 的自然语言推理任务中，MobileBERT 可以实现
    77.7 的 GLUE 分数，比 BERT-base 仅低 0.6，且在 Pixel 4 手机上的延迟为 62 毫秒。虽然量化可以进一步压缩 MobileBERT
    达到 4 倍，但几乎没有性能下降。SqueezeBERT 的体积大约是 BERT-base 的一半。MobileBERT 的体积是 SqueezeBERT
    的一半。SqueezeBERT 比 BERT-base 快 4.3 倍，而 MobileBERT 比 BERT-base 快 3.0 倍。在 GLUE 的
    4 个任务中，SqueezeBERT 的准确率超过了 MobileBERT，而在其他 4 个 GLUE 任务中，MobileBERT 超过了 SqueezeBERT。MobileBERT
    和 SqueezeBERT 在所有任务中都显著优于 BERT-base。
- en: To summarize, KD is a popular method for text based model compression. Various
    methods have proposed information copying using logits, softmax output, attention
    sub-layer output, value relation, relative dissimilarity information from both
    the last layer as well as intermediate layers of the teacher. Many methods have
    been proposed to handle complex teacher-student configuration mismatches in terms
    of vocabulary, number of attention heads, and hidden layer sizes. Also, KD has
    been found to be very effective in complex problem settings like multi-lingual
    tasks and tasks with large number of classes. Learning from noisy teachers, teacher
    assistants, an ensemble of teachers has been found to be effective as well. KD
    is the best model compression method especially in settings where a large amount
    of unlabeled data exists; distillation with data pseudo-labeled by teacher leads
    to very effective students.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，KD 是一种流行的基于文本的模型压缩方法。各种方法提出了使用 logits、softmax 输出、注意力子层输出、值关系、来自教师的最后一层以及中间层的相对不相似信息来复制信息。许多方法已被提出以处理词汇、注意力头数量和隐藏层大小方面的复杂教师-学生配置不匹配问题。此外，KD
    在多语言任务和类别数目较多的任务等复杂问题设置中被发现非常有效。从嘈杂的教师、教师助理、教师集成中学习也被发现有效。KD 是最佳的模型压缩方法，特别是在存在大量未标记数据的情况下；由教师伪标记的数据进行蒸馏会产生非常有效的学生模型。
- en: 5\. Parameter sharing
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 参数共享
- en: 'Rather than removing weights or reducing #bits to store them, parameter sharing
    methods reduce model size by finding weight blocks that can share the same weight.
    Character-based language models learn embeddings for characters and use them to
    compose word embeddings. In some senses, we can think of various words sharing
    these character embedding parameters. Further, various parameter sharing methods
    have been proposed to reduce the large word embedding matrix size. Finally, there
    are multiple Transformer architectures which benefit from the parameter sharing
    philosophy. We discuss these methods in this section.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 与其移除权重或减少存储它们的#bits，参数共享方法通过找到可以共享相同权重的权重块来减少模型大小。基于字符的语言模型学习字符的嵌入并使用这些嵌入来组合词嵌入。在某些方面，我们可以认为各种词共享这些字符嵌入参数。此外，已经提出了各种参数共享方法来减少大型词嵌入矩阵的大小。最后，有多种
    Transformer 架构受益于参数共享的理念。本节将讨论这些方法。
- en: 5.1\. Character-aware Language Models
  id: totrans-537
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 基于字符的语言模型
- en: 'Fig. [5](#S5.F5 "Figure 5 ‣ 5.1\. Character-aware Language Models ‣ 5\. Parameter
    sharing ‣ Compression of Deep Learning Models for Text: A Survey") illustrates
    various character-aware language model architectures. Ling et al. (Ling et al.,
    [2015](#bib.bib72)) proposed their character to word (C2W) model which constructs
    vector representations of words by composing characters using BiLSTMs. Relative
    to traditional word representation models that have independent vectors for each
    word type, C2W requires only a single vector per character type and a fixed set
    of parameters for the compositional model. As input, we define an alphabet of
    characters $C$. For English, this vocabulary would contain an entry for each uppercase
    and lowercase letter as well as numbers and punctuation. Thus compared to the
    word embedding matrix, this model is much smaller. Despite the compactness of
    this model, this “composed” word representations yield comparable results across
    multiple text classification tasks.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S5.F5 "Figure 5 ‣ 5.1\. Character-aware Language Models ‣ 5\. Parameter
    sharing ‣ Compression of Deep Learning Models for Text: A Survey")展示了各种基于字符的语言模型架构。Ling
    等人（Ling et al., [2015](#bib.bib72)）提出了他们的字符到词（C2W）模型，该模型通过使用 BiLSTMs 组合字符来构建词的向量表示。与传统的词表示模型每个词类型都有独立的向量不同，C2W
    每个字符类型只需要一个向量和一个固定的组合模型参数集。我们将字符 $C$ 定义为输入字母表。对于英语，这个词汇表将包含每个大写字母、小写字母、数字和标点符号的条目。因此，与词嵌入矩阵相比，该模型要小得多。尽管模型紧凑，这种“组合”的词表示在多个文本分类任务中仍能产生可比的结果。'
- en: Jozefowicz et al. (Jozefowicz et al., [2016](#bib.bib52)) propose two variants
    for composing word embeddings using character embeddings. In the first CNN-Softmax
    variant, they use character CNNs (Convolutional Neural Networks) to compose word
    embeddings from character embeddings both at the input side as well as at the
    output softmax layer. The character-CNN sub-networks at the input or the output
    do not share weights. The composed word embeddings are fed to an LSTM to generate
    the output. In the second Char-LSTM variant, character CNN is used to compose
    word embeddings on the input side. The composed word embeddings are fed to an
    LSTM to generate an output which is further fed to a small LSTM that predicts
    the target word one character at a time. Thus, the word and character-level models
    are combined, and predictions are made one character at a time, thus allowing
    to compute probabilities over a much smaller vocabulary. Kim et al. (Kim et al.,
    [2016](#bib.bib56)) propose another variant where at the output side they continue
    to use word embeddings, but at the input side they compose word embeddings using
    a highway network on top of a character CNN. The highway network’s output is used
    as the input to a multi-layer LSTM, whose last hidden state output is fed to the
    output softmax layer.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: Jozefowicz等人（Jozefowicz et al., [2016](#bib.bib52)）提出了两种使用字符嵌入来构建词嵌入的变体。在第一个CNN-Softmax变体中，他们使用字符CNN（卷积神经网络）从字符嵌入构建词嵌入，既在输入端也在输出softmax层。输入端或输出端的字符-CNN子网络不共享权重。构建的词嵌入被馈送到LSTM中生成输出。在第二个Char-LSTM变体中，字符CNN用于在输入端构建词嵌入。构建的词嵌入被馈送到LSTM中生成输出，然后进一步馈送到一个小型LSTM，该LSTM逐字符预测目标词。因此，词级和字符级模型被结合，预测是逐字符进行的，从而允许在更小的词汇表上计算概率。Kim等人（Kim
    et al., [2016](#bib.bib56)）提出了另一种变体，其中在输出端他们继续使用词嵌入，但在输入端他们使用字符CNN上面的高速公路网络构建词嵌入。高速公路网络的输出作为多层LSTM的输入，最后隐藏状态输出被馈送到输出softmax层。
- en: '![Refer to caption](img/959519ae9d3d51ee05373e26c7fc5021.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/959519ae9d3d51ee05373e26c7fc5021.png)'
- en: Figure 5\. Character-aware Language Models
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 具有字符感知的语言模型
- en: 5.2\. Parameter Sharing in the Embedding Matrix
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 嵌入矩阵中的参数共享
- en: Given a weight matrix $W$ and a budget $K$, we want to share weights within
    $W$ to have a max of $K$ unique values. A naïve implementation of random weight
    sharing can be trivially achieved by maintaining a secondary matrix consisting
    of each connection’s group assignment. But this needs memory space itself. Hence,
    Chen et al. (Chen et al., [2015b](#bib.bib13)) propose to use hashing. HashedNets
    use a low-cost hash function (like xxhash³³3[https://code.google.com/p/xxhash/](https://code.google.com/p/xxhash/))
    to randomly group connection weights into hash buckets, and all connections within
    the same hash bucket share a single parameter value.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个权重矩阵$W$和一个预算$K$，我们希望在$W$中共享权重，使得唯一值的最大数量为$K$。通过维护一个包含每个连接的组分配的辅助矩阵，朴素的随机权重共享实现是非常简单的。但这本身需要内存空间。因此，Chen等人（Chen
    et al., [2015b](#bib.bib13)）提出使用哈希。HashedNets使用低成本的哈希函数（如xxhash³³3[https://code.google.com/p/xxhash/](https://code.google.com/p/xxhash/)）将连接权重随机分组到哈希桶中，所有在同一个哈希桶中的连接共享一个参数值。
- en: Unlike HashedNets where weights are randomly grouped, parameter sharing mechanisms
    in Toeplitz-like structured matrices (Lu et al., [2016](#bib.bib77)) are highly
    specific and deterministic. Toeplitz matrices have parameters tied along diagonals.
    The displacement rank of all Toeplitz matrices is up to 2\. Toeplitz-like matrices
    allow the displacement rank $r$ to be higher. They include products and inverses
    of Toeplitz matrices, and their linear combinations. The displacement rank $r$
    serves as a knob on modeling capacity. High displacement rank matrices are increasingly
    unstructured. With displacement rank $r$, there are $2nr$ free parameters in the
    Toeplitz-like structured matrix. Toeplitz transforms can be applied not just to
    embedding matrix but to all weight matrices in an RNN model. Tay et al. (Tay et al.,
    [2019](#bib.bib117)) use a similar Toeplitz-like structured matrix method with
    Hamilton Products in Quaternion Algebra to propose Quaternion Transformers which
    lead to 75% parameter reduction in the Transformer architecture.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 与 HashedNets 中权重随机分组不同，Toeplitz-like 结构矩阵中的参数共享机制 (Lu et al., [2016](#bib.bib77))
    是高度特定且确定性的。Toeplitz 矩阵的参数沿对角线绑定。所有 Toeplitz 矩阵的位移秩高达 2。Toeplitz-like 矩阵允许位移秩 $r$
    更高。它们包括 Toeplitz 矩阵的乘积和逆矩阵及其线性组合。位移秩 $r$ 作为建模能力的调整旋钮。高位移秩矩阵越来越不规则。具有位移秩 $r$ 的
    Toeplitz-like 结构矩阵中有 $2nr$ 个自由参数。Toeplitz 变换不仅可以应用于嵌入矩阵，还可以应用于 RNN 模型中的所有权重矩阵。Tay
    et al. (Tay et al., [2019](#bib.bib117)) 使用类似的 Toeplitz-like 结构矩阵方法与四元数代数中的 Hamilton
    产品结合，提出了四元数 Transformer，从而在 Transformer 架构中实现了 75% 的参数减少。
- en: Another method for parameter sharing is to share low-rank factors across layers
    in a recurrent model. In this method, we first represent a weight matrix $W$ using
    matrix factorization as $W=W_{a}W_{b}$. Thus, hidden layer output for layer $l$
    at time $t$ can be written as follows.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种参数共享的方法是在递归模型中跨层共享低秩因子。在这种方法中，我们首先将权重矩阵 $W$ 通过矩阵分解表示为 $W=W_{a}W_{b}$。因此，时间
    $t$ 时层 $l$ 的隐藏层输出可以写成如下形式。
- en: '| (59) |  | $\displaystyle h_{t}^{l}=\sigma\left[W_{a}^{l}W_{b}^{l}h_{t}^{l-1}+U_{a}^{l}U_{b}^{l}h_{t-1}^{l}+b^{l}\right]$
    |  |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| (59) |  | $\displaystyle h_{t}^{l}=\sigma\left[W_{a}^{l}W_{b}^{l}h_{t}^{l-1}+U_{a}^{l}U_{b}^{l}h_{t-1}^{l}+b^{l}\right]$
    |  |'
- en: But we can share some low-rank factors by setting $W_{b}^{l}=U_{b}^{l-1}$. The
    combination of matrix factorization and parameter sharing leads to large model
    compression.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以通过设置 $W_{b}^{l}=U_{b}^{l-1}$ 来共享一些低秩因子。矩阵分解与参数共享的组合导致了模型压缩的**巨大**效果。
- en: Another way of compressing the embedding matrix is to divide the vocabulary
    $V$ into frequent and infrequent word sets $B$ and $C$ respectively. Infrequent
    words’ embeddings are represented with frequent words’ by sparse linear combinations (Chen
    et al., [2016](#bib.bib14)). This is inspired by the observation that, in a dictionary,
    an unfamiliar word is typically defined by common words. A dense embedding is
    assigned to each common word; an infrequent word, on the other hand, computes
    its vector representation by a sparse combination of common words’ embeddings.
    This compression is useful for both word embedding matrix as well as output layer
    of RNNs/LSTMs. Let $U\in R^{E\times|B|}$ be the learned embedding matrix of common
    words where $E$ is the embedding dimension. For a word $w\in C$, we shall learn
    a sparse vector $x\in R^{|B|}$ as the sparse code of the word. Once we know $x$,
    embedding for a word $w\in C$ can be written as follows.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩嵌入矩阵的另一种方法是将词汇 $V$ 分成频繁词和不频繁词集合 $B$ 和 $C$。不频繁词的嵌入通过频繁词的稀疏线性组合进行表示 (Chen et
    al., [2016](#bib.bib14))。这源于以下观察：在字典中，一个不熟悉的词通常由常见词定义。每个常见词分配一个稠密的嵌入；另一方面，不频繁词通过常见词嵌入的稀疏组合计算其向量表示。这种压缩对于词嵌入矩阵以及
    RNNs/LSTMs 的输出层都很有用。设 $U\in R^{E\times|B|}$ 为常见词的学习嵌入矩阵，其中 $E$ 是嵌入维度。对于一个词 $w\in
    C$，我们将学习一个稀疏向量 $x\in R^{|B|}$ 作为该词的稀疏编码。一旦我们知道了 $x$，则可以将词 $w\in C$ 的嵌入写成如下形式。
- en: '| (60) |  | $\displaystyle\text{embedding}(w)=\sum_{j=1}^{B}x_{j}U_{j}$ |  |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| (60) |  | $\displaystyle\text{embedding}(w)=\sum_{j=1}^{B}x_{j}U_{j}$ |  |'
- en: where $U_{j}$ is the $j^{th}$ column of $U$. To learn the sparse representation
    of word $w\in C$, the following problem needs to be solved.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U_{j}$ 是 $U$ 的第 $j^{th}$ 列。为了学习单词 $w\in C$ 的稀疏表示，需要解决以下问题。
- en: '| (61) |  | $\displaystyle\min_{x}&#124;&#124;Ux-A&#124;&#124;_{2}^{2}+\alpha&#124;&#124;x&#124;&#124;_{1}+\beta&#124;1^{T}x-1&#124;+\gamma
    1^{T}\max(0,-x)$ |  |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| (61) |  | $\displaystyle\min_{x}&#124;&#124;Ux-A&#124;&#124;_{2}^{2}+\alpha&#124;&#124;x&#124;&#124;_{1}+\beta&#124;1^{T}x-1&#124;+\gamma
    1^{T}\max(0,-x)$ |  |'
- en: where $A$ is embedding for the rare word $w$. The last two regularization terms
    favor a solution that sums to 1 and that is non-negative (for psychological interpretation
    concerns), respectively.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 是稀有词 $w$ 的嵌入。最后两个正则化项分别倾向于求和为 1 和非负（出于心理解释考虑）。
- en: 'LightRNN (Li et al., [2016a](#bib.bib68)) compresses word embedding matrix
    from $O(|V|)$ to $O(\sqrt{|V|})$. It uses a 2-Component shared embedding for word
    representations. We allocate every word in the vocabulary into a word-allocation
    table, each row of which is associated with a learned vector, and each column
    associated with another learned vector. Table [5](#S5.T5 "Table 5 ‣ 5.2\. Parameter
    Sharing in the Embedding Matrix ‣ 5\. Parameter sharing ‣ Compression of Deep
    Learning Models for Text: A Survey") shows an example of a word allocation table.
    Depending on its position in the table, a word is jointly represented by two components:
    a row vector and a column vector. Thus, we only need $2\sqrt{|V|}$ vectors to
    represent a vocabulary of $|V|$ unique words, which are far less than the $|V|$
    vectors. The input and output use different embedding row/column vectors but they
    share the same word-allocation table. Word Allocation table creation uses a bootstrap
    procedure to iteratively refine word allocation based on the learned word embedding.
    Embeddings (i.e. row and column vectors) are learned using language modeling loss
    using an RNN on top of the embedding layer.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 'LightRNN（Li 等，[2016a](#bib.bib68)）将词嵌入矩阵从 $O(|V|)$ 压缩到 $O(\sqrt{|V|})$。它使用了一个
    2 组件共享嵌入来表示词汇。我们将词汇中的每个词分配到一个词分配表中，该表的每一行与一个学习的向量相关联，每一列与另一个学习的向量相关联。表[5](#S5.T5
    "Table 5 ‣ 5.2\. Parameter Sharing in the Embedding Matrix ‣ 5\. Parameter sharing
    ‣ Compression of Deep Learning Models for Text: A Survey")展示了一个词分配表的示例。根据词在表中的位置，一个词由两个组件联合表示：一行向量和一列向量。因此，我们只需要
    $2\sqrt{|V|}$ 个向量来表示 $|V|$ 个唯一词汇，这比 $|V|$ 个向量要少得多。输入和输出使用不同的嵌入行/列向量，但它们共享相同的词分配表。词分配表的创建使用了引导程序，以基于学习到的词嵌入迭代地优化词分配。嵌入（即行向量和列向量）使用语言建模损失通过在嵌入层之上的
    RNN 进行学习。'
- en: '| Embedding | $x_{1}^{c}$ | $x_{2}^{c}$ | $x_{3}^{c}$ | $x_{4}^{c}$ |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | $x_{1}^{c}$ | $x_{2}^{c}$ | $x_{3}^{c}$ | $x_{4}^{c}$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $x_{1}^{r}$ | january | february | … | … |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| $x_{1}^{r}$ | january | february | … | … |'
- en: '| $x_{2}^{r}$ | one | two | … | … |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| $x_{2}^{r}$ | one | two | … | … |'
- en: '| $x_{3}^{r}$ | … | … | … | … |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| $x_{3}^{r}$ | … | … | … | … |'
- en: '| $x_{4}^{r}$ | … | … | … | … |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| $x_{4}^{r}$ | … | … | … | … |'
- en: Table 5\. An Example of a Word Allocation Table
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 词分配表示例
- en: Finally, Suzuki et al. (Suzuki and Nagata, [2016](#bib.bib113)) propose a Skipgram (Mikolov
    et al., [2013](#bib.bib81)) training method with parameter sharing as follows.
    Split every embedding vector of size $D$ into $B$ equal sub-vectors of size $C$.
    Thus $D=B\times C$. We assign a limited number of reference vectors to each block
    of block-splitting vectors. E.g., the number of reference vectors becomes $K\times
    B$ if we assign $K$ reference vectors to each block. Each reference vector is
    of size $C$. Skipgram training optimization remains the same except for these
    extra parameter sharing constraints (applied to both the input and output embedding
    vectors). Liu et al. (Li et al., [2018](#bib.bib69)) propose a very similar method,
    Slim Embeddings, where the embeddings are learned using a RNN language model rather
    than the Skipgram method. Slim Embeddings is very related to HashedNets (Chen
    et al., [2015b](#bib.bib13)), LightRNN (Li et al., [2016a](#bib.bib68)) and Character
    Aware Language model (Kim et al., [2016](#bib.bib56)). In HashedNets, all elements
    in a parameter matrix are mapped into a vector through a hash function. However
    in Slim Embeddings approach, we randomly share subvectors instead of single elements.
    Slim Embeddings differs from LightRNN in that it is able to control the compression
    ratio to any arbitrary value, while LightRNN can only compress at the rate of
    square or cube root of vocabulary size, which could be too harsh in practical
    applications. In character aware language model, if we treat the sequence of subvector
    ids (virtual characters) as each word’s representation, the word embedding then
    can be treated as concatenated unigram character feature vectors. The drawback
    of such an approach is that the model is more complicated and to speed up inference,
    it needs to pre-compute the word embeddings for the words, so it couldn’t stay
    in its compact form during inference. The Slim embeddings model is much simpler,
    and easier to tune. And during inference, it uses much less space and can even
    decrease the complexity of inference.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，铃木等人（Suzuki 和 Nagata，[2016](#bib.bib113)）提出了一种带有参数共享的 Skipgram（Mikolov 等人，[2013](#bib.bib81)）训练方法如下。将每个大小为
    $D$ 的嵌入向量分割为 $B$ 个大小为 $C$ 的相等子向量。因此，$D=B\times C$。我们为每个分块向量块分配有限数量的参考向量。例如，如果我们为每个块分配
    $K$ 个参考向量，则参考向量的数量变为 $K\times B$。每个参考向量的大小为 $C$。Skipgram 训练优化保持不变，只是这些额外的参数共享约束（适用于输入和输出嵌入向量）。刘等人（Li
    等人，[2018](#bib.bib69)）提出了一种非常相似的方法，Slim Embeddings，其中嵌入是通过 RNN 语言模型学习的，而不是 Skipgram
    方法。Slim Embeddings 与 HashedNets（Chen 等人，[2015b](#bib.bib13)）、LightRNN（Li 等人，[2016a](#bib.bib68)）和
    Character Aware 语言模型（Kim 等人，[2016](#bib.bib56)）关系密切。在 HashedNets 中，参数矩阵中的所有元素通过哈希函数映射到一个向量。然而，在
    Slim Embeddings 方法中，我们随机共享子向量，而不是单个元素。Slim Embeddings 与 LightRNN 的不同之处在于它能够将压缩比控制到任意值，而
    LightRNN 只能以词汇大小的平方根或立方根的速率进行压缩，这在实际应用中可能过于严苛。在字符感知语言模型中，如果我们将子向量 ID（虚拟字符）的序列视为每个单词的表示，则单词嵌入可以视为连接的
    unigram 字符特征向量。这种方法的缺点是模型更复杂，为了加速推理，需要预先计算单词嵌入，因此在推理过程中不能保持紧凑形式。Slim Embeddings
    模型要简单得多，更容易调整。在推理过程中，它使用的空间要少得多，甚至可以降低推理的复杂性。
- en: 5.3\. Parameter Sharing in Transformers
  id: totrans-562
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. Transformers 中的参数共享
- en: 'A standard Transformer does not share parameters across layers and also has
    a fixed number of encoder layers. ALBERT (Lan et al., [2019](#bib.bib61)) incorporates
    two parameter reduction techniques:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 Transformer 不在层间共享参数，并且有固定数量的编码器层。ALBERT（Lan 等人，[2019](#bib.bib61)）结合了两种参数减少技术：
- en: •
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Factorized embedding parameterization. That is, it decomposes large vocabulary
    embedding matrix into two small matrices. Thus, it reduces the embedding parameters
    from $O(V\times H)$ to $O(V\times E+E\times H)$ where $H>>E$.
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因子化嵌入参数化。即，将大型词汇嵌入矩阵分解为两个小矩阵。这样，它将嵌入参数从 $O(V\times H)$ 减少到 $O(V\times E+E\times
    H)$，其中 $H>>E$。
- en: •
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-layer parameter sharing: There are multiple ways to share parameters,
    e.g., only sharing feed-forward network (FFN) parameters across layers, or only
    sharing attention parameters. The default decision for ALBERT is to share all
    parameters across layers.'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨层参数共享：有多种共享参数的方式，例如，仅在层间共享前馈网络（FFN）参数，或仅共享注意力参数。ALBERT 的默认决策是跨层共享所有参数。
- en: An ALBERT configuration similar to BERT-large has 18x fewer parameters and can
    be trained about 1.7x faster. Dehghani et al. (Dehghani et al., [2018](#bib.bib22))
    propose Universal Transformers where the number of encoder layers are not pre-decided,
    and all the encoder layers share the parameters. Certain symbols (e.g. some words
    or phonemes) are usually more ambiguous than others. It is therefore reasonable
    to allocate more processing resources to these more ambiguous symbols. Thus, ambiguous
    symbols undergo more self-attention transformations compared to non-ambiguous
    ones. Thus, they provide a dynamic per-position halting mechanism for dynamically
    modulating the number of computational steps needed to process each input symbol
    (called the “ponder time”) before the representation is passed on as input to
    the decoder. The idea of sharing weights across layers in Transformers has also
    been explored in (Bai et al., [2019](#bib.bib5)).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于BERT-large的ALBERT配置具有18倍更少的参数，并且训练速度约为1.7倍更快。Dehghani等人（Dehghani et al.,
    [2018](#bib.bib22)）提出了Universal Transformers，其中编码器层的数量不是预先决定的，所有编码器层共享参数。某些符号（例如一些词或音素）通常比其他符号更模糊。因此，合理地将更多的处理资源分配给这些更模糊的符号。因此，模糊符号经历了更多的自注意力变换，与非模糊符号相比。因此，它们为动态调节处理每个输入符号所需计算步骤的数量（称为“思考时间”）提供了一种动态每位置停顿机制，然后再将表示作为输入传递给解码器。在Transformers中跨层共享权重的想法也在
    (Bai et al., [2019](#bib.bib5)) 中进行了探索。
- en: 5.4\. Summary
  id: totrans-569
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 总结
- en: '| Task | Dataset | Method | Base Model | Metric | Size (Comp; Orig) | Eval.
    (Comp; Orig) |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 方法 | 基础模型 | 指标 | 大小 (压缩; 原始) | 评估 (压缩; 原始) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Language modeling | 1B Word Benchmark | Char-CNN (input embeddings) (Jozefowicz
    et al., [2016](#bib.bib52)) | 2-layer word-BiLSTM | Perplexity (L) | 1.04B; 1.8B
    | 30.0; 30.6 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B词汇基准 | Char-CNN (输入嵌入) (Jozefowicz et al., [2016](#bib.bib52)) |
    2层word-BiLSTM | 困惑度 (L) | 1.04B; 1.8B | 30.0; 30.6 |'
- en: '| Language modeling | 1B Word Benchmark | Char-CNN (in/output embeddings) (Jozefowicz
    et al., [2016](#bib.bib52)) | 2-layer word-BiLSTM | Perplexity (L) | 0.39B; 1.8B
    | 35.8; 30.6 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B词汇基准 | Char-CNN (输入/输出嵌入) (Jozefowicz et al., [2016](#bib.bib52))
    | 2层word-BiLSTM | 困惑度 (L) | 0.39B; 1.8B | 35.8; 30.6 |'
- en: '| Language modeling | 1B Word Benchmark | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 41M; 1.6G | 66; 85 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B词汇基准 | LightRNN (Li et al., [2016a](#bib.bib68)) | word-LSTM | 困惑度
    (L) | 41M; 1.6G | 66; 85 |'
- en: '| Language modeling | 1B Word Benchmark | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | 2-layer word-BiLSTM | Perplexity (L) | 0.25B; 1.8B | 38.3; 30.6 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B词汇基准 | Slim Embeddings (Li et al., [2018](#bib.bib69)) | 2层word-BiLSTM
    | 困惑度 (L) | 0.25B; 1.8B | 38.3; 30.6 |'
- en: '| Language modeling | ACLW-Czech | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 64M; 83M | 578; 701 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-捷克语 | CNN+Highway网络 (Kim et al., [2016](#bib.bib56)) | word-LSTM
    | 困惑度 (L) | 64M; 83M | 578; 701 |'
- en: '| Language modeling | ACLW-Czech | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 18M; 83M | 558; 701 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-捷克语 | LightRNN (Li et al., [2016a](#bib.bib68)) | word-LSTM |
    困惑度 (L) | 18M; 83M | 558; 701 |'
- en: '| Language modeling | ACLW-Czech | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 17M; 83M | 528; 701 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-捷克语 | Slim Embeddings (Li et al., [2018](#bib.bib69)) | word-LSTM
    | 困惑度 (L) | 17M; 83M | 528; 701 |'
- en: '| Language modeling | ACLW-English | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 20M; 25M | 216; 236 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-英语 | CNN+Highway网络 (Kim et al., [2016](#bib.bib56)) | word-LSTM
    | 困惑度 (L) | 20M; 25M | 216; 236 |'
- en: '| Language modeling | ACLW-English | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 17M; 25M | 191; 236 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-英语 | LightRNN (Li et al., [2016a](#bib.bib68)) | word-LSTM |
    困惑度 (L) | 17M; 25M | 191; 236 |'
- en: '| Language modeling | ACLW-English | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 7M; 25M | 187; 236 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-英语 | Slim Embeddings (Li et al., [2018](#bib.bib69)) | word-LSTM
    | 困惑度 (L) | 7M; 25M | 187; 236 |'
- en: '| Language modeling | ACLW-French | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 44M; 56M | 190; 202 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-法语 | CNN+Highway网络 (Kim et al., [2016](#bib.bib56)) | word-LSTM
    | 困惑度 (L) | 44M; 56M | 190; 202 |'
- en: '| Language modeling | ACLW-French | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 17M; 56M | 176; 202 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-法语 | LightRNN (Li et al., [2016a](#bib.bib68)) | word-LSTM |
    困惑度 (L) | 17M; 56M | 176; 202 |'
- en: '| Language modeling | ACLW-French | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 12M; 56M | 162; 202 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-法语 | Slim Embeddings (Li et al., [2018](#bib.bib69)) | word-LSTM
    | 困惑度 (L) | 12M; 56M | 162; 202 |'
- en: '| Language modeling | ACLW-German | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 104M; 137M | 305; 347 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-德语 | CNN+高速网络 （Kim 等，[2016](#bib.bib56)） | 词-LSTM | 困惑度 (L) |
    104M; 137M | 305; 347 |'
- en: '| Language modeling | ACLW-German | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 18M; 137M | 281; 347 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-德语 | LightRNN （Li 等，[2016a](#bib.bib68)） | 词-LSTM | 困惑度 (L) |
    18M; 137M | 281; 347 |'
- en: '| Language modeling | ACLW-German | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 17M; 137M | 261; 347 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-德语 | 精简嵌入 （Li 等，[2018](#bib.bib69)） | 词-LSTM | 困惑度 (L) | 17M;
    137M | 261; 347 |'
- en: '| Language modeling | ACLW-Russian | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 152M; 200M | 313; 353 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-俄语 | CNN+高速网络 （Kim 等，[2016](#bib.bib56)） | 词-LSTM | 困惑度 (L) |
    152M; 200M | 313; 353 |'
- en: '| Language modeling | ACLW-Russian | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 19M; 200M | 288; 353 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-俄语 | LightRNN （Li 等，[2016a](#bib.bib68)） | 词-LSTM | 困惑度 (L) |
    19M; 200M | 288; 353 |'
- en: '| Language modeling | ACLW-Russian | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 19M; 200M | 274; 353 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-俄语 | 精简嵌入 （Li 等，[2018](#bib.bib69)） | 词-LSTM | 困惑度 (L) | 19M;
    200M | 274; 353 |'
- en: '| Language modeling | ACLW-Spanish | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 48M; 61M | 169; 186 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-西班牙语 | CNN+高速网络 （Kim 等，[2016](#bib.bib56)） | 词-LSTM | 困惑度 (L)
    | 48M; 61M | 169; 186 |'
- en: '| Language modeling | ACLW-Spanish | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 18M; 61M | 157; 186 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-西班牙语 | LightRNN （Li 等，[2016a](#bib.bib68)） | 词-LSTM | 困惑度 (L)
    | 18M; 61M | 157; 186 |'
- en: '| Language modeling | ACLW-Spanish | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 8M; 61M | 149; 186 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | ACLW-西班牙语 | 精简嵌入 （Li 等，[2018](#bib.bib69)） | 词-LSTM | 困惑度 (L) | 8M;
    61M | 149; 186 |'
- en: '| Language modeling | PTB | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 5M; 20M | 92.3; 85.4 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | CNN+高速网络 （Kim 等，[2016](#bib.bib56)） | 词-LSTM | 困惑度 (L) | 5M;
    20M | 92.3; 85.4 |'
- en: '| Language modeling | Wikipedia articles (ca) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 182K; 4.3M | 34.92; 35.34 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikipedia 文章（加） | C2W （Ling 等，[2015](#bib.bib72)） | 词嵌入 | 困惑度 (L)
    | 182K; 4.3M | 34.92; 35.34 |'
- en: '| Language modeling | Wikipedia articles (de) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 183K; 6.3M | 41.94; 43.02 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikipedia 文章（德） | C2W （Ling 等，[2015](#bib.bib72)） | 词嵌入 | 困惑度 (L)
    | 183K; 6.3M | 41.94; 43.02 |'
- en: '| Language modeling | Wikipedia articles (en) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 180K; 4.3M | 57.39; 59.38 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikipedia 文章（英语） | C2W （Ling 等，[2015](#bib.bib72)） | 词嵌入 | 困惑度 (L)
    | 180K; 4.3M | 57.39; 59.38 |'
- en: '| Language modeling | Wikipedia articles (pt) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 178K; 4.2M | 40.92; 46.17 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikipedia 文章（葡萄牙） | C2W （Ling 等，[2015](#bib.bib72)） | 词嵌入 | 困惑度 (L)
    | 178K; 4.2M | 40.92; 46.17 |'
- en: '| Language modeling | Wikipedia articles (tr) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 174K; 5.7M | 32.88; 44.01 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Wikipedia 文章（土耳其） | C2W （Ling 等，[2015](#bib.bib72)） | 词嵌入 | 困惑度 (L)
    | 174K; 5.7M | 32.88; 44.01 |'
- en: '| Machine reading comp. | ReAding Comprehension from Examinations | ALBERT (Lan
    et al., [2019](#bib.bib61)) | BERT-B | Acc (H) | 18M; 108M | 68.5; 68.2 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读理解 | 考试中的阅读理解 | ALBERT （Lan 等，[2019](#bib.bib61)） | BERT-B | 准确率 (H)
    | 18M; 108M | 68.5; 68.2 |'
- en: '| NLI | MNLI | Quaternion Attention (Tay et al., [2019](#bib.bib117)) | 2-layer
    att-GloVe | Acc (H) | 200K; 700K | 72.3; 73.6 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | MNLI | 四元数注意力 （Tay 等，[2019](#bib.bib117)） | 2层注意力-GloVe | 准确率 (H)
    | 200K; 700K | 72.3; 73.6 |'
- en: '| NLI | SciTail | Quaternion Attention (Tay et al., [2019](#bib.bib117)) |
    2-layer att-GloVe | Acc (H) | 200K; 700K | 79.6; 79.0 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | SciTail | 四元数注意力 （Tay 等，[2019](#bib.bib117)） | 2层注意力-GloVe | 准确率
    (H) | 200K; 700K | 79.6; 79.0 |'
- en: '| NLI | SNLI | Quaternion Attention (Tay et al., [2019](#bib.bib117)) | 2-layer
    att-GloVe | Acc (H) | 200K; 700K | 85.4; 86.2 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | SNLI | 四元数注意力 （Tay 等，[2019](#bib.bib117)） | 2层注意力-GloVe | 准确率 (H)
    | 200K; 700K | 85.4; 86.2 |'
- en: '| NMT (en$\rightarrow$et) | IWSLT15 | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | Transformer | BLEU (H) | 11M; 44M | 13.1; 14.1 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$et) | IWSLT15 | 四元数变换器 （Tay 等，[2019](#bib.bib117)）
    | 变换器 | BLEU (H) | 11M; 44M | 13.1; 14.1 |'
- en: '| NMT (en$\rightarrow$ro) | IWSLT15 | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | Transformer | BLEU (H) | 11M; 44M | 18.5; 22.8 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$ro) | IWSLT15 | 四元数变换器 （Tay 等，[2019](#bib.bib117)）
    | 变换器 | BLEU (H) | 11M; 44M | 18.5; 22.8 |'
- en: '| NMT (en$\rightarrow$vi) | IWSLT15 | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | Transformer | BLEU (H) | 11M; 44M | 28.0; 28.4 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$vi) | IWSLT15 | 四元数变换器 (Tay 等, [2019](#bib.bib117)) |
    Transformer | BLEU (H) | 11M; 44M | 28.0; 28.4 |'
- en: '| POS Tagging | PTB (ca) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 98.92; 98.09 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | PTB (ca) | C2W (Ling 等, [2015](#bib.bib72)) | word-BiLSTM | 准确率 (H)
    | 150K; 2M | 98.92; 98.09 |'
- en: '| POS Tagging | PTB (de) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 98.08; 97.51 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | PTB (de) | C2W (Ling 等, [2015](#bib.bib72)) | word-BiLSTM | 准确率 (H)
    | 150K; 2M | 98.08; 97.51 |'
- en: '| POS Tagging | PTB (en) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 97.36; 96.97 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | PTB (en) | C2W (Ling 等, [2015](#bib.bib72)) | word-BiLSTM | 准确率 (H)
    | 150K; 2M | 97.36; 96.97 |'
- en: '| POS Tagging | PTB (pt) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 97.47; 95.67 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | PTB (pt) | C2W (Ling 等, [2015](#bib.bib72)) | word-BiLSTM | 准确率 (H)
    | 150K; 2M | 97.47; 95.67 |'
- en: '| POS Tagging | PTB (tr) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 91.59; 83.43 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | PTB (tr) | C2W (Ling 等, [2015](#bib.bib72)) | word-BiLSTM | 准确率 (H)
    | 150K; 2M | 91.59; 83.43 |'
- en: '| Question answering | BABI | Universal Transformer (Dehghani et al., [2018](#bib.bib22))
    | Transformer | Avg error (L) | 7.3M; 44M | 0.21; 15.2 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | BABI | 通用变换器 (Dehghani 等, [2018](#bib.bib22)) | Transformer | 平均误差
    (L) | 7.3M; 44M | 0.21; 15.2 |'
- en: '| Question answering | WikiQA | Quaternion Attention (Tay et al., [2019](#bib.bib117))
    | 2-layer att-GloVe | MAP (H) | 200K; 700K | 66.2; 67.2 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | WikiQA | 四元数注意力 (Tay 等, [2019](#bib.bib117)) | 2层 att-GloVe | MAP
    (H) | 200K; 700K | 66.2; 67.2 |'
- en: '| Sentiment analysis | IMDB | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | 2-layer Transformer | Acc (H) | 100K; 400K | 83.9; 82.6 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | 四元数变换器 (Tay 等, [2019](#bib.bib117)) | 2层 Transformer | 准确率
    (H) | 100K; 400K | 83.9; 82.6 |'
- en: '| Sentiment analysis | SST | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | 2-layer Transformer | Acc (H) | 100K; 400K | 80.5; 78.9 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST | 四元数变换器 (Tay 等, [2019](#bib.bib117)) | 2层 Transformer | 准确率 (H)
    | 100K; 400K | 80.5; 78.9 |'
- en: '| Speech recognition | 2000 hour En. Speech | Toeplitz-like (Lu et al., [2016](#bib.bib77))
    | 3-layer RNN | WER (L) | 790K; 1.85M | 48.4; 43.5 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2000小时英语语音 | Toeplitz-like (Lu 等, [2016](#bib.bib77)) | 3层 RNN | WER
    (L) | 790K; 1.85M | 48.4; 43.5 |'
- en: '| Speech recognition | 2000 hour En. Speech | Toeplitz-like (Lu et al., [2016](#bib.bib77))
    | 5-layer LSTM | WER (L) | 2.00M; 9.12M | 33.5; 33.1 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2000小时英语语音 | Toeplitz-like (Lu 等, [2016](#bib.bib77)) | 5层 LSTM |
    WER (L) | 2.00M; 9.12M | 33.5; 33.1 |'
- en: '| Subject verb agreement | SVA dataset | Quaternion Transformer (Tay et al.,
    [2019](#bib.bib117)) | 2-layer Transformer | Acc (H) | 100K; 400K | 94.7; 94.8
    |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| 主谓一致 | SVA 数据集 | 四元数变换器 (Tay 等, [2019](#bib.bib117)) | 2层 Transformer | 准确率
    (H) | 100K; 400K | 94.7; 94.8 |'
- en: '| Subject verb agreement | SVA dataset | Universal Transformer (Dehghani et al.,
    [2018](#bib.bib22)) | Transformer | Acc (H) | 7.3M; 44M | 0.992; 0.962 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| 主谓一致 | SVA 数据集 | 通用变换器 (Dehghani 等, [2018](#bib.bib22)) | Transformer | 准确率
    (H) | 7.3M; 44M | 0.992; 0.962 |'
- en: '| Word analogy | GSEm, GSYN, MSYN | Shared ref. vec. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 64.6/64.5;
    64.7 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 单词类比 | GSEm, GSYN, MSYN | 共享参考向量 ($K$=16/256, $B$=256) (Suzuki 和 Nagata,
    [2016](#bib.bib113)) | SGNS | 准确率 (H) | 98/196MB; 1565MB | 64.6/64.5; 64.7 |'
- en: '| Word analogy | GSEm, GSYN, MSYN | k-means quant. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 64.0/64.5;
    64.7 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| 单词类比 | GSEm, GSYN, MSYN | k-means 量化 ($K$=16/256, $B$=256) (Suzuki 和 Nagata,
    [2016](#bib.bib113)) | SGNS | 准确率 (H) | 98/196MB; 1565MB | 64.0/64.5; 64.7 |'
- en: '| Word similarity | 7 datasets | Shared ref. vec. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 65.5/67.1;
    67.2 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| 单词相似度 | 7个数据集 | 共享参考向量 ($K$=16/256, $B$=256) (Suzuki 和 Nagata, [2016](#bib.bib113))
    | SGNS | 准确率 (H) | 98/196MB; 1565MB | 65.5/67.1; 67.2 |'
- en: '| Word similarity | 7 datasets | k-means quant. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 64.4/67.0;
    67.2 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| 单词相似度 | 7个数据集 | k-means 量化 ($K$=16/256, $B$=256) (Suzuki 和 Nagata, [2016](#bib.bib113))
    | SGNS | 准确率 (H) | 98/196MB; 1565MB | 64.4/67.0; 67.2 |'
- en: Table 6\. Comparison of various parameter sharing methods (sorted by Task and
    then Dataset). 7 datasets for word similarity are MEN, MTurk, RARE, SLex, SCWS,
    WSR, WSS. SGNS=SkipGram with negative sampling. In the metric column, H means
    high is better while L means low is better.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 各种参数共享方法的比较（按任务和数据集排序）。7个单词相似度数据集为 MEN, MTurk, RARE, SLex, SCWS, WSR,
    WSS。SGNS=带有负采样的 SkipGram。在度量列中，H 表示越高越好，而 L 表示越低越好。
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.4\. Summary ‣ 5\. Parameter sharing ‣ Compression
    of Deep Learning Models for Text: A Survey") compares various parameter sharing
    methods across different tasks and datasets. Accuracy of both the original and
    the compressed (comp.) model are shown. Also, we report model size (in terms of
    number of parameters or memory size) for both the original as well as the compressed
    models. For the same task, dataset and model combination, different papers report
    different accuracy of the original model because of slight changes in training
    hyper-parameters; hence we report accuracy of the original model for each row.
    Since many parameter sharing methods have been used to compress word embeddings,
    the most common application is language modeling.'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S5.T6 "Table 6 ‣ 5.4\. Summary ‣ 5\. Parameter sharing ‣ Compression
    of Deep Learning Models for Text: A Survey") 比较了不同任务和数据集上的各种参数共享方法。展示了原始模型和压缩模型（comp.）的准确度。此外，我们报告了原始模型和压缩模型的模型大小（以参数数量或内存大小为单位）。对于相同的任务、数据集和模型组合，不同的论文报告了原始模型的不同准确度，因为训练超参数的细微变化；因此我们报告了每一行的原始模型准确度。由于许多参数共享方法已被用于压缩词汇嵌入，最常见的应用是语言建模。'
- en: For language modeling, experiments have been done on the One Billion Word Benchmark,
    ACLW, PTB and Wikipedia articles datasets across multiple languages using parameter
    sharing methods like Char-CNN (Jozefowicz et al., [2016](#bib.bib52)), LightRNN (Li
    et al., [2016a](#bib.bib68)), Slim embeddings (Li et al., [2018](#bib.bib69)),
    CNN+Highway networks (Kim et al., [2016](#bib.bib56)) and C2W (Ling et al., [2015](#bib.bib72)).
    C2W (Ling et al., [2015](#bib.bib72)) always outperforms word lookup tables and
    improvements are especially pronounced in Turkish, which is a highly morphological
    language, where word meanings differ radically depending on the suffixes used.
    Jozefowicz et al. (Jozefowicz et al., [2016](#bib.bib52)) observe that Char-CNNs
    especially with character embeddings being used both at input as well as output
    can lead to 4.6x model size reduction with a slight increase in perplexity. On
    the One-Billion-Word benchmark, LightRNN (Li et al., [2016a](#bib.bib68)) achieves
    much lower perplexity compared to word-LSTMs, whilst reducing the model size by
    a factor of 40-100, and speeding up the training process by a factor of 2\. LightRNN (Li
    et al., [2016a](#bib.bib68)) significantly reduces the model size, while at the
    same time outperforms CNN+Highway network (Kim et al., [2016](#bib.bib56)) method.
    While the model sizes of the CNN+Highway network method increase linearly with
    respect to the vocabulary size, the model size of LightRNN almost keeps constant
    on the ACLW datasets. Slim embeddings (Li et al., [2018](#bib.bib69)) is way better
    than LightRNN (low perplexity with smaller models). On PTB and 44M giga word corpus
    datasets, Slim embeddings applied at input layer can maintain same perplexity
    for a word-LSTM using just 1% (and 0.2%) of trainable parameters respectively.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言建模中，已经在多个语言的 One Billion Word Benchmark、ACLW、PTB 和 Wikipedia 文章数据集上进行了实验，使用了如
    Char-CNN （Jozefowicz 等，[2016](#bib.bib52)）、LightRNN （Li 等，[2016a](#bib.bib68)）、Slim
    embeddings （Li 等，[2018](#bib.bib69)）、CNN+Highway 网络 （Kim 等，[2016](#bib.bib56)）和
    C2W （Ling 等，[2015](#bib.bib72)）等参数共享方法。C2W （Ling 等，[2015](#bib.bib72)）总是优于词汇查找表，并且在土耳其语中效果尤为明显，土耳其语是一种高度形态变化的语言，词义会因所用的后缀不同而有很大变化。Jozefowicz
    等（Jozefowicz 等，[2016](#bib.bib52)）观察到，Char-CNN 尤其是在输入和输出都使用字符嵌入时，可以使模型大小减少 4.6
    倍，同时困惑度略有增加。在 One-Billion-Word 基准测试中，LightRNN （Li 等，[2016a](#bib.bib68)）相比于 word-LSTM
    实现了更低的困惑度，同时将模型大小减少了 40-100 倍，并将训练过程加速了 2 倍。LightRNN （Li 等，[2016a](#bib.bib68)）显著减小了模型大小，同时优于
    CNN+Highway 网络 （Kim 等，[2016](#bib.bib56)）方法。虽然 CNN+Highway 网络方法的模型大小随着词汇量的增加而线性增长，但
    LightRNN 的模型大小在 ACLW 数据集上几乎保持不变。Slim embeddings （Li 等，[2018](#bib.bib69)）比 LightRNN
    要好得多（低困惑度且模型更小）。在 PTB 和 44M giga word 语料库数据集上，应用于输入层的 Slim embeddings 可以使用仅 1%（和
    0.2%）的可训练参数来保持与 word-LSTM 相同的困惑度。
- en: k-means quantization and shared reference vectors are also methods for compression
    of word embeddings using parameter sharing. Suzuki et al. (Suzuki and Nagata,
    [2016](#bib.bib113)) showed significant gains over typical skipgram (SGNS) embeddings
    in terms of model size reduction while retaining similar accuracies for word analogy
    and similarity tasks. The model size of their method with shared reference vectors
    with ‘K = 16, B = 64’ was just 24MB, approximately 65 times smaller than that
    of original SGNS method. They also showed that SGNS with shared reference vectors
    was better than SGNS with block-wise k-means post-processing method. Unfortunately,
    there exists no good comparison between the slim embeddings and the shared reference
    vectors methods.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 量化和共享参考向量也是使用参数共享来压缩词嵌入的方法。Suzuki et al. (Suzuki and Nagata, [2016](#bib.bib113))
    显示，相较于典型的 skipgram (SGNS) 嵌入，他们的方法在模型大小缩减方面有显著提升，同时保持了相似的词类比和相似度任务准确性。他们的方法中，带有
    'K = 16, B = 64' 的共享参考向量的模型大小仅为 24MB，约为原 SGNS 方法的 65 分之一。他们还显示，使用共享参考向量的 SGNS
    比使用块状 k-means 后处理方法的 SGNS 更优。遗憾的是，目前还没有关于精简嵌入与共享参考向量方法的良好比较。
- en: For the MRC task, ALBERT (Lan et al., [2019](#bib.bib61)) pushed the accuracy
    from 68.5 to 68.2 using 6x fewer parameters compared to BERT-base. On the NLI
    task, a tiny Quaternion-Att model (50 dimensions) achieves comparable (or occasionally
    marginally better or worse) performance compared to typical attention over GloVe
    (200 dimensions), gaining a 68% parameter savings across three datasets. For sentiment
    analysis, Quaternion Transformers (Tay et al., [2019](#bib.bib117)) leads by +1.3%/1.6%
    gains on IMDb and SST datasets respectively while maintaining a 75% reduction
    in parameter cost. Quaternion Transformers (Tay et al., [2019](#bib.bib117)) have
    been shown to outperform the vanilla Transformer for the mathematical language
    understanding task as well, with 25% parameters. At the same compression rate,
    Quaternion Transformers lose only very minor BLEU on IWSLT 2015 NMT datasets.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MRC 任务，ALBERT (Lan et al., [2019](#bib.bib61)) 将准确率从 68.5 提升到 68.2，使用的参数比
    BERT-base 少 6 倍。在 NLI 任务上，一个微型 Quaternion-Att 模型（50 维）与典型的 GloVe（200 维）注意力模型相比，性能相当（或偶尔略好或略差），在三个数据集上节省了
    68% 的参数。对于情感分析，Quaternion Transformers (Tay et al., [2019](#bib.bib117)) 在 IMDb
    和 SST 数据集上分别领先 +1.3%/1.6%，同时保持了 75% 的参数成本减少。Quaternion Transformers (Tay et al.,
    [2019](#bib.bib117)) 已被证明在数学语言理解任务中也优于普通 Transformer，参数减少了 25%。在相同的压缩率下，Quaternion
    Transformers 在 IWSLT 2015 NMT 数据集上仅丧失了非常少量的 BLEU 分数。
- en: For POS tagging, we can observe that the model using word lookup tables performs
    consistently worse than the C2W model (Ling et al., [2015](#bib.bib72)). Universal
    Transformers (Dehghani et al., [2018](#bib.bib22)) (while being 1/6th the size)
    outperform standard Transformers on a wide range of algorithmic and language understanding
    tasks, including the challenging LAMBADA language modeling task. On speech recognition,
    Lu et al. (Lu et al., [2016](#bib.bib77)) study mechanisms for learning compact
    RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. A hybrid
    strategy of using structured matrices in the bottom layers and shared low-rank
    factors on the top layers is found to be particularly effective, reducing the
    parameters of a standard LSTM by 75%, at a small cost of 0.3% increase in WER,
    on a 2000-hr English Voice Search task. For LSTM parameter reduction, architecting
    upper layers with projection nodes to moderate rank, and bottom layers with Toeplitz-like
    transforms was found to be a particularly effective strategy.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 POS 标注，我们可以观察到，使用词汇查找表的模型表现始终不如 C2W 模型 (Ling et al., [2015](#bib.bib72))。Universal
    Transformers (Dehghani et al., [2018](#bib.bib22))（虽然体积仅为标准模型的 1/6）在广泛的算法和语言理解任务中表现优于标准
    Transformers，包括具有挑战性的 LAMBADA 语言建模任务。在语音识别方面，Lu et al. (Lu et al., [2016](#bib.bib77))
    研究了通过低秩分解和参数共享机制学习紧凑 RNN 和 LSTM 的方法。使用结构化矩阵在底层以及在顶层共享低秩因子的混合策略被发现特别有效，将标准 LSTM
    的参数减少了 75%，在 2000 小时的英语语音搜索任务中，仅以 0.3% 的 WER 增加为代价。对于 LSTM 参数减少，将上层设计为投影节点以调节秩，并将底层设计为类似
    Toeplitz 的变换被发现是一个特别有效的策略。
- en: Overall, besides model compression, parameter sharing methods also act as a
    good regularizer. Parameter sharing in Transformers has been very successful.
    ALBERT was at the top of the GLUE leaderboard when it was proposed. Parameter
    sharing methods have also been widely used for compressing embedding matrix. Slim
    embeddings has the best method for compressing embedding matrices.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，除了模型压缩，参数共享方法还作为一种良好的正则化器。Transformers 中的参数共享取得了很大成功。ALBERT 在提出时位居 GLUE
    排行榜首。参数共享方法也被广泛用于压缩嵌入矩阵。Slim embeddings 是压缩嵌入矩阵的最佳方法。
- en: 6\. Tensor decomposition
  id: totrans-631
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 张量分解
- en: Sparse Matrix decomposition has been traditionally used for applications like
    feature selection, collaborative filtering, topic mining from text, etc. In this
    section, we discuss how various popular tensor decomposition methods like Singular
    Value Decomposition (SVD), Tensor-Train (Oseledets, [2011](#bib.bib89)), CP (CANDECOMP/PARAFAC) (Carroll
    and Chang, [1970](#bib.bib11)) and Tucker (Tucker, [1966](#bib.bib119)) can be
    used for model compression.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵分解传统上用于特征选择、协同过滤、文本主题挖掘等应用。在这一部分，我们讨论了如何使用各种流行的张量分解方法，如奇异值分解（SVD）、张量列车（Oseledets，[2011](#bib.bib89)）、CP（CANDECOMP/PARAFAC）（Carroll
    和 Chang，[1970](#bib.bib11)）和 Tucker（Tucker，[1966](#bib.bib119)），进行模型压缩。
- en: 6.1\. Two Low-Rank Factors
  id: totrans-633
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 两个低秩因子
- en: In this part, we will discuss methods where a matrix is factorized into two
    low-rank factors. Specifically, we replace a weight matrix $W$ with $W_{1}\times
    W_{2}$ such that the total number of parameters are significantly lesser.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论将矩阵分解为两个低秩因子的各种方法。具体来说，我们用 $W_{1}\times W_{2}$ 替换权重矩阵 $W$，从而显著减少总的参数数量。
- en: A multi-layer RNN can be represented as follows.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 多层 RNN 可以表示如下。
- en: '| (62) |  | $\displaystyle h_{t}^{l}=\sigma(W_{x}^{l-1}h_{t}^{l-1}+W_{h}^{l}h_{t-1}^{l}+b^{l})$
    |  |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| (62) |  | $\displaystyle h_{t}^{l}=\sigma(W_{x}^{l-1}h_{t}^{l-1}+W_{h}^{l}h_{t-1}^{l}+b^{l})$
    |  |'
- en: '| (63) |  | $\displaystyle h_{t}^{l+1}=\sigma(W_{x}^{l}h_{t}^{l}+W_{h}^{l+1}h_{t-1}^{l+1}+b^{l+1})$
    |  |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| (63) |  | $\displaystyle h_{t}^{l+1}=\sigma(W_{x}^{l}h_{t}^{l}+W_{h}^{l+1}h_{t-1}^{l+1}+b^{l+1})$
    |  |'
- en: 'Thus, there are two important weight matrices: the recurrent $W_{h}^{l}$ and
    inter-layer matrices $W_{l}^{x}$. Prabhavalkar et al. (Prabhavalkar et al., [2016](#bib.bib93))
    propose a method to jointly compress the recurrent and inter-layer matrices corresponding
    to a specific layer $l$ by determining a suitable recurrent projection matrix,
    denoted by $P^{l}\in R^{r_{l}\times N_{l}}$ of rank $r^{l}<N^{l}$ such that $W_{h}^{l}=Z_{h}^{l}P^{l}$
    and $W_{l}^{x}=Z_{x}^{l}P^{l}$. First, $P^{l}$ is determined by computing a truncated
    SVD of the recurrent weight matrix, which we then truncate, retaining only the
    top $r^{l}$ singular values. Thus, $W_{h}^{l}$ can be written as follows.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有两个重要的权重矩阵：递归的 $W_{h}^{l}$ 和层间矩阵 $W_{l}^{x}$。Prabhavalkar 等人（Prabhavalkar
    等人，[2016](#bib.bib93)）提出了一种通过确定合适的递归投影矩阵 $P^{l}\in R^{r_{l}\times N_{l}}$ 来联合压缩对应于特定层
    $l$ 的递归和层间矩阵的方法，其中 $r^{l}<N^{l}$，使得 $W_{h}^{l}=Z_{h}^{l}P^{l}$ 和 $W_{l}^{x}=Z_{x}^{l}P^{l}$。首先，通过计算递归权重矩阵的截断
    SVD 来确定 $P^{l}$，然后进行截断，仅保留前 $r^{l}$ 个奇异值。因此，$W_{h}^{l}$ 可以写作如下。
- en: '| (64) |  | $\displaystyle W_{h}^{l}=(U_{h}^{l}\Sigma_{h}^{l})(V_{h}^{l})^{T}=Z_{h}^{l}P^{l}$
    |  |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| (64) |  | $\displaystyle W_{h}^{l}=(U_{h}^{l}\Sigma_{h}^{l})(V_{h}^{l})^{T}=Z_{h}^{l}P^{l}$
    |  |'
- en: Thus, $P^{l}$ is set to $(V_{h}^{l})^{T}$. Further, we determine $Z_{x}^{l}$
    as the solution to the following least-squares problem.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$P^{l}$ 被设定为 $(V_{h}^{l})^{T}$。进一步地，我们将 $Z_{x}^{l}$ 确定为以下最小二乘问题的解。
- en: '| (65) |  | $\displaystyle Z_{x}^{l}=\operatorname*{argmin}_{Y}&#124;&#124;YP^{l}-W_{x}^{l}&#124;&#124;_{2}^{2}$
    |  |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| (65) |  | $\displaystyle Z_{x}^{l}=\operatorname*{argmin}_{Y}&#124;&#124;YP^{l}-W_{x}^{l}&#124;&#124;_{2}^{2}$
    |  |'
- en: This solution can also be easily extended to LSTMs. Sak et al. (Sak et al.,
    [2014](#bib.bib101)) also proposed a similar solution based on a combination of
    parameter sharing and matrix decomposition but without SVD initialization. However,
    typically SVD initialization has been found to perform better.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案也可以很容易地扩展到 LSTM。Sak 等人（Sak 等人，[2014](#bib.bib101)）也提出了一种基于参数共享和矩阵分解的类似解决方案，但没有使用
    SVD 初始化。然而，通常发现 SVD 初始化的表现更好。
- en: Besides SVD, another way of matrix decomposition is sparse coding. Faruqui et
    al. (Faruqui et al., [2015](#bib.bib29)) propose using sparse coding to decompose
    word embedding matrices. Thus, given vocabulary of size $V$, word embedding matrix
    $X\in R^{L\times V}$, sparse coding aims at representing each input vector $x_{i}$
    as a sparse linear combination of basis vectors $a_{i}$ by solving the following
    problem.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 SVD，矩阵分解的另一种方法是稀疏编码。Faruqui 等人 (Faruqui et al., [2015](#bib.bib29)) 提出了使用稀疏编码来分解词嵌入矩阵。因此，给定大小为
    $V$ 的词汇表，词嵌入矩阵 $X\in R^{L\times V}$，稀疏编码的目标是通过解决以下问题，将每个输入向量 $x_{i}$ 表示为基向量 $a_{i}$
    的稀疏线性组合。
- en: '| (66) |  | $\displaystyle\operatorname*{argmin}_{D,A}\sum_{i=1}^{V}&#124;&#124;x_{i}-Da_{i}&#124;&#124;_{2}^{2}+\lambda&#124;&#124;a_{i}&#124;&#124;_{1}+\tau&#124;&#124;D&#124;&#124;_{2}^{2}$
    |  |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| (66) |  | $\displaystyle\operatorname*{argmin}_{D,A}\sum_{i=1}^{V}&#124;&#124;x_{i}-Da_{i}&#124;&#124;_{2}^{2}+\lambda&#124;&#124;a_{i}&#124;&#124;_{1}+\tau&#124;&#124;D&#124;&#124;_{2}^{2}$
    |  |'
- en: where $D\in R^{L\times K}$ and $A\in R^{K\times V}$. Further, for interpretability,
    one can enforce all elements of $A$ and $D$ to be non-negative. For further compression,
    one can also enforce $A$ to be binary or ensure that each column of $A$ is a $K$
    sized one hot vector (Shu and Nakayama, [2017](#bib.bib108)).
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D\in R^{L\times K}$ 和 $A\in R^{K\times V}$。此外，为了可解释性，可以强制 $A$ 和 $D$ 的所有元素均为非负。为了进一步压缩，还可以强制
    $A$ 为二值，或确保 $A$ 的每一列都是一个大小为 $K$ 的独热向量（Shu 和 Nakayama，[2017](#bib.bib108)）。
- en: 'Lastly, Wang et al. (Wang et al., [2019c](#bib.bib131)) combine pruning with
    matrix factorization for model compression and propose the FLOP (Factorized Low-rank
    Pruning) method. Let $W$ be a weight matrix. Structured pruning (removing a neuron,
    i.e., removing a column from weight matrix) can be achieved by replacing the computation
    $Wx$ by $WGx$ where diagonal sparsity-inducing matrix $G$ is learned using $L_{0}$
    regularization over $WG$ along with the supervised loss. This effectively removes
    a subset of columns of $W$ for column indices $k$ with $z_{k}=0$. One limitation
    is that this structured pruning method tends to produce lower performance than
    its unstructured counterpart. Hence, in the FL0P (Factorized L0 Pruning) model,
    we first factorize $W=PQ$. Let $r$ be #columns of $P$ (or equivalently #rows of
    $Q$), $p_{k}$ and $q_{k}$ be the $k$-th column of $P$ and $k$-th row of $Q$ respectively.
    We achieve structured pruning by introducing a pruning variable $z_{k}$ for each
    component. Thus, now, we can write $W$ as follows.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Wang 等人 (Wang et al., [2019c](#bib.bib131)) 将修剪与矩阵分解结合用于模型压缩，并提出了 FLOP（因式分解低秩修剪）方法。设
    $W$ 为权重矩阵。结构化修剪（删除一个神经元，即从权重矩阵中删除一列）可以通过将计算 $Wx$ 替换为 $WGx$ 来实现，其中对角稀疏诱导矩阵 $G$
    是通过 $L_{0}$ 正则化与监督损失一起学习的。这有效地去除了 $W$ 中列索引 $k$ 对应的子集，其中 $z_{k}=0$。一个限制是，这种结构化修剪方法往往会导致比其非结构化对手更低的性能。因此，在
    FL0P（因式分解 L0 修剪）模型中，我们首先对 $W$ 进行因式分解 $W=PQ$。设 $r$ 为 $P$ 的列数（或等效地，$Q$ 的行数），$p_{k}$
    和 $q_{k}$ 分别为 $P$ 的第 $k$ 列和 $Q$ 的第 $k$ 行。我们通过为每个组件引入修剪变量 $z_{k}$ 来实现结构化修剪。因此，现在我们可以将
    $W$ 写成如下形式。
- en: '| (67) |  | $\displaystyle W=PGQ=\sum_{k=1}^{r}z_{k}\times(p_{k}q_{k})$ |  |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| (67) |  | $\displaystyle W=PGQ=\sum_{k=1}^{r}z_{k}\times(p_{k}q_{k})$ |  |'
- en: where $G$ is again the diagonal matrix of pruning variables. After training,
    only columns and rows corresponding to non-zero diagonal values need to be stored,
    resulting in much smaller (but still dense) matrices $P$ and $Q$. The nonzero
    values of $G$ can be absorbed into either $P$ or $Q$. This structured pruning
    with factorization is much more effective compared to the vanilla structured pruning.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G$ 再次是修剪变量的对角矩阵。训练后，只需存储对应于非零对角值的列和行，从而得到更小（但仍然稠密）的矩阵 $P$ 和 $Q$。$G$ 的非零值可以被吸收到
    $P$ 或 $Q$ 中。这种与分解结合的结构化修剪比传统的结构化修剪更为有效。
- en: 6.2\. Factorizing into Block Diagonal Matrices
  id: totrans-649
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 分解为块对角矩阵
- en: The last layer of a language model is very large of the size $HV$ where $H$
    is the size of the hidden layer and $V$ is vocabulary size. Each word by an output
    embedding of the same size $H$. Chen et al. (Chen et al., [2015a](#bib.bib12))
    propose a differentiated softmax method which varies the dimension of the output
    embeddings across words depending on how much model capacity is deemed suitable
    for a given word. In particular, it is meaningful to assign more parameters to
    frequent words than to rare words. By definition, frequent words occur more often
    in the training data than rare words and therefore allow to fit more parameters.
    They define partitions of the output vocabulary based on word frequency and the
    words in each partition share the same embedding size. Partitioning results in
    a sparse final weight matrix which arranges the embeddings of the output words
    in blocks, each one corresponding to a separate partition. The size of the final
    hidden layer $H$ is the sum of the embedding sizes of the partitions. While this
    method does not involve creation of multiple factors, it factorizes the original
    matrix into multiple blocks while setting the remaining part of the matrix to
    0.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的最后一层非常大，大小为 $HV$，其中 $H$ 是隐藏层的大小，$V$ 是词汇表的大小。每个词由大小为 $H$ 的输出嵌入表示。Chen 等（Chen
    等，[2015a](#bib.bib12)）提出了一种差异化 softmax 方法，该方法根据给定词的模型容量适用程度，调整输出嵌入的维度。特别是，为频繁词分配更多参数比为稀有词分配的参数更多是有意义的。根据定义，频繁词在训练数据中出现的频率高于稀有词，因此允许拟合更多参数。他们根据词频定义了输出词汇表的分区，每个分区中的词共享相同的嵌入大小。分区会导致稀疏的最终权重矩阵，该矩阵将输出词的嵌入按块排列，每块对应一个单独的分区。最终隐藏层的大小
    $H$ 是分区嵌入大小的总和。虽然这种方法不涉及创建多个因子，但它将原始矩阵分解为多个块，同时将矩阵的其余部分设置为 0。
- en: Variani et al. (Variani et al., [2019](#bib.bib121)) propose a method called
    Word Encoded Sequence Transducers (WEST) which factorizes a matrix $E=C\times
    D$ where $D$ is constrained to be a block diagonal matrix. The block diagonal
    nature of the second factor leads to large compression rates.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: Variani 等（Variani 等，[2019](#bib.bib121)）提出了一种称为词编码序列变换器（WEST）的方法，该方法将矩阵 $E=C\times
    D$ 分解，其中 $D$ 被限制为块对角矩阵。第二个因子的块对角特性导致了较大的压缩率。
- en: 6.3\. Tensor Train and Block Term Decomposition
  id: totrans-652
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 张量列车与块项分解
- en: Tensor train decomposition (TTD) (Oseledets, [2011](#bib.bib89)) is a standard
    tensor decomposition technique which decomposes a high dimensional tensor into
    multiple 2D and 3D tensors which can be multiplied together to reconstruct the
    original tensor. These factors are called TT-cores and the other dimensions are
    referred to as TT-ranks. TTD can be leveraged to compress various weight matrices
    in RNNs and LSTMs (Tjandra et al., [2017](#bib.bib118); Khrulkov et al., [2019](#bib.bib55)).
    The first step is to represent a matrix as a multi-dimensional tensor by simple
    reshaping transformation and then use TTD on it. The values of TT–ranks directly
    define the compression ratio, so choosing them to be too small or too large will
    result into either significant performance drop or little reduction of the number
    of parameters. Typically TT-ranks around 16 for small matrices and 64-192 for
    larger matrices result in a good trade-off between compression ratio and the accuracy
    metric of interest. Also, when we use TTD for weight matrices, we also need change
    the inputs appropriately to be compatible in terms of dimensions.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 张量列车分解（TTD）（Oseledets，[2011](#bib.bib89)）是一种标准的张量分解技术，它将高维张量分解为多个二维和三维张量，这些张量可以相乘以重构原始张量。这些因素称为TT-核心，其他维度被称为TT-秩。TTD
    可以用来压缩 RNN 和 LSTM 中的各种权重矩阵（Tjandra 等，[2017](#bib.bib118)；Khrulkov 等，[2019](#bib.bib55)）。第一步是通过简单的重塑变换将矩阵表示为多维张量，然后在其上使用
    TTD。TT-秩的值直接定义了压缩比，因此选择它们过小或过大会导致性能显著下降或参数数量减少不多。通常，对于小矩阵，TT-秩在 16 左右，对于大矩阵则在
    64-192 之间，可以在压缩比和感兴趣的准确性指标之间取得良好的权衡。此外，当我们使用 TTD 处理权重矩阵时，还需要适当地更改输入，以便在维度上兼容。
- en: Compared with TT-RNN, Block-Term RNN (BTRNN) (Ye et al., [2018](#bib.bib137))
    is not only more concise (when using the same rank), but also able to attain a
    better approximation to the original RNNs with much fewer parameters. BTD decomposes
    a high order tensor into a sum of multiple Tucker decomposition models. The redundant
    dense connections between input and hidden state is first tensorized to a $d$-dimensional
    tensor and then decomposed using low-rank BTD into a sum of $N$ different Tucker
    decompositions where $N$ is the CP-rank. Each Tucker decomposition in turn consists
    of a core $d$-dimensional tensor and $d$ 3-dimensional factor tensors. While Ye
    et al. (Ye et al., [2018](#bib.bib137)) used BTD to compress RNNs, Ma et al. (Ma
    et al., [2019](#bib.bib78)) used BTD to compress the self-attention matrix in
    Transformers. They first build a single-block attention based on the Tucker decomposition
    where the query, key and value are mapped into three factor matrices and the core
    tensor is trainable and randomly initialized. It is then straightforward to represent
    the multi-head attention using BTD.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TT-RNN 相比，Block-Term RNN (BTRNN) (Ye et al., [2018](#bib.bib137)) 不仅更加简洁（在使用相同秩的情况下），而且能够用更少的参数达到对原始
    RNN 更好的近似。BTD 将高阶张量分解为多个 Tucker 分解模型的和。首先，将输入和隐藏状态之间的冗余密集连接张量化为一个 $d$-维张量，然后使用低秩
    BTD 将其分解为 $N$ 个不同的 Tucker 分解，其中 $N$ 是 CP-秩。每个 Tucker 分解又由一个核心 $d$-维张量和 $d$ 个 3-维因子张量组成。虽然
    Ye et al. (Ye et al., [2018](#bib.bib137)) 使用 BTD 压缩 RNN，Ma et al. (Ma et al.,
    [2019](#bib.bib78)) 使用 BTD 压缩 Transformers 中的自注意力矩阵。他们首先基于 Tucker 分解构建一个单块注意力，其中查询、键和值被映射到三个因子矩阵中，核心张量是可训练的并随机初始化。然后，使用
    BTD 表示多头注意力是很直接的。
- en: 6.4\. Summary
  id: totrans-655
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 摘要
- en: '| Task | Dataset | Method | Base Model | Metric | Size (Comp; Orig) | Eval.
    (Comp; Orig) |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 方法 | 基础模型 | 指标 | 大小 (压缩; 原始) | 评估 (压缩; 原始) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CTR prediction | Criteo CTR Challenge | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | MLP | LogLoss (L) | 4.7M; 41.2M | 0.4433; 0.4440 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| CTR 预测 | Criteo CTR Challenge | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | MLP | LogLoss (L) | 4.7M; 41.2M | 0.4433; 0.4440 |'
- en: '| Language modeling | 1B Word Benchmark | BTD (Ma et al., [2019](#bib.bib78))
    | Transformer-XL Large | Perplexity (L) | 0.16B; 0.8B | 19.5; 21.8 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B Word Benchmark | BTD (Ma et al., [2019](#bib.bib78)) | Transformer-XL
    Large | 困惑度 (L) | 0.16B; 0.8B | 19.5; 21.8 |'
- en: '| Language modeling | Enwiki-8 | FLOP (Wang et al., [2019c](#bib.bib131)) |
    Transformer | BPC (L) | 8M; 41M | 1.13; 1.08 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Enwiki-8 | FLOP (Wang et al., [2019c](#bib.bib131)) | Transformer
    | BPC (L) | 8M; 41M | 1.13; 1.08 |'
- en: '| Language modeling | PTB | WEST (Variani et al., [2019](#bib.bib121)) | LSTM
    | Perplexity (L) | 3.5M; 4.51M | 116.84; 115.91 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | WEST (Variani et al., [2019](#bib.bib121)) | LSTM | 困惑度 (L)
    | 3.5M; 4.51M | 116.84; 115.91 |'
- en: '| Language modeling | PTB | BTD (Ma et al., [2019](#bib.bib78)) | Transformer-XL-Base
    | Perplexity (L) | 12M; 24M | 49.8; 54.52 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | BTD (Ma et al., [2019](#bib.bib78)) | Transformer-XL-Base |
    困惑度 (L) | 12M; 24M | 49.8; 54.52 |'
- en: '| Language modeling | PTB | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL-Base | Perplexity (L) | 18M; 24M | 55.4; 54.52 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | PTB | TT-embedding (Khrulkov et al., [2019](#bib.bib55)) | Transformer-XL-Base
    | 困惑度 (L) | 18M; 24M | 55.4; 54.52 |'
- en: '| Language modeling | WikiText-103 | FLOP (Wang et al., [2019c](#bib.bib131))
    | Transformer | Perplexity (L) | 50M; 151M | 25.3; 24.1 |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | WikiText-103 | FLOP (Wang et al., [2019c](#bib.bib131)) | Transformer
    | 困惑度 (L) | 50M; 151M | 25.3; 24.1 |'
- en: '| Language modeling | WikiText-103 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL | Perplexity (L) | 91M; 192M | 25.67; 24.37 |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | WikiText-103 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL | 困惑度 (L) | 91M; 192M | 25.67; 24.37 |'
- en: '| Language modeling | WikiText-103 | BTD (Ma et al., [2019](#bib.bib78)) |
    Transformer-XL-Base | Perplexity (L) | 85.3M; 151M | 20.9; 24.0 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | WikiText-103 | BTD (Ma et al., [2019](#bib.bib78)) | Transformer-XL-Base
    | 困惑度 (L) | 85.3M; 151M | 20.9; 24.0 |'
- en: '| Language modeling | WikiText-103 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL-Base | Perplexity (L) | 130M; 151M | 25.7; 24.0 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | WikiText-103 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL-Base | 困惑度 (L) | 130M; 151M | 25.7; 24.0 |'
- en: '| NMT (en$\rightarrow$ja) | ASPEC | Compositional codes (Shu and Nakayama,
    [2017](#bib.bib108)) | LSTM | BLEU (H) | 2.97MB; 274MB | 38.89; 37.93 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| NMT (en$\rightarrow$ja) | ASPEC | 组合编码 (Shu and Nakayama, [2017](#bib.bib108))
    | LSTM | BLEU (H) | 2.97MB; 274MB | 38.89; 37.93 |'
- en: '| NMT (de$\rightarrow$en) | IWSLT14 | Compositional codes (Shu and Nakayama,
    [2017](#bib.bib108)) | LSTM | BLEU (H) | 2.11MB; 35MB | 29.56; 29.45 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| NMT (de$\rightarrow$en) | IWSLT14 | 组合编码 (Shu and Nakayama, [2017](#bib.bib108))
    | LSTM | BLEU (H) | 2.11MB; 35MB | 29.56; 29.45 |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-Big | BLEU (H) | 179M; 210M | 28.53; 28.84 |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$de) | WMT14 | TT-嵌入（Khrulkov 等，[2019](#bib.bib55)）
    | Transformer-Big | BLEU (H) | 179M; 210M | 28.53; 28.84 |'
- en: '| NMT (en$\rightarrow$de) | WMT16 | BTD (Ma et al., [2019](#bib.bib78)) | Transformer
    | BLEU (H) | 21.2M; 52M | 34.91; 34.5 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$de) | WMT16 | BTD（Ma 等，[2019](#bib.bib78)） | Transformer
    | BLEU (H) | 21.2M; 52M | 34.91; 34.5 |'
- en: '| NP bracketing | Subset of PTB | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    | Logistic regression | Acc (H) | 120M; 120M | 82.3; 77.9 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| NP 括号标注 | PTB 子集 | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 准确率 (H) |
    120M; 120M | 82.3; 77.9 |'
- en: '| Question classification | TREC Questions | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 81.5; 76.2
    |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| 问题分类 | TREC 问题 | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 准确率 (H) | 120M;
    120M | 81.5; 76.2 |'
- en: '| Sentiment analysis | IMDB | Compositional codes (Shu and Nakayama, [2017](#bib.bib108))
    | LSTM | Acc (H) | 1.23MB; 78MB | 87.37; 87.18 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | 组合编码（Shu 和 Nakayama，[2017](#bib.bib108)） | LSTM | 准确率 (H) |
    1.23MB; 78MB | 87.37; 87.18 |'
- en: '| Sentiment analysis | IMDB | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | LSTM | Acc (H) | 0.81M; 7.19M | 89.7; 88.6 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | TT-嵌入（Khrulkov 等，[2019](#bib.bib55)） | LSTM | 准确率 (H) | 0.81M;
    7.19M | 89.7; 88.6 |'
- en: '| Sentiment analysis | SST | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    | Logistic regression | Acc (H) | 120M; 120M | 81.4; 77.7 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 准确率 (H) | 120M;
    120M | 81.4; 77.7 |'
- en: '| Speech recognition | 3M Google voice utterances | Joint-SVD (Prabhavalkar
    et al., [2016](#bib.bib93)) | 5-layer RNN | WER (L) | 3.1M; 9.7M | 12.9; 12.4
    |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 3M Google 语音语料 | 联合 SVD（Prabhavalkar 等，[2016](#bib.bib93)） | 5层 RNN
    | WER (L) | 3.1M; 9.7M | 12.9; 12.4 |'
- en: '| Speech recognition | 3M Google voice utterances | Projections (Sak et al.,
    [2014](#bib.bib101)) | LSTM | WER (L) | 2M; 2.2M | 14.8; 17.5 |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 3M Google 语音语料 | 投影（Sak 等，[2014](#bib.bib101)） | LSTM | WER (L) |
    2M; 2.2M | 14.8; 17.5 |'
- en: '| Speech recognition | Live traffic utterances | WEST (Variani et al., [2019](#bib.bib121))
    | 3-layer LSTM | WER (L) | 4.75MB; 15MB | 13.6; 13.7 |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 实时交通语料 | WEST（Variani 等，[2019](#bib.bib121)） | 3层 LSTM | WER (L) |
    4.75MB; 15MB | 13.6; 13.7 |'
- en: '| Speech recognition | Live traffic utterances | WEST+Quantization (Variani
    et al., [2019](#bib.bib121)) | 3-layer LSTM | WER (L) | 1.35MB; 15MB | 13.7; 13.7
    |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 实时交通语料 | WEST+量化（Variani 等，[2019](#bib.bib121)） | 3层 LSTM | WER (L)
    | 1.35MB; 15MB | 13.7; 13.7 |'
- en: '| Text classification | 20 Newsgroup (Computers) | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 87.0; 79.7
    |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | 20 Newsgroup（计算机） | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 准确率
    (H) | 120M; 120M | 87.0; 79.7 |'
- en: '| Text classification | 20 Newsgroup (Religion) | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 88.8; 86.7
    |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | 20 Newsgroup（宗教） | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 准确率
    (H) | 120M; 120M | 88.8; 86.7 |'
- en: '| Text classification | 20 Newsgroup (Sports) | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 96.3; 95.9
    |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | 20 Newsgroup（体育） | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 准确率
    (H) | 120M; 120M | 96.3; 95.9 |'
- en: '| Word similarity | Simlex-999 | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    | Logistic regression | Correlation (H) | 120M; 120M | 38.9; 36.9 |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| 词语相似度 | Simlex-999 | 稀疏编码（Faruqui 等，[2015](#bib.bib29)） | 逻辑回归 | 相关性 (H)
    | 120M; 120M | 38.9; 36.9 |'
- en: Table 7\. Comparison of various tensor decomposition methods (sorted by Task
    and then Dataset). In the metric column, H means high is better while L means
    low is better. For compositional codes, 16x32 coding was used. For BTD, two block
    term tensors were used. In (Faruqui et al., [2015](#bib.bib29)), logistic regression
    uses GloVe embeddings.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 各种张量分解方法的比较（按任务和数据集排序）。在指标列中，H 表示越高越好，L 表示越低越好。对于组合编码，使用了 16x32 编码。对于
    BTD，使用了两个块项张量。在（Faruqui 等，[2015](#bib.bib29)）中，逻辑回归使用了 GloVe 嵌入。
- en: 'Table [7](#S6.T7 "Table 7 ‣ 6.4\. Summary ‣ 6\. Tensor decomposition ‣ Compression
    of Deep Learning Models for Text: A Survey") compares various tensor decomposition
    methods across different tasks and datasets. Accuracy of both the original and
    the compressed (comp.) model are shown. Also, we report model size (in terms of
    number of parameters or memory size) for both the student as well as the teacher
    models. For the same task, dataset and model combination, different papers report
    different accuracy of the original model because of slight changes in training
    hyper-parameters; hence we report accuracy of the original model for each row.'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#S6.T7 "表 7 ‣ 6.4\. 总结 ‣ 6\. 张量分解 ‣ 深度学习模型压缩：综述")比较了各种张量分解方法在不同任务和数据集上的表现。显示了原始模型和压缩（comp.）模型的准确度。同时，我们报告了学生模型和教师模型的模型大小（以参数数量或内存大小为单位）。对于相同的任务、数据集和模型组合，不同的论文报告了原始模型的不同准确度，因为训练超参数的微小变化；因此，我们为每一行报告了原始模型的准确度。
- en: 'For CTR prediction, the TT-embedding method (Khrulkov et al., [2019](#bib.bib55))
    in Table [7](#S6.T7 "Table 7 ‣ 6.4\. Summary ‣ 6\. Tensor decomposition ‣ Compression
    of Deep Learning Models for Text: A Survey") uses 3 factors with TT-rank of 16\.
    It actually leads to an embedding compression of 16\. With 4 factors and TT-rank=2,
    test loss increases to 0.4530 with a massive embedding compression of 4193 and
    overall model size of 0.53M. Thus, substitution of large embedding layers with
    TT–embeddings leads to significant compression ratios (up to 2011 times) with
    a slight improvement in the test loss, and up to 4200 with a small drop in the
    test loss.'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CTR预测，表[7](#S6.T7 "表 7 ‣ 6.4\. 总结 ‣ 6\. 张量分解 ‣ 深度学习模型压缩：综述")中的TT-embedding方法（Khrulkov等，[2019](#bib.bib55)）使用了3个因子和TT-rank为16。它实际上导致了16的嵌入压缩。使用4个因子和TT-rank=2时，测试损失增加到0.4530，嵌入压缩为4193，总体模型大小为0.53M。因此，用TT嵌入替代大嵌入层会显著提高压缩比（高达2011倍），测试损失略有改善，并且压缩比高达4200，测试损失小幅下降。
- en: For language modeling, BTD (Ma et al., [2019](#bib.bib78)) leads to an improved
    model with 20% of the Transformer-XL large model. For character level language
    modeling using FLOP (Wang et al., [2019b](#bib.bib126)), an 8M sized FLOP model
    achieves 1.13 on Enwiki8 while gradual pruning achieves 1.14\. Thus, FLOP based
    pruning which combines pruning with matrix factorization is better than both structured
    neuron pruning as well as gradual unstructured pruning. On PTB, BTD is clearly
    much better than TT-embedding. With half the model size compared to Transformer-XL-Base,
    BTD leads to a model with $\sim$10% lower perplexity. On WikiText-103, while FLOP
    pruned model achieves 25.3 perplexity with model size of 50M, gradual unstructured
    pruning and neuron pruning achieve 25.7 and 26.7 respectively with the same model
    size for language modeling on Wiki-103 dataset. Thus, FLOP is better than other
    pruning methods. Again on Wiki-103, BTD is superior to TT-embedding.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言建模，BTD（Ma等，[2019](#bib.bib78)）使得模型得到了20%的Transformer-XL大模型。对于使用FLOP（Wang等，[2019b](#bib.bib126)）的字符级语言建模，8M大小的FLOP模型在Enwiki8上达到了1.13，而逐步剪枝达到了1.14。因此，将剪枝与矩阵分解结合的FLOP剪枝优于结构化神经元剪枝和逐步无结构剪枝。在PTB上，BTD显然比TT-embedding更好。与Transformer-XL-Base相比，BTD使模型的困惑度降低了约10%。在WikiText-103上，虽然FLOP剪枝模型在50M模型大小下达到了25.3的困惑度，但逐步无结构剪枝和神经元剪枝在相同模型大小下分别达到了25.7和26.7。因此，FLOP优于其他剪枝方法。在Wiki-103上，BTD再次优于TT-embedding。
- en: For NMT, the loss-free compression rate reaches 92% on ASPEC dataset by pruning
    90% of the connections. However, with the same pruning ratio, a modest performance
    loss is observed in IWSLT14 dataset. For the models using compositional coding (Shu
    and Nakayama, [2017](#bib.bib108)), the loss-free compression rate is 94% for
    the IWSLT14 dataset and 99% for the ASPEC dataset. Thus, compositional codes are
    much better compared to pruning. TT-embedding with TT-rank=64 leads to embedding
    compression of 15.3 on WMT14 dataset with marginal loss in the BLEU score. Lastly,
    BTD has been used to reduce Transformer size by more than half with improved BLEU
    on WMT16 (en$\rightarrow$de) data.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NMT，通过剪枝90%的连接，ASPEC数据集上的无损压缩率达到了92%。然而，在相同的剪枝比下，IWSLT14数据集中观察到了适度的性能损失。对于使用组合编码（Shu和Nakayama，[2017](#bib.bib108)）的模型，IWSLT14数据集上的无损压缩率为94%，而ASPEC数据集为99%。因此，与剪枝相比，组合编码效果更好。TT-embedding与TT-rank=64在WMT14数据集上的嵌入压缩为15.3，BLEU分数损失极小。最后，BTD已经被用于将Transformer的大小减少一半以上，并在WMT16（en$\rightarrow$de）数据上提升了BLEU分数。
- en: Sparse coding for GloVe vectors (Faruqui et al., [2015](#bib.bib29)) has led
    to improved accuracies for multiple tasks like Noun Phrase (NP) bracketing, question
    classification, text classification and word similarity, while retaining the same
    model size. For sentiment analysis on the IMDB dataset, compositional codes method
    achieves a compression rate of 98% without performance loss. Further, TT-embedding
    method leads to a much smaller model compared to compositional codes with better
    accuracy. In the TT-embedding case, embedding compression rate is 441 with TT-rank=16.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe向量的稀疏编码（Faruqui等，[2015](#bib.bib29)）在名词短语（NP）括弧、问题分类、文本分类和词汇相似性等多个任务中提高了准确性，同时保持了相同的模型大小。在IMDB数据集上的情感分析中，组合编码方法实现了98%的压缩率而没有性能损失。此外，TT-embedding方法相比于组合编码，模型更小且准确性更高。在TT-embedding的情况下，嵌入压缩率为441，TT-rank=16。
- en: For Speech recognition, Prabhavalkar et al. (Prabhavalkar et al., [2016](#bib.bib93))
    experimented with a 3M Google voice utterances dataset and found that a joint
    SVD with explained variance retained after the SVD as 0.6 leads to a 3x smaller
    RNN model without much performance loss. They improved upon Sak et al.’s projections
    method which led to a much higher WER on the same dataset. On live traffic utterances
    dataset, WEST (Variani et al., [2019](#bib.bib121)) leads to a 3x smaller model
    with a slightly reduced word error rate when using 3-layer LSTMs. Variani et al. (Variani
    et al., [2019](#bib.bib121)) further compress this model using quantization to
    obtain a 11x compressed 3-layer LSTM model without any performance loss. Thus,
    using quantization along with matrix decomposition seems to work well.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语音识别，Prabhavalkar等（Prabhavalkar等，[2016](#bib.bib93)）对3M Google语音发音数据集进行了实验，发现联合SVD在SVD后解释的方差保持为0.6可以使RNN模型缩小3倍而几乎没有性能损失。他们在Sak等的方法上进行了改进，该方法在相同数据集上导致了更高的WER。在实时流量发音数据集上，WEST（Variani等，[2019](#bib.bib121)）在使用3层LSTM时将模型缩小了3倍，同时略微减少了词错误率。Variani等（Variani等，[2019](#bib.bib121)）进一步使用量化压缩了该模型，获得了11倍压缩的3层LSTM模型而没有性能损失。因此，使用量化和矩阵分解似乎效果很好。
- en: Overall, matrix decomposition techniques are usually used in combination with
    parameter sharing and sometimes with quantization. They have been very effective
    in dealing with large input/output embedding matrices in RNNs and LSTMs. SVD,
    Tensor Train, CP, Tucker, BTD have been the most popular decomposition techniques
    found to be useful for model compression.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，矩阵分解技术通常与参数共享结合使用，有时还与量化一起使用。这些技术在处理RNN和LSTM中的大规模输入/输出嵌入矩阵时非常有效。SVD、Tensor
    Train、CP、Tucker、BTD是被发现对模型压缩最有用的最流行的分解技术。
- en: 7\. Transformers with Sub-Quadratic Complexity
  id: totrans-693
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 具有亚二次复杂度的变压器
- en: 'Memory usage throughout neural network training can be categorized into three
    main types: (1) Model memory is used to store model parameters; (2) Optimizer
    memory is the additional memory used by the specific learning algorithm during
    the process; (3) Activation memory consists of the outputs of each layer, which
    are cached for reuse in backpropagation to compute gradients. For a BERT-base
    model, model memory is around 0.2 GB, optimizer memory is around 1GB, while activation
    memory is around 8.5GB (Qiu et al., [2020](#bib.bib95)). Time and activation memory
    in Transformers grows quadratically with the sequence length. This is because
    in every layer, every attention head attempts to come up with a transformed representation
    for every position by “paying attention” to tokens at every other position. Quadratic
    complexity implies that practically the maximum input size is rather limited.
    Thus, we cannot extract semantic representation for long documents by passing
    them as input to Transformers.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练过程中内存使用可以分为三种主要类型：(1) 模型内存用于存储模型参数；(2) 优化器内存是特定学习算法在过程中使用的额外内存；(3) 激活内存包含每层的输出，这些输出在反向传播中缓存以用于计算梯度。对于BERT-base模型，模型内存约为0.2
    GB，优化器内存约为1GB，而激活内存约为8.5GB（Qiu等，[2020](#bib.bib95)）。在变压器中，时间和激活内存随序列长度呈二次增长。这是因为在每一层中，每个注意力头尝试通过“关注”每个其他位置的标记，为每个位置生成一个变换表示。二次复杂度意味着实际最大输入大小相当有限。因此，我们不能通过将长文档作为输入传递给变压器来提取语义表示。
- en: 7.1\. Transformers with Super-Linear Complexity
  id: totrans-695
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 具有超线性复杂度的变压器
- en: A few efforts like BlockBERT (Qiu et al., [2020](#bib.bib95)) try to reduce
    this quadratic complexity by a constant factor by introducing sparse block structures
    into the attention matrix. If we split the length-$N$ input sequence into $n$
    blocks, $N\times N$ attention matrix gets partitioned into $n\times n$ blocks,
    where each block matrix is of the size $\frac{N}{n}\times\frac{N}{n}$. Thus, BlockBERT
    reduces $O(N^{2})$ memory consumption by a factor of $n$.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 一些努力如 BlockBERT（Qiu 等，[2020](#bib.bib95)）尝试通过将稀疏块结构引入注意力矩阵来将二次复杂度降低一个常数因子。如果我们将长度为
    $N$ 的输入序列拆分为 $n$ 个块，则 $N\times N$ 的注意力矩阵被划分为 $n\times n$ 个块，其中每个块矩阵的大小为 $\frac{N}{n}\times\frac{N}{n}$。因此，BlockBERT
    将 $O(N^{2})$ 的内存消耗减少了一个因子 $n$。
- en: 'Child et al. (Child et al., [2019](#bib.bib17)) propose sparse transformers
    where sparse factorizations of the attention matrix reduce the quadratic complexity
    to $O(n\sqrt{n})$. The key idea is to reduce the dense attention matrix to a sparse
    version by only computing attention on a sparse number of (query, key) pairs.
    They propose two kinds of sparse factorizations: strided and fixed. Strided attention
    implies having one head attend to the previous $l$ locations, and the other head
    attend to every $l^{th}$ location, where $l$ is the stride and chosen to be close
    to $\sqrt{n}$. More heads could be used with a different stride value. Fixed attention
    assumes that specific positions summarize previous locations and propagate that
    information to all future positions.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: Child 等（Child 等，[2019](#bib.bib17)）提出了稀疏 Transformer，其中注意力矩阵的稀疏分解将二次复杂度减少到 $O(n\sqrt{n})$。关键思想是通过仅计算稀疏数量的（查询，键）对，将稠密注意力矩阵减少为稀疏版本。他们提出了两种稀疏分解方法：跨步（strided）和固定（fixed）。跨步注意力意味着一个头关注于前
    $l$ 个位置，另一个头关注于每 $l^{th}$ 个位置，其中 $l$ 是步长并选择接近 $\sqrt{n}$。可以使用更多的头与不同的步长值。固定注意力假设特定位置总结了之前的位置，并将该信息传播到所有未来的位置。
- en: The Reformer architecture (Kitaev et al., [2020](#bib.bib59)) replaces the dot-product
    attention in a typical Transformer by one that uses locality-sensitive hashing
    (LSH), changing its complexity from $O(n^{2})$ to $O(n\log n)$, where $n$ is the
    length of the sequence. In a standard Transformer, we compute scaled dot-product
    attention as follows.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: Reformer 架构（Kitaev 等，[2020](#bib.bib59)）用局部敏感哈希（LSH）替换了典型 Transformer 中的点积注意力，将其复杂度从
    $O(n^{2})$ 改变为 $O(n\log n)$，其中 $n$ 是序列的长度。在标准 Transformer 中，我们计算缩放点积注意力如下。
- en: '| (68) |  | $\displaystyle\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)V$
    |  |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| (68) |  | $\displaystyle\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)V$
    |  |'
- en: where $Q$, $K$ and $V$ are the standard query, key and value components and
    $d$ is a scaling factor. Reformer uses a Shared QK Transformer, i.e., $Q=K$ enabled
    by sharing the matrix that projects words/hidden layer to $Q$ or $K$. Further,
    note that we are actually only interested in $\text{softmax}(QK^{T})$. Since softmax
    is dominated by the largest elements, for each query $q_{i}$ we only need to focus
    on the keys in $K$ that are closest to $q_{i}$. How can we find the nearest neighbors
    among the keys? Reformer uses LSH. LSH is used to cluster (hash-bucket) the positions
    into various groups, and then every position needs to focus only on others within
    the same bucket.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q$、$K$ 和 $V$ 是标准的查询、键和值组件，$d$ 是一个缩放因子。Reformer 使用共享 QK Transformer，即 $Q=K$
    通过共享将单词/隐藏层投影到 $Q$ 或 $K$ 的矩阵来实现。此外，请注意我们实际上只对 $\text{softmax}(QK^{T})$ 感兴趣。由于
    softmax 受最大元素的支配，对于每个查询 $q_{i}$，我们只需关注 $K$ 中与 $q_{i}$ 最近的键。我们如何找到最近的键邻居？Reformer
    使用 LSH。LSH 用于将位置聚类（哈希桶）到不同的组中，然后每个位置只需关注同一桶中的其他位置。
- en: 7.2\. Transformers with Linear Complexity
  id: totrans-701
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 线性复杂度的 Transformer
- en: Even better, there have been several efforts recently to reduce this quadratic
    complexity to linear. Most of these efforts choose a constant number of other
    positions to “pay attention” to so as to compute a transformed representation
    for any given position. They can model sequences tens of thousands of timesteps
    long using hundreds of layers. The methods differ in their approach towards selecting
    this constant number of other positions. We discuss a few of such recently proposed
    methods in this section.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，最近有几项努力将这种二次复杂度降低到线性复杂度。这些努力大多选择一个常数数量的其他位置来“关注”，以便为任何给定位置计算变换表示。它们可以使用数百层来建模长达数万时间步的序列。这些方法在选择这个常数数量的其他位置的方式上有所不同。本节讨论了一些最近提出的方法。
- en: In Star-Transformers (Guo et al., [2019b](#bib.bib34)), to reduce model complexity
    from $O(n^{2})$ to linear, we replace the fully-connected attention matrix structure
    with a star-shaped topology, in which every two non-adjacent nodes are connected
    through a shared relay node. While ring connections connect a satellite node with
    two other satellite nodes, a radical connection connects a satellite node with
    the relay node. The idea is to update the star-center relay node based on satellite
    nodes and then update satellite nodes using information from the star node, and
    adjacent satellite nodes.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Star-Transformers（Guo 等，[2019b](#bib.bib34)）中，为了将模型复杂度从 $O(n^{2})$ 降低到线性，我们用星形拓扑结构替代了全连接注意力矩阵结构，其中每两个非相邻的节点通过共享中继节点连接。虽然环形连接将一个卫星节点与两个其他卫星节点连接，辐射连接则将一个卫星节点与中继节点连接。这个想法是基于卫星节点更新星中心中继节点，然后使用来自星节点和相邻卫星节点的信息更新卫星节点。
- en: 'Linformer architecture (Wang et al., [2020a](#bib.bib129)) exploits low-rank
    factorization of the self-attention matrix to reduce overall self-attention complexity
    from $O(n^{2})$ to $O(n)$ in both time and space. The main idea is to add two
    linear projection matrices $E_{i},F_{i}\in R^{n\times k}$ when computing key and
    value. We first project the original $(n\times d)$-dimensional key and value layers
    into $(k\times d)$-dimensional projected key and value layers. We then compute
    an $(n\times k)$-dimensional context mapping matrix using scaled dot-product attention.
    If we can choose a very small projected dimension $k$, such that $k<<n$, then
    we can significantly reduce the memory and space consumption. Overall, it is $O(nk)$.
    Further, we can do three other forms of parameter sharing: (1) Headwise sharing:
    $E_{i}=E$ and $F_{i}=F$ across all heads $i$ in a layer. (2) Key-value sharing:
    $E_{i}=F_{i}=E$ across all heads $i$ in a layer. (3) Layerwise sharing: Single
    projection matrix $E$ is used across all layers, all heads for both key and value.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: Linformer 架构（Wang 等，[2020a](#bib.bib129)）利用自注意力矩阵的低秩分解，将整体自注意力复杂度从 $O(n^{2})$
    降低到 $O(n)$，无论是在时间还是空间上。主要思想是在计算键和值时添加两个线性投影矩阵 $E_{i},F_{i}\in R^{n\times k}$。我们首先将原始的
    $(n\times d)$ 维键和值层投影到 $(k\times d)$ 维投影键和值层。然后，我们使用缩放点积注意力计算一个 $(n\times k)$
    维的上下文映射矩阵。如果我们可以选择一个非常小的投影维度 $k$，使得 $k<<n$，那么我们可以显著减少内存和空间消耗。总体复杂度为 $O(nk)$。此外，我们可以进行三种其他形式的参数共享：（1）头部共享：在层中的所有头部
    $i$ 上 $E_{i}=E$ 和 $F_{i}=F$。（2）键值共享：在层中的所有头部 $i$ 上 $E_{i}=F_{i}=E$。（3）层级共享：在所有层、所有头部中对键和值使用单一的投影矩阵
    $E$。
- en: Sparse Sinkhorn Attention based Transformer (Tay et al., [2020](#bib.bib116))
    is based on differentiable sorting of internal representations. First, they divide
    the input sequence into $B$ equal sized blocks each of size $n/B$. A meta sorting
    network learns to generate latent permutations over these block sequences. Given
    sorted sequences, we are then able to compute quasi-global attention with only
    local windows, improving the memory efficiency of the attention module. They also
    propose Causal Sinkhorn Balancing and SortCut algorithms for causal scenarios
    for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Their
    method reduces the memory complexity from $O(n^{2})$ to $O(B^{2}+(n/B)^{2})$.
    The SortCut variant further reduces complexity to linear-time, i.e., $O(nk)$ where
    $k$ is a user defined budget hyper-parameter much smaller than $n$.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 基于稀疏 Sinkhorn 注意力的 Transformer（Tay 等，[2020](#bib.bib116)）是基于内部表示的可微分排序。首先，他们将输入序列划分为
    $B$ 个大小相等的块，每个块的大小为 $n/B$。一个元排序网络学习在这些块序列上生成潜在的排列。给定已排序的序列后，我们能够仅使用局部窗口计算准全局注意力，从而提高注意力模块的内存效率。他们还提出了
    Causal Sinkhorn Balancing 和 SortCut 算法，用于因果场景，以便将 Sinkhorn 注意力用于编码和/或解码目的。他们的方法将内存复杂度从
    $O(n^{2})$ 降低到 $O(B^{2}+(n/B)^{2})$。SortCut 变体进一步将复杂度减少到线性时间，即 $O(nk)$，其中 $k$
    是一个用户定义的预算超参数，远小于 $n$。
- en: Shen et al. (Shen et al., [2018](#bib.bib106)) propose a very simple mathematical
    trick to reduce quadratic complexity to linear. A typical dot-product attention
    can be written as $\text{softmax}(QK^{T})V$ ignoring the scale factor. This is
    quadratic because $QK^{T}$ is $n^{2}$ in size. This can be rewritten as follows.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: Shen 等（Shen 等，[2018](#bib.bib106)）提出了一个非常简单的数学技巧，将二次复杂度减少到线性。典型的点积注意力可以写作 $\text{softmax}(QK^{T})V$，忽略了缩放因子。这是二次的，因为
    $QK^{T}$ 的大小为 $n^{2}$。这可以重写如下。
- en: '| (69) |  | $\displaystyle\text{Attention}(Q,K,V)=\text{softmax}_{r}(Q)(\text{softmax}_{c}(K^{T}V))$
    |  |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| (69) |  | $\displaystyle\text{Attention}(Q,K,V)=\text{softmax}_{r}(Q)(\text{softmax}_{c}(K^{T}V))$
    |  |'
- en: where $\text{softmax}_{r}$ and $\text{softmax}_{c}$ are softmax applied to rows
    and columns respectively. This revised formulation has terms which are only linear
    in $n$. Finally, Katharopoulos et al. (Katharopoulos et al., [2020](#bib.bib54))
    express the self-attention as a linear dot-product of kernel feature maps and
    make use of the associativity property of matrix products to reduce the complexity
    from $O(n^{2})$ to $O(n)$, where $n$ is the sequence length.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\text{softmax}_{r}$ 和 $\text{softmax}_{c}$ 分别对行和列应用 softmax。这个修订后的公式包含的项仅在
    $n$ 上是线性的。最后，Katharopoulos et al. (Katharopoulos et al., [2020](#bib.bib54)) 将自注意力表示为核特征图的线性点积，并利用矩阵乘法的结合律将复杂度从
    $O(n^{2})$ 降低到 $O(n)$，其中 $n$ 是序列长度。
- en: 'Finally, Longformer (Beltagy et al., [2020](#bib.bib7)) propose to reduce Transformer
    complexity to linear by sparsifying the full self-attention matrix using multiple
    kinds of attention patterns. The simplest attention pattern is the sliding window
    pattern which employs a fixed-size window attention surrounding each token. Given
    a fixed window size $w$, each token attends to $0.5w$ tokens on each side. The
    computation complexity of this pattern is $O(nw)$ which scales linearly with input
    sequence length $n$. To further increase the receptive field without increasing
    computation, the sliding window can be “dilated”. This is analogous to dilated
    CNNs where the window has gaps of size dilation $d$. The windowed and dilated
    attention are not flexible enough to learn task-specific representations. Accordingly,
    Longformer also has “global attention” on few pre-selected input locations. Moreover,
    this attention operation is symmetric: that is, a token with a global attention
    attends to all tokens across the sequence, and all tokens in the sequence attend
    to it. Further, two different sets of projections are used: $Q_{s}$, $K_{s}$,
    $V_{s}$ to compute attention scores of sliding window attention, and $Q_{g}$,
    $K_{g}$, $V_{g}$ to compute attention scores for the global attention. The pretrained
    Longformer consistently outperforms RoBERTa on multiple downstream long document
    tasks.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Longformer (Beltagy et al., [2020](#bib.bib7)) 提出通过使用多种注意力模式稀疏化全自注意力矩阵来将变换器复杂度减少到线性。最简单的注意力模式是滑动窗口模式，它使用固定大小的窗口注意力来围绕每个令牌。给定固定的窗口大小
    $w$，每个令牌关注窗口两侧的 $0.5w$ 个令牌。该模式的计算复杂度为 $O(nw)$，随着输入序列长度 $n$ 线性增长。为了进一步增加感受野而不增加计算量，可以“扩张”滑动窗口。这类似于扩张
    CNN，其中窗口有大小为扩张 $d$ 的间隙。窗口和扩张的注意力不够灵活，无法学习特定任务的表示。因此，Longformer 还在少数预选输入位置上具有“全局注意力”。此外，这种注意力操作是对称的：即具有全局注意力的令牌关注序列中的所有令牌，序列中的所有令牌都关注它。此外，使用两组不同的投影：$Q_{s}$、$K_{s}$、$V_{s}$
    用于计算滑动窗口注意力的注意力分数，$Q_{g}$、$K_{g}$、$V_{g}$ 用于计算全局注意力的注意力分数。预训练的 Longformer 在多个下游长文档任务上始终优于
    RoBERTa。
- en: 7.3\. Summary
  id: totrans-710
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 总结
- en: '| Task | Dataset | Model | Base Model | Metric | Eval. (Opt.; Orig) |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 模型 | 基础模型 | 指标 | 评估 (最佳；原始) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Char-level language modeling | 1B Word Benchmark | Sinkhorn Mixture Big (Tay
    et al., [2020](#bib.bib116)) | Transformer Big | BPC (L) | 1.134; 1.825 |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| 字符级语言建模 | 1B 单词基准 | Sinkhorn Mixture Big (Tay et al., [2020](#bib.bib116))
    | 大型变换器 | BPC (L) | 1.134; 1.825 |'
- en: '| Char-level language modeling | 1B Word Benchmark | Sparse Transformer (Child
    et al., [2019](#bib.bib17)) | Transformer Big | BPC (L) | 1.119; 1.825 |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| 字符级语言建模 | 1B 单词基准 | 稀疏变换器 (Child et al., [2019](#bib.bib17)) | 大型变换器 | BPC
    (L) | 1.119; 1.825 |'
- en: '| Char-level language modeling | Enwik8 | Longformer (Beltagy et al., [2020](#bib.bib7))
    | Transformer | BPC (L) | 1.00; 1.11 |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '| 字符级语言建模 | Enwik8 | Longformer (Beltagy et al., [2020](#bib.bib7)) | 变换器 |
    BPC (L) | 1.00; 1.11 |'
- en: '| Char-level language modeling | Enwik8 | Reformer (Kitaev et al., [2020](#bib.bib59))
    | Transformer | BPC (L) | 1.05; 1.11 |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| 字符级语言建模 | Enwik8 | Reformer (Kitaev et al., [2020](#bib.bib59)) | 变换器 | BPC
    (L) | 1.05; 1.11 |'
- en: '| Char-level language modeling | text8 | Longformer (Beltagy et al., [2020](#bib.bib7))
    | Transformer | BPC (L) | 1.10; 1.18 |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| 字符级语言建模 | text8 | Longformer (Beltagy et al., [2020](#bib.bib7)) | 变换器 |
    BPC (L) | 1.10; 1.18 |'
- en: '| Coreference resolution | OntoNotes | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 78.6; 78.4 |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '| 共指消解 | OntoNotes | Longformer-base (Beltagy et al., [2020](#bib.bib7)) |
    RoBERTa-base | F1 (H) | 78.6; 78.4 |'
- en: '| Language modeling | 1B Word Benchmark | Sinkhorn Mixture Big (Tay et al.,
    [2020](#bib.bib116)) | Transformer Big | Perplexity (L) | 27.34; 27.59 |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B 单词基准 | Sinkhorn Mixture Big (Tay et al., [2020](#bib.bib116)) |
    大型变换器 | 困惑度 (L) | 27.34; 27.59 |'
- en: '| Language modeling | 1B Word Benchmark | Sparse Transformer (Child et al.,
    [2019](#bib.bib17)) | Transformer Big | Perplexity (L) | 28.77; 27.59 |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | 1B Word Benchmark | Sparse Transformer (Child et al., [2019](#bib.bib17))
    | Transformer Big | 困惑度 (L) | 28.77; 27.59 |'
- en: '| Named Entity Recognition | CoNLL2003 | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 90.93; 86.48 |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别 | CoNLL2003 | Star Transformer (Guo et al., [2019b](#bib.bib34)) |
    Transformer | 准确率 (H) | 90.93; 86.48 |'
- en: '| Named Entity Recognition | CoNLL2012 | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 86.30; 83.57 |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别 | CoNLL2012 | Star Transformer (Guo et al., [2019b](#bib.bib34)) |
    Transformer | 准确率 (H) | 86.30; 83.57 |'
- en: '| NLI | MNLI | SortCut Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer
    | Acc (H) | 55.80; 53.69 |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | MNLI | SortCut Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer
    | 准确率 (H) | 55.80; 53.69 |'
- en: '| NLI | QNLI | Linformer (Wang et al., [2020a](#bib.bib129)) | BERT-base |
    Acc (H) | 91.2; 91.8 |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | QNLI | Linformer (Wang et al., [2020a](#bib.bib129)) | BERT-base
    | 准确率 (H) | 91.2; 91.8 |'
- en: '| NLI | SNLI | Star Transformer (Guo et al., [2019b](#bib.bib34)) | Transformer
    | Acc (H) | 86.0; 82.2 |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | SNLI | Star Transformer (Guo et al., [2019b](#bib.bib34)) | Transformer
    | 准确率 (H) | 86.0; 82.2 |'
- en: '| NLI | SNLI | SortCut Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer
    | Acc (H) | 80.30; 78.87 |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | SNLI | SortCut Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer
    | 准确率 (H) | 80.30; 78.87 |'
- en: '| NMT (en$\rightarrow$de) | WMT14 | Reformer big (Kitaev et al., [2020](#bib.bib59))
    | Transformer Big | BLEU (H) | 29.1; 27.3 |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (en$\rightarrow$de) | WMT14 | Reformer big (Kitaev et al., [2020](#bib.bib59))
    | Transformer Big | BLEU (H) | 29.1; 27.3 |'
- en: '| POS Tagging | PTB | Star Transformer (Guo et al., [2019b](#bib.bib34)) |
    Transformer | Acc (H) | 97.14; 96.31 |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | PTB | Star Transformer (Guo et al., [2019b](#bib.bib34)) | Transformer
    | 准确率 (H) | 97.14; 96.31 |'
- en: '| Question answering | HotpotQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 78.94; 77.08 |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | HotpotQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 78.94; 77.08 |'
- en: '| Question answering | HotpotQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 76.02; 77.08 |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | HotpotQA | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT |
    F1 (H) | 76.02; 77.08 |'
- en: '| Question answering | NaturalQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 79.39; 78.29 |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | NaturalQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 79.39; 78.29 |'
- en: '| Question answering | NaturalQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 77.31; 78.29 |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | NaturalQA | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT |
    F1 (H) | 77.31; 78.29 |'
- en: '| Question answering | NewsQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 70.08; 66.25 |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | NewsQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 70.08; 66.25 |'
- en: '| Question answering | NewsQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 67.16; 66.25 |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | NewsQA | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT | F1
    (H) | 67.16; 66.25 |'
- en: '| Question answering | SearchQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 83.51; 80.37 |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SearchQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 83.51; 80.37 |'
- en: '| Question answering | SearchQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 80.54; 80.37 |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SearchQA | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT |
    F1 (H) | 80.54; 80.37 |'
- en: '| Question answering | SQuAD 1.1 | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 90.74; 88.45 |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD 1.1 | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 90.74; 88.45 |'
- en: '| Question answering | SQuAD 1.1 | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 88.37; 88.45 |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD 1.1 | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT |
    F1 (H) | 88.37; 88.45 |'
- en: '| Question answering | SQuAD 2.0 | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 81.45; 77.16 |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD 2.0 | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 81.45; 77.16 |'
- en: '| Question answering | SQuAD 2.0 | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 77.57; 77.16 |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | SQuAD 2.0 | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT |
    F1 (H) | 77.57; 77.16 |'
- en: '| Question answering | TriviaQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 79.41; 75.35 |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | TriviaQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 79.41; 75.35 |'
- en: '| Question answering | TriviaQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 75.34; 75.35 |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | TriviaQA | SparseBERT (Child et al., [2019](#bib.bib17)) | BERT |
    F1 (H) | 75.34; 75.35 |'
- en: '| Question answering | TriviaQA | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 75.2; 74.3 |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | TriviaQA | Longformer-base (Beltagy et al., [2020](#bib.bib7)) | RoBERTa-base
    | F1 (H) | 75.2; 74.3 |'
- en: '| Question answering | WikiHop | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | Acc (H) | 75.0; 72.4 |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | WikiHop | Longformer-base (Beltagy et al., [2020](#bib.bib7)) | RoBERTa-base
    | 准确率 (H) | 75.0; 72.4 |'
- en: '| Sentiment analysis | IMDB | Linformer (Wang et al., [2020a](#bib.bib129))
    | BERT-base | Acc (H) | 94.1; 93.5 |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | Linformer (Wang et al., [2020a](#bib.bib129)) | BERT-base |
    准确率 (H) | 94.1; 93.5 |'
- en: '| Sentiment analysis | IMDB | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | Acc (H) | 95.7; 95.3 |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB | Longformer-base (Beltagy et al., [2020](#bib.bib7)) | RoBERTa-base
    | 准确率 (H) | 95.7; 95.3 |'
- en: '| Sentiment analysis | SST | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 52.9; 50.4 |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST | Star Transformer (Guo et al., [2019b](#bib.bib34)) | Transformer
    | 准确率 (H) | 52.9; 50.4 |'
- en: '| Sentiment analysis | SST | Sinkhorn  (Tay et al., [2020](#bib.bib116)) |
    Transformer | Acc (H) | 77.52; 76.83 |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST | Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer | 准确率
    (H) | 77.52; 76.83 |'
- en: '| Sentiment analysis | SST-2 | Linformer (Wang et al., [2020a](#bib.bib129))
    | BERT-base | Acc (H) | 93.1; 92.7 |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | SST-2 | Linformer (Wang et al., [2020a](#bib.bib129)) | BERT-base
    | 准确率 (H) | 93.1; 92.7 |'
- en: '| Speech recognition | WSJ | Linear Transformer (Katharopoulos et al., [2020](#bib.bib54))
    | Reformer | Phoneme Error Rate (L) | 8.08; 9.33 |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | WSJ | Linear Transformer (Katharopoulos et al., [2020](#bib.bib54))
    | Reformer | 音素错误率 (L) | 8.08; 9.33 |'
- en: '| Text classification | Hyperpartisan | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 94.8; 87.4 |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | Hyperpartisan | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 94.8; 87.4 |'
- en: '| Text classification | MTL-16 | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 86.98; 82.78 |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | MTL-16 | Star Transformer (Guo et al., [2019b](#bib.bib34)) | Transformer
    | 准确率 (H) | 86.98; 82.78 |'
- en: '| Textual similarity | QQP | Linformer (Wang et al., [2020a](#bib.bib129))
    | BERT-base | Acc (H) | 90.8; 89.6 |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| 文本相似度 | QQP | Linformer (Wang et al., [2020a](#bib.bib129)) | BERT-base |
    准确率 (H) | 90.8; 89.6 |'
- en: 'Table 8\. Comparison of various sub-quadratic complexity Transformer methods
    (sorted by Task and then Dataset). In the metric column, H means high is better
    while L means low is better. Note that in this case, model sizes do not reduce
    much; activation memory reduces as described in Section [7](#S7 "7\. Transformers
    with Sub-Quadratic Complexity ‣ Compression of Deep Learning Models for Text:
    A Survey") (with comparable or better accuracy) compared to the standard Transformer.'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '表8\. 各种次线性复杂度Transformer方法的比较（按任务和数据集排序）。在度量列中，H表示越高越好，而L表示越低越好。请注意，在这种情况下，模型尺寸变化不大；激活内存减少，如第[7](#S7
    "7\. Transformers with Sub-Quadratic Complexity ‣ Compression of Deep Learning
    Models for Text: A Survey")节所述（准确性相当或更好）与标准Transformer相比。'
- en: 'Table [8](#S7.T8 "Table 8 ‣ 7.3\. Summary ‣ 7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey") compares
    various sub-quadratic Transformer methods across different tasks and datasets.
    Accuracy of both the original Transformer and the optimized (opt.) model are shown.
    These models have been applied for various applications like language modeling
    (both word level as well as character level), coreference resolution, NER, NLI,
    NMT, POS, question answering, sentiment analysis, speech recognition, text classification
    and text similarity analysis. For the same task, dataset and model combination,
    different papers report different accuracy of the original model because of slight
    changes in training hyper-parameters; hence we report accuracy of the original
    model for each row.'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](#S7.T8 "Table 8 ‣ 7.3\. Summary ‣ 7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey")比较了各种次线性复杂度的Transformer方法在不同任务和数据集上的表现。展示了原始Transformer模型和优化（opt.）模型的准确性。这些模型已应用于语言建模（包括词级和字符级）、共指消解、命名实体识别、自然语言推断、机器翻译、词性标注、问答、情感分析、语音识别、文本分类和文本相似度分析等多种应用。由于训练超参数的微小变化，不同论文报告了相同任务、数据集和模型组合的原始模型的不同准确性；因此，我们报告了每一行的原始模型准确性。'
- en: Star Transformers were shown to outperform the vanilla Transformer model across
    various tasks like Sentiment analysis, Text classification, NLI, POS and NER.
    On the SST, the Star Transformer achieves 2.5 points improvement against the standard
    Transformer. On MTL-16, the Star-Transformer outperform the standard Transformer
    in all 16 datasets, the improvement of the average accuracy is 4.2\. Average test
    times are 10.94 and 49.31ms per batch with batch-size=128 for Star Transformer
    and standard Transformer respectively.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: Star Transformers被证明在各种任务如情感分析、文本分类、自然语言推断、词性标注和命名实体识别上优于原始Transformer模型。在SST上，Star
    Transformer相比标准Transformer提高了2.5分。在MTL-16上，Star Transformer在所有16个数据集中均优于标准Transformer，平均准确性提高了4.2。Star
    Transformer和标准Transformer的平均测试时间分别为10.94和49.31毫秒（批量大小=128）。
- en: Longformer achieves a new state-of-the-art on both text8 and enwik8 using the
    small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively. Longformer-large
    model of the same size as Sparse Transformer achieves a BPC of 0.99 on Enwik8,
    which is the same as that obtained using Sparse Transformer. Also, for both character-level
    as well as word-level language modeling, on 1B word benchmark, we observe that
    Sinkhorn mixture (which is a combination of the Sinkhorn attention by mixing it
    with the vanilla standard dot product attention) performs better than Sparse Transformer.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer 在 text8 和 enwik8 上使用小模型分别达到 1.10 和 1.00 的 BPC，创下了新的最先进水平。与 Sparse
    Transformer 大小相同的 Longformer-large 模型在 Enwik8 上取得了 0.99 的 BPC，与使用 Sparse Transformer
    的结果相同。此外，在字符级和词级语言建模上，在 1B 词基准测试中，我们观察到 Sinkhorn 混合（将 Sinkhorn 注意力与标准点积注意力混合）优于
    Sparse Transformer。
- en: On question answering, results have been reported across multiple datasets like
    HotpotQA, NaturalQA, NewsQA, SearchQA, TriviaQA, WikiHop, SQuAD 1.1 and SQuAD
    2.0\. We observe that BlockBERT performs better than SparseBERT. Also, it is not
    surprising that BlockBERT with 2 blocks (n = 2) performs better than that with
    3 blocks (n = 3), because it keeps more attention matrix entries. Also, not shown
    in the table, Longformer-large achieves scores of 81.9 and 73.2 for WikiHop and
    HotpotQA beating state-of-the-art results by 3.6 and 4 points respectively.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 在问答任务上，结果已在多个数据集上报告，如 HotpotQA、NaturalQA、NewsQA、SearchQA、TriviaQA、WikiHop、SQuAD
    1.1 和 SQuAD 2.0。我们观察到 BlockBERT 优于 SparseBERT。此外，BlockBERT 在 2 个块（n = 2）时优于 3
    个块（n = 3），因为它保留了更多的注意力矩阵条目。表中未显示的是，Longformer-large 在 WikiHop 和 HotpotQA 上分别取得了
    81.9 和 73.2 的分数，分别比最先进结果高出 3.6 和 4 分。
- en: Longformer consistently outperforms the RoBERTa baseline across many tasks like
    coreference resolution, question answering, sentiment analysis and text classification.
    Its performance gain is especially obvious for tasks that require long context
    such as WikiHop (question answering) and Hyperpartisan (text classification).
    For TriviaQA (question answering), the improvement is more modest as the local
    context is often sufficient to answer the question. In the case of HotpotQA (question
    answering), the supporting fact auxiliary supervision allows models to easily
    find relevant contexts and then focus on local context, leading to smaller gains.
    On the IMDB (sentiment analysis) and OntoNotes (coreference resolution) datasets
    the performance gains are smaller. For IMDB, the majority of the dataset consists
    of short documents and thus it is expected to see smaller improvements. For OntoNotes,
    the distance between any two mentions is typically quite small so that a baseline
    that processes smaller chunks separately is able to stitch together mentions into
    coreference chains without considering cross chunk interactions.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer 在核心指代解析、问答、情感分析和文本分类等多个任务中始终优于 RoBERTa 基线。对于需要长上下文的任务，如 WikiHop（问答）和
    Hyperpartisan（文本分类），其性能提升尤为明显。对于 TriviaQA（问答），改进则比较温和，因为局部上下文通常足以回答问题。对于 HotpotQA（问答），支持性事实辅助监督使得模型能够轻松找到相关上下文，然后专注于局部上下文，导致性能提升较小。在
    IMDB（情感分析）和 OntoNotes（核心指代解析）数据集上，性能提升较小。对于 IMDB，数据集大部分由短文档组成，因此期望看到较小的改进。对于 OntoNotes，任何两个提及之间的距离通常较小，因此一个处理较小片段的基线能够在不考虑跨片段交互的情况下，将提及拼接成指代链。
- en: On speech recognition task, Linear transformers achieve similar performance
    to vanilla transformers and they are up to 4000x faster on autoregressive prediction
    of very long sequences. The linear Transformer model outperforms the LSTM and
    Reformer while being faster to train and evaluate. Reformer takes 2250 seconds
    per epoch, while Linear Transformers take just 824s/epoch.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音识别任务中，线性变换器在自回归预测非常长的序列时的表现与普通变换器相似，但速度快达 4000 倍。线性变换器模型在训练和评估时比 LSTM 和 Reformer
    更快。Reformer 每个周期需 2250 秒，而线性变换器仅需 824 秒/周期。
- en: To summarize, multiple methods have been proposed to reduce the quadratic complexity
    of the standard Transformer model. While Sparse Transformers reduce it to $O(n\sqrt{n})$,
    Reformers reduce it to $O(n\log n)$. Other methods like Star Transformer, Linformer,
    Sparse Sinkhorn Transformer, Efficient Attention, Linear Transformers and Longformer
    promise linear complexity. In particular Sparse Sinkhorn Transformer and Longformer
    have been shown to result into very good accuracy, latency and RAM tradeoff across
    many tasks.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，多种方法已被提出以降低标准Transformer模型的二次复杂度。虽然稀疏Transformer将其降低到$O(n\sqrt{n})$，Reformers将其降低到$O(n\log
    n)$。其他方法如Star Transformer、Linformer、Sparse Sinkhorn Transformer、Efficient Attention、Linear
    Transformers和Longformer承诺线性复杂度。特别是Sparse Sinkhorn Transformer和Longformer在许多任务中显示了很好的准确性、延迟和RAM权衡。
- en: 8\. Summary and Future Directions
  id: totrans-762
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 总结与未来方向
- en: We discussed various methods for compression of deep learning models for text.
    Broadly, we discussed pruning, quantization, knowledge distillation, parameter
    sharing, tensor decomposition, and sub-quadratic Transformer based methods. These
    methods not just help reduce the model size, but also lead to lower prediction
    latencies and low power consumption due to reduced computations. Pruning is the
    oldest method, but not commonly applied for Transformers. Quantization is effective
    however it is important to use mixed precision balanced quantization with GPU
    architectures that support efficient low-bit computations. Knowledge Distillation
    is the most popular method for compression of Transformer models. Parameter sharing
    is a very useful method but often needs to be combined with other techniques.
    Matrix decomposition is not very common but has lots of potential, especially
    the BTD method. Sub-quadratic Transformers are very important to enable processing
    long documents in applications like query-document similarity, long document summarization,
    etc.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了文本深度学习模型的多种压缩方法。总体上，我们讨论了剪枝、量化、知识蒸馏、参数共享、张量分解和亚二次Transformer方法。这些方法不仅有助于减少模型大小，还由于减少计算而降低预测延迟和功耗。剪枝是最古老的方法，但在Transformer中不常用。量化有效，但重要的是使用支持高效低位计算的GPU架构的混合精度平衡量化。知识蒸馏是压缩Transformer模型的最流行方法。参数共享是一种非常有用的方法，但通常需要与其他技术结合使用。矩阵分解不常见，但有很大潜力，特别是BTD方法。亚二次Transformer在处理长文档的应用中非常重要，如查询-文档相似性、长文档摘要等。
- en: 8.1\. Comparison across model compression method types.
  id: totrans-764
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 各种模型压缩方法类型的比较。
- en: In the previous six sections, we have compared across multiple methods within
    each of the six broad types of model compression. In this subsection, we attempt
    to compare across multiple model compression method types.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 在前六节中，我们比较了六种广泛模型压缩类型中的多种方法。在这一小节中，我们尝试比较不同的模型压缩方法类型。
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey") provides a comparison
    of model size versus accuracy for various methods across various GLUE (Wang et al.,
    [2019b](#bib.bib126)) and SQuAD tasks. We observe that most of the methods tried
    on GLUE datasets have been based on knowledge distillation. However, some pruning
    methods (iterative magnitude pruning, RPP and LayerDrop), quantization (mixed
    precision QBERT), parameter sharing (ALBERT), tensor decomposition (FLOP) and
    linear Transformer (Linformer) have also been tried. Quantization methods do not
    reduce the number of parameters but reduce the number of bits per parameter. Particularly,
    mixed precision QBERT uses 8 bits for embeddings and 2/3/4 bits for encoder layer
    weights. Similarly, Linformer does not reduce number of weights but reduces activation
    memory as well as latency of the overall Transformer model. For the remaining
    models, for each model, we first computed an average GLUE score based on any of
    the 9 tasks for which scores have been reported. Next, we computed the ratio GLUE
    score/model size (in Millions). We find the following as the top three'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S4.T4 "Table 4 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD) ‣ Compression
    of Deep Learning Models for Text: A Survey")提供了不同方法在各GLUE (Wang et al., [2019b](#bib.bib126))和SQuAD任务上的模型大小与准确性的比较。我们观察到，大多数在GLUE数据集上尝试的方法都基于知识蒸馏。然而，也尝试了一些剪枝方法（迭代幅度剪枝、RPP和LayerDrop）、量化（混合精度QBERT）、参数共享（ALBERT）、张量分解（FLOP）和线性Transformer（Linformer）。量化方法不会减少参数的数量，而是减少每个参数的位数。特别地，混合精度QBERT使用8位表示嵌入，2/3/4位表示编码器层权重。类似地，Linformer并不减少权重的数量，而是减少了激活内存以及整个Transformer模型的延迟。对于其余的模型，对于每个模型，我们首先计算了基于报告得分的9个任务中的平均GLUE得分。接着，我们计算了GLUE得分/模型大小（以百万为单位）的比率。我们发现以下三种是排名前三的：'
- en: •
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)) (ratio=79.3)
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)) (比例=79.3)
- en: •
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixed-vocab. training (Zhao et al., [2019](#bib.bib143)) (ratio=7.8)
  id: totrans-770
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合词汇训练 (Zhao et al., [2019](#bib.bib143)) (比例=7.8)
- en: •
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ALBERT-B (Lan et al., [2019](#bib.bib61)) (ratio=7.2)
  id: totrans-772
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ALBERT-B (Lan et al., [2019](#bib.bib61)) (比例=7.2)
- en: This clearly tells us that Distilled BiLSTMs provide us the best accuracy versus
    size tradeoff on GLUE. However, each of these three models actually report results
    on only 4 out of 9 GLUE tasks. Hence, further, we considered only those methods
    for which results on at least 5 tasks have been reported and computed the GLUE
    score/model size ratio. We find the following as the top three
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地告诉我们，Distilled BiLSTM在GLUE上提供了最佳的准确性与模型大小的折中。然而，这三种模型实际上仅在9个GLUE任务中的4个上报告了结果。因此，我们仅考虑了在至少5个任务上报告了结果的方法，并计算了GLUE得分/模型大小比。我们发现以下三种是排名前三的：
- en: •
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: TinyBERT (Jiao et al., [2019](#bib.bib51)) (ratio=5.31)
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TinyBERT (Jiao et al., [2019](#bib.bib51)) (比例=5.31)
- en: •
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BERT-EMD (Li et al., [2020](#bib.bib67)) (ratio=5.15)
  id: totrans-777
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BERT-EMD (Li et al., [2020](#bib.bib67)) (比例=5.15)
- en: •
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MobileBERT+Quantization (Sun et al., [2020](#bib.bib112)) (ratio=3.49)
  id: totrans-779
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MobileBERT+量化 (Sun et al., [2020](#bib.bib112)) (比例=3.49)
- en: Thus, on GLUE tasks, it is clear that distillation based methods (combined with
    quantization) are better than other types of methods.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在GLUE任务上，显然基于蒸馏的方法（结合量化）优于其他类型的方法。
- en: 'Tables [1](#S2.T1 "Table 1 ‣ 2.4.3\. Pruning General Structures ‣ 2.4\. Pruning
    Heads and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text:
    A Survey"), [2](#S3.T2 "Table 2 ‣ 3.4\. Summary ‣ 3\. Quantization ‣ Compression
    of Deep Learning Models for Text: A Survey"), [3](#S4.T3 "Table 3 ‣ 4.5\. Summary
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey"), [6](#S5.T6 "Table 6 ‣ 5.4\. Summary ‣ 5\. Parameter sharing ‣ Compression
    of Deep Learning Models for Text: A Survey"), [7](#S6.T7 "Table 7 ‣ 6.4\. Summary
    ‣ 6\. Tensor decomposition ‣ Compression of Deep Learning Models for Text: A Survey")
    and [8](#S7.T8 "Table 8 ‣ 7.3\. Summary ‣ 7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey") compare
    various pruning, quantization, knowledge distillation, parameter sharing, tensor
    decomposition and sub-quadratic Transformer methods across different tasks and
    datasets. Overall, there are 123 (task, dataset) combinations across these six
    tables which implies that unlike GLUE, not many methods have been applied on the
    same set of (task, dataset) combinations⁴⁴4We make the entire statistics available
    as an excel file at [https://bit.ly/3vmaxZ9](https://bit.ly/3vmaxZ9).. The most
    popular tasks are language modeling (on PTB and 1B word benchmark), sentiment
    analysis (on SST) and NMT (WMT14 en$\rightarrow$de).'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S2.T1 "表格 1 ‣ 2.4.3\. 剪枝通用结构 ‣ 2.4\. 剪枝头部和层 ‣ 2\. 剪枝 ‣ 文本深度学习模型的压缩：一项综述"), [2](#S3.T2
    "表格 2 ‣ 3.4\. 总结 ‣ 3\. 量化 ‣ 文本深度学习模型的压缩：一项综述"), [3](#S4.T3 "表格 3 ‣ 4.5\. 总结 ‣
    4\. 知识蒸馏 (KD) ‣ 文本深度学习模型的压缩：一项综述"), [6](#S5.T6 "表格 6 ‣ 5.4\. 总结 ‣ 5\. 参数共享 ‣ 文本深度学习模型的压缩：一项综述"), [7](#S6.T7
    "表格 7 ‣ 6.4\. 总结 ‣ 6\. 张量分解 ‣ 文本深度学习模型的压缩：一项综述") 和 [8](#S7.T8 "表格 8 ‣ 7.3\. 总结
    ‣ 7\. 子二次复杂度的变换器 ‣ 文本深度学习模型的压缩：一项综述") 比较了在不同任务和数据集上的各种剪枝、量化、知识蒸馏、参数共享、张量分解和子二次变换器方法。总体来看，这六个表格中共有
    123 种 (任务, 数据集) 组合，这意味着与 GLUE 不同，并不是很多方法都应用于相同的 (任务, 数据集) 组合⁴⁴4 我们将所有统计数据以 Excel
    文件形式提供，网址为 [https://bit.ly/3vmaxZ9](https://bit.ly/3vmaxZ9).. 最受欢迎的任务是语言建模（在 PTB
    和 1B 单词基准上）、情感分析（在 SST 上）和 NMT（WMT14 en$\rightarrow$de）。
- en: For language modeling on PTB, on LSTM models, bank balanced sparsity (Cao et al.,
    [2019](#bib.bib10)) based pruning method worked best. With Transformer models,
    Block Term Decomposition (BTD) (Ma et al., [2019](#bib.bib78)) method seems to
    work best. Among various methods like parameter sharing, tensor decomposition
    and sub-quadratic complexity Transformer which have been tried for language modeling
    on 1B Word Benchmark, again, BTD (Ma et al., [2019](#bib.bib78)) method seems
    to work best leading to a model with 0.16B parameters and a perplexity as low
    as 19.5\. Multiple datasets have been used for Neural machine translation (NMT).
    Datasets from WMT and IWSLT are the most popular. Among these, the en$\rightarrow$de
    from WMT14 is the most popular dataset used for testing various NMT models. For
    en$\rightarrow$de NMT with WMT14, using 2-layer LSTMs, the best accuracy versus
    size tradeoff is using Pruned Seq-KD + Seq-Inter (Kim and Rush, [2016](#bib.bib57))
    which gives a 8M size model leading to 18.5 BLEU. Among Transformer based models,
    BTD (Ma et al., [2019](#bib.bib78)) leads to a Transformer model which provides
    34.91 BLEU with 21.2M model size.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 PTB 上的语言建模，在 LSTM 模型上，基于银行平衡稀疏性 (Cao et al., [2019](#bib.bib10)) 的剪枝方法效果最好。在变换器模型中，Block
    Term Decomposition (BTD) (Ma et al., [2019](#bib.bib78)) 方法似乎效果最佳。在尝试了多种方法如参数共享、张量分解和子二次复杂度变换器用于
    1B 单词基准上的语言建模后，BTD (Ma et al., [2019](#bib.bib78)) 方法似乎再次表现最佳，得到一个具有 0.16B 参数和
    19.5 的困惑度的模型。多种数据集被用于神经机器翻译 (NMT)。WMT 和 IWSLT 的数据集最为流行。其中，WMT14 的 en$\rightarrow$de
    数据集是用于测试各种 NMT 模型的最受欢迎的数据集。对于 WMT14 的 en$\rightarrow$de NMT，使用 2 层 LSTM 时，最佳的准确性与大小折衷是使用
    Pruned Seq-KD + Seq-Inter (Kim 和 Rush, [2016](#bib.bib57))，这会得到一个 8M 大小的模型，BLEU
    分数为 18.5。在基于变换器的模型中，BTD (Ma et al., [2019](#bib.bib78)) 生成了一个变换器模型，提供了 34.91 的
    BLEU 和 21.2M 的模型大小。
- en: '| Task | Popular Datasets | References |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 流行数据集 | 参考文献 |'
- en: '| --- | --- | --- |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Language modeling | Penn TreeBank Corpus, One billion word benchmark, Europarl,
    WikiText-103, text8, source code of Linux kernel, 2013 ACL Workshop Morphological
    Language Datasets (ACLW), Arabic news commentary corpus, 2013 ACL workshop on
    MT, enwik8 (from Wikipedia), Lambada | Neuron Pruning (Murray and Chiang, [2015](#bib.bib86)),
    Iterative magnitude pruning (Zhu and Gupta, [2017](#bib.bib146)), Block sparsity (Cao
    et al., [2019](#bib.bib10)),Loss -Aware Quantization (Xu et al., [2018](#bib.bib133);
    Hou and Kwok, [2018](#bib.bib45)), Uniform Quantization (He et al., [2016](#bib.bib41);
    Kapur et al., [2017](#bib.bib53)), Binary Quantization (Hubara et al., [2017](#bib.bib48)),
    HitNet (Wang et al., [2018](#bib.bib128)), Sparse Word Representations (Chen et al.,
    [2016](#bib.bib14)), LightRNN (Li et al., [2016a](#bib.bib68)), Slim Embeddings (Li
    et al., [2018](#bib.bib69)), C2W (Ling et al., [2015](#bib.bib72)), LayerDrop (Fan
    et al., [2019](#bib.bib28)), Reformer (Kitaev et al., [2020](#bib.bib59)), Linformer (Wang
    et al., [2020a](#bib.bib129)), Char-CNN (Jozefowicz et al., [2016](#bib.bib52)),
    CNN+Highway Network (Kim et al., [2016](#bib.bib56)), SparseBERT (Child et al.,
    [2019](#bib.bib17)), FLOP (Wang et al., [2019c](#bib.bib131)), Deep Equilibrium
    Models (Bai et al., [2019](#bib.bib5)), WEST (Variani et al., [2019](#bib.bib121)),
    Sparse Sinkhorn Attention (Tay et al., [2020](#bib.bib116)), BTD (Ma et al., [2019](#bib.bib78)),
    Universal Transformers (Dehghani et al., [2018](#bib.bib22)), TT-embedding (Khrulkov
    et al., [2019](#bib.bib55)), multiple methods (Grachev et al., [2019](#bib.bib32))
    |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 | Penn TreeBank 语料库, 十亿词基准, Europarl, WikiText-103, text8, Linux 内核源代码,
    2013 ACL 研讨会形态学语言数据集 (ACLW), 阿拉伯语新闻评论语料库, 2013 ACL 机器翻译研讨会, enwik8 (来自 Wikipedia),
    Lambada | 神经元剪枝 (Murray 和 Chiang, [2015](#bib.bib86)), 迭代大小剪枝 (Zhu 和 Gupta, [2017](#bib.bib146)),
    块稀疏性 (Cao 等, [2019](#bib.bib10)), 损失感知量化 (Xu 等, [2018](#bib.bib133); Hou 和 Kwok,
    [2018](#bib.bib45)), 均匀量化 (He 等, [2016](#bib.bib41); Kapur 等, [2017](#bib.bib53)),
    二值量化 (Hubara 等, [2017](#bib.bib48)), HitNet (Wang 等, [2018](#bib.bib128)), 稀疏词表示
    (Chen 等, [2016](#bib.bib14)), LightRNN (Li 等, [2016a](#bib.bib68)), 精简嵌入 (Li 等,
    [2018](#bib.bib69)), C2W (Ling 等, [2015](#bib.bib72)), LayerDrop (Fan 等, [2019](#bib.bib28)),
    Reformer (Kitaev 等, [2020](#bib.bib59)), Linformer (Wang 等, [2020a](#bib.bib129)),
    Char-CNN (Jozefowicz 等, [2016](#bib.bib52)), CNN+高速公路网络 (Kim 等, [2016](#bib.bib56)),
    SparseBERT (Child 等, [2019](#bib.bib17)), FLOP (Wang 等, [2019c](#bib.bib131)),
    深度平衡模型 (Bai 等, [2019](#bib.bib5)), WEST (Variani 等, [2019](#bib.bib121)), 稀疏 Sinkhorn
    注意力 (Tay 等, [2020](#bib.bib116)), BTD (Ma 等, [2019](#bib.bib78)), 通用变换器 (Dehghani
    等, [2018](#bib.bib22)), TT-嵌入 (Khrulkov 等, [2019](#bib.bib55)), 多种方法 (Grachev
    等, [2019](#bib.bib32)) |'
- en: '| Neural Machine translation (NMT) | IWSLT German-English, IWSLT Thai-English,
    ASPEC English-Japanese, WMT English-German, WMT German-English, WMT English-Russian,
    IWSLT English Vietnamese, WMT English-Romanian, WMT English-Estonian, Ted Talk
    | Compositional codes (Shu and Nakayama, [2017](#bib.bib108)), LayerDrop (Fan
    et al., [2019](#bib.bib28)), Pruning attention heads (Voita et al., [2019](#bib.bib123);
    Michel et al., [2019](#bib.bib80)), Neuron Pruning (Murray and Chiang, [2015](#bib.bib86)),
    Magnitude Pruning (See et al., [2016](#bib.bib104)), Iterative magnitude pruning (Zhu
    and Gupta, [2017](#bib.bib146); Cheong and Daniel, [2019](#bib.bib16)), Pruned
    Seq-KD + Seq-Inter (Kim and Rush, [2016](#bib.bib57)), Quantized Distillation (Polino
    et al., [2018](#bib.bib92)), Teacher ensembles KD (Freitag et al., [2017](#bib.bib30)),
    Multiple teachers KD (Tan et al., [2019](#bib.bib114)), BTD (Ma et al., [2019](#bib.bib78)),
    Quaternion Attention (Tay et al., [2019](#bib.bib117)), Universal Transformers (Dehghani
    et al., [2018](#bib.bib22)), TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '| 神经机器翻译 (NMT) | IWSLT 德语-英语, IWSLT 泰语-英语, ASPEC 英语-日语, WMT 英语-德语, WMT 德语-英语,
    WMT 英语-俄语, IWSLT 英语-越南语, WMT 英语-罗马尼亚语, WMT 英语-爱沙尼亚语, Ted Talk | 组合编码 (Shu 和 Nakayama,
    [2017](#bib.bib108)), LayerDrop (Fan 等, [2019](#bib.bib28)), 剪枝注意力头 (Voita 等,
    [2019](#bib.bib123); Michel 等, [2019](#bib.bib80)), 神经元剪枝 (Murray 和 Chiang, [2015](#bib.bib86)),
    大小剪枝 (See 等, [2016](#bib.bib104)), 迭代大小剪枝 (Zhu 和 Gupta, [2017](#bib.bib146); Cheong
    和 Daniel, [2019](#bib.bib16)), 剪枝 Seq-KD + Seq-Inter (Kim 和 Rush, [2016](#bib.bib57)),
    量化蒸馏 (Polino 等, [2018](#bib.bib92)), 教师集成 KD (Freitag 等, [2017](#bib.bib30)),
    多教师 KD (Tan 等, [2019](#bib.bib114)), BTD (Ma 等, [2019](#bib.bib78)), 四元数注意力 (Tay
    等, [2019](#bib.bib117)), 通用变换器 (Dehghani 等, [2018](#bib.bib22)), TT-嵌入 (Khrulkov
    等, [2019](#bib.bib55)) |'
- en: '| Sentiment Analysis | IMDB movie review, SST, SST-2, Elec (electronic product
    reviews) | Compositional codes (Shu and Nakayama, [2017](#bib.bib108)), Star Transformer (Guo
    et al., [2019b](#bib.bib34)), TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang
    et al., [2020b](#bib.bib130)), Linformer (Wang et al., [2020a](#bib.bib129)),
    XtremeDistil (Mukherjee and Awadallah, [2020](#bib.bib84)), Gaussian Quantization (Alom
    et al., [2018](#bib.bib2)), Uniform Quantization (He et al., [2016](#bib.bib41)),
    Sparse coding (Faruqui et al., [2015](#bib.bib29)), Quaternion Attention (Tay
    et al., [2019](#bib.bib117)), Sparse Sinkhorn Attention (Tay et al., [2020](#bib.bib116)),
    RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan et al., [2019](#bib.bib61)),
    Patient KD (Sun et al., [2019](#bib.bib111)), Mixed-vocabulary KD training (Zhao
    et al., [2019](#bib.bib143)), Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)),
    MTDNN (Liu et al., [2019b](#bib.bib75)), TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | IMDB 电影评论，SST，SST-2，电子产品评论 | 组合代码（Shu and Nakayama, [2017](#bib.bib108)），Star
    Transformer（Guo et al., [2019b](#bib.bib34)），TinyBERT（Jiao et al., [2019](#bib.bib51)），MiniLM（Wang
    et al., [2020b](#bib.bib130)），Linformer（Wang et al., [2020a](#bib.bib129)），XtremeDistil（Mukherjee
    and Awadallah, [2020](#bib.bib84)），高斯量化（Alom et al., [2018](#bib.bib2)），均匀量化（He
    et al., [2016](#bib.bib41)），稀疏编码（Faruqui et al., [2015](#bib.bib29)），四元数注意力（Tay
    et al., [2019](#bib.bib117)），稀疏 Sinkhorn 注意力（Tay et al., [2020](#bib.bib116)），RPP（Guo
    et al., [2019a](#bib.bib33)），ALBERT（Lan et al., [2019](#bib.bib61)），Patient KD（Sun
    et al., [2019](#bib.bib111)），混合词汇 KD 训练（Zhao et al., [2019](#bib.bib143)），Distilled-BiLSTM（Tang
    et al., [2019](#bib.bib115)），MTDNN（Liu et al., [2019b](#bib.bib75)），TT-embedding（Khrulkov
    et al., [2019](#bib.bib55)) |'
- en: '| Question Answering | SQuAD1.1, SQuAD2.0, ELI5, SemEval, BABI | LayerDrop (Fan
    et al., [2019](#bib.bib28)), MiniLM (Wang et al., [2020b](#bib.bib130)), RPP (Guo
    et al., [2019a](#bib.bib33)), BS-Fixed Quantization (Lam, [2018](#bib.bib60)),
    ALBERT (Lan et al., [2019](#bib.bib61)), KD (Damani et al., [2020](#bib.bib21)),
    Universal Transformers (Dehghani et al., [2018](#bib.bib22)) |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
  zh: '| 问答 | SQuAD1.1，SQuAD2.0，ELI5，SemEval，BABI | LayerDrop（Fan et al., [2019](#bib.bib28)），MiniLM（Wang
    et al., [2020b](#bib.bib130)），RPP（Guo et al., [2019a](#bib.bib33)），BS-Fixed 量化（Lam,
    [2018](#bib.bib60)），ALBERT（Lan et al., [2019](#bib.bib61)），KD（Damani et al., [2020](#bib.bib21)），通用变换器（Dehghani
    et al., [2018](#bib.bib22)) |'
- en: '| Natural Language Inference | SNLI, MNLI-m, MNLI-mm, QNLI, RTE, WNLI, XNLI
    | Star Transformer (Guo et al., [2019b](#bib.bib34)), LayerDrop (Fan et al., [2019](#bib.bib28)),
    TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang et al., [2020b](#bib.bib130)),
    Linformer (Wang et al., [2020a](#bib.bib129)), Sparse Sinkhorn Attention (Tay
    et al., [2020](#bib.bib116)), RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan
    et al., [2019](#bib.bib61)), Patient KD (Sun et al., [2019](#bib.bib111)), Mixed-vocabulary
    KD training (Zhao et al., [2019](#bib.bib143)), Distilled-BiLSTM (Tang et al.,
    [2019](#bib.bib115)), MTDNN (Liu et al., [2019b](#bib.bib75)) |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 | SNLI，MNLI-m，MNLI-mm，QNLI，RTE，WNLI，XNLI | Star Transformer（Guo et
    al., [2019b](#bib.bib34)），LayerDrop（Fan et al., [2019](#bib.bib28)），TinyBERT（Jiao
    et al., [2019](#bib.bib51)），MiniLM（Wang et al., [2020b](#bib.bib130)），Linformer（Wang
    et al., [2020a](#bib.bib129)），稀疏 Sinkhorn 注意力（Tay et al., [2020](#bib.bib116)），RPP（Guo
    et al., [2019a](#bib.bib33)），ALBERT（Lan et al., [2019](#bib.bib61)），Patient KD（Sun
    et al., [2019](#bib.bib111)），混合词汇 KD 训练（Zhao et al., [2019](#bib.bib143)），Distilled-BiLSTM（Tang
    et al., [2019](#bib.bib115)），MTDNN（Liu et al., [2019b](#bib.bib75)) |'
- en: '| Paraphrasing | QQP, STS-B | TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang
    et al., [2020b](#bib.bib130)), Linformer (Wang et al., [2020a](#bib.bib129)),
    RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan et al., [2019](#bib.bib61)),
    Patient KD (Sun et al., [2019](#bib.bib111)), Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)),
    MTDNN (Liu et al., [2019b](#bib.bib75)) |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '| 释义 | QQP, STS-B | TinyBERT（Jiao et al., [2019](#bib.bib51)），MiniLM（Wang et
    al., [2020b](#bib.bib130)），Linformer（Wang et al., [2020a](#bib.bib129)），RPP（Guo
    et al., [2019a](#bib.bib33)），ALBERT（Lan et al., [2019](#bib.bib61)），Patient KD（Sun
    et al., [2019](#bib.bib111)），Distilled-BiLSTM（Tang et al., [2019](#bib.bib115)），MTDNN（Liu
    et al., [2019b](#bib.bib75)) |'
- en: '| Image captioning | MSCOCO | Grow and Prune (Dai et al., [2018](#bib.bib20)),
    Magnitude Pruning (Han et al., [2015a](#bib.bib37)), Iterative Magnitude Pruning
    and Densification (Han et al., [2016b](#bib.bib38)) |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '| 图像标题生成 | MSCOCO | 增长和修剪（Dai et al., [2018](#bib.bib20)），幅度修剪（Han et al.,
    [2015a](#bib.bib37)），迭代幅度修剪和稠密化（Han et al., [2016b](#bib.bib38)) |'
- en: '| Handwritten character recognition | ICDAR | SVD and Pruning(Yang et al.,
    [2018](#bib.bib135)) |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
  zh: '| 手写字符识别 | ICDAR | SVD 和修剪（Yang et al., [2018](#bib.bib135)) |'
- en: '| Part-of-speech (POS) tagging | Wall Street Journal of the Penn Treebank dataset,
    WikiAnn NER corpus | C2W (Ling et al., [2015](#bib.bib72)), XtremeDistil (Mukherjee
    and Awadallah, [2020](#bib.bib84)) |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注（POS） | 彭树银行语料库的《华尔街日报》，WikiAnn NER 语料库 | C2W（Ling et al., [2015](#bib.bib72)），XtremeDistil（Mukherjee
    and Awadallah, [2020](#bib.bib84)) |'
- en: '| Summarization | CNN-DailyMail, XSum | LayerDrop (Fan et al., [2019](#bib.bib28)),
    MiniLM (Wang et al., [2020b](#bib.bib130)) |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '| 摘要生成 | CNN-DailyMail, XSum | LayerDrop (Fan 等, [2019](#bib.bib28)), MiniLM (Wang
    等, [2020b](#bib.bib130)) |'
- en: '| Machine Reading Comprehension | Microsoft Research Paraphrase Corpus (MRPC),
    ReAding Comprehension from Examinations (RACE) | LayerDrop (Fan et al., [2019](#bib.bib28)),
    TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang et al., [2020b](#bib.bib130)),
    RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan et al., [2019](#bib.bib61)),
    Patient KD (Sun et al., [2019](#bib.bib111)), Mixed-vocabulary KD training (Zhao
    et al., [2019](#bib.bib143)), MTDNN (Liu et al., [2019b](#bib.bib75)) |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读理解 | Microsoft Research Paraphrase Corpus (MRPC), 试卷阅读理解 (RACE) | LayerDrop (Fan
    等, [2019](#bib.bib28)), TinyBERT (Jiao 等, [2019](#bib.bib51)), MiniLM (Wang 等,
    [2020b](#bib.bib130)), RPP (Guo 等, [2019a](#bib.bib33)), ALBERT (Lan 等, [2019](#bib.bib61)),
    Patient KD (Sun 等, [2019](#bib.bib111)), 混合词汇 KD 训练 (Zhao 等, [2019](#bib.bib143)),
    MTDNN (Liu 等, [2019b](#bib.bib75)) |'
- en: '| Linguistic Acceptability | CoLA | TinyBERT (Jiao et al., [2019](#bib.bib51)),
    MiniLM (Wang et al., [2020b](#bib.bib130)), RPP (Guo et al., [2019a](#bib.bib33)),
    ALBERT (Lan et al., [2019](#bib.bib61)), MTDNN (Liu et al., [2019b](#bib.bib75))
    |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '| 语言可接受性 | CoLA | TinyBERT (Jiao 等, [2019](#bib.bib51)), MiniLM (Wang 等, [2020b](#bib.bib130)),
    RPP (Guo 等, [2019a](#bib.bib33)), ALBERT (Lan 等, [2019](#bib.bib61)), MTDNN (Liu
    等, [2019b](#bib.bib75)) |'
- en: '| Topic Classification | DbPedia, Ag News,20 Newsgroup | XtremeDistil (Mukherjee
    and Awadallah, [2020](#bib.bib84)), Sparse coding (Faruqui et al., [2015](#bib.bib29))
    |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '| 主题分类 | DbPedia, Ag News,20 Newsgroup | XtremeDistil (Mukherjee 和 Awadallah,
    [2020](#bib.bib84)), 稀疏编码 (Faruqui 等, [2015](#bib.bib29)) |'
- en: '| Question Type Classification | TREC | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '| 问题类型分类 | TREC | 稀疏编码 (Faruqui 等, [2015](#bib.bib29)) |'
- en: '| Noun Phrase Bracketing | Lazaridou (Lazaridou et al., [2013](#bib.bib62))
    | Sparse coding (Faruqui et al., [2015](#bib.bib29)) |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '| 名词短语括注 | Lazaridou (Lazaridou 等, [2013](#bib.bib62)) | 稀疏编码 (Faruqui 等, [2015](#bib.bib29))
    |'
- en: '| Word Similarity | SimLex-999, MEN, MTurk, RARE, SCWS, WSR, WSS | Sparse coding (Faruqui
    et al., [2015](#bib.bib29)), Shared reference vectors (Suzuki and Nagata, [2016](#bib.bib113))
    |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '| 词汇相似性 | SimLex-999, MEN, MTurk, RARE, SCWS, WSR, WSS | 稀疏编码 (Faruqui 等, [2015](#bib.bib29)),
    共享参考向量 (Suzuki 和 Nagata, [2016](#bib.bib113)) |'
- en: '| Mathematical Language Understanding | Wangperawong’s MLU (Wangperawong, [2018](#bib.bib132))
    | Quaternion Attention (Tay et al., [2019](#bib.bib117)) |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
  zh: '| 数学语言理解 | Wangperawong’s MLU (Wangperawong, [2018](#bib.bib132)) | 四元数注意力 (Tay
    等, [2019](#bib.bib117)) |'
- en: '| Subject Verb Agreement | Linzen (Linzen et al., [2016](#bib.bib73)) | Quaternion
    Attention (Tay et al., [2019](#bib.bib117)), Universal Transformers (Dehghani
    et al., [2018](#bib.bib22)) |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '| 主谓一致 | Linzen (Linzen 等, [2016](#bib.bib73)) | 四元数注意力 (Tay 等, [2019](#bib.bib117)),
    通用变换器 (Dehghani 等, [2018](#bib.bib22)) |'
- en: '| Word Analogy | GSEM, GSYN, MSYN | Shared reference vectors (Suzuki and Nagata,
    [2016](#bib.bib113)) |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '| 词汇类比 | GSEM, GSYN, MSYN | 共享参考向量 (Suzuki 和 Nagata, [2016](#bib.bib113)) |'
- en: '| Sentence Completion | MSC | Shared reference vectors (Suzuki and Nagata,
    [2016](#bib.bib113)) |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '| 句子完成 | MSC | 共享参考向量 (Suzuki 和 Nagata, [2016](#bib.bib113)) |'
- en: '| Learning to execute | Zaremba and Sutskever (Zaremba and Sutskever, [2014](#bib.bib141))
    | Universal Transformers (Dehghani et al., [2018](#bib.bib22)) |'
  id: totrans-805
  prefs: []
  type: TYPE_TB
  zh: '| 学习执行 | Zaremba 和 Sutskever (Zaremba 和 Sutskever, [2014](#bib.bib141)) | 通用变换器 (Dehghani
    等, [2018](#bib.bib22)) |'
- en: '| Ad Click Through Rate Prediction | Criteo Kaggle | TT-embedding (Khrulkov
    et al., [2019](#bib.bib55)) |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
  zh: '| 广告点击率预测 | Criteo Kaggle | TT-嵌入 (Khrulkov 等, [2019](#bib.bib55)) |'
- en: '| Speech Recognition | 2100 hours English Speech, AN4, Switchboard, TIMIT,
    WSJ 92, WSJ 93, TIDIGITS, 3M Google voice utterances, Live traffic utterances
    | Iterative Magnitude Pruning (Narang et al., [2017a](#bib.bib87)), Grow and Prune (Dai
    et al., [2018](#bib.bib20)), Neuron Pruning (He et al., [2014](#bib.bib42)), Block
    Sparsity (Cao et al., [2019](#bib.bib10)), BBS (Cao et al., [2019](#bib.bib10)),
    DSD (Han et al., [2016b](#bib.bib38)), Pow2 Ternarization (Ott et al., [2016](#bib.bib90)),
    Loss Aware Quantization (Hwang and Sung, [2014](#bib.bib49)), Toeplitz-like (Lu
    et al., [2016](#bib.bib77)), Joint-SVD (Prabhavalkar et al., [2016](#bib.bib93)),
    Projections (Sak et al., [2014](#bib.bib101)), WEST (Variani et al., [2019](#bib.bib121))
    |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 | 2100小时英文语音，AN4，Switchboard，TIMIT，WSJ 92，WSJ 93，TIDIGITS，3M Google语音语句，实时交通语句
    | 迭代幅度剪枝（Narang等，[2017a](#bib.bib87)），生长和修剪（Dai等，[2018](#bib.bib20），神经元剪枝（He等，[2014](#bib.bib42），块稀疏性（Cao等，[2019](#bib.bib10），BBS（Cao等，[2019](#bib.bib10），DSD（Han等，[2016b](#bib.bib38），Pow2三值化（Ott等，[2016](#bib.bib90），损失感知量化（Hwang和Sung，[2014](#bib.bib49），类Toeplitz（陆等，[2016](#bib.bib77），Joint-SVD（Prabhavalkar等，[2016](#bib.bib93），投影（Sak等，[2014](#bib.bib101），WEST（Variani等，[2019](#bib.bib121）|'
- en: '| Named entity recognition (NER) | CoNLL2003, Wikiann-41, CoNLL2012 | QBERT (Shen
    et al., [2019](#bib.bib105)), XtremeDistill (Mukherjee and Awadallah, [2020](#bib.bib84)),
    Star Transformer (Guo et al., [2019b](#bib.bib34)) |'
  id: totrans-808
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别（NER）| CoNLL2003，Wikiann-41，CoNLL2012 | QBERT（Shen等，[2019](#bib.bib105），XtremeDistill（Mukherjee和Awadallah，[2020](#bib.bib84），星级Transformer（郭等，[2019b](#bib.bib34)）|'
- en: '| Intent Detection | SNIPS | Mixed-vocabulary KD training (Zhao et al., [2019](#bib.bib143))
    |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
  zh: '| 意图检测 | SNIPS | 混合词汇知识蒸馏训练（赵等，[2019](#bib.bib143)）|'
- en: '| Question Generation | SQuAD 1.1 | MiniLM (Wang et al., [2020b](#bib.bib130))
    |'
  id: totrans-810
  prefs: []
  type: TYPE_TB
  zh: '| 问题生成 | SQuAD 1.1 | MiniLM (王等，[2020b](#bib.bib130)) |'
- en: '| Slot Filling | SNIPS | Mixed-vocabulary KD training (Zhao et al., [2019](#bib.bib143))
    |'
  id: totrans-811
  prefs: []
  type: TYPE_TB
  zh: '| 槽填充 | SNIPS | 混合词汇知识蒸馏训练（赵等，[2019](#bib.bib143)）|'
- en: '| Text classification | 20 Newsgroup, Hyperpartisan, MTL-16 | Sparse coding (Faruqui
    et al., [2015](#bib.bib29)), Longformer (Beltagy et al., [2020](#bib.bib7)), Star
    Transformer (Guo et al., [2019b](#bib.bib34)) |'
  id: totrans-812
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | 20新闻组，偏见性报道，MTL-16 | 稀疏编码（Faruqui等，[2015](#bib.bib29)，Longformer（Beltagy等，[2020](#bib.bib7)，星级Transformer（Guo等，[2019b](#bib.bib34)）|'
- en: '| Coreference Resolution | OntoNotes | Longformer (Beltagy et al., [2020](#bib.bib7))
    |'
  id: totrans-813
  prefs: []
  type: TYPE_TB
  zh: '| 共指消解 | OntoNotes | Longformer（Beltagy等，[2020](#bib.bib7)）|'
- en: Table 9\. Applications of Model Compression Methods for Text
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 表9. 文本模型压缩方法的应用
- en: 'Combinations of multiple model compression method types has also been experimented
    with and found to be effective. Some examples of such combinations include the
    following:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 多种模型压缩方法的组合也已经进行了实验，并被证明是有效的。一些此类组合的例子包括以下内容：
- en: •
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pruning + Tensor Decomposition  (He et al., [2014](#bib.bib42); Wang et al.,
    [2019c](#bib.bib131))
  id: totrans-817
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝 + 张量分解（He等，[2014](#bib.bib42); 王等，[2019c](#bib.bib131)）
- en: •
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pruning + Quantization (Cao et al., [2019](#bib.bib10))
  id: totrans-819
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝 + 量化（Cao等，[2019](#bib.bib10)）
- en: •
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Knowledge distillation + Quantization (Bengio et al., [2013](#bib.bib8); Mishra
    and Marr, [2017](#bib.bib83); Polino et al., [2018](#bib.bib92); Sun et al., [2020](#bib.bib112))
  id: totrans-821
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识蒸馏 + 量化（Bengio等，[2013](#bib.bib8); Mishra和Marr，[2017](#bib.bib83); Polino等，[2018](#bib.bib92);
    Sun等，[2020](#bib.bib112)）
- en: •
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Knowledge distillation + Pruning (Kim and Rush, [2016](#bib.bib57))
  id: totrans-823
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识蒸馏 + 剪枝（Kim和Rush，[2016](#bib.bib57)）
- en: •
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tensor decomposition + Parameter sharing (Lu et al., [2016](#bib.bib77); Lan
    et al., [2019](#bib.bib61))
  id: totrans-825
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 张量分解 + 参数共享（陆等，[2016](#bib.bib77); 兰等，[2019](#bib.bib61)）
- en: •
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tensor decomposition + Quantization (Shu and Nakayama, [2017](#bib.bib108))
  id: totrans-827
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 张量分解 + 量化（书和中山, [2017](#bib.bib108)）
- en: Recently, Kim et al. (Kim and Awadalla, [2020](#bib.bib58)) combined knowledge
    distillation, structured pruning and quantization leading to drastic improvements
    on inference efficiency. First, they investigate the efficacy of various Knowledge
    Distillation techniques to significantly reduce the size of the models with respect
    to the depth and hidden state sizes while preserving the accuracy. Second, they
    explore Structured Pruning that further reduces the size of the models by reducing
    the number of self-attention heads and the number of intermediate hidden states
    in the feedforward layers to achieve more efficiency while trying to preserve
    the accuracy as well. Finally, they explore model quantization which enables faster
    model executions by optimally utilizing hardware acceleration capabilities. Such
    a combined method leads to heavily reduced model size, 12.4x GPU speed-up and
    6.9x-125.8x reduction in energy consumption.
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Kim 等人（Kim 和 Awadalla，[2020](#bib.bib58)）将知识蒸馏、结构化剪枝和量化结合起来，从而显著提高了推理效率。首先，他们研究了各种知识蒸馏技术的有效性，以显著减小模型的大小，同时保持准确性。其次，他们探讨了结构化剪枝，该方法通过减少自注意力头的数量和前馈层中间隐藏状态的数量来进一步减小模型的大小，以提高效率，同时尽量保持准确性。最后，他们研究了模型量化，通过优化利用硬件加速能力来实现更快的模型执行。这种结合方法大幅度减少了模型大小，实现了12.4倍的
    GPU 加速和6.9倍到125.8倍的能耗减少。
- en: 8.2\. Summary of Applications
  id: totrans-829
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 应用总结
- en: 'The model compression methods mentioned in this survey have been used across
    a wide variety of text processing tasks. In Table [9](#S8.T9 "Table 9 ‣ 8.1\.
    Comparison across model compression method types. ‣ 8\. Summary and Future Directions
    ‣ Compression of Deep Learning Models for Text: A Survey"), we list down the tasks,
    popular datasets and references where the readers can find more discussion around
    model size versus accuracy tradeoff.'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查中提到的模型压缩方法已被广泛应用于各种文本处理任务。在表[9](#S8.T9 "表 9 ‣ 8.1\. 各模型压缩方法类型的比较。 ‣ 8\.
    总结与未来方向 ‣ 深度学习模型压缩综述")中，我们列出了任务、流行数据集和参考文献，读者可以在这些地方找到关于模型大小与准确性权衡的更多讨论。
- en: 8.3\. Future Trends
  id: totrans-831
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 未来趋势
- en: Although there has been so much of work already in this field, there is a lot
    more work to be done.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这一领域已有大量研究，但仍有许多工作需要完成。
- en: •
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: With linear Transformer models, one can afford to have input with tens of thousands
    of tokens. Hence, many tasks need to be redesigned where large context can now
    be included as input to improve accuracy.
  id: totrans-834
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用线性 Transformer 模型，可以处理具有数万个 token 的输入。因此，许多任务需要重新设计，以便将大上下文作为输入以提高准确性。
- en: •
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combinations of several methods have not been tested well. Recently, Fastformers
    method (Kim and Awadalla, [2020](#bib.bib58)) showed that combining multiple methods
    like knowledge distillation, 16-bit quantization, structured pruning and numerical
    optimizations can lead to drastic improvements. However, lot of experiments are
    needed to further check how models respond to combination of model compression
    methods.
  id: totrans-836
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组合几种方法的效果尚未得到很好的测试。最近，Fastformers 方法（Kim 和 Awadalla，[2020](#bib.bib58)）表明，结合知识蒸馏、16位量化、结构化剪枝和数值优化等多种方法可以带来显著的改进。然而，还需要大量实验来进一步检验模型如何响应模型压缩方法的组合。
- en: •
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Latency results vary based on GPU architectures. With new GPU architectures
    (Nvidia RTX 3080, Nvidia T4), some methods like quantization may become more impactful.
  id: totrans-838
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 延迟结果因 GPU 架构而异。随着新 GPU 架构（Nvidia RTX 3080、Nvidia T4）的出现，某些方法如量化可能变得更具影响力。
- en: •
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real world settings are often complex: multi-modal, multi-task, multi-label,
    small-data, noisy labels, multi-teachers, mismatching teacher-student architectures.
    Efficient ways of recommending the most promising method is necessary.'
  id: totrans-840
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现实世界的环境通常很复杂：多模态、多任务、多标签、小数据、噪声标签、多教师、教师-学生架构不匹配。需要高效的方法来推荐最有前景的方法。
- en: •
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different components/structures of a model may respond to different kinds of
    compression methods with specific hyper-parameters. A generic method to choose
    the right method for various structures is needed.
  id: totrans-842
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的不同组件/结构可能对不同类型的压缩方法和特定超参数作出不同的响应。需要一种通用的方法来为各种结构选择合适的方法。
- en: •
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How does compression of models impact their interpretability? Can we design
    model compression mechanisms aimed at looking at a tradeoff between model accuracy,
    size, latency and interpretability.
  id: totrans-844
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的压缩如何影响其可解释性？我们能否设计出旨在权衡模型准确性、大小、延迟和可解释性的模型压缩机制？
- en: •
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: None of the model compression methods performs any application specific compression.
    Can we obtain further compression by exploiting some task-specific patterns?
  id: totrans-846
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前没有模型压缩方法针对具体应用进行压缩。我们能否通过利用一些任务特定的模式来获得进一步的压缩？
- en: •
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recently, deep reinforcement learning based methods have been proposed in the
    computer vision community (He et al., [2018](#bib.bib43); Yuan et al., [2019](#bib.bib140)).
    It will be nice to check the effectiveness of such methods for NLP tasks.
  id: totrans-848
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近，基于深度强化学习的方法在计算机视觉领域被提出 (He 等，[2018](#bib.bib43); Yuan 等，[2019](#bib.bib140))。检查这些方法在
    NLP 任务中的有效性将是非常好的。
- en: We hope that this survey acts as a good guide to folks across academia and industry.
    Also, we hope that a significantly large chunk of research gets done in the area
    of model compression to enable good accuracy across many NLP tasks while keeping
    model sizes and latencies in check.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这项调查能成为学术界和工业界的良好指南。同时，我们希望在模型压缩领域进行大量研究，以实现许多 NLP 任务的良好准确性，同时保持模型规模和延迟在合理范围内。
- en: References
  id: totrans-850
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Alom et al. (2018) Md Zahangir Alom, Adam T Moody, Naoya Maruyama, Brian C Van Essen,
    and Tarek M Taha. 2018. Effective quantization approaches for recurrent neural
    networks. In *IJCNN*. IEEE, 1–8.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alom 等 (2018) Md Zahangir Alom, Adam T Moody, Naoya Maruyama, Brian C Van Essen
    和 Tarek M Taha. 2018. 递归神经网络的有效量化方法。发表于 *IJCNN*。IEEE, 1–8。
- en: Anil et al. (2018) Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi,
    George E Dahl, and Geoffrey E Hinton. 2018. Large scale distributed neural network
    training through online distillation. *arXiv:1804.03235* (2018).
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等 (2018) Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi,
    George E Dahl 和 Geoffrey E Hinton. 2018. 通过在线蒸馏进行大规模分布式神经网络训练。*arXiv:1804.03235*
    (2018)。
- en: Ba and Caruana (2014) Jimmy Ba and Rich Caruana. 2014. Do deep nets really need
    to be deep?. In *NIPS*. 2654–2662.
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba 和 Caruana (2014) Jimmy Ba 和 Rich Caruana. 2014. 深度网络真的需要那么深吗？发表于 *NIPS*。2654–2662。
- en: Bai et al. (2019) Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2019. Deep
    equilibrium models. *arXiv:1909.01377* (2019).
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等 (2019) Shaojie Bai, J Zico Kolter 和 Vladlen Koltun. 2019. 深度平衡模型。*arXiv:1909.01377*
    (2019)。
- en: Bartol et al. (2015) Thomas M Bartol, Cailey Bromer, Justin Kinney, Michael A
    Chirillo, Jennifer N Bourne, Kristen M Harris, and Terrence J Sejnowski. 2015.
    Hippocampal spine head sizes are highly precise. *bioRxiv* (2015), 016329.
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartol 等 (2015) Thomas M Bartol, Cailey Bromer, Justin Kinney, Michael A Chirillo,
    Jennifer N Bourne, Kristen M Harris 和 Terrence J Sejnowski. 2015. 海马体棘突头部的尺寸非常精确。*bioRxiv*
    (2015), 016329。
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150* (2020).'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等 (2020) Iz Beltagy, Matthew E Peters 和 Arman Cohan. 2020. Longformer:
    长文档变换器。*arXiv 预印本 arXiv:2004.05150* (2020)。'
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv:1308.3432* (2013).
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 (2013) Yoshua Bengio, Nicholas Léonard 和 Aaron Courville. 2013. 通过随机神经元估计或传播梯度以进行条件计算。*arXiv:1308.3432*
    (2013)。
- en: Bianco et al. (2018) Simone Bianco, Remi Cadene, Luigi Celona, and Paolo Napoletano.
    2018. Benchmark analysis of representative deep neural network architectures.
    *IEEE Access* 6 (2018), 64270–64277.
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianco 等 (2018) Simone Bianco, Remi Cadene, Luigi Celona 和 Paolo Napoletano.
    2018. 代表性深度神经网络架构的基准分析。*IEEE Access* 6 (2018), 64270–64277。
- en: Cao et al. (2019) Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun
    Nie, Dechen Zhan, Yunxin Liu, Ming Wu, and Lintao Zhang. 2019. Efficient and effective
    sparse LSTM on FPGA with Bank-Balanced Sparsity. In *SIGDA Intl. Symp. on FPGA*.
    ACM, 63–72.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 (2019) Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun Nie,
    Dechen Zhan, Yunxin Liu, Ming Wu 和 Lintao Zhang. 2019. 基于 FPGA 的高效且有效的稀疏 LSTM
    设计。发表于 *SIGDA Intl. Symp. on FPGA*。ACM, 63–72。
- en: Carroll and Chang (1970) J Douglas Carroll and Jih-Jie Chang. 1970. Analysis
    of individual differences in multidimensional scaling via an N-way generalization
    of “Eckart-Young” decomposition. *Psychometrika* 35, 3 (1970), 283–319.
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carroll 和 Chang (1970) J Douglas Carroll 和 Jih-Jie Chang. 1970. 通过“N-way”广义的“Eckart-Young”分解分析多维标度中的个体差异。*Psychometrika*
    35, 3 (1970), 283–319。
- en: Chen et al. (2015a) Welin Chen, David Grangier, and Michael Auli. 2015a. Strategies
    for training large vocabulary neural language models. *arXiv:1512.04906* (2015).
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2015a) Welin Chen, David Grangier 和 Michael Auli. 2015a. 训练大词汇量神经语言模型的策略。*arXiv:1512.04906*
    (2015)。
- en: Chen et al. (2015b) Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger,
    and Yixin Chen. 2015b. Compressing neural networks with the hashing trick. In
    *ICML*. 2285–2294.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2015b) Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger 和
    Yixin Chen. 2015b. 利用哈希技巧压缩神经网络。发表于 *ICML*。2285–2294。
- en: Chen et al. (2016) Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016.
    Compressing neural language models by sparse word representations. *arXiv:1610.03950*
    (2016).
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2016）Yunchuan Chen，Lili Mou，Yan Xu，Ge Li 和 Zhi Jin。2016。Compressing
    neural language models by sparse word representations。*arXiv:1610.03950*（2016）。
- en: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey
    of model compression and acceleration for deep neural networks. *arXiv:1710.09282*
    (2017).
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人（2017）Yu Cheng，Duo Wang，Pan Zhou 和 Tao Zhang。2017。A survey of model
    compression and acceleration for deep neural networks。*arXiv:1710.09282*（2017）。
- en: 'Cheong and Daniel (2019) Robin Cheong and Robel Daniel. 2019. *transformers.
    zip: Compressing Transformers with Pruning and Quantization*. Technical Report.
    Technical report, Stanford University, Stanford, California, 2019.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheong 和 Daniel（2019）Robin Cheong 和 Robel Daniel。2019。*transformers.zip：Compressing
    Transformers with Pruning and Quantization*。技术报告。斯坦福大学，加州，斯坦福，2019年。
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    2019. Generating long sequences with sparse transformers. *arXiv:1904.10509* (2019).
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人（2019）Rewon Child，Scott Gray，Alec Radford 和 Ilya Sutskever。2019。Generating
    long sequences with sparse transformers。*arXiv:1904.10509*（2019）。
- en: 'Courbariaux et al. (2015) Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
    David. 2015. Binaryconnect: Training deep neural networks with binary weights
    during propagations. In *NIPS*. 3123–3131.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Courbariaux 等人（2015）Matthieu Courbariaux，Yoshua Bengio 和 Jean-Pierre David。2015。Binaryconnect:
    Training deep neural networks with binary weights during propagations。在 *NIPS*
    中。3123–3131。'
- en: Czarnecki et al. (2017) Wojciech M Czarnecki, Simon Osindero, Max Jaderberg,
    Grzegorz Swirszcz, and Razvan Pascanu. 2017. Sobolev training for neural networks.
    In *NIPS*. 4278–4287.
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czarnecki 等人（2017）Wojciech M Czarnecki，Simon Osindero，Max Jaderberg，Grzegorz
    Swirszcz，和 Razvan Pascanu。2017。Sobolev training for neural networks。在 *NIPS* 中。4278–4287。
- en: Dai et al. (2018) Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. 2018. Grow and
    prune compact, fast, and accurate LSTMs. *arXiv:1805.11797* (2018).
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2018）Xiaoliang Dai，Hongxu Yin 和 Niraj K Jha。2018。Grow and prune compact,
    fast, and accurate LSTMs。*arXiv:1805.11797*（2018）。
- en: Damani et al. (2020) Sonam Damani, Kedhar Nath Narahari, Ankush Chatterjee,
    Manish Gupta, and Puneet Agrawal. 2020. Optimized Transformer Models for FAQ Answering.
    In *PAKDD*. To appear.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Damani 等人（2020）Sonam Damani，Kedhar Nath Narahari，Ankush Chatterjee，Manish Gupta
    和 Puneet Agrawal。2020。Optimized Transformer Models for FAQ Answering。在 *PAKDD*
    中。即将出现。
- en: Dehghani et al. (2018) Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob
    Uszkoreit, and Łukasz Kaiser. 2018. Universal transformers. *arXiv:1807.03819*
    (2018).
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dehghani 等人（2018）Mostafa Dehghani，Stephan Gouws，Oriol Vinyals，Jakob Uszkoreit
    和 Łukasz Kaiser。2018。Universal transformers。*arXiv:1807.03819*（2018）。
- en: 'Deng et al. (2020) Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.
    2020. Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive
    Survey. *IEEE* 108, 4 (2020), 485–532.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人（2020）Lei Deng，Guoqi Li，Song Han，Luping Shi 和 Yuan Xie。2020。Model Compression
    and Hardware Acceleration for Neural Networks: A Comprehensive Survey。*IEEE* 108，4（2020），485–532。'
- en: Denil et al. (2013) Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato,
    and Nando De Freitas. 2013. Predicting parameters in deep learning. In *NIPS*.
    2148–2156.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denil 等人（2013）Misha Denil，Babak Shakibi，Laurent Dinh，Marc' Aurelio Ranzato 和
    Nando De Freitas。2013。Predicting parameters in deep learning。在 *NIPS* 中。2148–2156。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv:1810.04805* (2018).'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人（2018）Jacob Devlin，Ming-Wei Chang，Kenton Lee 和 Kristina Toutanova。2018。Bert:
    Pre-training of deep bidirectional transformers for language understanding。*arXiv:1810.04805*（2018）。'
- en: 'Diamos et al. (2016) Greg Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski,
    Adam Coates, Erich Elsen, Jesse Engel, Awni Hannun, and Sanjeev Satheesh. 2016.
    Persistent rnns: Stashing recurrent weights on-chip. In *ICML*. 2024–2033.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Diamos 等人（2016）Greg Diamos，Shubho Sengupta，Bryan Catanzaro，Mike Chrzanowski，Adam
    Coates，Erich Elsen，Jesse Engel，Awni Hannun 和 Sanjeev Satheesh。2016。Persistent
    rnns: Stashing recurrent weights on-chip。在 *ICML* 中。2024–2033。'
- en: Ding et al. (2017) Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017.
    Visualizing and understanding neural machine translation. In *ACL*. 1150–1159.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2017）Yanzhuo Ding，Yang Liu，Huanbo Luan 和 Maosong Sun。2017。Visualizing
    and understanding neural machine translation。在 *ACL* 中。1150–1159。
- en: Fan et al. (2019) Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing
    transformer depth on demand with structured dropout. *arXiv:1909.11556* (2019).
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2019）Angela Fan，Edouard Grave 和 Armand Joulin。2019。Reducing transformer
    depth on demand with structured dropout。*arXiv:1909.11556*（2019）。
- en: Faruqui et al. (2015) Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer,
    and Noah Smith. 2015. Sparse overcomplete word vector representations. *arXiv:1506.02004*
    (2015).
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faruqui 等人（2015）Manaal Faruqui，Yulia Tsvetkov，Dani Yogatama，Chris Dyer 和 Noah
    Smith。2015。Sparse overcomplete word vector representations。*arXiv:1506.02004*（2015）。
- en: Freitag et al. (2017) Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran.
    2017. Ensemble distillation for neural machine translation. *arXiv:1702.01802*
    (2017).
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag et al. (2017) Markus Freitag, Yaser Al-Onaizan, 和 Baskaran Sankaran.
    2017. 神经机器翻译的集成蒸馏。*arXiv:1702.01802* (2017)。
- en: Gong et al. (2014) Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. 2014.
    Compressing deep convolutional networks using vector quantization. *arXiv:1412.6115*
    (2014).
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2014) Yunchao Gong, Liu Liu, Ming Yang, 和 Lubomir Bourdev. 2014.
    使用向量量化压缩深度卷积网络。*arXiv:1412.6115* (2014)。
- en: Grachev et al. (2019) Artem M Grachev, Dmitry I Ignatov, and Andrey V Savchenko.
    2019. Compression of recurrent neural networks for efficient language modeling.
    *Applied Soft Computing* 79 (2019), 354–362.
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grachev et al. (2019) Artem M Grachev, Dmitry I Ignatov, 和 Andrey V Savchenko.
    2019. 递归神经网络的压缩用于高效语言建模。*Applied Soft Computing* 79 (2019), 354–362。
- en: Guo et al. (2019a) Fu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi
    Wang. 2019a. Reweighted Proximal Pruning for Large-Scale Language Representation.
    *arXiv:1909.12486* (2019).
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2019a) Fu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, 和 Yanzhi
    Wang. 2019a. 大规模语言表示的重加权近端修剪。*arXiv:1909.12486* (2019)。
- en: Guo et al. (2019b) Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang
    Xue, and Zheng Zhang. 2019b. Star-Transformer. In *NAACL-HLT*. 1315–1325.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2019b) Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang
    Xue, 和 Zheng Zhang. 2019b. 星形变换器。发表于 *NAACL-HLT*。1315–1325。
- en: 'Guo et al. (2017) Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen. 2017. Network
    sketching: Exploiting binary structure in deep cnns. In *CVPR*. 5955–5963.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2017) Yiwen Guo, Anbang Yao, Hao Zhao, 和 Yurong Chen. 2017. 网络素描：利用深度卷积神经网络中的二进制结构。发表于
    *CVPR*。5955–5963。
- en: 'Han et al. (2016a) Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram,
    Mark A Horowitz, and William J Dally. 2016a. EIE: efficient inference engine on
    compressed deep neural network. *ACM SIGARCH Computer Architecture News* 44, 3
    (2016), 243–254.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2016a) Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram,
    Mark A Horowitz, 和 William J Dally. 2016a. EIE：在压缩深度神经网络上进行高效推理引擎。*ACM SIGARCH
    Computer Architecture News* 44, 3 (2016), 243–254。
- en: 'Han et al. (2015a) Song Han, Huizi Mao, and William J Dally. 2015a. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv:1510.00149* (2015).'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015a) Song Han, Huizi Mao, 和 William J Dally. 2015a. 深度压缩：通过修剪、训练量化和霍夫曼编码压缩深度神经网络。*arXiv:1510.00149*
    (2015)。
- en: 'Han et al. (2016b) Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong,
    Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, et al. 2016b.
    DSD: Dense-sparse-dense training for deep neural networks. *arXiv:1607.04381*
    (2016).'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2016b) Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong,
    Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, 等. 2016b. DSD：深度神经网络的稠密-稀疏-稠密训练。*arXiv:1607.04381*
    (2016)。
- en: Han et al. (2015b) Song Han, Jeff Pool, John Tran, and William Dally. 2015b.
    Learning both weights and connections for efficient neural network. In *NIPS*.
    1135–1143.
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015b) Song Han, Jeff Pool, John Tran, 和 William Dally. 2015b. 学习权重和连接以实现高效神经网络。发表于
    *NIPS*。1135–1143。
- en: 'Hassibi and Stork (1993) Babak Hassibi and David G Stork. 1993. Second order
    derivatives for network pruning: Optimal brain surgeon. In *NIPS*. 164–171.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi 和 Stork (1993) Babak Hassibi 和 David G Stork. 1993. 网络修剪的二阶导数：最佳脑外科医生。发表于
    *NIPS*。164–171。
- en: He et al. (2016) Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu
    Zhou, and Yuheng Zou. 2016. Effective quantization methods for recurrent neural
    networks. *arXiv:1611.10176* (2016).
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu
    Zhou, 和 Yuheng Zou. 2016. 循环神经网络的有效量化方法。*arXiv:1611.10176* (2016)。
- en: He et al. (2014) Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu.
    2014. Reshaping deep neural network for fast decoding by node-pruning. In *ICASSP*.
    IEEE, 245–249.
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2014) Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, 和 Kai Yu. 2014.
    通过节点修剪重塑深度神经网络以实现快速解码。发表于 *ICASSP*。IEEE, 245–249。
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. 2018. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European Conference on Computer Vision (ECCV)*. 784–800.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, 和 Song
    Han. 2018. AMC：用于移动设备的自动模型压缩和加速。发表于 *Proceedings of the European Conference on
    Computer Vision (ECCV)*。784–800。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv:1503.02531* (2015).
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 2015. 提取神经网络中的知识。*arXiv:1503.02531*
    (2015)。
- en: Hou and Kwok (2018) Lu Hou and James T Kwok. 2018. Loss-aware weight quantization
    of deep networks. *arXiv:1802.08635* (2018).
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 和 Kwok (2018) Lu Hou 和 James T Kwok. 2018. 深度网络的损失感知权重量化。*arXiv:1802.08635*
    (2018)。
- en: Hou et al. (2016) Lu Hou, Quanming Yao, and James T Kwok. 2016. Loss-aware binarization
    of deep networks. *arXiv:1611.01600* (2016).
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. (2016) Lu Hou, Quanming Yao, 和 James T Kwok. 2016. Loss-aware binarization
    of deep networks. *arXiv:1611.01600* (2016)。
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *NIPS*. 4107–4115.
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    和 Yoshua Bengio. 2016. Binarized neural networks. 在 *NIPS*。4107–4115。
- en: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural
    networks with low precision weights and activations. *JMLR* 18, 1 (2017), 6869–6898.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, 和 Yoshua Bengio. 2017. Quantized neural networks: Training neural networks
    with low precision weights and activations. *JMLR* 18, 1 (2017), 6869–6898。'
- en: Hwang and Sung (2014) Kyuyeon Hwang and Wonyong Sung. 2014. Fixed-point feedforward
    deep neural network design using weights +1, 0, and -1\. In *SiPS*. IEEE, 1–6.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang and Sung (2014) Kyuyeon Hwang 和 Wonyong Sung. 2014. Fixed-point feedforward
    deep neural network design using weights +1, 0, and -1。 在 *SiPS*。IEEE, 1–6。
- en: 'Iandola et al. (2020) Forrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W
    Keutzer. 2020. SqueezeBERT: What can computer vision teach NLP about efficient
    neural networks? *arXiv preprint arXiv:2006.11316* (2020).'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iandola et al. (2020) Forrest N Iandola, Albert E Shaw, Ravi Krishna, 和 Kurt
    W Keutzer. 2020. SqueezeBERT: What can computer vision teach NLP about efficient
    neural networks? *arXiv preprint arXiv:2006.11316* (2020)。'
- en: 'Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural
    language understanding. *arXiv:1909.10351* (2019).'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, 和 Qun Liu. 2019. Tinybert: Distilling bert for natural language
    understanding. *arXiv:1909.10351* (2019)。'
- en: Jozefowicz et al. (2016) Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
    Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. *arXiv:1602.02410*
    (2016).
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jozefowicz et al. (2016) Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
    Shazeer, 和 Yonghui Wu. 2016. Exploring the limits of language modeling. *arXiv:1602.02410*
    (2016)。
- en: 'Kapur et al. (2017) Supriya Kapur, Asit Mishra, and Debbie Marr. 2017. Low
    precision RNNs: Quantizing RNNs without losing accuracy. *arXiv:1710.07706* (2017).'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kapur et al. (2017) Supriya Kapur, Asit Mishra, 和 Debbie Marr. 2017. Low precision
    RNNs: Quantizing RNNs without losing accuracy. *arXiv:1710.07706* (2017)。'
- en: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers
    with Linear Attention. *arXiv:2006.16236* (2020).'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    和 François Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers
    with Linear Attention. *arXiv:2006.16236* (2020)。'
- en: Khrulkov et al. (2019) Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova,
    and Ivan Oseledets. 2019. Tensorized Embedding Layers for Efficient Model Compression.
    *arXiv:1901.10787* (2019).
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khrulkov et al. (2019) Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova,
    和 Ivan Oseledets. 2019. Tensorized Embedding Layers for Efficient Model Compression.
    *arXiv:1901.10787* (2019)。
- en: Kim et al. (2016) Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush.
    2016. Character-aware neural language models. In *AAAI*.
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2016) Yoon Kim, Yacine Jernite, David Sontag, 和 Alexander M Rush.
    2016. Character-aware neural language models. 在 *AAAI*。
- en: Kim and Rush (2016) Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge
    distillation. *arXiv:1606.07947* (2016).
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim and Rush (2016) Yoon Kim 和 Alexander M Rush. 2016. Sequence-level knowledge
    distillation. *arXiv:1606.07947* (2016)。
- en: 'Kim and Awadalla (2020) Young Jin Kim and Hany Hassan Awadalla. 2020. Fastformers:
    Highly efficient transformer models for natural language understanding. *arXiv
    preprint arXiv:2010.13382* (2020).'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim and Awadalla (2020) Young Jin Kim 和 Hany Hassan Awadalla. 2020. Fastformers:
    Highly efficient transformer models for natural language understanding. *arXiv
    preprint arXiv:2010.13382* (2020)。'
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The efficient transformer. *arXiv:2001.04451* (2020).'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, 和 Anselm Levskaya. 2020.
    Reformer: The efficient transformer. *arXiv:2001.04451* (2020)。'
- en: Lam (2018) Maximilian Lam. 2018. Word2bits-quantized word vectors. *arXiv:1803.05651*
    (2018).
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lam (2018) Maximilian Lam. 2018. Word2bits-quantized word vectors. *arXiv:1803.05651*
    (2018)。
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. ALBERT: A lite BERT for self-supervised
    learning of language representations. *arXiv:1909.11942* (2019).'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, 和 Radu Soricut. 2019. ALBERT: A lite BERT for self-supervised learning
    of language representations. *arXiv:1909.11942* (2019)。'
- en: 'Lazaridou et al. (2013) Angeliki Lazaridou, Eva Maria Vecchi, and Marco Baroni.
    2013. Fish transporters and miracle homes: How compositional distributional semantics
    can help NP parsing. In *EMNLP*. 1908–1913.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lazaridou et al. (2013) Angeliki Lazaridou, Eva Maria Vecchi, 和 Marco Baroni.
    2013. Fish transporters and miracle homes: How compositional distributional semantics
    can help NP parsing. 在 *EMNLP*。1908–1913。'
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal
    brain damage. In *NIPS*. 598–605.
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1990）扬·勒昆、约翰·S·登克、萨拉·A·索拉。1990年。《最佳脑损伤》。在 *NIPS* 会议中。598–605。
- en: Lee et al. (2014) Jason D Lee, Yuekai Sun, and Michael A Saunders. 2014. Proximal
    Newton-type methods for minimizing composite functions. *J. Optimization* 24,
    3 (2014), 1420–1443.
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2014）杰森·D·李、孙跃凯、迈克尔·A·桑德斯。2014年。《用于最小化复合函数的近端牛顿型方法》。*优化杂志* 24, 3（2014），1420–1443。
- en: 'Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
    2020. Gshard: Scaling giant models with conditional computation and automatic
    sharding. *arXiv:2006.16668* (2020).'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lepikhin 等（2020）德米特里·列皮金、李赫俊、许元中、陈德豪、奥尔罕·菲拉特、黄燕平、马克西姆·克里昆、诺姆·沙泽尔、陈志峰。2020年。《Gshard：通过条件计算和自动分片扩展巨型模型》。*arXiv:2006.16668*（2020）。
- en: Li et al. (2016b) Fengfu Li, Bo Zhang, and Bin Liu. 2016b. Ternary weight networks.
    *arXiv:1605.04711* (2016).
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2016b）李丰富、张博、刘斌。2016b。《三元权重网络》。*arXiv:1605.04711*（2016）。
- en: 'Li et al. (2020) Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min
    Yang, and Yaohong Jin. 2020. BERT-EMD: Many-to-Many Layer Mapping for BERT Compression
    with Earth Mover’s Distance. *arXiv preprint arXiv:2010.06133* (2020).'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）李建全、刘晓康、赵洪洪、徐瑞峰、杨敏、金耀宏。2020年。《BERT-EMD：基于地球移动者距离的 BERT 压缩的多对多层映射》。*arXiv
    预印本 arXiv:2010.06133*（2020）。
- en: 'Li et al. (2016a) Xiang Li, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016a. LightRNN:
    Memory and computation-efficient recurrent neural networks. In *NIPS*. 4385–4393.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2016a）李翔、秦涛、杨健、刘铁岩。2016a。《LightRNN：内存和计算高效的递归神经网络》。在 *NIPS* 会议中。4385–4393。
- en: Li et al. (2018) Zhongliang Li, Raymond Kulhanek, Shaojun Wang, Yunxin Zhao,
    and Shuang Wu. 2018. Slim embedding layers for recurrent neural language models.
    In *AAAI*.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018）李忠良、雷蒙德·库尔哈内克、王少军、赵云欣、吴爽。2018年。《用于递归神经语言模型的精简嵌入层》。在 *AAAI* 会议中。
- en: Lin et al. (2015) Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua
    Bengio. 2015. Neural networks with few multiplications. *arXiv:1510.03009* (2015).
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2015）林周汉、马修·库尔巴伊、罗兰·梅梅塞维奇、约书亚·本吉奥。2015年。《少量乘法的神经网络》。*arXiv:1510.03009*（2015）。
- en: 'Linden (2018) David J Linden. 2018. *Think Tank: Forty Neuroscientists Explore
    the Biological Roots of Human Experience*. Yale University Press.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Linden（2018）大卫·J·林登。2018年。《*Think Tank: Forty Neuroscientists Explore the Biological
    Roots of Human Experience*》。耶鲁大学出版社。'
- en: 'Ling et al. (2015) Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo,
    Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Finding function
    in form: Compositional character models for open vocabulary word representation.
    *arXiv:1508.02096* (2015).'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等（2015）王玲、蒂亚戈·路易斯、路易斯·马鲁乔、拉蒙·费尔南德斯·阿斯图迪洛、西尔维奥·阿米尔、克里斯·戴尔、艾伦·W·布莱克、伊莎贝尔·特朗科索。2015年。《在形式中寻找功能：用于开放词汇表词表示的组合字符模型》。*arXiv:1508.02096*（2015）。
- en: Linzen et al. (2016) Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing
    the ability of LSTMs to learn syntax-sensitive dependencies. *TACL* 4 (2016),
    521–535.
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linzen 等（2016）塔尔·林曾、埃马纽埃尔·杜普克斯、约阿夫·戈德伯格。2016年。《评估 LSTM 学习语法敏感依赖的能力》。*TACL* 4（2016），521–535。
- en: Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019a. Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language Understanding. *arXiv:1904.09482* (2019).
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019a）刘晓东、何鹏程、陈伟柱、高剑锋。2019a。《通过知识蒸馏改进多任务深度神经网络以进行自然语言理解》。*arXiv:1904.09482*（2019）。
- en: Liu et al. (2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019b. Multi-task deep neural networks for natural language understanding. *arXiv:1901.11504*
    (2019).
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019b）刘晓东、何鹏程、陈伟柱、高剑锋。2019b。《用于自然语言理解的多任务深度神经网络》。*arXiv:1901.11504*（2019）。
- en: Lloyd (1982) Stuart Lloyd. 1982. Least squares quantization in PCM. *Tran. on
    information theory* 28, 2 (1982), 129–137.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lloyd（1982）斯图尔特·劳埃德。1982年。《PCM 中的最小二乘量化》。*信息理论杂志* 28, 2（1982），129–137。
- en: Lu et al. (2016) Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. 2016. Learning
    compact recurrent neural networks. In *ICASSP*. IEEE, 5960–5964.
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2016）陆智云、维卡斯·辛德瓦尼、塔拉·N·赛纳斯。2016年。《学习紧凑的递归神经网络》。在 *ICASSP* 会议中。IEEE，5960–5964。
- en: Ma et al. (2019) Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou,
    Dawei Song, and Ming Zhou. 2019. A Tensorized Transformer for Language Modeling.
    *arXiv:1906.09777* (2019).
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2019）马心电、张鹏、张帅、段楠、侯悦贤、宋大伟、周明。2019年。《一种用于语言建模的张量化 Transformer》。*arXiv:1906.09777*（2019）。
- en: McClure and Kriegeskorte (2016) Patrick McClure and Nikolaus Kriegeskorte. 2016.
    Representational distance learning for deep neural networks. *Frontiers in computational
    neuroscience* 10 (2016), 131.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McClure 和 Kriegeskorte (2016) Patrick McClure 和 Nikolaus Kriegeskorte。2016。《深度神经网络的表征距离学习》。*Frontiers
    in computational neuroscience* 10 (2016), 131。
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen
    Heads Really Better than One? *arXiv:1905.10650* (2019).
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel 等 (2019) Paul Michel、Omer Levy 和 Graham Neubig。2019。《十六个头真的比一个头更好吗？》*arXiv:1905.10650*
    (2019)。
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, L
    Sutskever, and G Zweig. 2013. word2vec. *URL https://code. google. com/p/word2vec*
    22 (2013).
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2013) Tomas Mikolov、Kai Chen、Greg Corrado、Jeffrey Dean、L Sutskever
    和 G Zweig。2013。《word2vec》。*URL https://code.google.com/p/word2vec* 22 (2013)。
- en: Mirzadeh et al. (2019) Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
    Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2019. Improved Knowledge Distillation
    via Teacher Assistant. *arXiv:1902.03393* (2019).
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirzadeh 等 (2019) Seyed-Iman Mirzadeh、Mehrdad Farajtabar、Ang Li、Nir Levine、Akihiro
    Matsukawa 和 Hassan Ghasemzadeh。2019。《通过教师助手改进知识蒸馏》。*arXiv:1902.03393* (2019)。
- en: 'Mishra and Marr (2017) Asit Mishra and Debbie Marr. 2017. Apprentice: Using
    knowledge distillation techniques to improve low-precision network accuracy. *arXiv:1711.05852*
    (2017).'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 和 Marr (2017) Asit Mishra 和 Debbie Marr。2017。《学徒：使用知识蒸馏技术提高低精度网络的准确性》。*arXiv:1711.05852*
    (2017)。
- en: 'Mukherjee and Awadallah (2020) Subhabrata Mukherjee and Ahmed Hassan Awadallah.
    2020. XtremeDistil: Multi-stage Distillation for Massive Multilingual Models.
    In *ACL*. 2221–2234.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukherjee 和 Awadallah (2020) Subhabrata Mukherjee 和 Ahmed Hassan Awadallah。2020。《XtremeDistil：大规模多语言模型的多阶段蒸馏》。在
    *ACL* 中。2221–2234。
- en: Muller and Indiveri (2015) Lorenz K Muller and Giacomo Indiveri. 2015. Rounding
    methods for neural networks with low resolution synaptic weights. *arXiv:1504.05767*
    (2015).
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muller 和 Indiveri (2015) Lorenz K Muller 和 Giacomo Indiveri。2015。《低分辨率突触权重的神经网络的舍入方法》。*arXiv:1504.05767*
    (2015)。
- en: 'Murray and Chiang (2015) Kenton Murray and David Chiang. 2015. Auto-sizing
    neural networks: With applications to n-gram language models. *arXiv:1508.05051*
    (2015).'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murray 和 Chiang (2015) Kenton Murray 和 David Chiang。2015。《自动调整神经网络大小：在 n-gram
    语言模型中的应用》。*arXiv:1508.05051* (2015)。
- en: Narang et al. (2017a) Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho
    Sengupta. 2017a. Exploring sparsity in recurrent neural networks. *arXiv:1704.05119*
    (2017).
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narang 等 (2017a) Sharan Narang、Erich Elsen、Gregory Diamos 和 Shubho Sengupta。2017a。《探索递归神经网络中的稀疏性》。*arXiv:1704.05119*
    (2017)。
- en: Narang et al. (2017b) Sharan Narang, Eric Undersander, and Gregory Diamos. 2017b.
    Block-sparse recurrent neural networks. *arXiv:1711.02782* (2017).
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narang 等 (2017b) Sharan Narang、Eric Undersander 和 Gregory Diamos。2017b。《块稀疏递归神经网络》。*arXiv:1711.02782*
    (2017)。
- en: Oseledets (2011) Ivan V Oseledets. 2011. Tensor-train decomposition. *SIAM J.
    on Scientific Computing* 33, 5 (2011), 2295–2317.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oseledets (2011) Ivan V Oseledets。2011。《张量训练分解》。*SIAM J. on Scientific Computing*
    33, 5 (2011), 2295–2317。
- en: Ott et al. (2016) Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua
    Bengio. 2016. Recurrent neural networks with limited numerical precision. *arXiv:1608.06902*
    (2016).
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ott 等 (2016) Joachim Ott、Zhouhan Lin、Ying Zhang、Shih-Chii Liu 和 Yoshua Bengio。2016。《具有有限数值精度的递归神经网络》。*arXiv:1608.06902*
    (2016)。
- en: 'Pan et al. (2016) Wei Pan, Hao Dong, and Yike Guo. 2016. Dropneuron: Simplifying
    the structure of deep neural networks. *arXiv:1606.07326* (2016).'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等 (2016) Wei Pan、Hao Dong 和 Yike Guo。2016。《Dropneuron：简化深度神经网络的结构》。*arXiv:1606.07326*
    (2016)。
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model compression via distillation and quantization. *arXiv:1802.05668* (2018).
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polino 等 (2018) Antonio Polino、Razvan Pascanu 和 Dan Alistarh。2018。《通过蒸馏和量化进行模型压缩》。*arXiv:1802.05668*
    (2018)。
- en: Prabhavalkar et al. (2016) Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier,
    and Lan McGraw. 2016. On the compression of recurrent neural networks with an
    application to LVCSR acoustic modeling for embedded speech recognition. In *ICASSP*.
    IEEE, 5970–5974.
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prabhavalkar 等 (2016) Rohit Prabhavalkar、Ouais Alsharif、Antoine Bruguier 和 Lan
    McGraw。2016。《关于递归神经网络的压缩及其在嵌入式语音识别中的应用》。在 *ICASSP* 中。IEEE, 5970–5974。
- en: Prasanna et al. (2020) Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.
    When BERT Plays the Lottery, All Tickets Are Winning. *arXiv:2005.00561* (2020).
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prasanna 等 (2020) Sai Prasanna、Anna Rogers 和 Anna Rumshisky。2020。《当 BERT 玩彩票时，所有票都中奖》。*arXiv:2005.00561*
    (2020)。
- en: 'Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang,
    and Jie Tang. 2020. Blockwise Self-Attention for Long Document Understanding.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: Findings*. 2555–2565.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang,
    和 Jie Tang. 2020. 块状自注意力用于长文档理解。发表于 *2020年自然语言处理会议：发现*。2555–2565。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
    *OpenAI Blog* 1, 8 (2019).
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, 和 Ilya Sutskever. 2019. 语言模型是无监督的多任务学习者。*OpenAI Blog* 1, 8 (2019)。
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *arXiv:1910.10683*
    (2019).
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2019. 探索统一文本到文本转换器的迁移学习极限。*arXiv:1910.10683*
    (2019)。
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In *ECCV*. Springer, 525–542.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    和 Ali Farhadi. 2016. Xnor-net: 使用二进制卷积神经网络进行 Imagenet 分类。发表于 *ECCV*。Springer,
    525–542。'
- en: 'Romero et al. (2014) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
    Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin
    deep nets. *arXiv:1412.6550* (2014).'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Romero et al. (2014) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
    Antoine Chassang, Carlo Gatta, 和 Yoshua Bengio. 2014. Fitnets: 细化深度网络的提示。*arXiv:1412.6550*
    (2014)。'
- en: 'Rosset (2019) C Rosset. 2019. Turing-nlg: A 17-billion-parameter language model
    by microsoft. *Microsoft Blog* (2019).'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosset (2019) C Rosset. 2019. Turing-nlg: 微软的 170 亿参数语言模型。*Microsoft Blog*
    (2019)。'
- en: Sak et al. (2014) Haşim Sak, Andrew Senior, and Françoise Beaufays. 2014. Long
    short-term memory based recurrent neural network architectures for large vocabulary
    speech recognition. *arXiv:1402.1128* (2014).
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sak et al. (2014) Haşim Sak, Andrew Senior, 和 Françoise Beaufays. 2014. 基于长短期记忆的递归神经网络架构用于大词汇量语音识别。*arXiv:1402.1128*
    (2014)。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. *arXiv:1910.01108* (2019).'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, 和 Thomas Wolf.
    2019. DistilBERT, BERT 的精简版：更小、更快、更便宜、更轻。*arXiv:1910.01108* (2019)。
- en: 'Sau and Balasubramanian (2016) Bharat Bhusan Sau and Vineeth N Balasubramanian.
    2016. Deep model compression: Distilling knowledge from noisy teachers. *arXiv:1610.09650*
    (2016).'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sau and Balasubramanian (2016) Bharat Bhusan Sau 和 Vineeth N Balasubramanian.
    2016. 深度模型压缩：从噪声教师中提取知识。*arXiv:1610.09650* (2016)。
- en: See et al. (2016) Abigail See, Minh-Thang Luong, and Christopher D Manning.
    2016. Compression of neural machine translation models via pruning. *arXiv:1606.09274*
    (2016).
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See et al. (2016) Abigail See, Minh-Thang Luong, 和 Christopher D Manning. 2016.
    通过剪枝压缩神经机器翻译模型。*arXiv:1606.09274* (2016)。
- en: 'Shen et al. (2019) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Q-bert: Hessian based
    ultra low precision quantization of bert. *arXiv:1909.05840* (2019).'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2019) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, 和 Kurt Keutzer. 2019. Q-bert: 基于 Hessian 的超低精度量化
    BERT。*arXiv:1909.05840* (2019)。'
- en: 'Shen et al. (2018) Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and
    Hongsheng Li. 2018. Efficient Attention: Attention with Linear Complexities. *arXiv:1812.01243*
    (2018).'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2018) Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, 和 Hongsheng
    Li. 2018. 高效注意力：具有线性复杂度的注意力。*arXiv:1812.01243* (2018)。
- en: 'Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion
    parameter language models using gpu model parallelism. *arXiv:1909.08053* (2019).'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, 和 Bryan Catanzaro. 2019. Megatron-lm: 使用 GPU 模型并行训练多十亿参数语言模型。*arXiv:1909.08053*
    (2019)。'
- en: Shu and Nakayama (2017) Raphael Shu and Hideki Nakayama. 2017. Compressing word
    embeddings via deep compositional code learning. *arXiv:1711.01068* (2017).
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu and Nakayama (2017) Raphael Shu 和 Hideki Nakayama. 2017. 通过深度组合编码学习压缩词嵌入。*arXiv:1711.01068*
    (2017)。
- en: Srinivas and Babu (2015) Suraj Srinivas and R Venkatesh Babu. 2015. Data-free
    parameter pruning for deep neural networks. *arXiv:1507.06149* (2015).
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivas and Babu (2015) Suraj Srinivas 和 R Venkatesh Babu. 2015. 无数据参数剪枝用于深度神经网络。*arXiv:1507.06149*
    (2015)。
- en: Strubell et al. (2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019.
    Energy and policy considerations for deep learning in NLP. *arXiv preprint arXiv:1906.02243*
    (2019).
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strubell et al. (2019) Emma Strubell, Ananya Ganesh, 和 Andrew McCallum. 2019.
    深度学习在 NLP 中的能源和政策考虑。*arXiv 预印本 arXiv:1906.02243* (2019)。
- en: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient
    knowledge distillation for bert model compression. *arXiv:1908.09355* (2019).
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, 和 Jingjing Liu. 2019. 用于 BERT
    模型压缩的患者知识蒸馏。*arXiv:1908.09355* (2019)。
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *arXiv preprint arXiv:2004.02984* (2020).'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, 和 Denny Zhou. 2020. Mobilebert: 针对资源受限设备的紧凑型任务无关 BERT。*arXiv 预印本 arXiv:2004.02984*
    (2020)。'
- en: Suzuki and Nagata (2016) Jun Suzuki and Masaaki Nagata. 2016. Learning Compact
    Neural Word Embeddings by Parameter Space Sharing.. In *IJCAI*. 2046–2052.
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzuki and Nagata (2016) Jun Suzuki 和 Masaaki Nagata. 2016. 通过参数空间共享学习紧凑的神经词嵌入。
    在 *IJCAI*。2046–2052。
- en: Tan et al. (2019) Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
    2019. Multilingual neural machine translation with knowledge distillation. *arXiv:1902.10461*
    (2019).
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2019) Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, 和 Tie-Yan Liu.
    2019. 使用知识蒸馏的多语言神经机器翻译。*arXiv:1902.10461* (2019)。
- en: Tang et al. (2019) Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova,
    and Jimmy Lin. 2019. Distilling Task-Specific Knowledge from BERT into Simple
    Neural Networks. *arXiv:1903.12136* (2019).
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2019) Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova,
    和 Jimmy Lin. 2019. 从 BERT 中提炼任务特定知识到简单的神经网络。*arXiv:1903.12136* (2019)。
- en: Tay et al. (2020) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng
    Juan. 2020. Sparse Sinkhorn Attention. *arXiv:2002.11296* (2020).
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay et al. (2020) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, 和 Da-Cheng Juan.
    2020. 稀疏 Sinkhorn 注意力。*arXiv:2002.11296* (2020)。
- en: Tay et al. (2019) Yi Tay, Aston Zhang, Luu Anh Tuan, Jinfeng Rao, Shuai Zhang,
    Shuohang Wang, Jie Fu, and Siu Cheung Hui. 2019. Lightweight and Efficient Neural
    Natural Language Processing with Quaternion Networks. *arXiv:1906.04393* (2019).
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay et al. (2019) Yi Tay, Aston Zhang, Luu Anh Tuan, Jinfeng Rao, Shuai Zhang,
    Shuohang Wang, Jie Fu, 和 Siu Cheung Hui. 2019. 使用四元数网络的轻量高效神经自然语言处理。*arXiv:1906.04393*
    (2019)。
- en: Tjandra et al. (2017) Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura.
    2017. Compressing recurrent neural network with tensor train. In *IJCNN*. IEEE,
    4451–4458.
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjandra et al. (2017) Andros Tjandra, Sakriani Sakti, 和 Satoshi Nakamura. 2017.
    使用张量列压缩递归神经网络。在 *IJCNN*。IEEE, 4451–4458。
- en: Tucker (1966) Ledyard R Tucker. 1966. Some mathematical notes on three-mode
    factor analysis. *Psychometrika* 31, 3 (1966), 279–311.
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tucker (1966) Ledyard R Tucker. 1966. 三模因子分析的数学笔记。*Psychometrika* 31, 3 (1966),
    279–311。
- en: 'Turc et al. (2019) Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    2019. Well-read students learn better: The impact of student initialization on
    knowledge distillation. *arXiv preprint arXiv:1908.08962* 13 (2019).'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turc et al. (2019) Iulia Turc, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. 学得更好的学生：学生初始化对知识蒸馏的影响。*arXiv 预印本 arXiv:1908.08962* 13 (2019)。
- en: 'Variani et al. (2019) Ehsan Variani, Ananda Theertha Suresh, and Mitchel Weintraub.
    2019. WEST: Word Encoded Sequence Transducers. In *ICASSP*. IEEE, 7340–7344.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Variani et al. (2019) Ehsan Variani, Ananda Theertha Suresh, 和 Mitchel Weintraub.
    2019. WEST: 词编码序列转换器。在 *ICASSP*。IEEE, 7340–7344。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NIPS*. 5998–6008.
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制就是一切。
    在 *NIPS*。5998–6008。
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do
    the heavy lifting, the rest can be pruned. *arXiv:1905.09418* (2019).'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    和 Ivan Titov. 2019. 分析多头自注意力：专门的头部承担了重任，其余部分可以被剪枝。*arXiv:1905.09418* (2019)。
- en: Walsh (2013) Christopher A Walsh. 2013. Peter Huttenlocher (1931–2013). *Nature*
    502, 7470 (2013), 172–172.
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walsh (2013) Christopher A Walsh. 2013. Peter Huttenlocher (1931–2013)。*Nature*
    502, 7470 (2013), 172–172。
- en: 'Wang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE:
    A Stickier Benchmark for General-Purpose Language Understanding Systems. *1905.00537*
    (2019).'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, 和 Samuel R. Bowman. 2019a. SuperGLUE:
    更具挑战性的通用语言理解系统基准。*1905.00537* (2019)。'
- en: 'Wang et al. (2019b) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A Multi-Task Benchmark and Analysis
    Platform for Natural Language Understanding. In *ICLR*.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2019b) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, 和 Samuel R. Bowman. 2019b. GLUE: 自然语言理解的多任务基准与分析平台。 在 *ICLR*。'
- en: 'Wang and Yoon (2020) Lin Wang and Kuk-Jin Yoon. 2020. Knowledge distillation
    and student-teacher learning for visual intelligence: A review and new outlooks.
    In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020*. IEEE
    CVPR.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Yoon (2020) Lin Wang 和 Kuk-Jin Yoon. 2020. 知识蒸馏和学生-教师学习在视觉智能中的应用：综述与新展望。
    在 *IEEE计算机视觉与模式识别会议, CVPR 2020*。 IEEE CVPR。
- en: 'Wang et al. (2018) Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang,
    and Yuan Xie. 2018. HitNet: hybrid ternary recurrent neural network. In *NIPS*.
    604–614.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2018) Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang,
    和 Yuan Xie. 2018. HitNet: 混合三元递归神经网络。 在 *NIPS*。 604–614。'
- en: 'Wang et al. (2020a) Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao
    Ma. 2020a. Linformer: Self-Attention with Linear Complexity. *arXiv:2006.04768*
    (2020).'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020a) Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, 和 Hao
    Ma. 2020a. Linformer: 具有线性复杂度的自注意力。 *arXiv:2006.04768* (2020)。'
- en: 'Wang et al. (2020b) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and
    Ming Zhou. 2020b. Minilm: Deep self-attention distillation for task-agnostic compression
    of pre-trained transformers. *arXiv:2002.10957* (2020).'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020b) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, 和
    Ming Zhou. 2020b. Minilm: 针对任务无关的预训练变换器的深度自注意力蒸馏。 *arXiv:2002.10957* (2020)。'
- en: Wang et al. (2019c) Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019c. Structured
    Pruning of Large Language Models. *arXiv:1910.04732* (2019).
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019c) Ziheng Wang, Jeremy Wohlwend, 和 Tao Lei. 2019c. 大型语言模型的结构化剪枝。
    *arXiv:1910.04732* (2019)。
- en: Wangperawong (2018) Artit Wangperawong. 2018. Attending to mathematical language
    with transformers. *arXiv:1812.02825* (2018).
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wangperawong (2018) Artit Wangperawong. 2018. 用变换器关注数学语言。 *arXiv:1812.02825*
    (2018)。
- en: Xu et al. (2018) Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao,
    Zhirong Wang, and Hongbin Zha. 2018. Alternating multi-bit quantization for recurrent
    neural networks. *arXiv:1802.00150* (2018).
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2018) Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao,
    Zhirong Wang, 和 Hongbin Zha. 2018. 递归神经网络的交替多位量化。 *arXiv:1802.00150* (2018)。
- en: 'Xu et al. (2020) Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
    2020. Bert-of-theseus: Compressing bert by progressive module replacing. *arXiv
    preprint arXiv:2002.02925* (2020).'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2020) Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, 和 Ming Zhou.
    2020. Bert-of-theseus: 通过逐步模块替换压缩BERT。 *arXiv preprint arXiv:2002.02925* (2020)。'
- en: Yang et al. (2018) Yafeng Yang, Kaihuan Liang, Xuefeng Xiao, Zecheng Xie, Lianwen
    Jin, Jun Sun, and Weiying Zhou. 2018. Accelerating and Compressing LSTM Based
    Model for Online Handwritten Chinese Character Recognition. In *ICFHR*. IEEE,
    110–115.
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018) Yafeng Yang, Kaihuan Liang, Xuefeng Xiao, Zecheng Xie, Lianwen
    Jin, Jun Sun, 和 Weiying Zhou. 2018. 加速和压缩基于LSTM的在线手写汉字识别模型。 在 *ICFHR*。 IEEE, 110–115。
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
    Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining
    for Language Understanding. *arXiv:1906.08237* (2019).'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
    Salakhutdinov, 和 Quoc V Le. 2019. XLNet: 一种用于语言理解的广义自回归预训练。 *arXiv:1906.08237*
    (2019)。'
- en: Ye et al. (2018) Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe,
    Xinqi Chu, and Zenglin Xu. 2018. Learning compact recurrent neural networks with
    block-term tensor decomposition. In *CVPR*. 9378–9387.
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2018) Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe,
    Xinqi Chu, 和 Zenglin Xu. 2018. 通过块-项张量分解学习紧凑的递归神经网络。 在 *CVPR*。 9378–9387。
- en: 'Yim et al. (2017) Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017.
    A gift from knowledge distillation: Fast optimization, network minimization and
    transfer learning. In *CVPR*. 4133–4141.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yim et al. (2017) Junho Yim, Donggyu Joo, Jihoon Bae, 和 Junmo Kim. 2017. 知识蒸馏的赠礼：快速优化、网络最小化和迁移学习。
    在 *CVPR*。 4133–4141。
- en: You et al. (2017) Shan You, Chang Xu, Chao Xu, and Dacheng Tao. 2017. Learning
    from multiple teacher networks. In *KDD*. 1285–1294.
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2017) Shan You, Chang Xu, Chao Xu, 和 Dacheng Tao. 2017. 从多个教师网络中学习。
    在 *KDD*。 1285–1294。
- en: Yuan et al. (2019) Xin Yuan, Liangliang Ren, Jiwen Lu, and Jie Zhou. 2019. Enhanced
    bayesian compression via deep reinforcement learning. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 6946–6955.
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2019) Xin Yuan, Liangliang Ren, Jiwen Lu, 和 Jie Zhou. 2019. 通过深度强化学习增强的贝叶斯压缩。
    在 *IEEE/CVF计算机视觉与模式识别会议论文集*。 6946–6955。
- en: Zaremba and Sutskever (2014) Wojciech Zaremba and Ilya Sutskever. 2014. Learning
    to execute. *arXiv:1410.4615* (2014).
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaremba 和 Sutskever（2014）沃伊切赫·扎伦巴和伊利亚·苏茨凯弗。2014年。学习执行。*arXiv:1410.4615*（2014年）。
- en: Zhang et al. (2018) Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan
    Lu. 2018. Deep mutual learning. In *CVPR*. 4320–4328.
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2018）英张、陶湘、蒂莫西·M·霍斯佩达勒斯和胡川陆。2018年。深度互学习。在*CVPR*。4320–4328。
- en: Zhao et al. (2019) Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou. 2019.
    Extreme Language Model Compression with Optimal Subwords and Shared Projections.
    *arXiv:1909.11687* (2019).
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2019）三强赵、拉赫夫·古普塔、杨宋和丹尼·周。2019年。通过最优子词和共享投影进行极端语言模型压缩。*arXiv:1909.11687*（2019年）。
- en: 'Zhou et al. (2017) Shu-Chang Zhou, Yu-Zhi Wang, He Wen, Qin-Yao He, and Yu-Heng
    Zou. 2017. Balanced quantization: An effective and efficient approach to quantized
    neural networks. *J. of Computer Science and Technology* 32, 4 (2017), 667–682.'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2017）舒昌周、余志王、何文、秦瑶和余恒邹。2017年。平衡量化：一种有效且高效的量化神经网络方法。*计算机科学与技术学报* 32,
    4（2017年），667–682。
- en: Zhu et al. (2016) Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. 2016.
    Trained ternary quantization. *arXiv:1612.01064* (2016).
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2016）陈卓朱、宋涵、胡子茂和威廉·J·达利。2016年。训练的三值量化。*arXiv:1612.01064*（2016年）。
- en: 'Zhu and Gupta (2017) Michael Zhu and Suyog Gupta. 2017. To prune, or not to
    prune: exploring the efficacy of pruning for model compression. *arXiv:1710.01878*
    (2017).'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Gupta（2017）迈克尔·朱和苏约格·古普塔。2017年。剪枝还是不剪枝：探索剪枝在模型压缩中的有效性。*arXiv:1710.01878*（2017年）。
