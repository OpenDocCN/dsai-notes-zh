- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:59:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2008.05221] Compression of Deep Learning Models for Text: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.05221](https://ar5iv.labs.arxiv.org/html/2008.05221)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Compression of Deep Learning Models for Text: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manish Gupta [gmanish@microsoft.com](mailto:gmanish@microsoft.com) [0002-2843-3110](https://orcid.org/0002-2843-3110
    "ORCID identifier")  and  Puneet Agrawal [punagr@microsoft.com](mailto:punagr@microsoft.com)
    Microsoft, India(2020)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, the fields of natural language processing (NLP) and information
    retrieval (IR) have made tremendous progress thanks to deep learning models like
    Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term
    Memory (LSTMs) networks, and Transformer (Vaswani et al., [2017](#bib.bib122))
    based models like Bidirectional Encoder Representations from Transformers (BERT) (Devlin
    et al., [2018](#bib.bib25)), Generative Pre-training Transformer (GPT-2) (Radford
    et al., [2019](#bib.bib96)), Multi-task Deep Neural Network (MT-DNN) (Liu et al.,
    [2019b](#bib.bib75)), Extra-Long Network (XLNet) (Yang et al., [2019](#bib.bib136)),
    Text-to-text transfer transformer (T5) (Raffel et al., [2019](#bib.bib97)), T-NLG (Rosset,
    [2019](#bib.bib100)) and GShard (Lepikhin et al., [2020](#bib.bib65)). But these
    models are humongous in size. On the other hand, real world applications demand
    small model size, low response times and low computational power wattage. In this
    survey, we discuss six different types of methods (Pruning, Quantization, Knowledge
    Distillation, Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer
    based methods) for compression of such models to enable their deployment in real
    industry NLP projects. Given the critical need of building applications with efficient
    and small models, and the large amount of recently published work in this area,
    we believe that this survey organizes the plethora of work done by the ‘deep learning
    for NLP’ community in the past few years and presents it as a coherent story.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model compression, Deep Learning, Pruning, Quantization, Knowledge Distillation,
    Parameter Sharing, Tensor Factorization, Sub-Quadratic Transformers^†^†copyright:
    acmcopyright^†^†journalyear: 2020^†^†doi: 10.1145/1122445.1122456^†^†conference:
    TKDD; Aug 2020; TKDD^†^†booktitle: TKDD^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Neural networks^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: General and reference Surveys and overviews'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models have revolutionized multiple fields of information systems
    including natural language processing (NLP), computer vision, and speech analysis.
    While they have enabled multiple tasks to attain very high accuracy values, model
    sizes and prediction latencies have increased significantly as well. Specific
    to text, Recurrent neural networks (RNNs), Gated Recurrent Units (GRUs) and long
    short term memory (LSTM) networks have been used for quite some time for various
    natural language processing (NLP) tasks. These models are large especially because
    of the input and output embedding parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past three years, the field of NLP has made significant progress as
    is evident from the GLUE (Wang et al., [2019b](#bib.bib126)) and SuperGLUE (Wang
    et al., [2019a](#bib.bib125)) leaderboards¹¹1[https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)^,²²2[https://super.gluebenchmark.com/leaderboard](https://super.gluebenchmark.com/leaderboard).
    Transformer (Vaswani et al., [2017](#bib.bib122)) based models like Bidirectional
    Encoder Representations from Transformers (BERT) (Devlin et al., [2018](#bib.bib25)),
    Generative Pre-training Transformer (GPT-2) (Radford et al., [2019](#bib.bib96)),
    Multi-task Deep Neural Network (MT-DNN) (Liu et al., [2019b](#bib.bib75)), Extra-Long
    Network (XLNet) (Yang et al., [2019](#bib.bib136)), MegatronLM (Shoeybi et al.,
    [2019](#bib.bib107)), Text-to-text transfer transformer (T5) (Raffel et al., [2019](#bib.bib97)),
    T-NLG (Rosset, [2019](#bib.bib100)) and GShard (Lepikhin et al., [2020](#bib.bib65))
    have been major contributors to this success. But these models are humongous in
    size: BERT (340M parameters), GPT-2 (1.5B parameters), MegatronLM (8.3B parameters),
    T5 (11B parameters), T-NLG (17B parameters) and GShard (600B parameters). Bianco
    et al. (Bianco et al., [2018](#bib.bib9)) and Sanh et al. (Sanh et al., [2019](#bib.bib102))
    provide a great overview of the sizes of recent deep learning models in computer
    vision and NLP respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Training these large models needs a lot of GPU infrastructure, and leads to
    a large power consumption. Training a BERT-base model on GPUs is roughly equivalent
    to a trans-American flight in terms of power and carbon footprint (Strubell et al.,
    [2019](#bib.bib110)). Deployment of such gigantic models is difficult even on
    high-end servers. Indeed a large number of real world applications run on machines
    with resource constrained environments, for example, edge devices, sensors and
    mobile phones. Edge devices could include offline medical equipment, and modules
    on drones, robots, glasses, etc. Often times, besides desiring a small model size,
    low response times are critical. For example, applications like driverless cars
    or apps to aid the blind require processing speeds of around 30 frames per second.
    Similarly, search engines need to be able to process billions of queries per day.
    Overall, the following factors motivate us to study compression of deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory (RAM) usage
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction latency
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power dissipation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference on resource constrained devices
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of training/finetuning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of deployment and update
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of distributed training
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6a2d1fdcb7453301d50fff20ce52403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Overview of Model Compression Methods for Text
  prefs: []
  type: TYPE_NORMAL
- en: Large networks do not fit in on-chip storage and hence require the more costly
    external DRAM accesses. Running a 1 billion connection neural network, for example,
    at 30 frames per second would require (30Hz)(1G)(640pJ) = 19.2W just for DRAM
    access – well beyond the power envelope of a typical mobile device. This implies
    that a mobile phone running such an app could suffer from fast draining of the
    battery, leading to overheating of the phone. Han et al. (Han et al., [2016a](#bib.bib36))
    discuss details of power dissipation for deep learning models. Another option
    to avoid large RAM, high prediction times and high power dissipation, is to run
    such massive deep learning models on cloud servers. But for many real world applications,
    it is desirable to run them on local client devices to avoid network delay, to
    guard user privacy and to avoid power dissipation in terms of input/output data
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Small models can indeed also lead to low prediction latencies. For example,
    Diamos et al. (Diamos et al., [2016](#bib.bib26)) showed that for small models,
    one can cache the RNN weights in on-chip memory such as caches, block RAM, or
    register files across multiple timesteps. This could lead to as much as 146x speedup
    if the entire RNN layer can be stored in registers rather than the GPU DRAM of
    an NVIDIA TitanX GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is easier to perform software development, deployment and updates
    with smaller models. Large models are difficult to handle. For example, it is
    impossible to fine-tune pretrained BERT-large model on a GPU with 12-16 GB RAM.
    This poses a large barrier of entry for communities without the resources to purchase
    several large Graphic Processing Units (GPUs). For large models, tuning various
    configuration parameters needs lots of resources. Smaller models lead to improved
    speed of learning and allow for more hyper-parameter configurations to be evaluated.
    Mobile-first companies dislike large apps. App stores are very sensitive to the
    size of the binary files. For example, iPhone App Store has the restriction “apps
    above 150 MB will not download until you connect to Wi-Fi”. Smaller models are
    easier to use and deploy in real world systems. Large models need multiple server
    nodes. On the other hand, multiple instances of smaller models can be run simultaneously
    on the same machine leading to higher QPS (queries per second). Lastly, smaller
    models also decrease the communication overhead of distributed training of the
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a large amount of redundancy among the weights of these
    large neural networks. A small subset of the weights are sufficient to reconstruct
    the entire network. Denil et al. (Denil et al., [2013](#bib.bib24)) showed that
    by simply using $\sim$5% of the weights, it is possible to predict the remaining
    weights without any drop in accuracy. This observation led to a large number of
    research efforts across multiple communities on compression of large deep learning
    models. In this survey, we aim to systematically explore this large body of literature
    in the NLP community by organizing it into various categories. Figure [1](#S0.F1
    "Figure 1 ‣ Compression of Deep Learning Models for Text: A Survey") shows a broad
    organization of model compression methods for text. In this survey we do not focus
    on specific methods proposed in other communities like vision and speech only
    and which make use of image/audio specific architectures and hence cannot be applied
    to text. Also, we do not discuss methods on hardware optimizations to reduce latency.
    While there are other surveys in the broad area of model compression (Cheng et al.,
    [2017](#bib.bib15); Deng et al., [2020](#bib.bib23)) also specifically on knowledge
    distillation (Wang and Yoon, [2020](#bib.bib127)), they are either old or focus
    on computer vision related problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Model Compression Methods: Overview'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we discuss compression methods using pruning, quantization,
    knowledge distillation, parameter sharing, tensor decomposition and sub-quadratic
    Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious way to reduce model size is to sparsify weight matrices. Pruning
    methods differ based on what is pruned and the actual logic used to prune. Given
    a matrix, one can prune
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some weight entries
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows or columns (i.e., neurons)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight blocks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention heads (in case of Transformer based methods)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers or a combination of the structures.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Other interesting aspects of pruning methods include the following.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to decide which weights/neurons/blocks/heads to prune?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should you prune large networks or build small networks?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should you do one-shot pruning versus iterative/gradual pruning?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does regularization help when pruning?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We discuss these aspects of pruning based methods in Section [2](#S2 "2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides removing the weights themselves, another way to reduce the size of
    weight matrices is to reduce the number of bits needed to represent each weight.
    Typically weights are stored as 32-bit double values. In an extreme case, weights
    can be quantized to two values (binary 1 bit). But other popular ways include
    quantization to three values (ternary) or multiple bits. Quantization can be uniform
    or non-uniform. Quantization methods can be deterministic or stochastic. Quantization
    can be performed in a loss-aware or unaware manner. Quantization bin ranges can
    be trained versus tuned. Finally, the level of quantization needs to be different
    across layers of RNNs, LSTMs or Transformers to attain a favorable model size
    versus accuracy tradeoff. We discuss these aspects of quantization based methods
    in Section [3](#S3 "3\. Quantization ‣ Compression of Deep Learning Models for
    Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: A third way of doing model compression is using knowledge distillation (also
    broadly known as student teacher networks). In such methods, the idea is to first
    train a deep teacher model using labeled data, and then transfer “dark knowledge”
    from teacher to train a shallow student network. Thus, the student model is trained
    to mimic a pre-trained, larger teacher. After the student is trained, it is deployed
    while the teacher network can be thrown. Distillation methods vary based on the
    following design choices.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of teacher model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of loss function like squared error between the logits of the
    models, KL divergence between the predictive distributions, or some other measure
    of agreement between the model predictions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different choices for what dataset the student model trains on (a large unlabeled
    dataset, a held-out data set, or the original training set)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different choices for what to mimic from the teacher – teacher’s class probabilities,
    teacher’s feature representation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn from whom – teacher, teacher assistant, or other fellow students.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We discuss these aspects of knowledge distillation based methods in Section [4](#S4
    "4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Another way that reduces overall weights is to have parameters shared across
    multiple weight structures. Broadly, the method is called parameter sharing. Methods
    differ depending on the following aspects.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which parameters are shared
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technique used to share parameters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The level at which sharing is performed
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We discuss these aspects of parameter sharing based methods in Section [5](#S5
    "5\. Parameter sharing ‣ Compression of Deep Learning Models for Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way to avoid large matrices is to approximate them using a combination
    of smaller matrices. Such tensor decomposition methods for model compression factorize
    large tensors into multiple smaller components. Methods differ based on the following
    aspects.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type of factorization technique
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrices being factorized
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The property of weight matrix being exploited
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We discuss these aspects of tensor decomposition methods in Section [6](#S6
    "6\. Tensor decomposition ‣ Compression of Deep Learning Models for Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Transformer based models, besides the model size, latency is a concern because
    of quadratic complexity in terms of the input sequence size. Also, RAM needed
    for Transformer models is quadratic in nature. Hence, very recently, there have
    been several efforts on designing Transformers with sub-quadratic complexity.
    Some of these methods are super-linear while many are linear. Linear complexity
    methods use various techniques for enforcing linearity – the broad idea is to
    compute a transformed representation for every token using attention over a fixed
    small number of other tokens. Methods differ in terms of defining the set of other
    tokens to be used to compute a transformed representation for the current token.
    We discuss such methods in Section [7](#S7 "7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first proposed methods for model compression were based on pruning. Pruning
    can be done in two ways: structured versus unstructured. In unstructured pruning,
    individual weight connections are removed from a network by setting them to 0\.
    One can prune away weights from a weight matrix depending on various criteria
    (e.g., prune away low magnitude weights). Such unstructured pruning methods lead
    to sparse matrices and need special sparse matrix manipulation libraries at inference
    time. Hence, various structured pruning methods have also been proposed which
    aim to prune away structures like neurons, weight matrix blocks, attention heads
    or layers. Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning
    Models for Text: A Survey") provides a broad overview of various pruning styles.
    Fig [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey")(B) depicts unstructured pruning. Fig [2](#S2.F2 "Figure 2 ‣ 2\.
    Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(C)-(G) shows
    various structured pruning methods. In this section, we discuss unstructured weight
    pruning methods in Section [2.1](#S2.SS1 "2.1\. Pruning Weights ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey") and structured pruning
    methods in Sections [2.2](#S2.SS2 "2.2\. Pruning Neurons ‣ 2\. Pruning ‣ Compression
    of Deep Learning Models for Text: A Survey")-[2.4](#S2.SS4 "2.4\. Pruning Heads
    and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4959ac34477ed48696e7936d054d55b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. Different Types of Pruning: (A) represents no pruning. Filled cells
    represent pruned entries. (B), (H) and (I) are unstructured pruning methods discussed
    in Section [2.1](#S2.SS1 "2.1\. Pruning Weights ‣ 2\. Pruning ‣ Compression of
    Deep Learning Models for Text: A Survey"). Remaining are structured pruning methods:
    (C) is discussed in Section [2.2](#S2.SS2 "2.2\. Pruning Neurons ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey"), (D) in Section [2.3](#S2.SS3
    "2.3\. Pruning Blocks ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey"), (E), (F) and (G) in Section [2.4](#S2.SS4 "2.4\. Pruning Heads
    and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: In pruning, the main idea is to grow a large model and then prune away weights
    to end up with a much smaller but effective model. This is inspired by the following
    biological observation. Trillions of synapses are generated in the human brain
    during the first few months of birth. At one year old, synapse count peaks at
    1000 trillion. And then natural pruning begins to occur. A ten year old child
    has nearly 500 trillion synapses. This ‘pruning’ mechanism removes redundant connections
    in the brain (Walsh, [2013](#bib.bib124)).
  prefs: []
  type: TYPE_NORMAL
- en: One natural question is should you prune large networks or build small dense
    networks? Pruning involves extra processing plus sparse matrices need special
    handling. Can we avoid it by training smaller models? Zhu et al. (Zhu and Gupta,
    [2017](#bib.bib146)) experimented with models of various sizes with/ without pruning
    of stacked LSTMs models for language modeling, and seq2seq models for Neural Machine
    Translation (NMT). They found that large-sparse models consistently outperform
    small-dense models and achieve up to 10x reduction in number of non-zero parameters
    with minimal loss in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Pruning Weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The weights to be pruned can be chosen using two heuristics: (1) Using Hessian
    matrix computation, or (2) using magnitude of the weights. Further, magnitude
    based pruning methods could do pruning in one shot (typically statically after
    training is done), or iteratively (also called dynamic or gradual pruning), or
    iteratively with pruning and densification. Accordingly, there are four main ways
    of performing unstructured weight pruning: (1) Hessian based methods, (2) magnitude
    pruning, (3) iterative magnitude pruning, and (4) iterative magnitude pruning
    and densification. In this subsection, we discuss these methods in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Hessian based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In their seminal work (Optimal Brain Damage or OBD) on proposing weight pruning
    as a method for model compression, LeCun et al. (LeCun et al., [1990](#bib.bib63))
    defined saliency of a weight as change in the objective function $E$ caused by
    deleting that parameter. Using Taylor series and making multiple assumptions,
    they propose to use the following as a measure of saliency of weight $u_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\displaystyle s(u_{i})=\frac{1}{2}\sum_{i}h_{ii}\delta u_{i}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $h_{ii}=\frac{\partial^{2}E}{\partial u_{i}\partial u_{j}}$ is the $i^{th}$
    element on the diagonal of the Hessian matrix. Weights with low saliency can be
    pruned and the pruned network can be retrained to adjust the remaining weights.
    The procedure for computation of the diagonal of the Hessian is very similar to
    the back-propagation algorithm used for computing the first derivatives. Hence,
    computing the diagonal of the Hessian is of the same order of complexity as computing
    the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: OBD ignores cross terms in the Hessian matrix. But on most real datasets, Hessian
    is strongly non-diagonal. Hence, to avoid pruning of important weights, Hassibi
    et al. (Hassibi and Stork, [1993](#bib.bib40)) proposed a method called Optimal
    Brain Surgeon (OBS) which considers cross terms as well. Using a similar derivative
    of $E$ wrt weight $w_{i}$, saliency of the weight is computed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle L_{i}=\frac{1}{2}\frac{w_{i}^{2}}{[H^{-1}]_{ii}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Computing $H^{-1}$ is difficult. Hence, they provide a faster recursion relation
    for calculating $H^{-1}$ from training data and structural information of the
    network. Also, unlike other methods (like OBD or magnitude pruning), OBS does
    not demand (typically slow) retraining after the pruning of a weight. In every
    step, we delete $w_{i}$ with min $L_{i}$ and update all weights as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\displaystyle\delta w=-\frac{w_{i}}{[H^{-1}]_{ii}}H^{-1}e_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $e_{i}$ is the unit vector in weight space corresponding to (scalar) weight
    $w_{i}$. Unfortunately, these methods (OBD and OBS) are computationally prohibitive
    as second derivative (i.e. Hessian) computations are expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Magnitude Pruning Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A more computationally feasible method for pruning connections and relearning
    weights based solely on the magnitude of the original weights is to simply prune
    away weights with small magnitudes. The idea was first proposed by Han et al. (Han
    et al., [2015a](#bib.bib37)). Pruning away low magnitude weights makes the matrix
    sparse. Sparse matrices can be stored in Compressed Sparse Row/Column (CSR/CSC)
    formats. Further space can be saved by storing the index difference instead of
    the absolute position, and encoding this difference using small fixed number of
    bits. See et al. (See et al., [2016](#bib.bib104)) experimented with these pruning
    methods on encoder-decoder LSTM NMT (neural machine translation) models. They
    perform magnitude pruning on all weight matrices of a 4-layer LSTM. They find
    that higher layers, attention and softmax weights are the most important, while
    lower layers and the embedding weights hold a lot of redundancy. At the lower
    layers the parameters for the input are most crucial, but at higher layers the
    parameters for the gates also become important. These methods typically have a
    target pruning percent as a hyper-parameter and pruning is either performed statically
    (after training the full model) or dynamically (while training itself). Retraining
    the sparse pruned network helps in improving accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a typical encoder-decoder LSTM model, there are these weight classes: source
    embedding weights, target embedding weights, source layer weights, target layer
    weights, attention weights and softmax weights. An important consideration related
    to magnitude pruning is how do we distribute the pruning over these different
    weight classes of a model, given a target $x$% pruning? Three ways suggested by
    See et al. (See et al., [2016](#bib.bib104)) include class-blind, class-uniform
    and class-distribution. In the class-blind way, we take all parameters, sort them
    by magnitude and prune the $x$% with smallest magnitude, regardless of the weight
    class. So some classes are pruned proportionally more than others. In the class-uniform
    way, Within each class, we sort the weights by magnitude and prune the x% with
    smallest magnitude. So all classes have exactly x% of their parameters pruned.
    In the class-distribution scheme, for each class $c$, weights with magnitude less
    than $\lambda\sigma_{c}$ are pruned. Here, $\sigma_{c}$ is the standard deviation
    of that class and $\lambda$ is a universal parameter chosen such that in total,
    $x$% of all parameters are pruned. Class-blind pruning is the simplest and adheres
    to the principle that pruning weights (or equivalently, setting them to zero)
    is least damaging when those weights are small, regardless of their locations
    in the architecture. Class-uniform pruning and class-distribution pruning both
    seek to prune proportionally within each weight class, either absolutely, or relative
    to the standard deviation of that class. They observe that class-blind pruning
    outperforms both other schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3\. Iterative Magnitude Pruning Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Typically, it has been seen that rather than pruning in one-shot, it is a good
    idea to prune gradually over epochs. This way of pruning is called iterative or
    gradual pruning (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of
    Deep Learning Models for Text: A Survey")(H)). For starting proportion $x$% and
    ending proportion $y$%, iterative magnitude pruning procedure prunes $x$% of each
    of the weights, does re-training, and then prunes $(y-x)/T$% of the weights every
    $K$ iterations. $T$ is the number of times, pruning is done. Sometimes, pruning
    is started after few warmup iterations have already been performed. Magnitude
    pruning has been seen to be very effective with regularization ($L_{1}/L_{2}$)
    while training. Dropouts also help in effective pruning. In some pruning methods,
    a weight once pruned cannot be a part of the network in future iterations. On
    the other hand, other methods do not modify the gradients of a pruned weight in
    the back-propagation step. In that case, it is possible for the updates of a pruned
    weight to be larger than the threshold of that layer, and then the weight will
    be involved in the forward pass again. Also, in every pruning iteration, we could
    either use a fixed threshold (Han et al., [2015b](#bib.bib39)) or monotonically
    increase it (Narang et al., [2017a](#bib.bib87)).'
  prefs: []
  type: TYPE_NORMAL
- en: In case of gradual pruning (Narang et al., [2017a](#bib.bib87)), where pruning
    threshold $\epsilon$ is monotonically increased, $\epsilon$ is determined as follows
    in every iteration $i$. Let $f$ be the number of iterations after which $\epsilon$
    is updated. Also, after a few warmup iterations, weights are sorted using absolute
    values and we pick the weight corresponding to the $90^{th}$ percentile as $q$.
    Pruning threshold $\epsilon$ is increased in two stages. In the first stage (which
    starts at start iteration $s$ and continues until ramp iteration $r$, we use $\theta$
    as the initial slope to prune weights. In the second stage (which starts at ramp
    iteration $r$ and continues until end iteration $e$), we use $\phi$ as the ramp
    slope to change the rate of pruning. Typically, $\phi$ is set to 1.5$\theta$ where
    $\theta$ is calculated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle\theta=\frac{2qf}{2(r-s)+3(e-r)}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, from iteration $s$ to $r$, we set the pruning threshold as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\displaystyle\epsilon=\frac{\theta(i-s+1)}{f}$ |  |'
  prefs: []
  type: TYPE_TB
- en: From iterations $r+1$ to $e$, we set the pruning threshold as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\displaystyle\epsilon=\frac{\theta(r-s+1)+\phi(i-r+1)}{f}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Typically when pruning, biases are not pruned since they are much fewer in number.
    Overall, RNN/LSTM model size can be reduced by 90% and speed-up is around 2x to
    7x using gradual pruning with no deterioration in accuracy. Also, layers closer
    to input are pruned more aggressively compared to the final layers.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of performing iterative pruning is to set a pruning target per iteration (Zhu
    and Gupta, [2017](#bib.bib146)). In this scheme, we start with an initial sparsity
    value $s_{0}$. To achieve a final sparsity value of $s_{f}$ after $n$ pruning
    steps with pruning frequency $f$, pruning target in iteration $i$ can be computed
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\displaystyle s_{i}=s_{f}+(s_{0}-s_{f})\left(1-\frac{i}{nf}\right)^{3}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, the sparsity of the network is gradually increased while allowing the
    network training steps to recover from any pruning-induced loss in accuracy. We
    prune the network rapidly in the initial phase when the redundant connections
    are abundant and gradually reduce the number of weights being pruned each time
    as there are fewer and fewer weights remaining in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) found that iterative pruning
    leads to poor results when pruning Transformer models like BERT. Guo et al. (Guo
    et al., [2019a](#bib.bib33)) found that there are two problems with pruning especially
    when done with regularization.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The larger weights $w_{j}$ are penalized more heavily than smaller weights $w_{i}$
    in $L_{1}$ regularization, which violates the original intention of weight pruning,
    “removing the unimportant connections”.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct optimization of a regularization penalty term causes divergence from
    the original loss function and has negative effect on the effectiveness of gradient-based
    update.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: They propose to perform reweighted $L_{1}$ minimization where $\alpha_{i}>0$
    are inversely proportional to magnitude of corresponding weights $|w_{i}|$. Thus,
    they solve the following optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\displaystyle\min_{w}f(w)+\gamma\sum_{i}\alpha_{i}&#124;w_{i}&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f(w)$ is the original loss function for the network. This optimization
    is solved using a reweighted proximal pruning (RPP) method (which depends on proximal
    operators). RPP decouples the goals of high sparsity from minimizing loss, and
    hence leads to improved accuracy even with high levels of pruning for BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4\. Iterative Magnitude Pruning and Densification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Further, the effectiveness of pruning can be improved by performing pruning
    and densification (Han et al., [2016b](#bib.bib38); Dai et al., [2018](#bib.bib20))
    alternately across multiple iterations (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey")(I)). There are two
    ways of doing this. In the first method (Han et al., [2016b](#bib.bib38)), in
    each iteration, either pruning is performed or densification. The sparse training
    regularizes the model, and the dense training restores the pruned weights, increasing
    the model capacity without overfitting. Sparsification helps the optimizer escape
    saddle points, and leads to regularized training which converges to a significantly
    better minima. In the second method (Dai et al., [2018](#bib.bib20)), in every
    iteration some dormant weights can reappear in the network while other active
    ones can get pruned out. A dormant $w\in W$ is activated iff $|w.grad|$ is larger
    than the $(100\alpha)^{th}$ percentile of all elements in $|W.grad|$. A $w\in
    W$ is removed iff $|w|$ is smaller than the $(100\beta)^{th}$ percentile of all
    elements in $|W|$. $\alpha$ and $\beta$ refer to growth ratio, and pruning ratio,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Pruning Neurons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is difficult to implement unstructured pruning practically since, at inference
    time, special support is needed for matrix multiplication in the sparse space.
    Pruning away neurons leads to removal of a row or a column from a weight matrix,
    thereby avoiding sparse matrix handling (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning
    ‣ Compression of Deep Learning Models for Text: A Survey")(C)). However, compared
    to pruning weights, pruning neurons is less flexible since we need to find entire
    rows/columns for deletion. In this section, we discuss ways of determining neurons
    that can be pruned.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Removing Low Importance Neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: He et al. (He et al., [2014](#bib.bib42)) proposed three node importance functions
    to determine importance score for neurons.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Entropy: For a neuron $i$, let $a_{i}$ ($d_{i}$) be the #instances with node
    output $>(or\leq)$ 0.5 for binary classification with a sigmoid activation. Then
    entropy for node $i$ can be written as follows.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (9) |  | $\displaystyle\text{Entropy}(i)=\frac{d_{i}}{a_{i}+d_{i}}\log_{2}\frac{d_{i}}{a_{i}+d_{i}}+\frac{a_{i}}{a_{i}+d_{i}}\log_{2}\frac{a_{i}}{a_{i}+d_{i}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The intuition is that if one node’s outputs are almost identical on all training
    data, these outputs do not generate variations to later layers and consequently
    the node may not be useful.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output-weights Norm (onorm): average $L_{1}$-norm of the weights of its outgoing
    links.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input-weights norm (inorm): average $L_{1}$-norm of the weights of its incoming
    links.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: All the neurons are sorted by their scores and nodes with less importance values
    are removed. In most cases, onorm has been found to be the best among these importance
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Special regularizers have also been proposed to force neurons to push either
    all incoming or outgoing connection weights towards zero (Murray and Chiang, [2015](#bib.bib86);
    Pan et al., [2016](#bib.bib91)). Specifically, for handling incoming connections,
    the following two regularizers are popular.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $L_{2}$ norm on weight matrix $W$ defined as follows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (10) |  | $\displaystyle\sum_{i}&#124;&#124;W_{i:}&#124;&#124;_{2}=\sum_{i}\left(\sum_{j}W^{2}_{ij}\right)^{1/2}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: This puts equal pressure on each row, but within each row, the larger values
    contribute more, and therefore there is more pressure on larger values towards
    zero.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $L_{\infty}$ norm on weight matrix $W$ defined as follows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (11) |  | $\displaystyle\sum_{i}&#124;&#124;W_{i:}&#124;&#124;_{\infty}=\sum_{i}\max_{j}&#124;W_{ij}&#124;$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: This puts equal pressure on each row, but within each row, only the maximum
    value (or values) matter, and therefore the pressure towards zero is entirely
    on the maximum value(s).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar regularizers can easily be defined for outgoing connections as well.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Removing Redundant Neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider a simple network with one hidden layer with $n$ neurons. Thus, the
    output can be computed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\displaystyle z=a_{1}h(W_{1}^{T}X)+a_{2}h(W_{2}^{T}X)+...+a_{n}h(W_{n}^{T}X)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $a_{i}$ and $W_{i}$ indicate weights. In case $W_{1}==W_{2}$, $h(w_{1}^{T}X)=h(w_{2}^{T}X)$.
    Thus, we can compute output as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\displaystyle z=(a_{1}+a_{2})h(W_{1}^{T}X)+0.h(W_{2}^{T}X)+...+a_{n}h(W_{n}^{T}X)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In general, whenever two weight sets ($W_{1}$, $W_{2}$) are equal, one of them
    can effectively be removed. This should be done with a surgery step, i.e., we
    need to alter the co-efficient $a_{1}$ to $a_{1}+a_{2}$. Of course, for many pairs
    of weight sets (i.e., neurons), $W_{1}$ and $W_{2}$ are not exactly the same.
    Hence, Srinivas et al. (Srinivas and Babu, [2015](#bib.bib109)) proposed this
    3 step method for redundant neuron identification and removal.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute saliency $s_{ij}$ for all possible neuron pairs (i, j) as follows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (14) |  | $\displaystyle s_{ij}=\langle a_{j}^{2}\rangle&#124;&#124;\epsilon_{ij}&#124;&#124;_{2}^{2}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\langle a_{j}^{2}\rangle$ denotes the average of the quantity over all
    output neurons. Let $S$ be the matrix with all $s_{ij}$ values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pick the indices $(i^{\prime},j^{\prime})$ corresponding to the minimum $s_{ij}$.
    Delete the $j^{\prime}$ neuron, and update $a_{i}^{\prime}$ as follows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (15) |  | $\displaystyle a_{i}^{\prime}\leftarrow a_{i}^{\prime}+a_{j}^{\prime}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update $S$ by removing the $j^{\prime th}$ column and row, and updating the
    $i^{\prime th}$ column (to account for the updated $a_{i}^{\prime}$).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3\. Pruning Blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In weight pruning, irregularity of sparse matrices limits the maximum performance
    and energy efficiency achievable on hardware accelerators. Pruning neurons avoids
    sparse matrix issues but is limited in term of overall pruning possible. Hence,
    block based pruning methods were introduced (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\.
    Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(D)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Block-sparse formats store blocks contiguously in memory reducing irregular
    memory accesses. If the maximum magnitude weight of a block is below the current
    threshold, we set all the weights in that block to zeros. Similar to iterative
    weight pruning, block pruning (Narang et al., [2017b](#bib.bib88)) can also be
    done iteratively using a monotonically growing threshold. Any blocks that had
    been zeroed out are held at zero even after pruning has ended resulting in a sparse
    model at the end of training. Just like weight pruning (as discussed in Section [2.1](#S2.SS1
    "2.1\. Pruning Weights ‣ 2\. Pruning ‣ Compression of Deep Learning Models for
    Text: A Survey")), the start slope $\theta$ and ramp slope $\phi$ determine the
    rate at which the threshold increases. For block pruning, we need to modify the
    start slope to account for the number of elements in a block ($N_{b}$). Thus,
    the start slope for block pruning is typically set as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\displaystyle\theta_{b}=\theta\times\sqrt[4]{N_{b}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Further, to enable effective removal of blocks, Narang et al. (Narang et al.,
    [2017b](#bib.bib88)) propose group Lasso regularization method. Group lasso is
    a type of weight regularization that works on groups of weights and can zero out
    all the weights in a group. For each block, we add a loss term proportional to
    the $L_{2}$ norm of the block. Thus, we optimize for the following.
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\displaystyle\min_{w}f(w)+\lambda_{g}\sum_{g=1}^{G}&#124;&#124;w^{(g)}&#124;&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: When we combine group lasso with block pruning, group lasso guides the selection
    of blocks to prune. Group lasso regularization is applied to coincide with the
    pruning schedule, i.e., we turn off regularization when the pruning schedule ends.
    Typically, inducing block sparsity with 4x4 blocks in vanilla RNNs and GRUs works
    well, compared to larger block sizes. Larger blocks require lower sparsity to
    maintain similar accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it becomes challenging to maintain the same model accuracy when
    block sparsity is applied. Also, block sizes (i.e., pruning granularity) are application-sensitive,
    making it another hyper-parameter to tune. To avoid these problems, Cao et al. (Cao
    et al., [2019](#bib.bib10)) proposed a new method called Bank-Balanced Sparsity
    (BBS). BBS splits each weight matrix row into multiple equal-sized banks, and
    adopts fine-grained pruning to each bank independently to obtain identical sparsity
    among banks. Each bank has the same number of non-zero values. For example, retaining
    top two weights in each bank of size 4 implies a sparsity of 50%. We apply the
    BBS pruning method iteratively to a pre-trained network, and fine-tune the network
    after each pruning iteration to restore the model accuracy. BBS achieves almost
    the same model accuracy as unstructured sparsity and significantly outperforms
    block sparsity when pruning weights at the same sparsity level. BBS is also amenable
    to FPGA (Field Programmable Gate Arrays) acceleration because it inherently provides
    a balanced matrix partitioning for parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Pruning Heads and Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides neurons and blocks, for Transformer based models, structured pruning
    can also be applied to attention heads and entire layers. In this section, we
    discuss such methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1\. Pruning Attention Heads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'BERT-base model consists of 12 layers each with 12 attention heads. Similarly,
    a typical NMT encoder-decoder Transformer with 6 layers each for encoder as well
    as decoder contains 16 attention heads per layer. Michel et al. (Michel et al.,
    [2019](#bib.bib80)) found that majority of attention heads can be removed without
    deviating too much from the original score. Surprisingly, in some cases removing
    an attention head (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of
    Deep Learning Models for Text: A Survey")(E)) results in an increase in accuracy.
    When these heads are removed individually, only 8 (out of 96) heads in 6-layer
    WMT NMT Transformer (16 heads/layer) cause a statistically significant change
    in performance when they are removed from the model, half of which actually result
    in a higher BLEU score. For most layers, one head is indeed sufficient at test
    time, even though the network was trained with 12 (BERT) or 16 (WMT Transformer)
    attention heads. One can also do iterative pruning of multiple heads (rather than
    just one at a time) across layers. For iterative pruning, head importance score
    is defined using the expected sensitivity of the model to the mask variables $\xi_{h}$
    as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $I_{h}=E_{x\sim X}\left&#124;\frac{\partial L(x)}{\partial\xi_{h}}\right&#124;=E_{x\sim
    X}\left&#124;\text{Att}_{h}(x)^{T}\frac{\partial L(x)}{\partial\text{Att}_{h}(x)}\right&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $X$ is the data distribution, $L(x)$ is the loss on sample $x$, and $Att_{h}(x)$
    is the output of the attention head $h$ for instance $x$, . Intuitively, if $I_{h}$
    has a high value then changing $\xi_{h}$ is liable to have a large effect on the
    model. Hence, in every iteration heads with low $I_{h}$ values are pruned out.
    Michel et al. (Michel et al., [2019](#bib.bib80)) observed that pruning up to
    20% and 40% of heads from NMT and BERT models respectively, did not lead to any
    noticeable negative impact on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Voita et al. (Voita et al., [2019](#bib.bib123)) used two other head importance
    scores to prune attention heads from the NMT model. The two scoring methods were:
    (1) Layer-wise relevance propagation (LRP) (Ding et al., [2017](#bib.bib27)).
    LRP is a method for computing the relative contribution of neurons at one point
    in a network to neurons at another. (2) “confidence” of a head which is computed
    as the average of its maximum attention weight excluding the end of sentence symbol,
    where the average is taken over tokens in a set of sentences used for evaluation.
    For pruning the heads, they propose a method based on stochastic gates and a differentiable
    relaxation of the $L_{0}$ penalty. $L_{0}$ norm equals the number of non-zero
    components and pushes the model to switch off less important heads. They find
    that only a small subset of heads are important for translation. On the English-Russian
    WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15
    BLEU.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2\. Pruning Layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Note that dropping attention heads does not reduce runtime as they are usually
    computed in parallel. While one can prune weights, neurons or attention heads,
    how can we design a scheme to prune away layers (see Fig. [2](#S2.F2 "Figure 2
    ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text: A Survey")(G))?
    The LayerDrop idea proposed in (Fan et al., [2019](#bib.bib28)) is inspired by
    DropConnect. DropConnect randomly drops weights while training on a batch. LayerDrop
    does structured dropout: it drops groups of weights, heads, feed forward network
    (FFN) matrices, or layers. The layers to be pruned can be decided using one of
    these ways:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Every Other: Prune every other layer (with rate $p$), e.g., every $3^{rd}$
    layer in a 12-layer BERT model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Search on Validation: Search for a set of layers to be pruned by checking their
    impact on a validation set. This entails trying various combinations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Driven Pruning: Learn the drop rate $p_{d}$ of each layer in a data driven
    manner.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given a target drop rate $p$, we learn an individual drop rate $p_{d}$ for the
    layer at depth $d$ such that the average rate over layers is equal to $p$. At
    inference time, we forward only the fixed top-$k$ highest scoring layers based
    on the softmax output. Across the three methods, “Every Other” strategy works
    surprisingly well across many tasks and configurations. “Search on Validation”
    and “Data Driven Pruning” only offer marginal gains.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3\. Pruning General Structures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lastly, Prasanna et al. (Prasanna et al., [2020](#bib.bib94)) experiment with
    pruning both the FFN layers (see Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression
    of Deep Learning Models for Text: A Survey")(F)) as well as attention heads (see
    Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Pruning ‣ Compression of Deep Learning Models
    for Text: A Survey")(E)) in a BERT network. Just like (Michel et al., [2019](#bib.bib80)),
    they assign a mask variable to each of these structures. To decide which structures
    to prune, we look at the expected sensitivity of the model to the mask variables.
    High sensitivity implies large impact on the model output and hence corresponding
    structures should be retained. They find that it is possible to find a subnetwork
    of elements that achieves performance comparable with that of the full model,
    and similarly-sized subnetworks sampled from the rest of the model perform worse.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Dataset | Model | Method | Size (Pruned; Orig) | Eval. (Pruned; Orig)
    | Metric |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Europarl v7 English | 2-layer MLP | Prune Neurons (Murray
    and Chiang, [2015](#bib.bib86)) | 5.06M; 5.1M | 57; 100 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | 2-layer LSTM | Iter. Mag. (Zhu and Gupta, [2017](#bib.bib146))
    | 6.6M; 66M | 80.24; 78.45 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Iter. Mag. (Cao et al., [2019](#bib.bib10))
    | 20M; 66M | 78.5; 78.8 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Block Sparsity (Cao et al., [2019](#bib.bib10))
    | 20M; 66M | 83; 78.8 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | BBS (Cao et al., [2019](#bib.bib10)) | 20M;
    66M | 78.5; 78.8 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikitext-103 | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 19.5; 18.2 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | AFP from English Gigaword | 2-layer MLP | Prune Neurons (Murray
    and Chiang, [2015](#bib.bib86)) | 5.07M; 5.1M | 107; 100 | Perplexity (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Linguistic acceptability | CoLA | BERT-large | RPP/Iter. Mag. (Guo et al.,
    [2019a](#bib.bib33)) | 138M/170M; 340M | 82.8/76.3; 81.5 | Matthews (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Machine reading comp. | MRPC | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 88.1/83.5; 89.3 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Machine reading comp. | MRPC | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 85.3; 88.9 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | Multi-layer LSTM | Mag. (See et al., [2016](#bib.bib104))
    | 43M; 216M | 20.91; 20.5 | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT16 | 4-layer LSTM | Iter. Mag. (Zhu and Gupta,
    [2017](#bib.bib146)) | 23M; 211M | 26.19; 26.77 | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT16 | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 29; 29 | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | Iter. Mag. (Cheong and Daniel,
    [2019](#bib.bib16)) | 22M; 44M | 26.4; 28.09 | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Paraphrasing | QQP | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 91.2/85.1; 91.2 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD 1.1 | BERT-large | RPP/Iter. Mag. (Guo et al.,
    [2019a](#bib.bib33)) | 138M/170M; 340M | 90.23/85.3; 90.9 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD 2.0 | BERT-large | RPP/Iter. Mag. (Guo et al.,
    [2019a](#bib.bib33)) | 138M/170M; 340M | 81.3/75.3; 81.9 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST-2 | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 92.4/91.3; 93.2 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST-2 | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 92.5; 93.5 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2100 hours English Speech | 2 CONV+7 BiRNNs+CTC | Iter.
    Mag. (Narang et al., [2017a](#bib.bib87)) | 11.1M; 67M | 10.59; 10.67 | CER (L)
    on dev |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2100 hours English Speech | 2 CONV+7 BiGRUs+CTC | Iter.
    Mag. (Narang et al., [2017a](#bib.bib87)) | 17.8M; 115M | 9.76; 9.55 | CER (L)
    on dev |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+7 BiRNNs+CTC | Block
    Sparsity (Narang et al., [2017b](#bib.bib88)) | 25.8M; 67M | 15.66; 15.36 | CER
    (L) on test |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+7 BiRNNs+CTC | Block
    Sparsity+Group Lasso (Narang et al., [2017b](#bib.bib88)) | 12.9M; 67M | 15.89;
    15.36 | CER (L) on test |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+3 BiGRUs+CTC | Block
    Sparsity (Narang et al., [2017b](#bib.bib88)) | 25.6M; 115M | 16.23; 15.42 | CER
    (L) on test |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2100 hours English speech | 2 CONV+3 BiGRUs+CTC | Block
    Sparsity+Group Lasso (Narang et al., [2017b](#bib.bib88)) | 10.8M; 115M | 16.78;
    15.42 | CER (L) on test |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | AN4 | 2 CONV+3 HLSTMs+CTC | Grow and Prune (Dai et al.,
    [2018](#bib.bib20)) | 2.6M; 44.72M | 10.37; 8.92 | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | Switchboard (swb/fsh) | 7-layer MLP | Prune Neurons (He
    et al., [2014](#bib.bib42)) | 12.2M; 32.19M | 25.5/28.8; 25.7/28.8 | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | TIMIT | 5-layer MLP | Prune Neurons (He et al., [2014](#bib.bib42))
    | 3.5M; 5.76M | 20.7; 20.79 | PER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | TIMIT | LSTM | Iter. Mag. (Cao et al., [2019](#bib.bib10))
    | 0.32M; 3.2M | 23.5; 23.5 | PER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | TIMIT | LSTM | Block Sparsity (Cao et al., [2019](#bib.bib10))
    | 0.32M; 3.2M | 26.5; 23.5 | PER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | TIMIT | LSTM | BBS (Cao et al., [2019](#bib.bib10))
    | 0.32M; 3.2M | 23.5; 23.5 | PER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | WSJ 92 | 1 CONV+3 FC+1 BiRNN | DSD (Han et al., [2016b](#bib.bib38))
    | 4.07M; 8.14M | 27.9; 27.45 | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | WSJ 92 | 2 CONV+7 BiRNNs+CTC | DSD (Han et al., [2016b](#bib.bib38))
    | 33.5M; 67M | 10.65; 9.02 | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | WSJ 93 | 1 CONV+3 FC+1 BiRNN | DSD (Han et al., [2016b](#bib.bib38))
    | 4.07M; 8.14M | 32.99; 31.6 | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | WSJ 93 | 2 CONV+7 BiRNNs+CTC | DSD (Han et al., [2016b](#bib.bib38))
    | 33.5M; 67M | 14.84; 13.44 | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Summarization | CNN-Dailymail | Transformer | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 22M; 44M | 39; 40 | ROUGE (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 86.1/77; 86.1 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI-m | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 85.7/82.5; 85.9 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI-m | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 82.9; 84.6 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | QNLI | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 92.3/90.2; 91.3 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | QNLI | BERT-base | LayerDrop (Fan et al., [2019](#bib.bib28))
    | 66M; 110M | 89.4; 90.5 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | RTE | BERT-large | RPP/Iter. Mag. (Guo et al., [2019a](#bib.bib33))
    | 138M/170M; 340M | 70.1/68.6; 70.1 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Comparison of various pruning methods (sorted by Task and then Dataset).
    CONV=Convolution. CTC=Connectionist temporal classification. FC=Fully connected.
    HLSTM=hidden-layer LSTM (Dai et al., [2018](#bib.bib20)). In the metric column,
    H means high is better while L means low is better. PER/CER/WER=Phone/Character/Word
    error rate. For (Murray and Chiang, [2015](#bib.bib86)), embedding weights have
    not been considered when computing model size in the table. Block sparsity methods
    use block size of 4x4\. BBS uses 64 banks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2.4.3\. Pruning General Structures ‣ 2.4\. Pruning
    Heads and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text:
    A Survey") compares various pruning methods across different tasks and datasets.
    Size and accuracy of both the original and the pruned model are shown. The papers
    report multiple (model size, model accuracy) pairs but we carefully chose the
    pair such that accuracy is typically within 10% of the original or the best pruned
    model accuracy reported. For language modeling, most popular datasets include
    PTB, Europarl v7 English, Wikitext-103 and AFP from English Gigaword. On PTB using
    LSTMs, we observe that Bank-Balanced Sparsity method (Cao et al., [2019](#bib.bib10))
    leads to lowest perplexity. For the Linguistic Acceptability CoLA task, RPP (Guo
    et al., [2019a](#bib.bib33)) resulted into a smaller and a more accurate model
    compared to iterative magnitude pruning. As expected in some senses, the pruned
    model provides better accuracy than the unpruned one since pruning causes regularization.
    For the machine reading comprehension, question answering and paraphrasing tasks
    also, RPP seems to work better than iterative magnitude pruning. For the machine
    translation (NMT) task, on English-German WMT datasets, pruned Transformer models
    provide better accuracy than pruned LSTM with comparable number of parameters.
    For the sentiment analysis task, on SST-2, although RPP leads to a better pruned
    model compared to iterative pruning, LayerDrop (Fan et al., [2019](#bib.bib28))
    improves further on it with a model less than half of the RPP-pruned model. For
    the speech recognition task, experiments have been reported on 2100 hours English
    speech data, TIMIT, WSJ, Switchboard and AN4 datasets. On 2100 hours English speech
    data, Block Sparsity+Group Lasso is better than Block sparsity without regularization.
    Also, it is better than plain iterative magnitude pruning. On TIMIT, block sparsity (Narang
    et al., [2017b](#bib.bib88)) leads to a more accurate 90% pruned LSTM model compared
    to the unpruned one. For the summarization task, LayerDrop (Fan et al., [2019](#bib.bib28))
    can compress the model to half without any noticeable accuracy change. Finally,
    for the textual entailment task, experiments have been done on GLUE (Wang et al.,
    [2019b](#bib.bib126)) datasets: MNLI, MNLI-m, QNLI and RTE. Models pruned from
    BERT-large perform better than models pruned from BERT-base; RPP performs better
    than iterative magnitude pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While older methods (LeCun et al., [1990](#bib.bib63); Hassibi and Stork, [1993](#bib.bib40))
    claimed that Hessian based methods were more effective than magnitude based pruning,
    almost all recent methods have been based on magnitude based pruning. See et al. (See
    et al., [2016](#bib.bib104)) proposed three pruning schemes. They make the following
    observations: (1) Class-blind pruning outperforms both other schemes. Further,
    the overall performance loss is caused disproportionately by a few classes: softmax
    weights, source and target embedding weights. (2) It seems that higher layers
    are more important than lower layers, and that attention and softmax weights are
    crucial in LSTMs. (3) After retraining the pruned NMT models, baseline performance
    (20.48 BLEU) is both recovered and improved upon, up to 80% pruning (20.91 BLEU),
    with only a small performance loss at 90% pruning (20.13 BLEU). (4) In LSTMs,
    the parameters corresponding to the less common words are more dispensable. Weights
    connecting to the input are most crucial, followed by the input gate, then the
    output gate, then the forget gate. This is particularly true of the lower layers,
    which focus primarily on the input. However for higher layers, especially on the
    target side, weights connecting to the gates are as important as those connecting
    to the input.'
  prefs: []
  type: TYPE_NORMAL
- en: Narang et al. (Narang et al., [2017a](#bib.bib87)) observe that for approximately
    same number of parameters, gradual/iterative pruning is 7% to 9% better than hard
    pruning. They also conclude that the initial layers are pruned more aggressively
    compared to the final layers. Zhu et al. (Zhu and Gupta, [2017](#bib.bib146))
    advise that in order to get the best-performing sparse model of a certain size,
    we should train a dense model that is 5x-10x larger and then prune to the desired
    number of parameters rather than taking the largest and best-performing dense
    model and pruning this model by 20x or more to the desired number of parameters.
    Guo et al. (Guo et al., [2019a](#bib.bib33)) find that RPP is much better than
    typical iterative pruning. In their experiments with BERT they find that for both
    original BERT and BERT pruned with RPP, the low-dimensional manifolds of the language
    representations are similar, showing the similar projection. This implies that
    the BERT applied with RPP keeps most of the language representation information
    similar to that from the original BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'For block pruning, Narang et al. (Narang et al., [2017b](#bib.bib88)) make
    the following observations: (1) We can create block-sparse RNNs with sparsity
    ranging from 80% to 90% with small loss in accuracy. This allows us to reduce
    the model size by roughly 10×. Block sparsity works with a variety of block sizes
    up to 32×32\. (2) For block size 4×4, models with sparsity greater 90% yield a
    relative accuracy loss of 30% or higher. Similarly, for blocks of 16×16, models
    with sparsity greater than 86% have 30% or more accuracy loss. A similar trend
    is observed for block size 32×32\. This indicates that there is a tradeoff between
    sparsity, block size and accuracy of the model. (3) For both block pruning and
    weight pruning, we see that the initial layers are pruned more aggressively compared
    to the final layers. Increasing sparsity in the layers closer to the output results
    in poor accuracy. Additionally, the variance in sparsity across the layers increases
    with the block size. Further, Cao et al. (Cao et al., [2019](#bib.bib10)) make
    the following observations comparing block sparsity with BBS: (1) BBS achieves
    almost the same model accuracy regardless of the change of bank size. For block
    sparsity, however, increasing the block size adversely affects model accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: For pruning of attention heads, Michel et al. (Michel et al., [2019](#bib.bib80))
    observe that one can prune up to 20% and 40% of heads from 6-layer NMT Transformer
    and BERT resp., without incurring any noticeable negative impact. When trying
    to remove a head at a time, only 8 (out of 96) heads in 6-layer NMT Transformer
    (16 heads/layer) cause a statistically significant change in performance when
    they are removed from the model, half of which actually result in a higher BLEU
    score. Further Voita et al. (Voita et al., [2019](#bib.bib123)) find that on the
    English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop
    of only 0.15 BLEU.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, to summarize, pruning has been the most popular method for model compression.
    Pruning methods can be unstructured (prune weights) or structured (prune neurons,
    blocks, attention heads, layers). While weight pruning theoretically leads to
    pruning to a large extent, practical implementation of sparse data structures
    is difficult. Pruning and regularization need to be done together carefully. Also,
    it is critical to define the importance functions for various structures carefully.
    Among weight pruning methods, while iterative magnitude pruning with regularization
    works well for RNNs and LSTMs, RPP performs better for Transformer based models.
    Pruning blocks using BBS is better than pruning neurons. For Transformer models,
    pruning just the heads do not provide much model compression, but dropping a combination
    of attention heads and layers is better.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While pruning saves on the model size by removing weights, quantization aims
    to reduce the number of bits needed to store weights. Most computer architectures
    use 32 bits to represent weights. However, estimated precision of the brain (hippocampal
    spine) synapses is around 4.6 bits (Bartol et al., [2015](#bib.bib6)). Empirical
    evidence suggests that most quantities in the nervous system (for instance, firing
    of the neurons) have variability of a few percent due to biological noise, or
    a precision of 1 in 100 at best (Linden, [2018](#bib.bib71)). Thus, each decision
    could depend on $\log_{2}(100)$=6.64 bits. Thus, we should be able to store weights
    in our artificial neural networks on average in a space of 4–7 bits. Given this
    motivation, various methods have been proposed which perform 1-bit (or binary
    quantization), ternary quantization, and general quantization exploring the spectrum
    between 3 and 32 bits. We discuss such methods in this section. Fig. [3](#S3.F3
    "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models for Text: A
    Survey") provides a broad overview of various quantization styles.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bad909a793e8a1f59531e71a8b3bb59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. Different Types of Quantization: Binary (A), Ternary (B) and General
    Quantized (C and D). Note that X axis denotes the weight value while the Y axis
    denotes frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Binarized Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Binarized networks use binary quantization (see Fig. [3](#S3.F3 "Figure 3 ‣
    3\. Quantization ‣ Compression of Deep Learning Models for Text: A Survey")(A)),
    which quantizes weights using 1 bit. Quantizing weights to 1 bit provides a compression
    of 32x but leads to a significant drop in accuracy across many tasks. However,
    in a hybrid quantization scheme, such binary quantization can be very helpful
    for some layers in a network. Binarization can be done using deterministic methods
    or could be stochastic in nature. Also, while naïve binarization has a very simple
    way of fixing the binary boundary threshold, one could perform a complex loss
    aware binarization as well. We discuss these variants of binarization in this
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Deterministic Binarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Simplest way of binary quantization is to set the weight as 1 for non-negative
    weights, and to -1 for negative weights. This leads to 32x compression. Also,
    the matrix multiplication for binary matrices is $\sim$7x faster (Hubara et al.,
    [2017](#bib.bib48)) leading to faster model inference. In the forward pass, binary
    networks drastically reduce memory size and accesses, and replace most arithmetic
    operations with bit-wise operations, which leads to great increases of power efficiency.
    Also, in the simplest version, binarization can be performed in a static manner,
    i.e., after the training is done. However, this method leads to large loss in
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: A variant of this simple method is to set the weight to a constant $c_{1}$ for
    non-negative weights, and to another constant $c_{2}$ for negative weights. Binary
    Scheme (BS)-Fixed method (Lam, [2018](#bib.bib60)) stores the original weights
    and during the forward pass replaces the values with a masked value of $c_{1}$
    or $c_{2}$, where $c_{1}$ and $c_{2}$ are fixed and chosen with hyperparameter
    tuning. Full precision weights are used during training. At the end of training,
    the weights are replaced with the index of its masked value. Choosing the values
    of $c_{1}$ and $c_{2}$ can be difficult and time-consuming in BS-Fixed. Thus,
    in the BS-flexible method (Cheong and Daniel, [2019](#bib.bib16)), we initialize
    $c_{1}$ and $c_{2}$ using KMeans with two centroids over the weights, and then
    update $c_{1}$ and $c_{2}$ using back-propagation. Also, in the BS-Flexible method,
    weights are quantized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $w_{b}=\begin{cases}c_{1}&amp;\text{if }w\geq(c_{1}+c_{2})/2\\
    c_{2}&amp;\text{if }w<(c_{1}+c_{2})/2\end{cases}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Note that $w$ is the original weight value while $w_{b}$ is the binarized weight
    value. These changes eliminate the need for hyper-parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Stochastic Binarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Stochastic (Courbariaux et al., [2015](#bib.bib18)) binarization is performed
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | $w_{b}=\begin{cases}+1&amp;\text{with probability }p=\sigma(w)\\
    -1&amp;\text{with probability }1-p\end{cases}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $\displaystyle\sigma(w)=\text{clip}\left(\frac{w+1}{2},0,1\right)=\max\left(0,\min\left(1,\frac{w+1}{2}\right)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We only binarize the weights during the forward and backward propagations but
    not during the parameter update. Keeping good precision weights during the updates
    is necessary for Stochastic Gradient Descent (SGD). This is possible using something
    called as “Straight Through Estimator (STE) trick” (Bengio et al., [2013](#bib.bib8)).
    As per STE, as the quantized value is an approximation of the original value,
    we can substitute the gradient with respect to the quantized value for the gradient
    of original value. The trick allows the inclusion of quantization into the computation
    graph of back-propagation and allows QNNs to represent parameters, activations
    and gradients with low bitwidth numbers. For test-time inference, there are three
    options using such a quantization method:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the resulting binary weights $w_{b}$ (this makes most sense with the deterministic
    binarization).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the stochastic case, many different networks can be sampled by sampling a
    $w_{b}$ for each weight. The ensemble output of these networks can then be obtained
    by averaging the outputs from individual networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use original weights. But this does not reduce model size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides this, there have been further efforts that make train/test faster but
    do not reduce model size. For example, Lin et al. (Lin et al., [2015](#bib.bib70))
    convert multiplications in the backward pass into bit-shifts by restricting the
    activations to be power-of-two integers. Hubara et al. (Hubara et al., [2016](#bib.bib47))
    binarize weights and activations, at the inference phase and the entire training
    phase of a deep network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. Loss Aware Binarization (LAB)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The naïve binary quantization methods divide the real number line into two parts
    and each part was mapped to a quantized weight value. Can we decide per weight
    value which of the two weights it should be quantized to? Thus the idea behind
    Binary Weight Networks (BWN) (Rastegari et al., [2016](#bib.bib98)) is to approximate
    the weight vector $W\in R^{n}$ using a binary vector $B\in\{+1,-1\}^{n}$ and a
    scaling factor $\alpha\in R^{+}$ such that $W\approx\alpha B$. This can be expressed
    as an optimization problem as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $\displaystyle\alpha^{*},B^{*}=\operatorname*{argmin}_{\alpha,B}&#124;&#124;W-\alpha
    B&#124;&#124;^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: We can expand and write this as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\displaystyle&#124;&#124;W-\alpha B&#124;&#124;^{2}=\alpha^{2}B^{T}B-2\alpha
    W^{T}B+W^{T}W$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since $B\in{+1,-1}^{n}$, $B^{T}B=n$. Also $W^{T}W$ is a constant. Thus $B^{*}=\operatorname*{argmax}_{B}W^{B}$
    such that $B\in{+1,-1}^{n}$. This optimization can be solved by simply assigning
    $B_{i}=+1$ when $W_{i}\geq 0$, and $B_{i}=-1$ otherwise. To compute $\alpha^{*}$,
    we set the derivative of $||W-\alpha B||^{2}$ wrt $\alpha$ to 0 and get the solution
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $\displaystyle\alpha^{*}=\frac{\sum&#124;W_{i}&#124;}{n}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, besides the binarized weight matrix, a scaling parameter is also learned
    in BWN.
  prefs: []
  type: TYPE_NORMAL
- en: To take this idea further, can we learn $\alpha$ and $B$ to minimize the overall
    network’s loss function? Thus, now, the Weight binarization can be formulated
    as the following optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $\displaystyle\min_{\hat{w}}\text{loss}(\hat{w})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (26) |  | $\displaystyle\text{such that }\hat{w}_{l}=\alpha_{l}b_{l};\alpha_{l}>0;b_{l}\in\{+1,-1\}^{n_{l}};l=1,...,L$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $L$ is the number of layers, $n_{l}$ is the number of weights in layer
    $l$. This loss aware binarization (Hou et al., [2016](#bib.bib46)) problem can
    be solved using proximal Newton algorithm (Lee et al., [2014](#bib.bib64)) to
    find the best $\alpha_{l}$ and $B_{l}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Ternarized Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unfortunately, binary quantization of the recurrent weights in RNNs/LSTMs never
    worked (Ott et al., [2016](#bib.bib90)). When the true value of a weight is near
    zero, its quantized value is either set to -1 or 1\. This results into an artificial
    increase in the magnitude of the weights and the vanishing/exploding gradients
    problem becomes more severe. Hence, another popular form of quantization is ternary
    quantization (see Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of
    Deep Learning Models for Text: A Survey")(B)). Ternary quantization can help achieve
    a min of 16x compression (up to 32x compression if hardware allows to avoid storing
    zeros). In this section, we discuss different variants of ternary quantization
    from the simplest ternary connect networks to hybrid ternary networks like HitNets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Ternary Weight Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The simplest method for ternary quantization is ternary connect (Lin et al.,
    [2015](#bib.bib70)) whose deterministic form is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (27) |  | <math id="S3.E27.m1.6" class="ltx_Math" alttext="w_{t}=\begin{cases}+1&amp;\text{if
    }w>0.5\\ 0&amp;\text{if }-0.5<w\leq 0.5\\'
  prefs: []
  type: TYPE_NORMAL
- en: -1&amp;\text{if }w\leq-0.5\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics id="S3.E27.m1.6a"><mrow id="S3.E27.m1.6.7"
    xref="S3.E27.m1.6.7.cmml"><msub id="S3.E27.m1.6.7.2" xref="S3.E27.m1.6.7.2.cmml"><mi
    id="S3.E27.m1.6.7.2.2" xref="S3.E27.m1.6.7.2.2.cmml">w</mi><mi id="S3.E27.m1.6.7.2.3"
    xref="S3.E27.m1.6.7.2.3.cmml">t</mi></msub><mo id="S3.E27.m1.6.7.1" xref="S3.E27.m1.6.7.1.cmml">=</mo><mrow
    id="S3.E27.m1.6.6" xref="S3.E27.m1.6.7.3.1.cmml"><mo id="S3.E27.m1.6.6.7" xref="S3.E27.m1.6.7.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E27.m1.6.6.6"
    xref="S3.E27.m1.6.7.3.1.cmml"><mtr id="S3.E27.m1.6.6.6a" xref="S3.E27.m1.6.7.3.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S3.E27.m1.6.6.6b" xref="S3.E27.m1.6.7.3.1.cmml"><mrow
    id="S3.E27.m1.1.1.1.1.1.1" xref="S3.E27.m1.1.1.1.1.1.1.cmml"><mo id="S3.E27.m1.1.1.1.1.1.1a"
    xref="S3.E27.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E27.m1.1.1.1.1.1.1.2" xref="S3.E27.m1.1.1.1.1.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E27.m1.6.6.6c" xref="S3.E27.m1.6.7.3.1.cmml"><mrow
    id="S3.E27.m1.2.2.2.2.2.1" xref="S3.E27.m1.2.2.2.2.2.1.cmml"><mrow id="S3.E27.m1.2.2.2.2.2.1.2"
    xref="S3.E27.m1.2.2.2.2.2.1.2.cmml"><mtext id="S3.E27.m1.2.2.2.2.2.1.2.2" xref="S3.E27.m1.2.2.2.2.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E27.m1.2.2.2.2.2.1.2.1" xref="S3.E27.m1.2.2.2.2.2.1.2.1.cmml">​</mo><mi
    id="S3.E27.m1.2.2.2.2.2.1.2.3" xref="S3.E27.m1.2.2.2.2.2.1.2.3.cmml">w</mi></mrow><mo
    id="S3.E27.m1.2.2.2.2.2.1.1" xref="S3.E27.m1.2.2.2.2.2.1.1.cmml">></mo><mn id="S3.E27.m1.2.2.2.2.2.1.3"
    xref="S3.E27.m1.2.2.2.2.2.1.3.cmml">0.5</mn></mrow></mtd></mtr><mtr id="S3.E27.m1.6.6.6d"
    xref="S3.E27.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E27.m1.6.6.6e"
    xref="S3.E27.m1.6.7.3.1.cmml"><mn id="S3.E27.m1.3.3.3.3.1.1" xref="S3.E27.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E27.m1.6.6.6f" xref="S3.E27.m1.6.7.3.1.cmml"><mrow
    id="S3.E27.m1.4.4.4.4.2.1" xref="S3.E27.m1.4.4.4.4.2.1.cmml"><mrow id="S3.E27.m1.4.4.4.4.2.1.2"
    xref="S3.E27.m1.4.4.4.4.2.1.2.cmml"><mtext id="S3.E27.m1.4.4.4.4.2.1.2.2" xref="S3.E27.m1.4.4.4.4.2.1.2.2a.cmml">if </mtext><mo
    id="S3.E27.m1.4.4.4.4.2.1.2.1" xref="S3.E27.m1.4.4.4.4.2.1.2.1.cmml">−</mo><mn
    id="S3.E27.m1.4.4.4.4.2.1.2.3" xref="S3.E27.m1.4.4.4.4.2.1.2.3.cmml">0.5</mn></mrow><mo
    id="S3.E27.m1.4.4.4.4.2.1.3" xref="S3.E27.m1.4.4.4.4.2.1.3.cmml"><</mo><mi id="S3.E27.m1.4.4.4.4.2.1.4"
    xref="S3.E27.m1.4.4.4.4.2.1.4.cmml">w</mi><mo id="S3.E27.m1.4.4.4.4.2.1.5" xref="S3.E27.m1.4.4.4.4.2.1.5.cmml">≤</mo><mn
    id="S3.E27.m1.4.4.4.4.2.1.6" xref="S3.E27.m1.4.4.4.4.2.1.6.cmml">0.5</mn></mrow></mtd></mtr><mtr
    id="S3.E27.m1.6.6.6g" xref="S3.E27.m1.6.7.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E27.m1.6.6.6h" xref="S3.E27.m1.6.7.3.1.cmml"><mrow id="S3.E27.m1.5.5.5.5.1.1"
    xref="S3.E27.m1.5.5.5.5.1.1.cmml"><mo id="S3.E27.m1.5.5.5.5.1.1a" xref="S3.E27.m1.5.5.5.5.1.1.cmml">−</mo><mn
    id="S3.E27.m1.5.5.5.5.1.1.2" xref="S3.E27.m1.5.5.5.5.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E27.m1.6.6.6i" xref="S3.E27.m1.6.7.3.1.cmml"><mrow
    id="S3.E27.m1.6.6.6.6.2.1" xref="S3.E27.m1.6.6.6.6.2.1.cmml"><mrow id="S3.E27.m1.6.6.6.6.2.1.2"
    xref="S3.E27.m1.6.6.6.6.2.1.2.cmml"><mtext id="S3.E27.m1.6.6.6.6.2.1.2.2" xref="S3.E27.m1.6.6.6.6.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E27.m1.6.6.6.6.2.1.2.1" xref="S3.E27.m1.6.6.6.6.2.1.2.1.cmml">​</mo><mi
    id="S3.E27.m1.6.6.6.6.2.1.2.3" xref="S3.E27.m1.6.6.6.6.2.1.2.3.cmml">w</mi></mrow><mo
    id="S3.E27.m1.6.6.6.6.2.1.1" xref="S3.E27.m1.6.6.6.6.2.1.1.cmml">≤</mo><mrow id="S3.E27.m1.6.6.6.6.2.1.3"
    xref="S3.E27.m1.6.6.6.6.2.1.3.cmml"><mo id="S3.E27.m1.6.6.6.6.2.1.3a" xref="S3.E27.m1.6.6.6.6.2.1.3.cmml">−</mo><mn
    id="S3.E27.m1.6.6.6.6.2.1.3.2" xref="S3.E27.m1.6.6.6.6.2.1.3.2.cmml">0.5</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E27.m1.6b"><apply id="S3.E27.m1.6.7.cmml" xref="S3.E27.m1.6.7"><apply
    id="S3.E27.m1.6.7.2.cmml" xref="S3.E27.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E27.m1.6.7.2.1.cmml"
    xref="S3.E27.m1.6.7.2">subscript</csymbol><ci id="S3.E27.m1.6.7.2.2.cmml" xref="S3.E27.m1.6.7.2.2">𝑤</ci><ci
    id="S3.E27.m1.6.7.2.3.cmml" xref="S3.E27.m1.6.7.2.3">𝑡</ci></apply><apply id="S3.E27.m1.6.7.3.1.cmml"
    xref="S3.E27.m1.6.6"><csymbol cd="latexml" id="S3.E27.m1.6.7.3.1.1.cmml" xref="S3.E27.m1.6.6.7">cases</csymbol><apply
    id="S3.E27.m1.1.1.1.1.1.1.cmml" xref="S3.E27.m1.1.1.1.1.1.1"><cn type="integer"
    id="S3.E27.m1.1.1.1.1.1.1.2.cmml" xref="S3.E27.m1.1.1.1.1.1.1.2">1</cn></apply><apply
    id="S3.E27.m1.2.2.2.2.2.1.cmml" xref="S3.E27.m1.2.2.2.2.2.1"><apply id="S3.E27.m1.2.2.2.2.2.1.2.cmml"
    xref="S3.E27.m1.2.2.2.2.2.1.2"><ci id="S3.E27.m1.2.2.2.2.2.1.2.2a.cmml" xref="S3.E27.m1.2.2.2.2.2.1.2.2"><mtext
    id="S3.E27.m1.2.2.2.2.2.1.2.2.cmml" xref="S3.E27.m1.2.2.2.2.2.1.2.2">if </mtext></ci><ci
    id="S3.E27.m1.2.2.2.2.2.1.2.3.cmml" xref="S3.E27.m1.2.2.2.2.2.1.2.3">𝑤</ci></apply><cn
    type="float" id="S3.E27.m1.2.2.2.2.2.1.3.cmml" xref="S3.E27.m1.2.2.2.2.2.1.3">0.5</cn></apply><cn
    type="integer" id="S3.E27.m1.3.3.3.3.1.1.cmml" xref="S3.E27.m1.3.3.3.3.1.1">0</cn><apply
    id="S3.E27.m1.4.4.4.4.2.1.cmml" xref="S3.E27.m1.4.4.4.4.2.1"><apply id="S3.E27.m1.4.4.4.4.2.1b.cmml"
    xref="S3.E27.m1.4.4.4.4.2.1"><apply id="S3.E27.m1.4.4.4.4.2.1.2.cmml" xref="S3.E27.m1.4.4.4.4.2.1.2"><ci
    id="S3.E27.m1.4.4.4.4.2.1.2.2a.cmml" xref="S3.E27.m1.4.4.4.4.2.1.2.2"><mtext id="S3.E27.m1.4.4.4.4.2.1.2.2.cmml"
    xref="S3.E27.m1.4.4.4.4.2.1.2.2">if </mtext></ci><cn type="float" id="S3.E27.m1.4.4.4.4.2.1.2.3.cmml"
    xref="S3.E27.m1.4.4.4.4.2.1.2.3">0.5</cn></apply><ci id="S3.E27.m1.4.4.4.4.2.1.4.cmml"
    xref="S3.E27.m1.4.4.4.4.2.1.4">𝑤</ci></apply><apply id="S3.E27.m1.4.4.4.4.2.1c.cmml"
    xref="S3.E27.m1.4.4.4.4.2.1"><cn type="float" id="S3.E27.m1.4.4.4.4.2.1.6.cmml"
    xref="S3.E27.m1.4.4.4.4.2.1.6">0.5</cn></apply></apply><apply id="S3.E27.m1.5.5.5.5.1.1.cmml"
    xref="S3.E27.m1.5.5.5.5.1.1"><cn type="integer" id="S3.E27.m1.5.5.5.5.1.1.2.cmml"
    xref="S3.E27.m1.5.5.5.5.1.1.2">1</cn></apply><apply id="S3.E27.m1.6.6.6.6.2.1.cmml"
    xref="S3.E27.m1.6.6.6.6.2.1"><apply id="S3.E27.m1.6.6.6.6.2.1.2.cmml" xref="S3.E27.m1.6.6.6.6.2.1.2"><ci
    id="S3.E27.m1.6.6.6.6.2.1.2.2a.cmml" xref="S3.E27.m1.6.6.6.6.2.1.2.2"><mtext id="S3.E27.m1.6.6.6.6.2.1.2.2.cmml"
    xref="S3.E27.m1.6.6.6.6.2.1.2.2">if </mtext></ci><ci id="S3.E27.m1.6.6.6.6.2.1.2.3.cmml"
    xref="S3.E27.m1.6.6.6.6.2.1.2.3">𝑤</ci></apply><apply id="S3.E27.m1.6.6.6.6.2.1.3.cmml"
    xref="S3.E27.m1.6.6.6.6.2.1.3"><cn type="float" id="S3.E27.m1.6.6.6.6.2.1.3.2.cmml"
    xref="S3.E27.m1.6.6.6.6.2.1.3.2">0.5</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E27.m1.6c">w_{t}=\begin{cases}+1&\text{if
    }w>0.5\\ 0&\text{if }-0.5<w\leq 0.5\\ -1&\text{if }w\leq-0.5\\ \end{cases}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: Note that $w$ is the original weight value while $w_{t}$ is the ternarized weight
    value. Like binary connect, ternary connect also eliminates all multiplications
    in the forward pass. In the stochastic form, assuming original weights have been
    normalized to be in the range [-1,1], ternary quantization is done as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | <math id="S3.E28.m1.8" class="ltx_Math" alttext="w_{t}=\begin{cases}+1&amp;\text{with
    prob }w\text{ if }w\in(0,1]\\ 0&amp;\text{with prob }1-w\text{ if }w\in(0,1]\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{with prob }1+w\text{ if }w\in[-1,0]\\
  prefs: []
  type: TYPE_NORMAL
- en: -1&amp;\text{with prob }-w\text{ if }w\in[-1,0]\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics id="S3.E28.m1.8a"><mrow id="S3.E28.m1.8.9"
    xref="S3.E28.m1.8.9.cmml"><msub id="S3.E28.m1.8.9.2" xref="S3.E28.m1.8.9.2.cmml"><mi
    id="S3.E28.m1.8.9.2.2" xref="S3.E28.m1.8.9.2.2.cmml">w</mi><mi id="S3.E28.m1.8.9.2.3"
    xref="S3.E28.m1.8.9.2.3.cmml">t</mi></msub><mo id="S3.E28.m1.8.9.1" xref="S3.E28.m1.8.9.1.cmml">=</mo><mrow
    id="S3.E28.m1.8.8" xref="S3.E28.m1.8.9.3.1.cmml"><mo id="S3.E28.m1.8.8.9" xref="S3.E28.m1.8.9.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E28.m1.8.8.8"
    xref="S3.E28.m1.8.9.3.1.cmml"><mtr id="S3.E28.m1.8.8.8a" xref="S3.E28.m1.8.9.3.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S3.E28.m1.8.8.8b" xref="S3.E28.m1.8.9.3.1.cmml"><mrow
    id="S3.E28.m1.1.1.1.1.1.1" xref="S3.E28.m1.1.1.1.1.1.1.cmml"><mo id="S3.E28.m1.1.1.1.1.1.1a"
    xref="S3.E28.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E28.m1.1.1.1.1.1.1.2" xref="S3.E28.m1.1.1.1.1.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E28.m1.8.8.8c" xref="S3.E28.m1.8.9.3.1.cmml"><mrow
    id="S3.E28.m1.2.2.2.2.2.1" xref="S3.E28.m1.2.2.2.2.2.1.cmml"><mrow id="S3.E28.m1.2.2.2.2.2.1.4"
    xref="S3.E28.m1.2.2.2.2.2.1.4.cmml"><mtext id="S3.E28.m1.2.2.2.2.2.1.4.2" xref="S3.E28.m1.2.2.2.2.2.1.4.2a.cmml">with
    prob </mtext><mo lspace="0em" rspace="0em" id="S3.E28.m1.2.2.2.2.2.1.4.1" xref="S3.E28.m1.2.2.2.2.2.1.4.1.cmml">​</mo><mi
    id="S3.E28.m1.2.2.2.2.2.1.4.3" xref="S3.E28.m1.2.2.2.2.2.1.4.3.cmml">w</mi><mo
    lspace="0em" rspace="0em" id="S3.E28.m1.2.2.2.2.2.1.4.1a" xref="S3.E28.m1.2.2.2.2.2.1.4.1.cmml">​</mo><mtext
    id="S3.E28.m1.2.2.2.2.2.1.4.4" xref="S3.E28.m1.2.2.2.2.2.1.4.4a.cmml"> if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E28.m1.2.2.2.2.2.1.4.1b" xref="S3.E28.m1.2.2.2.2.2.1.4.1.cmml">​</mo><mi
    id="S3.E28.m1.2.2.2.2.2.1.4.5" xref="S3.E28.m1.2.2.2.2.2.1.4.5.cmml">w</mi></mrow><mo
    id="S3.E28.m1.2.2.2.2.2.1.3" xref="S3.E28.m1.2.2.2.2.2.1.3.cmml">∈</mo><mrow id="S3.E28.m1.2.2.2.2.2.1.5.2"
    xref="S3.E28.m1.2.2.2.2.2.1.5.1.cmml"><mo stretchy="false" id="S3.E28.m1.2.2.2.2.2.1.5.2.1"
    xref="S3.E28.m1.2.2.2.2.2.1.5.1.cmml">(</mo><mn id="S3.E28.m1.2.2.2.2.2.1.1" xref="S3.E28.m1.2.2.2.2.2.1.1.cmml">0</mn><mo
    id="S3.E28.m1.2.2.2.2.2.1.5.2.2" xref="S3.E28.m1.2.2.2.2.2.1.5.1.cmml">,</mo><mn
    id="S3.E28.m1.2.2.2.2.2.1.2" xref="S3.E28.m1.2.2.2.2.2.1.2.cmml">1</mn><mo stretchy="false"
    id="S3.E28.m1.2.2.2.2.2.1.5.2.3" xref="S3.E28.m1.2.2.2.2.2.1.5.1.cmml">]</mo></mrow></mrow></mtd></mtr><mtr
    id="S3.E28.m1.8.8.8d" xref="S3.E28.m1.8.9.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E28.m1.8.8.8e" xref="S3.E28.m1.8.9.3.1.cmml"><mn id="S3.E28.m1.3.3.3.3.1.1"
    xref="S3.E28.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left"
    id="S3.E28.m1.8.8.8f" xref="S3.E28.m1.8.9.3.1.cmml"><mrow id="S3.E28.m1.4.4.4.4.2.1"
    xref="S3.E28.m1.4.4.4.4.2.1.cmml"><mrow id="S3.E28.m1.4.4.4.4.2.1.4" xref="S3.E28.m1.4.4.4.4.2.1.4.cmml"><mrow
    id="S3.E28.m1.4.4.4.4.2.1.4.2" xref="S3.E28.m1.4.4.4.4.2.1.4.2.cmml"><mtext id="S3.E28.m1.4.4.4.4.2.1.4.2.2"
    xref="S3.E28.m1.4.4.4.4.2.1.4.2.2a.cmml">with prob </mtext><mo lspace="0em" rspace="0em"
    id="S3.E28.m1.4.4.4.4.2.1.4.2.1" xref="S3.E28.m1.4.4.4.4.2.1.4.2.1.cmml">​</mo><mn
    id="S3.E28.m1.4.4.4.4.2.1.4.2.3" xref="S3.E28.m1.4.4.4.4.2.1.4.2.3.cmml">1</mn></mrow><mo
    id="S3.E28.m1.4.4.4.4.2.1.4.1" xref="S3.E28.m1.4.4.4.4.2.1.4.1.cmml">−</mo><mrow
    id="S3.E28.m1.4.4.4.4.2.1.4.3" xref="S3.E28.m1.4.4.4.4.2.1.4.3.cmml"><mi id="S3.E28.m1.4.4.4.4.2.1.4.3.2"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E28.m1.4.4.4.4.2.1.4.3.1"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.1.cmml">​</mo><mtext id="S3.E28.m1.4.4.4.4.2.1.4.3.3"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.3a.cmml"> if </mtext><mo lspace="0em" rspace="0em"
    id="S3.E28.m1.4.4.4.4.2.1.4.3.1a" xref="S3.E28.m1.4.4.4.4.2.1.4.3.1.cmml">​</mo><mi
    id="S3.E28.m1.4.4.4.4.2.1.4.3.4" xref="S3.E28.m1.4.4.4.4.2.1.4.3.4.cmml">w</mi></mrow></mrow><mo
    id="S3.E28.m1.4.4.4.4.2.1.3" xref="S3.E28.m1.4.4.4.4.2.1.3.cmml">∈</mo><mrow id="S3.E28.m1.4.4.4.4.2.1.5.2"
    xref="S3.E28.m1.4.4.4.4.2.1.5.1.cmml"><mo stretchy="false" id="S3.E28.m1.4.4.4.4.2.1.5.2.1"
    xref="S3.E28.m1.4.4.4.4.2.1.5.1.cmml">(</mo><mn id="S3.E28.m1.4.4.4.4.2.1.1" xref="S3.E28.m1.4.4.4.4.2.1.1.cmml">0</mn><mo
    id="S3.E28.m1.4.4.4.4.2.1.5.2.2" xref="S3.E28.m1.4.4.4.4.2.1.5.1.cmml">,</mo><mn
    id="S3.E28.m1.4.4.4.4.2.1.2" xref="S3.E28.m1.4.4.4.4.2.1.2.cmml">1</mn><mo stretchy="false"
    id="S3.E28.m1.4.4.4.4.2.1.5.2.3" xref="S3.E28.m1.4.4.4.4.2.1.5.1.cmml">]</mo></mrow></mrow></mtd></mtr><mtr
    id="S3.E28.m1.8.8.8g" xref="S3.E28.m1.8.9.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E28.m1.8.8.8h" xref="S3.E28.m1.8.9.3.1.cmml"><mn id="S3.E28.m1.5.5.5.5.1.1"
    xref="S3.E28.m1.5.5.5.5.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left"
    id="S3.E28.m1.8.8.8i" xref="S3.E28.m1.8.9.3.1.cmml"><mrow id="S3.E28.m1.6.6.6.6.2.1"
    xref="S3.E28.m1.6.6.6.6.2.1.cmml"><mrow id="S3.E28.m1.6.6.6.6.2.1.4" xref="S3.E28.m1.6.6.6.6.2.1.4.cmml"><mrow
    id="S3.E28.m1.6.6.6.6.2.1.4.2" xref="S3.E28.m1.6.6.6.6.2.1.4.2.cmml"><mtext id="S3.E28.m1.6.6.6.6.2.1.4.2.2"
    xref="S3.E28.m1.6.6.6.6.2.1.4.2.2a.cmml">with prob </mtext><mo lspace="0em" rspace="0em"
    id="S3.E28.m1.6.6.6.6.2.1.4.2.1" xref="S3.E28.m1.6.6.6.6.2.1.4.2.1.cmml">​</mo><mn
    id="S3.E28.m1.6.6.6.6.2.1.4.2.3" xref="S3.E28.m1.6.6.6.6.2.1.4.2.3.cmml">1</mn></mrow><mo
    id="S3.E28.m1.6.6.6.6.2.1.4.1" xref="S3.E28.m1.6.6.6.6.2.1.4.1.cmml">+</mo><mrow
    id="S3.E28.m1.6.6.6.6.2.1.4.3" xref="S3.E28.m1.6.6.6.6.2.1.4.3.cmml"><mi id="S3.E28.m1.6.6.6.6.2.1.4.3.2"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E28.m1.6.6.6.6.2.1.4.3.1"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.1.cmml">​</mo><mtext id="S3.E28.m1.6.6.6.6.2.1.4.3.3"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.3a.cmml"> if </mtext><mo lspace="0em" rspace="0em"
    id="S3.E28.m1.6.6.6.6.2.1.4.3.1a" xref="S3.E28.m1.6.6.6.6.2.1.4.3.1.cmml">​</mo><mi
    id="S3.E28.m1.6.6.6.6.2.1.4.3.4" xref="S3.E28.m1.6.6.6.6.2.1.4.3.4.cmml">w</mi></mrow></mrow><mo
    id="S3.E28.m1.6.6.6.6.2.1.3" xref="S3.E28.m1.6.6.6.6.2.1.3.cmml">∈</mo><mrow id="S3.E28.m1.6.6.6.6.2.1.2.1"
    xref="S3.E28.m1.6.6.6.6.2.1.2.2.cmml"><mo stretchy="false" id="S3.E28.m1.6.6.6.6.2.1.2.1.2"
    xref="S3.E28.m1.6.6.6.6.2.1.2.2.cmml">[</mo><mrow id="S3.E28.m1.6.6.6.6.2.1.2.1.1"
    xref="S3.E28.m1.6.6.6.6.2.1.2.1.1.cmml"><mo id="S3.E28.m1.6.6.6.6.2.1.2.1.1a"
    xref="S3.E28.m1.6.6.6.6.2.1.2.1.1.cmml">−</mo><mn id="S3.E28.m1.6.6.6.6.2.1.2.1.1.2"
    xref="S3.E28.m1.6.6.6.6.2.1.2.1.1.2.cmml">1</mn></mrow><mo id="S3.E28.m1.6.6.6.6.2.1.2.1.3"
    xref="S3.E28.m1.6.6.6.6.2.1.2.2.cmml">,</mo><mn id="S3.E28.m1.6.6.6.6.2.1.1" xref="S3.E28.m1.6.6.6.6.2.1.1.cmml">0</mn><mo
    stretchy="false" id="S3.E28.m1.6.6.6.6.2.1.2.1.4" xref="S3.E28.m1.6.6.6.6.2.1.2.2.cmml">]</mo></mrow></mrow></mtd></mtr><mtr
    id="S3.E28.m1.8.8.8j" xref="S3.E28.m1.8.9.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E28.m1.8.8.8k" xref="S3.E28.m1.8.9.3.1.cmml"><mrow id="S3.E28.m1.7.7.7.7.1.1"
    xref="S3.E28.m1.7.7.7.7.1.1.cmml"><mo id="S3.E28.m1.7.7.7.7.1.1a" xref="S3.E28.m1.7.7.7.7.1.1.cmml">−</mo><mn
    id="S3.E28.m1.7.7.7.7.1.1.2" xref="S3.E28.m1.7.7.7.7.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E28.m1.8.8.8l" xref="S3.E28.m1.8.9.3.1.cmml"><mrow
    id="S3.E28.m1.8.8.8.8.2.1" xref="S3.E28.m1.8.8.8.8.2.1.cmml"><mrow id="S3.E28.m1.8.8.8.8.2.1.4"
    xref="S3.E28.m1.8.8.8.8.2.1.4.cmml"><mtext id="S3.E28.m1.8.8.8.8.2.1.4.2" xref="S3.E28.m1.8.8.8.8.2.1.4.2a.cmml">with
    prob </mtext><mo id="S3.E28.m1.8.8.8.8.2.1.4.1" xref="S3.E28.m1.8.8.8.8.2.1.4.1.cmml">−</mo><mrow
    id="S3.E28.m1.8.8.8.8.2.1.4.3" xref="S3.E28.m1.8.8.8.8.2.1.4.3.cmml"><mi id="S3.E28.m1.8.8.8.8.2.1.4.3.2"
    xref="S3.E28.m1.8.8.8.8.2.1.4.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E28.m1.8.8.8.8.2.1.4.3.1"
    xref="S3.E28.m1.8.8.8.8.2.1.4.3.1.cmml">​</mo><mtext id="S3.E28.m1.8.8.8.8.2.1.4.3.3"
    xref="S3.E28.m1.8.8.8.8.2.1.4.3.3a.cmml"> if </mtext><mo lspace="0em" rspace="0em"
    id="S3.E28.m1.8.8.8.8.2.1.4.3.1a" xref="S3.E28.m1.8.8.8.8.2.1.4.3.1.cmml">​</mo><mi
    id="S3.E28.m1.8.8.8.8.2.1.4.3.4" xref="S3.E28.m1.8.8.8.8.2.1.4.3.4.cmml">w</mi></mrow></mrow><mo
    id="S3.E28.m1.8.8.8.8.2.1.3" xref="S3.E28.m1.8.8.8.8.2.1.3.cmml">∈</mo><mrow id="S3.E28.m1.8.8.8.8.2.1.2.1"
    xref="S3.E28.m1.8.8.8.8.2.1.2.2.cmml"><mo stretchy="false" id="S3.E28.m1.8.8.8.8.2.1.2.1.2"
    xref="S3.E28.m1.8.8.8.8.2.1.2.2.cmml">[</mo><mrow id="S3.E28.m1.8.8.8.8.2.1.2.1.1"
    xref="S3.E28.m1.8.8.8.8.2.1.2.1.1.cmml"><mo id="S3.E28.m1.8.8.8.8.2.1.2.1.1a"
    xref="S3.E28.m1.8.8.8.8.2.1.2.1.1.cmml">−</mo><mn id="S3.E28.m1.8.8.8.8.2.1.2.1.1.2"
    xref="S3.E28.m1.8.8.8.8.2.1.2.1.1.2.cmml">1</mn></mrow><mo id="S3.E28.m1.8.8.8.8.2.1.2.1.3"
    xref="S3.E28.m1.8.8.8.8.2.1.2.2.cmml">,</mo><mn id="S3.E28.m1.8.8.8.8.2.1.1" xref="S3.E28.m1.8.8.8.8.2.1.1.cmml">0</mn><mo
    stretchy="false" id="S3.E28.m1.8.8.8.8.2.1.2.1.4" xref="S3.E28.m1.8.8.8.8.2.1.2.2.cmml">]</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E28.m1.8b"><apply id="S3.E28.m1.8.9.cmml" xref="S3.E28.m1.8.9"><apply
    id="S3.E28.m1.8.9.2.cmml" xref="S3.E28.m1.8.9.2"><csymbol cd="ambiguous" id="S3.E28.m1.8.9.2.1.cmml"
    xref="S3.E28.m1.8.9.2">subscript</csymbol><ci id="S3.E28.m1.8.9.2.2.cmml" xref="S3.E28.m1.8.9.2.2">𝑤</ci><ci
    id="S3.E28.m1.8.9.2.3.cmml" xref="S3.E28.m1.8.9.2.3">𝑡</ci></apply><apply id="S3.E28.m1.8.9.3.1.cmml"
    xref="S3.E28.m1.8.8"><csymbol cd="latexml" id="S3.E28.m1.8.9.3.1.1.cmml" xref="S3.E28.m1.8.8.9">cases</csymbol><apply
    id="S3.E28.m1.1.1.1.1.1.1.cmml" xref="S3.E28.m1.1.1.1.1.1.1"><cn type="integer"
    id="S3.E28.m1.1.1.1.1.1.1.2.cmml" xref="S3.E28.m1.1.1.1.1.1.1.2">1</cn></apply><apply
    id="S3.E28.m1.2.2.2.2.2.1.cmml" xref="S3.E28.m1.2.2.2.2.2.1"><apply id="S3.E28.m1.2.2.2.2.2.1.4.cmml"
    xref="S3.E28.m1.2.2.2.2.2.1.4"><ci id="S3.E28.m1.2.2.2.2.2.1.4.2a.cmml" xref="S3.E28.m1.2.2.2.2.2.1.4.2"><mtext
    id="S3.E28.m1.2.2.2.2.2.1.4.2.cmml" xref="S3.E28.m1.2.2.2.2.2.1.4.2">with prob </mtext></ci><ci
    id="S3.E28.m1.2.2.2.2.2.1.4.3.cmml" xref="S3.E28.m1.2.2.2.2.2.1.4.3">𝑤</ci><ci
    id="S3.E28.m1.2.2.2.2.2.1.4.4a.cmml" xref="S3.E28.m1.2.2.2.2.2.1.4.4"><mtext id="S3.E28.m1.2.2.2.2.2.1.4.4.cmml"
    xref="S3.E28.m1.2.2.2.2.2.1.4.4"> if </mtext></ci><ci id="S3.E28.m1.2.2.2.2.2.1.4.5.cmml"
    xref="S3.E28.m1.2.2.2.2.2.1.4.5">𝑤</ci></apply><interval closure="open-closed"
    id="S3.E28.m1.2.2.2.2.2.1.5.1.cmml" xref="S3.E28.m1.2.2.2.2.2.1.5.2"><cn type="integer"
    id="S3.E28.m1.2.2.2.2.2.1.1.cmml" xref="S3.E28.m1.2.2.2.2.2.1.1">0</cn><cn type="integer"
    id="S3.E28.m1.2.2.2.2.2.1.2.cmml" xref="S3.E28.m1.2.2.2.2.2.1.2">1</cn></interval></apply><cn
    type="integer" id="S3.E28.m1.3.3.3.3.1.1.cmml" xref="S3.E28.m1.3.3.3.3.1.1">0</cn><apply
    id="S3.E28.m1.4.4.4.4.2.1.cmml" xref="S3.E28.m1.4.4.4.4.2.1"><apply id="S3.E28.m1.4.4.4.4.2.1.4.cmml"
    xref="S3.E28.m1.4.4.4.4.2.1.4"><apply id="S3.E28.m1.4.4.4.4.2.1.4.2.cmml" xref="S3.E28.m1.4.4.4.4.2.1.4.2"><ci
    id="S3.E28.m1.4.4.4.4.2.1.4.2.2a.cmml" xref="S3.E28.m1.4.4.4.4.2.1.4.2.2"><mtext
    id="S3.E28.m1.4.4.4.4.2.1.4.2.2.cmml" xref="S3.E28.m1.4.4.4.4.2.1.4.2.2">with
    prob </mtext></ci><cn type="integer" id="S3.E28.m1.4.4.4.4.2.1.4.2.3.cmml" xref="S3.E28.m1.4.4.4.4.2.1.4.2.3">1</cn></apply><apply
    id="S3.E28.m1.4.4.4.4.2.1.4.3.cmml" xref="S3.E28.m1.4.4.4.4.2.1.4.3"><ci id="S3.E28.m1.4.4.4.4.2.1.4.3.2.cmml"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.2">𝑤</ci><ci id="S3.E28.m1.4.4.4.4.2.1.4.3.3a.cmml"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.3"><mtext id="S3.E28.m1.4.4.4.4.2.1.4.3.3.cmml"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.3"> if </mtext></ci><ci id="S3.E28.m1.4.4.4.4.2.1.4.3.4.cmml"
    xref="S3.E28.m1.4.4.4.4.2.1.4.3.4">𝑤</ci></apply></apply><interval closure="open-closed"
    id="S3.E28.m1.4.4.4.4.2.1.5.1.cmml" xref="S3.E28.m1.4.4.4.4.2.1.5.2"><cn type="integer"
    id="S3.E28.m1.4.4.4.4.2.1.1.cmml" xref="S3.E28.m1.4.4.4.4.2.1.1">0</cn><cn type="integer"
    id="S3.E28.m1.4.4.4.4.2.1.2.cmml" xref="S3.E28.m1.4.4.4.4.2.1.2">1</cn></interval></apply><cn
    type="integer" id="S3.E28.m1.5.5.5.5.1.1.cmml" xref="S3.E28.m1.5.5.5.5.1.1">0</cn><apply
    id="S3.E28.m1.6.6.6.6.2.1.cmml" xref="S3.E28.m1.6.6.6.6.2.1"><apply id="S3.E28.m1.6.6.6.6.2.1.4.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.4"><apply id="S3.E28.m1.6.6.6.6.2.1.4.2.cmml" xref="S3.E28.m1.6.6.6.6.2.1.4.2"><ci
    id="S3.E28.m1.6.6.6.6.2.1.4.2.2a.cmml" xref="S3.E28.m1.6.6.6.6.2.1.4.2.2"><mtext
    id="S3.E28.m1.6.6.6.6.2.1.4.2.2.cmml" xref="S3.E28.m1.6.6.6.6.2.1.4.2.2">with
    prob </mtext></ci><cn type="integer" id="S3.E28.m1.6.6.6.6.2.1.4.2.3.cmml" xref="S3.E28.m1.6.6.6.6.2.1.4.2.3">1</cn></apply><apply
    id="S3.E28.m1.6.6.6.6.2.1.4.3.cmml" xref="S3.E28.m1.6.6.6.6.2.1.4.3"><ci id="S3.E28.m1.6.6.6.6.2.1.4.3.2.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.2">𝑤</ci><ci id="S3.E28.m1.6.6.6.6.2.1.4.3.3a.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.3"><mtext id="S3.E28.m1.6.6.6.6.2.1.4.3.3.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.3"> if </mtext></ci><ci id="S3.E28.m1.6.6.6.6.2.1.4.3.4.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.4.3.4">𝑤</ci></apply></apply><interval closure="closed"
    id="S3.E28.m1.6.6.6.6.2.1.2.2.cmml" xref="S3.E28.m1.6.6.6.6.2.1.2.1"><apply id="S3.E28.m1.6.6.6.6.2.1.2.1.1.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.2.1.1"><cn type="integer" id="S3.E28.m1.6.6.6.6.2.1.2.1.1.2.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.2.1.1.2">1</cn></apply><cn type="integer" id="S3.E28.m1.6.6.6.6.2.1.1.cmml"
    xref="S3.E28.m1.6.6.6.6.2.1.1">0</cn></interval></apply><apply id="S3.E28.m1.7.7.7.7.1.1.cmml"
    xref="S3.E28.m1.7.7.7.7.1.1"><cn type="integer" id="S3.E28.m1.7.7.7.7.1.1.2.cmml"
    xref="S3.E28.m1.7.7.7.7.1.1.2">1</cn></apply><apply id="S3.E28.m1.8.8.8.8.2.1.cmml"
    xref="S3.E28.m1.8.8.8.8.2.1"><apply id="S3.E28.m1.8.8.8.8.2.1.4.cmml" xref="S3.E28.m1.8.8.8.8.2.1.4"><ci
    id="S3.E28.m1.8.8.8.8.2.1.4.2a.cmml" xref="S3.E28.m1.8.8.8.8.2.1.4.2"><mtext id="S3.E28.m1.8.8.8.8.2.1.4.2.cmml"
    xref="S3.E28.m1.8.8.8.8.2.1.4.2">with prob </mtext></ci><apply id="S3.E28.m1.8.8.8.8.2.1.4.3.cmml"
    xref="S3.E28.m1.8.8.8.8.2.1.4.3"><ci id="S3.E28.m1.8.8.8.8.2.1.4.3.2.cmml" xref="S3.E28.m1.8.8.8.8.2.1.4.3.2">𝑤</ci><ci
    id="S3.E28.m1.8.8.8.8.2.1.4.3.3a.cmml" xref="S3.E28.m1.8.8.8.8.2.1.4.3.3"><mtext
    id="S3.E28.m1.8.8.8.8.2.1.4.3.3.cmml" xref="S3.E28.m1.8.8.8.8.2.1.4.3.3"> if </mtext></ci><ci
    id="S3.E28.m1.8.8.8.8.2.1.4.3.4.cmml" xref="S3.E28.m1.8.8.8.8.2.1.4.3.4">𝑤</ci></apply></apply><interval
    closure="closed" id="S3.E28.m1.8.8.8.8.2.1.2.2.cmml" xref="S3.E28.m1.8.8.8.8.2.1.2.1"><apply
    id="S3.E28.m1.8.8.8.8.2.1.2.1.1.cmml" xref="S3.E28.m1.8.8.8.8.2.1.2.1.1"><cn type="integer"
    id="S3.E28.m1.8.8.8.8.2.1.2.1.1.2.cmml" xref="S3.E28.m1.8.8.8.8.2.1.2.1.1.2">1</cn></apply><cn
    type="integer" id="S3.E28.m1.8.8.8.8.2.1.1.cmml" xref="S3.E28.m1.8.8.8.8.2.1.1">0</cn></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E28.m1.8c">w_{t}=\begin{cases}+1&\text{with
    prob }w\text{ if }w\in(0,1]\\ 0&\text{with prob }1-w\text{ if }w\in(0,1]\\ 0&\text{with
    prob }1+w\text{ if }w\in[-1,0]\\ -1&\text{with prob }-w\text{ if }w\in[-1,0]\\
    \end{cases}</annotation></semantics></math> |  |
  prefs: []
  type: TYPE_NORMAL
- en: A slightly related way called as Bernoulli Ternary Quantization where $w_{t}$
    is set to +1 (or -1) with prob $p$ if $w>0$ (or) $<0$, and set to 0 with prob
    1-p where $p\sim$Bernoulli($|x|$). Yet another way to set the boundaries for the
    three ranges is to use Gaussian based ternary weights (Alom et al., [2018](#bib.bib2))
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (29) |  | <math id="S3.E29.m1.6" class="ltx_Math" alttext="w_{t}=\begin{cases}+1&amp;\text{if
    }w>-(\mu+\sigma/2)\\ 0&amp;\text{if }-(\mu+\sigma/2)<w\leq(\mu+\sigma/2)\\'
  prefs: []
  type: TYPE_NORMAL
- en: -1&amp;\text{if }w\leq-(\mu+\sigma/2)\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics id="S3.E29.m1.6a"><mrow id="S3.E29.m1.6.7"
    xref="S3.E29.m1.6.7.cmml"><msub id="S3.E29.m1.6.7.2" xref="S3.E29.m1.6.7.2.cmml"><mi
    id="S3.E29.m1.6.7.2.2" xref="S3.E29.m1.6.7.2.2.cmml">w</mi><mi id="S3.E29.m1.6.7.2.3"
    xref="S3.E29.m1.6.7.2.3.cmml">t</mi></msub><mo id="S3.E29.m1.6.7.1" xref="S3.E29.m1.6.7.1.cmml">=</mo><mrow
    id="S3.E29.m1.6.6" xref="S3.E29.m1.6.7.3.1.cmml"><mo id="S3.E29.m1.6.6.7" xref="S3.E29.m1.6.7.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E29.m1.6.6.6"
    xref="S3.E29.m1.6.7.3.1.cmml"><mtr id="S3.E29.m1.6.6.6a" xref="S3.E29.m1.6.7.3.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S3.E29.m1.6.6.6b" xref="S3.E29.m1.6.7.3.1.cmml"><mrow
    id="S3.E29.m1.1.1.1.1.1.1" xref="S3.E29.m1.1.1.1.1.1.1.cmml"><mo id="S3.E29.m1.1.1.1.1.1.1a"
    xref="S3.E29.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E29.m1.1.1.1.1.1.1.2" xref="S3.E29.m1.1.1.1.1.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E29.m1.6.6.6c" xref="S3.E29.m1.6.7.3.1.cmml"><mrow
    id="S3.E29.m1.2.2.2.2.2.1" xref="S3.E29.m1.2.2.2.2.2.1.cmml"><mrow id="S3.E29.m1.2.2.2.2.2.1.3"
    xref="S3.E29.m1.2.2.2.2.2.1.3.cmml"><mtext id="S3.E29.m1.2.2.2.2.2.1.3.2" xref="S3.E29.m1.2.2.2.2.2.1.3.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E29.m1.2.2.2.2.2.1.3.1" xref="S3.E29.m1.2.2.2.2.2.1.3.1.cmml">​</mo><mi
    id="S3.E29.m1.2.2.2.2.2.1.3.3" xref="S3.E29.m1.2.2.2.2.2.1.3.3.cmml">w</mi></mrow><mo
    id="S3.E29.m1.2.2.2.2.2.1.2" xref="S3.E29.m1.2.2.2.2.2.1.2.cmml">></mo><mrow id="S3.E29.m1.2.2.2.2.2.1.1"
    xref="S3.E29.m1.2.2.2.2.2.1.1.cmml"><mo id="S3.E29.m1.2.2.2.2.2.1.1a" xref="S3.E29.m1.2.2.2.2.2.1.1.cmml">−</mo><mrow
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S3.E29.m1.2.2.2.2.2.1.1.1.1.2" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.cmml">(</mo><mrow
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.cmml"><mi
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.2" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.2.cmml">μ</mi><mo
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.1" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.1.cmml">+</mo><mrow
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.2" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.2.cmml">σ</mi><mo
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.1" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.1.cmml">/</mo><mn
    id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.3" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.3.cmml">2</mn></mrow></mrow><mo
    stretchy="false" id="S3.E29.m1.2.2.2.2.2.1.1.1.1.3" xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E29.m1.6.6.6d" xref="S3.E29.m1.6.7.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E29.m1.6.6.6e" xref="S3.E29.m1.6.7.3.1.cmml"><mn id="S3.E29.m1.3.3.3.3.1.1"
    xref="S3.E29.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left"
    id="S3.E29.m1.6.6.6f" xref="S3.E29.m1.6.7.3.1.cmml"><mrow id="S3.E29.m1.4.4.4.4.2.1"
    xref="S3.E29.m1.4.4.4.4.2.1.cmml"><mrow id="S3.E29.m1.4.4.4.4.2.1.1" xref="S3.E29.m1.4.4.4.4.2.1.1.cmml"><mtext
    id="S3.E29.m1.4.4.4.4.2.1.1.3" xref="S3.E29.m1.4.4.4.4.2.1.1.3a.cmml">if </mtext><mo
    id="S3.E29.m1.4.4.4.4.2.1.1.2" xref="S3.E29.m1.4.4.4.4.2.1.1.2.cmml">−</mo><mrow
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S3.E29.m1.4.4.4.4.2.1.1.1.1.2" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.cmml">(</mo><mrow
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.cmml"><mi
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.2" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.2.cmml">μ</mi><mo
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.1" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.1.cmml">+</mo><mrow
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.cmml"><mi
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.2" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.2.cmml">σ</mi><mo
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.1" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.1.cmml">/</mo><mn
    id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.3" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.3.cmml">2</mn></mrow></mrow><mo
    stretchy="false" id="S3.E29.m1.4.4.4.4.2.1.1.1.1.3" xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo
    id="S3.E29.m1.4.4.4.4.2.1.4" xref="S3.E29.m1.4.4.4.4.2.1.4.cmml"><</mo><mi id="S3.E29.m1.4.4.4.4.2.1.5"
    xref="S3.E29.m1.4.4.4.4.2.1.5.cmml">w</mi><mo id="S3.E29.m1.4.4.4.4.2.1.6" xref="S3.E29.m1.4.4.4.4.2.1.6.cmml">≤</mo><mrow
    id="S3.E29.m1.4.4.4.4.2.1.2.1" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.cmml"><mo stretchy="false"
    id="S3.E29.m1.4.4.4.4.2.1.2.1.2" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.cmml">(</mo><mrow
    id="S3.E29.m1.4.4.4.4.2.1.2.1.1" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.cmml"><mi id="S3.E29.m1.4.4.4.4.2.1.2.1.1.2"
    xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.2.cmml">μ</mi><mo id="S3.E29.m1.4.4.4.4.2.1.2.1.1.1"
    xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.1.cmml">+</mo><mrow id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3"
    xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.cmml"><mi id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.2"
    xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.2.cmml">σ</mi><mo id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.1"
    xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.1.cmml">/</mo><mn id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.3"
    xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.3.cmml">2</mn></mrow></mrow><mo stretchy="false"
    id="S3.E29.m1.4.4.4.4.2.1.2.1.3" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.cmml">)</mo></mrow></mrow></mtd></mtr><mtr
    id="S3.E29.m1.6.6.6g" xref="S3.E29.m1.6.7.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E29.m1.6.6.6h" xref="S3.E29.m1.6.7.3.1.cmml"><mrow id="S3.E29.m1.5.5.5.5.1.1"
    xref="S3.E29.m1.5.5.5.5.1.1.cmml"><mo id="S3.E29.m1.5.5.5.5.1.1a" xref="S3.E29.m1.5.5.5.5.1.1.cmml">−</mo><mn
    id="S3.E29.m1.5.5.5.5.1.1.2" xref="S3.E29.m1.5.5.5.5.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E29.m1.6.6.6i" xref="S3.E29.m1.6.7.3.1.cmml"><mrow
    id="S3.E29.m1.6.6.6.6.2.1" xref="S3.E29.m1.6.6.6.6.2.1.cmml"><mrow id="S3.E29.m1.6.6.6.6.2.1.3"
    xref="S3.E29.m1.6.6.6.6.2.1.3.cmml"><mtext id="S3.E29.m1.6.6.6.6.2.1.3.2" xref="S3.E29.m1.6.6.6.6.2.1.3.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E29.m1.6.6.6.6.2.1.3.1" xref="S3.E29.m1.6.6.6.6.2.1.3.1.cmml">​</mo><mi
    id="S3.E29.m1.6.6.6.6.2.1.3.3" xref="S3.E29.m1.6.6.6.6.2.1.3.3.cmml">w</mi></mrow><mo
    id="S3.E29.m1.6.6.6.6.2.1.2" xref="S3.E29.m1.6.6.6.6.2.1.2.cmml">≤</mo><mrow id="S3.E29.m1.6.6.6.6.2.1.1"
    xref="S3.E29.m1.6.6.6.6.2.1.1.cmml"><mo id="S3.E29.m1.6.6.6.6.2.1.1a" xref="S3.E29.m1.6.6.6.6.2.1.1.cmml">−</mo><mrow
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S3.E29.m1.6.6.6.6.2.1.1.1.1.2" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.cmml">(</mo><mrow
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.cmml"><mi
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.2" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.2.cmml">μ</mi><mo
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.1" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.1.cmml">+</mo><mrow
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.cmml"><mi
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.2" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.2.cmml">σ</mi><mo
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.1" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.1.cmml">/</mo><mn
    id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.3" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.3.cmml">2</mn></mrow></mrow><mo
    stretchy="false" id="S3.E29.m1.6.6.6.6.2.1.1.1.1.3" xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E29.m1.6b"><apply id="S3.E29.m1.6.7.cmml" xref="S3.E29.m1.6.7"><apply
    id="S3.E29.m1.6.7.2.cmml" xref="S3.E29.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E29.m1.6.7.2.1.cmml"
    xref="S3.E29.m1.6.7.2">subscript</csymbol><ci id="S3.E29.m1.6.7.2.2.cmml" xref="S3.E29.m1.6.7.2.2">𝑤</ci><ci
    id="S3.E29.m1.6.7.2.3.cmml" xref="S3.E29.m1.6.7.2.3">𝑡</ci></apply><apply id="S3.E29.m1.6.7.3.1.cmml"
    xref="S3.E29.m1.6.6"><csymbol cd="latexml" id="S3.E29.m1.6.7.3.1.1.cmml" xref="S3.E29.m1.6.6.7">cases</csymbol><apply
    id="S3.E29.m1.1.1.1.1.1.1.cmml" xref="S3.E29.m1.1.1.1.1.1.1"><cn type="integer"
    id="S3.E29.m1.1.1.1.1.1.1.2.cmml" xref="S3.E29.m1.1.1.1.1.1.1.2">1</cn></apply><apply
    id="S3.E29.m1.2.2.2.2.2.1.cmml" xref="S3.E29.m1.2.2.2.2.2.1"><apply id="S3.E29.m1.2.2.2.2.2.1.3.cmml"
    xref="S3.E29.m1.2.2.2.2.2.1.3"><ci id="S3.E29.m1.2.2.2.2.2.1.3.2a.cmml" xref="S3.E29.m1.2.2.2.2.2.1.3.2"><mtext
    id="S3.E29.m1.2.2.2.2.2.1.3.2.cmml" xref="S3.E29.m1.2.2.2.2.2.1.3.2">if </mtext></ci><ci
    id="S3.E29.m1.2.2.2.2.2.1.3.3.cmml" xref="S3.E29.m1.2.2.2.2.2.1.3.3">𝑤</ci></apply><apply
    id="S3.E29.m1.2.2.2.2.2.1.1.cmml" xref="S3.E29.m1.2.2.2.2.2.1.1"><apply id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.cmml"
    xref="S3.E29.m1.2.2.2.2.2.1.1.1.1"><ci id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.2.cmml"
    xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.2">𝜇</ci><apply id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.cmml"
    xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3"><ci id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.2.cmml"
    xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.2">𝜎</ci><cn type="integer" id="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.3.cmml"
    xref="S3.E29.m1.2.2.2.2.2.1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply><cn
    type="integer" id="S3.E29.m1.3.3.3.3.1.1.cmml" xref="S3.E29.m1.3.3.3.3.1.1">0</cn><apply
    id="S3.E29.m1.4.4.4.4.2.1.cmml" xref="S3.E29.m1.4.4.4.4.2.1"><apply id="S3.E29.m1.4.4.4.4.2.1b.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1"><apply id="S3.E29.m1.4.4.4.4.2.1.1.cmml" xref="S3.E29.m1.4.4.4.4.2.1.1"><ci
    id="S3.E29.m1.4.4.4.4.2.1.1.3a.cmml" xref="S3.E29.m1.4.4.4.4.2.1.1.3"><mtext id="S3.E29.m1.4.4.4.4.2.1.1.3.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.1.3">if </mtext></ci><apply id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.1.1.1"><ci id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.2.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.2">𝜇</ci><apply id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3"><ci id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.2.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.2">𝜎</ci><cn type="integer" id="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.3.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.1.1.1.1.3.3">2</cn></apply></apply></apply><ci id="S3.E29.m1.4.4.4.4.2.1.5.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1.5">𝑤</ci></apply><apply id="S3.E29.m1.4.4.4.4.2.1c.cmml"
    xref="S3.E29.m1.4.4.4.4.2.1"><apply id="S3.E29.m1.4.4.4.4.2.1.2.1.1.cmml" xref="S3.E29.m1.4.4.4.4.2.1.2.1"><ci
    id="S3.E29.m1.4.4.4.4.2.1.2.1.1.2.cmml" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.2">𝜇</ci><apply
    id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.cmml" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3"><ci
    id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.2.cmml" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.2">𝜎</ci><cn
    type="integer" id="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.3.cmml" xref="S3.E29.m1.4.4.4.4.2.1.2.1.1.3.3">2</cn></apply></apply></apply></apply><apply
    id="S3.E29.m1.5.5.5.5.1.1.cmml" xref="S3.E29.m1.5.5.5.5.1.1"><cn type="integer"
    id="S3.E29.m1.5.5.5.5.1.1.2.cmml" xref="S3.E29.m1.5.5.5.5.1.1.2">1</cn></apply><apply
    id="S3.E29.m1.6.6.6.6.2.1.cmml" xref="S3.E29.m1.6.6.6.6.2.1"><apply id="S3.E29.m1.6.6.6.6.2.1.3.cmml"
    xref="S3.E29.m1.6.6.6.6.2.1.3"><ci id="S3.E29.m1.6.6.6.6.2.1.3.2a.cmml" xref="S3.E29.m1.6.6.6.6.2.1.3.2"><mtext
    id="S3.E29.m1.6.6.6.6.2.1.3.2.cmml" xref="S3.E29.m1.6.6.6.6.2.1.3.2">if </mtext></ci><ci
    id="S3.E29.m1.6.6.6.6.2.1.3.3.cmml" xref="S3.E29.m1.6.6.6.6.2.1.3.3">𝑤</ci></apply><apply
    id="S3.E29.m1.6.6.6.6.2.1.1.cmml" xref="S3.E29.m1.6.6.6.6.2.1.1"><apply id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.cmml"
    xref="S3.E29.m1.6.6.6.6.2.1.1.1.1"><ci id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.2.cmml"
    xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.2">𝜇</ci><apply id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.cmml"
    xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3"><ci id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.2.cmml"
    xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.2">𝜎</ci><cn type="integer" id="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.3.cmml"
    xref="S3.E29.m1.6.6.6.6.2.1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E29.m1.6c">w_{t}=\begin{cases}+1&\text{if
    }w>-(\mu+\sigma/2)\\ 0&\text{if }-(\mu+\sigma/2)<w\leq(\mu+\sigma/2)\\ -1&\text{if
    }w\leq-(\mu+\sigma/2)\\ \end{cases}</annotation></semantics></math> |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mu$ and $\sigma$ are the mean and standard deviation of the weight matrix
    being quantized.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Trained Ternary Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rather than using the rules for ternary quantization as mentioned above, one
    can learn the boundary ranges or the quantized values for individual weights.
    One way of learning the right ternary representation per weight value is to minimize
    the Euclidean distance between full precision weights $W$ and the ternary weights
    $T$ along with a scaling factor (Li et al., [2016b](#bib.bib66)). This can be
    expressed as the following optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $\displaystyle\alpha^{*},T^{*}=\operatorname*{argmin}_{\alpha,T}&#124;&#124;W-\alpha
    T&#124;&#124;_{2}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (31) |  | $\displaystyle\text{such that }\alpha\geq 0;T_{i}\in\{-1,0,1\};i=1,2,...,n$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that this is equivalent to the BWN method (Rastegari et al., [2016](#bib.bib98)).
    This does not lead to a closed form solution. Hence, we approximate the solution
    with threshold-based ternary function.
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | <math id="S3.E32.m1.6" class="ltx_Math" alttext="w_{t}=\begin{cases}+1&amp;\text{if
    }w>\Delta\\ 0&amp;\text{if }-\Delta<w\leq\Delta\\'
  prefs: []
  type: TYPE_NORMAL
- en: -1&amp;\text{if }w\leq-\Delta\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics id="S3.E32.m1.6a"><mrow id="S3.E32.m1.6.7"
    xref="S3.E32.m1.6.7.cmml"><msub id="S3.E32.m1.6.7.2" xref="S3.E32.m1.6.7.2.cmml"><mi
    id="S3.E32.m1.6.7.2.2" xref="S3.E32.m1.6.7.2.2.cmml">w</mi><mi id="S3.E32.m1.6.7.2.3"
    xref="S3.E32.m1.6.7.2.3.cmml">t</mi></msub><mo id="S3.E32.m1.6.7.1" xref="S3.E32.m1.6.7.1.cmml">=</mo><mrow
    id="S3.E32.m1.6.6" xref="S3.E32.m1.6.7.3.1.cmml"><mo id="S3.E32.m1.6.6.7" xref="S3.E32.m1.6.7.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E32.m1.6.6.6"
    xref="S3.E32.m1.6.7.3.1.cmml"><mtr id="S3.E32.m1.6.6.6a" xref="S3.E32.m1.6.7.3.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S3.E32.m1.6.6.6b" xref="S3.E32.m1.6.7.3.1.cmml"><mrow
    id="S3.E32.m1.1.1.1.1.1.1" xref="S3.E32.m1.1.1.1.1.1.1.cmml"><mo id="S3.E32.m1.1.1.1.1.1.1a"
    xref="S3.E32.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E32.m1.1.1.1.1.1.1.2" xref="S3.E32.m1.1.1.1.1.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E32.m1.6.6.6c" xref="S3.E32.m1.6.7.3.1.cmml"><mrow
    id="S3.E32.m1.2.2.2.2.2.1" xref="S3.E32.m1.2.2.2.2.2.1.cmml"><mrow id="S3.E32.m1.2.2.2.2.2.1.2"
    xref="S3.E32.m1.2.2.2.2.2.1.2.cmml"><mtext id="S3.E32.m1.2.2.2.2.2.1.2.2" xref="S3.E32.m1.2.2.2.2.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E32.m1.2.2.2.2.2.1.2.1" xref="S3.E32.m1.2.2.2.2.2.1.2.1.cmml">​</mo><mi
    id="S3.E32.m1.2.2.2.2.2.1.2.3" xref="S3.E32.m1.2.2.2.2.2.1.2.3.cmml">w</mi></mrow><mo
    id="S3.E32.m1.2.2.2.2.2.1.1" xref="S3.E32.m1.2.2.2.2.2.1.1.cmml">></mo><mi mathvariant="normal"
    id="S3.E32.m1.2.2.2.2.2.1.3" xref="S3.E32.m1.2.2.2.2.2.1.3.cmml">Δ</mi></mrow></mtd></mtr><mtr
    id="S3.E32.m1.6.6.6d" xref="S3.E32.m1.6.7.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E32.m1.6.6.6e" xref="S3.E32.m1.6.7.3.1.cmml"><mn id="S3.E32.m1.3.3.3.3.1.1"
    xref="S3.E32.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left"
    id="S3.E32.m1.6.6.6f" xref="S3.E32.m1.6.7.3.1.cmml"><mrow id="S3.E32.m1.4.4.4.4.2.1"
    xref="S3.E32.m1.4.4.4.4.2.1.cmml"><mrow id="S3.E32.m1.4.4.4.4.2.1.2" xref="S3.E32.m1.4.4.4.4.2.1.2.cmml"><mtext
    id="S3.E32.m1.4.4.4.4.2.1.2.2" xref="S3.E32.m1.4.4.4.4.2.1.2.2a.cmml">if </mtext><mo
    id="S3.E32.m1.4.4.4.4.2.1.2.1" xref="S3.E32.m1.4.4.4.4.2.1.2.1.cmml">−</mo><mi
    mathvariant="normal" id="S3.E32.m1.4.4.4.4.2.1.2.3" xref="S3.E32.m1.4.4.4.4.2.1.2.3.cmml">Δ</mi></mrow><mo
    id="S3.E32.m1.4.4.4.4.2.1.3" xref="S3.E32.m1.4.4.4.4.2.1.3.cmml"><</mo><mi id="S3.E32.m1.4.4.4.4.2.1.4"
    xref="S3.E32.m1.4.4.4.4.2.1.4.cmml">w</mi><mo id="S3.E32.m1.4.4.4.4.2.1.5" xref="S3.E32.m1.4.4.4.4.2.1.5.cmml">≤</mo><mi
    mathvariant="normal" id="S3.E32.m1.4.4.4.4.2.1.6" xref="S3.E32.m1.4.4.4.4.2.1.6.cmml">Δ</mi></mrow></mtd></mtr><mtr
    id="S3.E32.m1.6.6.6g" xref="S3.E32.m1.6.7.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E32.m1.6.6.6h" xref="S3.E32.m1.6.7.3.1.cmml"><mrow id="S3.E32.m1.5.5.5.5.1.1"
    xref="S3.E32.m1.5.5.5.5.1.1.cmml"><mo id="S3.E32.m1.5.5.5.5.1.1a" xref="S3.E32.m1.5.5.5.5.1.1.cmml">−</mo><mn
    id="S3.E32.m1.5.5.5.5.1.1.2" xref="S3.E32.m1.5.5.5.5.1.1.2.cmml">1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E32.m1.6.6.6i" xref="S3.E32.m1.6.7.3.1.cmml"><mrow
    id="S3.E32.m1.6.6.6.6.2.1" xref="S3.E32.m1.6.6.6.6.2.1.cmml"><mrow id="S3.E32.m1.6.6.6.6.2.1.2"
    xref="S3.E32.m1.6.6.6.6.2.1.2.cmml"><mtext id="S3.E32.m1.6.6.6.6.2.1.2.2" xref="S3.E32.m1.6.6.6.6.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S3.E32.m1.6.6.6.6.2.1.2.1" xref="S3.E32.m1.6.6.6.6.2.1.2.1.cmml">​</mo><mi
    id="S3.E32.m1.6.6.6.6.2.1.2.3" xref="S3.E32.m1.6.6.6.6.2.1.2.3.cmml">w</mi></mrow><mo
    id="S3.E32.m1.6.6.6.6.2.1.1" xref="S3.E32.m1.6.6.6.6.2.1.1.cmml">≤</mo><mrow id="S3.E32.m1.6.6.6.6.2.1.3"
    xref="S3.E32.m1.6.6.6.6.2.1.3.cmml"><mo id="S3.E32.m1.6.6.6.6.2.1.3a" xref="S3.E32.m1.6.6.6.6.2.1.3.cmml">−</mo><mi
    mathvariant="normal" id="S3.E32.m1.6.6.6.6.2.1.3.2" xref="S3.E32.m1.6.6.6.6.2.1.3.2.cmml">Δ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E32.m1.6b"><apply id="S3.E32.m1.6.7.cmml" xref="S3.E32.m1.6.7"><apply
    id="S3.E32.m1.6.7.2.cmml" xref="S3.E32.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E32.m1.6.7.2.1.cmml"
    xref="S3.E32.m1.6.7.2">subscript</csymbol><ci id="S3.E32.m1.6.7.2.2.cmml" xref="S3.E32.m1.6.7.2.2">𝑤</ci><ci
    id="S3.E32.m1.6.7.2.3.cmml" xref="S3.E32.m1.6.7.2.3">𝑡</ci></apply><apply id="S3.E32.m1.6.7.3.1.cmml"
    xref="S3.E32.m1.6.6"><csymbol cd="latexml" id="S3.E32.m1.6.7.3.1.1.cmml" xref="S3.E32.m1.6.6.7">cases</csymbol><apply
    id="S3.E32.m1.1.1.1.1.1.1.cmml" xref="S3.E32.m1.1.1.1.1.1.1"><cn type="integer"
    id="S3.E32.m1.1.1.1.1.1.1.2.cmml" xref="S3.E32.m1.1.1.1.1.1.1.2">1</cn></apply><apply
    id="S3.E32.m1.2.2.2.2.2.1.cmml" xref="S3.E32.m1.2.2.2.2.2.1"><apply id="S3.E32.m1.2.2.2.2.2.1.2.cmml"
    xref="S3.E32.m1.2.2.2.2.2.1.2"><ci id="S3.E32.m1.2.2.2.2.2.1.2.2a.cmml" xref="S3.E32.m1.2.2.2.2.2.1.2.2"><mtext
    id="S3.E32.m1.2.2.2.2.2.1.2.2.cmml" xref="S3.E32.m1.2.2.2.2.2.1.2.2">if </mtext></ci><ci
    id="S3.E32.m1.2.2.2.2.2.1.2.3.cmml" xref="S3.E32.m1.2.2.2.2.2.1.2.3">𝑤</ci></apply><ci
    id="S3.E32.m1.2.2.2.2.2.1.3.cmml" xref="S3.E32.m1.2.2.2.2.2.1.3">Δ</ci></apply><cn
    type="integer" id="S3.E32.m1.3.3.3.3.1.1.cmml" xref="S3.E32.m1.3.3.3.3.1.1">0</cn><apply
    id="S3.E32.m1.4.4.4.4.2.1.cmml" xref="S3.E32.m1.4.4.4.4.2.1"><apply id="S3.E32.m1.4.4.4.4.2.1b.cmml"
    xref="S3.E32.m1.4.4.4.4.2.1"><apply id="S3.E32.m1.4.4.4.4.2.1.2.cmml" xref="S3.E32.m1.4.4.4.4.2.1.2"><ci
    id="S3.E32.m1.4.4.4.4.2.1.2.2a.cmml" xref="S3.E32.m1.4.4.4.4.2.1.2.2"><mtext id="S3.E32.m1.4.4.4.4.2.1.2.2.cmml"
    xref="S3.E32.m1.4.4.4.4.2.1.2.2">if </mtext></ci><ci id="S3.E32.m1.4.4.4.4.2.1.2.3.cmml"
    xref="S3.E32.m1.4.4.4.4.2.1.2.3">Δ</ci></apply><ci id="S3.E32.m1.4.4.4.4.2.1.4.cmml"
    xref="S3.E32.m1.4.4.4.4.2.1.4">𝑤</ci></apply><apply id="S3.E32.m1.4.4.4.4.2.1c.cmml"
    xref="S3.E32.m1.4.4.4.4.2.1"><ci id="S3.E32.m1.4.4.4.4.2.1.6.cmml" xref="S3.E32.m1.4.4.4.4.2.1.6">Δ</ci></apply></apply><apply
    id="S3.E32.m1.5.5.5.5.1.1.cmml" xref="S3.E32.m1.5.5.5.5.1.1"><cn type="integer"
    id="S3.E32.m1.5.5.5.5.1.1.2.cmml" xref="S3.E32.m1.5.5.5.5.1.1.2">1</cn></apply><apply
    id="S3.E32.m1.6.6.6.6.2.1.cmml" xref="S3.E32.m1.6.6.6.6.2.1"><apply id="S3.E32.m1.6.6.6.6.2.1.2.cmml"
    xref="S3.E32.m1.6.6.6.6.2.1.2"><ci id="S3.E32.m1.6.6.6.6.2.1.2.2a.cmml" xref="S3.E32.m1.6.6.6.6.2.1.2.2"><mtext
    id="S3.E32.m1.6.6.6.6.2.1.2.2.cmml" xref="S3.E32.m1.6.6.6.6.2.1.2.2">if </mtext></ci><ci
    id="S3.E32.m1.6.6.6.6.2.1.2.3.cmml" xref="S3.E32.m1.6.6.6.6.2.1.2.3">𝑤</ci></apply><apply
    id="S3.E32.m1.6.6.6.6.2.1.3.cmml" xref="S3.E32.m1.6.6.6.6.2.1.3"><ci id="S3.E32.m1.6.6.6.6.2.1.3.2.cmml"
    xref="S3.E32.m1.6.6.6.6.2.1.3.2">Δ</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E32.m1.6c">w_{t}=\begin{cases}+1&\text{if
    }w>\Delta\\ 0&\text{if }-\Delta<w\leq\Delta\\ -1&\text{if }w\leq-\Delta\\ \end{cases}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: The approximation works when we set $\Delta$ as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (33) |  | $\displaystyle\Delta^{*}=\operatorname*{argmax}_{\Delta>0}\frac{1}{&#124;I_{\Delta}&#124;}\left(\sum_{i\in
    I_{\Delta}}&#124;W_{i}&#124;\right)^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $I_{\Delta}$ is the number of weights with magnitude$>\Delta$. Again,
    this has no straightforward solution, unless we assume that original weights $W_{i}$’s
    are generated from uniform or normal distribution. When $W_{i}$’s are uniformly
    distributed in $[-a,a]$ and $\Delta$ lies in $(0,a]$, the approximated $\Delta^{*}$
    is $a/3$, which equals to $\frac{2}{3}E(W)$. When $W_{i}$’s are generated from
    normal distributions $N(0,\sigma^{2})$, the approximated $\Delta^{*}$ is 0.6$\sigma$
    which equals to 0.75$E(|W|)$. Thus, we can use the following rule of thumb for
    fast and easy computation.
  prefs: []
  type: TYPE_NORMAL
- en: '| (34) |  | $\displaystyle\Delta^{*}\approx 0.7E(W)=\frac{0.7}{n}\sum_{i=1}^{n}&#124;W_{i}&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Another way to learn the quantization step size $\Delta$ in Eq. [32](#S3.E32
    "In 3.2.2\. Trained Ternary Quantization ‣ 3.2\. Ternarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey") is to learn in a loss-aware
    manner (Hwang and Sung, [2014](#bib.bib49)), i.e., tuning it to minimize the overall
    network loss. Given a multi-layered network, we need to perform such quantization
    layer by layer in a greedy manner. We first train the network with full precision
    weights. We quantize all input data and signals of hidden layers. Next, we start
    with the weight quantizer between the input layer and the first hidden layer,
    try several step sizes around the initial step size and measure the output error
    of the network with the training set. The initial step size is determined using
    Lloyd-Max algorithm (Lloyd, [1982](#bib.bib76)). Choose the step size that minimizes
    the output error and quantize the weights. Further, we perform these steps for
    the next few layers until the output layer. Finally, the quantized neural network
    is retrained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet another way of training ternary quantization (Zhu et al., [2016](#bib.bib145))
    is to quantize weights to one of $-W_{l}^{n}$, 0, $W_{l}^{p}$ for each layer $l$,
    where $W_{l}^{n}$ and $W_{l}^{p}$ are trainable parameters, learned using back-propagation.
    First, we normalize the full-precision weights to the range [-1, +1] by dividing
    each weight by the maximum weight. During SGD, we back propagate the gradient
    to both $W_{l}^{n}$ and $W_{l}^{p}$ and to the latent full-precision weights.
    This makes it possible to adjust the ternary assignment (i.e. which of the three
    values a weight is assigned). To decide the quantization step size $\Delta_{l}$
    for a layer $l$, two heuristics can be used: (1) set $\Delta_{l}=t\times\max(|w_{l}|)$
    where $t$ is a constant and $w_{l}$ are the full precision weights in layer $l$.
    (2) maintain a constant sparsity $r$ for all layers throughout training. By adjusting
    the hyper-parameter $r$ we can obtain ternary weight networks with various sparsities.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Hybrid Ternary Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given various ternary quantization methods proposed so far, one can combine
    them and use different methods for different layers. Wang et al. (Wang et al.,
    [2018](#bib.bib128)) found that threshold ternary quantization (TTQ) (Eq. [32](#S3.E32
    "In 3.2.2\. Trained Ternary Quantization ‣ 3.2\. Ternarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey")) is preferable for
    weights in an RNN while Bernoulli Ternary Quantization (BTQ) is preferable for
    activations. This is based on the observation that in an RNN, the distribution
    of weights follows normal distribution (with different ranges across different
    weight matrices), while for activations, the range is [0,1] and most of the values
    are located near to the two poles instead of the middle of the range. In the training
    phase (where we need to store the full precision weights), ternary quantization
    of weights only saves 1.4x memory consumption but quantizing both weights and
    activations can achieve up to 16x memory savings.'
  prefs: []
  type: TYPE_NORMAL
- en: The HitNet architecture (Wang et al., [2018](#bib.bib128)) with this hybrid
    ternary quantization can be defined using these equations, where $i_{t},f_{t},o_{t}$
    are the input, forget and output gates; $x_{t}$ is input at time $t$; $c_{t}$
    is the cell output; and $h_{t}$ is the hidden layer output; $W_{x}$, $W_{h}$,
    $b_{x}$, $b_{h}$ are weights.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle i_{t},f_{t},g_{t},o_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(\text{TTQ}(W_{x})x_{t}+\text{TTQ}(b_{x})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+$ | $\displaystyle\text{TTQ}(W_{h})h_{t-1}+\text{TTQ}(b_{h}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle c_{t}$ | $\displaystyle=$ | $\displaystyle f_{t}\times
    c_{t-1}+i_{t}\times g_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (35) |  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\text{BTQ}(o_{t}\times\sigma(c_{t}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 3.3\. General Quantized Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far we discussed methods designed specifically for binary and ternary quantization.
    Now, we discuss general $k$-bit quantization methods. We will discuss (1) uniform
    quantization methods which perform equal width binning, (2) non-uniform methods
    which are closer to equal frequency binning, (3) loss-aware quantization methods,
    and (4) methods specifically designed for Transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Uniform Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Uniform $k$-bit Quantization simply splits the range of original weights into
    $2^{k}-1$ equal size intervals (Rastegari et al., [2016](#bib.bib98); Hubara et al.,
    [2017](#bib.bib48); He et al., [2016](#bib.bib41)). Refer Fig. [3](#S3.F3 "Figure
    3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models for Text: A Survey")(C).
    If original weights are in range [-1,1], they can be quantized as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (36) |  | $q_{k}(x)=2\left(\frac{\text{round}[(2^{k}-1)\left(\frac{x+1}{2}\right)]}{2^{k}-1}-\frac{1}{2}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, if entries are in range [0,1], we could use the following formula.
  prefs: []
  type: TYPE_NORMAL
- en: '| (37) |  | $\displaystyle q_{k}(x)=\frac{1}{2^{k}-1}\lfloor(2^{k}-1)x+\frac{1}{2}\rfloor$
    |  |'
  prefs: []
  type: TYPE_TB
- en: When the weights in matrix $X$ are not in the range [0,1], we can first scale
    weights as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (38) |  | $\displaystyle\tilde{X}=\frac{X-\beta}{\alpha}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha=\max(X)-\min(X)$ and $\beta=\min(X)$. After quantization, we can
    apply a reverse transform to approximate the original values. Overall, the quantized
    result can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (39) |  | $\displaystyle Q(X)=\alpha q_{k}(\tilde{X})+\beta$ |  |'
  prefs: []
  type: TYPE_TB
- en: Given any quantization function $q_{k}(x)$, one can use it for quantizing weight
    matrices of various recurrent models like RNNs, GRUs and LSTMs (Ott et al., [2016](#bib.bib90)).
    Typical inference equations for a GRU can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (40) |  | $\displaystyle z_{t}=\sigma(W_{z}.[h_{t-1},x_{t}]);r_{t}=\sigma(W_{r}.[h_{t-1},x_{t}])$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (41) |  | $\displaystyle\tilde{h_{t}}=\text{tanh}(W.[r_{t}\times h_{t-1},x_{t}]);h_{t}=(1-z_{t})h_{t-1}+z_{t}\tilde{h_{t}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Besides the matrix multiplications needed to compute $z_{t}$, $r_{t}$ and $\tilde{h_{t}}$,
    the gate structure of $\tilde{h_{t}}$ and $h_{t}$ brings in the need for element-wise
    multiplication. As $\tilde{h_{t}}$ and $h_{t}$ are also the inputs to computations
    at the next timestamp, and noting that a quantized value multiplied by a quantized
    value will have a larger bit-width, we need to insert additional quantization
    steps after element-wise multiplications. Another problem with quantization of
    GRU structure lies in the different value range of gates. The range of tanh is
    [-1, 1], which is different from the value range [0, 1] of $z_{t}$ and $r_{t}$.
    Keeping in mind these observations, the equations for a quantized GRU can be written
    as follows, after the weights $W_{z}$, $W_{r}$ and $W$ and input $x_{t}$ have
    already been quantized to [-1,1].
  prefs: []
  type: TYPE_NORMAL
- en: '| (42) |  | $\displaystyle z_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W_{z}.[h_{t-1},x_{t}])$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (43) |  | $\displaystyle r_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W_{r}.[h_{t-1},x_{t}])$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (44) |  | $\displaystyle\tilde{h_{t}}$ | $\displaystyle=$ | $\displaystyle
    tanh\left(W.\left[2q_{k}\left(\frac{1}{2}(r_{t}h_{t-1})+\frac{1}{2}\right)-1,x_{t}\right]\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (45) |  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle 2q_{k}\left(\frac{1}{2}((1-z_{t})h_{t-1}+z_{t}\tilde{h_{t}})+\frac{1}{2}\right)-1$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Following a similar method, we can also quantize LSTM networks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Balanced Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Uniform quantization is easy to implement but far from optimum when quantizing
    non-uniform data, which is believed to be the trained weights and activations
    of deep neural network. One way of performing non-uniform quantization is exponential
    quantization (Ott et al., [2016](#bib.bib90)). It quantizes the weight values
    to an integer power of 2\. If we let
  prefs: []
  type: TYPE_NORMAL
- en: '| (46) |  | $\displaystyle p=\frac{&#124;W&#124;}{2^{\lfloor\log_{2}&#124;W&#124;\rfloor}}-1$
    |  |'
  prefs: []
  type: TYPE_TB
- en: deterministic exponential quantization can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (47) |  | $\log_{2}W_{q}=\begin{cases}\lceil\log_{2}&#124;W&#124;\rceil&amp;\text{if
    }p>0.5\\ \lfloor\log_{2}&#124;W&#124;\rfloor&amp;\text{otherwise }\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, stochastic exponential quantization can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (48) |  | $\log_{2}W_{q}=\begin{cases}\lceil\log_{2}&#124;W&#124;\rceil&amp;\text{with
    prob }p\\ \lfloor\log_{2}&#124;W&#124;\rfloor&amp;\text{with prob }1-p\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Exponential quantization enables storing weights in low precision and eliminating
    multiplications. However, it still does not perform quantization in a way which
    is sensitive to the distribution of the weights. Distributions of parameters in
    neural networks are often imbalanced, such that the uniform quantization determined
    from extremal values may under utilize available bitwidth. When we quantize values,
    it may be desirable to make the quantized values have balanced distributions,
    to take full advantage of the available parameter space. Balanced quantization
    method (Zhou et al., [2017](#bib.bib144)) starts by partitioning numbers into
    $2^{k}$ bins containing roughly the same number of entries (percentiles). Refer
    Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey")(D). Each partition is then mapped to an evenly-divided interval
    in the closed interval [0, 1]. Finally, the quantization step maps intervals into
    discrete values using Eq. [36](#S3.E36 "In 3.3.1\. Uniform Quantization ‣ 3.3\.
    General Quantized Networks ‣ 3\. Quantization ‣ Compression of Deep Learning Models
    for Text: A Survey") and transforms the value range to be approximately the same
    as input.'
  prefs: []
  type: TYPE_NORMAL
- en: A naïve implementation using percentiles as thresholds would require sorting
    of weight values during each forward operation in back-propagation, which may
    slow down the training process. The $2^{k}$ evenly spaced percentiles required
    in histogram equalization can be computed from the recursive application of partitioning
    of numbers by medians. Further, the mean $\mu$ can be used to approximate the
    median $m$. Thus, we can perform approximate histogram equalization without doing
    sorting.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. KMeans based Quantization Schemes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Yet another way of performing non-uniform quantization is to decide bin boundaries
    using clustering in a static manner. In this static-KMeans method (Muller and
    Indiveri, [2015](#bib.bib85)), We first train the neural network with full-precision
    parameters. Then apply KMeans to the weights. After clustering, the value of each
    pixel is set to the value of the center of the cluster it belongs to. We also
    need to store mapping from integers to cluster centers. Given $k$ clusters, we
    only need $\log(k)$ bits to code the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: A better approach is to perform KMeans clustering during training. In this method (Han
    et al., [2015a](#bib.bib37)), multiple connections (belonging to the same cluster)
    share the same weight, and we fine-tune those shared weights. For the forward
    pass, the cluster index stored for each connection is mapped to a centroid which
    is then used as the weight. For back-propagation, during update, all the gradients
    are grouped by the cluster index and summed together, multiplied by the learning
    rate and subtracted from the shared centroids from last iteration. We use KMeans
    clustering to identify the shared weights for each layer of a trained network,
    so that all the weights that fall into the same cluster will share the same weight.
    Weights are not shared across layers. To calculate the compression rate, given
    $k$ clusters, we only need $\log_{2}k$ bits to encode the index. In general, for
    a network with $n$ connections and each connection is represented with $b$ bits,
    constraining the connections to have only $k$ shared weights will result in a
    compression rate of
  prefs: []
  type: TYPE_NORMAL
- en: '| (49) |  | $\displaystyle r=\frac{nb}{n\log_{2}k+kb}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'There are two other ways of using KMeans for non-uniform quantization: Product
    Quantization (PQ) and Residual Quantization (RQ) (Gong et al., [2014](#bib.bib31)).
    In product quantization (PQ), we partition the vector space into many disjoint
    subspaces, and perform quantization (KMeans) in each subspace. Weight matrix $W$
    is partitioned columnwise: $W=[W^{1},W^{2},...,W^{s}]$ where $W^{i}\in R^{m\times
    n/s}$ assuming $n$ is divisible by $s$. Then we perform KMeans on each submatrix
    $W^{i}$ to obtain clusters $c^{i}_{1},...,c^{i}_{k}$. Thus, we get $s$ codebooks.
    The reconstructed matrix is $\hat{W}=[\hat{W}^{1},\hat{W}^{2},...,\hat{W}^{s}]$
    where $\hat{W}^{i}_{j}$ is the closest centroid $c^{i}_{j}$. PQ can be applied
    to either the x-axis or the y-axis of the matrix. We need to store the cluster
    indexes and codebooks for each subvector. The compression rate for this method
    can be written as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (50) |  | $\displaystyle r=\frac{32mn}{32kn+log_{2}(k)ms}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Residual quantization (RQ) is similar. In RQ, we first quantize the vectors
    into k-centers. Next we find out the residuals for each data point ($w-c$) and
    perform KMeans on the residuals. Do it recursively $t$ times. Then the resultant
    weight vectors are calculated as $\hat{W}_{z}=c^{1}_{j}+c^{2}_{j}+...+c^{t}_{j}$
    given we have recursively performed $t$ iterations. We need to store all the codebooks
    for each iteration, which potentially needs large amount of memory. The compression
    rate for this method can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (51) |  | $\displaystyle r=\frac{m}{tk+log_{2}(k)tn}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 3.3.4\. Loss Aware Quantization (LAQ)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generalizing the loss aware binarization approach (Sec. [3.1.3](#S3.SS1.SSS3
    "3.1.3\. Loss Aware Binarization (LAB) ‣ 3.1\. Binarized Networks ‣ 3\. Quantization
    ‣ Compression of Deep Learning Models for Text: A Survey")) (Rastegari et al.,
    [2016](#bib.bib98)), we can perform $k$-bit quantization (Guo et al., [2017](#bib.bib35))
    by attempting to solve the following problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (52) |  | $\displaystyle\min_{\{\alpha_{i},b_{i}\}_{i=1}^{k}}\left&#124;\left&#124;w-\sum_{i=1}^{k}\alpha_{i}b_{i}\right&#124;\right&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $w\in R^{n}$ is the original weight vector, $\alpha_{i}\in R$ and $b_{i}\in\{-1,+1\}^{n}$
    are variables to be learned. This NP-hard problem can be solved using an iterative
    greedy approximation which sequentially minimizes the residue. In each iteration,
    first the residue is computed as
  prefs: []
  type: TYPE_NORMAL
- en: '| (53) |  | $\displaystyle r_{i-1}=w-\sum_{j=1}^{i-1}\alpha_{j}b_{j},$ |  |'
  prefs: []
  type: TYPE_TB
- en: and then $\alpha_{i}$ and $b_{i}$ are computed as $\alpha_{i}=\frac{1}{n}||r_{i-1}||_{1}$
    and $b_{i}=\text{sign}(r_{i-1})$. Further, refined greedy approximation (Guo et al.,
    [2017](#bib.bib35)) extends this to further decrease the quantization error. In
    the $j^{th}$ iteration after $\alpha_{j}$ and $b_{j}$ have been updated, the method
    adds one extra step to refine all computed $\{\alpha_{i}\}_{i=1}^{j}$ with the
    least squares solution as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (54) |  | $\displaystyle[\alpha_{1},...,\alpha_{j}]=((B_{j}^{T}B_{j})^{-1}B_{j}^{T}w)^{T}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $B_{j}=[b_{1},...,b_{j}]$. Typically refined greedy is more accurate than
    the greedy approach. In refined greedy approximation, after modification on the
    computed $\alpha$’s, $b$’s are no longer optimal while the method keeps all of
    them fixed. To improve the refined greedy approximation, alternating minimizing
    $\alpha$’s and $b$’s becomes a natural choice. Xu et al. (Xu et al., [2018](#bib.bib133))
    find that only two alternating cycles is good enough to find high precision quantization.
    Further, similar to (Rastegari et al., [2016](#bib.bib98)), for an LSTM, we can
    combine overall network loss minimization with the multi-bit quantization loss
    minimization using this bi-level optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '| (55) |  | $\displaystyle\min_{w,\{\alpha_{i},b_{i}\}_{i=1}^{k}}\text{LSTM}\left(\sum_{i=1}^{k}\alpha_{i}b_{i}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (56) |  | $\displaystyle\text{such that }\{\alpha_{i},b_{i}\}_{i=1}^{k}=\operatorname*{argmin}_{\{\alpha_{i}^{\prime},b_{i}^{\prime}\}_{i=1}^{k}}\left&#124;\left&#124;w-\sum_{i=1}^{k}\alpha_{i}^{\prime}b_{i}^{\prime}\right&#124;\right&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 3.3.5\. Quantization for Word Embeddings and Transformers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each word vector is typically represented as a 300–500 dimensional vector, with
    each parameter being 32 bits. As there are millions of words, word vectors may
    take up to 3–6 GB of memory/storage. Can we quantize word vectors? We can clearly
    quantize them after training. But, we could also quantize when learning word embeddings.
    For example, Lam et al. (Lam, [2018](#bib.bib60)) perform 1-bit and 2-bit quantization
    while performing word2vec (Mikolov et al., [2013](#bib.bib81)) training using
    the Continuous Bag of Words (CBOW) method. They observe that quantization while
    training leads to better results compared to quantization after training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) applied BS-Fixed and
    BS-Flexible binary quantization to Transformer models. They observed that the
    Transformer architecture is highly resistant to quantization, and is able to match
    the original model up to a 4-bit representation. Simple iterative pruning is much
    worse compared to quantization. Lastly, Shen et al. (Shen et al., [2019](#bib.bib105))
    propose mixed-precision quantization for BERT based on the observation that different
    encoder layers should use different number of bits for quantization. Layers that
    exhibit flatter curvature of the loss gradient surface can be quantized to lower
    bit precision. Thus, they use different number of bits at different levels of
    granularity: layers, attention heads and groups of neurons. They observe that
    quantizing embedding layers with 8 bits and other weight matrices with 2–4 bits
    leads to results comparable with full-precision BERT.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Task | Dataset | Model | Method | Eval | Bits (weights) | Bits (activation)
    | Metric |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | IMDB | GRU | Uniform Q. (He et al., [2016](#bib.bib41))
    | 0.882; 0.905 | 4 | 4 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Linux Kernel | LSTM | BinaryConnect (Courbariaux et al.,
    [2015](#bib.bib18)) | 3.532; 1.329 | 1 | FP | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Linux Kernel | LSTM | Loss Aware B. (Hou et al., [2016](#bib.bib46))
    | 1.305/1.409; 1.329 | 1 | FP/1 | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Linux Kernel | LSTM | BNN (Hubara et al., [2017](#bib.bib48))
    | 3.624; 1.329 | 1 | 1 | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Linux Kernel | LSTM | BWN (Rastegari et al., [2016](#bib.bib98))
    | 1.307; 1.329 | 1 | FP | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | GRU | Uniform Q. (He et al., [2016](#bib.bib41))
    | 102; 100 | 4 | 4 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | GRU | Balanced Q. (Zhou et al., [2017](#bib.bib144))
    | 116; 100 | 4 | 4 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Greedy (Guo et al., [2017](#bib.bib35))
    | 118.9; 89.8 | 2 | FP | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Refined Loss Aware (Guo et al., [2017](#bib.bib35))
    | 95.6; 89.8 | 2 | 3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Uniform Q. (He et al., [2016](#bib.bib41))
    | 152/114; 109 | 2/4 | 2/4 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | BNN (Hubara et al., [2017](#bib.bib48))
    | 100; 97 | 4 | 4 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | HitNet (Wang et al., [2018](#bib.bib128))
    | 110.3; 97.2 | 2 | 2 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 103.1/91.4; 89.8 | 2/4 | FP | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 91.9; 89.8 | 2 | 3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | LSTM | Balanced Q. (Zhou et al., [2017](#bib.bib144))
    | 126/123; 106 | 2 | 2/3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Text-8 | LSTM | Refined Loss Aware (Guo et al., [2017](#bib.bib35))
    | 122.3; 101.1 | 2 | 3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Text-8 | LSTM | HitNet (Wang et al., [2018](#bib.bib128))
    | 169.1; 151.4 | 2 | 2 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Text-8 | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 105.1; 101.1 | 2 | 3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Text-8 | RNN | Exponential Q. (Ott et al., [2016](#bib.bib90))
    | 1.639; 1.588 | 2 | FP | BPC (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | War and Peace | LSTM | BinaryConnect (Courbariaux et al.,
    [2015](#bib.bib18)) | 2.942; 1.268 | 1 | FP | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | War and Peace | LSTM | Loss Aware B. (Hou et al., [2016](#bib.bib46))
    | 1.291/1.376; 1.268 | 1 | FP/1 | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | War and Peace | LSTM | BNN (Hubara et al., [2017](#bib.bib48))
    | 3.05; 1.268 | 1 | 1 | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | War and Peace | LSTM | BWN (Rastegari et al., [2016](#bib.bib98))
    | 1.313; 1.268 | 1 | FP | CE (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikidata-2 | LSTM | HitNet (Wang et al., [2018](#bib.bib128))
    | 126.72; 114.37 | 2 | 2 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | WikiText-2 | LSTM | Refined Loss Aware (Guo et al., [2017](#bib.bib35))
    | 105.8; 114.37 | 2 | 3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | WikiText-2 | LSTM | Alternating LAQ (Xu et al., [2018](#bib.bib133))
    | 102.7; 100.1 | 2 | 3 | PPW (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Named Entity Recognition | CoNLL-03 | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 91.06; 95 | 2w8e | 8 | F1 (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Named Entity Recognition | CoNLL-03 | BERT-base | Mixed-precision Q. (Shen
    et al., [2019](#bib.bib105)) | 94.37; 95 | 2-3w8e | 8 | F1 (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | BS-Fixed/BS-Flexible (Cheong
    and Daniel, [2019](#bib.bib16)) | 11.61/12.11; 28.09 | 1 | FP | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | K-Means 1/4-bit Q. (Cheong
    and Daniel, [2019](#bib.bib16)) | 12.07/27.65; 28.09 | 1 | FP | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | K-Means 1-bit att-Q. (Cheong
    and Daniel, [2019](#bib.bib16)) | 24.96; 28.09 | 1 | FP | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT17 | Transformer | BS-Flexible 1-bit att-Q. (Cheong
    and Daniel, [2019](#bib.bib16)) | 25.54; 28.09 | 1 | FP | BLEU (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 79.6; 88.69 | 2w8e | 8 | F1 (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 86.95; 88.69 | 2-3w8e | 8 | F1 (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD | Facebook’s DrQA | BS-Fixed (Lam, [2018](#bib.bib60))
    | 77.04; 75.28 | 2 | FP | F1 (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | LSTM | Gaussian Q. (Alom et al., [2018](#bib.bib2))
    | 79.64; 82.87 | 1 | FP | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | LSTM | Gaussian T./B. (Alom et al., [2018](#bib.bib2))
    | 76.86/76.25; 82.87 | 2 | FP | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST-2 | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 84.63; 93 | 2w8e | 8 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST-2 | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 92.08; 93 | 2-3w8e | 8 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | TIDIGITS | GRU | Pow2 T. (Ott et al., [2016](#bib.bib90))
    | 99.18; 99.1 | 2 | FP | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | TIMIT | 4-layer MLP | Loss Aware T. (Hwang and Sung,
    [2014](#bib.bib49)) | 29.97/28.35; 26.24 | 1/2 | 1 | FER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | WSJ | 4-layer BiLSTM | Pow2 T. (Ott et al., [2016](#bib.bib90))
    | 10.49; 11.16 | 2 | FP | WER (L) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 77.02; 84.4 | 2w8e | 8 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 82.29; 84.4 | 2-3w8e | 8 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI-m | BERT-base | QBERT (Shen et al., [2019](#bib.bib105))
    | 76.56; 84 | 2w8e | 8 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Textual entailment | MNLI-m | BERT-base | Mixed-precision Q. (Shen et al.,
    [2019](#bib.bib105)) | 81.75; 84 | 2-3w8e | 8 | Acc (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | M. Turk | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.602; 0.617 | 2 | FP | CHR (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | MEN | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.764; 0.745 | 2 | FP | CHR (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | Rare Words | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.362; 0.4 | 2 | FP | CHR (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | SimLex | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.387; 0.358 | 2 | FP | CHR (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | WordSim Relatedness | Word embeddings | BS-Fixed (Lam,
    [2018](#bib.bib60)) | 0.594; 0.529 | 2 | FP | CHR (H) |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | WordSim Similarity | Word embeddings | BS-Fixed (Lam, [2018](#bib.bib60))
    | 0.752; 0.741 | 2 | FP | CHR (H) |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Comparison of various quantization methods (sorted by Task and then
    Dataset). Q.=Quantization, B.=Binarization, T.=Ternarization, PPW=Perplexity per
    word, BPC=Bits per character, CE=cross-entropy, FER=frame error rate, CHR=correlation
    with human rankings. FP=full precision (32 bits). For (Lam, [2018](#bib.bib60)),
    we report results with word embedding dimensions set to 1000\. In the metric column,
    H means high is better while L means low is better. For quantization of the BERT-base
    model (Shen et al., [2019](#bib.bib105)), we report number of bits used for encoders
    as well as for embeddings. ‘2-3w8e’ means 2 or 3 bits were used for encoder weights
    while 8 bits were used for embeddings. For NMT results by Cheong et al. (Cheong
    and Daniel, [2019](#bib.bib16)), “att-Q” means only attention layers were quantized.
  prefs: []
  type: TYPE_NORMAL
- en: Ott et al. (Ott et al., [2016](#bib.bib90)) observed that the weight binarization
    methods do not work with RNNs. Hubara et al. (Hubara et al., [2017](#bib.bib48))
    were the first to attempt to quantize both weights and activations by trying to
    evaluate the accuracy of quantized recurrent models trained on the Penn Treebank
    dataset. Similar to (Ott et al., [2016](#bib.bib90)), Hubara et al. (Hubara et al.,
    [2017](#bib.bib48)) found that binarization of weight matrices lead to large accuracy
    degradation. Later techniques like the one by Xu et al. (Xu et al., [2018](#bib.bib133))
    with 2 bits for weights and 3 bits for activations showed better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3.4\. Summary ‣ 3\. Quantization ‣ Compression
    of Deep Learning Models for Text: A Survey") compares various quantization methods
    across different tasks and datasets. Accuracy of both the original and the quantized
    model are shown. Also, we report number of bits used for weights (which indicate
    the model size) as well as activations. For the same task, dataset and model combination,
    different papers report different accuracy of the full precision model because
    of slight changes in training hyper-parameters; hence we report accuracy of full
    precision model for each row.'
  prefs: []
  type: TYPE_NORMAL
- en: For language modeling, PTB, Text-8, WikiText-2, Linux Kernel, IMDB and “War
    and Peace” are the popular datasets. Across all the datasets, loss aware binarization
    outperforms other weight binarization schemes. On the Linux Kernel dataset, it
    is even better than the full-precision network. BinaryConnect does not work well
    here because of the problem of exploding gradients. On PTB, Xu et al.’s Alternating
    LAQ (Xu et al., [2018](#bib.bib133)) with 2 bits for weights and 3 bits for activations
    leads to an LSTM which is just 2.1 points worse in terms of perplexity per word.
    By 3-bit quantization, Alternating LAQ can achieve $\sim$10.5x memory saving and
    $\sim$3× real inference acceleration. Uniform and Balanced quantization are rule-based
    and not aim at minimizing the error. Balanced quantization proposed by Zhou et
    al. (Zhou et al., [2017](#bib.bib144)) performs better than HitNet (Wang et al.,
    [2018](#bib.bib128)) and uniform quantization (He et al., [2016](#bib.bib41)).
    Balanced quantization leads to better results compared to unbalanced counterparts,
    especially when quantizing to 2-bit weights. However, for 4-bit weights, there
    is no clear gap between scaling by mean and scaling by max (i.e. balanced and
    unbalanced quantization).
  prefs: []
  type: TYPE_NORMAL
- en: Across multiple tasks like named entity recognition with CoNLL-03, question
    answering with SQuAD, sentiment analysis with SST-2, textual entailment using
    MNLI, Shen et al. (Shen et al., [2019](#bib.bib105)) show that mixed precision
    quantization (where different number of bits are used for different groups of
    neurons – 128 groups in each layer) of BERT is better than QBERT. The reason behind
    this is the discrepancy (in QBERT) that not all the layers have the same sensitivity
    to quantization. For more sensitive layers, higher bit precision needs to be set,
    while for layers that are less sensitive, 2-bits setting is already sufficient.
    With only additional 5MB memory storage, 2/3-bits mixed-precision Q-BERT is able
    to retain the performance drop within 2.3% for MNLI, SQuAD and 1.1% for SST-2,
    CoNLL-03, with up to 13x compression ratio in weights. For machine translation,
    Cheong et al. (Cheong and Daniel, [2019](#bib.bib16)) observe that Transformer
    architecture is highly resistant to quantization (unlike to pruning), and are,
    in essence, able to match the original model up to a 4-bit representation. Binary
    Scheme Flexible outperforms 1-bit k-means in both pure 1-bit compression and quantizing
    only the attention layers, suggesting not tying the weights to particular centroids
    improves performance, and outperform Binary Scheme Fixed, indicating learning
    the values to be a superior method. After binarizing only the attention layers,
    we are still able to recover 90% of the model’s performance. Lam et al. (Lam,
    [2018](#bib.bib60)) experimented with quantization of word embeddings and showed
    that 2-bit quantized word vectors outperform full precision vectors on word similarity
    tasks, but do worse on word analogy tasks. Intuitively, they reason that full
    precision Word2Vec is prone to overfitting with increased epochs of training;
    quantized training does not seem to suffer as much from this. For sentiment analysis
    on IMDB, Alom et al. (Alom et al., [2018](#bib.bib2)) show that quantization to
    4 bits is better than to 3 or 2 bits, which is expected. They also show that the
    normal distribution shows better performance against uniform distribution with
    quantized weights. For speech recognition, Ott et al. (Ott et al., [2016](#bib.bib90))
    show that pow-2 ternarization is the best.
  prefs: []
  type: TYPE_NORMAL
- en: Cheong et al (Cheong and Daniel, [2019](#bib.bib16)) were the first to quantize
    Transformers. They observed that in the last attention layer of the decoder over
    the encoder hidden states, the attention distribution of the original and 4-bit
    model are highly similar, indicating 4 bit weights, i.e weights that take on one
    of 16 values, is enough to get the full effects of attention. Attention distributions
    in the encoder layers of the Transformer for the original and 4-bit models are
    almost indistinguishable from one another. This again highlights the idea that
    self-attention is highly resistant to quantization and could be heavily compressed.
    Later Shen et al. (Shen et al., [2019](#bib.bib105)) showed that comparable performance
    to full precision BERT can be achieved with at most 2.3% performance degradation
    across many tasks, even with ultra-low precision quantization down to 2 bits,
    corresponding up to 13x compression of the model parameters, and up to 4x compression
    of the embedding table as well as activations.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, quantization performs model compression by reducing the number of bits
    per weight value. Binary quantization does not work well by itself for text based
    neural models. But ternary and higher-bit quantization lead to significant model
    size reduction without loss in accuracy across tasks. One consideration for quantization
    is that 3-bit quantized execution is typically not supported in hardware. It is
    however possible to load 3-bit quantized values and cast them to higher bit precision
    such as 4 or 8 bits in the execution units. This would still have the benefit
    of reduced memory volume to/from DRAM. Non-uniform quantization methods like balanced
    quantization or KMeans based quantization methods are better than uniform quantization
    methods. Loss aware quantization done while training is better than static loss-unaware
    quantization. Mixed-precision quantization combined with pruning is highly effective
    for Transformer based models.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Knowledge Distillation (KD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KD methods are the most popular model compression methods for Transformer networks.
    Also called student-teacher networks, the main idea is to first train a deep teacher
    network, and then learn a shallow student network that mimics the teacher. After
    training, the student model is deployed. What information (“dark knowledge”) from
    the teacher can be used to train the student? What loss functions can be used
    to ensure right flow of information from teacher to student? Can we have an ensemble
    of teachers, or teacher assistants or rather fellow students who can train the
    student? We discuss these aspects in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Various Distillation Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ba and Caruna (Ba and Caruana, [2014](#bib.bib4)) proposed Student Teacher
    networks (or mimic models) where the student uses the logits before softmax from
    the teacher network for training (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various
    Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of
    Deep Learning Models for Text: A Survey")(A)). The student model is not trained
    on the original labels; it is trained to learn the function that was learned by
    the teacher model. Thus, the student model is optimized to minimize the L2 loss
    between the teacher logits and the student logits across all training instances.
    Such distilled student models are more accurate than the same shallow student
    trained directly on the original labeled training data mainly because: (1) Teacher
    removes noisy labels, if any. (2) The uncertainty from the teacher is more informative
    to the student than the original 0/1 labels. (3) The original targets may depend
    in part on features not available as inputs for learning, but the student sees
    targets that depend only on the input features. The dependence on unavailable
    features has been eliminated by filtering targets through the teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way of utilizing logits is to have the student learn from noisy
    teacher logits (Sau and Balasubramanian, [2016](#bib.bib103)). After obtaining
    logits from the teacher, Gaussian noise with mean 0 and standard deviation $\sigma$
    is added to teacher’s logits. This perturbation can be applied to samples selected
    with probability $\alpha$. The perturbed outputs produce the effect of a regularizer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5f5552f20332eafbca8058c6457e48d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Different Types of Knowledge Distillation methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Ba and Caruna (Ba and Caruana, [2014](#bib.bib4)) suggested using only
    the logits, Hinton et al. (Hinton et al., [2015](#bib.bib44)) suggested training
    the student by minimizing the cross entropy loss between the teacher softmax output
    and the student softmax output, besides minimizing the cross entropy between student
    prediction and actual label. The first part is called the soft loss and the second
    one is called the hard loss. Typically hard loss is given much lower weight compared
    to the soft loss term. To make the softmax output non-peaked and thereby transfer
    more useful information from teacher to student, softmax with temperature $>$1
    should be used. The same temperature should be used for training both the teacher
    and the student, but after the student has been trained the temperature can be
    set to 1 at test time. Besides logits and softmax output, Sobolev training for
    neural networks is a method for incorporating target derivatives in addition to
    the target values while training student network (see Fig. [4](#S4.F4 "Figure
    4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey")(C)). Czarnecki et al. (Czarnecki
    et al., [2017](#bib.bib19)) experiment with first two derivatives of the targets.'
  prefs: []
  type: TYPE_NORMAL
- en: KD has also been used along with quantization for better model compression (Bengio
    et al., [2013](#bib.bib8); Mishra and Marr, [2017](#bib.bib83); Polino et al.,
    [2018](#bib.bib92)). We start with a trained full-precision large teacher network
    and an apprentice (student) network that has been initialised with full-precision
    weights. The apprentice network’s precision is lowered and is fine-tuned using
    KD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why just use the output from the last layer of the teacher for training the
    student? In FitNets (Romero et al., [2014](#bib.bib99)), the student performs
    hint-based training, i.e., the student is trained using not only the outputs but
    also the intermediate representations learned by the teacher as hints to improve
    the training process and final performance of the student (see Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation
    (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(B)). we choose
    a hidden layer of the FitNet, the guided layer, to learn from the teacher’s hint
    layer. Because the student intermediate hidden layer will generally be smaller
    than the teacher’s intermediate hidden layer, additional parameters are introduced
    to map the student hidden layer to the prediction of the teacher hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the methods discussed so far use logits, softmax output or their derivatives
    to transfer knowledge, Yim et al. (Yim et al., [2017](#bib.bib138)) proposed a
    “flow of solution procedure (FSP)” method where the distilled knowledge is transferred
    in terms of flow between layers, which is calculated by computing the inner product
    between features from two layers (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various
    Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of
    Deep Learning Models for Text: A Survey")(D)). What does this “flow” capture intuitively?
    If we view the input of a deep network as the question and the output as the answer,
    we can think of the generated features at the middle of the network as the intermediate
    result in the solution process. There are many ways to solve the problem of generating
    the output from the input. Hence, mimicking the generated features of the teacher
    can be a hard constraint for the student. Learning the solution process from teacher
    is important. More concretely, the student is trained to minimize the L2 difference
    between the teacher and student FSP matrices computed across various pairs of
    layers and across multiple training instances. A similar method called Representational
    distance learning (RDL) has also been proposed in (McClure and Kriegeskorte, [2016](#bib.bib79)).'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, multiple KD variants have been proposed for sequence-level predictions (Kim
    and Rush, [2016](#bib.bib57); Freitag et al., [2017](#bib.bib30)), e.g., for neural
    machine translation (NMT). In word-level KD, cross-entropy is minimized between
    the student/teacher distributions for each word in the actual target sequence,
    as well as between the student distribution and the degenerate data distribution,
    which has all of its probability mass on one word. In sequence-level KD (Seq-KD)
    the student network is trained on the output from beam search of the teacher network
    that had the highest score. In sequence-level interpolation (Seq-Inter) the student
    is trained on the output from beam search of the teacher network that had the
    highest similarity (say using BLEU score) with the target sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Collaborative Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Can multiple students learn from each other? Is a powerful teacher really required?
    In the deep mutual learning (DML) method (Zhang et al., [2018](#bib.bib142)),
    different from the one-way transfer between a static pre-defined teacher and a
    student in model distillation, with DML, an ensemble of students learn collaboratively
    and teach each other throughout the training process. Surprisingly, no prior powerful
    teacher network is necessary – mutual learning of a collection of simple student
    networks works, and moreover outperforms distillation from a more powerful yet
    static teacher. Specifically, each student is trained with two losses: a conventional
    supervised learning loss, and a mimicry loss that aligns each student’s class
    posterior with the class probabilities of other students.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Anil et al. (Anil et al., [2018](#bib.bib3)) propose a similar method but suggest
    letting the students learn independently just using the conventional supervised
    learning (hard) loss at least for a few burn in iterations (see Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation
    (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(E)). After this,
    the mutual learning can be done as in DML. They also propose a variant of their
    Co-Distillation method to perform this training in a distributed scenario where
    communication efficiency is also important. To update the parameters of one network
    using co-distillation one only needs the predictions of the other networks, which
    can be computed locally from copies of the other networks’ weights. Empirically,
    using stale predictions instead of up-to-date predictions for the other neural
    networks has little to no adverse effect on the quality of the final trained model
    produced by co-distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Multiple Teachers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far we have talked about a student mimicing a single teacher. However, it
    is interesting to explore if the student can learn better in presence of multiple
    teachers or from a teacher assistant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively and also observed empirically, student network performance degrades
    when the gap between student and teacher is large. Given a fixed student network,
    one cannot employ an arbitrarily large teacher, or in other words, a teacher can
    effectively transfer its knowledge to students up to a certain size, not smaller.
    To alleviate this shortcoming, Mirzadeh et al. (Mirzadeh et al., [2019](#bib.bib82))
    introduced multi-step KD, which employs an intermediate-sized network (teacher
    assistant) to bridge the gap between the student and the teacher (see Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1\. Various Distillation Architectures ‣ 4\. Knowledge Distillation
    (KD) ‣ Compression of Deep Learning Models for Text: A Survey")(F)). The teacher
    assistant (TA) models are distilled from the teacher, and the student is then
    only distilled from the TAs. One could also perform multi-step TA distillation,
    for example, distillation path could be $10\rightarrow 6\rightarrow 4\rightarrow
    2$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple way to do KD with multiple teachers is to train student with cross
    entropy loss between student predictions and average prediction from multiple
    teachers (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(G)). A more effective method is to augment this with a relative dissimilarity
    (RD) loss (You et al., [2017](#bib.bib139)) defined over intermediate layer outputs
    generated for a triplet of instances between the student and an ensemble of teachers.
    For the student, the middle layer is selected. For each teacher, we select the
    layer such that most teachers are consistent with the resulting order relationships
    under the voting strategy. We discuss the RD loss given a student and a teacher.
    Consider a triplet of instances ($x_{i}$, $x_{i}^{+}$, $x_{i}^{-}$) such that
    at an intermediate layer of the teacher network, distance between activations
    for $x_{i}^{+}$ and $x_{i}$ is smaller than the distance between activations for
    $x_{i}^{-}$ and $x_{i}$. Let $p_{i}$ be the intermediate output from student for
    example $x_{i}$. Then the RD loss for the triplet ($x_{i}$, $x_{i}^{+}$, $x_{i}^{-}$)
    can be written as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (57) |  | $\displaystyle\text{Loss}=\max(0,d(p_{i},p_{i}^{+})-d(p_{i},p_{i}^{-})+\delta)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $d$ is the distance function, and $\delta$ is a small constant to prevent
    the trivial solution. To extend this loss function definition to multiple teachers,
    the order between the instances $x_{i}^{+}$ and $x_{i}^{-}$ given $x_{i}$ is decided
    based on majority voting between the teachers.
  prefs: []
  type: TYPE_NORMAL
- en: There are also specific settings when distilling from multiple teachers becomes
    natural, e.g., when the number of classes is large (Hinton et al., [2015](#bib.bib44))
    or in multi-lingual settings (Tan et al., [2019](#bib.bib114)). When the number
    of classes is very large, the teacher model could be an ensemble that contains
    one generalist model trained on all the data and many “specialist” models, each
    of which is trained on data that is highly enriched in examples from a very confusable
    subset of the classes (like different types of mushroom). Softmax distribution
    vector of this type of specialist can be made much smaller by combining all of
    the classes it does not care about into a single dustbin class. Each specialist
    model is initialized with the weights of the generalist model. These weights are
    then slightly modified by training the specialist with half its examples coming
    from its special subset and half sampled at random from the remainder of the training
    set. To derive groupings of object categories for the specialists, we focus on
    categories that the full generalist network often confuses. When training the
    student, for each instance, we first find the $set$k$ofn$ most probable classes
    according to the generalist model. Then, we take all the specialist models, $m$,
    whose special subset of confusable classes has a non-empty intersection with $k$
    and call this the active set of specialists $A_{k}$. Given student’s full probability
    distribution $q$ over all the classes, we minimize the following.
  prefs: []
  type: TYPE_NORMAL
- en: '| (58) |  | $\displaystyle\text{Loss}=KL(p^{g},q)+\sum_{m\in A_{k}}KL(p^{m},q)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $p^{g}$ is output distribution from the generalist model, and $p^{m}$
    is the output distribution from the $m^{th}$ specialist model.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of teachers is also very useful in a multi-lingual NMT setting (Tan
    et al., [2019](#bib.bib114)). Individual models for each language pair are first
    trained and regarded as teachers, and then the multilingual model is trained to
    fit the training data and match the outputs of individual models simultaneously
    through KD. When the accuracy of multilingual model surpasses the individual model
    for the accuracy threshold $\tau$ on a certain language pair, we remove the distillation
    loss and just train the model with original negative log-likelihood loss for this
    pair. Lastly, when learning from a teacher ensemble, it is burdensome to load
    all the teacher models in the GPU memory for distillation. Alternatively, we first
    generate the output probability distribution of each teacher model for each instance
    offline, and then just load the top-$K$ probabilities of the distribution into
    memory and normalize them so that they sum to 1 for distillation. This reduces
    the memory cost from the scale of $|V|$ (the vocabulary size) to $K$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Distilling Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, there has been a lot of work around distilling Transformers to smaller
    Transformers with less number of layers or to Bidirectional LSTMs. Some of these
    methods aim at improving the accuracy versus model size tradeoff, while others
    focus on complex settings like mismatching student-teacher vocabulary (Zhao et al.,
    [2019](#bib.bib143)) or mismatch number of attention heads.
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. (Zhao et al., [2019](#bib.bib143)) learn a student with small vocabulary
    compared to the teacher using a dual training method. During distillation, for
    a given training sequence input to the teacher model, they mix the teacher and
    student vocabularies by randomly selecting tokens from the sequence to segment
    using the student vocabulary, with the other tokens segmented using the teacher
    vocabulary. As part of the masked language model (MLM) task, the model now needs
    to learn to predict words from the student vocabulary using context words segmented
    using the teacher vocabulary, and vice versa. The expectation is that the student
    embeddings can be learned effectively this way from the teacher embeddings as
    well as teacher model parameters. We perform dual training only for the teacher
    model inputs. The student model receives words segmented exclusively using the
    student vocabulary. Also, during MLM, the model uses different softmax layers
    for the teacher and the student vocabularies depending on which one was used to
    segment the word in question. Instead of distilling solely on the teacher model’s
    final-layer outputs, layer-wise teacher model parameters can also be leveraged
    to directly optimize parameters of corresponding layers in the student model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Patient KD (PKD) (Sun et al., [2019](#bib.bib111)), the student learns from
    the teacher’s output after every $k$ layers (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\.
    Various Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression
    of Deep Learning Models for Text: A Survey")(H)) or the output from the last few
    layers of the teacher (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation
    Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning
    Models for Text: A Survey")(I)). The student BERT is initialized using some layers
    of the pre-trained teacher BERT. TinyBERT (Jiao et al., [2019](#bib.bib51)) further
    extends this idea by using extensive knowledge from embedding layer, and attention
    and hidden sub-layers of multiple teacher layers, and also the overall teacher
    output (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(J)). Each student layer is first mapped to a teacher layer before the
    student training. Liu et al. (Liu et al., [2019a](#bib.bib74)) distill a multi-task
    student from a multi-task teacher, given the soft targets of the training data
    across multiple tasks. If task $t$ has a teacher, the task-specific loss is the
    average of two objective functions, one for the correct targets and the other
    for the soft targets assigned by the teacher. In MiniLM (Wang et al., [2020b](#bib.bib130)),
    the student is trained by deeply mimicking the self-attention behavior of the
    last Transformer layer of the teacher (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various
    Distillation Architectures ‣ 4\. Knowledge Distillation (KD) ‣ Compression of
    Deep Learning Models for Text: A Survey")(K)). Besides self-attention distributions,
    MiniLM introduces the self-attention value-relation transfer to help the student
    achieve a deeper mimicry. The value-relation is computed as pairwise correlation
    between different components of the value matrix across various attention heads
    of the final layer. Pretrained Distillation (Turc et al., [2019](#bib.bib120))
    pretrains the student model with a self-supervised masked LM objective on a large
    corpus first, then performs a standard KD on supervised tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of these models learn one-to-one layer mapping, where each student layer
    is guided by only one specific teacher layer. Li et al. (Li et al., [2020](#bib.bib67))
    propose a method where each student intermediate layer learns from every teacher
    intermediate layer with learnable attention weights. Both the embedding-layer
    distillation and the prediction-layer distillation employ the one-to-one layer
    mapping as in TinyBERT and BERT-PKD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tang et al. (Tang et al., [2019](#bib.bib115)) propose distillation of a BERT
    model to a single layer BiLSTM using KL divergence between student and teacher
    logits (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1\. Various Distillation Architectures
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey")(L)). Mukherjee et al. (Mukherjee and Awadallah, [2020](#bib.bib84))
    also distill a multi-lingual BERT (mBERT) model to a BiLSTM. Representation transfer
    is done from Transformer-based teacher model to BiLSTM-based student model with
    different embedding dimensions and disparate output spaces. Distillation features
    include teacher logits and internal teacher representations for one teacher layer.
    To make all output spaces compatible, a non-linear projection of the parameters
    in student representation is done to have same shape as teacher representation
    for each token. The projection parameters are learned by minimizing the KL-divergence
    (KLD) between the representations of the student and the chosen layer from the
    teacher. Overall there are multiple loss functions for the student training: supervised
    hard loss, soft loss wrt output logits, and soft loss wrt internal teacher layer.
    Rather than optimizing for all loss functions jointly, stage-wise training is
    performed where each loss function is sequentially used for optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, there have been recent efforts (Sun et al., [2020](#bib.bib112); Iandola
    et al., [2020](#bib.bib50)) to distill Transformer models to slightly modified
    Transformer architectures. MobileBERT (Sun et al., [2020](#bib.bib112)) is a thin
    version of BERT-large, while equipped with bottleneck structures and a carefully
    designed balance between self-attentions and feed-forward networks (FFN). To train
    MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck
    incorporated BERT-large model (IB-BERT). The IB-BERT uses the inverted-bottleneck
    structures to adjust its feature map size to 512\. Thus, in the bottleneck structure,
    the inputs to the multi-head attention (MHA) module are from wider feature maps
    (of inter-block size), while the inputs to the FFN are from narrower bottlenecks
    (of intra-block size). To fix this issue, MobileBERT uses stacked feed-forward
    networks to re-balance the relative size between MHA and FFN. Each MobileBERT
    layer contains one MHA but 4 stacked FFN after each MHA. Then, we conduct knowledge
    transfer from this teacher to MobileBERT using feature map transfer and attention
    transfer across all layers. Also, while distilling they perform progressive knowledge
    transfer, i.e., they progressively train each layer in $L$ stages, where $L$ is
    the number of layers. When the $l$-th layer is trained, all the trainable parameters
    in the layers below are frozen. Another difference in the MobileBERT student architecture
    is usage of high information flow residual connections between the high-channel-count
    layers. MobileBERT is 4.3x smaller and 5.5x faster than BERT-base while achieving
    competitive results on well-known benchmarks. Iandola et al. (Iandola et al.,
    [2020](#bib.bib50)) propose a new Transformer model architecture called SqueezeBERT
    which is much like BERT-base, but with the position-wise fully connected layers
    implemented as convolutions, and grouped convolutions for many of the layers.
    Distillation for SqueezeBERT is rather straightforward. Distillation is applied
    only to the final layer, and only during finetuning using soft cross entropy loss
    with respect to a weighted sum of the teacher’s logits and a one-hot encoding
    of the ground-truth. Teacher model is a BERT-base model finetuned independently
    on each GLUE task, and these task-specific teacher weights are used for distillation.
    Xu et al. (Xu et al., [2020](#bib.bib134)) propose a method for progressive module
    replacement for compressing BERT. Their approach first divides the original BERT
    into several modules and builds their compact substitutes. Then, the original
    modules are randomly replaced with their substitutes to train the compact modules
    to mimic the behavior of the original modules. We progressively increase the probability
    of replacement through the training. In this way, their approach brings a deeper
    level of interaction between the original and compact models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Task | Dataset | Teacher/Student models | Method | Metric | Size (Distilled;
    Orig) | Eval. (Distilled; Orig) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Abs. summarization | CNN/DailyMail | UniLM-large/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | ROUGE-L (H) | 33M; 340M | 39.73; 40.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Ad CTR prediction | Criteo | No teacher/DNN | CoDistillation (Anil et al.,
    [2018](#bib.bib3)) | MAE (L) | - | 0.019; 0.022 |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-lingual NLI | XNLI (15 langs.) | XLM-R-base/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | Acc (H) | 21M; 85M | 71.1; 74.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-lingual QA | MLQA (7 langs.) | XLM-R-base/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | F1 (H) | 21M; 85M | 63.2; 64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Intent detection | SNIPS (7-class) | BERT-base/6-layer BERT | Mixed-vocabulary
    training (Zhao et al., [2019](#bib.bib143)) | Acc (H) | 6.2M; 109M | 98.7; 98.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Common Crawl | No teacher/2-layer LSTM | CoDistillation (Anil
    et al., [2018](#bib.bib3)) | CE (L) | - | 3.91; 3.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Machine reading comp. | RACE | BERT-base/6-layer BERT | Patient KD (Sun et al.,
    [2019](#bib.bib111)) | Acc (H) | 67M; 109M | 60.34; 65.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Machine reading comp. | RACE | BERT-base/6-layer BERT | Vanilla KD (Sun et al.,
    [2019](#bib.bib111)) | Acc (H) | 67M; 109M | 58.74; 65.30 |'
  prefs: []
  type: TYPE_TB
- en: '| NER (41 langs.) | Wikiann-41 | Multiple mBERT/BiLSTM | XtremeDistill (Mukherjee
    and Awadallah, [2020](#bib.bib84)) | F1 (H) | 31.8M; 179M*41 | 88.64; 90.76 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (de$\rightarrow$en) | OpenNMT | 4-layer LSTM/2-layer LSTM | Vanilla KD (Polino
    et al., [2018](#bib.bib92)) | BLEU (H) | 64.8M; 84.8M | 15.48; 15.88 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (de$\rightarrow$en) | OpenNMT | 4-layer LSTM/2-layer LSTM | Quantized
    Distillation (4 bits) (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 64.8M;
    84.8M | 15.26; 15.88 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (de$\rightarrow$en) | WMT13 | 4-layer LSTM/2-layer LSTM | Quantized Distillation
    (4 bits) (Polino et al., [2018](#bib.bib92)) | BLEU (H) | 81.6M; 84.8M | 35.32;
    34.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (de$\rightarrow$en) | WMT13 | 4-layer LSTM/2-layer LSTM | Vanilla KD (Polino
    et al., [2018](#bib.bib92)) | BLEU (H) | 81.6M; 84.8M | 30.21; 34.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$Others) | IWSLT (13 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*12
    | 22.96; 22.72 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$Others) | IWSLT (13 langs.) | Multiple Transformers/Transformer
    | Seq-KD with Multiple teachers (Kim and Rush, [2016](#bib.bib57)) | BLEU (H)
    | 44M; 44M*12 | 21.98; 22.72 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$Others) | WMT (7 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*6
    | 24.47; 24.50 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Word-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 14.9; 17.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Seq-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 18.1; 17.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Seq-KD + Seq-Inter
    + Word-KD (Kim and Rush, [2016](#bib.bib57)) | BLEU (H) | 49M; 221M | 18.5; 17.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | 4-layer LSTM/2-layer LSTM | Pruned Seq-KD
    + Seq-Inter (Kim and Rush, [2016](#bib.bib57)) | BLEU@5 (H) | 8M/17M; 221M | 18.5/19.1;
    19.5 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (Others$\rightarrow$en) | IWSLT (13 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*12
    | 30.34; 29.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (Others$\rightarrow$en) | Ted Talk (45 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*43
    | 28.95; 25.17 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (Others$\rightarrow$en) | WMT (7 langs.) | Multiple Transformers/Transformer
    | Multiple teachers (Tan et al., [2019](#bib.bib114)) | BLEU (H) | 44M; 44M*6
    | 28.61; 27.07 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (th$\rightarrow$en) | IWSLT15 | 4-layer LSTM/2-layer LSTM | Word-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 11.8; 14.3 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (th$\rightarrow$en) | IWSLT15 | 4-layer LSTM/2-layer LSTM | Seq-KD (Kim
    and Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M | 12.8; 14.3 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (th$\rightarrow$en) | IWSLT15 | 4-layer LSTM/2-layer LSTM | Seq-KD +
    Seq-Inter + Word-KD (Kim and Rush, [2016](#bib.bib57)) | BLEU (H) | 8M; 47M |
    14.2; 14.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Question generation | SQuAD 1.1 | UniLM-large/12-layer BERT | MiniLM (Wang
    et al., [2020b](#bib.bib130)) | BLEU@4 (H) | 33M; 340M | 23.27; 24.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Slot filling | SNIPS (39 slots) | BERT-base/6-layer BERT | Mixed-vocabulary
    training (Zhao et al., [2019](#bib.bib143)) | F1 (H) | 6.2M; 109M | 95.0; 97.0
    |'
  prefs: []
  type: TYPE_TB
- en: Table 3\. Comparison of various knowledge distillation methods (sorted by Task
    and then Dataset). CE=cross entropy, MAE=mean absolute error. en=English, th=Thai.
    MRC=Machine Reading Comprehension. NLI=Natural Language Inference. QA=Question
    Answering. NER=Named Entity Recognition. In the metric column, H means high is
    better while L means low is better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey") compares various knowledge
    distillation methods across different tasks and datasets. Accuracy of both the
    original and the distilled model are shown in the Eval column. Also, we report
    model size for both the student as well as the teacher models. Note that sometimes
    smaller student models perform better than the teacher models. This could be because
    (1) for some (task, dataset) pairs, the smaller models are a better regularized
    fit compared to potentially overfitted teacher models, and (2) student models
    can be rigorously trained using additional semi-supervision while teacher models
    depend on limited labeled training data.'
  prefs: []
  type: TYPE_NORMAL
- en: While knowledge distillation has been used for distilling NLP models across
    many applications, NMT is the most popular one. For abstractive summarization,
    MiniLM (Wang et al., [2020b](#bib.bib130)) leads to a student models which is
    less than one tenth of the teacher without much loss in ROUGE-L. Similarly, MiniLM
    shows good results for cross-lingual NLI and multi-lingual QA as well. For Ad
    click through rate (CTR) prediction and language modeling, Anil et al. (Anil et al.,
    [2018](#bib.bib3)) show that co-distillation leads to lower MAE and cross entropy
    respectively compared to the individually trained models. Zhao et al. (Zhao et al.,
    [2019](#bib.bib143))’s mixed-vocab training leads to 6-layer model that retains
    over 95% of the BERT-base model’s slot filling F1 score while being 30x smaller
    ($<$10 MB without quantization) and 57x faster on a mobile device, yet task-agnostic.
    For Named Entity Recognition (NER), Mukherjee et al. (Mukherjee and Awadallah,
    [2020](#bib.bib84)) show that XtremeDistil leads to massive compression of teacher
    models like mBERT by upto 35x in terms of parameters and 51x in terms of latency
    for batch inference while retaining 95% of its F1-score for NER over 41 languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'For NMT, experiments have been done on OpenNMT, WMT, IWSLT and TedTalk datasets.
    Kim et al. (Kim and Rush, [2016](#bib.bib57)) make the following observations:
    (1) Sequence-level knowledge distillation (Seq-KD) does better than word-level
    knowledge distillation (Word-KD) on English $\rightarrow$ German and performs
    similarly on Thai $\rightarrow$ English. (2) Combining them (Seq-KD + Word-KD)
    results in further gains, indicating that these methods provide orthogonal means
    of transferring knowledge from the teacher to the student: Word-KD is transferring
    knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge
    at the global (i.e. sequence) level. (3) Applying weight pruning on top of knowledge
    distillation results in a student model that has 13x fewer parameters than the
    original teacher model, with a decrease of 0.4 BLEU. Tan et al. (Tan et al., [2019](#bib.bib114))
    show that one model is enough to handle multiple languages (up to 44 languages),
    with comparable or even better accuracy than individual models. Their method achieves
    larger improvements on some languages, such as Da, Et, Fi, Hi and Hy, than others.
    This is correlated with the data size of the languages. When a language is of
    smaller data size, it may get more improvement due to the benefit of multilingual
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Method | Teacher/Student | Size | MRPC | MNLI | MNLI-m | SST-2 |
    QQP | QNLI | RTE | CoLA | STS-B | SQuAD | SQuAD |'
  prefs: []
  type: TYPE_TB
- en: '| Category |  | models |  | F1 | Acc | Acc | Acc | F1 | Acc | Acc | MCC | Spearman
    | 1.1 F1 | 2.0 F1 |'
  prefs: []
  type: TYPE_TB
- en: '| Original Models | BERT-B (Devlin et al., [2018](#bib.bib25)) | - | 109M |
    88.9 | 83.4 | 84.6 | 93.5 | 71.2 | 90.5 | 66.4 | 52.1 | 85.8 | 88.4 | 77.7 |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-L (Devlin et al., [2018](#bib.bib25)) | - | 340M | 89.3 | 85.9 | 86.7
    | 94.9 | 72.1 | 92.7 | 70.1 | 60.5 | 86.5 | 90.9 | 81.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MTDNN (ensemble) (Liu et al., [2019b](#bib.bib75)) | - | 340*4M | 90.0 |
    87.2 | 86.7 | 95.6 | 72.4 | 93.9 | 80.9 | 61.5 | 88.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge Distillation Methods | Distilled-BiLSTM (Tang et al., [2019](#bib.bib115))
    | BERT-L/BiLSTM | 0.96M | - | 72.6 | 73.0 | 90.7 | 68.2 | - | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Mixed-vocab. training (Zhao et al., [2019](#bib.bib143)) | BERT-L/BERT-12
    | 10.9M | 87.2 | 80.5 | 80.7 | 90.6 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TinyBERT (Jiao et al., [2019](#bib.bib51)) | BERT-B/BERT-4 | 14.5M | 86.4
    | 81.8 | 82.5 | 92.6 | 71.3 | 87.7 | 66.6 | 44.1 | 80.4 | 82.1 | 71.8 |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-EMD (Li et al., [2020](#bib.bib67)) | BERT-B/BERT-4 | 14.5M | 87.6 |
    80.6 | 82.1 | 91.0 | 69.3 | 87.2 | 66.2 | 25.6 | 82.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MobileBERT (Sun et al., [2020](#bib.bib112)) | BERT-L/BERT-6 | 25.3M | 88.8
    | 82.6 | 83.3 | 92.8 | 70.2 | 90.6 | 66.2 | 50.5 | 84.4 | 90.0 | 79.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MobileBERT+Quantization (Sun et al., [2020](#bib.bib112)) | BERT-L/BERT-6
    | 25.3M | 87.0 | - | 83.9 | 91.9 | - | 90.8 | - | - | - | 90.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MiniLM (Wang et al., [2020b](#bib.bib130)) | BERT-B/BERT-12 | 33M | 89.5
    | - | 85.7 | 93.0 | 91.3 | 91.5 | 73.3 | 58.5 | - | - | 81.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SqueezeBERT (Iandola et al., [2020](#bib.bib50)) | BERT-B/SqueezeBERT | 51.1M
    | 87.8 | 81.1 | 82.0 | 91.4 | 80.3 | 90.1 | 73.2 | 46.5 | 86.7 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DistilBERT (Sanh et al., [2019](#bib.bib102)) | BERT-B/BERT-4 | 52.2M | 82.4
    | 78.0 | 78.9 | 91.4 | 68.5 | 85.2 | 54.1 | 32.8 | 76.1 | 81.2 | 64.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Patient KD (Sun et al., [2019](#bib.bib111)) | BERT-B/BERT-4 | 52.2M | 82.6
    | 79.3 | 79.9 | 89.4 | 70.2 | 85.1 | 62.3 | 24.8 | 79.8 | 79.5 | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-of-Theseus (Xu et al., [2020](#bib.bib134)) | BERT-B/BERT-6 | 66M |
    87.6 | 82.1 | 82.4 | 92.2 | 71.6 | 89.6 | 66.2 | 47.8 | 84.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MiniLM (Wang et al., [2020b](#bib.bib130)) | BERT-B/BERT-6 | 66M | 88.4 |
    - | 84.0 | 92.0 | 91.0 | 91.0 | 71.5 | 49.2 | - | - | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-EMD (Li et al., [2020](#bib.bib67)) | BERT-B/BERT-6 | 66M | 89.8 | 83.5
    | 84.7 | 93.3 | 72.0 | 90.7 | 71.7 | 47.5 | 86.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Patient KD (Sun et al., [2019](#bib.bib111)) | BERT-B/BERT-6 | 67M | 85.0
    | 81.0 | 81.5 | 92.0 | 70.7 | 89.0 | 65.5 | 43.5 | 81.6 | 85.3 | 69.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Vanilla KD (Hinton et al., [2015](#bib.bib44)) | BERT-B/BERT-6 | 67M | 86.2
    | 79.8 | 80.2 | 91.5 | 70.1 | 88.3 | 64.7 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DistilBERT (Sanh et al., [2019](#bib.bib102)) | BERT-B/BERT-6 | 67M | 86.9
    | 81.3 | 82.6 | 92.5 | 70.1 | 88.9 | 58.4 | 49.0 | 81.3 | 86.2 | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: '| TinyBERT (Jiao et al., [2019](#bib.bib51)) | BERT-B/BERT-6 | 67M | 87.3 |
    83.2 | 84.6 | 93.1 | 71.6 | 90.4 | 70.0 | 51.1 | 83.7 | 87.5 | 77.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Pretrained Distillation (Turc et al., [2019](#bib.bib120)) | BERT-B/BERT-6
    | 67M | 86.8 | 82.2 | 82.8 | 91.8 | 70.4 | 88.9 | 65.3 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MTDNN-KD (Liu et al., [2019a](#bib.bib74)) | MTDNN/MTDNN-KD | 340M | 91.1
    | 86.7 | 87.5 | 95.6 | 72.7 | 96.0 | 85.1 | 65.4 | 89.6 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Param. | ALBERT-B (Lan et al., [2019](#bib.bib61)) (dev) | - | 12M | - |
    - | 81.6 | 90.3 | - | - | - | - | - | 89.3 | 80 |'
  prefs: []
  type: TYPE_TB
- en: '| Sharing | ALBERT-L (Lan et al., [2019](#bib.bib61)) (dev) | - | 18M | - |
    - | 83.5 | 91.7 | - | - | - | - | - | 90.6 | 82.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor Decomp. | FLOP (Wang et al., [2019c](#bib.bib131)) | - | 80M | 88.61
    | - | - | 92.09 | - | 89.05 | - | - | 88.18 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Pruning | RPP Iterative Magnitude Pruning (Guo et al., [2019a](#bib.bib33))
    | - | 138M | 88.1 | 86.1 | 85.7 | 92.4 | 91.2 | 92.3 | 70.1 | 82.8 | - | 90.23
    | 75.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Iterative Magnitude Pruning (Guo et al., [2019a](#bib.bib33)) | - | 170M
    | 83.5 | 77 | 82.5 | 91.3 | 85.1 | 90.2 | 68.6 | 76.3 | - | 85.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LayerDrop (Fan et al., [2019](#bib.bib28)) | - | 66M | 85.3 | - | 82.9 |
    92.5 | - | 89.4 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | Mixed-precision quant. (QBERTMP) (Shen et al., [2019](#bib.bib105))
    | - | - | - | 82.29 | 81.75 | 92.08 | - | - | - | - | - | 86.95 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SubQuad Trans. | Linformer (Wang et al., [2020a](#bib.bib129)) | - | - |
    - | - | - | 93.1 | 90.8 | 91.2 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Comparison of various methods across various GLUE (Wang et al., [2019b](#bib.bib126))
    and SQuAD tasks. Please refer (Wang et al., [2019b](#bib.bib126)) for detailed
    description of tasks. Top part (first three rows) shows results for basic Transformer
    methods (teacher models). Middle part shows results for knowledge distillation
    methods. Bottom part shows results for a mix of other methods across categories.
    BERT-L=BERT-large, BERT-B=BERT-base, BERT-$i$=$i$-layer BERT. MCC refers to Matthews
    correlation. Results for SQuAD are on dev set. Empty entries indicate that the
    papers do not report those results, or NA entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey") compares various knowledge
    distillation methods (besides other model compression methods) across various
    GLUE (Wang et al., [2019b](#bib.bib126)) and SQuAD tasks. Also, we report model
    size for both the student as well as the teacher models. Different distillation
    methods use one of these as the teacher: BERT-large, BERT-base or MTDNN. PKD (Sun
    et al., [2019](#bib.bib111)) outperforms the Vanilla KD (Hinton et al., [2015](#bib.bib44))
    on almost all the datasets except for MRPC. TinyBERT-4 significantly outperforms
    the 4-layer BERT-PKD and DistilBERT by a margin of at least 4.4%, with  28% parameters
    and 3.1x inference speedup. Compared with the teacher BERT-base, 4-layer TinyBERT
    is 7.5x smaller and 9.4x faster in the model efficiency, while maintaining competitive
    performances. The 6-layer TinyBERT achieves comparable results with the teacher.
    Overall, TinyBERT consistently outperforms both the 4-layer and 6-layer baselines
    like PKD, DistilBERT and MiniLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Turc et al. (Turc et al., [2019](#bib.bib120)) show how appropriate pretraining
    can improve the quality of distillation. MiniLM outperforms DistilBERT and TinyBERT
    across most tasks. The 6-layer MiniLM is 2.0x faster than original BERTBASE, while
    retaining more than 99% performance on a variety of tasks, such as SQuAD 2.0 and
    MNLI. Distilled MT-DNN significantly outperforms the original MT-DNN on 7 out
    of 9 GLUE tasks. 4-layer BERT-EMD outperforms 4-layer DistilBERT and BERT-PKD
    by a substantial margin, even with only 30% parameters and inference time. Furthermore,
    it exceeds the TinyBERT model by 2.3% accuracy on RTE, 2.2% F1 on MRPC, and 1.9%
    Spearman correlation on STS-B. 6-layer BERT-EMD performs better than the 12-layer
    BERT-base model on 7 out of 9 tasks, with only about 50% parameters and inference
    time of the original BERT-base model. Tang et al. (Tang et al., [2019](#bib.bib115))
    distill BERT to BiLSTMs. They observe that the distilled BiLSTM model uses 349
    times fewer parameters than BERT-large and is 434 times faster. Also, mixed vocab
    training by Zhao et al. (Zhao et al., [2019](#bib.bib143)) produces a small 12-layer
    model that performs competitively with 6-layer PKD and 4-layer DistilBERT while
    being $\sim$5-6x smaller.
  prefs: []
  type: TYPE_NORMAL
- en: MobileBERT is 4.3x smaller and 5.5x faster than BERT-base. On the SQuAD v1.1/v2.0
    question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1
    higher than BERT-base). On the natural language inference tasks of GLUE, MobileBERT
    can achieve a GLUE score of 77.7, which is only 0.6 lower than BERT-base, with
    a latency of 62 ms on a Pixel 4 phone. While quantization can further compress
    MobileBERT by 4x, there is nearly no performance degradation from it. SqueezeBERT
    is approximately half the size of BERT-base. MobileBERT is half the size of SqueezeBERT.
    SqueezeBERT is 4.3x faster than BERT-base, while MobileBERT is 3.0x faster than
    BERT-base. On 4 of GLUE tasks SqueezeBERT outperforms the accuracy of MobileBERT,
    while on the other 4 of the GLUE tasks MobileBERT outperforms SqueezeBERT. MobileBERT
    and SqueezeBERT outperform BERT-base significantly across all tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, KD is a popular method for text based model compression. Various
    methods have proposed information copying using logits, softmax output, attention
    sub-layer output, value relation, relative dissimilarity information from both
    the last layer as well as intermediate layers of the teacher. Many methods have
    been proposed to handle complex teacher-student configuration mismatches in terms
    of vocabulary, number of attention heads, and hidden layer sizes. Also, KD has
    been found to be very effective in complex problem settings like multi-lingual
    tasks and tasks with large number of classes. Learning from noisy teachers, teacher
    assistants, an ensemble of teachers has been found to be effective as well. KD
    is the best model compression method especially in settings where a large amount
    of unlabeled data exists; distillation with data pseudo-labeled by teacher leads
    to very effective students.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Parameter sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rather than removing weights or reducing #bits to store them, parameter sharing
    methods reduce model size by finding weight blocks that can share the same weight.
    Character-based language models learn embeddings for characters and use them to
    compose word embeddings. In some senses, we can think of various words sharing
    these character embedding parameters. Further, various parameter sharing methods
    have been proposed to reduce the large word embedding matrix size. Finally, there
    are multiple Transformer architectures which benefit from the parameter sharing
    philosophy. We discuss these methods in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Character-aware Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fig. [5](#S5.F5 "Figure 5 ‣ 5.1\. Character-aware Language Models ‣ 5\. Parameter
    sharing ‣ Compression of Deep Learning Models for Text: A Survey") illustrates
    various character-aware language model architectures. Ling et al. (Ling et al.,
    [2015](#bib.bib72)) proposed their character to word (C2W) model which constructs
    vector representations of words by composing characters using BiLSTMs. Relative
    to traditional word representation models that have independent vectors for each
    word type, C2W requires only a single vector per character type and a fixed set
    of parameters for the compositional model. As input, we define an alphabet of
    characters $C$. For English, this vocabulary would contain an entry for each uppercase
    and lowercase letter as well as numbers and punctuation. Thus compared to the
    word embedding matrix, this model is much smaller. Despite the compactness of
    this model, this “composed” word representations yield comparable results across
    multiple text classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Jozefowicz et al. (Jozefowicz et al., [2016](#bib.bib52)) propose two variants
    for composing word embeddings using character embeddings. In the first CNN-Softmax
    variant, they use character CNNs (Convolutional Neural Networks) to compose word
    embeddings from character embeddings both at the input side as well as at the
    output softmax layer. The character-CNN sub-networks at the input or the output
    do not share weights. The composed word embeddings are fed to an LSTM to generate
    the output. In the second Char-LSTM variant, character CNN is used to compose
    word embeddings on the input side. The composed word embeddings are fed to an
    LSTM to generate an output which is further fed to a small LSTM that predicts
    the target word one character at a time. Thus, the word and character-level models
    are combined, and predictions are made one character at a time, thus allowing
    to compute probabilities over a much smaller vocabulary. Kim et al. (Kim et al.,
    [2016](#bib.bib56)) propose another variant where at the output side they continue
    to use word embeddings, but at the input side they compose word embeddings using
    a highway network on top of a character CNN. The highway network’s output is used
    as the input to a multi-layer LSTM, whose last hidden state output is fed to the
    output softmax layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/959519ae9d3d51ee05373e26c7fc5021.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Character-aware Language Models
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Parameter Sharing in the Embedding Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a weight matrix $W$ and a budget $K$, we want to share weights within
    $W$ to have a max of $K$ unique values. A naïve implementation of random weight
    sharing can be trivially achieved by maintaining a secondary matrix consisting
    of each connection’s group assignment. But this needs memory space itself. Hence,
    Chen et al. (Chen et al., [2015b](#bib.bib13)) propose to use hashing. HashedNets
    use a low-cost hash function (like xxhash³³3[https://code.google.com/p/xxhash/](https://code.google.com/p/xxhash/))
    to randomly group connection weights into hash buckets, and all connections within
    the same hash bucket share a single parameter value.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike HashedNets where weights are randomly grouped, parameter sharing mechanisms
    in Toeplitz-like structured matrices (Lu et al., [2016](#bib.bib77)) are highly
    specific and deterministic. Toeplitz matrices have parameters tied along diagonals.
    The displacement rank of all Toeplitz matrices is up to 2\. Toeplitz-like matrices
    allow the displacement rank $r$ to be higher. They include products and inverses
    of Toeplitz matrices, and their linear combinations. The displacement rank $r$
    serves as a knob on modeling capacity. High displacement rank matrices are increasingly
    unstructured. With displacement rank $r$, there are $2nr$ free parameters in the
    Toeplitz-like structured matrix. Toeplitz transforms can be applied not just to
    embedding matrix but to all weight matrices in an RNN model. Tay et al. (Tay et al.,
    [2019](#bib.bib117)) use a similar Toeplitz-like structured matrix method with
    Hamilton Products in Quaternion Algebra to propose Quaternion Transformers which
    lead to 75% parameter reduction in the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Another method for parameter sharing is to share low-rank factors across layers
    in a recurrent model. In this method, we first represent a weight matrix $W$ using
    matrix factorization as $W=W_{a}W_{b}$. Thus, hidden layer output for layer $l$
    at time $t$ can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (59) |  | $\displaystyle h_{t}^{l}=\sigma\left[W_{a}^{l}W_{b}^{l}h_{t}^{l-1}+U_{a}^{l}U_{b}^{l}h_{t-1}^{l}+b^{l}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: But we can share some low-rank factors by setting $W_{b}^{l}=U_{b}^{l-1}$. The
    combination of matrix factorization and parameter sharing leads to large model
    compression.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of compressing the embedding matrix is to divide the vocabulary
    $V$ into frequent and infrequent word sets $B$ and $C$ respectively. Infrequent
    words’ embeddings are represented with frequent words’ by sparse linear combinations (Chen
    et al., [2016](#bib.bib14)). This is inspired by the observation that, in a dictionary,
    an unfamiliar word is typically defined by common words. A dense embedding is
    assigned to each common word; an infrequent word, on the other hand, computes
    its vector representation by a sparse combination of common words’ embeddings.
    This compression is useful for both word embedding matrix as well as output layer
    of RNNs/LSTMs. Let $U\in R^{E\times|B|}$ be the learned embedding matrix of common
    words where $E$ is the embedding dimension. For a word $w\in C$, we shall learn
    a sparse vector $x\in R^{|B|}$ as the sparse code of the word. Once we know $x$,
    embedding for a word $w\in C$ can be written as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (60) |  | $\displaystyle\text{embedding}(w)=\sum_{j=1}^{B}x_{j}U_{j}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $U_{j}$ is the $j^{th}$ column of $U$. To learn the sparse representation
    of word $w\in C$, the following problem needs to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: '| (61) |  | $\displaystyle\min_{x}&#124;&#124;Ux-A&#124;&#124;_{2}^{2}+\alpha&#124;&#124;x&#124;&#124;_{1}+\beta&#124;1^{T}x-1&#124;+\gamma
    1^{T}\max(0,-x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $A$ is embedding for the rare word $w$. The last two regularization terms
    favor a solution that sums to 1 and that is non-negative (for psychological interpretation
    concerns), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'LightRNN (Li et al., [2016a](#bib.bib68)) compresses word embedding matrix
    from $O(|V|)$ to $O(\sqrt{|V|})$. It uses a 2-Component shared embedding for word
    representations. We allocate every word in the vocabulary into a word-allocation
    table, each row of which is associated with a learned vector, and each column
    associated with another learned vector. Table [5](#S5.T5 "Table 5 ‣ 5.2\. Parameter
    Sharing in the Embedding Matrix ‣ 5\. Parameter sharing ‣ Compression of Deep
    Learning Models for Text: A Survey") shows an example of a word allocation table.
    Depending on its position in the table, a word is jointly represented by two components:
    a row vector and a column vector. Thus, we only need $2\sqrt{|V|}$ vectors to
    represent a vocabulary of $|V|$ unique words, which are far less than the $|V|$
    vectors. The input and output use different embedding row/column vectors but they
    share the same word-allocation table. Word Allocation table creation uses a bootstrap
    procedure to iteratively refine word allocation based on the learned word embedding.
    Embeddings (i.e. row and column vectors) are learned using language modeling loss
    using an RNN on top of the embedding layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Embedding | $x_{1}^{c}$ | $x_{2}^{c}$ | $x_{3}^{c}$ | $x_{4}^{c}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $x_{1}^{r}$ | january | february | … | … |'
  prefs: []
  type: TYPE_TB
- en: '| $x_{2}^{r}$ | one | two | … | … |'
  prefs: []
  type: TYPE_TB
- en: '| $x_{3}^{r}$ | … | … | … | … |'
  prefs: []
  type: TYPE_TB
- en: '| $x_{4}^{r}$ | … | … | … | … |'
  prefs: []
  type: TYPE_TB
- en: Table 5\. An Example of a Word Allocation Table
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Suzuki et al. (Suzuki and Nagata, [2016](#bib.bib113)) propose a Skipgram (Mikolov
    et al., [2013](#bib.bib81)) training method with parameter sharing as follows.
    Split every embedding vector of size $D$ into $B$ equal sub-vectors of size $C$.
    Thus $D=B\times C$. We assign a limited number of reference vectors to each block
    of block-splitting vectors. E.g., the number of reference vectors becomes $K\times
    B$ if we assign $K$ reference vectors to each block. Each reference vector is
    of size $C$. Skipgram training optimization remains the same except for these
    extra parameter sharing constraints (applied to both the input and output embedding
    vectors). Liu et al. (Li et al., [2018](#bib.bib69)) propose a very similar method,
    Slim Embeddings, where the embeddings are learned using a RNN language model rather
    than the Skipgram method. Slim Embeddings is very related to HashedNets (Chen
    et al., [2015b](#bib.bib13)), LightRNN (Li et al., [2016a](#bib.bib68)) and Character
    Aware Language model (Kim et al., [2016](#bib.bib56)). In HashedNets, all elements
    in a parameter matrix are mapped into a vector through a hash function. However
    in Slim Embeddings approach, we randomly share subvectors instead of single elements.
    Slim Embeddings differs from LightRNN in that it is able to control the compression
    ratio to any arbitrary value, while LightRNN can only compress at the rate of
    square or cube root of vocabulary size, which could be too harsh in practical
    applications. In character aware language model, if we treat the sequence of subvector
    ids (virtual characters) as each word’s representation, the word embedding then
    can be treated as concatenated unigram character feature vectors. The drawback
    of such an approach is that the model is more complicated and to speed up inference,
    it needs to pre-compute the word embeddings for the words, so it couldn’t stay
    in its compact form during inference. The Slim embeddings model is much simpler,
    and easier to tune. And during inference, it uses much less space and can even
    decrease the complexity of inference.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Parameter Sharing in Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A standard Transformer does not share parameters across layers and also has
    a fixed number of encoder layers. ALBERT (Lan et al., [2019](#bib.bib61)) incorporates
    two parameter reduction techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorized embedding parameterization. That is, it decomposes large vocabulary
    embedding matrix into two small matrices. Thus, it reduces the embedding parameters
    from $O(V\times H)$ to $O(V\times E+E\times H)$ where $H>>E$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-layer parameter sharing: There are multiple ways to share parameters,
    e.g., only sharing feed-forward network (FFN) parameters across layers, or only
    sharing attention parameters. The default decision for ALBERT is to share all
    parameters across layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An ALBERT configuration similar to BERT-large has 18x fewer parameters and can
    be trained about 1.7x faster. Dehghani et al. (Dehghani et al., [2018](#bib.bib22))
    propose Universal Transformers where the number of encoder layers are not pre-decided,
    and all the encoder layers share the parameters. Certain symbols (e.g. some words
    or phonemes) are usually more ambiguous than others. It is therefore reasonable
    to allocate more processing resources to these more ambiguous symbols. Thus, ambiguous
    symbols undergo more self-attention transformations compared to non-ambiguous
    ones. Thus, they provide a dynamic per-position halting mechanism for dynamically
    modulating the number of computational steps needed to process each input symbol
    (called the “ponder time”) before the representation is passed on as input to
    the decoder. The idea of sharing weights across layers in Transformers has also
    been explored in (Bai et al., [2019](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Task | Dataset | Method | Base Model | Metric | Size (Comp; Orig) | Eval.
    (Comp; Orig) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | Char-CNN (input embeddings) (Jozefowicz
    et al., [2016](#bib.bib52)) | 2-layer word-BiLSTM | Perplexity (L) | 1.04B; 1.8B
    | 30.0; 30.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | Char-CNN (in/output embeddings) (Jozefowicz
    et al., [2016](#bib.bib52)) | 2-layer word-BiLSTM | Perplexity (L) | 0.39B; 1.8B
    | 35.8; 30.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 41M; 1.6G | 66; 85 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | 2-layer word-BiLSTM | Perplexity (L) | 0.25B; 1.8B | 38.3; 30.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Czech | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 64M; 83M | 578; 701 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Czech | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 18M; 83M | 558; 701 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Czech | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 17M; 83M | 528; 701 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-English | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 20M; 25M | 216; 236 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-English | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 17M; 25M | 191; 236 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-English | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 7M; 25M | 187; 236 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-French | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 44M; 56M | 190; 202 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-French | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 17M; 56M | 176; 202 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-French | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 12M; 56M | 162; 202 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-German | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 104M; 137M | 305; 347 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-German | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 18M; 137M | 281; 347 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-German | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 17M; 137M | 261; 347 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Russian | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 152M; 200M | 313; 353 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Russian | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 19M; 200M | 288; 353 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Russian | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 19M; 200M | 274; 353 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Spanish | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 48M; 61M | 169; 186 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Spanish | LightRNN (Li et al., [2016a](#bib.bib68))
    | word-LSTM | Perplexity (L) | 18M; 61M | 157; 186 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | ACLW-Spanish | Slim Embeddings (Li et al., [2018](#bib.bib69))
    | word-LSTM | Perplexity (L) | 8M; 61M | 149; 186 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | CNN+Highway network (Kim et al., [2016](#bib.bib56))
    | word-LSTM | Perplexity (L) | 5M; 20M | 92.3; 85.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikipedia articles (ca) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 182K; 4.3M | 34.92; 35.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikipedia articles (de) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 183K; 6.3M | 41.94; 43.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikipedia articles (en) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 180K; 4.3M | 57.39; 59.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikipedia articles (pt) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 178K; 4.2M | 40.92; 46.17 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Wikipedia articles (tr) | C2W (Ling et al., [2015](#bib.bib72))
    | word emb. | Perplexity (L) | 174K; 5.7M | 32.88; 44.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Machine reading comp. | ReAding Comprehension from Examinations | ALBERT (Lan
    et al., [2019](#bib.bib61)) | BERT-B | Acc (H) | 18M; 108M | 68.5; 68.2 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | MNLI | Quaternion Attention (Tay et al., [2019](#bib.bib117)) | 2-layer
    att-GloVe | Acc (H) | 200K; 700K | 72.3; 73.6 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | SciTail | Quaternion Attention (Tay et al., [2019](#bib.bib117)) |
    2-layer att-GloVe | Acc (H) | 200K; 700K | 79.6; 79.0 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | SNLI | Quaternion Attention (Tay et al., [2019](#bib.bib117)) | 2-layer
    att-GloVe | Acc (H) | 200K; 700K | 85.4; 86.2 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$et) | IWSLT15 | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | Transformer | BLEU (H) | 11M; 44M | 13.1; 14.1 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$ro) | IWSLT15 | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | Transformer | BLEU (H) | 11M; 44M | 18.5; 22.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$vi) | IWSLT15 | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | Transformer | BLEU (H) | 11M; 44M | 28.0; 28.4 |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | PTB (ca) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 98.92; 98.09 |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | PTB (de) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 98.08; 97.51 |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | PTB (en) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 97.36; 96.97 |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | PTB (pt) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 97.47; 95.67 |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | PTB (tr) | C2W (Ling et al., [2015](#bib.bib72)) | word-BiLSTM
    | Acc (H) | 150K; 2M | 91.59; 83.43 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | BABI | Universal Transformer (Dehghani et al., [2018](#bib.bib22))
    | Transformer | Avg error (L) | 7.3M; 44M | 0.21; 15.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | WikiQA | Quaternion Attention (Tay et al., [2019](#bib.bib117))
    | 2-layer att-GloVe | MAP (H) | 200K; 700K | 66.2; 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | 2-layer Transformer | Acc (H) | 100K; 400K | 83.9; 82.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST | Quaternion Transformer (Tay et al., [2019](#bib.bib117))
    | 2-layer Transformer | Acc (H) | 100K; 400K | 80.5; 78.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2000 hour En. Speech | Toeplitz-like (Lu et al., [2016](#bib.bib77))
    | 3-layer RNN | WER (L) | 790K; 1.85M | 48.4; 43.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 2000 hour En. Speech | Toeplitz-like (Lu et al., [2016](#bib.bib77))
    | 5-layer LSTM | WER (L) | 2.00M; 9.12M | 33.5; 33.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Subject verb agreement | SVA dataset | Quaternion Transformer (Tay et al.,
    [2019](#bib.bib117)) | 2-layer Transformer | Acc (H) | 100K; 400K | 94.7; 94.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Subject verb agreement | SVA dataset | Universal Transformer (Dehghani et al.,
    [2018](#bib.bib22)) | Transformer | Acc (H) | 7.3M; 44M | 0.992; 0.962 |'
  prefs: []
  type: TYPE_TB
- en: '| Word analogy | GSEm, GSYN, MSYN | Shared ref. vec. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 64.6/64.5;
    64.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Word analogy | GSEm, GSYN, MSYN | k-means quant. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 64.0/64.5;
    64.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | 7 datasets | Shared ref. vec. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 65.5/67.1;
    67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | 7 datasets | k-means quant. ($K$=16/256, $B$=256) (Suzuki
    and Nagata, [2016](#bib.bib113)) | SGNS | Acc (H) | 98/196MB; 1565MB | 64.4/67.0;
    67.2 |'
  prefs: []
  type: TYPE_TB
- en: Table 6\. Comparison of various parameter sharing methods (sorted by Task and
    then Dataset). 7 datasets for word similarity are MEN, MTurk, RARE, SLex, SCWS,
    WSR, WSS. SGNS=SkipGram with negative sampling. In the metric column, H means
    high is better while L means low is better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.4\. Summary ‣ 5\. Parameter sharing ‣ Compression
    of Deep Learning Models for Text: A Survey") compares various parameter sharing
    methods across different tasks and datasets. Accuracy of both the original and
    the compressed (comp.) model are shown. Also, we report model size (in terms of
    number of parameters or memory size) for both the original as well as the compressed
    models. For the same task, dataset and model combination, different papers report
    different accuracy of the original model because of slight changes in training
    hyper-parameters; hence we report accuracy of the original model for each row.
    Since many parameter sharing methods have been used to compress word embeddings,
    the most common application is language modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: For language modeling, experiments have been done on the One Billion Word Benchmark,
    ACLW, PTB and Wikipedia articles datasets across multiple languages using parameter
    sharing methods like Char-CNN (Jozefowicz et al., [2016](#bib.bib52)), LightRNN (Li
    et al., [2016a](#bib.bib68)), Slim embeddings (Li et al., [2018](#bib.bib69)),
    CNN+Highway networks (Kim et al., [2016](#bib.bib56)) and C2W (Ling et al., [2015](#bib.bib72)).
    C2W (Ling et al., [2015](#bib.bib72)) always outperforms word lookup tables and
    improvements are especially pronounced in Turkish, which is a highly morphological
    language, where word meanings differ radically depending on the suffixes used.
    Jozefowicz et al. (Jozefowicz et al., [2016](#bib.bib52)) observe that Char-CNNs
    especially with character embeddings being used both at input as well as output
    can lead to 4.6x model size reduction with a slight increase in perplexity. On
    the One-Billion-Word benchmark, LightRNN (Li et al., [2016a](#bib.bib68)) achieves
    much lower perplexity compared to word-LSTMs, whilst reducing the model size by
    a factor of 40-100, and speeding up the training process by a factor of 2\. LightRNN (Li
    et al., [2016a](#bib.bib68)) significantly reduces the model size, while at the
    same time outperforms CNN+Highway network (Kim et al., [2016](#bib.bib56)) method.
    While the model sizes of the CNN+Highway network method increase linearly with
    respect to the vocabulary size, the model size of LightRNN almost keeps constant
    on the ACLW datasets. Slim embeddings (Li et al., [2018](#bib.bib69)) is way better
    than LightRNN (low perplexity with smaller models). On PTB and 44M giga word corpus
    datasets, Slim embeddings applied at input layer can maintain same perplexity
    for a word-LSTM using just 1% (and 0.2%) of trainable parameters respectively.
  prefs: []
  type: TYPE_NORMAL
- en: k-means quantization and shared reference vectors are also methods for compression
    of word embeddings using parameter sharing. Suzuki et al. (Suzuki and Nagata,
    [2016](#bib.bib113)) showed significant gains over typical skipgram (SGNS) embeddings
    in terms of model size reduction while retaining similar accuracies for word analogy
    and similarity tasks. The model size of their method with shared reference vectors
    with ‘K = 16, B = 64’ was just 24MB, approximately 65 times smaller than that
    of original SGNS method. They also showed that SGNS with shared reference vectors
    was better than SGNS with block-wise k-means post-processing method. Unfortunately,
    there exists no good comparison between the slim embeddings and the shared reference
    vectors methods.
  prefs: []
  type: TYPE_NORMAL
- en: For the MRC task, ALBERT (Lan et al., [2019](#bib.bib61)) pushed the accuracy
    from 68.5 to 68.2 using 6x fewer parameters compared to BERT-base. On the NLI
    task, a tiny Quaternion-Att model (50 dimensions) achieves comparable (or occasionally
    marginally better or worse) performance compared to typical attention over GloVe
    (200 dimensions), gaining a 68% parameter savings across three datasets. For sentiment
    analysis, Quaternion Transformers (Tay et al., [2019](#bib.bib117)) leads by +1.3%/1.6%
    gains on IMDb and SST datasets respectively while maintaining a 75% reduction
    in parameter cost. Quaternion Transformers (Tay et al., [2019](#bib.bib117)) have
    been shown to outperform the vanilla Transformer for the mathematical language
    understanding task as well, with 25% parameters. At the same compression rate,
    Quaternion Transformers lose only very minor BLEU on IWSLT 2015 NMT datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For POS tagging, we can observe that the model using word lookup tables performs
    consistently worse than the C2W model (Ling et al., [2015](#bib.bib72)). Universal
    Transformers (Dehghani et al., [2018](#bib.bib22)) (while being 1/6th the size)
    outperform standard Transformers on a wide range of algorithmic and language understanding
    tasks, including the challenging LAMBADA language modeling task. On speech recognition,
    Lu et al. (Lu et al., [2016](#bib.bib77)) study mechanisms for learning compact
    RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. A hybrid
    strategy of using structured matrices in the bottom layers and shared low-rank
    factors on the top layers is found to be particularly effective, reducing the
    parameters of a standard LSTM by 75%, at a small cost of 0.3% increase in WER,
    on a 2000-hr English Voice Search task. For LSTM parameter reduction, architecting
    upper layers with projection nodes to moderate rank, and bottom layers with Toeplitz-like
    transforms was found to be a particularly effective strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, besides model compression, parameter sharing methods also act as a
    good regularizer. Parameter sharing in Transformers has been very successful.
    ALBERT was at the top of the GLUE leaderboard when it was proposed. Parameter
    sharing methods have also been widely used for compressing embedding matrix. Slim
    embeddings has the best method for compressing embedding matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Tensor decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sparse Matrix decomposition has been traditionally used for applications like
    feature selection, collaborative filtering, topic mining from text, etc. In this
    section, we discuss how various popular tensor decomposition methods like Singular
    Value Decomposition (SVD), Tensor-Train (Oseledets, [2011](#bib.bib89)), CP (CANDECOMP/PARAFAC) (Carroll
    and Chang, [1970](#bib.bib11)) and Tucker (Tucker, [1966](#bib.bib119)) can be
    used for model compression.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Two Low-Rank Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we will discuss methods where a matrix is factorized into two
    low-rank factors. Specifically, we replace a weight matrix $W$ with $W_{1}\times
    W_{2}$ such that the total number of parameters are significantly lesser.
  prefs: []
  type: TYPE_NORMAL
- en: A multi-layer RNN can be represented as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (62) |  | $\displaystyle h_{t}^{l}=\sigma(W_{x}^{l-1}h_{t}^{l-1}+W_{h}^{l}h_{t-1}^{l}+b^{l})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (63) |  | $\displaystyle h_{t}^{l+1}=\sigma(W_{x}^{l}h_{t}^{l}+W_{h}^{l+1}h_{t-1}^{l+1}+b^{l+1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, there are two important weight matrices: the recurrent $W_{h}^{l}$ and
    inter-layer matrices $W_{l}^{x}$. Prabhavalkar et al. (Prabhavalkar et al., [2016](#bib.bib93))
    propose a method to jointly compress the recurrent and inter-layer matrices corresponding
    to a specific layer $l$ by determining a suitable recurrent projection matrix,
    denoted by $P^{l}\in R^{r_{l}\times N_{l}}$ of rank $r^{l}<N^{l}$ such that $W_{h}^{l}=Z_{h}^{l}P^{l}$
    and $W_{l}^{x}=Z_{x}^{l}P^{l}$. First, $P^{l}$ is determined by computing a truncated
    SVD of the recurrent weight matrix, which we then truncate, retaining only the
    top $r^{l}$ singular values. Thus, $W_{h}^{l}$ can be written as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (64) |  | $\displaystyle W_{h}^{l}=(U_{h}^{l}\Sigma_{h}^{l})(V_{h}^{l})^{T}=Z_{h}^{l}P^{l}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, $P^{l}$ is set to $(V_{h}^{l})^{T}$. Further, we determine $Z_{x}^{l}$
    as the solution to the following least-squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: '| (65) |  | $\displaystyle Z_{x}^{l}=\operatorname*{argmin}_{Y}&#124;&#124;YP^{l}-W_{x}^{l}&#124;&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This solution can also be easily extended to LSTMs. Sak et al. (Sak et al.,
    [2014](#bib.bib101)) also proposed a similar solution based on a combination of
    parameter sharing and matrix decomposition but without SVD initialization. However,
    typically SVD initialization has been found to perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Besides SVD, another way of matrix decomposition is sparse coding. Faruqui et
    al. (Faruqui et al., [2015](#bib.bib29)) propose using sparse coding to decompose
    word embedding matrices. Thus, given vocabulary of size $V$, word embedding matrix
    $X\in R^{L\times V}$, sparse coding aims at representing each input vector $x_{i}$
    as a sparse linear combination of basis vectors $a_{i}$ by solving the following
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '| (66) |  | $\displaystyle\operatorname*{argmin}_{D,A}\sum_{i=1}^{V}&#124;&#124;x_{i}-Da_{i}&#124;&#124;_{2}^{2}+\lambda&#124;&#124;a_{i}&#124;&#124;_{1}+\tau&#124;&#124;D&#124;&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $D\in R^{L\times K}$ and $A\in R^{K\times V}$. Further, for interpretability,
    one can enforce all elements of $A$ and $D$ to be non-negative. For further compression,
    one can also enforce $A$ to be binary or ensure that each column of $A$ is a $K$
    sized one hot vector (Shu and Nakayama, [2017](#bib.bib108)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, Wang et al. (Wang et al., [2019c](#bib.bib131)) combine pruning with
    matrix factorization for model compression and propose the FLOP (Factorized Low-rank
    Pruning) method. Let $W$ be a weight matrix. Structured pruning (removing a neuron,
    i.e., removing a column from weight matrix) can be achieved by replacing the computation
    $Wx$ by $WGx$ where diagonal sparsity-inducing matrix $G$ is learned using $L_{0}$
    regularization over $WG$ along with the supervised loss. This effectively removes
    a subset of columns of $W$ for column indices $k$ with $z_{k}=0$. One limitation
    is that this structured pruning method tends to produce lower performance than
    its unstructured counterpart. Hence, in the FL0P (Factorized L0 Pruning) model,
    we first factorize $W=PQ$. Let $r$ be #columns of $P$ (or equivalently #rows of
    $Q$), $p_{k}$ and $q_{k}$ be the $k$-th column of $P$ and $k$-th row of $Q$ respectively.
    We achieve structured pruning by introducing a pruning variable $z_{k}$ for each
    component. Thus, now, we can write $W$ as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (67) |  | $\displaystyle W=PGQ=\sum_{k=1}^{r}z_{k}\times(p_{k}q_{k})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $G$ is again the diagonal matrix of pruning variables. After training,
    only columns and rows corresponding to non-zero diagonal values need to be stored,
    resulting in much smaller (but still dense) matrices $P$ and $Q$. The nonzero
    values of $G$ can be absorbed into either $P$ or $Q$. This structured pruning
    with factorization is much more effective compared to the vanilla structured pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Factorizing into Block Diagonal Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last layer of a language model is very large of the size $HV$ where $H$
    is the size of the hidden layer and $V$ is vocabulary size. Each word by an output
    embedding of the same size $H$. Chen et al. (Chen et al., [2015a](#bib.bib12))
    propose a differentiated softmax method which varies the dimension of the output
    embeddings across words depending on how much model capacity is deemed suitable
    for a given word. In particular, it is meaningful to assign more parameters to
    frequent words than to rare words. By definition, frequent words occur more often
    in the training data than rare words and therefore allow to fit more parameters.
    They define partitions of the output vocabulary based on word frequency and the
    words in each partition share the same embedding size. Partitioning results in
    a sparse final weight matrix which arranges the embeddings of the output words
    in blocks, each one corresponding to a separate partition. The size of the final
    hidden layer $H$ is the sum of the embedding sizes of the partitions. While this
    method does not involve creation of multiple factors, it factorizes the original
    matrix into multiple blocks while setting the remaining part of the matrix to
    0.
  prefs: []
  type: TYPE_NORMAL
- en: Variani et al. (Variani et al., [2019](#bib.bib121)) propose a method called
    Word Encoded Sequence Transducers (WEST) which factorizes a matrix $E=C\times
    D$ where $D$ is constrained to be a block diagonal matrix. The block diagonal
    nature of the second factor leads to large compression rates.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Tensor Train and Block Term Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tensor train decomposition (TTD) (Oseledets, [2011](#bib.bib89)) is a standard
    tensor decomposition technique which decomposes a high dimensional tensor into
    multiple 2D and 3D tensors which can be multiplied together to reconstruct the
    original tensor. These factors are called TT-cores and the other dimensions are
    referred to as TT-ranks. TTD can be leveraged to compress various weight matrices
    in RNNs and LSTMs (Tjandra et al., [2017](#bib.bib118); Khrulkov et al., [2019](#bib.bib55)).
    The first step is to represent a matrix as a multi-dimensional tensor by simple
    reshaping transformation and then use TTD on it. The values of TT–ranks directly
    define the compression ratio, so choosing them to be too small or too large will
    result into either significant performance drop or little reduction of the number
    of parameters. Typically TT-ranks around 16 for small matrices and 64-192 for
    larger matrices result in a good trade-off between compression ratio and the accuracy
    metric of interest. Also, when we use TTD for weight matrices, we also need change
    the inputs appropriately to be compatible in terms of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with TT-RNN, Block-Term RNN (BTRNN) (Ye et al., [2018](#bib.bib137))
    is not only more concise (when using the same rank), but also able to attain a
    better approximation to the original RNNs with much fewer parameters. BTD decomposes
    a high order tensor into a sum of multiple Tucker decomposition models. The redundant
    dense connections between input and hidden state is first tensorized to a $d$-dimensional
    tensor and then decomposed using low-rank BTD into a sum of $N$ different Tucker
    decompositions where $N$ is the CP-rank. Each Tucker decomposition in turn consists
    of a core $d$-dimensional tensor and $d$ 3-dimensional factor tensors. While Ye
    et al. (Ye et al., [2018](#bib.bib137)) used BTD to compress RNNs, Ma et al. (Ma
    et al., [2019](#bib.bib78)) used BTD to compress the self-attention matrix in
    Transformers. They first build a single-block attention based on the Tucker decomposition
    where the query, key and value are mapped into three factor matrices and the core
    tensor is trainable and randomly initialized. It is then straightforward to represent
    the multi-head attention using BTD.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Task | Dataset | Method | Base Model | Metric | Size (Comp; Orig) | Eval.
    (Comp; Orig) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CTR prediction | Criteo CTR Challenge | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | MLP | LogLoss (L) | 4.7M; 41.2M | 0.4433; 0.4440 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | BTD (Ma et al., [2019](#bib.bib78))
    | Transformer-XL Large | Perplexity (L) | 0.16B; 0.8B | 19.5; 21.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Enwiki-8 | FLOP (Wang et al., [2019c](#bib.bib131)) |
    Transformer | BPC (L) | 8M; 41M | 1.13; 1.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | WEST (Variani et al., [2019](#bib.bib121)) | LSTM
    | Perplexity (L) | 3.5M; 4.51M | 116.84; 115.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | BTD (Ma et al., [2019](#bib.bib78)) | Transformer-XL-Base
    | Perplexity (L) | 12M; 24M | 49.8; 54.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | PTB | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL-Base | Perplexity (L) | 18M; 24M | 55.4; 54.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | WikiText-103 | FLOP (Wang et al., [2019c](#bib.bib131))
    | Transformer | Perplexity (L) | 50M; 151M | 25.3; 24.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | WikiText-103 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL | Perplexity (L) | 91M; 192M | 25.67; 24.37 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | WikiText-103 | BTD (Ma et al., [2019](#bib.bib78)) |
    Transformer-XL-Base | Perplexity (L) | 85.3M; 151M | 20.9; 24.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | WikiText-103 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-XL-Base | Perplexity (L) | 130M; 151M | 25.7; 24.0 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$ja) | ASPEC | Compositional codes (Shu and Nakayama,
    [2017](#bib.bib108)) | LSTM | BLEU (H) | 2.97MB; 274MB | 38.89; 37.93 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (de$\rightarrow$en) | IWSLT14 | Compositional codes (Shu and Nakayama,
    [2017](#bib.bib108)) | LSTM | BLEU (H) | 2.11MB; 35MB | 29.56; 29.45 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | Transformer-Big | BLEU (H) | 179M; 210M | 28.53; 28.84 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT16 | BTD (Ma et al., [2019](#bib.bib78)) | Transformer
    | BLEU (H) | 21.2M; 52M | 34.91; 34.5 |'
  prefs: []
  type: TYPE_TB
- en: '| NP bracketing | Subset of PTB | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    | Logistic regression | Acc (H) | 120M; 120M | 82.3; 77.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Question classification | TREC Questions | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 81.5; 76.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | Compositional codes (Shu and Nakayama, [2017](#bib.bib108))
    | LSTM | Acc (H) | 1.23MB; 78MB | 87.37; 87.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    | LSTM | Acc (H) | 0.81M; 7.19M | 89.7; 88.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    | Logistic regression | Acc (H) | 120M; 120M | 81.4; 77.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 3M Google voice utterances | Joint-SVD (Prabhavalkar
    et al., [2016](#bib.bib93)) | 5-layer RNN | WER (L) | 3.1M; 9.7M | 12.9; 12.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | 3M Google voice utterances | Projections (Sak et al.,
    [2014](#bib.bib101)) | LSTM | WER (L) | 2M; 2.2M | 14.8; 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | Live traffic utterances | WEST (Variani et al., [2019](#bib.bib121))
    | 3-layer LSTM | WER (L) | 4.75MB; 15MB | 13.6; 13.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | Live traffic utterances | WEST+Quantization (Variani
    et al., [2019](#bib.bib121)) | 3-layer LSTM | WER (L) | 1.35MB; 15MB | 13.7; 13.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text classification | 20 Newsgroup (Computers) | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 87.0; 79.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text classification | 20 Newsgroup (Religion) | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 88.8; 86.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text classification | 20 Newsgroup (Sports) | Sparse coding (Faruqui et al.,
    [2015](#bib.bib29)) | Logistic regression | Acc (H) | 120M; 120M | 96.3; 95.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Word similarity | Simlex-999 | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    | Logistic regression | Correlation (H) | 120M; 120M | 38.9; 36.9 |'
  prefs: []
  type: TYPE_TB
- en: Table 7\. Comparison of various tensor decomposition methods (sorted by Task
    and then Dataset). In the metric column, H means high is better while L means
    low is better. For compositional codes, 16x32 coding was used. For BTD, two block
    term tensors were used. In (Faruqui et al., [2015](#bib.bib29)), logistic regression
    uses GloVe embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [7](#S6.T7 "Table 7 ‣ 6.4\. Summary ‣ 6\. Tensor decomposition ‣ Compression
    of Deep Learning Models for Text: A Survey") compares various tensor decomposition
    methods across different tasks and datasets. Accuracy of both the original and
    the compressed (comp.) model are shown. Also, we report model size (in terms of
    number of parameters or memory size) for both the student as well as the teacher
    models. For the same task, dataset and model combination, different papers report
    different accuracy of the original model because of slight changes in training
    hyper-parameters; hence we report accuracy of the original model for each row.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For CTR prediction, the TT-embedding method (Khrulkov et al., [2019](#bib.bib55))
    in Table [7](#S6.T7 "Table 7 ‣ 6.4\. Summary ‣ 6\. Tensor decomposition ‣ Compression
    of Deep Learning Models for Text: A Survey") uses 3 factors with TT-rank of 16\.
    It actually leads to an embedding compression of 16\. With 4 factors and TT-rank=2,
    test loss increases to 0.4530 with a massive embedding compression of 4193 and
    overall model size of 0.53M. Thus, substitution of large embedding layers with
    TT–embeddings leads to significant compression ratios (up to 2011 times) with
    a slight improvement in the test loss, and up to 4200 with a small drop in the
    test loss.'
  prefs: []
  type: TYPE_NORMAL
- en: For language modeling, BTD (Ma et al., [2019](#bib.bib78)) leads to an improved
    model with 20% of the Transformer-XL large model. For character level language
    modeling using FLOP (Wang et al., [2019b](#bib.bib126)), an 8M sized FLOP model
    achieves 1.13 on Enwiki8 while gradual pruning achieves 1.14\. Thus, FLOP based
    pruning which combines pruning with matrix factorization is better than both structured
    neuron pruning as well as gradual unstructured pruning. On PTB, BTD is clearly
    much better than TT-embedding. With half the model size compared to Transformer-XL-Base,
    BTD leads to a model with $\sim$10% lower perplexity. On WikiText-103, while FLOP
    pruned model achieves 25.3 perplexity with model size of 50M, gradual unstructured
    pruning and neuron pruning achieve 25.7 and 26.7 respectively with the same model
    size for language modeling on Wiki-103 dataset. Thus, FLOP is better than other
    pruning methods. Again on Wiki-103, BTD is superior to TT-embedding.
  prefs: []
  type: TYPE_NORMAL
- en: For NMT, the loss-free compression rate reaches 92% on ASPEC dataset by pruning
    90% of the connections. However, with the same pruning ratio, a modest performance
    loss is observed in IWSLT14 dataset. For the models using compositional coding (Shu
    and Nakayama, [2017](#bib.bib108)), the loss-free compression rate is 94% for
    the IWSLT14 dataset and 99% for the ASPEC dataset. Thus, compositional codes are
    much better compared to pruning. TT-embedding with TT-rank=64 leads to embedding
    compression of 15.3 on WMT14 dataset with marginal loss in the BLEU score. Lastly,
    BTD has been used to reduce Transformer size by more than half with improved BLEU
    on WMT16 (en$\rightarrow$de) data.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse coding for GloVe vectors (Faruqui et al., [2015](#bib.bib29)) has led
    to improved accuracies for multiple tasks like Noun Phrase (NP) bracketing, question
    classification, text classification and word similarity, while retaining the same
    model size. For sentiment analysis on the IMDB dataset, compositional codes method
    achieves a compression rate of 98% without performance loss. Further, TT-embedding
    method leads to a much smaller model compared to compositional codes with better
    accuracy. In the TT-embedding case, embedding compression rate is 441 with TT-rank=16.
  prefs: []
  type: TYPE_NORMAL
- en: For Speech recognition, Prabhavalkar et al. (Prabhavalkar et al., [2016](#bib.bib93))
    experimented with a 3M Google voice utterances dataset and found that a joint
    SVD with explained variance retained after the SVD as 0.6 leads to a 3x smaller
    RNN model without much performance loss. They improved upon Sak et al.’s projections
    method which led to a much higher WER on the same dataset. On live traffic utterances
    dataset, WEST (Variani et al., [2019](#bib.bib121)) leads to a 3x smaller model
    with a slightly reduced word error rate when using 3-layer LSTMs. Variani et al. (Variani
    et al., [2019](#bib.bib121)) further compress this model using quantization to
    obtain a 11x compressed 3-layer LSTM model without any performance loss. Thus,
    using quantization along with matrix decomposition seems to work well.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, matrix decomposition techniques are usually used in combination with
    parameter sharing and sometimes with quantization. They have been very effective
    in dealing with large input/output embedding matrices in RNNs and LSTMs. SVD,
    Tensor Train, CP, Tucker, BTD have been the most popular decomposition techniques
    found to be useful for model compression.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Transformers with Sub-Quadratic Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Memory usage throughout neural network training can be categorized into three
    main types: (1) Model memory is used to store model parameters; (2) Optimizer
    memory is the additional memory used by the specific learning algorithm during
    the process; (3) Activation memory consists of the outputs of each layer, which
    are cached for reuse in backpropagation to compute gradients. For a BERT-base
    model, model memory is around 0.2 GB, optimizer memory is around 1GB, while activation
    memory is around 8.5GB (Qiu et al., [2020](#bib.bib95)). Time and activation memory
    in Transformers grows quadratically with the sequence length. This is because
    in every layer, every attention head attempts to come up with a transformed representation
    for every position by “paying attention” to tokens at every other position. Quadratic
    complexity implies that practically the maximum input size is rather limited.
    Thus, we cannot extract semantic representation for long documents by passing
    them as input to Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Transformers with Super-Linear Complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A few efforts like BlockBERT (Qiu et al., [2020](#bib.bib95)) try to reduce
    this quadratic complexity by a constant factor by introducing sparse block structures
    into the attention matrix. If we split the length-$N$ input sequence into $n$
    blocks, $N\times N$ attention matrix gets partitioned into $n\times n$ blocks,
    where each block matrix is of the size $\frac{N}{n}\times\frac{N}{n}$. Thus, BlockBERT
    reduces $O(N^{2})$ memory consumption by a factor of $n$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Child et al. (Child et al., [2019](#bib.bib17)) propose sparse transformers
    where sparse factorizations of the attention matrix reduce the quadratic complexity
    to $O(n\sqrt{n})$. The key idea is to reduce the dense attention matrix to a sparse
    version by only computing attention on a sparse number of (query, key) pairs.
    They propose two kinds of sparse factorizations: strided and fixed. Strided attention
    implies having one head attend to the previous $l$ locations, and the other head
    attend to every $l^{th}$ location, where $l$ is the stride and chosen to be close
    to $\sqrt{n}$. More heads could be used with a different stride value. Fixed attention
    assumes that specific positions summarize previous locations and propagate that
    information to all future positions.'
  prefs: []
  type: TYPE_NORMAL
- en: The Reformer architecture (Kitaev et al., [2020](#bib.bib59)) replaces the dot-product
    attention in a typical Transformer by one that uses locality-sensitive hashing
    (LSH), changing its complexity from $O(n^{2})$ to $O(n\log n)$, where $n$ is the
    length of the sequence. In a standard Transformer, we compute scaled dot-product
    attention as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (68) |  | $\displaystyle\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)V$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Q$, $K$ and $V$ are the standard query, key and value components and
    $d$ is a scaling factor. Reformer uses a Shared QK Transformer, i.e., $Q=K$ enabled
    by sharing the matrix that projects words/hidden layer to $Q$ or $K$. Further,
    note that we are actually only interested in $\text{softmax}(QK^{T})$. Since softmax
    is dominated by the largest elements, for each query $q_{i}$ we only need to focus
    on the keys in $K$ that are closest to $q_{i}$. How can we find the nearest neighbors
    among the keys? Reformer uses LSH. LSH is used to cluster (hash-bucket) the positions
    into various groups, and then every position needs to focus only on others within
    the same bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Transformers with Linear Complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even better, there have been several efforts recently to reduce this quadratic
    complexity to linear. Most of these efforts choose a constant number of other
    positions to “pay attention” to so as to compute a transformed representation
    for any given position. They can model sequences tens of thousands of timesteps
    long using hundreds of layers. The methods differ in their approach towards selecting
    this constant number of other positions. We discuss a few of such recently proposed
    methods in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In Star-Transformers (Guo et al., [2019b](#bib.bib34)), to reduce model complexity
    from $O(n^{2})$ to linear, we replace the fully-connected attention matrix structure
    with a star-shaped topology, in which every two non-adjacent nodes are connected
    through a shared relay node. While ring connections connect a satellite node with
    two other satellite nodes, a radical connection connects a satellite node with
    the relay node. The idea is to update the star-center relay node based on satellite
    nodes and then update satellite nodes using information from the star node, and
    adjacent satellite nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linformer architecture (Wang et al., [2020a](#bib.bib129)) exploits low-rank
    factorization of the self-attention matrix to reduce overall self-attention complexity
    from $O(n^{2})$ to $O(n)$ in both time and space. The main idea is to add two
    linear projection matrices $E_{i},F_{i}\in R^{n\times k}$ when computing key and
    value. We first project the original $(n\times d)$-dimensional key and value layers
    into $(k\times d)$-dimensional projected key and value layers. We then compute
    an $(n\times k)$-dimensional context mapping matrix using scaled dot-product attention.
    If we can choose a very small projected dimension $k$, such that $k<<n$, then
    we can significantly reduce the memory and space consumption. Overall, it is $O(nk)$.
    Further, we can do three other forms of parameter sharing: (1) Headwise sharing:
    $E_{i}=E$ and $F_{i}=F$ across all heads $i$ in a layer. (2) Key-value sharing:
    $E_{i}=F_{i}=E$ across all heads $i$ in a layer. (3) Layerwise sharing: Single
    projection matrix $E$ is used across all layers, all heads for both key and value.'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Sinkhorn Attention based Transformer (Tay et al., [2020](#bib.bib116))
    is based on differentiable sorting of internal representations. First, they divide
    the input sequence into $B$ equal sized blocks each of size $n/B$. A meta sorting
    network learns to generate latent permutations over these block sequences. Given
    sorted sequences, we are then able to compute quasi-global attention with only
    local windows, improving the memory efficiency of the attention module. They also
    propose Causal Sinkhorn Balancing and SortCut algorithms for causal scenarios
    for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Their
    method reduces the memory complexity from $O(n^{2})$ to $O(B^{2}+(n/B)^{2})$.
    The SortCut variant further reduces complexity to linear-time, i.e., $O(nk)$ where
    $k$ is a user defined budget hyper-parameter much smaller than $n$.
  prefs: []
  type: TYPE_NORMAL
- en: Shen et al. (Shen et al., [2018](#bib.bib106)) propose a very simple mathematical
    trick to reduce quadratic complexity to linear. A typical dot-product attention
    can be written as $\text{softmax}(QK^{T})V$ ignoring the scale factor. This is
    quadratic because $QK^{T}$ is $n^{2}$ in size. This can be rewritten as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (69) |  | $\displaystyle\text{Attention}(Q,K,V)=\text{softmax}_{r}(Q)(\text{softmax}_{c}(K^{T}V))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\text{softmax}_{r}$ and $\text{softmax}_{c}$ are softmax applied to rows
    and columns respectively. This revised formulation has terms which are only linear
    in $n$. Finally, Katharopoulos et al. (Katharopoulos et al., [2020](#bib.bib54))
    express the self-attention as a linear dot-product of kernel feature maps and
    make use of the associativity property of matrix products to reduce the complexity
    from $O(n^{2})$ to $O(n)$, where $n$ is the sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Longformer (Beltagy et al., [2020](#bib.bib7)) propose to reduce Transformer
    complexity to linear by sparsifying the full self-attention matrix using multiple
    kinds of attention patterns. The simplest attention pattern is the sliding window
    pattern which employs a fixed-size window attention surrounding each token. Given
    a fixed window size $w$, each token attends to $0.5w$ tokens on each side. The
    computation complexity of this pattern is $O(nw)$ which scales linearly with input
    sequence length $n$. To further increase the receptive field without increasing
    computation, the sliding window can be “dilated”. This is analogous to dilated
    CNNs where the window has gaps of size dilation $d$. The windowed and dilated
    attention are not flexible enough to learn task-specific representations. Accordingly,
    Longformer also has “global attention” on few pre-selected input locations. Moreover,
    this attention operation is symmetric: that is, a token with a global attention
    attends to all tokens across the sequence, and all tokens in the sequence attend
    to it. Further, two different sets of projections are used: $Q_{s}$, $K_{s}$,
    $V_{s}$ to compute attention scores of sliding window attention, and $Q_{g}$,
    $K_{g}$, $V_{g}$ to compute attention scores for the global attention. The pretrained
    Longformer consistently outperforms RoBERTa on multiple downstream long document
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Task | Dataset | Model | Base Model | Metric | Eval. (Opt.; Orig) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Char-level language modeling | 1B Word Benchmark | Sinkhorn Mixture Big (Tay
    et al., [2020](#bib.bib116)) | Transformer Big | BPC (L) | 1.134; 1.825 |'
  prefs: []
  type: TYPE_TB
- en: '| Char-level language modeling | 1B Word Benchmark | Sparse Transformer (Child
    et al., [2019](#bib.bib17)) | Transformer Big | BPC (L) | 1.119; 1.825 |'
  prefs: []
  type: TYPE_TB
- en: '| Char-level language modeling | Enwik8 | Longformer (Beltagy et al., [2020](#bib.bib7))
    | Transformer | BPC (L) | 1.00; 1.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Char-level language modeling | Enwik8 | Reformer (Kitaev et al., [2020](#bib.bib59))
    | Transformer | BPC (L) | 1.05; 1.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Char-level language modeling | text8 | Longformer (Beltagy et al., [2020](#bib.bib7))
    | Transformer | BPC (L) | 1.10; 1.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Coreference resolution | OntoNotes | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 78.6; 78.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | Sinkhorn Mixture Big (Tay et al.,
    [2020](#bib.bib116)) | Transformer Big | Perplexity (L) | 27.34; 27.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | 1B Word Benchmark | Sparse Transformer (Child et al.,
    [2019](#bib.bib17)) | Transformer Big | Perplexity (L) | 28.77; 27.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Named Entity Recognition | CoNLL2003 | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 90.93; 86.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Named Entity Recognition | CoNLL2012 | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 86.30; 83.57 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | MNLI | SortCut Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer
    | Acc (H) | 55.80; 53.69 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | QNLI | Linformer (Wang et al., [2020a](#bib.bib129)) | BERT-base |
    Acc (H) | 91.2; 91.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | SNLI | Star Transformer (Guo et al., [2019b](#bib.bib34)) | Transformer
    | Acc (H) | 86.0; 82.2 |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | SNLI | SortCut Sinkhorn (Tay et al., [2020](#bib.bib116)) | Transformer
    | Acc (H) | 80.30; 78.87 |'
  prefs: []
  type: TYPE_TB
- en: '| NMT (en$\rightarrow$de) | WMT14 | Reformer big (Kitaev et al., [2020](#bib.bib59))
    | Transformer Big | BLEU (H) | 29.1; 27.3 |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | PTB | Star Transformer (Guo et al., [2019b](#bib.bib34)) |
    Transformer | Acc (H) | 97.14; 96.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | HotpotQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 78.94; 77.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | HotpotQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 76.02; 77.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | NaturalQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 79.39; 78.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | NaturalQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 77.31; 78.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | NewsQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 70.08; 66.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | NewsQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 67.16; 66.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SearchQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 83.51; 80.37 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SearchQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 80.54; 80.37 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD 1.1 | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 90.74; 88.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD 1.1 | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 88.37; 88.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD 2.0 | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 81.45; 77.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | SQuAD 2.0 | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 77.57; 77.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | TriviaQA | BlockBERT (n=2, N=1024) (Qiu et al., [2020](#bib.bib95))
    | BERT | F1 (H) | 79.41; 75.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | TriviaQA | SparseBERT (Child et al., [2019](#bib.bib17))
    | BERT | F1 (H) | 75.34; 75.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | TriviaQA | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 75.2; 74.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | WikiHop | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | Acc (H) | 75.0; 72.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | Linformer (Wang et al., [2020a](#bib.bib129))
    | BERT-base | Acc (H) | 94.1; 93.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | IMDB | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | Acc (H) | 95.7; 95.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 52.9; 50.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST | Sinkhorn  (Tay et al., [2020](#bib.bib116)) |
    Transformer | Acc (H) | 77.52; 76.83 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | SST-2 | Linformer (Wang et al., [2020a](#bib.bib129))
    | BERT-base | Acc (H) | 93.1; 92.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech recognition | WSJ | Linear Transformer (Katharopoulos et al., [2020](#bib.bib54))
    | Reformer | Phoneme Error Rate (L) | 8.08; 9.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Text classification | Hyperpartisan | Longformer-base (Beltagy et al., [2020](#bib.bib7))
    | RoBERTa-base | F1 (H) | 94.8; 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Text classification | MTL-16 | Star Transformer (Guo et al., [2019b](#bib.bib34))
    | Transformer | Acc (H) | 86.98; 82.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Textual similarity | QQP | Linformer (Wang et al., [2020a](#bib.bib129))
    | BERT-base | Acc (H) | 90.8; 89.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8\. Comparison of various sub-quadratic complexity Transformer methods
    (sorted by Task and then Dataset). In the metric column, H means high is better
    while L means low is better. Note that in this case, model sizes do not reduce
    much; activation memory reduces as described in Section [7](#S7 "7\. Transformers
    with Sub-Quadratic Complexity ‣ Compression of Deep Learning Models for Text:
    A Survey") (with comparable or better accuracy) compared to the standard Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [8](#S7.T8 "Table 8 ‣ 7.3\. Summary ‣ 7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey") compares
    various sub-quadratic Transformer methods across different tasks and datasets.
    Accuracy of both the original Transformer and the optimized (opt.) model are shown.
    These models have been applied for various applications like language modeling
    (both word level as well as character level), coreference resolution, NER, NLI,
    NMT, POS, question answering, sentiment analysis, speech recognition, text classification
    and text similarity analysis. For the same task, dataset and model combination,
    different papers report different accuracy of the original model because of slight
    changes in training hyper-parameters; hence we report accuracy of the original
    model for each row.'
  prefs: []
  type: TYPE_NORMAL
- en: Star Transformers were shown to outperform the vanilla Transformer model across
    various tasks like Sentiment analysis, Text classification, NLI, POS and NER.
    On the SST, the Star Transformer achieves 2.5 points improvement against the standard
    Transformer. On MTL-16, the Star-Transformer outperform the standard Transformer
    in all 16 datasets, the improvement of the average accuracy is 4.2\. Average test
    times are 10.94 and 49.31ms per batch with batch-size=128 for Star Transformer
    and standard Transformer respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Longformer achieves a new state-of-the-art on both text8 and enwik8 using the
    small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively. Longformer-large
    model of the same size as Sparse Transformer achieves a BPC of 0.99 on Enwik8,
    which is the same as that obtained using Sparse Transformer. Also, for both character-level
    as well as word-level language modeling, on 1B word benchmark, we observe that
    Sinkhorn mixture (which is a combination of the Sinkhorn attention by mixing it
    with the vanilla standard dot product attention) performs better than Sparse Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: On question answering, results have been reported across multiple datasets like
    HotpotQA, NaturalQA, NewsQA, SearchQA, TriviaQA, WikiHop, SQuAD 1.1 and SQuAD
    2.0\. We observe that BlockBERT performs better than SparseBERT. Also, it is not
    surprising that BlockBERT with 2 blocks (n = 2) performs better than that with
    3 blocks (n = 3), because it keeps more attention matrix entries. Also, not shown
    in the table, Longformer-large achieves scores of 81.9 and 73.2 for WikiHop and
    HotpotQA beating state-of-the-art results by 3.6 and 4 points respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Longformer consistently outperforms the RoBERTa baseline across many tasks like
    coreference resolution, question answering, sentiment analysis and text classification.
    Its performance gain is especially obvious for tasks that require long context
    such as WikiHop (question answering) and Hyperpartisan (text classification).
    For TriviaQA (question answering), the improvement is more modest as the local
    context is often sufficient to answer the question. In the case of HotpotQA (question
    answering), the supporting fact auxiliary supervision allows models to easily
    find relevant contexts and then focus on local context, leading to smaller gains.
    On the IMDB (sentiment analysis) and OntoNotes (coreference resolution) datasets
    the performance gains are smaller. For IMDB, the majority of the dataset consists
    of short documents and thus it is expected to see smaller improvements. For OntoNotes,
    the distance between any two mentions is typically quite small so that a baseline
    that processes smaller chunks separately is able to stitch together mentions into
    coreference chains without considering cross chunk interactions.
  prefs: []
  type: TYPE_NORMAL
- en: On speech recognition task, Linear transformers achieve similar performance
    to vanilla transformers and they are up to 4000x faster on autoregressive prediction
    of very long sequences. The linear Transformer model outperforms the LSTM and
    Reformer while being faster to train and evaluate. Reformer takes 2250 seconds
    per epoch, while Linear Transformers take just 824s/epoch.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, multiple methods have been proposed to reduce the quadratic complexity
    of the standard Transformer model. While Sparse Transformers reduce it to $O(n\sqrt{n})$,
    Reformers reduce it to $O(n\log n)$. Other methods like Star Transformer, Linformer,
    Sparse Sinkhorn Transformer, Efficient Attention, Linear Transformers and Longformer
    promise linear complexity. In particular Sparse Sinkhorn Transformer and Longformer
    have been shown to result into very good accuracy, latency and RAM tradeoff across
    many tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Summary and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed various methods for compression of deep learning models for text.
    Broadly, we discussed pruning, quantization, knowledge distillation, parameter
    sharing, tensor decomposition, and sub-quadratic Transformer based methods. These
    methods not just help reduce the model size, but also lead to lower prediction
    latencies and low power consumption due to reduced computations. Pruning is the
    oldest method, but not commonly applied for Transformers. Quantization is effective
    however it is important to use mixed precision balanced quantization with GPU
    architectures that support efficient low-bit computations. Knowledge Distillation
    is the most popular method for compression of Transformer models. Parameter sharing
    is a very useful method but often needs to be combined with other techniques.
    Matrix decomposition is not very common but has lots of potential, especially
    the BTD method. Sub-quadratic Transformers are very important to enable processing
    long documents in applications like query-document similarity, long document summarization,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Comparison across model compression method types.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous six sections, we have compared across multiple methods within
    each of the six broad types of model compression. In this subsection, we attempt
    to compare across multiple model compression method types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5\. Summary ‣ 4\. Knowledge Distillation (KD)
    ‣ Compression of Deep Learning Models for Text: A Survey") provides a comparison
    of model size versus accuracy for various methods across various GLUE (Wang et al.,
    [2019b](#bib.bib126)) and SQuAD tasks. We observe that most of the methods tried
    on GLUE datasets have been based on knowledge distillation. However, some pruning
    methods (iterative magnitude pruning, RPP and LayerDrop), quantization (mixed
    precision QBERT), parameter sharing (ALBERT), tensor decomposition (FLOP) and
    linear Transformer (Linformer) have also been tried. Quantization methods do not
    reduce the number of parameters but reduce the number of bits per parameter. Particularly,
    mixed precision QBERT uses 8 bits for embeddings and 2/3/4 bits for encoder layer
    weights. Similarly, Linformer does not reduce number of weights but reduces activation
    memory as well as latency of the overall Transformer model. For the remaining
    models, for each model, we first computed an average GLUE score based on any of
    the 9 tasks for which scores have been reported. Next, we computed the ratio GLUE
    score/model size (in Millions). We find the following as the top three'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)) (ratio=79.3)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed-vocab. training (Zhao et al., [2019](#bib.bib143)) (ratio=7.8)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ALBERT-B (Lan et al., [2019](#bib.bib61)) (ratio=7.2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This clearly tells us that Distilled BiLSTMs provide us the best accuracy versus
    size tradeoff on GLUE. However, each of these three models actually report results
    on only 4 out of 9 GLUE tasks. Hence, further, we considered only those methods
    for which results on at least 5 tasks have been reported and computed the GLUE
    score/model size ratio. We find the following as the top three
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TinyBERT (Jiao et al., [2019](#bib.bib51)) (ratio=5.31)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT-EMD (Li et al., [2020](#bib.bib67)) (ratio=5.15)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileBERT+Quantization (Sun et al., [2020](#bib.bib112)) (ratio=3.49)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus, on GLUE tasks, it is clear that distillation based methods (combined with
    quantization) are better than other types of methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tables [1](#S2.T1 "Table 1 ‣ 2.4.3\. Pruning General Structures ‣ 2.4\. Pruning
    Heads and Layers ‣ 2\. Pruning ‣ Compression of Deep Learning Models for Text:
    A Survey"), [2](#S3.T2 "Table 2 ‣ 3.4\. Summary ‣ 3\. Quantization ‣ Compression
    of Deep Learning Models for Text: A Survey"), [3](#S4.T3 "Table 3 ‣ 4.5\. Summary
    ‣ 4\. Knowledge Distillation (KD) ‣ Compression of Deep Learning Models for Text:
    A Survey"), [6](#S5.T6 "Table 6 ‣ 5.4\. Summary ‣ 5\. Parameter sharing ‣ Compression
    of Deep Learning Models for Text: A Survey"), [7](#S6.T7 "Table 7 ‣ 6.4\. Summary
    ‣ 6\. Tensor decomposition ‣ Compression of Deep Learning Models for Text: A Survey")
    and [8](#S7.T8 "Table 8 ‣ 7.3\. Summary ‣ 7\. Transformers with Sub-Quadratic
    Complexity ‣ Compression of Deep Learning Models for Text: A Survey") compare
    various pruning, quantization, knowledge distillation, parameter sharing, tensor
    decomposition and sub-quadratic Transformer methods across different tasks and
    datasets. Overall, there are 123 (task, dataset) combinations across these six
    tables which implies that unlike GLUE, not many methods have been applied on the
    same set of (task, dataset) combinations⁴⁴4We make the entire statistics available
    as an excel file at [https://bit.ly/3vmaxZ9](https://bit.ly/3vmaxZ9).. The most
    popular tasks are language modeling (on PTB and 1B word benchmark), sentiment
    analysis (on SST) and NMT (WMT14 en$\rightarrow$de).'
  prefs: []
  type: TYPE_NORMAL
- en: For language modeling on PTB, on LSTM models, bank balanced sparsity (Cao et al.,
    [2019](#bib.bib10)) based pruning method worked best. With Transformer models,
    Block Term Decomposition (BTD) (Ma et al., [2019](#bib.bib78)) method seems to
    work best. Among various methods like parameter sharing, tensor decomposition
    and sub-quadratic complexity Transformer which have been tried for language modeling
    on 1B Word Benchmark, again, BTD (Ma et al., [2019](#bib.bib78)) method seems
    to work best leading to a model with 0.16B parameters and a perplexity as low
    as 19.5\. Multiple datasets have been used for Neural machine translation (NMT).
    Datasets from WMT and IWSLT are the most popular. Among these, the en$\rightarrow$de
    from WMT14 is the most popular dataset used for testing various NMT models. For
    en$\rightarrow$de NMT with WMT14, using 2-layer LSTMs, the best accuracy versus
    size tradeoff is using Pruned Seq-KD + Seq-Inter (Kim and Rush, [2016](#bib.bib57))
    which gives a 8M size model leading to 18.5 BLEU. Among Transformer based models,
    BTD (Ma et al., [2019](#bib.bib78)) leads to a Transformer model which provides
    34.91 BLEU with 21.2M model size.
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Popular Datasets | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Language modeling | Penn TreeBank Corpus, One billion word benchmark, Europarl,
    WikiText-103, text8, source code of Linux kernel, 2013 ACL Workshop Morphological
    Language Datasets (ACLW), Arabic news commentary corpus, 2013 ACL workshop on
    MT, enwik8 (from Wikipedia), Lambada | Neuron Pruning (Murray and Chiang, [2015](#bib.bib86)),
    Iterative magnitude pruning (Zhu and Gupta, [2017](#bib.bib146)), Block sparsity (Cao
    et al., [2019](#bib.bib10)),Loss -Aware Quantization (Xu et al., [2018](#bib.bib133);
    Hou and Kwok, [2018](#bib.bib45)), Uniform Quantization (He et al., [2016](#bib.bib41);
    Kapur et al., [2017](#bib.bib53)), Binary Quantization (Hubara et al., [2017](#bib.bib48)),
    HitNet (Wang et al., [2018](#bib.bib128)), Sparse Word Representations (Chen et al.,
    [2016](#bib.bib14)), LightRNN (Li et al., [2016a](#bib.bib68)), Slim Embeddings (Li
    et al., [2018](#bib.bib69)), C2W (Ling et al., [2015](#bib.bib72)), LayerDrop (Fan
    et al., [2019](#bib.bib28)), Reformer (Kitaev et al., [2020](#bib.bib59)), Linformer (Wang
    et al., [2020a](#bib.bib129)), Char-CNN (Jozefowicz et al., [2016](#bib.bib52)),
    CNN+Highway Network (Kim et al., [2016](#bib.bib56)), SparseBERT (Child et al.,
    [2019](#bib.bib17)), FLOP (Wang et al., [2019c](#bib.bib131)), Deep Equilibrium
    Models (Bai et al., [2019](#bib.bib5)), WEST (Variani et al., [2019](#bib.bib121)),
    Sparse Sinkhorn Attention (Tay et al., [2020](#bib.bib116)), BTD (Ma et al., [2019](#bib.bib78)),
    Universal Transformers (Dehghani et al., [2018](#bib.bib22)), TT-embedding (Khrulkov
    et al., [2019](#bib.bib55)), multiple methods (Grachev et al., [2019](#bib.bib32))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Neural Machine translation (NMT) | IWSLT German-English, IWSLT Thai-English,
    ASPEC English-Japanese, WMT English-German, WMT German-English, WMT English-Russian,
    IWSLT English Vietnamese, WMT English-Romanian, WMT English-Estonian, Ted Talk
    | Compositional codes (Shu and Nakayama, [2017](#bib.bib108)), LayerDrop (Fan
    et al., [2019](#bib.bib28)), Pruning attention heads (Voita et al., [2019](#bib.bib123);
    Michel et al., [2019](#bib.bib80)), Neuron Pruning (Murray and Chiang, [2015](#bib.bib86)),
    Magnitude Pruning (See et al., [2016](#bib.bib104)), Iterative magnitude pruning (Zhu
    and Gupta, [2017](#bib.bib146); Cheong and Daniel, [2019](#bib.bib16)), Pruned
    Seq-KD + Seq-Inter (Kim and Rush, [2016](#bib.bib57)), Quantized Distillation (Polino
    et al., [2018](#bib.bib92)), Teacher ensembles KD (Freitag et al., [2017](#bib.bib30)),
    Multiple teachers KD (Tan et al., [2019](#bib.bib114)), BTD (Ma et al., [2019](#bib.bib78)),
    Quaternion Attention (Tay et al., [2019](#bib.bib117)), Universal Transformers (Dehghani
    et al., [2018](#bib.bib22)), TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment Analysis | IMDB movie review, SST, SST-2, Elec (electronic product
    reviews) | Compositional codes (Shu and Nakayama, [2017](#bib.bib108)), Star Transformer (Guo
    et al., [2019b](#bib.bib34)), TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang
    et al., [2020b](#bib.bib130)), Linformer (Wang et al., [2020a](#bib.bib129)),
    XtremeDistil (Mukherjee and Awadallah, [2020](#bib.bib84)), Gaussian Quantization (Alom
    et al., [2018](#bib.bib2)), Uniform Quantization (He et al., [2016](#bib.bib41)),
    Sparse coding (Faruqui et al., [2015](#bib.bib29)), Quaternion Attention (Tay
    et al., [2019](#bib.bib117)), Sparse Sinkhorn Attention (Tay et al., [2020](#bib.bib116)),
    RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan et al., [2019](#bib.bib61)),
    Patient KD (Sun et al., [2019](#bib.bib111)), Mixed-vocabulary KD training (Zhao
    et al., [2019](#bib.bib143)), Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)),
    MTDNN (Liu et al., [2019b](#bib.bib75)), TT-embedding (Khrulkov et al., [2019](#bib.bib55))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Question Answering | SQuAD1.1, SQuAD2.0, ELI5, SemEval, BABI | LayerDrop (Fan
    et al., [2019](#bib.bib28)), MiniLM (Wang et al., [2020b](#bib.bib130)), RPP (Guo
    et al., [2019a](#bib.bib33)), BS-Fixed Quantization (Lam, [2018](#bib.bib60)),
    ALBERT (Lan et al., [2019](#bib.bib61)), KD (Damani et al., [2020](#bib.bib21)),
    Universal Transformers (Dehghani et al., [2018](#bib.bib22)) |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Language Inference | SNLI, MNLI-m, MNLI-mm, QNLI, RTE, WNLI, XNLI
    | Star Transformer (Guo et al., [2019b](#bib.bib34)), LayerDrop (Fan et al., [2019](#bib.bib28)),
    TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang et al., [2020b](#bib.bib130)),
    Linformer (Wang et al., [2020a](#bib.bib129)), Sparse Sinkhorn Attention (Tay
    et al., [2020](#bib.bib116)), RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan
    et al., [2019](#bib.bib61)), Patient KD (Sun et al., [2019](#bib.bib111)), Mixed-vocabulary
    KD training (Zhao et al., [2019](#bib.bib143)), Distilled-BiLSTM (Tang et al.,
    [2019](#bib.bib115)), MTDNN (Liu et al., [2019b](#bib.bib75)) |'
  prefs: []
  type: TYPE_TB
- en: '| Paraphrasing | QQP, STS-B | TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang
    et al., [2020b](#bib.bib130)), Linformer (Wang et al., [2020a](#bib.bib129)),
    RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan et al., [2019](#bib.bib61)),
    Patient KD (Sun et al., [2019](#bib.bib111)), Distilled-BiLSTM (Tang et al., [2019](#bib.bib115)),
    MTDNN (Liu et al., [2019b](#bib.bib75)) |'
  prefs: []
  type: TYPE_TB
- en: '| Image captioning | MSCOCO | Grow and Prune (Dai et al., [2018](#bib.bib20)),
    Magnitude Pruning (Han et al., [2015a](#bib.bib37)), Iterative Magnitude Pruning
    and Densification (Han et al., [2016b](#bib.bib38)) |'
  prefs: []
  type: TYPE_TB
- en: '| Handwritten character recognition | ICDAR | SVD and Pruning(Yang et al.,
    [2018](#bib.bib135)) |'
  prefs: []
  type: TYPE_TB
- en: '| Part-of-speech (POS) tagging | Wall Street Journal of the Penn Treebank dataset,
    WikiAnn NER corpus | C2W (Ling et al., [2015](#bib.bib72)), XtremeDistil (Mukherjee
    and Awadallah, [2020](#bib.bib84)) |'
  prefs: []
  type: TYPE_TB
- en: '| Summarization | CNN-DailyMail, XSum | LayerDrop (Fan et al., [2019](#bib.bib28)),
    MiniLM (Wang et al., [2020b](#bib.bib130)) |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Reading Comprehension | Microsoft Research Paraphrase Corpus (MRPC),
    ReAding Comprehension from Examinations (RACE) | LayerDrop (Fan et al., [2019](#bib.bib28)),
    TinyBERT (Jiao et al., [2019](#bib.bib51)), MiniLM (Wang et al., [2020b](#bib.bib130)),
    RPP (Guo et al., [2019a](#bib.bib33)), ALBERT (Lan et al., [2019](#bib.bib61)),
    Patient KD (Sun et al., [2019](#bib.bib111)), Mixed-vocabulary KD training (Zhao
    et al., [2019](#bib.bib143)), MTDNN (Liu et al., [2019b](#bib.bib75)) |'
  prefs: []
  type: TYPE_TB
- en: '| Linguistic Acceptability | CoLA | TinyBERT (Jiao et al., [2019](#bib.bib51)),
    MiniLM (Wang et al., [2020b](#bib.bib130)), RPP (Guo et al., [2019a](#bib.bib33)),
    ALBERT (Lan et al., [2019](#bib.bib61)), MTDNN (Liu et al., [2019b](#bib.bib75))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Topic Classification | DbPedia, Ag News,20 Newsgroup | XtremeDistil (Mukherjee
    and Awadallah, [2020](#bib.bib84)), Sparse coding (Faruqui et al., [2015](#bib.bib29))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Question Type Classification | TREC | Sparse coding (Faruqui et al., [2015](#bib.bib29))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Noun Phrase Bracketing | Lazaridou (Lazaridou et al., [2013](#bib.bib62))
    | Sparse coding (Faruqui et al., [2015](#bib.bib29)) |'
  prefs: []
  type: TYPE_TB
- en: '| Word Similarity | SimLex-999, MEN, MTurk, RARE, SCWS, WSR, WSS | Sparse coding (Faruqui
    et al., [2015](#bib.bib29)), Shared reference vectors (Suzuki and Nagata, [2016](#bib.bib113))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mathematical Language Understanding | Wangperawong’s MLU (Wangperawong, [2018](#bib.bib132))
    | Quaternion Attention (Tay et al., [2019](#bib.bib117)) |'
  prefs: []
  type: TYPE_TB
- en: '| Subject Verb Agreement | Linzen (Linzen et al., [2016](#bib.bib73)) | Quaternion
    Attention (Tay et al., [2019](#bib.bib117)), Universal Transformers (Dehghani
    et al., [2018](#bib.bib22)) |'
  prefs: []
  type: TYPE_TB
- en: '| Word Analogy | GSEM, GSYN, MSYN | Shared reference vectors (Suzuki and Nagata,
    [2016](#bib.bib113)) |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence Completion | MSC | Shared reference vectors (Suzuki and Nagata,
    [2016](#bib.bib113)) |'
  prefs: []
  type: TYPE_TB
- en: '| Learning to execute | Zaremba and Sutskever (Zaremba and Sutskever, [2014](#bib.bib141))
    | Universal Transformers (Dehghani et al., [2018](#bib.bib22)) |'
  prefs: []
  type: TYPE_TB
- en: '| Ad Click Through Rate Prediction | Criteo Kaggle | TT-embedding (Khrulkov
    et al., [2019](#bib.bib55)) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech Recognition | 2100 hours English Speech, AN4, Switchboard, TIMIT,
    WSJ 92, WSJ 93, TIDIGITS, 3M Google voice utterances, Live traffic utterances
    | Iterative Magnitude Pruning (Narang et al., [2017a](#bib.bib87)), Grow and Prune (Dai
    et al., [2018](#bib.bib20)), Neuron Pruning (He et al., [2014](#bib.bib42)), Block
    Sparsity (Cao et al., [2019](#bib.bib10)), BBS (Cao et al., [2019](#bib.bib10)),
    DSD (Han et al., [2016b](#bib.bib38)), Pow2 Ternarization (Ott et al., [2016](#bib.bib90)),
    Loss Aware Quantization (Hwang and Sung, [2014](#bib.bib49)), Toeplitz-like (Lu
    et al., [2016](#bib.bib77)), Joint-SVD (Prabhavalkar et al., [2016](#bib.bib93)),
    Projections (Sak et al., [2014](#bib.bib101)), WEST (Variani et al., [2019](#bib.bib121))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Named entity recognition (NER) | CoNLL2003, Wikiann-41, CoNLL2012 | QBERT (Shen
    et al., [2019](#bib.bib105)), XtremeDistill (Mukherjee and Awadallah, [2020](#bib.bib84)),
    Star Transformer (Guo et al., [2019b](#bib.bib34)) |'
  prefs: []
  type: TYPE_TB
- en: '| Intent Detection | SNIPS | Mixed-vocabulary KD training (Zhao et al., [2019](#bib.bib143))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Question Generation | SQuAD 1.1 | MiniLM (Wang et al., [2020b](#bib.bib130))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Slot Filling | SNIPS | Mixed-vocabulary KD training (Zhao et al., [2019](#bib.bib143))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text classification | 20 Newsgroup, Hyperpartisan, MTL-16 | Sparse coding (Faruqui
    et al., [2015](#bib.bib29)), Longformer (Beltagy et al., [2020](#bib.bib7)), Star
    Transformer (Guo et al., [2019b](#bib.bib34)) |'
  prefs: []
  type: TYPE_TB
- en: '| Coreference Resolution | OntoNotes | Longformer (Beltagy et al., [2020](#bib.bib7))
    |'
  prefs: []
  type: TYPE_TB
- en: Table 9\. Applications of Model Compression Methods for Text
  prefs: []
  type: TYPE_NORMAL
- en: 'Combinations of multiple model compression method types has also been experimented
    with and found to be effective. Some examples of such combinations include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning + Tensor Decomposition  (He et al., [2014](#bib.bib42); Wang et al.,
    [2019c](#bib.bib131))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning + Quantization (Cao et al., [2019](#bib.bib10))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge distillation + Quantization (Bengio et al., [2013](#bib.bib8); Mishra
    and Marr, [2017](#bib.bib83); Polino et al., [2018](#bib.bib92); Sun et al., [2020](#bib.bib112))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge distillation + Pruning (Kim and Rush, [2016](#bib.bib57))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensor decomposition + Parameter sharing (Lu et al., [2016](#bib.bib77); Lan
    et al., [2019](#bib.bib61))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensor decomposition + Quantization (Shu and Nakayama, [2017](#bib.bib108))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Recently, Kim et al. (Kim and Awadalla, [2020](#bib.bib58)) combined knowledge
    distillation, structured pruning and quantization leading to drastic improvements
    on inference efficiency. First, they investigate the efficacy of various Knowledge
    Distillation techniques to significantly reduce the size of the models with respect
    to the depth and hidden state sizes while preserving the accuracy. Second, they
    explore Structured Pruning that further reduces the size of the models by reducing
    the number of self-attention heads and the number of intermediate hidden states
    in the feedforward layers to achieve more efficiency while trying to preserve
    the accuracy as well. Finally, they explore model quantization which enables faster
    model executions by optimally utilizing hardware acceleration capabilities. Such
    a combined method leads to heavily reduced model size, 12.4x GPU speed-up and
    6.9x-125.8x reduction in energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2\. Summary of Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model compression methods mentioned in this survey have been used across
    a wide variety of text processing tasks. In Table [9](#S8.T9 "Table 9 ‣ 8.1\.
    Comparison across model compression method types. ‣ 8\. Summary and Future Directions
    ‣ Compression of Deep Learning Models for Text: A Survey"), we list down the tasks,
    popular datasets and references where the readers can find more discussion around
    model size versus accuracy tradeoff.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3\. Future Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although there has been so much of work already in this field, there is a lot
    more work to be done.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With linear Transformer models, one can afford to have input with tens of thousands
    of tokens. Hence, many tasks need to be redesigned where large context can now
    be included as input to improve accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combinations of several methods have not been tested well. Recently, Fastformers
    method (Kim and Awadalla, [2020](#bib.bib58)) showed that combining multiple methods
    like knowledge distillation, 16-bit quantization, structured pruning and numerical
    optimizations can lead to drastic improvements. However, lot of experiments are
    needed to further check how models respond to combination of model compression
    methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency results vary based on GPU architectures. With new GPU architectures
    (Nvidia RTX 3080, Nvidia T4), some methods like quantization may become more impactful.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Real world settings are often complex: multi-modal, multi-task, multi-label,
    small-data, noisy labels, multi-teachers, mismatching teacher-student architectures.
    Efficient ways of recommending the most promising method is necessary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different components/structures of a model may respond to different kinds of
    compression methods with specific hyper-parameters. A generic method to choose
    the right method for various structures is needed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does compression of models impact their interpretability? Can we design
    model compression mechanisms aimed at looking at a tradeoff between model accuracy,
    size, latency and interpretability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None of the model compression methods performs any application specific compression.
    Can we obtain further compression by exploiting some task-specific patterns?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently, deep reinforcement learning based methods have been proposed in the
    computer vision community (He et al., [2018](#bib.bib43); Yuan et al., [2019](#bib.bib140)).
    It will be nice to check the effectiveness of such methods for NLP tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We hope that this survey acts as a good guide to folks across academia and industry.
    Also, we hope that a significantly large chunk of research gets done in the area
    of model compression to enable good accuracy across many NLP tasks while keeping
    model sizes and latencies in check.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alom et al. (2018) Md Zahangir Alom, Adam T Moody, Naoya Maruyama, Brian C Van Essen,
    and Tarek M Taha. 2018. Effective quantization approaches for recurrent neural
    networks. In *IJCNN*. IEEE, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2018) Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi,
    George E Dahl, and Geoffrey E Hinton. 2018. Large scale distributed neural network
    training through online distillation. *arXiv:1804.03235* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba and Caruana (2014) Jimmy Ba and Rich Caruana. 2014. Do deep nets really need
    to be deep?. In *NIPS*. 2654–2662.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2019) Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2019. Deep
    equilibrium models. *arXiv:1909.01377* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bartol et al. (2015) Thomas M Bartol, Cailey Bromer, Justin Kinney, Michael A
    Chirillo, Jennifer N Bourne, Kristen M Harris, and Terrence J Sejnowski. 2015.
    Hippocampal spine head sizes are highly precise. *bioRxiv* (2015), 016329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv:1308.3432* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianco et al. (2018) Simone Bianco, Remi Cadene, Luigi Celona, and Paolo Napoletano.
    2018. Benchmark analysis of representative deep neural network architectures.
    *IEEE Access* 6 (2018), 64270–64277.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2019) Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun
    Nie, Dechen Zhan, Yunxin Liu, Ming Wu, and Lintao Zhang. 2019. Efficient and effective
    sparse LSTM on FPGA with Bank-Balanced Sparsity. In *SIGDA Intl. Symp. on FPGA*.
    ACM, 63–72.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carroll and Chang (1970) J Douglas Carroll and Jih-Jie Chang. 1970. Analysis
    of individual differences in multidimensional scaling via an N-way generalization
    of “Eckart-Young” decomposition. *Psychometrika* 35, 3 (1970), 283–319.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2015a) Welin Chen, David Grangier, and Michael Auli. 2015a. Strategies
    for training large vocabulary neural language models. *arXiv:1512.04906* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2015b) Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger,
    and Yixin Chen. 2015b. Compressing neural networks with the hashing trick. In
    *ICML*. 2285–2294.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016) Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016.
    Compressing neural language models by sparse word representations. *arXiv:1610.03950*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey
    of model compression and acceleration for deep neural networks. *arXiv:1710.09282*
    (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheong and Daniel (2019) Robin Cheong and Robel Daniel. 2019. *transformers.
    zip: Compressing Transformers with Pruning and Quantization*. Technical Report.
    Technical report, Stanford University, Stanford, California, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    2019. Generating long sequences with sparse transformers. *arXiv:1904.10509* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Courbariaux et al. (2015) Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
    David. 2015. Binaryconnect: Training deep neural networks with binary weights
    during propagations. In *NIPS*. 3123–3131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Czarnecki et al. (2017) Wojciech M Czarnecki, Simon Osindero, Max Jaderberg,
    Grzegorz Swirszcz, and Razvan Pascanu. 2017. Sobolev training for neural networks.
    In *NIPS*. 4278–4287.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2018) Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. 2018. Grow and
    prune compact, fast, and accurate LSTMs. *arXiv:1805.11797* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Damani et al. (2020) Sonam Damani, Kedhar Nath Narahari, Ankush Chatterjee,
    Manish Gupta, and Puneet Agrawal. 2020. Optimized Transformer Models for FAQ Answering.
    In *PAKDD*. To appear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dehghani et al. (2018) Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob
    Uszkoreit, and Łukasz Kaiser. 2018. Universal transformers. *arXiv:1807.03819*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2020) Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.
    2020. Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive
    Survey. *IEEE* 108, 4 (2020), 485–532.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denil et al. (2013) Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato,
    and Nando De Freitas. 2013. Predicting parameters in deep learning. In *NIPS*.
    2148–2156.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diamos et al. (2016) Greg Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski,
    Adam Coates, Erich Elsen, Jesse Engel, Awni Hannun, and Sanjeev Satheesh. 2016.
    Persistent rnns: Stashing recurrent weights on-chip. In *ICML*. 2024–2033.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2017) Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017.
    Visualizing and understanding neural machine translation. In *ACL*. 1150–1159.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2019) Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing
    transformer depth on demand with structured dropout. *arXiv:1909.11556* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faruqui et al. (2015) Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer,
    and Noah Smith. 2015. Sparse overcomplete word vector representations. *arXiv:1506.02004*
    (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freitag et al. (2017) Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran.
    2017. Ensemble distillation for neural machine translation. *arXiv:1702.01802*
    (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2014) Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. 2014.
    Compressing deep convolutional networks using vector quantization. *arXiv:1412.6115*
    (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grachev et al. (2019) Artem M Grachev, Dmitry I Ignatov, and Andrey V Savchenko.
    2019. Compression of recurrent neural networks for efficient language modeling.
    *Applied Soft Computing* 79 (2019), 354–362.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019a) Fu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi
    Wang. 2019a. Reweighted Proximal Pruning for Large-Scale Language Representation.
    *arXiv:1909.12486* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019b) Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang
    Xue, and Zheng Zhang. 2019b. Star-Transformer. In *NAACL-HLT*. 1315–1325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2017) Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen. 2017. Network
    sketching: Exploiting binary structure in deep cnns. In *CVPR*. 5955–5963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2016a) Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram,
    Mark A Horowitz, and William J Dally. 2016a. EIE: efficient inference engine on
    compressed deep neural network. *ACM SIGARCH Computer Architecture News* 44, 3
    (2016), 243–254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015a) Song Han, Huizi Mao, and William J Dally. 2015a. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv:1510.00149* (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2016b) Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong,
    Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, et al. 2016b.
    DSD: Dense-sparse-dense training for deep neural networks. *arXiv:1607.04381*
    (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015b) Song Han, Jeff Pool, John Tran, and William Dally. 2015b.
    Learning both weights and connections for efficient neural network. In *NIPS*.
    1135–1143.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassibi and Stork (1993) Babak Hassibi and David G Stork. 1993. Second order
    derivatives for network pruning: Optimal brain surgeon. In *NIPS*. 164–171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu
    Zhou, and Yuheng Zou. 2016. Effective quantization methods for recurrent neural
    networks. *arXiv:1611.10176* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2014) Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu.
    2014. Reshaping deep neural network for fast decoding by node-pruning. In *ICASSP*.
    IEEE, 245–249.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. 2018. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European Conference on Computer Vision (ECCV)*. 784–800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv:1503.02531* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou and Kwok (2018) Lu Hou and James T Kwok. 2018. Loss-aware weight quantization
    of deep networks. *arXiv:1802.08635* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. (2016) Lu Hou, Quanming Yao, and James T Kwok. 2016. Loss-aware binarization
    of deep networks. *arXiv:1611.01600* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *NIPS*. 4107–4115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural
    networks with low precision weights and activations. *JMLR* 18, 1 (2017), 6869–6898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang and Sung (2014) Kyuyeon Hwang and Wonyong Sung. 2014. Fixed-point feedforward
    deep neural network design using weights +1, 0, and -1\. In *SiPS*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2020) Forrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W
    Keutzer. 2020. SqueezeBERT: What can computer vision teach NLP about efficient
    neural networks? *arXiv preprint arXiv:2006.11316* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural
    language understanding. *arXiv:1909.10351* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jozefowicz et al. (2016) Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
    Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. *arXiv:1602.02410*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kapur et al. (2017) Supriya Kapur, Asit Mishra, and Debbie Marr. 2017. Low
    precision RNNs: Quantizing RNNs without losing accuracy. *arXiv:1710.07706* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers
    with Linear Attention. *arXiv:2006.16236* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khrulkov et al. (2019) Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova,
    and Ivan Oseledets. 2019. Tensorized Embedding Layers for Efficient Model Compression.
    *arXiv:1901.10787* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2016) Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush.
    2016. Character-aware neural language models. In *AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim and Rush (2016) Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge
    distillation. *arXiv:1606.07947* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim and Awadalla (2020) Young Jin Kim and Hany Hassan Awadalla. 2020. Fastformers:
    Highly efficient transformer models for natural language understanding. *arXiv
    preprint arXiv:2010.13382* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The efficient transformer. *arXiv:2001.04451* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lam (2018) Maximilian Lam. 2018. Word2bits-quantized word vectors. *arXiv:1803.05651*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. ALBERT: A lite BERT for self-supervised
    learning of language representations. *arXiv:1909.11942* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lazaridou et al. (2013) Angeliki Lazaridou, Eva Maria Vecchi, and Marco Baroni.
    2013. Fish transporters and miracle homes: How compositional distributional semantics
    can help NP parsing. In *EMNLP*. 1908–1913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal
    brain damage. In *NIPS*. 598–605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2014) Jason D Lee, Yuekai Sun, and Michael A Saunders. 2014. Proximal
    Newton-type methods for minimizing composite functions. *J. Optimization* 24,
    3 (2014), 1420–1443.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
    2020. Gshard: Scaling giant models with conditional computation and automatic
    sharding. *arXiv:2006.16668* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016b) Fengfu Li, Bo Zhang, and Bin Liu. 2016b. Ternary weight networks.
    *arXiv:1605.04711* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min
    Yang, and Yaohong Jin. 2020. BERT-EMD: Many-to-Many Layer Mapping for BERT Compression
    with Earth Mover’s Distance. *arXiv preprint arXiv:2010.06133* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2016a) Xiang Li, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016a. LightRNN:
    Memory and computation-efficient recurrent neural networks. In *NIPS*. 4385–4393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018) Zhongliang Li, Raymond Kulhanek, Shaojun Wang, Yunxin Zhao,
    and Shuang Wu. 2018. Slim embedding layers for recurrent neural language models.
    In *AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2015) Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua
    Bengio. 2015. Neural networks with few multiplications. *arXiv:1510.03009* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linden (2018) David J Linden. 2018. *Think Tank: Forty Neuroscientists Explore
    the Biological Roots of Human Experience*. Yale University Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2015) Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo,
    Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Finding function
    in form: Compositional character models for open vocabulary word representation.
    *arXiv:1508.02096* (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linzen et al. (2016) Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing
    the ability of LSTMs to learn syntax-sensitive dependencies. *TACL* 4 (2016),
    521–535.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019a. Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language Understanding. *arXiv:1904.09482* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019b. Multi-task deep neural networks for natural language understanding. *arXiv:1901.11504*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lloyd (1982) Stuart Lloyd. 1982. Least squares quantization in PCM. *Tran. on
    information theory* 28, 2 (1982), 129–137.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2016) Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. 2016. Learning
    compact recurrent neural networks. In *ICASSP*. IEEE, 5960–5964.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2019) Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou,
    Dawei Song, and Ming Zhou. 2019. A Tensorized Transformer for Language Modeling.
    *arXiv:1906.09777* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McClure and Kriegeskorte (2016) Patrick McClure and Nikolaus Kriegeskorte. 2016.
    Representational distance learning for deep neural networks. *Frontiers in computational
    neuroscience* 10 (2016), 131.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen
    Heads Really Better than One? *arXiv:1905.10650* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, L
    Sutskever, and G Zweig. 2013. word2vec. *URL https://code. google. com/p/word2vec*
    22 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirzadeh et al. (2019) Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
    Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2019. Improved Knowledge Distillation
    via Teacher Assistant. *arXiv:1902.03393* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishra and Marr (2017) Asit Mishra and Debbie Marr. 2017. Apprentice: Using
    knowledge distillation techniques to improve low-precision network accuracy. *arXiv:1711.05852*
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee and Awadallah (2020) Subhabrata Mukherjee and Ahmed Hassan Awadallah.
    2020. XtremeDistil: Multi-stage Distillation for Massive Multilingual Models.
    In *ACL*. 2221–2234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muller and Indiveri (2015) Lorenz K Muller and Giacomo Indiveri. 2015. Rounding
    methods for neural networks with low resolution synaptic weights. *arXiv:1504.05767*
    (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murray and Chiang (2015) Kenton Murray and David Chiang. 2015. Auto-sizing
    neural networks: With applications to n-gram language models. *arXiv:1508.05051*
    (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narang et al. (2017a) Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho
    Sengupta. 2017a. Exploring sparsity in recurrent neural networks. *arXiv:1704.05119*
    (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narang et al. (2017b) Sharan Narang, Eric Undersander, and Gregory Diamos. 2017b.
    Block-sparse recurrent neural networks. *arXiv:1711.02782* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oseledets (2011) Ivan V Oseledets. 2011. Tensor-train decomposition. *SIAM J.
    on Scientific Computing* 33, 5 (2011), 2295–2317.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ott et al. (2016) Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua
    Bengio. 2016. Recurrent neural networks with limited numerical precision. *arXiv:1608.06902*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2016) Wei Pan, Hao Dong, and Yike Guo. 2016. Dropneuron: Simplifying
    the structure of deep neural networks. *arXiv:1606.07326* (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model compression via distillation and quantization. *arXiv:1802.05668* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prabhavalkar et al. (2016) Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier,
    and Lan McGraw. 2016. On the compression of recurrent neural networks with an
    application to LVCSR acoustic modeling for embedded speech recognition. In *ICASSP*.
    IEEE, 5970–5974.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prasanna et al. (2020) Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.
    When BERT Plays the Lottery, All Tickets Are Winning. *arXiv:2005.00561* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang,
    and Jie Tang. 2020. Blockwise Self-Attention for Long Document Understanding.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: Findings*. 2555–2565.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
    *OpenAI Blog* 1, 8 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *arXiv:1910.10683*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In *ECCV*. Springer, 525–542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romero et al. (2014) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
    Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin
    deep nets. *arXiv:1412.6550* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosset (2019) C Rosset. 2019. Turing-nlg: A 17-billion-parameter language model
    by microsoft. *Microsoft Blog* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sak et al. (2014) Haşim Sak, Andrew Senior, and Françoise Beaufays. 2014. Long
    short-term memory based recurrent neural network architectures for large vocabulary
    speech recognition. *arXiv:1402.1128* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. *arXiv:1910.01108* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sau and Balasubramanian (2016) Bharat Bhusan Sau and Vineeth N Balasubramanian.
    2016. Deep model compression: Distilling knowledge from noisy teachers. *arXiv:1610.09650*
    (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See et al. (2016) Abigail See, Minh-Thang Luong, and Christopher D Manning.
    2016. Compression of neural machine translation models via pruning. *arXiv:1606.09274*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2019) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Q-bert: Hessian based
    ultra low precision quantization of bert. *arXiv:1909.05840* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2018) Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and
    Hongsheng Li. 2018. Efficient Attention: Attention with Linear Complexities. *arXiv:1812.01243*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion
    parameter language models using gpu model parallelism. *arXiv:1909.08053* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu and Nakayama (2017) Raphael Shu and Hideki Nakayama. 2017. Compressing word
    embeddings via deep compositional code learning. *arXiv:1711.01068* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas and Babu (2015) Suraj Srinivas and R Venkatesh Babu. 2015. Data-free
    parameter pruning for deep neural networks. *arXiv:1507.06149* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strubell et al. (2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019.
    Energy and policy considerations for deep learning in NLP. *arXiv preprint arXiv:1906.02243*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient
    knowledge distillation for bert model compression. *arXiv:1908.09355* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *arXiv preprint arXiv:2004.02984* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzuki and Nagata (2016) Jun Suzuki and Masaaki Nagata. 2016. Learning Compact
    Neural Word Embeddings by Parameter Space Sharing.. In *IJCAI*. 2046–2052.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019) Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
    2019. Multilingual neural machine translation with knowledge distillation. *arXiv:1902.10461*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019) Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova,
    and Jimmy Lin. 2019. Distilling Task-Specific Knowledge from BERT into Simple
    Neural Networks. *arXiv:1903.12136* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tay et al. (2020) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng
    Juan. 2020. Sparse Sinkhorn Attention. *arXiv:2002.11296* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tay et al. (2019) Yi Tay, Aston Zhang, Luu Anh Tuan, Jinfeng Rao, Shuai Zhang,
    Shuohang Wang, Jie Fu, and Siu Cheung Hui. 2019. Lightweight and Efficient Neural
    Natural Language Processing with Quaternion Networks. *arXiv:1906.04393* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tjandra et al. (2017) Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura.
    2017. Compressing recurrent neural network with tensor train. In *IJCNN*. IEEE,
    4451–4458.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tucker (1966) Ledyard R Tucker. 1966. Some mathematical notes on three-mode
    factor analysis. *Psychometrika* 31, 3 (1966), 279–311.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turc et al. (2019) Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    2019. Well-read students learn better: The impact of student initialization on
    knowledge distillation. *arXiv preprint arXiv:1908.08962* 13 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Variani et al. (2019) Ehsan Variani, Ananda Theertha Suresh, and Mitchel Weintraub.
    2019. WEST: Word Encoded Sequence Transducers. In *ICASSP*. IEEE, 7340–7344.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NIPS*. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do
    the heavy lifting, the rest can be pruned. *arXiv:1905.09418* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walsh (2013) Christopher A Walsh. 2013. Peter Huttenlocher (1931–2013). *Nature*
    502, 7470 (2013), 172–172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE:
    A Stickier Benchmark for General-Purpose Language Understanding Systems. *1905.00537*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A Multi-Task Benchmark and Analysis
    Platform for Natural Language Understanding. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Yoon (2020) Lin Wang and Kuk-Jin Yoon. 2020. Knowledge distillation
    and student-teacher learning for visual intelligence: A review and new outlooks.
    In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020*. IEEE
    CVPR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang,
    and Yuan Xie. 2018. HitNet: hybrid ternary recurrent neural network. In *NIPS*.
    604–614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao
    Ma. 2020a. Linformer: Self-Attention with Linear Complexity. *arXiv:2006.04768*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and
    Ming Zhou. 2020b. Minilm: Deep self-attention distillation for task-agnostic compression
    of pre-trained transformers. *arXiv:2002.10957* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019c) Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019c. Structured
    Pruning of Large Language Models. *arXiv:1910.04732* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wangperawong (2018) Artit Wangperawong. 2018. Attending to mathematical language
    with transformers. *arXiv:1812.02825* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018) Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao,
    Zhirong Wang, and Hongbin Zha. 2018. Alternating multi-bit quantization for recurrent
    neural networks. *arXiv:1802.00150* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020) Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
    2020. Bert-of-theseus: Compressing bert by progressive module replacing. *arXiv
    preprint arXiv:2002.02925* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Yafeng Yang, Kaihuan Liang, Xuefeng Xiao, Zecheng Xie, Lianwen
    Jin, Jun Sun, and Weiying Zhou. 2018. Accelerating and Compressing LSTM Based
    Model for Online Handwritten Chinese Character Recognition. In *ICFHR*. IEEE,
    110–115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
    Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining
    for Language Understanding. *arXiv:1906.08237* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2018) Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe,
    Xinqi Chu, and Zenglin Xu. 2018. Learning compact recurrent neural networks with
    block-term tensor decomposition. In *CVPR*. 9378–9387.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yim et al. (2017) Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017.
    A gift from knowledge distillation: Fast optimization, network minimization and
    transfer learning. In *CVPR*. 4133–4141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2017) Shan You, Chang Xu, Chao Xu, and Dacheng Tao. 2017. Learning
    from multiple teacher networks. In *KDD*. 1285–1294.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2019) Xin Yuan, Liangliang Ren, Jiwen Lu, and Jie Zhou. 2019. Enhanced
    bayesian compression via deep reinforcement learning. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 6946–6955.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaremba and Sutskever (2014) Wojciech Zaremba and Ilya Sutskever. 2014. Learning
    to execute. *arXiv:1410.4615* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan
    Lu. 2018. Deep mutual learning. In *CVPR*. 4320–4328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou. 2019.
    Extreme Language Model Compression with Optimal Subwords and Shared Projections.
    *arXiv:1909.11687* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2017) Shu-Chang Zhou, Yu-Zhi Wang, He Wen, Qin-Yao He, and Yu-Heng
    Zou. 2017. Balanced quantization: An effective and efficient approach to quantized
    neural networks. *J. of Computer Science and Technology* 32, 4 (2017), 667–682.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2016) Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. 2016.
    Trained ternary quantization. *arXiv:1612.01064* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu and Gupta (2017) Michael Zhu and Suyog Gupta. 2017. To prune, or not to
    prune: exploring the efficacy of pruning for model compression. *arXiv:1710.01878*
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
