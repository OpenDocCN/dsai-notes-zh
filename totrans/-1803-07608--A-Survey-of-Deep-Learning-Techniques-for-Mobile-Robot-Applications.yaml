- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:07:56'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1803.07608] A Survey of Deep Learning Techniques for Mobile Robot Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1803.07608](https://ar5iv.labs.arxiv.org/html/1803.07608)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning Techniques for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mobile Robot Applications
  prefs: []
  type: TYPE_NORMAL
- en: Jahanzaib Shabbir, and Tarique Anwer
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Advancements in deep learning over the years have attracted research into how
    deep artificial neural networks can be used in robotic systems. It is on this
    basis that the following research survey will present a discussion of the applications,
    gains, and obstacles to deep learning in comparison to physical robotic systems
    while using modern research as examples. The research survey will present a summarization
    of the current research with specific focus on the gains and obstacles in comparison
    to robotics. This will be followed by a primer on discussing how notable deep
    learning structures can be used in robotics with relevant examples. The next section
    will show the practical considerations robotics researchers desire to use in regard
    to deep learning neural networks. Finally, the research survey will show the shortcomings
    and solutions to mitigate them in addition to discussion of the future trends.
    The intention of this research is to show how recent advancements in the broader
    robotics field can inspire additional research in applying deep learning in robotics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, robotic vision, navigation, autonomous driving, deep reinforcement
    learning, algorithms for robotic perception, Semi-supervised and self-supervised
    learning, Deep learning architectures, multimodal, decision making and control.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1 Defining Deep Learning in the Context of Robotic Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning is defined as the field of science that involves training extensive
    artificial neural networks using complex functions, for example, nonlinear dynamics
    to change data from a raw, high-dimension, multimodal state to that which can
    be understood by a robotic system [[1](#bib.bib1)]. However, deep learning entails
    certain shortcomings which affect physical robotic systems whereby generation
    of training data in overall is costly and therefore sub-optimal performance in
    the course of training poses a risk in certain applications. Yet, even with such
    difficulties, robotics researchers are searching for creative options, for instance,
    leveraging training data through digital manipulation, automated training and
    using multiple deep neural networks to improve the performance and lower the time
    for training [[2](#bib.bib2)]. The idea of using machine learning to control robots
    needs humans to show the willingness to lose a certain measure of control. This
    is seemingly counterintuitive in the beginning although the gain for doing this
    is to allow the system to begin learning on its own [[3](#bib.bib3)]. This makes
    the systems capable of adaptation such that the potential of ultimately improving
    their direction is that originating from human control. This makes deep neural
    networks well suited to be used with robots since they are flexible and can be
    used in frameworks that cannot be supported by other machine learning models [[4](#bib.bib4)].
    For a long time, the most notable method for optimization in neural networks is
    known as the stochastic gradient descent. However, improved techniques, for instance,
    RMSProp, as well as Adam of recent, have gained widespread use. Each of the many
    types of deep learning models is made through the stacking of several layers of
    regression models [[5](#bib.bib5)]. Within these models, distinct types of layers
    have undergone evolution for many aims.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Forms of Deep Learning Applied to Mobile Robotic Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One type of layer that demands specific mention is convolutional layers. Unlike
    traditional layers that are fully connected, convolutional layers apply the same
    weights in order to operate in all the input space. This brings about a significant
    reduction of the overall number of weights in the neural network which is specifically
    vital with images that normally compose of hundreds of thousands and millions
    of pixels that require processing [[6](#bib.bib6)]. It should be noted that processing
    these kinds of images which have fully connected layers would need over 100K2
    to 1M2 weights which connect to each layer which makes it entirely impractical.
    The inspiration of convolutional layers came from cortical neurons within the
    visual cortex which only respond to stimuli in a receptive environment. Since
    convolution estimates such behavior, convolutional layers can be expected to excel
    at image processing assignments [[7](#bib.bib7)]. The pioneering research in neural
    networks using convolutional layers uses image recognition tasks which we built
    on the advancements of ImageNet recognition competitions around 2012\. The lessons
    learned in this period gained widespread interest in convolutional layers being
    able to gain super-human recognition of images [[8](#bib.bib8)]. Currently, convolutional
    neural networks have been come well known and highly effective as a deep learning
    model for many image-based applications. These applications comprise of semantic
    image segmentation, scaling images using super-resolution, scene recognition,
    object localization with images, human gesture recognition and facial recognition
    [[9](#bib.bib9)] [[10](#bib.bib10)]. Images are not the only form of a signal
    which illustrates the excellence of convolutional neural networks. Their capability
    is also effective in any form of a signal which demonstrates spatiotemporal proximity
    for example speech recognition as well as speech and audio synthesis [[11](#bib.bib11)].
    Naturally, these have also started to be dominative in the domain of signal processing
    and heavily used in robotics, for instance, pedestrian detection with the use
    of LIDAR, mico-Doppler signatures, as well as depth-map, estimating [[12](#bib.bib12)]
    [[13](#bib.bib13)]. Recent projects have even started to integrate signals from
    several modalities and combine them for unified recognition and perception [[14](#bib.bib14)].
    Ultimately, the philosophy that underlies and prevails in the deep learning community
    is that every component of a complex system can be taught to ”learn.” Therefore,
    the actual power of deep learning does not come from applying just one of the
    described structures in the previous section as a part in robotics systems by
    in connecting components of all these structures to form a complete system that
    learns entirely [[15](#bib.bib15)]. This is the point where deep learning starts
    to make its impact such that each component of the system is capable of learning
    as a whole and is capable of adapting to sophisticated methods. For example, neuroscientists
    have even started recognizing the many patterns evolving in the deep learning
    community and in all artificial intelligence are starting to mirror patterns previously
    evolved in the human brain [[16](#bib.bib16)]. In the process of learning complex,
    high-dimensional as well as novel dynamics, the analysis of derivatives within
    these complex dynamics needs human expertise. However, this process normally consumes
    a lot of time and can bring about a trade-off between the dimensionality and tractability
    of states [[17](#bib.bib17)]. Therefore making these models robust to unforeseen
    impact is challenging and in most cases, full state information is normally unknown.
    In this case, systems that are able to rapidly and autonomously adapt to modern
    dynamics are required to solve problems for instance moving over services with
    unknown or uncertain attributes, managing interaction in a new environment or
    adapting or degrading robot subsystems [[18](#bib.bib18)]. Therefore, we need
    methods that are able to accomplish possession of hundreds or thousands of degrees
    of freedom and demonstrate high measures of uncertainty which are only available
    in a state of partial information. On the other hand, the process of learning
    control policies in dynamic environments and dynamic control systems is able to
    accommodate high measures of freedom for applications such as swarm robotics,
    anthropomorphic hands, robot vision, autonomous robot driving and robotic arm
    manipulation [[19](#bib.bib19)]. However, despite the advancements gained over
    the years in active research, robust and overall solutions for tasks, for instance,
    moving in deformed surfaces or navigating complex geometries with the use of tools
    and actuator systems have remained elusive more so in novel scenarios. This shortcoming
    is inclusive of kinematic and path planning tasks which are inherent in advanced
    movement [[20](#bib.bib20)]. On the other hand, in terms of advanced object recognition,
    deep neural networks have proved to be increasingly adapt to the recognition and
    classification of objects. Examples of advanced application include recognition
    of deformed objects and estimation of their state and pose for movement, semantic
    task, and path specification, for example, moving around the table [[20](#bib.bib20)].
    In addition, it includes recognizing the attributes of an object and surface whereby
    for instance a sharp object could present a danger to human collaborators in certain
    environments such as rough terrain. In the face of such difficulties, deep learning
    models can be used in the approximation of functions from sample input-output
    pairs. These can be the most general purpose deep learning structures since there
    are several distinct functions in robotics which researchers can use in approximating
    from sample observations [[21](#bib.bib21)]. Certain examples of these observations
    entail mapping from actions to corresponding changes in the stage, mapping these
    changes in state to actions that can cause it or mapping from force to motion.
    While in certain cases particular physical equations for such functions may already
    be defined, there are several situations where the environment is highly complex
    for such equations to generate acceptable accuracy [[22](#bib.bib22)]. However,
    in such scenarios learning approximation of functions from sample observations
    can yield accuracy that is significantly better. In other words, approximated
    functions do not need to be continuous. However, function approximation models
    are also excellent at classification tasks, for instance, determining the type
    of obstacles before a robot, the overall path planning strategy well suited for
    present environments or the state of a certain complex object which the object
    is interacting with [[23](#bib.bib23)]. Furthermore, function approximation deep
    learning architecture using rectifiers can model the high coupled dynamics of
    an autonomous mobile robot to solve the analytic derivatives and challenging system
    identification problems. Deep neural networks have superseded other models in
    detection and perception since they are capable of engaging in direct operation
    with highly-dimensional input rather than needing feature vectors based on hand-engineered
    designs by humans [[24](#bib.bib24)]. This lowers the dependence on humans such
    that additional training time can be partially offset by lowering initial engineering
    efforts [[25](#bib.bib25)]. Extraction of meaning from video or still imagery
    is another application where deep learning has gained impressive progression.
    This process demands simultaneously addressing using the four independent factors
    of object detection and a single deep neural network. These factors include feature
    extraction, motion handling, classification, articulation and occlusion handling
    [[26](#bib.bib26)]. Unified systems limit suboptimal interactions between normally
    separate systems by predicting the physical results of dynamic scenes using vision
    alone. This is based on the actions of humans to be able to predict the results
    of a dynamic scene from visual information, for example, a rock falling down and
    impacting another rock [[27](#bib.bib27)]. It is therefore on this premise that
    deep learning has been identified as being effective in managing multimodal data
    generation in robotic sensor applications. These applications include integration
    of vision and haptic sensor data, incorporating depth data and image information
    from RGB-D camera data. Due to the extensive number of meta-parameters, deep neural
    networks have evolved somewhat a reputation of being challenging for non-experts
    to be used effectively [[28](#bib.bib28)]. However, such parameters also avail
    significant flexibility which is a vital factor in their general success. Therefore,
    training deep neural networks needs the user to be able to develop at least an
    elementary level of familiarization with many concepts. Specifically, applying
    these techniques will help in tacking advanced object recognition challenges and
    reduced the extent of the entire changes as well [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Learning for Robotic Perception
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Current Robotic Perception Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although current trends are more leaning to deep and big models, a simplified
    neural network with just a single hidden layer and a basic sigmoid shaped activation
    function will train faster and provide a baseline that is used to give meaning
    to any deeper model improvements. When we use deeper models, Leaky Rectifiers
    are able to normally promote faster training by lowering the impact of the diminishing
    gradient challenge and improving accuracy through using simplified monotonic derivatives
    [[30](#bib.bib30)]. Furthermore, since models with additional weights have increased
    flexibility to over fitting training data, regularization is a vital technique
    in training the best model. In addition, an elastic net is a combination of well-established
    regularization methods used in promoting robustness against weight saturation
    and also promotion sparsity in weights [[31](#bib.bib31)]. However, newer regularization
    methods inclusive of drop-out and drop-connect has attained even better empirical
    outcomes. Furthermore, many regularization methods are also in existence in specifically
    improving the robustness of autoencoders. In this case, special-purpose layers
    can also make a significant distinction with deep neural networks [[32](#bib.bib32)].
    It is a common method to alternate between convolutional and max pool layers.
    These pool layers can lower the general number of weights in the network and also
    allow the model to be able to recognize objects independent of where they are
    placed in the visual field [[33](#bib.bib33)]. On the other hand, batch normalization
    can provide us with significant improvements in rating the convergence by ensuring
    the gradient in range affects the weights of all neurons. In addition, residual
    layers can allow a deeper and consequently more flexible model to be trained [[34](#bib.bib34)].
    To make effective use of deep learning models, it is vital to train one or many
    General Purpose Graphical Processing Units since the other methods of parallelization
    of deep neural networks have been tried but none of them have yet provided gains
    in beneficial performance of General Purpose Graphical Processing Units [[35](#bib.bib35)].
    For a long time until in recent years, robots have long been used in industrial
    environments. In industrial environments, robotic systems are pre-programmed with
    repetitive assignments which lack the capability of autonomy and as such operate
    on the basis of a structured approach. Such an environment cannot be adaptive
    for a mobile robot since it eliminates the need for autonomy. On the other hand,
    mobile robots need less structured environments such that they can be able to
    make their own decisions such as navigate paths, determine whether objects are
    obstacles, recognize images and audio as well as map their environments. As such,
    surviving and adapting in the real world is more complex for any robotic system
    in comparison to the industrial setting since the risk of failure, system error,
    external factors, obstacles, corrupt data, human error and unrecognizable environments
    is more prevalent [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Machine Learning Usage in Training Robotic System Perception
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The difference between deep learning and machine learning is that deep learning
    place emphasis on the subset of machine learning resources and method and uses
    them to solve any difficulties that need ”thought” whether human or artificial.
    Deep learning is also introduced as a means of making sense of data with the use
    of multiple abstraction layers. In the course of the training process, deep neural
    networks are able to learn the means of discovering useful patterns to digitally
    represent data such as sounds and images. This is specifically why we observe
    more advances in the areas of image recognition and natural language processing
    originating from deep learning [[36](#bib.bib36)]. It is on this backdrop that
    deep learning has taken the forefront position in helping researchers develop
    breakthrough methods to the perception capabilities of robotics systems [[16](#bib.bib16)].
    In more simplified language, perception refers to the functionality of robots
    being able to detect its surroundings. It is therefore heavily reliant on multiple
    sources of sensory information. However, with traditional robot technology extracting
    data from raw sensor by using rudimentary constructed sensors theseold methods
    were limited by constraints of adapting to generic settings [[17](#bib.bib17)].
    In situations where these robotic systems faced dynamic environments, they operated
    in an unstructured manner by combining hybrid and autonomous functionality to
    process information about their surroundings [[18](#bib.bib18)]. As such, with
    deep learning came the introduction of new methods of processing data from robotic
    sensorsof a robotic system’s surroundings using a feature known as perception.
    These methods comprise of robot motion, perception, human-interaction, manipulation
    and grasping, automation, self-supervision, self training and learning as well
    as robot vision [[19](#bib.bib19)]. Deep learning models utilize automated actions
    technology at only half the cost by using supervised learning to attain their
    goals [[20](#bib.bib20)]. For example, in order to perfect an image recognition
    application, a neural net will be required to be trained with a collection of
    labeled data. On the other hand, unsupervised learning is how deep learning operates
    and allows for the discovery of new patterns and insights by tackling problems
    with little or no insight on what the results should be perceived by a robot [[21](#bib.bib21)].
    The method by which a mobile robot is able to detect it’s environment with the
    use perception is by using definitive decision-making policies [[20](#bib.bib20)].
    For instance, mobile robots using deep learning are able to navigate with rationality
    by using motion and track precision sensors which are driven machine learning
    algorithms [[21](#bib.bib21)]. However, in difficult environments such as a congested
    room, the level of accurately perceiving their environment is limited. Therefore,
    deep learning based solutions can tackle this challenge by using artificial intelligence,
    high computational hardware and processing layers known as deep convolution neural
    networks to solve this dilemma by successfully deciphering intricate environment
    perception difficulties [[22](#bib.bib22)]. The various obstacles that exist in
    a robot’s environment are an indication of the immense high-dimensional data processing
    capabilitiesrequiredby a robotic system to be able to perceive its surroundings
    [[23](#bib.bib23)]. By using self-supervised, semi-supervised and full supervised
    training coupled with learning, robotic systems are able to utilize their machine
    learning and pattern recognition capabilities to process raw data such as images,
    objects, and semantics, audio as well as process natural language in the real
    time. This segment of deep learning is the best since it uses feed-forward artificial
    neural networks to successfully analyze visual imagery. It also uses multiple
    multilayer perceptrons based on designs that need low preprocessing [[22](#bib.bib22)].
    In comparison to other image classification algorithms, it uses minimal pre-processing
    which means that the network is able to learn filters using automated procedures
    unlike traditional algorithms that are manually engineered. Therefore, by not
    relying on past knowledge and human efforts in the design of features makes it
    a major advantage [[24](#bib.bib24)]. As such in general, this makes deep learning
    capable of the extraction of multi-level attributes from raw sensor data in a
    direct manner with no need for human assisted robotic control [[24](#bib.bib24)].
    This presents researchers with the implication that deep learning programming
    librariessuch as TensorflowTheano of Python, Caffe of C++, darch in R, CNTK, Convent.js
    of Javascript, and Deeplearning4j derived from C++ and Java among others are extremely
    of use in providing robotic systems with a platform to develop their sensory data
    analysis and environmentlearningby using deep learning algorithms [[25](#bib.bib25)].
    Robotic system perception concerns auxiliary functions within mobile robotic are
    vital in interactingwith a robot’s environment.Sensing and intelligent perception
    are some of the applications which are vital since they determine the performance
    of a robotic system. These performances are largely dependent on how robot sensors
    perform. Modern sensors and their functionality can provide impressive robot perception
    which is the foundation of self-adaptive as well as robotic artificial intelligence
    [[26](#bib.bib26)]. The process of changing from sensory input to control output
    using sensory-motor control structures presentsa big difficultyin robotic perception
    [[27](#bib.bib27)]. Some of the vital mobile robot components include the manipulator
    comprising of many joints and connections, a locomotion device, sensors, a controller
    and an endeffector [[37](#bib.bib37)]. Mobile robots are automated systems capable
    of moving. They also have the ability to move around their surroundings and are
    not fixed to a single physical location. With these features mobile robots are
    able to perceive their environments usingsensory data andautonomous control commands
    [[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Shortcomings of Deep Learning in Robotic Perception
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: However, certain challenges remain unresolved in these robotic systems particularly
    the areas of perception and intelligent control. Some of these challenges are
    reflected in the process needing a lot of data to be able to train and teach algorithms
    progressively. Large datasets are required to ensure machines deliver the desired
    outcomes. In the same way as the human brain needs rich experience to learn and
    deduce information, artificial neural networks also need abundant amounts of data.
    This means for more powerful abstractions, more parameters are required and hence
    more data. Another challenge is the tendency of over fitting in neural networks
    whereby in certain cases, there is a sharp distinction between an error within
    a training set and that encountered into new untrained datasets. This problem
    arises when many models make the relative number of parameters fail to reliably
    perform. Therefore, the model only memorizes training examples and fails to learn
    generalization of new situations and new datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 How Mobile Robotic Perception Models Can Be Used to Attain Complete Situational
    Awareness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These capabilities will be surveyed within the research survey through deep
    learning perception model algorithms that are ableto determine how a robot responds
    to the dynamic changes within an environment [[29](#bib.bib29)]. The basis of
    these models revolves around control theory affiliated paradigms for instance
    system stability, control as well as observation [[30](#bib.bib30)]. This theory
    states that a robotic system is able to perceive its environment by using hierarchical
    extensions or enhancements of learning by maximizing the range of its sensor capabilities
    while using path planning algorithms to maneuver around obstacles or paths [[31](#bib.bib31)].
    Most real-time map algorithms are concerned with the acquisition of compact 3D
    mapping within indoor settings with the use of range as well as imaging sensory
    capabilities [[32](#bib.bib32)]. The process of developing models of a robot’s
    environment is a vital problem to deal with more so in regard to managing its
    workspace especially when it is shared with other machinery [[33](#bib.bib33)].
    A mobile robot interacts with the environment by using control systems which define
    structures or obstacles as geometrical areas so as to be able to cover all the
    likely configurations on the robot. Objects or structures are defined according
    to parallelepipeds, spheres, planes, and cylinders. With such a simplified model,
    the mobile robot system is able to define many geometrical areas of this nature
    to cover nearly all objects within its surrounding, for example, moving objects,
    stationary items such as furniture and machinery. Therefore, we propose elementary
    geometrical volumes as a means of modeling a mobile robot’s perception capabilities
    of its environment [[34](#bib.bib34)]. This method will allow the robot to be
    able to move within an environment with the certainty of not colliding with regions
    that are forbidden; these regions must already be defined, declared and activated
    so as to be able to correctly work [[35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: The geometrical perception algorithm will check if the robot end effector is
    within the controlled area or warning zone. This checking is done through the
    use of already stored geometrical areas already defined by the user [[38](#bib.bib38)].
    Similarly, in this context, the position of the dynamic object avails the perception
    system with the likelihood of connecting the geometrical regions with arbitrary
    moving points [[39](#bib.bib39)]. These points can be read from external sensors
    such as encoders. Control of speed is undertaken with the user of geometrical
    area blocks able to detect the shape typology and choose the correct movement
    law to be used so as to modify the robot override and avoid collisions with user-defined
    zones [[40](#bib.bib40)]. The speed override is transformed smoothly when the
    robot end effector collides with a spherical zone in accordance with the perception
    law in ([1](#S2.E1 "In 2.4 How Mobile Robotic Perception Models Can Be Used to
    Attain Complete Situational Awareness ‣ 2 Deep Learning for Robotic Perception
    ‣ A Survey of Deep Learning Techniques for Mobile Robot Applications")) [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E1.m1.44" class="ltx_Math" alttext="\begin{split}V=v_{0}.\frac{d-r}{\delta},r\leq
    d\leq r+\delta,\\ V=v_{0},d>r+\delta,and\\'
  prefs: []
  type: TYPE_NORMAL
- en: V=0,d<r\end{split}" display="block"><semantics id="S2.E1.m1.44a"><mtable displaystyle="true"
    rowspacing="0pt" id="S2.E1.m1.44.44.8"><mtr id="S2.E1.m1.44.44.8a"><mtd class="ltx_align_right"
    columnalign="right" id="S2.E1.m1.44.44.8b"><mrow id="S2.E1.m1.40.40.4.37.16.16.16"><mrow
    id="S2.E1.m1.40.40.4.37.16.16.16.1"><mrow id="S2.E1.m1.40.40.4.37.16.16.16.1.1.1"><mi
    id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">V</mi><mo id="S2.E1.m1.2.2.2.2.2.2"
    xref="S2.E1.m1.2.2.2.2.2.2.cmml">=</mo><msub id="S2.E1.m1.40.40.4.37.16.16.16.1.1.1.1"><mi
    id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml">v</mi><mn id="S2.E1.m1.4.4.4.4.4.4.1"
    xref="S2.E1.m1.4.4.4.4.4.4.1.cmml">0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
    id="S2.E1.m1.5.5.5.5.5.5" xref="S2.E1.m1.39.39.3.4.cmml">.</mo><mrow id="S2.E1.m1.40.40.4.37.16.16.16.1.2.2"><mrow
    id="S2.E1.m1.40.40.4.37.16.16.16.1.2.2.2"><mfrac id="S2.E1.m1.6.6.6.6.6.6" xref="S2.E1.m1.6.6.6.6.6.6.cmml"><mrow
    id="S2.E1.m1.6.6.6.6.6.6.2" xref="S2.E1.m1.6.6.6.6.6.6.2.cmml"><mi id="S2.E1.m1.6.6.6.6.6.6.2.2"
    xref="S2.E1.m1.6.6.6.6.6.6.2.2.cmml">d</mi><mo id="S2.E1.m1.6.6.6.6.6.6.2.1" xref="S2.E1.m1.6.6.6.6.6.6.2.1.cmml">−</mo><mi
    id="S2.E1.m1.6.6.6.6.6.6.2.3" xref="S2.E1.m1.6.6.6.6.6.6.2.3.cmml">r</mi></mrow><mi
    id="S2.E1.m1.6.6.6.6.6.6.3" xref="S2.E1.m1.6.6.6.6.6.6.3.cmml">δ</mi></mfrac><mo
    id="S2.E1.m1.7.7.7.7.7.7" xref="S2.E1.m1.39.39.3.4.cmml">,</mo><mi id="S2.E1.m1.8.8.8.8.8.8"
    xref="S2.E1.m1.8.8.8.8.8.8.cmml">r</mi></mrow><mo id="S2.E1.m1.9.9.9.9.9.9" xref="S2.E1.m1.9.9.9.9.9.9.cmml">≤</mo><mi
    id="S2.E1.m1.10.10.10.10.10.10" xref="S2.E1.m1.10.10.10.10.10.10.cmml">d</mi><mo
    id="S2.E1.m1.11.11.11.11.11.11" xref="S2.E1.m1.11.11.11.11.11.11.cmml">≤</mo><mrow
    id="S2.E1.m1.40.40.4.37.16.16.16.1.2.2.3"><mi id="S2.E1.m1.12.12.12.12.12.12"
    xref="S2.E1.m1.12.12.12.12.12.12.cmml">r</mi><mo id="S2.E1.m1.13.13.13.13.13.13"
    xref="S2.E1.m1.13.13.13.13.13.13.cmml">+</mo><mi id="S2.E1.m1.14.14.14.14.14.14"
    xref="S2.E1.m1.14.14.14.14.14.14.cmml">δ</mi></mrow></mrow></mrow><mo id="S2.E1.m1.15.15.15.15.15.15"
    xref="S2.E1.m1.39.39.3.4.cmml">,</mo></mrow></mtd></mtr><mtr id="S2.E1.m1.44.44.8c"><mtd
    class="ltx_align_right" columnalign="right" id="S2.E1.m1.44.44.8d"><mrow id="S2.E1.m1.42.42.6.39.16.16.16"><mrow
    id="S2.E1.m1.41.41.5.38.15.15.15.1"><mi id="S2.E1.m1.16.16.16.1.1.1" xref="S2.E1.m1.16.16.16.1.1.1.cmml">V</mi><mo
    id="S2.E1.m1.17.17.17.2.2.2" xref="S2.E1.m1.17.17.17.2.2.2.cmml">=</mo><msub id="S2.E1.m1.41.41.5.38.15.15.15.1.1"><mi
    id="S2.E1.m1.18.18.18.3.3.3" xref="S2.E1.m1.18.18.18.3.3.3.cmml">v</mi><mn id="S2.E1.m1.19.19.19.4.4.4.1"
    xref="S2.E1.m1.19.19.19.4.4.4.1.cmml">0</mn></msub></mrow><mo id="S2.E1.m1.20.20.20.5.5.5"
    xref="S2.E1.m1.39.39.3.4.cmml">,</mo><mrow id="S2.E1.m1.42.42.6.39.16.16.16.2"><mi
    id="S2.E1.m1.21.21.21.6.6.6" xref="S2.E1.m1.21.21.21.6.6.6.cmml">d</mi><mo id="S2.E1.m1.22.22.22.7.7.7"
    xref="S2.E1.m1.22.22.22.7.7.7.cmml">></mo><mrow id="S2.E1.m1.42.42.6.39.16.16.16.2.2.2"><mrow
    id="S2.E1.m1.42.42.6.39.16.16.16.2.1.1.1"><mi id="S2.E1.m1.23.23.23.8.8.8" xref="S2.E1.m1.23.23.23.8.8.8.cmml">r</mi><mo
    id="S2.E1.m1.24.24.24.9.9.9" xref="S2.E1.m1.24.24.24.9.9.9.cmml">+</mo><mi id="S2.E1.m1.25.25.25.10.10.10"
    xref="S2.E1.m1.25.25.25.10.10.10.cmml">δ</mi></mrow><mo id="S2.E1.m1.26.26.26.11.11.11"
    xref="S2.E1.m1.39.39.3.4.cmml">,</mo><mrow id="S2.E1.m1.42.42.6.39.16.16.16.2.2.2.2"><mi
    id="S2.E1.m1.27.27.27.12.12.12" xref="S2.E1.m1.27.27.27.12.12.12.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.42.42.6.39.16.16.16.2.2.2.2.1" xref="S2.E1.m1.39.39.3.4.cmml">​</mo><mi
    id="S2.E1.m1.28.28.28.13.13.13" xref="S2.E1.m1.28.28.28.13.13.13.cmml">n</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.42.42.6.39.16.16.16.2.2.2.2.1a" xref="S2.E1.m1.39.39.3.4.cmml">​</mo><mi
    id="S2.E1.m1.29.29.29.14.14.14" xref="S2.E1.m1.29.29.29.14.14.14.cmml">d</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S2.E1.m1.44.44.8e"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.44.44.8f"><mrow
    id="S2.E1.m1.44.44.8.41.9.9.9"><mrow id="S2.E1.m1.43.43.7.40.8.8.8.1"><mi id="S2.E1.m1.30.30.30.1.1.1"
    xref="S2.E1.m1.30.30.30.1.1.1.cmml">V</mi><mo id="S2.E1.m1.31.31.31.2.2.2" xref="S2.E1.m1.31.31.31.2.2.2.cmml">=</mo><mn
    id="S2.E1.m1.32.32.32.3.3.3" xref="S2.E1.m1.32.32.32.3.3.3.cmml">0</mn></mrow><mo
    id="S2.E1.m1.33.33.33.4.4.4" xref="S2.E1.m1.39.39.3.4.cmml">,</mo><mrow id="S2.E1.m1.44.44.8.41.9.9.9.2"><mi
    id="S2.E1.m1.34.34.34.5.5.5" xref="S2.E1.m1.34.34.34.5.5.5.cmml">d</mi><mo id="S2.E1.m1.35.35.35.6.6.6"
    xref="S2.E1.m1.35.35.35.6.6.6.cmml"><</mo><mi id="S2.E1.m1.36.36.36.7.7.7" xref="S2.E1.m1.36.36.36.7.7.7.cmml">r</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S2.E1.m1.44b"><apply id="S2.E1.m1.39.39.3.4.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.39.39.3.4a.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5">formulae-sequence</csymbol><apply id="S2.E1.m1.37.37.1.1.1.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><ci id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">𝑉</ci><apply
    id="S2.E1.m1.37.37.1.1.1.3.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous"
    id="S2.E1.m1.37.37.1.1.1.3.1.cmml" xref="S2.E1.m1.5.5.5.5.5.5">subscript</csymbol><ci
    id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3">𝑣</ci><cn type="integer"
    id="S2.E1.m1.4.4.4.4.4.4.1.cmml" xref="S2.E1.m1.4.4.4.4.4.4.1">0</cn></apply></apply><apply
    id="S2.E1.m1.38.38.2.2.2.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><apply id="S2.E1.m1.38.38.2.2.2b.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><list id="S2.E1.m1.38.38.2.2.2.2.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><apply
    id="S2.E1.m1.6.6.6.6.6.6.cmml" xref="S2.E1.m1.6.6.6.6.6.6"><apply id="S2.E1.m1.6.6.6.6.6.6.2.cmml"
    xref="S2.E1.m1.6.6.6.6.6.6.2"><ci id="S2.E1.m1.6.6.6.6.6.6.2.2.cmml" xref="S2.E1.m1.6.6.6.6.6.6.2.2">𝑑</ci><ci
    id="S2.E1.m1.6.6.6.6.6.6.2.3.cmml" xref="S2.E1.m1.6.6.6.6.6.6.2.3">𝑟</ci></apply><ci
    id="S2.E1.m1.6.6.6.6.6.6.3.cmml" xref="S2.E1.m1.6.6.6.6.6.6.3">𝛿</ci></apply><ci
    id="S2.E1.m1.8.8.8.8.8.8.cmml" xref="S2.E1.m1.8.8.8.8.8.8">𝑟</ci></list><ci id="S2.E1.m1.10.10.10.10.10.10.cmml"
    xref="S2.E1.m1.10.10.10.10.10.10">𝑑</ci></apply><apply id="S2.E1.m1.38.38.2.2.2c.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><apply id="S2.E1.m1.38.38.2.2.2.6.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><ci
    id="S2.E1.m1.12.12.12.12.12.12.cmml" xref="S2.E1.m1.12.12.12.12.12.12">𝑟</ci><ci
    id="S2.E1.m1.14.14.14.14.14.14.cmml" xref="S2.E1.m1.14.14.14.14.14.14">𝛿</ci></apply></apply></apply><apply
    id="S2.E1.m1.39.39.3.3.3.3.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous"
    id="S2.E1.m1.39.39.3.3.3.3a.cmml" xref="S2.E1.m1.5.5.5.5.5.5">formulae-sequence</csymbol><apply
    id="S2.E1.m1.39.39.3.3.3.1.1.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><ci id="S2.E1.m1.16.16.16.1.1.1.cmml"
    xref="S2.E1.m1.16.16.16.1.1.1">𝑉</ci><apply id="S2.E1.m1.39.39.3.3.3.1.1.3.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.39.39.3.3.3.1.1.3.1.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S2.E1.m1.18.18.18.3.3.3.cmml"
    xref="S2.E1.m1.18.18.18.3.3.3">𝑣</ci><cn type="integer" id="S2.E1.m1.19.19.19.4.4.4.1.cmml"
    xref="S2.E1.m1.19.19.19.4.4.4.1">0</cn></apply></apply><apply id="S2.E1.m1.39.39.3.3.3.2.2.3.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.39.39.3.3.3.2.2.3a.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5">formulae-sequence</csymbol><apply id="S2.E1.m1.39.39.3.3.3.2.2.1.1.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><ci id="S2.E1.m1.21.21.21.6.6.6.cmml" xref="S2.E1.m1.21.21.21.6.6.6">𝑑</ci><apply
    id="S2.E1.m1.39.39.3.3.3.2.2.1.1.3.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><ci id="S2.E1.m1.23.23.23.8.8.8.cmml"
    xref="S2.E1.m1.23.23.23.8.8.8">𝑟</ci><ci id="S2.E1.m1.25.25.25.10.10.10.cmml"
    xref="S2.E1.m1.25.25.25.10.10.10">𝛿</ci></apply></apply><apply id="S2.E1.m1.39.39.3.3.3.2.2.2.2.3.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.39.39.3.3.3.2.2.2.2.3a.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5">formulae-sequence</csymbol><apply id="S2.E1.m1.39.39.3.3.3.2.2.2.2.1.1.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><apply id="S2.E1.m1.39.39.3.3.3.2.2.2.2.1.1.2.cmml"
    xref="S2.E1.m1.5.5.5.5.5.5"><ci id="S2.E1.m1.27.27.27.12.12.12.cmml" xref="S2.E1.m1.27.27.27.12.12.12">𝑎</ci><ci
    id="S2.E1.m1.28.28.28.13.13.13.cmml" xref="S2.E1.m1.28.28.28.13.13.13">𝑛</ci><ci
    id="S2.E1.m1.29.29.29.14.14.14.cmml" xref="S2.E1.m1.29.29.29.14.14.14">𝑑</ci><ci
    id="S2.E1.m1.30.30.30.1.1.1.cmml" xref="S2.E1.m1.30.30.30.1.1.1">𝑉</ci></apply><cn
    type="integer" id="S2.E1.m1.32.32.32.3.3.3.cmml" xref="S2.E1.m1.32.32.32.3.3.3">0</cn></apply><apply
    id="S2.E1.m1.39.39.3.3.3.2.2.2.2.2.2.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><ci id="S2.E1.m1.34.34.34.5.5.5.cmml"
    xref="S2.E1.m1.34.34.34.5.5.5">𝑑</ci><ci id="S2.E1.m1.36.36.36.7.7.7.cmml" xref="S2.E1.m1.36.36.36.7.7.7">𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E1.m1.44c">\begin{split}V=v_{0}.\frac{d-r}{\delta},r\leq
    d\leq r+\delta,\\ V=v_{0},d>r+\delta,and\\ V=0,d<r\end{split}</annotation></semantics></math>
    |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: In this case, $v$ is represented as the robot end effector actual speed override,
    $v_{0}$ as the old override, $d$ as the distance between the robot end-effector
    and the main spherical area, the thickness of the warning zone is represented
    as $\delta$ while $r$ is the sphere radius [[41](#bib.bib41)]. At thestage when
    the robot meets a cylindrical zone, the speed override will be subjected to the
    perception law in ([2](#S2.E2 "In 2.4 How Mobile Robotic Perception Models Can
    Be Used to Attain Complete Situational Awareness ‣ 2 Deep Learning for Robotic
    Perception ‣ A Survey of Deep Learning Techniques for Mobile Robot Applications"))
    [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E2.m1.89" class="ltx_Math" alttext="\begin{split}V=v_{0}.\frac{d_{1}-r}{R-r},0\leq
    z\leq h,\\ V=v_{0}.\frac{d_{2}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl,and\\'
  prefs: []
  type: TYPE_NORMAL
- en: V=v_{0}.\frac{d_{3}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in cyl\end{split}"
    display="block"><semantics id="S2.E2.m1.89a"><mtable displaystyle="true" rowspacing="0pt"
    id="S2.E2.m1.89.89.18"><mtr id="S2.E2.m1.89.89.18a"><mtd class="ltx_align_right"
    columnalign="right" id="S2.E2.m1.89.89.18b"><mrow id="S2.E2.m1.81.81.10.72.14.14.14"><mrow
    id="S2.E2.m1.81.81.10.72.14.14.14.1"><mrow id="S2.E2.m1.81.81.10.72.14.14.14.1.1.1"><mi
    id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml">V</mi><mo id="S2.E2.m1.2.2.2.2.2.2"
    xref="S2.E2.m1.2.2.2.2.2.2.cmml">=</mo><msub id="S2.E2.m1.81.81.10.72.14.14.14.1.1.1.1"><mi
    id="S2.E2.m1.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.3.cmml">v</mi><mn id="S2.E2.m1.4.4.4.4.4.4.1"
    xref="S2.E2.m1.4.4.4.4.4.4.1.cmml">0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
    id="S2.E2.m1.5.5.5.5.5.5" xref="S2.E2.m1.80.80.9.10.cmml">.</mo><mrow id="S2.E2.m1.81.81.10.72.14.14.14.1.2.2"><mrow
    id="S2.E2.m1.81.81.10.72.14.14.14.1.2.2.2"><mfrac id="S2.E2.m1.6.6.6.6.6.6" xref="S2.E2.m1.6.6.6.6.6.6.cmml"><mrow
    id="S2.E2.m1.6.6.6.6.6.6.2" xref="S2.E2.m1.6.6.6.6.6.6.2.cmml"><msub id="S2.E2.m1.6.6.6.6.6.6.2.2"
    xref="S2.E2.m1.6.6.6.6.6.6.2.2.cmml"><mi id="S2.E2.m1.6.6.6.6.6.6.2.2.2" xref="S2.E2.m1.6.6.6.6.6.6.2.2.2.cmml">d</mi><mn
    id="S2.E2.m1.6.6.6.6.6.6.2.2.3" xref="S2.E2.m1.6.6.6.6.6.6.2.2.3.cmml">1</mn></msub><mo
    id="S2.E2.m1.6.6.6.6.6.6.2.1" xref="S2.E2.m1.6.6.6.6.6.6.2.1.cmml">−</mo><mi id="S2.E2.m1.6.6.6.6.6.6.2.3"
    xref="S2.E2.m1.6.6.6.6.6.6.2.3.cmml">r</mi></mrow><mrow id="S2.E2.m1.6.6.6.6.6.6.3"
    xref="S2.E2.m1.6.6.6.6.6.6.3.cmml"><mi id="S2.E2.m1.6.6.6.6.6.6.3.2" xref="S2.E2.m1.6.6.6.6.6.6.3.2.cmml">R</mi><mo
    id="S2.E2.m1.6.6.6.6.6.6.3.1" xref="S2.E2.m1.6.6.6.6.6.6.3.1.cmml">−</mo><mi id="S2.E2.m1.6.6.6.6.6.6.3.3"
    xref="S2.E2.m1.6.6.6.6.6.6.3.3.cmml">r</mi></mrow></mfrac><mo id="S2.E2.m1.7.7.7.7.7.7"
    xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mn id="S2.E2.m1.8.8.8.8.8.8" xref="S2.E2.m1.8.8.8.8.8.8.cmml">0</mn></mrow><mo
    id="S2.E2.m1.9.9.9.9.9.9" xref="S2.E2.m1.9.9.9.9.9.9.cmml">≤</mo><mi id="S2.E2.m1.10.10.10.10.10.10"
    xref="S2.E2.m1.10.10.10.10.10.10.cmml">z</mi><mo id="S2.E2.m1.11.11.11.11.11.11"
    xref="S2.E2.m1.11.11.11.11.11.11.cmml">≤</mo><mi id="S2.E2.m1.12.12.12.12.12.12"
    xref="S2.E2.m1.12.12.12.12.12.12.cmml">h</mi></mrow></mrow><mo id="S2.E2.m1.13.13.13.13.13.13"
    xref="S2.E2.m1.80.80.9.10.cmml">,</mo></mrow></mtd></mtr><mtr id="S2.E2.m1.89.89.18c"><mtd
    class="ltx_align_right" columnalign="right" id="S2.E2.m1.89.89.18d"><mrow id="S2.E2.m1.85.85.14.76.35.35.35"><mrow
    id="S2.E2.m1.82.82.11.73.32.32.32.1"><mi id="S2.E2.m1.14.14.14.1.1.1" xref="S2.E2.m1.14.14.14.1.1.1.cmml">V</mi><mo
    id="S2.E2.m1.15.15.15.2.2.2" xref="S2.E2.m1.15.15.15.2.2.2.cmml">=</mo><msub id="S2.E2.m1.82.82.11.73.32.32.32.1.1"><mi
    id="S2.E2.m1.16.16.16.3.3.3" xref="S2.E2.m1.16.16.16.3.3.3.cmml">v</mi><mn id="S2.E2.m1.17.17.17.4.4.4.1"
    xref="S2.E2.m1.17.17.17.4.4.4.1.cmml">0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
    id="S2.E2.m1.18.18.18.5.5.5" xref="S2.E2.m1.80.80.9.10.cmml">.</mo><mrow id="S2.E2.m1.83.83.12.74.33.33.33.2"><mrow
    id="S2.E2.m1.83.83.12.74.33.33.33.2.2"><mfrac id="S2.E2.m1.19.19.19.6.6.6" xref="S2.E2.m1.19.19.19.6.6.6.cmml"><msub
    id="S2.E2.m1.19.19.19.6.6.6.2" xref="S2.E2.m1.19.19.19.6.6.6.2.cmml"><mi id="S2.E2.m1.19.19.19.6.6.6.2.2"
    xref="S2.E2.m1.19.19.19.6.6.6.2.2.cmml">d</mi><mn id="S2.E2.m1.19.19.19.6.6.6.2.3"
    xref="S2.E2.m1.19.19.19.6.6.6.2.3.cmml">2</mn></msub><mi id="S2.E2.m1.19.19.19.6.6.6.3"
    xref="S2.E2.m1.19.19.19.6.6.6.3.cmml">δ</mi></mfrac><mo id="S2.E2.m1.20.20.20.7.7.7"
    xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mi id="S2.E2.m1.21.21.21.8.8.8" xref="S2.E2.m1.21.21.21.8.8.8.cmml">h</mi></mrow><mo
    id="S2.E2.m1.22.22.22.9.9.9" xref="S2.E2.m1.22.22.22.9.9.9.cmml">≤</mo><mi id="S2.E2.m1.23.23.23.10.10.10"
    xref="S2.E2.m1.23.23.23.10.10.10.cmml">z</mi><mo id="S2.E2.m1.24.24.24.11.11.11"
    xref="S2.E2.m1.24.24.24.11.11.11.cmml">≤</mo><mrow id="S2.E2.m1.83.83.12.74.33.33.33.2.3"><mi
    id="S2.E2.m1.25.25.25.12.12.12" xref="S2.E2.m1.25.25.25.12.12.12.cmml">h</mi><mo
    id="S2.E2.m1.26.26.26.13.13.13" xref="S2.E2.m1.26.26.26.13.13.13.cmml">+</mo><mi
    id="S2.E2.m1.27.27.27.14.14.14" xref="S2.E2.m1.27.27.27.14.14.14.cmml">δ</mi></mrow></mrow><mo
    id="S2.E2.m1.28.28.28.15.15.15" xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mrow id="S2.E2.m1.84.84.13.75.34.34.34.3"><mrow
    id="S2.E2.m1.84.84.13.75.34.34.34.3.2"><mo id="S2.E2.m1.84.84.13.75.34.34.34.3.2a"
    xref="S2.E2.m1.80.80.9.10.cmml">−</mo><mi id="S2.E2.m1.30.30.30.17.17.17" xref="S2.E2.m1.30.30.30.17.17.17.cmml">δ</mi></mrow><mo
    id="S2.E2.m1.31.31.31.18.18.18" xref="S2.E2.m1.31.31.31.18.18.18.cmml">≤</mo><mi
    id="S2.E2.m1.32.32.32.19.19.19" xref="S2.E2.m1.32.32.32.19.19.19.cmml">z</mi><mo
    id="S2.E2.m1.33.33.33.20.20.20" xref="S2.E2.m1.33.33.33.20.20.20.cmml"><</mo><mn
    id="S2.E2.m1.34.34.34.21.21.21" xref="S2.E2.m1.34.34.34.21.21.21.cmml">0</mn></mrow><mo
    id="S2.E2.m1.35.35.35.22.22.22" xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mrow id="S2.E2.m1.85.85.14.76.35.35.35.4"><mi
    id="S2.E2.m1.36.36.36.23.23.23" xref="S2.E2.m1.36.36.36.23.23.23.cmml">p</mi><mo
    id="S2.E2.m1.37.37.37.24.24.24" xref="S2.E2.m1.37.37.37.24.24.24.cmml">∈</mo><mrow
    id="S2.E2.m1.85.85.14.76.35.35.35.4.2.2"><mrow id="S2.E2.m1.85.85.14.76.35.35.35.4.1.1.1"><mi
    id="S2.E2.m1.38.38.38.25.25.25" xref="S2.E2.m1.38.38.38.25.25.25.cmml">c</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.85.85.14.76.35.35.35.4.1.1.1.1" xref="S2.E2.m1.80.80.9.10.cmml">​</mo><mi
    id="S2.E2.m1.39.39.39.26.26.26" xref="S2.E2.m1.39.39.39.26.26.26.cmml">y</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.85.85.14.76.35.35.35.4.1.1.1.1a" xref="S2.E2.m1.80.80.9.10.cmml">​</mo><mi
    id="S2.E2.m1.40.40.40.27.27.27" xref="S2.E2.m1.40.40.40.27.27.27.cmml">l</mi></mrow><mo
    id="S2.E2.m1.41.41.41.28.28.28" xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mrow id="S2.E2.m1.85.85.14.76.35.35.35.4.2.2.2"><mi
    id="S2.E2.m1.42.42.42.29.29.29" xref="S2.E2.m1.42.42.42.29.29.29.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.85.85.14.76.35.35.35.4.2.2.2.1" xref="S2.E2.m1.80.80.9.10.cmml">​</mo><mi
    id="S2.E2.m1.43.43.43.30.30.30" xref="S2.E2.m1.43.43.43.30.30.30.cmml">n</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.85.85.14.76.35.35.35.4.2.2.2.1a" xref="S2.E2.m1.80.80.9.10.cmml">​</mo><mi
    id="S2.E2.m1.44.44.44.31.31.31" xref="S2.E2.m1.44.44.44.31.31.31.cmml">d</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    id="S2.E2.m1.89.89.18e"><mtd class="ltx_align_right" columnalign="right" id="S2.E2.m1.89.89.18f"><mrow
    id="S2.E2.m1.89.89.18.80.31.31.31"><mrow id="S2.E2.m1.86.86.15.77.28.28.28.1"><mi
    id="S2.E2.m1.45.45.45.1.1.1" xref="S2.E2.m1.45.45.45.1.1.1.cmml">V</mi><mo id="S2.E2.m1.46.46.46.2.2.2"
    xref="S2.E2.m1.46.46.46.2.2.2.cmml">=</mo><msub id="S2.E2.m1.86.86.15.77.28.28.28.1.1"><mi
    id="S2.E2.m1.47.47.47.3.3.3" xref="S2.E2.m1.47.47.47.3.3.3.cmml">v</mi><mn id="S2.E2.m1.48.48.48.4.4.4.1"
    xref="S2.E2.m1.48.48.48.4.4.4.1.cmml">0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
    id="S2.E2.m1.49.49.49.5.5.5" xref="S2.E2.m1.80.80.9.10.cmml">.</mo><mrow id="S2.E2.m1.87.87.16.78.29.29.29.2"><mrow
    id="S2.E2.m1.87.87.16.78.29.29.29.2.2"><mfrac id="S2.E2.m1.50.50.50.6.6.6" xref="S2.E2.m1.50.50.50.6.6.6.cmml"><msub
    id="S2.E2.m1.50.50.50.6.6.6.2" xref="S2.E2.m1.50.50.50.6.6.6.2.cmml"><mi id="S2.E2.m1.50.50.50.6.6.6.2.2"
    xref="S2.E2.m1.50.50.50.6.6.6.2.2.cmml">d</mi><mn id="S2.E2.m1.50.50.50.6.6.6.2.3"
    xref="S2.E2.m1.50.50.50.6.6.6.2.3.cmml">3</mn></msub><mi id="S2.E2.m1.50.50.50.6.6.6.3"
    xref="S2.E2.m1.50.50.50.6.6.6.3.cmml">δ</mi></mfrac><mo id="S2.E2.m1.51.51.51.7.7.7"
    xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mi id="S2.E2.m1.52.52.52.8.8.8" xref="S2.E2.m1.52.52.52.8.8.8.cmml">h</mi></mrow><mo
    id="S2.E2.m1.53.53.53.9.9.9" xref="S2.E2.m1.53.53.53.9.9.9.cmml">≤</mo><mi id="S2.E2.m1.54.54.54.10.10.10"
    xref="S2.E2.m1.54.54.54.10.10.10.cmml">z</mi><mo id="S2.E2.m1.55.55.55.11.11.11"
    xref="S2.E2.m1.55.55.55.11.11.11.cmml">≤</mo><mrow id="S2.E2.m1.87.87.16.78.29.29.29.2.3"><mi
    id="S2.E2.m1.56.56.56.12.12.12" xref="S2.E2.m1.56.56.56.12.12.12.cmml">h</mi><mo
    id="S2.E2.m1.57.57.57.13.13.13" xref="S2.E2.m1.57.57.57.13.13.13.cmml">+</mo><mi
    id="S2.E2.m1.58.58.58.14.14.14" xref="S2.E2.m1.58.58.58.14.14.14.cmml">δ</mi></mrow></mrow><mo
    id="S2.E2.m1.59.59.59.15.15.15" xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mrow id="S2.E2.m1.88.88.17.79.30.30.30.3"><mrow
    id="S2.E2.m1.88.88.17.79.30.30.30.3.2"><mo id="S2.E2.m1.88.88.17.79.30.30.30.3.2a"
    xref="S2.E2.m1.80.80.9.10.cmml">−</mo><mi id="S2.E2.m1.61.61.61.17.17.17" xref="S2.E2.m1.61.61.61.17.17.17.cmml">δ</mi></mrow><mo
    id="S2.E2.m1.62.62.62.18.18.18" xref="S2.E2.m1.62.62.62.18.18.18.cmml">≤</mo><mi
    id="S2.E2.m1.63.63.63.19.19.19" xref="S2.E2.m1.63.63.63.19.19.19.cmml">z</mi><mo
    id="S2.E2.m1.64.64.64.20.20.20" xref="S2.E2.m1.64.64.64.20.20.20.cmml"><</mo><mn
    id="S2.E2.m1.65.65.65.21.21.21" xref="S2.E2.m1.65.65.65.21.21.21.cmml">0</mn></mrow><mo
    id="S2.E2.m1.66.66.66.22.22.22" xref="S2.E2.m1.80.80.9.10.cmml">,</mo><mrow id="S2.E2.m1.89.89.18.80.31.31.31.4"><mi
    id="S2.E2.m1.67.67.67.23.23.23" xref="S2.E2.m1.67.67.67.23.23.23.cmml">p</mi><mo
    id="S2.E2.m1.68.68.68.24.24.24" xref="S2.E2.m1.68.68.68.24.24.24.cmml">∈</mo><mrow
    id="S2.E2.m1.89.89.18.80.31.31.31.4.1"><mi id="S2.E2.m1.69.69.69.25.25.25" xref="S2.E2.m1.69.69.69.25.25.25.cmml">c</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.89.89.18.80.31.31.31.4.1.1" xref="S2.E2.m1.80.80.9.10.cmml">​</mo><mi
    id="S2.E2.m1.70.70.70.26.26.26" xref="S2.E2.m1.70.70.70.26.26.26.cmml">y</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.89.89.18.80.31.31.31.4.1.1a" xref="S2.E2.m1.80.80.9.10.cmml">​</mo><mi
    id="S2.E2.m1.71.71.71.27.27.27" xref="S2.E2.m1.71.71.71.27.27.27.cmml">l</mi></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S2.E2.m1.89b"><apply id="S2.E2.m1.80.80.9.10.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S2.E2.m1.80.80.9.10a.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5">formulae-sequence</csymbol><apply id="S2.E2.m1.72.72.1.1.1.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1">𝑉</ci><apply
    id="S2.E2.m1.72.72.1.1.1.3.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><csymbol cd="ambiguous"
    id="S2.E2.m1.72.72.1.1.1.3.1.cmml" xref="S2.E2.m1.5.5.5.5.5.5">subscript</csymbol><ci
    id="S2.E2.m1.3.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3.3">𝑣</ci><cn type="integer"
    id="S2.E2.m1.4.4.4.4.4.4.1.cmml" xref="S2.E2.m1.4.4.4.4.4.4.1">0</cn></apply></apply><apply
    id="S2.E2.m1.73.73.2.2.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.73.73.2.2.2b.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><list id="S2.E2.m1.73.73.2.2.2.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply
    id="S2.E2.m1.6.6.6.6.6.6.cmml" xref="S2.E2.m1.6.6.6.6.6.6"><apply id="S2.E2.m1.6.6.6.6.6.6.2.cmml"
    xref="S2.E2.m1.6.6.6.6.6.6.2"><apply id="S2.E2.m1.6.6.6.6.6.6.2.2.cmml" xref="S2.E2.m1.6.6.6.6.6.6.2.2"><csymbol
    cd="ambiguous" id="S2.E2.m1.6.6.6.6.6.6.2.2.1.cmml" xref="S2.E2.m1.6.6.6.6.6.6.2.2">subscript</csymbol><ci
    id="S2.E2.m1.6.6.6.6.6.6.2.2.2.cmml" xref="S2.E2.m1.6.6.6.6.6.6.2.2.2">𝑑</ci><cn
    type="integer" id="S2.E2.m1.6.6.6.6.6.6.2.2.3.cmml" xref="S2.E2.m1.6.6.6.6.6.6.2.2.3">1</cn></apply><ci
    id="S2.E2.m1.6.6.6.6.6.6.2.3.cmml" xref="S2.E2.m1.6.6.6.6.6.6.2.3">𝑟</ci></apply><apply
    id="S2.E2.m1.6.6.6.6.6.6.3.cmml" xref="S2.E2.m1.6.6.6.6.6.6.3"><ci id="S2.E2.m1.6.6.6.6.6.6.3.2.cmml"
    xref="S2.E2.m1.6.6.6.6.6.6.3.2">𝑅</ci><ci id="S2.E2.m1.6.6.6.6.6.6.3.3.cmml" xref="S2.E2.m1.6.6.6.6.6.6.3.3">𝑟</ci></apply></apply><cn
    type="integer" id="S2.E2.m1.8.8.8.8.8.8.cmml" xref="S2.E2.m1.8.8.8.8.8.8">0</cn></list><ci
    id="S2.E2.m1.10.10.10.10.10.10.cmml" xref="S2.E2.m1.10.10.10.10.10.10">𝑧</ci></apply><apply
    id="S2.E2.m1.73.73.2.2.2c.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.12.12.12.12.12.12.cmml"
    xref="S2.E2.m1.12.12.12.12.12.12">ℎ</ci></apply></apply><apply id="S2.E2.m1.74.74.3.3.3.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.14.14.14.1.1.1.cmml" xref="S2.E2.m1.14.14.14.1.1.1">𝑉</ci><apply
    id="S2.E2.m1.74.74.3.3.3.3.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><csymbol cd="ambiguous"
    id="S2.E2.m1.74.74.3.3.3.3.1.cmml" xref="S2.E2.m1.5.5.5.5.5.5">subscript</csymbol><ci
    id="S2.E2.m1.16.16.16.3.3.3.cmml" xref="S2.E2.m1.16.16.16.3.3.3">𝑣</ci><cn type="integer"
    id="S2.E2.m1.17.17.17.4.4.4.1.cmml" xref="S2.E2.m1.17.17.17.4.4.4.1">0</cn></apply></apply><apply
    id="S2.E2.m1.75.75.4.4.4.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.75.75.4.4.4b.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><list id="S2.E2.m1.75.75.4.4.4.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply
    id="S2.E2.m1.19.19.19.6.6.6.cmml" xref="S2.E2.m1.19.19.19.6.6.6"><apply id="S2.E2.m1.19.19.19.6.6.6.2.cmml"
    xref="S2.E2.m1.19.19.19.6.6.6.2"><csymbol cd="ambiguous" id="S2.E2.m1.19.19.19.6.6.6.2.1.cmml"
    xref="S2.E2.m1.19.19.19.6.6.6.2">subscript</csymbol><ci id="S2.E2.m1.19.19.19.6.6.6.2.2.cmml"
    xref="S2.E2.m1.19.19.19.6.6.6.2.2">𝑑</ci><cn type="integer" id="S2.E2.m1.19.19.19.6.6.6.2.3.cmml"
    xref="S2.E2.m1.19.19.19.6.6.6.2.3">2</cn></apply><ci id="S2.E2.m1.19.19.19.6.6.6.3.cmml"
    xref="S2.E2.m1.19.19.19.6.6.6.3">𝛿</ci></apply><ci id="S2.E2.m1.21.21.21.8.8.8.cmml"
    xref="S2.E2.m1.21.21.21.8.8.8">ℎ</ci></list><ci id="S2.E2.m1.23.23.23.10.10.10.cmml"
    xref="S2.E2.m1.23.23.23.10.10.10">𝑧</ci></apply><apply id="S2.E2.m1.75.75.4.4.4c.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.75.75.4.4.4.6.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci
    id="S2.E2.m1.25.25.25.12.12.12.cmml" xref="S2.E2.m1.25.25.25.12.12.12">ℎ</ci><ci
    id="S2.E2.m1.27.27.27.14.14.14.cmml" xref="S2.E2.m1.27.27.27.14.14.14">𝛿</ci></apply></apply></apply><apply
    id="S2.E2.m1.76.76.5.5.5.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.76.76.5.5.5b.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.76.76.5.5.5.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci
    id="S2.E2.m1.30.30.30.17.17.17.cmml" xref="S2.E2.m1.30.30.30.17.17.17">𝛿</ci></apply><ci
    id="S2.E2.m1.32.32.32.19.19.19.cmml" xref="S2.E2.m1.32.32.32.19.19.19">𝑧</ci></apply><apply
    id="S2.E2.m1.76.76.5.5.5c.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><cn type="integer"
    id="S2.E2.m1.34.34.34.21.21.21.cmml" xref="S2.E2.m1.34.34.34.21.21.21">0</cn></apply></apply><apply
    id="S2.E2.m1.77.77.6.6.6.3.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><csymbol cd="ambiguous"
    id="S2.E2.m1.77.77.6.6.6.3a.cmml" xref="S2.E2.m1.5.5.5.5.5.5">formulae-sequence</csymbol><apply
    id="S2.E2.m1.77.77.6.6.6.1.1.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.36.36.36.23.23.23.cmml"
    xref="S2.E2.m1.36.36.36.23.23.23">𝑝</ci><apply id="S2.E2.m1.77.77.6.6.6.1.1.3.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.38.38.38.25.25.25.cmml" xref="S2.E2.m1.38.38.38.25.25.25">𝑐</ci><ci
    id="S2.E2.m1.39.39.39.26.26.26.cmml" xref="S2.E2.m1.39.39.39.26.26.26">𝑦</ci><ci
    id="S2.E2.m1.40.40.40.27.27.27.cmml" xref="S2.E2.m1.40.40.40.27.27.27">𝑙</ci></apply></apply><apply
    id="S2.E2.m1.77.77.6.6.6.2.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.77.77.6.6.6.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.42.42.42.29.29.29.cmml" xref="S2.E2.m1.42.42.42.29.29.29">𝑎</ci><ci
    id="S2.E2.m1.43.43.43.30.30.30.cmml" xref="S2.E2.m1.43.43.43.30.30.30">𝑛</ci><ci
    id="S2.E2.m1.44.44.44.31.31.31.cmml" xref="S2.E2.m1.44.44.44.31.31.31">𝑑</ci><ci
    id="S2.E2.m1.45.45.45.1.1.1.cmml" xref="S2.E2.m1.45.45.45.1.1.1">𝑉</ci></apply><apply
    id="S2.E2.m1.77.77.6.6.6.2.2.3.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><csymbol cd="ambiguous"
    id="S2.E2.m1.77.77.6.6.6.2.2.3.1.cmml" xref="S2.E2.m1.5.5.5.5.5.5">subscript</csymbol><ci
    id="S2.E2.m1.47.47.47.3.3.3.cmml" xref="S2.E2.m1.47.47.47.3.3.3">𝑣</ci><cn type="integer"
    id="S2.E2.m1.48.48.48.4.4.4.1.cmml" xref="S2.E2.m1.48.48.48.4.4.4.1">0</cn></apply></apply></apply><apply
    id="S2.E2.m1.78.78.7.7.7.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.78.78.7.7.7b.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><list id="S2.E2.m1.78.78.7.7.7.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply
    id="S2.E2.m1.50.50.50.6.6.6.cmml" xref="S2.E2.m1.50.50.50.6.6.6"><apply id="S2.E2.m1.50.50.50.6.6.6.2.cmml"
    xref="S2.E2.m1.50.50.50.6.6.6.2"><csymbol cd="ambiguous" id="S2.E2.m1.50.50.50.6.6.6.2.1.cmml"
    xref="S2.E2.m1.50.50.50.6.6.6.2">subscript</csymbol><ci id="S2.E2.m1.50.50.50.6.6.6.2.2.cmml"
    xref="S2.E2.m1.50.50.50.6.6.6.2.2">𝑑</ci><cn type="integer" id="S2.E2.m1.50.50.50.6.6.6.2.3.cmml"
    xref="S2.E2.m1.50.50.50.6.6.6.2.3">3</cn></apply><ci id="S2.E2.m1.50.50.50.6.6.6.3.cmml"
    xref="S2.E2.m1.50.50.50.6.6.6.3">𝛿</ci></apply><ci id="S2.E2.m1.52.52.52.8.8.8.cmml"
    xref="S2.E2.m1.52.52.52.8.8.8">ℎ</ci></list><ci id="S2.E2.m1.54.54.54.10.10.10.cmml"
    xref="S2.E2.m1.54.54.54.10.10.10">𝑧</ci></apply><apply id="S2.E2.m1.78.78.7.7.7c.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.78.78.7.7.7.6.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci
    id="S2.E2.m1.56.56.56.12.12.12.cmml" xref="S2.E2.m1.56.56.56.12.12.12">ℎ</ci><ci
    id="S2.E2.m1.58.58.58.14.14.14.cmml" xref="S2.E2.m1.58.58.58.14.14.14">𝛿</ci></apply></apply></apply><apply
    id="S2.E2.m1.79.79.8.8.8.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.79.79.8.8.8b.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><apply id="S2.E2.m1.79.79.8.8.8.2.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci
    id="S2.E2.m1.61.61.61.17.17.17.cmml" xref="S2.E2.m1.61.61.61.17.17.17">𝛿</ci></apply><ci
    id="S2.E2.m1.63.63.63.19.19.19.cmml" xref="S2.E2.m1.63.63.63.19.19.19">𝑧</ci></apply><apply
    id="S2.E2.m1.79.79.8.8.8c.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><cn type="integer"
    id="S2.E2.m1.65.65.65.21.21.21.cmml" xref="S2.E2.m1.65.65.65.21.21.21">0</cn></apply></apply><apply
    id="S2.E2.m1.80.80.9.9.9.cmml" xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.67.67.67.23.23.23.cmml"
    xref="S2.E2.m1.67.67.67.23.23.23">𝑝</ci><apply id="S2.E2.m1.80.80.9.9.9.3.cmml"
    xref="S2.E2.m1.5.5.5.5.5.5"><ci id="S2.E2.m1.69.69.69.25.25.25.cmml" xref="S2.E2.m1.69.69.69.25.25.25">𝑐</ci><ci
    id="S2.E2.m1.70.70.70.26.26.26.cmml" xref="S2.E2.m1.70.70.70.26.26.26">𝑦</ci><ci
    id="S2.E2.m1.71.71.71.27.27.27.cmml" xref="S2.E2.m1.71.71.71.27.27.27">𝑙</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E2.m1.89c">\begin{split}V=v_{0}.\frac{d_{1}-r}{R-r},0\leq
    z\leq h,\\ V=v_{0}.\frac{d_{2}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl,and\\ V=v_{0}.\frac{d_{3}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl\end{split}</annotation></semantics></math> |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: The perception law above represents $h$ as the cylinder height, the position
    of the robot end effector is denoted as $p$, the distance between the center of
    the cylinder and the position of the robot is denoted as $d_{1}$ while the $d_{2}$
    represents the distance between the top or bottom base of the cylinder as well
    as the position of the robot [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the least position between the position of the robot and the top/bottom
    circumference points of the cylinder base is denoted as $d_{3}$. Furthermore,
    the robot speed override coincides with the past speed override at the stage when
    the robot end effector is outside the warning zone [[43](#bib.bib43)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning for Robotic Control and Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 How Autonomous Robotic Systems Use Deep Learning to Control and Explore
    Their Enivornments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Realizing the benefits of autonomous robot exploration presents robotics researchers
    with many applications of considerable community and financial impact [[44](#bib.bib44)].
    Robotics research relies on perfect knowledge and control of the environment [[45](#bib.bib45)].
    The problems related to unstructured environments are an outcome of the high-dimensional
    state space as well as the inherent likelihood in mapping sensory perceptions
    on particular states. It should be noted that the high dimensionality of the state
    space is representative of the most basic difficulty since robots leave highly
    controlled environments of a laboratory and enter into unstructured surroundings.
    For example autonomous unmanned aerial vehicles used deep learning to classify
    terrain and solve any exploration shortcomings by generating control commands
    for its human operator so as to adapt to a certain trade-off [[46](#bib.bib46)].
    The major hypothesis of this approach is therefore for mobile robots to succeed
    in unstructured surroundings such that they can carefully choose assignment specific
    attributes and identify the relevant real-time structures to lower their state
    space without impacting the performance of their exploration objectives [[47](#bib.bib47)].
    Robots perform assignments by exploring their surroundings. As such, given our
    focus on autonomous mobile exploration, we shall direct most attention to exploration
    in service of movement, that is to say collision-free movement for end-effector
    placement [[48](#bib.bib48)]. The challenge of generating such movement is an
    example of the problem faced in motion planning. Motion planning for robotic systems
    with many levels of freedom is computationally challenging even in environments
    that are highly structured due to the increased-dimensional configuration space
    [[49](#bib.bib49)]. In addition, unstructured environments are associated with
    the imposition of added difficulties in motion generation in comparison to the
    traditional motion planning process. Furthermore, in environments that are unstructured,
    a robot is only able to possess limited knowledge of its environment, objects
    are able to change their unknown to a known state for the robot by manipulating
    assignments may using the end effector [[50](#bib.bib50)]. However, this capability
    can be challenged by to a constrained trajectory such that the mobile robot is
    unable to reach a particular location with ease [[51](#bib.bib51)]. Each of these
    problems makes the challenge of motion generation more complex. The explicit coordination
    of planning and sensing necessary to manage dynamic environments increases the
    dimension of the state space. In addition, robotic assignment requirements impose
    stringent terms in form of high-frequency feedback [[52](#bib.bib52)]. Therefore,
    existing motion planners tend to make assumptions that are highly restrictive
    for environments that are unstructured since they are highly computationally difficult
    to satisfy in terms of gaining operator-to-robot feedback [[53](#bib.bib53)].
    These assumptions, as well as the computational difficulty, are an outcome of
    the foundation of motion planning with its high-dimensional configuration spacing
    which makes it highly unsuited in solving space problems. Planners instead can
    be able to solve this paradigm by solely using workspace information for collision
    avoidance. Nearly all real-world surroundings, however, comprised of a considerable
    measure of structure [[54](#bib.bib54)]. For instance, buildings are segmented
    into hallways, doors, and rooms; outdoor environments comprise of paths, streets,
    and intersections while objects for example tables, shelves and chairs have more
    favorable approach directions. This information, however, is neglected when robot
    exploration planners exclusively operate on the basis of a configuration space
    [[55](#bib.bib55)]. Therefore, the outcome for most robotic exploration planners
    is to operate by the assumption that any environment is perfectly defined and
    sustained as static in the course of planning [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning in Robotic Robotic Navigation and Autonomous Driving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using deep learning to attain autonomous driving assignment is not a perfectly
    controlled and modeled task as most people think. Instead, it needs optimal perceptual
    capabilities [[57](#bib.bib57)]. The process of perception of a robot’s environment
    and interpreting the information it acquires allows it to understand the condition
    of its surroundings, devise plans to change the state and observe how its actions
    impact its environment. In unstructured environments, recognition of objects has
    been proven to be highly challenging. With immense volumes of sensor data and
    increased variation of objects within similar object categories, for example,
    a paved and unpaved road as well as recognizing objects [[58](#bib.bib58)]. Deep
    learning uses machine learning motion capturing abilities as well as optimal perceptual
    functionalities. This is a prerequisite of several vital applications for robots
    for instance flexible manufacturing, planetary exploration, collaboration with
    human experts and elder care [[59](#bib.bib59)]. The challenge of driving in an
    environment includes problems of movement of the robot in navigating varied obstacles
    by pushing and pulling. Even in structured environments, automated driving is
    difficult due to the complexities of the related state space [[60](#bib.bib60)].
    This state space comprises of appearance, dimension, position as well as the weight
    of objects within the scene. It also comprises of several other relevant attributes
    which provide indications of where to pull, push or grasp as well as the level
    of force to apply [[61](#bib.bib61)]. Deep learning and associated machine learning
    collective actions improved the performance of robots in making the decision by
    embedding the capability within these systems to choose between possible actions
    and determine the required parameters for their controllers.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 How Autonomous Robotic Systems Use Deep Learning to Navigate Their Enivornments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Autonomous driving in unstructured environments faces many challenges which
    do not exist in structured environments. In unstructured environments, object
    attributes needed for driving cannot be defined as priori. Information concerning
    objects has to be gained through sensors even though these are normally ambiguous
    and therefore introduce uncertainty and avail information that is redundant. Furthermore,
    autonomous driving in unstructured and dynamic surroundings normally needs responding
    in a fashion that is timely to a rapidly transforming environment [[62](#bib.bib62)].
    Problems of autonomous driving can be made simple through the exploitation of
    the structure which is inherent to human surroundings. Most objects in the real
    world are based on designs to perform certain functions with the intention of
    being utilized by humans [[44](#bib.bib44)]. As an outcome, several real-world
    objects can share common attributes which allude to their intended usage. By placing
    emphasis on these assignment-related object attributes, the complexity of autonomous
    driving is lowered. For example, visual data can be analyzed for the identification
    of few points which correspond to positive locations at which a robot can maneuver.
    Furthermore, since movement features are similar across many objects, robots could
    be trained to identify them. As a consequence, the state space that requires exploration
    in order to move is significantly lowered [[63](#bib.bib63)]. In this case, researchers
    normally make assumptions to lower the complexity of autonomous driving in unstructured
    environments. For instance, it is normally assumed that full models of objects
    in the environment can be availed a priori or it can be gained through sensors
    while the environment remains the same in the process of interaction [[64](#bib.bib64)].
    However, in practical terms, it is impossible to avail autonomous driving with
    full priori models in the actual world. However, models that are perfect are not
    a prerequisite for successful autonomous driving [[65](#bib.bib65)]. Robot mobile
    driving can be guided using existing structures in the world and in most cases
    those which are easy to perceive. As such, by leveraging this structure, the complexity
    of autonomous driving in unstructured environments is lowered significantly. Similarly,
    understanding the intrinsic measures of freedom of objects in an environment is
    also able to lower the complexity of autonomous driving in unstructured surroundings
    [[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: 5 Semi-Supervised and Self-Supervised Learning for Robotics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imitation based learning is a promising approach to tackling the difficult robotic
    assignments, for instance, autonomous navigation. However, it needs human supervision
    to oversee the process of training and sending correct control commands to robots
    without feedback. It is this form of procedure that is prone to failure and high
    cost [[66](#bib.bib66)]. Therefore, in order to lower human involvement and limit
    manual data labeling of autonomous robotic navigation using imitation learning,
    the techniques of semi-supervised and self-supervised learning can be introduced.
    It should be noted however that these techniques need to operate according to
    a multi-sensory design approach. The solution should comprise of a suboptimal
    sensor policy founded on sensor fusion and automatic labeling of states the robot
    could encounter [[67](#bib.bib67)]. This is also aimed at eliminating human supervision
    in the course of the learning process [[68](#bib.bib68)] [[69](#bib.bib69)]. Furthermore,
    a recording policy needs to be developed to provide throttling of the adversarial
    impact of too much data being learned from the suboptimal sensor policy. As such,
    this solution will equip the robot with the capability of achieving near-human
    performance to a large extent in most of its assignments [[70](#bib.bib70)]. It
    is also capable of surpassing human performance in situations of unexpected outcomes
    for instance hardware failure or human operator error. Furthermore, the semi-supervised
    method can be considered as a solution to the problem of track classification
    in congested environments such as a room. This problem entails object classification
    undergoing segmenting and tracking without using class models [[71](#bib.bib71)].
    Therefore, we introduce semi-supervised learning as a technique capable of solving
    this problem by iteratively training a classifier and extracting vital training
    examples from the data in its unlabeled state. This is achieved by exploiting
    the tracking data. In addition, the process also involves evaluating large multiclass
    difficulties presented by data sourced from congested artificial natural environments
    such as a street [[72](#bib.bib72)]. As such, when provided with manually labeled
    training tracks of individual object classes, then semi-supervised learning performance
    in comparison to self-supervised learning is able to use thousands of training
    tracks [[73](#bib.bib73)]. In addition, when also provided with augmented unlabeled
    data, semi-supervised learning has demonstrated the capability of outperforming
    the self-supervised learning method. In this case, semi-supervised learning presents
    itself as the most simplified algorithmic approach to speeding up incremental
    updating of booster classifiers by lowering the learning time factor by three
    [[74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: 6 Multimodal Deep Learning Methods for Robotic Vision and Control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performing tasks in an imperfect controlled and modeled surrounding means robots
    need to have optimal multimodal capabilities. The procedure of perceiving the
    environment and interpreting the gained information allows robots to perceive
    the state of the environment, devise methods to alter the state and observe the
    impacts of their actions on the environment
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 How Autonomous Robotic Systems Use Semi-Supervised and Self-Supervised Learning
    to Learn Their Enivornments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The environment of a robot can be controlled too many levels. In principle,
    less constrained environments are more difficult to perceive. In the real-world
    and its unstructured and dynamic surroundings such as vegetation landscape and
    terrain, the perception of a mobile robot needs to be capable of navigating this
    unknown environment by using sensor modalities. More so, even without the introduction
    of uncertainty, sensors in themselves are ambiguous [[75](#bib.bib75)]. For example,
    a lemon and a soccer ball can look similar from a certain perspective. In addition,
    a cup could be invisible in case the cupboard is shut and it can be challenging
    to tell the difference between a remote control and cell phone is they are both
    facing down. These factors are all contributive to the challenges of perceiving
    the state of the environment. Furthermore, for example, advances in face recognition
    normally operate under the assumption concerning the position and orientation
    of the individual in the image. The outcomes of object segmentation are normally
    founded on the capability of telling the difference between an object and background
    on the basis of differences in color [[76](#bib.bib76)]. In addition, object recognition
    is normally reduced to similarities in computing to a limited collection of given
    objects. On the other hand, in an unstructured environment, the position and orientation
    are uncontrollable since assumptions concerning color and shades and problematic
    to justify. Furthermore, the range and likely objects the robot could encounter
    are intractable [[77](#bib.bib77)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 How Autonomous Robotic Systems Use Multimodal and Deep Learning Methods
    to Percieve Their Enivornments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Therefore, in order to tackle perception in unstructured surroundings, robots
    need to be able to lower the state space that requires being analyzed. to provide
    facilitation of certain perceptual assignments by limiting uncertainty and as
    such lowering the dimensionality of the state space. For instance, in order to
    compute the distance of objects in an environment, robots need to relate depth
    to visual information [[78](#bib.bib78)]. This is normally done with the use of
    a stereo vision system and solving the correspondence challenge between two static
    2D images. In addressing the correspondence problem, however, it is complicated
    due to noise, many likely matches and the uncertainty in calibrating the camera
    [[79](#bib.bib79)]. On the other hand, in a system capable of the capture of at
    least three view angles in one image, this lowers the state space by reducing
    a multi-sensor system to a single sensor. Furthermore, in an unstructured environment,
    recognition of objects has proven to be highly challenging [[57](#bib.bib57)].
    This is due to large volumes of sensor data and an increased variation within
    objects of a similar category. In this case, object recognition is an increased
    dimensional challenge. However, even in the face of these challenges, objects
    in the similar category do share similar attributes. As such, by applying this
    insight, robots are able to place emphasis to only a minimal subset of the state
    space that comprises of the most relevant characteristics for classification [[58](#bib.bib58)].
  prefs: []
  type: TYPE_NORMAL
- en: 7 Application of Deep Models to Problems in Vision and Robotics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding overview of machine learning applications in robotics will highlight
    five major areas where considerable impacts have been made by robotic technologies
    currently and in the development levels for long-term use. However, by no means
    inclusive, the aim of this summarization is to provide the reader with a preview
    of the form of machine learning applications in existence within robotics and
    motivate the desire for extended research in such and other fields [[80](#bib.bib80)].
    The growth of big data which is to day visual information provided on the internet
    with the inclusion of annotated images and video has pushed forward advancements
    in computer vision which has in turn assisted in extending machine-based learning
    systems to prediction learning methods such as those presented by research at
    Carnegie Mellon [[81](#bib.bib81)]. This presentation involved unveiling offshoot
    examples such as the anomaly detection using supervised learning have been applied
    in building structures with the capability of searching and assessing damages
    in silicon wafers with the use of convolutional neural networks [[81](#bib.bib81)].
    In addition, extrasensory technologies for instance lidar, ultrasound, and radar
    such as those developed by Nvidia as also propelling the creation of 360-degree
    vision-based systems for autonomous vehicles and UAVs [[82](#bib.bib82)]. Imitation
    learning which is closely associated with observational learning is also a field
    categorized by reinforcement learning or the difficulties of gaining an agent
    to act towards maximizing rewards. One example is Bayesian or probabilistic models
    which stand out as a common machine learning method used an integral component
    of field robotics where attributes of mobility in fields such as construction,
    rescue, and inverse optimal control methods have been utilized in humanoid robotics,
    off-road terrain navigation, and legged locomotion [[83](#bib.bib83)]. Self-supervised
    learning is another method that allows robots to generate personalized training
    instances so as to refine performance. This has been integrated robots and optical
    devices for instance in detection and rejection of objects such as dust and snow,
    identification of obstacles, vehicle dynamics modeling, and 3D-scene analysis
    [[84](#bib.bib84)]. Assistive and medical technologies are another application
    where assistive robots entail devices capable of sensing, processing sensory data
    and undertaking actions that gain individuals with disabilities. Even as smart
    assistive technologies are existent for the overall population, for instance,
    driver assistance resources, movement therapy robots avail diagnostic or therapeutic
    gains. Furthermore, multi-agent learning concerning coordination and negotiation
    are vital components involving machine learning based robots also known as agents.
    This method is broadly utilized in games that are capable of adapting to a transforming
    landscape of other robots or agents and searching for equilibrium strategies [[76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: The interdisciplinary arena of computer vision concerned with how computers
    are able to be developed for attaining an increased measure of perception from
    the digital imagery of video. Computer vision assignments comprise of techniques
    for acquisition, processing, analysis, perceiving digital imagery as well as extracting
    high dimensional information from the actual world so as to yield numerical or
    symbolic data for example in the format of decisions [[85](#bib.bib85)]. Artificial
    intelligence areas concerned with autonomous planning or deliberated robotic systems
    navigation demand a thorough perception of such settings since information concerning
    the environment can be availed by a computer vision system in action as a vision
    sensor [[57](#bib.bib57)]. Therefore, artificial intelligence, as well as computer
    vision, share other fields, for instance, pattern recognition and learning methods.
    The consequence is that in certain cases, computer vision is viewed as a component
    of artificial intelligence. Another application of computer vision is solid state
    physics since a large portion of computer vision systems are reliant on imagery
    sensors that provide detection of electromagnetic radiation which normally takes
    a form that is either visible or infra-red lighting [[81](#bib.bib81)] [[86](#bib.bib86)].
    These sensors operate according to quantum physics designs with the procedure
    by which light interacts with the surface better explained by optics behaviors.
    Such intricate inner working demonstrates how even complex image sensors even
    need quantum mechanics to avail a full understanding of the process of image formation.
    Furthermore, another application of computer vision is the multiple measurement
    challenges in physics which can be tackled by utilizing computer vision for instance
    fluids motion [[39](#bib.bib39)].
  prefs: []
  type: TYPE_NORMAL
- en: 8 Benefits and Drawbacks of Deep Learning to be Applied in Mobile Robots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 8.1 Benefits of Deep Learning in the Context of Mobile Robots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The gains of deep learning as a component of the wider family of machine learning
    techniques founded on representations of learning data in opposition to assignment-particular
    algorithms through supervised, unsupervised and semi-supervised learning allows
    for structured on the interpretation of information processing and patterns of
    communications which can be viewed as trials at defining a relation between multiple
    stimuli and related neuronal responses [[87](#bib.bib87)]. Deep learning architectures
    for instance deep neural, deep beliefs as well as recurrent neural networks have
    been utilized in arenas inclusive of computer vision, natural language processing,
    social network filtering, speech recognition, bioinformatics and audio recognition.
    In these mentioned fields, deep learning architecture has produced outcomes in
    comparison to an in certain case more advanced to human expertise. Furthermore,
    deep learning algorithms utilize a cascade of many nonlinear processing unit layers
    for extraction of features and transformation with particular layers applying
    the output from the past layer as input [[88](#bib.bib88)].
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning proposes a simplified conceptual light framework
    that utilizes asynchronous gradient descent to cater for deep neural network controller
    optimization. The presented asynchronous variants in standard reinforcement learning
    algorithms reveal that parallel actor learner holds a stabilizing influence on
    training which allows for the successful training of the neural network controller
    [[19](#bib.bib19)]. It is on this premise that asynchronous variants are presented
    as the most appealing Deep reinforcement learning approach.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Drawbacks of Deep Learning in the Context of Mobile Robots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The drawbacks of deep learning in applied robotics is that the storage of the
    agent data using replay memory does not allow for re-batching or sampling at randomly
    from varied time-stages. As such, memory aggregation in this approach lowers non-stationary
    and eroded updates while in simultaneously limiting the techniques to off-policy
    reinforcement learning algorithms [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is set to transform the arena of artificial intelligence as well
    as represent a measure in the direction of developing autonomous systems with
    an increased scope of perceiving the visual world. Presently, deep learning is
    allowing scaling of challenges that were traditionally intractable for instance
    learning to directly play video games for pixels. Furthermore, deep learning algorithms
    are also utilized in robotics to foster the capability of control functionality
    for robots indirectly learning from cameral inputs in the actual world. It is
    on this premise that the survey above illustrates the major advances and approaches
    of reinforcement learning in regard to the main streams of value and policy-driven
    methods as well as associated coverage of central algorithms in deep learning
    for instance deep network, asynchronous advantage actor-critic as well as trust
    region policy optimization. Furthermore, the research survey has highlighted the
    gains of deep neural networking with emphasis on visual perception through deep
    learning. One of the core objectives of the discipline of artificial intelligence
    it the production of completely autonomous agents that are able to interact with
    their settings to learn optimal behavior and demonstrate improvements with time
    by trial and error regiments. It is therefore on this premise that creating artificial
    intelligence systems that are responsive and with the capability of learning has
    long been an elusive challenge. However, hope is found in the principled mathematical
    framework of deep learning with utilizes experience driven autonomous learning
    to apply a functional approximation to represent learning attributes of deep neural
    networks to overcome these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, p. 436, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,
    and D. Wierstra, “Continuous control with deep reinforcement learning,” *arXiv
    preprint arXiv:1509.02971*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. S. Esteves, A. Carvalho, and C. Couto, “Generalized geometric triangulation
    algorithm for mobile robot absolute self-localization,” in *Industrial Electronics,
    2003\. ISIE’03\. 2003 IEEE International Symposium on*, vol. 1.   IEEE, 2003,
    pp. 346–351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] A. Vedaldi and K. Lenc, “Matconvnet: Convolutional neural networks for
    matlab,” in *Proceedings of the 23rd ACM international conference on Multimedia*.   ACM,
    2015, pp. 689–692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, and W. Burgard,
    “Multimodal deep learning for robust rgb-d object recognition,” in *Intelligent
    Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on*.   IEEE,
    2015, pp. 681–687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Y. Yang and Y. Li, “Robot learning manipulation action plans by” watching”
    unconstrained videos from the world wide web.” 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. C. Cireşan, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Deep,
    big, simple neural nets for handwritten digit recognition,” *Neural computation*,
    vol. 22, no. 12, pp. 3207–3220, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Lu, V. Behbood, P. Hao, H. Zuo, S. Xue, and G. Zhang, “Transfer learning
    using computational intelligence: a survey,” *Knowledge-Based Systems*, vol. 80,
    pp. 14–23, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. Turan, J. Shabbir, H. Araujo, E. Konukoglu, and M. Sitti, “A deep learning
    based fusion of rgb camera information and magnetic localization information for
    endoscopic capsule robots,” *International journal of intelligent robotics and
    applications*, vol. 1, no. 4, pp. 442–450, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives,
    and prospects,” *Science*, vol. 349, no. 6245, pp. 255–260, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, “Deep
    endovo: A recurrent convolutional neural network (rcnn) based visual odometry
    approach for endoscopic capsule robots,” *Neurocomputing*, vol. 275, pp. 1861–1870,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    and M. Milford, “Place recognition with convnet landmarks: Viewpoint-robust, condition-robust,
    training-free,” *Proceedings of Robotics: Science and Systems XII*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Turan, Y. Y. Pilavci, R. Jamiruddin, H. Araujo, E. Konukoglu, and M. Sitti,
    “A fully dense and globally consistent 3d map reconstruction approach for gi tract
    to enhance therapeutic relevance of the endoscopic capsule robot,” *arXiv preprint
    arXiv:1705.06524*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Turan, Y. Y. Pilavci, I. Ganiyusufoglu, H. Araujo, E. Konukoglu, and
    M. Sitti, “Sparse-then-dense alignment-based 3d map reconstruction method for
    endoscopic capsule robots,” *Machine Vision and Applications*, vol. 29, no. 2,
    pp. 345–359, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-level concept
    learning through probabilistic program induction,” *Science*, vol. 350, no. 6266,
    pp. 1332–1338, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. Marsland, “Machine learning, an algorithmic perspective, chapman &
    hall/crc machine learning & pattern recognition,” *CRC, Boca Raton, Fla*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic grasps,”
    *The International Journal of Robotics Research*, vol. 34, no. 4-5, pp. 705–724,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Ghahramani, “Probabilistic machine learning and artificial intelligence,”
    *Nature*, vol. 521, no. 7553, p. 452, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Benchmarking
    deep reinforcement learning for continuous control,” in *International Conference
    on Machine Learning*, 2016, pp. 1329–1338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning hand-eye
    coordination for robotic grasping with large-scale data collection,” in *International
    Symposium on Experimental Robotics*.   Springer, 2016, pp. 173–184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Yang, C. Fermuller, Y. Li, and Y. Aloimonos, “Grasp type revisited:
    A modern perspective on a classical feature for vision,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2015, pp. 400–408.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
    deep convolutional networks,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 38, no. 2, pp. 295–307, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Gongal, S. Amatya, M. Karkee, Q. Zhang, and K. Lewis, “Sensors and
    systems for fruit detection and localization: A review,” *Computers and Electronics
    in Agriculture*, vol. 116, pp. 8–19, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. M. Nguyen, J. Yosinski, and J. Clune, “Innovation engines: Automated
    creativity and improved stochastic optimization via deep learning,” in *Proceedings
    of the 2015 Annual Conference on Genetic and Evolutionary Computation*.   ACM,
    2015, pp. 959–966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural network for
    skeleton based action recognition,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2015, pp. 1110–1118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Tang, “Deep learning using linear support vector machines,” *arXiv
    preprint arXiv:1306.0239*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On
    the performance of convnet features for place recognition,” in *Intelligent Robots
    and Systems (IROS), 2015 IEEE/RSJ International Conference on*.   IEEE, 2015,
    pp. 4297–4304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning affordance
    for direct perception in autonomous driving,” in *Computer Vision (ICCV), 2015
    IEEE International Conference on*.   IEEE, 2015, pp. 2722–2730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Brighton and H. Selina, *Introducing Artificial Intelligence: A Graphic
    Guide*, ser. Introducing…   Icon Books Limited, 2015\. [Online]. Available: https://books.google.com.pk/books?id=4GxGCgAAQBAJ'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Bai, Y. Wu, J. Zhang, and F. Chen, “Subset based deep learning for
    rgb-d object recognition,” *Neurocomputing*, vol. 165, pp. 280–292, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] V. Veeriah, N. Zhuang, and G.-J. Qi, “Differential recurrent neural networks
    for action recognition,” in *Computer Vision (ICCV), 2015 IEEE International Conference
    on*.   IEEE, 2015, pp. 4041–4049.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] R. Xu, C. Xiong, W. Chen, and J. J. Corso, “Jointly modeling deep video
    and compositional text to bridge vision and language in a unified framework.”
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] K. Narasimhan, T. Kulkarni, and R. Barzilay, “Language understanding for
    text-based games using deep reinforcement learning,” *arXiv preprint arXiv:1506.08941*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum, “Galileo:
    Perceiving physical object properties by integrating a physics engine with deep
    learning,” in *Advances in neural information processing systems*, 2015, pp. 127–135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. Turan, E. P. Ornek, N. Ibrahimli, C. Giracoglu, Y. Almalioglu, M. F.
    Yanik, and M. Sitti, “Unsupervised odometry and depth learning for endoscopic
    capsule robots,” *arXiv preprint arXiv:1803.01047*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Turan, Y. Almalioglu, H. Araujo, T. Cemgil, and M. Sitti, “Endosensorfusion:
    Particle filtering-based multi-sensory data fusion with switching state-space
    model for endoscopic capsule robots using recurrent neural network kinematics,”
    *arXiv preprint arXiv:1709.03401*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] R. Lun and W. Zhao, “A survey of applications and human motion recognition
    with microsoft kinect,” *International Journal of Pattern Recognition and Artificial
    Intelligence*, vol. 29, no. 05, p. 1555008, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Kuen, K. M. Lim, and C. P. Lee, “Self-taught learning of a deep invariant
    representation for visual tracking via temporal slowness principle,” *Pattern
    recognition*, vol. 48, no. 10, pp. 2964–2982, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Y. Hou, H. Zhang, and S. Zhou, “Convolutional neural network-based image
    representation for visual loop closure detection,” in *Information and Automation,
    2015 IEEE International Conference on*.   IEEE, 2015, pp. 2238–2245.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Y. Qian, J. Dong, W. Wang, and T. Tan, “Deep learning for steganalysis
    via convolutional neural networks,” in *Media Watermarking, Security, and Forensics
    2015*, vol. 9409.   International Society for Optics and Photonics, 2015, p. 94090J.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultaneous deep transfer
    across domains and tasks,” in *Computer Vision (ICCV), 2015 IEEE International
    Conference on*.   IEEE, 2015, pp. 4068–4076.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] L. Pinto, D. Gandhi, Y. Han, Y.-L. Park, and A. Gupta, “The curious robot:
    Learning visual representations via physical interactions,” in *European Conference
    on Computer Vision*.   Springer, 2016, pp. 3–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, “Building
    machines that learn and think like people,” *Behavioral and Brain Sciences*, vol. 40,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] G. Chen, D. Clarke, M. Giuliani, A. Gaschler, and A. Knoll, “Combining
    unsupervised learning and discrimination for 3d action recognition,” *Signal Processing*,
    vol. 110, pp. 67–81, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Wulff and M. J. Black, “Efficient sparse-to-dense optical flow estimation
    using a learned basis and layers,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2015, pp. 120–130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J.-R. Ruiz-Sarmiento, C. Galindo, and J. Gonzalez-Jimenez, “Scene object
    recognition for mobile robots through semantic knowledge and probabilistic graphical
    models,” *Expert Systems with Applications*, vol. 42, no. 22, pp. 8805–8816, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] N. Das, E. Ohn-Bar, and M. M. Trivedi, “On performance evaluation of driver
    hand detection algorithms: Challenges, dataset, and metrics,” in *Intelligent
    Transportation Systems (ITSC), 2015 IEEE 18th International Conference on*.   IEEE,
    2015, pp. 2953–2958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] R. Salakhutdinov, “Learning deep generative models,” *Annual Review of
    Statistics and Its Application*, vol. 2, pp. 361–385, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Schmidhuber, “On learning to think: Algorithmic information theory
    for novel combinations of reinforcement learning controllers and recurrent neural
    world models,” *arXiv preprint arXiv:1511.09249*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, “A non-rigid
    map fusion-based direct slam method for endoscopic capsule robots,” *International
    journal of intelligent robotics and applications*, vol. 1, no. 4, pp. 399–409,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] T. Chen, Z. Chen, Q. Shi, and X. Huang, “Road marking detection and classification
    using machine learning algorithms,” in *Intelligent Vehicles Symposium (IV), 2015
    IEEE*.   IEEE, 2015, pp. 617–621.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Vrigkas, C. Nikou, and I. A. Kakadiaris, “A review of human activity
    recognition methods,” *Frontiers in Robotics and AI*, vol. 2, p. 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] O. K. Oyedotun and A. Khashman, “Deep learning in vision-based static
    hand gesture recognition,” *Neural Computing and Applications*, vol. 28, no. 12,
    pp. 3941–3951, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] R. K. Moore, “From talking and listening robots to intelligent communicative
    machines.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for
    deep belief nets,” *Neural computation*, vol. 18, no. 7, pp. 1527–1554, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *Robotics and Automation (ICRA), 2017 IEEE International Conference on*.   IEEE,
    2017, pp. 3357–3364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] F. Cruz, J. Twiefel, S. Magg, C. Weber, and S. Wermter, “Interactive reinforcement
    learning through speech guidance in a domestic scenario,” in *Neural Networks
    (IJCNN), 2015 International Joint Conference on*.   IEEE, 2015, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Vinciarelli, A. Esposito, E. André, F. Bonin, M. Chetouani, J. F. Cohn,
    M. Cristani, F. Fuhrmann, E. Gilmartin, Z. Hammal *et al.*, “Open challenges in
    modelling, analysis and synthesis of human behaviour in human–human and human–machine
    interactions,” *Cognitive Computation*, vol. 7, no. 4, pp. 397–413, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. Doshi, Z. Kira, and A. Wagner, “From deep learning to episodic memories:
    Creating categories of visual experiences,” in *Proceedings of the third annual
    conference on advances in cognitive systems ACS*, 2015, p. 15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for surface
    normal estimation,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2015, pp. 539–547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. Cuayáhuitl, S. Keizer, and O. Lemon, “Strategic dialogue management
    via deep reinforcement learning,” *arXiv preprint arXiv:1511.08099*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] E. Ohn-Bar and M. M. Trivedi, “Looking at humans in the age of self-driving
    and highly automated vehicles,” *IEEE Transactions on Intelligent Vehicles*, vol. 1,
    no. 1, pp. 90–104, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Wei, H. Liu, G. Yan, and F. Sun, “Robotic grasping recognition using
    multi-modal deep extreme learning machine,” *Multidimensional Systems and Signal
    Processing*, vol. 28, no. 3, pp. 817–833, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video prediction
    beyond mean square error,” *arXiv preprint arXiv:1511.05440*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 2818–2826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] M. Turan, Y. Almalioglu, E. Konukoglu, and M. Sitti, “A deep learning
    based 6 degree-of-freedom localization method for endoscopic capsule robots,”
    *arXiv preprint arXiv:1705.05435*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for multilayer
    perceptron,” *IEEE transactions on neural networks and learning systems*, vol. 27,
    no. 4, pp. 809–821, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, “A non-rigid
    map fusion-based rgb-depth slam method for endoscopic capsule robots,” *arXiv
    preprint arXiv:1705.05444*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *arXiv preprint arXiv:1606.00915*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J.-C. Chen, V. M. Patel, and R. Chellappa, “Unconstrained face verification
    using deep cnn features,” in *Applications of Computer Vision (WACV), 2016 IEEE
    Winter Conference on*.   IEEE, 2016, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The limitations of deep learning in adversarial settings,” in *Security and Privacy
    (EuroS&P), 2016 IEEE European Symposium on*.   IEEE, 2016, pp. 372–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, vol. 17,
    no. 1, pp. 1334–1373, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks
    with stochastic depth,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 646–661.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. Niekum, S. Osentoski, G. Konidaris, S. Chitta, B. Marthi, and A. G.
    Barto, “Learning grounded finite-state representations from unstructured demonstrations,”
    *The International Journal of Robotics Research*, vol. 34, no. 2, pp. 131–157,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning modular
    neural network policies for multi-task and multi-robot transfer,” in *Robotics
    and Automation (ICRA), 2017 IEEE International Conference on*.   IEEE, 2017, pp.
    2169–2176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel, “Deep
    spatial autoencoders for visuomotor learning,” in *Robotics and Automation (ICRA),
    2016 IEEE International Conference on*.   IEEE, 2016, pp. 512–519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell,
    “Sim-to-real robot learning from pixels with progressive nets,” *arXiv preprint
    arXiv:1610.04286*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Mohamed and D. J. Rezende, “Variational information maximisation for
    intrinsically motivated reinforcement learning,” in *Advances in neural information
    processing systems*, 2015, pp. 2125–2133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *Intelligent Robots and Systems (IROS),
    2015 IEEE/RSJ International Conference on*.   IEEE, 2015, pp. 922–928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving object
    detection with deep convolutional networks via bayesian optimization and structured
    prediction,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2015, pp. 249–258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Turan, Y. Almalioglu, E. P. Ornek, H. Araujo, M. F. Yanik, and M. Sitti,
    “Magnetic-visual sensor fusion-based dense 3d reconstruction and localization
    for endoscopic capsule robots,” *arXiv preprint arXiv:1803.01048*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C. Finn and S. Levine, “Deep visual foresight for planning robot motion,”
    in *Robotics and Automation (ICRA), 2017 IEEE International Conference on*.   IEEE,
    2017, pp. 2786–2793.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] V. Campos, A. Salvador, X. Giro-i Nieto, and B. Jou, “Diving deep into
    sentiment: Understanding fine-tuned cnns for visual sentiment prediction,” in
    *Proceedings of the 1st International Workshop on Affect & Sentiment in Multimedia*.   ACM,
    2015, pp. 57–62.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” in *Robotics and
    Automation (ICRA), 2017 IEEE International Conference on*.   IEEE, 2017, pp. 3389–3396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] M. Turan, Y. Almalioglu, H. Gilbert, A. E. Sari, U. Soylu, and M. Sitti,
    “Endo-vmfusenet: deep visual-magnetic sensor fusion approach for uncalibrated,
    unsynchronized and asymmetric endoscopic capsule robot localization data,” *arXiv
    preprint arXiv:1709.06041*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] J. Sanchez-Riera, K.-L. Hua, Y.-S. Hsiao, T. Lim, S. C. Hidayati, and
    W.-H. Cheng, “A comparative study of data fusion for rgb-d based visual recognition,”
    *Pattern Recognition Letters*, vol. 73, pp. 1–6, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado,
    A. Davis, J. Dean, M. Devin *et al.*, “Tensorflow: Large-scale machine learning
    on heterogeneous distributed systems,” *arXiv preprint arXiv:1603.04467*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
