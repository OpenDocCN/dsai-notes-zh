["```py\n@property\ndef model(self): return self.models.model\n```", "```py\nm=learn.model; m*EmbeddingDotBias (\n  (u): Embedding(671, 50)\n  (i): Embedding(9066, 50)\n  (ub): Embedding(671, 1)\n  (ib): Embedding(9066, 1)\n)*\n```", "```py\nmovie_bias = to_np(m.ib(V(topMovieIdx)))\n```", "```py\ndef load_model(m, p): m.load_state_dict(torch.load(p, map_location=lambda storage, loc: storage))\n```", "```py\nmovie_ratings = [(b[0], movie_names[i]) **for** i,b **in** zip(topMovies,movie_bias)]\n```", "```py\nsorted(movie_ratings, key=**lambda** o: o[0])[:15]*[(-0.96070349, 'Battlefield Earth (2000)'),\n (-0.76858485, 'Speed 2: Cruise Control (1997)'),\n (-0.73675376, 'Wild Wild West (1999)'),\n (-0.73655486, 'Anaconda (1997)'),\n ...]*sorted(movie_ratings, key=**itemgetter**(0))[:15]\n```", "```py\nsorted(movie_ratings, key=**lambda** o: o[0], reverse=**True**)[:15]*[(1.3070084, 'Shawshank Redemption, The (1994)'),\n (1.1196285, 'Godfather, The (1972)'),\n (1.0844109, 'Usual Suspects, The (1995)'),\n (0.96578616, \"Schindler's List (1993)\"),\n ...]*\n```", "```py\nmovie_emb = to_np(m.i(V(topMovieIdx)))\nmovie_emb.shape*(3000, 50)*from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nmovie_pca = pca.fit(movie_emb.T).components_\nmovie_pca.shape*(3, 3000)*\n```", "```py\nfac0 = movie_pca[0] \nmovie_comp = [(f, movie_names[i]) **for** f,i **in** zip(fac0, topMovies)]\nsorted(movie_comp, key=itemgetter(0), reverse=True)[:10]sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]*[(0.06748189, 'Independence Day (a.k.a. ID4) (1996)'),\n (0.061572548, 'Police Academy 4: Citizens on Patrol (1987)'),\n (0.061050549, 'Waterworld (1995)'),\n (0.057877172, 'Rocky V (1990)'),\n ...\n]*sorted(movie_comp, key=itemgetter(0))[:10]*[(-0.078433245, 'Godfather: Part II, The (1974)'),\n (-0.072180331, 'Fargo (1996)'),\n (-0.071351372, 'Pulp Fiction (1994)'),\n (-0.068537779, 'Goodfellas (1990)'),\n ...\n]*\n```", "```py\nfac1 = movie_pca[1]\nmovie_comp = [(f, movie_names[i]) for f,i in zip(fac1, topMovies)]\nsorted(movie_comp, key=itemgetter(0), reverse=True)[:10]*[(0.058975246, 'Bonfire of the Vanities (1990)'),\n (0.055992026, '2001: A Space Odyssey (1968)'),\n (0.054682467, 'Tank Girl (1995)'),\n (0.054429606, 'Purple Rose of Cairo, The (1985)'),\n ...]*sorted(movie_comp, key=itemgetter(0))[:10]*[(-0.1064609, 'Lord of the Rings: The Return of the King, The (2003)'),\n (-0.090635143, 'Aladdin (1992)'),\n (-0.089208141, 'Star Wars: Episode V - The Empire Strikes Back (1980)'),\n (-0.088854566, 'Star Wars: Episode IV - A New Hope (1977)'),\n ...]*\n```", "```py\nidxs = np.random.choice(len(topMovies), 50, replace=False)\nX = fac0[idxs]\nY = fac1[idxs]\nplt.figure(figsize=(15,15))\nplt.scatter(X, Y)\nfor i, x, y in zip(topMovies[idxs], X, Y):\n    plt.text(x,y,movie_names[i], color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n```", "```py\n*# Here we generate some fake data*\n**def** lin(a,b,x): **return** a*x+b\n\n**def** gen_fake_data(n, a, b):\n    x = s = np.random.uniform(0,1,n) \n    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n    **return** x, y\n\nx, y = gen_fake_data(50, 3., 8.)\n\nplt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\");\n```", "```py\n**def** **mse**(y_hat, y): **return** ((y_hat - y) ** 2).mean()**def** **mse_loss**(a, b, x, y): **return** mse(lin(a,b,x), y)\n```", "```py\nx, y = gen_fake_data(10000, 3., 8.) \nx,y = V(x),V(y)\n```", "```py\na = V(np.random.randn(1), requires_grad=**True**) \nb = V(np.random.randn(1), requires_grad=**True**)\n```", "```py\nlearning_rate = 1e-3\n**for** t **in** range(10000):\n    *# Forward pass: compute predicted y using operations on Variables*\n    loss = mse_loss(a,b,x,y)\n    **if** t % 1000 == 0: print(loss.data[0])\n\n    *# Computes the gradient of loss with respect to all Variables with requires_grad=True.*\n    *# After this call a.grad and b.grad will be Variables holding the gradient*\n    *# of the loss with respect to a and b respectively*\n    loss.backward()\n\n    *# Update a and b using gradient descent; a.data and b.data are Tensors,*\n    *# a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors*\n    a.data -= learning_rate * a.grad.data\n    b.data -= learning_rate * b.grad.data\n\n    *# Zero the gradients*\n    a.grad.data.zero_()\n    b.grad.data.zero_()\n```", "```py\nx, y = gen_fake_data(50, 3., 8.)a_guess,b_guess = -1., 1.\nmse_loss(y, a_guess, b_guess, x)lr=0.01 \n**def** **upd**():\n     **global** a_guess, b_guess\n     y_pred = lin(a_guess, b_guess, x)\n     dydb = 2 * (y_pred - y)\n     dyda = x*dydb\n     a_guess -= lr*dyda.mean()\n     b_guess -= lr*dydb.mean()\n```", "```py\ntext = open(f'{PATH}nietzsche.txt').read()\nprint(text[:400])*'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself '*chars = sorted(list(set(text))) \nvocab_size = len(chars)+1 \nprint('total chars:', vocab_size)*total chars: 85*\n```", "```py\nchars.insert(0, \"\\0\")\n```", "```py\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n```", "```py\n**idx** = [char_indices[c] for c in text]\nidx[:10]*[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]*\n```", "```py\ncs = 3 \nc1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\nc2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\nc3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\nc4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]\n```", "```py\nx1 = np.stack(c1_dat) \nx2 = np.stack(c2_dat) \nx3 = np.stack(c3_dat) \ny = np.stack(c4_dat)\n```", "```py\nn_hidden = 256 \nn_fac = 42\n```", "```py\n**class** **Char3Model**(nn.Module):\n     **def** **__init__**(self, vocab_size, n_fac):\n         super().__init__()\n                  self.e = nn.Embedding(vocab_size, n_fac)\n                  self.l_in = nn.Linear(n_fac, n_hidden)\n                   self.l_hidden = nn.Linear(n_hidden, n_hidden)\n                  self.l_out = nn.Linear(n_hidden, vocab_size) **def** **forward**(self, c1, c2, c3):\n         in1 = F.relu(self.l_in(self.e(c1)))\n         in2 = F.relu(self.l_in(self.e(c2)))\n         in3 = F.relu(self.l_in(self.e(c3)))\n\n         h = V(torch.zeros(in1.size()).cuda())\n         h = F.tanh(self.l_hidden(h+in1))\n         h = F.tanh(self.l_hidden(h+in2))\n         h = F.tanh(self.l_hidden(h+in3))\n\n         **return** F.log_softmax(self.l_out(h))\n```", "```py\nmd = ColumnarModelData.from_arrays('.', [-1], np.stack(**[x1,x2,x3]**, axis=1), y, bs=512)\n```", "```py\nm = Char3Model(vocab_size, n_fac).cuda()\n```", "```py\nit = iter(md.trn_dl)\n*xs,yt = next(it)\nt = m(*V(xs)\n```", "```py\nopt = optim.Adam(m.parameters(), 1e-2)\n```", "```py\nfit(m, md, 1, opt, F.nll_loss)\nset_lrs(opt, 0.001)\nfit(m, md, 1, opt, F.nll_loss)\n```", "```py\n**def** **get_next**(inp):\n     idxs = T(np.array([char_indices[c] **for** c **in** inp]))\n     p = m(*VV(idxs))\n     i = np.argmax(to_np(p))\n     **return** chars[i]\n```", "```py\nget_next('y. ')\n*'T'**get_next('ppl')\n'e'*get_next(' th')\n*'e'*get_next('and')\n' '\n```", "```py\ncs = 8c_in_dat = [[idx[i+j] **for** i **in** range(cs)] **for** j **in** range(len(idx)-cs)]c_out_dat = [idx[j+cs] **for** j **in** range(len(idx)-cs)]xs = np.stack(c_in_dat, axis=0)y = np.stack(c_out_dat)xs[:cs,:cs]\n*array([[40, 42, 29, 30, 25, 27, 29,  1],\n       [42, 29, 30, 25, 27, 29,  1,  1],\n       [29, 30, 25, 27, 29,  1,  1,  1],\n       [30, 25, 27, 29,  1,  1,  1, 43],\n       [25, 27, 29,  1,  1,  1, 43, 45],\n       [27, 29,  1,  1,  1, 43, 45, 40],\n       [29,  1,  1,  1, 43, 45, 40, 40],\n       [ 1,  1,  1, 43, 45, 40, 40, 39]])*y[:cs]\n*array([ 1,  1, 43, 45, 40, 40, 39, 43])*\n```", "```py\nval_idx = get_cv_idxs(len(idx)-cs-1)\nmd = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)\n```", "```py\n**class** **CharLoopModel**(nn.Module):\n    *# This is an RNN!*\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(n_fac, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        **for** c **in** cs:\n            inp = F.relu(self.l_in(self.e(c)))\n            h = F.tanh(self.l_hidden(h+inp))\n\n        **return** F.log_softmax(self.l_out(h), dim=-1)\n```", "```py\nm = CharLoopModel(vocab_size, n_fac).cuda() \nopt = optim.Adam(m.parameters(), 1e-2)\nfit(m, md, 1, opt, F.nll_loss)\nset_lrs(opt, 0.001)\nfit(m, md, 1, opt, F.nll_loss)\n```", "```py\n**class** **CharLoopConcatModel**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(**n_fac+n_hidden**, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        **for** c **in** cs:\n            inp = **torch.cat**((h, self.e(c)), 1)\n            inp = F.relu(self.l_in(inp))\n            h = F.tanh(self.l_hidden(inp))\n\n        **return** F.log_softmax(self.l_out(h), dim=-1)\n```", "```py\n**class** **CharRnn**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        **self.rnn = nn.RNN(n_fac, n_hidden)**\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        **outp,h = self.rnn(inp, h)**\n\n        **return** F.log_softmax(self.l_out(**outp[-1]**), dim=-1)\n```", "```py\nm = CharRnn(vocab_size, n_fac).cuda() \nopt = optim.Adam(m.parameters(), 1e-3)ht = V(torch.zeros(1, 512,n_hidden)) \noutp, hn = m.rnn(t, ht) \noutp.size(), hn.size()\n\n*(torch.Size([8, 512, 256]), torch.Size([1, 512, 256]))*\n```", "```py\n**def** get_next(inp):\n    idxs = T(np.array([char_indices[c] **for** c **in** inp]))\n    p = m(*VV(idxs))\n    i = np.argmax(to_np(p))\n    **return** chars[i]**def** get_next_n(inp, n):\n    res = inp\n    **for** i **in** range(n):\n        c = get_next(inp)\n        res += c\n        inp = inp[1:]+c\n    **return** resget_next_n('for thos', 40) *'for those the same the same the same the same th'*\n```", "```py\narray([[40, 42, 29, 30, 25, 27, 29,  1],\n       [42, 29, 30, 25, 27, 29,  1,  1],\n       [29, 30, 25, 27, 29,  1,  1,  1],\n       [30, 25, 27, 29,  1,  1,  1, 43],\n       [25, 27, 29,  1,  1,  1, 43, 45],\n       [27, 29,  1,  1,  1, 43, 45, 40],\n       [29,  1,  1,  1, 43, 45, 40, 40],\n       [ 1,  1,  1, 43, 45, 40, 40, 39]])\n```", "```py\nxs[:cs,:cs]array([[40, 42, 29, 30, 25, 27, 29,  1],\n       [ 1,  1, 43, 45, 40, 40, 39, 43],\n       [33, 38, 31,  2, 73, 61, 54, 73],\n       [ 2, 44, 71, 74, 73, 61,  2, 62],\n       [72,  2, 54,  2, 76, 68, 66, 54],\n       [67,  9,  9, 76, 61, 54, 73,  2],\n       [73, 61, 58, 67, 24,  2, 33, 72],\n       [ 2, 73, 61, 58, 71, 58,  2, 67]])ys[:cs,:cs]\narray([[42, 29, 30, 25, 27, 29,  1,  1],\n       [ 1, 43, 45, 40, 40, 39, 43, 33],\n       [38, 31,  2, 73, 61, 54, 73,  2],\n       [44, 71, 74, 73, 61,  2, 62, 72],\n       [ 2, 54,  2, 76, 68, 66, 54, 67],\n       [ 9,  9, 76, 61, 54, 73,  2, 73],\n       [61, 58, 67, 24,  2, 33, 72,  2],\n       [73, 61, 58, 71, 58,  2, 67, 68]])\n```", "```py\n**class** **CharSeqRnn**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h)\n        **return** F.log_softmax(self.l_out(**outp**), dim=-1)\n```", "```py\n**def** nll_loss_seq(inp, targ):\n    sl,bs,nh = inp.size()\n    targ = targ.transpose(0,1).contiguous().view(-1)\n    **return** F.nll_loss(inp.view(-1,nh), targ)\n```", "```py\nfit(m, md, 4, opt, null_loss_seq)\n```", "```py\n**class** **CharSeqRnn**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        **h = V(torch.zeros(1, bs, n_hidden))**\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h)\n        **return** F.log_softmax(self.l_out(outp), dim=-1)\n```", "```py\nm.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))\n```"]