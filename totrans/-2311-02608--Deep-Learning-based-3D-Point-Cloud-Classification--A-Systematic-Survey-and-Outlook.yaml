- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:04'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2311.02608] Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02608](https://ar5iv.labs.arxiv.org/html/2311.02608)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \WarningFilter
  prefs: []
  type: TYPE_NORMAL
- en: latexText page
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and
    Outlook'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Huang Zhang†, Changshuo Wang†, Shengwei Tian*, Baoli Lu, Liping Zhang, Xin
    Ning, Xiao Bai Huang Zhang and Shengwei Tian are with the School of Software,
    Xinjiang University, Xinjiang 830000, China (E-mail: zhhh1998@outlook.com; 357348035@qq.com).Changshuo
    Wang is with the Institute of Semiconductors, Chinese Academy of Sciences, Beijing,
    100083, China, Center of Materials Science and Optoelectronics Engineering $\&amp;$
    School of Microelectronics, Beijing Key Laboratory of Semiconductor Neural Network
    Intelligent Sensing and Computing Technology, Beijing 100083, China, University
    of Chinese Academy of Sciences, Beijing, 100049, China, and Cognitive Computing
    Technology Joint Laboratory, Wave Group, Beijing, 102208, China (E-mail: wangchangshuo@semi.ac.cn).Baoli
    Lu and Liping Zhang are with the Institute of Semiconductors, Chinese Academy
    of Sciences, Beijing, 100083, China, and Beijing Key Laboratory of Semiconductor
    Neural Network Intelligent Sensing and Computing Technology, Beijing, 100083,
    China (E-mail: lubaoli@semi.ac.cn; zliping@semi.ac.cn).Xin Ning is with the Institute
    of Semiconductors, Chinese Academy of Sciences, Beijing, 100083, China, and Cognitive
    Computing Technology Joint Laboratory, Wave Group, Beijing, 102208, China (E-mail:
    ningxin@semi.ac.cn).Xiao Bai is with the School of Computer Science and Engineering,
    Beihang University, Beijing, 100083, China (E-mail: baixiao@buaa.edu.cn).†The
    author contributed equally to this work and should be considered co-first author.*Shengwei
    Tian is the corresponding authors.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, point cloud representation has become one of the research hotspots
    in the field of computer vision, and has been widely used in many fields, such
    as autonomous driving, virtual reality, robotics, etc. Although deep learning
    techniques have achieved great success in processing regular structured 2D grid
    image data, there are still great challenges in processing irregular, unstructured
    point cloud data. Point cloud classification is the basis of point cloud analysis,
    and many deep learning-based methods have been widely used in this task. Therefore,
    the purpose of this paper is to provide researchers in this field with the latest
    research progress and future trends. First, we introduce point cloud acquisition,
    characteristics, and challenges. Second, we review 3D data representations, storage
    formats, and commonly used datasets for point cloud classification. We then summarize
    deep learning-based methods for point cloud classification and complement recent
    research work. Next, we compare and analyze the performance of the main methods.
    Finally, we discuss some challenges and future directions for point cloud classification.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent computer vision field, the processing technology of two-dimensional
    images is close to maturity [[1](#bib.bib1)] [[2](#bib.bib2)] [[3](#bib.bib3)]
    [[4](#bib.bib4)], and many researchers have shifted their research focus to three-dimensional
    scenes that are more in line with the real world. In a 3D scene, point cloud [[5](#bib.bib5)]
    plays an important role in representing the 3D scene because of its rich expression
    information. Therefore, point cloud has become a common form of data expression
    in the study of 3D vision. As technology advances, the acquisition of point cloud
    data is becoming increasingly
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5df7790fa9a982c96945ff58567ea0a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: 3D data representation'
  prefs: []
  type: TYPE_NORMAL
- en: 'intelligent and convenient, and there are many acquisition methods, such as:
    LIDAR laser detection, point cloud acquisition through 3D model calculation, point
    cloud acquisition through 3D reconstruction through 2D images [[6](#bib.bib6)],
    etc.'
  prefs: []
  type: TYPE_NORMAL
- en: As the most basic point cloud analysis task, point cloud classification has
    been widely used in many fields such as security detection [[7](#bib.bib7)] [[8](#bib.bib8)],
    target object detection [[9](#bib.bib9)] [[10](#bib.bib10)], medicine [[11](#bib.bib11)]
    [[12](#bib.bib12)], and three-dimensional reconstruction [[13](#bib.bib13)] [[14](#bib.bib14)].
    The purpose of point cloud classification is to equip each point in the point
    cloud with a marker to identify the overall or part properties of the point cloud.
    Since the component attributes of point clouds belong to the category of point
    cloud segmentation, in this paper, we mainly focus on the overall attributes of
    point clouds, namely point cloud classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook"), 3D data comes
    in a variety of representations. Currently, it is possible to convert point clouds
    into mesh, voxel, or multi-view data to learn 3D object representation through
    indirect methods, but these methods are prone to problems such as loss of 3D geometric
    information of objects or excessive memory consumption. Before PointNet, due to
    the disorder and irregularity of point cloud, deep learning technology cannot
    directly process point cloud. Early point cloud processing used hand-designed
    rules for feature extraction, and then used machine learning-based classifiers
    (such as Support Vector Machines (SVM) [[15](#bib.bib15)], AdaBoost [[16](#bib.bib16)],
    Random Forest (RF) [[17](#bib.bib17)], etc.) to predict the class label of the
    point cloud, but these Class methods have poor adaptive ability and are prone
    to noise [[18](#bib.bib18)]. Some researches solved the noise problem by synthesizing
    context information, such as Conditional Random Field (CRF) [[19](#bib.bib19)],
    Markov Random Field (MRF) [[20](#bib.bib20)], etc., which improved the classification
    performance to a certain extent. However, the feature expression ability of hand-designed
    rule extraction is limited, especially in complex scenes, the accuracy and generalization
    ability of the model cannot meet the requirements of human beings, and this method
    relies heavily on researchers with professional knowledge and experience.'
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid development of computer computing and data processing capabilities,
    the application of deep learning technology in point cloud analysis has also been
    promoted. The paper published by Charles et al. [[21](#bib.bib21)] of Stanford
    University in 2017 proposes a deep learning network, PointNet, that directly processes
    point clouds. This paper is a landmark, and methods for directly processing point
    clouds gradually dominate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Faced with the irregularity, disorder, and sparsity of 3D point clouds, point
    cloud classification is still a challenging problem. There are currently some
    reviews that analyze and summarize deep learning-based 3D point cloud classification
    methods. This paper improves on the previous work and adds new deep learning-based
    3D point cloud classification methods, such as the recently popular transformer-based
    method. Finally, the future research direction of 3D point cloud classification
    technology is prospected. The overall structure of the article is shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a29297e5096f542dc14c046bd75ee3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overall structure of the article. First, the point cloud is used
    as input, and the point cloud can be transformed by voxel or multi-view. Secondly,
    features are extracted from the original point cloud, the transformed voxels or
    multi-views. The final output is the probability value of each class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the main contributions of our work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We first give a detailed introduction to the 3D data and make a deeper
    interpretation of the point cloud for the reader’s understanding, and then give
    the datasets used for point cloud classification and their acquisition methods.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We summarize recently published research on point cloud classification
    reviews, building on which to complement state-of-the-art research methods. These
    methods are classified into four categories according to their characteristics,
    including multi-view-based, voxel-based, point-cloud-based methods, and polymorphic
    fusion-based methods. And then the point-cloud-based methods are subdivided.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We discuss the advantages and limitations of subcategories of methods
    based on their classification. This classification is more suitable for researchers
    to explore these methods on actual needs.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We give the evaluation metrics and performance comparisons of the
    methods to better demonstrate the performance of various methods on the dataset,
    and then analyze some current challenges and future trends in this field.
  prefs: []
  type: TYPE_NORMAL
- en: II 3D data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A 3D data representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various representations of 3D data [[22](#bib.bib22)], such as point
    cloud, mesh, and voxel. Here we introduce them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Point cloud: A point cloud is essentially a large collection of tiny points
    drawn in 3D space, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep
    Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(a),
    which consists of a large collection of points captured using a 3D laser scanner.
    These points can express the spatial distribution and surface characteristics
    of the target. Each point in the point cloud contains rich information, such as:
    three-dimensional coordinates (x, y, z), color information (r, g, b) and surface
    normal vector, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mesh: 3D data can also be represented by a mesh grid, which can be viewed as
    a collection of points that build local relationships between points. Triangular
    mesh, also known as triangular patch (as shown in Fig. [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook")(b)), is one of the commonly used mesh grids to describe 3D
    objects. A collection of points and edges of a slice is called a mesh.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Voxel: In 3D object representation, voxels are also an important form of 3D
    data representation, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣
    Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(c),
    voxels are good at representing non-uniformly filled regularly sampled spaces,
    therefore, voxels can effectively represent point cloud data with a lot of empty
    or evenly filled spaces. By voxelizing the point cloud data, it is beneficial
    to increase the data computing efficiency and reduce the access to random memory,
    but the voxelization of the point cloud data will inevitably bring a certain degree
    of information loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-view: A multi-view image (as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(d))
    is also a representation of point cloud data, which is derived from a single-view
    image, and is an image that renders a 3D object into multiple viewpoints at a
    specific angle. The challenges are mainly the choice of perspective and perspective
    fusion.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Point cloud data storage format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are hundreds of 3D file formats available for point clouds, and different
    scanners produce raw data in many formats. The biggest difference between point
    cloud data files is the use of ASCII and binary. Binary systems store data directly
    in binary code. Common point cloud binary formats include FLS, PCD, LAS, etc.
    Several other common file types can support both ASCII and binary formats. These
    include PLY, FBX. The E57 stores data in both binary and ASCII formats, combining
    many of the advantages of both in one file type. Below we introduce some commonly
    used point cloud data storage formats:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obj: The point cloud file in obj format is developed by Wavefront Technologies.
    It is a text file. It is a simple data format that only represents the geometry,
    normal, color, and texture information of the 3D data. This format is usually
    represented in ASCII, but there are also proprietary obj binary versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Las: The las format is mainly used to store LIDAR point cloud data, which is
    essentially a binary format file. A LAS file consists of three parts: the header
    file area (including total number of points, data range, dimension information
    of each point), variable-length record area (including coordinate system, extra
    dimensions, etc.), and point set record area(including point coordinate information,
    R, G, B information, classification information, intensity information, etc.),
    the las format takes into account the characteristics of LIDAR data, the structure
    is reasonable, and it is easy to expand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ply: The full name of ply is Polygon File Format, which is inspired by obj
    and is specially used to store 3D data. Ply uses a nominally flat list of polygons
    to represent objects. It can store information including color, transparency,
    surface normal vector, texture coordinates and data confidence, and can set different
    properties for the front and back sides of the polygon. There are two versions
    of this file, an ASCII version, and a binary version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'E57: E57 is a vendor neutral file format for point cloud storage. It can also
    be used to store image and metadata information generated by laser scanners and
    other 3D imaging systems and is a strict format using fixed-size fields and records.
    It saves data using ASCII and binary codes and provides most of the accessibility
    of ASCII and the speed of binary, and it can store 3D point cloud data, attributes,
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCD: PCD is the official designated format of Point Cloud Library. It consists
    of two parts: header file and point cloud data. It is used to describe the overall
    information of point cloud. It has two data storage types, ASCII, and binary,
    but the header file of PCD file must use ASCII. Encoding, a nice benefit of PCD
    is that it adapts well to PCL, resulting in the highest performance compared to
    PCL applications.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C 3D point cloud public datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, there are many point cloud datasets provided by industries and universities.
    The performance of different methods on these datasets reflects the reliability
    and accuracy of the methods. These datasets consist of virtual or real scenes,
    which can provide ground-truth labels for training the network. In this section,
    we will introduce some commonly used point cloud classification datasets, and
    the division of each dataset is shown in Table. [I](#S2.T1 "TABLE I ‣ II-C 3D
    point cloud public datasets ‣ II 3D data ‣ Deep Learning-based 3D Point Cloud
    Classification: A Systematic Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 'ModelNet40 [[23](#bib.bib23)]: The dataset was developed by the Vision and
    Robotics Laboratory at Princeton University. The ModelNet40 dataset consists of
    synthetic CAD objects. As the most widely used benchmark for point cloud analysis,
    ModelNet40 is popular for its diverse categories, clear shapes, and well-structured
    datasets. The dataset consists of objects of 40 categories (e.g. airplane, car,
    plant, lamp), of which 9843 are used for training and 2468 are used for testing.
    The corresponding points are uniformly sampled from the mesh surface and then
    further preprocessed by moving to the origin and scaling to a unit sphere. Download
    link: [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ModelNet-C [[24](#bib.bib24)]: The ModelNet-C set contains 185,000 different
    point clouds and was created based on the ModelNet40 validation set. This dataset
    is mainly used to benchmark damage robustness for 3D point cloud recognition,
    with 15 damage types and 5 severity levels for each damage type, such as noise,
    density, etc. Helps to understand the robustness of the model. Download link:
    [https://sites.google.com/umich.edu/modelnet40c](https://sites.google.com/umich.edu/modelnet40c)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ModelNet10 [[23](#bib.bib23)]: ModelNet10: ModelNet10 is a subset of ModelNet40,
    the dataset contains only 10 classes and it is divided into 3991 training and
    908 testing shapes. Download link: [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sydney Urban Objects [[25](#bib.bib25)]: The dataset, collected in Sydney CBD,
    contains a variety of common urban road objects, including 631 scanned objects
    in the categories of vehicles, pedestrians, signs, and trees. Download link: [https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ShapeNet [[26](#bib.bib26)]: ShapeNet is a large repository of 3D CAD models
    developed by researchers at Stanford University, Princeton University, and the
    Toyota Institute of Technology in Chicago, USA. The repository contains over 300
    million models, of which 220,000 models are classified into 3,135 classes arranged
    using WordNet hypernym-hyponymy relations. ShapeNetCore is a subset of ShapeNet
    that includes nearly 51,300 unique 3D models. It provides 55 common object categories
    and annotations. ShapeNetSem is also a subset of ShapeNet, which contains 12,000
    models. It is smaller in scale but has a wider coverage, including 270 categories.
    Download link: [https://shapenet.org/](https://shapenet.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ScanNet [[27](#bib.bib27)]: ScanNet is an instance-level indoor RGB-D dataset
    containing 2D and 3D data. It is a collection of labeled voxels, not points or
    objects. As of now, ScanNet v2, the latest version of ScanNet, has collected 1513
    annotated scans with about 90$\%$ surface coverage. In the semantic segmentation
    task, this dataset is labeled with 20 classes of annotated 3D voxelized objects.
    Download link: [http://www.scan-net.org/](http://www.scan-net.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ScanObjectNN [[28](#bib.bib28)]: ScanObjectNN is a real-world dataset consisting
    of 2902 3D objects divided into 15 categories, which is a challenging point cloud
    classification dataset due to background, missing parts, and deformations in the
    dataset. Download link: [https://hkust-vgd.github.io/scanobjectnn/](https://hkust-vgd.github.io/scanobjectnn/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Point cloud classification datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Samples | Classes | Training | Test | Type | Form |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet40[[23](#bib.bib23)] | 2015 | 12311 | 40 | 9843 | 2468 | Synthetic
    | Mesh |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet40-C[[24](#bib.bib24)] | 2015 | 185000 | 15 | - | - | Synthetic |
    Point Cloud |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet10[[23](#bib.bib23)] | 2015 | 4899 | 10 | 3991 | 605 | Synthetic
    | Mesh |'
  prefs: []
  type: TYPE_TB
- en: '| SyDney Urban Objects[[25](#bib.bib25)] | 2013 | 588 | 14 | - | - | Real Word
    | Point Cloud |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeNet[[26](#bib.bib26)] | 2015 | 51190 | 55 | - | - | Synthetic | Mesh
    |'
  prefs: []
  type: TYPE_TB
- en: '| ScanNet[[27](#bib.bib27)] | 2017 | 12283 | 17 | 9677 | 2606 | Real Word |
    RGB-D |'
  prefs: []
  type: TYPE_TB
- en: '| ScanObjectNN[[28](#bib.bib28)] | 2019 | 2902 | 15 | 2321 | 581 | Real Word
    | Point Cloud |'
  prefs: []
  type: TYPE_TB
- en: III Point cloud classification method based on deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8ce289c8692fdad4609220e38b3b1f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Timeline of the development of classification methods'
  prefs: []
  type: TYPE_NORMAL
- en: 'The point cloud classification model based on deep learning [[29](#bib.bib29)]
    has been widely used in point cloud analysis due to its advantages of strong generalization
    ability and high classification accuracy. This section provides a detailed division
    of deep learning-based point cloud classification methods and supplements some
    recent research works. Fig. [3](#S3.F3 "Figure 3 ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook") shows the publication timeline of each classification
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Multi-view-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-view learning is a machine learning framework in which data is represented
    by multiple distinct feature groups, each of which is called a specific view.
    The multi-view-based method is a deep learning based on 2D images. This method
    is divided into three steps: First, project the 3D image into multiple views.
    Second, extract the view function. Third, fuse these functions to accurately classify
    3D shapes. In 2015, Su et al. [[30](#bib.bib30)] first proposed the multi-view
    convolutional neural network MVCNN method. Since the collection of 2D views can
    provide a lot of information for 3D shape recognition, this method recognizes
    3D shapes from the collection of rendering views on 2D images. Representing 3D
    Shapes” is a longstanding problem. This method first demonstrates a standard CNN
    architecture trained to recognize shapes independently of rendered views and shows
    that 3D shapes can be recognized even from a single view. When multiple views
    of an object are provided, the recognition rate is further improved by using the
    CNN architecture to combine the information of multiple views into a compact shape
    descriptor. The network architecture is shown in Fig. [4](#S3.F4 "Figure 4 ‣ III-A
    Multi-view-based methods ‣ III Point cloud classification method based on deep
    learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic Survey
    and Outlook"). This method requires a large amount of computation in the projection
    retrieval process, and when converting multiple view features into global features
    through max-pooling, other non-maximum element information is ignored, so it will
    inevitably cause information missing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1307fc58883875de044416d77d177ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Schematic diagram of MVCNN[[30](#bib.bib30)] structure'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in view of the large amount of computation and efficiency of MVCNN,
    Bai et al. [[31](#bib.bib31)] proposed a real-time 3D shape search engine, namely
    GIFT, this method uses GPU acceleration in the stages of projection and view feature
    extraction, which greatly shortens the time spent on retrieval tasks, and has
    high efficiency and ability to handle large-scale data.
  prefs: []
  type: TYPE_NORMAL
- en: Based on MVCNN, the GVCNN architecture proposed by Feng et al. [[32](#bib.bib32)]
    introduces the ”view-group-shape” architecture to extract descriptors, which can
    effectively utilize the feature relationship between views. In order to overcome
    the problem of information loss due to max pooling, Wang et al. [[33](#bib.bib33)]
    introduced the view clustering and pooling layer based on the dominant set to
    improve the MVCNN method and proposed RCPCNN. It entails building a view similarity
    graph, clustering nodes (views) in this graph based on dominance sets, and pooling
    information from within each cluster. The recurrent clustering and pooling layer
    are to aggregate multi-view features in a way that provides more discriminative
    power for 3D object recognition. This recurrent clustering and pooling convolutional
    neural network (RCPCNN) module is plugged into ready-made pre-trained CNN which
    improves the performance of multi-view 3D object recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MHBN method proposed by Yu et al. [[34](#bib.bib34)] aggregates local convolutional
    features by bilinear pooling to obtain efficient 3D object representations. Fig. [5](#S3.F5
    "Figure 5 ‣ III-A Multi-view-based methods ‣ III Point cloud classification method
    based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A
    Systematic Survey and Outlook") shows the network architecture of this method.
    To achieve an end-to-end trainable framework, this method coordinates the merging
    of bilinear pooling into one layer in the network, from the perspective of patch-to-patch
    similarity measurement, to address the problem of view-based methods pooling view
    features that differ.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3583d25173a15d2690a3c0a35592978.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Architecture of MHBN[[34](#bib.bib34)]'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the lack of dynamism of current multi-view methods, Hamdi et al. [[35](#bib.bib35)]
    propose the Multi-View Transition Network (MVTN), which proposes a differentiable
    module that predicts the best viewpoint for a task-specific multi-view network,
    regressing the best for 3D shape recognition Viewpoint. MVTN can be trained end-to-end
    with any multi-view network, showing significant performance gains in 3D shape
    classification and retrieval tasks without additional training supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: Compared with the traditional manual extraction feature classification,
    the multi-view-based method has a better effect in point cloud classification,
    but it is still difficult to make full use of information. The application of
    large-scale scenes, and the inherent geometric relationship of 3D data is the
    challenge we need to face.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Voxel-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of approach converts a 3D point cloud model into voxel form that approximates
    the shape of an object, each voxel block contains a set of associated points,
    and then uses 3D CNNs to classify the voxels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Maturana et al. [[36](#bib.bib36)] proposed a convolutional neural network
    architecture called VoxNet to represent 3D information with a volumetric occupancy
    grid. VoxNet is the earliest voxel-based 3DCNN model. As shown in Fig. [6](#S3.F6
    "Figure 6 ‣ III-B Voxel-based methods ‣ III Point cloud classification method
    based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A
    Systematic Survey and Outlook"), this method normalizes each grid, then constructs
    a feature map through convolution and max pooling a single voxel block. This architecture
    uses 2.5D to represent the features of the local description scan, and adopts
    a full volume representation, which improves the ability to express environmental
    information and enables powerful 3D object recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5cf1ed3d9a8a66cc9ee597538067adf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Architecture of VoxNet[[36](#bib.bib36)]'
  prefs: []
  type: TYPE_NORMAL
- en: Wu et al. [[23](#bib.bib23)] proposed 3D ShapeNets to recognize 3D objects.
    The model represents a 3D shape as a probability distribution of binary variables
    on a grid of 3D voxels, each of which can be represented by a binary tensor, and
    predicts the next best view in the presence of uncertain initial recognition.
    Finally, 3D ShapeNets can incorporate new views to identify objects in common
    with all views.
  prefs: []
  type: TYPE_NORMAL
- en: Since 3D convolutions are computationally expensive, the spatial resolution
    increases model complexity when convolving 3D voxels. In response to this phenomenon,
    Li et al. [[37](#bib.bib37)] proposed a field detection neural network (FPNN),
    which represented 3D data as a 3D field, then sampled the input field through
    a set of detection filters, and finally used the field detection filter to characterize
    extract. The field detection layer can be used with other inference layers, but
    this approach makes the semantic segmentation results lower resolution.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce memory consumption and improve computational efficiency, some scholars
    use octree structure instead of fixed-resolution voxel structure. Riegler et al.
    [[38](#bib.bib38)] proposed OctNet, which exploits the sparsity of 3D data to
    hierarchically partition the space with a set of unbalanced octrees, where each
    leaf node stores a pooled feature representation. This method enables deeper networks
    without affecting their resolution. Wang et al. [[39](#bib.bib39)] proposed an
    octree-based convolutional neural network O-CNN, which uses octrees to represent
    3D data information and discretize its surface. The 3D CNN operation is only performed
    on the octant occupied by the 3D shape surface, which improves the computational
    efficiency and power.
  prefs: []
  type: TYPE_NORMAL
- en: Following the use of the octree structure in the 3D data representation, the
    Kd-tree structure is also used in the point cloud classification model. The Kd-network
    proposed by Klokov et al. [[40](#bib.bib40)] adopts, compared with voxel and mesh,
    the ability of Kd-tree to index and structure 3D data has been improved, so Kd-network
    has a smaller memory footprint and more efficient computation during training
    and testing. The 3D Context Net proposed by Zeng et al. [[41](#bib.bib41)] utilizes
    the method of local and global context cues imposed by Kd-tree for semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Octree structure and Kd-tree structure reduce memory consumption to a certain
    extent and improve computational efficiency, but due to the influence of voxel
    boundary value, these two structures cannot make full use of local data features
    and the accuracy is affected . Wang et al. [[42](#bib.bib42)] proposed a multi-scale
    convolutional network, MSNet. This method first divides the space into voxels
    of different scales, then applies MSNet on multiple scales simultaneously to learn
    local features, and finally uses a conditional random field (CRF) The prediction
    results of MSNet are globally optimized to achieve a more accurate point cloud
    classification task.
  prefs: []
  type: TYPE_NORMAL
- en: The VV-Net proposed by Meng et al. [[43](#bib.bib43)] uses a kernel-based interpolating
    variational autoencoder (VAE) to encode localities in voxels, each voxel is further
    subdivided into sub-voxels, which are within voxels interpolate sparse point samples
    to efficiently handle noisy point cloud datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: Compared with the multi-view method, the voxel-based method pays attention
    to the relationship between 3D data, and groups the point clouds with internal
    connections into a set of points, thereby establishing voxels. Although the voxel-based
    model solves the point cloud disorder and unstructured problem, the sparseness
    and incomplete information of point cloud data lead to the low efficiency of the
    classification task, so the information in the point cloud cannot be fully utilized.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Point cloud-based method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many current research methods focus more on directly processing point clouds
    using deep learning techniques. Feature aggregation operator is the core of processing
    point cloud, which realizes the information transfer of discrete points. Feature
    aggregation operators are mainly divided into two categories: local feature aggregation
    and global feature aggregation. In this section, from the perspective of feature
    aggregation, the two categories of methods are divided and introduced in more
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2017, the PointNet proposed by Qi et al. [[21](#bib.bib21)] (shown in Fig. [7](#S3.F7
    "Figure 7 ‣ III-C Point cloud-based method ‣ III Point cloud classification method
    based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A
    Systematic Survey and Outlook")) is a pioneering study of point cloud-based methods,
    which is a method of global feature aggregation. This method directly takes the
    point cloud as input, transforms it through the T-Net module, then learns each
    point by sharing the full connection, and finally aggregates the features of the
    point into global features through the maximum pooling function. Although PointNet
    is a pioneer based on deep learning, it still has defects. For example, PointNet
    only captures the feature information of a single point and global points but
    does not consider the relational representation of adjacent points, which makes
    PointNet unable to effectively perform fine-grained classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25c416b5884c5c1eaf4742c7ad7a0a31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: PointNet[[21](#bib.bib21)] network architecture'
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Local feature aggregation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Point-by-point method
  prefs: []
  type: TYPE_NORMAL
- en: 'Qi et al.[[44](#bib.bib44)] successively proposed PointNet++ based on PointNet.
    This method processes point clouds in a hierarchical manner, and each layer consists
    of a sampling layer, a grouping layer, and a PointNet layer. Among them, the sampling
    layer obtains the centroid of the local neighborhood, the grouping layer constructs
    a subset of the local neighborhood, and the PointNet layer obtains the relationship
    between the points in the local area, as shown in Fig. [8](#S3.F8 "Figure 8 ‣
    1 ‣ III-C1 Local feature aggregation ‣ III-C Point cloud-based method ‣ III Point
    cloud classification method based on deep learning ‣ Deep Learning-based 3D Point
    Cloud Classification: A Systematic Survey and Outlook"). PointNet++ needs to solve
    two problems: dividing the generated point set and aggregating local features
    through a local feature learner. But PointNet++ still does not ignore the prior
    relationship between points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/193e3315911eef939f544b39bff19dae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: PointNet++[[44](#bib.bib44)] point cloud segmentation and classification
    architecture diagram'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on PointNet++, Qian et al.[[45](#bib.bib45)] improved the training and
    training strategy to improve the performance of PointNet++ and introduced a separable
    MLP and an inverted residual bottleneck design in the PointNet++ framework, and
    named its framework PointNeXt (as shown in Fig. [9](#S3.F9 "Figure 9 ‣ 1 ‣ III-C1
    Local feature aggregation ‣ III-C Point cloud-based method ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0dec9d202da08e03c845fd5fc8400925.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: PointNeXt[[45](#bib.bib45)] architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al.[[46](#bib.bib46)] proposed a method of extracting features from
    the local neighborhood of point clouds - PointWeb, which connects the points in
    the local neighborhood with each other, and proposes a new module, Adaptive Feature
    Adjustment (AFA), which is used to find information transfer between points. This
    method makes full use of the local features of points and realizes adaptive adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: Hu et al. [[47](#bib.bib47)] proposed an efficient and lightweight neural architecture
    - RandLA-Net, which uses random point sampling, and increases the receptive field
    of each point through an efficient local feature aggregation module, so as to
    better capturing complex local structures reduces memory footprint and computational
    cost, but this approach may discard some key features of sparse points.
  prefs: []
  type: TYPE_NORMAL
- en: The SO-Net proposed by Li et al. [[48](#bib.bib48)] constructs a self-organizing
    map (SOM), extracts the features of each point and SOM node hierarchically, and
    uses a single feature vector to represent the point cloud, by appending a 3-layer
    perception to the encoded global feature vector Multi-Layer Perceptron(MLP) is
    used to classify point clouds. SO-Net has good parallelism and simple structure,
    but it has limitations in processing large-scale point cloud data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to fully capture the most critical geometric information, Xu et al.
    [[49](#bib.bib49)] proposed the geometric disentanglement attention network GDANet,
    and introduced the Geometry-Disentangle module to decompose the original point
    cloud into two parts: contour and plane, so as to capture and refine 3D semantics
    to supplementing local information. The method has good robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goyal et al. [[50](#bib.bib50)] proposed a projection-based method, SimpleView,
    which showed that training and evaluation factors independent of the network architecture
    have a large impact on point cloud classification performance. The PointSCNet
    proposed by Chen et al. [[51](#bib.bib51)] is used to capture the geometric information
    and local area information of the point cloud. It consists of three modules: a
    space-filling curve-guided sampling module, an information fusion module, and
    a channel spatial attention module. In PointSCNet the raw point cloud is fed to
    the sampling and grouping block, which is sampled using a Z-order curve to obtain
    the high correlation between points and local regions. After extracting the sampled
    point cloud features, a feature fusion module is designed to learn the structure
    and related information. Finally, the keypoint features are enhanced by the channel
    space module.'
  prefs: []
  type: TYPE_NORMAL
- en: Ma et al. [[52](#bib.bib52)] noticed that detailed local geometric information
    may not be the key to analyzing point clouds, so they introduced a pure residual
    network called PointMLP, which does not have a complex local geometry extractor,
    but is equipped with a lightweight geometric affine module, there is a significant
    improvement in inference speed. Huang et al. [[53](#bib.bib53)] calculated the
    rate of change of recognition confidence when a shape-invariant perturbation with
    explicit constraints was added to each point in the point cloud, and according
    to this method, they proposed a point cloud sensitivity map, which was then used
    to propose a shape-invariant adversarial point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ran et al. [[54](#bib.bib54)] proposed using RepSurf (representative surfaces)
    to represent point clouds, which has two variants: Triangular RepSurf and Umbrella
    RepSurf. This representation can be used as a module in the point cloud classification
    and segmentation framework to improve the performance of the point cloud framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Convolution-based methods
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network (CNN) plays an important role in deep learning
    and is the most basic deep learning model. Its excellent performance in the field
    of 2D image processing has led researchers to apply it to 3D point clouds and
    design point convolution for point cloud classification.
  prefs: []
  type: TYPE_NORMAL
- en: Atzmon et al. [[55](#bib.bib55)] proposed the point convolutional neural network
    (PCNN), which applied convolutional Neural Network (CNN) to point cloud. First,
    the function on the point cloud is extended to a continuous volume function in
    space; then a continuous volume convolution is applied to the function; the final
    result is a constrained point cloud, as shown in Figure 13.
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[56](#bib.bib56)] proposed Relational Shape Convolutional Networks
    (RS-CNN) to extend regular CNNs to irregular point clouds for analysis of point
    clouds. Yousefhussien et al. [[57](#bib.bib57)] proposed an one-dimensional fully
    convolutional network. Wang et al. [[58](#bib.bib58)] proposed a deep neural network
    DNNSP with spatial pooling to classify large-scale point clouds. This method can
    learn the features from the entire region to the center point of the point cluster,
    to achieve a robust representation of point features. Komarichev et al. [[59](#bib.bib59)]
    propose a point cloud-based annular convolutional neural network (A-CNN) model.
    Ran et al. [[60](#bib.bib60)] proposed RPNet based on a group relation aggregation
    module, which is robust to rigid transformations and noise. The SCN (ShapeContextNet)
    proposed by Xie et al. [[61](#bib.bib61)] is represented by using the shape context
    as a building block, so that it can capture and propagate object part information.
    SCN is an end-to-end model.
  prefs: []
  type: TYPE_NORMAL
- en: Since directly convolving kernels with point-related features would result in
    the discarding of shape information and variance in point ordering, Li et al.
    [[62](#bib.bib62)] proposed PointCNN to address this problem, which confirmed
    the development of local structures for point cloud classification networks importance.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the sparsity, irregularity, and disorder of the point cloud, it is difficult
    to directly perform the convolution operation on it. Wu et al. [[63](#bib.bib63)]
    proposed to apply the dynamic filter to the convolution operation called PointConv,
    which is simple and reduces the computer storage pressure. MCCNN [[64](#bib.bib64)]
    represents the convolution kernel itself as a multilayer perceptron and describes
    the convolution as a Monte Carlo integration. SpiderCNN [[65](#bib.bib65)] inherits
    the multi-scale hierarchical structure of CNN and consists of SpiderConv units,
    which extend the convolution operation from regular grids to irregular point sets
    that can be embedded in n-dimensional space by parameterizing a series of convolution
    filters to effectively extract geometric features from point clouds. Mao et al.
    [[66](#bib.bib66)] designed interpolating convolutional neural networks (InterpCNNs)
    based on combining InterConv(interpolating convolution operation).
  prefs: []
  type: TYPE_NORMAL
- en: Esteves et al. [[67](#bib.bib67)] use multi-valued spherical functions to model
    3D data, and propose a spherical convolutional network that implements them by
    implementing precise convolutions on spheres in the spherical harmonic domain,
    thus solving the 3D rotation variance problem in convolutional neural networks.
    SPHNet [[68](#bib.bib68)] is based on PCNN to achieve rotation invariance by using
    spherical harmonics in different layers of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Since the local features of point clouds are difficult to aggregate and transfer
    effectively, Wang et al.[[69](#bib.bib69)] proposed a spatial coverage convolutional
    neural network (SC-CNN), the core of which is the spatial coverage convolution
    (SC-Conv). Anisotropic spatial geometry is constructed in the point cloud to implement
    depthwise separable convolution, replacing depthwise convolution with the spatial
    coverage operator (SCOP).
  prefs: []
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Graph-based methods
  prefs: []
  type: TYPE_NORMAL
- en: The graph neural network (GNN) was first proposed by Scarselli et al. [[70](#bib.bib70)].
    Bruna et al. [[71](#bib.bib71)] are the first to apply convolution to low-dimensional
    graph structures to effectively represent depth. Kipf et al. [[72](#bib.bib72)]
    further proposed that a graph convolutional network (GCN) works well in semi-supervised
    classification tasks, and in fact, GCN is an optimization of CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Simonovsky [[73](#bib.bib73)] proposed an ECC(edge conditional convolutional)
    network that can be applied to any graph structure in combination with the application
    of edge labels. This method uses the points as the vertices of the graph and the
    distance between the points as the weight, and performs a weighted average convolution
    operation, using the maximum sampling the information of aggregated vertices.
    It can be used for large-scale point cloud segmentation, but the amount of computation
    is large.
  prefs: []
  type: TYPE_NORMAL
- en: SpecGCN [[74](#bib.bib74)] replaces the standard max-pooling step with a recursive
    clustering and pooling strategy. Grid-GCN [[75](#bib.bib75)] uses a coverage-aware
    network query (CAGQ), which improves spatial coverage and reduces theoretical
    time complexity by exploiting the efficiency of grid space.
  prefs: []
  type: TYPE_NORMAL
- en: Mohammadi et al. [[76](#bib.bib76)] proposed PointView-GCN with a multi-level
    graph convolutional network (GCN) to hierarchically aggregate shape features of
    single-view point clouds, which facilitates encoding of object geometric cues
    and multi-view relationships, resulting in more specific global features.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. [[77](#bib.bib77)] proposed a dynamic graph CNN (DGCNN) for point
    cloud learning and proposed an edge convolutional (EdgeConv) network module, which
    can better capture the local geometric features of point clouds and maintain arrangement
    invariance, which demonstrates the importance of local geometric features for
    3D recognition tasks. Zhang et al. [[78](#bib.bib78)] further optimized DGCNN
    and proposed a Linked Dynamic Graph Convolutional Neural Network (LDGCNN), which
    removed the transformation network in DGCNN to simplify the network model and
    optimized the network by connecting hierarchical features of different dynamic
    graphs, which can better solve the gradient vanishing problem.
  prefs: []
  type: TYPE_NORMAL
- en: The PointNGCNN proposed by Lu et al. [[79](#bib.bib79)] describes the relationship
    between points in the neighborhood in a neighborhood graph and uses neighborhood
    graph filters to extract neighborhood feature information and spatial distribution
    information in feature space and Cartesian space.
  prefs: []
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Attention-based methods
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea of the attention mechanism is to apply human perception to the
    machine, but human perception selectively focuses on part of the scene instead
    of processing the entire scene at a time, so researchers focus on the attention
    mechanism to carry out research and apply to the field of point cloud classification.
  prefs: []
  type: TYPE_NORMAL
- en: Yang et al. [[80](#bib.bib80)] developed a point attention transformer (PAT)
    based on point cloud reasoning. It is proposed to use efficient GSA (Group-Shuffle
    Attention) instead of expensive MHA (Multi-Head Attention) for modeling the relationship
    between points, and propose a method called Gumbel Subset Sampling (GSS) to select
    a subset of representative points, which reduces the computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Li et al. [[81](#bib.bib81)] proposed Feature Pyramid Attention Module (FPA)
    and Global Attention Upsampling Module (GAU) by combining attention mechanism
    and spatial pyramid. Chen et al. [[82](#bib.bib82)] designed a Local Spatial Awareness
    (LSA) layer and proposed an LSANet network architecture based on the LSA layer.
    LSA can learn the spatial relationship layer in the local area to generate spatially
    distributed weights, so that spatially independent operations can be performed.
    The spatial information extraction function of this method is powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. [[83](#bib.bib83)] proposed GACNet based on graph attention convolution
    (GAC). The GAPointNet proposed by Chen et al. [[84](#bib.bib84)] combines the
    self-attention mechanism with graph convolution, learns local information representation
    by embedding a graph attention mechanism in stacked MLP layers, and uses a parallel
    mechanism to aggregate the attention features of different GAPLayer layers, where
    GAPLayer layers and attention layers can be embedded into existing trained models
    to better extract local contextual features from unordered point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Global feature aggregation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Transformer-based methods
  prefs: []
  type: TYPE_NORMAL
- en: Since the transformer was first proposed in 2017 [[85](#bib.bib85)], it has
    achieved world-renowned results in the field of computer vision. Many researchers
    also use this structure in point cloud processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Engel et al. [[86](#bib.bib86)] propose a deep neural network Point Transformer
    that operates directly on unordered and unstructured point sets, and propose a
    learning score-based focus module, ScorNet, as part of the Point Transformer.
    The point cloud is used as the input of the Point Transformer, and local and global
    features are extracted from it, and then SortNet is used to rank the local features,
    and finally the local global attention is used to associate the local global features,
    as shown in Fig. [10](#S3.F10 "Figure 10 ‣ 1 ‣ III-C2 Global feature aggregation
    ‣ III-C Point cloud-based method ‣ III Point cloud classification method based
    on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51e36a9434fbcbf90a5e5ccfaee1bb87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Architecture of Point Transformer[[86](#bib.bib86)]'
  prefs: []
  type: TYPE_NORMAL
- en: Berg et al. [[87](#bib.bib87)] found that the self-attention operator grows
    rapidly and inefficiently with the growth of the input point set, and the attention
    mechanism is difficult to find the relationship between each point in the global,
    so they propose a two-stage method - Point TnT, this method enables a single point
    and a point set to pay attention to each other effectively.
  prefs: []
  type: TYPE_NORMAL
- en: The Visual Transformer (VT) proposed by Wu et al. [[88](#bib.bib88)], which
    applies Transformer to label-based images from feature maps, can learn and associate
    high-level concepts of sparse distributions more efficiently. Carion et al. [[89](#bib.bib89)]
    propose a method to treat object detection as a direct ensemble prediction problem
    called Detection Transformer (DETR), which is an end-to-end detection transformer
    that takes CNN features as input and uses a Transformer encoder-decoder to generate
    boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Guo et al. [[90](#bib.bib90)] proposed a Transformer-based point cloud learning
    framework - Point Cloud Transformer (PCT), and proposed offset attention with
    implicit Laplacian operator and normalization refinement, the framework has permutation
    invariance and is more suitable for point cloud learning
  prefs: []
  type: TYPE_NORMAL
- en: 3DMedPT is a Transformer network proposed by Yu et al. [[12](#bib.bib12)] for
    3D medical point cloud analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by BERT, Yu et al. [[91](#bib.bib91)] proposed a new method for learning
    Transformer called Point-BERT. This method first divides the point cloud into
    several local blocks and generates discrete point labels of local information
    through a point cloud marker, and then by randomly masking some input point clouds
    and feeding them into the backbone Transformer, this method can generalize the
    concept of BERT to point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: Pang et al. [[92](#bib.bib92)] proposed Point-MAE, which is a masked autoencoder
    method for point cloud self-supervised learning to solve the problems caused by
    point cloud location information leakage and uneven information density. question.
  prefs: []
  type: TYPE_NORMAL
- en: He et al. [[93](#bib.bib93)] introduced a voxel-based set attention module (VSA)
    to establish the Voxel Set Transformer (VoxSeT) architecture. VoxSeT can manage
    point clusters through the VSA module and process them in parallel with linear
    complexity. This method combines the high performance of Transformer with the
    high efficiency of voxel-based model, it has good performance in point cloud modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Global module-based methods
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. [[94](#bib.bib94)] propose a global module that computes the response
    at a location as a weighted sum of all location features, providing a solution
    to aggregating global features, but the global point-to-point mapping may still
    be insufficient to extract the underlying patterns implied by the point cloud
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yan et al. [[95](#bib.bib95)] propose an end-to-end network PointASNL that
    combines an adaptive sampling module (AS) and a local non-local module (L-NL),
    which can effectively deal with noisy point clouds. The AS module updates the
    features of the points by reasoning, then normalizes the weight parameters and
    re-weights the initial sampling points, which can effectively alleviate the bias
    effect. The L-NL module consists of local and non-local units of points, reducing
    the sensitivity of the learning process to noise. The PointASNL architecture is
    shown in Fig. [11](#S3.F11 "Figure 11 ‣ 2 ‣ III-C2 Global feature aggregation
    ‣ III-C Point cloud-based method ‣ III Point cloud classification method based
    on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5771018cc87042b3fc8298277adc886.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Architecture of PointASNL[[95](#bib.bib95)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Li et al. [[96](#bib.bib96)] adopted some CNN methods to support a deep GCN
    architecture, called DeepGCN, which consists of three blocks: GCN Backbone block
    for input point cloud feature transformation, fusion block for generating and
    fusing global features, MLP block prediction block used to predict labels. To
    solve the problem of gradient disappearance during GCN training, it is possible
    to train deeper GCN networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Xiang et al. [[97](#bib.bib97)] proposed a method based on aggregating hypothetical
    curves in point clouds, CurveNet, and effectively implemented the aggregation
    strategy, including a curve grouping operator and a curve aggregation operator.
    The network consists of a bunch of building blocks, FPS represents the farthest
    point sampling method.
  prefs: []
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: RNN or LSTM-based methods
  prefs: []
  type: TYPE_NORMAL
- en: RNN (Recurrent Neural Network) can usually effectively utilize context information
    to process sequence data. LSTM (Long Short-term Memory) is a special RNN, which
    can effectively solve the problems of gradient disappearance and gradient explosion
    in the training process of longer sequence data.
  prefs: []
  type: TYPE_NORMAL
- en: Engelmann et al. [[98](#bib.bib98)] extended the input-level context information
    and output-level context information based on PointNet, which enables PointNet
    to be applied in large-scale scenarios. It can make PointNet applied in large-scale
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[99](#bib.bib99)] proposed a 3DCNN-DQN-RNN method, which fuses 3D
    Convolutional Neural Network (CNN), Deep Q Network (DQN) and residual Recurrent
    Neural Network (RNN). First, the feature representation of points is obtained
    by 3DCNN. Second, DQN can detect and localize objects, and can automatically perceive
    the scene and adjust the feedback of 3DCNN. Finally, the RNN is used to identify
    the connections and differences of multi-scale features. Where LSTM (Long Short-term
    Memory) units are used to prevent vanishing gradients and make the network have
    long-term memory, the method improves the accuracy of processing large-scale point
    clouds.
  prefs: []
  type: TYPE_NORMAL
- en: The RSNet network proposed by Huang et al. [[100](#bib.bib100)] takes the original
    point cloud as input, and then performs feature extraction, and then passes through
    the slice pooling layers in the three directions of x, y, and z. Each layer uses
    a bidirectional RNN to extract local features, and then uses slice parsing. The
    layer assigns the features of the point cloud sequence to each point, and finally
    outputs the predicted semantic label of each point.
  prefs: []
  type: TYPE_NORMAL
- en: Ye et al. [[101](#bib.bib101)] proposed an end-to-end semantic segmentation
    method, 3P-RNN, by combining CNN and RNN, which consists of a point pyramid module
    and a bidirectional hierarchical RNN module. In the work of distinguishing the
    same semantics, this method has certain limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Point2Sequence proposed by Liu et al. [[102](#bib.bib102)] uses RNN to capture
    fine-grained contextual information to learn 3D shape features, it introduces
    an attention mechanism to enhance feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Polymorphic Fusion-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The strategy of combining PointGrid with grids proposed by Le et al. [[103](#bib.bib103)]
    is to mix points and grids for representation. PointGrid is composed of several
    convolution blocks, which represent the features of different layers through maximum
    pooling. Each convolution layer includes a convolution kernel, and the over-fitting
    phenomenon is controlled by the pooling layer, and then completed by full connection.
    For inference, PointGrid has two fully connected layers, and finally performs
    regression with a softmax classifier, which can better identify fine-grained models
    and represent local shapes.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. [[104](#bib.bib104)] proposed a novel point cloud learning method,
    PVT (Point-Voxel Transformer), combining sparse window attention module (SWA)
    and relative attention module (RA), which combines voxel-based and point-based
    model idea, this method excels in the accuracy of point cloud classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'PointCLIP proposed by Zhang et al. [[105](#bib.bib105)] learns point clouds
    based on pre-trained CLIP. Encoding the point cloud by projecting it into a multi-view
    depth map without rendering, enables zero-shot recognition by transferring the
    2D pretrained knowledge to the 3D domain. And an inter-view adapter is designed
    to better extract global features. The network architecture is shown in Fig. [12](#S3.F12
    "Figure 12 ‣ III-D Polymorphic Fusion-based methods ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad1ff5843a6af6faf59cc24a3934f7a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: PointCLIP[[105](#bib.bib105)] architecture'
  prefs: []
  type: TYPE_NORMAL
- en: CrossPoint [[106](#bib.bib106)] achieves 2D-to-3D correspondence by maximizing
    the point cloud and the corresponding rendered 2D image in invariant space and
    keeping the point cloud unchanged across transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: Compared with the multi-view-based method and the voxel-based method,
    the point cloud-based method directly processes the original points and can make
    full use of the point cloud information. Therefore, the point cloud-based method
    is also a future research direction, and the transformer-based method will be
    more widely used in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evaluation index is used to evaluate the performance of the point cloud
    classification method. The accuracy, space complexity, execution time, etc. are
    the evaluation indexes of the method, and the accuracy is the key index for evaluating
    various methods. Generally, accuracy (Acc), precision (Pre), recall (Rec), and
    intersection-over-union (IoU) are used to evaluate the accuracy of the method.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Accuracy refers to the ratio of the number of correctly predicted
    samples to the total number of predicted samples.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ The accuracy rate refers to the proportion of the true positive class
    that is predicted as a positive class.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Recall refers to the ratio of samples predicted to be positive classes
    to the total number of true positive classes.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ The intersection ratio refers to the ratio of the intersection and
    union of the predicted value and the true value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy, precision, recall, and intersection ratio can be calculated by
    the following formulas: TP is the sample that is predicted to be a positive class
    and is actually a positive class, TN is a sample that is predicted to be a positive
    class and is actually a negative class, and FP is the sample. The samples that
    are predicted to be negative classes are actually negative classes, and FN is
    the samples that are predicted to be negative classes and are actually positive
    classes. Suppose there are N classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy of the i-th class:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Acc_{i}=\frac{TP_{i}+TN_{i}}{TP_{i}+TN_{i}+FP_{i}+FN_{i}}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'The precision of the i-th class:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{Pre}_{i}=\frac{TP_{i}}{TP_{i}+FP_{i}}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Rec=\frac{TP}{TP+FN}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'The intersection ratio of the i-th class:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $IoU_{i}=\frac{TP_{i}}{TP_{i}+FP_{i}+FN_{i}}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'The current accuracy of point cloud classification is measured by indicators:
    overall accuracy (OA), average accuracy (MA), and average intersection-over-union
    ratio (mIoU), which are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $OA=\frac{\sum_{i=1}^{N}TP_{i}}{\sum_{i=1}^{N}\left(TP_{i}+FP_{i}\right)}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $MA=\frac{1}{N}\sum_{i=1}^{N}Acc_{i}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $mIoU=\frac{1}{N}\sum_{i=1}^{N}IoU$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'This section summarizes the performance of the mentioned methods on different
    datasets in Table. [II](#S4.T2 "TABLE II ‣ IV Evaluation ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Accuracy comparison of point cloud classification methods on different
    datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Year | Accuracy/% on different datasets |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet 40 | ModelNet 10 | ScanObjectNN | ShapeNet |'
  prefs: []
  type: TYPE_TB
- en: '| OA | MA | mIoU | OA | MA | mIoU | OA | MA | mIoU | OA | MA | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-view based methods | MVCNN[[30](#bib.bib30)] | 2016 | 90.10 | 78.90
    | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT[[31](#bib.bib31)] | 2016 | 83.10 | 81.94 | - | 92.35 | 91.12 | - | -
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GCVNN[[32](#bib.bib32)] | 2018 | 93.10 | 79.70 | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MHBN[[34](#bib.bib34)] | 2018 | 94.91 | - | - | 92.93 | - | - | - | - | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RCPCNN[[33](#bib.bib33)] | 2019 | 93.80 | - | - | - | - | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MVTN[[35](#bib.bib35)] | 2021 | 92.00 | 93.80 | - | - | - | - | 82.80 | -
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Voxel-based methods | VoxNet[[36](#bib.bib36)] | 2015 | 85.90 | 83.00 | -
    | - | 92.00 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3D shapeNet[[23](#bib.bib23)] | 2015 | 84.70 | 77.30 | - | - | 83.50 | -
    | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FPNN[[37](#bib.bib37)] | 2016 | 87.50 | - | - | - | - | - | - | - | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OctNet[[38](#bib.bib38)] | 2017 | 86.50 | - | - | 90.90 | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| O-CNN[[39](#bib.bib39)] | 2017 | 90.60 | - | 85.90 | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Kd-Net[[40](#bib.bib40)] | 2017 | 91.80 | 88.50 | - | 94.00 | 93.50 | 77.20
    | - | - | - | - | - | 82.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D Context Net[[41](#bib.bib41)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MSNet[[42](#bib.bib42)] | 2018 | - | - | - | - | - | - | - | - | - | - |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| VV-Net[[43](#bib.bib43)] | 2019 | - | - | - | - | - | - | - | - | - | - |
    - | 87.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Point cloud-based method | Local feature aggregation | Point-by-point methods
    | PointNet++[[44](#bib.bib44)] | 2017 | 91.90 | - | - | - | - | - | 77.90 | 75.40
    | - | - | - | 85.10 |'
  prefs: []
  type: TYPE_TB
- en: '| PointNeXt[[45](#bib.bib45)] | 2022 | - | - | - | - | - | - | 87.70 | 85.80
    | - | - | - | 87.20 |'
  prefs: []
  type: TYPE_TB
- en: '| PointWeb[[46](#bib.bib46)] | 2019 | 92.30 | 89.40 | - | - | - | - | - | -
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RandLA-Net[[47](#bib.bib47)] | 2020 | - | - | - | - | - | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SO-Net[[48](#bib.bib48)] | 2018 | 90.80 | 87.30 | - | 94.10 | 93.90 | - |
    - | - | - | - | - | 84.60 |'
  prefs: []
  type: TYPE_TB
- en: '| GDANet[[49](#bib.bib49)] | 2021 | 93.80 | - | - | - | - | - | 88.50 | - |
    - | - | - | 86.50 |'
  prefs: []
  type: TYPE_TB
- en: '| SimpleView[[50](#bib.bib50)] | 2020 | 93.00 | - | - | - | - | - | 80.50 |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PintSCNet[[51](#bib.bib51)] | 2021 | 93.70 | - | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | PointMLP[[52](#bib.bib52)] | 2022 | 94.50 | 91.40 | - | - | - | - | 85.40
    | 83.90 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | RepSurf[[54](#bib.bib54)] | 2022 | 94.70 | 91.70 | - | - | - | - | 84.60
    | 81.90 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Convolution based methods | PCNN[[55](#bib.bib55)] | 2018 | 92.30 | - | -
    | 94.90 | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RS-CNN[[56](#bib.bib56)] | 2019 | 93.60 | - | - | - | - | - | - | - | - |
    - | - | 86.20 |'
  prefs: []
  type: TYPE_TB
- en: '| DNNSP[[58](#bib.bib58)] | 2018 | - | - | - | - | - | - | - | - | - | - |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| A-CNN[[59](#bib.bib59)] | 2019 | 92.60 | 90.30 | - | 95.50 | 95.30 | - |
    - | - | - | - | - | 86.10 |'
  prefs: []
  type: TYPE_TB
- en: '| RP-Net[[60](#bib.bib60)] | 2021 | 94.10 | - | - | - | - | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SCN[[61](#bib.bib61)] | 2018 | 90.00 | - | - | - | - | - | - | - | - | -
    | - | 84.60 |'
  prefs: []
  type: TYPE_TB
- en: '| PointCNN[[62](#bib.bib62)] | 2018 | 92.20 | 88.10 | - | - | - | - | 87.90
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PointCove[[63](#bib.bib63)] | 2019 | 92.50 | - | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MCCNN[[64](#bib.bib64)] | 2018 | 90.90 | - | - | - | - | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SpiderCNN[[65](#bib.bib65)] | 2018 | 92.40 | - | - | - | - | - | 73.70 |
    69.80 | - | - | - | 85.30 |'
  prefs: []
  type: TYPE_TB
- en: '| InterpCNN[[66](#bib.bib66)] | 2019 | 93.00 | - | - | - | - | - | - | - |
    - | - | - | 86.30 |'
  prefs: []
  type: TYPE_TB
- en: '| SPHNet[[68](#bib.bib68)] | 2019 | 87.70 | - | - | - | - | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | SC-CNN[[69](#bib.bib69)] | 2022 | 93.8 | - | - | 96.4 | - | - | - | -
    | - | - | - | 86.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based methods | ECC[[73](#bib.bib73)] | 2017 | - | 83.20 | - | - |
    90.00 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SpecGCN[[74](#bib.bib74)] | 2018 | 91.50 | - | - | - | - | - | - | - | -
    | - | - | 84.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Grid-GCN[[75](#bib.bib75)] | 2020 | 93.10 | 91.30 | - | 97.50 | 97.40 | -
    | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PointView-GCN[[76](#bib.bib76)] | 2021 | 95.40 | - | - | - | - | - | 85.50
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DGCNN[[77](#bib.bib77)] | 2019 | 92.20 | 90.20 | - | - | - | - | 86.20 |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LDGCNN[[78](#bib.bib78)] | 2019 | 92.90 | 90.30 | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PointNGCNN[[79](#bib.bib79)] | 2020 | 92.80 | - | - | - | - | - | - | - |
    - | - | - | 85.60 |'
  prefs: []
  type: TYPE_TB
- en: '| attention-based methods | PAT[[80](#bib.bib80)] | 2019 | 91.70 | - | - |
    - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FPA/GAU[[81](#bib.bib81)] | 2018 | - | - | - | - | - | - | - | - | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LSANet[[82](#bib.bib82)] | 2019 | 92.30 | 89.20 | - | - | - | - | - | - |
    - | - | - | 83.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GACNet[[83](#bib.bib83)] | 2019 | - | - | - | - | - | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | GAPointNet[[84](#bib.bib84)] | 2021 | 92.40 | 89.70 | - | - | - | - |
    - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Global feature aggregation | Transformer-based methods | Point Transformer[[86](#bib.bib86)]
    | 2021 | 92.80 | - | - | - | - | - | - | - | - | 85.90 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Visual Transformer[[88](#bib.bib88)] | 2020 | - | - | - | - | - | - | - |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PCT[[90](#bib.bib90)] | 2021 | 93.20 | - | - | - | - | - | - | - | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3DMedPT[[12](#bib.bib12)] | 2021 | 93.40 | - | - | - | - | - | - | - | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Point-BERT[[91](#bib.bib91)] | 2021 | 93.80 | - | - | - | - | - | 83.07 |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Point-TnT[[87](#bib.bib87)] | 2022 | 92.60 | - | - | - | - | - | 84.60 |
    82.60 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Point-MAE[[92](#bib.bib92)] | 2022 | 93.80 | - | - | - | - | - | 85.18
    | - | - | - | - | 86.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Global module-based methods | PointNet[[21](#bib.bib21)] | 2017 | 89.20 |
    86.20 | - | - | - | - | 68.20 | 63.40 | - | - | - | 83.70 |'
  prefs: []
  type: TYPE_TB
- en: '| PointASNL[[95](#bib.bib95)] | 2020 | 93.20 | - | - | 95.90 | - | - | - |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CurveNet[[97](#bib.bib97)] | 2021 | 94.20 | - | - | 96.30 | - | - | - | -
    | - | - | - | 86.80 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeepGCNs[[96](#bib.bib96)] | 2021 | 93.20 | 90.30 | - | - | - | - | -
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | RNN or LSTM based methods | 3DCNN-DQN-RNN[[99](#bib.bib99)] | 2017 | -
    | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | RSNet[[100](#bib.bib100)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 3P-RNN[[101](#bib.bib101)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Point2Sequence[[102](#bib.bib102)] | 2019 | 92.60 | 90.40 | - | 95.30
    | 95.10 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Polymorphic Fusion based methods | PointGrid[[103](#bib.bib103)] | 2018 |
    92.00 | 88.90 | - | - | - | - | - | - | - | 86.10 | 80.50 | - |'
  prefs: []
  type: TYPE_TB
- en: '| PVT[[104](#bib.bib104)] | 2021 | 94.00 | - | - | - | - | - | - | - | - |
    - | - | 86.60 |'
  prefs: []
  type: TYPE_TB
- en: '| PointCLIP[[105](#bib.bib105)] | 2022 | 94.05 | - | - | - | - | - | - | -
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CrossPoint[[106](#bib.bib106)] | 2022 | 94.90 | - | - | - | - | - | 79.00
    | - | - | - | - | 85.50 |'
  prefs: []
  type: TYPE_TB
- en: V Future trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the development of today’s technology, the demand for point cloud classification
    methods in various fields is getting higher and higher. There are many methods
    of point cloud classification, researchers are constantly proposing new methods
    to improve accuracy and efficiency. This continues to drive the development of
    3D applications. However, the existing methods still have different limitations.
    This section will summarize the problems in the current deep learning-based point
    cloud classification methods and prospect the future research directions, details
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Some of the current point cloud classification methods focus on improving
    accuracy, while others focus on improving efficiency. We need to solve the problem
    of ”how to achieve high accuracy while being efficient?”
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ In the practical application of 3D, the information structure of outdoor
    scenes is complex and changeable. Although some methods have been applied in outdoor
    scenes, the efficiency and accuracy need to be further improved. Therefore, future
    research direction should focus more on the network for outdoor scenes.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ From the above data, the method based on the original point cloud
    has certain advantages in algorithm performance. But the network model is more
    complicated, because the input original point cloud data has information integrity,
    therefore simple point cloud-based methods are a future research trend.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Most of the current methods are aimed at improving rather than changing,
    so the innovative type of methods needs our continued research and discussion.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ In the network model we will propose in the future, we should pay
    attention to the optimization of the network architecture, which can reduce the
    computational complexity and memory usage while facing the complex and irregular
    point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper provides a comprehensive survey and discussion of deep learning-based
    point cloud classification methods in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we introduced the point cloud and its application in the introduction
    and discussed the characteristics and processing difficulties of the point cloud.
    In the second section, the 3D data were introduced, and the commonly used 3D data
    representations, point cloud data storage formats and point cloud classification
    datasets are summarized. Building on the previous sections, we comprehensively
    review deep learning-based point cloud classification methods, classifying these
    methods into four broad categories: multi-view-based methods, voxel-based methods,
    point cloud-based methods, and polymorphic fusion-based methods. And then we compare
    the performance of existing methods. Finally, this paper points out the problems
    of the current method and prospects the future research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by Xinjiang University School-Enterprise Joint Project.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] L. Zhang, L. Sun, W. Li, J. Zhang, W. Cai, C. Cheng, and X. Ning, “A joint
    bayesian framework based on partial least squares discriminant analysis for finger
    vein recognition,” *IEEE Sensors Journal*, vol. 22, no. 1, pp. 785–794, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] L. Zhou, X. Bai, X. Liu, J. Zhou, and E. R. Hancock, “Learning binary code
    for fast nearest subspace search,” *Pattern Recognition*, vol. 98, p. 107040,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] X. Ning, W. Tian, Z. Yu, W. Li, and X. Bai, “Hcfnn: High-order coverage
    function neural network for image classification,” *Pattern Recognition*, p. 108873,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Wang, C. Wang, W. Li, and H. Wang, “A brief survey on rgb-d semantic
    segmentation using deep learning,” *Displays*, vol. 70, p. 102080, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Wang, H. Wang, X. Ning, S. Tian, and W. Li, “3d point cloud classification
    method based on dynamic coverage of local area,” *Journal of Software*, vol. 34,
    no. 4, pp. 1962–1976, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] W. Cai, D. Liu, X. Ning, C. Wang, and G. Xie, “Voxel-based three-view hybrid
    parallel network for 3d object classification,” *Displays*, vol. 69, p. 102076,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Yan, G. Pang, X. Bai, C. Liu, X. Ning, L. Gu, and J. Zhou, “Beyond triplet
    loss: person re-identification with fine-grained difference-aware pairwise loss,”
    *IEEE Transactions on Multimedia*, vol. 24, pp. 1665–1677, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] X. Ning, K. Gong, W. Li, and L. Zhang, “Jwsaa: joint weak saliency and
    attention aware for person re-identification,” *Neurocomputing*, vol. 453, pp.
    801–811, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W. Zhangyu, Y. Guizhen, W. Xinkai, L. Haoran, and L. Da, “A camera and
    lidar data fusion method for railway object detection,” *IEEE Sensors Journal*,
    vol. 21, no. 12, pp. 13 442–13 454, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3d object detection,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 4490–4499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Guo, X. Yao, M. Shen, J. Wang, and W. Liao, “A deep learning network
    for point cloud of medicine structure,” in *2018 9th International Conference
    on Information Technology in Medicine and Education (ITME)*.   IEEE, 2018, pp.
    683–687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu, and W. Cai,
    “3d medical point transformer: Introducing convolution to attention networks for
    medical point cloud analysis,” *arXiv preprint arXiv:2112.04863*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Xu, D. An, Y. Jia, and Y. Yue, “A review: Point cloud-based 3d human
    joints estimation,” *Sensors*, vol. 21, no. 5, p. 1684, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] B. Yang, R. Huang, J. Li, M. Tian, W. Dai, and R. Zhong, “Automated reconstruction
    of building lods from airborne lidar point clouds using an improved morphological
    scale space,” *Remote Sensing*, vol. 9, no. 1, p. 14, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf, “Support
    vector machines,” *IEEE Intelligent Systems and their applications*, vol. 13,
    no. 4, pp. 18–28, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] R. Kumar and R. Verma, “Classification algorithms for data mining: A survey,”
    *International Journal of Innovations in Engineering and Technology (IJIET)*,
    vol. 1, no. 2, pp. 7–14, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z. Gan, L. Zhong, Y. Li, and H. Guan, “A random forest based method for
    urban object classification using lidar data and aerial imagery,” in *2015 23rd
    International Conference on Geoinformatics*.   IEEE, 2015, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] L. Zhou, Y. Liu, Z. Pencheng, B. Xiao, Y. Yazhou, J. Zhou, G. Lin, T. Harada,
    and E. R. Hancock, “Information bottleneck and selective noise supervision for
    zero-shot learning,” *Machine Learning*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] N. Plath, M. Toussaint, and S. Nakajima, “Multi-class image segmentation
    using conditional random fields and global classification,” in *Proceedings of
    the 26th annual international conference on machine learning*, 2009, pp. 817–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] D. Munoz, J. A. Bagnell, N. Vandapel, and M. Hebert, “Contextual classification
    with functional max-margin markov networks,” in *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*.   IEEE, 2009, pp. 975–982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Bai, J. Zhou, X. Ning, and C. Wang, “3d data computation and visualization,”
    p. 102169, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Sun, Q. Zhang, B. Kailkhura, Z. Yu, C. Xiao, and Z. M. Mao, “Benchmarking
    robustness of 3d point cloud recognition against common corruptions,” *arXiv preprint
    arXiv:2201.12296*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. De Deuge, A. Quadros, C. Hung, and B. Douillard, “Unsupervised feature
    learning for classification of outdoor 3d scans,” in *Australasian conference
    on robitics and automation*, vol. 2.   University of New South Wales Kensington,
    Australia, 2013, p. 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proc. Computer
    Vision and Pattern Recognition (CVPR), IEEE*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *International Conference on Computer Vision (ICCV)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] C. Wang, X. Wang, J. Zhang, L. Zhang, X. Bai, X. Ning, J. Zhou, and E. Hancock,
    “Uncertainty estimation for stereo matching based on evidential deep learning,”
    *Pattern Recognition*, vol. 124, p. 108498, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional
    neural networks for 3d shape recognition,” in *IEEE International Conference on
    Computer Vision*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] B. Song, B. Xiang, Z. Zhou, Z. Zhang, and L. J. Latecki, “Gift: A real-time
    and scalable 3d shape search engine,” in *2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and G. Yue, “Gvcnn: Group-view convolutional
    neural networks for 3d shape recognition,” in *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. Wang, M. Pelillo, and K. Siddiqi, “Dominant set clustering and pooling
    for multi-view 3d object recognition,” *arXiv preprint arXiv:1906.01592*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] T. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinear network for
    3d object recognition,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 186–194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Hamdi, S. Giancola, and B. Ghanem, “Mvtn: Multi-view transformation
    network for 3d shape recognition,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *2015 IEEE/RSJ international conference
    on intelligent robots and systems (IROS)*.   IEEE, 2015, pp. 922–928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas, “Fpnn: Field probing
    neural networks for 3d data,” *Advances in neural information processing systems*,
    vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] G. Riegler, A. Osman Ulusoy, and A. Geiger, “Octnet: Learning deep 3d
    representations at high resolutions,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2017, pp. 3577–3586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, “O-cnn: Octree-based
    convolutional neural networks for 3d shape analysis,” *ACM Transactions On Graphics
    (TOG)*, vol. 36, no. 4, pp. 1–11, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] R. Klokov and V. Lempitsky, “Escape from cells: Deep kd-networks for the
    recognition of 3d point cloud models,” in *Proceedings of the IEEE international
    conference on computer vision*, 2017, pp. 863–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] W. Zeng and T. Gevers, “3dcontextnet: Kd tree guided hierarchical learning
    of point clouds using local and global contextual cues,” in *Proceedings of the
    European Conference on Computer Vision (ECCV) Workshops*, 2018, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] L. Wang, Y. Huang, J. Shan, and L. He, “Msnet: Multi-scale convolutional
    network for point cloud classification,” *Remote Sensing*, vol. 10, no. 4, p.
    612, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha, “Vv-net: Voxel vae net
    with group convolutions for point cloud segmentation,” in *Proceedings of the
    IEEE/CVF international conference on computer vision*, 2019, pp. 8500–8508.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] G. Qian, Y. Li, H. Peng, J. Mai, H. A. A. K. Hammoud, M. Elhoseiny, and
    B. Ghanem, “Pointnext: Revisiting pointnet++ with improved training and scaling
    strategies,” *arXiv preprint arXiv:2206.04670*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia, “Pointweb: Enhancing local neighborhood
    features for point cloud processing,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 5565–5573.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham,
    “Randla-net: Efficient semantic segmentation of large-scale point clouds,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 11 108–11 117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Li, B. M. Chen, and G. H. Lee, “So-net: Self-organizing network for
    point cloud analysis,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 9397–9406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Xu, J. Zhang, Z. Zhou, M. Xu, X. Qi, and Y. Qiao, “Learning geometry-disentangled
    representation for complementary understanding of 3d object point cloud,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 35, no. 4, 2021, pp.
    3056–3064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Goyal, H. Law, B. Liu, A. Newell, and J. Deng, “Revisiting point cloud
    classification with a simple and effective baseline,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] X. Chen, Y. Wu, W. Xu, J. Li, H. Dong, and Y. Chen, “Pointscnet: Point
    cloud structure and correlation learning based on space-filling curve-guided sampling,”
    *Symmetry*, vol. 14, no. 1, p. 8, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] X. Ma, C. Qin, H. You, H. Ran, and Y. Fu, “Rethinking network design and
    local geometry in point cloud: A simple residual mlp framework,” *arXiv preprint
    arXiv:2202.07123*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Q. Huang, X. Dong, D. Chen, H. Zhou, W. Zhang, and N. Yu, “Shape-invariant
    3d adversarial point clouds,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 15 335–15 344.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] H. Ran, J. Liu, and C. Wang, “Surface representation for point clouds,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 18 942–18 952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Atzmon, H. Maron, and Y. Lipman, “Point convolutional neural networks
    by extension operators,” *arXiv preprint arXiv:1803.10091*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolutional neural
    network for point cloud analysis,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 8895–8904.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Yousefhussien, D. J. Kelbe, E. J. Ientilucci, and C. Salvaggio, “A
    multi-scale fully convolutional network for semantic labeling of 3d point clouds,”
    *ISPRS journal of photogrammetry and remote sensing*, vol. 143, pp. 191–204, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Z. Wang, L. Zhang, L. Zhang, R. Li, Y. Zheng, and Z. Zhu, “A deep neural
    network with spatial pooling (dnnsp) for 3-d point cloud classification,” *IEEE
    Transactions on Geoscience and Remote Sensing*, vol. 56, no. 8, pp. 4594–4604,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Komarichev, Z. Zhong, and J. Hua, “A-cnn: Annularly convolutional neural
    networks on point clouds,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2019, pp. 7421–7430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Ran, W. Zhuo, J. Liu, and L. Lu, “Learning inner-group relations on
    point clouds,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2021, pp. 15 477–15 487.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. Xie, S. Liu, Z. Chen, and Z. Tu, “Attentional shapecontextnet for point
    cloud recognition,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 4606–4615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn: Convolution
    on x-transformed points,” *Advances in neural information processing systems*,
    vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Wu, Z. Qi, and L. Fuxin, “Pointconv: Deep convolutional networks on
    3d point clouds,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019, pp. 9621–9630.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] P. Hermosilla, T. Ritschel, P.-P. Vázquez, À. Vinacua, and T. Ropinski,
    “Monte carlo convolution for learning on non-uniformly sampled point clouds,”
    *ACM Transactions on Graphics (TOG)*, vol. 37, no. 6, pp. 1–12, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Xu, T. Fan, M. Xu, L. Zeng, and Y. Qiao, “Spidercnn: Deep learning
    on point sets with parameterized convolutional filters,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 87–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Mao, X. Wang, and H. Li, “Interpolated convolutional networks for 3d
    point cloud understanding,” in *Proceedings of the IEEE/CVF international conference
    on computer vision*, 2019, pp. 1578–1587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis, “Learning
    so (3) equivariant representations with spherical cnns,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 52–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Poulenard, M.-J. Rakotosaona, Y. Ponty, and M. Ovsjanikov, “Effective
    rotation-invariant point cnn with spherical harmonics kernels,” in *2019 International
    Conference on 3D Vision (3DV)*.   IEEE, 2019, pp. 47–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Wang, X. Ning, L. Sun, L. Zhang, W. Li, and X. Bai, “Learning discriminative
    features by covering local geometric space for point cloud analysis,” *IEEE Transactions
    on Geoscience and Remote Sensing*, vol. 60, pp. 1–15, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally
    connected networks on graphs,” *arXiv preprint arXiv:1312.6203*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *arXiv preprint arXiv:1609.02907*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. Simonovsky and N. Komodakis, “Dynamic edge-conditioned filters in convolutional
    neural networks on graphs,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 3693–3702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. Wang, B. Samari, and K. Siddiqi, “Local spectral graph convolution
    for point set feature learning,” in *Proceedings of the European conference on
    computer vision (ECCV)*, 2018, pp. 52–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Q. Xu, X. Sun, C.-Y. Wu, P. Wang, and U. Neumann, “Grid-gcn for fast and
    scalable point cloud learning,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2020, pp. 5661–5670.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. S. Mohammadi, Y. Wang, and A. Del Bue, “Pointview-gcn: 3d shape classification
    with multi-view point clouds,” in *2021 IEEE International Conference on Image
    Processing (ICIP)*.   IEEE, 2021, pp. 3103–3107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *Acm Transactions On Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] K. Zhang, M. Hao, J. Wang, C. W. de Silva, and C. Fu, “Linked dynamic
    graph cnn: Learning on point cloud via linking hierarchical features,” *arXiv
    preprint arXiv:1904.10014*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Q. Lu, C. Chen, W. Xie, and Y. Luo, “Pointngcnn: Deep convolutional networks
    on 3d point clouds with neighborhood graph filters,” *Computers & Graphics*, vol. 86,
    pp. 42–51, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, “Modeling
    point clouds with self-attention and gumbel subset sampling,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2019,
    pp. 3323–3332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H. Li, P. Xiong, J. An, and L. Wang, “Pyramid attention network for semantic
    segmentation,” *arXiv preprint arXiv:1805.10180*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] L.-Z. Chen, X.-Y. Li, D.-P. Fan, K. Wang, S.-P. Lu, and M.-M. Cheng, “Lsanet:
    Feature learning on point sets by local spatial aware layer,” *arXiv preprint
    arXiv:1905.05442*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] L. Wang, Y. Huang, Y. Hou, S. Zhang, and J. Shan, “Graph attention convolution
    for point cloud semantic segmentation,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 10 296–10 305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C. Chen, L. Z. Fragonara, and A. Tsourdos, “Gapointnet: Graph attention
    based point neural network for exploiting local feature of point cloud,” *Neurocomputing*,
    vol. 438, pp. 122–132, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] N. Engel, V. Belagiannis, and K. Dietmayer, “Point transformer,” *IEEE
    Access*, vol. 9, pp. 134 826–134 840, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Berg, M. Oskarsson, and M. O’Connor, “Points to patches: Enabling the
    use of self-attention for 3d shape recognition,” *arXiv preprint arXiv:2204.03957*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez,
    K. Keutzer, and P. Vajda, “Visual transformers: Token-based image representation
    and processing for computer vision,” *arXiv preprint arXiv:2006.03677*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *European conference on computer
    vision*.   Springer, 2020, pp. 213–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,
    “Pct: Point cloud transformer,” *Computational Visual Media*, vol. 7, no. 2, pp.
    187–199, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-bert: Pre-training
    3d point cloud transformers with masked point modeling,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 19 313–19 322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, “Masked autoencoders
    for point cloud self-supervised learning,” *arXiv preprint arXiv:2203.06604*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. He, R. Li, S. Li, and L. Zhang, “Voxel set transformer: A set-to-set
    approach to 3d object detection from point clouds,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2022, pp. 8417–8427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2018, pp. 7794–7803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, “Pointasnl: Robust point
    clouds processing using nonlocal neural networks with adaptive sampling,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 5589–5598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] G. Li, M. Müller, G. Qian, I. C. D. Perez, A. Abualshour, A. K. Thabet,
    and B. Ghanem, “Deepgcns: Making gcns go as deep as cnns,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] T. Xiang, C. Zhang, Y. Song, J. Yu, and W. Cai, “Walk in the cloud: Learning
    curves for point clouds shape analysis,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 915–924.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] F. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe, “Exploring spatial
    context for 3d semantic segmentation of point clouds,” in *Proceedings of the
    IEEE international conference on computer vision workshops*, 2017, pp. 716–724.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y. Wang, and J. Lu, “3dcnn-dqn-rnn:
    A deep reinforcement learning framework for semantic parsing of large-scale 3d
    point clouds,” in *Proceedings of the IEEE international conference on computer
    vision*, 2017, pp. 5678–5687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networks for 3d segmentation
    of point clouds,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2018, pp. 2626–2635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3d recurrent neural networks
    with context fusion for point cloud semantic segmentation,” in *Proceedings of
    the European conference on computer vision (ECCV)*, 2018, pp. 403–417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, “Point2sequence: Learning
    the shape representation of 3d point clouds with an attention-based sequence to
    sequence network,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 33, no. 01, 2019, pp. 8778–8785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Le and Y. Duan, “Pointgrid: A deep network for 3d shape understanding,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2018, pp. 9204–9214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] C. Zhang, H. Wan, X. Shen, and Z. Wu, “Pvt: Point-voxel transformer for
    point cloud learning,” *arXiv preprint arXiv:2108.06076*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    and H. Li, “Pointclip: Point cloud understanding by clip,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    8552–8562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    and R. Rodrigo, “Crosspoint: Self-supervised cross-modal contrastive learning
    for 3d point cloud understanding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 9902–9912.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
