- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:40:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.02579] From Statistical Methods to Deep Learning, Automatic Keyphrase
    Prediction: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.02579] 从统计方法到深度学习，自动关键短语预测：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.02579](https://ar5iv.labs.arxiv.org/html/2305.02579)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.02579](https://ar5iv.labs.arxiv.org/html/2305.02579)
- en: '[orcid=0000-0001-7832-1507, style=chinese]\cormark[1] [orcid=0000-0002-0415-7748,
    style=chinese]\cormark[1] [orcid=0000-0002-2230-9952, style=chinese]\cormark[1]
    [style=chinese] [style=chinese] [style=chinese] [style=chinese] [style=chinese]
    [style=chinese, orcid=0000-0001-5606-7122]\cormark[2] \cortext[cor1]Equally contribution.
    \cortext[cor2]Corresponding authors. Tel.: +86 18750236638'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0001-7832-1507, style=chinese]\cormark[1] [orcid=0000-0002-0415-7748,
    style=chinese]\cormark[1] [orcid=0000-0002-2230-9952, style=chinese]\cormark[1]
    [style=chinese] [style=chinese] [style=chinese] [style=chinese] [style=chinese]
    [style=chinese, orcid=0000-0001-5606-7122]\cormark[2] \cortext[cor1]同等贡献。 \cortext[cor2]通讯作者。电话：+86
    18750236638'
- en: \tnotemark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \tnotemark
- en: '[1] \tnotetext[1]This work was supported in part by National Natural Science
    Foundation of China under Grant 62276219, in part by Natural Science Foundation
    of Fujian Province of China under Grant 2020J06001, and in part by Youth Innovation
    Fund of Xiamen under Grant 3502Z20206059\. (Correspongding author: Jinsong Su.)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] \tnotetext[1]本研究部分由中国国家自然科学基金资助（资助号：62276219），部分由福建省自然科学基金资助（资助号：2020J06001），部分由厦门青年创新基金资助（资助号：3502Z20206059）。
    (通讯作者：孙金松。)'
- en: 'From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction:
    A Survey'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从统计方法到深度学习，自动关键短语预测：综述
- en: Binbin Xie xdblb@stu.xmu.edu.cn    Jia Song songjia@stu.xmu.edu.cn    Liangying
    Shao liangyingshao@stu.xmu.edu.cn    Suhang Wu wush xmu@outlook.com    Xiangpeng
    Wei pemywei@gmail.com    Baosong Yang yangbaosong.ybs@alibaba-inc.com    Huan
    Lin lilai.lh@alibaba-inc.com    Jun Xie qingjing.xj@alibaba-inc.com    Jinsong
    Su jssu@xmu.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Binbin Xie xdblb@stu.xmu.edu.cn    Jia Song songjia@stu.xmu.edu.cn    Liangying
    Shao liangyingshao@stu.xmu.edu.cn    Suhang Wu wush xmu@outlook.com    Xiangpeng
    Wei pemywei@gmail.com    Baosong Yang yangbaosong.ybs@alibaba-inc.com    Huan
    Lin lilai.lh@alibaba-inc.com    Jun Xie qingjing.xj@alibaba-inc.com    Jinsong
    Su jssu@xmu.edu.cn
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Keyphrase prediction aims to generate phrases (keyphrases) that highly summarizes
    a given document. Recently, researchers have conducted in-depth studies on this
    task from various perspectives. In this paper, we comprehensively summarize representative
    studies from the perspectives of dominant models, datasets and evaluation metrics.
    Our work analyzes up to 167 previous works, achieving greater coverage of this
    task than previous surveys. Particularly, we focus highly on deep learning-based
    keyphrase prediction, which attracts increasing attention of this task in recent
    years. Afterwards, we conduct several groups of experiments to carefully compare
    representative models. To the best of our knowledge, our work is the first attempt
    to compare these models using the identical commonly-used datasets and evaluation
    metric, facilitating in-depth analyses of their disadvantages and advantages.
    Finally, we discuss the possible research directions of this task in the future.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关键短语预测旨在生成高度概括给定文档的短语（关键短语）。最近，研究人员从不同角度对这一任务进行了深入研究。本文从主要模型、数据集和评价指标的角度全面总结了代表性研究。我们的工作分析了多达167篇相关文献，涵盖范围超过了以往的综述。特别地，我们高度关注基于深度学习的关键短语预测，这在近年来引起了越来越多的关注。随后，我们进行了一系列实验，以细致比较代表性模型。据我们所知，我们的工作首次使用相同的常用数据集和评价指标对这些模型进行比较，促进了对其优缺点的深入分析。最后，我们讨论了未来该任务可能的研究方向。
- en: 'keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: keyphrase prediction \sepautomatic keyphrase extraction \sepautomatic keyphrase
    generation \sepdeep learning
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关键短语预测 \sep 自动关键短语提取 \sep 自动关键短语生成 \sep 深度学习
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the rapid development of the Internet and the explosion of information,
    how to efficiently acquire information from tremendous text data becomes more
    and more important. To do this, several information compression tasks have been
    proposed, such as automatic summarization and automatic keyphrase prediction.
    Compared with other tasks, automatic keyphrase prediction brings forward a higher
    request to the ability of information compression, since it aims to automatically
    produce a few keyphrases representing the core contents of the input document.
    As keyphrases can facilitate understanding documents and provide useful information
    to downstream tasks, such as information retrieval (Gutwin et al., [1999](#bib.bib44)),
    document classification (M et al., [2005](#bib.bib78); Hulth and Megyesi, [2006](#bib.bib49)),
    document summarization (Zhang et al., [2004](#bib.bib163); Wang and Cardie, [2013](#bib.bib134);
    Pasunuru and Bansal, [2018](#bib.bib102)), question generation (Subramanian et al.,
    [2018](#bib.bib121)) and opinion mining (Wilson et al., [2005](#bib.bib140); Berend,
    [2011](#bib.bib8)), automatic keyphrase prediction has attracted increasing attention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网的快速发展和信息爆炸，从大量文本数据中高效获取信息变得越来越重要。为此，提出了几种信息压缩任务，如自动摘要和自动关键词预测。与其他任务相比，自动关键词预测对信息压缩能力提出了更高的要求，因为它旨在自动生成少量代表输入文档核心内容的关键词。由于关键词可以促进文档理解，并为下游任务提供有用信息，如信息检索（Gutwin
    et al.，[1999](#bib.bib44)）、文档分类（M et al.，[2005](#bib.bib78)；Hulth和Megyesi，[2006](#bib.bib49)）、文档摘要（Zhang
    et al.，[2004](#bib.bib163)；Wang和Cardie，[2013](#bib.bib134)；Pasunuru和Bansal，[2018](#bib.bib102)）、问题生成（Subramanian
    et al.，[2018](#bib.bib121)）和意见挖掘（Wilson et al.，[2005](#bib.bib140)；Berend，[2011](#bib.bib8)），自动关键词预测引起了越来越多的关注。
- en: 'Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ From Statistical Methods to Deep
    Learning, Automatic Keyphrase Prediction: A Survey") shows an example of automatic
    keyphrase prediction. Generally, keyphrases can be divided into two categories:
    *present keyphrases* that continuously appear in the input document and *absent
    keyphrases* that do not match any contiguous subsequence of the document. To achieve
    high-quality keyphrase prediction, early studies mainly focus on *automatic keyphrase
    extraction* (Hulth, [2003](#bib.bib48); Mihalcea and Tarau, [2004](#bib.bib89);
    Nguyen and Kan, [2007](#bib.bib94); Wan and Xiao, [2008](#bib.bib131)), which
    aims to directly extract keyphrases from the input document. Recently, the rise
    of deep learning prompts researchers to focus on *automatic keyphrase generation*
    (Meng et al., [2017](#bib.bib88); Yuan et al., [2020](#bib.bib153); Ye et al.,
    [2021b](#bib.bib151)), where dominant models can generate not only present but
    also absent keyphrases. Tables [2](#S1.T2 "Table 2 ‣ 1 Introduction ‣ From Statistical
    Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey") shows the
    number of papers related to automatic keyphrase prediction, published at the main
    computer science conferences. It can be said that automatic keyphrase prediction
    has always been one of the research hotpots.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[1](#S1.T1 "表格 1 ‣ 1 引言 ‣ 从统计方法到深度学习，自动关键词预测：综述")展示了自动关键词预测的一个例子。一般来说，关键词可以分为两类：*存在的关键词*，即在输入文档中持续出现的关键词，以及*不存在的关键词*，即与文档中任何连续子序列都不匹配的关键词。为了实现高质量的关键词预测，早期研究主要集中于*自动关键词提取*（Hulth，[2003](#bib.bib48)；Mihalcea和Tarau，[2004](#bib.bib89)；Nguyen和Kan，[2007](#bib.bib94)；Wan和Xiao，[2008](#bib.bib131)），其目的是直接从输入文档中提取关键词。最近，深度学习的兴起促使研究人员关注*自动关键词生成*（Meng
    et al.，[2017](#bib.bib88)；Yuan et al.，[2020](#bib.bib153)；Ye et al.，[2021b](#bib.bib151)），其中主流模型不仅可以生成存在的关键词，还可以生成不存在的关键词。表格[2](#S1.T2
    "表格 2 ‣ 1 引言 ‣ 从统计方法到深度学习，自动关键词预测：综述")展示了主要计算机科学会议上与自动关键词预测相关的论文数量。可以说，自动关键词预测一直是研究热点之一。
- en: 'Table 1: An example of keyphrase prediction and present keyphrases that appear
    in the document are underlined.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：关键词预测的一个例子，文档中出现的存在的关键词已加下划线。
- en: '[width=0.95] *Input Document:* A nonmonotonic observation logic. A variant
    of Reiter’s default logic is proposed as a logic for reasoning with defeasible
    observations. Traditionally, default rules are assumed to represent generic information
    and the facts are assumed to represent specific information about the situation,
    but in this paper, the specific information derives from defeasible observations
    represented by (normal free) default rules, and the facts represent (hard) background
    knowledge. Whenever the evidence underlying some observation is more refined than
    the evidence underlying another observation, this is modelled by means of a priority
    between the default rules representing the observations. We thus arrive at an
    interpretation of prioritized normal free default logic as an observation logic,
    and we propose a semantics for this observation logic. Finally, we discuss how
    the proposed observation logic relates to the multiple extension problem and the
    problem of sensor fusion. *Keyphrases:* defeasible observations; nonmonotonic
    logic; prioritized default logic'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[width=0.95] *输入文档：* 一种非单调观察逻辑。提出了一种 Reiter 的默认逻辑变体作为处理可推翻观察的逻辑。传统上，默认规则被认为表示通用信息，而事实被认为表示关于情况的特定信息，但在本文中，特定信息来源于由（正常自由）默认规则表示的可推翻观察，而事实表示（硬性）背景知识。每当某些观察的证据比另一些观察的证据更为精细时，这通过表示观察的默认规则之间的优先级来建模。因此，我们得出了优先级正常自由默认逻辑作为观察逻辑的解释，并提出了这种观察逻辑的语义。最后，我们讨论了所提出的观察逻辑如何与多重扩展问题和传感器融合问题相关。
    *关键词：* 可推翻观察；非单调逻辑；优先级默认逻辑'
- en: 'Table 2: Paper publications of keyphrase extraction and keyphrase generation
    at the main computer science conferences, and ‘–’ denotes that the conference
    is not held or has not been held yet.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：主要计算机科学会议上关键词提取和关键词生成的论文出版情况，“–” 表示该会议未举办或尚未举办。
- en: '| Conf. | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 会议 | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 |'
- en: '| Keyphrase Extraction |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 关键词提取 |'
- en: '| ACL | 2 | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ACL | 2 | 0 | 1 | 0 | 0 | 0 |'
- en: '| EMNLP | 0 | 0 | 1 | 1 | 3 | 0 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| EMNLP | 0 | 0 | 1 | 1 | 3 | 0 |'
- en: '| NAACL | – | 1 | 1 | – | 2 | 1 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| NAACL | – | 1 | 1 | – | 2 | 1 |'
- en: '| COLING | – | 0 | – | 4 | – | 3 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| COLING | – | 0 | – | 4 | – | 3 |'
- en: '| AAAI | 3 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| AAAI | 3 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Keyphrase Generation |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 关键词生成 |'
- en: '| ACL | 1 | 0 | 3 | 3 | 2 | 0 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ACL | 1 | 0 | 3 | 3 | 2 | 0 |'
- en: '| EMNLP | 0 | 2 | 0 | 3 | 3 | 5 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| EMNLP | 0 | 2 | 0 | 3 | 3 | 5 |'
- en: '| NAACL | – | 0 | 2 | – | 3 | 2 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| NAACL | – | 0 | 2 | – | 3 | 2 |'
- en: '| COLING | – | 0 | – | 1 | – | 0 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| COLING | – | 0 | – | 1 | – | 0 |'
- en: '| AAAI | 0 | 0 | 1 | 0 | 1 | 2 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| AAAI | 0 | 0 | 1 | 0 | 1 | 2 |'
- en: '| Total. | 6 | 3 | 9 | 11 | 14 | 13 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 总计。 | 6 | 3 | 9 | 11 | 14 | 13 |'
- en: 'In this paper, we first provide a comprehensive review of automatic keyphrase
    prediction from the following aspects: dominant models, datasets and evaluation
    metrics. Compared with previous surveys (Hasan and Ng, [2014](#bib.bib47); Siddiqi
    and Sharan, [2015](#bib.bib118); Çano and Bojar, [2019](#bib.bib16); Alami Merrouni
    et al., [2020](#bib.bib2); Nasar et al., [2019](#bib.bib91)), our work summarizes
    up to 167 previous works, achieving greater coverage of this task. More importantly,
    our work is not only the first attempt to thoroughly summarize keyphrase extraction
    based on neural networks, but also focusing highly on the recent advancements
    of neural keyphrase generation on different investigated problems. Please note
    that neural keyphrase generation has become the hot research topic in this community,
    since it is able to predict not only present keyphrases but also absent keyphrases,
    which accounts a large proportion in the commonly-used keyphrase generation datasets.
    Particularly, we further introduce the recent advancements in keyphrase generation,
    including pre-trained model based keyphrase generation models, echoing with the
    development trend of natural language processing.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先从以下几个方面对自动关键词预测进行了全面回顾：主导模型、数据集和评估指标。与以前的调查（Hasan 和 Ng, [2014](#bib.bib47)；Siddiqi
    和 Sharan, [2015](#bib.bib118)；Çano 和 Bojar, [2019](#bib.bib16)；Alami Merrouni
    等, [2020](#bib.bib2)；Nasar 等, [2019](#bib.bib91)）相比，我们的工作总结了多达 167 项以前的工作，实现了对该任务更广泛的覆盖。更重要的是，我们的工作不仅是首次全面总结基于神经网络的关键词提取，而且高度关注神经关键词生成在不同研究问题上的最新进展。请注意，神经关键词生成已成为该领域的热门研究主题，因为它不仅能够预测当前的关键词，还能预测缺失的关键词，这在常用的关键词生成数据集中占有很大比例。特别地，我们进一步介绍了关键词生成的最新进展，包括基于预训练模型的关键词生成模型，与自然语言处理的发展趋势相呼应。
- en: 'Then, we conduct several groups of experiments to carefully compare representative
    models, so as to analyze their characteristics. Unlike previous studies generally
    using different datasets and metrics to evaluate models, we use the identical
    commonly-used datasets and evaluation metric to ensure fair comparions among these
    representative models, and then analyze their advantages and disadvantages in
    different scenarios. Via our experiments, we can reach some interesting conclusions:
    1) Generally, unsupervised extraction models perform worst among all kinds of
    unsupervised and supervised models. However, when it exists a serious domain discrepancy
    between the training set and test set, the unsupervised extraction models may
    achieve comparable performance with the supervised ones. 2) Among three commonly-used
    paradigms for keyphrase generation, ONE2SET surpasses the others and achieve the
    best performance, while is still inferior to the extraction models in predicting
    present keyphrases. 3) Combining with extraction, generation and retrieval-based
    methods have potential to achieve better overall results for both present and
    absent keyphrase predictions.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们进行几组实验，以仔细比较代表性模型，从而分析它们的特性。与以往研究通常使用不同的数据集和指标评估模型不同，我们使用相同的常用数据集和评估指标，以确保这些代表性模型之间的公平比较，然后分析它们在不同场景中的优缺点。通过我们的实验，我们可以得出一些有趣的结论：1）一般来说，无监督提取模型在所有无监督和监督模型中表现最差。然而，当训练集和测试集之间存在严重领域差异时，无监督提取模型的表现可能与监督模型相当。2）在三种常用的关键短语生成范式中，**ONE2SET**超越了其他模型，取得了最佳表现，但在预测现有关键短语时仍然不如提取模型。3）结合提取、生成和基于检索的方法有潜力在预测现有和缺失关键短语时实现更好的总体结果。
- en: Finally, we point out the future research directions of keyphrase prediction
    task, which will play a positive role in guiding the follow-up studies. Note that
    we propose some directions that were not considered in previous surveys, such
    as multi-modality keyphrase prediction, and multilingual keyphrase prediction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指出了关键短语预测任务的未来研究方向，这将对指导后续研究起到积极作用。请注意，我们提出了一些之前调查中未考虑的方向，如多模态关键短语预测和多语言关键短语预测。
- en: '{forest}'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: for tree=grow=east, forked edges, draw, rounded corners, node options= align=center
    , text width=2.7cm, anchor=west [Automatic Keyphrase Extraction, parent [Hand-Engineered
    Features, for tree=child [ Internal Document-based Features [Statistical Features,
    Position Features, Linguistic Features, Logical Structure,chars ]] [ External
    Document-Based Features [The Similarity based on Wiki, The Frequence based on
    External Documents, Citation, Web Linkage, chars]] ] [ Models, for tree=child
    [ Unsupervised Models [Statistical Models, [(El-Beltagy and Rafea, [2009](#bib.bib32);
    Campos et al., [2018](#bib.bib15); Won et al., [2019](#bib.bib142)), model]] [Graph-based
    Models, [Clustering, [(Ohsawa et al., [1998](#bib.bib98); Grineva et al., [2009](#bib.bib41);
    Liu et al., [2009](#bib.bib71)),model]] [Graph Propagation, [Topic Information,
    [(Liu et al., [2010](#bib.bib70); Sterckx et al., [2015](#bib.bib120); Teneva
    and Cheng, [2017](#bib.bib127); Bougouin et al., [2013](#bib.bib14); Boudin, [2018](#bib.bib11)),
    model]] [Others, [(Mihalcea and Tarau, [2004](#bib.bib89); Wan and Xiao, [2008](#bib.bib131);
    Danesh et al., [2015](#bib.bib27); Florescu and Caragea, [2017](#bib.bib33); Gollapalli
    and Caragea, [2014](#bib.bib39); Vega-Oliveros et al., [2019](#bib.bib130)), model]]
    ] ] [Deep Learning-based Models, [Phrase-Document Similarity, [(Papagiannopoulou
    and Tsoumakas, [2018](#bib.bib101); Bennani-Smires et al., [2018](#bib.bib7);
    Sun et al., [2020](#bib.bib123); Li and Daoutis, [2021](#bib.bib66)), model]]
    [Graph-based Ranking, [(Mahata et al., [2018](#bib.bib80); Asl and Banda, [2020](#bib.bib3);
    Liang and Zaki, [2021](#bib.bib68); Liang et al., [2021](#bib.bib67)), model]]
    [Semantic Importance of Keyphrase, [(Zhang et al., [2021](#bib.bib156); Joshi
    et al., [2022](#bib.bib52)), model]] [Attention Mechanism Information, [(Ding
    and Luo, [2021](#bib.bib29); Gu et al., [2021](#bib.bib43)), model]]] ] [ Supervised
    Models [Statistical Models [Modeling Approaches [Sequence Labeling, [(Zhang, [2008](#bib.bib154);
    Gollapalli et al., [2017](#bib.bib40)), model]] [Binary Classification, [(Witten
    et al., [1999](#bib.bib141); Frank et al., [1999](#bib.bib34); Turney, [2002](#bib.bib129);
    Hulth, [2003](#bib.bib48); Kelleher and Luz, [2005](#bib.bib53); Yih et al., [2006](#bib.bib152);
    Medelyan and Witten, [2006](#bib.bib85); Zhang et al., [2006](#bib.bib155); Nguyen
    and Kan, [2007](#bib.bib94); Shi et al., [2008](#bib.bib117); Medelyan et al.,
    [2009](#bib.bib84); Lopez and Romary, [2010](#bib.bib73); Nguyen and Luong, [2010](#bib.bib95);
    Haddoud and Abdeddaïm, [2014](#bib.bib46); Caragea et al., [2014](#bib.bib17);
    Xie et al., [2017](#bib.bib147); Wang and Li, [2017](#bib.bib135)), model]] [Ranking,
    [(Jiang et al., [2009](#bib.bib51); Zhang et al., [2017a](#bib.bib158)), model]]
    ] [ Algorithms, [ CRF, [(Zhang, [2008](#bib.bib154); Gollapalli et al., [2017](#bib.bib40);
    Lu and Chow, [2021](#bib.bib74)), model]] [ Logistic Regression, [(Yih et al.,
    [2006](#bib.bib152); Shi et al., [2008](#bib.bib117); Haddoud and Abdeddaïm, [2014](#bib.bib46)),
    model]] [ Naive Bayes, [(Witten et al., [1999](#bib.bib141); Frank et al., [1999](#bib.bib34);
    Kelleher and Luz, [2005](#bib.bib53); Medelyan and Witten, [2006](#bib.bib85);
    Nguyen and Kan, [2007](#bib.bib94); Nguyen and Luong, [2010](#bib.bib95); Caragea
    et al., [2014](#bib.bib17); Xie et al., [2017](#bib.bib147)), model]] [ SVM, [(Zhang
    et al., [2006](#bib.bib155); Jiang et al., [2009](#bib.bib51)), model]] [ Bagged
    Decision Trees, [(Turney, [2002](#bib.bib129); Medelyan et al., [2009](#bib.bib84);
    Lopez and Romary, [2010](#bib.bib73)), model]] [ Other Ensemble Models,[(Hulth,
    [2003](#bib.bib48); Wang and Li, [2017](#bib.bib135)), model]] ] ] [Deep Learning-based
    Models, [Modeling Approaches [Sequence Labeling, [(Zhang et al., [2016](#bib.bib157),
    [2018](#bib.bib161); Saputra et al., [2018](#bib.bib113); Zhang and Zhang, [2019](#bib.bib162);
    Chowdhury et al., [2019](#bib.bib24); Sahrawat et al., [2019](#bib.bib107); Mahfuzh
    et al., [2020](#bib.bib81); Garg et al., [2020](#bib.bib36); Wang et al., [2020b](#bib.bib137);
    Santosh et al., [2020b](#bib.bib110); Gero and Ho, [2021](#bib.bib38); Nikzad-Khasmakhi
    et al., [2021](#bib.bib96)), model]] [Binary Classification, [(Wang et al., [2005](#bib.bib133);
    Xiong et al., [2019](#bib.bib148); Prasad and Kan, [2019](#bib.bib105)), model]]
    [Ranking, [(Sarkar et al., [2010](#bib.bib114); Mu et al., [2020](#bib.bib90);
    Sun et al., [2021](#bib.bib122); Song et al., [2021](#bib.bib119)), model]]] [Data
    Utilization, [(Luan et al., [2017](#bib.bib75); Lai et al., [2020](#bib.bib61);
    Lei et al., [2021](#bib.bib64); Kontoulis et al., [2021](#bib.bib57)), model]]
    ] ] ] ]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树形结构=生长=东，分叉边，绘制，圆角，节点选项=对齐=center，文本宽度=2.7厘米，锚点=西 [自动关键短语提取，父级 [人工特征，对于树形结构=子级
    [内部文档特征 [统计特征，位置特征，语言特征，逻辑结构，字符]] [外部文档特征 [基于维基的相似性，基于外部文档的频率，引用，网页链接，字符]] ] [模型，对于树形结构=子级
    [无监督模型 [统计模型，[(El-Beltagy 和 Rafea, [2009](#bib.bib32)；Campos 等, [2018](#bib.bib15)；Won
    等, [2019](#bib.bib142)），模型]] [基于图的模型 [聚类，[(Ohsawa 等, [1998](#bib.bib98)；Grineva
    等, [2009](#bib.bib41)；Liu 等, [2009](#bib.bib71)），模型]] [图传播，[主题信息，[(Liu 等, [2010](#bib.bib70)；Sterckx
    等, [2015](#bib.bib120)；Teneva 和 Cheng, [2017](#bib.bib127)；Bougouin 等, [2013](#bib.bib14)；Boudin,
    [2018](#bib.bib11)），模型]] [其他，[(Mihalcea 和 Tarau, [2004](#bib.bib89)；Wan 和 Xiao,
    [2008](#bib.bib131)；Danesh 等, [2015](#bib.bib27)；Florescu 和 Caragea, [2017](#bib.bib33)；Gollapalli
    和 Caragea, [2014](#bib.bib39)；Vega-Oliveros 等, [2019](#bib.bib130)），模型]] ] ] [基于深度学习的模型
    [短语-文档相似性，[(Papagiannopoulou 和 Tsoumakas, [2018](#bib.bib101)；Bennani-Smires 等,
    [2018](#bib.bib7)；Sun 等, [2020](#bib.bib123)；Li 和 Daoutis, [2021](#bib.bib66)），模型]]
    [基于图的排序，[(Mahata 等, [2018](#bib.bib80)；Asl 和 Banda, [2020](#bib.bib3)；Liang 和
    Zaki, [2021](#bib.bib68)；Liang 等, [2021](#bib.bib67)），模型]] [关键短语的语义重要性，[(Zhang
    等, [2021](#bib.bib156)；Joshi 等, [2022](#bib.bib52)），模型]] [注意力机制信息，[(Ding 和 Luo,
    [2021](#bib.bib29)；Gu 等, [2021](#bib.bib43)），模型]]] ] [有监督模型 [统计模型 [建模方法 [序列标注，[(Zhang,
    [2008](#bib.bib154)；Gollapalli 等, [2017](#bib.bib40)），模型]] [二分类，[(Witten 等, [1999](#bib.bib141)；Frank
    等, [1999](#bib.bib34)；Turney, [2002](#bib.bib129)；Hulth, [2003](#bib.bib48)；Kelleher
    和 Luz, [2005](#bib.bib53)；Yih 等, [2006](#bib.bib152)；Medelyan 和 Witten, [2006](#bib.bib85)；Zhang
    等, [2006](#bib.bib155)；Nguyen 和 Kan, [2007](#bib.bib94)；Shi 等, [2008](#bib.bib117)；Medelyan
    等, [2009](#bib.bib84)；Lopez 和 Romary, [2010](#bib.bib73)；Nguyen 和 Luong, [2010](#bib.bib95)；Haddoud
    和 Abdeddaïm, [2014](#bib.bib46)；Caragea 等, [2014](#bib.bib17)；Xie 等, [2017](#bib.bib147)；Wang
    和 Li, [2017](#bib.bib135)），模型]] [排序，[(Jiang 等, [2009](#bib.bib51)；Zhang 等, [2017a](#bib.bib158)），模型]]
    ] [算法 [CRF，[(Zhang, [2008](#bib.bib154)；Gollapalli 等, [2017](#bib.bib40)；Lu 和
    Chow, [2021](#bib.bib74)），模型]] [逻辑回归，[(Yih 等, [2006](#bib.bib152)；Shi 等, [2008](#bib.bib117)；Haddoud
    和 Abdeddaïm, [2014](#bib.bib46)），模型]] [朴素贝叶斯，[(Witten 等, [1999](#bib.bib141)；Frank
    等, [1999](#bib.bib34)；Kelleher 和 Luz, [2005](#bib.bib53)；Medelyan 和 Witten, [2006](#bib.bib85)；Nguyen
    和 Kan, [2007](#bib.bib94)；Nguyen 和 Luong, [2010](#bib.bib95)；Caragea 等, [2014](#bib.bib17)；Xie
    等, [2017](#bib.bib147)），模型]] [SVM，[(Zhang 等, [2006](#bib.bib155)；Jiang 等, [2009](#bib.bib51)），模型]]
    [袋装决策树，[(Turney, [2002](#bib.bib129)；Medelyan 等, [2009](#bib.bib84)；Lopez 和 Romary,
    [2010](#bib.bib73)），模型]] [其他集成模型，[(Hulth, [2003](#bib.bib48)；Wang 和 Li, [2017](#bib.bib135)），模型]]
    ] ] [基于深度学习的模型 [建模方法 [序列标注，[(Zhang 等, [2016](#bib.bib157)，[2018](#bib.bib161)；Saputra
    等, [2018](#bib.bib113)；Zhang 和 Zhang, [2019](#bib.bib162)；Chowdhury 等, [2019](#bib.bib24)；Sahrawat
    等, [2019](#bib.bib107)；Mahfuzh 等, [2020](#bib.bib81)；Garg 等, [2020](#bib.bib36)；Wang
    等, [2020b](#bib.bib137)；Santosh 等, [2020b](#bib.bib110)；Gero 和 Ho, [2021](#bib.bib38)；Nikzad-Khasmakhi
    等, [2021](#bib.bib96)），模型]] [二分类，[(Wang 等, [2005](#bib.bib133)；Xiong 等, [2019](#bib.bib148)；Prasad
    和 Kan, [2019](#bib.bib105)），模型]] [排序，[(Sarkar 等, [2010](#bib.bib114)；Mu 等, [2020](#bib.bib90)；Sun
    等, [2021](#bib.bib122)；Song 等, [2021](#bib.bib119)），模型]] [数据利用，[(Luan 等, [2017](#bib.bib75)；Lai
    等, [2020](#bib.bib61)；Lei 等, [2021](#bib.bib64)；Kontoulis 等, [2021](#bib.bib57)），模型]]
    ] ] ]
- en: 'Figure 1: The Taxonomy of Representative Studies on Automatic Keyphrase Extraction.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自动关键词提取的代表性研究分类。
- en: 2 Automatic Keyphrase Extraction
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 自动关键词提取
- en: 'Figure [1](#S1.F1.fig1 "Figure 1 ‣ 1 Introduction ‣ From Statistical Methods
    to Deep Learning, Automatic Keyphrase Prediction: A Survey") shows the taxonomy
    of representative studies on automatic keyphrase extraction. This line of research
    mainly focuses on how to directly extract keyphrases from an input document. Usually,
    it consists of three steps: 1) applying hand-crafted rules to obtain candidate
    phrases, such as removing stop words (Liu et al., [2009](#bib.bib71)), applying
    POS tagging (Mihalcea and Tarau, [2004](#bib.bib89)), extracting n-grams (Witten
    et al., [1999](#bib.bib141)), and using knowledge bases (Nguyen and Phan, [2009](#bib.bib93)),
    2) designing various hand-engineered features to represent candidate keyphrases,
    and 3) determining the final keyphrases based on features using unsupervised or
    supervised models.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1.fig1 "Figure 1 ‣ 1 Introduction ‣ From Statistical Methods to
    Deep Learning, Automatic Keyphrase Prediction: A Survey") 显示了自动关键词提取的代表性研究分类。这一研究方向主要集中于如何直接从输入文档中提取关键词。通常包括三个步骤：1）应用手工设计的规则获取候选短语，如去除停用词（Liu
    et al., [2009](#bib.bib71)），应用词性标注（Mihalcea and Tarau, [2004](#bib.bib89)），提取
    n-grams（Witten et al., [1999](#bib.bib141)），以及使用知识库（Nguyen and Phan, [2009](#bib.bib93)）；2）设计各种手工设计的特征来表示候选关键词；3）基于特征使用无监督或监督模型确定最终的关键词。'
- en: In the following subsections, we will first briefly introduce the hand-engineered
    features, and then describe the unsupervised and supervised models using these
    features in detail.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们将首先简要介绍手工设计的特征，然后详细描述使用这些特征的无监督和监督模型。
- en: 2.1 Hand-engineered Features
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 手工设计特征
- en: 'There are mainly four kinds of the internal document-based features used (Witten
    et al., [1999](#bib.bib141); Turney, [2002](#bib.bib129); Hulth, [2003](#bib.bib48);
    Zhang et al., [2006](#bib.bib155); Campos et al., [2018](#bib.bib15); Ohsawa et al.,
    [1998](#bib.bib98)): statistical features (phrase length, TF-IDF, the number of
    sentences containing phrases, co-occurrence frequency, etc.), positional features
    (occurrence positions, sentence boundaries, etc.), linguistics features (POS tags,
    case information, surrounding words, etc.), and logical structure features (the
    hierarchy, title, author list of the input document, etc.).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有四种基于内部文档的特征（Witten et al., [1999](#bib.bib141); Turney, [2002](#bib.bib129);
    Hulth, [2003](#bib.bib48); Zhang et al., [2006](#bib.bib155); Campos et al., [2018](#bib.bib15);
    Ohsawa et al., [1998](#bib.bib98)）：统计特征（短语长度、TF-IDF、包含短语的句子数量、共现频率等）、位置特征（出现位置、句子边界等）、语言特征（词性标记、案例信息、周围词汇等）以及逻辑结构特征（输入文档的层次结构、标题、作者列表等）。
- en: In addition, many features are proposed using the external documents, such as
    the similarity based on Wikipedia, candidate frequency based on the external documents,
    citation and web linkage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还提出了许多使用外部文档的特征，例如基于维基百科的相似度、基于外部文档的候选频率、引用和网页链接。
- en: 2.2 Unsupervised Keyphrase Extraction
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 无监督关键词提取
- en: Generally, unsupervised models for keyphrase extraction can be roughly divided
    into statistical models, graph-based models and deep learning-based models, which
    will be briefly introduced below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，无监督关键词提取模型可以粗略分为统计模型、基于图的模型和基于深度学习的模型，以下将对此进行简要介绍。
- en: 2.2.1 Unsupervised Statistical Models
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 无监督统计模型
- en: 'These models are directly conducted based on the abundant hand-engineered features.
    Among these features, the most important one is TF-IDF (Salton and Buckley, [1988](#bib.bib108)),
    which can quantify the importance of each candidate phrase and thus becomes the
    basis of many follow-up models. For example, El-Beltagy and Rafea ([2009](#bib.bib32))
    consider the position of each candidate in the input document and introduce a
    length-related weight to adjust its TF-IDF value. Furthermore, Campos et al. ([2018](#bib.bib15))
    propose YAKE involving five hand-engineered features: case information, phrase
    position, term frequency, the frequency of phrase appearing within different sentences,
    and the number of surrounding words. Based on these features, Won et al. ([2019](#bib.bib142))
    further determine the number of keyphrases according to the length of the input
    document.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型直接基于丰富的手工设计特征进行。其中特别重要的一个是 TF-IDF (Salton 和 Buckley，[1988](#bib.bib108))，它能够量化每个候选短语的重要性，因此成为许多后续模型的基础。例如，El-Beltagy
    和 Rafea ([2009](#bib.bib32)) 考虑了每个候选在输入文档中的位置，并引入了与长度相关的权重来调整其 TF-IDF 值。此外，Campos
    等人 ([2018](#bib.bib15)) 提出了 YAKE，涉及五个手工设计特征：案例信息、短语位置、术语频率、短语在不同句子中出现的频率，以及周围词的数量。基于这些特征，Won
    等人 ([2019](#bib.bib142)) 进一步根据输入文档的长度确定关键词的数量。
- en: 2.2.2 Unsupervised Graph-based Models
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 无监督图模型
- en: KeyGraph (Ohsawa et al., [1998](#bib.bib98)) is the first graph-based model
    for keyphrase extraction. In this model, frequently co-occurrent phrases are connected
    to form a graph, which is then partitioned into subgraphs via clustering. Finally,
    the importance of each candidate phrase is quantified according to the subgraph
    based statistical information. Grineva et al. ([2009](#bib.bib41)) firstly calculate
    edge weights as the phrase-level semantic relatedness based on Wikipedia, and
    then apply the community detection algorithm (Newman and Girvan, [2004](#bib.bib92))
    to obtain dense subgraphs, where phrases from the most important subgraphs are
    considered as keyphrases. Similarly, Liu et al. ([2009](#bib.bib71)) construct
    a word graph and cluster words according to the semantic distances based on the
    word co-occurrence frequency or Wikipedia statistics. Then, the noun phrases expanded
    from cluster centers are chosen as keyphrases.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: KeyGraph (Ohsawa 等人，[1998](#bib.bib98)) 是首个用于关键词提取的图模型。在该模型中，频繁共现的短语被连接成图，然后通过聚类将其划分为子图。最后，根据子图的统计信息量化每个候选短语的重要性。Grineva
    等人 ([2009](#bib.bib41)) 首次计算边权重作为基于维基百科的短语级语义相关性，然后应用社区检测算法 (Newman 和 Girvan，[2004](#bib.bib92))
    来获得密集子图，从最重要子图中的短语被视为关键词。类似地，Liu 等人 ([2009](#bib.bib71)) 构建了一个词图，并根据词的共现频率或维基百科统计数据的语义距离对单词进行聚类。然后，从聚类中心扩展的名词短语被选为关键词。
- en: Inspired by PageRank (Page et al., [1999](#bib.bib99)), Mihalcea and Tarau ([2004](#bib.bib89))
    propose TextRank that iteratively conducts importance propagation on a co-occurrent
    word graph. Along this line, Danesh et al. ([2015](#bib.bib27)) extend TextRank
    by using phrases as graph nodes. Then, many features are explored to adjust edge
    weights, including statistical features (phrase frequency and length (Danesh et al.,
    [2015](#bib.bib27)), word co-occurrence frequency (Wan and Xiao, [2008](#bib.bib131)))
    and position information (Florescu and Caragea, [2017](#bib.bib33)). Besides,
    to exploit more contexts, Wan and Xiao ([2008](#bib.bib131)), Gollapalli and Caragea
    ([2014](#bib.bib39)) extend the single-document word graph with similar documents
    and citation network, respectively. In addition to the PageRank-based centrality
    measure, Vega-Oliveros et al. ([2019](#bib.bib130)) consider other commonly-used
    centrality measures,and then propose an optimal combination of centrality measures
    to extract keywords from an undirected and unweighted word graph.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 PageRank (Page 等人，[1999](#bib.bib99)) 启发，Mihalcea 和 Tarau ([2004](#bib.bib89))
    提出了 TextRank，该方法在共现词图上迭代地进行重要性传播。在此基础上，Danesh 等人 ([2015](#bib.bib27)) 通过使用短语作为图节点扩展了
    TextRank。随后，探索了许多特征来调整边权重，包括统计特征（短语频率和长度 (Danesh 等人，[2015](#bib.bib27))，词共现频率
    (Wan 和 Xiao，[2008](#bib.bib131))) 和位置信息 (Florescu 和 Caragea，[2017](#bib.bib33))。此外，为了利用更多上下文，Wan
    和 Xiao ([2008](#bib.bib131))、Gollapalli 和 Caragea ([2014](#bib.bib39)) 分别通过类似文档和引用网络扩展了单文档词图。除了基于
    PageRank 的中心性度量外，Vega-Oliveros 等人 ([2019](#bib.bib130)) 考虑了其他常用的中心性度量，然后提出了中心性度量的最佳组合来从无向且无权重的词图中提取关键词。
- en: Intuitively, ideal keyphrases should be consistent with the topics of the input
    document. Thus, researchers introduce the topic information to refine graph-based
    models. Typically, Liu et al. ([2010](#bib.bib70)) propose TPR that adopts LDA
    (Blei et al., [2003](#bib.bib9)) to obtain topic information and then separately
    performs PageRank for each topic. To alleviate the huge computational cost of
    TPR, researchers extend TPR into Single Topical PageRank (Sterckx et al., [2015](#bib.bib120))
    and SalienceRank (Teneva and Cheng, [2017](#bib.bib127)), both of which perform
    PageRank once for each document. Compared to the former, the latter can extract
    not only topic-specific but also corpus-correlated keyphrases. Unlike the above
    studies based on LDA, Bougouin et al. ([2013](#bib.bib14)) propose TopicRank,
    which firstly clusters similar phrases to form topics and then constructs a topic
    graph for PageRank. Afterwards, they select the most representative phrases from
    each topic as keyphrases. To refine TopicRank, Boudin ([2018](#bib.bib11)) represents
    candidate phrases and topics in a single graph and exploits their mutual reinforcement
    to improve candidate ranking.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，理想的关键短语应该与输入文档的主题一致。因此，研究人员引入主题信息来细化基于图的模型。通常，Liu 等（[2010](#bib.bib70)）提出了
    TPR，采用 LDA（Blei 等，[2003](#bib.bib9)）获取主题信息，然后分别对每个主题进行 PageRank。为了缓解 TPR 的巨大计算成本，研究人员将
    TPR 扩展为 Single Topical PageRank（Sterckx 等，[2015](#bib.bib120)）和 SalienceRank（Teneva
    和 Cheng，[2017](#bib.bib127)），这两者都对每个文档执行一次 PageRank。与前者相比，后者不仅可以提取特定主题的关键短语，还能提取与语料库相关的关键短语。不同于基于
    LDA 的上述研究，Bougouin 等（[2013](#bib.bib14)）提出了 TopicRank，该方法首先将相似短语聚类以形成主题，然后构建一个主题图进行
    PageRank。之后，他们从每个主题中选择最具代表性的短语作为关键短语。为了改进 TopicRank，Boudin（[2018](#bib.bib11)）在一个图中表示候选短语和主题，并利用它们的相互强化来提高候选短语的排名。
- en: 2.2.3 Unsupervised Deep Learning-based Models
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 无监督深度学习模型
- en: 'With the prosperous development of deep learning, researchers introduce neural
    networks to learn semantic representations of input documents and candidate phrases
    for ranking, of which studies can be roughly divided into the following four categories:
    Phrase-Document Similarity. The common practice is to measure the importance of
    each candidate phrase according to the phrase-document representation similarity.
    To do this, EmbedRank (Bennani-Smires et al., [2018](#bib.bib7)) uses Sent2Vec
    (Pagliardini et al., [2018](#bib.bib100)) and Doc2Vec (Le and Mikolov, [2014](#bib.bib63))
    to represent candidates and input documents as vectors. As an extension, $\text{EmbedRank}^{+}$
    additionally considers the similarities between candidates to generate diverse
    keyphrases. Unlike EmbedRank using Sent2vec and Doc2vec, SIFRank (Sun et al.,
    [2020](#bib.bib123)) defines the vector representations of candidates, sentences
    and input documents as weighted averages of their corresponding ELMo embeddings
    (Peters et al., [2018](#bib.bib104)), respectively. Further, $\text{SIFRank}^{+}$
    considers the positions of candidates within the document. Subsequently, Li and
    Daoutis ([2021](#bib.bib66)) improve SIFRank by incorporating domain relevance
    and phrase quality into ranking scores. Papagiannopoulou and Tsoumakas ([2018](#bib.bib101))
    use entire documents to learn Glove (Pennington et al., [2014](#bib.bib103)) embeddings,
    and then rank candidates according to the sum of word-document similarities.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的繁荣发展，研究人员引入神经网络来学习输入文档和候选短语的语义表示，这些研究大致可以分为以下四类：短语-文档相似度。常见的做法是根据短语-文档表示相似度来衡量每个候选短语的重要性。为此，EmbedRank（Bennani-Smires
    等，[2018](#bib.bib7)）使用 Sent2Vec（Pagliardini 等，[2018](#bib.bib100)）和 Doc2Vec（Le
    和 Mikolov，[2014](#bib.bib63)）将候选短语和输入文档表示为向量。作为扩展，$\text{EmbedRank}^{+}$ 还额外考虑候选短语之间的相似度，以生成多样化的关键短语。与使用
    Sent2vec 和 Doc2vec 的 EmbedRank 不同，SIFRank（Sun 等，[2020](#bib.bib123)）将候选短语、句子和输入文档的向量表示定义为其对应
    ELMo 嵌入（Peters 等，[2018](#bib.bib104)）的加权平均。进一步地，$\text{SIFRank}^{+}$ 考虑了候选短语在文档中的位置。随后，Li
    和 Daoutis（[2021](#bib.bib66)）通过将领域相关性和短语质量纳入排名评分来改进 SIFRank。Papagiannopoulou 和
    Tsoumakas（[2018](#bib.bib101)）使用整个文档来学习 Glove（Pennington 等，[2014](#bib.bib103)）嵌入，然后根据词-文档相似度的总和来对候选短语进行排名。
- en: Graph-based Ranking. Besides, researchers apply deep learning to refine the
    unsupervised models based on phrase graphs. For example, Key2Vec (Mahata et al.,
    [2018](#bib.bib80)) directly trains FastText to learn representations of candidate
    phrases and document themes, and then uses candidate-theme similarities to adjust
    the edge weights of PageRank. Similarly, Liang and Zaki ([2021](#bib.bib68)) consider
    the co-occurrence and similarities between candidates for more accurate edge weighting
    of PageRank. Using embedding-based graph, Asl and Banda ([2020](#bib.bib3)) apply
    PageRank or centrality algorithm to obtain the importance of candidates for ranking.
    Liang et al. ([2021](#bib.bib67)) find that the phrase-document representation
    similarity (i.e. EmbedRank) is insufficient to capture different contexts for
    keyphrase extraction. To address this issue, they define a boundary-aware centrality
    to capture local salient information and positional information of candidates
    for ranking.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的排序。除此之外，研究人员将深度学习应用于基于短语图的无监督模型的优化。例如，Key2Vec（Mahata 等人，[2018](#bib.bib80)）直接训练
    FastText 来学习候选短语和文档主题的表示，然后使用候选-主题相似性来调整 PageRank 的边权重。类似地，Liang 和 Zaki ([2021](#bib.bib68))
    考虑了候选短语之间的共现和相似性，以便更准确地进行 PageRank 的边权重调整。使用基于嵌入的图，Asl 和 Banda ([2020](#bib.bib3))
    应用 PageRank 或中心性算法来获得候选短语的排名重要性。Liang 等人 ([2021](#bib.bib67)) 发现短语-文档表示相似性（即 EmbedRank）不足以捕捉关键短语提取的不同上下文。为了解决这个问题，他们定义了一个边界感知的中心性，以捕捉局部显著信息和候选短语的位置性信息以进行排名。
- en: Semantic Importance of Keyphrases. Keyphrases play an important role in the
    representation learning of the input document. Thus, the representation of the
    input document will change if any keyphrase is missing. To model this intuition,
    Zhang et al. ([2021](#bib.bib156)) alternatively mask each candidate phrase and
    evaluate its importance according to the representation difference between the
    original document and the masked one. Recently, Joshi et al. ([2022](#bib.bib52))
    adopt a similar strategy that mainly focuses on the change of topic distributions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关键短语的语义重要性。关键短语在输入文档的表示学习中扮演着重要角色。因此，如果缺少任何关键短语，输入文档的表示将会发生变化。为了建模这种直觉，Zhang
    等人 ([2021](#bib.bib156)) 通过对每个候选短语进行替代性屏蔽，并根据原始文档和屏蔽文档之间的表示差异来评估其重要性。最近，Joshi
    等人 ([2022](#bib.bib52)) 采用了类似的策略，主要关注主题分布的变化。
- en: Attention Mechanism Information. Different from the above studies based on deep
    learning similarities, (Ding and Luo, [2021](#bib.bib29)) use self-attention weights
    to quantify the importance of each candidate phrase within the sentence and measure
    its semantic relatedness to the document according to its cross-attention weights.
    Additionally, Gu et al. ([2021](#bib.bib43)) generate pseudo keyphrases for unlabeled
    documents using unsupervised statistic models or an existing knowledge base, and
    then train a keyphrase classifier fed with the self-attention map from RoBERTa
    (Zhuang et al., [2021](#bib.bib167)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制信息。不同于上述基于深度学习相似性的研究，（Ding 和 Luo，[2021](#bib.bib29)）使用自注意力权重来量化每个候选短语在句子中的重要性，并根据其交叉注意力权重衡量其与文档的语义相关性。此外，Gu
    等人 ([2021](#bib.bib43)) 使用无监督统计模型或现有知识库为未标记文档生成伪关键短语，然后训练一个关键短语分类器，输入为 RoBERTa
    的自注意力图（Zhuang 等人，[2021](#bib.bib167)）。
- en: 2.3 Supervised Keyphrase Extraction
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 监督式关键短语提取
- en: Usually, supervised models for keyphrase extraction can be divided into statistical
    and deep learning-based models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，关键短语提取的监督模型可以分为统计模型和基于深度学习的模型。
- en: 2.3.1 Supervised Statistical Models
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 监督式统计模型
- en: Similar to unsupervised keyphrase extraction, abundant supervised statistical
    models leverage well-designed features, including statistical features (Witten
    et al., [1999](#bib.bib141); Turney, [2002](#bib.bib129); Kelleher and Luz, [2005](#bib.bib53);
    Haddoud and Abdeddaïm, [2014](#bib.bib46); Xie et al., [2017](#bib.bib147)), positional
    features (Frank et al., [1999](#bib.bib34); Medelyan and Witten, [2006](#bib.bib85);
    Zhang, [2008](#bib.bib154); Jiang et al., [2009](#bib.bib51)), linguistic features
    (Hulth, [2003](#bib.bib48); Gollapalli et al., [2017](#bib.bib40)), logical structures
    (Yih et al., [2006](#bib.bib152); Zhang et al., [2006](#bib.bib155); Nguyen and
    Kan, [2007](#bib.bib94); Nguyen and Luong, [2010](#bib.bib95)), and external document-based
    features (Shi et al., [2008](#bib.bib117); Medelyan et al., [2009](#bib.bib84);
    Lopez and Romary, [2010](#bib.bib73); Gollapalli and Caragea, [2014](#bib.bib39);
    Wang and Li, [2017](#bib.bib135); Zhang et al., [2017a](#bib.bib158)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督关键短语提取类似，丰富的监督统计模型利用了精心设计的特征，包括统计特征（Witten 等，[1999](#bib.bib141)；Turney，[2002](#bib.bib129)；Kelleher
    和 Luz，[2005](#bib.bib53)；Haddoud 和 Abdeddaïm，[2014](#bib.bib46)；Xie 等，[2017](#bib.bib147)），位置特征（Frank
    等，[1999](#bib.bib34)；Medelyan 和 Witten，[2006](#bib.bib85)；Zhang，[2008](#bib.bib154)；Jiang
    等，[2009](#bib.bib51)），语言特征（Hulth，[2003](#bib.bib48)；Gollapalli 等，[2017](#bib.bib40)），逻辑结构（Yih
    等，[2006](#bib.bib152)；Zhang 等，[2006](#bib.bib155)；Nguyen 和 Kan，[2007](#bib.bib94)；Nguyen
    和 Luong，[2010](#bib.bib95)），以及基于外部文档的特征（Shi 等，[2008](#bib.bib117)；Medelyan 等，[2009](#bib.bib84)；Lopez
    和 Romary，[2010](#bib.bib73)；Gollapalli 和 Caragea，[2014](#bib.bib39)；Wang 和 Li，[2017](#bib.bib135)；Zhang
    等，[2017a](#bib.bib158)）。
- en: Based on these features, researchers model keyphrase extraction as a sequence
    labeling task (Zhang, [2008](#bib.bib154); Gollapalli et al., [2017](#bib.bib40)),
    a binary classification task or a ranking task (Jiang et al., [2009](#bib.bib51);
    Zhang et al., [2017a](#bib.bib158)) with various machine learning algorithms,
    such as conditional random field (Zhang, [2008](#bib.bib154); Gollapalli et al.,
    [2017](#bib.bib40)), logistic regression (Yih et al., [2006](#bib.bib152); Shi
    et al., [2008](#bib.bib117); Haddoud and Abdeddaïm, [2014](#bib.bib46)), Naive
    Bayes (Witten et al., [1999](#bib.bib141); Frank et al., [1999](#bib.bib34); Kelleher
    and Luz, [2005](#bib.bib53); Medelyan and Witten, [2006](#bib.bib85); Nguyen and
    Kan, [2007](#bib.bib94); Nguyen and Luong, [2010](#bib.bib95); Caragea et al.,
    [2014](#bib.bib17); Xie et al., [2017](#bib.bib147)), SVM (Zhang et al., [2006](#bib.bib155)),
    bagged decision trees (Turney, [2002](#bib.bib129); Medelyan et al., [2009](#bib.bib84);
    Lopez and Romary, [2010](#bib.bib73)) and other ensemble models (Hulth, [2003](#bib.bib48);
    Wang and Li, [2017](#bib.bib135)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些特征，研究人员将关键短语提取建模为序列标注任务（Zhang，[2008](#bib.bib154)；Gollapalli 等，[2017](#bib.bib40)），二分类任务或排序任务（Jiang
    等，[2009](#bib.bib51)；Zhang 等，[2017a](#bib.bib158)），使用各种机器学习算法，如条件随机场（Zhang，[2008](#bib.bib154)；Gollapalli
    等，[2017](#bib.bib40)），逻辑回归（Yih 等，[2006](#bib.bib152)；Shi 等，[2008](#bib.bib117)；Haddoud
    和 Abdeddaïm，[2014](#bib.bib46)），朴素贝叶斯（Witten 等，[1999](#bib.bib141)；Frank 等，[1999](#bib.bib34)；Kelleher
    和 Luz，[2005](#bib.bib53)；Medelyan 和 Witten，[2006](#bib.bib85)；Nguyen 和 Kan，[2007](#bib.bib94)；Nguyen
    和 Luong，[2010](#bib.bib95)；Caragea 等，[2014](#bib.bib17)；Xie 等，[2017](#bib.bib147)），支持向量机（Zhang
    等，[2006](#bib.bib155)），袋装决策树（Turney，[2002](#bib.bib129)；Medelyan 等，[2009](#bib.bib84)；Lopez
    和 Romary，[2010](#bib.bib73)）以及其他集成模型（Hulth，[2003](#bib.bib48)；Wang 和 Li，[2017](#bib.bib135)）。
- en: '{forest}'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: for tree= grow’=east, forked edges, draw, rounded corners, node options= align=center
    , text width=2.7cm, anchor=west, [Automatic Keyphrase Generation, parent [ Paradigms,
    for tree=child [One2One [(Meng et al., [2017](#bib.bib88)), model]] [One2Seq[(Yuan
    et al., [2020](#bib.bib153)), model]] [One2Set[(Ye et al., [2021b](#bib.bib151)),
    model]] ] [Document Encoding, for tree=child[(Zhang et al., [2017b](#bib.bib159);
    Chen et al., [2019b](#bib.bib23); Zhao and Zhang, [2019](#bib.bib166); Wang et al.,
    [2019](#bib.bib138); Luo et al., [2020](#bib.bib76); Ahmad et al., [2021](#bib.bib1);
    Kim et al., [2021b](#bib.bib55)), model]] [Decoding Strategies, for tree=child[(Sun
    et al., [2019](#bib.bib124); Chen et al., [2020](#bib.bib22); Zhao et al., [2021](#bib.bib165);
    Liu et al., [2021](#bib.bib69); Wu et al., [2021](#bib.bib144), [2022b](#bib.bib145);
    Liu et al., [2021](#bib.bib69); Zhang et al., [2022](#bib.bib160)), model]] [Exploitation
    of External Information, for tree=child [(Diao et al., [2020](#bib.bib28); Chen
    et al., [2019a](#bib.bib21); Garg et al., [2021](#bib.bib37); Santosh et al.,
    [2021a](#bib.bib111); Ye et al., [2021a](#bib.bib150); Kim et al., [2021a](#bib.bib54);
    Wang et al., [2020c](#bib.bib139)), model]] [Solving Duplication and Coverage
    Issues of Generated Keyphrases, for tree=child[(Chen et al., [2018](#bib.bib20);
    Yuan et al., [2020](#bib.bib153); Bahuleyan and Asri, [2020](#bib.bib4); Chen
    et al., [2020](#bib.bib22)), model]] [Model Training Strategies, for tree=child
    [Reinforcement Learning, [(Chan et al., [2019](#bib.bib18); Luo et al., [2021](#bib.bib77)),
    model] ] [Generative Adversarial Networks, [(Swaminathan et al., [2020a](#bib.bib125),
    [b](#bib.bib126); Lancioni et al., [2020](#bib.bib62)), model]] [Multitask Learning,
    [(Ye and Wang, [2018](#bib.bib149); Chen et al., [2019a](#bib.bib21); Zhao and
    Zhang, [2019](#bib.bib166); Ahmad et al., [2021](#bib.bib1)), model]] ] [Application
    of Pre-trained Models, for tree=child [BART, [(Kulkarni et al., [2022](#bib.bib60);
    Chowdhury et al., [2022](#bib.bib25); Wu et al., [2022a](#bib.bib143), [b](#bib.bib145)),
    model]] [Others, [(Wu et al., [2021](#bib.bib144); Garg et al., [2021](#bib.bib37)),
    model]] ] [Low-resource Keyphrase Generation, for tree=child[(Ye and Wang, [2018](#bib.bib149);
    Shen et al., [2022](#bib.bib116)), model]] [Keyphrase Ranking, for tree=child[(Ye
    and Wang, [2018](#bib.bib149); Ni’mah et al., [2019](#bib.bib97); Chen et al.,
    [2019a](#bib.bib21); Shen et al., [2022](#bib.bib116)), model]] ] ]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: for tree= grow’=east, forked edges, draw, rounded corners, node options= align=center
    , text width=2.7cm, anchor=west, [自动关键短语生成, parent [范式, for tree=child [One2One
    [(Meng et al., [2017](#bib.bib88)), model]] [One2Seq[(Yuan et al., [2020](#bib.bib153)),
    model]] [One2Set[(Ye et al., [2021b](#bib.bib151)), model]] ] [文档编码, for tree=child[(Zhang
    et al., [2017b](#bib.bib159); Chen et al., [2019b](#bib.bib23); Zhao and Zhang,
    [2019](#bib.bib166); Wang et al., [2019](#bib.bib138); Luo et al., [2020](#bib.bib76);
    Ahmad et al., [2021](#bib.bib1); Kim et al., [2021b](#bib.bib55)), model]] [解码策略,
    for tree=child[(Sun et al., [2019](#bib.bib124); Chen et al., [2020](#bib.bib22);
    Zhao et al., [2021](#bib.bib165); Liu et al., [2021](#bib.bib69); Wu et al., [2021](#bib.bib144),
    [2022b](#bib.bib145); Liu et al., [2021](#bib.bib69); Zhang et al., [2022](#bib.bib160)),
    model]] [外部信息的利用, for tree=child [(Diao et al., [2020](#bib.bib28); Chen et al.,
    [2019a](#bib.bib21); Garg et al., [2021](#bib.bib37); Santosh et al., [2021a](#bib.bib111);
    Ye et al., [2021a](#bib.bib150); Kim et al., [2021a](#bib.bib54); Wang et al.,
    [2020c](#bib.bib139)), model]] [解决生成关键短语的重复性和覆盖性问题, for tree=child[(Chen et al.,
    [2018](#bib.bib20); Yuan et al., [2020](#bib.bib153); Bahuleyan and Asri, [2020](#bib.bib4);
    Chen et al., [2020](#bib.bib22)), model]] [模型训练策略, for tree=child [强化学习, [(Chan
    et al., [2019](#bib.bib18); Luo et al., [2021](#bib.bib77)), model] ] [生成对抗网络,
    [(Swaminathan et al., [2020a](#bib.bib125), [b](#bib.bib126); Lancioni et al.,
    [2020](#bib.bib62)), model]] [多任务学习, [(Ye and Wang, [2018](#bib.bib149); Chen
    et al., [2019a](#bib.bib21); Zhao and Zhang, [2019](#bib.bib166); Ahmad et al.,
    [2021](#bib.bib1)), model]] ] [预训练模型的应用, for tree=child [BART, [(Kulkarni et al.,
    [2022](#bib.bib60); Chowdhury et al., [2022](#bib.bib25); Wu et al., [2022a](#bib.bib143),
    [b](#bib.bib145)), model]] [其他, [(Wu et al., [2021](#bib.bib144); Garg et al.,
    [2021](#bib.bib37)), model]] ] [低资源关键短语生成, for tree=child[(Ye and Wang, [2018](#bib.bib149);
    Shen et al., [2022](#bib.bib116)), model]] [关键短语排序, for tree=child[(Ye and Wang,
    [2018](#bib.bib149); Ni’mah et al., [2019](#bib.bib97); Chen et al., [2019a](#bib.bib21);
    Shen et al., [2022](#bib.bib116)), model]] ] ]
- en: 'Figure 2: The Taxonomy of Representative Studies on Automatic Keyphrase Generation.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：自动关键短语生成的代表性研究分类。
- en: 2.3.2 Supervised Deep Learning-based Models
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 监督性深度学习模型
- en: Wang et al. ([2005](#bib.bib133)) first propose a feedforward neural network
    based classifier for supervised keyphrase extraction. Henceforth, deep learning-based
    supervised keyphrase extraction has gradually become one of the hot topics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Wang et al. ([2005](#bib.bib133)) 首次提出了一种基于前馈神经网络的监督性关键短语提取分类器。从此以后，基于深度学习的监督性关键短语提取逐渐成为热门话题。
- en: 'Sequence Labeling. Supervised keyphrase extraction is often modeled as a deep
    learning-based sequence labeling task. Typically, Zhang et al. ([2016](#bib.bib157))
    propose Joint-Layer RNN to extract keyphrases at different discrimination levels:
    judging whether the current word is a keyword and employing BIOES tagging scheme
    to identify keyphrases. Based on Joint-Layer RNN, Zhang et al. ([2018](#bib.bib161))
    introduce conversation context to enrich the vector representations of microblog
    posts. To simulate the human attention of reading during keyphrase annotating,
    Zhang and Zhang ([2019](#bib.bib162)) integrate an attention mechanism into Joint-Layer
    RNN. Meanwhile, researchers also explore more features for this model, such as
    medical concepts from an external knowledge base (Saputra et al., [2018](#bib.bib113)),
    phonetics, phonological features (Chowdhury et al., [2019](#bib.bib24)), and syntactical
    features (Mahfuzh et al., [2020](#bib.bib81)).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 序列标注。监督式关键短语提取通常被建模为基于深度学习的序列标注任务。通常，Zhang 等人 ([2016](#bib.bib157)) 提出了 Joint-Layer
    RNN，以在不同的判别层次上提取关键短语：判断当前词是否为关键词，并采用 BIOES 标注方案来识别关键短语。基于 Joint-Layer RNN，Zhang
    等人 ([2018](#bib.bib161)) 引入对话上下文以丰富微博帖子向量表示。为了模拟关键短语标注过程中阅读的人工注意力，Zhang 和 Zhang
    ([2019](#bib.bib162)) 将注意力机制集成到 Joint-Layer RNN 中。同时，研究人员还探索了更多特征，如来自外部知识库的医学概念（Saputra
    等人，[2018](#bib.bib113)）、语音学、音韵特征（Chowdhury 等人，[2019](#bib.bib24)）和句法特征（Mahfuzh
    等人，[2020](#bib.bib81)）。
- en: Also, applying pre-trained models to supervised keyphrase extraction has become
    dominant. For example, on the basis of SciBERT (Beltagy et al., [2019](#bib.bib5)),
    (Sahrawat et al., [2019](#bib.bib107)) and (Garg et al., [2020](#bib.bib36)) stack
    BiLSTM+CRF and LSTM+CRF to identify keyphrases, respectively. Using the same model,
    (Santosh et al., [2020a](#bib.bib109)) introduce a document-level attention and
    a gating mechanism to refine representation learning. Wang et al. ([2020b](#bib.bib137))
    separately leverage BERT and Transformer to encode the document and multi-modal
    information in web pages for keyphrase extraction. Gero and Ho ([2021](#bib.bib38))
    use BERT-LSTM or BioBERT-LSTM to obtain the topic representations of input documents,
    encouraging the extraction of topic-consistent words.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将预训练模型应用于监督式关键短语提取已成为主流。例如，基于 SciBERT（Beltagy 等人，[2019](#bib.bib5)），（Sahrawat
    等人，[2019](#bib.bib107)）和（Garg 等人，[2020](#bib.bib36)）分别堆叠 BiLSTM+CRF 和 LSTM+CRF
    来识别关键短语。使用相同的模型，（Santosh 等人，[2020a](#bib.bib109)）引入了文档级注意力和门控机制来优化表示学习。Wang 等人
    ([2020b](#bib.bib137)) 分别利用 BERT 和 Transformer 对文档和网页中的多模态信息进行编码，以提取关键短语。Gero
    和 Ho ([2021](#bib.bib38)) 使用 BERT-LSTM 或 BioBERT-LSTM 来获得输入文档的主题表示，鼓励提取主题一致的词。
- en: Different from these studies, Santosh et al. ([2020b](#bib.bib110)) utilize
    graph encoders to separately incorporate syntactic and semantic dependency information
    for better encoder representation. On the basis of the input document and the
    co-occurence graph, Nikzad-Khasmakhi et al. ([2021](#bib.bib96)) adopt BERT and
    graph embedding techniques to learn the word-level textual and structure representations,
    which are combined and fed into a sequence labeling tagger.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些研究不同，Santosh 等人 ([2020b](#bib.bib110)) 利用图编码器分别整合句法和语义依赖信息，以获得更好的编码器表示。在输入文档和共现图的基础上，Nikzad-Khasmakhi
    等人 ([2021](#bib.bib96)) 采用 BERT 和图嵌入技术来学习词级别的文本和结构表示，然后将这些表示组合并输入到序列标注器中。
- en: Binary Classification. Researchers also explore supervised keyphrase extraction
    as a binary classification task. Xiong et al. ([2019](#bib.bib148)) integrate
    the visual representation of the input document into ELMo word embeddings, and
    then use a convolutional Transformer to model interactions among candidate phrases
    for keyphrase classification. Besides, they introduce query prediction as a pre-training
    task. Prasad and Kan ([2019](#bib.bib105)) propose Glocal, an improved GCN, which
    incorporates the global importance of each node relative to other nodes to learn
    word representations from a word graph. Based on these representations, keywords
    are identified via classification and finally used to reconstruct keyphrases via
    re-ranking.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类。研究人员还探讨了将监督式关键短语提取作为一个二分类任务。Xiong 等人 ([2019](#bib.bib148)) 将输入文档的视觉表示集成到
    ELMo 词嵌入中，然后使用卷积 Transformer 来建模候选短语之间的交互，以进行关键短语分类。此外，他们引入了查询预测作为预训练任务。Prasad
    和 Kan ([2019](#bib.bib105)) 提出了 Glocal，一个改进的 GCN，它结合了每个节点相对于其他节点的全局重要性，以从词图中学习词表示。基于这些表示，关键词通过分类识别，并最终通过重新排序来重建关键短语。
- en: 'Ranking. Sarkar et al. ([2010](#bib.bib114)) first apply a deep learning-based
    ranking model to achieve supervised keyphrase extraction. Mu et al. ([2020](#bib.bib90))
    use BERT stacked with BiLSTM to model semantic interactions among candidate phrases,
    and then rank them according to the binary classification score and the hinge
    loss between the considered phrase and others. Sun et al. ([2021](#bib.bib122))
    propose JointKPE that learns to rank candidate phrases according to their document-level
    informativeness. Particularly, it is jointly trained with keyphrase chunking to
    guarantee the phraseness of candidates. Song et al. ([2021](#bib.bib119)) investigate
    three kinds of features for ranking: the syntactic accuracy of the candidate phrase,
    the information saliency between the candidate and input document, and the concept
    consistency between the candidate and the input document.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 排名。Sarkar 等人 ([2010](#bib.bib114)) 首次应用基于深度学习的排名模型来实现监督式关键词提取。Mu 等人 ([2020](#bib.bib90))
    使用 BERT 叠加 BiLSTM 来建模候选短语之间的语义交互，然后根据二分类分数和考虑短语与其他短语之间的铰链损失进行排名。Sun 等人 ([2021](#bib.bib122))
    提出了 JointKPE，该模型学习根据候选短语的文档级信息量进行排名。特别地，它与关键词短语分块共同训练，以保证候选短语的短语性。Song 等人 ([2021](#bib.bib119))
    调查了三种用于排名的特征：候选短语的句法准确性、候选短语与输入文档之间的信息显著性，以及候选短语与输入文档之间的概念一致性。
- en: Data Utilization. Based on a word graph, Luan et al. ([2017](#bib.bib75)) employ
    label propagation together with a data selection scheme to leverage unlabeled
    documents. Lai et al. ([2020](#bib.bib61)) propose a self-distillation model for
    keyphrase extraction. In this approach, a teacher model is trained on labeled
    examples, while a student model is trained on both labeled examples and pseudo
    examples generated by the teacher model. During the subsequent training procedure,
    the teacher model is re-initialized with the student model and repeats the above
    procedure. To address the issue of incomplete annotated training data, Lei et al.
    ([2021](#bib.bib64)) introduce negative sampling to adjust the training loss on
    unlabeled data. From a different perspective, Kontoulis et al. ([2021](#bib.bib57))
    believe that full-texts can provide richer information while containing more noise
    than the input abstract. Thus, they leverage summaries induced from full-texts
    to refine keyphrase extraction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据利用。基于词图，Luan 等人 ([2017](#bib.bib75)) 采用标签传播结合数据选择方案来利用未标记文档。Lai 等人 ([2020](#bib.bib61))
    提出了一个用于关键词提取的自蒸馏模型。在这种方法中，教师模型在标记样本上进行训练，而学生模型则在标记样本和由教师模型生成的伪样本上进行训练。在随后的训练过程中，教师模型会用学生模型重新初始化，并重复上述过程。为了应对标注训练数据不完整的问题，Lei
    等人 ([2021](#bib.bib64)) 引入了负采样来调整未标记数据上的训练损失。从不同的角度来看，Kontoulis 等人 ([2021](#bib.bib57))
    认为全文可以提供更丰富的信息，但包含比输入摘要更多的噪声。因此，他们利用从全文中引导的摘要来优化关键词提取。
- en: 3 Automatic Keyphrase Generation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 自动关键词生成
- en: Unlike the studies on keyphrase extraction, keyphrase generation models can
    produce absent keyphrases that do not appear in the input document. In this respect,
    Meng et al. ([2017](#bib.bib88)) propose the first keyphrase generation model,
    CopyRNN, which inspires many subsequent models. Usually, these models are based
    on an encoder-decoder framework, where the encoder learns the semantic representation
    of each input document, and then the decoder equipped with a copying mechanism
    (Gu et al., [2016](#bib.bib42)) automatically produces keyphrases.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与关键词提取研究不同，关键词生成模型可以生成输入文档中未出现的缺失关键词。在这方面，Meng 等人 ([2017](#bib.bib88)) 提出了第一个关键词生成模型
    CopyRNN，这一模型激发了许多后续模型。通常，这些模型基于编码器-解码器框架，其中编码器学习每个输入文档的语义表示，然后配备有复制机制的解码器 (Gu
    等人, [2016](#bib.bib42)) 自动生成关键词。
- en: 'In the following subsections, we summarize representative advancements of keyphrase
    generation according to different investigated problems. The taxonomy of representative
    studies on automatic keyphrase generation is shown in Figure [2](#S2.F2.fig1 "Figure
    2 ‣ 2.3.1 Supervised Statistical Models ‣ 2.3 Supervised Keyphrase Extraction
    ‣ 2 Automatic Keyphrase Extraction ‣ From Statistical Methods to Deep Learning,
    Automatic Keyphrase Prediction: A Survey").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们总结了针对不同研究问题的关键词生成代表性进展。代表性研究的自动关键词生成分类如图[2](#S2.F2.fig1 "图 2 ‣ 2.3.1
    监督统计模型 ‣ 2.3 监督关键词提取 ‣ 2 自动关键词提取 ‣ 从统计方法到深度学习，自动关键词预测：综述")所示。
- en: '![Refer to caption](img/3e4f450a97aec0fb9ccffdcaad86923c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/3e4f450a97aec0fb9ccffdcaad86923c.png)'
- en: (a) One2One
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (a) One2One
- en: '![Refer to caption](img/be8d244060fd04ba39afb2838dd39de7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/be8d244060fd04ba39afb2838dd39de7.png)'
- en: (b) One2Seq
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (b) One2Seq
- en: '![Refer to caption](img/d3e42364e44a8216810adc9e922b9be6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d3e42364e44a8216810adc9e922b9be6.png)'
- en: (c) One2Set
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (c) One2Set
- en: 'Figure 3: The three dominant paradigms for keyphrase generation.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：关键短语生成的三种主流范式。
- en: 3.1 Paradigms
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 范式
- en: 'Generally, paradigms of dominant keyphrase generation models can be classified
    into One2One (Meng et al., [2017](#bib.bib88)), One2Seq (Yuan et al., [2020](#bib.bib153))
    and One2Set (Ye et al., [2021b](#bib.bib151)), as shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3 Automatic Keyphrase Generation ‣ From Statistical Methods to Deep Learning,
    Automatic Keyphrase Prediction: A Survey").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，主流关键短语生成模型的范式可以分为 One2One（Meng et al., [2017](#bib.bib88)）、One2Seq（Yuan et
    al., [2020](#bib.bib153)）和 One2Set（Ye et al., [2021b](#bib.bib151)），如图 [3](#S3.F3
    "图 3 ‣ 3 自动关键短语生成 ‣ 从统计方法到深度学习，自动关键短语预测：综述")所示。
- en: One2One. Typically, during model training, each training instance contains an
    input document and only one corresponding keyphrase from the splitted target keyphrases.
    During inference, One2One models adopt beam search to produce candidate phrases
    and then pick the top-K ranked ones as the final keyphrases.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: One2One。通常，在模型训练过程中，每个训练实例包含一个输入文档和来自拆分目标关键短语中的唯一一个相应关键短语。在推理过程中，One2One 模型采用束搜索生成候选短语，然后选择排名前
    K 的短语作为最终关键短语。
- en: As the earliest paradigm, it has a far-reaching impact but neglects the correlation
    among keyphrases, limiting the potential of keyphrase generation models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最早的范式，它具有深远的影响，但忽略了关键短语之间的相关性，限制了关键短语生成模型的潜力。
- en: One2Seq. To deal with the above issue, the One2Seq paradigm models keyphrase
    generation as a sequence generation task. To this end, target keyphrases are sorted
    in a predefined order and concated as a sequence with delimiters. Usually, present
    keyphrases are firstly sorted according to their occurrence, while absent keyphrases
    are then randomly sorted (Meng et al., [2019](#bib.bib86), [2021](#bib.bib87)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: One2Seq。为了处理上述问题，One2Seq 范式将关键短语生成建模为序列生成任务。为此，目标关键短语按照预定义的顺序排序，并用分隔符连接成一个序列。通常，当前的关键短语首先按照其出现频率排序，而缺失的关键短语则随机排序（Meng
    et al., [2019](#bib.bib86), [2021](#bib.bib87)）。
- en: Due to the advantage of exploiting the semantic interdependence between keyphrases,
    One2Seq has become the most commonly-used paradigm. However, its premise of a
    predefined order introduces a bias into model training, especially when the order
    of generated keyphrases is inconsistent with the predefined one. Besides, One2Seq
    models tend to generate duplicated keyphrases (Chen et al., [2020](#bib.bib22);
    Ye et al., [2021b](#bib.bib151)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于利用关键短语之间语义相互依赖的优势，One2Seq 成为最常用的范式。然而，它的预定义顺序的前提给模型训练引入了偏差，特别是当生成的关键短语的顺序与预定义的顺序不一致时。此外，One2Seq
    模型往往会生成重复的关键短语（Chen et al., [2020](#bib.bib22); Ye et al., [2021b](#bib.bib151)）。
- en: One2Set. Furthermore, to address the above bias defect of One2Seq, Ye et al.
    ([2021b](#bib.bib151)) propose One2Set, where the keyphrase generation is modeled
    as a set generation task. Typically, its decoder utilizes different learnable
    control codes to generate a set of keyphrases in parallel. During model training,
    the training loss is calculated according to the one-to-one alignments between
    the predicted keyphrases and target ones determined by the Hungarian Algorithm
    (Kuhn, [1955](#bib.bib59)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: One2Set。此外，为了应对 One2Seq 的偏差缺陷，Ye et al. ([2021b](#bib.bib151)) 提出了 One2Set，其中关键短语生成被建模为集合生成任务。通常，其解码器利用不同的可学习控制代码来并行生成一组关键短语。在模型训练过程中，训练损失是根据预测的关键短语与目标关键短语之间的一个对一个对齐关系计算的，这一关系由匈牙利算法确定（Kuhn,
    [1955](#bib.bib59)）。
- en: 3.2 Document Encoding
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 文档编码
- en: Typically, CopyRNN (Meng et al., [2017](#bib.bib88)) adopts RNN as its encoder
    and thus suffers from low efficiency when handling long documents. To solve this
    problem, Zhang et al. ([2017b](#bib.bib159)) replace RNN with CNN to boost encoding
    efficiency.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CopyRNN（Meng et al., [2017](#bib.bib88)）采用 RNN 作为其编码器，因此在处理长文档时效率较低。为了解决这个问题，Zhang
    et al.（[2017b](#bib.bib159)）用 CNN 替代 RNN，以提高编码效率。
- en: Besides, some researchers argue that sentences should be treated differently
    due to their unequal importance in document encoding. Chen et al. ([2019b](#bib.bib23))
    design Title-Guided Network, which additionally uses the title as a query to gather
    the information of title-relevant words in the input document. Kim et al. ([2021b](#bib.bib55))
    takes into account useful structures of web documents such as title, body, header,
    query, to build a word graph representing both position-based proximity and structural
    relations. Luo et al. ([2020](#bib.bib76)) use a selection network to filter unimportant
    sentences, while Ahmad et al. ([2021](#bib.bib1)) apply this network to adjust
    the weights of the decoder copying mechanism.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究者认为由于句子在文档编码中的重要性不同，应该对其进行不同处理。陈等人（[2019b](#bib.bib23)）设计了**标题引导网络**，该网络额外使用标题作为查询，以收集输入文档中与标题相关的词汇信息。金等人（[2021b](#bib.bib55)）考虑了网页文档中的有用结构，如标题、正文、头部、查询，以构建一个词图，表示基于位置的接近性和结构关系。罗等人（[2020](#bib.bib76)）使用选择网络来过滤不重要的句子，而艾哈迈德等人（[2021](#bib.bib1)）将此网络应用于调整解码器复制机制的权重。
- en: Meanwhile, researchers also focus on incorporating more information into the
    encoder. For instance, Zhao and Zhang ([2019](#bib.bib166)) explore linguistic
    information for document encoding. To alleviate data sparsity in social media,
    Wang et al. ([2019](#bib.bib138)) apply a variational neural network to incorporate
    topic information into the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，研究人员也专注于将更多信息融入编码器。例如，赵和张（[2019](#bib.bib166)）探索了文档编码中的语言信息。为了缓解社交媒体中的数据稀疏问题，王等人（[2019](#bib.bib138)）应用了变分神经网络，将主题信息融入模型中。
- en: 3.3 Decoding Strategies
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 解码策略
- en: Unlike the conventional decoder that can predict both present and absent keyphrases,
    Sun et al. ([2019](#bib.bib124)) propose a diversified Pointer Network decoder
    for the One2One paradigm, which only copies a set of diverse present keyphrases.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统解码器可以预测当前和缺失的关键短语不同，孙等人（[2019](#bib.bib124)）提出了一种多样化的指针网络解码器，用于One2One范式，该解码器只复制一组多样化的当前关键短语。
- en: Meanwhile, more researchers focus on refining the decoding manners under One2Seq
    paradigm. For example, Chen et al. ([2020](#bib.bib22)) propose an exclusive hierarchical
    decoder that involves two levels of decoding to exploit the phrase-level and word-level
    correlation for keyphrase generation. Similarly, Santosh et al. ([2021b](#bib.bib112))
    model the above-mentioned hierarchical structure by incorporating a conditional
    variational autoencoder. Besides, Zhang et al. ([2022](#bib.bib160)) propose a
    hierarchical topic-guided variational neural network by integrating the hierarchical
    topic information to guide the keyphrases generation. Some researchers argue that
    uniformly modeling the generation of present and absent keyphrases is unreasonable,
    since their prediction difficulties are significantly different. Zhao et al. ([2021](#bib.bib165))
    propose a Select-Guide-Generate decoding strategy, which firstly selects present
    keyphrases from the input document and then exploits these keyphrases to guide
    the generation of absent ones. Similarly, Liu et al. ([2021](#bib.bib69)) first
    fine-tune a BERT-based model to identify present keyphrases from the input document,
    and then utilize the BERT, which fully encodes the knowledge of present keyphrases,
    to benefit the generation of absent ones. Wu et al. ([2021](#bib.bib144)) jointly
    train present keyphrase extraction and absent keyphrase generation, exploiting
    their mutual relation via stacker relation layer and bag-of-words constraints.
    Very recently, Wu et al. ([2022b](#bib.bib145)) propose a mask-predict decoder
    to explore constrained and non-autoregressive generation for absent keyphrase
    generation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，更多的研究者集中于在 One2Seq 模式下优化解码方式。例如，Chen et al. ([2020](#bib.bib22)) 提出了一个独特的层次解码器，它涉及两个解码层次，以利用短语级和词级的关联来生成关键短语。类似地，Santosh
    et al. ([2021b](#bib.bib112)) 通过结合条件变分自编码器对上述层次结构进行了建模。此外，Zhang et al. ([2022](#bib.bib160))
    提出了一个层次主题引导的变分神经网络，通过整合层次主题信息来引导关键短语生成。一些研究者认为，统一建模存在和缺失关键短语的生成是不合理的，因为它们的预测难度显著不同。Zhao
    et al. ([2021](#bib.bib165)) 提出了一个选择-引导-生成解码策略，该策略首先从输入文档中选择存在的关键短语，然后利用这些关键短语来引导缺失关键短语的生成。类似地，Liu
    et al. ([2021](#bib.bib69)) 首先对基于 BERT 的模型进行微调，以识别输入文档中的存在关键短语，然后利用 BERT，充分编码存在关键短语的知识，以促进缺失关键短语的生成。Wu
    et al. ([2021](#bib.bib144)) 联合训练存在的关键短语提取和缺失关键短语生成，通过堆叠关系层和词袋约束来利用它们的相互关系。最近，Wu
    et al. ([2022b](#bib.bib145)) 提出了一个掩码预测解码器，以探索约束性和非自回归生成缺失关键短语。
- en: 3.4 Model Training Strategies
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 模型训练策略
- en: Chan et al. ([2019](#bib.bib18)) propose a reinforcement learning (RL) approach
    with an adaptive reward for keyphrase generation. If the model does not generate
    enough keyphrases, the reward is defined as the recall score that encourages the
    model to generate enough keyphrases. Otherwise, the $F_{1}$ score is used as the
    reward to prevent the model from over-generating incorrect keyphrases. To ease
    the synonym problem, Luo et al. ([2021](#bib.bib77)) further improve the RL reward
    function by considering word-level $F_{1}$ score, edit distance, duplication rate,
    and generation quantity.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Chan et al. ([2019](#bib.bib18)) 提出了一个具有自适应奖励的强化学习 (RL) 方法来生成关键短语。如果模型没有生成足够的关键短语，则奖励定义为召回率，以鼓励模型生成足够的关键短语。否则，$F_{1}$
    分数被用作奖励，以防止模型过度生成不正确的关键短语。为了缓解同义词问题，Luo et al. ([2021](#bib.bib77)) 通过考虑词级 $F_{1}$
    分数、编辑距离、重复率和生成数量进一步改进了 RL 奖励函数。
- en: Besides, researchers apply generative adversarial networks to the keyphrase
    generation task (Swaminathan et al., [2020a](#bib.bib125), [b](#bib.bib126); Lancioni
    et al., [2020](#bib.bib62)), where the generator is trained to produce accurate
    keyphrases and the discriminator is expected to distinguish machine-generated
    and human-curated keyphrases.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员将生成对抗网络应用于关键短语生成任务 (Swaminathan et al., [2020a](#bib.bib125), [b](#bib.bib126);
    Lancioni et al., [2020](#bib.bib62))，其中生成器被训练以生成准确的关键短语，而判别器则期望区分机器生成的和人工筛选的关键短语。
- en: Many researchers apply multitask model to the keyphrase generation task (Chen
    et al., [2019a](#bib.bib21); Ahmad et al., [2021](#bib.bib1)). Typically, Ye and
    Wang ([2018](#bib.bib149)) jointly train keyphrase generation and title generation
    to improve the generalization ability of the model. Similarly, Zhao and Zhang
    ([2019](#bib.bib166)) introduce POS tagging as an auxiliary task of keyphrase
    generation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员将多任务模型应用于关键短语生成任务 (Chen 等人，[2019a](#bib.bib21); Ahmad 等人，[2021](#bib.bib1))。通常，Ye
    和 Wang ([2018](#bib.bib149)) 联合训练关键短语生成和标题生成，以提高模型的泛化能力。同样，Zhao 和 Zhang ([2019](#bib.bib166))
    将 POS 标注引入作为关键短语生成的辅助任务。
- en: 3.5 Exploitation of External Information
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 外部信息的利用
- en: Inspired by the studies of other NLP tasks (Liu et al., [2018](#bib.bib72);
    Wang et al., [2018](#bib.bib132); Zhang et al., [2019](#bib.bib164)), researchers
    explore the information beyond input documents to generate better keyphrases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 受到其他 NLP 任务研究的启发 (Liu 等人，[2018](#bib.bib72); Wang 等人，[2018](#bib.bib132); Zhang
    等人，[2019](#bib.bib164))，研究人员探索了输入文档之外的信息以生成更好的关键短语。
- en: In this regard, Diao et al. ([2020](#bib.bib28)) employ a cross-document attention
    to leverage similar documents for better document encoding. Garg et al. ([2021](#bib.bib37))
    explore numerous ways to incorporate additional data for keyphrase generation
    and find that the summary of the article is the most beneficial. Besides, researchers
    consider the keyphrases of similar documents. Chen et al. ([2019a](#bib.bib21))
    leverage the retrieved keyphrases from similar documents to guide the keyphrase
    generation and re-ranking. Santosh et al. ([2021a](#bib.bib111)) also collect
    additional keyphrases from similar documents to automatically form a gazetteer,
    which is used to enrich the vocabulary for improving keyphrase generation. To
    exploit both similar documents and their keyphrases, Ye et al. ([2021a](#bib.bib150))
    construct a heterogeneous keyword-document graph model, which is equipped with
    a reference-aware decoder to copy words from the input document and its similar
    ones. To deal with the data without title, Kim et al. ([2021a](#bib.bib54)) construct
    a structure graph using the input document and its related but absent keyphrases
    retrieved from other documents. This graph can provide structure-aware representations
    for better keyphrase generation. Besides, Wang et al. ([2020c](#bib.bib139)) utilize
    the rich features embedded in the matching images to explore the joint effects
    of texts and images for keyphrase prediction.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Diao 等人 ([2020](#bib.bib28)) 使用跨文档注意力机制来利用相似文档以更好地进行文档编码。Garg 等人 ([2021](#bib.bib37))
    探索了多种方法来将额外数据纳入关键短语生成，并发现文章的摘要是最有益的。此外，研究人员还考虑了相似文档的关键短语。Chen 等人 ([2019a](#bib.bib21))
    利用从相似文档中检索到的关键短语来指导关键短语的生成和重新排序。Santosh 等人 ([2021a](#bib.bib111)) 还从相似文档中收集额外的关键短语，以自动形成词典，用于丰富词汇量以提高关键短语生成效果。为了利用相似文档及其关键短语，Ye
    等人 ([2021a](#bib.bib150)) 构建了一个异质关键词-文档图模型，该模型配备了一个参考感知解码器，用于从输入文档及其相似文档中复制词语。为了处理没有标题的数据，Kim
    等人 ([2021a](#bib.bib54)) 使用输入文档和从其他文档中检索到的相关但缺失的关键短语构建结构图。这个图可以提供结构感知的表示，以便更好地生成关键短语。此外，Wang
    等人 ([2020c](#bib.bib139)) 利用匹配图像中嵌入的丰富特征来探索文本和图像的联合效应以进行关键短语预测。
- en: 3.6 Solving Duplication and Coverage Issues of Generated Keyphrases
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 解决生成的关键短语的重复和覆盖问题
- en: Chen et al. ([2018](#bib.bib20)) point out that the One2One paradigm neglects
    the correlation among keyphrases, leading to duplication and coverage issues of
    generated keyphrases. To solve these issues, they propose CoryRNN that reviews
    preceding keyphrases to eliminate duplicates, and utilizes the coverage mechanism
    (Tu et al., [2016](#bib.bib128)) to improve the coverage for keyphrases.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 ([2018](#bib.bib20)) 指出，One2One 模式忽略了关键短语之间的相关性，导致生成的关键短语出现重复和覆盖问题。为了解决这些问题，他们提出了
    CoryRNN，它回顾前面的关键短语以消除重复，并利用覆盖机制 (Tu 等人，[2016](#bib.bib128)) 来提高关键短语的覆盖率。
- en: The One2Seq paradigm has the same issues, which become more serious when generating
    long keyphrase sequences. To deal with this defect, Yuan et al. ([2020](#bib.bib153))
    employ orthogonal regularization to explicitly distinguish the delimiter-generated
    hidden states, so as to improve the diversity of generated keyphrases. Bahuleyan
    and Asri ([2020](#bib.bib4)) use an unlikelihood training loss to produce diverse
    keyphrases. Along this line, Chen et al. ([2020](#bib.bib22)) explore not only
    an training strategy with an exclusive loss, but also an exclusive search strategy
    to avoid generating duplicate keyphrases. In this way, the model is encouraged
    to generate keyphrases with different first words.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: One2Seq 模型面临相同的问题，当生成长关键短语序列时问题会更加严重。为了解决这一缺陷，Yuan 等人（[2020](#bib.bib153)）采用了正交正则化来明确区分分隔符生成的隐藏状态，从而提高生成关键短语的多样性。Bahuleyan
    和 Asri（[2020](#bib.bib4)）使用不可能性训练损失来生成多样化的关键短语。沿着这个方向，Chen 等人（[2020](#bib.bib22)）不仅探索了具有排他性损失的训练策略，还探索了排他性搜索策略以避免生成重复的关键短语。通过这种方式，模型被鼓励生成具有不同首词的关键短语。
- en: 3.7 Low-resource Keyphrase Generation
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 低资源关键短语生成
- en: The performance of keyphrase generation models deeply depends on the quantity
    and quality of training data. Unfortunately, the commonly-used labeled datasets
    are often relatively small, making low-resource keyphrase generation a realistic
    and valuable research direction
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 关键短语生成模型的性能深受训练数据的数量和质量的影响。不幸的是，常用的标注数据集通常相对较小，使得低资源关键短语生成成为一个现实且有价值的研究方向。
- en: Ye and Wang ([2018](#bib.bib149)) propose a semi-supervised model that first
    generates pseudo keyphrases for unlabeled documents and then use them as incremental
    training data. Besides, Shen et al. ([2022](#bib.bib116)) use unsupervised extraction
    models to collect keyphrases and then draw pesudo keyphrases for each document
    based on lexical and semantic level similarities. Finally, the pesudo absent keyphrases
    are used to train and update the model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Ye 和 Wang（[2018](#bib.bib149)）提出了一种半监督模型，该模型首先为未标记的文档生成伪关键短语，然后将其作为增量训练数据。此外，Shen
    等人（[2022](#bib.bib116)）使用无监督提取模型收集关键短语，然后根据词汇和语义层级的相似性为每个文档生成伪关键短语。最后，伪缺失关键短语用于训练和更新模型。
- en: Recently, due to pre-trained models contain abundant knowledge that may benefit
    keyphrase generation, keyphrase generation based on pre-trained models have received
    a rising interest. In this respect, Wu et al. ([2021](#bib.bib144)) first introduce
    the pre-trained model UniLM (Dong et al., [2019](#bib.bib30)) into keyphrase generation.
    Additionally, Garg et al. ([2021](#bib.bib37)) utilize Longformer (Beltagy et al.,
    [2020](#bib.bib6)) to deal with the keyphrase generation for long documents. Besides,
    BART (Lewis et al., [2020](#bib.bib65)), a denoising self-supervised autoencoder,
    is extensively applied due to its great potential in text generation tasks. For
    instance, Chowdhury et al. ([2022](#bib.bib25)) directly construct an One2Seq
    model based on the fine-tuned BART. Kulkarni et al. ([2022](#bib.bib60)) propose
    KeyBART, which uses boundary tokens and position embeddings to predict the masked
    keyphrase and then determine whether a keyphrase is replaced or retained. In addition
    to the above masked keyphrase prediction, Wu et al. ([2022a](#bib.bib143)) introduce
    salient span recovery to fine-tune BART for learning better intermediate representations.
    Wu et al. ([2022b](#bib.bib145)) apply a prompt-based learning approach for constrained
    absent keyphrase generation. They firstly define overlapping words between absent
    keyphrase and document as keywords, and then use a mask-predict decoder to generate
    the final absent keyphrase under the constraints of prompt.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于预训练模型包含丰富的知识，这些知识可能对关键短语生成有帮助，因此基于预训练模型的关键短语生成受到了越来越多的关注。在这方面，Wu 等人（[2021](#bib.bib144)）首次将预训练模型
    UniLM（Dong 等人，[2019](#bib.bib30)）引入关键短语生成。此外，Garg 等人（[2021](#bib.bib37)）利用 Longformer（Beltagy
    等人，[2020](#bib.bib6)）处理长文档的关键短语生成。此外，BART（Lewis 等人，[2020](#bib.bib65)），一种去噪自监督自动编码器，由于其在文本生成任务中的巨大潜力而被广泛应用。例如，Chowdhury
    等人（[2022](#bib.bib25)）直接基于微调后的 BART 构建了 One2Seq 模型。Kulkarni 等人（[2022](#bib.bib60)）提出了
    KeyBART，它使用边界标记和位置嵌入来预测被遮盖的关键短语，然后确定关键短语是被替换还是保留。除了上述被遮盖的关键短语预测外，Wu 等人（[2022a](#bib.bib143)）引入了显著跨度恢复以微调
    BART，以学习更好的中间表示。Wu 等人（[2022b](#bib.bib145)）应用基于提示的学习方法来约束缺失关键短语的生成。他们首先将缺失关键短语与文档之间的重叠词定义为关键词，然后使用掩码预测解码器在提示的约束下生成最终的缺失关键短语。
- en: 3.8 Keyphrase Ranking
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 关键短语排名
- en: Due to the property of beam search, One2One models tend to select short phrases.
    To deal with this issue, Ni’mah et al. ([2019](#bib.bib97)) introduce word-level
    and ngram-level attention scores to boost the ranking scores of long keyphrases.
    Besides, Shen et al. ([2022](#bib.bib116)) combine the TF-IDF relatedness and
    embedding-based keyphrase-document cosine similarity to rank phrases. When reranking
    phrases, Chen et al. ([2019a](#bib.bib21)) also consider phrases retrieved from
    similar documents and phrases extracted from documents. In addition, Ye and Wang
    ([2018](#bib.bib149)) apply beam search into an One2Seq paradigm based model,
    which generates multiple candidate phrase sequences and then collect unique keyphrases
    from the top-ranked beams in descending order.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于束搜索的特性，One2One模型倾向于选择短语。为了解决这个问题，Ni’mah等人（[2019](#bib.bib97)）引入了词级和n-gram级别的注意力分数来提升长关键短语的排名分数。此外，Shen等人（[2022](#bib.bib116)）结合了TF-IDF相关性和基于嵌入的关键短语-文档余弦相似度来排序短语。在重新排序短语时，Chen等人（[2019a](#bib.bib21)）也考虑了从相似文档中检索的短语和从文档中提取的短语。此外，Ye和Wang（[2018](#bib.bib149)）将束搜索应用于One2Seq范式模型，该模型生成多个候选短语序列，然后从按降序排列的前-ranked束中收集独特的关键短语。
- en: 4 Datasets
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集
- en: 'Table 3: The commonly-used datasets for keyphrase predictions.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：用于关键短语预测的常用数据集。
- en: '| Dataset | Domain | Language | Docs |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 领域 | 语言 | 文档 |'
- en: '| Inspec (Hulth, [2003](#bib.bib48)) | Papers | EN | 2.0K |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Inspec (Hulth, [2003](#bib.bib48)) | 论文 | 英语 | 2.0K |'
- en: '| NUS (Nguyen and Kan, [2007](#bib.bib94)) | Papers | EN | 211 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| NUS (Nguyen and Kan, [2007](#bib.bib94)) | 论文 | 英语 | 211 |'
- en: '| PubMed (Schutz et al., [2008](#bib.bib115)) | Papers | EN | 1.3K |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| PubMed (Schutz et al., [2008](#bib.bib115)) | 论文 | 英语 | 1.3K |'
- en: '| Krapivin (Krapivin et al., [2009](#bib.bib58)) | Papers | EN | 2.3K |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Krapivin (Krapivin et al., [2009](#bib.bib58)) | 论文 | 英语 | 2.3K |'
- en: '| Citeulike-180 (Medelyan et al., [2009](#bib.bib84)) | Papers | EN | 181 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Citeulike-180 (Medelyan et al., [2009](#bib.bib84)) | 论文 | 英语 | 181 |'
- en: '| SemEval-2010 (Kim et al., [2010](#bib.bib56)) | Papers | EN | 244 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SemEval-2010 (Kim et al., [2010](#bib.bib56)) | 论文 | 英语 | 244 |'
- en: '| TALN (Boudin, [2013](#bib.bib10)) | Papers | EN/FR | 521/1.2K |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| TALN (Boudin, [2013](#bib.bib10)) | 论文 | 英语/法语 | 521/1.2K |'
- en: '| KDD (Gollapalli and Caragea, [2014](#bib.bib39)) | Papers | EN | 755 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| KDD (Gollapalli and Caragea, [2014](#bib.bib39)) | 论文 | 英语 | 755 |'
- en: '| WWW (Gollapalli and Caragea, [2014](#bib.bib39)) | Papers | EN | 1.3K |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| WWW (Gollapalli and Caragea, [2014](#bib.bib39)) | 论文 | 英语 | 1.3K |'
- en: '| TermLTH-Eval (Bougouin et al., [2016](#bib.bib13)) | Papers | FR | 400 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| TermLTH-Eval (Bougouin et al., [2016](#bib.bib13)) | 论文 | 法语 | 400 |'
- en: '| KP20k (Meng et al., [2017](#bib.bib88)) | Papers | EN | 567.8K |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| KP20k (Meng et al., [2017](#bib.bib88)) | 论文 | 英语 | 567.8K |'
- en: '| LDPK3K (Mahata et al., [2022](#bib.bib79)) | Papers | EN | 96.8K |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| LDPK3K (Mahata et al., [2022](#bib.bib79)) | 论文 | 英语 | 96.8K |'
- en: '| LDPK10K (Mahata et al., [2022](#bib.bib79)) | Papers | EN | 1.3M |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LDPK10K (Mahata et al., [2022](#bib.bib79)) | 论文 | 英语 | 1.3M |'
- en: '| DUC (Wan and Xiao, [2008](#bib.bib131)) | News | EN | 308 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DUC (Wan and Xiao, [2008](#bib.bib131)) | 新闻 | 英语 | 308 |'
- en: '| 110-PT-BN-KP (Marujo et al., [2011](#bib.bib83)) | News | PT | 110 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 110-PT-BN-KP (Marujo et al., [2011](#bib.bib83)) | 新闻 | 葡萄牙语 | 110 |'
- en: '| 500N-KPCrowd (Marujo et al., [2012](#bib.bib82)) | News | EN | 500 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 500N-KPCrowd (Marujo et al., [2012](#bib.bib82)) | 新闻 | 英语 | 500 |'
- en: '| Wikinews (Bougouin et al., [2013](#bib.bib14)) | News | FR | 100 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Wikinews (Bougouin et al., [2013](#bib.bib14)) | 新闻 | 法语 | 100 |'
- en: '| PerKey (Doostmohammadi et al., [2018](#bib.bib31)) | News | PER | 553.1K
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| PerKey (Doostmohammadi et al., [2018](#bib.bib31)) | 新闻 | 波斯语 | 553.1K |'
- en: '| KPTimes (Gallina et al., [2019](#bib.bib35)) | News | EN | 279.9K |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| KPTimes (Gallina et al., [2019](#bib.bib35)) | 新闻 | 英语 | 279.9K |'
- en: '| Twitter (Zhang et al., [2016](#bib.bib157)) | Tweets | EN | 112.5K |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Twitter (Zhang et al., [2016](#bib.bib157)) | 推文 | 英语 | 112.5K |'
- en: '| Weibo (Wang et al., [2019](#bib.bib138)) | Tweets | ZH | 46.3K |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Weibo (Wang et al., [2019](#bib.bib138)) | 推文 | 中文 | 46.3K |'
- en: '| Text-Image Tweets (Wang et al., [2020c](#bib.bib139)) | Tweets | EN | 53.7K
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Text-Image Tweets (Wang et al., [2020c](#bib.bib139)) | 推文 | 英语 | 53.7K |'
- en: '| NZDL (Witten et al., [1999](#bib.bib141)) | Reports | EN | 1.8K |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| NZDL (Witten et al., [1999](#bib.bib141)) | 报告 | 英语 | 1.8K |'
- en: '| Blogs (Grineva et al., [2009](#bib.bib41)) | Web pages | EN | 252 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Blogs (Grineva et al., [2009](#bib.bib41)) | 网页 | 英语 | 252 |'
- en: '| StackExchange (Wang et al., [2019](#bib.bib138)) | QA | EN | 49.4K |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| StackExchange (Wang et al., [2019](#bib.bib138)) | 问答 | 英语 | 49.4K |'
- en: 'The commonly-used datasets for keyphrase prediction are shown in Table [3](#S4.T3
    "Table 3 ‣ 4 Datasets ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase
    Prediction: A Survey"). According to domains, they could be divided into reports,
    News, tweets, web pages, QA and scientific articles. Most of these datasets are
    in English, a few are in French, Persian, Chinese, and Portuguese.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '常用的关键词预测数据集见表 [3](#S4.T3 "Table 3 ‣ 4 Datasets ‣ From Statistical Methods to
    Deep Learning, Automatic Keyphrase Prediction: A Survey")。根据领域，它们可以分为报告、新闻、推文、网页、问答和科学文章。这些数据集大多是英语的，少数是法语、波斯语、中文和葡萄牙语。'
- en: As the most widely-used dataset, KP20k consists of articles in computer science
    from various online digital libraries. Overall, these datasets are relatively
    small, which is not applicable to industrial applications. Hence, it is urgent
    to construct large-quantity and high-quality multilingual datasets, so as to further
    promote the development of keyphrases prediction.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最广泛使用的数据集，KP20k 由各种在线数字图书馆中的计算机科学文章组成。总体而言，这些数据集相对较小，不适用于工业应用。因此，急需构建大规模、高质量的多语言数据集，以进一步推动关键词预测的发展。
- en: Considering the tradeoff between cost and quality of expert annotations, Chau
    et al. ([2020](#bib.bib19)) explore multiple annotation strategies, including
    self review, peer review, and so on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到专家注释的成本与质量之间的权衡，Chau et al. ([2020](#bib.bib19)) 探索了多种注释策略，包括自我审查、同行审查等。
- en: 5 Evaluation Metrics
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评价指标
- en: Let $\hat{Y}$=$(\hat{y}_{1},\hat{y}_{2},...,\hat{y}_{m})$ and $Y$=$(y_{2},y_{2},...,y_{n})$
    to be the predicted and target keyphrases, respectively. The common practice is
    to use only top $k$ predictions with the highest scores for evaluation, where
    $k$ is a pre-defined constant (usually 5 or 10). Particularly, to eliminate the
    influence of morphology, the predicted keyphrases are stemmed by applying Porter
    Stemmer¹¹1[https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py)
    (Meng et al., [2017](#bib.bib88)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\hat{Y}$=$(\hat{y}_{1},\hat{y}_{2},...,\hat{y}_{m})$ 和 $Y$=$(y_{2},y_{2},...,y_{n})$
    分别为预测的和目标的关键词。通常的做法是仅使用前 $k$ 个得分最高的预测进行评估，其中 $k$ 是预定义的常量（通常为 5 或 10）。特别地，为了消除形态的影响，预测的关键词通过应用
    Porter Stemmer¹¹1[https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py)
    (Meng et al., [2017](#bib.bib88)) 进行词干提取。
- en: 'The commonly-used metrics include precision, recall and $F_{1}$ scores. Early
    studies use $F_{1}@5$ and $F_{1}@10$ to evaluate the quality of generated present
    keyphrases, and $R@5$ and $R@10$ to measure the quality of generated absent keyphrases
    (Meng et al., [2017](#bib.bib88); Chen et al., [2018](#bib.bib20), [2019b](#bib.bib23)).
    Formally, these metrics are defined as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的评价指标包括精确度、召回率和 $F_{1}$ 分数。早期研究使用 $F_{1}@5$ 和 $F_{1}@10$ 来评估生成的当前关键词的质量，使用
    $R@5$ 和 $R@10$ 来衡量生成的缺失关键词的质量（Meng et al., [2017](#bib.bib88); Chen et al., [2018](#bib.bib20),
    [2019b](#bib.bib23)）。这些指标的正式定义如下：
- en: '|  | $P@k=\frac{&#124;\hat{Y}_{:k}\cap Y&#124;}{&#124;\hat{Y}_{:k}&#124;},$
    |  | (1) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $P@k=\frac{|\hat{Y}_{:k}\cap Y|}{|\hat{Y}_{:k}|},$ |  | (1) |'
- en: representing the correct proportion of keyphrases in predictions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表示预测中关键词的正确比例。
- en: '|  | $R@k=\frac{&#124;\hat{Y}_{:k}\cap Y&#124;}{&#124;Y&#124;},$ |  | (2) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $R@k=\frac{|\hat{Y}_{:k}\cap Y|}{|Y|},$ |  | (2) |'
- en: measuring the correct rate of the predicted keyphrase in references.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量参考文献中预测关键词的正确率。
- en: '|  | $F_{1}@k=\frac{2\*P@k\*R@k}{P@k+R@k},$ |  | (3) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{1}@k=\frac{2*P@k*R@k}{P@k+R@k},$ |  | (3) |'
- en: which is a tradeoff between $P@k$ and $R@k$.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 $P@k$ 和 $R@k$ 之间的权衡。
- en: 'Considering the fact that a model often predicts varying numbers of keyphrases,
    Yuan et al. ([2020](#bib.bib153)) argue that the metrics with the pre-defined
    constant $k$ cannot accurately evaluate the quality of predicted keyphrases. Thus,
    they extend $F_{1}@k$ to two metrics: 1) $F_{1}@O$: this metric sets $k$ as the
    number of target keyphrases instead of a pre-defined constant; 2) $F_{1}@M$: this
    metric takes all predictions into account. Futhermore, Chan et al. ([2019](#bib.bib18))
    improve *$F_{1}@M$* by filling target keyphrases with blanks when their number
    is less than the number of predicted keyphrases.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到模型通常预测的关键词数量不同，Yuan et al. ([2020](#bib.bib153)) 认为，具有预定义常量 $k$ 的指标无法准确评估预测关键词的质量。因此，他们将
    $F_{1}@k$ 扩展为两种指标：1) $F_{1}@O$：该指标将 $k$ 设定为目标关键词的数量，而不是预定义的常量；2) $F_{1}@M$：该指标考虑所有预测。此外，Chan
    et al. ([2019](#bib.bib18)) 通过在目标关键词数量少于预测关键词数量时用空白填充目标关键词来改进 *$F_{1}@M$*。
- en: However, conventional metrics, such as *$F_{1}$*, which assess the prediction
    quality at the phrase level, do not take into account the partially matched predictions.
    To deal with this issue, Luo et al. ([2021](#bib.bib77)) propose Fine-Grained
    ($FG$) evaluation score that considers prediction orders and qualities at the
    token level, and prediction diversity and numbers at the instance level.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传统的度量标准，例如 *$F_{1}$*，评估短语级别的预测质量，但未考虑部分匹配的预测。为了解决这个问题，Luo 等人 ([2021](#bib.bib77))
    提出了细粒度（$FG$）评估分数，该分数考虑了标记级别的预测顺序和质量，以及实例级别的预测多样性和数量。
- en: Besides, Habibi and Popescu-Belis ([2013](#bib.bib45)) introduce *$\alpha$-nDCG*
    (Clarke et al., [2008](#bib.bib26)) to measure the diversity of predicted keyphrases,
    where *nDCG* represents Normalized Discounted Cumulative Gain measure (Järvelin
    and Kekäläinen, [2002](#bib.bib50)) and the parameter $\alpha$ is a trade-off
    between relevance and diversity. Chan et al. ([2019](#bib.bib18)) measure the
    mean absolute error (*MAE*) between the number of predicted keyphrases and the
    number of target keyphrases. In addition, Chen et al. ([2020](#bib.bib22)) define
    *DupRatio* to evaluate the duplication rate of the predicted keyphrases.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Habibi 和 Popescu-Belis ([2013](#bib.bib45)) 引入了 *$\alpha$-nDCG*（Clarke 等人,
    [2008](#bib.bib26)）来衡量预测关键词的多样性，其中 *nDCG* 代表标准化折扣累积增益度量（Järvelin 和 Kekäläinen,
    [2002](#bib.bib50)），参数 $\alpha$ 是相关性和多样性之间的权衡。Chan 等人 ([2019](#bib.bib18)) 测量了预测关键词数量与目标关键词数量之间的平均绝对误差（*MAE*）。此外，Chen
    等人 ([2020](#bib.bib22)) 定义了 *DupRatio* 来评估预测关键词的重复率。
- en: 5.1 Implementation Details
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实施细节
- en: 'In the experiments of keyphrase extraction, we consider the following typical
    unsupervised models: statistical models including TF-IDF (Salton and Buckley,
    [1988](#bib.bib108)), YAKE (Campos et al., [2018](#bib.bib15)), graph-based models
    consisting of TextRank (Mihalcea and Tarau, [2004](#bib.bib89)), SingleRank (Wan
    and Xiao, [2008](#bib.bib131)), TopicRank (Bougouin et al., [2013](#bib.bib14)),
    PositionRank (Florescu and Caragea, [2017](#bib.bib33)), MultipartieRank (Boudin,
    [2018](#bib.bib11)), and deep learning-based models such as EmbedRank (Bennani-Smires
    et al., [2018](#bib.bib7)), SIFRank (Sun et al., [2020](#bib.bib123)), SIFRank+
    (Sun et al., [2020](#bib.bib123)), UKERank (Liang et al., [2021](#bib.bib67)),
    and JointKPE (Sun et al., [2021](#bib.bib122)). Please Note that JointKPE is the
    current SOTA supervised model. In the experiments of keyphrase generation, we
    compare the typical generation models under three kinds of common paradigms: 1)
    One2One, CopyRNN (Meng et al., [2017](#bib.bib88)) and KG-KE-KR-M (Chen et al.,
    [2019a](#bib.bib21)), 2) One2Seq, CatSeq (Yuan et al., [2020](#bib.bib153)), catSeqTG-2RF[1]
    (Chan et al., [2019](#bib.bib18)) and Transformer (Ye et al., [2021b](#bib.bib151)),
    and 3) One2Set, SetTrans (Ye et al., [2021b](#bib.bib151)) and WR-SetTrans(Xie
    et al., [2022](#bib.bib146)). Besides, we compare the performance of large language
    models in keyphrase prediction, including BART (Lewis et al., [2020](#bib.bib65)),
    T5 (Raffel et al., [2020](#bib.bib106)), KeyBART (Kulkarni et al., [2022](#bib.bib60))
    and ChatGPT²²2[https://chat.openai.com/chat](https://chat.openai.com/chat).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键词提取的实验中，我们考虑了以下典型的无监督模型：统计模型包括 TF-IDF（Salton 和 Buckley, [1988](#bib.bib108)）、YAKE（Campos
    等人, [2018](#bib.bib15)）、基于图的模型包括 TextRank（Mihalcea 和 Tarau, [2004](#bib.bib89)）、SingleRank（Wan
    和 Xiao, [2008](#bib.bib131)）、TopicRank（Bougouin 等人, [2013](#bib.bib14)）、PositionRank（Florescu
    和 Caragea, [2017](#bib.bib33)）、MultipartieRank（Boudin, [2018](#bib.bib11)）以及基于深度学习的模型如
    EmbedRank（Bennani-Smires 等人, [2018](#bib.bib7)）、SIFRank（Sun 等人, [2020](#bib.bib123)）、SIFRank+（Sun
    等人, [2020](#bib.bib123)）、UKERank（Liang 等人, [2021](#bib.bib67)）和 JointKPE（Sun 等人,
    [2021](#bib.bib122)）。请注意，JointKPE 是当前的 SOTA 监督模型。在关键词生成的实验中，我们比较了三种常见范式下的典型生成模型：1）One2One，包括
    CopyRNN（Meng 等人, [2017](#bib.bib88)）和 KG-KE-KR-M（Chen 等人, [2019a](#bib.bib21)），2）One2Seq，包括
    CatSeq（Yuan 等人, [2020](#bib.bib153)）、catSeqTG-2RF[1]（Chan 等人, [2019](#bib.bib18)）和
    Transformer（Ye 等人, [2021b](#bib.bib151)），以及 3）One2Set，包括 SetTrans（Ye 等人, [2021b](#bib.bib151)）和
    WR-SetTrans（Xie 等人, [2022](#bib.bib146)）。此外，我们还比较了大语言模型在关键词预测中的表现，包括 BART（Lewis
    等人, [2020](#bib.bib65)）、T5（Raffel 等人, [2020](#bib.bib106)）、KeyBART（Kulkarni 等人,
    [2022](#bib.bib60)）和 ChatGPT²²2[https://chat.openai.com/chat](https://chat.openai.com/chat)。
- en: During model training, we strictly use the same experiment settings as their
    original papers. For the model involving multiple variants, we only report the
    performance of its variant with the best performance. Particularly, following
    Yuan et al. ([2020](#bib.bib153)), we use two experimental settings for One2Seq
    paradigm models. When using ChatGPT, we explore three commonly-used settings,
    including zero-shot³³3We use the official released prompt ([https://platform.openai.com/examples/default-keywords](https://platform.openai.com/examples/default-keywords))
    for keyphrase prediction., 1-shot, and 5-shot. Specifically, we retrieve the most
    relevant training instances for the given input document according to the cosine
    distance of the MiniLM (Wang et al., [2020a](#bib.bib136)) embedding. These pertinent
    training instances are concatenated at the beginning of the input document and
    then fed into the ChatGPT to obtain the ultimate predictions for keyphrases. Particularly,
    to alleviate the instability of neural networks, we run the generation models
    for 3 times with different seeds and report the average results. Finally, we evaluate
    the present keyphrase and absent keyphrase predictions, respectively.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，我们严格使用与原论文相同的实验设置。对于涉及多个变体的模型，我们仅报告其最佳性能变体的表现。特别地，参考 Yuan 等人 ([2020](#bib.bib153))，我们对
    One2Seq 模式模型使用了两种实验设置。在使用 ChatGPT 时，我们探索了三种常用设置，包括零-shot³³3我们使用了官方发布的提示 ([https://platform.openai.com/examples/default-keywords](https://platform.openai.com/examples/default-keywords))
    进行关键短语预测、1-shot 和 5-shot。具体地，我们根据 MiniLM (Wang 等人，[2020a](#bib.bib136)) 嵌入的余弦距离检索与给定输入文档最相关的训练实例。这些相关训练实例被拼接在输入文档的开头，然后输入到
    ChatGPT 中以获得关键短语的**最终**预测。特别地，为了缓解神经网络的不稳定性，我们运行生成模型 3 次，使用不同的种子，并报告平均结果。最后，我们分别评估当前关键短语和缺失关键短语的预测。
- en: 'Table 4: Results of present keyphrase prediction using extraction models. To
    ensure fair comparsions, we only use the target present keyphrase to evaluate
    the performance of extraction models, while the previous studies use all target
    keyphrases.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：当前关键短语预测的提取模型结果。为了确保公平比较，我们仅使用目标当前关键短语来评估提取模型的性能，而以往的研究使用了所有目标关键短语。
- en: '[width=0.5] Model Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5
    F1@M F1@5 F1@M F1@5 F1@M Unsupervised Statistical Extraction Models TF-IDF (Salton
    and Buckley, [1988](#bib.bib108)) 0.132 0.175 0.214 0.213 0.145 0.131 0.151 0.190
    0.172 0.146 YAKE (Campos et al., [2018](#bib.bib15)) 0.183 0.193 0.221 0.212 0.188
    0.131 0.202 0.204 0.189 0.145 Unsupervised Graph-based Extraction Models TextRank
    (Mihalcea and Tarau, [2004](#bib.bib89)) 0.321 0.363 0.092 0.169 0.118 0.144 0.093
    0.200 0.091 0.120 SingleRank (Wan and Xiao, [2008](#bib.bib131)) 0.325 0.362 0.151
    0.195 0.152 0.147 0.146 0.212 0.134 0.131 TopicRank (Bougouin et al., [2013](#bib.bib14))
    0.266 0.301 0.210 0.154 0.168 0.118 0.201 0.163 0.167 0.114 PositionRank (Florescu
    and Caragea, [2017](#bib.bib33)) 0.306 0.338 0.228 0.208 0.186 0.143 0.245 0.229
    0.183 0.138 MultipartiteRank (Boudin, [2018](#bib.bib11)) 0.269 0.322 0.244 0.188
    0.181 0.132 0.227 0.206 0.185 0.132 Unsupervised Deep Learning-based Extraction
    Models EmbedRank (Bennani-Smires et al., [2018](#bib.bib7)) 0.333 0.376 0.166
    0.199 0.167 0.150 0.185 0.233 0.153 0.135 SIFRank (Sun et al., [2020](#bib.bib123))
    0.368 0.385 0.143 0.193 0.164 0.151 0.165 0.213 0.138 0.133 SIFRank+ (Sun et al.,
    [2020](#bib.bib123)) 0.348 0.384 0.246 0.203 0.194 0.153 0.244 0.223 0.195 0.138
    UKERank (Liang et al., [2021](#bib.bib67)) 0.350 0.384 0.238 0.202 0.187 0.162
    0.250 0.228 0.178 0.138 Supervised Deep Learning-based Extraction Models Sequence
    Tagging(Roberta-base) (Sun et al., [2021](#bib.bib122)) 0.331 0.336 0.321 0.177
    0.476 0.319 0.379 0.291 0.416 0.240 JointKPE (Sun et al., [2021](#bib.bib122))
    0.352 0.348 0.476 0.335 0.360 0.202 0.393 0.306 0.417 0.239'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[width=0.5] 模型 Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5 F1@M
    F1@5 F1@M F1@5 F1@M 无监督统计提取模型 TF-IDF (Salton and Buckley, [1988](#bib.bib108))
    0.132 0.175 0.214 0.213 0.145 0.131 0.151 0.190 0.172 0.146 YAKE (Campos et al.,
    [2018](#bib.bib15)) 0.183 0.193 0.221 0.212 0.188 0.131 0.202 0.204 0.189 0.145
    无监督图基提取模型 TextRank (Mihalcea and Tarau, [2004](#bib.bib89)) 0.321 0.363 0.092
    0.169 0.118 0.144 0.093 0.200 0.091 0.120 SingleRank (Wan and Xiao, [2008](#bib.bib131))
    0.325 0.362 0.151 0.195 0.152 0.147 0.146 0.212 0.134 0.131 TopicRank (Bougouin
    et al., [2013](#bib.bib14)) 0.266 0.301 0.210 0.154 0.168 0.118 0.201 0.163 0.167
    0.114 PositionRank (Florescu and Caragea, [2017](#bib.bib33)) 0.306 0.338 0.228
    0.208 0.186 0.143 0.245 0.229 0.183 0.138 MultipartiteRank (Boudin, [2018](#bib.bib11))
    0.269 0.322 0.244 0.188 0.181 0.132 0.227 0.206 0.185 0.132 无监督深度学习提取模型 EmbedRank
    (Bennani-Smires et al., [2018](#bib.bib7)) 0.333 0.376 0.166 0.199 0.167 0.150
    0.185 0.233 0.153 0.135 SIFRank (Sun et al., [2020](#bib.bib123)) 0.368 0.385
    0.143 0.193 0.164 0.151 0.165 0.213 0.138 0.133 SIFRank+ (Sun et al., [2020](#bib.bib123))
    0.348 0.384 0.246 0.203 0.194 0.153 0.244 0.223 0.195 0.138 UKERank (Liang et
    al., [2021](#bib.bib67)) 0.350 0.384 0.238 0.202 0.187 0.162 0.250 0.228 0.178
    0.138 监督深度学习提取模型 Sequence Tagging(Roberta-base) (Sun et al., [2021](#bib.bib122))
    0.331 0.336 0.321 0.177 0.476 0.319 0.379 0.291 0.416 0.240 JointKPE (Sun et al.,
    [2021](#bib.bib122)) 0.352 0.348 0.476 0.335 0.360 0.202 0.393 0.306 0.417 0.239'
- en: 'Table 5: Results of present keyphrase prediction using generation models. #bs denotes
    beam size. ^† indicates previously reported scores.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：当前关键短语预测使用生成模型的结果。#bs 表示束大小。^† 表示先前报告的分数。
- en: '[width=0.8] Model Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5
    F1@M F1@5 F1@M F1@5 F1@M One2One Paradigm-based Models CopyRNN(#bs=200) (Meng
    et al., [2017](#bib.bib88)) 0.272 0.293 0.356 0.306 0.283 0.214 0.294 0.257 0.336
    0.255 KG-KE-KR-M(#bs=200) (Chen et al., [2019a](#bib.bib21)) 0.324 0.362 0.421
    0.342 0.304 0.273 0.325 0.293 0.400 0.277 One2Seq Paradigm-based Models CatSeq(#bs=1)
    (Yuan et al., [2020](#bib.bib153)) 0.229 0.266 0.324 0.394 0.270 0.344 0.245 0.296
    0.292 0.365 CatSeq(#bs=50) (Yuan et al., [2020](#bib.bib153)) 0.328 0.398 0.417
    0.395 0.352 0.316 0.343 0.334 0.360 0.302 catSeqTG-2RF[1](#bs=1) (Chan et al.,
    [2019](#bib.bib18)) 0.253 0.301 0.375 0.433 0.300 0.369 0.287 0.329 0.321 0.386
    Transformer(#bs=1) (Ye et al., [2021b](#bib.bib151)) 0.285 0.331 0.371 0.418 0.308
    0.356 0.287 0.319 0.330 0.373 One2Set Paradigm-based Models SetTrans(#bs=1) (Ye
    et al., [2021b](#bib.bib151)) 0.281 0.318 0.406 0.452 0.339 0.374 0.322 0.354
    0.354 0.390 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146)) 0.330 0.351 0.428
    0.452 0.360 0.362 0.360 0.370 0.370 0.378'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[width=0.8] 模型 Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5 F1@M
    F1@5 F1@M F1@5 F1@M One2One 基于范式的模型 CopyRNN(#bs=200) (Meng et al., [2017](#bib.bib88))
    0.272 0.293 0.356 0.306 0.283 0.214 0.294 0.257 0.336 0.255 KG-KE-KR-M(#bs=200)
    (Chen et al., [2019a](#bib.bib21)) 0.324 0.362 0.421 0.342 0.304 0.273 0.325 0.293
    0.400 0.277 One2Seq 基于范式的模型 CatSeq(#bs=1) (Yuan et al., [2020](#bib.bib153)) 0.229
    0.266 0.324 0.394 0.270 0.344 0.245 0.296 0.292 0.365 CatSeq(#bs=50) (Yuan et
    al., [2020](#bib.bib153)) 0.328 0.398 0.417 0.395 0.352 0.316 0.343 0.334 0.360
    0.302 catSeqTG-2RF[1](#bs=1) (Chan et al., [2019](#bib.bib18)) 0.253 0.301 0.375
    0.433 0.300 0.369 0.287 0.329 0.321 0.386 Transformer(#bs=1) (Ye et al., [2021b](#bib.bib151))
    0.285 0.331 0.371 0.418 0.308 0.356 0.287 0.319 0.330 0.373 One2Set 基于范式的模型 SetTrans(#bs=1)
    (Ye et al., [2021b](#bib.bib151)) 0.281 0.318 0.406 0.452 0.339 0.374 0.322 0.354
    0.354 0.390 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146)) 0.330 0.351 0.428
    0.452 0.360 0.362 0.360 0.370 0.370 0.378'
- en: 'Table 6: Results of absent keyphrase prediction using generation models.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：缺失关键短语预测使用生成模型的结果。
- en: '[width=0.8] Model Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5
    F1@M F1@5 F1@M F1@5 F1@M One2One Paradigm-based Models CopyRNN(#bs=200) (Meng
    et al., [2017](#bib.bib88)) 0.007 0.007 0.009 0.012 0.013 0.019 0.007 0.011 0.011
    0.013 KG-KE-KR-M(#bs=200) (Chen et al., [2019a](#bib.bib21))^† 0.024 0.028 0.060
    0.076 0.059 0.063 0.031 0.040 0.070 0.083 One2Seq Paradigm-based Models CatSeq(#bs=1)
    (Yuan et al., [2020](#bib.bib153)) 0.005 0.009 0.015 0.026 0.018 0.034 0.015 0.022
    0.014 0.030 CatSeq(#b=50) (Yuan et al., [2020](#bib.bib153)) 0.021 0.028 0.038
    0.052 0.051 0.065 0.030 0.038 0.041 0.058 catSeqTG-2RF[1](#bs=1) (Chan et al.,
    [2019](#bib.bib18)) 0.012 0.021 0.019 0.031 0.030 0.053 0.021 0.030 0.027 0.050
    Transformer(#bs=1) (Ye et al., [2021b](#bib.bib151)) 0.008 0.017 0.028 0.050 0.030
    0.055 0.016 0.022 0.021 0.043 One2Set Paradigm-based Models SetTrans(#bs=1) (Ye
    et al., [2021b](#bib.bib151)) 0.018 0.029 0.041 0.061 0.046 0.073 0.029 0.035
    0.035 0.056 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146)) 0.025 0.034 0.057
    0.071 0.057 0.074 0.040 0.043 0.050 0.064'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[width=0.8] 模型 Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5 F1@M
    F1@5 F1@M F1@5 F1@M One2One 基于范式的模型 CopyRNN(#bs=200) (Meng et al., [2017](#bib.bib88))
    0.007 0.007 0.009 0.012 0.013 0.019 0.007 0.011 0.011 0.013 KG-KE-KR-M(#bs=200)
    (Chen et al., [2019a](#bib.bib21))^† 0.024 0.028 0.060 0.076 0.059 0.063 0.031
    0.040 0.070 0.083 One2Seq 基于范式的模型 CatSeq(#bs=1) (Yuan et al., [2020](#bib.bib153))
    0.005 0.009 0.015 0.026 0.018 0.034 0.015 0.022 0.014 0.030 CatSeq(#b=50) (Yuan
    et al., [2020](#bib.bib153)) 0.021 0.028 0.038 0.052 0.051 0.065 0.030 0.038 0.041
    0.058 catSeqTG-2RF[1](#bs=1) (Chan et al., [2019](#bib.bib18)) 0.012 0.021 0.019
    0.031 0.030 0.053 0.021 0.030 0.027 0.050 Transformer(#bs=1) (Ye et al., [2021b](#bib.bib151))
    0.008 0.017 0.028 0.050 0.030 0.055 0.016 0.022 0.021 0.043 One2Set 基于范式的模型 SetTrans(#bs=1)
    (Ye et al., [2021b](#bib.bib151)) 0.018 0.029 0.041 0.061 0.046 0.073 0.029 0.035
    0.035 0.056 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146)) 0.025 0.034 0.057
    0.071 0.057 0.074 0.040 0.043 0.050 0.064'
- en: 6 Comparison between Existing Models
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 现有模型比较
- en: 'To better understand advantages and disadvantages of different models, we conduct
    several groups of experiments to compare representative models in different settings.
    To this end, we use the KP20k training set to train various models, and then apply
    the same script⁴⁴4[https://github.com/kenchan0226/keyphrase-generation-rl/blob/master/evaluate_prediction.py](https://github.com/kenchan0226/keyphrase-generation-rl/blob/master/evaluate_prediction.py)
    to evaluate the model predictions on five commonly-used test sets: Inspec, NUS,
    Krapivin, SemEval, and KP20k.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解不同模型的优缺点，我们进行了几组实验以比较不同设置下的代表性模型。为此，我们使用 KP20k 训练集训练各种模型，然后应用相同的脚本⁴⁴[https://github.com/kenchan0226/keyphrase-generation-rl/blob/master/evaluate_prediction.py](https://github.com/kenchan0226/keyphrase-generation-rl/blob/master/evaluate_prediction.py)
    来评估模型在五个常用测试集上的预测结果：Inspec、NUS、Krapivin、SemEval 和 KP20k。
- en: 6.1 Comparison of Extraction Models
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 提取模型比较
- en: 'The performance of extraction models is reported in Table [4](#S5.T4 "Table
    4 ‣ 5.1 Implementation Details ‣ 5 Evaluation Metrics ‣ From Statistical Methods
    to Deep Learning, Automatic Keyphrase Prediction: A Survey"). Note that the previous
    studies in this aspect report the evaluation scores with respect to all target
    keyphrases. To ensure fair comparisons, we only use the present keyphrases to
    evaluate the performance of various extraction models.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '提取模型的性能见表格[4](#S5.T4 "Table 4 ‣ 5.1 Implementation Details ‣ 5 Evaluation Metrics
    ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A
    Survey")。注意，以前在这一方面的研究报告了针对所有目标关键词的评估分数。为了确保公平比较，我们仅使用当前关键词来评估各种提取模型的性能。'
- en: Overall, unsupervised statistical extraction models perform worst in this setting,
    and unsupervised graph-based extraction models surpass statistical ones. This
    result is not surprising, because unsupervised graph-based extraction models not
    only use statistical features but also employ effective graph algorithms, such
    as clustering, graph propagation, etc. Moreover, due to the advantage of semantic
    representation learning, deep learning-based models achieve the best result, echoing
    the development trend of natural language processing studies from statistical
    models to deep learning-based models.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，非监督统计提取模型在这种设置下表现最差，而非监督图基提取模型优于统计模型。这一结果并不令人惊讶，因为非监督图基提取模型不仅使用统计特征，还采用了有效的图算法，如聚类、图传播等。此外，由于语义表示学习的优势，基于深度学习的模型取得了最佳结果，这与自然语言处理研究从统计模型到基于深度学习的模型的发展趋势相呼应。
- en: '![Refer to caption](img/abdc70a4c1dca0ffd76580bf22258661.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/abdc70a4c1dca0ffd76580bf22258661.png)'
- en: 'Figure 4: The training losses of representative models under three paradigms.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：三种范式下代表性模型的训练损失。
- en: 'Besides, comparing unsupervised and supervised extraction models, we can observe
    that supervised extraction models outperform unsupervised ones on most test sets
    except Inspec. Further analysis on Inspec will be provided in Section [6.2](#S6.SS2
    "6.2 Comparison of Gneration Models ‣ 6 Comparison between Existing Models ‣ From
    Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey").'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，比较无监督和监督提取模型，我们可以观察到，除了 Inspec 外，监督提取模型在大多数测试集上的表现优于无监督模型。关于 Inspec 的进一步分析将在第[6.2](#S6.SS2
    "6.2 生成模型的比较 ‣ 6 现有模型比较 ‣ 从统计方法到深度学习，自动关键词预测：综述")节中提供。
- en: 6.2 Comparison of Gneration Models
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 生成模型的比较
- en: Three Training Paradigms
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 三种训练范式
- en: 'Figure [4](#S6.F4.1 "Figure 4 ‣ 6.1 Comparison of Extraction Models ‣ 6 Comparison
    between Existing Models ‣ From Statistical Methods to Deep Learning, Automatic
    Keyphrase Prediction: A Survey") shows the training losses of CopyRNN, CatSeq
    and SetTrans, which are the representative models under three paradigms. CopyRNN
    suffers from the highest loss, due to the difficulty of model training brought
    by the One2One paradigm where one input corresponds to multiple targets. One2Seq
    paradigm alleviates the problem of inconsistent training instances by concatenating
    target keyphrases into a sequence and Cat2Seq achieves a relatively lower loss
    than CopyRNN. Among three representative models, SetTrans has the lowest loss
    after convergence, demonstrating the advantage of the ONE2SET paradigm.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S6.F4.1 "图 4 ‣ 6.1 提取模型比较 ‣ 6 现有模型比较 ‣ 从统计方法到深度学习，自动关键词预测：综述")展示了 CopyRNN、CatSeq
    和 SetTrans 的训练损失，这些模型分别代表了三种范式。由于 One2One 范式中一个输入对应多个目标，模型训练难度较大，导致 CopyRNN 的损失最高。One2Seq
    范式通过将目标关键词短语连接成一个序列来缓解训练实例不一致的问题，Cat2Seq 的损失相对低于 CopyRNN。在这三种代表性模型中，SetTrans 在收敛后具有最低的损失，显示了
    ONE2SET 范式的优势。
- en: '![Refer to caption](img/14fbe0f17f8cecde2b7973bb6dfac214.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/14fbe0f17f8cecde2b7973bb6dfac214.png)'
- en: 'Figure 5: The first occurrence position distribution of present keyphrases
    in input documents'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：输入文档中当前关键词短语的首次出现位置分布
- en: Comparison of SOTA Extraction Model and Generation Models
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SOTA 提取模型与生成模型的比较
- en: 'From the last rows of Table [4](#S5.T4 "Table 4 ‣ 5.1 Implementation Details
    ‣ 5 Evaluation Metrics ‣ From Statistical Methods to Deep Learning, Automatic
    Keyphrase Prediction: A Survey") and Table [5](#S5.T5 "Table 5 ‣ 5.1 Implementation
    Details ‣ 5 Evaluation Metrics ‣ From Statistical Methods to Deep Learning, Automatic
    Keyphrase Prediction: A Survey"), we observe that JointKPE (Sun et al., [2021](#bib.bib122))
    outperfoms all generation models in terms of *$F_{1}@5$*. However, extraction
    models cannot dynamically decide the number of extracted keyphrases. If the pre-defined
    number of extracted keyphrases is larger than the actual number of target keyphrases,
    it may introduce noise into the extracted phrases, resulting in a low *$F_{1}@M$*.
    Worse still, extraction models are unable to deal with the predictions of absent
    keyphrases, which account for a large proportion of target keyphrases. Therefore,
    we argue that a combination of extraction and generation model, such as KG-KE-KR-M
    (Chen et al., [2019a](#bib.bib21)), has the potential to achieve better overall
    results than single-mode models.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[4](#S5.T4 "表 4 ‣ 5.1 实施细节 ‣ 5 评估指标 ‣ 从统计方法到深度学习，自动关键词预测：综述")和表[5](#S5.T5
    "表 5 ‣ 5.1 实施细节 ‣ 5 评估指标 ‣ 从统计方法到深度学习，自动关键词预测：综述")的最后几行，我们观察到 JointKPE (Sun et
    al., [2021](#bib.bib122)) 在 *$F_{1}@5$* 上优于所有生成模型。然而，提取模型无法动态决定提取的关键词数量。如果预定义的提取关键词数量大于实际目标关键词数量，它可能会引入噪声到提取的短语中，从而导致
    *$F_{1}@M$* 较低。更糟糕的是，提取模型无法处理缺失关键词的预测，而这些缺失关键词占目标关键词的很大比例。因此，我们认为结合提取模型和生成模型的组合，如
    KG-KE-KR-M (Chen et al., [2019a](#bib.bib21))，有可能实现比单一模式模型更好的整体结果。
- en: 'Table 7: Statistical features of five datasets.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：五个数据集的统计特征。
- en: '| Dataset | #pre. KP/doc | #abs. KP/doc | #token/doc | Length of pre. KP |
    Length of abs. KP | #doc |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 每文档预测关键词数量 | 每文档绝对关键词数量 | 每文档标记数 | 预测关键词长度 | 绝对关键词长度 | 文档数量 |'
- en: '| Inspec | 7.23 | 2.59 | 134.10 | 2.44 | 2.72 | 500 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Inspec | 7.23 | 2.59 | 134.10 | 2.44 | 2.72 | 500 |'
- en: '| NUS | 6.34 | 5.31 | 230.13 | 1.95 | 2.56 | 211 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| NUS | 6.34 | 5.31 | 230.13 | 1.95 | 2.56 | 211 |'
- en: '| Krapivin | 3.26 | 2.59 | 189.32 | 2.16 | 2.29 | 400 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Krapivin | 3.26 | 2.59 | 189.32 | 2.16 | 2.29 | 400 |'
- en: '| SemEval | 6.25 | 8.41 | 245.89 | 2.08 | 2.61 | 100 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| SemEval | 6.25 | 8.41 | 245.89 | 2.08 | 2.61 | 100 |'
- en: '| KP20k | 3.24 | 2.84 | 179.02 | 1.85 | 2.55 | 570, 802 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| KP20k | 3.24 | 2.84 | 179.02 | 1.85 | 2.55 | 570, 802 |'
- en: '![Refer to caption](img/98aff8dc87d8c8b05c64e078d0c9250c.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/98aff8dc87d8c8b05c64e078d0c9250c.png)'
- en: 'Figure 6: The first occurrence position distributions of the target present
    keyphrases and present keyphrases predicted by SIFRank, JointKPE and SetTrans
    in Inspec. The document has been divided into ten equal parts, and the x-axis
    indicates the index of the divided sub-document, for example, x = p1 means that
    the first 10% of the document. The y-axis is the proportion of the keyphrase in
    this sub-document.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：目标出现的关键词和 SIFRank、JointKPE 和 SetTrans 在 Inspec 中预测的关键词的首次出现位置分布。文档被分成了十个相等的部分，x
    轴表示被划分的子文档的索引，例如，x = p1 意味着文档的前 10%。y 轴表示该子文档中关键词的比例。
- en: 'Back to Table [5](#S5.T5 "Table 5 ‣ 5.1 Implementation Details ‣ 5 Evaluation
    Metrics ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction:
    A Survey"), KG-KE-KR-M performs significantly better than CopyRNN, proving the
    superiority of combing generation and extraction. Note that although KG-KE-KR-M
    incorporates retrieval and reranking techniques into One2One paradigm, SetTrans(Ye
    et al., [2021b](#bib.bib151)) still outperforms KG-KE-KR-M and other generation
    models in F1@M without special techniques, showing its advantages in predicting
    the keyphrase number for documents.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 返回表格 [5](#S5.T5 "表格 5 ‣ 5.1 实现细节 ‣ 5 评价指标 ‣ 从统计方法到深度学习，自动关键词预测：综述")，KG-KE-KR-M
    的表现显著优于 CopyRNN，证明了生成和提取相结合的优势。请注意，尽管 KG-KE-KR-M 将检索和重排序技术融入 One2One 模式，SetTrans(Ye
    等，[2021b](#bib.bib151)) 在 F1@M 上仍然优于 KG-KE-KR-M 和其他生成模型，未使用特殊技术，显示出在预测文档关键词数量方面的优势。
- en: Analysis of the Inspec Dataset
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Inspec 数据集分析
- en: 'From Table [4](#S5.T4 "Table 4 ‣ 5.1 Implementation Details ‣ 5 Evaluation
    Metrics ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction:
    A Survey") and Table [5](#S5.T5 "Table 5 ‣ 5.1 Implementation Details ‣ 5 Evaluation
    Metrics ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction:
    A Survey"), we observe that unsupervised deep learning-based extraction models
    achieve comparable or better performance than supervised deep learning-based extraction
    and generation models when predicting present keyphrases on Inspec. To explain
    this phenomena, we further conduct the following analyses:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格 [4](#S5.T4 "表格 4 ‣ 5.1 实现细节 ‣ 5 评价指标 ‣ 从统计方法到深度学习，自动关键词预测：综述") 和表格 [5](#S5.T5
    "表格 5 ‣ 5.1 实现细节 ‣ 5 评价指标 ‣ 从统计方法到深度学习，自动关键词预测：综述") 中，我们观察到，基于无监督深度学习的提取模型在预测
    Inspec 上的当前关键词时表现出与基于监督深度学习的提取和生成模型相当或更好的性能。为了说明这一现象，我们进一步进行了以下分析：
- en: '1) Table [7](#S6.T7 "Table 7 ‣ Comparison of SOTA Extraction Model and Generation
    Models ‣ 6.2 Comparison of Gneration Models ‣ 6 Comparison between Existing Models
    ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A
    Survey") shows the statistical features of datasets. Compared with other test
    sets, Inspec has the shortest average document length, the longest average length
    of present keyphrase, and the maximum number of present keyphrase, indicating
    Inspec is more suitable for extraction models than other datasets.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 表格 [7](#S6.T7 "表格 7 ‣ SOTA 提取模型与生成模型比较 ‣ 6.2 生成模型比较 ‣ 6 现有模型比较 ‣ 从统计方法到深度学习，自动关键词预测：综述")
    显示了数据集的统计特征。与其他测试集相比，Inspec 的平均文档长度最短，当前关键词的平均长度最长，当前关键词的最大数量也最多，表明 Inspec 更适合用于提取模型。
- en: '2) In Figure [5](#S6.F5 "Figure 5 ‣ Three Training Paradigms ‣ 6.2 Comparison
    of Gneration Models ‣ 6 Comparison between Existing Models ‣ From Statistical
    Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey"), we also
    visualize the occurrence position distributions of present keyphrases in each
    dataset. It reveals that the present keyphrases of the KP20k training set tend
    to occur in the front of the document. This phenomenon becomes even more evident
    when analyzing the first occurrence positions of present keyphases. As a result,
    the supervised models trained on KP20k tend to predict present keyphrases from
    the front of the document, which, however, is not applicable for Inspec, of which
    present keyphrases distribute evenly in the document.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '2) 在图[5](#S6.F5 "Figure 5 ‣ Three Training Paradigms ‣ 6.2 Comparison of Gneration
    Models ‣ 6 Comparison between Existing Models ‣ From Statistical Methods to Deep
    Learning, Automatic Keyphrase Prediction: A Survey")中，我们还可视化了每个数据集中当前关键短语的出现位置分布。结果显示，KP20k训练集的当前关键短语往往出现在文档的前面。当分析当前关键短语的首次出现位置时，这一现象变得更加明显。因此，基于KP20k训练的监督模型倾向于从文档前面预测当前关键短语，但这对Inspec不适用，因为Inspec中的当前关键短语在文档中均匀分布。'
- en: '3) Figure [6](#S6.F6 "Figure 6 ‣ Comparison of SOTA Extraction Model and Generation
    Models ‣ 6.2 Comparison of Gneration Models ‣ 6 Comparison between Existing Models
    ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A
    Survey") depicts the position distributions of the target present keyphrases and
    present keyphrases predicted by SIFRank, JointKPE, and SetTrans in Inspec. Please
    note that they are the best unsupervised keyphrase extraction, supervised keyphrase
    extraction and keyphrase generation models, respectively. The distribution of
    present keyphrases predicted by SIFRank is very close to the distribution of Inspec,
    while other supervised models are quite different. It supports our hypothesis
    that supervised models, are deeply affected by the occurrence position distribution
    of keyphrases in training data, which leads to the degradation of model performance
    when the test set is domain-mismatch with the training data.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '3) 图[6](#S6.F6 "Figure 6 ‣ Comparison of SOTA Extraction Model and Generation
    Models ‣ 6.2 Comparison of Gneration Models ‣ 6 Comparison between Existing Models
    ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A
    Survey")描绘了目标当前关键短语及SIFRank、JointKPE和SetTrans在Inspec中预测的当前关键短语的位置分布。请注意，它们分别是最佳的无监督关键短语提取、监督关键短语提取和关键短语生成模型。SIFRank预测的当前关键短语分布与Inspec的分布非常接近，而其他监督模型则差异较大。这支持了我们的假设，即监督模型受到训练数据中关键短语出现位置分布的深刻影响，当测试集与训练数据的领域不匹配时，模型性能会下降。'
- en: 6.3 Comparison of LLMs
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 LLMs的比较
- en: Recently, large language models (LLMs) have achieved remarkable success in various
    NLP tasks and have displayed a variety of capabilities. To evaluate the keyphrase
    prediction ability of these models, we compare the performance of commonly-used
    LLMs, including BART, T5, KeyBART, and ChatGPT, across five benchmark datasets.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）在各种NLP任务中取得了显著成功，并展示了多种能力。为了评估这些模型的关键短语预测能力，我们比较了包括BART、T5、KeyBART和ChatGPT在内的常用LLMs在五个基准数据集上的表现。
- en: 'Table [8](#S6.T8 "Table 8 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison between Existing
    Models ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction:
    A Survey") and Table [9](#S6.T9 "Table 9 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison
    between Existing Models ‣ From Statistical Methods to Deep Learning, Automatic
    Keyphrase Prediction: A Survey") reports the experimental results. We find that
    compared with previous SOTA models, such as CatSeq, Transformer, SetTrans and
    WR-SetTrans, LLMs show modest improvements in both present and absent keyphrase
    predictions. Additionally, our comparison of SetTrans and LLMs suggests that the
    impact of increasing model parameters is overshadowed by the adoption of new training
    and inference paradigms.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](#S6.T8 "Table 8 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison between Existing
    Models ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction:
    A Survey")和表[9](#S6.T9 "Table 9 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison between
    Existing Models ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase
    Prediction: A Survey")报告了实验结果。我们发现，与之前的SOTA模型（如CatSeq、Transformer、SetTrans和WR-SetTrans）相比，LLMs在当前和缺失关键短语预测方面均有适度的提升。此外，我们对SetTrans和LLMs的比较表明，增加模型参数的影响被新训练和推理范式的采纳所掩盖。'
- en: 'By synthesizing all results of Table [4](#S5.T4 "Table 4 ‣ 5.1 Implementation
    Details ‣ 5 Evaluation Metrics ‣ From Statistical Methods to Deep Learning, Automatic
    Keyphrase Prediction: A Survey"), Table [8](#S6.T8 "Table 8 ‣ 6.3 Comparison of
    LLMs ‣ 6 Comparison between Existing Models ‣ From Statistical Methods to Deep
    Learning, Automatic Keyphrase Prediction: A Survey") and Table [9](#S6.T9 "Table
    9 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison between Existing Models ‣ From Statistical
    Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey"), we conclude
    that ChatGPT outperforms all other unsupervised keyphrase extraction methods in
    terms of F1@5-score and F1@M-score under the zero-shot setting, but is still inferior
    to the existing SOTA supervised models on almost all datasets. With more training
    instances, the prediction ability of ChatGPT for both present and absent keyphrases
    can be significantly improved. Its superior performance on multiple benchmark
    datasets highlights its significance for practical applications in various domains.
    Further research could explore the more effective use of ChatGPT to fully exert
    its potential.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '通过综合表格[4](#S5.T4 "Table 4 ‣ 5.1 Implementation Details ‣ 5 Evaluation Metrics
    ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A
    Survey")、表格[8](#S6.T8 "Table 8 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison between
    Existing Models ‣ From Statistical Methods to Deep Learning, Automatic Keyphrase
    Prediction: A Survey")和表格[9](#S6.T9 "Table 9 ‣ 6.3 Comparison of LLMs ‣ 6 Comparison
    between Existing Models ‣ From Statistical Methods to Deep Learning, Automatic
    Keyphrase Prediction: A Survey")的所有结果，我们得出结论：在零样本设置下，ChatGPT在F1@5-score和F1@M-score方面优于所有其他无监督关键词提取方法，但在几乎所有数据集上仍然不及现有的SOTA有监督模型。通过增加训练实例，ChatGPT对当前和缺失关键词的预测能力可以显著提高。它在多个基准数据集上的优越表现凸显了其在各个领域实际应用中的重要性。进一步的研究可以深入探索ChatGPT的更有效使用，以充分发挥其潜力。'
- en: 'Table 8: Results of present keyphrase prediction using large language models.
    #bs denotes beam size. ^† indicates previously reported scores.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 8: 使用大型语言模型进行当前关键词预测的结果。#bs 表示束宽。^† 表示之前报告的分数。'
- en: '[width=0.8] Model Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5
    F1@M F1@5 F1@M F1@5 F1@M CatSeq(#bs=1) (Yuan et al., [2020](#bib.bib153)) 0.229
    0.266 0.324 0.394 0.270 0.344 0.245 0.296 0.292 0.365 Transformer(#bs=1) (Ye et al.,
    [2021b](#bib.bib151)) 0.285 0.331 0.371 0.418 0.308 0.356 0.287 0.319 0.330 0.373
    SetTrans(#bs=1) (Ye et al., [2021b](#bib.bib151)) 0.281 0.318 0.406 0.452 0.339
    0.374 0.322 0.354 0.354 0.390 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146))
    0.330 0.351 0.428 0.452 0.360 0.362 0.360 0.370 0.370 0.378 BART-base(#bs=1) (Lewis
    et al., [2020](#bib.bib65)) 0.270 0.323 0.366 0.424 0.270 0.336 0.271 0.321 0.322
    0.388 BART-large(#bs=1) (Lewis et al., [2020](#bib.bib65)) 0.276 0.333 0.380 0.435
    0.284 0.347 0.274 0.311 0.332 0.392 T5-base(#bs=1) (Raffel et al., [2020](#bib.bib106))
    0.288 0.339 0.388 0.440 0.302 0.350 0.295 0.326 0.336 0.388 T5-large(#bs=1) (Raffel
    et al., [2020](#bib.bib106)) 0.295 0.343 0.398 0.438 0.315 0.359 0.297 0.321 0.343
    0.393 KeyBART(#bs=1) (Kulkarni et al., [2022](#bib.bib60)) 0.268 0.325 0.373 0.430
    0.287 0.365 0.260 0.289 0.325 0.398 zero-shot ChatGPT(#bs=1) 0.309 0.428 0.338
    0.258 0.237 0.189 0.274 0.252 0.192 0.158 1-shot ChatGPT(#bs=1) 0.421 0.480 0.355
    0.359 0.297 0.298 0.319 0.326 0.298 0.295 5-shot ChatGPT(#bs=1) 0.431 0.497 0.365
    0.351 0.285 0.287 0.312 0.300 0.297 0.288'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[width=0.8] 模型 Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5 F1@M
    F1@5 F1@M F1@5 F1@M CatSeq(#bs=1) (Yuan et al., [2020](#bib.bib153)) 0.229 0.266
    0.324 0.394 0.270 0.344 0.245 0.296 0.292 0.365 Transformer(#bs=1) (Ye et al.,
    [2021b](#bib.bib151)) 0.285 0.331 0.371 0.418 0.308 0.356 0.287 0.319 0.330 0.373
    SetTrans(#bs=1) (Ye et al., [2021b](#bib.bib151)) 0.281 0.318 0.406 0.452 0.339
    0.374 0.322 0.354 0.354 0.390 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146))
    0.330 0.351 0.428 0.452 0.360 0.362 0.360 0.370 0.370 0.378 BART-base(#bs=1) (Lewis
    et al., [2020](#bib.bib65)) 0.270 0.323 0.366 0.424 0.270 0.336 0.271 0.321 0.322
    0.388 BART-large(#bs=1) (Lewis et al., [2020](#bib.bib65)) 0.276 0.333 0.380 0.435
    0.284 0.347 0.274 0.311 0.332 0.392 T5-base(#bs=1) (Raffel et al., [2020](#bib.bib106))
    0.288 0.339 0.388 0.440 0.302 0.350 0.295 0.326 0.336 0.388 T5-large(#bs=1) (Raffel
    et al., [2020](#bib.bib106)) 0.295 0.343 0.398 0.438 0.315 0.359 0.297 0.321 0.343
    0.393 KeyBART(#bs=1) (Kulkarni et al., [2022](#bib.bib60)) 0.268 0.325 0.373 0.430
    0.287 0.365 0.260 0.289 0.325 0.398 zero-shot ChatGPT(#bs=1) 0.309 0.428 0.338
    0.258 0.237 0.189 0.274 0.252 0.192 0.158 1-shot ChatGPT(#bs=1) 0.421 0.480 0.355
    0.359 0.297 0.298 0.319 0.326 0.298 0.295 5-shot ChatGPT(#bs=1) 0.431 0.497 0.365
    0.351 0.285 0.287 0.312 0.300 0.297 0.288'
- en: 'Table 9: Results of absent keyphrase prediction using large language models.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 9: 使用大型语言模型进行缺失关键词预测的结果。'
- en: '[width=0.8] Model Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5
    F1@M F1@5 F1@M F1@5 F1@M CatSeq(#bs=1) (Yuan et al., [2020](#bib.bib153)) 0.005
    0.009 0.015 0.026 0.018 0.034 0.015 0.022 0.014 0.030 Transformer(#bs=1) (Ye et al.,
    [2021b](#bib.bib151)) 0.008 0.017 0.028 0.050 0.030 0.055 0.016 0.022 0.021 0.043
    SetTrans(#bs=1) (Ye et al., [2021b](#bib.bib151)) 0.018 0.029 0.041 0.061 0.046
    0.073 0.029 0.035 0.035 0.056 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146))
    0.025 0.034 0.057 0.071 0.057 0.074 0.040 0.043 0.050 0.064 BART-base(#bs=1) (Lewis
    et al., [2020](#bib.bib65)) 0.010 0.017 0.026 0.042 0.028 0.049 0.016 0.021 0.022
    0.042 BART-large(#bs=1) (Lewis et al., [2020](#bib.bib65)) 0.015 0.024 0.031 0.048
    0.031 0.051 0.019 0.024 0.027 0.047 T5-base(#bs=1) (Raffel et al., [2020](#bib.bib106))
    0.011 0.020 0.027 0.051 0.023 0.043 0.014 0.020 0.017 0.034 T5-large(#bs=1) (Raffel
    et al., [2020](#bib.bib106)) 0.011 0.021 0.025 0.042 0.023 0.045 0.015 0.020 0.017
    0.035 KeyBART(#bs=1) (Kulkarni et al., [2022](#bib.bib60)) 0.014 0.023 0.031 0.055
    0.036 0.064 0.016 0.022 0.026 0.047 zero-shot ChatGPT(#bs=1) 0.014 0.027 0.003
    0.005 0.002 0.004 0.002 0.003 0.025 0.030 1-shot ChatGPT(#bs=1) 0.027 0.048 0.011
    0.017 0.015 0.028 0.009 0.011 0.015 0.027 5-shot ChatGPT(#bs=1) 0.028 0.046 0.010
    0.015 0.016 0.031 0.016 0.021 0.015 0.027'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[width=0.8] 模型 Inspec NUS Krapivin SemEval KP20k F1@5 F1@M F1@5 F1@M F1@5 F1@M
    F1@5 F1@M F1@5 F1@M CatSeq(#bs=1) (Yuan et al., [2020](#bib.bib153)) 0.005 0.009
    0.015 0.026 0.018 0.034 0.015 0.022 0.014 0.030 Transformer(#bs=1) (Ye et al.,
    [2021b](#bib.bib151)) 0.008 0.017 0.028 0.050 0.030 0.055 0.016 0.022 0.021 0.043
    SetTrans(#bs=1) (Ye et al., [2021b](#bib.bib151)) 0.018 0.029 0.041 0.061 0.046
    0.073 0.029 0.035 0.035 0.056 WR-SetTrans(#bs=1) (Xie et al., [2022](#bib.bib146))
    0.025 0.034 0.057 0.071 0.057 0.074 0.040 0.043 0.050 0.064 BART-base(#bs=1) (Lewis
    et al., [2020](#bib.bib65)) 0.010 0.017 0.026 0.042 0.028 0.049 0.016 0.021 0.022
    0.042 BART-large(#bs=1) (Lewis et al., [2020](#bib.bib65)) 0.015 0.024 0.031 0.048
    0.031 0.051 0.019 0.024 0.027 0.047 T5-base(#bs=1) (Raffel et al., [2020](#bib.bib106))
    0.011 0.020 0.027 0.051 0.023 0.043 0.014 0.020 0.017 0.034 T5-large(#bs=1) (Raffel
    et al., [2020](#bib.bib106)) 0.011 0.021 0.025 0.042 0.023 0.045 0.015 0.020 0.017
    0.035 KeyBART(#bs=1) (Kulkarni et al., [2022](#bib.bib60)) 0.014 0.023 0.031 0.055
    0.036 0.064 0.016 0.022 0.026 0.047 zero-shot ChatGPT(#bs=1) 0.014 0.027 0.003
    0.005 0.002 0.004 0.002 0.003 0.025 0.030 1-shot ChatGPT(#bs=1) 0.027 0.048 0.011
    0.017 0.015 0.028 0.009 0.011 0.015 0.027 5-shot ChatGPT(#bs=1) 0.028 0.046 0.010
    0.015 0.016 0.031 0.016 0.021 0.015 0.027'
- en: 7 Future Directions
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来方向
- en: 'In summary, automatic keyphrase prediction has attracted extensive attention
    from academia and industry currently. However, it still remains a challenging
    task in the following aspects:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，自动关键短语预测目前已受到学术界和工业界的广泛关注。然而，它仍然在以下方面保持挑战性：
- en: 1) The quality of generated absent keyphrases directly determines the availability
    of keyphrase generation models. However, dominant models are still unable to produce
    satisfactory absent keyphrases. Therefore, how to improve the prediction performance
    on absent keyphrases will be the focus of future research.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 生成的缺失关键短语的质量直接决定了关键短语生成模型的可用性。然而，主流模型仍然无法生成令人满意的缺失关键短语。因此，如何提高对缺失关键短语的预测性能将是未来研究的重点。
- en: 2) Intuitively, humans often exploit the information beyond the input document
    to predict keyphrases. Hence, how to fully exploit more information, such as the
    extra information from external knowledge base or pre-trained model, for better
    keyphrase predictions is worth exploring.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 从直观上看，人类通常利用输入文档之外的信息来预测关键短语。因此，如何充分利用更多的信息，例如来自外部知识库或预训练模型的额外信息，以改善关键短语预测，值得深入探索。
- en: 3) Short videos have recently emerged as a widespread type of social media due
    to the explosive growth of the Internet. Two new forms of multi-modal information
    introduced in the search and recommendation scenarios, video and audio, place
    additional demand on keyphrase prediction. Thus, we believe that multi-modal keyphrase
    prediction is also the future development trend of keyphrase prediction.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 由于互联网的爆炸性增长，短视频最近成为一种广泛使用的社交媒体类型。在搜索和推荐场景中引入的两种新的多模态信息——视频和音频，对关键短语预测提出了额外的要求。因此，我们认为多模态关键短语预测也是关键短语预测的未来发展趋势。
- en: 4) Existing studies mainly focus on using domain-specific data to train models,
    such as scientific documents. However, it is unable to handle different domains
    of data from the Internet. Consequently, how to effectively transfer these models
    to other domains becomes one problem to be solved in practical applications.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 现有研究主要集中于使用领域特定数据来训练模型，如科学文献。然而，这无法处理来自互联网的不同领域的数据。因此，如何有效地将这些模型迁移到其他领域，成为实际应用中需要解决的问题。
- en: 5) The conventional evaluation metrics mainly focus on the comparison between
    the surface representations of stemmed phrases. However, two phrases may possess
    the same meaning although their expressions are different. Hence, the quality
    evaluation of generated keyphrases should consider the comparison between semantic
    representations of phrases and the application effect in downstream tasks such
    as retrieval systems (Boudin et al., [2020](#bib.bib12)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 传统的评估指标主要关注词干短语表面表示之间的比较。然而，尽管表达不同，两短语可能具有相同的含义。因此，生成关键短语的质量评估应考虑短语的语义表示之间的比较以及在检索系统等下游任务中的应用效果（Boudin等人，[2020](#bib.bib12)）。
- en: 6) Dominant studies model the generations of present and absent keyphrases in
    a unified manner, although their prediction difficulties vary greatly. Intuitively,
    it is more reasonable to individually model the generations of absent and present
    keyphrases. Please note that Wu et al. ([2022b](#bib.bib145)) verifies the feasibility
    of this direction.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 6) 主流研究将当前和缺失关键短语的生成以统一的方式建模，尽管它们的预测难度差异很大。直观上，分别建模缺失和当前关键短语的生成更为合理。请注意，吴等人（[2022b](#bib.bib145)）验证了这一方向的可行性。
- en: 7) The generation of keyphrases can draw lesson from the process of human reading
    and refining keyphrases. For example, humans tend to distill the overall idea
    first and grasp the specifics later, and thus, an ideal process for keyphrase
    prediction is to predict keyphrase in a coarse-to-fine manner.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 7) 关键短语的生成可以借鉴人类阅读和提炼关键短语的过程。例如，人类往往首先提炼整体观点，然后把握细节，因此，关键短语预测的理想过程是以粗略到精细的方式进行预测。
- en: 8) Very recently, ChatGPT has demonstrated effectiveness proficiency across
    a range of NLP tasks. As such, it is imperative to explore the optimal utilization
    of ChatGPT in keyphrase prediction, in order to fully exert its remarkable potential.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 8) 最近，ChatGPT在各种自然语言处理任务中表现出了卓越的效果。因此，探索ChatGPT在关键短语预测中的最佳利用方式，以充分发挥其显著潜力是至关重要的。
- en: References
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahmad et al. (2021) Ahmad, W., Bai, X., Lee, S., Chang, K.W., 2021. Select,
    extract and generate: Neural keyphrase generation with layer-wise coverage attention,
    in: Proc. ACL-IJCNLP Conf., pp. 1389–1404. doi:[10.18653/v1/2021.acl-long.111](https:/doi.org/10.18653/v1/2021.acl-long.111).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad等人（2021）Ahmad, W., Bai, X., Lee, S., Chang, K.W., 2021. 选择、提取和生成：具有层次覆盖注意力的神经关键短语生成，见：Proc.
    ACL-IJCNLP Conf., 第1389–1404页。doi:[10.18653/v1/2021.acl-long.111](https:/doi.org/10.18653/v1/2021.acl-long.111)。
- en: 'Alami Merrouni et al. (2020) Alami Merrouni, Z., Frikh, B., Ouhbi, B., 2020.
    Automatic keyphrase extraction: a survey and trends. J. Intell. Inf. Syst. , 391–424doi:[10.1007/s10844-019-00558-9](https:/doi.org/10.1007/s10844-019-00558-9).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alami Merrouni等人（2020）Alami Merrouni, Z., Frikh, B., Ouhbi, B., 2020. 自动关键短语提取：调查与趋势。J.
    Intell. Inf. Syst., 391–424 doi:[10.1007/s10844-019-00558-9](https:/doi.org/10.1007/s10844-019-00558-9)。
- en: 'Asl and Banda (2020) Asl, J.R., Banda, J.M., 2020. Gleake: Global and local
    embedding automatic keyphrase extraction. CoRR doi:[10.48550/arXiv.2005.09740](https:/doi.org/10.48550/arXiv.2005.09740),
    [arXiv:2005.09740](http://arxiv.org/abs/2005.09740).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asl和Banda（2020）Asl, J.R., Banda, J.M., 2020. Gleake：全球与局部嵌入自动关键短语提取。CoRR doi:[10.48550/arXiv.2005.09740](https:/doi.org/10.48550/arXiv.2005.09740)，[arXiv:2005.09740](http://arxiv.org/abs/2005.09740)。
- en: 'Bahuleyan and Asri (2020) Bahuleyan, H., Asri, L.E., 2020. Diverse keyphrase
    generation with neural unlikelihood training, in: Proc. COLING Conf., pp. 5271--5287.
    doi:[10.18653/v1/2020.coling-main.462](https:/doi.org/10.18653/v1/2020.coling-main.462).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahuleyan和Asri（2020）Bahuleyan, H., Asri, L.E., 2020. 使用神经不相似性训练的多样化关键短语生成，见：Proc.
    COLING Conf., 第5271-5287页。doi:[10.18653/v1/2020.coling-main.462](https:/doi.org/10.18653/v1/2020.coling-main.462)。
- en: 'Beltagy et al. (2019) Beltagy, I., Lo, K., Cohan, A., 2019. Scibert: A pretrained
    language model for scientific text, in: Proc. EMNLP-IJCNLP Conf., pp. 3613--3618.
    doi:[10.48550/arXiv.1903.10676](https:/doi.org/10.48550/arXiv.1903.10676).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy等人（2019）Beltagy, I., Lo, K., Cohan, A., 2019. Scibert：用于科学文本的预训练语言模型，见：Proc.
    EMNLP-IJCNLP Conf., 第3613-3618页。doi:[10.48550/arXiv.1903.10676](https:/doi.org/10.48550/arXiv.1903.10676)。
- en: 'Beltagy et al. (2020) Beltagy, I., Peters, M.E., Cohan, A., 2020. Longformer:
    The long-document transformer. CoRR doi:[10.48550/ARXIV.2004.05150](https:/doi.org/10.48550/ARXIV.2004.05150).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy等人（2020）Beltagy, I., Peters, M.E., Cohan, A., 2020. Longformer: 长文档变换器。CoRR
    doi:[10.48550/ARXIV.2004.05150](https:/doi.org/10.48550/ARXIV.2004.05150)。'
- en: 'Bennani-Smires et al. (2018) Bennani-Smires, K., Musat, C., Jaggi, M., Hossmann,
    A., Baeriswyl, M., 2018. Embedrank: Unsupervised keyphrase extraction using sentence
    embeddings. CoRR doi:[10.48550/ARXIV.1801.04470](https:/doi.org/10.48550/ARXIV.1801.04470),
    [arXiv:1801.04470](http://arxiv.org/abs/1801.04470).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennani-Smires 等（2018）Bennani-Smires, K., Musat, C., Jaggi, M., Hossmann, A.,
    Baeriswyl, M., 2018. Embedrank：使用句子嵌入的无监督关键短语提取。CoRR doi:[10.48550/ARXIV.1801.04470](https:/doi.org/10.48550/ARXIV.1801.04470)，[arXiv:1801.04470](http://arxiv.org/abs/1801.04470)。
- en: 'Berend (2011) Berend, G., 2011. Opinion expression mining by exploiting keyphrase
    extraction, in: Proc. IJCNLP Conf., pp. 1162--1170.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berend（2011）Berend, G., 2011. 通过利用关键短语提取进行意见表达挖掘，见：Proc. IJCNLP Conf., 第1162--1170页。
- en: Blei et al. (2003) Blei, D.M., Ng, A.Y., Jordan, M.I., 2003. Latent dirichlet
    allocation. J. Mach. Learn. Res. , 993--1022.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blei 等（2003）Blei, D.M., Ng, A.Y., Jordan, M.I., 2003. 潜在狄利克雷分配。J. Mach. Learn.
    Res. , 第993--1022页。
- en: 'Boudin (2013) Boudin, F., 2013. TALN archives : a digital archive of french
    research articles in natural language processing, in: Proc. TALN Conf., pp. 507--514.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boudin（2013）Boudin, F., 2013. TALN 归档：法语自然语言处理研究文章的数字档案，见：Proc. TALN Conf.,
    第507--514页。
- en: 'Boudin (2018) Boudin, F., 2018. Unsupervised keyphrase extraction with multipartite
    graphs, in: Proc. NAACL, Short Papers Conf., pp. 667--672. doi:[10.18653/v1/n18-2105](https:/doi.org/10.18653/v1/n18-2105).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boudin（2018）Boudin, F., 2018. 使用多分图的无监督关键短语提取，见：Proc. NAACL, Short Papers Conf.,
    第667--672页。doi:[10.18653/v1/n18-2105](https:/doi.org/10.18653/v1/n18-2105)。
- en: 'Boudin et al. (2020) Boudin, F., Gallina, Y., Aizawa, A., 2020. Keyphrase generation
    for scientific document retrieval, in: Proc. ACL Conf., p. 00. doi:[10.18653/v1/2020.acl-main.105](https:/doi.org/10.18653/v1/2020.acl-main.105).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boudin 等（2020）Boudin, F., Gallina, Y., Aizawa, A., 2020. 科学文档检索的关键短语生成，见：Proc.
    ACL Conf., 第00页。doi:[10.18653/v1/2020.acl-main.105](https:/doi.org/10.18653/v1/2020.acl-main.105)。
- en: 'Bougouin et al. (2016) Bougouin, A., Barreaux, S., Romary, L., Boudin, F.,
    Daille, B., 2016. TermITH-eval: a French standard-based resource for keyphrase
    extraction evaluation, in: Proc. LREC Conf., pp. 1924--1927.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bougouin 等（2016）Bougouin, A., Barreaux, S., Romary, L., Boudin, F., Daille,
    B., 2016. TermITH-eval：基于法语标准的关键短语提取评估资源，见：Proc. LREC Conf., 第1924--1927页。
- en: 'Bougouin et al. (2013) Bougouin, A., Boudin, F., Daille, B., 2013. Topicrank:
    Graph-based topic ranking for keyphrase extraction, in: Proc. IJCNLP Conf., pp.
    543--551.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bougouin 等（2013）Bougouin, A., Boudin, F., Daille, B., 2013. Topicrank：基于图的主题排名用于关键短语提取，见：Proc.
    IJCNLP Conf., 第543--551页。
- en: 'Campos et al. (2018) Campos, R., Mangaravite, V., Pasquali, A., Jorge, A.M.,
    Nunes, C., Jatowt, A., 2018. Yake! collection-independent automatic keyword extractor,
    in: Proc. ECIR Conf., pp. 806--810. doi:[10.1007/978-3-319-76941-7_80](https:/doi.org/10.1007/978-3-319-76941-7_80).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campos 等（2018）Campos, R., Mangaravite, V., Pasquali, A., Jorge, A.M., Nunes,
    C., Jatowt, A., 2018. Yake! 集合独立的自动关键词提取器，见：Proc. ECIR Conf., 第806--810页。doi:[10.1007/978-3-319-76941-7_80](https:/doi.org/10.1007/978-3-319-76941-7_80)。
- en: 'Çano and Bojar (2019) Çano, E., Bojar, O., 2019. Keyphrase generation: A multi-aspect
    survey, in: Proc. FRUCT Conf., pp. 85--94. doi:[10.23919/FRUCT48121.2019.8981519](https:/doi.org/10.23919/FRUCT48121.2019.8981519).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Çano 和 Bojar（2019）Çano, E., Bojar, O., 2019. 关键短语生成：一个多方面的综述，见：Proc. FRUCT Conf.,
    第85--94页。doi:[10.23919/FRUCT48121.2019.8981519](https:/doi.org/10.23919/FRUCT48121.2019.8981519)。
- en: 'Caragea et al. (2014) Caragea, C., Bulgarov, F.A., Godea, A., Das Gollapalli,
    S., 2014. Citation-enhanced keyphrase extraction from research papers: A supervised
    approach, in: Proc. EMNLP Conf., pp. 1435--1446. doi:[10.3115/v1/d14-1150](https:/doi.org/10.3115/v1/d14-1150).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caragea 等（2014）Caragea, C., Bulgarov, F.A., Godea, A., Das Gollapalli, S., 2014.
    从研究论文中增强引文的关键短语提取：一种监督方法，见：Proc. EMNLP Conf., 第1435--1446页。doi:[10.3115/v1/d14-1150](https:/doi.org/10.3115/v1/d14-1150)。
- en: 'Chan et al. (2019) Chan, H.P., Chen, W., Wang, L., King, I., 2019. Neural keyphrase
    generation via reinforcement learning with adaptive rewards, in: Proc. ACL Conf.,
    p. 00. doi:[10.18653/v1/p19-1208](https:/doi.org/10.18653/v1/p19-1208).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等（2019）Chan, H.P., Chen, W., Wang, L., King, I., 2019. 通过强化学习与自适应奖励的神经关键短语生成，见：Proc.
    ACL Conf., 第00页。doi:[10.18653/v1/p19-1208](https:/doi.org/10.18653/v1/p19-1208)。
- en: 'Chau et al. (2020) Chau, H., Balaneshin, S., Liu, K., Linda, O., 2020. Understanding
    the tradeoff between cost and quality of expert annotations for keyphrase extraction,
    in: Proc. LAW@COLING Conf., pp. 74--86.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chau 等（2020）Chau, H., Balaneshin, S., Liu, K., Linda, O., 2020. 理解专家注释在关键短语提取中的成本与质量之间的权衡，见：Proc.
    LAW@COLING Conf., 第74--86页。
- en: 'Chen et al. (2018) Chen, J., Zhang, X., Wu, Y., Yan, Z., Li, Z., 2018. Keyphrase
    generation with correlation constraints, in: Proc. EMNLP Conf., pp. 4057--4066.
    doi:[10.18653/v1/d18-1439](https:/doi.org/10.18653/v1/d18-1439).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2018）Chen, J., Zhang, X., Wu, Y., Yan, Z., Li, Z., 2018. 带有相关性约束的关键短语生成，见：Proc.
    EMNLP Conf., 第4057--4066页。doi:[10.18653/v1/d18-1439](https:/doi.org/10.18653/v1/d18-1439)。
- en: 'Chen et al. (2019a) Chen, W., Chan, H.P., Li, P., Bing, L., King, I., 2019a.
    An integrated approach for keyphrase generation via exploring the power of retrieval
    and extraction, in: Proc. NAACL Conf., pp. 2846--2856. doi:[10.48550/arXiv.1904.03454](https:/doi.org/10.48550/arXiv.1904.03454).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019a) Chen, W., Chan, H.P., Li, P., Bing, L., King, I., 2019a.
    通过探索检索和提取的能力的集成方法生成关键短语，见：Proc. NAACL Conf., pp. 2846--2856. doi:[10.48550/arXiv.1904.03454](https:/doi.org/10.48550/arXiv.1904.03454)。
- en: 'Chen et al. (2020) Chen, W., Chan, H.P., Li, P., King, I., 2020. Exclusive
    hierarchical decoding for deep keyphrase generation, in: Proc. ACL Conf., pp.
    1095--1105. doi:[10.18653/v1/2020.acl-main.103](https:/doi.org/10.18653/v1/2020.acl-main.103).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Chen, W., Chan, H.P., Li, P., King, I., 2020. 深度关键短语生成的独占层次解码，见：Proc.
    ACL Conf., pp. 1095--1105. doi:[10.18653/v1/2020.acl-main.103](https:/doi.org/10.18653/v1/2020.acl-main.103)。
- en: 'Chen et al. (2019b) Chen, W., Gao, Y., Zhang, J., King, I., Lyu, M.R., 2019b.
    Title-guided encoding for keyphrase generation, in: Proc. AAAI Conf., pp. 6268--6275.
    doi:[10.1609/aaai.v33i01.33016268](https:/doi.org/10.1609/aaai.v33i01.33016268).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019b) Chen, W., Gao, Y., Zhang, J., King, I., Lyu, M.R., 2019b.
    基于标题引导的关键短语生成编码，见：Proc. AAAI Conf., pp. 6268--6275. doi:[10.1609/aaai.v33i01.33016268](https:/doi.org/10.1609/aaai.v33i01.33016268)。
- en: 'Chowdhury et al. (2019) Chowdhury, J.R., Caragea, C., Caragea, D., 2019. Keyphrase
    extraction from disaster-related tweets, in: Proc. WWW Conf., pp. 1555--1566.
    doi:[10.1145/3308558.3313696](https:/doi.org/10.1145/3308558.3313696).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury et al. (2019) Chowdhury, J.R., Caragea, C., Caragea, D., 2019. 从灾难相关推文中提取关键短语，见：Proc.
    WWW Conf., pp. 1555--1566. doi:[10.1145/3308558.3313696](https:/doi.org/10.1145/3308558.3313696)。
- en: Chowdhury et al. (2022) Chowdhury, M.F.M., Rossiello, G., Glass, M.R., Mihindukulasooriya,
    N., Gliozzo, A., 2022. Applying a generic sequence-to-sequence model for simple
    and effective keyphrase generation. CoRR doi:[10.48550/ARXIV.2201.05302](https:/doi.org/10.48550/ARXIV.2201.05302),
    [arXiv:2201.05302](http://arxiv.org/abs/2201.05302).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury et al. (2022) Chowdhury, M.F.M., Rossiello, G., Glass, M.R., Mihindukulasooriya,
    N., Gliozzo, A., 2022. 应用通用序列到序列模型进行简单而有效的关键短语生成。CoRR doi:[10.48550/ARXIV.2201.05302](https:/doi.org/10.48550/ARXIV.2201.05302)，[arXiv:2201.05302](http://arxiv.org/abs/2201.05302)。
- en: 'Clarke et al. (2008) Clarke, C.L., Kolla, M., Cormack, G.V., Vechtomova, O.,
    Ashkan, A., Büttcher, S., MacKinnon, I., 2008. Novelty and diversity in information
    retrieval evaluation, in: Proc. SIGIR Conf., pp. 659--666. doi:[10.1145/1390334.1390446](https:/doi.org/10.1145/1390334.1390446).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clarke et al. (2008) Clarke, C.L., Kolla, M., Cormack, G.V., Vechtomova, O.,
    Ashkan, A., Büttcher, S., MacKinnon, I., 2008. 信息检索评估中的新颖性和多样性，见：Proc. SIGIR Conf.,
    pp. 659--666. doi:[10.1145/1390334.1390446](https:/doi.org/10.1145/1390334.1390446)。
- en: 'Danesh et al. (2015) Danesh, S., Sumner, T., Martin, J.H., 2015. SGRank: Combining
    statistical and graphical methods to improve the state of the art in unsupervised
    keyphrase extraction, in: Proc. *SEM@NAACL Conf., pp. 117--126. doi:[10.18653/v1/s15-1013](https:/doi.org/10.18653/v1/s15-1013).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Danesh et al. (2015) Danesh, S., Sumner, T., Martin, J.H., 2015. SGRank: 结合统计和图形方法提高无监督关键短语提取的最新水平，见：*SEM@NAACL
    Conf., pp. 117--126. doi:[10.18653/v1/s15-1013](https:/doi.org/10.18653/v1/s15-1013)。'
- en: Diao et al. (2020) Diao, S., Song, Y., Zhang, T., 2020. Keyphrase generation
    with cross-document attention. CoRR doi:[10.48550/arXiv.2004.09800](https:/doi.org/10.48550/arXiv.2004.09800).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao et al. (2020) Diao, S., Song, Y., Zhang, T., 2020. 跨文档注意力的关键短语生成。CoRR doi:[10.48550/arXiv.2004.09800](https:/doi.org/10.48550/arXiv.2004.09800)。
- en: 'Ding and Luo (2021) Ding, H., Luo, X., 2021. AttentionRank: Unsupervised keyphrase
    extraction using self and cross attentions, in: Proc. EMNLP Conf., pp. 1919--1928.
    doi:[10.18653/v1/2021.emnlp-main.146](https:/doi.org/10.18653/v1/2021.emnlp-main.146).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding and Luo (2021) Ding, H., Luo, X., 2021. AttentionRank: 使用自我和交叉注意力的无监督关键短语提取，见：Proc.
    EMNLP Conf., pp. 1919--1928. doi:[10.18653/v1/2021.emnlp-main.146](https:/doi.org/10.18653/v1/2021.emnlp-main.146)。'
- en: 'Dong et al. (2019) Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y.,
    Gao, J., Zhou, M., Hon, H., 2019. Unified language model pre-training for natural
    language understanding and generation, in: Proc. NeurIPS Conf., pp. 13042--13054.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019) Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y.,
    Gao, J., Zhou, M., Hon, H., 2019. 统一语言模型预训练用于自然语言理解和生成，见：Proc. NeurIPS Conf.,
    pp. 13042--13054。
- en: 'Doostmohammadi et al. (2018) Doostmohammadi, E., Bokaei, M.H., Sameti, H.,
    2018. Perkey: A persian news corpus for keyphrase extraction and generation, in:
    Proc. IST Conf., pp. 460--465. doi:[10.1109/ISTEL.2018.8661095](https:/doi.org/10.1109/ISTEL.2018.8661095).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Doostmohammadi et al. (2018) Doostmohammadi, E., Bokaei, M.H., Sameti, H.,
    2018. Perkey: 一个用于关键短语提取和生成的波斯新闻语料库，见：Proc. IST Conf., pp. 460--465. doi:[10.1109/ISTEL.2018.8661095](https:/doi.org/10.1109/ISTEL.2018.8661095)。'
- en: 'El-Beltagy and Rafea (2009) El-Beltagy, S.R., Rafea, A.A., 2009. Kp-miner:
    A keyphrase extraction system for english and arabic documents. Inf. Syst. , 132--144doi:[10.1016/j.is.2008.05.002](https:/doi.org/10.1016/j.is.2008.05.002).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El-Beltagy 和 Rafea（2009）El-Beltagy, S.R., Rafea, A.A., 2009. Kp-miner：一个用于英语和阿拉伯语文档的关键短语提取系统。信息系统，132--144。doi：[10.1016/j.is.2008.05.002](https:/doi.org/10.1016/j.is.2008.05.002)。
- en: 'Florescu and Caragea (2017) Florescu, C., Caragea, C., 2017. PositionRank:
    An unsupervised approach to keyphrase extraction from scholarly documents, in:
    Proc. ACL Conf., pp. 1105--1115. doi:[10.18653/v1/P17-1102](https:/doi.org/10.18653/v1/P17-1102).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florescu 和 Caragea（2017）Florescu, C., Caragea, C., 2017. PositionRank：一种无监督的学术文档关键短语提取方法，见：Proc.
    ACL Conf., 页 1105--1115。doi：[10.18653/v1/P17-1102](https:/doi.org/10.18653/v1/P17-1102)。
- en: 'Frank et al. (1999) Frank, E., Paynter, G.W., Witten, I.H., Gutwin, C., Nevill-Manning,
    C.G., 1999. Domain-specific keyphrase extraction, in: Proc. IJCAI Conf., pp. 668--673.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frank 等人（1999）Frank, E., Paynter, G.W., Witten, I.H., Gutwin, C., Nevill-Manning,
    C.G., 1999. 特定领域关键短语提取，见：Proc. IJCAI Conf., 页 668--673。
- en: 'Gallina et al. (2019) Gallina, Y., Boudin, F., Daille, B., 2019. Kptimes: A
    large-scale dataset for keyphrase generation on news documents, in: Proc. INLG
    Conf., pp. 130--135. doi:[10.18653/v1/W19-8617](https:/doi.org/10.18653/v1/W19-8617).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gallina 等人（2019）Gallina, Y., Boudin, F., Daille, B., 2019. Kptimes：一个用于新闻文档关键短语生成的大规模数据集，见：Proc.
    INLG Conf., 页 130--135。doi：[10.18653/v1/W19-8617](https:/doi.org/10.18653/v1/W19-8617)。
- en: 'Garg et al. (2020) Garg, A., Kagi, S.S., Singh, M., 2020. SEAL: scientific
    keyphrase extraction and classification, in: Proc. JCDL Conf., pp. 527--528. doi:[10.1145/3383583.3398625](https:/doi.org/10.1145/3383583.3398625).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg 等人（2020）Garg, A., Kagi, S.S., Singh, M., 2020. SEAL：科学关键短语提取与分类，见：Proc.
    JCDL Conf., 页 527--528。doi：[10.1145/3383583.3398625](https:/doi.org/10.1145/3383583.3398625)。
- en: Garg et al. (2021) Garg, K., Chowdhury, J.R., Caragea, C., 2021. Keyphrase generation
    beyond the boundaries of title and abstract. CoRR doi:[10.48550/ARXIV.2112.06776](https:/doi.org/10.48550/ARXIV.2112.06776),
    [arXiv:2112.06776](http://arxiv.org/abs/2112.06776).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg 等人（2021）Garg, K., Chowdhury, J.R., Caragea, C., 2021. 超越标题和摘要的关键短语生成。CoRR
    doi：[10.48550/ARXIV.2112.06776](https:/doi.org/10.48550/ARXIV.2112.06776)，[arXiv:2112.06776](http://arxiv.org/abs/2112.06776)。
- en: 'Gero and Ho (2021) Gero, Z., Ho, J.C., 2021. Word centrality constrained representation
    for keyphrase extraction, in: Proc. BioNLP@NAACL Conf., pp. 155--161. doi:[10.18653/v1/2021.bionlp-1.17](https:/doi.org/10.18653/v1/2021.bionlp-1.17).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gero 和 Ho（2021）Gero, Z., Ho, J.C., 2021. 基于词中心性约束的关键短语提取表示，见：Proc. BioNLP@NAACL
    Conf., 页 155--161。doi：[10.18653/v1/2021.bionlp-1.17](https:/doi.org/10.18653/v1/2021.bionlp-1.17)。
- en: 'Gollapalli and Caragea (2014) Gollapalli, S.D., Caragea, C., 2014. Extracting
    keyphrases from research papers using citation networks, in: Proc. AAAI Conf.,
    pp. 1629--1635. doi:[10.1609/aaai.v28i1.8946](https:/doi.org/10.1609/aaai.v28i1.8946).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gollapalli 和 Caragea（2014）Gollapalli, S.D., Caragea, C., 2014. 使用引用网络从研究论文中提取关键短语，见：Proc.
    AAAI Conf., 页 1629--1635。doi：[10.1609/aaai.v28i1.8946](https:/doi.org/10.1609/aaai.v28i1.8946)。
- en: 'Gollapalli et al. (2017) Gollapalli, S.D., Li, X., Yang, P., 2017. Incorporating
    expert knowledge into keyphrase extraction, in: Proc. AAAI Conf., pp. 3180--3187.
    doi:[10.1609/aaai.v31i1.10986](https:/doi.org/10.1609/aaai.v31i1.10986).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gollapalli 等人（2017）Gollapalli, S.D., Li, X., Yang, P., 2017. 将专家知识融入关键短语提取，见：Proc.
    AAAI Conf., 页 3180--3187。doi：[10.1609/aaai.v31i1.10986](https:/doi.org/10.1609/aaai.v31i1.10986)。
- en: 'Grineva et al. (2009) Grineva, M., Grinev, M., Lizorkin, D., 2009. Extracting
    key terms from noisy and multitheme documents, in: Proc. WWW Conf., pp. 661--670.
    doi:[10.1145/1526709.1526798](https:/doi.org/10.1145/1526709.1526798).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grineva 等人（2009）Grineva, M., Grinev, M., Lizorkin, D., 2009. 从嘈杂和多主题文档中提取关键术语，见：Proc.
    WWW Conf., 页 661--670。doi：[10.1145/1526709.1526798](https:/doi.org/10.1145/1526709.1526798)。
- en: 'Gu et al. (2016) Gu, J., Lu, Z., Li, H., Li, V.O., 2016. Incorporating copying
    mechanism in sequence-to-sequence learning, in: Proc. ACL Conf., pp. 1631--1640.
    doi:[10.18653/v1/P16-1154](https:/doi.org/10.18653/v1/P16-1154).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2016）Gu, J., Lu, Z., Li, H., Li, V.O., 2016. 在序列到序列学习中融入复制机制，见：Proc. ACL
    Conf., 页 1631--1640。doi：[10.18653/v1/P16-1154](https:/doi.org/10.18653/v1/P16-1154)。
- en: 'Gu et al. (2021) Gu, X., Wang, Z., Bi, Z., Meng, Y., Liu, L., Han, J., Shang,
    J., 2021. Ucphrase: Unsupervised context-aware quality phrase tagging, in: Proc.
    KDD Conf., pp. 478--486. doi:[10.1145/3447548.3467397](https:/doi.org/10.1145/3447548.3467397).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2021）Gu, X., Wang, Z., Bi, Z., Meng, Y., Liu, L., Han, J., Shang, J.,
    2021. Ucphrase：无监督的上下文感知优质短语标注，见：Proc. KDD Conf., 页 478--486。doi：[10.1145/3447548.3467397](https:/doi.org/10.1145/3447548.3467397)。
- en: Gutwin et al. (1999) Gutwin, C., Paynter, G.W., Witten, I.H., Nevill-Manning,
    C.G., Frank, E., 1999. Improving browsing in digital libraries with keyphrase
    indexes. Decis. Support Syst. , 81--104doi:[10.1016/S0167-9236(99)00038-X](https:/doi.org/10.1016/S0167-9236(99)00038-X).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gutwin 等 (1999) Gutwin, C., Paynter, G.W., Witten, I.H., Nevill-Manning, C.G.,
    Frank, E., 1999. 利用关键词索引改进数字图书馆的浏览。Decis. Support Syst. , 81--104 doi:[10.1016/S0167-9236(99)00038-X](https:/doi.org/10.1016/S0167-9236(99)00038-X)。
- en: 'Habibi and Popescu-Belis (2013) Habibi, M., Popescu-Belis, A., 2013. Diverse
    keyword extraction from conversations, in: Proc. ACL, Short Papers Conf., pp.
    651--657.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Habibi 和 Popescu-Belis (2013) Habibi, M., Popescu-Belis, A., 2013. 从对话中提取多样化关键词，见：Proc.
    ACL, Short Papers Conf., pp. 651--657。
- en: Haddoud and Abdeddaïm (2014) Haddoud, M., Abdeddaïm, S., 2014. Accurate keyphrase
    extraction by discriminating overlapping phrases. J. Inf. Sci. , 488--500doi:[10.1177/0165551514530210](https:/doi.org/10.1177/0165551514530210).
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haddoud 和 Abdeddaïm (2014) Haddoud, M., Abdeddaïm, S., 2014. 通过区分重叠短语进行准确的关键词短语提取。J.
    Inf. Sci. , 488--500 doi:[10.1177/0165551514530210](https:/doi.org/10.1177/0165551514530210)。
- en: 'Hasan and Ng (2014) Hasan, K.S., Ng, V., 2014. Automatic keyphrase extraction:
    A survey of the state of the art, in: Proc. ACL Conf., pp. 1262--1273. doi:[10.3115/v1/p14-1119](https:/doi.org/10.3115/v1/p14-1119).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan 和 Ng (2014) Hasan, K.S., Ng, V., 2014. 自动关键词短语提取：技术现状的综述，见：Proc. ACL Conf.,
    pp. 1262--1273。doi:[10.3115/v1/p14-1119](https:/doi.org/10.3115/v1/p14-1119)。
- en: 'Hulth (2003) Hulth, A., 2003. Improved automatic keyword extraction given more
    linguistic knowledge, in: Proc. EMNLP Conf., pp. 216--223. doi:[10.3115/1119355.1119383](https:/doi.org/10.3115/1119355.1119383).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hulth (2003) Hulth, A., 2003. 在获得更多语言知识的情况下改进自动关键词提取，见：Proc. EMNLP Conf., pp.
    216--223。doi:[10.3115/1119355.1119383](https:/doi.org/10.3115/1119355.1119383)。
- en: 'Hulth and Megyesi (2006) Hulth, A., Megyesi, B., 2006. A study on automatically
    extracted keywords in text categorization, in: Proc. ACL Conf., pp. 537--544.
    doi:[10.3115/1220175.1220243](https:/doi.org/10.3115/1220175.1220243).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hulth 和 Megyesi (2006) Hulth, A., Megyesi, B., 2006. 关于文本分类中自动提取关键词的研究，见：Proc.
    ACL Conf., pp. 537--544。doi:[10.3115/1220175.1220243](https:/doi.org/10.3115/1220175.1220243)。
- en: Järvelin and Kekäläinen (2002) Järvelin, K., Kekäläinen, J., 2002. Cumulated
    gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. , 422--446doi:[10.1145/582415.582418](https:/doi.org/10.1145/582415.582418).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Järvelin 和 Kekäläinen (2002) Järvelin, K., Kekäläinen, J., 2002. 基于累积增益的 IR
    技术评估。ACM Trans. Inf. Syst. , 422--446 doi:[10.1145/582415.582418](https:/doi.org/10.1145/582415.582418)。
- en: 'Jiang et al. (2009) Jiang, X., Hu, Y., Li, H., 2009. A ranking approach to
    keyphrase extraction, in: Proc. SIGIR Conf., pp. 756--757. doi:[10.1145/1571941.1572113](https:/doi.org/10.1145/1571941.1572113).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2009) Jiang, X., Hu, Y., Li, H., 2009. 一种用于关键词短语提取的排序方法，见：Proc. SIGIR
    Conf., pp. 756--757。doi:[10.1145/1571941.1572113](https:/doi.org/10.1145/1571941.1572113)。
- en: Joshi et al. (2022) Joshi, R., Balachandran, V., Saldanha, E., Glenski, M.,
    Volkova, S., Tsvetkov, Y., 2022. Unsupervised keyphrase extraction via interpretable
    neural networks. CoRR doi:[10.48550/ARXIV.2203.07640](https:/doi.org/10.48550/ARXIV.2203.07640).
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等 (2022) Joshi, R., Balachandran, V., Saldanha, E., Glenski, M., Volkova,
    S., Tsvetkov, Y., 2022. 通过可解释的神经网络进行无监督关键词短语提取。CoRR doi:[10.48550/ARXIV.2203.07640](https:/doi.org/10.48550/ARXIV.2203.07640)。
- en: 'Kelleher and Luz (2005) Kelleher, D., Luz, S., 2005. Automatic hypertext keyphrase
    detection, in: Proc. IJCAI Conf., pp. 1608--1609.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kelleher 和 Luz (2005) Kelleher, D., Luz, S., 2005. 自动超文本关键词短语检测，见：Proc. IJCAI
    Conf., pp. 1608--1609。
- en: 'Kim et al. (2021a) Kim, J., Jeong, M., Choi, S., Hwang, S., 2021a. Structure-augmented
    keyphrase generation, in: Proc. EMNLP Conf., pp. 2657--2667. doi:[10.18653/v1/2021.emnlp-main.209](https:/doi.org/10.18653/v1/2021.emnlp-main.209).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2021a) Kim, J., Jeong, M., Choi, S., Hwang, S., 2021a. 结构增强的关键词短语生成，见：Proc.
    EMNLP Conf., pp. 2657--2667。doi:[10.18653/v1/2021.emnlp-main.209](https:/doi.org/10.18653/v1/2021.emnlp-main.209)。
- en: 'Kim et al. (2021b) Kim, J., Song, Y., Hwang, S., 2021b. Web document encoding
    for structure-aware keyphrase extraction, in: Proc. SIGIR Conf., pp. 1823--1827.
    doi:[10.1145/3404835.3463067](https:/doi.org/10.1145/3404835.3463067).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2021b) Kim, J., Song, Y., Hwang, S., 2021b. 针对结构感知的网页文档编码进行关键词短语提取，见：Proc.
    SIGIR Conf., pp. 1823--1827。doi:[10.1145/3404835.3463067](https:/doi.org/10.1145/3404835.3463067)。
- en: 'Kim et al. (2010) Kim, S.N., Medelyan, O., Kan, M., Baldwin, T., 2010. Semeval-2010
    task 5 : Automatic keyphrase extraction from scientific articles, in: Proc. SemEval@ACL
    Conf., pp. 21--26. doi:[10.1007/s10579-012-9210-3](https:/doi.org/10.1007/s10579-012-9210-3).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2010) Kim, S.N., Medelyan, O., Kan, M., Baldwin, T., 2010. Semeval-2010
    任务 5：从科学文章中自动提取关键词短语，见：Proc. SemEval@ACL Conf., pp. 21--26。doi:[10.1007/s10579-012-9210-3](https:/doi.org/10.1007/s10579-012-9210-3)。
- en: 'Kontoulis et al. (2021) Kontoulis, C.G., Papagiannopoulou, E., Tsoumakas, G.,
    2021. Keyphrase extraction from scientific articles via extractive summarization,
    in: Proc. SDP@NAACL Conf., pp. 49--55. doi:[10.18653/v1/2021.sdp-1.6](https:/doi.org/10.18653/v1/2021.sdp-1.6).'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kontoulis 等人（2021）Kontoulis, C.G., Papagiannopoulou, E., Tsoumakas, G., 2021.
    通过提取摘要从科学文章中提取关键词，发表于：SDP@NAACL 会议，页码 49--55。doi:[10.18653/v1/2021.sdp-1.6](https:/doi.org/10.18653/v1/2021.sdp-1.6)。
- en: Krapivin et al. (2009) Krapivin, M., Autaeu, A., Marchese, M., 2009. Large dataset
    for keyphrases extraction. University of Trento .
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krapivin 等人（2009）Krapivin, M., Autaeu, A., Marchese, M., 2009. 大型关键词提取数据集。特伦托大学。
- en: Kuhn (1955) Kuhn, H.W., 1955. The hungarian method for the assignment problem.
    Nav. Res. Logist. Q. , 83--97doi:[10.1007/978-3-540-68279-0_2](https:/doi.org/10.1007/978-3-540-68279-0_2).
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhn（1955）Kuhn, H.W., 1955. 匈牙利算法解决分配问题。海军研究物流季刊，83--97。doi:[10.1007/978-3-540-68279-0_2](https:/doi.org/10.1007/978-3-540-68279-0_2)。
- en: 'Kulkarni et al. (2022) Kulkarni, M., Mahata, D., Arora, R., Bhowmik, R., 2022.
    Learning rich representation of keyphrases from text, in: Findings of the NAACL,
    pp. 891--906. doi:[10.18653/v1/2022.findings-naacl.67](https:/doi.org/10.18653/v1/2022.findings-naacl.67).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等人（2022）Kulkarni, M., Mahata, D., Arora, R., Bhowmik, R., 2022. 从文本中学习丰富的关键词表示，发表于：NAACL
    会议，页码 891--906。doi:[10.18653/v1/2022.findings-naacl.67](https:/doi.org/10.18653/v1/2022.findings-naacl.67)。
- en: 'Lai et al. (2020) Lai, T.M., Bui, T., Kim, D.S., Tran, Q.H., 2020. A joint
    learning approach based on self-distillation for keyphrase extraction from scientific
    documents, in: Proc. COLING Conf., pp. 649--656. doi:[10.48550/arXiv.2010.11980](https:/doi.org/10.48550/arXiv.2010.11980).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等人（2020）Lai, T.M., Bui, T., Kim, D.S., Tran, Q.H., 2020. 基于自蒸馏的联合学习方法用于从科学文献中提取关键词，发表于：COLING
    会议，页码 649--656。doi:[10.48550/arXiv.2010.11980](https:/doi.org/10.48550/arXiv.2010.11980)。
- en: 'Lancioni et al. (2020) Lancioni, G., S.Mohamed, S., Portelli, B., Serra, G.,
    Tasso, C., 2020. Keyphrase generation with GANs in low-resources scenarios, in:
    Proc. SustaiNLP@EMNLP Conf., pp. 89--96. doi:[10.18653/v1/2020.sustainlp-1.12](https:/doi.org/10.18653/v1/2020.sustainlp-1.12).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lancioni 等人（2020）Lancioni, G., S.Mohamed, S., Portelli, B., Serra, G., Tasso,
    C., 2020. 在资源匮乏的场景中使用GAN进行关键词生成，发表于：SustaiNLP@EMNLP 会议，页码 89--96。doi:[10.18653/v1/2020.sustainlp-1.12](https:/doi.org/10.18653/v1/2020.sustainlp-1.12)。
- en: 'Le and Mikolov (2014) Le, Q., Mikolov, T., 2014. Distributed representations
    of sentences and documents, in: Proc. ICML Conf., pp. 1188--1196.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 和 Mikolov（2014）Le, Q., Mikolov, T., 2014. 句子和文档的分布式表示，发表于：ICML 会议，页码 1188--1196。
- en: 'Lei et al. (2021) Lei, Y., Hu, C., Ma, G., Zhang, R., 2021. Keyphrase extraction
    with incomplete annotated training data, in: Proc. W-NUT Conf., pp. 26--34. doi:[10.18653/v1/2021.wnut-1.4](https:/doi.org/10.18653/v1/2021.wnut-1.4).'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人（2021）Lei, Y., Hu, C., Ma, G., Zhang, R., 2021. 使用不完整标注训练数据的关键词提取，发表于：W-NUT
    会议，页码 26--34。doi:[10.18653/v1/2021.wnut-1.4](https:/doi.org/10.18653/v1/2021.wnut-1.4)。
- en: 'Lewis et al. (2020) Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed,
    A., Levy, O., Stoyanov, V., Zettlemoyer, L., 2020. BART: Denoising sequence-to-sequence
    pre-training for natural language generation, translation, and comprehension,
    in: Proc. ACL Conf., pp. 7871--7880. doi:[10.18653/v1/2020.acl-main.703](https:/doi.org/10.18653/v1/2020.acl-main.703).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A.,
    Levy, O., Stoyanov, V., Zettlemoyer, L., 2020. BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练，发表于：ACL
    会议，页码 7871--7880。doi:[10.18653/v1/2020.acl-main.703](https:/doi.org/10.18653/v1/2020.acl-main.703)。
- en: 'Li and Daoutis (2021) Li, X., Daoutis, M., 2021. Unsupervised key-phrase extraction
    and clustering for classification scheme in scientific publications, in: Proc.
    SDU@AAAI Conf., p. 0. doi:[10.48550/ARXIV.2101.09990](https:/doi.org/10.48550/ARXIV.2101.09990).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Daoutis（2021）Li, X., Daoutis, M., 2021. 无监督关键词提取和聚类用于科学出版物的分类方案，发表于：SDU@AAAI
    会议，页码 0。doi:[10.48550/ARXIV.2101.09990](https:/doi.org/10.48550/ARXIV.2101.09990)。
- en: 'Liang et al. (2021) Liang, X., Wu, S., Li, M., Li, Z., 2021. Unsupervised keyphrase
    extraction by jointly modeling local and global context, in: Proc. EMNLP Conf.,
    pp. 155--164. doi:[10.18653/v1/2021.emnlp-main.14](https:/doi.org/10.18653/v1/2021.emnlp-main.14).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2021）Liang, X., Wu, S., Li, M., Li, Z., 2021. 通过联合建模局部和全局上下文的无监督关键词提取，发表于：EMNLP
    会议，页码 155--164。doi:[10.18653/v1/2021.emnlp-main.14](https:/doi.org/10.18653/v1/2021.emnlp-main.14)。
- en: Liang and Zaki (2021) Liang, Y., Zaki, M.J., 2021. Keyphrase extraction using
    neighborhood knowledge based on word embeddings. CoRR doi:[10.48550/arXiv.2111.07198](https:/doi.org/10.48550/arXiv.2111.07198).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 和 Zaki（2021）Liang, Y., Zaki, M.J., 2021. 使用基于词嵌入的邻域知识进行关键词提取。CoRR doi:[10.48550/arXiv.2111.07198](https:/doi.org/10.48550/arXiv.2111.07198)。
- en: 'Liu et al. (2021) Liu, R., Lin, Z., Wang, W., 2021. Addressing extraction and
    generation separately: Keyphrase prediction with pre-trained language models.
    IEEE ACM Trans. Audio Speech Lang. Process. , 3180--3191doi:[10.1109/TASLP.2021.3120587](https:/doi.org/10.1109/TASLP.2021.3120587).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2021) Liu, R., Lin, Z., Wang, W., 2021. 分别处理提取和生成：使用预训练语言模型进行关键短语预测。IEEE
    ACM Trans. Audio Speech Lang. Process. , 3180--3191doi:[10.1109/TASLP.2021.3120587](https:/doi.org/10.1109/TASLP.2021.3120587)。
- en: 'Liu et al. (2010) Liu, Z., Huang, W., Zheng, Y., Sun, M., 2010. Automatic keyphrase
    extraction via topic decomposition, in: Proc. EMNLP Conf., pp. 366--376.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2010) Liu, Z., Huang, W., Zheng, Y., Sun, M., 2010. 通过主题分解进行自动关键短语提取，见：Proc.
    EMNLP Conf., pp. 366--376。
- en: 'Liu et al. (2009) Liu, Z., Li, P., Zheng, Y., Sun, M., 2009. Clustering to
    find exemplar terms for keyphrase extraction, in: Proc. EMNLP Conf., pp. 257--266.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2009) Liu, Z., Li, P., Zheng, Y., Sun, M., 2009. 聚类以寻找关键短语提取的示例术语，见：Proc.
    EMNLP Conf., pp. 257--266。
- en: 'Liu et al. (2018) Liu, Z., Xiong, C., Sun, M., Liu, Z., 2018. Entity-duet neural
    ranking: Understanding the role of knowledge graph semantics in neural information
    retrieval, in: Proc. ACL Conf., pp. 2395--2405. doi:[10.18653/v1/P18-1223](https:/doi.org/10.18653/v1/P18-1223).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2018) Liu, Z., Xiong, C., Sun, M., Liu, Z., 2018. 实体-双重神经排序：理解知识图谱语义在神经信息检索中的作用，见：Proc.
    ACL Conf., pp. 2395--2405. doi:[10.18653/v1/P18-1223](https:/doi.org/10.18653/v1/P18-1223)。
- en: 'Lopez and Romary (2010) Lopez, P., Romary, L., 2010. HUMB: Automatic key term
    extraction from scientific articles in GROBID, in: Proc. SemEval@ACL Conf., pp.
    248--251.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lopez 和 Romary (2010) Lopez, P., Romary, L., 2010. HUMB: 在GROBID中从科学文章中自动提取关键术语，见：Proc.
    SemEval@ACL Conf., pp. 248--251。'
- en: Lu and Chow (2021) Lu, X., Chow, T.W.S., 2021. Duration modeling with semi-markov
    conditional random fields for keyphrase extraction. IEEE Trans. Knowl. Data Eng.
    , 1453--1466doi:[10.1109/TKDE.2019.2942295](https:/doi.org/10.1109/TKDE.2019.2942295).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 和 Chow (2021) Lu, X., Chow, T.W.S., 2021. 使用半马尔可夫条件随机场进行时长建模以提取关键短语。IEEE
    Trans. Knowl. Data Eng. , 1453--1466doi:[10.1109/TKDE.2019.2942295](https:/doi.org/10.1109/TKDE.2019.2942295)。
- en: 'Luan et al. (2017) Luan, Y., Ostendorf, M., Hajishirzi, H., 2017. Scientific
    information extraction with semi-supervised neural tagging, in: Proc. EMNLP Conf.,
    pp. 2641--2651. doi:[10.48550/arXiv.1708.06075](https:/doi.org/10.48550/arXiv.1708.06075).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luan 等 (2017) Luan, Y., Ostendorf, M., Hajishirzi, H., 2017. 使用半监督神经标记进行科学信息提取，见：Proc.
    EMNLP Conf., pp. 2641--2651. doi:[10.48550/arXiv.1708.06075](https:/doi.org/10.48550/arXiv.1708.06075)。
- en: 'Luo et al. (2020) Luo, Y., Li, Z., Wang, B., Xing, X., Zhang, Q., Huang, X.,
    2020. Sensenet: Neural keyphrase generation with document structure. CoRR doi:[10.48550/arXiv.2012.06754](https:/doi.org/10.48550/arXiv.2012.06754).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo 等 (2020) Luo, Y., Li, Z., Wang, B., Xing, X., Zhang, Q., Huang, X., 2020.
    Sensenet: 使用文档结构的神经网络关键短语生成。CoRR doi:[10.48550/arXiv.2012.06754](https:/doi.org/10.48550/arXiv.2012.06754)。'
- en: 'Luo et al. (2021) Luo, Y., Xu, Y., Ye, J., Qiu, X., Zhang, Q., 2021. Keyphrase
    generation with fine-grained evaluation-guided reinforcement learning, in: Proc.
    Findings of EMNLP Conf., pp. 497--507. doi:[10.18653/v1/2021.findings-emnlp.45](https:/doi.org/10.18653/v1/2021.findings-emnlp.45).'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等 (2021) Luo, Y., Xu, Y., Ye, J., Qiu, X., Zhang, Q., 2021. 基于细粒度评价引导的强化学习进行关键短语生成，见：Proc.
    Findings of EMNLP Conf., pp. 497--507. doi:[10.18653/v1/2021.findings-emnlp.45](https:/doi.org/10.18653/v1/2021.findings-emnlp.45)。
- en: 'M et al. (2005) M, H.K., N, M.D., S, K.M., 2005. Corephrase: Keyphrase extraction
    for document clustering, in: Proc. MLDM Conf., pp. 265--274. doi:[10.1007/11510888_26](https:/doi.org/10.1007/11510888_26).'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'M 等 (2005) M, H.K., N, M.D., S, K.M., 2005. Corephrase: 用于文档聚类的关键短语提取，见：Proc.
    MLDM Conf., pp. 265--274. doi:[10.1007/11510888_26](https:/doi.org/10.1007/11510888_26)。'
- en: 'Mahata et al. (2022) Mahata, D., Agarwal, N., Gautam, D., Kumar, A., Parekh,
    S., Singla, Y.K., Acharya, A., Shah, R.R., 2022. LDKP: A dataset for identifying
    keyphrases from long scientific documents. CoRR doi:[10.48550/arXiv.2203.15349](https:/doi.org/10.48550/arXiv.2203.15349).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mahata 等 (2022) Mahata, D., Agarwal, N., Gautam, D., Kumar, A., Parekh, S.,
    Singla, Y.K., Acharya, A., Shah, R.R., 2022. LDKP: 一个用于识别长科学文档中关键短语的数据集。CoRR doi:[10.48550/arXiv.2203.15349](https:/doi.org/10.48550/arXiv.2203.15349)。'
- en: 'Mahata et al. (2018) Mahata, D., Kuriakose, J., Shah, R.R., Zimmermann, R.,
    2018. Key2vec: Automatic ranked keyphrase extraction from scientific articles
    using phrase embeddings, in: Proc. NAACL, Short Papers Conf., pp. 634--639. doi:[10.18653/v1/n18-2100](https:/doi.org/10.18653/v1/n18-2100).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mahata 等 (2018) Mahata, D., Kuriakose, J., Shah, R.R., Zimmermann, R., 2018.
    Key2vec: 使用短语嵌入从科学文章中自动排名提取关键短语，见：Proc. NAACL, Short Papers Conf., pp. 634--639.
    doi:[10.18653/v1/n18-2100](https:/doi.org/10.18653/v1/n18-2100)。'
- en: Mahfuzh et al. (2020) Mahfuzh, M., Soleman, S., Purwarianti, A., 2020. Improving
    joint layer RNN based keyphrase extraction by using syntactical features. CoRR
    doi:[10.1109/ICAICTA.2019.8904194](https:/doi.org/10.1109/ICAICTA.2019.8904194),
    [arXiv:2009.07119](http://arxiv.org/abs/2009.07119).
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahfuzh 等人（2020）Mahfuzh, M., Soleman, S., Purwarianti, A., 2020. 通过使用句法特征改进基于联合层
    RNN 的关键短语提取。CoRR doi:[10.1109/ICAICTA.2019.8904194](https:/doi.org/10.1109/ICAICTA.2019.8904194),
    [arXiv:2009.07119](http://arxiv.org/abs/2009.07119)。
- en: 'Marujo et al. (2012) Marujo, L., Gershman, A., Carbonell, J., Frederking, R.,
    Neto, J.P., 2012. Supervised topical key phrase extraction of news stories using
    crowdsourcing, light filtering and co-reference normalization, in: Proc. LREC
    Conf., pp. 399--403.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marujo 等人（2012）Marujo, L., Gershman, A., Carbonell, J., Frederking, R., Neto,
    J.P., 2012. 使用众包、轻过滤和共指归一化的监督式主题关键短语提取，见：Proc. LREC Conf., pp. 399--403。
- en: 'Marujo et al. (2011) Marujo, L., Viveiros, M., Neto, J.P., 2011. Keyphrase
    cloud generation of broadcast news, in: Proc. INTERSPEECH Conf., pp. 2393--2396.
    [arXiv:1306.4606](http://arxiv.org/abs/1306.4606).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marujo 等人（2011）Marujo, L., Viveiros, M., Neto, J.P., 2011. 广播新闻的关键短语云生成，见：Proc.
    INTERSPEECH Conf., pp. 2393--2396。 [arXiv:1306.4606](http://arxiv.org/abs/1306.4606)。
- en: 'Medelyan et al. (2009) Medelyan, O., Frank, E., Witten, I.H., 2009. Human-competitive
    tagging using automatic keyphrase extraction, in: Proc. EMNLP Conf., pp. 1318--1327.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Medelyan 等人（2009）Medelyan, O., Frank, E., Witten, I.H., 2009. 使用自动关键短语提取进行人类竞争性标记，见：Proc.
    EMNLP Conf., pp. 1318--1327。
- en: 'Medelyan and Witten (2006) Medelyan, O., Witten, I.H., 2006. Thesaurus based
    automatic keyphrase indexing, in: Proc. JCDL Conf., pp. 296--297. doi:[10.1145/1141753.1141819](https:/doi.org/10.1145/1141753.1141819).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Medelyan 和 Witten（2006）Medelyan, O., Witten, I.H., 2006. 基于词库的自动关键短语索引，见：Proc.
    JCDL Conf., pp. 296--297。 doi:[10.1145/1141753.1141819](https:/doi.org/10.1145/1141753.1141819)。
- en: Meng et al. (2019) Meng, R., Yuan, X., Wang, T., Brusilovsky, P., Trischler,
    A., He, D., 2019. Does order matter? an empirical study on generating multiple
    keyphrases as a sequence. CoRR doi:[10.48550/arXiv.1909.03590](https:/doi.org/10.48550/arXiv.1909.03590).
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人（2019）Meng, R., Yuan, X., Wang, T., Brusilovsky, P., Trischler, A., He,
    D., 2019. 顺序重要吗？关于生成多个关键短语作为序列的实证研究。CoRR doi:[10.48550/arXiv.1909.03590](https:/doi.org/10.48550/arXiv.1909.03590)。
- en: 'Meng et al. (2021) Meng, R., Yuan, X., Wang, T., Zhao, S., Trischler, A., He,
    D., 2021. An empirical study on neural keyphrase generation, in: Proc. NAACL Conf.,
    pp. 4985--5007. doi:[10.18653/v1/2021.naacl-main.396](https:/doi.org/10.18653/v1/2021.naacl-main.396).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人（2021）Meng, R., Yuan, X., Wang, T., Zhao, S., Trischler, A., He, D.,
    2021. 关于神经关键短语生成的实证研究，见：Proc. NAACL Conf., pp. 4985--5007。 doi:[10.18653/v1/2021.naacl-main.396](https:/doi.org/10.18653/v1/2021.naacl-main.396)。
- en: 'Meng et al. (2017) Meng, R., Zhao, S., Han, S., He, D., Brusilovsky, P., Chi,
    Y., 2017. Deep keyphrase generation, in: Proc. ACL Conf., pp. 582--592. doi:[10.18653/v1/P17-1054](https:/doi.org/10.18653/v1/P17-1054).'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人（2017）Meng, R., Zhao, S., Han, S., He, D., Brusilovsky, P., Chi, Y.,
    2017. 深度关键短语生成，见：Proc. ACL Conf., pp. 582--592。 doi:[10.18653/v1/P17-1054](https:/doi.org/10.18653/v1/P17-1054)。
- en: 'Mihalcea and Tarau (2004) Mihalcea, R., Tarau, P., 2004. Textrank: Bringing
    order into text, in: Proc. EMNLP Conf., pp. 404--411.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mihalcea 和 Tarau（2004）Mihalcea, R., Tarau, P., 2004. Textrank: 将秩序引入文本，见：Proc.
    EMNLP Conf., pp. 404--411。'
- en: Mu et al. (2020) Mu, F., Yu, Z., Wang, L., Wang, Y., Yin, Q., Sun, Y., Liu,
    L., Ma, T., Tang, J., Zhou, X., 2020. Keyphrase extraction with span-based feature
    representations. CoRR doi:[10.48550/arXiv.2002.05407](https:/doi.org/10.48550/arXiv.2002.05407),
    [arXiv:2002.05407](http://arxiv.org/abs/2002.05407).
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等人（2020）Mu, F., Yu, Z., Wang, L., Wang, Y., Yin, Q., Sun, Y., Liu, L., Ma,
    T., Tang, J., Zhou, X., 2020. 基于跨度特征表示的关键短语提取。CoRR doi:[10.48550/arXiv.2002.05407](https:/doi.org/10.48550/arXiv.2002.05407),
    [arXiv:2002.05407](http://arxiv.org/abs/2002.05407)。
- en: 'Nasar et al. (2019) Nasar, Z., Jaffry, S.W., Malik, M.K., 2019. Textual keyword
    extraction and summarization: State-of-the-art. Inf. Process. Manag. , 102088doi:[10.1016/j.ipm.2019.102088](https:/doi.org/10.1016/j.ipm.2019.102088).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasar 等人（2019）Nasar, Z., Jaffry, S.W., Malik, M.K., 2019. 文本关键词提取与摘要：最新进展。Inf.
    Process. Manag. , 102088doi:[10.1016/j.ipm.2019.102088](https:/doi.org/10.1016/j.ipm.2019.102088)。
- en: Newman and Girvan (2004) Newman, M.E.J., Girvan, M., 2004. Finding and evaluating
    community structure in networks. Phys. Rev. E. , 026113doi:[10.1103/PhysRevE.69.026113](https:/doi.org/10.1103/PhysRevE.69.026113).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newman 和 Girvan（2004）Newman, M.E.J., Girvan, M., 2004. 在网络中发现和评估社区结构。Phys. Rev.
    E. , 026113doi:[10.1103/PhysRevE.69.026113](https:/doi.org/10.1103/PhysRevE.69.026113)。
- en: 'Nguyen and Phan (2009) Nguyen, C.Q., Phan, T.T., 2009. An ontology-based approach
    for key phrase extraction, in: Proc. ACL-IJCNLP, Short Papers Conf., pp. 181--184.
    doi:[10.3115/1667583.1667639](https:/doi.org/10.3115/1667583.1667639).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 Phan (2009) Nguyen, C.Q., Phan, T.T., 2009. 基于本体的方法进行关键词提取，见：Proc.
    ACL-IJCNLP，短论文会议，页码 181--184。doi：[10.3115/1667583.1667639](https:/doi.org/10.3115/1667583.1667639)。
- en: 'Nguyen and Kan (2007) Nguyen, T.D., Kan, M., 2007. Keyphrase extraction in
    scientific publications, in: Proc. ICADL Conf., pp. 317--326. doi:[10.1007/978-3-540-77094-7_41](https:/doi.org/10.1007/978-3-540-77094-7_41).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 Kan (2007) Nguyen, T.D., Kan, M., 2007. 科学出版物中的关键词提取，见：Proc. ICADL
    会议，页码 317--326。doi：[10.1007/978-3-540-77094-7_41](https:/doi.org/10.1007/978-3-540-77094-7_41)。
- en: 'Nguyen and Luong (2010) Nguyen, T.D., Luong, M.T., 2010. WINGNUS: Keyphrase
    extraction utilizing document logical structure, in: Proc. SemEval@ACL Conf.,
    pp. 166--169.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 Luong (2010) Nguyen, T.D., Luong, M.T., 2010. WINGNUS：利用文档逻辑结构进行关键词提取，见：Proc.
    SemEval@ACL 会议，页码 166--169。
- en: 'Nikzad-Khasmakhi et al. (2021) Nikzad-Khasmakhi, N., Feizi-Derakhshi, M., Asgari-Chenaghlu,
    M., Balafar, M.A., Feizi-Derakhshi, A., Rahkar-Farshi, T., Ramezani, M., Jahanbakhsh-Nagadeh,
    Z., Zafarani-Moattar, E., Ranjbar-Khadivi, M., 2021. Phraseformer: Multimodal
    key-phrase extraction using transformer and graph embedding. CoRR doi:[10.48550/ARXIV.2106.04939](https:/doi.org/10.48550/ARXIV.2106.04939).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nikzad-Khasmakhi 等 (2021) Nikzad-Khasmakhi, N., Feizi-Derakhshi, M., Asgari-Chenaghlu,
    M., Balafar, M.A., Feizi-Derakhshi, A., Rahkar-Farshi, T., Ramezani, M., Jahanbakhsh-Nagadeh,
    Z., Zafarani-Moattar, E., Ranjbar-Khadivi, M., 2021. Phraseformer：使用变换器和图嵌入的多模态关键短语提取。CoRR
    doi：[10.48550/ARXIV.2106.04939](https:/doi.org/10.48550/ARXIV.2106.04939)。
- en: 'Ni’mah et al. (2019) Ni’mah, I., Menkovski, V., Pechenizkiy, M., 2019. BSDAR:
    beam search decoding with attention reward in neural keyphrase generation. CoRR
    doi:[10.48550/arXiv.1909.09485](https:/doi.org/10.48550/arXiv.1909.09485).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni’mah 等 (2019) Ni’mah, I., Menkovski, V., Pechenizkiy, M., 2019. BSDAR：具有注意力奖励的束搜索解码在神经关键词生成中的应用。CoRR
    doi：[10.48550/arXiv.1909.09485](https:/doi.org/10.48550/arXiv.1909.09485)。
- en: 'Ohsawa et al. (1998) Ohsawa, Y., Benson, N.E., Yachida, M., 1998. Keygraph:
    Automatic indexing by co-occurrence graph based on building construction metaphor,
    in: Proc. ADL Conf., pp. 12--18. doi:[10.1109/ADL.1998.670375](https:/doi.org/10.1109/ADL.1998.670375).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ohsawa 等 (1998) Ohsawa, Y., Benson, N.E., Yachida, M., 1998. Keygraph：基于建筑构造隐喻的共现图自动索引，见：Proc.
    ADL 会议，页码 12--18。doi：[10.1109/ADL.1998.670375](https:/doi.org/10.1109/ADL.1998.670375)。
- en: 'Page et al. (1999) Page, L., Brin, S., Motwani, R., Winograd, T., 1999. The
    PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
    InfoLab.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Page 等 (1999) Page, L., Brin, S., Motwani, R., Winograd, T., 1999. PageRank
    引文排名：为网络带来秩序。技术报告。斯坦福信息实验室。
- en: 'Pagliardini et al. (2018) Pagliardini, M., Gupta, P., Jaggi, M., 2018. Unsupervised
    learning of sentence embeddings using compositional n-gram features, in: Proc.
    ACL Conf., pp. 528--540. doi:[10.18653/v1/n18-1049](https:/doi.org/10.18653/v1/n18-1049).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pagliardini 等 (2018) Pagliardini, M., Gupta, P., Jaggi, M., 2018. 使用组合 n-gram
    特征的无监督句子嵌入学习，见：Proc. ACL 会议，页码 528--540。doi：[10.18653/v1/n18-1049](https:/doi.org/10.18653/v1/n18-1049)。
- en: Papagiannopoulou and Tsoumakas (2018) Papagiannopoulou, E., Tsoumakas, G., 2018.
    Local word vectors guiding keyphrase extraction. Inf. Process. Manag. , 888--902doi:[10.1016/j.ipm.2018.06.004](https:/doi.org/10.1016/j.ipm.2018.06.004).
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papagiannopoulou 和 Tsoumakas (2018) Papagiannopoulou, E., Tsoumakas, G., 2018.
    局部词向量指导的关键词提取。信息处理与管理，888--902 doi：[10.1016/j.ipm.2018.06.004](https:/doi.org/10.1016/j.ipm.2018.06.004)。
- en: 'Pasunuru and Bansal (2018) Pasunuru, R., Bansal, M., 2018. Multi-reward reinforced
    summarization with saliency and entailment, in: Proc. ACL Conf., pp. 646--653.
    doi:[10.18653/v1/n18-2102](https:/doi.org/10.18653/v1/n18-2102).'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pasunuru 和 Bansal (2018) Pasunuru, R., Bansal, M., 2018. 多奖励强化总结，具有显著性和蕴涵，见：Proc.
    ACL 会议，页码 646--653。doi：[10.18653/v1/n18-2102](https:/doi.org/10.18653/v1/n18-2102)。
- en: 'Pennington et al. (2014) Pennington, J., Socher, R., Manning, C.D., 2014. Glove:
    Global vectors for word representation, in: Proc. EMNLP Conf., pp. 1532--1543.
    doi:[10.3115/v1/D14-1162](https:/doi.org/10.3115/v1/D14-1162).'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington 等 (2014) Pennington, J., Socher, R., Manning, C.D., 2014. GloVe：全球词向量表示，见：Proc.
    EMNLP 会议，页码 1532--1543。doi：[10.3115/v1/D14-1162](https:/doi.org/10.3115/v1/D14-1162)。
- en: 'Peters et al. (2018) Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark,
    C., Lee, K., Zettlemoyer, L., 2018. Deep contextualized word representations,
    in: Proc. NAACL Conf., pp. 2227--2237. doi:[10.18653/v1/n18-1202](https:/doi.org/10.18653/v1/n18-1202).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等 (2018) Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,
    Lee, K., Zettlemoyer, L., 2018. 深度上下文化词表示，见：Proc. NAACL 会议，页码 2227--2237。doi：[10.18653/v1/n18-1202](https:/doi.org/10.18653/v1/n18-1202)。
- en: 'Prasad and Kan (2019) Prasad, A., Kan, M., 2019. Glocal: Incorporating global
    information in local convolution for keyphrase extraction, in: Proc. NAACL Conf.,
    pp. 1837--1846. doi:[10.18653/v1/n19-1182](https:/doi.org/10.18653/v1/n19-1182).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prasad 和 Kan (2019) Prasad, A., Kan, M., 2019. Glocal: 在本地卷积中融合全球信息以提取关键词，见于：Proc.
    NAACL Conf., pp. 1837--1846. doi:[10.18653/v1/n19-1182](https:/doi.org/10.18653/v1/n19-1182)。'
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., Liu, P.J., 2020. Exploring the limits of transfer
    learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 140:1--140:67.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., Liu, P.J., 2020. 探索统一文本到文本变换器的迁移学习极限。J. Mach. Learn. Res.
    , 140:1--140:67。
- en: Sahrawat et al. (2019) Sahrawat, D., Mahata, D., Kulkarni, M., Zhang, H., Gosangi,
    R., Stent, A., Sharma, A., Kumar, Y., Shah, R.R., Zimmermann, R., 2019. Keyphrase
    extraction from scholarly articles as sequence labeling using contextualized embeddings.
    CoRR doi:[10.48550/arXiv.1910.08840](https:/doi.org/10.48550/arXiv.1910.08840),
    [arXiv:1910.08840](http://arxiv.org/abs/1910.08840).
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahrawat 等 (2019) Sahrawat, D., Mahata, D., Kulkarni, M., Zhang, H., Gosangi,
    R., Stent, A., Sharma, A., Kumar, Y., Shah, R.R., Zimmermann, R., 2019. 从学术文章中提取关键词作为序列标注使用上下文化嵌入。CoRR
    doi:[10.48550/arXiv.1910.08840](https:/doi.org/10.48550/arXiv.1910.08840), [arXiv:1910.08840](http://arxiv.org/abs/1910.08840)。
- en: Salton and Buckley (1988) Salton, G., Buckley, C., 1988. Term-weighting approaches
    in automatic text retrieval. Inf. Process. Manag. , 513--523doi:[10.1016/0306-4573(88)90021-0](https:/doi.org/10.1016/0306-4573(88)90021-0).
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salton 和 Buckley (1988) Salton, G., Buckley, C., 1988. 自动文本检索中的术语加权方法。Inf. Process.
    Manag. , 513--523 doi:[10.1016/0306-4573(88)90021-0](https:/doi.org/10.1016/0306-4573(88)90021-0)。
- en: 'Santosh et al. (2020a) Santosh, T.Y.S.S., Sanyal, D.K., Bhowmick, P.K., Das,
    P.P., 2020a. DAKE: document-level attention for keyphrase extraction, in: Proc.
    ECIR Conf., pp. 392--401. doi:[10.1007/978-3-030-45442-5_49](https:/doi.org/10.1007/978-3-030-45442-5_49).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Santosh 等 (2020a) Santosh, T.Y.S.S., Sanyal, D.K., Bhowmick, P.K., Das, P.P.,
    2020a. DAKE: 文档级注意力用于关键词提取，见于：Proc. ECIR Conf., pp. 392--401. doi:[10.1007/978-3-030-45442-5_49](https:/doi.org/10.1007/978-3-030-45442-5_49)。'
- en: 'Santosh et al. (2020b) Santosh, T.Y.S.S., Sanyal, D.K., Bhowmick, P.K., Das,
    P.P., 2020b. Sasake: Syntax and semantics aware keyphrase extraction from research
    papers, in: Proc. COLING Conf., pp. 5372--5383. doi:[10.18653/v1/2020.coling-main.469](https:/doi.org/10.18653/v1/2020.coling-main.469).'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Santosh 等 (2020b) Santosh, T.Y.S.S., Sanyal, D.K., Bhowmick, P.K., Das, P.P.,
    2020b. Sasake: 语法和语义感知的研究论文关键词提取，见于：Proc. COLING Conf., pp. 5372--5383. doi:[10.18653/v1/2020.coling-main.469](https:/doi.org/10.18653/v1/2020.coling-main.469)。'
- en: 'Santosh et al. (2021a) Santosh, T.Y.S.S., Sanyal, D.K., Bhowmick, P.K., Das,
    P.P., 2021a. Gazetteer-guided keyphrase generation from research papers, in: Proc.
    PAKDD Conf., pp. 655--667. doi:[10.1007/978-3-030-75762-5_52](https:/doi.org/10.1007/978-3-030-75762-5_52).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santosh 等 (2021a) Santosh, T.Y.S.S., Sanyal, D.K., Bhowmick, P.K., Das, P.P.,
    2021a. 基于地名指南的研究论文关键词生成，见于：Proc. PAKDD Conf., pp. 655--667. doi:[10.1007/978-3-030-75762-5_52](https:/doi.org/10.1007/978-3-030-75762-5_52)。
- en: 'Santosh et al. (2021b) Santosh, T.Y.S.S., Varimalla, N.R., Vallabhajosyula,
    A., Sanyal, D.K., Das, P.P., 2021b. Hicova: Hierarchical conditional variational
    autoencoder for keyphrase generation, in: Proc. CIKM Conf., pp. 3448--3452. doi:[10.1145/3459637.3482119](https:/doi.org/10.1145/3459637.3482119).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Santosh 等 (2021b) Santosh, T.Y.S.S., Varimalla, N.R., Vallabhajosyula, A.,
    Sanyal, D.K., Das, P.P., 2021b. Hicova: 用于关键词生成的层次条件变分自编码器，见于：Proc. CIKM Conf.,
    pp. 3448--3452. doi:[10.1145/3459637.3482119](https:/doi.org/10.1145/3459637.3482119)。'
- en: 'Saputra et al. (2018) Saputra, I.F., Mahendra, R., Wicaksono, A.F., 2018. Keyphrases
    extraction from user-generated contents in healthcare domain using long short-term
    memory networks, in: Proc. BioNLP Conf., pp. 28--34. doi:[10.18653/v1/w18-2304](https:/doi.org/10.18653/v1/w18-2304).'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saputra 等 (2018) Saputra, I.F., Mahendra, R., Wicaksono, A.F., 2018. 使用长短期记忆网络从医疗领域用户生成内容中提取关键词，见于：Proc.
    BioNLP Conf., pp. 28--34. doi:[10.18653/v1/w18-2304](https:/doi.org/10.18653/v1/w18-2304)。
- en: Sarkar et al. (2010) Sarkar, K., Nasipuri, M., Ghose, S., 2010. A new approach
    to keyphrase extraction using neural networks. CoRR doi:[10.1109/IranianCEE.2019.8786505](https:/doi.org/10.1109/IranianCEE.2019.8786505).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarkar 等 (2010) Sarkar, K., Nasipuri, M., Ghose, S., 2010. 使用神经网络的关键词提取新方法。CoRR
    doi:[10.1109/IranianCEE.2019.8786505](https:/doi.org/10.1109/IranianCEE.2019.8786505)。
- en: Schutz et al. (2008) Schutz, A.T., et al., 2008. Keyphrase extraction from single
    documents in the open domain exploiting linguistic and statistical methods. M.
    App. Sc Thesis .
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schutz 等 (2008) Schutz, A.T., 等，2008. 利用语言学和统计方法从开放领域单个文档中提取关键词。M. App. Sc Thesis。
- en: 'Shen et al. (2022) Shen, X., Wang, Y., Meng, R., Shang, J., 2022. Unsupervised
    deep keyphrase generation, in: Proc. AAAI Conf., pp. 11303--11311. doi:[10.1609/aaai.v36i10.21381](https:/doi.org/10.1609/aaai.v36i10.21381).'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等（2022）Shen, X., Wang, Y., Meng, R., Shang, J., 2022. 无监督深度关键词短语生成，见于：Proc.
    AAAI Conf., pp. 11303--11311. doi:[10.1609/aaai.v36i10.21381](https:/doi.org/10.1609/aaai.v36i10.21381)。
- en: 'Shi et al. (2008) Shi, T., Jiao, S., Hou, J., Li, M., 2008. Improving keyphrase
    extraction using wikipedia semantics, in: Proc. IITA Conf., pp. 42--46. doi:[10.1109/IITA.2008.211](https:/doi.org/10.1109/IITA.2008.211).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等（2008）Shi, T., Jiao, S., Hou, J., Li, M., 2008. 利用维基百科语义改进关键词短语提取，见于：Proc.
    IITA Conf., pp. 42--46. doi:[10.1109/IITA.2008.211](https:/doi.org/10.1109/IITA.2008.211)。
- en: 'Siddiqi and Sharan (2015) Siddiqi, S., Sharan, A., 2015. Keyword and keyphrase
    extraction techniques: a literature review. International Journal of Computer
    Applications doi:[10.5120/19161-0607](https:/doi.org/10.5120/19161-0607).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siddiqi和Sharan（2015）Siddiqi, S., Sharan, A., 2015. 关键词和关键词短语提取技术：文献综述。国际计算机应用期刊
    doi:[10.5120/19161-0607](https:/doi.org/10.5120/19161-0607)。
- en: 'Song et al. (2021) Song, M., Jing, L., Xiao, L., 2021. Importance estimation
    from multiple perspectives for keyphrase extraction, in: Proc. EMNLP Conf., pp.
    2726--2736. doi:[10.18653/v1/2021.emnlp-main.215](https:/doi.org/10.18653/v1/2021.emnlp-main.215).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song等（2021）Song, M., Jing, L., Xiao, L., 2021. 从多个视角估计关键词短语的重要性，见于：Proc. EMNLP
    Conf., pp. 2726--2736. doi:[10.18653/v1/2021.emnlp-main.215](https:/doi.org/10.18653/v1/2021.emnlp-main.215)。
- en: 'Sterckx et al. (2015) Sterckx, L., Demeester, T., Deleu, J., Develder, C.,
    2015. Topical word importance for fast keyphrase extraction, in: Proc. WWW Conf.,
    pp. 121--122. doi:[10.1145/2740908.2742730](https:/doi.org/10.1145/2740908.2742730).'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sterckx等（2015）Sterckx, L., Demeester, T., Deleu, J., Develder, C., 2015. 快速关键词短语提取的主题词重要性，见于：Proc.
    WWW Conf., pp. 121--122. doi:[10.1145/2740908.2742730](https:/doi.org/10.1145/2740908.2742730)。
- en: 'Subramanian et al. (2018) Subramanian, S., Wang, T., Yuan, X., Zhang, S., Trischler,
    A., Bengio, Y., 2018. Neural models for key phrase extraction and question generation,
    in: Proc. QA@ACL Conf., pp. 78--88. doi:[10.18653/v1/W18-2609](https:/doi.org/10.18653/v1/W18-2609).'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramanian等（2018）Subramanian, S., Wang, T., Yuan, X., Zhang, S., Trischler,
    A., Bengio, Y., 2018. 用于关键词短语提取和问题生成的神经模型，见于：Proc. QA@ACL Conf., pp. 78--88. doi:[10.18653/v1/W18-2609](https:/doi.org/10.18653/v1/W18-2609)。
- en: 'Sun et al. (2021) Sun, S., Liu, Z., Xiong, C., Liu, Z., Bao, J., 2021. Capturing
    global informativeness in open domain keyphrase extraction, in: Proc. NLPCC Conf.,
    pp. 275--287. doi:[10.1007/978-3-030-88483-3_21](https:/doi.org/10.1007/978-3-030-88483-3_21).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2021）Sun, S., Liu, Z., Xiong, C., Liu, Z., Bao, J., 2021. 捕捉开放领域关键词短语提取中的全球信息性，见于：Proc.
    NLPCC Conf., pp. 275--287. doi:[10.1007/978-3-030-88483-3_21](https:/doi.org/10.1007/978-3-030-88483-3_21)。
- en: 'Sun et al. (2020) Sun, Y., Qiu, H., Zheng, Y., Wang, Z., Zhang, C., 2020. Sifrank:
    A new baseline for unsupervised keyphrase extraction based on pre-trained language
    model. IEEE Access , 10896--10906doi:[10.1109/ACCESS.2020.2965087](https:/doi.org/10.1109/ACCESS.2020.2965087).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2020）Sun, Y., Qiu, H., Zheng, Y., Wang, Z., Zhang, C., 2020. Sifrank：基于预训练语言模型的无监督关键词短语提取的新基准。IEEE
    Access , 10896--10906. doi:[10.1109/ACCESS.2020.2965087](https:/doi.org/10.1109/ACCESS.2020.2965087)。
- en: 'Sun et al. (2019) Sun, Z., Tang, J., Du, P., Deng, Z., Nie, J., 2019. Divgraphpointer:
    A graph pointer network for extracting diverse keyphrases, in: Proc. SIGIR Conf.,
    pp. 755--764. doi:[10.1145/3331184.3331219](https:/doi.org/10.1145/3331184.3331219).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2019）Sun, Z., Tang, J., Du, P., Deng, Z., Nie, J., 2019. Divgraphpointer：一种用于提取多样化关键词短语的图指针网络，见于：Proc.
    SIGIR Conf., pp. 755--764. doi:[10.1145/3331184.3331219](https:/doi.org/10.1145/3331184.3331219)。
- en: 'Swaminathan et al. (2020a) Swaminathan, A., Gupta, R.K., Zhang, H., Mahata,
    D., Gosangi, R., Shah, R.R., 2020a. Keyphrase generation for scientific articles
    using gans, in: Proc. Student Abstrct@AAAI Conf., pp. 13931--13932. doi:[10.1609/aaai.v34i10.7238](https:/doi.org/10.1609/aaai.v34i10.7238).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swaminathan等（2020a）Swaminathan, A., Gupta, R.K., Zhang, H., Mahata, D., Gosangi,
    R., Shah, R.R., 2020a. 使用GANs生成科学文章的关键词短语，见于：Proc. Student Abstract@AAAI Conf.,
    pp. 13931--13932. doi:[10.1609/aaai.v34i10.7238](https:/doi.org/10.1609/aaai.v34i10.7238)。
- en: 'Swaminathan et al. (2020b) Swaminathan, A., Zhang, H., Mahata, D., Gosangi,
    R., Shah, R.R., Stent, A., 2020b. A preliminary exploration of GANs for keyphrase
    generation, in: Proc. EMNLP Conf., pp. 8021--8030. doi:[10.18653/v1/2020.emnlp-main.645](https:/doi.org/10.18653/v1/2020.emnlp-main.645).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swaminathan等（2020b）Swaminathan, A., Zhang, H., Mahata, D., Gosangi, R., Shah,
    R.R., Stent, A., 2020b. 对GANs在关键词短语生成中的初步探索，见于：Proc. EMNLP Conf., pp. 8021--8030.
    doi:[10.18653/v1/2020.emnlp-main.645](https:/doi.org/10.18653/v1/2020.emnlp-main.645)。
- en: 'Teneva and Cheng (2017) Teneva, N., Cheng, W., 2017. Salience rank: Efficient
    keyphrase extraction with topic modeling, in: Proc. ACL, Short Papers Conf., pp.
    530--535. doi:[10.18653/v1/P17-2084](https:/doi.org/10.18653/v1/P17-2084).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teneva 和 Cheng（2017）Teneva, N., Cheng, W., 2017. 重要性排名：具有主题建模的高效关键词提取，见：Proc.
    ACL, Short Papers Conf., pp. 530--535. doi:[10.18653/v1/P17-2084](https:/doi.org/10.18653/v1/P17-2084)。
- en: 'Tu et al. (2016) Tu, Z., Lu, Z., Liu, Y., Liu, X., Li, H., 2016. Modeling coverage
    for neural machine translation, in: Proc. ACL Conf., pp. 76--85. doi:[10.18653/v1/p16-1008](https:/doi.org/10.18653/v1/p16-1008).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等（2016）Tu, Z., Lu, Z., Liu, Y., Liu, X., Li, H., 2016. 神经机器翻译中的覆盖建模，见：Proc.
    ACL Conf., pp. 76--85. doi:[10.18653/v1/p16-1008](https:/doi.org/10.18653/v1/p16-1008)。
- en: Turney (2002) Turney, P.D., 2002. Learning to extract keyphrases from text.
    CoRR cs.LG/0212013. doi:[10.48550/ARXIV.CS/0212013](https:/doi.org/10.48550/ARXIV.CS/0212013).
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turney（2002）Turney, P.D., 2002. 从文本中提取关键词的学习。CoRR cs.LG/0212013. doi:[10.48550/ARXIV.CS/0212013](https:/doi.org/10.48550/ARXIV.CS/0212013)。
- en: Vega-Oliveros et al. (2019) Vega-Oliveros, D.A., Gomes, P.S., Milios, E.E.,
    Berton, L., 2019. A multi-centrality index for graph-based keyword extraction.
    Inf. Process. Manag. , 102063doi:[10.1016/j.ipm.2019.102063](https:/doi.org/10.1016/j.ipm.2019.102063).
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vega-Oliveros 等（2019）Vega-Oliveros, D.A., Gomes, P.S., Milios, E.E., Berton,
    L., 2019. 一种用于基于图的关键词提取的多中心性指标。信息处理与管理，102063doi:[10.1016/j.ipm.2019.102063](https:/doi.org/10.1016/j.ipm.2019.102063)。
- en: 'Wan and Xiao (2008) Wan, X., Xiao, J., 2008. Single document keyphrase extraction
    using neighborhood knowledge, in: Proc. AAAI Conf., pp. 855--860. doi:[10.5555/1620163.1620205](https:/doi.org/10.5555/1620163.1620205).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 和 Xiao（2008）Wan, X., Xiao, J., 2008. 使用邻域知识的单文档关键词提取，见：Proc. AAAI Conf.,
    pp. 855--860. doi:[10.5555/1620163.1620205](https:/doi.org/10.5555/1620163.1620205)。
- en: 'Wang et al. (2018) Wang, H., Zhang, F., Wang, J., Zhao, M., Li, W., Xie, X.,
    Guo, M., 2018. Ripplenet: Propagating user preferences on the knowledge graph
    for recommender systems, in: Proc. CIKM Conf., pp. 417--426. doi:[10.1145/3269206.3271739](https:/doi.org/10.1145/3269206.3271739).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018）Wang, H., Zhang, F., Wang, J., Zhao, M., Li, W., Xie, X., Guo, M.,
    2018. Ripplenet：在知识图谱中传播用户偏好用于推荐系统，见：Proc. CIKM Conf., pp. 417--426. doi:[10.1145/3269206.3271739](https:/doi.org/10.1145/3269206.3271739)。
- en: 'Wang et al. (2005) Wang, J., Peng, H., Hu, J., 2005. Automatic keyphrases extraction
    from document using neural network, in: Proc. ICMLC Conf., pp. 633--641. doi:[10.1007/11739685_66](https:/doi.org/10.1007/11739685_66).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2005）Wang, J., Peng, H., Hu, J., 2005. 使用神经网络从文档中自动提取关键词，见：Proc. ICMLC
    Conf., pp. 633--641. doi:[10.1007/11739685_66](https:/doi.org/10.1007/11739685_66)。
- en: 'Wang and Cardie (2013) Wang, L., Cardie, C., 2013. Domain-independent abstract
    generation for focused meeting summarization, in: Proc. ACL Conf., pp. 1395--1405.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Cardie（2013）Wang, L., Cardie, C., 2013. 针对焦点会议摘要的领域独立摘要生成，见：Proc. ACL
    Conf., pp. 1395--1405。
- en: 'Wang and Li (2017) Wang, L., Li, S., 2017. PKU_ICL at SemEval-2017 task 10:
    Keyphrase extraction with model ensemble and external knowledge, in: Proc. SemEval@ACL
    Conf., pp. 934--937. doi:[10.18653/v1/S17-2161](https:/doi.org/10.18653/v1/S17-2161).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Li（2017）Wang, L., Li, S., 2017. PKU_ICL 在 SemEval-2017 任务 10 中：通过模型集成和外部知识进行关键词提取，见：Proc.
    SemEval@ACL Conf., pp. 934--937. doi:[10.18653/v1/S17-2161](https:/doi.org/10.18653/v1/S17-2161)。
- en: 'Wang et al. (2020a) Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M.,
    2020a. Minilm: Deep self-attention distillation for task-agnostic compression
    of pre-trained transformers, in: Proc. NeurIPS Cof., pp. 5776--5788. doi:[https://doi.org/10.48550/arXiv.2002.10957](https:/doi.org/https://doi.org/10.48550/arXiv.2002.10957).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020a）Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M., 2020a.
    Minilm：深度自注意力蒸馏用于任务无关的预训练变换器压缩，见：Proc. NeurIPS Cof., pp. 5776--5788. doi:[https://doi.org/10.48550/arXiv.2002.10957](https:/doi.org/https://doi.org/10.48550/arXiv.2002.10957)。
- en: 'Wang et al. (2020b) Wang, Y., Fan, Z., Rosé, C.P., 2020b. Incorporating multimodal
    information in open-domain web keyphrase extraction, in: Proc. EMNLP Conf., pp.
    1790--1800. doi:[10.18653/v1/2020.emnlp-main.140](https:/doi.org/10.18653/v1/2020.emnlp-main.140).'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020b）Wang, Y., Fan, Z., Rosé, C.P., 2020b. 在开放域网页关键词提取中融入多模态信息，见：Proc.
    EMNLP Conf., pp. 1790--1800. doi:[10.18653/v1/2020.emnlp-main.140](https:/doi.org/10.18653/v1/2020.emnlp-main.140)。
- en: 'Wang et al. (2019) Wang, Y., Li, J., Chan, H.P., King, I., Lyu, M.R., Shi,
    S., 2019. Topic-aware neural keyphrase generation for social media language, in:
    Proc. ACL Conf., pp. 2516--2526. doi:[10.18653/v1/p19-1240](https:/doi.org/10.18653/v1/p19-1240).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019）Wang, Y., Li, J., Chan, H.P., King, I., Lyu, M.R., Shi, S., 2019.
    面向社交媒体语言的主题感知神经关键词生成，见：Proc. ACL Conf., pp. 2516--2526. doi:[10.18653/v1/p19-1240](https:/doi.org/10.18653/v1/p19-1240)。
- en: 'Wang et al. (2020c) Wang, Y., Li, J., Lyu, M., King, I., 2020c. Cross-media
    keyphrase prediction: A unified framework with multi-modality multi-head attention
    and image wordings, in: Proc. EMNLP Conf., pp. 3311--3324. doi:[10.18653/v1/2020.emnlp-main.268](https:/doi.org/10.18653/v1/2020.emnlp-main.268).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2020c）Wang, Y., Li, J., Lyu, M., King, I., 2020c. 跨媒体关键词短语预测：一个统一框架，具有多模态多头注意力和图像文字，见：Proc.
    EMNLP Conf., 页码3311--3324. doi:[10.18653/v1/2020.emnlp-main.268](https:/doi.org/10.18653/v1/2020.emnlp-main.268)。
- en: 'Wilson et al. (2005) Wilson, T., Wiebe, J., Hoffmann, P., 2005. Recognizing
    contextual polarity in phrase-level sentiment analysis, in: Proc. HLT/EMNLP Conf.,
    pp. 347--354.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson等人（2005）Wilson, T., Wiebe, J., Hoffmann, P., 2005. 短语级情感分析中的上下文极性识别，见：Proc.
    HLT/EMNLP Conf., 页码347--354。
- en: 'Witten et al. (1999) Witten, I.H., Paynter, G.W., Frank, E., Gutwin, C., Nevill-Manning,
    C.G., 1999. KEA: practical automatic keyphrase extraction, in: Proc. ACM DL Conf.,
    pp. 254--255. doi:[10.1145/313238.313437](https:/doi.org/10.1145/313238.313437).'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Witten等人（1999）Witten, I.H., Paynter, G.W., Frank, E., Gutwin, C., Nevill-Manning,
    C.G., 1999. KEA：实用的自动关键词短语提取，见：Proc. ACM DL Conf., 页码254--255. doi:[10.1145/313238.313437](https:/doi.org/10.1145/313238.313437)。
- en: 'Won et al. (2019) Won, M., Martins, B., Raimundo, F., 2019. Automatic extraction
    of relevant keyphrases for the study of issue competition, in: Proc. COLING Conf.,
    pp. 7--13. doi:[10.29007/mmk4](https:/doi.org/10.29007/mmk4).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Won等人（2019）Won, M., Martins, B., Raimundo, F., 2019. 自动提取与问题竞争研究相关的关键词短语，见：Proc.
    COLING Conf., 页码7--13. doi:[10.29007/mmk4](https:/doi.org/10.29007/mmk4)。
- en: Wu et al. (2022a) Wu, D., Ahmad, W.U., Dev, S., Chang, K., 2022a. Representation
    learning for resource-constrained keyphrase generation. CoRR doi:[10.48550/arXiv.2203.08118](https:/doi.org/10.48550/arXiv.2203.08118).
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2022a）Wu, D., Ahmad, W.U., Dev, S., Chang, K., 2022a. 面向资源受限的关键词短语生成的表示学习。CoRR
    doi:[10.48550/arXiv.2203.08118](https:/doi.org/10.48550/arXiv.2203.08118)。
- en: 'Wu et al. (2021) Wu, H., Liu, W., Li, L., Nie, D., Chen, T., Zhang, F., Wang,
    D., 2021. Unikeyphrase: A unified extraction and generation framework for keyphrase
    prediction, in: Proc. Findings of ACL-IJCNLP Conf., pp. 825--835. doi:[10.18653/v1/2021.findings-acl.73](https:/doi.org/10.18653/v1/2021.findings-acl.73).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2021）Wu, H., Liu, W., Li, L., Nie, D., Chen, T., Zhang, F., Wang, D., 2021.
    Unikeyphrase：用于关键词短语预测的统一提取和生成框架，见：Proc. Findings of ACL-IJCNLP Conf., 页码825--835.
    doi:[10.18653/v1/2021.findings-acl.73](https:/doi.org/10.18653/v1/2021.findings-acl.73)。
- en: 'Wu et al. (2022b) Wu, H., Ma, B., Liu, W., Chen, T., Nie, D., 2022b. Fast and
    constrained absent keyphrase generation by prompt-based learning, in: Proc. AAAI
    Conf., pp. 11495--11503. doi:[10.1609/aaai.v36i10.21402](https:/doi.org/10.1609/aaai.v36i10.21402).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2022b）Wu, H., Ma, B., Liu, W., Chen, T., Nie, D., 2022b. 基于提示学习的快速且受限的缺失关键词短语生成，见：Proc.
    AAAI Conf., 页码11495--11503. doi:[10.1609/aaai.v36i10.21402](https:/doi.org/10.1609/aaai.v36i10.21402)。
- en: 'Xie et al. (2022) Xie, B., Wei, X., Yang, B., Lin, H., Xie, J., Wang, X., Zhang,
    M., Su, J., 2022. Wr-one2set: Towards well-calibrated keyphrase generation, in:
    Proc. EMNLP Conf., pp. 7283--7293.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等人（2022）Xie, B., Wei, X., Yang, B., Lin, H., Xie, J., Wang, X., Zhang, M.,
    Su, J., 2022. Wr-one2set：朝向良好校准的关键词短语生成，见：Proc. EMNLP Conf., 页码7283--7293。
- en: Xie et al. (2017) Xie, F., Wu, X., Zhu, X., 2017. Efficient sequential pattern
    mining with wildcards for keyphrase extraction. Knowl. Based Syst. , 27--39doi:[10.1016/j.knosys.2016.10.011](https:/doi.org/10.1016/j.knosys.2016.10.011).
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等人（2017）Xie, F., Wu, X., Zhu, X., 2017. 使用通配符的高效序列模式挖掘用于关键词短语提取。知识基础系统，27--39doi:[10.1016/j.knosys.2016.10.011](https:/doi.org/10.1016/j.knosys.2016.10.011)。
- en: 'Xiong et al. (2019) Xiong, L., Hu, C., Xiong, C., Campos, D., Overwijk, A.,
    2019. Open domain web keyphrase extraction beyond language modeling, in: Proc.
    EMNLP Conf., pp. 5174--5183. doi:[10.48550/arXiv.1911.02671](https:/doi.org/10.48550/arXiv.1911.02671).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong等人（2019）Xiong, L., Hu, C., Xiong, C., Campos, D., Overwijk, A., 2019. 超越语言建模的开放域网络关键词短语提取，见：Proc.
    EMNLP Conf., 页码5174--5183. doi:[10.48550/arXiv.1911.02671](https:/doi.org/10.48550/arXiv.1911.02671)。
- en: 'Ye and Wang (2018) Ye, H., Wang, L., 2018. Semi-supervised learning for neural
    keyphrase generation, in: Proc. EMNLP Conf., pp. 4142--4153. doi:[10.18653/v1/p19-1515](https:/doi.org/10.18653/v1/p19-1515).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye和Wang（2018）Ye, H., Wang, L., 2018. 用于神经关键词短语生成的半监督学习，见：Proc. EMNLP Conf.,
    页码4142--4153. doi:[10.18653/v1/p19-1515](https:/doi.org/10.18653/v1/p19-1515)。
- en: 'Ye et al. (2021a) Ye, J., Cai, R., Gui, T., Zhang, Q., 2021a. Heterogeneous
    graph neural networks for keyphrase generation, in: Proc. EMNLP Conf., pp. 2705--2715.
    doi:[10.18653/v1/2021.emnlp-main.213](https:/doi.org/10.18653/v1/2021.emnlp-main.213).'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人（2021a）Ye, J., Cai, R., Gui, T., Zhang, Q., 2021a. 用于关键词短语生成的异质图神经网络，见：Proc.
    EMNLP Conf., 页码2705--2715. doi:[10.18653/v1/2021.emnlp-main.213](https:/doi.org/10.18653/v1/2021.emnlp-main.213)。
- en: 'Ye et al. (2021b) Ye, J., Gui, T., Luo, Y., Xu, Y., Zhang, Q., 2021b. One2set:
    Generating diverse keyphrases as a set, in: Proc. ACL Conf., pp. 4598--4608. doi:[10.18653/v1/2021.acl-long.354](https:/doi.org/10.18653/v1/2021.acl-long.354).'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶等（2021b） 叶，J.，桂，T.，罗，Y.，徐，Y.，张，Q.，2021b。One2set：作为一个集合生成多样化的关键词，见：ACL会议论文集，第4598--4608页。doi：[10.18653/v1/2021.acl-long.354](https:/doi.org/10.18653/v1/2021.acl-long.354)。
- en: 'Yih et al. (2006) Yih, W., Goodman, J., Carvalho, V.R., 2006. Finding advertising
    keywords on web pages, in: Proc. WWW Conf., pp. 213--222. doi:[10.1145/1135777.1135813](https:/doi.org/10.1145/1135777.1135813).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赫等（2006） 依赫，W.，古德曼，J.，卡瓦略，V.R.，2006。在网页上寻找广告关键词，见：WWW会议论文集，第213--222页。doi：[10.1145/1135777.1135813](https:/doi.org/10.1145/1135777.1135813)。
- en: 'Yuan et al. (2020) Yuan, X., Wang, T., Meng, R., Thaker, K., Brusilovsky, P.,
    He, D., Trischler, A., 2020. One size does not fit all: Generating and evaluating
    variable number of keyphrases, in: Proc. ACL Conf., pp. 7961--7975. doi:[10.18653/v1/2020.acl-main.710](https:/doi.org/10.18653/v1/2020.acl-main.710).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袁等（2020） 袁，X.，王，T.，孟，R.，塔克，K.，布鲁西洛夫斯基，P.，赫，D.，特里施勒，A.，2020。一刀切的解决方案行不通：生成和评估可变数量的关键词，见：ACL会议论文集，第7961--7975页。doi：[10.18653/v1/2020.acl-main.710](https:/doi.org/10.18653/v1/2020.acl-main.710)。
- en: Zhang (2008) Zhang, C., 2008. Automatic keyword extraction from documents using
    conditional random fields. J. Comput. Inf. Syst. , 1169--1180.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张（2008） 张，C.，2008。使用条件随机场从文档中自动提取关键词。《计算机与信息系统》，1169--1180。
- en: 'Zhang et al. (2006) Zhang, K., Xu, H., Tang, J., Li, J., 2006. Keyword extraction
    using support vector machine, in: Proc. WAIM Conf., pp. 85--96. doi:[10.1007/11775300_8](https:/doi.org/10.1007/11775300_8).'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2006） 张，K.，徐，H.，唐，J.，李，J.，2006。使用支持向量机进行关键词提取，见：WAIM会议论文集，第85--96页。doi：[10.1007/11775300_8](https:/doi.org/10.1007/11775300_8)。
- en: 'Zhang et al. (2021) Zhang, L., Chen, Q., Wang, W., Deng, C., Zhang, S., Li,
    B., Wang, W., Cao, X., 2021. Mderank: A masked document embedding rank approach
    for unsupervised keyphrase extraction. CoRR doi:[10.48550/ARXIV.2110.06651](https:/doi.org/10.48550/ARXIV.2110.06651).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2021） 张，L.，陈，Q.，王，W.，邓，C.，张，S.，李，B.，王，W.，曹，X.，2021。Mderank：一种掩蔽文档嵌入排名方法用于无监督关键词提取。CoRR
    doi：[10.48550/ARXIV.2110.06651](https:/doi.org/10.48550/ARXIV.2110.06651)。
- en: 'Zhang et al. (2016) Zhang, Q., Wang, Y., Gong, Y., Huang, X., 2016. Keyphrase
    extraction using deep recurrent neural networks on twitter, in: Proc. EMNLP Conf.,
    pp. 836--845. doi:[10.18653/v1/d16-1080](https:/doi.org/10.18653/v1/d16-1080).'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2016） 张，Q.，王，Y.，龚，Y.，黄，X.，2016。使用深度递归神经网络在Twitter上提取关键词，见：EMNLP会议论文集，第836--845页。doi：[10.18653/v1/d16-1080](https:/doi.org/10.18653/v1/d16-1080)。
- en: 'Zhang et al. (2017a) Zhang, Y., Chang, Y., Liu, X., Gollapalli, S.D., Li, X.,
    Xiao, C., 2017a. MIKE: keyphrase extraction by integrating multidimensional information,
    in: Proc. CIKM Conf., pp. 1349--1358. doi:[10.1145/3132847.3132956](https:/doi.org/10.1145/3132847.3132956).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2017a） 张，Y.，张，Y.，刘，X.，戈拉帕利，S.D.，李，X.，肖，C.，2017a。MIKE：通过整合多维信息进行关键词提取，见：CIKM会议论文集，第1349--1358页。doi：[10.1145/3132847.3132956](https:/doi.org/10.1145/3132847.3132956)。
- en: 'Zhang et al. (2017b) Zhang, Y., Fang, Y., Xiao, W., 2017b. Deep keyphrase generation
    with a convolutional sequence to sequence model, in: Proc. ICSAI Conf., pp. 1477--1485.
    doi:[10.1109/ICSAI.2017.8248519](https:/doi.org/10.1109/ICSAI.2017.8248519).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2017b） 张，Y.，方，Y.，肖，W.，2017b。利用卷积序列到序列模型进行深度关键词生成，见：ICSAI会议论文集，第1477--1485页。doi：[10.1109/ICSAI.2017.8248519](https:/doi.org/10.1109/ICSAI.2017.8248519)。
- en: 'Zhang et al. (2022) Zhang, Y., Jiang, T., Yang, T., Li, X., Wang, S., 2022.
    HTKG: deep keyphrase generation with neural hierarchical topic guidance, in: Proc.
    SIGIR Conf., pp. 1044--1054. doi:[10.1145/3477495.3531990](https:/doi.org/10.1145/3477495.3531990).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2022） 张，Y.，蒋，T.，杨，T.，李，X.，王，S.，2022。HTKG：利用神经层级主题指导的深度关键词生成，见：SIGIR会议论文集，第1044--1054页。doi：[10.1145/3477495.3531990](https:/doi.org/10.1145/3477495.3531990)。
- en: 'Zhang et al. (2018) Zhang, Y., Li, J., Song, Y., Zhang, C., 2018. Encoding
    conversation context for neural keyphrase extraction from microblog posts, in:
    Proc. NAACL Conf., pp. 1676--1686. doi:[10.18653/v1/n18-1151](https:/doi.org/10.18653/v1/n18-1151).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2018） 张，Y.，李，J.，宋，Y.，张，C.，2018。对话上下文编码用于从微博帖子中提取神经关键词，见：NAACL会议论文集，第1676--1686页。doi：[10.18653/v1/n18-1151](https:/doi.org/10.18653/v1/n18-1151)。
- en: 'Zhang and Zhang (2019) Zhang, Y., Zhang, C., 2019. Using human attention to
    extract keyphrase from microblog post, in: Proc. ACL Conf., pp. 5867--5872. doi:[10.18653/v1/p19-1588](https:/doi.org/10.18653/v1/p19-1588).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张和张（2019） 张，Y.，张，C.，2019。利用人工注意力从微博帖子中提取关键词，见：ACL会议论文集，第5867--5872页。doi：[10.18653/v1/p19-1588](https:/doi.org/10.18653/v1/p19-1588)。
- en: Zhang et al. (2004) Zhang, Y., Zincir-Heywood, A.N., Milios, E.E., 2004. World
    wide web site summarization. Web Intell. Agent Syst. , 39--53doi:[10.5555/1039791.1039794](https:/doi.org/10.5555/1039791.1039794).
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2004）张扬、津吉尔-海伍德、米利奥斯，2004。网站摘要。Web Intell. Agent Syst. , 39--53doi：[10.5555/1039791.1039794](https:/doi.org/10.5555/1039791.1039794)。
- en: 'Zhang et al. (2019) Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q.,
    2019. ERNIE: enhanced language representation with informative entities, in: Proc.
    ACL Conf., pp. 1441--1451. doi:[10.18653/v1/p19-1139](https:/doi.org/10.18653/v1/p19-1139).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2019）张哲、韩旭、刘轶、姜鑫、孙敏、刘群，2019。ERNIE：具有信息性实体的增强语言表示，见：ACL会议论文集，第1441--1451页。doi：[10.18653/v1/p19-1139](https:/doi.org/10.18653/v1/p19-1139)。
- en: 'Zhao et al. (2021) Zhao, J., Bao, J., Wang, Y., Wu, Y., He, X., Zhou, B., 2021.
    SGG: learning to select, guide, and generate for keyphrase generation, in: Proc.
    NAACL Conf., pp. 5717--5726. doi:[10.18653/v1/2021.naacl-main.455](https:/doi.org/10.18653/v1/2021.naacl-main.455).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2021）赵骏、包建、王阳、吴颖、何轩、周波，2021。SGG：学习选择、引导和生成关键短语，见：NAACL会议论文集，第5717--5726页。doi：[10.18653/v1/2021.naacl-main.455](https:/doi.org/10.18653/v1/2021.naacl-main.455)。
- en: 'Zhao and Zhang (2019) Zhao, J., Zhang, Y., 2019. Incorporating linguistic constraints
    into keyphrase generation, in: Proc. ACL Conf., pp. 5224--5233. doi:[10.18653/v1/P19-1515](https:/doi.org/10.18653/v1/P19-1515).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵与张（2019）赵骏、张扬，2019。将语言学约束融入关键短语生成，见：ACL会议论文集，第5224--5233页。doi：[10.18653/v1/P19-1515](https:/doi.org/10.18653/v1/P19-1515)。
- en: 'Zhuang et al. (2021) Zhuang, L., Wayne, L., Ya, S., Jun, Z., 2021. A robustly
    optimized BERT pre-training approach with post-training, in: Proc. CCL Conf.,
    pp. 471--484. doi:[10.1007/978-3-030-84186-7_31](https:/doi.org/10.1007/978-3-030-84186-7_31).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄等（2021）庄磊、韦恩·李、雅舒、君泽，2021。具有后训练的稳健优化BERT预训练方法，见：CCL会议论文集，第471--484页。doi：[10.1007/978-3-030-84186-7_31](https:/doi.org/10.1007/978-3-030-84186-7_31)。
- en: \bio
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: pic/xbb.jpg Binbin Xie received the Bachelor degree in the school of informatics,
    Xiamen University, in 2021. And she is studying for a master’s degree under the
    supervision of Prof. Jinsong Su now. Her research interests include code generation,
    keyphrase generation and machine translation.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: pic/xbb.jpg 谢彬彬于2021年在厦门大学信息学院获得学士学位。她目前在**苏金松**教授的指导下攻读硕士学位。她的研究兴趣包括代码生成、关键短语生成和机器翻译。
- en: \endbio
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: pic/sj.jpg Jia Song was born in 2000\. She received her Bachelor degree in Economic
    Information Engineering School from Southwest University of Finance and Economics,
    and is a graduate student under the supervision of Prof. Jinsong Su now. Her major
    research interests are natural language processing and keyphrase generation.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: pic/sj.jpg 贾松于2000年出生。她在西南财经大学经济信息工程学院获得了学士学位，目前是**苏金松**教授的研究生。她的主要研究兴趣是自然语言处理和关键短语生成。
- en: \endbio
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: pic/sly.jpeg Liangying Shao was born in 2000\. She received her Bachelor degree
    in the school of informatics, Xiamen University, and is a graduate student under
    the supervision of Prof. Jinsong Su now. Her major research interests are natural
    language processing and keyphrase generation.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: pic/sly.jpeg 梁莹于2000年出生。她在厦门大学信息学院获得了学士学位，目前是**苏金松**教授的研究生。她的主要研究兴趣是自然语言处理和关键短语生成。
- en: \endbio
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: pic/wsh.png Suhang Wu was born in 2000\. He is a undergraduate student at the
    College of Computer Science and Electronic Engineering of Hunan University now.
    He will become a graduate student at Xiamen University under the supervision of
    Prof. Jinsong Su.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: pic/wsh.png 吴苏航于2000年出生。他现在是湖南大学计算机科学与电子工程学院的本科生。未来，他将成为厦门大学**苏金松**教授的研究生。
- en: \endbio
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: pic/wxp.jpg Xiangpeng Wei received the Ph.D. degree from the University of Chinese
    Academy of Sciences (UCAS) in 2021, supervised by Prof. Yue Hu. He is now a senior
    algorithm engineer in the Language Technology Lab at Alibaba DAMO Academy. His
    research interests include natural language processing and neural machine translation.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: pic/wxp.jpg 魏向鹏于2021年获得中国科学院大学（UCAS）的博士学位，导师为**胡越**教授。他目前是阿里巴巴达摩院语言技术实验室的高级算法工程师。他的研究兴趣包括自然语言处理和神经机器翻译。
- en: \endbio
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: pic/ybs.jpg Baosong Yang received the Ph.D at NLP2CT Lab of University of Macau,
    advised by Prof. Derek F. Wong, and is currently an algorithm expert in the Language
    Technology Lab at Alibaba DAMO Academy. His research interests include natural
    language processing and machine translation.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图片/ybs.jpg 杨宝松博士毕业于澳门大学NLP2CT实验室，师从王凯教授，目前是阿里达摩院语言技术实验室的算法专家。他的研究兴趣包括自然语言处理和机器翻译。
- en: \endbio
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
- en: \bio
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
- en: pic/lh.JPG Huan Lin received a master’s degree in Xiamen University supervised
    by Prof. Jinsong Su, and is now an algorithm engineer in the Language Technology
    Lab at Alibaba DAMO Academy. Her research interests include natural language processing
    and machine translation.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图片/lh.JPG 林欢在厦门大学获得硕士学位，师从苏劲松教授，现为阿里达摩院语言技术实验室的算法工程师。她的研究兴趣包括自然语言处理和机器翻译。
- en: \endbio
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
- en: \bio
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
- en: pic/jx.pdf Jun Xie received the Ph.D. degree in computer science from the Chinese
    Academy of Sciences, Beijing, China. He is currently a senior staff algorithm
    engineer in the Language Technology Lab at Alibaba DAMO Academy. His research
    interests include natural language processing and machine translation.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图片/jx.pdf 谢军在中国科学院获得计算机科学博士学位。目前是阿里达摩院语言技术实验室的高级算法工程师。他的研究兴趣包括自然语言处理和机器翻译。
- en: \endbio
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
- en: \bio
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
- en: pic/sjs.jpg Jinsong Su was born in 1982\. He received the Ph.D. degree in Chinese
    Academy of Sciences, and is now a professor in Xiamen University. His research
    interests include natural language processing and machine translation.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图片/sjs.jpg 苏劲松于1982年出生。他在中国科学院获得博士学位，现为厦门大学教授。他的研究兴趣包括自然语言处理和机器翻译。
- en: \endbio
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 个人简介
