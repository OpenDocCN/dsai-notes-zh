- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:40:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2303.03757] Deep Learning for Inertial Positioning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2303.03757] 深度学习在惯性定位中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.03757](https://ar5iv.labs.arxiv.org/html/2303.03757)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2303.03757](https://ar5iv.labs.arxiv.org/html/2303.03757)
- en: 'Deep Learning for Inertial Positioning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在惯性定位中的应用：综述
- en: 'Changhao Chen and Xianfei Pan The authors are with the College of Intelligence
    Science and Technology, National University of Defense Technology, Changsha, 410073,
    China Changhao Chen and Xianfei Pan are co-first authors. Changhao Chen is the
    corresponding author. (Email: changhao.chen66@outlook.com)This work was supported
    by National Natural Science Foundation of China (NFSC) under the Grant Number
    of 62103427, 62073331, 62103430, 62103429.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Changhao Chen 和 Xianfei Pan 作者隶属于国防科技大学智能科学与技术学院，中国长沙，410073。Changhao Chen 和
    Xianfei Pan 是共同第一作者。Changhao Chen 是通讯作者。（电子邮件：changhao.chen66@outlook.com）本工作得到了中国国家自然科学基金（NFSC）资助，资助号为62103427、62073331、62103430、62103429。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Inertial sensors are widely utilized in smartphones, drones, robots, and IoT
    devices, playing a crucial role in enabling ubiquitous and reliable localization.
    Inertial sensor-based positioning is essential in various applications, including
    personal navigation, location-based security, and human-device interaction. However,
    low-cost MEMS inertial sensors’ measurements are inevitably corrupted by various
    error sources, leading to unbounded drifts when integrated doubly in traditional
    inertial navigation algorithms, subjecting inertial positioning to the problem
    of error drifts. In recent years, with the rapid increase in sensor data and computational
    power, deep learning techniques have been developed, sparking significant research
    into addressing the problem of inertial positioning. Relevant literature in this
    field spans across mobile computing, robotics, and machine learning. In this article,
    we provide a comprehensive review of deep learning-based inertial positioning
    and its applications in tracking pedestrians, drones, vehicles, and robots. We
    connect efforts from different fields and discuss how deep learning can be applied
    to address issues such as sensor calibration, positioning error drift reduction,
    and multi-sensor fusion. This article aims to attract readers from various backgrounds,
    including researchers and practitioners interested in the potential of deep learning-based
    techniques to solve inertial positioning problems. Our review demonstrates the
    exciting possibilities that deep learning brings to the table and provides a roadmap
    for future research in this field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性传感器在智能手机、无人机、机器人和物联网设备中被广泛使用，在实现普遍和可靠的定位中发挥了关键作用。基于惯性传感器的定位在个人导航、基于位置的安全性和人机交互等各种应用中至关重要。然而，低成本MEMS惯性传感器的测量不可避免地受到各种误差源的干扰，导致在传统惯性导航算法中进行双重积分时出现无界漂移，使惯性定位面临误差漂移的问题。近年来，随着传感器数据和计算能力的快速增长，深度学习技术得到了发展，引发了大量研究以解决惯性定位的问题。该领域的相关文献涵盖了移动计算、机器人技术和机器学习等多个领域。本文对基于深度学习的惯性定位及其在行人、无人机、车辆和机器人的跟踪应用中的应用进行了全面回顾。我们将不同领域的努力联系起来，讨论深度学习如何应用于解决传感器校准、定位误差漂移减少和多传感器融合等问题。本文旨在吸引包括研究人员和实践者在内的各类读者，特别是对深度学习技术在解决惯性定位问题方面的潜力感兴趣的读者。我们的回顾展示了深度学习带来的令人兴奋的可能性，并为未来在这一领域的研究提供了路线图。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Inertial Navigation, Deep Learning, Inertial Sensor Calibration, Pedestrian
    Dead Reckoning, Sensor Fusion, Visual-inertial Odometry
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性导航，深度学习，惯性传感器校准，行人推测，传感器融合，视觉-惯性里程计
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'The inertial Measurement Unit (IMU) is widely used in smartphones, drones,
    VR/AR devices, robotics, and Internet of Things (IoT) devices. It continuously
    measures linear velocity and angular rate and tracks the motion of these platforms,
    as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning
    for Inertial Positioning: A Survey"). With the advancements in Micro-Electro-Mechanical
    Systems (MEMS) technology, today’s MEMS IMUs are small, energy-efficient, and
    cost-effective. Inertial positioning (navigation) calculates attitude, velocity,
    and position based on inertial measurements, making it a crucial element in various
    location-based applications, including locating and navigating individuals in
    public places (e.g., universities, malls, airports), supporting security and safety
    services (e.g., aiding first-responders), enabling smart city/infrastructure,
    and facilitating human-device interaction. Compared to other positioning solutions
    such as vision or radio, inertial positioning is completely ego-centric, works
    indoors and outdoors, and is less affected by environmental factors such as complex
    lighting conditions and scene dynamics.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '惯性测量单元（IMU）广泛应用于智能手机、无人机、虚拟现实/增强现实设备、机器人和物联网（IoT）设备。它持续测量线性速度和角速度，并跟踪这些平台的运动，如图
    [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning for Inertial Positioning:
    A Survey") 所示。随着微电机械系统（MEMS）技术的发展，今天的MEMS IMU体积小、能效高、成本低。惯性定位（导航）基于惯性测量计算姿态、速度和位置，使其在各种基于位置的应用中至关重要，包括在公共场所（如大学、购物中心、机场）定位和导航个人、支持安全服务（如协助急救人员）、实现智能城市/基础设施，以及促进人机交互。与其他定位解决方案如视觉或无线电相比，惯性定位完全以自我为中心，能在室内和室外工作，并且不易受到复杂光照条件和场景动态等环境因素的影响。'
- en: '![Refer to caption](img/b9f237afb2f132524a9de20008c976e3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9f237afb2f132524a9de20008c976e3.png)'
- en: 'Figure 1: Inertial sensors are ubiquitous in modern platforms such as smartphones,
    drones, intelligent vehicles, and VR/AR devices. They play a critical role in
    enabling completely egocentric motion tracking and positioning, making them essential
    for a range of applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：惯性传感器在现代平台中无处不在，如智能手机、无人机、智能车辆和虚拟现实/增强现实设备。它们在实现完全自我中心的运动跟踪和定位方面发挥了关键作用，使其在各种应用中不可或缺。
- en: Unfortunately, the measurements obtained from low-cost MEMS IMUs are subject
    to several error sources such as bias error, temperature-dependent error, random
    sensor noise, and random-walk noise. In classical inertial navigation mechanisms,
    angular rates are integrated into orientation, and based on the acquired attitude,
    acceleration measurements are transformed into the navigation frame. Finally,
    the transformed accelerations are doubly integrated into locations [[1](#bib.bib1),
    [2](#bib.bib2)]. Traditional inertial navigation algorithms are designed and described
    using concrete physical and mathematical rules. Under ideal conditions, sensor
    errors are small enough to allow hand-designed inertial navigation algorithms
    to produce accurate and reliable pose estimates. However, in real-world applications,
    inevitable measurement errors cause significant problems for inertial positioning
    systems without constraints, which can fail within seconds. In this process, even
    a minor error can be amplified exponentially, resulting in unbounded error drifts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，低成本MEMS IMU获得的测量值受到多个误差源的影响，如偏置误差、温度依赖误差、随机传感器噪声和随机游走噪声。在经典惯性导航机制中，角速度被积分为方向，并根据获得的姿态将加速度测量转换为导航框架。最后，转换后的加速度被双重积分到位置
    [[1](#bib.bib1), [2](#bib.bib2)]。传统的惯性导航算法使用具体的物理和数学规则进行设计和描述。在理想条件下，传感器误差足够小，使得手工设计的惯性导航算法能产生准确可靠的姿态估计。然而，在实际应用中，不可避免的测量误差会导致惯性定位系统出现重大问题，系统可能在几秒钟内失败。在这个过程中，即使是微小的误差也可能被指数放大，导致无界的误差漂移。
- en: Previous researchers have attempted to address the problem of error drifts in
    inertial navigation by incorporating domain-specific knowledge or other sensor.
    In the context of pedestrian tracking, exploiting the periodicity of human walking
    is important, and the process of pedestrian dead reckoning (PDR) involves detecting
    steps, estimating step length and heading, and updating the user’s location to
    mitigate error drifts from exponential to linear increase [[3](#bib.bib3)]. Zero-velocity
    update (ZUPT) involves attaching the IMU to the user’s foot and detecting the
    zero-velocity phase, which is then used in Kalman filtering to correct inertial
    navigation states [[4](#bib.bib4)]. Platforms such as drones or robots equipped
    with other sensors such as cameras or LiDAR can significantly improve the performance
    of pure inertial solutions by effectively integrating inertial sensors with these
    modalities through filtering or smoothing [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)].
    However, these solutions have limitations in specific application domains and
    are unable to address the fundamental problem of inertial navigation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的研究者尝试通过引入领域特定知识或其他传感器来解决惯性导航中的误差漂移问题。在行人跟踪的背景下，利用人体步态的周期性是重要的，行人自动定位（PDR）过程包括检测步伐、估计步长和方向，并更新用户的位置，以将误差漂移从指数增长减小为线性增长[[3](#bib.bib3)]。零速度更新（ZUPT）涉及将IMU附着到用户的脚上，并检测零速度阶段，然后在卡尔曼滤波中使用这一阶段来校正惯性导航状态[[4](#bib.bib4)]。配备其他传感器如相机或LiDAR的无人机或机器人平台可以通过有效地将惯性传感器与这些模式融合来显著提升纯惯性解决方案的性能[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]。然而，这些解决方案在特定应用领域存在局限性，无法解决惯性导航的根本问题。
- en: Recently, deep learning has shown impressive performance in various fields,
    including computer vision, robotics, and signal processing [[8](#bib.bib8)]. It
    has also been introduced to address the challenges of inertial positioning. Deep
    neural network models have been leveraged to calibrate inertial sensor noises,
    reduce the drifts of inertial navigation mechanisms, and fuse inertial data with
    other sensor information. These research works have attracted significant attention,
    as they show potential for exploiting massive data to generate data-driven models
    instead of relying on concrete physical or mathematical models. With the rapid
    development of deep learning techniques, learning-based inertial solutions have
    become even more promising.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习在计算机视觉、机器人技术和信号处理等多个领域表现出了令人印象深刻的性能[[8](#bib.bib8)]。它也被引入以应对惯性定位的挑战。深度神经网络模型被用于校准惯性传感器噪声，减少惯性导航机制的漂移，并将惯性数据与其他传感器信息融合。这些研究工作吸引了大量关注，因为它们展示了利用海量数据生成数据驱动模型的潜力，而不是依赖于具体的物理或数学模型。随着深度学习技术的迅速发展，基于学习的惯性解决方案变得更加有前景。
- en: In this survey, we provide a comprehensive review of deep-learning-based approaches
    to inertial positioning, including measurement calibration, inertial positioning
    algorithms, and sensor fusion. We discuss the benefits and limitations of existing
    works and identify challenges and future opportunities in this research direction.
    Compared with other deep learning surveys, such as those focused on object detection
    [[9](#bib.bib9)], semantic segmentation [[10](#bib.bib10)], and robotics [[11](#bib.bib11)],
    survey on deep learning based inertial positioning is relatively scarce and hard
    to find. While a broader survey on machine learning enhanced inertial sensing
    does exist [[12](#bib.bib12)], our survey narrows the focus to deep learning based
    inertial positioning, providing deeper insights and analysis of the fast-evolving
    developments in this area over the past five years (2018-2022). Other relevant
    surveys, such as those focused on inertial pedestrian positioning [[3](#bib.bib3)],
    indoor positioning [[13](#bib.bib13)], step length estimation [[14](#bib.bib14)],
    and pedestrian dead reckoning [[15](#bib.bib15)], do not cover recent deep learning
    based solutions. To the best of our knowledge, this article is the first survey
    that discusses deep learning based inertial positioning thoroughly and deeply.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们提供了基于深度学习的惯性定位方法的全面回顾，包括测量校准、惯性定位算法和传感器融合。我们讨论了现有工作的优点和局限性，并识别了该研究方向中的挑战和未来机会。与其他深度学习调查相比，例如那些专注于物体检测[[9](#bib.bib9)]、语义分割[[10](#bib.bib10)]和机器人技术[[11](#bib.bib11)]的调查，基于深度学习的惯性定位调查相对稀缺且难以找到。虽然存在更广泛的机器学习增强惯性传感的调查[[12](#bib.bib12)]，但我们的调查将重点缩小到基于深度学习的惯性定位，提供了对过去五年（2018-2022）该领域快速发展的深入见解和分析。其他相关调查，如那些专注于惯性步态定位[[3](#bib.bib3)]、室内定位[[13](#bib.bib13)]、步长估计[[14](#bib.bib14)]和步态推测[[15](#bib.bib15)]的调查，并未涵盖最新的基于深度学习的解决方案。根据我们所知，本文是第一个彻底且深入讨论基于深度学习的惯性定位的调查。
- en: The rest of this survey is organized as follows. Section II briefly introduces
    classical inertial navigation mechanisms. Section III, IV and V survey deep learning
    based sensor calibration, inertial navigation algorithms, and sensor fusion. Section
    VII finally discusses the benefits, challenges and opportunities.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查的其余部分组织如下：第II节简要介绍了经典惯性导航机制。第III、IV和V节调查了基于深度学习的传感器校准、惯性导航算法和传感器融合。第VII节最后讨论了益处、挑战和机会。
- en: II Classical Inertial Navigation Mechanisms
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 经典惯性导航机制
- en: This section provides an overview of classical inertial navigation mechanisms
    and highlights their limitations. It begins by presenting the inertial measurement
    model and classical strapdown inertial navigation method. Subsequently, two solutions
    that aim to reduce the drifts of inertial navigation system, namely pedestrian
    dead reckoning (PDR) and zero-velocity update (ZUPT), are discussed, with a specific
    focus on their applicability in pedestrian tracking scenarios. The section finally
    introduces sensor fusion approaches that integrate inertial data with information
    from other sensors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了经典惯性导航机制的概述，并突出其局限性。它首先介绍了惯性测量模型和经典的带式惯性导航方法。随后，讨论了旨在减少惯性导航系统漂移的两种解决方案，即步态推测（PDR）和零速更新（ZUPT），并特别关注它们在步态跟踪场景中的适用性。最后，本节介绍了将惯性数据与其他传感器信息集成的传感器融合方法。
- en: II-A Inertial Measurement Model
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 惯性测量模型
- en: 'Inertial measurements acquired from low-cost MEMS IMUs are often corrupted
    by various types of error sources, resulting in unbounded error drifts when integrated
    in strapdown inertial navigation systems (SINS). These error sources can be classified
    into two categories: deterministic errors and random errors [[16](#bib.bib16)].
    Deterministic errors comprise bias error, non-orthogonality error, misalignment
    error, scale-factor error, and temperature-dependent error. On the other hand,
    random errors include random sensor noise and random-walk noise resulting from
    long-term operation, which are challenging to model and eliminate.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从低成本MEMS IMU获得的惯性测量数据通常受到各种错误源的影响，导致在带式惯性导航系统（SINS）中积分时出现不受限的误差漂移。这些错误源可以分为两类：确定性误差和随机误差[[16](#bib.bib16)]。确定性误差包括偏差误差、非正交误差、对准误差、尺度因子误差和温度依赖误差。另一方面，随机误差包括随机传感器噪声和由于长期操作产生的随机游走噪声，这些噪声很难建模和消除。
- en: Raw IMU measurements, i.e. accelerations $\hat{\mathbf{a}}$ and angular rates
    $\hat{\bm{\omega}}$, can be formulated by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 原始IMU测量值，即加速度$\hat{\mathbf{a}}$和角速度$\hat{\bm{\omega}}$，可以表示为
- en: '|  | $\hat{\mathbf{a}}=\mathbf{a}+\mathbf{b}_{a}+\mathbf{n}_{a}$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{a}}=\mathbf{a}+\mathbf{b}_{a}+\mathbf{n}_{a}$ |  | (1) |'
- en: '|  | $\hat{\bm{\omega}}=\bm{\omega}+\mathbf{b}_{\omega}+\mathbf{n}_{\omega}$
    |  | (2) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\bm{\omega}}=\bm{\omega}+\mathbf{b}_{\omega}+\mathbf{n}_{\omega}$
    |  | (2) |'
- en: where $\mathbf{b}_{a}$ and $\mathbf{b}_{\omega}$ are acceleration bias and gyroscope
    bias, $\mathbf{n}_{a}$ and $\mathbf{n}_{\omega}$ are additive noises above accelerometer
    and gyroscope.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{b}_{a}$和$\mathbf{b}_{\omega}$分别是加速度偏差和陀螺仪偏差，$\mathbf{n}_{a}$和$\mathbf{n}_{\omega}$是加速度计和陀螺仪上的加性噪声。
- en: Traditionally, it is important to calibrate inertial sensors before running
    an inertial navigation algorithm that involves integrating inertial data into
    system states. One effective tool for achieving this is the Allan variance method
    [[17](#bib.bib17)], which models the random process of inertial sensor errors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，在运行涉及将惯性数据集成到系统状态中的惯性导航算法之前，对惯性传感器进行校准非常重要。实现这一目标的一个有效工具是Allan方差方法[[17](#bib.bib17)]，该方法对惯性传感器误差的随机过程进行建模。
- en: II-B Strapdown Inertial Navigation System
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 固定惯性导航系统
- en: 'Inertial sensor measures linear accelerations $\mathbf{a}_{b}(t)$ and angular
    rates $\bm{\omega}_{b}^{n}(t)$ of attached user body at the timestep $t$. $b$
    represents the body frame, while $n$ denotes the navigation (world) frame, i.e.
    the navigation frame. $\bm{\omega}_{b}^{n}(t)$ means that the angular rates of
    body frame with respect to the navigation frame. To simplify inertial motion model,
    this article assumes that the biases and noises of sensor in Equation [1](#S2.E1
    "In II-A Inertial Measurement Model ‣ II Classical Inertial Navigation Mechanisms
    ‣ Deep Learning for Inertial Positioning: A Survey") and [2](#S2.E2 "In II-A Inertial
    Measurement Model ‣ II Classical Inertial Navigation Mechanisms ‣ Deep Learning
    for Inertial Positioning: A Survey") have been removed in the stage of inertial
    sensor calibration. $(\mathbf{R},\mathbf{p})$ are defined orientation and position
    variables. From the kinematic model of IMU, we can have'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '惯性传感器测量附加用户身体在时间步长$t$上的线性加速度$\mathbf{a}_{b}(t)$和角速度$\bm{\omega}_{b}^{n}(t)$。$b$表示身体坐标系，而$n$表示导航（世界）坐标系，即导航坐标系。$\bm{\omega}_{b}^{n}(t)$表示身体坐标系相对于导航坐标系的角速度。为了简化惯性运动模型，本文假设传感器在方程[1](#S2.E1
    "In II-A Inertial Measurement Model ‣ II Classical Inertial Navigation Mechanisms
    ‣ Deep Learning for Inertial Positioning: A Survey")和[2](#S2.E2 "In II-A Inertial
    Measurement Model ‣ II Classical Inertial Navigation Mechanisms ‣ Deep Learning
    for Inertial Positioning: A Survey")中的偏差和噪声已在惯性传感器校准阶段被去除。$(\mathbf{R},\mathbf{p})$是定义的方向和位置变量。根据IMU的运动学模型，我们可以得到'
- en: '|  | <math   alttext="\begin{cases}\mathbf{R}_{b}^{n}(t+1)=\mathbf{R}_{b}^{n}(t)\mathbf{R}_{b_{t+1}}^{b_{t}}\\
    \mathbf{v}_{n}(t+1)=\mathbf{v}_{n}(t)+\mathbf{a}_{n}(t)dt\\'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{cases}\mathbf{R}_{b}^{n}(t+1)=\mathbf{R}_{b}^{n}(t)\mathbf{R}_{b_{t+1}}^{b_{t}}\\
    \mathbf{v}_{n}(t+1)=\mathbf{v}_{n}(t)+\mathbf{a}_{n}(t)dt\\'
- en: \mathbf{p}_{n}(t+1)=\mathbf{p}_{n}(t)+\mathbf{v}_{n}(t)dt+\frac{1}{2}\mathbf{a}_{n}(t)dt^{2}\end{cases}"
    display="block"><semantics ><mrow  ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd  columnalign="left" ><mrow ><mrow  ><msubsup ><mi
    >𝐑</mi><mi >b</mi><mi >n</mi></msubsup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow ><msubsup  ><mi >𝐑</mi><mi
    >b</mi><mi >n</mi></msubsup><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><msubsup
    ><mi >𝐑</mi><msub ><mi >b</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><msub
    ><mi  >b</mi><mi >t</mi></msub></msubsup></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow  ><msub ><mi >𝐯</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow ><mrow  ><msub ><mi >𝐯</mi><mi
    >n</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >+</mo><mrow ><msub
    ><mi  >𝐚</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >t</mi></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow  ><msub ><mi >𝐩</mi><mi >n</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi
    >t</mi><mo >+</mo><mn >1</mn></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mrow  ><msub ><mi >𝐩</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >+</mo><mrow ><msub ><mi  >𝐯</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow><mo >+</mo><mrow ><mstyle displaystyle="false" ><mfrac ><mn >1</mn><mn
    >2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >𝐚</mi><mi
    >n</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi >t</mi><mn >2</mn></msup></mrow></mrow></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="latexml"  >cases</csymbol><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐑</ci><ci >𝑏</ci></apply><ci >𝑛</ci></apply><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐑</ci><ci >𝑏</ci></apply><ci >𝑛</ci></apply><ci >𝑡</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐑</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑏</ci><apply ><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑏</ci><ci >𝑡</ci></apply></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐯</ci><ci >𝑛</ci></apply><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐯</ci><ci
    >𝑛</ci></apply><ci >𝑡</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐚</ci><ci >𝑛</ci></apply><ci >𝑡</ci><ci  >𝑑</ci><ci >𝑡</ci></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐩</ci><ci >𝑛</ci></apply><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐩</ci><ci
    >𝑛</ci></apply><ci >𝑡</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐯</ci><ci >𝑛</ci></apply><ci >𝑡</ci><ci  >𝑑</ci><ci >𝑡</ci></apply><apply ><apply
    ><cn type="integer" >1</cn><cn type="integer" >2</cn></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐚</ci><ci >𝑛</ci></apply><ci >𝑡</ci><ci  >𝑑</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑡</ci><cn type="integer"  >2</cn></apply></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{cases}\mathbf{R}_{b}^{n}(t+1)=\mathbf{R}_{b}^{n}(t)\mathbf{R}_{b_{t+1}}^{b_{t}}\\
    \mathbf{v}_{n}(t+1)=\mathbf{v}_{n}(t)+\mathbf{a}_{n}(t)dt\\ \mathbf{p}_{n}(t+1)=\mathbf{p}_{n}(t)+\mathbf{v}_{n}(t)dt+\frac{1}{2}\mathbf{a}_{n}(t)dt^{2}\end{cases}</annotation></semantics></math>
    |  | (3) |
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbf{a}_{n}$, $\mathbf{v}_{n}$, $\mathbf{p}_{n}$ are acceleration,
    velocity and position in the navigation frame, $\mathbf{R}_{b}^{n}$ represents
    the rotation from the body frame to the navigation frame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{a}_{n}$、$\mathbf{v}_{n}$、$\mathbf{p}_{n}$分别是导航框架中的加速度、速度和位置，$\mathbf{R}_{b}^{n}$表示从机体框架到导航框架的旋转。
- en: 'Firstly, orientation is updated by inferring the rotation matrix $\bm{\Omega(t)}$
    via Rodriguez formula:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过罗德里格斯公式推断旋转矩阵$\bm{\Omega(t)}$来更新方向：
- en: '|  | $\begin{split}\bm{\Omega}(t)&amp;=\mathbf{R}_{b_{t+1}}^{b_{t}}\\ &amp;=\mathbf{I}+\sin(\bm{\sigma})\frac{[\bm{\sigma}\times]}{\bm{\sigma}}+(1-\cos(\bm{\sigma}))\frac{[\bm{\sigma}\times]^{2}}{\bm{\sigma}^{2}},\end{split}$
    |  | (4) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\bm{\Omega}(t)&amp;=\mathbf{R}_{b_{t+1}}^{b_{t}}\\ &amp;=\mathbf{I}+\sin(\bm{\sigma})\frac{[\bm{\sigma}\times]}{\bm{\sigma}}+(1-\cos(\bm{\sigma}))\frac{[\bm{\sigma}\times]^{2}}{\bm{\sigma}^{2}},\end{split}$
    |  | (4) |'
- en: where rotation vector $\bm{\sigma}=\bm{\omega}(t)dt$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中旋转向量$\bm{\sigma}=\bm{\omega}(t)dt$。
- en: To update velocity, the accelerations in navigation frame can be expressed as
    a function of measured accelerations, i.e.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新速度，导航框架中的加速度可以表示为测量加速度的函数，即：
- en: '|  | $\mathbf{a}_{n}(t)=\mathbf{R}_{b}^{n}(t-1)\mathbf{a}_{b}(t)-\mathbf{g}_{n}$
    |  | (5) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{a}_{n}(t)=\mathbf{R}_{b}^{n}(t-1)\mathbf{a}_{b}(t)-\mathbf{g}_{n}$
    |  | (5) |'
- en: 'Then, the accelerations in navigation frame $\mathbf{a}_{n}(t)$ are integrated
    into the velocity in the navigation frame $\mathbf{v}_{n}(t)$, and the location
    $\mathbf{p}_{n}(t)$ is finally updated by integrating the velocity via Equation
    [4](#S2.E4 "In II-B Strapdown Inertial Navigation System ‣ II Classical Inertial
    Navigation Mechanisms ‣ Deep Learning for Inertial Positioning: A Survey").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将导航框架中的加速度$\mathbf{a}_{n}(t)$积分到导航框架中的速度$\mathbf{v}_{n}(t)$中，最后通过方程[4](#S2.E4
    "在 II-B Strapdown 惯性导航系统 ‣ II 经典惯性导航机制 ‣ 惯性定位的深度学习：综述")通过积分速度来更新位置$\mathbf{p}_{n}(t)$。
- en: As we can see, in this process, even a small measurement error can be exponentially
    amplified, leading to the problem of inertial error drifts. In the past, high-precision
    inertial sensors such as laser or fiber inertial sensors could keep the measurement
    error small enough to maintain the accuracy of INS. However, due to the size and
    cost limitations of current MEMS IMUs, compensation methods are necessary to mitigate
    the corresponding error drifts. One approach is to introduce domain-specific knowledge
    or other sensor information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在这个过程中，即使是很小的测量误差也可能会被指数级放大，导致惯性误差漂移的问题。过去，高精度惯性传感器如激光或光纤惯性传感器能够保持测量误差足够小，从而维持INS的准确性。然而，由于当前MEMS
    IMU的尺寸和成本限制，补偿方法变得必要，以减轻相应的误差漂移。一种方法是引入领域特定知识或其他传感器信息。
- en: II-C Domain Specific Knowledge
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 领域特定知识
- en: II-C1 Pedestrian Dead Reckoning
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 步态死计算
- en: 'Pedestrian dead reckoning (PDR) is a method that leverages domain-specific
    knowledge about human walking to track pedestrian motion. PDR comprises three
    main steps: step detection, heading and stride length estimation, and location
    update [[3](#bib.bib3)]. In step detection, PDR uses the threshold of inertial
    data to identify step peaks or stances and segment the corresponding inertial
    data. Dynamic stride length estimation is then achieved via an empirical formula,
    known as the Weinberg formula [[18](#bib.bib18)], which considers the segmented
    accelerations and user’s height. Similar to SINS, heading estimation is done by
    integrating gyroscope signals into orientation changes and adding orientation
    changes to the initial orientation to obtain the current heading. Finally, the
    estimated heading and stride length are used to update the pedestrian’s location.
    By avoiding double integration of accelerations and incorporating a reliable stride
    estimation model, PDR effectively reduces inertial positioning drifts. However,
    inaccurate step detection and stride estimation can still occur, leading to large
    system error drifts. Moreover, PDR is limited to pedestrian navigation as it depends
    on the periodicity of human walking.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 行人死算（PDR）是一种利用有关人体步态的领域特定知识来跟踪行人运动的方法。PDR包括三个主要步骤：步态检测、方向和步幅估计，以及位置更新[[3](#bib.bib3)]。在步态检测中，PDR使用惯性数据的阈值来识别步态峰值或姿态，并分割相应的惯性数据。然后，通过一个经验公式（称为Weinberg公式[[18](#bib.bib18)]）动态估计步幅，该公式考虑了分割的加速度和用户的身高。与SINS类似，方向估计是通过将陀螺仪信号积分到方向变化中，并将方向变化加到初始方向上以获得当前方向。最后，估计的方向和步幅用于更新行人的位置。通过避免加速度的双重积分并结合可靠的步幅估计模型，PDR有效地减少了惯性定位漂移。然而，不准确的步态检测和步幅估计仍然可能发生，从而导致系统误差漂移。此外，由于PDR依赖于人体步态的周期性，因此它仅限于步行导航。
- en: II-C2 Zero-velocity Update
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 零速度更新
- en: The Zero-velocity update (ZUPT) algorithm is designed to compensate for the
    errors of SINS by identifying the still phase of human walking and using zero-velocity
    as observations in a Kalman filter [[4](#bib.bib4)]. To facilitate the detection
    of the still phase, the IMU is typically attached to the user’s foot, as it undergoes
    significant motion and reflects walking patterns well. Techniques such as peak-detection
    [[19](#bib.bib19)], zero-crossings [[20](#bib.bib20)], or auto-correlation [[21](#bib.bib21)]
    can be used to analyze the inertial data and segment the zero-velocity phase.
    Once the still phase is detected, zero-velocity is used as pseudo-measurements
    in the filtering process, thereby limiting the error drifts of open-loop integration.
    However, the effectiveness of ZUPT depends on the assumption that the user’s foot
    remains completely still, and any incorrect still phase detection or small motion
    disturbances can cause navigation system drifts. Additionally, ZUPT is limited
    to pedestrian tracking.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 零速度更新（ZUPT）算法旨在通过识别人体步态的静止阶段并使用零速度作为卡尔曼滤波器中的观测值来补偿SINS的误差[[4](#bib.bib4)]。为了方便检测静止阶段，IMU通常被安装在用户的脚上，因为脚部运动显著并能很好地反映步态模式。可以使用峰值检测[[19](#bib.bib19)]、零交叉[[20](#bib.bib20)]或自相关[[21](#bib.bib21)]等技术来分析惯性数据并分割零速度阶段。一旦检测到静止阶段，零速度将作为伪测量值用于过滤过程，从而限制开环积分的误差漂移。然而，ZUPT的有效性依赖于用户的脚完全静止的假设，任何不正确的静止阶段检测或小的运动干扰都可能导致导航系统的漂移。此外，ZUPT仅限于步行跟踪。
- en: II-D Integrating IMU with Other Sensors
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D IMU与其他传感器的集成
- en: Integrating the IMU with other sensors, such as camera [[6](#bib.bib6)], LiDAR
    [[7](#bib.bib7)], UWB [[22](#bib.bib22)], and magnetometer [[23](#bib.bib23)],
    can provide promising results as it allows for exploiting their complementary
    properties. By fusing the data from multiple sensors, the accuracy and robustness
    of pose estimation can be significantly improved, making it a general solution
    for all platforms. However, in some scenarios, certain sensors, such as visual
    perception, may not be available or highly dependent on the environment, which
    can negatively affect the egocentric property of inertial positioning. Additionally,
    in sensor fusion approaches, it is essential to consider various factors such
    as sensor calibration, initialization, and time-synchronization.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将IMU与其他传感器（如相机 [[6](#bib.bib6)]、激光雷达 [[7](#bib.bib7)]、超宽带 [[22](#bib.bib22)]
    和磁力计 [[23](#bib.bib23)]）集成，可以获得有希望的结果，因为这允许利用它们的互补特性。通过融合多个传感器的数据，可以显著提高姿态估计的准确性和鲁棒性，使其成为所有平台的通用解决方案。然而，在某些场景中，某些传感器（如视觉感知）可能不可用或高度依赖环境，这可能会对惯性定位的自我中心属性产生负面影响。此外，在传感器融合方法中，考虑各种因素如传感器标定、初始化和时间同步是至关重要的。
- en: 'TABLE I: A summary of existing methods on deep learning based inertial sensor
    calibration.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 基于深度学习的惯性传感器标定现有方法的汇总。'
- en: '| name | year | sensor | model | learning | target |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 传感器 | 模型 | 学习 | 目标 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Xiyuan et al.[[24](#bib.bib24)] | 2003 | gyro | 1-layer NN | SL | gyro drifts
    compensation |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Xiyuan et al.[[24](#bib.bib24)] | 2003 | 陀螺仪 | 单层神经网络 | 监督学习 | 陀螺仪漂移补偿 |'
- en: '| Chen et al. [[25](#bib.bib25)] | 2018 | gyro, acc | ConvNet | SL | inertial
    noise compensation |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. [[25](#bib.bib25)] | 2018 | 陀螺仪、加速度计 | 卷积网络 | 监督学习 | 惯性噪声补偿 |'
- en: '| Esfahani et al. [[26](#bib.bib26)] | 2019 | gyro | LSTM | SL | gyroscope
    calibration |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Esfahani et al. [[26](#bib.bib26)] | 2019 | 陀螺仪 | 长短期记忆网络 | 监督学习 | 陀螺仪标定
    |'
- en: '| Nobre et al. [[27](#bib.bib27)] | 2019 | gyro, acc | Deep Q-Network | RL
    | optimal calibration parameters |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Nobre et al. [[27](#bib.bib27)] | 2019 | 陀螺仪、加速度计 | 深度 Q 网络 | 强化学习 | 最优标定参数
    |'
- en: '| Brossard et al. [[28](#bib.bib28)] | 2020 | gyro | ConvNet | SL | gyro corrections
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Brossard et al. [[28](#bib.bib28)] | 2020 | 陀螺仪 | 卷积网络 | 监督学习 | 陀螺仪修正 |'
- en: '| Zhao et al.[[29](#bib.bib29)] | 2020 | gyro | LSTM | SL | gyroscope calibration
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al.[[29](#bib.bib29)] | 2020 | 陀螺仪 | 长短期记忆网络 | 监督学习 | 陀螺仪标定 |'
- en: '| Huang et al.[[30](#bib.bib30)] | 2022 | gyro | Temporal ConvNet | SL | gyroscope
    calibration |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al.[[30](#bib.bib30)] | 2022 | 陀螺仪 | 时序卷积网络 | 监督学习 | 陀螺仪标定 |'
- en: '| Calib-Net [[31](#bib.bib31)] | 2022 | gyro | Dilated ConvNet | SL | gyroscope
    denoising |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Calib-Net [[31](#bib.bib31)] | 2022 | 陀螺仪 | 扩张卷积网络 | 监督学习 | 陀螺仪去噪 |'
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Years indicates the publication year of each work.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 年份指示每项工作的出版年份。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensors indicates the sensors involved in each work. gyro and acc represent
    gyroscope and accelerometer respectively.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传感器指示每项工作中涉及的传感器。**陀螺仪**和**加速度计**分别代表陀螺仪和加速度计。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model indicates which module the framework consists of.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型指示框架由哪个模块组成。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning indicates how to train neural networks. SL and RL represent Supervised
    Learning and Reinforcement Learning.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习表明如何训练神经网络。SL 和 RL 分别代表**监督学习**和**强化学习**。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Target indicates what the model aims to solve or produce.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标指示模型旨在解决或产生的内容。
- en: II-E Discussion
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 讨论
- en: As previously mentioned, classical inertial navigation methods are designed
    to solve specific problems within their respective domains. However, their performance
    is often limited due to real-world issues such as imperfect modeling, measurement
    errors, and environmental influences, resulting in inevitable error drifts. Researchers
    in the field of inertial navigation are therefore constantly searching for ways
    to build models that can tolerate measurement errors and mitigate system drifts.
    In addition to relying on Newtonian physical rules, it has been observed that
    domain-specific knowledge, whether it be an experienced human walking model or
    scene geometry, can serve as a useful constraint in reducing the error drifts
    of inertial positioning systems. One potential approach to improving inertial
    positioning accuracy and robustness is to exploit massive inertial data to extract
    domain-specific knowledge and construct a data-driven model. In the next sections,
    we will delve deeper into this problem and explore potential solutions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，经典的惯性导航方法旨在解决各自领域中的特定问题。然而，由于现实世界中的问题如模型不完善、测量误差和环境影响，它们的性能往往受到限制，导致不可避免的误差漂移。因此，惯性导航领域的研究人员不断寻求构建能够容忍测量误差并减轻系统漂移的模型的方法。除了依赖牛顿物理规则外，观察到领域特定知识，无论是经验丰富的人行走模型还是场景几何形状，都可以作为减少惯性定位系统误差漂移的有用约束。提高惯性定位准确性和鲁棒性的一个潜在方法是利用大量惯性数据提取领域特定知识并构建数据驱动模型。在接下来的部分，我们将更深入地探讨这个问题并探索潜在的解决方案。
- en: '![Refer to caption](img/d9b883bc03d786c177f037b788968df9.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9b883bc03d786c177f037b788968df9.png)'
- en: 'Figure 2: An overview of existing deep learning based inertial sensor calibration
    methods'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 现有深度学习基于惯性传感器校准方法的概述'
- en: III Deep Learning Based Inertial Sensor Calibration
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于深度学习的惯性传感器校准
- en: Inertial measurements obtained from low-cost IMUs are often affected by various
    sources of noise, making it challenging to distinguish the true values from the
    sources of error. The error sources are a complex interplay of deterministic and
    random factors, further complicating the issue. To address the impact of measurement
    errors, the powerful nonlinear approximator capabilities of deep neural networks
    can be exploited. A natural approach is to develop a deep neural network that
    receives the raw inertial measurements as input and produces the calibrated inertial
    measurements as output, representing the actual platform motion. By training this
    neural model on labeled datasets using stochastic gradient descent (SGD) [[32](#bib.bib32)],
    the inertial measurement errors can be implicitly learned and corrected by the
    neural network. It is important to note that the quality of the collected training
    dataset has a significant impact on the performance of the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从低成本惯性测量单元（IMU）获得的惯性测量数据常常受到各种噪声源的影响，这使得区分真实值与误差源变得具有挑战性。这些误差源是确定性和随机因素的复杂交互，进一步使问题复杂化。为了解决测量误差的影响，可以利用深度神经网络强大的非线性逼近能力。一种自然的方法是开发一个深度神经网络，该网络接收原始惯性测量作为输入，并生成校准后的惯性测量作为输出，代表实际平台运动。通过使用随机梯度下降（SGD）[[32](#bib.bib32)]对标记数据集进行训练，这种神经模型可以隐式地学习和纠正惯性测量误差。值得注意的是，收集的训练数据集的质量对模型性能有显著影响。
- en: Before the age of deep learning, attempts were made to use neural networks to
    learn the measurement errors of inertial sensors. For example, a 1-layer artificial
    neural network (ANN) [[33](#bib.bib33)] is proposed to model the distribution
    of gyro drifts, and is able to successfully approximate gyro drifts with such
    a ’shallow’ network [[24](#bib.bib24)] . This method has an advantage over Kalman
    filtering (KF) based calibration methods in that it does not require setting hyper-parameters
    before use, such as the sensor noise matrix in KF.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代之前，曾尝试使用神经网络学习惯性传感器的测量误差。例如，提出了一种1层人工神经网络（ANN）[[33](#bib.bib33)]来建模陀螺漂移的分布，并能够成功地用这样一个“浅层”网络来逼近陀螺漂移[[24](#bib.bib24)]。这种方法相比于基于卡尔曼滤波（KF）的校准方法的一个优势在于，它不需要在使用前设置超参数，如KF中的传感器噪声矩阵。
- en: '![Refer to caption](img/cbd6c97a736d7f132bef1a34d0197877.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbd6c97a736d7f132bef1a34d0197877.png)'
- en: 'Figure 3: An example of gyro calibration results (reprint from Calib-Net [[31](#bib.bib31)]).
    Compared with raw IMU integration, deep learning based calibration models significantly
    reduce attitude drifts.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 陀螺仪校准结果示例（转载自 Calib-Net [[31](#bib.bib31)]）。与原始IMU积分相比，基于深度学习的校准模型显著减少了姿态漂移。'
- en: 'TABLE II: A summary of existing methods on deep learning based inertial positioning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 基于深度学习的惯性定位现有方法汇总。'
- en: '| name | Year | Carrier | model | learning | target |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| name | 年份 | 承载体 | 模型 | 学习 | 目标 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| IONet [[34](#bib.bib34)] | 2018 | Pedestrian, Trolley | LSTM | SL | location
    displacement |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| IONet [[34](#bib.bib34)] | 2018 | 行人, 手推车 | LSTM | SL | 位置位移 |'
- en: '| RIDI [[35](#bib.bib35)] | 2018 | Pedestrian | SVM, SVR | SL | velocity for
    inertial data calibration |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| RIDI [[35](#bib.bib35)] | 2018 | 行人 | SVM, SVR | SL | 用于惯性数据校准的速度 |'
- en: '| Cortes et al.[[36](#bib.bib36)] | 2018 | Pedestrian | ConvNet | SL | velocity
    to constrain system drifts |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Cortes et al.[[36](#bib.bib36)] | 2018 | 行人 | ConvNet | SL | 速度约束系统漂移 |'
- en: '| Wagstaff et al.[[37](#bib.bib37)] | 2018 | Pedestrian | LSTM | SL | zero-velocity
    detection for ZUPT |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Wagstaff et al.[[37](#bib.bib37)] | 2018 | 行人 | LSTM | SL | ZUPT的零速度检测 |'
- en: '| Chen et al.[[38](#bib.bib38)] | 2019 | Pedestrian, Trolley | LSTM | TL |
    location displacement |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al.[[38](#bib.bib38)] | 2019 | 行人, 手推车 | LSTM | TL | 位置位移 |'
- en: '| AbolDeepIO [[39](#bib.bib39)] | 2019 | UAV | LSTM | SL | location displacement
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| AbolDeepIO [[39](#bib.bib39)] | 2019 | UAV | LSTM | SL | 位置位移 |'
- en: '| RINS-W [[40](#bib.bib40)] | 2019 | Vehicle | RNN | SL | zero-velocity dection
    for KF |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| RINS-W [[40](#bib.bib40)] | 2019 | 车辆 | RNN | SL | KF的零速度检测 |'
- en: '| Feigl et al. [[41](#bib.bib41)] | 2019 | Pedestrian | LSTM | SL | walking
    velocity |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Feigl et al. [[41](#bib.bib41)] | 2019 | 行人 | LSTM | SL | 行走速度 |'
- en: '| Wang et al. [[42](#bib.bib42)] | 2019 | Pedestrian | LSTM | SL | walking
    heading for ZUPT |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[42](#bib.bib42)] | 2019 | 行人 | LSTM | SL | ZUPT的行走方向 |'
- en: '| Yu et al. [[43](#bib.bib43)] | 2019 | Pedestrian | ConvNet | SL | adaptive
    zero-velocity detection |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Yu et al. [[43](#bib.bib43)] | 2019 | 行人 | ConvNet | SL | 自适应零速度检测 |'
- en: '| TLIO [[44](#bib.bib44)] | 2020 | Pedestrian | ConvNet | SL | 3D displacement
    and uncertainty for EKF |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| TLIO [[44](#bib.bib44)] | 2020 | 行人 | ConvNet | SL | EKF的3D位移和不确定性 |'
- en: '| LIONet [[45](#bib.bib45)] | 2020 | Pedestrian | Dilated ConvNet | SL | lightweight
    inertial model |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LIONet [[45](#bib.bib45)] | 2020 | 行人 | 扩张卷积网络 | SL | 轻量级惯性模型 |'
- en: '| RoNIN[[46](#bib.bib46)] | 2020 | Pedestrian | LSTM, TCN | SL | velocity for
    inertial data calibration |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| RoNIN[[46](#bib.bib46)] | 2020 | 行人 | LSTM, TCN | SL | 用于惯性数据校准的速度 |'
- en: '| Brossard et al.[[47](#bib.bib47)] | 2020 | Vehicle | ConvNet | SL | co-variance
    noise for KF |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Brossard et al.[[47](#bib.bib47)] | 2020 | 车辆 | ConvNet | SL | KF的协方差噪声 |'
- en: '| StepNet[[48](#bib.bib48)] | 2020 | Pedestrian | ConvNet, LSTM | SL | dynamic
    step length for PDR |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| StepNet[[48](#bib.bib48)] | 2020 | 行人 | ConvNet, LSTM | SL | PDR的动态步长 |'
- en: '| Wang et al.[[49](#bib.bib49)] | 2020 | Pedestrian | ConvNet | SL | measurement
    noise for Kalman Filter |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al.[[49](#bib.bib49)] | 2020 | 行人 | ConvNet | SL | 卡尔曼滤波器的测量噪声 |'
- en: '| ARPDR [[50](#bib.bib50)] | 2020 | Pedestrian | TCN | SL | stride length and
    walking heading for PDR |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ARPDR [[50](#bib.bib50)] | 2020 | 行人 | TCN | SL | PDR的步幅长度和行走方向 |'
- en: '| IDOL[[51](#bib.bib51)] | 2021 | Pedestrian | LSTM | SL | device orientation
    and location |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| IDOL[[51](#bib.bib51)] | 2021 | 行人 | LSTM | SL | 设备方向和位置 |'
- en: '| PDRNet[[52](#bib.bib52)] | 2021 | Pedestrian | ConvNet | SL | step length
    and heading for PDR |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| PDRNet[[52](#bib.bib52)] | 2021 | 行人 | ConvNet | SL | PDR的步长和方向 |'
- en: '| Buchanan et al. [[53](#bib.bib53)] | 2021 | Legged Robot | ConvNet | SL |
    integrate location displacement with leg odometry |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Buchanan et al. [[53](#bib.bib53)] | 2021 | 四足机器人 | ConvNet | SL | 将位置位移与腿部里程计结合
    |'
- en: '| Zhang et al.[[54](#bib.bib54)] | 2021 | Vehicle, UAV | RNN | SL | independent
    motion terms |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al.[[54](#bib.bib54)] | 2021 | 车辆, UAV | RNN | SL | 独立运动项 |'
- en: '| Gong et al. [[55](#bib.bib55)] | 2021 | Pedestrian | LSTM | SL | fusing inertial
    data from two devices |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Gong et al. [[55](#bib.bib55)] | 2021 | 行人 | LSTM | SL | 融合来自两个设备的惯性数据 |'
- en: '| NILoc[[56](#bib.bib56)] | 2022 | Pedestrian | ConvNet | SL | inertial relocalization
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| NILoc[[56](#bib.bib56)] | 2022 | 行人 | ConvNet | SL | 惯性重新定位 |'
- en: '| RIO[[57](#bib.bib57)] | 2022 | Pedestrian | DNN | UL | rotation-equivariance
    as supervision signal |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| RIO[[57](#bib.bib57)] | 2022 | 行人 | DNN | UL | 旋转等变性作为监督信号 |'
- en: '| Wang et al. [[58](#bib.bib58)] | 2022 | Pedestrian | DNN | SL | efficient
    and low-latent model |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[58](#bib.bib58)] | 2022 | 行人 | DNN | SL | 高效且低延迟的模型 |'
- en: '| TinyOdom[[59](#bib.bib59)] | 2022 | Pedestrian, Vehicle | TCN+NAS | SL |
    deployment on resource-constrained device |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| TinyOdom[[59](#bib.bib59)] | 2022 | 行人，车辆 | TCN+NAS | SL | 部署在资源受限设备上 |'
- en: '| CTIN[[60](#bib.bib60)] | 2022 | Pedestrian | Transformer | SL | velocity
    and trajectory prediction |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CTIN[[60](#bib.bib60)] | 2022 | 行人 | Transformer | SL | 速度和轨迹预测 |'
- en: '| DeepVIP[[61](#bib.bib61)] | 2022 | Vehicle | ConvNet, LSTM | SL | velocity
    and heading for car localization |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DeepVIP[[61](#bib.bib61)] | 2022 | 车辆 | ConvNet, LSTM | SL | 车辆定位的速度和航向 |'
- en: '| Bo et al.[[62](#bib.bib62)] | 2022 | Pedestrian | ConvNet | TL | model-independent
    stride learning |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Bo et al.[[62](#bib.bib62)] | 2022 | 行人 | ConvNet | TL | 模型独立的步态学习 |'
- en: '| OdoNet[[63](#bib.bib63)] | 2022 | Vehicle | ConvNet | SL | speed learning
    for ZUPT |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| OdoNet[[63](#bib.bib63)] | 2022 | 车辆 | ConvNet | SL | ZUPT 的速度学习 |'
- en: '| A2DIO[[64](#bib.bib64)] | 2022 | Pedestrian | ConvNet, LSTM | SL | pose invariant
    odometry |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| A2DIO[[64](#bib.bib64)] | 2022 | 行人 | ConvNet, LSTM | SL | 姿态不变的里程计 |'
- en: '| LLIO [[65](#bib.bib65)] | 2022 | Pedestrian | MLP | SL | 3D displacement
    for lightweight odometry |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLIO [[65](#bib.bib65)] | 2022 | 行人 | MLP | SL | 轻量级里程计的 3D 位移 |'
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Year indicates the publication year of each work.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Year 指示每项工作的出版年份。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Carrier indicates the platform running inertial navigation.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Carrier 指示运行惯性导航的平台。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model indicates which module the framework consists of.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Model 指示框架由哪些模块组成。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning indicates how to train neural networks. SL, TL and UL represent Supervised
    Learning, Transfer Learning and Unsupervised Learning.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Learning 指示如何训练神经网络。SL、TL 和 UL 分别表示监督学习、迁移学习和无监督学习。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Target indicates what the model aims to solve or produce.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Target 指示模型旨在解决或生成的内容。
- en: 'In recent years, there has been increasing interest in using deep neural networks
    (DNN) with multiple layers to solve the inertial sensor calibration problem. With
    the addition of more layers, neural networks become more expressive and can learn
    complex relationships between the raw inertial measurements and the true motion
    of the vehicle. One approach, proposed by [[25](#bib.bib25)], uses a Convolutional
    Neural Network (ConvNet) to remove error noises from inertial measurements. They
    collected inertial data from two grades of IMU under given constant accelerations
    and angular rates. The ConvNet framework takes raw inertial measurements (from
    low-precision IMU) as inputs and tries to output acceleration and angular rate
    references (from high-precision IMU). Their experiment shows that deep learning
    can remove some of the sensor error and improve test accuracy. However, this work
    has not been validated in a real navigation setup, and thus it cannot demonstrate
    how learning-based sensor calibration reduces error drifts in inertial navigation.
    Both of the mentioned methods require reference data from high-precision IMUs
    as labels to train the networks, as shown in Figure [2](#S2.F2 "Figure 2 ‣ II-E
    Discussion ‣ II Classical Inertial Navigation Mechanisms ‣ Deep Learning for Inertial
    Positioning: A Survey") (a). However, acquiring reference data from high-precision
    IMUs can be costly.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，使用具有多层的深度神经网络（DNN）解决惯性传感器标定问题的兴趣日益增加。随着层数的增加，神经网络变得更加有表现力，可以学习原始惯性测量值与车辆真实运动之间的复杂关系。一个方法，[[25](#bib.bib25)]
    提出的，使用卷积神经网络（ConvNet）从惯性测量中去除误差噪声。他们在给定的恒定加速度和角速度下，收集了两种等级IMU的惯性数据。ConvNet框架以原始惯性测量值（来自低精度IMU）为输入，尝试输出加速度和角速度参考值（来自高精度IMU）。他们的实验表明，深度学习可以去除一些传感器误差并提高测试精度。然而，这项工作尚未在实际导航设置中验证，因此不能展示基于学习的传感器标定如何减少惯性导航中的误差漂移。上述方法都需要高精度IMU的参考数据作为标签来训练网络，如图[2](#S2.F2
    "Figure 2 ‣ II-E Discussion ‣ II Classical Inertial Navigation Mechanisms ‣ Deep
    Learning for Inertial Positioning: A Survey") (a)所示。然而，从高精度IMU获取参考数据可能是昂贵的。'
- en: 'In addition to directly learning from pseudo ground-truth IMU labels, another
    approach is to enable neural network-based calibration models to produce inertial
    data that can be integrated into more accurate orientation estimation. This is
    illustrated in Figure [2](#S2.F2 "Figure 2 ‣ II-E Discussion ‣ II Classical Inertial
    Navigation Mechanisms ‣ Deep Learning for Inertial Positioning: A Survey") (b).
    By producing more accurate orientation values, the neural network implicitly removes
    the corrupted noises above inertial data. For example, OriNet [[26](#bib.bib26)]
    inputs 3-dimensional gyroscope signals into an LSTM network [[66](#bib.bib66)]
    to obtain calibrated gyroscope signals, which are then integrated with the orientation
    at the previous timestep to generate orientation estimates at the current timestep.
    A loss function between orientation estimates and real orientation is defined
    and minimized for model training. OriNet has been evaluated on a public drone
    dataset, demonstrating an improvement in orientation performance of approximately
    80%. A similar approach is [[28](#bib.bib28)], who calibrates gyroscope using
    ConvNet, reporting good attitude estimation accuracy. Calib-Net [[31](#bib.bib31)]
    is another ConvNet framework that denoises gyroscope data by extracting effective
    spatio-temporal features from inertial data. Calib-Net is based on dilation ConvNet
    [[67](#bib.bib67)] to compensate the gyro noise, as illustrated in Figure [3](#S3.F3
    "Figure 3 ‣ III Deep Learning Based Inertial Sensor Calibration ‣ Deep Learning
    for Inertial Positioning: A Survey"). This model is able to significantly reduce
    orientation error compared to raw IMU integration. When this learned inertial
    calibration model is incorporated into a visual-inertial odometry (VIO), it further
    improves localization performance and outperforms representative VIOs such as
    VINS-mono [[6](#bib.bib6)]. Other efforts in this direction include works by [[29](#bib.bib29),
    [30](#bib.bib30)].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接从伪地面真值IMU标签中学习外，另一种方法是使基于神经网络的校准模型生成可集成到更准确方向估计中的惯性数据。这在图[2](#S2.F2 "图 2
    ‣ II-E 讨论 ‣ II 经典惯性导航机制 ‣ 深度学习在惯性定位中的应用：综述") (b)中有所说明。通过生成更准确的方向值，神经网络隐式地去除了惯性数据上的噪声。例如，OriNet
    [[26](#bib.bib26)] 将三维陀螺仪信号输入到LSTM网络[[66](#bib.bib66)]中以获得校准后的陀螺仪信号，然后将其与前一时间步的方向进行集成，以生成当前时间步的方向估计。定义并最小化方向估计与实际方向之间的损失函数以进行模型训练。OriNet在公开的无人机数据集上进行了评估，显示出约80%的方向性能提升。类似的方法包括[[28](#bib.bib28)]，他们使用ConvNet对陀螺仪进行校准，报告了良好的姿态估计精度。Calib-Net
    [[31](#bib.bib31)] 是另一种ConvNet框架，通过从惯性数据中提取有效的时空特征来去噪陀螺仪数据。Calib-Net基于膨胀ConvNet[[67](#bib.bib67)]以补偿陀螺仪噪声，如图[3](#S3.F3
    "图 3 ‣ III 基于深度学习的惯性传感器校准 ‣ 深度学习在惯性定位中的应用：综述")所示。与原始IMU积分相比，这种模型能够显著减少方向误差。当这个学习到的惯性校准模型被纳入到视觉惯性里程计（VIO）中时，它进一步提升了定位性能，并优于代表性VIO如VINS-mono
    [[6](#bib.bib6)]。在这个方向上的其他努力包括[[29](#bib.bib29), [30](#bib.bib30)]的工作。
- en: 'Instead of directly calibrating inertial sensors with DNNs, some researchers
    have explored using DNNs to generate parameters that improve classical calibration
    algorithms, as shown in Figure [2](#S2.F2 "Figure 2 ‣ II-E Discussion ‣ II Classical
    Inertial Navigation Mechanisms ‣ Deep Learning for Inertial Positioning: A Survey")
    (c). One example is the work by [[27](#bib.bib27)], who models inertial sensor
    calibration as a Markov Decision Process and proposes to use deep reinforcement
    learning [[68](#bib.bib68)] to learn the optimal calibration parameters. The authors
    demonstrated the effectiveness of their approach in calibrating inertial sensors
    for a visual-inertial odometry (VIO) system.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员探索了使用DNN生成改进经典校准算法的参数，而不是直接用DNN校准惯性传感器，如图[2](#S2.F2 "图 2 ‣ II-E 讨论 ‣ II
    经典惯性导航机制 ‣ 深度学习在惯性定位中的应用：综述") (c)所示。一个例子是[[27](#bib.bib27)]的工作，他们将惯性传感器校准建模为马尔可夫决策过程，并提出使用深度强化学习[[68](#bib.bib68)]来学习最佳校准参数。作者展示了他们的方法在为视觉惯性里程计（VIO）系统校准惯性传感器方面的有效性。
- en: '![Refer to caption](img/a39a2ea86b1036781e9de2bdd5a44cbb.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a39a2ea86b1036781e9de2bdd5a44cbb.png)'
- en: 'Figure 4: The velocity of attached platform can be inferred from a sequence
    of inertial measurements via deep neural networks. (reprint from L-IONet [[45](#bib.bib45)])'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：通过深度神经网络可以从一系列惯性测量中推断附加平台的速度。 (摘自L-IONet [[45](#bib.bib45)])
- en: '![Refer to caption](img/63b76f8cedcb1b8957f320d8faf3eb13.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/63b76f8cedcb1b8957f320d8faf3eb13.png)'
- en: 'Figure 5: An overview of existing methods on learning to correct IMU integration'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：现有 IMU 集成校正学习方法的概述
- en: 'As discussed above, deep learning-aided inertial sensor calibration methods
    (listed in Table [I](#S2.T1 "TABLE I ‣ II-D Integrating IMU with Other Sensors
    ‣ II Classical Inertial Navigation Mechanisms ‣ Deep Learning for Inertial Positioning:
    A Survey")) have shown promising results in removing corrupted sensor noises and
    improving the accuracy of inertial positioning systems. These methods do not require
    human intervention and can automatically learn error models. However, it is important
    to note that the learned error model is typically dependent on the specific sensor
    or platform used. Therefore, a change in sensor or user can result in different
    data distributions, leading to reduced performance of the learned model. Additionally,
    further analysis is needed to determine which types of noise can be effectively
    removed by learning-based calibration methods.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '如上所述，深度学习辅助的惯性传感器校准方法（见表 [I](#S2.T1 "TABLE I ‣ II-D Integrating IMU with Other
    Sensors ‣ II Classical Inertial Navigation Mechanisms ‣ Deep Learning for Inertial
    Positioning: A Survey")）在去除传感器噪声和提高惯性定位系统准确性方面显示了良好的前景。这些方法不需要人工干预，并且可以自动学习误差模型。然而，需要注意的是，学习到的误差模型通常依赖于特定的传感器或平台。因此，传感器或用户的变化可能会导致数据分布的不同，从而降低学习模型的性能。此外，还需进一步分析以确定哪些类型的噪声可以通过学习型校准方法有效去除。'
- en: IV Learning to correct IMU Integration
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 学习校正 IMU 集成
- en: 'In addition to sensor calibration, researchers are exploring various methods
    for using deep learning to construct inertial positioning models that can either
    partially or completely replace classical inertial navigation mechanisms. This
    section provides an overview of how deep learning can be used to correct IMU integration
    in general. Next sections will discuss deep learning approaches for pedestrian
    tracking applications, and present deep inertial solutions for vehicles, UAVs,
    and robots. A summary of existing works and their contributions is provided in
    Table [II](#S3.T2 "TABLE II ‣ III Deep Learning Based Inertial Sensor Calibration
    ‣ Deep Learning for Inertial Positioning: A Survey").'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '除了传感器校准外，研究人员还在探索各种使用深度学习构建惯性定位模型的方法，这些模型可以部分或完全替代经典的惯性导航机制。本节概述了深度学习如何用于一般性的
    IMU 集成校正。接下来的章节将讨论用于行人跟踪应用的深度学习方法，并展示用于车辆、无人机和机器人深度惯性解决方案。表 [II](#S3.T2 "TABLE
    II ‣ III Deep Learning Based Inertial Sensor Calibration ‣ Deep Learning for Inertial
    Positioning: A Survey") 提供了现有工作及其贡献的总结。'
- en: 'In deep learning-based inertial positioning approaches, a user’s absolute velocity
    can be inferred from a sequence of IMU data using a deep neural network. This
    velocity information can then be used as a key constraint to reduce the drifts
    in IMU double integration. Figure [4](#S3.F4 "Figure 4 ‣ III Deep Learning Based
    Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning: A Survey")
    provides an example of velocity learning from IMU sequence, where the periodicity
    of human walking makes it easy to infer the user’s moving velocity. Similar observations
    have been made for vehicles, UAVs, and robotic platforms, which will be discussed
    in Section [VI](#S6 "VI Learning to Correct Inertial Positioning on Vehicles,
    UAV and robotic platforms ‣ Deep Learning for Inertial Positioning: A Survey").
    Existing works on applying learned velocity to correct IMU integration can generally
    be divided into three categories, as shown in Figure [5](#S3.F5 "Figure 5 ‣ III
    Deep Learning Based Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning:
    A Survey"), and will be discussed as follows.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '在基于深度学习的惯性定位方法中，用户的绝对速度可以通过深度神经网络从 IMU 数据序列中推断出来。然后，这些速度信息可以作为关键约束，以减少 IMU
    双重积分中的漂移。图 [4](#S3.F4 "Figure 4 ‣ III Deep Learning Based Inertial Sensor Calibration
    ‣ Deep Learning for Inertial Positioning: A Survey") 提供了从 IMU 序列中学习速度的示例，其中人类步态的周期性使得推断用户的移动速度变得容易。对于车辆、无人机和机器人平台也有类似的观察，这将在第
    [VI](#S6 "VI Learning to Correct Inertial Positioning on Vehicles, UAV and robotic
    platforms ‣ Deep Learning for Inertial Positioning: A Survey") 节中讨论。现有的将学习到的速度应用于
    IMU 集成校正的工作通常可以分为三类，如图 [5](#S3.F5 "Figure 5 ‣ III Deep Learning Based Inertial
    Sensor Calibration ‣ Deep Learning for Inertial Positioning: A Survey") 所示，下面将进行讨论。'
- en: 'One category of deep learning models aims to learn location displacement, which
    is the average velocity multiplied by a fixed period of time, as illustrated in
    Figure [5](#S3.F5 "Figure 5 ‣ III Deep Learning Based Inertial Sensor Calibration
    ‣ Deep Learning for Inertial Positioning: A Survey")(a). The approach proposed
    by [[34](#bib.bib34)] formulates inertial positioning as a sequential learning
    problem, where 2D motion displacements in the polar coordinate, also known as
    polar vectors, are learned from independent windows of segmented inertial data.
    This is because the frequency of platform vibrations is relevant to the absolute
    moving speed, which can be measured by IMU, when tracking human or wheeled configurations.
    Based on this observation, they propose IONet, an LSTM-based framework for end-to-end
    learning of relative poses. Trajectories are generated by adding motion displacements
    together with initial locations. To train neural models, a large collection of
    data was collected from a smartphone-based IMU in a room with a high-precision
    visual motion tracking system (i.e., Vicon) to provide ground-truth pose labels.
    Once the model is trained, the IONet model can be used in areas outside the data-collection
    room. In a two-minute random pedestrian walking scenario, the localization error
    of IONet is within 3 meters 90% of the time, when evaluating across users, devices,
    and attachments, outperforming some classical PDR algorithms. In tracking trolley,
    IONet shows comparable performance over representative visual-inertial odometry
    and is even more robust in featureless areas. However, supervised learning-based
    IONet requires high-precision pose as training labels. When testing with data
    different from those in the training set, there will be performance degradation.
    To improve the generalization ability, [[38](#bib.bib38)] proposes MotionTransformer,
    which allows the inertial positioning model to self-adapt into new domains via
    generative adversarial network (GAN) [[69](#bib.bib69)] and domain adaptation
    [[70](#bib.bib70)], without the need for labels in new domains. To encourage more
    reliable inertial positioning, [[71](#bib.bib71)] is able to produce pose uncertainties
    along with poses, offering the belief in the extent to which the learned pose
    can be trusted. To allow full 3D localization, TLIO [[44](#bib.bib44)] proposes
    to learn 3D location displacements and covariances from a sequence of gravity-aligned
    inertial data. To avoid the impacts from initial orientation, the inertial data
    are transformed into a local gravity-aligned frame. The learned displacements
    and covariances are then incorporated into an extended Kalman filter as observation
    states that estimate full-states of orientation, velocity, location, and IMU bias.
    In a 3-7 minute human motion scenario, the localization error of TLIO is within
    3 meters 90% of the time.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习模型的一个类别旨在学习位置位移，即平均速度乘以固定时间段，如图[5](#S3.F5 "Figure 5 ‣ III Deep Learning
    Based Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning: A
    Survey")(a)所示。[[34](#bib.bib34)]提出的方法将惯性定位形式化为一个序列学习问题，其中极坐标中的二维运动位移，即极向量，从独立的惯性数据段窗口中学习。这是因为平台振动的频率与绝对移动速度相关，而绝对移动速度可以通过IMU测量，特别是在跟踪人类或轮式配置时。基于这一观察，他们提出了IONet，一个基于LSTM的框架，用于端到端学习相对姿态。通过将运动位移与初始位置相加来生成轨迹。为了训练神经模型，收集了大量数据，这些数据来自一个基于智能手机的IMU，并在一个具有高精度视觉运动跟踪系统（即Vicon）的房间内提供真实姿态标签。一旦模型训练完成，IONet模型可以用于数据收集房间之外的领域。在一个两分钟的随机行人步态场景中，当在用户、设备和附件之间评估时，IONet的定位误差90%的时间在3米以内，超越了一些经典的PDR算法。在跟踪手推车时，IONet显示出与代表性的视觉惯性里程计相当的性能，并且在无特征区域中更加稳健。然而，基于监督学习的IONet需要高精度的姿态作为训练标签。当测试数据与训练集中的数据不同时时，会出现性能下降。为了提高泛化能力，[[38](#bib.bib38)]提出了MotionTransformer，它允许惯性定位模型通过生成对抗网络（GAN）[[69](#bib.bib69)]和领域适应[[70](#bib.bib70)]自适应到新领域，无需新领域中的标签。为了鼓励更可靠的惯性定位，[[71](#bib.bib71)]能够生成姿态的不确定性，提供对学习到的姿态可以信任程度的信念。为了实现完全的三维定位，TLIO
    [[44](#bib.bib44)]建议从一系列与重力对齐的惯性数据中学习三维位置位移和协方差。为了避免初始方向的影响，惯性数据被转换到局部重力对齐框架中。然后，将学习到的位移和协方差纳入扩展卡尔曼滤波器作为观测状态，估计方向、速度、位置和IMU偏差的全状态。在一个3-7分钟的人体运动场景中，TLIO的定位误差90%的时间在3米以内。'
- en: 'Another category of deep learning models aims to leverage learned velocity
    to correct accelerations, as illustrated in Figure [5](#S3.F5 "Figure 5 ‣ III
    Deep Learning Based Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning:
    A Survey")(b). A prominent example is RIDI [[35](#bib.bib35)], which trains a
    deep neural network to predict velocity vectors from inertial data, which are
    then used to correct linear accelerations by subtracting gravity, aligning with
    the constraints of learned velocities. The corrected linear accelerations are
    then doubly integrated to estimate positions. To enhance the accuracy of inertial
    accelerations, RIDI leverages human walking speed as a prior, which compensates
    for the drifts in inertial positioning, effectively constraining them to a lower
    level. RoNIN [[46](#bib.bib46)] improves upon RIDI by transforming inertial measurements
    and learned velocity vectors into a heading-agnostic coordinate frame and introducing
    several novel velocity losses. To minimize the impact of orientation estimation,
    RoNIN employs device orientation to transform inertial data into a frame with
    its Z-axis aligned with gravity. However, a limitation of RoNIN is its reliance
    on orientation estimation. NILoc [[56](#bib.bib56)] is an intriguing trial based
    on RoNIN, which tackles the neural inertial localization problem, aiming to infer
    global location from inertial motion history only. This work recognizes that human
    motion patterns are unique in different locations, which can be utilized as a
    ”fingerprint” to determine the location, similar to WiFi or magnetic-field fingerprinting.
    NILoc first calculates a sequence of velocity from inertial data and then employs
    a Transformer-based DNN framework [[72](#bib.bib72)] to transform the velocity
    sequence into location. However, one fundamental limitation of NILoc is that in
    some areas, such as open spaces, symmetrical or repetitive places, there may not
    be a unique motion pattern.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '另一类深度学习模型旨在利用学习到的速度来修正加速度，如图[5](#S3.F5 "Figure 5 ‣ III Deep Learning Based
    Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning: A Survey")(b)所示。一个突出的例子是RIDI
    [[35](#bib.bib35)]，该模型训练深度神经网络从惯性数据中预测速度矢量，然后用这些速度矢量通过减去重力来修正线性加速度，从而与学习到的速度约束对齐。修正后的线性加速度随后经过二重积分以估计位置。为了提高惯性加速度的准确性，RIDI利用人类步速作为先验，以补偿惯性定位中的漂移，有效地将其约束在较低水平。RoNIN
    [[46](#bib.bib46)]通过将惯性测量和学习到的速度矢量转换为一个与航向无关的坐标框架，并引入若干新颖的速度损失，改进了RIDI。为了最小化方向估计的影响，RoNIN利用设备方向将惯性数据转换为其Z轴与重力对齐的框架。然而，RoNIN的一个限制是它依赖于方向估计。NILoc
    [[56](#bib.bib56)]是基于RoNIN的一个有趣尝试，旨在解决神经惯性定位问题，目标是仅通过惯性运动历史推断全球位置。这项工作认识到，人类运动模式在不同地点是独特的，可以作为“指纹”来确定位置，类似于WiFi或磁场指纹。NILoc首先从惯性数据中计算出速度序列，然后使用基于Transformer的DNN框架
    [[72](#bib.bib72)]将速度序列转换为位置。然而，NILoc的一个基本限制是，在某些区域，如开放空间、对称或重复的地方，可能不存在独特的运动模式。'
- en: 'An alternative approach involves incorporating learned velocity into the updating
    process of a Kalman filter (KF), as shown in Figure [5](#S3.F5 "Figure 5 ‣ III
    Deep Learning Based Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning:
    A Survey") (c). [[36](#bib.bib36)] uses a ConvNet to infer current speed from
    IMU sequences and incorporates this speed into the Kalman filter as a velocity
    observation to constrain the drifts of SINS-based inertial positioning. This approach
    is similar to the zero-velocity update (ZUPT) method, which detects and uses zero-velocity
    in KF as observations, but instead uses full speeds as observations in KF. Incorporating
    learned velocity allows the KF to handle more complex human motion. A similar
    trial is [[49](#bib.bib49)], that is based on a DNN that infers walking velocity
    in the body frame and combines it with an extended KF. In addition to the learned
    velocity, [[49](#bib.bib49)] produces a noise parameter for KF to dynamically
    update parameters, rather than setting a fixed noise parameter.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种方法是将学习得到的速度融入卡尔曼滤波器（KF）的更新过程中，如图 [5](#S3.F5 "Figure 5 ‣ III Deep Learning
    Based Inertial Sensor Calibration ‣ Deep Learning for Inertial Positioning: A
    Survey") (c) 所示。[[36](#bib.bib36)] 使用卷积网络（ConvNet）从IMU序列推断当前速度，并将此速度作为速度观测值融入卡尔曼滤波器，以约束基于SINS的惯性定位的漂移。这种方法类似于零速度更新（ZUPT）方法，该方法检测并利用卡尔曼滤波器中的零速度作为观测值，但在KF中使用完整速度作为观测值。融入学习得到的速度可以使KF处理更复杂的人体运动。类似的试验是
    [[49](#bib.bib49)]，其基于DNN推断身体框架中的步行速度，并将其与扩展KF结合。在学习得到的速度之外，[[49](#bib.bib49)]
    为KF生成噪声参数，以动态更新参数，而不是设置固定的噪声参数。'
- en: Inertial positioning heavily relies on accurately estimating the device’s attitude.
    Several methods aim to improve orientation estimation to enhance the performance
    of deep learning based inertial odometry. RIDI, RoNIN, and TLIO still depend on
    device orientation to rotate inertial data into a suitable frame. To address this
    problem, IDOL [[51](#bib.bib51)] proposes a two-stage process that first learns
    orientation from data and then rotates inertial data into the appropriate frame,
    followed by learning the position. [[58](#bib.bib58)] estimates orientation using
    magnetic data and combines it with learned odometry to reduce positioning drifts
    while minimizing reliance on device orientation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性定位在很大程度上依赖于准确估计设备的姿态。一些方法旨在改进方向估计，以增强基于深度学习的惯性测距的性能。RIDi、RoNIN和TLIO仍然依赖于设备方向将惯性数据旋转到适当的框架中。为了解决这个问题，IDOL
    [[51](#bib.bib51)] 提出了一个两阶段的过程，首先从数据中学习方向，然后将惯性数据旋转到适当的框架中，接着学习位置。[[58](#bib.bib58)]
    使用磁数据估计方向，并将其与学习到的测距结合，以减少定位漂移，同时最小化对设备方向的依赖。
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ IV Learning to correct IMU Integration ‣ Deep
    Learning for Inertial Positioning: A Survey") showcases several examples of deep
    learning based inertial positioning results.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#S4.F6 "Figure 6 ‣ IV Learning to correct IMU Integration ‣ Deep Learning
    for Inertial Positioning: A Survey") 展示了几种基于深度学习的惯性定位结果示例。'
- en: '![Refer to caption](img/97693b4300fb0d29c2e5ba2e1f9e3d41.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/97693b4300fb0d29c2e5ba2e1f9e3d41.png)'
- en: 'Figure 6: Sample results of deep learning based inertial positioning from (a)
    VR device for pedestrian tracking (reprint from TLIO [[44](#bib.bib44)]) (b) smartphone
    for trolly tracking (reprint from IONet [[34](#bib.bib34)])'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于深度学习的惯性定位样本结果（a）来自VR设备的行人跟踪（转载自 TLIO [[44](#bib.bib44)]） (b) 来自智能手机的推车跟踪（转载自
    IONet [[34](#bib.bib34)]）
- en: V Learning to Correct Pedestrian Inertial Positioning
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 校正行人惯性定位的学习
- en: The previous subsection addressed the general application of deep learning in
    correcting inertial positioning drifts. This subsection focuses on the specific
    use of deep learning to address particular aspects of pedestrian navigation algorithms,
    namely Pedestrian Dead Reckoning (PDR) and Zero-Velocity Update (ZUPT).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节讨论了深度学习在校正惯性定位漂移中的一般应用。本节重点关注深度学习在解决行人导航算法中特定方面的应用，即行人死记法（PDR）和零速度更新（ZUPT）。
- en: V-A Learning to correct PDR
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 校正PDR的学习
- en: Pedestrian dead reckoning (PDR) error drifts often stem from inaccurate stride
    and heading estimates. To address these issues, researchers have incorporated
    deep learning techniques into the process of step detection, dynamic step length
    estimation, and walking heading estimation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 行人死记法（PDR）误差漂移通常源于不准确的步幅和方向估计。为了应对这些问题，研究人员将深度学习技术融入了步态检测、动态步长估计和步态方向估计的过程。
- en: To estimate walking stride more robustly, researchers have sought to solve it
    in a data-driven way. One such method is SmartStep [[73](#bib.bib73)], a deep
    learning-based step detection framework that achieves 99% accuracy in step detection
    tasks across various motion modes. Compared to peak/valley detection-based methods,
    data-driven methods do not require IMUs to be fixed in position, specific motion
    modes, or pre-calibration and threshold setting. Another approach involves using
    LSTM to regress walking stride from raw inertial data [[41](#bib.bib41)]. This
    method has demonstrated effectiveness in various human motions, such as walking,
    running, jogging, and random movements. Additionally, StepNet [[48](#bib.bib48)]
    learns to estimate step length dynamically, i.e., the change in distance, which
    achieves an impressive performance with only a 2.1%-3.2% error rate when compared
    to traditional static step length estimation. The attachment mode of the device,
    such as in hand or in pocket, can also influence walking stride estimation. To
    address this problem, Bo et al. [[62](#bib.bib62)] employed domain adaptation
    [[70](#bib.bib70)] to extract domain-invariant features for stride estimation,
    which enhanced the performance in new domains, such as holding, calling, pocket,
    and swinging.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更稳健地估计步幅，研究人员寻求通过数据驱动的方法来解决这个问题。其中一种方法是 SmartStep [[73](#bib.bib73)]，这是一种基于深度学习的步态检测框架，在各种运动模式下的步态检测任务中达到了
    99% 的准确率。与基于峰值/谷值检测的方法相比，数据驱动的方法不需要 IMU 固定位置、特定运动模式或预校准和阈值设置。另一种方法涉及使用 LSTM 从原始惯性数据中回归步幅
    [[41](#bib.bib41)]。这种方法在各种人体运动中显示了有效性，例如步行、跑步、慢跑和随机运动。此外，StepNet [[48](#bib.bib48)]
    学习动态估计步长，即距离的变化，在与传统静态步长估计相比时，实现了令人印象深刻的表现，误差率仅为 2.1%-3.2%。设备的附着模式，如手持或放在口袋中，也会影响步幅估计。为了解决这个问题，Bo
    等人 [[62](#bib.bib62)] 采用了领域适应 [[70](#bib.bib70)] 来提取领域不变特征进行步幅估计，从而在新领域（如持物、通话、口袋和摆动）中提高了性能。
- en: Accurate heading estimation is crucial for updating position in the right direction
    in PDR. To achieve more accurate and robust heading estimation, Wang et al. [[42](#bib.bib42)]
    utilize a Spatial Transformer Network [[74](#bib.bib74)] and LSTM to learn heading
    direction from the inertial sensor attached to an unconstrained device. However,
    one problem that arises is the misalignment between the device heading and pedestrian
    heading, making it difficult to estimate the real walking heading based on sensor
    data. To address this misalignment issue, [[75](#bib.bib75)] introduces a deep
    neural network to estimate walking direction in the sensor’s frame. They derive
    a geometric model to convert walking direction from the sensor’s frame into a
    reference frame (i.e., north and east coordinates) by exploiting acceleration
    and magnetic data. This geometric model is combined with a learning framework
    to produce heading estimates. When tested on unseen data, this work reports a
    median heading error of 10°. PDRNet [[52](#bib.bib52)] follows the process of
    a traditional PDR algorithm but replaces the step length and heading estimation
    modules with deep neural networks. Their experiments indicate that learning step
    length and heading together outperforms regressing them separately.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 准确的方向估计对于在 PDR 中沿正确方向更新位置至关重要。为了实现更准确和稳健的方向估计，Wang 等人 [[42](#bib.bib42)] 利用空间变换网络
    [[74](#bib.bib74)] 和 LSTM 从附加到不受约束设备的惯性传感器中学习方向。然而，出现的问题是设备方向与行人方向之间的错位，使得基于传感器数据估计真实的步行方向变得困难。为了解决这个错位问题，[[75](#bib.bib75)]
    引入了深度神经网络来估计传感器框架中的行走方向。他们推导出一个几何模型，通过利用加速度和磁数据，将传感器框架中的行走方向转换为参考框架（即北方和东方坐标）。这个几何模型与学习框架结合以产生方向估计。当在未见过的数据上进行测试时，这项工作报告了
    10° 的中位方向误差。PDRNet [[52](#bib.bib52)] 遵循传统 PDR 算法的过程，但用深度神经网络替代了步长和方向估计模块。他们的实验表明，学习步长和方向一起优于单独回归它们。
- en: V-B Learning to correct ZUPT
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 学习纠正 ZUPT
- en: In pedestrian inertial navigation systems (INS) based on zero-velocity update
    (ZUPT), the zero-velocity phase is utilized to correct inertial positioning errors
    through Kalman filtering. Therefore, the accuracy of zero-velocity detection is
    crucial in determining when to update the system states. However, traditional
    threshold-based zero-velocity detection is complicated by the mixed variety of
    motions experienced by humans, making it challenging to set a reliable threshold
    when the user is still.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于零速度更新（ZUPT）的行人惯性导航系统（INS）中，零速度阶段被用来通过卡尔曼滤波修正惯性定位误差。因此，零速度检测的准确性对于确定何时更新系统状态至关重要。然而，传统的基于阈值的零速度检测受到人类经历的各种混合运动的影响，使得在用户静止时设置可靠的阈值具有挑战性。
- en: To address this issue, researchers have explored data-driven approaches that
    utilize the powerful feature extraction and classification capabilities of deep
    learning to classify whether the user is in the ZUPT phase. For instance, [[37](#bib.bib37)]
    proposes a six-layer long short-term memory (LSTM) network to detect zero-velocity.
    The LSTM inputs a sequence of IMU data, typically 100 consecutive data points,
    and outputs the probability of whether the user is still or in motion at the current
    timestep. The results from the LSTM-based zero-velocity detection are then fed
    into a ZUPT-based INS. The proposed approach achieves a reduction in localization
    error by over 34% compared to fixed threshold-based ZVDs and was shown to be more
    robust during a mixed variety of motions, such as walking, running, and climbing
    stairs. Similarly, [[43](#bib.bib43)] designs an adaptive ZUPT using convolutional
    neural networks (ConvNet) to classify ZVDs based on IMU sequences. Deep learning
    approaches, such as LSTM and ConvNet, have demonstrated excellent performance
    in extracting robust and useful features for zero-velocity identification, irrespective
    of different users, motion modes, and attachment places.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，研究人员探索了数据驱动的方法，利用深度学习强大的特征提取和分类能力来分类用户是否处于 ZUPT 阶段。例如，[[37](#bib.bib37)]
    提出了一个六层长短期记忆（LSTM）网络来检测零速度。LSTM 输入 IMU 数据序列，通常是 100 个连续的数据点，并输出当前时间步用户是否静止或运动的概率。LSTM
    基于零速度检测的结果随后输入 ZUPT 基础的 INS。该方法与基于固定阈值的 ZVD 相比，实现了超过 34% 的定位误差减少，并且在步行、跑步和爬楼梯等混合运动中表现出更强的鲁棒性。类似地，[[43](#bib.bib43)]
    设计了一种使用卷积神经网络（ConvNet）的自适应 ZUPT，根据 IMU 序列分类 ZVD。深度学习方法，如 LSTM 和 ConvNet，在零速度识别中展现了卓越的性能，无论是不同用户、运动模式还是附件位置。
- en: VI Learning to Correct Inertial Positioning on Vehicles, UAV and robotic platforms
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 学习在车辆、无人机和机器人平台上修正惯性定位
- en: As previously mentioned, deep learning methods have shown great potential in
    addressing the challenges of pedestrian inertial navigation. However, these techniques
    can also be applied to other platforms, such as vehicles, UAVs, robots, and more.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习方法在解决行人惯性导航的挑战方面展现了巨大潜力。然而，这些技术也可以应用于其他平台，如车辆、无人机、机器人等。
- en: These platforms share similarities with pedestrians, such as the ability to
    infer movement velocity from inertial data. This is because inertial data contains
    vibration information that reflects the fundamental frequency proportional to
    the vehicle speed. Building on the success of IONet [[34](#bib.bib34)], [[39](#bib.bib39)]
    proposes AbolDeepIO, an improved triple-channel LSTM network that predicts polar
    vectors for drone localization from inertial data sequences. AbolDeepIO has been
    evaluated on a public drone dataset and has shown competitive performance compared
    to traditional visual-inertial odometry methods like VINS-mono.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些平台与行人类似，比如能够从惯性数据推断运动速度。这是因为惯性数据包含的振动信息反映了与车辆速度成正比的基本频率。在 IONet 的成功基础上，[[34](#bib.bib34)]
    提出了 AbolDeepIO，这是一种改进的三通道 LSTM 网络，用于从惯性数据序列中预测无人机定位的极坐标向量。AbolDeepIO 已在一个公共无人机数据集上进行了评估，并显示出相较于传统的视觉-惯性里程计方法，如
    VINS-mono，具有竞争力的表现。
- en: When deploying deep learning-based inertial navigation on real-world devices,
    prediction accuracy and model efficiency must be considered. To address this,
    TinyOdom [[59](#bib.bib59)] aims to deploy neural inertial odometry models on
    resource-constrained devices. It proposes a lightweight model based on temporal
    convolutional networks (TCN) [[76](#bib.bib76)] to learn position displacement
    and optimizes the model through neural architecture search (NAS) [[77](#bib.bib77)]
    to reduce model size between 31 and 134 times. TinyOdom was extensively evaluated
    on tracking pedestrians, animals, aerial, and underwater vehicles. Within 60 seconds,
    its localization error is between 2.5 and 12 meters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际设备上部署基于深度学习的惯性导航时，必须考虑预测精度和模型效率。为此，TinyOdom [[59](#bib.bib59)] 旨在将神经惯性里程计模型部署到资源受限的设备上。它提出了一种基于时间卷积网络（TCN）[[76](#bib.bib76)]
    的轻量级模型，用于学习位置位移，并通过神经架构搜索（NAS）[[77](#bib.bib77)] 优化模型，以将模型尺寸减少31至134倍。TinyOdom
    在跟踪行人、动物、空中和水下车辆方面进行了广泛评估。在60秒内，其定位误差在2.5至12米之间。
- en: Learning-based inertial odometry has also been extended to legged robots by
    [[53](#bib.bib53)]. The learned location displacement is combined with kinematic
    motion models to estimate robot system states at high frequencies (400 Hz). In
    this work, the robot successfully navigated a field experiment, where a legged
    robot walked around for 20 minutes in a mine with poor illumination and visual
    feature tracking failures.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的惯性里程计也已经扩展到有腿机器人[[53](#bib.bib53)]。学习到的位置位移与运动学模型结合，以高频率（400 Hz）估计机器人系统状态。在这项工作中，机器人成功地进行了场地实验，在一个光线较差且视觉特征跟踪失败的矿区中，腿式机器人走了20分钟。
- en: 'TABLE III: A summary of existing methods on deep learning based sensor fusion.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：关于深度学习传感器融合的现有方法的总结。
- en: '| name | year | sensor | model | learning | target |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| name | year | sensor | model | learning | target |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| VINet[[78](#bib.bib78)] | 2017 | MC+I | ConvNet, LSTM | SL | formulating
    VIO as a sequential learning problem |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| VINet[[78](#bib.bib78)] | 2017 | MC+I | ConvNet, LSTM | SL | 将VIO形式化为序列学习问题
    |'
- en: '| VIOLearner[[79](#bib.bib79)] | 2018 | MC+I | ConvNet | UL | VIO with online
    correction module |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| VIOLearner[[79](#bib.bib79)] | 2018 | MC+I | ConvNet | UL | 带在线修正模块的VIO |'
- en: '| Chen et al.[[80](#bib.bib80)] | 2019 | MC+I | ConvNet, LSTM, Attention |
    SL | feature selection for deep VIO |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al.[[80](#bib.bib80)] | 2019 | MC+I | ConvNet, LSTM, Attention |
    SL | 深度VIO的特征选择 |'
- en: '| DeepVIO[[81](#bib.bib81)] | 2019 | SC+I | ConvNet, LSTM | UL | learning VIO
    from stereo images and IMU |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| DeepVIO[[81](#bib.bib81)] | 2019 | SC+I | ConvNet, LSTM | UL | 从立体图像和IMU中学习VIO
    |'
- en: '| DeepTIO[[82](#bib.bib82)] | 2020 | T+I | ConvNet, LSTM, Attention | SL |
    learning pose from thermal and inertial data |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| DeepTIO[[82](#bib.bib82)] | 2020 | T+I | ConvNet, LSTM, Attention | SL |
    从热成像和惯性数据中学习姿态 |'
- en: '| MilliEgo [[83](#bib.bib83)] | 2020 | MR+I | ConvNet, LSTM, Attention | SL
    | learning pose from mmWare radar and inertial data |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| MilliEgo [[83](#bib.bib83)] | 2020 | MR+I | ConvNet, LSTM, Attention | SL
    | 从毫米波雷达和惯性数据中学习姿态 |'
- en: '| UnVIO [[84](#bib.bib84)] | 2021 | MC+I | ConvNet, LSTM, Attention | UL |
    unsupervised learning of VIO |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| UnVIO [[84](#bib.bib84)] | 2021 | MC+I | ConvNet, LSTM, Attention | UL |
    无监督学习VIO |'
- en: '| DynaNet [[85](#bib.bib85)] | 2021 | MC+I | ConvNet, LSTM | SL | combining
    DNN with Kalman filtering |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DynaNet [[85](#bib.bib85)] | 2021 | MC+I | ConvNet, LSTM | SL | 结合DNN与卡尔曼滤波
    |'
- en: '| SelfVIO [[86](#bib.bib86)] | 2022 | MC+I | ConvNet, LSTM, Attention | UL
    | unsupervised VIO with GAN-based depth generator |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| SelfVIO [[86](#bib.bib86)] | 2022 | MC+I | ConvNet, LSTM, Attention | UL
    | 基于GAN的深度生成器的无监督VIO |'
- en: '| Tu et al. [[87](#bib.bib87)] | 2022 | L+I | ConvNet, LSTM, Attention | UL
    | unsupervised learning of LIDAR-inertial odometry |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Tu et al. [[87](#bib.bib87)] | 2022 | L+I | ConvNet, LSTM, Attention | UL
    | 无监督学习LIDAR-惯性里程计 |'
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Year indicates the publication year of each work.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Year 指每项工作的出版年份。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensor indicates the sensors involved in each work. I, MC, SC, T, MR, L, A represent
    inertial sensor, monocular camera, stereo camera, thermal camera, millimeter wave
    radar, LIDAR and airflow sensor respectively.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sensor 指每项工作的传感器。我、MC、SC、T、MR、L、A 分别代表惯性传感器、单目相机、立体相机、热成像相机、毫米波雷达、LIDAR 和气流传感器。
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning indicates how to train neural networks. SL and UL represent Supervised
    Learning and Unsupervised Learning.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Learning 指如何训练神经网络。SL 和 UL 分别代表监督学习和无监督学习。
- en: In the realm of inertial positioning for vehicles, researchers have proposed
    various methods to mitigate error drifts and improve accuracy. One such method
    is presented in [[47](#bib.bib47)], where error covariances are learned from inertial
    data and incorporated into Kalman filtering for updating system states. This approach
    has been shown to improve inertial positioning performance. Similar to ZUPT-based
    pedestrian positioning, zero-velocity-update (ZUPT) can also be used for car-equipped
    inertial navigation systems. The zero-velocity phase provides valuable context
    information to correct system error drifts via Kalman filtering. OdoNet, presented
    in [[63](#bib.bib63)], is an example of a system that learns and utilizes car
    speed along with a zero-velocity detector to reduce error drifts in car-equipped
    IMU systems. Deep learning techniques have also been explored for detecting zero-velocity
    phases in vehicle navigation. For example, [[40](#bib.bib40)] proposes a deep
    learning-based method for detecting zero-velocity phases in vehicle navigation.
    In another study, [[54](#bib.bib54)] derives a model with motion terms that are
    relevant only to the IMU data sequence. This model provides theoretical guidance
    for learning models to infer useful terms and has been evaluated on a drone dataset,
    where it outperformed TLIO and other learning methods.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在车辆惯性定位领域，研究人员提出了各种方法来减轻误差漂移并提高准确性。其中一种方法在[[47](#bib.bib47)]中提出，通过从惯性数据中学习误差协方差并将其纳入卡尔曼滤波器中更新系统状态。该方法已被证明能改善惯性定位性能。类似于基于ZUPT的行人定位，零速度更新（ZUPT）也可以用于车载惯性导航系统。零速度阶段提供了宝贵的上下文信息，通过卡尔曼滤波来纠正系统误差漂移。[[63](#bib.bib63)]中提出的OdoNet系统就是一个例子，它通过学习和利用车辆速度以及零速度检测器来减少车载IMU系统中的误差漂移。深度学习技术也被用于检测车辆导航中的零速度阶段。例如，[[40](#bib.bib40)]提出了一种基于深度学习的方法来检测车辆导航中的零速度阶段。在另一项研究中，[[54](#bib.bib54)]推导了一个仅与IMU数据序列相关的运动项模型。该模型为学习模型推断有用的项提供了理论指导，并在无人机数据集上进行了评估，结果优于TLIO和其他学习方法。
- en: Overall, these studies demonstrate the potential of deep learning-based methods
    in improving inertial navigation for various platforms, including pedestrians,
    vehicles, drones, and robots. By leveraging the rich information contained within
    IMU data, deep learning models can effectively mitigate error drifts and improve
    the accuracy of inertial positioning systems. Furthermore, by optimizing the model
    efficiency and considering deployment on resource-constrained devices, these techniques
    can be applied in real-world scenarios.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些研究展示了基于深度学习的方法在提高各种平台（包括行人、车辆、无人机和机器人）惯性导航性能方面的潜力。通过利用IMU数据中包含的丰富信息，深度学习模型能够有效地减轻误差漂移，提高惯性定位系统的准确性。此外，通过优化模型效率并考虑在资源受限设备上的部署，这些技术可以应用于实际场景中。
- en: VII Deep Learning based Sensor Fusion
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 基于深度学习的传感器融合
- en: Integrating inertial sensors with other sensors as a multisensor navigation
    system has been an area of research for several decades. Nowadays, platforms such
    as robots, vehicles, and VR/AR devices are equipped with cameras, IMUs, and LIDAR
    sensors. Hence, it is natural to consider introducing multimodal learning techniques
    [[88](#bib.bib88)] and designing learning models capable of fusing multimodal
    information to construct a mapping function from sensor data to pose.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 将惯性传感器与其他传感器集成作为多传感器导航系统已经成为数十年的研究领域。如今，机器人、车辆以及虚拟现实/增强现实设备等平台都配备了摄像头、IMU和激光雷达传感器。因此，考虑引入多模态学习技术[[88](#bib.bib88)]并设计能够融合多模态信息的学习模型，以构建从传感器数据到姿态的映射函数是很自然的。
- en: '![Refer to caption](img/fbfe2f99ce1a6a040dcb7dbb78e7709d.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fbfe2f99ce1a6a040dcb7dbb78e7709d.png)'
- en: 'Figure 7: An overview of existing methods on deep learning based sensor fusion'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：基于深度学习的传感器融合现有方法概述
- en: Visual-inertial odometry (VIO) has garnered attention as a means of integrating
    low-cost, complementary camera and IMU sensors that are widely deployed. Monocular
    vision can capture the appearance and geometry of a scene, but cannot recover
    the scale metric. IMU provides metric scale and improves motion tracking in featureless
    areas, complex lighting conditions, and motion blur. However, a pure inertial
    solution can only last for a short period. Therefore, an effective fusion of these
    two complementary sensors is necessary for accurate pose estimation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional VIO methods integrate visual and inertial information based on
    filtering [[89](#bib.bib89), [5](#bib.bib5)], fixed-lag smoothing [[90](#bib.bib90)],
    or full smoothing [[91](#bib.bib91)]. Recently, deep learning-based VIO models
    have emerged, directly constructing a mapping function from images and IMU to
    pose in a data-driven manner. VINet [[78](#bib.bib78)] is an end-to-end deep VIO
    model consisting of a ConvNet-based visual encoder to extract visual features
    from two images and an LSTM-based inertial encoder to extract inertial features
    from a sequence of inertial data between the two images. As shown in Figure [7](#S7.F7
    "Figure 7 ‣ VII Deep Learning based Sensor Fusion ‣ Deep Learning for Inertial
    Positioning: A Survey") (a), the visual and inertial features are concatenated
    together as one tensor, followed by an LSTM and fully-connected layer that finally
    maps features into a 6-dimensional pose. VINet is trained on public driving datasets
    such as the KITTI dataset [[92](#bib.bib92)] and a public drone dataset such as
    the EuroC dataset [[93](#bib.bib93)]. The learned VIO model is generally more
    robust to sensor noises compared to traditional VIO methods, although its model
    performance still cannot compete with state-of-the-art VIO methods.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'To effectively integrate visual and inertial information, [[80](#bib.bib80)]
    proposes a selective sensor fusion mechanism that learns to choose important features
    conditioned on sensor observations, as demonstrated in Figure [7](#S7.F7 "Figure
    7 ‣ VII Deep Learning based Sensor Fusion ‣ Deep Learning for Inertial Positioning:
    A Survey") (b). Specifically, this work proposes two types of fusion: soft fusion,
    which is based on an attention mechanism and generates a soft mask to reweight
    features based on their importance, and hard fusion, which is based on Gumbel
    Soft-max and generates a hard mask consisting of either 1 or 0 to either propagate
    or ignore a feature. Experimental evaluation on the KITTI dataset demonstrates
    that compared with directly concatenating features [[78](#bib.bib78)], selective
    fusion enhances the performance of deep VIO by 5%-10%. An interesting observation
    is that the number of useful features is relevant to the amount of linear/rotational
    velocity, with inertial features contributing more to rotation rate (e.g., turning),
    while more visual features are used to increase linear velocity.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Both [[78](#bib.bib78)] and [[80](#bib.bib80)] are trained in a supervised
    learning manner using datasets with high-precision ground-truth poses as training
    labels. However, obtaining high-precision poses can be difficult or costly in
    certain cases. Consequently, self-supervised learning-based VIOs, which do not
    require pose labels, have attracted attention. Self-supervised VIOs leverage the
    multi-view geometry relation of consecutive images, such as novel view synthesis,
    as a supervision signal [[79](#bib.bib79), [81](#bib.bib81), [84](#bib.bib84),
    [86](#bib.bib86)]. The task of novel view synthesis involves transforming a source
    image into a target view and comparing the differences between the synthesized
    target images and real target images as loss. In VIOLearner [[79](#bib.bib79)]
    and DeepVIO [[81](#bib.bib81)], as shown in Figure [7](#S7.F7 "Figure 7 ‣ VII
    Deep Learning based Sensor Fusion ‣ Deep Learning for Inertial Positioning: A
    Survey") (c), the pose transformation is generated from an inertial data sequence
    and used in the novel view synthesis process. In UnVIO [[84](#bib.bib84)] and
    SelfVIO [[86](#bib.bib86)], inertial data is integrated with visual data via an
    attention module applied to the concatenated visual and inertial features extracted
    from the images and IMU sequence. They show that incorporating inertial data with
    visual data improves the accuracy of pose estimation, particularly rotation estimation.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[[78](#bib.bib78)]和[[80](#bib.bib80)]都采用监督学习方法，使用高精度真实姿态的数据集作为训练标签。然而，在某些情况下，获取高精度的姿态可能困难或成本高昂。因此，基于自监督学习的VIO，因不需要姿态标签而受到关注。自监督VIO利用连续图像的多视角几何关系，如新视图合成，作为监督信号[[79](#bib.bib79),
    [81](#bib.bib81), [84](#bib.bib84), [86](#bib.bib86)]。新视图合成的任务是将源图像转换为目标视图，并将合成的目标图像与真实目标图像之间的差异作为损失。在VIOLearner
    [[79](#bib.bib79)]和DeepVIO [[81](#bib.bib81)]中，如图[7](#S7.F7 "Figure 7 ‣ VII Deep
    Learning based Sensor Fusion ‣ Deep Learning for Inertial Positioning: A Survey")
    (c)所示，姿态变换是由惯性数据序列生成的，并用于新视图合成过程。在UnVIO [[84](#bib.bib84)]和SelfVIO [[86](#bib.bib86)]中，惯性数据通过一个注意力模块与视觉数据整合，注意力模块应用于从图像和IMU序列中提取的视觉和惯性特征的连接。这些方法表明，结合惯性数据和视觉数据可以提高姿态估计的准确性，特别是旋转估计的准确性。'
- en: The use of learning-based sensor fusion extends beyond visual-inertial odometry
    (VIO) to include other sensor modalities such as Lidar-inertial odometry (LIO),
    thermal-inertial odometry, and radar-inertial odometry [[87](#bib.bib87), [82](#bib.bib82),
    [83](#bib.bib83)]. DeepTIO [[82](#bib.bib82)] and MilliEgo [[83](#bib.bib83)]
    employ attention-based selective fusion mechanisms, similar to soft fusion [[80](#bib.bib80)],
    to reweight and fuse features from inertial and visual data, resulting in improved
    pose accuracy. In addition, unsupervised learning-based LIDAR-inertial odometry
    [[87](#bib.bib87)] generates motion transformation from IMU sequence and uses
    it for LIDAR novel view synthesis to facilitate self-supervised learning of egomotion,
    similar to VIOLearner [[79](#bib.bib79)]. In all these cases, the inclusion of
    IMU data in deep neural networks enhances pose estimation accuracy and robustness.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的传感器融合不仅延伸到视觉-惯性里程计（VIO），还包括其他传感器模态，如激光雷达-惯性里程计（LIO）、热成像-惯性里程计和雷达-惯性里程计[[87](#bib.bib87),
    [82](#bib.bib82), [83](#bib.bib83)]。DeepTIO [[82](#bib.bib82)]和MilliEgo [[83](#bib.bib83)]采用基于注意力的选择性融合机制，类似于软融合[[80](#bib.bib80)]，对惯性和视觉数据的特征进行加权融合，从而提高了姿态的准确性。此外，无监督学习的激光雷达-惯性里程计[[87](#bib.bib87)]从IMU序列生成运动变换，并将其用于激光雷达的新视图合成，以促进自监督学习的自运动，类似于VIOLearner
    [[79](#bib.bib79)]。在所有这些情况下，将IMU数据纳入深度神经网络可以提高姿态估计的准确性和鲁棒性。
- en: VIII Deep Learning based Human Motion analysis and Activity Recognition
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 基于深度学习的人体运动分析与活动识别
- en: Inertial sensors have diverse applications beyond positioning, such as motion
    tracking, activity recognition, and more. Although these tasks are not the primary
    focus of this survey, this section provides a brief yet comprehensive overview
    of how deep learning is utilized in these domains.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 陀螺仪传感器的应用不仅限于定位，还包括运动跟踪、活动识别等。尽管这些任务不是本调查的主要焦点，本节仍提供了一个简明而全面的概述，说明深度学习在这些领域中的应用。
- en: VIII-A Human Motion Analysis
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 人体运动分析
- en: Data-driven approaches are utilized to reconstruct human pose and motion using
    either a single IMU or multiple IMUs attached to the body. These models primarily
    focus on analyzing human motion rather than localizing users, which differentiates
    them from inertial positioning. Several studies have applied machine learning
    to gait and pose analysis, such as knee angle estimation for human walking using
    supervised support vector regression in [[94](#bib.bib94)] and probabilistic parameter
    learning for human gesture recognition in [[95](#bib.bib95)] through handcrafted
    motion features extracted from inertial data. In addition, machine learning methods,
    such as multi-layer perceptrons (MLPs), have been utilized in IMU data to learn
    sensor displacement for human motion reconstruction in [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98)].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的方法用于重建人体姿态和运动，无论是使用单个IMU还是多个附着在身体上的IMU。这些模型主要集中在分析人体运动上，而不是定位用户，这使它们与惯性定位有所区别。一些研究应用了机器学习来分析步态和姿态，例如
    [[94](#bib.bib94)] 中的基于监督支持向量回归的人体步态角度估计和 [[95](#bib.bib95)] 中基于从惯性数据中提取的手工制作运动特征的人体手势识别。此外，机器学习方法，如多层感知机（MLP），也被应用于IMU数据中，以学习传感器位移，用于
    [[96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)] 中的人体运动重建。
- en: Recently, deep learning has shown promising performance in human pose reconstruction.
    For example, [[99](#bib.bib99)] proposed Deep Inertial Poser, a recurrent neural
    network (RNN)-based framework that can reconstruct full-body pose from six IMUs
    attached to the user’s body. TransPose [[100](#bib.bib100)], another RNN-based
    framework, enables real-time human pose estimation using six body-attached IMUs.
    Furthermore, [[101](#bib.bib101)] combines a neural kinematics estimator with
    a physics-aware motion optimizer to improve the accuracy of human motion tracking.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习在人体姿态重建方面展现了令人期待的性能。例如，[[99](#bib.bib99)] 提出了Deep Inertial Poser，这是一个基于递归神经网络（RNN）的框架，可以从附着在用户身体上的六个IMU中重建全身姿态。TransPose
    [[100](#bib.bib100)] 是另一个基于RNN的框架，能够使用六个身体附着的IMU进行实时人体姿态估计。此外，[[101](#bib.bib101)]
    结合了神经动力学估计器和物理感知运动优化器，以提高人体运动跟踪的准确性。
- en: VIII-B Human Activity recognition (HAR)
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 人体活动识别（HAR）
- en: Deep learning can be utilized to exploit inertial information from body-worn
    IMUs for human activity recognition. For instance, [[102](#bib.bib102)] published
    a popular public dataset of human activity recognition and successfully classified
    current activity among six classes, including walking, standing still, sitting,
    walking downstairs, walking upstairs, and laying down, using support vector machines
    (SVM). In addition, [[103](#bib.bib103)] presents an LSTM-based HAR model that
    inputted a sequence of inertial data and outputted class probability. Moreover,
    [[104](#bib.bib104)] introduces a ConvNet-based HAR model that achieved a classification
    accuracy of 97%, outperforming an accuracy of 96% from SVM-based HAR models. To
    reduce onboard computational requirements, [[105](#bib.bib105)] presents a learning
    framework that exploited both features automatically extracted by DNN and hand-crafted
    features to achieve accurate and real-time human activity recognition on low-end
    devices.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以用于利用体佩戴惯性测量单元（IMU）中的惯性信息来进行人体活动识别。例如，[[102](#bib.bib102)] 发布了一个流行的公共人体活动识别数据集，并成功地将当前活动分类为六类，包括行走、静止、坐着、下楼、上楼和躺下，使用了支持向量机（SVM）。此外，[[103](#bib.bib103)]
    提出了一个基于LSTM的HAR模型，该模型输入惯性数据序列并输出类别概率。此外，[[104](#bib.bib104)] 介绍了一个基于ConvNet的HAR模型，该模型实现了97%的分类准确率，超越了SVM基于HAR模型的96%准确率。为了减少车载计算需求，[[105](#bib.bib105)]
    提出了一个学习框架，该框架利用DNN自动提取的特征和手工制作的特征来实现低端设备上的准确和实时人体活动识别。
- en: Learning from inertial data can also benefit sports and health applications.
    For instance, [[106](#bib.bib106)] shows that deep learning is effective in detecting
    Parkinson’s disease by assessing the patient’s daily activity through the analysis
    of inertial information from wearable sensors. Additionally, [[107](#bib.bib107)]
    provides instructions for athletes’ sports training based on sensor data and activity
    information.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从惯性数据中学习也可以使运动和健康应用受益。例如，[[106](#bib.bib106)] 表明，深度学习在通过分析可穿戴传感器中的惯性信息来评估患者的日常活动，从而有效检测帕金森病方面表现出色。此外，[[107](#bib.bib107)]
    提供了基于传感器数据和活动信息的运动员训练指导。
- en: IX Conclusions and Discussions
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 结论与讨论
- en: In recent years, there has been a growing interest in using deep learning to
    address the problem of inertial positioning. This article provides a comprehensive
    review of the area of deep learning-based inertial positioning. The rapid advances
    in this field have already provided promising solutions to address problems such
    as inertial sensor calibration, the compensation of error drifts in inertial positioning,
    and multimodal sensor fusion. This section concludes and discusses the benefits
    that deep learning can bring to inertial navigation research, analyzes the challenges
    that existing research faces, and highlights the future opportunities of this
    evolving field.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，使用深度学习解决惯性定位问题的兴趣日益增长。本文提供了基于深度学习的惯性定位领域的综合评述。该领域的快速进展已经提供了有希望的解决方案来解决惯性传感器校准、惯性定位误差漂移补偿和多模态传感器融合等问题。本节总结并讨论了深度学习对惯性导航研究的好处，分析了现有研究面临的挑战，并突出了这一不断发展的领域的未来机会。
- en: IX-A Benefits
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A 好处
- en: 'Unlike traditional geometric or physical inertial positioning models, the integration
    of deep learning into inertial positioning has led to the development of a range
    of alternative solutions to address the issue of positioning error drifts. The
    corresponding benefits can be summarized as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的几何或物理惯性定位模型不同，将深度学习融入惯性定位已促使开发出一系列替代解决方案来解决定位误差漂移问题。相应的好处可以总结如下：
- en: IX-A1 Learn to approximate complex and varying function
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A1 学习近似复杂和变化的函数
- en: The deep neural network has proven to be a powerful and versatile nonlinear
    function that can approximate the complex and variable factors involved in inertial
    positioning, which are difficult to model manually. For example, when calibrating
    sensors, the corrupt noises that exist in inertial measurements can be modeled
    and eliminated in a data-driven way by training on a large dataset using a DNN.
    Deep learning can also directly generate absolute velocity and position displacement
    from data, without the need for IMU integration, thus reducing positioning drifts.
    In pedestrian dead reckoning (PDR), deep learning can estimate step length based
    on data, rather than empirical equations, and implicitly remove the effects of
    different users. These works demonstrate that using a large dataset to build a
    data-driven model can produce more accurate motion estimates, as well as reduce
    and constrain the rapid error drifts of inertial navigation systems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络已被证明是一种强大且多用途的非线性函数，能够近似惯性定位中涉及的复杂和变化因素，这些因素很难手动建模。例如，在校准传感器时，惯性测量中存在的噪声可以通过在大型数据集上训练
    DNN 以数据驱动的方式建模并消除。深度学习还可以直接从数据中生成绝对速度和位置位移，而无需IMU积分，从而减少定位漂移。在步态推算（PDR）中，深度学习可以基于数据估计步长，而不是经验方程，并隐式去除不同用户的影响。这些工作表明，使用大型数据集构建数据驱动模型可以产生更准确的运动估计，并减少和约束惯性导航系统的快速误差漂移。
- en: IX-A2 Learn to estimate parameters
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A2 学习估计参数
- en: Automatic identification of parameters through data-driven models contributes
    to paving the way for next-generation intelligent navigation systems that can
    actively exploit input data and evolve over time without human intervention. In
    classical inertial navigation mechanisms, certain parameters or modules need to
    be manually set and tuned before use. For instance, experts with experience need
    to settle parameters in Kalman filtering, such as observation noise, covariance,
    and process noise. Deep learning has proven effective in automatically producing
    suitable parameters for Kalman filtering based on input data [[47](#bib.bib47),
    [85](#bib.bib85), [42](#bib.bib42)]. In sensor calibration, reinforcement learning
    algorithms are used to discover optimal parameters for inertial calibration algorithms
    [[27](#bib.bib27)]. In ZUPT-based pedestrian inertial positioning, deep learning
    is a viable solution for classifying zero-velocity phases and determining when
    to update system states.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: IX-A3 Learn to self-adapt in new domains
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unforeseen or ever-changing issues in new application domains, such as changes
    in motion mode, carrier, and sensor noise, can significantly impact the performance
    of inertial systems. Learning models offer opportunities for inertial systems
    to adapt to new changes and overcome these influential factors implicitly by discovering
    and exploiting the differences in data distributions between domains. For instance,
    [[38](#bib.bib38)] leverages transfer learning to allow INS to extract domain-invariant
    features from data, maintaining localization accuracy when sensor attachment is
    changed. The introduction of self-supervised learning enables navigation systems
    to learn from data without high-precision pose as training labels, allowing unlabelled
    inertial data to be effectively used for model performance improvement. In visual-inertial
    odometry, [[79](#bib.bib79), [81](#bib.bib81), [84](#bib.bib84)] introduce novel
    view synthesis as a supervision signal to train deep VIO in a self-supervised
    learning way. This self-adaptation ability is promising for mobile agents to continuously
    improve their localization performance in new application scenes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: IX-B Challenges and Opportunities
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the impressive and promising results that deep learning has already
    offered in inertial positioning, there are still challenges in existing methods
    when they are applied and deployed in real-world scenarios. To overcome these
    limitations, several opportunities and potential research directions are discussed
    below.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: IX-B1 Generalization and Self-learning
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The generalization problem is a major concern for deep learning-based methods
    because these models are trained on one domain (i.e., training set) but need to
    be tested on other domains (i.e., testing set). The possible differences in data
    between domains can lead to a degradation of prediction performance. Although
    deep learning-based inertial navigation models have reported impressive results
    on the author’s own datasets, these works have not been evaluated in comprehensive
    experiments during long-term operation and across various devices, users, and
    application scenes. Thus, it is challenging to determine the real performance
    of these models in open environments. To address the generalization problem, new
    learning techniques such as transfer learning [[108](#bib.bib108)], lifelong learning,
    and contrastive learning [[109](#bib.bib109)] can be introduced into inertial
    positioning systems, which is a promising direction. For instance, in the future,
    by exploiting information from physical/geometric rules or other sensors (e.g.,
    GNSS, camera), the learning-based inertial positioning model can be self-supervisedly
    trained and enable mobile agents to learn from data in a lifelong manner.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化问题是深度学习方法的主要关注点，因为这些模型是在一个领域（即训练集）上训练的，但需要在其他领域（即测试集）上进行测试。领域之间的数据可能差异会导致预测性能的下降。尽管基于深度学习的惯性导航模型在作者自己的数据集上报告了令人印象深刻的结果，但这些工作尚未在长期运行和各种设备、用户和应用场景中经过全面实验评估。因此，很难确定这些模型在开放环境中的真实表现。为了解决泛化问题，可以将新学习技术如迁移学习[[108](#bib.bib108)]、终身学习和对比学习[[109](#bib.bib109)]引入惯性定位系统，这是一个有前景的方向。例如，未来通过利用物理/几何规则或其他传感器（例如GNSS、摄像头）的信息，基于学习的惯性定位模型可以进行自监督训练，使移动代理能够以终身方式从数据中学习。
- en: IX-B2 Black-box and Explainability
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B2 黑箱与可解释性
- en: Deep neural networks have been criticized as being a ’black-box’ model due to
    their lack of explainability and interpretability. As these models are often used
    to support real-world tasks, it is crucial to investigate what is learned inside
    deep nets before deploying them to ensure their safety and reliability. Despite
    the good results shown by deep learning models in estimating important terms such
    as location displacement, sensor measurement errors, and filtering parameters,
    these terms lack concrete mathematical models, unlike traditional inertial navigation.
    To determine whether these terms are trustworthy, uncertainties should be estimated
    in conjunction with the inertial positioning method [[71](#bib.bib71)] and used
    as indicators for users or systems to understand the extent to which model predictions
    can be trusted. In future research, it is important to reveal the governing mathematical
    or physical models behind the learned inertial positioning neural model and identify
    which parts of inertial positioning can be learned by deep nets. Introducing Bayesian
    deep learning into inertial positioning is also a promising direction that could
    offer interpretability for model predictions [[110](#bib.bib110)].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络因其缺乏可解释性和可解释性而被批评为“黑箱”模型。由于这些模型常用于支持现实世界的任务，在部署之前，调查深度网络内部学到的内容对于确保其安全性和可靠性至关重要。尽管深度学习模型在估计诸如位置偏移、传感器测量误差和过滤参数等重要术语方面显示了良好的结果，但这些术语缺乏具体的数学模型，与传统的惯性导航不同。为了确定这些术语是否可信，应该估算不确定性，并结合惯性定位方法[[71](#bib.bib71)]，作为用户或系统了解模型预测可信度的指标。在未来的研究中，揭示学习到的惯性定位神经模型背后的数学或物理模型，并识别深度网络能够学习的惯性定位部分是重要的。将贝叶斯深度学习引入惯性定位也是一个有前景的方向，能够为模型预测提供可解释性[[110](#bib.bib110)]。
- en: IX-B3 Efficiency and Real-world Deployment
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B3 效率与现实世界部署
- en: When deploying deep positioning models on user devices, it is crucial to consider
    the consumption of computation, storage, and energy in system design, in addition
    to prediction accuracy. Compared to classical inertial navigation algorithms,
    DNN-based inertial positioning models have a relatively large computational and
    memory burden, as they contain millions of neural parameters that require GPUs
    for parallel training and testing. Therefore, online inference of learning models,
    especially on low-end devices such as IoT consoles, VR/AR devices, and miniature
    drones, requires lightweight, efficient, and effective models. To achieve this
    goal, neural model compression techniques, such as knowledge distillation [[111](#bib.bib111)],
    should be introduced to discover the optimal neural structure that balances prediction
    accuracy and model size. [[45](#bib.bib45)] and [[63](#bib.bib63)] have conducted
    initial trials on minimizing the model size of inertial odometry. Moreover, safety
    and reliability are also crucial factors to consider. In the future, it is worth
    exploring the optimal structure of learning-based inertial positioning models,
    considering model performance, parameter size, latency, safety, and reliability
    for real-world deployment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户设备上部署深度定位模型时，除了预测准确性外，还必须考虑计算、存储和能耗的消耗。与经典的惯性导航算法相比，基于 DNN 的惯性定位模型具有相对较大的计算和内存负担，因为它们包含数百万个神经参数，需要
    GPU 进行并行训练和测试。因此，特别是在低端设备如 IoT 控制台、VR/AR 设备和微型无人机上进行在线推理时，需要轻量级、高效且有效的模型。为实现这一目标，应该引入神经模型压缩技术，如知识蒸馏
    [[111](#bib.bib111)]，以发现最佳的神经结构，平衡预测准确性和模型大小。 [[45](#bib.bib45)] 和 [[63](#bib.bib63)]
    已对惯性里程计的模型大小最小化进行了初步试验。此外，安全性和可靠性也是重要因素。未来，值得探索学习基于惯性定位模型的最佳结构，考虑模型性能、参数大小、延迟、安全性和实际部署的可靠性。
- en: IX-B4 Data Collection and Benchmark
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B4 数据收集与基准测试
- en: As a data-driven approach, the performance of deep learning models depends on
    the quality of the data, such as the size of the dataset, data diversity, and
    the differences between the training and testing sets. Under ideal conditions,
    deep learning-based inertial positioning models should be trained on diverse data
    across different users, platforms, motion dynamics, and inertial sensors to enhance
    their generalization in testing domains. However, collecting such data in diverse
    domains can be costly and time-consuming. Additionally, obtaining high-precision
    ground-truth poses as training and evaluation labels can be challenging in some
    cases. Previous research has used different training/evaluation data, model hyperparameters
    (e.g., learning rate, batch size, layer dimension), and evaluation metrics, making
    it difficult to compare these methods fairly. In visual navigation tasks, such
    as visual odometry/SLAM, the KITTI dataset [[92](#bib.bib92)] is commonly used
    as a benchmark to train and evaluate learning-based VO models. However, although
    published datasets for inertial navigation exist [[45](#bib.bib45), [46](#bib.bib46)],
    there is still a lack of a common benchmark that is adopted and recognized by
    mainstream methods in inertial positioning. In the future, a widely adopted dataset
    and benchmark, covering a variety of application scenarios, will greatly benefit
    and foster research in data-driven inertial positioning.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种数据驱动的方法，深度学习模型的性能依赖于数据的质量，如数据集的大小、数据的多样性以及训练集与测试集之间的差异。在理想情况下，基于深度学习的惯性定位模型应在不同用户、平台、运动动态和惯性传感器上的多样数据上进行训练，以增强其在测试领域的泛化能力。然而，收集如此多样化领域的数据可能会非常昂贵且耗时。此外，在某些情况下，获取高精度的地面真实值作为训练和评估标签可能具有挑战性。以往的研究使用了不同的训练/评估数据、模型超参数（例如学习率、批量大小、层维度）和评估指标，使得这些方法的公平比较变得困难。在视觉导航任务中，例如视觉里程计/SLAM，KITTI
    数据集 [[92](#bib.bib92)] 通常作为基准来训练和评估基于学习的 VO 模型。然而，尽管存在惯性导航的公开数据集 [[45](#bib.bib45),
    [46](#bib.bib46)]，但仍然缺乏一个被主流惯性定位方法广泛采用和认可的共同基准。在未来，覆盖各种应用场景的广泛采用的数据集和基准将极大地促进数据驱动的惯性定位研究。
- en: IX-B5 Failure Cases and Physical Constraints
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B5 失败案例与物理限制
- en: 'Deep learning has demonstrated its capability in reducing the drifts of inertial
    positioning and contributing to various aspects of inertial navigation systems,
    as discussed in Section [IV](#S4 "IV Learning to correct IMU Integration ‣ Deep
    Learning for Inertial Positioning: A Survey"). However, DNN models are not always
    reliable and may occasionally produce large and abrupt prediction errors. Unlike
    traditional inertial navigation algorithms that are based on concrete physical
    and mathematical rules, DNN predictions lack constraints, and the failure cases
    must be considered in real-world applications with safety concerns. To enhance
    the robustness of DNN predictions, possible solutions include imposing physical
    constraints on DNN models or combining deep learning with physical models as hybrid
    inertial positioning models. By doing so, the benefits from both learning and
    physics-based positioning models can be leveraged.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习在减少惯性定位漂移和促进惯性导航系统的各个方面中展示了其能力，如[IV](#S4 "IV Learning to correct IMU Integration
    ‣ Deep Learning for Inertial Positioning: A Survey")节中所讨论的。然而，DNN模型并不总是可靠的，可能会偶尔产生大的、突兀的预测误差。与基于具体物理和数学规则的传统惯性导航算法不同，DNN预测缺乏约束，失败情况必须在具有安全隐患的实际应用中加以考虑。为了提高DNN预测的鲁棒性，可能的解决方案包括对DNN模型施加物理约束，或将深度学习与物理模型结合为混合惯性定位模型。通过这样做，可以利用学习和基于物理的定位模型的双重优势。'
- en: IX-B6 New deep learning methods
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B6 新型深度学习方法
- en: Machine/deep learning is one of the fastest growing areas of AI, and its advances
    have influenced numerous fields such as computer vision, robotics, natural language
    processing, and signal processing. There are significant opportunities for applying
    deep learning techniques to inertial navigation and analyzing their effectiveness
    and theoretical underpinnings. In the future, new model structures such as transformer
    [[72](#bib.bib72)], diffusion models [[112](#bib.bib112)], and generative models
    [[69](#bib.bib69)], and new learning methods such as transfer learning, reinforcement
    learning, contrastive learning [[109](#bib.bib109)], unsupervised learning, and
    meta-learning [[113](#bib.bib113)], all hold promise for enhancing inertial positioning
    systems. Furthermore, advances in other domains such as neural rendering [[114](#bib.bib114)]
    and voice synthesis [[115](#bib.bib115)] may provide valuable insights into developing
    more effective inertial positioning systems. Therefore, incorporating these rapidly-evolving
    deep learning methods into inertial navigation will be a significant area of research
    in the future.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 机器/深度学习是人工智能领域中增长最快的领域之一，其进展已经影响了计算机视觉、机器人技术、自然语言处理和信号处理等众多领域。深度学习技术在惯性导航中的应用及其效果和理论基础有着重要的机会。未来，新型模型结构如变换器
    [[72](#bib.bib72)]、扩散模型 [[112](#bib.bib112)] 和生成模型 [[69](#bib.bib69)]，以及新的学习方法如迁移学习、强化学习、对比学习
    [[109](#bib.bib109)]、无监督学习和元学习 [[113](#bib.bib113)]，都有可能提升惯性定位系统的性能。此外，其他领域如神经渲染
    [[114](#bib.bib114)] 和语音合成 [[115](#bib.bib115)] 的进展可能为开发更有效的惯性定位系统提供宝贵的见解。因此，将这些迅速发展的深度学习方法融入惯性导航将成为未来的重要研究方向。
- en: References
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] P. G. Savage, “Strapdown Inertial Navigation Integration Algorithm Design
    Part 1: Attitude Algorithms,” Journal of Guidance, Control, and Dynamics, vol. 21,
    no. 1, pp. 19–28, 1998.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. G. Savage, “带式惯性导航融合算法设计第1部分：姿态算法”，《导引、控制与动力学学报》，第21卷，第1期，页19–28，1998年。'
- en: '[2] P. G. Savage, “Strapdown Inertial Navigation Integration Algorithm Design
    Part 2: Velocity and Position Algorithms,” Journal of Guidance, Control, and Dynamics,
    vol. 21, no. 1, pp. 19–28, 1998.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. G. Savage, “带式惯性导航融合算法设计第2部分：速度和位置算法”，《导引、控制与动力学学报》，第21卷，第1期，页19–28，1998年。'
- en: '[3] R. Harle, “A survey of indoor inertial positioning systems for pedestrians,”
    IEEE Communications Surveys & Tutorials, vol. 15, no. 3, pp. 1281–1293, 2013.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Harle, “步行者室内惯性定位系统的调查”，《IEEE通讯调查与教程》，第15卷，第3期，页1281–1293，2013年。'
- en: '[4] I. Skog, P. Händel, J.-O. Nilsson, and J. Rantakokko, “Zero-Velocity Detection
    — an Algorithm Evaluation.,” IEEE transactions on bio-medical engineering, vol. 57,
    no. 11, pp. 2657–2666, 2010.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] I. Skog, P. Händel, J.-O. Nilsson, 和 J. Rantakokko, “零速检测——一种算法评估”，《IEEE生物医学工程汇刊》，第57卷，第11期，页2657–2666，2010年。'
- en: '[5] M. Li and A. I. Mourikis, “High-precision, Consistent EKF-based Visual-Inertial
    Odometry,” The International Journal of Robotics Research, vol. 32, no. 6, pp. 690–711,
    2013.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile Monocular
    Visual-Inertial State Estimator,” IEEE Transactions on Robotics, vol. 34, no. 4,
    pp. 1004–1020, 2018.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] W. Xu, Y. Cai, D. He, J. Lin, and F. Zhang, “Fast-lio2: Fast direct lidar-inertial
    odometry,” IEEE Transactions on Robotics, 2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Y. Bengio, I. Goodfellow, and A. Courville, Deep learning, vol. 1. MIT
    press Cambridge, MA, USA, 2017.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
    learning: A review,” IEEE transactions on neural networks and learning systems,
    vol. 30, no. 11, pp. 3212–3232, 2019.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Hao, Y. Zhou, and Y. Guo, “A brief survey on semantic segmentation
    with deep learning,” Neurocomputing, vol. 406, pp. 302–321, 2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner,
    B. Upcroft, P. Abbeel, W. Burgard, M. Milford, et al., “The limits and potentials
    of deep learning for robotics,” The International journal of robotics research,
    vol. 37, no. 4-5, pp. 405–420, 2018.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Y. Li, R. Chen, X. Niu, Y. Zhuang, Z. Gao, X. Hu, and N. El-Sheimy, “Inertial
    sensing meets machine learning: Opportunity or challenge?,” IEEE Transactions
    on Intelligent Transportation Systems, 2021.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. S. Farahsari, A. Farahzadi, J. Rezazadeh, and A. Bagheri, “A survey
    on indoor positioning systems for iot-based applications,” IEEE Internet of Things
    Journal, vol. 9, no. 10, pp. 7680–7699, 2022.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. E. Díez, A. Bahillo, J. Otegui, and T. Otim, “Step length estimation
    methods based on inertial sensors: A review,” IEEE Sensors Journal, vol. 18, no. 17,
    pp. 6908–6926, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Wu, H.-B. Zhu, Q.-X. Du, and S.-M. Tang, “A survey of the research
    status of pedestrian dead reckoning systems based on inertial sensors,” International
    Journal of Automation and Computing, vol. 16, no. 1, pp. 65–83, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] X. Ru, N. Gu, H. Shang, and H. Zhang, “Mems inertial sensor calibration
    technology: Current status and future trends,” Micromachines, vol. 13, no. 6,
    p. 879, 2022.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N. Naser, El-Sheimy; Haiying, Hou; Xiaojii, “Analysis and Modeling of
    Inertial Sensors Using Allan Variance,” IEEE Transactions on Instrumentation and
    Measurement, vol. 57, no. JANUARY, pp. 684–694, 2008.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] H. Weinberg, “Using the adxl202 in pedometer and personal navigation applications,”
    Analog Devices AN-602 application note, vol. 2, no. 2, pp. 1–6, 2002.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Fang, P. J. Antsaklis, L. A. Montestruque, M. B. McMickell, M. Lemmon,
    Y. Sun, H. Fang, I. Koutroulis, M. Haenggi, M. Xie, et al., “Design of a wireless
    assisted pedestrian dead reckoning system-the navmote experience,” IEEE transactions
    on Instrumentation and Measurement, vol. 54, no. 6, pp. 2342–2358, 2005.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] P. Goyal, V. J. Ribeiro, H. Saran, and A. Kumar, “Strap-down pedestrian
    dead-reckoning system,” in 2011 international conference on indoor positioning
    and indoor navigation, pp. 1–7, IEEE, 2011.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] B. Huang, G. Qi, X. Yang, L. Zhao, and H. Zou, “Exploiting cyclic features
    of walking for pedestrian dead reckoning with unconstrained smartphones,” in Proceedings
    of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,
    pp. 374–385, 2016.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. Feng, C. Wang, C. He, Y. Zhuang, and X.-G. Xia, “Kalman-filter-based
    integration of imu and uwb for high-accuracy indoor positioning and navigation,”
    IEEE Internet of Things Journal, vol. 7, no. 4, pp. 3133–3146, 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Yang, J. Liu, X. Gong, G. Huang, and Y. Bai, “A robust heading estimation
    solution for smartphone multisensor-integrated indoor positioning,” IEEE Internet
    of Things Journal, vol. 8, no. 23, pp. 17186–17198, 2021.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. Xiyuan, “Modeling random gyro drift by time series neural networks
    and by traditional method,” in International Conference on Neural Networks and
    Signal Processing, 2003\. Proceedings of the 2003, vol. 1, pp. 810–813, IEEE,
    2003.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Chen, P. Aggarwal, T. M. Taha, and V. P. Chodavarapu, “Improving inertial
    sensor by reducing errors using deep learning methodology,” in NAECON 2018-IEEE
    National Aerospace and Electronics Conference, pp. 197–202, IEEE, 2018.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. A. Esfahani, H. Wang, K. Wu, and S. Yuan, “Orinet: Robust 3-d orientation
    estimation with a single particular imu,” IEEE Robotics and Automation Letters,
    vol. 5, no. 2, pp. 399–406, 2019.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] F. Nobre and C. Heckman, “Learning to calibrate: Reinforcement learning
    for guided calibration of visual–inertial rigs,” The International Journal of
    Robotics Research, vol. 38, no. 12-13, pp. 1388–1402, 2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. Brossard, S. Bonnabel, and A. Barrau, “Denoising imu gyroscopes with
    deep learning for open-loop attitude estimation,” IEEE Robotics and Automation
    Letters, vol. 5, no. 3, pp. 4796–4803, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] X. Zhao, C. Deng, X. Kong, J. Xu, and Y. Liu, “Learning to compensate
    for the drift and error of gyroscope in vehicle localization,” in 2020 IEEE Intelligent
    Vehicles Symposium (IV), pp. 852–857, IEEE, 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] F. Huang, Z. Wang, L. Xing, and C. Gao, “A mems imu gyroscope calibration
    method based on deep learning,” IEEE Transactions on Instrumentation and Measurement,
    vol. 71, pp. 1–9, 2022.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] R. Li, C. Fu, W. Yi, and X. Yi, “Calib-net: Calibrating the low-cost imu
    via deep convolutional neural network,” Frontiers in Robotics and AI, vol. 8,
    p. 772583, 2022.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S.-i. Amari, “Backpropagation and stochastic gradient descent method,”
    Neurocomputing, vol. 5, no. 4-5, pp. 185–196, 1993.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. K. Jain, J. Mao, and K. M. Mohiuddin, “Artificial neural networks:
    A tutorial,” Computer, vol. 29, no. 3, pp. 31–44, 1996.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C. Chen, X. Lu, A. Markham, and N. Trigoni, “Ionet: Learning to cure the
    curse of drift in inertial odometry,” in The Conference on Artificial Intelligence
    (AAAI), 2018.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H. Yan, Q. Shan, and Y. Furukawa, “Ridi: Robust imu double integration,”
    in Proceedings of the European Conference on Computer Vision (ECCV), pp. 621–636,
    2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Cortés, A. Solin, and J. Kannala, “Deep learning based speed estimation
    for constraining strapdown inertial navigation on smartphones,” in 2018 IEEE 28th
    International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1–6,
    IEEE, 2018.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] B. Wagstaff and J. Kelly, “Lstm-based zero-velocity detection for robust
    inertial navigation,” in 2018 International Conference on Indoor Positioning and
    Indoor Navigation (IPIN), pp. 1–8, IEEE, 2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] C. Chen, Y. Miao, C. X. Lu, L. Xie, P. Blunsom, A. Markham, and N. Trigoni,
    “Motiontransformer: Transferring neural inertial tracking between domains,” in
    The Conference on Artificial Intelligence (AAAI), vol. 33, 2019.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M. A. Esfahani, H. Wang, K. Wu, and S. Yuan, “Aboldeepio: A novel deep
    inertial odometry network for autonomous vehicles,” IEEE Transactions on Intelligent
    Transportation Systems, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Brossard, A. Barrau, and S. Bonnabel, “Rins-w: Robust inertial navigation
    system on wheels,” The IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. Feigl, S. Kram, P. Woller, R. H. Siddiqui, M. Philippsen, and C. Mutschler,
    “A bidirectional lstm for estimating dynamic human velocities from a single imu,”
    in 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN),
    pp. 1–8, IEEE, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Q. Wang, H. Luo, L. Ye, A. Men, F. Zhao, Y. Huang, and C. Ou, “Pedestrian
    heading estimation based on spatial transformer networks and hierarchical lstm,”
    IEEE Access, vol. 7, pp. 162309–162322, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Yu, B. Liu, X. Lan, Z. Xiao, S. Lin, B. Yan, and L. Zhou, “Azupt: Adaptive
    zero velocity update based on neural networks for pedestrian tracking,” in 2019
    IEEE Global Communications Conference (GLOBECOM), pp. 1–6, IEEE, 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] W. Liu, D. Caruso, E. Ilg, J. Dong, A. I. Mourikis, K. Daniilidis, V. Kumar,
    and J. Engel, “Tlio: Tight learned inertial odometry,” IEEE Robotics and Automation
    Letters, vol. 5, no. 4, pp. 5653–5660, 2020.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Chen, P. Zhao, C. X. Lu, W. Wang, A. Markham, and N. Trigoni, “Deep-learning-based
    pedestrian inertial navigation: Methods, data set, and on-device inference,” IEEE
    Internet of Things Journal, vol. 7, no. 5, pp. 4431–4441, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Herath, H. Yan, and Y. Furukawa, “Ronin: Robust neural inertial navigation
    in the wild: Benchmark, evaluations, & new methods,” in 2020 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 3146–3152, IEEE, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Brossard, A. Barrau, and S. Bonnabel, “Ai-imu dead-reckoning,” IEEE
    Transactions on Intelligent Vehicles, vol. 5, no. 4, pp. 585–595, 2020.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] I. Klein and O. Asraf, “Stepnet—deep learning approaches for step length
    estimation,” IEEE Access, vol. 8, pp. 85706–85713, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Wang, H. Cheng, and M. Q.-H. Meng, “Pedestrian motion tracking by using
    inertial sensors on the smartphone,” in 2020 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pp. 4426–4431, IEEE, 2020.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Teng, P. Xu, D. Guo, Y. Guo, R. Hu, H. Chai, and D. Chuxing, “Arpdr:
    An accurate and robust pedestrian dead reckoning system for indoor localization
    on handheld smartphones,” in 2020 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS), pp. 10888–10893, IEEE, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Sun, D. Melamed, and K. Kitani, “Idol: Inertial deep orientation-estimation
    and localization,” in Proceedings of the AAAI Conference on Artificial Intelligence,
    vol. 35, pp. 6128–6137, 2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] O. Asraf, F. Shama, and I. Klein, “Pdrnet: A deep-learning pedestrian
    dead reckoning framework,” IEEE Sensors Journal, vol. 22, no. 6, pp. 4932–4939,
    2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] R. Buchanan, M. Camurri, F. Dellaert, and M. Fallon, “Learning inertial
    odometry for dynamic legged robot state estimation,” in Conference on Robot Learning,
    pp. 1575–1584, PMLR, 2022.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Zhang, M. Zhang, Y. Chen, and M. Li, “Imu data processing for inertial
    aided navigation: A recurrent neural network based approach,” in 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 3992–3998, IEEE, 2021.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Gong, X. Zhang, Y. Huang, J. Ren, and Y. Zhang, “Robust inertial motion
    tracking through deep sensor fusion across smart earbuds and smartphone,” Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 5,
    no. 2, pp. 1–26, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Herath, D. Caruso, C. Liu, Y. Chen, and Y. Furukawa, “Neural inertial
    localization,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 6604–6613, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Cao, C. Zhou, D. Zeng, and Y. Wang, “Rio: Rotation-equivariance supervised
    learning of robust inertial odometry,” in Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 6614–6623, 2022.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Wang, J. Kuang, Y. Li, and X. Niu, “Magnetic field-enhanced learning-based
    inertial odometry for indoor pedestrian,” IEEE Transactions on Instrumentation
    and Measurement, vol. 71, pp. 1–13, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] S. S. Saha, S. S. Sandha, L. A. Garcia, and M. Srivastava, “Tinyodom:
    Hardware-aware efficient neural inertial navigation,” Proceedings of the ACM on
    Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 6, no. 2, pp. 1–32,
    2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] B. Rao, E. Kazemi, Y. Ding, D. M. Shila, F. M. Tucker, and L. Wang, “Ctin:
    Robust contextual transformer network for inertial navigation,” in Proceedings
    of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 5413–5421, 2022.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B. Zhou, Z. Gu, F. Gu, P. Wu, C. Yang, X. Liu, L. Li, Y. Li, and Q. Li,
    “Deepvip: Deep learning-based vehicle indoor positioning using smartphones,” IEEE
    Transactions on Vehicular Technology, 2022.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] F. Bo, J. Li, and W. Wang, “Mode-independent stride length estimation
    with imus in smartphones,” IEEE Sensors Journal, vol. 22, no. 6, pp. 5824–5833,
    2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] H. Tang, X. Niu, T. Zhang, Y. Li, and J. Liu, “Odonet: Untethered speed
    aiding for vehicle navigation without hardware wheeled odometer,” IEEE Sensors
    Journal, 2022.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Wang, H. Cheng, and M. Q.-H. Meng, “A2dio: Attention-driven deep inertial
    odometry for pedestrian localization based on 6d imu,” in 2022 International Conference
    on Robotics and Automation (ICRA), pp. 819–825, IEEE, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Wang, J. Kuang, X. Niu, and J. Liu, “Llio: Lightweight learned inertial
    odometer,” IEEE Internet of Things Journal, 2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,”
    The International Conference on Learning Representations (ICLR), 2016.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” Communications
    of the ACM, vol. 63, no. 11, pp. 139–144, 2020.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in Proceedings of the IEEE conference on computer vision and
    pattern recognition, pp. 7167–7176, 2017.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Chen, X. Lu, J. Wahlstrom, A. Markham, and N. Trigoni, “Deep neural
    network based inertial odometry using low-cost inertial measurement units,” IEEE
    Transactions on Mobile Computing, 2021.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural
    information processing systems, vol. 30, 2017.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] N. A. Abiad, Y. Kone, V. Renaudin, and T. Robert, “Smartstep: A robust
    step detection method based on smartphone inertial signals driven by gait learning,”
    IEEE Sensors Journal, vol. 22, pp. 12288–12297, 6 2022.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer
    networks,” Advances in neural information processing systems, vol. 28, 2015.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. Manos, T. Hazan, and I. Klein, “Walking direction estimation using
    smartphone sensors: A deep network-based framework,” IEEE Transactions on Instrumentation
    and Measurement, vol. 71, 2022.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, “Temporal convolutional
    networks for action segmentation and detection,” in proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 156–165, 2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, X. Chen, and X. Wang, “A
    comprehensive survey of neural architecture search: Challenges and solutions,”
    ACM Computing Surveys (CSUR), vol. 54, no. 4, pp. 1–34, 2021.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “VINet : Visual-Inertial
    Odometry as a Sequence-to-Sequence Learning Problem,” in The Conference on Artificial
    Intelligence (AAAI), pp. 3995–4001, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] E. J. Shamwell, K. Lindgren, S. Leung, and W. D. Nothwang, “Unsupervised
    deep visual-inertial odometry with online error correction for rgb-d imagery,”
    IEEE transactions on pattern analysis and machine intelligence, 2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, and N. Trigoni,
    “Selective sensor fusion for neural visual-inertial odometry,” in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10542–10551,
    2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. Han, Y. Lin, G. Du, and S. Lian, “Deepvio: Self-supervised deep learning
    of monocular visual inertial odometry using 3d geometric constraints,” in 2019
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6906–6913,
    IEEE, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. R. U. Saputra, P. P. de Gusmao, C. X. Lu, Y. Almalioglu, S. Rosa, C. Chen,
    J. Wahlström, W. Wang, A. Markham, and N. Trigoni, “Deeptio: A deep thermal-inertial
    odometry with visual hallucination,” IEEE Robotics and Automation Letters, vol. 5,
    no. 2, pp. 1672–1679, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C. X. Lu, M. R. U. Saputra, P. Zhao, Y. Almalioglu, P. P. De Gusmao, C. Chen,
    K. Sun, N. Trigoni, and A. Markham, “milliego: single-chip mmwave radar aided
    egomotion estimation via deep sensor fusion,” in Proceedings of the 18th Conference
    on Embedded Networked Sensor Systems, pp. 109–122, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] P. Wei, G. Hua, W. Huang, F. Meng, and H. Liu, “Unsupervised monocular
    visual-inertial odometry network,” in Proceedings of the Twenty-Ninth International
    Conference on International Joint Conferences on Artificial Intelligence, pp. 2347–2354,
    2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Chen, C. X. Lu, B. Wang, N. Trigoni, and A. Markham, “Dynanet: Neural
    kalman dynamical model for motion estimation and prediction,” IEEE Transactions
    on Neural Networks and Learning Systems, vol. 32, no. 12, pp. 5479–5491, 2021.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Almalioglu, M. Turan, M. R. U. Saputra, P. P. de Gusmão, A. Markham,
    and N. Trigoni, “Selfvio: Self-supervised deep monocular visual–inertial odometry
    and depth estimation,” Neural Networks, vol. 150, pp. 119–136, 2022.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Tu and J. Xie, “Undeeplio: Unsupervised deep lidar-inertial odometry,”
    in Asian Conference on Pattern Recognition, pp. 189–202, Springer, 2022.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] D. Ramachandram and G. W. Taylor, “Deep multimodal learning: A survey
    on recent advances and trends,” IEEE signal processing magazine, vol. 34, no. 6,
    pp. 96–108, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] E. S. Jones and S. Soatto, “Visual-inertial navigation, mapping and localization:
    A scalable real-time causal approach,” The International Journal of Robotics Research,
    vol. 30, no. 4, pp. 407–430, 2011.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, “Keyframe-based
    visual–inertial odometry using nonlinear optimization,” The International Journal
    of Robotics Research, vol. 34, no. 3, pp. 314–334, 2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, “On-manifold preintegration
    for real-time visual–inertial odometry,” IEEE Transactions on Robotics, vol. 33,
    no. 1, pp. 1–21, 2017.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” The International Journal of Robotics Research, vol. 32, no. 11,
    pp. 1231–1237, 2013.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
    Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” The International
    Journal of Robotics Research, vol. 35, no. 10, pp. 1157–1163, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S. Ahuja, W. Jirattigalachote, and A. Tosborvorn, “Improving Accuracy
    of Inertial Measurement Units using Support Vector Regression,” tech. rep., 2011.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A. Parate, M. C. Chiu, C. Chadowitz, D. Ganesan, and E. Kalogerakis, “RisQ:
    Recognizing smoking gestures with inertial sensors on a wristband,” in Annual
    International Conference on Mobile Systems, Applications, and Services (MobiSys),
    pp. 149–161, 2014.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Mannini and A. M. Sabatini, “Machine learning methods for classifying
    human physical activity from on-body accelerometers,” Sensors, vol. 10, no. 2,
    pp. 1154–1175, 2010.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Valtazanos, D. Arvind, and S. Ramamoorthy, “Using wearable inertial
    sensors for posture and position tracking in unconstrained environments through
    learned translation manifolds,” in 2013 ACM/IEEE International Conference on Information
    Processing in Sensor Networks (IPSN), pp. 241–252, IEEE, 2013.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Yuwono, S. W. Su, Y. Guo, B. D. Moulton, and H. T. Nguyen, “Unsupervised
    nonparametric method for gait analysis using a waist-worn inertial sensor,” Applied
    Soft Computing, vol. 14, pp. 72–80, 2014.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y. Huang, M. Kaufmann, E. Aksan, M. J. Black, O. Hilliges, and G. Pons-Moll,
    “Deep inertial poser: Learning to reconstruct human pose from sparse inertial
    measurements in real time,” ACM Transactions on Graphics (TOG), vol. 37, no. 6,
    pp. 1–15, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Yi, Y. Zhou, and F. Xu, “Transpose: real-time 3d human translation
    and pose estimation with six inertial sensors,” ACM Transactions on Graphics (TOG),
    vol. 40, no. 4, pp. 1–13, 2021.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Yi, Y. Zhou, M. Habermann, S. Shimada, V. Golyanik, C. Theobalt, and
    F. Xu, “Physical inertial poser (pip): Physics-aware real-time human motion tracking
    from sparse inertial sensors,” in Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 13167–13178, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] D. Anguita, A. Ghio, L. Oneto, X. Parra Perez, and J. L. Reyes Ortiz,
    “A public domain dataset for human activity recognition using smartphones,” in
    Proceedings of the 21th international European symposium on artificial neural
    networks, computational intelligence and machine learning, pp. 437–442, 2013.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] G. Chevalier, “Lstms for human activity recognition,” 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Zebin, P. J. Scully, and K. B. Ozanyan, “Human activity recognition
    with inertial sensors using a deep learning approach,” in 2016 IEEE sensors, pp. 1–3,
    IEEE, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] D. Ravi, C. Wong, B. Lo, and G.-Z. Yang, “A deep learning approach to
    on-node sensor data analytics for mobile or wearable devices,” IEEE journal of
    biomedical and health informatics, vol. 21, no. 1, pp. 56–64, 2016.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] B. M. Eskofier, S. I. Lee, J.-F. Daneault, F. N. Golabchi, G. Ferreira-Carvalho,
    G. Vergara-Diaz, S. Sapienza, G. Costante, J. Klucken, T. Kautz, et al., “Recent
    machine learning advancements in sensor-based mobility analysis: Deep learning
    for parkinson’s disease assessment,” in 2016 38th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 655–658, IEEE,
    2016.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Windau and L. Itti, “Inertial-based motion capturing and smart training
    system,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS), pp. 4027–4034, IEEE, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,”
    Journal of Big data, vol. 3, no. 1, pp. 1–40, 2016.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola, “What
    makes for good views for contrastive learning?,” Advances in Neural Information
    Processing Systems, vol. 33, pp. 6827–6839, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?,” Advances in neural information processing systems,
    vol. 30, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A
    survey,” International Journal of Computer Vision, vol. 129, no. 6, pp. 1789–1819,
    2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S.
    Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes, et al., “Photorealistic text-to-image
    diffusion models with deep language understanding,” Neural Information Processing
    Systems, 2022.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in International conference on machine learning,
    pp. 1126–1135, PMLR, 2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    Communications of the ACM, vol. 65, no. 1, pp. 99–106, 2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu,
    G. Driessche, E. Lockhart, L. Cobo, F. Stimberg, et al., “Parallel wavenet: Fast
    high-fidelity speech synthesis,” in International conference on machine learning,
    pp. 3918–3926, PMLR, 2018.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu,
    G. Driessche, E. Lockhart, L. Cobo, F. Stimberg 等人，“Parallel wavenet: Fast high-fidelity
    speech synthesis，” 发表在国际机器学习大会，页码 3918–3926，PMLR，2018年。'
- en: '| ![[Uncaptioned image]](img/241a85796e8a35d57f6d4a3d77089abc.png) | Changhao
    Chen is a Lecturer at College of Intelligence Science and Technology, National
    University of Defense Technology (China). Before that, he obtained his Ph.D. degree
    at University of Oxford (UK), M.Eng. degree at National University of Defense
    Technology (China), and B.Eng. degree at Tongji University (China). His research
    interest lies in robotics, computer vision and cyberphysical systems. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/241a85796e8a35d57f6d4a3d77089abc.png) | Changhao Chen 是中国国防科技大学智能科学与技术学院的讲师。在此之前，他在牛津大学（英国）获得博士学位，在国防科技大学（中国）获得硕士学位，并在同济大学（中国）获得学士学位。他的研究兴趣包括机器人技术、计算机视觉和网络物理系统。
    |'
- en: '| ![[Uncaptioned image]](img/bff8f31cc2fb0546aee879d0baefd51a.png) | Xianfei
    Pan received the Ph.D. degree in control science and engineering from the National
    University of Defense Technology, Changsha, China, in 2008\. Currently, he is
    a professor of the College of Intelligence Science and Technology, National University
    of Defense Technology. His current research interests include Inertial navigation
    system and indoor navigation system. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/bff8f31cc2fb0546aee879d0baefd51a.png) | Xianfei Pan 于 2008
    年在中国长沙国防科技大学获得控制科学与工程博士学位。目前，他是国防科技大学智能科学与技术学院的教授。他目前的研究兴趣包括惯性导航系统和室内导航系统。 |'
