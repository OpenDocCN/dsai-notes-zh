- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:40:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:40:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2303.03757] Deep Learning for Inertial Positioning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2303.03757] æ·±åº¦å­¦ä¹ åœ¨æƒ¯æ€§å®šä½ä¸­çš„åº”ç”¨ï¼šç»¼è¿°'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2303.03757](https://ar5iv.labs.arxiv.org/html/2303.03757)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2303.03757](https://ar5iv.labs.arxiv.org/html/2303.03757)
- en: 'Deep Learning for Inertial Positioning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ åœ¨æƒ¯æ€§å®šä½ä¸­çš„åº”ç”¨ï¼šç»¼è¿°
- en: 'Changhao Chen and Xianfei Pan The authors are with the College of Intelligence
    Science and Technology, National University of Defense Technology, Changsha, 410073,
    China Changhao Chen and Xianfei Pan are co-first authors. Changhao Chen is the
    corresponding author. (Email: changhao.chen66@outlook.com)This work was supported
    by National Natural Science Foundation of China (NFSC) under the Grant Number
    of 62103427, 62073331, 62103430, 62103429.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Changhao Chen å’Œ Xianfei Pan ä½œè€…éš¶å±äºå›½é˜²ç§‘æŠ€å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ï¼Œä¸­å›½é•¿æ²™ï¼Œ410073ã€‚Changhao Chen å’Œ
    Xianfei Pan æ˜¯å…±åŒç¬¬ä¸€ä½œè€…ã€‚Changhao Chen æ˜¯é€šè®¯ä½œè€…ã€‚ï¼ˆç”µå­é‚®ä»¶ï¼šchanghao.chen66@outlook.comï¼‰æœ¬å·¥ä½œå¾—åˆ°äº†ä¸­å›½å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆNFSCï¼‰èµ„åŠ©ï¼Œèµ„åŠ©å·ä¸º62103427ã€62073331ã€62103430ã€62103429ã€‚
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Inertial sensors are widely utilized in smartphones, drones, robots, and IoT
    devices, playing a crucial role in enabling ubiquitous and reliable localization.
    Inertial sensor-based positioning is essential in various applications, including
    personal navigation, location-based security, and human-device interaction. However,
    low-cost MEMS inertial sensorsâ€™ measurements are inevitably corrupted by various
    error sources, leading to unbounded drifts when integrated doubly in traditional
    inertial navigation algorithms, subjecting inertial positioning to the problem
    of error drifts. In recent years, with the rapid increase in sensor data and computational
    power, deep learning techniques have been developed, sparking significant research
    into addressing the problem of inertial positioning. Relevant literature in this
    field spans across mobile computing, robotics, and machine learning. In this article,
    we provide a comprehensive review of deep learning-based inertial positioning
    and its applications in tracking pedestrians, drones, vehicles, and robots. We
    connect efforts from different fields and discuss how deep learning can be applied
    to address issues such as sensor calibration, positioning error drift reduction,
    and multi-sensor fusion. This article aims to attract readers from various backgrounds,
    including researchers and practitioners interested in the potential of deep learning-based
    techniques to solve inertial positioning problems. Our review demonstrates the
    exciting possibilities that deep learning brings to the table and provides a roadmap
    for future research in this field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ¯æ€§ä¼ æ„Ÿå™¨åœ¨æ™ºèƒ½æ‰‹æœºã€æ— äººæœºã€æœºå™¨äººå’Œç‰©è”ç½‘è®¾å¤‡ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œåœ¨å®ç°æ™®éå’Œå¯é çš„å®šä½ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚åŸºäºæƒ¯æ€§ä¼ æ„Ÿå™¨çš„å®šä½åœ¨ä¸ªäººå¯¼èˆªã€åŸºäºä½ç½®çš„å®‰å…¨æ€§å’Œäººæœºäº¤äº’ç­‰å„ç§åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½æˆæœ¬MEMSæƒ¯æ€§ä¼ æ„Ÿå™¨çš„æµ‹é‡ä¸å¯é¿å…åœ°å—åˆ°å„ç§è¯¯å·®æºçš„å¹²æ‰°ï¼Œå¯¼è‡´åœ¨ä¼ ç»Ÿæƒ¯æ€§å¯¼èˆªç®—æ³•ä¸­è¿›è¡ŒåŒé‡ç§¯åˆ†æ—¶å‡ºç°æ— ç•Œæ¼‚ç§»ï¼Œä½¿æƒ¯æ€§å®šä½é¢ä¸´è¯¯å·®æ¼‚ç§»çš„é—®é¢˜ã€‚è¿‘å¹´æ¥ï¼Œéšç€ä¼ æ„Ÿå™¨æ•°æ®å’Œè®¡ç®—èƒ½åŠ›çš„å¿«é€Ÿå¢é•¿ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å¾—åˆ°äº†å‘å±•ï¼Œå¼•å‘äº†å¤§é‡ç ”ç©¶ä»¥è§£å†³æƒ¯æ€§å®šä½çš„é—®é¢˜ã€‚è¯¥é¢†åŸŸçš„ç›¸å…³æ–‡çŒ®æ¶µç›–äº†ç§»åŠ¨è®¡ç®—ã€æœºå™¨äººæŠ€æœ¯å’Œæœºå™¨å­¦ä¹ ç­‰å¤šä¸ªé¢†åŸŸã€‚æœ¬æ–‡å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½åŠå…¶åœ¨è¡Œäººã€æ— äººæœºã€è½¦è¾†å’Œæœºå™¨äººçš„è·Ÿè¸ªåº”ç”¨ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬å°†ä¸åŒé¢†åŸŸçš„åŠªåŠ›è”ç³»èµ·æ¥ï¼Œè®¨è®ºæ·±åº¦å­¦ä¹ å¦‚ä½•åº”ç”¨äºè§£å†³ä¼ æ„Ÿå™¨æ ¡å‡†ã€å®šä½è¯¯å·®æ¼‚ç§»å‡å°‘å’Œå¤šä¼ æ„Ÿå™¨èåˆç­‰é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨å¸å¼•åŒ…æ‹¬ç ”ç©¶äººå‘˜å’Œå®è·µè€…åœ¨å†…çš„å„ç±»è¯»è€…ï¼Œç‰¹åˆ«æ˜¯å¯¹æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è§£å†³æƒ¯æ€§å®šä½é—®é¢˜æ–¹é¢çš„æ½œåŠ›æ„Ÿå…´è¶£çš„è¯»è€…ã€‚æˆ‘ä»¬çš„å›é¡¾å±•ç¤ºäº†æ·±åº¦å­¦ä¹ å¸¦æ¥çš„ä»¤äººå…´å¥‹çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸ºæœªæ¥åœ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶æä¾›äº†è·¯çº¿å›¾ã€‚
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç´¢å¼•è¯ï¼š
- en: Inertial Navigation, Deep Learning, Inertial Sensor Calibration, Pedestrian
    Dead Reckoning, Sensor Fusion, Visual-inertial Odometry
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ¯æ€§å¯¼èˆªï¼Œæ·±åº¦å­¦ä¹ ï¼Œæƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†ï¼Œè¡Œäººæ¨æµ‹ï¼Œä¼ æ„Ÿå™¨èåˆï¼Œè§†è§‰-æƒ¯æ€§é‡Œç¨‹è®¡
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I å¼•è¨€
- en: 'The inertial Measurement Unit (IMU) is widely used in smartphones, drones,
    VR/AR devices, robotics, and Internet of Things (IoT) devices. It continuously
    measures linear velocity and angular rate and tracks the motion of these platforms,
    as illustrated in Figure [1](#S1.F1 "Figure 1 â€£ I Introduction â€£ Deep Learning
    for Inertial Positioning: A Survey"). With the advancements in Micro-Electro-Mechanical
    Systems (MEMS) technology, todayâ€™s MEMS IMUs are small, energy-efficient, and
    cost-effective. Inertial positioning (navigation) calculates attitude, velocity,
    and position based on inertial measurements, making it a crucial element in various
    location-based applications, including locating and navigating individuals in
    public places (e.g., universities, malls, airports), supporting security and safety
    services (e.g., aiding first-responders), enabling smart city/infrastructure,
    and facilitating human-device interaction. Compared to other positioning solutions
    such as vision or radio, inertial positioning is completely ego-centric, works
    indoors and outdoors, and is less affected by environmental factors such as complex
    lighting conditions and scene dynamics.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'æƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆIMUï¼‰å¹¿æ³›åº”ç”¨äºæ™ºèƒ½æ‰‹æœºã€æ— äººæœºã€è™šæ‹Ÿç°å®/å¢å¼ºç°å®è®¾å¤‡ã€æœºå™¨äººå’Œç‰©è”ç½‘ï¼ˆIoTï¼‰è®¾å¤‡ã€‚å®ƒæŒç»­æµ‹é‡çº¿æ€§é€Ÿåº¦å’Œè§’é€Ÿåº¦ï¼Œå¹¶è·Ÿè¸ªè¿™äº›å¹³å°çš„è¿åŠ¨ï¼Œå¦‚å›¾
    [1](#S1.F1 "Figure 1 â€£ I Introduction â€£ Deep Learning for Inertial Positioning:
    A Survey") æ‰€ç¤ºã€‚éšç€å¾®ç”µæœºæ¢°ç³»ç»Ÿï¼ˆMEMSï¼‰æŠ€æœ¯çš„å‘å±•ï¼Œä»Šå¤©çš„MEMS IMUä½“ç§¯å°ã€èƒ½æ•ˆé«˜ã€æˆæœ¬ä½ã€‚æƒ¯æ€§å®šä½ï¼ˆå¯¼èˆªï¼‰åŸºäºæƒ¯æ€§æµ‹é‡è®¡ç®—å§¿æ€ã€é€Ÿåº¦å’Œä½ç½®ï¼Œä½¿å…¶åœ¨å„ç§åŸºäºä½ç½®çš„åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬åœ¨å…¬å…±åœºæ‰€ï¼ˆå¦‚å¤§å­¦ã€è´­ç‰©ä¸­å¿ƒã€æœºåœºï¼‰å®šä½å’Œå¯¼èˆªä¸ªäººã€æ”¯æŒå®‰å…¨æœåŠ¡ï¼ˆå¦‚ååŠ©æ€¥æ•‘äººå‘˜ï¼‰ã€å®ç°æ™ºèƒ½åŸå¸‚/åŸºç¡€è®¾æ–½ï¼Œä»¥åŠä¿ƒè¿›äººæœºäº¤äº’ã€‚ä¸å…¶ä»–å®šä½è§£å†³æ–¹æ¡ˆå¦‚è§†è§‰æˆ–æ— çº¿ç”µç›¸æ¯”ï¼Œæƒ¯æ€§å®šä½å®Œå…¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼Œèƒ½åœ¨å®¤å†…å’Œå®¤å¤–å·¥ä½œï¼Œå¹¶ä¸”ä¸æ˜“å—åˆ°å¤æ‚å…‰ç…§æ¡ä»¶å’Œåœºæ™¯åŠ¨æ€ç­‰ç¯å¢ƒå› ç´ çš„å½±å“ã€‚'
- en: '![Refer to caption](img/b9f237afb2f132524a9de20008c976e3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/b9f237afb2f132524a9de20008c976e3.png)'
- en: 'Figure 1: Inertial sensors are ubiquitous in modern platforms such as smartphones,
    drones, intelligent vehicles, and VR/AR devices. They play a critical role in
    enabling completely egocentric motion tracking and positioning, making them essential
    for a range of applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæƒ¯æ€§ä¼ æ„Ÿå™¨åœ¨ç°ä»£å¹³å°ä¸­æ— å¤„ä¸åœ¨ï¼Œå¦‚æ™ºèƒ½æ‰‹æœºã€æ— äººæœºã€æ™ºèƒ½è½¦è¾†å’Œè™šæ‹Ÿç°å®/å¢å¼ºç°å®è®¾å¤‡ã€‚å®ƒä»¬åœ¨å®ç°å®Œå…¨è‡ªæˆ‘ä¸­å¿ƒçš„è¿åŠ¨è·Ÿè¸ªå’Œå®šä½æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½¿å…¶åœ¨å„ç§åº”ç”¨ä¸­ä¸å¯æˆ–ç¼ºã€‚
- en: Unfortunately, the measurements obtained from low-cost MEMS IMUs are subject
    to several error sources such as bias error, temperature-dependent error, random
    sensor noise, and random-walk noise. In classical inertial navigation mechanisms,
    angular rates are integrated into orientation, and based on the acquired attitude,
    acceleration measurements are transformed into the navigation frame. Finally,
    the transformed accelerations are doubly integrated into locations [[1](#bib.bib1),
    [2](#bib.bib2)]. Traditional inertial navigation algorithms are designed and described
    using concrete physical and mathematical rules. Under ideal conditions, sensor
    errors are small enough to allow hand-designed inertial navigation algorithms
    to produce accurate and reliable pose estimates. However, in real-world applications,
    inevitable measurement errors cause significant problems for inertial positioning
    systems without constraints, which can fail within seconds. In this process, even
    a minor error can be amplified exponentially, resulting in unbounded error drifts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œä½æˆæœ¬MEMS IMUè·å¾—çš„æµ‹é‡å€¼å—åˆ°å¤šä¸ªè¯¯å·®æºçš„å½±å“ï¼Œå¦‚åç½®è¯¯å·®ã€æ¸©åº¦ä¾èµ–è¯¯å·®ã€éšæœºä¼ æ„Ÿå™¨å™ªå£°å’Œéšæœºæ¸¸èµ°å™ªå£°ã€‚åœ¨ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶ä¸­ï¼Œè§’é€Ÿåº¦è¢«ç§¯åˆ†ä¸ºæ–¹å‘ï¼Œå¹¶æ ¹æ®è·å¾—çš„å§¿æ€å°†åŠ é€Ÿåº¦æµ‹é‡è½¬æ¢ä¸ºå¯¼èˆªæ¡†æ¶ã€‚æœ€åï¼Œè½¬æ¢åçš„åŠ é€Ÿåº¦è¢«åŒé‡ç§¯åˆ†åˆ°ä½ç½®
    [[1](#bib.bib1), [2](#bib.bib2)]ã€‚ä¼ ç»Ÿçš„æƒ¯æ€§å¯¼èˆªç®—æ³•ä½¿ç”¨å…·ä½“çš„ç‰©ç†å’Œæ•°å­¦è§„åˆ™è¿›è¡Œè®¾è®¡å’Œæè¿°ã€‚åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œä¼ æ„Ÿå™¨è¯¯å·®è¶³å¤Ÿå°ï¼Œä½¿å¾—æ‰‹å·¥è®¾è®¡çš„æƒ¯æ€§å¯¼èˆªç®—æ³•èƒ½äº§ç”Ÿå‡†ç¡®å¯é çš„å§¿æ€ä¼°è®¡ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸å¯é¿å…çš„æµ‹é‡è¯¯å·®ä¼šå¯¼è‡´æƒ¯æ€§å®šä½ç³»ç»Ÿå‡ºç°é‡å¤§é—®é¢˜ï¼Œç³»ç»Ÿå¯èƒ½åœ¨å‡ ç§’é’Ÿå†…å¤±è´¥ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå³ä½¿æ˜¯å¾®å°çš„è¯¯å·®ä¹Ÿå¯èƒ½è¢«æŒ‡æ•°æ”¾å¤§ï¼Œå¯¼è‡´æ— ç•Œçš„è¯¯å·®æ¼‚ç§»ã€‚
- en: Previous researchers have attempted to address the problem of error drifts in
    inertial navigation by incorporating domain-specific knowledge or other sensor.
    In the context of pedestrian tracking, exploiting the periodicity of human walking
    is important, and the process of pedestrian dead reckoning (PDR) involves detecting
    steps, estimating step length and heading, and updating the userâ€™s location to
    mitigate error drifts from exponential to linear increase [[3](#bib.bib3)]. Zero-velocity
    update (ZUPT) involves attaching the IMU to the userâ€™s foot and detecting the
    zero-velocity phase, which is then used in Kalman filtering to correct inertial
    navigation states [[4](#bib.bib4)]. Platforms such as drones or robots equipped
    with other sensors such as cameras or LiDAR can significantly improve the performance
    of pure inertial solutions by effectively integrating inertial sensors with these
    modalities through filtering or smoothing [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)].
    However, these solutions have limitations in specific application domains and
    are unable to address the fundamental problem of inertial navigation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰çš„ç ”ç©¶è€…å°è¯•é€šè¿‡å¼•å…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†æˆ–å…¶ä»–ä¼ æ„Ÿå™¨æ¥è§£å†³æƒ¯æ€§å¯¼èˆªä¸­çš„è¯¯å·®æ¼‚ç§»é—®é¢˜ã€‚åœ¨è¡Œäººè·Ÿè¸ªçš„èƒŒæ™¯ä¸‹ï¼Œåˆ©ç”¨äººä½“æ­¥æ€çš„å‘¨æœŸæ€§æ˜¯é‡è¦çš„ï¼Œè¡Œäººè‡ªåŠ¨å®šä½ï¼ˆPDRï¼‰è¿‡ç¨‹åŒ…æ‹¬æ£€æµ‹æ­¥ä¼ã€ä¼°è®¡æ­¥é•¿å’Œæ–¹å‘ï¼Œå¹¶æ›´æ–°ç”¨æˆ·çš„ä½ç½®ï¼Œä»¥å°†è¯¯å·®æ¼‚ç§»ä»æŒ‡æ•°å¢é•¿å‡å°ä¸ºçº¿æ€§å¢é•¿[[3](#bib.bib3)]ã€‚é›¶é€Ÿåº¦æ›´æ–°ï¼ˆZUPTï¼‰æ¶‰åŠå°†IMUé™„ç€åˆ°ç”¨æˆ·çš„è„šä¸Šï¼Œå¹¶æ£€æµ‹é›¶é€Ÿåº¦é˜¶æ®µï¼Œç„¶ååœ¨å¡å°”æ›¼æ»¤æ³¢ä¸­ä½¿ç”¨è¿™ä¸€é˜¶æ®µæ¥æ ¡æ­£æƒ¯æ€§å¯¼èˆªçŠ¶æ€[[4](#bib.bib4)]ã€‚é…å¤‡å…¶ä»–ä¼ æ„Ÿå™¨å¦‚ç›¸æœºæˆ–LiDARçš„æ— äººæœºæˆ–æœºå™¨äººå¹³å°å¯ä»¥é€šè¿‡æœ‰æ•ˆåœ°å°†æƒ¯æ€§ä¼ æ„Ÿå™¨ä¸è¿™äº›æ¨¡å¼èåˆæ¥æ˜¾è‘—æå‡çº¯æƒ¯æ€§è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]ã€‚ç„¶è€Œï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆåœ¨ç‰¹å®šåº”ç”¨é¢†åŸŸå­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•è§£å†³æƒ¯æ€§å¯¼èˆªçš„æ ¹æœ¬é—®é¢˜ã€‚
- en: Recently, deep learning has shown impressive performance in various fields,
    including computer vision, robotics, and signal processing [[8](#bib.bib8)]. It
    has also been introduced to address the challenges of inertial positioning. Deep
    neural network models have been leveraged to calibrate inertial sensor noises,
    reduce the drifts of inertial navigation mechanisms, and fuse inertial data with
    other sensor information. These research works have attracted significant attention,
    as they show potential for exploiting massive data to generate data-driven models
    instead of relying on concrete physical or mathematical models. With the rapid
    development of deep learning techniques, learning-based inertial solutions have
    become even more promising.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººæŠ€æœ¯å’Œä¿¡å·å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½[[8](#bib.bib8)]ã€‚å®ƒä¹Ÿè¢«å¼•å…¥ä»¥åº”å¯¹æƒ¯æ€§å®šä½çš„æŒ‘æˆ˜ã€‚æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹è¢«ç”¨äºæ ¡å‡†æƒ¯æ€§ä¼ æ„Ÿå™¨å™ªå£°ï¼Œå‡å°‘æƒ¯æ€§å¯¼èˆªæœºåˆ¶çš„æ¼‚ç§»ï¼Œå¹¶å°†æƒ¯æ€§æ•°æ®ä¸å…¶ä»–ä¼ æ„Ÿå™¨ä¿¡æ¯èåˆã€‚è¿™äº›ç ”ç©¶å·¥ä½œå¸å¼•äº†å¤§é‡å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬å±•ç¤ºäº†åˆ©ç”¨æµ·é‡æ•°æ®ç”Ÿæˆæ•°æ®é©±åŠ¨æ¨¡å‹çš„æ½œåŠ›ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå…·ä½“çš„ç‰©ç†æˆ–æ•°å­¦æ¨¡å‹ã€‚éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¿…é€Ÿå‘å±•ï¼ŒåŸºäºå­¦ä¹ çš„æƒ¯æ€§è§£å†³æ–¹æ¡ˆå˜å¾—æ›´åŠ æœ‰å‰æ™¯ã€‚
- en: In this survey, we provide a comprehensive review of deep-learning-based approaches
    to inertial positioning, including measurement calibration, inertial positioning
    algorithms, and sensor fusion. We discuss the benefits and limitations of existing
    works and identify challenges and future opportunities in this research direction.
    Compared with other deep learning surveys, such as those focused on object detection
    [[9](#bib.bib9)], semantic segmentation [[10](#bib.bib10)], and robotics [[11](#bib.bib11)],
    survey on deep learning based inertial positioning is relatively scarce and hard
    to find. While a broader survey on machine learning enhanced inertial sensing
    does exist [[12](#bib.bib12)], our survey narrows the focus to deep learning based
    inertial positioning, providing deeper insights and analysis of the fast-evolving
    developments in this area over the past five years (2018-2022). Other relevant
    surveys, such as those focused on inertial pedestrian positioning [[3](#bib.bib3)],
    indoor positioning [[13](#bib.bib13)], step length estimation [[14](#bib.bib14)],
    and pedestrian dead reckoning [[15](#bib.bib15)], do not cover recent deep learning
    based solutions. To the best of our knowledge, this article is the first survey
    that discusses deep learning based inertial positioning thoroughly and deeply.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½æ–¹æ³•çš„å…¨é¢å›é¡¾ï¼ŒåŒ…æ‹¬æµ‹é‡æ ¡å‡†ã€æƒ¯æ€§å®šä½ç®—æ³•å’Œä¼ æ„Ÿå™¨èåˆã€‚æˆ‘ä»¬è®¨è®ºäº†ç°æœ‰å·¥ä½œçš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶è¯†åˆ«äº†è¯¥ç ”ç©¶æ–¹å‘ä¸­çš„æŒ‘æˆ˜å’Œæœªæ¥æœºä¼šã€‚ä¸å…¶ä»–æ·±åº¦å­¦ä¹ è°ƒæŸ¥ç›¸æ¯”ï¼Œä¾‹å¦‚é‚£äº›ä¸“æ³¨äºç‰©ä½“æ£€æµ‹[[9](#bib.bib9)]ã€è¯­ä¹‰åˆ†å‰²[[10](#bib.bib10)]å’Œæœºå™¨äººæŠ€æœ¯[[11](#bib.bib11)]çš„è°ƒæŸ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½è°ƒæŸ¥ç›¸å¯¹ç¨€ç¼ºä¸”éš¾ä»¥æ‰¾åˆ°ã€‚è™½ç„¶å­˜åœ¨æ›´å¹¿æ³›çš„æœºå™¨å­¦ä¹ å¢å¼ºæƒ¯æ€§ä¼ æ„Ÿçš„è°ƒæŸ¥[[12](#bib.bib12)]ï¼Œä½†æˆ‘ä»¬çš„è°ƒæŸ¥å°†é‡ç‚¹ç¼©å°åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½ï¼Œæä¾›äº†å¯¹è¿‡å»äº”å¹´ï¼ˆ2018-2022ï¼‰è¯¥é¢†åŸŸå¿«é€Ÿå‘å±•çš„æ·±å…¥è§è§£å’Œåˆ†æã€‚å…¶ä»–ç›¸å…³è°ƒæŸ¥ï¼Œå¦‚é‚£äº›ä¸“æ³¨äºæƒ¯æ€§æ­¥æ€å®šä½[[3](#bib.bib3)]ã€å®¤å†…å®šä½[[13](#bib.bib13)]ã€æ­¥é•¿ä¼°è®¡[[14](#bib.bib14)]å’Œæ­¥æ€æ¨æµ‹[[15](#bib.bib15)]çš„è°ƒæŸ¥ï¼Œå¹¶æœªæ¶µç›–æœ€æ–°çš„åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆã€‚æ ¹æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡æ˜¯ç¬¬ä¸€ä¸ªå½»åº•ä¸”æ·±å…¥è®¨è®ºåŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½çš„è°ƒæŸ¥ã€‚
- en: The rest of this survey is organized as follows. Section II briefly introduces
    classical inertial navigation mechanisms. Section III, IV and V survey deep learning
    based sensor calibration, inertial navigation algorithms, and sensor fusion. Section
    VII finally discusses the benefits, challenges and opportunities.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ¬¡è°ƒæŸ¥çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ï¼šç¬¬IIèŠ‚ç®€è¦ä»‹ç»äº†ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶ã€‚ç¬¬IIIã€IVå’ŒVèŠ‚è°ƒæŸ¥äº†åŸºäºæ·±åº¦å­¦ä¹ çš„ä¼ æ„Ÿå™¨æ ¡å‡†ã€æƒ¯æ€§å¯¼èˆªç®—æ³•å’Œä¼ æ„Ÿå™¨èåˆã€‚ç¬¬VIIèŠ‚æœ€åè®¨è®ºäº†ç›Šå¤„ã€æŒ‘æˆ˜å’Œæœºä¼šã€‚
- en: II Classical Inertial Navigation Mechanisms
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶
- en: This section provides an overview of classical inertial navigation mechanisms
    and highlights their limitations. It begins by presenting the inertial measurement
    model and classical strapdown inertial navigation method. Subsequently, two solutions
    that aim to reduce the drifts of inertial navigation system, namely pedestrian
    dead reckoning (PDR) and zero-velocity update (ZUPT), are discussed, with a specific
    focus on their applicability in pedestrian tracking scenarios. The section finally
    introduces sensor fusion approaches that integrate inertial data with information
    from other sensors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æä¾›äº†ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶çš„æ¦‚è¿°ï¼Œå¹¶çªå‡ºå…¶å±€é™æ€§ã€‚å®ƒé¦–å…ˆä»‹ç»äº†æƒ¯æ€§æµ‹é‡æ¨¡å‹å’Œç»å…¸çš„å¸¦å¼æƒ¯æ€§å¯¼èˆªæ–¹æ³•ã€‚éšåï¼Œè®¨è®ºäº†æ—¨åœ¨å‡å°‘æƒ¯æ€§å¯¼èˆªç³»ç»Ÿæ¼‚ç§»çš„ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼Œå³æ­¥æ€æ¨æµ‹ï¼ˆPDRï¼‰å’Œé›¶é€Ÿæ›´æ–°ï¼ˆZUPTï¼‰ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨å®ƒä»¬åœ¨æ­¥æ€è·Ÿè¸ªåœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚æœ€åï¼Œæœ¬èŠ‚ä»‹ç»äº†å°†æƒ¯æ€§æ•°æ®ä¸å…¶ä»–ä¼ æ„Ÿå™¨ä¿¡æ¯é›†æˆçš„ä¼ æ„Ÿå™¨èåˆæ–¹æ³•ã€‚
- en: II-A Inertial Measurement Model
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A æƒ¯æ€§æµ‹é‡æ¨¡å‹
- en: 'Inertial measurements acquired from low-cost MEMS IMUs are often corrupted
    by various types of error sources, resulting in unbounded error drifts when integrated
    in strapdown inertial navigation systems (SINS). These error sources can be classified
    into two categories: deterministic errors and random errors [[16](#bib.bib16)].
    Deterministic errors comprise bias error, non-orthogonality error, misalignment
    error, scale-factor error, and temperature-dependent error. On the other hand,
    random errors include random sensor noise and random-walk noise resulting from
    long-term operation, which are challenging to model and eliminate.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä½æˆæœ¬MEMS IMUè·å¾—çš„æƒ¯æ€§æµ‹é‡æ•°æ®é€šå¸¸å—åˆ°å„ç§é”™è¯¯æºçš„å½±å“ï¼Œå¯¼è‡´åœ¨å¸¦å¼æƒ¯æ€§å¯¼èˆªç³»ç»Ÿï¼ˆSINSï¼‰ä¸­ç§¯åˆ†æ—¶å‡ºç°ä¸å—é™çš„è¯¯å·®æ¼‚ç§»ã€‚è¿™äº›é”™è¯¯æºå¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šç¡®å®šæ€§è¯¯å·®å’Œéšæœºè¯¯å·®[[16](#bib.bib16)]ã€‚ç¡®å®šæ€§è¯¯å·®åŒ…æ‹¬åå·®è¯¯å·®ã€éæ­£äº¤è¯¯å·®ã€å¯¹å‡†è¯¯å·®ã€å°ºåº¦å› å­è¯¯å·®å’Œæ¸©åº¦ä¾èµ–è¯¯å·®ã€‚å¦ä¸€æ–¹é¢ï¼Œéšæœºè¯¯å·®åŒ…æ‹¬éšæœºä¼ æ„Ÿå™¨å™ªå£°å’Œç”±äºé•¿æœŸæ“ä½œäº§ç”Ÿçš„éšæœºæ¸¸èµ°å™ªå£°ï¼Œè¿™äº›å™ªå£°å¾ˆéš¾å»ºæ¨¡å’Œæ¶ˆé™¤ã€‚
- en: Raw IMU measurements, i.e. accelerations $\hat{\mathbf{a}}$ and angular rates
    $\hat{\bm{\omega}}$, can be formulated by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹IMUæµ‹é‡å€¼ï¼Œå³åŠ é€Ÿåº¦$\hat{\mathbf{a}}$å’Œè§’é€Ÿåº¦$\hat{\bm{\omega}}$ï¼Œå¯ä»¥è¡¨ç¤ºä¸º
- en: '|  | $\hat{\mathbf{a}}=\mathbf{a}+\mathbf{b}_{a}+\mathbf{n}_{a}$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{a}}=\mathbf{a}+\mathbf{b}_{a}+\mathbf{n}_{a}$ |  | (1) |'
- en: '|  | $\hat{\bm{\omega}}=\bm{\omega}+\mathbf{b}_{\omega}+\mathbf{n}_{\omega}$
    |  | (2) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\bm{\omega}}=\bm{\omega}+\mathbf{b}_{\omega}+\mathbf{n}_{\omega}$
    |  | (2) |'
- en: where $\mathbf{b}_{a}$ and $\mathbf{b}_{\omega}$ are acceleration bias and gyroscope
    bias, $\mathbf{n}_{a}$ and $\mathbf{n}_{\omega}$ are additive noises above accelerometer
    and gyroscope.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\mathbf{b}_{a}$å’Œ$\mathbf{b}_{\omega}$åˆ†åˆ«æ˜¯åŠ é€Ÿåº¦åå·®å’Œé™€èºä»ªåå·®ï¼Œ$\mathbf{n}_{a}$å’Œ$\mathbf{n}_{\omega}$æ˜¯åŠ é€Ÿåº¦è®¡å’Œé™€èºä»ªä¸Šçš„åŠ æ€§å™ªå£°ã€‚
- en: Traditionally, it is important to calibrate inertial sensors before running
    an inertial navigation algorithm that involves integrating inertial data into
    system states. One effective tool for achieving this is the Allan variance method
    [[17](#bib.bib17)], which models the random process of inertial sensor errors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šï¼Œåœ¨è¿è¡Œæ¶‰åŠå°†æƒ¯æ€§æ•°æ®é›†æˆåˆ°ç³»ç»ŸçŠ¶æ€ä¸­çš„æƒ¯æ€§å¯¼èˆªç®—æ³•ä¹‹å‰ï¼Œå¯¹æƒ¯æ€§ä¼ æ„Ÿå™¨è¿›è¡Œæ ¡å‡†éå¸¸é‡è¦ã€‚å®ç°è¿™ä¸€ç›®æ ‡çš„ä¸€ä¸ªæœ‰æ•ˆå·¥å…·æ˜¯Allanæ–¹å·®æ–¹æ³•[[17](#bib.bib17)]ï¼Œè¯¥æ–¹æ³•å¯¹æƒ¯æ€§ä¼ æ„Ÿå™¨è¯¯å·®çš„éšæœºè¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ã€‚
- en: II-B Strapdown Inertial Navigation System
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B å›ºå®šæƒ¯æ€§å¯¼èˆªç³»ç»Ÿ
- en: 'Inertial sensor measures linear accelerations $\mathbf{a}_{b}(t)$ and angular
    rates $\bm{\omega}_{b}^{n}(t)$ of attached user body at the timestep $t$. $b$
    represents the body frame, while $n$ denotes the navigation (world) frame, i.e.
    the navigation frame. $\bm{\omega}_{b}^{n}(t)$ means that the angular rates of
    body frame with respect to the navigation frame. To simplify inertial motion model,
    this article assumes that the biases and noises of sensor in Equation [1](#S2.E1
    "In II-A Inertial Measurement Model â€£ II Classical Inertial Navigation Mechanisms
    â€£ Deep Learning for Inertial Positioning: A Survey") and [2](#S2.E2 "In II-A Inertial
    Measurement Model â€£ II Classical Inertial Navigation Mechanisms â€£ Deep Learning
    for Inertial Positioning: A Survey") have been removed in the stage of inertial
    sensor calibration. $(\mathbf{R},\mathbf{p})$ are defined orientation and position
    variables. From the kinematic model of IMU, we can have'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'æƒ¯æ€§ä¼ æ„Ÿå™¨æµ‹é‡é™„åŠ ç”¨æˆ·èº«ä½“åœ¨æ—¶é—´æ­¥é•¿$t$ä¸Šçš„çº¿æ€§åŠ é€Ÿåº¦$\mathbf{a}_{b}(t)$å’Œè§’é€Ÿåº¦$\bm{\omega}_{b}^{n}(t)$ã€‚$b$è¡¨ç¤ºèº«ä½“åæ ‡ç³»ï¼Œè€Œ$n$è¡¨ç¤ºå¯¼èˆªï¼ˆä¸–ç•Œï¼‰åæ ‡ç³»ï¼Œå³å¯¼èˆªåæ ‡ç³»ã€‚$\bm{\omega}_{b}^{n}(t)$è¡¨ç¤ºèº«ä½“åæ ‡ç³»ç›¸å¯¹äºå¯¼èˆªåæ ‡ç³»çš„è§’é€Ÿåº¦ã€‚ä¸ºäº†ç®€åŒ–æƒ¯æ€§è¿åŠ¨æ¨¡å‹ï¼Œæœ¬æ–‡å‡è®¾ä¼ æ„Ÿå™¨åœ¨æ–¹ç¨‹[1](#S2.E1
    "In II-A Inertial Measurement Model â€£ II Classical Inertial Navigation Mechanisms
    â€£ Deep Learning for Inertial Positioning: A Survey")å’Œ[2](#S2.E2 "In II-A Inertial
    Measurement Model â€£ II Classical Inertial Navigation Mechanisms â€£ Deep Learning
    for Inertial Positioning: A Survey")ä¸­çš„åå·®å’Œå™ªå£°å·²åœ¨æƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†é˜¶æ®µè¢«å»é™¤ã€‚$(\mathbf{R},\mathbf{p})$æ˜¯å®šä¹‰çš„æ–¹å‘å’Œä½ç½®å˜é‡ã€‚æ ¹æ®IMUçš„è¿åŠ¨å­¦æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°'
- en: '|  | <math   alttext="\begin{cases}\mathbf{R}_{b}^{n}(t+1)=\mathbf{R}_{b}^{n}(t)\mathbf{R}_{b_{t+1}}^{b_{t}}\\
    \mathbf{v}_{n}(t+1)=\mathbf{v}_{n}(t)+\mathbf{a}_{n}(t)dt\\'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{cases}\mathbf{R}_{b}^{n}(t+1)=\mathbf{R}_{b}^{n}(t)\mathbf{R}_{b_{t+1}}^{b_{t}}\\
    \mathbf{v}_{n}(t+1)=\mathbf{v}_{n}(t)+\mathbf{a}_{n}(t)dt\\'
- en: \mathbf{p}_{n}(t+1)=\mathbf{p}_{n}(t)+\mathbf{v}_{n}(t)dt+\frac{1}{2}\mathbf{a}_{n}(t)dt^{2}\end{cases}"
    display="block"><semantics ><mrow  ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd  columnalign="left" ><mrow ><mrow  ><msubsup ><mi
    >ğ‘</mi><mi >b</mi><mi >n</mi></msubsup><mo lspace="0em" rspace="0em" >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow ><msubsup  ><mi >ğ‘</mi><mi
    >b</mi><mi >n</mi></msubsup><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo lspace="0em" rspace="0em"  >â€‹</mo><msubsup
    ><mi >ğ‘</mi><msub ><mi >b</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><msub
    ><mi  >b</mi><mi >t</mi></msub></msubsup></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow  ><msub ><mi >ğ¯</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow ><mrow  ><msub ><mi >ğ¯</mi><mi
    >n</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >+</mo><mrow ><msub
    ><mi  >ğš</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo
    stretchy="false" >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >t</mi></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow  ><msub ><mi >ğ©</mi><mi >n</mi></msub><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi
    >t</mi><mo >+</mo><mn >1</mn></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mrow  ><msub ><mi >ğ©</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >+</mo><mrow ><msub ><mi  >ğ¯</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >t</mi></mrow><mo >+</mo><mrow ><mstyle displaystyle="false" ><mfrac ><mn >1</mn><mn
    >2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >ğš</mi><mi
    >n</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><mi >t</mi><mo stretchy="false" >)</mo></mrow><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><msup ><mi >t</mi><mn >2</mn></msup></mrow></mrow></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="latexml"  >cases</csymbol><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘</ci></apply><ci >ğ‘›</ci></apply><apply
    ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘</ci></apply><ci >ğ‘›</ci></apply><ci >ğ‘¡</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><apply ><ci
    >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘¡</ci></apply></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ¯</ci><ci >ğ‘›</ci></apply><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ¯</ci><ci
    >ğ‘›</ci></apply><ci >ğ‘¡</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğš</ci><ci >ğ‘›</ci></apply><ci >ğ‘¡</ci><ci  >ğ‘‘</ci><ci >ğ‘¡</ci></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ©</ci><ci >ğ‘›</ci></apply><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ©</ci><ci
    >ğ‘›</ci></apply><ci >ğ‘¡</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ¯</ci><ci >ğ‘›</ci></apply><ci >ğ‘¡</ci><ci  >ğ‘‘</ci><ci >ğ‘¡</ci></apply><apply ><apply
    ><cn type="integer" >1</cn><cn type="integer" >2</cn></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğš</ci><ci >ğ‘›</ci></apply><ci >ğ‘¡</ci><ci  >ğ‘‘</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ‘¡</ci><cn type="integer"  >2</cn></apply></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{cases}\mathbf{R}_{b}^{n}(t+1)=\mathbf{R}_{b}^{n}(t)\mathbf{R}_{b_{t+1}}^{b_{t}}\\
    \mathbf{v}_{n}(t+1)=\mathbf{v}_{n}(t)+\mathbf{a}_{n}(t)dt\\ \mathbf{p}_{n}(t+1)=\mathbf{p}_{n}(t)+\mathbf{v}_{n}(t)dt+\frac{1}{2}\mathbf{a}_{n}(t)dt^{2}\end{cases}</annotation></semantics></math>
    |  | (3) |
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbf{a}_{n}$, $\mathbf{v}_{n}$, $\mathbf{p}_{n}$ are acceleration,
    velocity and position in the navigation frame, $\mathbf{R}_{b}^{n}$ represents
    the rotation from the body frame to the navigation frame.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\mathbf{a}_{n}$ã€$\mathbf{v}_{n}$ã€$\mathbf{p}_{n}$åˆ†åˆ«æ˜¯å¯¼èˆªæ¡†æ¶ä¸­çš„åŠ é€Ÿåº¦ã€é€Ÿåº¦å’Œä½ç½®ï¼Œ$\mathbf{R}_{b}^{n}$è¡¨ç¤ºä»æœºä½“æ¡†æ¶åˆ°å¯¼èˆªæ¡†æ¶çš„æ—‹è½¬ã€‚
- en: 'Firstly, orientation is updated by inferring the rotation matrix $\bm{\Omega(t)}$
    via Rodriguez formula:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œé€šè¿‡ç½—å¾·é‡Œæ ¼æ–¯å…¬å¼æ¨æ–­æ—‹è½¬çŸ©é˜µ$\bm{\Omega(t)}$æ¥æ›´æ–°æ–¹å‘ï¼š
- en: '|  | $\begin{split}\bm{\Omega}(t)&amp;=\mathbf{R}_{b_{t+1}}^{b_{t}}\\ &amp;=\mathbf{I}+\sin(\bm{\sigma})\frac{[\bm{\sigma}\times]}{\bm{\sigma}}+(1-\cos(\bm{\sigma}))\frac{[\bm{\sigma}\times]^{2}}{\bm{\sigma}^{2}},\end{split}$
    |  | (4) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\bm{\Omega}(t)&amp;=\mathbf{R}_{b_{t+1}}^{b_{t}}\\ &amp;=\mathbf{I}+\sin(\bm{\sigma})\frac{[\bm{\sigma}\times]}{\bm{\sigma}}+(1-\cos(\bm{\sigma}))\frac{[\bm{\sigma}\times]^{2}}{\bm{\sigma}^{2}},\end{split}$
    |  | (4) |'
- en: where rotation vector $\bm{\sigma}=\bm{\omega}(t)dt$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ—‹è½¬å‘é‡$\bm{\sigma}=\bm{\omega}(t)dt$ã€‚
- en: To update velocity, the accelerations in navigation frame can be expressed as
    a function of measured accelerations, i.e.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ›´æ–°é€Ÿåº¦ï¼Œå¯¼èˆªæ¡†æ¶ä¸­çš„åŠ é€Ÿåº¦å¯ä»¥è¡¨ç¤ºä¸ºæµ‹é‡åŠ é€Ÿåº¦çš„å‡½æ•°ï¼Œå³ï¼š
- en: '|  | $\mathbf{a}_{n}(t)=\mathbf{R}_{b}^{n}(t-1)\mathbf{a}_{b}(t)-\mathbf{g}_{n}$
    |  | (5) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{a}_{n}(t)=\mathbf{R}_{b}^{n}(t-1)\mathbf{a}_{b}(t)-\mathbf{g}_{n}$
    |  | (5) |'
- en: 'Then, the accelerations in navigation frame $\mathbf{a}_{n}(t)$ are integrated
    into the velocity in the navigation frame $\mathbf{v}_{n}(t)$, and the location
    $\mathbf{p}_{n}(t)$ is finally updated by integrating the velocity via Equation
    [4](#S2.E4 "In II-B Strapdown Inertial Navigation System â€£ II Classical Inertial
    Navigation Mechanisms â€£ Deep Learning for Inertial Positioning: A Survey").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå°†å¯¼èˆªæ¡†æ¶ä¸­çš„åŠ é€Ÿåº¦$\mathbf{a}_{n}(t)$ç§¯åˆ†åˆ°å¯¼èˆªæ¡†æ¶ä¸­çš„é€Ÿåº¦$\mathbf{v}_{n}(t)$ä¸­ï¼Œæœ€åé€šè¿‡æ–¹ç¨‹[4](#S2.E4
    "åœ¨ II-B Strapdown æƒ¯æ€§å¯¼èˆªç³»ç»Ÿ â€£ II ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶ â€£ æƒ¯æ€§å®šä½çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°")é€šè¿‡ç§¯åˆ†é€Ÿåº¦æ¥æ›´æ–°ä½ç½®$\mathbf{p}_{n}(t)$ã€‚
- en: As we can see, in this process, even a small measurement error can be exponentially
    amplified, leading to the problem of inertial error drifts. In the past, high-precision
    inertial sensors such as laser or fiber inertial sensors could keep the measurement
    error small enough to maintain the accuracy of INS. However, due to the size and
    cost limitations of current MEMS IMUs, compensation methods are necessary to mitigate
    the corresponding error drifts. One approach is to introduce domain-specific knowledge
    or other sensor information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå³ä½¿æ˜¯å¾ˆå°çš„æµ‹é‡è¯¯å·®ä¹Ÿå¯èƒ½ä¼šè¢«æŒ‡æ•°çº§æ”¾å¤§ï¼Œå¯¼è‡´æƒ¯æ€§è¯¯å·®æ¼‚ç§»çš„é—®é¢˜ã€‚è¿‡å»ï¼Œé«˜ç²¾åº¦æƒ¯æ€§ä¼ æ„Ÿå™¨å¦‚æ¿€å…‰æˆ–å…‰çº¤æƒ¯æ€§ä¼ æ„Ÿå™¨èƒ½å¤Ÿä¿æŒæµ‹é‡è¯¯å·®è¶³å¤Ÿå°ï¼Œä»è€Œç»´æŒINSçš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºå½“å‰MEMS
    IMUçš„å°ºå¯¸å’Œæˆæœ¬é™åˆ¶ï¼Œè¡¥å¿æ–¹æ³•å˜å¾—å¿…è¦ï¼Œä»¥å‡è½»ç›¸åº”çš„è¯¯å·®æ¼‚ç§»ã€‚ä¸€ç§æ–¹æ³•æ˜¯å¼•å…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†æˆ–å…¶ä»–ä¼ æ„Ÿå™¨ä¿¡æ¯ã€‚
- en: II-C Domain Specific Knowledge
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C é¢†åŸŸç‰¹å®šçŸ¥è¯†
- en: II-C1 Pedestrian Dead Reckoning
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 æ­¥æ€æ­»è®¡ç®—
- en: 'Pedestrian dead reckoning (PDR) is a method that leverages domain-specific
    knowledge about human walking to track pedestrian motion. PDR comprises three
    main steps: step detection, heading and stride length estimation, and location
    update [[3](#bib.bib3)]. In step detection, PDR uses the threshold of inertial
    data to identify step peaks or stances and segment the corresponding inertial
    data. Dynamic stride length estimation is then achieved via an empirical formula,
    known as the Weinberg formula [[18](#bib.bib18)], which considers the segmented
    accelerations and userâ€™s height. Similar to SINS, heading estimation is done by
    integrating gyroscope signals into orientation changes and adding orientation
    changes to the initial orientation to obtain the current heading. Finally, the
    estimated heading and stride length are used to update the pedestrianâ€™s location.
    By avoiding double integration of accelerations and incorporating a reliable stride
    estimation model, PDR effectively reduces inertial positioning drifts. However,
    inaccurate step detection and stride estimation can still occur, leading to large
    system error drifts. Moreover, PDR is limited to pedestrian navigation as it depends
    on the periodicity of human walking.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œäººæ­»ç®—ï¼ˆPDRï¼‰æ˜¯ä¸€ç§åˆ©ç”¨æœ‰å…³äººä½“æ­¥æ€çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†æ¥è·Ÿè¸ªè¡Œäººè¿åŠ¨çš„æ–¹æ³•ã€‚PDRåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šæ­¥æ€æ£€æµ‹ã€æ–¹å‘å’Œæ­¥å¹…ä¼°è®¡ï¼Œä»¥åŠä½ç½®æ›´æ–°[[3](#bib.bib3)]ã€‚åœ¨æ­¥æ€æ£€æµ‹ä¸­ï¼ŒPDRä½¿ç”¨æƒ¯æ€§æ•°æ®çš„é˜ˆå€¼æ¥è¯†åˆ«æ­¥æ€å³°å€¼æˆ–å§¿æ€ï¼Œå¹¶åˆ†å‰²ç›¸åº”çš„æƒ¯æ€§æ•°æ®ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªç»éªŒå…¬å¼ï¼ˆç§°ä¸ºWeinbergå…¬å¼[[18](#bib.bib18)]ï¼‰åŠ¨æ€ä¼°è®¡æ­¥å¹…ï¼Œè¯¥å…¬å¼è€ƒè™‘äº†åˆ†å‰²çš„åŠ é€Ÿåº¦å’Œç”¨æˆ·çš„èº«é«˜ã€‚ä¸SINSç±»ä¼¼ï¼Œæ–¹å‘ä¼°è®¡æ˜¯é€šè¿‡å°†é™€èºä»ªä¿¡å·ç§¯åˆ†åˆ°æ–¹å‘å˜åŒ–ä¸­ï¼Œå¹¶å°†æ–¹å‘å˜åŒ–åŠ åˆ°åˆå§‹æ–¹å‘ä¸Šä»¥è·å¾—å½“å‰æ–¹å‘ã€‚æœ€åï¼Œä¼°è®¡çš„æ–¹å‘å’Œæ­¥å¹…ç”¨äºæ›´æ–°è¡Œäººçš„ä½ç½®ã€‚é€šè¿‡é¿å…åŠ é€Ÿåº¦çš„åŒé‡ç§¯åˆ†å¹¶ç»“åˆå¯é çš„æ­¥å¹…ä¼°è®¡æ¨¡å‹ï¼ŒPDRæœ‰æ•ˆåœ°å‡å°‘äº†æƒ¯æ€§å®šä½æ¼‚ç§»ã€‚ç„¶è€Œï¼Œä¸å‡†ç¡®çš„æ­¥æ€æ£€æµ‹å’Œæ­¥å¹…ä¼°è®¡ä»ç„¶å¯èƒ½å‘ç”Ÿï¼Œä»è€Œå¯¼è‡´ç³»ç»Ÿè¯¯å·®æ¼‚ç§»ã€‚æ­¤å¤–ï¼Œç”±äºPDRä¾èµ–äºäººä½“æ­¥æ€çš„å‘¨æœŸæ€§ï¼Œå› æ­¤å®ƒä»…é™äºæ­¥è¡Œå¯¼èˆªã€‚
- en: II-C2 Zero-velocity Update
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 é›¶é€Ÿåº¦æ›´æ–°
- en: The Zero-velocity update (ZUPT) algorithm is designed to compensate for the
    errors of SINS by identifying the still phase of human walking and using zero-velocity
    as observations in a Kalman filter [[4](#bib.bib4)]. To facilitate the detection
    of the still phase, the IMU is typically attached to the userâ€™s foot, as it undergoes
    significant motion and reflects walking patterns well. Techniques such as peak-detection
    [[19](#bib.bib19)], zero-crossings [[20](#bib.bib20)], or auto-correlation [[21](#bib.bib21)]
    can be used to analyze the inertial data and segment the zero-velocity phase.
    Once the still phase is detected, zero-velocity is used as pseudo-measurements
    in the filtering process, thereby limiting the error drifts of open-loop integration.
    However, the effectiveness of ZUPT depends on the assumption that the userâ€™s foot
    remains completely still, and any incorrect still phase detection or small motion
    disturbances can cause navigation system drifts. Additionally, ZUPT is limited
    to pedestrian tracking.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: é›¶é€Ÿåº¦æ›´æ–°ï¼ˆZUPTï¼‰ç®—æ³•æ—¨åœ¨é€šè¿‡è¯†åˆ«äººä½“æ­¥æ€çš„é™æ­¢é˜¶æ®µå¹¶ä½¿ç”¨é›¶é€Ÿåº¦ä½œä¸ºå¡å°”æ›¼æ»¤æ³¢å™¨ä¸­çš„è§‚æµ‹å€¼æ¥è¡¥å¿SINSçš„è¯¯å·®[[4](#bib.bib4)]ã€‚ä¸ºäº†æ–¹ä¾¿æ£€æµ‹é™æ­¢é˜¶æ®µï¼ŒIMUé€šå¸¸è¢«å®‰è£…åœ¨ç”¨æˆ·çš„è„šä¸Šï¼Œå› ä¸ºè„šéƒ¨è¿åŠ¨æ˜¾è‘—å¹¶èƒ½å¾ˆå¥½åœ°åæ˜ æ­¥æ€æ¨¡å¼ã€‚å¯ä»¥ä½¿ç”¨å³°å€¼æ£€æµ‹[[19](#bib.bib19)]ã€é›¶äº¤å‰[[20](#bib.bib20)]æˆ–è‡ªç›¸å…³[[21](#bib.bib21)]ç­‰æŠ€æœ¯æ¥åˆ†ææƒ¯æ€§æ•°æ®å¹¶åˆ†å‰²é›¶é€Ÿåº¦é˜¶æ®µã€‚ä¸€æ—¦æ£€æµ‹åˆ°é™æ­¢é˜¶æ®µï¼Œé›¶é€Ÿåº¦å°†ä½œä¸ºä¼ªæµ‹é‡å€¼ç”¨äºè¿‡æ»¤è¿‡ç¨‹ï¼Œä»è€Œé™åˆ¶å¼€ç¯ç§¯åˆ†çš„è¯¯å·®æ¼‚ç§»ã€‚ç„¶è€Œï¼ŒZUPTçš„æœ‰æ•ˆæ€§ä¾èµ–äºç”¨æˆ·çš„è„šå®Œå…¨é™æ­¢çš„å‡è®¾ï¼Œä»»ä½•ä¸æ­£ç¡®çš„é™æ­¢é˜¶æ®µæ£€æµ‹æˆ–å°çš„è¿åŠ¨å¹²æ‰°éƒ½å¯èƒ½å¯¼è‡´å¯¼èˆªç³»ç»Ÿçš„æ¼‚ç§»ã€‚æ­¤å¤–ï¼ŒZUPTä»…é™äºæ­¥è¡Œè·Ÿè¸ªã€‚
- en: II-D Integrating IMU with Other Sensors
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D IMUä¸å…¶ä»–ä¼ æ„Ÿå™¨çš„é›†æˆ
- en: Integrating the IMU with other sensors, such as camera [[6](#bib.bib6)], LiDAR
    [[7](#bib.bib7)], UWB [[22](#bib.bib22)], and magnetometer [[23](#bib.bib23)],
    can provide promising results as it allows for exploiting their complementary
    properties. By fusing the data from multiple sensors, the accuracy and robustness
    of pose estimation can be significantly improved, making it a general solution
    for all platforms. However, in some scenarios, certain sensors, such as visual
    perception, may not be available or highly dependent on the environment, which
    can negatively affect the egocentric property of inertial positioning. Additionally,
    in sensor fusion approaches, it is essential to consider various factors such
    as sensor calibration, initialization, and time-synchronization.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å°†IMUä¸å…¶ä»–ä¼ æ„Ÿå™¨ï¼ˆå¦‚ç›¸æœº [[6](#bib.bib6)]ã€æ¿€å…‰é›·è¾¾ [[7](#bib.bib7)]ã€è¶…å®½å¸¦ [[22](#bib.bib22)]
    å’Œç£åŠ›è®¡ [[23](#bib.bib23)]ï¼‰é›†æˆï¼Œå¯ä»¥è·å¾—æœ‰å¸Œæœ›çš„ç»“æœï¼Œå› ä¸ºè¿™å…è®¸åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ç‰¹æ€§ã€‚é€šè¿‡èåˆå¤šä¸ªä¼ æ„Ÿå™¨çš„æ•°æ®ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä½¿å…¶æˆä¸ºæ‰€æœ‰å¹³å°çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨æŸäº›åœºæ™¯ä¸­ï¼ŒæŸäº›ä¼ æ„Ÿå™¨ï¼ˆå¦‚è§†è§‰æ„ŸçŸ¥ï¼‰å¯èƒ½ä¸å¯ç”¨æˆ–é«˜åº¦ä¾èµ–ç¯å¢ƒï¼Œè¿™å¯èƒ½ä¼šå¯¹æƒ¯æ€§å®šä½çš„è‡ªæˆ‘ä¸­å¿ƒå±æ€§äº§ç”Ÿè´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼Œåœ¨ä¼ æ„Ÿå™¨èåˆæ–¹æ³•ä¸­ï¼Œè€ƒè™‘å„ç§å› ç´ å¦‚ä¼ æ„Ÿå™¨æ ‡å®šã€åˆå§‹åŒ–å’Œæ—¶é—´åŒæ­¥æ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: 'TABLE I: A summary of existing methods on deep learning based inertial sensor
    calibration.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ I: åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§ä¼ æ„Ÿå™¨æ ‡å®šç°æœ‰æ–¹æ³•çš„æ±‡æ€»ã€‚'
- en: '| name | year | sensor | model | learning | target |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | å¹´ä»½ | ä¼ æ„Ÿå™¨ | æ¨¡å‹ | å­¦ä¹  | ç›®æ ‡ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Xiyuan et al.[[24](#bib.bib24)] | 2003 | gyro | 1-layer NN | SL | gyro drifts
    compensation |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Xiyuan et al.[[24](#bib.bib24)] | 2003 | é™€èºä»ª | å•å±‚ç¥ç»ç½‘ç»œ | ç›‘ç£å­¦ä¹  | é™€èºä»ªæ¼‚ç§»è¡¥å¿ |'
- en: '| Chen et al. [[25](#bib.bib25)] | 2018 | gyro, acc | ConvNet | SL | inertial
    noise compensation |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. [[25](#bib.bib25)] | 2018 | é™€èºä»ªã€åŠ é€Ÿåº¦è®¡ | å·ç§¯ç½‘ç»œ | ç›‘ç£å­¦ä¹  | æƒ¯æ€§å™ªå£°è¡¥å¿ |'
- en: '| Esfahani et al. [[26](#bib.bib26)] | 2019 | gyro | LSTM | SL | gyroscope
    calibration |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Esfahani et al. [[26](#bib.bib26)] | 2019 | é™€èºä»ª | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ | ç›‘ç£å­¦ä¹  | é™€èºä»ªæ ‡å®š
    |'
- en: '| Nobre et al. [[27](#bib.bib27)] | 2019 | gyro, acc | Deep Q-Network | RL
    | optimal calibration parameters |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Nobre et al. [[27](#bib.bib27)] | 2019 | é™€èºä»ªã€åŠ é€Ÿåº¦è®¡ | æ·±åº¦ Q ç½‘ç»œ | å¼ºåŒ–å­¦ä¹  | æœ€ä¼˜æ ‡å®šå‚æ•°
    |'
- en: '| Brossard et al. [[28](#bib.bib28)] | 2020 | gyro | ConvNet | SL | gyro corrections
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Brossard et al. [[28](#bib.bib28)] | 2020 | é™€èºä»ª | å·ç§¯ç½‘ç»œ | ç›‘ç£å­¦ä¹  | é™€èºä»ªä¿®æ­£ |'
- en: '| Zhao et al.[[29](#bib.bib29)] | 2020 | gyro | LSTM | SL | gyroscope calibration
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al.[[29](#bib.bib29)] | 2020 | é™€èºä»ª | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ | ç›‘ç£å­¦ä¹  | é™€èºä»ªæ ‡å®š |'
- en: '| Huang et al.[[30](#bib.bib30)] | 2022 | gyro | Temporal ConvNet | SL | gyroscope
    calibration |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al.[[30](#bib.bib30)] | 2022 | é™€èºä»ª | æ—¶åºå·ç§¯ç½‘ç»œ | ç›‘ç£å­¦ä¹  | é™€èºä»ªæ ‡å®š |'
- en: '| Calib-Net [[31](#bib.bib31)] | 2022 | gyro | Dilated ConvNet | SL | gyroscope
    denoising |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Calib-Net [[31](#bib.bib31)] | 2022 | é™€èºä»ª | æ‰©å¼ å·ç§¯ç½‘ç»œ | ç›‘ç£å­¦ä¹  | é™€èºä»ªå»å™ª |'
- en: â€¢
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Years indicates the publication year of each work.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¹´ä»½æŒ‡ç¤ºæ¯é¡¹å·¥ä½œçš„å‡ºç‰ˆå¹´ä»½ã€‚
- en: â€¢
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Sensors indicates the sensors involved in each work. gyro and acc represent
    gyroscope and accelerometer respectively.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¼ æ„Ÿå™¨æŒ‡ç¤ºæ¯é¡¹å·¥ä½œä¸­æ¶‰åŠçš„ä¼ æ„Ÿå™¨ã€‚**é™€èºä»ª**å’Œ**åŠ é€Ÿåº¦è®¡**åˆ†åˆ«ä»£è¡¨é™€èºä»ªå’ŒåŠ é€Ÿåº¦è®¡ã€‚
- en: â€¢
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Model indicates which module the framework consists of.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹æŒ‡ç¤ºæ¡†æ¶ç”±å“ªä¸ªæ¨¡å—ç»„æˆã€‚
- en: â€¢
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Learning indicates how to train neural networks. SL and RL represent Supervised
    Learning and Reinforcement Learning.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å­¦ä¹ è¡¨æ˜å¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œã€‚SL å’Œ RL åˆ†åˆ«ä»£è¡¨**ç›‘ç£å­¦ä¹ **å’Œ**å¼ºåŒ–å­¦ä¹ **ã€‚
- en: â€¢
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Target indicates what the model aims to solve or produce.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç›®æ ‡æŒ‡ç¤ºæ¨¡å‹æ—¨åœ¨è§£å†³æˆ–äº§ç”Ÿçš„å†…å®¹ã€‚
- en: II-E Discussion
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E è®¨è®º
- en: As previously mentioned, classical inertial navigation methods are designed
    to solve specific problems within their respective domains. However, their performance
    is often limited due to real-world issues such as imperfect modeling, measurement
    errors, and environmental influences, resulting in inevitable error drifts. Researchers
    in the field of inertial navigation are therefore constantly searching for ways
    to build models that can tolerate measurement errors and mitigate system drifts.
    In addition to relying on Newtonian physical rules, it has been observed that
    domain-specific knowledge, whether it be an experienced human walking model or
    scene geometry, can serve as a useful constraint in reducing the error drifts
    of inertial positioning systems. One potential approach to improving inertial
    positioning accuracy and robustness is to exploit massive inertial data to extract
    domain-specific knowledge and construct a data-driven model. In the next sections,
    we will delve deeper into this problem and explore potential solutions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œç»å…¸çš„æƒ¯æ€§å¯¼èˆªæ–¹æ³•æ—¨åœ¨è§£å†³å„è‡ªé¢†åŸŸä¸­çš„ç‰¹å®šé—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºç°å®ä¸–ç•Œä¸­çš„é—®é¢˜å¦‚æ¨¡å‹ä¸å®Œå–„ã€æµ‹é‡è¯¯å·®å’Œç¯å¢ƒå½±å“ï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€å—åˆ°é™åˆ¶ï¼Œå¯¼è‡´ä¸å¯é¿å…çš„è¯¯å·®æ¼‚ç§»ã€‚å› æ­¤ï¼Œæƒ¯æ€§å¯¼èˆªé¢†åŸŸçš„ç ”ç©¶äººå‘˜ä¸æ–­å¯»æ±‚æ„å»ºèƒ½å¤Ÿå®¹å¿æµ‹é‡è¯¯å·®å¹¶å‡è½»ç³»ç»Ÿæ¼‚ç§»çš„æ¨¡å‹çš„æ–¹æ³•ã€‚é™¤äº†ä¾èµ–ç‰›é¡¿ç‰©ç†è§„åˆ™å¤–ï¼Œè§‚å¯Ÿåˆ°é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œæ— è®ºæ˜¯ç»éªŒä¸°å¯Œçš„äººè¡Œèµ°æ¨¡å‹è¿˜æ˜¯åœºæ™¯å‡ ä½•å½¢çŠ¶ï¼Œéƒ½å¯ä»¥ä½œä¸ºå‡å°‘æƒ¯æ€§å®šä½ç³»ç»Ÿè¯¯å·®æ¼‚ç§»çš„æœ‰ç”¨çº¦æŸã€‚æé«˜æƒ¯æ€§å®šä½å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„ä¸€ä¸ªæ½œåœ¨æ–¹æ³•æ˜¯åˆ©ç”¨å¤§é‡æƒ¯æ€§æ•°æ®æå–é¢†åŸŸç‰¹å®šçŸ¥è¯†å¹¶æ„å»ºæ•°æ®é©±åŠ¨æ¨¡å‹ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°æ¢è®¨è¿™ä¸ªé—®é¢˜å¹¶æ¢ç´¢æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚
- en: '![Refer to caption](img/d9b883bc03d786c177f037b788968df9.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/d9b883bc03d786c177f037b788968df9.png)'
- en: 'Figure 2: An overview of existing deep learning based inertial sensor calibration
    methods'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 2: ç°æœ‰æ·±åº¦å­¦ä¹ åŸºäºæƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†æ–¹æ³•çš„æ¦‚è¿°'
- en: III Deep Learning Based Inertial Sensor Calibration
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†
- en: Inertial measurements obtained from low-cost IMUs are often affected by various
    sources of noise, making it challenging to distinguish the true values from the
    sources of error. The error sources are a complex interplay of deterministic and
    random factors, further complicating the issue. To address the impact of measurement
    errors, the powerful nonlinear approximator capabilities of deep neural networks
    can be exploited. A natural approach is to develop a deep neural network that
    receives the raw inertial measurements as input and produces the calibrated inertial
    measurements as output, representing the actual platform motion. By training this
    neural model on labeled datasets using stochastic gradient descent (SGD) [[32](#bib.bib32)],
    the inertial measurement errors can be implicitly learned and corrected by the
    neural network. It is important to note that the quality of the collected training
    dataset has a significant impact on the performance of the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä½æˆæœ¬æƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆIMUï¼‰è·å¾—çš„æƒ¯æ€§æµ‹é‡æ•°æ®å¸¸å¸¸å—åˆ°å„ç§å™ªå£°æºçš„å½±å“ï¼Œè¿™ä½¿å¾—åŒºåˆ†çœŸå®å€¼ä¸è¯¯å·®æºå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›è¯¯å·®æºæ˜¯ç¡®å®šæ€§å’Œéšæœºå› ç´ çš„å¤æ‚äº¤äº’ï¼Œè¿›ä¸€æ­¥ä½¿é—®é¢˜å¤æ‚åŒ–ã€‚ä¸ºäº†è§£å†³æµ‹é‡è¯¯å·®çš„å½±å“ï¼Œå¯ä»¥åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå¼ºå¤§çš„éçº¿æ€§é€¼è¿‘èƒ½åŠ›ã€‚ä¸€ç§è‡ªç„¶çš„æ–¹æ³•æ˜¯å¼€å‘ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œæ¥æ”¶åŸå§‹æƒ¯æ€§æµ‹é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ ¡å‡†åçš„æƒ¯æ€§æµ‹é‡ä½œä¸ºè¾“å‡ºï¼Œä»£è¡¨å®é™…å¹³å°è¿åŠ¨ã€‚é€šè¿‡ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰[[32](#bib.bib32)]å¯¹æ ‡è®°æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™ç§ç¥ç»æ¨¡å‹å¯ä»¥éšå¼åœ°å­¦ä¹ å’Œçº æ­£æƒ¯æ€§æµ‹é‡è¯¯å·®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ”¶é›†çš„è®­ç»ƒæ•°æ®é›†çš„è´¨é‡å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚
- en: Before the age of deep learning, attempts were made to use neural networks to
    learn the measurement errors of inertial sensors. For example, a 1-layer artificial
    neural network (ANN) [[33](#bib.bib33)] is proposed to model the distribution
    of gyro drifts, and is able to successfully approximate gyro drifts with such
    a â€™shallowâ€™ network [[24](#bib.bib24)] . This method has an advantage over Kalman
    filtering (KF) based calibration methods in that it does not require setting hyper-parameters
    before use, such as the sensor noise matrix in KF.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£ä¹‹å‰ï¼Œæ›¾å°è¯•ä½¿ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ æƒ¯æ€§ä¼ æ„Ÿå™¨çš„æµ‹é‡è¯¯å·®ã€‚ä¾‹å¦‚ï¼Œæå‡ºäº†ä¸€ç§1å±‚äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰[[33](#bib.bib33)]æ¥å»ºæ¨¡é™€èºæ¼‚ç§»çš„åˆ†å¸ƒï¼Œå¹¶èƒ½å¤ŸæˆåŠŸåœ°ç”¨è¿™æ ·ä¸€ä¸ªâ€œæµ…å±‚â€ç½‘ç»œæ¥é€¼è¿‘é™€èºæ¼‚ç§»[[24](#bib.bib24)]ã€‚è¿™ç§æ–¹æ³•ç›¸æ¯”äºåŸºäºå¡å°”æ›¼æ»¤æ³¢ï¼ˆKFï¼‰çš„æ ¡å‡†æ–¹æ³•çš„ä¸€ä¸ªä¼˜åŠ¿åœ¨äºï¼Œå®ƒä¸éœ€è¦åœ¨ä½¿ç”¨å‰è®¾ç½®è¶…å‚æ•°ï¼Œå¦‚KFä¸­çš„ä¼ æ„Ÿå™¨å™ªå£°çŸ©é˜µã€‚
- en: '![Refer to caption](img/cbd6c97a736d7f132bef1a34d0197877.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/cbd6c97a736d7f132bef1a34d0197877.png)'
- en: 'Figure 3: An example of gyro calibration results (reprint from Calib-Net [[31](#bib.bib31)]).
    Compared with raw IMU integration, deep learning based calibration models significantly
    reduce attitude drifts.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 3: é™€èºä»ªæ ¡å‡†ç»“æœç¤ºä¾‹ï¼ˆè½¬è½½è‡ª Calib-Net [[31](#bib.bib31)]ï¼‰ã€‚ä¸åŸå§‹IMUç§¯åˆ†ç›¸æ¯”ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ ¡å‡†æ¨¡å‹æ˜¾è‘—å‡å°‘äº†å§¿æ€æ¼‚ç§»ã€‚'
- en: 'TABLE II: A summary of existing methods on deep learning based inertial positioning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ II: åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½ç°æœ‰æ–¹æ³•æ±‡æ€»ã€‚'
- en: '| name | Year | Carrier | model | learning | target |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| name | å¹´ä»½ | æ‰¿è½½ä½“ | æ¨¡å‹ | å­¦ä¹  | ç›®æ ‡ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| IONet [[34](#bib.bib34)] | 2018 | Pedestrian, Trolley | LSTM | SL | location
    displacement |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| IONet [[34](#bib.bib34)] | 2018 | è¡Œäºº, æ‰‹æ¨è½¦ | LSTM | SL | ä½ç½®ä½ç§» |'
- en: '| RIDI [[35](#bib.bib35)] | 2018 | Pedestrian | SVM, SVR | SL | velocity for
    inertial data calibration |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| RIDI [[35](#bib.bib35)] | 2018 | è¡Œäºº | SVM, SVR | SL | ç”¨äºæƒ¯æ€§æ•°æ®æ ¡å‡†çš„é€Ÿåº¦ |'
- en: '| Cortes et al.[[36](#bib.bib36)] | 2018 | Pedestrian | ConvNet | SL | velocity
    to constrain system drifts |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Cortes et al.[[36](#bib.bib36)] | 2018 | è¡Œäºº | ConvNet | SL | é€Ÿåº¦çº¦æŸç³»ç»Ÿæ¼‚ç§» |'
- en: '| Wagstaff et al.[[37](#bib.bib37)] | 2018 | Pedestrian | LSTM | SL | zero-velocity
    detection for ZUPT |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Wagstaff et al.[[37](#bib.bib37)] | 2018 | è¡Œäºº | LSTM | SL | ZUPTçš„é›¶é€Ÿåº¦æ£€æµ‹ |'
- en: '| Chen et al.[[38](#bib.bib38)] | 2019 | Pedestrian, Trolley | LSTM | TL |
    location displacement |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al.[[38](#bib.bib38)] | 2019 | è¡Œäºº, æ‰‹æ¨è½¦ | LSTM | TL | ä½ç½®ä½ç§» |'
- en: '| AbolDeepIO [[39](#bib.bib39)] | 2019 | UAV | LSTM | SL | location displacement
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| AbolDeepIO [[39](#bib.bib39)] | 2019 | UAV | LSTM | SL | ä½ç½®ä½ç§» |'
- en: '| RINS-W [[40](#bib.bib40)] | 2019 | Vehicle | RNN | SL | zero-velocity dection
    for KF |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| RINS-W [[40](#bib.bib40)] | 2019 | è½¦è¾† | RNN | SL | KFçš„é›¶é€Ÿåº¦æ£€æµ‹ |'
- en: '| Feigl et al. [[41](#bib.bib41)] | 2019 | Pedestrian | LSTM | SL | walking
    velocity |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Feigl et al. [[41](#bib.bib41)] | 2019 | è¡Œäºº | LSTM | SL | è¡Œèµ°é€Ÿåº¦ |'
- en: '| Wang et al. [[42](#bib.bib42)] | 2019 | Pedestrian | LSTM | SL | walking
    heading for ZUPT |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[42](#bib.bib42)] | 2019 | è¡Œäºº | LSTM | SL | ZUPTçš„è¡Œèµ°æ–¹å‘ |'
- en: '| Yu et al. [[43](#bib.bib43)] | 2019 | Pedestrian | ConvNet | SL | adaptive
    zero-velocity detection |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Yu et al. [[43](#bib.bib43)] | 2019 | è¡Œäºº | ConvNet | SL | è‡ªé€‚åº”é›¶é€Ÿåº¦æ£€æµ‹ |'
- en: '| TLIO [[44](#bib.bib44)] | 2020 | Pedestrian | ConvNet | SL | 3D displacement
    and uncertainty for EKF |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| TLIO [[44](#bib.bib44)] | 2020 | è¡Œäºº | ConvNet | SL | EKFçš„3Dä½ç§»å’Œä¸ç¡®å®šæ€§ |'
- en: '| LIONet [[45](#bib.bib45)] | 2020 | Pedestrian | Dilated ConvNet | SL | lightweight
    inertial model |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LIONet [[45](#bib.bib45)] | 2020 | è¡Œäºº | æ‰©å¼ å·ç§¯ç½‘ç»œ | SL | è½»é‡çº§æƒ¯æ€§æ¨¡å‹ |'
- en: '| RoNIN[[46](#bib.bib46)] | 2020 | Pedestrian | LSTM, TCN | SL | velocity for
    inertial data calibration |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| RoNIN[[46](#bib.bib46)] | 2020 | è¡Œäºº | LSTM, TCN | SL | ç”¨äºæƒ¯æ€§æ•°æ®æ ¡å‡†çš„é€Ÿåº¦ |'
- en: '| Brossard et al.[[47](#bib.bib47)] | 2020 | Vehicle | ConvNet | SL | co-variance
    noise for KF |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Brossard et al.[[47](#bib.bib47)] | 2020 | è½¦è¾† | ConvNet | SL | KFçš„åæ–¹å·®å™ªå£° |'
- en: '| StepNet[[48](#bib.bib48)] | 2020 | Pedestrian | ConvNet, LSTM | SL | dynamic
    step length for PDR |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| StepNet[[48](#bib.bib48)] | 2020 | è¡Œäºº | ConvNet, LSTM | SL | PDRçš„åŠ¨æ€æ­¥é•¿ |'
- en: '| Wang et al.[[49](#bib.bib49)] | 2020 | Pedestrian | ConvNet | SL | measurement
    noise for Kalman Filter |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al.[[49](#bib.bib49)] | 2020 | è¡Œäºº | ConvNet | SL | å¡å°”æ›¼æ»¤æ³¢å™¨çš„æµ‹é‡å™ªå£° |'
- en: '| ARPDR [[50](#bib.bib50)] | 2020 | Pedestrian | TCN | SL | stride length and
    walking heading for PDR |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ARPDR [[50](#bib.bib50)] | 2020 | è¡Œäºº | TCN | SL | PDRçš„æ­¥å¹…é•¿åº¦å’Œè¡Œèµ°æ–¹å‘ |'
- en: '| IDOL[[51](#bib.bib51)] | 2021 | Pedestrian | LSTM | SL | device orientation
    and location |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| IDOL[[51](#bib.bib51)] | 2021 | è¡Œäºº | LSTM | SL | è®¾å¤‡æ–¹å‘å’Œä½ç½® |'
- en: '| PDRNet[[52](#bib.bib52)] | 2021 | Pedestrian | ConvNet | SL | step length
    and heading for PDR |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| PDRNet[[52](#bib.bib52)] | 2021 | è¡Œäºº | ConvNet | SL | PDRçš„æ­¥é•¿å’Œæ–¹å‘ |'
- en: '| Buchanan et al. [[53](#bib.bib53)] | 2021 | Legged Robot | ConvNet | SL |
    integrate location displacement with leg odometry |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Buchanan et al. [[53](#bib.bib53)] | 2021 | å››è¶³æœºå™¨äºº | ConvNet | SL | å°†ä½ç½®ä½ç§»ä¸è…¿éƒ¨é‡Œç¨‹è®¡ç»“åˆ
    |'
- en: '| Zhang et al.[[54](#bib.bib54)] | 2021 | Vehicle, UAV | RNN | SL | independent
    motion terms |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al.[[54](#bib.bib54)] | 2021 | è½¦è¾†, UAV | RNN | SL | ç‹¬ç«‹è¿åŠ¨é¡¹ |'
- en: '| Gong et al. [[55](#bib.bib55)] | 2021 | Pedestrian | LSTM | SL | fusing inertial
    data from two devices |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Gong et al. [[55](#bib.bib55)] | 2021 | è¡Œäºº | LSTM | SL | èåˆæ¥è‡ªä¸¤ä¸ªè®¾å¤‡çš„æƒ¯æ€§æ•°æ® |'
- en: '| NILoc[[56](#bib.bib56)] | 2022 | Pedestrian | ConvNet | SL | inertial relocalization
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| NILoc[[56](#bib.bib56)] | 2022 | è¡Œäºº | ConvNet | SL | æƒ¯æ€§é‡æ–°å®šä½ |'
- en: '| RIO[[57](#bib.bib57)] | 2022 | Pedestrian | DNN | UL | rotation-equivariance
    as supervision signal |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| RIO[[57](#bib.bib57)] | 2022 | è¡Œäºº | DNN | UL | æ—‹è½¬ç­‰å˜æ€§ä½œä¸ºç›‘ç£ä¿¡å· |'
- en: '| Wang et al. [[58](#bib.bib58)] | 2022 | Pedestrian | DNN | SL | efficient
    and low-latent model |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[58](#bib.bib58)] | 2022 | è¡Œäºº | DNN | SL | é«˜æ•ˆä¸”ä½å»¶è¿Ÿçš„æ¨¡å‹ |'
- en: '| TinyOdom[[59](#bib.bib59)] | 2022 | Pedestrian, Vehicle | TCN+NAS | SL |
    deployment on resource-constrained device |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| TinyOdom[[59](#bib.bib59)] | 2022 | è¡Œäººï¼Œè½¦è¾† | TCN+NAS | SL | éƒ¨ç½²åœ¨èµ„æºå—é™è®¾å¤‡ä¸Š |'
- en: '| CTIN[[60](#bib.bib60)] | 2022 | Pedestrian | Transformer | SL | velocity
    and trajectory prediction |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CTIN[[60](#bib.bib60)] | 2022 | è¡Œäºº | Transformer | SL | é€Ÿåº¦å’Œè½¨è¿¹é¢„æµ‹ |'
- en: '| DeepVIP[[61](#bib.bib61)] | 2022 | Vehicle | ConvNet, LSTM | SL | velocity
    and heading for car localization |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DeepVIP[[61](#bib.bib61)] | 2022 | è½¦è¾† | ConvNet, LSTM | SL | è½¦è¾†å®šä½çš„é€Ÿåº¦å’Œèˆªå‘ |'
- en: '| Bo et al.[[62](#bib.bib62)] | 2022 | Pedestrian | ConvNet | TL | model-independent
    stride learning |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Bo et al.[[62](#bib.bib62)] | 2022 | è¡Œäºº | ConvNet | TL | æ¨¡å‹ç‹¬ç«‹çš„æ­¥æ€å­¦ä¹  |'
- en: '| OdoNet[[63](#bib.bib63)] | 2022 | Vehicle | ConvNet | SL | speed learning
    for ZUPT |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| OdoNet[[63](#bib.bib63)] | 2022 | è½¦è¾† | ConvNet | SL | ZUPT çš„é€Ÿåº¦å­¦ä¹  |'
- en: '| A2DIO[[64](#bib.bib64)] | 2022 | Pedestrian | ConvNet, LSTM | SL | pose invariant
    odometry |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| A2DIO[[64](#bib.bib64)] | 2022 | è¡Œäºº | ConvNet, LSTM | SL | å§¿æ€ä¸å˜çš„é‡Œç¨‹è®¡ |'
- en: '| LLIO [[65](#bib.bib65)] | 2022 | Pedestrian | MLP | SL | 3D displacement
    for lightweight odometry |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLIO [[65](#bib.bib65)] | 2022 | è¡Œäºº | MLP | SL | è½»é‡çº§é‡Œç¨‹è®¡çš„ 3D ä½ç§» |'
- en: â€¢
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Year indicates the publication year of each work.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Year æŒ‡ç¤ºæ¯é¡¹å·¥ä½œçš„å‡ºç‰ˆå¹´ä»½ã€‚
- en: â€¢
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Carrier indicates the platform running inertial navigation.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Carrier æŒ‡ç¤ºè¿è¡Œæƒ¯æ€§å¯¼èˆªçš„å¹³å°ã€‚
- en: â€¢
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Model indicates which module the framework consists of.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Model æŒ‡ç¤ºæ¡†æ¶ç”±å“ªäº›æ¨¡å—ç»„æˆã€‚
- en: â€¢
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Learning indicates how to train neural networks. SL, TL and UL represent Supervised
    Learning, Transfer Learning and Unsupervised Learning.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Learning æŒ‡ç¤ºå¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œã€‚SLã€TL å’Œ UL åˆ†åˆ«è¡¨ç¤ºç›‘ç£å­¦ä¹ ã€è¿ç§»å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ã€‚
- en: â€¢
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Target indicates what the model aims to solve or produce.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Target æŒ‡ç¤ºæ¨¡å‹æ—¨åœ¨è§£å†³æˆ–ç”Ÿæˆçš„å†…å®¹ã€‚
- en: 'In recent years, there has been increasing interest in using deep neural networks
    (DNN) with multiple layers to solve the inertial sensor calibration problem. With
    the addition of more layers, neural networks become more expressive and can learn
    complex relationships between the raw inertial measurements and the true motion
    of the vehicle. One approach, proposed by [[25](#bib.bib25)], uses a Convolutional
    Neural Network (ConvNet) to remove error noises from inertial measurements. They
    collected inertial data from two grades of IMU under given constant accelerations
    and angular rates. The ConvNet framework takes raw inertial measurements (from
    low-precision IMU) as inputs and tries to output acceleration and angular rate
    references (from high-precision IMU). Their experiment shows that deep learning
    can remove some of the sensor error and improve test accuracy. However, this work
    has not been validated in a real navigation setup, and thus it cannot demonstrate
    how learning-based sensor calibration reduces error drifts in inertial navigation.
    Both of the mentioned methods require reference data from high-precision IMUs
    as labels to train the networks, as shown in Figure [2](#S2.F2 "Figure 2 â€£ II-E
    Discussion â€£ II Classical Inertial Navigation Mechanisms â€£ Deep Learning for Inertial
    Positioning: A Survey") (a). However, acquiring reference data from high-precision
    IMUs can be costly.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿‘å¹´æ¥ï¼Œä½¿ç”¨å…·æœ‰å¤šå±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è§£å†³æƒ¯æ€§ä¼ æ„Ÿå™¨æ ‡å®šé—®é¢˜çš„å…´è¶£æ—¥ç›Šå¢åŠ ã€‚éšç€å±‚æ•°çš„å¢åŠ ï¼Œç¥ç»ç½‘ç»œå˜å¾—æ›´åŠ æœ‰è¡¨ç°åŠ›ï¼Œå¯ä»¥å­¦ä¹ åŸå§‹æƒ¯æ€§æµ‹é‡å€¼ä¸è½¦è¾†çœŸå®è¿åŠ¨ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸€ä¸ªæ–¹æ³•ï¼Œ[[25](#bib.bib25)]
    æå‡ºçš„ï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetï¼‰ä»æƒ¯æ€§æµ‹é‡ä¸­å»é™¤è¯¯å·®å™ªå£°ã€‚ä»–ä»¬åœ¨ç»™å®šçš„æ’å®šåŠ é€Ÿåº¦å’Œè§’é€Ÿåº¦ä¸‹ï¼Œæ”¶é›†äº†ä¸¤ç§ç­‰çº§IMUçš„æƒ¯æ€§æ•°æ®ã€‚ConvNetæ¡†æ¶ä»¥åŸå§‹æƒ¯æ€§æµ‹é‡å€¼ï¼ˆæ¥è‡ªä½ç²¾åº¦IMUï¼‰ä¸ºè¾“å…¥ï¼Œå°è¯•è¾“å‡ºåŠ é€Ÿåº¦å’Œè§’é€Ÿåº¦å‚è€ƒå€¼ï¼ˆæ¥è‡ªé«˜ç²¾åº¦IMUï¼‰ã€‚ä»–ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ å¯ä»¥å»é™¤ä¸€äº›ä¼ æ„Ÿå™¨è¯¯å·®å¹¶æé«˜æµ‹è¯•ç²¾åº¦ã€‚ç„¶è€Œï¼Œè¿™é¡¹å·¥ä½œå°šæœªåœ¨å®é™…å¯¼èˆªè®¾ç½®ä¸­éªŒè¯ï¼Œå› æ­¤ä¸èƒ½å±•ç¤ºåŸºäºå­¦ä¹ çš„ä¼ æ„Ÿå™¨æ ‡å®šå¦‚ä½•å‡å°‘æƒ¯æ€§å¯¼èˆªä¸­çš„è¯¯å·®æ¼‚ç§»ã€‚ä¸Šè¿°æ–¹æ³•éƒ½éœ€è¦é«˜ç²¾åº¦IMUçš„å‚è€ƒæ•°æ®ä½œä¸ºæ ‡ç­¾æ¥è®­ç»ƒç½‘ç»œï¼Œå¦‚å›¾[2](#S2.F2
    "Figure 2 â€£ II-E Discussion â€£ II Classical Inertial Navigation Mechanisms â€£ Deep
    Learning for Inertial Positioning: A Survey") (a)æ‰€ç¤ºã€‚ç„¶è€Œï¼Œä»é«˜ç²¾åº¦IMUè·å–å‚è€ƒæ•°æ®å¯èƒ½æ˜¯æ˜‚è´µçš„ã€‚'
- en: 'In addition to directly learning from pseudo ground-truth IMU labels, another
    approach is to enable neural network-based calibration models to produce inertial
    data that can be integrated into more accurate orientation estimation. This is
    illustrated in Figure [2](#S2.F2 "Figure 2 â€£ II-E Discussion â€£ II Classical Inertial
    Navigation Mechanisms â€£ Deep Learning for Inertial Positioning: A Survey") (b).
    By producing more accurate orientation values, the neural network implicitly removes
    the corrupted noises above inertial data. For example, OriNet [[26](#bib.bib26)]
    inputs 3-dimensional gyroscope signals into an LSTM network [[66](#bib.bib66)]
    to obtain calibrated gyroscope signals, which are then integrated with the orientation
    at the previous timestep to generate orientation estimates at the current timestep.
    A loss function between orientation estimates and real orientation is defined
    and minimized for model training. OriNet has been evaluated on a public drone
    dataset, demonstrating an improvement in orientation performance of approximately
    80%. A similar approach is [[28](#bib.bib28)], who calibrates gyroscope using
    ConvNet, reporting good attitude estimation accuracy. Calib-Net [[31](#bib.bib31)]
    is another ConvNet framework that denoises gyroscope data by extracting effective
    spatio-temporal features from inertial data. Calib-Net is based on dilation ConvNet
    [[67](#bib.bib67)] to compensate the gyro noise, as illustrated in Figure [3](#S3.F3
    "Figure 3 â€£ III Deep Learning Based Inertial Sensor Calibration â€£ Deep Learning
    for Inertial Positioning: A Survey"). This model is able to significantly reduce
    orientation error compared to raw IMU integration. When this learned inertial
    calibration model is incorporated into a visual-inertial odometry (VIO), it further
    improves localization performance and outperforms representative VIOs such as
    VINS-mono [[6](#bib.bib6)]. Other efforts in this direction include works by [[29](#bib.bib29),
    [30](#bib.bib30)].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç›´æ¥ä»ä¼ªåœ°é¢çœŸå€¼IMUæ ‡ç­¾ä¸­å­¦ä¹ å¤–ï¼Œå¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿åŸºäºç¥ç»ç½‘ç»œçš„æ ¡å‡†æ¨¡å‹ç”Ÿæˆå¯é›†æˆåˆ°æ›´å‡†ç¡®æ–¹å‘ä¼°è®¡ä¸­çš„æƒ¯æ€§æ•°æ®ã€‚è¿™åœ¨å›¾[2](#S2.F2 "å›¾ 2
    â€£ II-E è®¨è®º â€£ II ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶ â€£ æ·±åº¦å­¦ä¹ åœ¨æƒ¯æ€§å®šä½ä¸­çš„åº”ç”¨ï¼šç»¼è¿°") (b)ä¸­æœ‰æ‰€è¯´æ˜ã€‚é€šè¿‡ç”Ÿæˆæ›´å‡†ç¡®çš„æ–¹å‘å€¼ï¼Œç¥ç»ç½‘ç»œéšå¼åœ°å»é™¤äº†æƒ¯æ€§æ•°æ®ä¸Šçš„å™ªå£°ã€‚ä¾‹å¦‚ï¼ŒOriNet
    [[26](#bib.bib26)] å°†ä¸‰ç»´é™€èºä»ªä¿¡å·è¾“å…¥åˆ°LSTMç½‘ç»œ[[66](#bib.bib66)]ä¸­ä»¥è·å¾—æ ¡å‡†åçš„é™€èºä»ªä¿¡å·ï¼Œç„¶åå°†å…¶ä¸å‰ä¸€æ—¶é—´æ­¥çš„æ–¹å‘è¿›è¡Œé›†æˆï¼Œä»¥ç”Ÿæˆå½“å‰æ—¶é—´æ­¥çš„æ–¹å‘ä¼°è®¡ã€‚å®šä¹‰å¹¶æœ€å°åŒ–æ–¹å‘ä¼°è®¡ä¸å®é™…æ–¹å‘ä¹‹é—´çš„æŸå¤±å‡½æ•°ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚OriNetåœ¨å…¬å¼€çš„æ— äººæœºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºçº¦80%çš„æ–¹å‘æ€§èƒ½æå‡ã€‚ç±»ä¼¼çš„æ–¹æ³•åŒ…æ‹¬[[28](#bib.bib28)]ï¼Œä»–ä»¬ä½¿ç”¨ConvNetå¯¹é™€èºä»ªè¿›è¡Œæ ¡å‡†ï¼ŒæŠ¥å‘Šäº†è‰¯å¥½çš„å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚Calib-Net
    [[31](#bib.bib31)] æ˜¯å¦ä¸€ç§ConvNetæ¡†æ¶ï¼Œé€šè¿‡ä»æƒ¯æ€§æ•°æ®ä¸­æå–æœ‰æ•ˆçš„æ—¶ç©ºç‰¹å¾æ¥å»å™ªé™€èºä»ªæ•°æ®ã€‚Calib-NetåŸºäºè†¨èƒ€ConvNet[[67](#bib.bib67)]ä»¥è¡¥å¿é™€èºä»ªå™ªå£°ï¼Œå¦‚å›¾[3](#S3.F3
    "å›¾ 3 â€£ III åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡† â€£ æ·±åº¦å­¦ä¹ åœ¨æƒ¯æ€§å®šä½ä¸­çš„åº”ç”¨ï¼šç»¼è¿°")æ‰€ç¤ºã€‚ä¸åŸå§‹IMUç§¯åˆ†ç›¸æ¯”ï¼Œè¿™ç§æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ–¹å‘è¯¯å·®ã€‚å½“è¿™ä¸ªå­¦ä¹ åˆ°çš„æƒ¯æ€§æ ¡å‡†æ¨¡å‹è¢«çº³å…¥åˆ°è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆVIOï¼‰ä¸­æ—¶ï¼Œå®ƒè¿›ä¸€æ­¥æå‡äº†å®šä½æ€§èƒ½ï¼Œå¹¶ä¼˜äºä»£è¡¨æ€§VIOå¦‚VINS-mono
    [[6](#bib.bib6)]ã€‚åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å…¶ä»–åŠªåŠ›åŒ…æ‹¬[[29](#bib.bib29), [30](#bib.bib30)]çš„å·¥ä½œã€‚
- en: 'Instead of directly calibrating inertial sensors with DNNs, some researchers
    have explored using DNNs to generate parameters that improve classical calibration
    algorithms, as shown in Figure [2](#S2.F2 "Figure 2 â€£ II-E Discussion â€£ II Classical
    Inertial Navigation Mechanisms â€£ Deep Learning for Inertial Positioning: A Survey")
    (c). One example is the work by [[27](#bib.bib27)], who models inertial sensor
    calibration as a Markov Decision Process and proposes to use deep reinforcement
    learning [[68](#bib.bib68)] to learn the optimal calibration parameters. The authors
    demonstrated the effectiveness of their approach in calibrating inertial sensors
    for a visual-inertial odometry (VIO) system.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç ”ç©¶äººå‘˜æ¢ç´¢äº†ä½¿ç”¨DNNç”Ÿæˆæ”¹è¿›ç»å…¸æ ¡å‡†ç®—æ³•çš„å‚æ•°ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨DNNæ ¡å‡†æƒ¯æ€§ä¼ æ„Ÿå™¨ï¼Œå¦‚å›¾[2](#S2.F2 "å›¾ 2 â€£ II-E è®¨è®º â€£ II
    ç»å…¸æƒ¯æ€§å¯¼èˆªæœºåˆ¶ â€£ æ·±åº¦å­¦ä¹ åœ¨æƒ¯æ€§å®šä½ä¸­çš„åº”ç”¨ï¼šç»¼è¿°") (c)æ‰€ç¤ºã€‚ä¸€ä¸ªä¾‹å­æ˜¯[[27](#bib.bib27)]çš„å·¥ä½œï¼Œä»–ä»¬å°†æƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶æå‡ºä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ [[68](#bib.bib68)]æ¥å­¦ä¹ æœ€ä½³æ ¡å‡†å‚æ•°ã€‚ä½œè€…å±•ç¤ºäº†ä»–ä»¬çš„æ–¹æ³•åœ¨ä¸ºè§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆVIOï¼‰ç³»ç»Ÿæ ¡å‡†æƒ¯æ€§ä¼ æ„Ÿå™¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚
- en: '![Refer to caption](img/a39a2ea86b1036781e9de2bdd5a44cbb.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/a39a2ea86b1036781e9de2bdd5a44cbb.png)'
- en: 'Figure 4: The velocity of attached platform can be inferred from a sequence
    of inertial measurements via deep neural networks. (reprint from L-IONet [[45](#bib.bib45)])'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šé€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œå¯ä»¥ä»ä¸€ç³»åˆ—æƒ¯æ€§æµ‹é‡ä¸­æ¨æ–­é™„åŠ å¹³å°çš„é€Ÿåº¦ã€‚ (æ‘˜è‡ªL-IONet [[45](#bib.bib45)])
- en: '![Refer to caption](img/63b76f8cedcb1b8957f320d8faf3eb13.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/63b76f8cedcb1b8957f320d8faf3eb13.png)'
- en: 'Figure 5: An overview of existing methods on learning to correct IMU integration'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šç°æœ‰ IMU é›†æˆæ ¡æ­£å­¦ä¹ æ–¹æ³•çš„æ¦‚è¿°
- en: 'As discussed above, deep learning-aided inertial sensor calibration methods
    (listed in Table [I](#S2.T1 "TABLE I â€£ II-D Integrating IMU with Other Sensors
    â€£ II Classical Inertial Navigation Mechanisms â€£ Deep Learning for Inertial Positioning:
    A Survey")) have shown promising results in removing corrupted sensor noises and
    improving the accuracy of inertial positioning systems. These methods do not require
    human intervention and can automatically learn error models. However, it is important
    to note that the learned error model is typically dependent on the specific sensor
    or platform used. Therefore, a change in sensor or user can result in different
    data distributions, leading to reduced performance of the learned model. Additionally,
    further analysis is needed to determine which types of noise can be effectively
    removed by learning-based calibration methods.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚ä¸Šæ‰€è¿°ï¼Œæ·±åº¦å­¦ä¹ è¾…åŠ©çš„æƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†æ–¹æ³•ï¼ˆè§è¡¨ [I](#S2.T1 "TABLE I â€£ II-D Integrating IMU with Other
    Sensors â€£ II Classical Inertial Navigation Mechanisms â€£ Deep Learning for Inertial
    Positioning: A Survey")ï¼‰åœ¨å»é™¤ä¼ æ„Ÿå™¨å™ªå£°å’Œæé«˜æƒ¯æ€§å®šä½ç³»ç»Ÿå‡†ç¡®æ€§æ–¹é¢æ˜¾ç¤ºäº†è‰¯å¥½çš„å‰æ™¯ã€‚è¿™äº›æ–¹æ³•ä¸éœ€è¦äººå·¥å¹²é¢„ï¼Œå¹¶ä¸”å¯ä»¥è‡ªåŠ¨å­¦ä¹ è¯¯å·®æ¨¡å‹ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå­¦ä¹ åˆ°çš„è¯¯å·®æ¨¡å‹é€šå¸¸ä¾èµ–äºç‰¹å®šçš„ä¼ æ„Ÿå™¨æˆ–å¹³å°ã€‚å› æ­¤ï¼Œä¼ æ„Ÿå™¨æˆ–ç”¨æˆ·çš„å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´æ•°æ®åˆ†å¸ƒçš„ä¸åŒï¼Œä»è€Œé™ä½å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¿›ä¸€æ­¥åˆ†æä»¥ç¡®å®šå“ªäº›ç±»å‹çš„å™ªå£°å¯ä»¥é€šè¿‡å­¦ä¹ å‹æ ¡å‡†æ–¹æ³•æœ‰æ•ˆå»é™¤ã€‚'
- en: IV Learning to correct IMU Integration
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV å­¦ä¹ æ ¡æ­£ IMU é›†æˆ
- en: 'In addition to sensor calibration, researchers are exploring various methods
    for using deep learning to construct inertial positioning models that can either
    partially or completely replace classical inertial navigation mechanisms. This
    section provides an overview of how deep learning can be used to correct IMU integration
    in general. Next sections will discuss deep learning approaches for pedestrian
    tracking applications, and present deep inertial solutions for vehicles, UAVs,
    and robots. A summary of existing works and their contributions is provided in
    Table [II](#S3.T2 "TABLE II â€£ III Deep Learning Based Inertial Sensor Calibration
    â€£ Deep Learning for Inertial Positioning: A Survey").'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'é™¤äº†ä¼ æ„Ÿå™¨æ ¡å‡†å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜åœ¨æ¢ç´¢å„ç§ä½¿ç”¨æ·±åº¦å­¦ä¹ æ„å»ºæƒ¯æ€§å®šä½æ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥éƒ¨åˆ†æˆ–å®Œå…¨æ›¿ä»£ç»å…¸çš„æƒ¯æ€§å¯¼èˆªæœºåˆ¶ã€‚æœ¬èŠ‚æ¦‚è¿°äº†æ·±åº¦å­¦ä¹ å¦‚ä½•ç”¨äºä¸€èˆ¬æ€§çš„
    IMU é›†æˆæ ¡æ­£ã€‚æ¥ä¸‹æ¥çš„ç« èŠ‚å°†è®¨è®ºç”¨äºè¡Œäººè·Ÿè¸ªåº”ç”¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¹¶å±•ç¤ºç”¨äºè½¦è¾†ã€æ— äººæœºå’Œæœºå™¨äººæ·±åº¦æƒ¯æ€§è§£å†³æ–¹æ¡ˆã€‚è¡¨ [II](#S3.T2 "TABLE
    II â€£ III Deep Learning Based Inertial Sensor Calibration â€£ Deep Learning for Inertial
    Positioning: A Survey") æä¾›äº†ç°æœ‰å·¥ä½œåŠå…¶è´¡çŒ®çš„æ€»ç»“ã€‚'
- en: 'In deep learning-based inertial positioning approaches, a userâ€™s absolute velocity
    can be inferred from a sequence of IMU data using a deep neural network. This
    velocity information can then be used as a key constraint to reduce the drifts
    in IMU double integration. Figure [4](#S3.F4 "Figure 4 â€£ III Deep Learning Based
    Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning: A Survey")
    provides an example of velocity learning from IMU sequence, where the periodicity
    of human walking makes it easy to infer the userâ€™s moving velocity. Similar observations
    have been made for vehicles, UAVs, and robotic platforms, which will be discussed
    in Section [VI](#S6 "VI Learning to Correct Inertial Positioning on Vehicles,
    UAV and robotic platforms â€£ Deep Learning for Inertial Positioning: A Survey").
    Existing works on applying learned velocity to correct IMU integration can generally
    be divided into three categories, as shown in Figure [5](#S3.F5 "Figure 5 â€£ III
    Deep Learning Based Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning:
    A Survey"), and will be discussed as follows.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½æ–¹æ³•ä¸­ï¼Œç”¨æˆ·çš„ç»å¯¹é€Ÿåº¦å¯ä»¥é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œä» IMU æ•°æ®åºåˆ—ä¸­æ¨æ–­å‡ºæ¥ã€‚ç„¶åï¼Œè¿™äº›é€Ÿåº¦ä¿¡æ¯å¯ä»¥ä½œä¸ºå…³é”®çº¦æŸï¼Œä»¥å‡å°‘ IMU
    åŒé‡ç§¯åˆ†ä¸­çš„æ¼‚ç§»ã€‚å›¾ [4](#S3.F4 "Figure 4 â€£ III Deep Learning Based Inertial Sensor Calibration
    â€£ Deep Learning for Inertial Positioning: A Survey") æä¾›äº†ä» IMU åºåˆ—ä¸­å­¦ä¹ é€Ÿåº¦çš„ç¤ºä¾‹ï¼Œå…¶ä¸­äººç±»æ­¥æ€çš„å‘¨æœŸæ€§ä½¿å¾—æ¨æ–­ç”¨æˆ·çš„ç§»åŠ¨é€Ÿåº¦å˜å¾—å®¹æ˜“ã€‚å¯¹äºè½¦è¾†ã€æ— äººæœºå’Œæœºå™¨äººå¹³å°ä¹Ÿæœ‰ç±»ä¼¼çš„è§‚å¯Ÿï¼Œè¿™å°†åœ¨ç¬¬
    [VI](#S6 "VI Learning to Correct Inertial Positioning on Vehicles, UAV and robotic
    platforms â€£ Deep Learning for Inertial Positioning: A Survey") èŠ‚ä¸­è®¨è®ºã€‚ç°æœ‰çš„å°†å­¦ä¹ åˆ°çš„é€Ÿåº¦åº”ç”¨äº
    IMU é›†æˆæ ¡æ­£çš„å·¥ä½œé€šå¸¸å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼Œå¦‚å›¾ [5](#S3.F5 "Figure 5 â€£ III Deep Learning Based Inertial
    Sensor Calibration â€£ Deep Learning for Inertial Positioning: A Survey") æ‰€ç¤ºï¼Œä¸‹é¢å°†è¿›è¡Œè®¨è®ºã€‚'
- en: 'One category of deep learning models aims to learn location displacement, which
    is the average velocity multiplied by a fixed period of time, as illustrated in
    Figure [5](#S3.F5 "Figure 5 â€£ III Deep Learning Based Inertial Sensor Calibration
    â€£ Deep Learning for Inertial Positioning: A Survey")(a). The approach proposed
    by [[34](#bib.bib34)] formulates inertial positioning as a sequential learning
    problem, where 2D motion displacements in the polar coordinate, also known as
    polar vectors, are learned from independent windows of segmented inertial data.
    This is because the frequency of platform vibrations is relevant to the absolute
    moving speed, which can be measured by IMU, when tracking human or wheeled configurations.
    Based on this observation, they propose IONet, an LSTM-based framework for end-to-end
    learning of relative poses. Trajectories are generated by adding motion displacements
    together with initial locations. To train neural models, a large collection of
    data was collected from a smartphone-based IMU in a room with a high-precision
    visual motion tracking system (i.e., Vicon) to provide ground-truth pose labels.
    Once the model is trained, the IONet model can be used in areas outside the data-collection
    room. In a two-minute random pedestrian walking scenario, the localization error
    of IONet is within 3 meters 90% of the time, when evaluating across users, devices,
    and attachments, outperforming some classical PDR algorithms. In tracking trolley,
    IONet shows comparable performance over representative visual-inertial odometry
    and is even more robust in featureless areas. However, supervised learning-based
    IONet requires high-precision pose as training labels. When testing with data
    different from those in the training set, there will be performance degradation.
    To improve the generalization ability, [[38](#bib.bib38)] proposes MotionTransformer,
    which allows the inertial positioning model to self-adapt into new domains via
    generative adversarial network (GAN) [[69](#bib.bib69)] and domain adaptation
    [[70](#bib.bib70)], without the need for labels in new domains. To encourage more
    reliable inertial positioning, [[71](#bib.bib71)] is able to produce pose uncertainties
    along with poses, offering the belief in the extent to which the learned pose
    can be trusted. To allow full 3D localization, TLIO [[44](#bib.bib44)] proposes
    to learn 3D location displacements and covariances from a sequence of gravity-aligned
    inertial data. To avoid the impacts from initial orientation, the inertial data
    are transformed into a local gravity-aligned frame. The learned displacements
    and covariances are then incorporated into an extended Kalman filter as observation
    states that estimate full-states of orientation, velocity, location, and IMU bias.
    In a 3-7 minute human motion scenario, the localization error of TLIO is within
    3 meters 90% of the time.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸€ä¸ªç±»åˆ«æ—¨åœ¨å­¦ä¹ ä½ç½®ä½ç§»ï¼Œå³å¹³å‡é€Ÿåº¦ä¹˜ä»¥å›ºå®šæ—¶é—´æ®µï¼Œå¦‚å›¾[5](#S3.F5 "Figure 5 â€£ III Deep Learning
    Based Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning: A
    Survey")(a)æ‰€ç¤ºã€‚[[34](#bib.bib34)]æå‡ºçš„æ–¹æ³•å°†æƒ¯æ€§å®šä½å½¢å¼åŒ–ä¸ºä¸€ä¸ªåºåˆ—å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­æåæ ‡ä¸­çš„äºŒç»´è¿åŠ¨ä½ç§»ï¼Œå³æå‘é‡ï¼Œä»ç‹¬ç«‹çš„æƒ¯æ€§æ•°æ®æ®µçª—å£ä¸­å­¦ä¹ ã€‚è¿™æ˜¯å› ä¸ºå¹³å°æŒ¯åŠ¨çš„é¢‘ç‡ä¸ç»å¯¹ç§»åŠ¨é€Ÿåº¦ç›¸å…³ï¼Œè€Œç»å¯¹ç§»åŠ¨é€Ÿåº¦å¯ä»¥é€šè¿‡IMUæµ‹é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è·Ÿè¸ªäººç±»æˆ–è½®å¼é…ç½®æ—¶ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œä»–ä»¬æå‡ºäº†IONetï¼Œä¸€ä¸ªåŸºäºLSTMçš„æ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯å­¦ä¹ ç›¸å¯¹å§¿æ€ã€‚é€šè¿‡å°†è¿åŠ¨ä½ç§»ä¸åˆå§‹ä½ç½®ç›¸åŠ æ¥ç”Ÿæˆè½¨è¿¹ã€‚ä¸ºäº†è®­ç»ƒç¥ç»æ¨¡å‹ï¼Œæ”¶é›†äº†å¤§é‡æ•°æ®ï¼Œè¿™äº›æ•°æ®æ¥è‡ªä¸€ä¸ªåŸºäºæ™ºèƒ½æ‰‹æœºçš„IMUï¼Œå¹¶åœ¨ä¸€ä¸ªå…·æœ‰é«˜ç²¾åº¦è§†è§‰è¿åŠ¨è·Ÿè¸ªç³»ç»Ÿï¼ˆå³Viconï¼‰çš„æˆ¿é—´å†…æä¾›çœŸå®å§¿æ€æ ‡ç­¾ã€‚ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒIONetæ¨¡å‹å¯ä»¥ç”¨äºæ•°æ®æ”¶é›†æˆ¿é—´ä¹‹å¤–çš„é¢†åŸŸã€‚åœ¨ä¸€ä¸ªä¸¤åˆ†é’Ÿçš„éšæœºè¡Œäººæ­¥æ€åœºæ™¯ä¸­ï¼Œå½“åœ¨ç”¨æˆ·ã€è®¾å¤‡å’Œé™„ä»¶ä¹‹é—´è¯„ä¼°æ—¶ï¼ŒIONetçš„å®šä½è¯¯å·®90%çš„æ—¶é—´åœ¨3ç±³ä»¥å†…ï¼Œè¶…è¶Šäº†ä¸€äº›ç»å…¸çš„PDRç®—æ³•ã€‚åœ¨è·Ÿè¸ªæ‰‹æ¨è½¦æ—¶ï¼ŒIONetæ˜¾ç¤ºå‡ºä¸ä»£è¡¨æ€§çš„è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ— ç‰¹å¾åŒºåŸŸä¸­æ›´åŠ ç¨³å¥ã€‚ç„¶è€Œï¼ŒåŸºäºç›‘ç£å­¦ä¹ çš„IONetéœ€è¦é«˜ç²¾åº¦çš„å§¿æ€ä½œä¸ºè®­ç»ƒæ ‡ç­¾ã€‚å½“æµ‹è¯•æ•°æ®ä¸è®­ç»ƒé›†ä¸­çš„æ•°æ®ä¸åŒæ—¶æ—¶ï¼Œä¼šå‡ºç°æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œ[[38](#bib.bib38)]æå‡ºäº†MotionTransformerï¼Œå®ƒå…è®¸æƒ¯æ€§å®šä½æ¨¡å‹é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰[[69](#bib.bib69)]å’Œé¢†åŸŸé€‚åº”[[70](#bib.bib70)]è‡ªé€‚åº”åˆ°æ–°é¢†åŸŸï¼Œæ— éœ€æ–°é¢†åŸŸä¸­çš„æ ‡ç­¾ã€‚ä¸ºäº†é¼“åŠ±æ›´å¯é çš„æƒ¯æ€§å®šä½ï¼Œ[[71](#bib.bib71)]èƒ½å¤Ÿç”Ÿæˆå§¿æ€çš„ä¸ç¡®å®šæ€§ï¼Œæä¾›å¯¹å­¦ä¹ åˆ°çš„å§¿æ€å¯ä»¥ä¿¡ä»»ç¨‹åº¦çš„ä¿¡å¿µã€‚ä¸ºäº†å®ç°å®Œå…¨çš„ä¸‰ç»´å®šä½ï¼ŒTLIO
    [[44](#bib.bib44)]å»ºè®®ä»ä¸€ç³»åˆ—ä¸é‡åŠ›å¯¹é½çš„æƒ¯æ€§æ•°æ®ä¸­å­¦ä¹ ä¸‰ç»´ä½ç½®ä½ç§»å’Œåæ–¹å·®ã€‚ä¸ºäº†é¿å…åˆå§‹æ–¹å‘çš„å½±å“ï¼Œæƒ¯æ€§æ•°æ®è¢«è½¬æ¢åˆ°å±€éƒ¨é‡åŠ›å¯¹é½æ¡†æ¶ä¸­ã€‚ç„¶åï¼Œå°†å­¦ä¹ åˆ°çš„ä½ç§»å’Œåæ–¹å·®çº³å…¥æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ä½œä¸ºè§‚æµ‹çŠ¶æ€ï¼Œä¼°è®¡æ–¹å‘ã€é€Ÿåº¦ã€ä½ç½®å’ŒIMUåå·®çš„å…¨çŠ¶æ€ã€‚åœ¨ä¸€ä¸ª3-7åˆ†é’Ÿçš„äººä½“è¿åŠ¨åœºæ™¯ä¸­ï¼ŒTLIOçš„å®šä½è¯¯å·®90%çš„æ—¶é—´åœ¨3ç±³ä»¥å†…ã€‚'
- en: 'Another category of deep learning models aims to leverage learned velocity
    to correct accelerations, as illustrated in Figure [5](#S3.F5 "Figure 5 â€£ III
    Deep Learning Based Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning:
    A Survey")(b). A prominent example is RIDI [[35](#bib.bib35)], which trains a
    deep neural network to predict velocity vectors from inertial data, which are
    then used to correct linear accelerations by subtracting gravity, aligning with
    the constraints of learned velocities. The corrected linear accelerations are
    then doubly integrated to estimate positions. To enhance the accuracy of inertial
    accelerations, RIDI leverages human walking speed as a prior, which compensates
    for the drifts in inertial positioning, effectively constraining them to a lower
    level. RoNIN [[46](#bib.bib46)] improves upon RIDI by transforming inertial measurements
    and learned velocity vectors into a heading-agnostic coordinate frame and introducing
    several novel velocity losses. To minimize the impact of orientation estimation,
    RoNIN employs device orientation to transform inertial data into a frame with
    its Z-axis aligned with gravity. However, a limitation of RoNIN is its reliance
    on orientation estimation. NILoc [[56](#bib.bib56)] is an intriguing trial based
    on RoNIN, which tackles the neural inertial localization problem, aiming to infer
    global location from inertial motion history only. This work recognizes that human
    motion patterns are unique in different locations, which can be utilized as a
    â€fingerprintâ€ to determine the location, similar to WiFi or magnetic-field fingerprinting.
    NILoc first calculates a sequence of velocity from inertial data and then employs
    a Transformer-based DNN framework [[72](#bib.bib72)] to transform the velocity
    sequence into location. However, one fundamental limitation of NILoc is that in
    some areas, such as open spaces, symmetrical or repetitive places, there may not
    be a unique motion pattern.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦ä¸€ç±»æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¨åœ¨åˆ©ç”¨å­¦ä¹ åˆ°çš„é€Ÿåº¦æ¥ä¿®æ­£åŠ é€Ÿåº¦ï¼Œå¦‚å›¾[5](#S3.F5 "Figure 5 â€£ III Deep Learning Based
    Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning: A Survey")(b)æ‰€ç¤ºã€‚ä¸€ä¸ªçªå‡ºçš„ä¾‹å­æ˜¯RIDI
    [[35](#bib.bib35)]ï¼Œè¯¥æ¨¡å‹è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œä»æƒ¯æ€§æ•°æ®ä¸­é¢„æµ‹é€Ÿåº¦çŸ¢é‡ï¼Œç„¶åç”¨è¿™äº›é€Ÿåº¦çŸ¢é‡é€šè¿‡å‡å»é‡åŠ›æ¥ä¿®æ­£çº¿æ€§åŠ é€Ÿåº¦ï¼Œä»è€Œä¸å­¦ä¹ åˆ°çš„é€Ÿåº¦çº¦æŸå¯¹é½ã€‚ä¿®æ­£åçš„çº¿æ€§åŠ é€Ÿåº¦éšåç»è¿‡äºŒé‡ç§¯åˆ†ä»¥ä¼°è®¡ä½ç½®ã€‚ä¸ºäº†æé«˜æƒ¯æ€§åŠ é€Ÿåº¦çš„å‡†ç¡®æ€§ï¼ŒRIDIåˆ©ç”¨äººç±»æ­¥é€Ÿä½œä¸ºå…ˆéªŒï¼Œä»¥è¡¥å¿æƒ¯æ€§å®šä½ä¸­çš„æ¼‚ç§»ï¼Œæœ‰æ•ˆåœ°å°†å…¶çº¦æŸåœ¨è¾ƒä½æ°´å¹³ã€‚RoNIN
    [[46](#bib.bib46)]é€šè¿‡å°†æƒ¯æ€§æµ‹é‡å’Œå­¦ä¹ åˆ°çš„é€Ÿåº¦çŸ¢é‡è½¬æ¢ä¸ºä¸€ä¸ªä¸èˆªå‘æ— å…³çš„åæ ‡æ¡†æ¶ï¼Œå¹¶å¼•å…¥è‹¥å¹²æ–°é¢–çš„é€Ÿåº¦æŸå¤±ï¼Œæ”¹è¿›äº†RIDIã€‚ä¸ºäº†æœ€å°åŒ–æ–¹å‘ä¼°è®¡çš„å½±å“ï¼ŒRoNINåˆ©ç”¨è®¾å¤‡æ–¹å‘å°†æƒ¯æ€§æ•°æ®è½¬æ¢ä¸ºå…¶Zè½´ä¸é‡åŠ›å¯¹é½çš„æ¡†æ¶ã€‚ç„¶è€Œï¼ŒRoNINçš„ä¸€ä¸ªé™åˆ¶æ˜¯å®ƒä¾èµ–äºæ–¹å‘ä¼°è®¡ã€‚NILoc
    [[56](#bib.bib56)]æ˜¯åŸºäºRoNINçš„ä¸€ä¸ªæœ‰è¶£å°è¯•ï¼Œæ—¨åœ¨è§£å†³ç¥ç»æƒ¯æ€§å®šä½é—®é¢˜ï¼Œç›®æ ‡æ˜¯ä»…é€šè¿‡æƒ¯æ€§è¿åŠ¨å†å²æ¨æ–­å…¨çƒä½ç½®ã€‚è¿™é¡¹å·¥ä½œè®¤è¯†åˆ°ï¼Œäººç±»è¿åŠ¨æ¨¡å¼åœ¨ä¸åŒåœ°ç‚¹æ˜¯ç‹¬ç‰¹çš„ï¼Œå¯ä»¥ä½œä¸ºâ€œæŒ‡çº¹â€æ¥ç¡®å®šä½ç½®ï¼Œç±»ä¼¼äºWiFiæˆ–ç£åœºæŒ‡çº¹ã€‚NILocé¦–å…ˆä»æƒ¯æ€§æ•°æ®ä¸­è®¡ç®—å‡ºé€Ÿåº¦åºåˆ—ï¼Œç„¶åä½¿ç”¨åŸºäºTransformerçš„DNNæ¡†æ¶
    [[72](#bib.bib72)]å°†é€Ÿåº¦åºåˆ—è½¬æ¢ä¸ºä½ç½®ã€‚ç„¶è€Œï¼ŒNILocçš„ä¸€ä¸ªåŸºæœ¬é™åˆ¶æ˜¯ï¼Œåœ¨æŸäº›åŒºåŸŸï¼Œå¦‚å¼€æ”¾ç©ºé—´ã€å¯¹ç§°æˆ–é‡å¤çš„åœ°æ–¹ï¼Œå¯èƒ½ä¸å­˜åœ¨ç‹¬ç‰¹çš„è¿åŠ¨æ¨¡å¼ã€‚'
- en: 'An alternative approach involves incorporating learned velocity into the updating
    process of a Kalman filter (KF), as shown in Figure [5](#S3.F5 "Figure 5 â€£ III
    Deep Learning Based Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning:
    A Survey") (c). [[36](#bib.bib36)] uses a ConvNet to infer current speed from
    IMU sequences and incorporates this speed into the Kalman filter as a velocity
    observation to constrain the drifts of SINS-based inertial positioning. This approach
    is similar to the zero-velocity update (ZUPT) method, which detects and uses zero-velocity
    in KF as observations, but instead uses full speeds as observations in KF. Incorporating
    learned velocity allows the KF to handle more complex human motion. A similar
    trial is [[49](#bib.bib49)], that is based on a DNN that infers walking velocity
    in the body frame and combines it with an extended KF. In addition to the learned
    velocity, [[49](#bib.bib49)] produces a noise parameter for KF to dynamically
    update parameters, rather than setting a fixed noise parameter.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦ä¸€ç§æ–¹æ³•æ˜¯å°†å­¦ä¹ å¾—åˆ°çš„é€Ÿåº¦èå…¥å¡å°”æ›¼æ»¤æ³¢å™¨ï¼ˆKFï¼‰çš„æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œå¦‚å›¾ [5](#S3.F5 "Figure 5 â€£ III Deep Learning
    Based Inertial Sensor Calibration â€£ Deep Learning for Inertial Positioning: A
    Survey") (c) æ‰€ç¤ºã€‚[[36](#bib.bib36)] ä½¿ç”¨å·ç§¯ç½‘ç»œï¼ˆConvNetï¼‰ä»IMUåºåˆ—æ¨æ–­å½“å‰é€Ÿåº¦ï¼Œå¹¶å°†æ­¤é€Ÿåº¦ä½œä¸ºé€Ÿåº¦è§‚æµ‹å€¼èå…¥å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œä»¥çº¦æŸåŸºäºSINSçš„æƒ¯æ€§å®šä½çš„æ¼‚ç§»ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºé›¶é€Ÿåº¦æ›´æ–°ï¼ˆZUPTï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ£€æµ‹å¹¶åˆ©ç”¨å¡å°”æ›¼æ»¤æ³¢å™¨ä¸­çš„é›¶é€Ÿåº¦ä½œä¸ºè§‚æµ‹å€¼ï¼Œä½†åœ¨KFä¸­ä½¿ç”¨å®Œæ•´é€Ÿåº¦ä½œä¸ºè§‚æµ‹å€¼ã€‚èå…¥å­¦ä¹ å¾—åˆ°çš„é€Ÿåº¦å¯ä»¥ä½¿KFå¤„ç†æ›´å¤æ‚çš„äººä½“è¿åŠ¨ã€‚ç±»ä¼¼çš„è¯•éªŒæ˜¯
    [[49](#bib.bib49)]ï¼Œå…¶åŸºäºDNNæ¨æ–­èº«ä½“æ¡†æ¶ä¸­çš„æ­¥è¡Œé€Ÿåº¦ï¼Œå¹¶å°†å…¶ä¸æ‰©å±•KFç»“åˆã€‚åœ¨å­¦ä¹ å¾—åˆ°çš„é€Ÿåº¦ä¹‹å¤–ï¼Œ[[49](#bib.bib49)]
    ä¸ºKFç”Ÿæˆå™ªå£°å‚æ•°ï¼Œä»¥åŠ¨æ€æ›´æ–°å‚æ•°ï¼Œè€Œä¸æ˜¯è®¾ç½®å›ºå®šçš„å™ªå£°å‚æ•°ã€‚'
- en: Inertial positioning heavily relies on accurately estimating the deviceâ€™s attitude.
    Several methods aim to improve orientation estimation to enhance the performance
    of deep learning based inertial odometry. RIDI, RoNIN, and TLIO still depend on
    device orientation to rotate inertial data into a suitable frame. To address this
    problem, IDOL [[51](#bib.bib51)] proposes a two-stage process that first learns
    orientation from data and then rotates inertial data into the appropriate frame,
    followed by learning the position. [[58](#bib.bib58)] estimates orientation using
    magnetic data and combines it with learned odometry to reduce positioning drifts
    while minimizing reliance on device orientation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ¯æ€§å®šä½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå‡†ç¡®ä¼°è®¡è®¾å¤‡çš„å§¿æ€ã€‚ä¸€äº›æ–¹æ³•æ—¨åœ¨æ”¹è¿›æ–¹å‘ä¼°è®¡ï¼Œä»¥å¢å¼ºåŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§æµ‹è·çš„æ€§èƒ½ã€‚RIDiã€RoNINå’ŒTLIOä»ç„¶ä¾èµ–äºè®¾å¤‡æ–¹å‘å°†æƒ¯æ€§æ•°æ®æ—‹è½¬åˆ°é€‚å½“çš„æ¡†æ¶ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒIDOL
    [[51](#bib.bib51)] æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¿‡ç¨‹ï¼Œé¦–å…ˆä»æ•°æ®ä¸­å­¦ä¹ æ–¹å‘ï¼Œç„¶åå°†æƒ¯æ€§æ•°æ®æ—‹è½¬åˆ°é€‚å½“çš„æ¡†æ¶ä¸­ï¼Œæ¥ç€å­¦ä¹ ä½ç½®ã€‚[[58](#bib.bib58)]
    ä½¿ç”¨ç£æ•°æ®ä¼°è®¡æ–¹å‘ï¼Œå¹¶å°†å…¶ä¸å­¦ä¹ åˆ°çš„æµ‹è·ç»“åˆï¼Œä»¥å‡å°‘å®šä½æ¼‚ç§»ï¼ŒåŒæ—¶æœ€å°åŒ–å¯¹è®¾å¤‡æ–¹å‘çš„ä¾èµ–ã€‚
- en: 'Figure [6](#S4.F6 "Figure 6 â€£ IV Learning to correct IMU Integration â€£ Deep
    Learning for Inertial Positioning: A Survey") showcases several examples of deep
    learning based inertial positioning results.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ [6](#S4.F6 "Figure 6 â€£ IV Learning to correct IMU Integration â€£ Deep Learning
    for Inertial Positioning: A Survey") å±•ç¤ºäº†å‡ ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½ç»“æœç¤ºä¾‹ã€‚'
- en: '![Refer to caption](img/97693b4300fb0d29c2e5ba2e1f9e3d41.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/97693b4300fb0d29c2e5ba2e1f9e3d41.png)'
- en: 'Figure 6: Sample results of deep learning based inertial positioning from (a)
    VR device for pedestrian tracking (reprint from TLIO [[44](#bib.bib44)]) (b) smartphone
    for trolly tracking (reprint from IONet [[34](#bib.bib34)])'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½æ ·æœ¬ç»“æœï¼ˆaï¼‰æ¥è‡ªVRè®¾å¤‡çš„è¡Œäººè·Ÿè¸ªï¼ˆè½¬è½½è‡ª TLIO [[44](#bib.bib44)]ï¼‰ (b) æ¥è‡ªæ™ºèƒ½æ‰‹æœºçš„æ¨è½¦è·Ÿè¸ªï¼ˆè½¬è½½è‡ª
    IONet [[34](#bib.bib34)]ï¼‰
- en: V Learning to Correct Pedestrian Inertial Positioning
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V æ ¡æ­£è¡Œäººæƒ¯æ€§å®šä½çš„å­¦ä¹ 
- en: The previous subsection addressed the general application of deep learning in
    correcting inertial positioning drifts. This subsection focuses on the specific
    use of deep learning to address particular aspects of pedestrian navigation algorithms,
    namely Pedestrian Dead Reckoning (PDR) and Zero-Velocity Update (ZUPT).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸€èŠ‚è®¨è®ºäº†æ·±åº¦å­¦ä¹ åœ¨æ ¡æ­£æƒ¯æ€§å®šä½æ¼‚ç§»ä¸­çš„ä¸€èˆ¬åº”ç”¨ã€‚æœ¬èŠ‚é‡ç‚¹å…³æ³¨æ·±åº¦å­¦ä¹ åœ¨è§£å†³è¡Œäººå¯¼èˆªç®—æ³•ä¸­ç‰¹å®šæ–¹é¢çš„åº”ç”¨ï¼Œå³è¡Œäººæ­»è®°æ³•ï¼ˆPDRï¼‰å’Œé›¶é€Ÿåº¦æ›´æ–°ï¼ˆZUPTï¼‰ã€‚
- en: V-A Learning to correct PDR
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A æ ¡æ­£PDRçš„å­¦ä¹ 
- en: Pedestrian dead reckoning (PDR) error drifts often stem from inaccurate stride
    and heading estimates. To address these issues, researchers have incorporated
    deep learning techniques into the process of step detection, dynamic step length
    estimation, and walking heading estimation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œäººæ­»è®°æ³•ï¼ˆPDRï¼‰è¯¯å·®æ¼‚ç§»é€šå¸¸æºäºä¸å‡†ç¡®çš„æ­¥å¹…å’Œæ–¹å‘ä¼°è®¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å°†æ·±åº¦å­¦ä¹ æŠ€æœ¯èå…¥äº†æ­¥æ€æ£€æµ‹ã€åŠ¨æ€æ­¥é•¿ä¼°è®¡å’Œæ­¥æ€æ–¹å‘ä¼°è®¡çš„è¿‡ç¨‹ã€‚
- en: To estimate walking stride more robustly, researchers have sought to solve it
    in a data-driven way. One such method is SmartStep [[73](#bib.bib73)], a deep
    learning-based step detection framework that achieves 99% accuracy in step detection
    tasks across various motion modes. Compared to peak/valley detection-based methods,
    data-driven methods do not require IMUs to be fixed in position, specific motion
    modes, or pre-calibration and threshold setting. Another approach involves using
    LSTM to regress walking stride from raw inertial data [[41](#bib.bib41)]. This
    method has demonstrated effectiveness in various human motions, such as walking,
    running, jogging, and random movements. Additionally, StepNet [[48](#bib.bib48)]
    learns to estimate step length dynamically, i.e., the change in distance, which
    achieves an impressive performance with only a 2.1%-3.2% error rate when compared
    to traditional static step length estimation. The attachment mode of the device,
    such as in hand or in pocket, can also influence walking stride estimation. To
    address this problem, Bo et al. [[62](#bib.bib62)] employed domain adaptation
    [[70](#bib.bib70)] to extract domain-invariant features for stride estimation,
    which enhanced the performance in new domains, such as holding, calling, pocket,
    and swinging.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´ç¨³å¥åœ°ä¼°è®¡æ­¥å¹…ï¼Œç ”ç©¶äººå‘˜å¯»æ±‚é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…¶ä¸­ä¸€ç§æ–¹æ³•æ˜¯ SmartStep [[73](#bib.bib73)]ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ­¥æ€æ£€æµ‹æ¡†æ¶ï¼Œåœ¨å„ç§è¿åŠ¨æ¨¡å¼ä¸‹çš„æ­¥æ€æ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†
    99% çš„å‡†ç¡®ç‡ã€‚ä¸åŸºäºå³°å€¼/è°·å€¼æ£€æµ‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ•°æ®é©±åŠ¨çš„æ–¹æ³•ä¸éœ€è¦ IMU å›ºå®šä½ç½®ã€ç‰¹å®šè¿åŠ¨æ¨¡å¼æˆ–é¢„æ ¡å‡†å’Œé˜ˆå€¼è®¾ç½®ã€‚å¦ä¸€ç§æ–¹æ³•æ¶‰åŠä½¿ç”¨ LSTM ä»åŸå§‹æƒ¯æ€§æ•°æ®ä¸­å›å½’æ­¥å¹…
    [[41](#bib.bib41)]ã€‚è¿™ç§æ–¹æ³•åœ¨å„ç§äººä½“è¿åŠ¨ä¸­æ˜¾ç¤ºäº†æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚æ­¥è¡Œã€è·‘æ­¥ã€æ…¢è·‘å’Œéšæœºè¿åŠ¨ã€‚æ­¤å¤–ï¼ŒStepNet [[48](#bib.bib48)]
    å­¦ä¹ åŠ¨æ€ä¼°è®¡æ­¥é•¿ï¼Œå³è·ç¦»çš„å˜åŒ–ï¼Œåœ¨ä¸ä¼ ç»Ÿé™æ€æ­¥é•¿ä¼°è®¡ç›¸æ¯”æ—¶ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œè¯¯å·®ç‡ä»…ä¸º 2.1%-3.2%ã€‚è®¾å¤‡çš„é™„ç€æ¨¡å¼ï¼Œå¦‚æ‰‹æŒæˆ–æ”¾åœ¨å£è¢‹ä¸­ï¼Œä¹Ÿä¼šå½±å“æ­¥å¹…ä¼°è®¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒBo
    ç­‰äºº [[62](#bib.bib62)] é‡‡ç”¨äº†é¢†åŸŸé€‚åº” [[70](#bib.bib70)] æ¥æå–é¢†åŸŸä¸å˜ç‰¹å¾è¿›è¡Œæ­¥å¹…ä¼°è®¡ï¼Œä»è€Œåœ¨æ–°é¢†åŸŸï¼ˆå¦‚æŒç‰©ã€é€šè¯ã€å£è¢‹å’Œæ‘†åŠ¨ï¼‰ä¸­æé«˜äº†æ€§èƒ½ã€‚
- en: Accurate heading estimation is crucial for updating position in the right direction
    in PDR. To achieve more accurate and robust heading estimation, Wang et al. [[42](#bib.bib42)]
    utilize a Spatial Transformer Network [[74](#bib.bib74)] and LSTM to learn heading
    direction from the inertial sensor attached to an unconstrained device. However,
    one problem that arises is the misalignment between the device heading and pedestrian
    heading, making it difficult to estimate the real walking heading based on sensor
    data. To address this misalignment issue, [[75](#bib.bib75)] introduces a deep
    neural network to estimate walking direction in the sensorâ€™s frame. They derive
    a geometric model to convert walking direction from the sensorâ€™s frame into a
    reference frame (i.e., north and east coordinates) by exploiting acceleration
    and magnetic data. This geometric model is combined with a learning framework
    to produce heading estimates. When tested on unseen data, this work reports a
    median heading error of 10Â°. PDRNet [[52](#bib.bib52)] follows the process of
    a traditional PDR algorithm but replaces the step length and heading estimation
    modules with deep neural networks. Their experiments indicate that learning step
    length and heading together outperforms regressing them separately.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®çš„æ–¹å‘ä¼°è®¡å¯¹äºåœ¨ PDR ä¸­æ²¿æ­£ç¡®æ–¹å‘æ›´æ–°ä½ç½®è‡³å…³é‡è¦ã€‚ä¸ºäº†å®ç°æ›´å‡†ç¡®å’Œç¨³å¥çš„æ–¹å‘ä¼°è®¡ï¼ŒWang ç­‰äºº [[42](#bib.bib42)] åˆ©ç”¨ç©ºé—´å˜æ¢ç½‘ç»œ
    [[74](#bib.bib74)] å’Œ LSTM ä»é™„åŠ åˆ°ä¸å—çº¦æŸè®¾å¤‡çš„æƒ¯æ€§ä¼ æ„Ÿå™¨ä¸­å­¦ä¹ æ–¹å‘ã€‚ç„¶è€Œï¼Œå‡ºç°çš„é—®é¢˜æ˜¯è®¾å¤‡æ–¹å‘ä¸è¡Œäººæ–¹å‘ä¹‹é—´çš„é”™ä½ï¼Œä½¿å¾—åŸºäºä¼ æ„Ÿå™¨æ•°æ®ä¼°è®¡çœŸå®çš„æ­¥è¡Œæ–¹å‘å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé”™ä½é—®é¢˜ï¼Œ[[75](#bib.bib75)]
    å¼•å…¥äº†æ·±åº¦ç¥ç»ç½‘ç»œæ¥ä¼°è®¡ä¼ æ„Ÿå™¨æ¡†æ¶ä¸­çš„è¡Œèµ°æ–¹å‘ã€‚ä»–ä»¬æ¨å¯¼å‡ºä¸€ä¸ªå‡ ä½•æ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨åŠ é€Ÿåº¦å’Œç£æ•°æ®ï¼Œå°†ä¼ æ„Ÿå™¨æ¡†æ¶ä¸­çš„è¡Œèµ°æ–¹å‘è½¬æ¢ä¸ºå‚è€ƒæ¡†æ¶ï¼ˆå³åŒ—æ–¹å’Œä¸œæ–¹åæ ‡ï¼‰ã€‚è¿™ä¸ªå‡ ä½•æ¨¡å‹ä¸å­¦ä¹ æ¡†æ¶ç»“åˆä»¥äº§ç”Ÿæ–¹å‘ä¼°è®¡ã€‚å½“åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œè¿™é¡¹å·¥ä½œæŠ¥å‘Šäº†
    10Â° çš„ä¸­ä½æ–¹å‘è¯¯å·®ã€‚PDRNet [[52](#bib.bib52)] éµå¾ªä¼ ç»Ÿ PDR ç®—æ³•çš„è¿‡ç¨‹ï¼Œä½†ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ›¿ä»£äº†æ­¥é•¿å’Œæ–¹å‘ä¼°è®¡æ¨¡å—ã€‚ä»–ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå­¦ä¹ æ­¥é•¿å’Œæ–¹å‘ä¸€èµ·ä¼˜äºå•ç‹¬å›å½’å®ƒä»¬ã€‚
- en: V-B Learning to correct ZUPT
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B å­¦ä¹ çº æ­£ ZUPT
- en: In pedestrian inertial navigation systems (INS) based on zero-velocity update
    (ZUPT), the zero-velocity phase is utilized to correct inertial positioning errors
    through Kalman filtering. Therefore, the accuracy of zero-velocity detection is
    crucial in determining when to update the system states. However, traditional
    threshold-based zero-velocity detection is complicated by the mixed variety of
    motions experienced by humans, making it challenging to set a reliable threshold
    when the user is still.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºé›¶é€Ÿåº¦æ›´æ–°ï¼ˆZUPTï¼‰çš„è¡Œäººæƒ¯æ€§å¯¼èˆªç³»ç»Ÿï¼ˆINSï¼‰ä¸­ï¼Œé›¶é€Ÿåº¦é˜¶æ®µè¢«ç”¨æ¥é€šè¿‡å¡å°”æ›¼æ»¤æ³¢ä¿®æ­£æƒ¯æ€§å®šä½è¯¯å·®ã€‚å› æ­¤ï¼Œé›¶é€Ÿåº¦æ£€æµ‹çš„å‡†ç¡®æ€§å¯¹äºç¡®å®šä½•æ—¶æ›´æ–°ç³»ç»ŸçŠ¶æ€è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºé˜ˆå€¼çš„é›¶é€Ÿåº¦æ£€æµ‹å—åˆ°äººç±»ç»å†çš„å„ç§æ··åˆè¿åŠ¨çš„å½±å“ï¼Œä½¿å¾—åœ¨ç”¨æˆ·é™æ­¢æ—¶è®¾ç½®å¯é çš„é˜ˆå€¼å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: To address this issue, researchers have explored data-driven approaches that
    utilize the powerful feature extraction and classification capabilities of deep
    learning to classify whether the user is in the ZUPT phase. For instance, [[37](#bib.bib37)]
    proposes a six-layer long short-term memory (LSTM) network to detect zero-velocity.
    The LSTM inputs a sequence of IMU data, typically 100 consecutive data points,
    and outputs the probability of whether the user is still or in motion at the current
    timestep. The results from the LSTM-based zero-velocity detection are then fed
    into a ZUPT-based INS. The proposed approach achieves a reduction in localization
    error by over 34% compared to fixed threshold-based ZVDs and was shown to be more
    robust during a mixed variety of motions, such as walking, running, and climbing
    stairs. Similarly, [[43](#bib.bib43)] designs an adaptive ZUPT using convolutional
    neural networks (ConvNet) to classify ZVDs based on IMU sequences. Deep learning
    approaches, such as LSTM and ConvNet, have demonstrated excellent performance
    in extracting robust and useful features for zero-velocity identification, irrespective
    of different users, motion modes, and attachment places.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜æ¢ç´¢äº†æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ å¼ºå¤§çš„ç‰¹å¾æå–å’Œåˆ†ç±»èƒ½åŠ›æ¥åˆ†ç±»ç”¨æˆ·æ˜¯å¦å¤„äº ZUPT é˜¶æ®µã€‚ä¾‹å¦‚ï¼Œ[[37](#bib.bib37)]
    æå‡ºäº†ä¸€ä¸ªå…­å±‚é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ç½‘ç»œæ¥æ£€æµ‹é›¶é€Ÿåº¦ã€‚LSTM è¾“å…¥ IMU æ•°æ®åºåˆ—ï¼Œé€šå¸¸æ˜¯ 100 ä¸ªè¿ç»­çš„æ•°æ®ç‚¹ï¼Œå¹¶è¾“å‡ºå½“å‰æ—¶é—´æ­¥ç”¨æˆ·æ˜¯å¦é™æ­¢æˆ–è¿åŠ¨çš„æ¦‚ç‡ã€‚LSTM
    åŸºäºé›¶é€Ÿåº¦æ£€æµ‹çš„ç»“æœéšåè¾“å…¥ ZUPT åŸºç¡€çš„ INSã€‚è¯¥æ–¹æ³•ä¸åŸºäºå›ºå®šé˜ˆå€¼çš„ ZVD ç›¸æ¯”ï¼Œå®ç°äº†è¶…è¿‡ 34% çš„å®šä½è¯¯å·®å‡å°‘ï¼Œå¹¶ä¸”åœ¨æ­¥è¡Œã€è·‘æ­¥å’Œçˆ¬æ¥¼æ¢¯ç­‰æ··åˆè¿åŠ¨ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚ç±»ä¼¼åœ°ï¼Œ[[43](#bib.bib43)]
    è®¾è®¡äº†ä¸€ç§ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetï¼‰çš„è‡ªé€‚åº” ZUPTï¼Œæ ¹æ® IMU åºåˆ—åˆ†ç±» ZVDã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ LSTM å’Œ ConvNetï¼Œåœ¨é›¶é€Ÿåº¦è¯†åˆ«ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯ä¸åŒç”¨æˆ·ã€è¿åŠ¨æ¨¡å¼è¿˜æ˜¯é™„ä»¶ä½ç½®ã€‚
- en: VI Learning to Correct Inertial Positioning on Vehicles, UAV and robotic platforms
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI å­¦ä¹ åœ¨è½¦è¾†ã€æ— äººæœºå’Œæœºå™¨äººå¹³å°ä¸Šä¿®æ­£æƒ¯æ€§å®šä½
- en: As previously mentioned, deep learning methods have shown great potential in
    addressing the challenges of pedestrian inertial navigation. However, these techniques
    can also be applied to other platforms, such as vehicles, UAVs, robots, and more.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è§£å†³è¡Œäººæƒ¯æ€§å¯¼èˆªçš„æŒ‘æˆ˜æ–¹é¢å±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–å¹³å°ï¼Œå¦‚è½¦è¾†ã€æ— äººæœºã€æœºå™¨äººç­‰ã€‚
- en: These platforms share similarities with pedestrians, such as the ability to
    infer movement velocity from inertial data. This is because inertial data contains
    vibration information that reflects the fundamental frequency proportional to
    the vehicle speed. Building on the success of IONet [[34](#bib.bib34)], [[39](#bib.bib39)]
    proposes AbolDeepIO, an improved triple-channel LSTM network that predicts polar
    vectors for drone localization from inertial data sequences. AbolDeepIO has been
    evaluated on a public drone dataset and has shown competitive performance compared
    to traditional visual-inertial odometry methods like VINS-mono.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¹³å°ä¸è¡Œäººç±»ä¼¼ï¼Œæ¯”å¦‚èƒ½å¤Ÿä»æƒ¯æ€§æ•°æ®æ¨æ–­è¿åŠ¨é€Ÿåº¦ã€‚è¿™æ˜¯å› ä¸ºæƒ¯æ€§æ•°æ®åŒ…å«çš„æŒ¯åŠ¨ä¿¡æ¯åæ˜ äº†ä¸è½¦è¾†é€Ÿåº¦æˆæ­£æ¯”çš„åŸºæœ¬é¢‘ç‡ã€‚åœ¨ IONet çš„æˆåŠŸåŸºç¡€ä¸Šï¼Œ[[34](#bib.bib34)]
    æå‡ºäº† AbolDeepIOï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›çš„ä¸‰é€šé“ LSTM ç½‘ç»œï¼Œç”¨äºä»æƒ¯æ€§æ•°æ®åºåˆ—ä¸­é¢„æµ‹æ— äººæœºå®šä½çš„æåæ ‡å‘é‡ã€‚AbolDeepIO å·²åœ¨ä¸€ä¸ªå…¬å…±æ— äººæœºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºå‡ºç›¸è¾ƒäºä¼ ç»Ÿçš„è§†è§‰-æƒ¯æ€§é‡Œç¨‹è®¡æ–¹æ³•ï¼Œå¦‚
    VINS-monoï¼Œå…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚
- en: When deploying deep learning-based inertial navigation on real-world devices,
    prediction accuracy and model efficiency must be considered. To address this,
    TinyOdom [[59](#bib.bib59)] aims to deploy neural inertial odometry models on
    resource-constrained devices. It proposes a lightweight model based on temporal
    convolutional networks (TCN) [[76](#bib.bib76)] to learn position displacement
    and optimizes the model through neural architecture search (NAS) [[77](#bib.bib77)]
    to reduce model size between 31 and 134 times. TinyOdom was extensively evaluated
    on tracking pedestrians, animals, aerial, and underwater vehicles. Within 60 seconds,
    its localization error is between 2.5 and 12 meters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…è®¾å¤‡ä¸Šéƒ¨ç½²åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å¯¼èˆªæ—¶ï¼Œå¿…é¡»è€ƒè™‘é¢„æµ‹ç²¾åº¦å’Œæ¨¡å‹æ•ˆç‡ã€‚ä¸ºæ­¤ï¼ŒTinyOdom [[59](#bib.bib59)] æ—¨åœ¨å°†ç¥ç»æƒ¯æ€§é‡Œç¨‹è®¡æ¨¡å‹éƒ¨ç½²åˆ°èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚å®ƒæå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´å·ç§¯ç½‘ç»œï¼ˆTCNï¼‰[[76](#bib.bib76)]
    çš„è½»é‡çº§æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ ä½ç½®ä½ç§»ï¼Œå¹¶é€šè¿‡ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰[[77](#bib.bib77)] ä¼˜åŒ–æ¨¡å‹ï¼Œä»¥å°†æ¨¡å‹å°ºå¯¸å‡å°‘31è‡³134å€ã€‚TinyOdom
    åœ¨è·Ÿè¸ªè¡Œäººã€åŠ¨ç‰©ã€ç©ºä¸­å’Œæ°´ä¸‹è½¦è¾†æ–¹é¢è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚åœ¨60ç§’å†…ï¼Œå…¶å®šä½è¯¯å·®åœ¨2.5è‡³12ç±³ä¹‹é—´ã€‚
- en: Learning-based inertial odometry has also been extended to legged robots by
    [[53](#bib.bib53)]. The learned location displacement is combined with kinematic
    motion models to estimate robot system states at high frequencies (400 Hz). In
    this work, the robot successfully navigated a field experiment, where a legged
    robot walked around for 20 minutes in a mine with poor illumination and visual
    feature tracking failures.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå­¦ä¹ çš„æƒ¯æ€§é‡Œç¨‹è®¡ä¹Ÿå·²ç»æ‰©å±•åˆ°æœ‰è…¿æœºå™¨äºº[[53](#bib.bib53)]ã€‚å­¦ä¹ åˆ°çš„ä½ç½®ä½ç§»ä¸è¿åŠ¨å­¦æ¨¡å‹ç»“åˆï¼Œä»¥é«˜é¢‘ç‡ï¼ˆ400 Hzï¼‰ä¼°è®¡æœºå™¨äººç³»ç»ŸçŠ¶æ€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæœºå™¨äººæˆåŠŸåœ°è¿›è¡Œäº†åœºåœ°å®éªŒï¼Œåœ¨ä¸€ä¸ªå…‰çº¿è¾ƒå·®ä¸”è§†è§‰ç‰¹å¾è·Ÿè¸ªå¤±è´¥çš„çŸ¿åŒºä¸­ï¼Œè…¿å¼æœºå™¨äººèµ°äº†20åˆ†é’Ÿã€‚
- en: 'TABLE III: A summary of existing methods on deep learning based sensor fusion.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨IIIï¼šå…³äºæ·±åº¦å­¦ä¹ ä¼ æ„Ÿå™¨èåˆçš„ç°æœ‰æ–¹æ³•çš„æ€»ç»“ã€‚
- en: '| name | year | sensor | model | learning | target |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| name | year | sensor | model | learning | target |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| VINet[[78](#bib.bib78)] | 2017 | MC+I | ConvNet, LSTM | SL | formulating
    VIO as a sequential learning problem |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| VINet[[78](#bib.bib78)] | 2017 | MC+I | ConvNet, LSTM | SL | å°†VIOå½¢å¼åŒ–ä¸ºåºåˆ—å­¦ä¹ é—®é¢˜
    |'
- en: '| VIOLearner[[79](#bib.bib79)] | 2018 | MC+I | ConvNet | UL | VIO with online
    correction module |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| VIOLearner[[79](#bib.bib79)] | 2018 | MC+I | ConvNet | UL | å¸¦åœ¨çº¿ä¿®æ­£æ¨¡å—çš„VIO |'
- en: '| Chen et al.[[80](#bib.bib80)] | 2019 | MC+I | ConvNet, LSTM, Attention |
    SL | feature selection for deep VIO |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al.[[80](#bib.bib80)] | 2019 | MC+I | ConvNet, LSTM, Attention |
    SL | æ·±åº¦VIOçš„ç‰¹å¾é€‰æ‹© |'
- en: '| DeepVIO[[81](#bib.bib81)] | 2019 | SC+I | ConvNet, LSTM | UL | learning VIO
    from stereo images and IMU |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| DeepVIO[[81](#bib.bib81)] | 2019 | SC+I | ConvNet, LSTM | UL | ä»ç«‹ä½“å›¾åƒå’ŒIMUä¸­å­¦ä¹ VIO
    |'
- en: '| DeepTIO[[82](#bib.bib82)] | 2020 | T+I | ConvNet, LSTM, Attention | SL |
    learning pose from thermal and inertial data |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| DeepTIO[[82](#bib.bib82)] | 2020 | T+I | ConvNet, LSTM, Attention | SL |
    ä»çƒ­æˆåƒå’Œæƒ¯æ€§æ•°æ®ä¸­å­¦ä¹ å§¿æ€ |'
- en: '| MilliEgo [[83](#bib.bib83)] | 2020 | MR+I | ConvNet, LSTM, Attention | SL
    | learning pose from mmWare radar and inertial data |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| MilliEgo [[83](#bib.bib83)] | 2020 | MR+I | ConvNet, LSTM, Attention | SL
    | ä»æ¯«ç±³æ³¢é›·è¾¾å’Œæƒ¯æ€§æ•°æ®ä¸­å­¦ä¹ å§¿æ€ |'
- en: '| UnVIO [[84](#bib.bib84)] | 2021 | MC+I | ConvNet, LSTM, Attention | UL |
    unsupervised learning of VIO |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| UnVIO [[84](#bib.bib84)] | 2021 | MC+I | ConvNet, LSTM, Attention | UL |
    æ— ç›‘ç£å­¦ä¹ VIO |'
- en: '| DynaNet [[85](#bib.bib85)] | 2021 | MC+I | ConvNet, LSTM | SL | combining
    DNN with Kalman filtering |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DynaNet [[85](#bib.bib85)] | 2021 | MC+I | ConvNet, LSTM | SL | ç»“åˆDNNä¸å¡å°”æ›¼æ»¤æ³¢
    |'
- en: '| SelfVIO [[86](#bib.bib86)] | 2022 | MC+I | ConvNet, LSTM, Attention | UL
    | unsupervised VIO with GAN-based depth generator |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| SelfVIO [[86](#bib.bib86)] | 2022 | MC+I | ConvNet, LSTM, Attention | UL
    | åŸºäºGANçš„æ·±åº¦ç”Ÿæˆå™¨çš„æ— ç›‘ç£VIO |'
- en: '| Tu et al. [[87](#bib.bib87)] | 2022 | L+I | ConvNet, LSTM, Attention | UL
    | unsupervised learning of LIDAR-inertial odometry |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Tu et al. [[87](#bib.bib87)] | 2022 | L+I | ConvNet, LSTM, Attention | UL
    | æ— ç›‘ç£å­¦ä¹ LIDAR-æƒ¯æ€§é‡Œç¨‹è®¡ |'
- en: â€¢
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Year indicates the publication year of each work.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Year æŒ‡æ¯é¡¹å·¥ä½œçš„å‡ºç‰ˆå¹´ä»½ã€‚
- en: â€¢
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Sensor indicates the sensors involved in each work. I, MC, SC, T, MR, L, A represent
    inertial sensor, monocular camera, stereo camera, thermal camera, millimeter wave
    radar, LIDAR and airflow sensor respectively.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sensor æŒ‡æ¯é¡¹å·¥ä½œçš„ä¼ æ„Ÿå™¨ã€‚æˆ‘ã€MCã€SCã€Tã€MRã€Lã€A åˆ†åˆ«ä»£è¡¨æƒ¯æ€§ä¼ æ„Ÿå™¨ã€å•ç›®ç›¸æœºã€ç«‹ä½“ç›¸æœºã€çƒ­æˆåƒç›¸æœºã€æ¯«ç±³æ³¢é›·è¾¾ã€LIDAR å’Œæ°”æµä¼ æ„Ÿå™¨ã€‚
- en: â€¢
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Learning indicates how to train neural networks. SL and UL represent Supervised
    Learning and Unsupervised Learning.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Learning æŒ‡å¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œã€‚SL å’Œ UL åˆ†åˆ«ä»£è¡¨ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ã€‚
- en: In the realm of inertial positioning for vehicles, researchers have proposed
    various methods to mitigate error drifts and improve accuracy. One such method
    is presented in [[47](#bib.bib47)], where error covariances are learned from inertial
    data and incorporated into Kalman filtering for updating system states. This approach
    has been shown to improve inertial positioning performance. Similar to ZUPT-based
    pedestrian positioning, zero-velocity-update (ZUPT) can also be used for car-equipped
    inertial navigation systems. The zero-velocity phase provides valuable context
    information to correct system error drifts via Kalman filtering. OdoNet, presented
    in [[63](#bib.bib63)], is an example of a system that learns and utilizes car
    speed along with a zero-velocity detector to reduce error drifts in car-equipped
    IMU systems. Deep learning techniques have also been explored for detecting zero-velocity
    phases in vehicle navigation. For example, [[40](#bib.bib40)] proposes a deep
    learning-based method for detecting zero-velocity phases in vehicle navigation.
    In another study, [[54](#bib.bib54)] derives a model with motion terms that are
    relevant only to the IMU data sequence. This model provides theoretical guidance
    for learning models to infer useful terms and has been evaluated on a drone dataset,
    where it outperformed TLIO and other learning methods.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è½¦è¾†æƒ¯æ€§å®šä½é¢†åŸŸï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å„ç§æ–¹æ³•æ¥å‡è½»è¯¯å·®æ¼‚ç§»å¹¶æé«˜å‡†ç¡®æ€§ã€‚å…¶ä¸­ä¸€ç§æ–¹æ³•åœ¨[[47](#bib.bib47)]ä¸­æå‡ºï¼Œé€šè¿‡ä»æƒ¯æ€§æ•°æ®ä¸­å­¦ä¹ è¯¯å·®åæ–¹å·®å¹¶å°†å…¶çº³å…¥å¡å°”æ›¼æ»¤æ³¢å™¨ä¸­æ›´æ–°ç³»ç»ŸçŠ¶æ€ã€‚è¯¥æ–¹æ³•å·²è¢«è¯æ˜èƒ½æ”¹å–„æƒ¯æ€§å®šä½æ€§èƒ½ã€‚ç±»ä¼¼äºåŸºäºZUPTçš„è¡Œäººå®šä½ï¼Œé›¶é€Ÿåº¦æ›´æ–°ï¼ˆZUPTï¼‰ä¹Ÿå¯ä»¥ç”¨äºè½¦è½½æƒ¯æ€§å¯¼èˆªç³»ç»Ÿã€‚é›¶é€Ÿåº¦é˜¶æ®µæä¾›äº†å®è´µçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡å¡å°”æ›¼æ»¤æ³¢æ¥çº æ­£ç³»ç»Ÿè¯¯å·®æ¼‚ç§»ã€‚[[63](#bib.bib63)]ä¸­æå‡ºçš„OdoNetç³»ç»Ÿå°±æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œå®ƒé€šè¿‡å­¦ä¹ å’Œåˆ©ç”¨è½¦è¾†é€Ÿåº¦ä»¥åŠé›¶é€Ÿåº¦æ£€æµ‹å™¨æ¥å‡å°‘è½¦è½½IMUç³»ç»Ÿä¸­çš„è¯¯å·®æ¼‚ç§»ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¹Ÿè¢«ç”¨äºæ£€æµ‹è½¦è¾†å¯¼èˆªä¸­çš„é›¶é€Ÿåº¦é˜¶æ®µã€‚ä¾‹å¦‚ï¼Œ[[40](#bib.bib40)]æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ¥æ£€æµ‹è½¦è¾†å¯¼èˆªä¸­çš„é›¶é€Ÿåº¦é˜¶æ®µã€‚åœ¨å¦ä¸€é¡¹ç ”ç©¶ä¸­ï¼Œ[[54](#bib.bib54)]æ¨å¯¼äº†ä¸€ä¸ªä»…ä¸IMUæ•°æ®åºåˆ—ç›¸å…³çš„è¿åŠ¨é¡¹æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ºå­¦ä¹ æ¨¡å‹æ¨æ–­æœ‰ç”¨çš„é¡¹æä¾›äº†ç†è®ºæŒ‡å¯¼ï¼Œå¹¶åœ¨æ— äººæœºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœä¼˜äºTLIOå’Œå…¶ä»–å­¦ä¹ æ–¹æ³•ã€‚
- en: Overall, these studies demonstrate the potential of deep learning-based methods
    in improving inertial navigation for various platforms, including pedestrians,
    vehicles, drones, and robots. By leveraging the rich information contained within
    IMU data, deep learning models can effectively mitigate error drifts and improve
    the accuracy of inertial positioning systems. Furthermore, by optimizing the model
    efficiency and considering deployment on resource-constrained devices, these techniques
    can be applied in real-world scenarios.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œè¿™äº›ç ”ç©¶å±•ç¤ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨æé«˜å„ç§å¹³å°ï¼ˆåŒ…æ‹¬è¡Œäººã€è½¦è¾†ã€æ— äººæœºå’Œæœºå™¨äººï¼‰æƒ¯æ€§å¯¼èˆªæ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨IMUæ•°æ®ä¸­åŒ…å«çš„ä¸°å¯Œä¿¡æ¯ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡è½»è¯¯å·®æ¼‚ç§»ï¼Œæé«˜æƒ¯æ€§å®šä½ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¼˜åŒ–æ¨¡å‹æ•ˆç‡å¹¶è€ƒè™‘åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥åº”ç”¨äºå®é™…åœºæ™¯ä¸­ã€‚
- en: VII Deep Learning based Sensor Fusion
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII åŸºäºæ·±åº¦å­¦ä¹ çš„ä¼ æ„Ÿå™¨èåˆ
- en: Integrating inertial sensors with other sensors as a multisensor navigation
    system has been an area of research for several decades. Nowadays, platforms such
    as robots, vehicles, and VR/AR devices are equipped with cameras, IMUs, and LIDAR
    sensors. Hence, it is natural to consider introducing multimodal learning techniques
    [[88](#bib.bib88)] and designing learning models capable of fusing multimodal
    information to construct a mapping function from sensor data to pose.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æƒ¯æ€§ä¼ æ„Ÿå™¨ä¸å…¶ä»–ä¼ æ„Ÿå™¨é›†æˆä½œä¸ºå¤šä¼ æ„Ÿå™¨å¯¼èˆªç³»ç»Ÿå·²ç»æˆä¸ºæ•°åå¹´çš„ç ”ç©¶é¢†åŸŸã€‚å¦‚ä»Šï¼Œæœºå™¨äººã€è½¦è¾†ä»¥åŠè™šæ‹Ÿç°å®/å¢å¼ºç°å®è®¾å¤‡ç­‰å¹³å°éƒ½é…å¤‡äº†æ‘„åƒå¤´ã€IMUå’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨ã€‚å› æ­¤ï¼Œè€ƒè™‘å¼•å…¥å¤šæ¨¡æ€å­¦ä¹ æŠ€æœ¯[[88](#bib.bib88)]å¹¶è®¾è®¡èƒ½å¤Ÿèåˆå¤šæ¨¡æ€ä¿¡æ¯çš„å­¦ä¹ æ¨¡å‹ï¼Œä»¥æ„å»ºä»ä¼ æ„Ÿå™¨æ•°æ®åˆ°å§¿æ€çš„æ˜ å°„å‡½æ•°æ˜¯å¾ˆè‡ªç„¶çš„ã€‚
- en: '![Refer to caption](img/fbfe2f99ce1a6a040dcb7dbb78e7709d.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/fbfe2f99ce1a6a040dcb7dbb78e7709d.png)'
- en: 'Figure 7: An overview of existing methods on deep learning based sensor fusion'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„ä¼ æ„Ÿå™¨èåˆç°æœ‰æ–¹æ³•æ¦‚è¿°
- en: Visual-inertial odometry (VIO) has garnered attention as a means of integrating
    low-cost, complementary camera and IMU sensors that are widely deployed. Monocular
    vision can capture the appearance and geometry of a scene, but cannot recover
    the scale metric. IMU provides metric scale and improves motion tracking in featureless
    areas, complex lighting conditions, and motion blur. However, a pure inertial
    solution can only last for a short period. Therefore, an effective fusion of these
    two complementary sensors is necessary for accurate pose estimation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional VIO methods integrate visual and inertial information based on
    filtering [[89](#bib.bib89), [5](#bib.bib5)], fixed-lag smoothing [[90](#bib.bib90)],
    or full smoothing [[91](#bib.bib91)]. Recently, deep learning-based VIO models
    have emerged, directly constructing a mapping function from images and IMU to
    pose in a data-driven manner. VINet [[78](#bib.bib78)] is an end-to-end deep VIO
    model consisting of a ConvNet-based visual encoder to extract visual features
    from two images and an LSTM-based inertial encoder to extract inertial features
    from a sequence of inertial data between the two images. As shown in Figure [7](#S7.F7
    "Figure 7 â€£ VII Deep Learning based Sensor Fusion â€£ Deep Learning for Inertial
    Positioning: A Survey") (a), the visual and inertial features are concatenated
    together as one tensor, followed by an LSTM and fully-connected layer that finally
    maps features into a 6-dimensional pose. VINet is trained on public driving datasets
    such as the KITTI dataset [[92](#bib.bib92)] and a public drone dataset such as
    the EuroC dataset [[93](#bib.bib93)]. The learned VIO model is generally more
    robust to sensor noises compared to traditional VIO methods, although its model
    performance still cannot compete with state-of-the-art VIO methods.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'To effectively integrate visual and inertial information, [[80](#bib.bib80)]
    proposes a selective sensor fusion mechanism that learns to choose important features
    conditioned on sensor observations, as demonstrated in Figure [7](#S7.F7 "Figure
    7 â€£ VII Deep Learning based Sensor Fusion â€£ Deep Learning for Inertial Positioning:
    A Survey") (b). Specifically, this work proposes two types of fusion: soft fusion,
    which is based on an attention mechanism and generates a soft mask to reweight
    features based on their importance, and hard fusion, which is based on Gumbel
    Soft-max and generates a hard mask consisting of either 1 or 0 to either propagate
    or ignore a feature. Experimental evaluation on the KITTI dataset demonstrates
    that compared with directly concatenating features [[78](#bib.bib78)], selective
    fusion enhances the performance of deep VIO by 5%-10%. An interesting observation
    is that the number of useful features is relevant to the amount of linear/rotational
    velocity, with inertial features contributing more to rotation rate (e.g., turning),
    while more visual features are used to increase linear velocity.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Both [[78](#bib.bib78)] and [[80](#bib.bib80)] are trained in a supervised
    learning manner using datasets with high-precision ground-truth poses as training
    labels. However, obtaining high-precision poses can be difficult or costly in
    certain cases. Consequently, self-supervised learning-based VIOs, which do not
    require pose labels, have attracted attention. Self-supervised VIOs leverage the
    multi-view geometry relation of consecutive images, such as novel view synthesis,
    as a supervision signal [[79](#bib.bib79), [81](#bib.bib81), [84](#bib.bib84),
    [86](#bib.bib86)]. The task of novel view synthesis involves transforming a source
    image into a target view and comparing the differences between the synthesized
    target images and real target images as loss. In VIOLearner [[79](#bib.bib79)]
    and DeepVIO [[81](#bib.bib81)], as shown in Figure [7](#S7.F7 "Figure 7 â€£ VII
    Deep Learning based Sensor Fusion â€£ Deep Learning for Inertial Positioning: A
    Survey") (c), the pose transformation is generated from an inertial data sequence
    and used in the novel view synthesis process. In UnVIO [[84](#bib.bib84)] and
    SelfVIO [[86](#bib.bib86)], inertial data is integrated with visual data via an
    attention module applied to the concatenated visual and inertial features extracted
    from the images and IMU sequence. They show that incorporating inertial data with
    visual data improves the accuracy of pose estimation, particularly rotation estimation.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[[78](#bib.bib78)]å’Œ[[80](#bib.bib80)]éƒ½é‡‡ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨é«˜ç²¾åº¦çœŸå®å§¿æ€çš„æ•°æ®é›†ä½œä¸ºè®­ç»ƒæ ‡ç­¾ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè·å–é«˜ç²¾åº¦çš„å§¿æ€å¯èƒ½å›°éš¾æˆ–æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼ŒåŸºäºè‡ªç›‘ç£å­¦ä¹ çš„VIOï¼Œå› ä¸éœ€è¦å§¿æ€æ ‡ç­¾è€Œå—åˆ°å…³æ³¨ã€‚è‡ªç›‘ç£VIOåˆ©ç”¨è¿ç»­å›¾åƒçš„å¤šè§†è§’å‡ ä½•å…³ç³»ï¼Œå¦‚æ–°è§†å›¾åˆæˆï¼Œä½œä¸ºç›‘ç£ä¿¡å·[[79](#bib.bib79),
    [81](#bib.bib81), [84](#bib.bib84), [86](#bib.bib86)]ã€‚æ–°è§†å›¾åˆæˆçš„ä»»åŠ¡æ˜¯å°†æºå›¾åƒè½¬æ¢ä¸ºç›®æ ‡è§†å›¾ï¼Œå¹¶å°†åˆæˆçš„ç›®æ ‡å›¾åƒä¸çœŸå®ç›®æ ‡å›¾åƒä¹‹é—´çš„å·®å¼‚ä½œä¸ºæŸå¤±ã€‚åœ¨VIOLearner
    [[79](#bib.bib79)]å’ŒDeepVIO [[81](#bib.bib81)]ä¸­ï¼Œå¦‚å›¾[7](#S7.F7 "Figure 7 â€£ VII Deep
    Learning based Sensor Fusion â€£ Deep Learning for Inertial Positioning: A Survey")
    (c)æ‰€ç¤ºï¼Œå§¿æ€å˜æ¢æ˜¯ç”±æƒ¯æ€§æ•°æ®åºåˆ—ç”Ÿæˆçš„ï¼Œå¹¶ç”¨äºæ–°è§†å›¾åˆæˆè¿‡ç¨‹ã€‚åœ¨UnVIO [[84](#bib.bib84)]å’ŒSelfVIO [[86](#bib.bib86)]ä¸­ï¼Œæƒ¯æ€§æ•°æ®é€šè¿‡ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—ä¸è§†è§‰æ•°æ®æ•´åˆï¼Œæ³¨æ„åŠ›æ¨¡å—åº”ç”¨äºä»å›¾åƒå’ŒIMUåºåˆ—ä¸­æå–çš„è§†è§‰å’Œæƒ¯æ€§ç‰¹å¾çš„è¿æ¥ã€‚è¿™äº›æ–¹æ³•è¡¨æ˜ï¼Œç»“åˆæƒ¯æ€§æ•°æ®å’Œè§†è§‰æ•°æ®å¯ä»¥æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯æ—‹è½¬ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚'
- en: The use of learning-based sensor fusion extends beyond visual-inertial odometry
    (VIO) to include other sensor modalities such as Lidar-inertial odometry (LIO),
    thermal-inertial odometry, and radar-inertial odometry [[87](#bib.bib87), [82](#bib.bib82),
    [83](#bib.bib83)]. DeepTIO [[82](#bib.bib82)] and MilliEgo [[83](#bib.bib83)]
    employ attention-based selective fusion mechanisms, similar to soft fusion [[80](#bib.bib80)],
    to reweight and fuse features from inertial and visual data, resulting in improved
    pose accuracy. In addition, unsupervised learning-based LIDAR-inertial odometry
    [[87](#bib.bib87)] generates motion transformation from IMU sequence and uses
    it for LIDAR novel view synthesis to facilitate self-supervised learning of egomotion,
    similar to VIOLearner [[79](#bib.bib79)]. In all these cases, the inclusion of
    IMU data in deep neural networks enhances pose estimation accuracy and robustness.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå­¦ä¹ çš„ä¼ æ„Ÿå™¨èåˆä¸ä»…å»¶ä¼¸åˆ°è§†è§‰-æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆVIOï¼‰ï¼Œè¿˜åŒ…æ‹¬å…¶ä»–ä¼ æ„Ÿå™¨æ¨¡æ€ï¼Œå¦‚æ¿€å…‰é›·è¾¾-æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆLIOï¼‰ã€çƒ­æˆåƒ-æƒ¯æ€§é‡Œç¨‹è®¡å’Œé›·è¾¾-æƒ¯æ€§é‡Œç¨‹è®¡[[87](#bib.bib87),
    [82](#bib.bib82), [83](#bib.bib83)]ã€‚DeepTIO [[82](#bib.bib82)]å’ŒMilliEgo [[83](#bib.bib83)]é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„é€‰æ‹©æ€§èåˆæœºåˆ¶ï¼Œç±»ä¼¼äºè½¯èåˆ[[80](#bib.bib80)]ï¼Œå¯¹æƒ¯æ€§å’Œè§†è§‰æ•°æ®çš„ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆï¼Œä»è€Œæé«˜äº†å§¿æ€çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£å­¦ä¹ çš„æ¿€å…‰é›·è¾¾-æƒ¯æ€§é‡Œç¨‹è®¡[[87](#bib.bib87)]ä»IMUåºåˆ—ç”Ÿæˆè¿åŠ¨å˜æ¢ï¼Œå¹¶å°†å…¶ç”¨äºæ¿€å…‰é›·è¾¾çš„æ–°è§†å›¾åˆæˆï¼Œä»¥ä¿ƒè¿›è‡ªç›‘ç£å­¦ä¹ çš„è‡ªè¿åŠ¨ï¼Œç±»ä¼¼äºVIOLearner
    [[79](#bib.bib79)]ã€‚åœ¨æ‰€æœ‰è¿™äº›æƒ…å†µä¸‹ï¼Œå°†IMUæ•°æ®çº³å…¥æ·±åº¦ç¥ç»ç½‘ç»œå¯ä»¥æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
- en: VIII Deep Learning based Human Motion analysis and Activity Recognition
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII åŸºäºæ·±åº¦å­¦ä¹ çš„äººä½“è¿åŠ¨åˆ†æä¸æ´»åŠ¨è¯†åˆ«
- en: Inertial sensors have diverse applications beyond positioning, such as motion
    tracking, activity recognition, and more. Although these tasks are not the primary
    focus of this survey, this section provides a brief yet comprehensive overview
    of how deep learning is utilized in these domains.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: é™€èºä»ªä¼ æ„Ÿå™¨çš„åº”ç”¨ä¸ä»…é™äºå®šä½ï¼Œè¿˜åŒ…æ‹¬è¿åŠ¨è·Ÿè¸ªã€æ´»åŠ¨è¯†åˆ«ç­‰ã€‚å°½ç®¡è¿™äº›ä»»åŠ¡ä¸æ˜¯æœ¬è°ƒæŸ¥çš„ä¸»è¦ç„¦ç‚¹ï¼Œæœ¬èŠ‚ä»æä¾›äº†ä¸€ä¸ªç®€æ˜è€Œå…¨é¢çš„æ¦‚è¿°ï¼Œè¯´æ˜æ·±åº¦å­¦ä¹ åœ¨è¿™äº›é¢†åŸŸä¸­çš„åº”ç”¨ã€‚
- en: VIII-A Human Motion Analysis
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A äººä½“è¿åŠ¨åˆ†æ
- en: Data-driven approaches are utilized to reconstruct human pose and motion using
    either a single IMU or multiple IMUs attached to the body. These models primarily
    focus on analyzing human motion rather than localizing users, which differentiates
    them from inertial positioning. Several studies have applied machine learning
    to gait and pose analysis, such as knee angle estimation for human walking using
    supervised support vector regression in [[94](#bib.bib94)] and probabilistic parameter
    learning for human gesture recognition in [[95](#bib.bib95)] through handcrafted
    motion features extracted from inertial data. In addition, machine learning methods,
    such as multi-layer perceptrons (MLPs), have been utilized in IMU data to learn
    sensor displacement for human motion reconstruction in [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98)].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é©±åŠ¨çš„æ–¹æ³•ç”¨äºé‡å»ºäººä½“å§¿æ€å’Œè¿åŠ¨ï¼Œæ— è®ºæ˜¯ä½¿ç”¨å•ä¸ªIMUè¿˜æ˜¯å¤šä¸ªé™„ç€åœ¨èº«ä½“ä¸Šçš„IMUã€‚è¿™äº›æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨åˆ†æäººä½“è¿åŠ¨ä¸Šï¼Œè€Œä¸æ˜¯å®šä½ç”¨æˆ·ï¼Œè¿™ä½¿å®ƒä»¬ä¸æƒ¯æ€§å®šä½æœ‰æ‰€åŒºåˆ«ã€‚ä¸€äº›ç ”ç©¶åº”ç”¨äº†æœºå™¨å­¦ä¹ æ¥åˆ†ææ­¥æ€å’Œå§¿æ€ï¼Œä¾‹å¦‚
    [[94](#bib.bib94)] ä¸­çš„åŸºäºç›‘ç£æ”¯æŒå‘é‡å›å½’çš„äººä½“æ­¥æ€è§’åº¦ä¼°è®¡å’Œ [[95](#bib.bib95)] ä¸­åŸºäºä»æƒ¯æ€§æ•°æ®ä¸­æå–çš„æ‰‹å·¥åˆ¶ä½œè¿åŠ¨ç‰¹å¾çš„äººä½“æ‰‹åŠ¿è¯†åˆ«ã€‚æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œä¹Ÿè¢«åº”ç”¨äºIMUæ•°æ®ä¸­ï¼Œä»¥å­¦ä¹ ä¼ æ„Ÿå™¨ä½ç§»ï¼Œç”¨äº
    [[96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)] ä¸­çš„äººä½“è¿åŠ¨é‡å»ºã€‚
- en: Recently, deep learning has shown promising performance in human pose reconstruction.
    For example, [[99](#bib.bib99)] proposed Deep Inertial Poser, a recurrent neural
    network (RNN)-based framework that can reconstruct full-body pose from six IMUs
    attached to the userâ€™s body. TransPose [[100](#bib.bib100)], another RNN-based
    framework, enables real-time human pose estimation using six body-attached IMUs.
    Furthermore, [[101](#bib.bib101)] combines a neural kinematics estimator with
    a physics-aware motion optimizer to improve the accuracy of human motion tracking.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ åœ¨äººä½“å§¿æ€é‡å»ºæ–¹é¢å±•ç°äº†ä»¤äººæœŸå¾…çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œ[[99](#bib.bib99)] æå‡ºäº†Deep Inertial Poserï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„æ¡†æ¶ï¼Œå¯ä»¥ä»é™„ç€åœ¨ç”¨æˆ·èº«ä½“ä¸Šçš„å…­ä¸ªIMUä¸­é‡å»ºå…¨èº«å§¿æ€ã€‚TransPose
    [[100](#bib.bib100)] æ˜¯å¦ä¸€ä¸ªåŸºäºRNNçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿ç”¨å…­ä¸ªèº«ä½“é™„ç€çš„IMUè¿›è¡Œå®æ—¶äººä½“å§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œ[[101](#bib.bib101)]
    ç»“åˆäº†ç¥ç»åŠ¨åŠ›å­¦ä¼°è®¡å™¨å’Œç‰©ç†æ„ŸçŸ¥è¿åŠ¨ä¼˜åŒ–å™¨ï¼Œä»¥æé«˜äººä½“è¿åŠ¨è·Ÿè¸ªçš„å‡†ç¡®æ€§ã€‚
- en: VIII-B Human Activity recognition (HAR)
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B äººä½“æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰
- en: Deep learning can be utilized to exploit inertial information from body-worn
    IMUs for human activity recognition. For instance, [[102](#bib.bib102)] published
    a popular public dataset of human activity recognition and successfully classified
    current activity among six classes, including walking, standing still, sitting,
    walking downstairs, walking upstairs, and laying down, using support vector machines
    (SVM). In addition, [[103](#bib.bib103)] presents an LSTM-based HAR model that
    inputted a sequence of inertial data and outputted class probability. Moreover,
    [[104](#bib.bib104)] introduces a ConvNet-based HAR model that achieved a classification
    accuracy of 97%, outperforming an accuracy of 96% from SVM-based HAR models. To
    reduce onboard computational requirements, [[105](#bib.bib105)] presents a learning
    framework that exploited both features automatically extracted by DNN and hand-crafted
    features to achieve accurate and real-time human activity recognition on low-end
    devices.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ å¯ä»¥ç”¨äºåˆ©ç”¨ä½“ä½©æˆ´æƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆIMUï¼‰ä¸­çš„æƒ¯æ€§ä¿¡æ¯æ¥è¿›è¡Œäººä½“æ´»åŠ¨è¯†åˆ«ã€‚ä¾‹å¦‚ï¼Œ[[102](#bib.bib102)] å‘å¸ƒäº†ä¸€ä¸ªæµè¡Œçš„å…¬å…±äººä½“æ´»åŠ¨è¯†åˆ«æ•°æ®é›†ï¼Œå¹¶æˆåŠŸåœ°å°†å½“å‰æ´»åŠ¨åˆ†ç±»ä¸ºå…­ç±»ï¼ŒåŒ…æ‹¬è¡Œèµ°ã€é™æ­¢ã€åç€ã€ä¸‹æ¥¼ã€ä¸Šæ¥¼å’Œèººä¸‹ï¼Œä½¿ç”¨äº†æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€‚æ­¤å¤–ï¼Œ[[103](#bib.bib103)]
    æå‡ºäº†ä¸€ä¸ªåŸºäºLSTMçš„HARæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¾“å…¥æƒ¯æ€§æ•°æ®åºåˆ—å¹¶è¾“å‡ºç±»åˆ«æ¦‚ç‡ã€‚æ­¤å¤–ï¼Œ[[104](#bib.bib104)] ä»‹ç»äº†ä¸€ä¸ªåŸºäºConvNetçš„HARæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å®ç°äº†97%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†SVMåŸºäºHARæ¨¡å‹çš„96%å‡†ç¡®ç‡ã€‚ä¸ºäº†å‡å°‘è½¦è½½è®¡ç®—éœ€æ±‚ï¼Œ[[105](#bib.bib105)]
    æå‡ºäº†ä¸€ä¸ªå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨DNNè‡ªåŠ¨æå–çš„ç‰¹å¾å’Œæ‰‹å·¥åˆ¶ä½œçš„ç‰¹å¾æ¥å®ç°ä½ç«¯è®¾å¤‡ä¸Šçš„å‡†ç¡®å’Œå®æ—¶äººä½“æ´»åŠ¨è¯†åˆ«ã€‚
- en: Learning from inertial data can also benefit sports and health applications.
    For instance, [[106](#bib.bib106)] shows that deep learning is effective in detecting
    Parkinsonâ€™s disease by assessing the patientâ€™s daily activity through the analysis
    of inertial information from wearable sensors. Additionally, [[107](#bib.bib107)]
    provides instructions for athletesâ€™ sports training based on sensor data and activity
    information.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æƒ¯æ€§æ•°æ®ä¸­å­¦ä¹ ä¹Ÿå¯ä»¥ä½¿è¿åŠ¨å’Œå¥åº·åº”ç”¨å—ç›Šã€‚ä¾‹å¦‚ï¼Œ[[106](#bib.bib106)] è¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ åœ¨é€šè¿‡åˆ†æå¯ç©¿æˆ´ä¼ æ„Ÿå™¨ä¸­çš„æƒ¯æ€§ä¿¡æ¯æ¥è¯„ä¼°æ‚£è€…çš„æ—¥å¸¸æ´»åŠ¨ï¼Œä»è€Œæœ‰æ•ˆæ£€æµ‹å¸•é‡‘æ£®ç—…æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œ[[107](#bib.bib107)]
    æä¾›äº†åŸºäºä¼ æ„Ÿå™¨æ•°æ®å’Œæ´»åŠ¨ä¿¡æ¯çš„è¿åŠ¨å‘˜è®­ç»ƒæŒ‡å¯¼ã€‚
- en: IX Conclusions and Discussions
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX ç»“è®ºä¸è®¨è®º
- en: In recent years, there has been a growing interest in using deep learning to
    address the problem of inertial positioning. This article provides a comprehensive
    review of the area of deep learning-based inertial positioning. The rapid advances
    in this field have already provided promising solutions to address problems such
    as inertial sensor calibration, the compensation of error drifts in inertial positioning,
    and multimodal sensor fusion. This section concludes and discusses the benefits
    that deep learning can bring to inertial navigation research, analyzes the challenges
    that existing research faces, and highlights the future opportunities of this
    evolving field.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ è§£å†³æƒ¯æ€§å®šä½é—®é¢˜çš„å…´è¶£æ—¥ç›Šå¢é•¿ã€‚æœ¬æ–‡æä¾›äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½é¢†åŸŸçš„ç»¼åˆè¯„è¿°ã€‚è¯¥é¢†åŸŸçš„å¿«é€Ÿè¿›å±•å·²ç»æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆæ¥è§£å†³æƒ¯æ€§ä¼ æ„Ÿå™¨æ ¡å‡†ã€æƒ¯æ€§å®šä½è¯¯å·®æ¼‚ç§»è¡¥å¿å’Œå¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆç­‰é—®é¢˜ã€‚æœ¬èŠ‚æ€»ç»“å¹¶è®¨è®ºäº†æ·±åº¦å­¦ä¹ å¯¹æƒ¯æ€§å¯¼èˆªç ”ç©¶çš„å¥½å¤„ï¼Œåˆ†æäº†ç°æœ‰ç ”ç©¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶çªå‡ºäº†è¿™ä¸€ä¸æ–­å‘å±•çš„é¢†åŸŸçš„æœªæ¥æœºä¼šã€‚
- en: IX-A Benefits
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A å¥½å¤„
- en: 'Unlike traditional geometric or physical inertial positioning models, the integration
    of deep learning into inertial positioning has led to the development of a range
    of alternative solutions to address the issue of positioning error drifts. The
    corresponding benefits can be summarized as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„å‡ ä½•æˆ–ç‰©ç†æƒ¯æ€§å®šä½æ¨¡å‹ä¸åŒï¼Œå°†æ·±åº¦å­¦ä¹ èå…¥æƒ¯æ€§å®šä½å·²ä¿ƒä½¿å¼€å‘å‡ºä¸€ç³»åˆ—æ›¿ä»£è§£å†³æ–¹æ¡ˆæ¥è§£å†³å®šä½è¯¯å·®æ¼‚ç§»é—®é¢˜ã€‚ç›¸åº”çš„å¥½å¤„å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š
- en: IX-A1 Learn to approximate complex and varying function
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A1 å­¦ä¹ è¿‘ä¼¼å¤æ‚å’Œå˜åŒ–çš„å‡½æ•°
- en: The deep neural network has proven to be a powerful and versatile nonlinear
    function that can approximate the complex and variable factors involved in inertial
    positioning, which are difficult to model manually. For example, when calibrating
    sensors, the corrupt noises that exist in inertial measurements can be modeled
    and eliminated in a data-driven way by training on a large dataset using a DNN.
    Deep learning can also directly generate absolute velocity and position displacement
    from data, without the need for IMU integration, thus reducing positioning drifts.
    In pedestrian dead reckoning (PDR), deep learning can estimate step length based
    on data, rather than empirical equations, and implicitly remove the effects of
    different users. These works demonstrate that using a large dataset to build a
    data-driven model can produce more accurate motion estimates, as well as reduce
    and constrain the rapid error drifts of inertial navigation systems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œå·²è¢«è¯æ˜æ˜¯ä¸€ç§å¼ºå¤§ä¸”å¤šç”¨é€”çš„éçº¿æ€§å‡½æ•°ï¼Œèƒ½å¤Ÿè¿‘ä¼¼æƒ¯æ€§å®šä½ä¸­æ¶‰åŠçš„å¤æ‚å’Œå˜åŒ–å› ç´ ï¼Œè¿™äº›å› ç´ å¾ˆéš¾æ‰‹åŠ¨å»ºæ¨¡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ ¡å‡†ä¼ æ„Ÿå™¨æ—¶ï¼Œæƒ¯æ€§æµ‹é‡ä¸­å­˜åœ¨çš„å™ªå£°å¯ä»¥é€šè¿‡åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒ
    DNN ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼å»ºæ¨¡å¹¶æ¶ˆé™¤ã€‚æ·±åº¦å­¦ä¹ è¿˜å¯ä»¥ç›´æ¥ä»æ•°æ®ä¸­ç”Ÿæˆç»å¯¹é€Ÿåº¦å’Œä½ç½®ä½ç§»ï¼Œè€Œæ— éœ€IMUç§¯åˆ†ï¼Œä»è€Œå‡å°‘å®šä½æ¼‚ç§»ã€‚åœ¨æ­¥æ€æ¨ç®—ï¼ˆPDRï¼‰ä¸­ï¼Œæ·±åº¦å­¦ä¹ å¯ä»¥åŸºäºæ•°æ®ä¼°è®¡æ­¥é•¿ï¼Œè€Œä¸æ˜¯ç»éªŒæ–¹ç¨‹ï¼Œå¹¶éšå¼å»é™¤ä¸åŒç”¨æˆ·çš„å½±å“ã€‚è¿™äº›å·¥ä½œè¡¨æ˜ï¼Œä½¿ç”¨å¤§å‹æ•°æ®é›†æ„å»ºæ•°æ®é©±åŠ¨æ¨¡å‹å¯ä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„è¿åŠ¨ä¼°è®¡ï¼Œå¹¶å‡å°‘å’Œçº¦æŸæƒ¯æ€§å¯¼èˆªç³»ç»Ÿçš„å¿«é€Ÿè¯¯å·®æ¼‚ç§»ã€‚
- en: IX-A2 Learn to estimate parameters
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A2 å­¦ä¹ ä¼°è®¡å‚æ•°
- en: Automatic identification of parameters through data-driven models contributes
    to paving the way for next-generation intelligent navigation systems that can
    actively exploit input data and evolve over time without human intervention. In
    classical inertial navigation mechanisms, certain parameters or modules need to
    be manually set and tuned before use. For instance, experts with experience need
    to settle parameters in Kalman filtering, such as observation noise, covariance,
    and process noise. Deep learning has proven effective in automatically producing
    suitable parameters for Kalman filtering based on input data [[47](#bib.bib47),
    [85](#bib.bib85), [42](#bib.bib42)]. In sensor calibration, reinforcement learning
    algorithms are used to discover optimal parameters for inertial calibration algorithms
    [[27](#bib.bib27)]. In ZUPT-based pedestrian inertial positioning, deep learning
    is a viable solution for classifying zero-velocity phases and determining when
    to update system states.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: IX-A3 Learn to self-adapt in new domains
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unforeseen or ever-changing issues in new application domains, such as changes
    in motion mode, carrier, and sensor noise, can significantly impact the performance
    of inertial systems. Learning models offer opportunities for inertial systems
    to adapt to new changes and overcome these influential factors implicitly by discovering
    and exploiting the differences in data distributions between domains. For instance,
    [[38](#bib.bib38)] leverages transfer learning to allow INS to extract domain-invariant
    features from data, maintaining localization accuracy when sensor attachment is
    changed. The introduction of self-supervised learning enables navigation systems
    to learn from data without high-precision pose as training labels, allowing unlabelled
    inertial data to be effectively used for model performance improvement. In visual-inertial
    odometry, [[79](#bib.bib79), [81](#bib.bib81), [84](#bib.bib84)] introduce novel
    view synthesis as a supervision signal to train deep VIO in a self-supervised
    learning way. This self-adaptation ability is promising for mobile agents to continuously
    improve their localization performance in new application scenes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: IX-B Challenges and Opportunities
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the impressive and promising results that deep learning has already
    offered in inertial positioning, there are still challenges in existing methods
    when they are applied and deployed in real-world scenarios. To overcome these
    limitations, several opportunities and potential research directions are discussed
    below.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: IX-B1 Generalization and Self-learning
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The generalization problem is a major concern for deep learning-based methods
    because these models are trained on one domain (i.e., training set) but need to
    be tested on other domains (i.e., testing set). The possible differences in data
    between domains can lead to a degradation of prediction performance. Although
    deep learning-based inertial navigation models have reported impressive results
    on the authorâ€™s own datasets, these works have not been evaluated in comprehensive
    experiments during long-term operation and across various devices, users, and
    application scenes. Thus, it is challenging to determine the real performance
    of these models in open environments. To address the generalization problem, new
    learning techniques such as transfer learning [[108](#bib.bib108)], lifelong learning,
    and contrastive learning [[109](#bib.bib109)] can be introduced into inertial
    positioning systems, which is a promising direction. For instance, in the future,
    by exploiting information from physical/geometric rules or other sensors (e.g.,
    GNSS, camera), the learning-based inertial positioning model can be self-supervisedly
    trained and enable mobile agents to learn from data in a lifelong manner.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: æ³›åŒ–é—®é¢˜æ˜¯æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ä¸»è¦å…³æ³¨ç‚¹ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªé¢†åŸŸï¼ˆå³è®­ç»ƒé›†ï¼‰ä¸Šè®­ç»ƒçš„ï¼Œä½†éœ€è¦åœ¨å…¶ä»–é¢†åŸŸï¼ˆå³æµ‹è¯•é›†ï¼‰ä¸Šè¿›è¡Œæµ‹è¯•ã€‚é¢†åŸŸä¹‹é—´çš„æ•°æ®å¯èƒ½å·®å¼‚ä¼šå¯¼è‡´é¢„æµ‹æ€§èƒ½çš„ä¸‹é™ã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å¯¼èˆªæ¨¡å‹åœ¨ä½œè€…è‡ªå·±çš„æ•°æ®é›†ä¸ŠæŠ¥å‘Šäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†è¿™äº›å·¥ä½œå°šæœªåœ¨é•¿æœŸè¿è¡Œå’Œå„ç§è®¾å¤‡ã€ç”¨æˆ·å’Œåº”ç”¨åœºæ™¯ä¸­ç»è¿‡å…¨é¢å®éªŒè¯„ä¼°ã€‚å› æ­¤ï¼Œå¾ˆéš¾ç¡®å®šè¿™äº›æ¨¡å‹åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„çœŸå®è¡¨ç°ã€‚ä¸ºäº†è§£å†³æ³›åŒ–é—®é¢˜ï¼Œå¯ä»¥å°†æ–°å­¦ä¹ æŠ€æœ¯å¦‚è¿ç§»å­¦ä¹ [[108](#bib.bib108)]ã€ç»ˆèº«å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ [[109](#bib.bib109)]å¼•å…¥æƒ¯æ€§å®šä½ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œæœªæ¥é€šè¿‡åˆ©ç”¨ç‰©ç†/å‡ ä½•è§„åˆ™æˆ–å…¶ä»–ä¼ æ„Ÿå™¨ï¼ˆä¾‹å¦‚GNSSã€æ‘„åƒå¤´ï¼‰çš„ä¿¡æ¯ï¼ŒåŸºäºå­¦ä¹ çš„æƒ¯æ€§å®šä½æ¨¡å‹å¯ä»¥è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒï¼Œä½¿ç§»åŠ¨ä»£ç†èƒ½å¤Ÿä»¥ç»ˆèº«æ–¹å¼ä»æ•°æ®ä¸­å­¦ä¹ ã€‚
- en: IX-B2 Black-box and Explainability
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B2 é»‘ç®±ä¸å¯è§£é‡Šæ€§
- en: Deep neural networks have been criticized as being a â€™black-boxâ€™ model due to
    their lack of explainability and interpretability. As these models are often used
    to support real-world tasks, it is crucial to investigate what is learned inside
    deep nets before deploying them to ensure their safety and reliability. Despite
    the good results shown by deep learning models in estimating important terms such
    as location displacement, sensor measurement errors, and filtering parameters,
    these terms lack concrete mathematical models, unlike traditional inertial navigation.
    To determine whether these terms are trustworthy, uncertainties should be estimated
    in conjunction with the inertial positioning method [[71](#bib.bib71)] and used
    as indicators for users or systems to understand the extent to which model predictions
    can be trusted. In future research, it is important to reveal the governing mathematical
    or physical models behind the learned inertial positioning neural model and identify
    which parts of inertial positioning can be learned by deep nets. Introducing Bayesian
    deep learning into inertial positioning is also a promising direction that could
    offer interpretability for model predictions [[110](#bib.bib110)].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œå› å…¶ç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§è€Œè¢«æ‰¹è¯„ä¸ºâ€œé»‘ç®±â€æ¨¡å‹ã€‚ç”±äºè¿™äº›æ¨¡å‹å¸¸ç”¨äºæ”¯æŒç°å®ä¸–ç•Œçš„ä»»åŠ¡ï¼Œåœ¨éƒ¨ç½²ä¹‹å‰ï¼Œè°ƒæŸ¥æ·±åº¦ç½‘ç»œå†…éƒ¨å­¦åˆ°çš„å†…å®¹å¯¹äºç¡®ä¿å…¶å®‰å…¨æ€§å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¼°è®¡è¯¸å¦‚ä½ç½®åç§»ã€ä¼ æ„Ÿå™¨æµ‹é‡è¯¯å·®å’Œè¿‡æ»¤å‚æ•°ç­‰é‡è¦æœ¯è¯­æ–¹é¢æ˜¾ç¤ºäº†è‰¯å¥½çš„ç»“æœï¼Œä½†è¿™äº›æœ¯è¯­ç¼ºä¹å…·ä½“çš„æ•°å­¦æ¨¡å‹ï¼Œä¸ä¼ ç»Ÿçš„æƒ¯æ€§å¯¼èˆªä¸åŒã€‚ä¸ºäº†ç¡®å®šè¿™äº›æœ¯è¯­æ˜¯å¦å¯ä¿¡ï¼Œåº”è¯¥ä¼°ç®—ä¸ç¡®å®šæ€§ï¼Œå¹¶ç»“åˆæƒ¯æ€§å®šä½æ–¹æ³•[[71](#bib.bib71)]ï¼Œä½œä¸ºç”¨æˆ·æˆ–ç³»ç»Ÿäº†è§£æ¨¡å‹é¢„æµ‹å¯ä¿¡åº¦çš„æŒ‡æ ‡ã€‚åœ¨æœªæ¥çš„ç ”ç©¶ä¸­ï¼Œæ­ç¤ºå­¦ä¹ åˆ°çš„æƒ¯æ€§å®šä½ç¥ç»æ¨¡å‹èƒŒåçš„æ•°å­¦æˆ–ç‰©ç†æ¨¡å‹ï¼Œå¹¶è¯†åˆ«æ·±åº¦ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ çš„æƒ¯æ€§å®šä½éƒ¨åˆ†æ˜¯é‡è¦çš„ã€‚å°†è´å¶æ–¯æ·±åº¦å­¦ä¹ å¼•å…¥æƒ¯æ€§å®šä½ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œèƒ½å¤Ÿä¸ºæ¨¡å‹é¢„æµ‹æä¾›å¯è§£é‡Šæ€§[[110](#bib.bib110)]ã€‚
- en: IX-B3 Efficiency and Real-world Deployment
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B3 æ•ˆç‡ä¸ç°å®ä¸–ç•Œéƒ¨ç½²
- en: When deploying deep positioning models on user devices, it is crucial to consider
    the consumption of computation, storage, and energy in system design, in addition
    to prediction accuracy. Compared to classical inertial navigation algorithms,
    DNN-based inertial positioning models have a relatively large computational and
    memory burden, as they contain millions of neural parameters that require GPUs
    for parallel training and testing. Therefore, online inference of learning models,
    especially on low-end devices such as IoT consoles, VR/AR devices, and miniature
    drones, requires lightweight, efficient, and effective models. To achieve this
    goal, neural model compression techniques, such as knowledge distillation [[111](#bib.bib111)],
    should be introduced to discover the optimal neural structure that balances prediction
    accuracy and model size. [[45](#bib.bib45)] and [[63](#bib.bib63)] have conducted
    initial trials on minimizing the model size of inertial odometry. Moreover, safety
    and reliability are also crucial factors to consider. In the future, it is worth
    exploring the optimal structure of learning-based inertial positioning models,
    considering model performance, parameter size, latency, safety, and reliability
    for real-world deployment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”¨æˆ·è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦å®šä½æ¨¡å‹æ—¶ï¼Œé™¤äº†é¢„æµ‹å‡†ç¡®æ€§å¤–ï¼Œè¿˜å¿…é¡»è€ƒè™‘è®¡ç®—ã€å­˜å‚¨å’Œèƒ½è€—çš„æ¶ˆè€—ã€‚ä¸ç»å…¸çš„æƒ¯æ€§å¯¼èˆªç®—æ³•ç›¸æ¯”ï¼ŒåŸºäº DNN çš„æƒ¯æ€§å®šä½æ¨¡å‹å…·æœ‰ç›¸å¯¹è¾ƒå¤§çš„è®¡ç®—å’Œå†…å­˜è´Ÿæ‹…ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«æ•°ç™¾ä¸‡ä¸ªç¥ç»å‚æ•°ï¼Œéœ€è¦
    GPU è¿›è¡Œå¹¶è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚å› æ­¤ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ç«¯è®¾å¤‡å¦‚ IoT æ§åˆ¶å°ã€VR/AR è®¾å¤‡å’Œå¾®å‹æ— äººæœºä¸Šè¿›è¡Œåœ¨çº¿æ¨ç†æ—¶ï¼Œéœ€è¦è½»é‡çº§ã€é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ¨¡å‹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œåº”è¯¥å¼•å…¥ç¥ç»æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå¦‚çŸ¥è¯†è’¸é¦
    [[111](#bib.bib111)]ï¼Œä»¥å‘ç°æœ€ä½³çš„ç¥ç»ç»“æ„ï¼Œå¹³è¡¡é¢„æµ‹å‡†ç¡®æ€§å’Œæ¨¡å‹å¤§å°ã€‚ [[45](#bib.bib45)] å’Œ [[63](#bib.bib63)]
    å·²å¯¹æƒ¯æ€§é‡Œç¨‹è®¡çš„æ¨¡å‹å¤§å°æœ€å°åŒ–è¿›è¡Œäº†åˆæ­¥è¯•éªŒã€‚æ­¤å¤–ï¼Œå®‰å…¨æ€§å’Œå¯é æ€§ä¹Ÿæ˜¯é‡è¦å› ç´ ã€‚æœªæ¥ï¼Œå€¼å¾—æ¢ç´¢å­¦ä¹ åŸºäºæƒ¯æ€§å®šä½æ¨¡å‹çš„æœ€ä½³ç»“æ„ï¼Œè€ƒè™‘æ¨¡å‹æ€§èƒ½ã€å‚æ•°å¤§å°ã€å»¶è¿Ÿã€å®‰å…¨æ€§å’Œå®é™…éƒ¨ç½²çš„å¯é æ€§ã€‚
- en: IX-B4 Data Collection and Benchmark
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B4 æ•°æ®æ”¶é›†ä¸åŸºå‡†æµ‹è¯•
- en: As a data-driven approach, the performance of deep learning models depends on
    the quality of the data, such as the size of the dataset, data diversity, and
    the differences between the training and testing sets. Under ideal conditions,
    deep learning-based inertial positioning models should be trained on diverse data
    across different users, platforms, motion dynamics, and inertial sensors to enhance
    their generalization in testing domains. However, collecting such data in diverse
    domains can be costly and time-consuming. Additionally, obtaining high-precision
    ground-truth poses as training and evaluation labels can be challenging in some
    cases. Previous research has used different training/evaluation data, model hyperparameters
    (e.g., learning rate, batch size, layer dimension), and evaluation metrics, making
    it difficult to compare these methods fairly. In visual navigation tasks, such
    as visual odometry/SLAM, the KITTI dataset [[92](#bib.bib92)] is commonly used
    as a benchmark to train and evaluate learning-based VO models. However, although
    published datasets for inertial navigation exist [[45](#bib.bib45), [46](#bib.bib46)],
    there is still a lack of a common benchmark that is adopted and recognized by
    mainstream methods in inertial positioning. In the future, a widely adopted dataset
    and benchmark, covering a variety of application scenarios, will greatly benefit
    and foster research in data-driven inertial positioning.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ä¾èµ–äºæ•°æ®çš„è´¨é‡ï¼Œå¦‚æ•°æ®é›†çš„å¤§å°ã€æ•°æ®çš„å¤šæ ·æ€§ä»¥åŠè®­ç»ƒé›†ä¸æµ‹è¯•é›†ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æƒ¯æ€§å®šä½æ¨¡å‹åº”åœ¨ä¸åŒç”¨æˆ·ã€å¹³å°ã€è¿åŠ¨åŠ¨æ€å’Œæƒ¯æ€§ä¼ æ„Ÿå™¨ä¸Šçš„å¤šæ ·æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å¢å¼ºå…¶åœ¨æµ‹è¯•é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ”¶é›†å¦‚æ­¤å¤šæ ·åŒ–é¢†åŸŸçš„æ•°æ®å¯èƒ½ä¼šéå¸¸æ˜‚è´µä¸”è€—æ—¶ã€‚æ­¤å¤–ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè·å–é«˜ç²¾åº¦çš„åœ°é¢çœŸå®å€¼ä½œä¸ºè®­ç»ƒå’Œè¯„ä¼°æ ‡ç­¾å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶ä½¿ç”¨äº†ä¸åŒçš„è®­ç»ƒ/è¯„ä¼°æ•°æ®ã€æ¨¡å‹è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€å±‚ç»´åº¦ï¼‰å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä½¿å¾—è¿™äº›æ–¹æ³•çš„å…¬å¹³æ¯”è¾ƒå˜å¾—å›°éš¾ã€‚åœ¨è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†è§‰é‡Œç¨‹è®¡/SLAMï¼ŒKITTI
    æ•°æ®é›† [[92](#bib.bib92)] é€šå¸¸ä½œä¸ºåŸºå‡†æ¥è®­ç»ƒå’Œè¯„ä¼°åŸºäºå­¦ä¹ çš„ VO æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°½ç®¡å­˜åœ¨æƒ¯æ€§å¯¼èˆªçš„å…¬å¼€æ•°æ®é›† [[45](#bib.bib45),
    [46](#bib.bib46)]ï¼Œä½†ä»ç„¶ç¼ºä¹ä¸€ä¸ªè¢«ä¸»æµæƒ¯æ€§å®šä½æ–¹æ³•å¹¿æ³›é‡‡ç”¨å’Œè®¤å¯çš„å…±åŒåŸºå‡†ã€‚åœ¨æœªæ¥ï¼Œè¦†ç›–å„ç§åº”ç”¨åœºæ™¯çš„å¹¿æ³›é‡‡ç”¨çš„æ•°æ®é›†å’ŒåŸºå‡†å°†æå¤§åœ°ä¿ƒè¿›æ•°æ®é©±åŠ¨çš„æƒ¯æ€§å®šä½ç ”ç©¶ã€‚
- en: IX-B5 Failure Cases and Physical Constraints
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B5 å¤±è´¥æ¡ˆä¾‹ä¸ç‰©ç†é™åˆ¶
- en: 'Deep learning has demonstrated its capability in reducing the drifts of inertial
    positioning and contributing to various aspects of inertial navigation systems,
    as discussed in Section [IV](#S4 "IV Learning to correct IMU Integration â€£ Deep
    Learning for Inertial Positioning: A Survey"). However, DNN models are not always
    reliable and may occasionally produce large and abrupt prediction errors. Unlike
    traditional inertial navigation algorithms that are based on concrete physical
    and mathematical rules, DNN predictions lack constraints, and the failure cases
    must be considered in real-world applications with safety concerns. To enhance
    the robustness of DNN predictions, possible solutions include imposing physical
    constraints on DNN models or combining deep learning with physical models as hybrid
    inertial positioning models. By doing so, the benefits from both learning and
    physics-based positioning models can be leveraged.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦å­¦ä¹ åœ¨å‡å°‘æƒ¯æ€§å®šä½æ¼‚ç§»å’Œä¿ƒè¿›æƒ¯æ€§å¯¼èˆªç³»ç»Ÿçš„å„ä¸ªæ–¹é¢ä¸­å±•ç¤ºäº†å…¶èƒ½åŠ›ï¼Œå¦‚[IV](#S4 "IV Learning to correct IMU Integration
    â€£ Deep Learning for Inertial Positioning: A Survey")èŠ‚ä¸­æ‰€è®¨è®ºçš„ã€‚ç„¶è€Œï¼ŒDNNæ¨¡å‹å¹¶ä¸æ€»æ˜¯å¯é çš„ï¼Œå¯èƒ½ä¼šå¶å°”äº§ç”Ÿå¤§çš„ã€çªå…€çš„é¢„æµ‹è¯¯å·®ã€‚ä¸åŸºäºå…·ä½“ç‰©ç†å’Œæ•°å­¦è§„åˆ™çš„ä¼ ç»Ÿæƒ¯æ€§å¯¼èˆªç®—æ³•ä¸åŒï¼ŒDNNé¢„æµ‹ç¼ºä¹çº¦æŸï¼Œå¤±è´¥æƒ…å†µå¿…é¡»åœ¨å…·æœ‰å®‰å…¨éšæ‚£çš„å®é™…åº”ç”¨ä¸­åŠ ä»¥è€ƒè™‘ã€‚ä¸ºäº†æé«˜DNNé¢„æµ‹çš„é²æ£’æ€§ï¼Œå¯èƒ½çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬å¯¹DNNæ¨¡å‹æ–½åŠ ç‰©ç†çº¦æŸï¼Œæˆ–å°†æ·±åº¦å­¦ä¹ ä¸ç‰©ç†æ¨¡å‹ç»“åˆä¸ºæ··åˆæƒ¯æ€§å®šä½æ¨¡å‹ã€‚é€šè¿‡è¿™æ ·åšï¼Œå¯ä»¥åˆ©ç”¨å­¦ä¹ å’ŒåŸºäºç‰©ç†çš„å®šä½æ¨¡å‹çš„åŒé‡ä¼˜åŠ¿ã€‚'
- en: IX-B6 New deep learning methods
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B6 æ–°å‹æ·±åº¦å­¦ä¹ æ–¹æ³•
- en: Machine/deep learning is one of the fastest growing areas of AI, and its advances
    have influenced numerous fields such as computer vision, robotics, natural language
    processing, and signal processing. There are significant opportunities for applying
    deep learning techniques to inertial navigation and analyzing their effectiveness
    and theoretical underpinnings. In the future, new model structures such as transformer
    [[72](#bib.bib72)], diffusion models [[112](#bib.bib112)], and generative models
    [[69](#bib.bib69)], and new learning methods such as transfer learning, reinforcement
    learning, contrastive learning [[109](#bib.bib109)], unsupervised learning, and
    meta-learning [[113](#bib.bib113)], all hold promise for enhancing inertial positioning
    systems. Furthermore, advances in other domains such as neural rendering [[114](#bib.bib114)]
    and voice synthesis [[115](#bib.bib115)] may provide valuable insights into developing
    more effective inertial positioning systems. Therefore, incorporating these rapidly-evolving
    deep learning methods into inertial navigation will be a significant area of research
    in the future.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨/æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸä¸­å¢é•¿æœ€å¿«çš„é¢†åŸŸä¹‹ä¸€ï¼Œå…¶è¿›å±•å·²ç»å½±å“äº†è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººæŠ€æœ¯ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¿¡å·å¤„ç†ç­‰ä¼—å¤šé¢†åŸŸã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨æƒ¯æ€§å¯¼èˆªä¸­çš„åº”ç”¨åŠå…¶æ•ˆæœå’Œç†è®ºåŸºç¡€æœ‰ç€é‡è¦çš„æœºä¼šã€‚æœªæ¥ï¼Œæ–°å‹æ¨¡å‹ç»“æ„å¦‚å˜æ¢å™¨
    [[72](#bib.bib72)]ã€æ‰©æ•£æ¨¡å‹ [[112](#bib.bib112)] å’Œç”Ÿæˆæ¨¡å‹ [[69](#bib.bib69)]ï¼Œä»¥åŠæ–°çš„å­¦ä¹ æ–¹æ³•å¦‚è¿ç§»å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€å¯¹æ¯”å­¦ä¹ 
    [[109](#bib.bib109)]ã€æ— ç›‘ç£å­¦ä¹ å’Œå…ƒå­¦ä¹  [[113](#bib.bib113)]ï¼Œéƒ½æœ‰å¯èƒ½æå‡æƒ¯æ€§å®šä½ç³»ç»Ÿçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…¶ä»–é¢†åŸŸå¦‚ç¥ç»æ¸²æŸ“
    [[114](#bib.bib114)] å’Œè¯­éŸ³åˆæˆ [[115](#bib.bib115)] çš„è¿›å±•å¯èƒ½ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„æƒ¯æ€§å®šä½ç³»ç»Ÿæä¾›å®è´µçš„è§è§£ã€‚å› æ­¤ï¼Œå°†è¿™äº›è¿…é€Ÿå‘å±•çš„æ·±åº¦å­¦ä¹ æ–¹æ³•èå…¥æƒ¯æ€§å¯¼èˆªå°†æˆä¸ºæœªæ¥çš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚
- en: References
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] P.Â G. Savage, â€œStrapdown Inertial Navigation Integration Algorithm Design
    Part 1: Attitude Algorithms,â€ Journal of Guidance, Control, and Dynamics, vol.Â 21,
    no.Â 1, pp.Â 19â€“28, 1998.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. G. Savage, â€œå¸¦å¼æƒ¯æ€§å¯¼èˆªèåˆç®—æ³•è®¾è®¡ç¬¬1éƒ¨åˆ†ï¼šå§¿æ€ç®—æ³•â€ï¼Œã€Šå¯¼å¼•ã€æ§åˆ¶ä¸åŠ¨åŠ›å­¦å­¦æŠ¥ã€‹ï¼Œç¬¬21å·ï¼Œç¬¬1æœŸï¼Œé¡µ19â€“28ï¼Œ1998å¹´ã€‚'
- en: '[2] P.Â G. Savage, â€œStrapdown Inertial Navigation Integration Algorithm Design
    Part 2: Velocity and Position Algorithms,â€ Journal of Guidance, Control, and Dynamics,
    vol.Â 21, no.Â 1, pp.Â 19â€“28, 1998.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. G. Savage, â€œå¸¦å¼æƒ¯æ€§å¯¼èˆªèåˆç®—æ³•è®¾è®¡ç¬¬2éƒ¨åˆ†ï¼šé€Ÿåº¦å’Œä½ç½®ç®—æ³•â€ï¼Œã€Šå¯¼å¼•ã€æ§åˆ¶ä¸åŠ¨åŠ›å­¦å­¦æŠ¥ã€‹ï¼Œç¬¬21å·ï¼Œç¬¬1æœŸï¼Œé¡µ19â€“28ï¼Œ1998å¹´ã€‚'
- en: '[3] R.Â Harle, â€œA survey of indoor inertial positioning systems for pedestrians,â€
    IEEE Communications Surveys & Tutorials, vol.Â 15, no.Â 3, pp.Â 1281â€“1293, 2013.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Harle, â€œæ­¥è¡Œè€…å®¤å†…æƒ¯æ€§å®šä½ç³»ç»Ÿçš„è°ƒæŸ¥â€ï¼Œã€ŠIEEEé€šè®¯è°ƒæŸ¥ä¸æ•™ç¨‹ã€‹ï¼Œç¬¬15å·ï¼Œç¬¬3æœŸï¼Œé¡µ1281â€“1293ï¼Œ2013å¹´ã€‚'
- en: '[4] I.Â Skog, P.Â HÃ¤ndel, J.-O. Nilsson, and J.Â Rantakokko, â€œZero-Velocity Detection
    â€” an Algorithm Evaluation.,â€ IEEE transactions on bio-medical engineering, vol.Â 57,
    no.Â 11, pp.Â 2657â€“2666, 2010.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] I. Skog, P. HÃ¤ndel, J.-O. Nilsson, å’Œ J. Rantakokko, â€œé›¶é€Ÿæ£€æµ‹â€”â€”ä¸€ç§ç®—æ³•è¯„ä¼°â€ï¼Œã€ŠIEEEç”Ÿç‰©åŒ»å­¦å·¥ç¨‹æ±‡åˆŠã€‹ï¼Œç¬¬57å·ï¼Œç¬¬11æœŸï¼Œé¡µ2657â€“2666ï¼Œ2010å¹´ã€‚'
- en: '[5] M.Â Li and A.Â I. Mourikis, â€œHigh-precision, Consistent EKF-based Visual-Inertial
    Odometry,â€ The International Journal of Robotics Research, vol.Â 32, no.Â 6, pp.Â 690â€“711,
    2013.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T.Â Qin, P.Â Li, and S.Â Shen, â€œVINS-Mono: A Robust and Versatile Monocular
    Visual-Inertial State Estimator,â€ IEEE Transactions on Robotics, vol.Â 34, no.Â 4,
    pp.Â 1004â€“1020, 2018.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] W.Â Xu, Y.Â Cai, D.Â He, J.Â Lin, and F.Â Zhang, â€œFast-lio2: Fast direct lidar-inertial
    odometry,â€ IEEE Transactions on Robotics, 2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Y.Â Bengio, I.Â Goodfellow, and A.Â Courville, Deep learning, vol.Â 1. MIT
    press Cambridge, MA, USA, 2017.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Z.-Q. Zhao, P.Â Zheng, S.-t. Xu, and X.Â Wu, â€œObject detection with deep
    learning: A review,â€ IEEE transactions on neural networks and learning systems,
    vol.Â 30, no.Â 11, pp.Â 3212â€“3232, 2019.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S.Â Hao, Y.Â Zhou, and Y.Â Guo, â€œA brief survey on semantic segmentation
    with deep learning,â€ Neurocomputing, vol.Â 406, pp.Â 302â€“321, 2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] N.Â SÃ¼nderhauf, O.Â Brock, W.Â Scheirer, R.Â Hadsell, D.Â Fox, J.Â Leitner,
    B.Â Upcroft, P.Â Abbeel, W.Â Burgard, M.Â Milford, etÂ al., â€œThe limits and potentials
    of deep learning for robotics,â€ The International journal of robotics research,
    vol.Â 37, no.Â 4-5, pp.Â 405â€“420, 2018.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Y.Â Li, R.Â Chen, X.Â Niu, Y.Â Zhuang, Z.Â Gao, X.Â Hu, and N.Â El-Sheimy, â€œInertial
    sensing meets machine learning: Opportunity or challenge?,â€ IEEE Transactions
    on Intelligent Transportation Systems, 2021.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P.Â S. Farahsari, A.Â Farahzadi, J.Â Rezazadeh, and A.Â Bagheri, â€œA survey
    on indoor positioning systems for iot-based applications,â€ IEEE Internet of Things
    Journal, vol.Â 9, no.Â 10, pp.Â 7680â€“7699, 2022.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L.Â E. DÃ­ez, A.Â Bahillo, J.Â Otegui, and T.Â Otim, â€œStep length estimation
    methods based on inertial sensors: A review,â€ IEEE Sensors Journal, vol.Â 18, no.Â 17,
    pp.Â 6908â€“6926, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y.Â Wu, H.-B. Zhu, Q.-X. Du, and S.-M. Tang, â€œA survey of the research
    status of pedestrian dead reckoning systems based on inertial sensors,â€ International
    Journal of Automation and Computing, vol.Â 16, no.Â 1, pp.Â 65â€“83, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] X.Â Ru, N.Â Gu, H.Â Shang, and H.Â Zhang, â€œMems inertial sensor calibration
    technology: Current status and future trends,â€ Micromachines, vol.Â 13, no.Â 6,
    p.Â 879, 2022.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N.Â Naser, El-Sheimy; Haiying, Hou; Xiaojii, â€œAnalysis and Modeling of
    Inertial Sensors Using Allan Variance,â€ IEEE Transactions on Instrumentation and
    Measurement, vol.Â 57, no.Â JANUARY, pp.Â 684â€“694, 2008.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] H.Â Weinberg, â€œUsing the adxl202 in pedometer and personal navigation applications,â€
    Analog Devices AN-602 application note, vol.Â 2, no.Â 2, pp.Â 1â€“6, 2002.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L.Â Fang, P.Â J. Antsaklis, L.Â A. Montestruque, M.Â B. McMickell, M.Â Lemmon,
    Y.Â Sun, H.Â Fang, I.Â Koutroulis, M.Â Haenggi, M.Â Xie, etÂ al., â€œDesign of a wireless
    assisted pedestrian dead reckoning system-the navmote experience,â€ IEEE transactions
    on Instrumentation and Measurement, vol.Â 54, no.Â 6, pp.Â 2342â€“2358, 2005.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] P.Â Goyal, V.Â J. Ribeiro, H.Â Saran, and A.Â Kumar, â€œStrap-down pedestrian
    dead-reckoning system,â€ in 2011 international conference on indoor positioning
    and indoor navigation, pp.Â 1â€“7, IEEE, 2011.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] B.Â Huang, G.Â Qi, X.Â Yang, L.Â Zhao, and H.Â Zou, â€œExploiting cyclic features
    of walking for pedestrian dead reckoning with unconstrained smartphones,â€ in Proceedings
    of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,
    pp.Â 374â€“385, 2016.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D.Â Feng, C.Â Wang, C.Â He, Y.Â Zhuang, and X.-G. Xia, â€œKalman-filter-based
    integration of imu and uwb for high-accuracy indoor positioning and navigation,â€
    IEEE Internet of Things Journal, vol.Â 7, no.Â 4, pp.Â 3133â€“3146, 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S.Â Yang, J.Â Liu, X.Â Gong, G.Â Huang, and Y.Â Bai, â€œA robust heading estimation
    solution for smartphone multisensor-integrated indoor positioning,â€ IEEE Internet
    of Things Journal, vol.Â 8, no.Â 23, pp.Â 17186â€“17198, 2021.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C.Â Xiyuan, â€œModeling random gyro drift by time series neural networks
    and by traditional method,â€ in International Conference on Neural Networks and
    Signal Processing, 2003\. Proceedings of the 2003, vol.Â 1, pp.Â 810â€“813, IEEE,
    2003.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H.Â Chen, P.Â Aggarwal, T.Â M. Taha, and V.Â P. Chodavarapu, â€œImproving inertial
    sensor by reducing errors using deep learning methodology,â€ in NAECON 2018-IEEE
    National Aerospace and Electronics Conference, pp.Â 197â€“202, IEEE, 2018.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M.Â A. Esfahani, H.Â Wang, K.Â Wu, and S.Â Yuan, â€œOrinet: Robust 3-d orientation
    estimation with a single particular imu,â€ IEEE Robotics and Automation Letters,
    vol.Â 5, no.Â 2, pp.Â 399â€“406, 2019.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] F.Â Nobre and C.Â Heckman, â€œLearning to calibrate: Reinforcement learning
    for guided calibration of visualâ€“inertial rigs,â€ The International Journal of
    Robotics Research, vol.Â 38, no.Â 12-13, pp.Â 1388â€“1402, 2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M.Â Brossard, S.Â Bonnabel, and A.Â Barrau, â€œDenoising imu gyroscopes with
    deep learning for open-loop attitude estimation,â€ IEEE Robotics and Automation
    Letters, vol.Â 5, no.Â 3, pp.Â 4796â€“4803, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] X.Â Zhao, C.Â Deng, X.Â Kong, J.Â Xu, and Y.Â Liu, â€œLearning to compensate
    for the drift and error of gyroscope in vehicle localization,â€ in 2020 IEEE Intelligent
    Vehicles Symposium (IV), pp.Â 852â€“857, IEEE, 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] F.Â Huang, Z.Â Wang, L.Â Xing, and C.Â Gao, â€œA mems imu gyroscope calibration
    method based on deep learning,â€ IEEE Transactions on Instrumentation and Measurement,
    vol.Â 71, pp.Â 1â€“9, 2022.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] R.Â Li, C.Â Fu, W.Â Yi, and X.Â Yi, â€œCalib-net: Calibrating the low-cost imu
    via deep convolutional neural network,â€ Frontiers in Robotics and AI, vol.Â 8,
    p.Â 772583, 2022.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S.-i. Amari, â€œBackpropagation and stochastic gradient descent method,â€
    Neurocomputing, vol.Â 5, no.Â 4-5, pp.Â 185â€“196, 1993.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A.Â K. Jain, J.Â Mao, and K.Â M. Mohiuddin, â€œArtificial neural networks:
    A tutorial,â€ Computer, vol.Â 29, no.Â 3, pp.Â 31â€“44, 1996.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C.Â Chen, X.Â Lu, A.Â Markham, and N.Â Trigoni, â€œIonet: Learning to cure the
    curse of drift in inertial odometry,â€ in The Conference on Artificial Intelligence
    (AAAI), 2018.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H.Â Yan, Q.Â Shan, and Y.Â Furukawa, â€œRidi: Robust imu double integration,â€
    in Proceedings of the European Conference on Computer Vision (ECCV), pp.Â 621â€“636,
    2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S.Â CortÃ©s, A.Â Solin, and J.Â Kannala, â€œDeep learning based speed estimation
    for constraining strapdown inertial navigation on smartphones,â€ in 2018 IEEE 28th
    International Workshop on Machine Learning for Signal Processing (MLSP), pp.Â 1â€“6,
    IEEE, 2018.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] B.Â Wagstaff and J.Â Kelly, â€œLstm-based zero-velocity detection for robust
    inertial navigation,â€ in 2018 International Conference on Indoor Positioning and
    Indoor Navigation (IPIN), pp.Â 1â€“8, IEEE, 2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] C.Â Chen, Y.Â Miao, C.Â X. Lu, L.Â Xie, P.Â Blunsom, A.Â Markham, and N.Â Trigoni,
    â€œMotiontransformer: Transferring neural inertial tracking between domains,â€ in
    The Conference on Artificial Intelligence (AAAI), vol.Â 33, 2019.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M.Â A. Esfahani, H.Â Wang, K.Â Wu, and S.Â Yuan, â€œAboldeepio: A novel deep
    inertial odometry network for autonomous vehicles,â€ IEEE Transactions on Intelligent
    Transportation Systems, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M.Â Brossard, A.Â Barrau, and S.Â Bonnabel, â€œRins-w: Robust inertial navigation
    system on wheels,â€ The IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T.Â Feigl, S.Â Kram, P.Â Woller, R.Â H. Siddiqui, M.Â Philippsen, and C.Â Mutschler,
    â€œA bidirectional lstm for estimating dynamic human velocities from a single imu,â€
    in 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN),
    pp.Â 1â€“8, IEEE, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Q.Â Wang, H.Â Luo, L.Â Ye, A.Â Men, F.Â Zhao, Y.Â Huang, and C.Â Ou, â€œPedestrian
    heading estimation based on spatial transformer networks and hierarchical lstm,â€
    IEEE Access, vol.Â 7, pp.Â 162309â€“162322, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X.Â Yu, B.Â Liu, X.Â Lan, Z.Â Xiao, S.Â Lin, B.Â Yan, and L.Â Zhou, â€œAzupt: Adaptive
    zero velocity update based on neural networks for pedestrian tracking,â€ in 2019
    IEEE Global Communications Conference (GLOBECOM), pp.Â 1â€“6, IEEE, 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] W.Â Liu, D.Â Caruso, E.Â Ilg, J.Â Dong, A.Â I. Mourikis, K.Â Daniilidis, V.Â Kumar,
    and J.Â Engel, â€œTlio: Tight learned inertial odometry,â€ IEEE Robotics and Automation
    Letters, vol.Â 5, no.Â 4, pp.Â 5653â€“5660, 2020.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C.Â Chen, P.Â Zhao, C.Â X. Lu, W.Â Wang, A.Â Markham, and N.Â Trigoni, â€œDeep-learning-based
    pedestrian inertial navigation: Methods, data set, and on-device inference,â€ IEEE
    Internet of Things Journal, vol.Â 7, no.Â 5, pp.Â 4431â€“4441, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S.Â Herath, H.Â Yan, and Y.Â Furukawa, â€œRonin: Robust neural inertial navigation
    in the wild: Benchmark, evaluations, & new methods,â€ in 2020 IEEE International
    Conference on Robotics and Automation (ICRA), pp.Â 3146â€“3152, IEEE, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M.Â Brossard, A.Â Barrau, and S.Â Bonnabel, â€œAi-imu dead-reckoning,â€ IEEE
    Transactions on Intelligent Vehicles, vol.Â 5, no.Â 4, pp.Â 585â€“595, 2020.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] I.Â Klein and O.Â Asraf, â€œStepnetâ€”deep learning approaches for step length
    estimation,â€ IEEE Access, vol.Â 8, pp.Â 85706â€“85713, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y.Â Wang, H.Â Cheng, and M.Â Q.-H. Meng, â€œPedestrian motion tracking by using
    inertial sensors on the smartphone,â€ in 2020 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pp.Â 4426â€“4431, IEEE, 2020.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X.Â Teng, P.Â Xu, D.Â Guo, Y.Â Guo, R.Â Hu, H.Â Chai, and D.Â Chuxing, â€œArpdr:
    An accurate and robust pedestrian dead reckoning system for indoor localization
    on handheld smartphones,â€ in 2020 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS), pp.Â 10888â€“10893, IEEE, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S.Â Sun, D.Â Melamed, and K.Â Kitani, â€œIdol: Inertial deep orientation-estimation
    and localization,â€ in Proceedings of the AAAI Conference on Artificial Intelligence,
    vol.Â 35, pp.Â 6128â€“6137, 2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] O.Â Asraf, F.Â Shama, and I.Â Klein, â€œPdrnet: A deep-learning pedestrian
    dead reckoning framework,â€ IEEE Sensors Journal, vol.Â 22, no.Â 6, pp.Â 4932â€“4939,
    2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] R.Â Buchanan, M.Â Camurri, F.Â Dellaert, and M.Â Fallon, â€œLearning inertial
    odometry for dynamic legged robot state estimation,â€ in Conference on Robot Learning,
    pp.Â 1575â€“1584, PMLR, 2022.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M.Â Zhang, M.Â Zhang, Y.Â Chen, and M.Â Li, â€œImu data processing for inertial
    aided navigation: A recurrent neural network based approach,â€ in 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pp.Â 3992â€“3998, IEEE, 2021.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J.Â Gong, X.Â Zhang, Y.Â Huang, J.Â Ren, and Y.Â Zhang, â€œRobust inertial motion
    tracking through deep sensor fusion across smart earbuds and smartphone,â€ Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol.Â 5,
    no.Â 2, pp.Â 1â€“26, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S.Â Herath, D.Â Caruso, C.Â Liu, Y.Â Chen, and Y.Â Furukawa, â€œNeural inertial
    localization,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp.Â 6604â€“6613, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X.Â Cao, C.Â Zhou, D.Â Zeng, and Y.Â Wang, â€œRio: Rotation-equivariance supervised
    learning of robust inertial odometry,â€ in Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp.Â 6614â€“6623, 2022.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y.Â Wang, J.Â Kuang, Y.Â Li, and X.Â Niu, â€œMagnetic field-enhanced learning-based
    inertial odometry for indoor pedestrian,â€ IEEE Transactions on Instrumentation
    and Measurement, vol.Â 71, pp.Â 1â€“13, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] S.Â S. Saha, S.Â S. Sandha, L.Â A. Garcia, and M.Â Srivastava, â€œTinyodom:
    Hardware-aware efficient neural inertial navigation,â€ Proceedings of the ACM on
    Interactive, Mobile, Wearable and Ubiquitous Technologies, vol.Â 6, no.Â 2, pp.Â 1â€“32,
    2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] B.Â Rao, E.Â Kazemi, Y.Â Ding, D.Â M. Shila, F.Â M. Tucker, and L.Â Wang, â€œCtin:
    Robust contextual transformer network for inertial navigation,â€ in Proceedings
    of the AAAI Conference on Artificial Intelligence, vol.Â 36, pp.Â 5413â€“5421, 2022.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B.Â Zhou, Z.Â Gu, F.Â Gu, P.Â Wu, C.Â Yang, X.Â Liu, L.Â Li, Y.Â Li, and Q.Â Li,
    â€œDeepvip: Deep learning-based vehicle indoor positioning using smartphones,â€ IEEE
    Transactions on Vehicular Technology, 2022.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] F.Â Bo, J.Â Li, and W.Â Wang, â€œMode-independent stride length estimation
    with imus in smartphones,â€ IEEE Sensors Journal, vol.Â 22, no.Â 6, pp.Â 5824â€“5833,
    2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] H.Â Tang, X.Â Niu, T.Â Zhang, Y.Â Li, and J.Â Liu, â€œOdonet: Untethered speed
    aiding for vehicle navigation without hardware wheeled odometer,â€ IEEE Sensors
    Journal, 2022.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y.Â Wang, H.Â Cheng, and M.Â Q.-H. Meng, â€œA2dio: Attention-driven deep inertial
    odometry for pedestrian localization based on 6d imu,â€ in 2022 International Conference
    on Robotics and Automation (ICRA), pp.Â 819â€“825, IEEE, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y.Â Wang, J.Â Kuang, X.Â Niu, and J.Â Liu, â€œLlio: Lightweight learned inertial
    odometer,â€ IEEE Internet of Things Journal, 2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S.Â Hochreiter and J.Â Schmidhuber, â€œLong short-term memory,â€ Neural computation,
    vol.Â 9, no.Â 8, pp.Â 1735â€“1780, 1997.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] F.Â Yu and V.Â Koltun, â€œMulti-scale context aggregation by dilated convolutions,â€
    The International Conference on Learning Representations (ICLR), 2016.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R.Â S. Sutton and A.Â G. Barto, Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] I.Â Goodfellow, J.Â Pouget-Abadie, M.Â Mirza, B.Â Xu, D.Â Warde-Farley, S.Â Ozair,
    A.Â Courville, and Y.Â Bengio, â€œGenerative adversarial networks,â€ Communications
    of the ACM, vol.Â 63, no.Â 11, pp.Â 139â€“144, 2020.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] E.Â Tzeng, J.Â Hoffman, K.Â Saenko, and T.Â Darrell, â€œAdversarial discriminative
    domain adaptation,â€ in Proceedings of the IEEE conference on computer vision and
    pattern recognition, pp.Â 7167â€“7176, 2017.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C.Â Chen, X.Â Lu, J.Â Wahlstrom, A.Â Markham, and N.Â Trigoni, â€œDeep neural
    network based inertial odometry using low-cost inertial measurement units,â€ IEEE
    Transactions on Mobile Computing, 2021.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez,
    Å.Â Kaiser, and I.Â Polosukhin, â€œAttention is all you need,â€ Advances in neural
    information processing systems, vol.Â 30, 2017.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] N.Â A. Abiad, Y.Â Kone, V.Â Renaudin, and T.Â Robert, â€œSmartstep: A robust
    step detection method based on smartphone inertial signals driven by gait learning,â€
    IEEE Sensors Journal, vol.Â 22, pp.Â 12288â€“12297, 6 2022.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M.Â Jaderberg, K.Â Simonyan, A.Â Zisserman, etÂ al., â€œSpatial transformer
    networks,â€ Advances in neural information processing systems, vol.Â 28, 2015.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A.Â Manos, T.Â Hazan, and I.Â Klein, â€œWalking direction estimation using
    smartphone sensors: A deep network-based framework,â€ IEEE Transactions on Instrumentation
    and Measurement, vol.Â 71, 2022.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C.Â Lea, M.Â D. Flynn, R.Â Vidal, A.Â Reiter, and G.Â D. Hager, â€œTemporal convolutional
    networks for action segmentation and detection,â€ in proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp.Â 156â€“165, 2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] P.Â Ren, Y.Â Xiao, X.Â Chang, P.-Y. Huang, Z.Â Li, X.Â Chen, and X.Â Wang, â€œA
    comprehensive survey of neural architecture search: Challenges and solutions,â€
    ACM Computing Surveys (CSUR), vol.Â 54, no.Â 4, pp.Â 1â€“34, 2021.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] R.Â Clark, S.Â Wang, H.Â Wen, A.Â Markham, and N.Â Trigoni, â€œVINet : Visual-Inertial
    Odometry as a Sequence-to-Sequence Learning Problem,â€ in The Conference on Artificial
    Intelligence (AAAI), pp.Â 3995â€“4001, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] E.Â J. Shamwell, K.Â Lindgren, S.Â Leung, and W.Â D. Nothwang, â€œUnsupervised
    deep visual-inertial odometry with online error correction for rgb-d imagery,â€
    IEEE transactions on pattern analysis and machine intelligence, 2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C.Â Chen, S.Â Rosa, Y.Â Miao, C.Â X. Lu, W.Â Wu, A.Â Markham, and N.Â Trigoni,
    â€œSelective sensor fusion for neural visual-inertial odometry,â€ in IEEE/CVF International
    Conference on Computer Vision and Pattern Recognition (CVPR), pp.Â 10542â€“10551,
    2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L.Â Han, Y.Â Lin, G.Â Du, and S.Â Lian, â€œDeepvio: Self-supervised deep learning
    of monocular visual inertial odometry using 3d geometric constraints,â€ in 2019
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.Â 6906â€“6913,
    IEEE, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M.Â R.Â U. Saputra, P.Â P. deÂ Gusmao, C.Â X. Lu, Y.Â Almalioglu, S.Â Rosa, C.Â Chen,
    J.Â WahlstrÃ¶m, W.Â Wang, A.Â Markham, and N.Â Trigoni, â€œDeeptio: A deep thermal-inertial
    odometry with visual hallucination,â€ IEEE Robotics and Automation Letters, vol.Â 5,
    no.Â 2, pp.Â 1672â€“1679, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C.Â X. Lu, M.Â R.Â U. Saputra, P.Â Zhao, Y.Â Almalioglu, P.Â P. DeÂ Gusmao, C.Â Chen,
    K.Â Sun, N.Â Trigoni, and A.Â Markham, â€œmilliego: single-chip mmwave radar aided
    egomotion estimation via deep sensor fusion,â€ in Proceedings of the 18th Conference
    on Embedded Networked Sensor Systems, pp.Â 109â€“122, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] P.Â Wei, G.Â Hua, W.Â Huang, F.Â Meng, and H.Â Liu, â€œUnsupervised monocular
    visual-inertial odometry network,â€ in Proceedings of the Twenty-Ninth International
    Conference on International Joint Conferences on Artificial Intelligence, pp.Â 2347â€“2354,
    2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C.Â Chen, C.Â X. Lu, B.Â Wang, N.Â Trigoni, and A.Â Markham, â€œDynanet: Neural
    kalman dynamical model for motion estimation and prediction,â€ IEEE Transactions
    on Neural Networks and Learning Systems, vol.Â 32, no.Â 12, pp.Â 5479â€“5491, 2021.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y.Â Almalioglu, M.Â Turan, M.Â R.Â U. Saputra, P.Â P. deÂ GusmÃ£o, A.Â Markham,
    and N.Â Trigoni, â€œSelfvio: Self-supervised deep monocular visualâ€“inertial odometry
    and depth estimation,â€ Neural Networks, vol.Â 150, pp.Â 119â€“136, 2022.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y.Â Tu and J.Â Xie, â€œUndeeplio: Unsupervised deep lidar-inertial odometry,â€
    in Asian Conference on Pattern Recognition, pp.Â 189â€“202, Springer, 2022.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] D.Â Ramachandram and G.Â W. Taylor, â€œDeep multimodal learning: A survey
    on recent advances and trends,â€ IEEE signal processing magazine, vol.Â 34, no.Â 6,
    pp.Â 96â€“108, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] E.Â S. Jones and S.Â Soatto, â€œVisual-inertial navigation, mapping and localization:
    A scalable real-time causal approach,â€ The International Journal of Robotics Research,
    vol.Â 30, no.Â 4, pp.Â 407â€“430, 2011.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S.Â Leutenegger, S.Â Lynen, M.Â Bosse, R.Â Siegwart, and P.Â Furgale, â€œKeyframe-based
    visualâ€“inertial odometry using nonlinear optimization,â€ The International Journal
    of Robotics Research, vol.Â 34, no.Â 3, pp.Â 314â€“334, 2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C.Â Forster, L.Â Carlone, F.Â Dellaert, and D.Â Scaramuzza, â€œOn-manifold preintegration
    for real-time visualâ€“inertial odometry,â€ IEEE Transactions on Robotics, vol.Â 33,
    no.Â 1, pp.Â 1â€“21, 2017.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] A.Â Geiger, P.Â Lenz, C.Â Stiller, and R.Â Urtasun, â€œVision meets robotics:
    The kitti dataset,â€ The International Journal of Robotics Research, vol.Â 32, no.Â 11,
    pp.Â 1231â€“1237, 2013.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] M.Â Burri, J.Â Nikolic, P.Â Gohl, T.Â Schneider, J.Â Rehder, S.Â Omari, M.Â W.
    Achtelik, and R.Â Siegwart, â€œThe euroc micro aerial vehicle datasets,â€ The International
    Journal of Robotics Research, vol.Â 35, no.Â 10, pp.Â 1157â€“1163, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S.Â Ahuja, W.Â Jirattigalachote, and A.Â Tosborvorn, â€œImproving Accuracy
    of Inertial Measurement Units using Support Vector Regression,â€ tech. rep., 2011.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A.Â Parate, M.Â C. Chiu, C.Â Chadowitz, D.Â Ganesan, and E.Â Kalogerakis, â€œRisQ:
    Recognizing smoking gestures with inertial sensors on a wristband,â€ in Annual
    International Conference on Mobile Systems, Applications, and Services (MobiSys),
    pp.Â 149â€“161, 2014.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A.Â Mannini and A.Â M. Sabatini, â€œMachine learning methods for classifying
    human physical activity from on-body accelerometers,â€ Sensors, vol.Â 10, no.Â 2,
    pp.Â 1154â€“1175, 2010.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A.Â Valtazanos, D.Â Arvind, and S.Â Ramamoorthy, â€œUsing wearable inertial
    sensors for posture and position tracking in unconstrained environments through
    learned translation manifolds,â€ in 2013 ACM/IEEE International Conference on Information
    Processing in Sensor Networks (IPSN), pp.Â 241â€“252, IEEE, 2013.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M.Â Yuwono, S.Â W. Su, Y.Â Guo, B.Â D. Moulton, and H.Â T. Nguyen, â€œUnsupervised
    nonparametric method for gait analysis using a waist-worn inertial sensor,â€ Applied
    Soft Computing, vol.Â 14, pp.Â 72â€“80, 2014.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y.Â Huang, M.Â Kaufmann, E.Â Aksan, M.Â J. Black, O.Â Hilliges, and G.Â Pons-Moll,
    â€œDeep inertial poser: Learning to reconstruct human pose from sparse inertial
    measurements in real time,â€ ACM Transactions on Graphics (TOG), vol.Â 37, no.Â 6,
    pp.Â 1â€“15, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X.Â Yi, Y.Â Zhou, and F.Â Xu, â€œTranspose: real-time 3d human translation
    and pose estimation with six inertial sensors,â€ ACM Transactions on Graphics (TOG),
    vol.Â 40, no.Â 4, pp.Â 1â€“13, 2021.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X.Â Yi, Y.Â Zhou, M.Â Habermann, S.Â Shimada, V.Â Golyanik, C.Â Theobalt, and
    F.Â Xu, â€œPhysical inertial poser (pip): Physics-aware real-time human motion tracking
    from sparse inertial sensors,â€ in Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp.Â 13167â€“13178, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] D.Â Anguita, A.Â Ghio, L.Â Oneto, X.Â ParraÂ Perez, and J.Â L. ReyesÂ Ortiz,
    â€œA public domain dataset for human activity recognition using smartphones,â€ in
    Proceedings of the 21th international European symposium on artificial neural
    networks, computational intelligence and machine learning, pp.Â 437â€“442, 2013.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] G.Â Chevalier, â€œLstms for human activity recognition,â€ 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T.Â Zebin, P.Â J. Scully, and K.Â B. Ozanyan, â€œHuman activity recognition
    with inertial sensors using a deep learning approach,â€ in 2016 IEEE sensors, pp.Â 1â€“3,
    IEEE, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] D.Â Ravi, C.Â Wong, B.Â Lo, and G.-Z. Yang, â€œA deep learning approach to
    on-node sensor data analytics for mobile or wearable devices,â€ IEEE journal of
    biomedical and health informatics, vol.Â 21, no.Â 1, pp.Â 56â€“64, 2016.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] B.Â M. Eskofier, S.Â I. Lee, J.-F. Daneault, F.Â N. Golabchi, G.Â Ferreira-Carvalho,
    G.Â Vergara-Diaz, S.Â Sapienza, G.Â Costante, J.Â Klucken, T.Â Kautz, etÂ al., â€œRecent
    machine learning advancements in sensor-based mobility analysis: Deep learning
    for parkinsonâ€™s disease assessment,â€ in 2016 38th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society (EMBC), pp.Â 655â€“658, IEEE,
    2016.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J.Â Windau and L.Â Itti, â€œInertial-based motion capturing and smart training
    system,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS), pp.Â 4027â€“4034, IEEE, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] K.Â Weiss, T.Â M. Khoshgoftaar, and D.Â Wang, â€œA survey of transfer learning,â€
    Journal of Big data, vol.Â 3, no.Â 1, pp.Â 1â€“40, 2016.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y.Â Tian, C.Â Sun, B.Â Poole, D.Â Krishnan, C.Â Schmid, and P.Â Isola, â€œWhat
    makes for good views for contrastive learning?,â€ Advances in Neural Information
    Processing Systems, vol.Â 33, pp.Â 6827â€“6839, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A.Â Kendall and Y.Â Gal, â€œWhat uncertainties do we need in bayesian deep
    learning for computer vision?,â€ Advances in neural information processing systems,
    vol.Â 30, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] J.Â Gou, B.Â Yu, S.Â J. Maybank, and D.Â Tao, â€œKnowledge distillation: A
    survey,â€ International Journal of Computer Vision, vol.Â 129, no.Â 6, pp.Â 1789â€“1819,
    2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] C.Â Saharia, W.Â Chan, S.Â Saxena, L.Â Li, J.Â Whang, E.Â Denton, S.Â K.Â S.
    Ghasemipour, B.Â K. Ayan, S.Â S. Mahdavi, R.Â G. Lopes, etÂ al., â€œPhotorealistic text-to-image
    diffusion models with deep language understanding,â€ Neural Information Processing
    Systems, 2022.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] C.Â Finn, P.Â Abbeel, and S.Â Levine, â€œModel-agnostic meta-learning for
    fast adaptation of deep networks,â€ in International conference on machine learning,
    pp.Â 1126â€“1135, PMLR, 2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] B.Â Mildenhall, P.Â P. Srinivasan, M.Â Tancik, J.Â T. Barron, R.Â Ramamoorthi,
    and R.Â Ng, â€œNerf: Representing scenes as neural radiance fields for view synthesis,â€
    Communications of the ACM, vol.Â 65, no.Â 1, pp.Â 99â€“106, 2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A.Â Oord, Y.Â Li, I.Â Babuschkin, K.Â Simonyan, O.Â Vinyals, K.Â Kavukcuoglu,
    G.Â Driessche, E.Â Lockhart, L.Â Cobo, F.Â Stimberg, etÂ al., â€œParallel wavenet: Fast
    high-fidelity speech synthesis,â€ in International conference on machine learning,
    pp.Â 3918â€“3926, PMLR, 2018.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu,
    G. Driessche, E. Lockhart, L. Cobo, F. Stimberg ç­‰äººï¼Œâ€œParallel wavenet: Fast high-fidelity
    speech synthesisï¼Œâ€ å‘è¡¨åœ¨å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼Œé¡µç  3918â€“3926ï¼ŒPMLRï¼Œ2018å¹´ã€‚'
- en: '| ![[Uncaptioned image]](img/241a85796e8a35d57f6d4a3d77089abc.png) | Changhao
    Chen is a Lecturer at College of Intelligence Science and Technology, National
    University of Defense Technology (China). Before that, he obtained his Ph.D. degree
    at University of Oxford (UK), M.Eng. degree at National University of Defense
    Technology (China), and B.Eng. degree at Tongji University (China). His research
    interest lies in robotics, computer vision and cyberphysical systems. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/241a85796e8a35d57f6d4a3d77089abc.png) | Changhao Chen æ˜¯ä¸­å›½å›½é˜²ç§‘æŠ€å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢çš„è®²å¸ˆã€‚åœ¨æ­¤ä¹‹å‰ï¼Œä»–åœ¨ç‰›æ´¥å¤§å­¦ï¼ˆè‹±å›½ï¼‰è·å¾—åšå£«å­¦ä½ï¼Œåœ¨å›½é˜²ç§‘æŠ€å¤§å­¦ï¼ˆä¸­å›½ï¼‰è·å¾—ç¡•å£«å­¦ä½ï¼Œå¹¶åœ¨åŒæµå¤§å­¦ï¼ˆä¸­å›½ï¼‰è·å¾—å­¦å£«å­¦ä½ã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æœºå™¨äººæŠ€æœ¯ã€è®¡ç®—æœºè§†è§‰å’Œç½‘ç»œç‰©ç†ç³»ç»Ÿã€‚
    |'
- en: '| ![[Uncaptioned image]](img/bff8f31cc2fb0546aee879d0baefd51a.png) | Xianfei
    Pan received the Ph.D. degree in control science and engineering from the National
    University of Defense Technology, Changsha, China, in 2008\. Currently, he is
    a professor of the College of Intelligence Science and Technology, National University
    of Defense Technology. His current research interests include Inertial navigation
    system and indoor navigation system. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/bff8f31cc2fb0546aee879d0baefd51a.png) | Xianfei Pan äº 2008
    å¹´åœ¨ä¸­å›½é•¿æ²™å›½é˜²ç§‘æŠ€å¤§å­¦è·å¾—æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹åšå£«å­¦ä½ã€‚ç›®å‰ï¼Œä»–æ˜¯å›½é˜²ç§‘æŠ€å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢çš„æ•™æˆã€‚ä»–ç›®å‰çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æƒ¯æ€§å¯¼èˆªç³»ç»Ÿå’Œå®¤å†…å¯¼èˆªç³»ç»Ÿã€‚ |'
