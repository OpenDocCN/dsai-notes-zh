- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2308.13998] Computation-efficient Deep Learning for Computer Vision: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.13998](https://ar5iv.labs.arxiv.org/html/2308.13998)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Computation-efficient Deep Learning for Computer Vision:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey
  prefs: []
  type: TYPE_NORMAL
- en: 'Yulin Wang^(†∗) ^∗Yulin Wang and Yizeng Han contribute equally to this work.    ^§Corresponding
    author: Gao Huang. Yizeng Han^(†∗) Chaofei Wang^† Shiji Song^† Qi Tian^‡ Gao Huang^(†§)'
  prefs: []
  type: TYPE_NORMAL
- en: ^†Department of Automation BNRist Tsinghua University     ^‡Huawei Inc.
  prefs: []
  type: TYPE_NORMAL
- en: wang-yl19@mails.tsinghua.edu.cn   gaohuang@tsinghua.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract\\
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Over the past decade, deep learning models have exhibited considerable advancements,
    reaching or even exceeding human-level performance in a range of visual perception
    tasks. This remarkable progress has sparked interest in applying deep networks
    to real-world applications, such as autonomous vehicles, mobile devices, robotics,
    and edge computing. However, the challenge remains that state-of-the-art models
    usually demand significant computational resources, leading to impractical power
    consumption, latency, or carbon emissions in real-world scenarios. This trade-off
    between effectiveness and efficiency has catalyzed the emergence of a new research
    focus: computationally efficient deep learning, which strives to achieve satisfactory
    performance while minimizing the computational cost during inference. This review
    offers an extensive analysis of this rapidly evolving field by examining four
    key areas: 1) the development of static or dynamic light-weighted backbone models
    for the efficient extraction of discriminative deep representations; 2) the specialized
    network architectures or algorithms tailored for specific computer vision tasks;
    3) the techniques employed for compressing deep learning models; and 4) the strategies
    for deploying efficient deep networks on hardware platforms. Additionally, we
    provide a systematic discussion on the critical challenges faced in this domain,
    such as network architecture design, training schemes, practical efficiency, and
    more realistic model compression approaches, as well as potential future research
    directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the past decade, the field of computer vision has experienced significant
    advancements in deep learning. Innovations in model architectures and learning
    algorithms [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)] have allowed deep networks to approach or even
    exceed human-level performance on benchmark competition datasets for a wide range
    of visual tasks, such as image recognition [[6](#bib.bib6), [7](#bib.bib7)], object
    detection [[8](#bib.bib8)], image segmentation [[9](#bib.bib9), [10](#bib.bib10)],
    video understanding [[11](#bib.bib11), [12](#bib.bib12)], and 3D perception [[13](#bib.bib13)].
    This considerable progress has stimulated interest in deploying deep models in
    practical applications, including self-driving cars, mobile devices, robotics,
    unmanned aerial vehicles, and internet of things devices [[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: However, the demands of real-world applications are distinct from those of competitions.
    Models achieving state-of-the-art accuracy in competitions often exhibit computational
    intensity and resource requirements during inference. In contrast, computation
    is typically equivalent to practical latency, power consumption, and carbon emissions.
    Low-latency or real-time inference is crucial for ensuring security and enhancing
    user experience [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)].
    Deep learning systems must prioritize low power consumption to improve battery
    life or reduce energy costs [[21](#bib.bib21), [22](#bib.bib22), [22](#bib.bib22),
    [23](#bib.bib23)]. Minimizing carbon emissions is also essential for environmental
    considerations [[24](#bib.bib24), [25](#bib.bib25)]. Motivated by these practical
    challenges, a substantial portion of recent literature focuses on achieving a
    balance between effectiveness and computational efficiency. Ideally, deep learning
    models should yield accurate predictions while minimizing the computational cost
    during inference. This topic has given rise to numerous intriguing research questions
    and garnered significant attention from both academic and industrial sectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In light of these developments, this survey presents a comprehensive and systematic
    review of the exploration towards computationally efficient deep learning. Our
    aim is to provide an overview of this rapidly evolving field, summarize recent
    advances, and identify important challenges and potential directions for future
    research. Specifically, we will discuss existing works from the perspective of
    the following five directions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/879c442a36a83996a3a2d22b78821fd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of the survey. We first review the design of backbone
    networks, which are divided into static and dynamic models. Then we discuss the
    design of task-specialized algorithms and network architectures. Finally, we summarize
    the model compression approaches and the efficient hardware deployment techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Efficient Backbone Models. Designing light-weighted backbone networks that
    effectively extract discriminative deep representations from images, videos or
    3D scenes with minimal computation by optimizing both efficient network micro-architectures
    (*e.g.*, operators, modules, and layers) [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] and improving the system-level organization of micro-architectures
    [[29](#bib.bib29), [30](#bib.bib30)]. Recent advances in neural architecture search
    (NAS) [[31](#bib.bib31), [32](#bib.bib32)] have further enabled the automatic
    design of backbones.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Dynamic Deep Networks. Developing dynamic networks is an important emerging
    research direction for improving computational efficiency. These networks break
    the limits of static computational graphs and propose adapting their structures
    or parameters to the input during inference [[33](#bib.bib33)]. For example, the
    model can selectively activate certain model components (*e.g.*, layers [[34](#bib.bib34)],
    channels [[35](#bib.bib35)], and sub-networks [[36](#bib.bib36)]) based on each
    test input or allocate less computation to less informative spatial/temporal regions
    [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)] of each input.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Task-specialized Efficient Models. Numerous works focus on building task-specific
    heads on top of the features from light-weighted static/dynamic backbones to efficiently
    accomplish specific computer vision tasks. Examples include fast one-stage models
    for real-time object detection [[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43)], the efficient multi-branch architecture for semantic segmentation
    [[44](#bib.bib44)], and end-to-end instance segmentation frameworks [[45](#bib.bib45),
    [46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: 4) Model Compression Techniques. Orthogonal to network architecture design,
    many algorithms have been proposed to compress relatively large models with minimal
    accuracy loss. This can be achieved by pruning less important network components
    [[47](#bib.bib47), [48](#bib.bib48)], quantizing parameters [[49](#bib.bib49),
    [50](#bib.bib50)], or distilling knowledge from large models to smaller models
    of interest [[51](#bib.bib51), [52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: 5) Efficient Deployment on Hardware. To achieve high practical efficiency, it
    is necessary to consider hardware requirements when developing deep learning applications.
    Reducing latency on specific hardware devices is usually treated as an objective
    in network design [[53](#bib.bib53), [54](#bib.bib54)] or algorithm-hardware co-design
    [[55](#bib.bib55), [56](#bib.bib56)]. Additionally, several acceleration tools
    have been developed for efficient deployment of deep learning models [[57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59)].
  prefs: []
  type: TYPE_NORMAL
- en: 'While some relevant surveys exist [[14](#bib.bib14), [60](#bib.bib60)], our
    survey is more up-to-date and comprehensive in several crucial aspects: 1) we
    systematically review model design techniques for images, videos, and 3D vision;
    2) we summarize the recent works on designing dynamic deep neural networks for
    efficient inference; and 3) we thoroughly discuss the specialized models for accomplishing
    the most common and challenging computer vision tasks, *e.g.*, object detection
    and image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The “*split-transform-merge*” architecture in representative computationally
    efficient deep networks. These blocks are typically adopted as basic components
    to build models. Here “Conv” refers to a convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ResNet-50 | ResNeXt | Res2Net | MobileNet V2/V3 | EfficientNet | ShuffleNet
    | ShuffleNet V2 | Vision Transformer |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[4](#bib.bib4)] | [[61](#bib.bib61)] | [[62](#bib.bib62)] | [[27](#bib.bib27),
    [28](#bib.bib28)] | [[29](#bib.bib29)] | [[63](#bib.bib63)] | [[64](#bib.bib64)]
    | [[6](#bib.bib6), [65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '| *Split* function | 1x1 Conv | 1x1 Conv | 1x1 Conv | 1x1 Conv | 1x1 Conv |
    1x1 Group Conv + Channel Shuffle | Channel Split + 1x1 Conv | Linear Projection
    (*i.e.*, 1x1 Conv) |'
  prefs: []
  type: TYPE_TB
- en: '| *Transform* function | 3x3 Conv | 3x3 Group Conv | Cascade 3x3 Conv | 3x3
    or 5x5 Depth-wise Conv | 3x3 or 5x5 Depth-wise Conv | 3x3 Depth-wise Conv | Identity
    and 3x3 Depth-wise Conv | Multi-head Self-attention |'
  prefs: []
  type: TYPE_TB
- en: '| *Merge* function | 1x1 Conv | Concat + 1x1 Conv | Concat + 1x1 Conv | Concat
    + 1x1 Conv | Concat + 1x1 Conv | 1x1 Group Conv | Concat [-0.2ex] + 1x1 Conv +
    Concat [-0.2ex] + Channel Shuffle | Concat + Linear Projection |'
  prefs: []
  type: TYPE_TB
- en: 'The rest of this survey is organized as follows (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Computation-efficient Deep Learning for Computer Vision:
    A Survey") for the overview). In Sec. [2](#S2 "2 Architecture Design of Backbone
    Networks ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")
    and [3](#S3 "3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey"), we introduce the design of efficient static and
    dynamic backbone networks, respectively. In Sec. [4](#S4 "4 Efficient Models for
    Downstream Computer Vision Tasks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey"), the methodology for designing task-specialized efficient models
    is reviewed. The techniques for compressing deep learning models are investigated
    in Sec. [5](#S5 "5 Model Compression Techniques ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey"). Efficient hardware deployment approaches are
    summarized in Sec. [6](#S6 "6 Efficient Deployment on Hardware ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey"). Lastly, we discuss existing challenges
    and future directions in Sec. [7](#S7 "7 Challenges and Future Directions ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Architecture Design of Backbone Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, deep learning models for computation vision tasks incorporate two
    components, *i.e.,* 1) a *backbone network* that extracts deep representations
    from the raw inputs (*e.g.*, images, video frames, and point clouds); and 2) a
    *task-specific head* that is designed specialized for the task of interest. The
    deep features obtained from backbone networks are fed into the head to accomplish
    the corresponding task. The outputs of backbones (*i.e.*, the inputs of the head)
    are usually assumed to have similar formats, while the outputs of the head are
    tailored for the tasks of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we focus on how to design a computational-efficient general-purpose
    backbone network. Our discussions will start from processing the most fundamental
    data form, 2D images, where a light-weighted network may be obtained by either
    *manual design* (Sec. [2.1](#S2.SS1 "2.1 Efficient Models by Manual Design ‣ 2
    Architecture Design of Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey")) or *automatic searching approaches* (Sec. [2.2](#S2.SS2
    "2.2 Automatic Architecture Design ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")). Then we
    will discuss the backbones for processing *videos* (Sec. [2.3](#S2.SS3 "2.3 Efficient
    Backbones for Video Understanding ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")) and understanding
    *3D scenes* (Sec. [2.4](#S2.SS4 "2.4 Efficient Backbones for 3D Vision ‣ 2 Architecture
    Design of Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Efficient Models by Manual Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A considerable number of efficient backbone networks are designed manually
    based on theoretical derivations, empirical observations, or heuristics. Existing
    works can be categorized into two levels according the granularity of modifying
    the network: *micro-architecture* (Sec. 2.1.1) and *macro-architecture* (Sec.
    2.1.2)'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Micro-architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The micro-architecture refers to the individual layers, modules, and neural
    operators of backbones. These basic components are the foundation for constructing
    deep networks. Many works seek to attain higher computational efficiency by improving
    them. Notably, these works usually serve as off-the-shelf plug-in components that
    can be employed together with other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Split-transform-merge Strategy. Typically, deep networks consist of multiple
    successively stacked layers with dense connection, where all the input neurons
    is connected to every output neuron. Formally, $\ell$-th layer $f^{\ell}$ with
    inputs $\mathbf{x}^{\ell-1}$ and outputs $\mathbf{x}^{\ell}$ can be expressed
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}^{\ell}=f^{\ell}(\mathbf{x}^{\ell-1}).$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'However, such dense layers tend to computationally intensive. To address this
    issue, researchers have proposed to replace the dense connection with particularly
    designed topologies [[3](#bib.bib3), [1](#bib.bib1), [61](#bib.bib61)], which
    dramatically reduces the computational complexity, yet yields a competitive or
    stronger representation ability. Among existing works, one of the most popular
    designs is the *split-transform-merge* strategy, as shown in the following (as
    a fundamental component, a residual connection [[4](#bib.bib4)] is added here):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}=f^{\ell}_{\textnormal{split}}(\mathbf{x}^{\ell-1}),\\
    \mathbf{x}^{\ell}&amp;=\mathbf{x}^{\ell-1}+f^{\ell}_{\textnormal{merge}}(f^{\ell}_{1}(\mathbf{x}^{\ell-1}_{1}),\ldots,f^{\ell}_{C}(\mathbf{x}^{\ell-1}_{C})),\end{split}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{x}^{\ell-1}$ is *split* into $C$ embeddings $\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}$
    with lower dimensions by a cheap operator $f^{\ell}_{\textnormal{split}}(\cdot)$.
    The low-dimensional embeddings are processed by the *transform* functions $f^{\ell}_{1}(\cdot),\ldots,f^{\ell}_{C}(\cdot)$,
    whose input/output dimensions are the same. Notably, each of these functions corresponds
    to a dense layer, but this procedure is efficient due to the reduced input feature
    dimension. The processed embeddings are *merged* by $f^{\ell}_{\textnormal{merge}}(\cdot)$.
    In the following, we will first discuss the design of the transform and split/merge
    functions respectively, and then introduce recent improvements over the “split-transform-merge”
    paradigm. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Computation-efficient Deep
    Learning for Computer Vision: A Survey") summarizes some representative *split-transform-merge*
    architectures in popular computationally efficient deep networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*a) “Transform” - homogeneous multi-branch architecture.* A straightforward
    choice is to let $f^{\ell}_{c}(\cdot),c\!=\!1,\dots,C$ have the same architecture,
    and differentiate each other only in the values of the learnable parameters. This
    design is named as *grouped convolution* [[1](#bib.bib1), [61](#bib.bib61)] when
    $f^{\ell}_{c}(\cdot)$ corresponds to a convolutional layer, and is adopted in
    many efficient ConvNets [[61](#bib.bib61), [66](#bib.bib66), [67](#bib.bib67)].
    IGCV [[68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)] further introduces
    a permutation operation to facilitate the interaction of different groups of embeddings
    (*i.e.*, $\mathbf{x}^{\ell-1}_{c},c\!=\!1,\dots,C$). In addition to convolution,
    this design is also widely adopted in the self-attention layers of vision Transformers
    (ViTs) [[6](#bib.bib6), [65](#bib.bib65)]. In ViTs, it is name as *multi-head
    self-attention*, where $f^{\ell}_{c}(\cdot)$ is the scaled dot-product attention
    [[71](#bib.bib71)].'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the *grouped convolution* is named as *depth-wise separable convolution*
    when $C$ is equal to the channel number of $\mathbf{x}^{\ell-1}$. Here $f^{\ell}_{c}(\cdot)$
    typically corresponds to a single convolution operation, where a large kernel
    size with sufficient receptive fields can be used without dramatically increasing
    the computational cost. This highly efficient component is first proposed in MobileNet
    [[26](#bib.bib26)], and adopted adopted in a wide variety of follow-up models
    [[72](#bib.bib72), [27](#bib.bib27), [28](#bib.bib28), [73](#bib.bib73), [74](#bib.bib74),
    [63](#bib.bib63), [64](#bib.bib64), [29](#bib.bib29), [75](#bib.bib75), [76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: '*b) “Transform” - heterogenous multi-branch architecture.* Another line of
    works focus on developing nonequivalent branches, where each $f^{\ell}_{c}(\cdot),c\!=\!1,\dots,C$
    is assigned with a specialized architecture or task. For example, the Inception
    architectures [[3](#bib.bib3), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)] adopt a varying receptive fields for different branches (*e.g.*,
    by changing the convolution kernel size), aiming to aggregate the discriminative
    information at multiple levels. Recent works further extend this idea by feeding
    the outputs of $f^{\ell}_{c}(\cdot)$ to $f^{\ell}_{c+1}(\cdot)$ [[62](#bib.bib62),
    [73](#bib.bib73), [81](#bib.bib81)], and thus integrating multi-scale features
    into the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*c) “Split/merge” functions* $f^{\ell}_{\textnormal{split}}(\cdot)$ and $f^{\ell}_{\textnormal{merge}}(\cdot)$
    are designed to map the features into or back from low-dimension embeddings with
    minimal cost. Most works adopt similar architectures for these two components:
    $f^{\ell}_{\textnormal{split}}(\cdot)$ corresponds to $1\!\times\!1$ convolution,
    while $f^{\ell}_{\textnormal{merge}}(\cdot)$ is accomplished by concatenation
    or concatenation + $1\!\times\!1$ convolution. Representative examples include
    ResNeXt [[61](#bib.bib61)], MobileNets [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]
    and Inception networks [[3](#bib.bib3), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)]. In particular, ShuffleNet [[63](#bib.bib63)] presents a more
    efficient design by combining $1\!\times\!1$ grouped convolution with channel
    shuffle.'
  prefs: []
  type: TYPE_NORMAL
- en: '*d) Improved paradigms over “split-transform-merge”.* More recently, some works
    start to rethink the limitations of the split-transform-merge paradigm, and find
    that more efficient deep networks can be obtained by breaking this design principle.
    For example, motivated by the success of ViTs, ConvNeXt [[74](#bib.bib74)] explicitly
    introduces an multilayer perceptron (MLP) at each layer by reverse $f^{\ell}_{\textnormal{split}}(\cdot)$
    and the depth-wise separable convolution (*i.e.*, $f^{\ell}_{c}(\cdot)$). EfficientNetV2
    [[82](#bib.bib82)] replace $f^{\ell}_{c}(\cdot)$ and $f^{\ell}_{\textnormal{split}}(\cdot)$
    with a regular dense convolutional layer at earlier layers, achieving higher practical
    efficiency on GPU devices. MobileNeXt [[83](#bib.bib83)] moves the depth-wise
    convolution layers to the two ends of the residual path to encode more expressive
    spatial information.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Inverted Bottleneck. Bottleneck [[4](#bib.bib4)] is a widely-used efficient
    component in ConvNets. Its basic architecture can be understood on top of Eq.
    (LABEL:eq:split): the total channel number of $\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}$
    will be reduced compared to $\mathbf{x}^{\ell-1}$ (*e.g.*, by $4\times$ in ResNet
    [[4](#bib.bib4)]). Consequently, the computationally intensive operations $f^{\ell}_{1}(\cdot),\ldots,f^{\ell}_{C}(\cdot)$
    are performed on the low-dimensional embeddings, and the overall cost is saved.
    The effectiveness of this bottleneck is validated in both dense layers ($C\!=\!1$)
    [[4](#bib.bib4)] and grouped convolution [[61](#bib.bib61)]. However, it may be
    sub-optimal in depth-wise separable convolution, where its low-dimensional transform
    results in information loss [[27](#bib.bib27)]. Inspired by this observation,
    MobileNetV2 [[27](#bib.bib27)] achieves an improved efficiency by proposing an
    inverted bottleneck, *i.e.*, $\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}$
    have more dimensions than $\mathbf{x}^{\ell-1}$ (*e.g.*, $6\times$ [[27](#bib.bib27)]).
    This designed is further adopted by a number of recent works [[70](#bib.bib70),
    [83](#bib.bib83), [74](#bib.bib74)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3) Feature Reusing. Conventionally, the successive linear connection is the
    dominant topology for network design. The inputs are fed into a layer and transformed
    to obtained the inputs of the next layer. Any feature will be utilized for only
    a single time. Although being straightforward, this design is usually sub-optimal
    from the lens of computational efficiency. An important idea for lighted-weighted
    models is to *reuse* the have-been-used features.
  prefs: []
  type: TYPE_NORMAL
- en: '*a) Inter-layer feature reusing.* A basic idea is to reuse the features from
    previous layers. The skip-layer residual connection [[84](#bib.bib84), [4](#bib.bib4)]
    adds the inputs of each layer to the outputs, contributing the effective training
    of very deep and computationally more efficient networks. A more general formulation
    is established by dense connection [[85](#bib.bib85), [5](#bib.bib5)], where all
    the previous features are fed into a next layer. CondenseNets [[86](#bib.bib86),
    [87](#bib.bib87)] extend this architecture by automatically learning the inter-layer
    connection topology. In contrast, other works like ShuffleNetV2 [[64](#bib.bib64)]
    and G-GhostNet [[88](#bib.bib88)] focus on manually designing inter-layer interaction
    mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: '*b) Intra-layer feature reusing.* The idea of feature reusing can also be leveraged
    within each network layers. For example, GhostNets [[89](#bib.bib89), [90](#bib.bib90)]
    demonstrate that there exist considerable redundancy in the outputs of each layer.
    They first obtain a small set of intrinsic output features, which are not only
    used as the inputs of the next layer, but also reused to generating other output
    features using cheap operations like linear transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4) Feature Down-sampling. Extracting deep representations from image-based data
    typically yields feature maps, which inherently have spatial sizes (*i.e.*, height
    and weight). This property can be leveraged to reduce the computational cost of
    models *e.g.*, introducing properly configured feature down-sampling modules.
  prefs: []
  type: TYPE_NORMAL
- en: '*a) Processing feature maps efficiently.* The cost of processing feature maps
    grows quadratically with respect to their height/weight. OctConv [[91](#bib.bib91)]
    finds that processing all the features with the same resolution is not an optimal
    design. They propose to process a group of features at a down-sampled scale to
    capture only the low-frequency information, while the remaining features are designed
    to recognize high-frequency patterns, and the two groups exchange information
    after each layer. Consequently, the overall computational cost is reduced. This
    idea is also effective in ViTs [[92](#bib.bib92)]. Similarly, HRNets [[93](#bib.bib93),
    [94](#bib.bib94)] and HRFormer [[95](#bib.bib95)] maintain multi-resolution features
    at each layer, aiming to efficiently extract multi-scale discriminative representations
    for various computer vision tasks in the meantime.'
  prefs: []
  type: TYPE_NORMAL
- en: '*b) Facilitating efficient self-attention.* Particularly, feature down-sampling
    can be embedded into self-attention operations in ViTs to improve its efficiency.
    For example, PVTs [[96](#bib.bib96), [97](#bib.bib97)] and ShuntedViT [[98](#bib.bib98)]
    propose to compute attention maps efficiently with down-sampled feature maps.
    Twins [[99](#bib.bib99)] perform self-attention on low-resolution features to
    aggregate global information efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 5) Efficient Self-attention. ViTs [[6](#bib.bib6)] have achieved remarkable
    success in the fields of computer vision. Their self-attention mechanisms enable
    adaptively aggregating information across the entire image, yielding excellent
    scalability with the growing dataset scale or model size. However, vanilla self-attention
    suffers from high computational cost. A considerable number of recent visual backbones
    focus on developing more efficient self-attention modules without sacrificing
    their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*a) Locality-inspired Self-attention.* In this direction, an important idea
    is drawn from the success of ConvNets: exploiting the locality of images, *i.e.*,
    encouraging the models to aggregate more information from adjacent spatial regions.
    Swin Transformers [[7](#bib.bib7), [100](#bib.bib100)] achieve this by performing
    self-attention only within a square windows. Some other works extending this idea
    by designing different shapes of attention windows [[101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]
    or introducing soft local constraints to attention maps [[107](#bib.bib107), [108](#bib.bib108)].
    An important challenge faced by these works is how to model the interaction of
    different windows effectively. Possible solutions to address this issue include
    changing window positions [[7](#bib.bib7), [100](#bib.bib100)], shuffling the
    channels [[109](#bib.bib109)], designing specialized window shapes [[101](#bib.bib101),
    [103](#bib.bib103), [105](#bib.bib105), [106](#bib.bib106)], or further introducing
    window-level global self-attention modules [[110](#bib.bib110), [111](#bib.bib111),
    [99](#bib.bib99)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*b) SoftMax-free Self-attention.* To reduce the inherent high computation complexity
    of self-attention, another line of research proposes to replace the SoftMax function
    in self-attention with separate kernel functions, yielding linear attention [[112](#bib.bib112)].
    As representative examples, Performer [[113](#bib.bib113)] approximates SoftMax
    with orthogonal random features, while Nyströmformer [[114](#bib.bib114)] and
    SOFT [[115](#bib.bib115)] attain this goal through matrix decomposition. Castling-ViT
    [[116](#bib.bib116)] measures the spectral similarity between tokens with linear
    angular kernels. EfficientViT [[117](#bib.bib117)] further leverages depth-wise
    convolution to improve the local feature extraction ability of linear attention.
    FLatten Transformer proposes a focused linear attention module to achieve high
    expressiveness. [[118](#bib.bib118)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Macro-architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The macro-architecture refers to the system-level methodology of organizing
    micro-architectures (*e.g.*, operators, modules and layers) and constructing the
    whole deep networks. Existing literature has revealed that, even with the same
    efficient micro-architectures, the approaches and configurations for combining
    them will significantly affect the computational efficiency of the resulting models.
    In the following, we will discuss the works and design principles relevant to
    this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Marrying Convolution and Attention Modules. Convolution and self-attention
    are both important modules with their own strengths. A considerable amount of
    literature has been published to study how to combine them for a higher overall
    computational efficiency. At the per-layer level, convolution can be leveraged
    to generate the inputs of self-attention, *e.g.*, queries/keys/values [[75](#bib.bib75),
    [76](#bib.bib76)] or position embeddings [[119](#bib.bib119)]. In addition, some
    works simultaneously utilize self-attention and a convolutional layer, and fuse
    their outputs [[120](#bib.bib120), [121](#bib.bib121)], which facilitates the
    learning of local features. Another promising idea is to integrate convolution
    into the feed-forward network after the self-attention module [[122](#bib.bib122),
    [123](#bib.bib123), [76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: At the network level, many existing works focus on the placing order of self-attention
    and depth-wise convolution blocks. In particular, leveraging convolution at earlier
    layers is proven beneficial [[124](#bib.bib124), [125](#bib.bib125), [126](#bib.bib126),
    [127](#bib.bib127), [128](#bib.bib128)], which enables the efficient extraction
    of local representations. Besides, convolutional blocks are usually adopted as
    light-weighted down-sample layers [[127](#bib.bib127), [129](#bib.bib129), [103](#bib.bib103)].
    Another line of works parallelizes both a self-attention path and a convolution
    path in a single model [[130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135)], where the two paths
    typically interact in a layer-wise fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '2) Depth-width Relationship. In the context of ConvNets and hierarchical ViTs,
    the backbone models consist of multiple stages with progressively reduced feature
    resolution. The layers within each stage usually have the same width, while later
    stages are wider. The stage-wise width growing rule is an important configuration,
    where it is popular to adopt an exponential growth with base two [[4](#bib.bib4),
    [5](#bib.bib5), [7](#bib.bib7)]. In contrast, RegNets [[136](#bib.bib136), [30](#bib.bib30)]
    further propose a more detailed principle: widths and depths of good networks
    can be explained by a quantized linear function.'
  prefs: []
  type: TYPE_NORMAL
- en: 3) Model Scaling. On top of designing a single efficient model, it is also important
    to obtain a family of models that can adapt to varying computational budgets.
    An important principle for addressing this issue is *compound scaling* [[29](#bib.bib29),
    [82](#bib.bib82)], which indicates that simultaneously increasing the depth, width
    and input resolution of a given base model will yield a family of efficient network
    architectures. Dollár *et al.* [[137](#bib.bib137)] further study how to design
    a proper model scaling rule in terms of the actual runtime. In addition, TinyNets
    [[138](#bib.bib138)] extend this idea to the shrinking of the model size.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Automatic Architecture Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to manually designing backbones, another appealing idea is to find
    proper network architectures automatically, which is usually referred to as *neural
    architecture search (NAS)*. In recent years, a number of existing works have investigated
    this idea through the lens of computational efficiency. In the following, we will
    discuss the basic computation-aware formulation of NAS (Sec. 2.2.1) and how the
    practical speed is considered in NAS (Sec. 2.2.2).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Computation-aware NAS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Typically, NAS consists of two components: a searching space that contain a
    number of candidate architectures, and an algorithm to search for an optimal architecture.
    The computational cost for inferring the model is usually treated as a constraint,
    which is either inherently controlled by the searching space or strictly restricted
    by a pre-defined rule. The optimization objective is to maximize the validation
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Early Works. Early NAS methods propose to formulate a discrete searching
    space [[31](#bib.bib31), [139](#bib.bib139), [140](#bib.bib140)]. The network
    is viewed as a graph with a number of nodes connected by edges, where each edge
    corresponds to an operation and one needs to find the optimal operation for each
    edge. Such a problem can be solved with discrete optimization algorithms. For
    example, by viewing the validation performance as the rewards, one can leveraged
    off-the-shelf reinforcement learning methods [[31](#bib.bib31), [139](#bib.bib139),
    [140](#bib.bib140)]. Moreover, evolutionary algorithms also achieve favorable
    performance for discrete NAS [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)].
  prefs: []
  type: TYPE_NORMAL
- en: 2) Efficient Searching Algorithms. The aforementioned NAS methods are able to
    find computationally more efficient network architectures than human design. However,
    their searching cost is a notable limitation, since their search procedure usually
    incorporates training many candidate networks from scratch to convergence to evaluate
    their validation accuracy. Motivated by this issue, a large number of works focus
    on developing low cost NAS algorithms. A basic idea in this direction is to reuse
    the previous candidates, *e.g.*, adding/deleting layers [[117](#bib.bib117), [144](#bib.bib144)]
    and paths [[145](#bib.bib145)] on top of currently found architectures or adopting
    existing architectures as network components [[146](#bib.bib146)].
  prefs: []
  type: TYPE_NORMAL
- en: Driven by these preliminary explorations, ENAS [[147](#bib.bib147)] and DARTS
    [[32](#bib.bib32)] propose a parameter-sharing paradigm. They propose to construct
    a large computational graph that contains all possible connections and operations,
    such that each subgraph within it corresponds to a network architecture. The large
    graph is named as a *super-net*, while all possible candidate networks share the
    same super-net parameters. Hence, one can train the super-net, and directly sample
    architectures from it without retraining any specific candidate network. The network
    selection process is usually formulated to be differentiable and accomplished
    efficiently via gradient-based optimization methods [[148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)]. Besides, some recent
    works focus on improving this procedure by introducing progressive searching mechanisms
    [[153](#bib.bib153), [154](#bib.bib154)], introducing hyper-networks [[155](#bib.bib155),
    [156](#bib.bib156)] or training more proper super-nets for NAS [[157](#bib.bib157),
    [158](#bib.bib158)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Latency-aware Neural Architecture Search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the lens of practical efficiency, an important challenge faced by NAS is
    the inference speed on real hardware (*e.g.*, GPUs or CPUs). Since NAS usually
    leads to irregular network architectures, the obtained model with low theoretical
    computational cost may not be efficient in practice. To address this issue, recent
    NAS methods explicitly incorporate real latency into the optimization objective
    to achieve a good trade-off between real speed and accuracy [[54](#bib.bib54),
    [53](#bib.bib53), [159](#bib.bib159)]. As representative examples, MobileNetV3
    [[28](#bib.bib28)] leverages hardware-aware NAS to obtain the basic architecture,
    and modifies it manually. Once-for-all [[24](#bib.bib24)] proposes to train a
    shared general super-nets, and perform NAS on top of it conditioned on the specific
    hardwares, yielding a state-of-the-art efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Efficient Backbones for Video Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will focus on the efficient backbones for processing
    videos. Notably, videos consist of a series of frames, each of which is an image.
    In general, the aforementioned techniques for processing images are typically
    compatible with videos. Hence, here we mainly review the efficient modeling of
    the temporal relationships of video frames, including *ConvNet-based* (Sec. 2.3.1)
    and *Transformer-based* (Sec. 2.3.2) approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Efficient 3D ConvNets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most straightforward approach to modeling temporal relationships may be
    introducing 3D convolutional layers [[160](#bib.bib160), [161](#bib.bib161)],
    such that one can directly perform convolution in the space formed by frame height,
    width, and video duration. However, 3D convolution is computationally expensive,
    and many efficient backbones have been proposed to alleviate this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Marrying 2D and 3D Convolution. A basic idea is to avoid designing a pure
    3D ConvNets, *i.e.*, most of the feature extraction process may be accomplished
    by the efficient 2D convolution, while 3D convolution is only introduced at several
    particular positions. From the lens of macro-architecture, this goal can be attained
    by sequentially mixing 2D and 3D blocks, either first using 3D and later 2D or
    first 2D and later 3D [[162](#bib.bib162), [163](#bib.bib163)]. At the micro-architecture
    level, the group-wise or depth-width 3D convolution can be integrated in to the
    *transform* module of 2D split-transform-merge architecture (Eq. (LABEL:eq:split))
    [[164](#bib.bib164), [165](#bib.bib165)].
  prefs: []
  type: TYPE_NORMAL
- en: '2) (2+1)D Networks. Another elegant idea is to decompose 3D convolution into
    two components: a 2D convolution that extract representation from video frames,
    and a temporal operation that only focuses on learning the temporal relationships.
    The former can directly adopt 2D neural operators, while the latter can be implemented
    using 1D temporal convolution [[166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)],
    adaptive 1D convolution [[169](#bib.bib169)], and MLPs [[170](#bib.bib170)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3) 2D Networks. In addition to the aforementioned approaches, the models with
    only 2D convolution may also be able to model temporal relationships. This is
    typically achieved by designing zero-parameter operations. For example, subtracting
    the features of adjacent frames to extract the motion information [[171](#bib.bib171),
    [172](#bib.bib172)]. The temporal-shift-based models [[173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175)] propose to shift part of the channels of 2D features along
    the temporal dimension, performing information exchange among neighboring frames
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 4) Long/Short-term Separable Networks. Another important idea is modeling long/short-term
    temporal dynamics with separate network architectures. An representative work
    in this direction is SlowFast [[176](#bib.bib176)], which incorporate a lower
    temporal resolution slow pathway and a higher temporal resolution fast pathway.
    Many recent works [[172](#bib.bib172), [177](#bib.bib177)] further extend this
    idea.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Transformer-based Video Backbones
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Driven by the success of ViTs [[6](#bib.bib6)], a considerable number of recent
    works focus on facilitating efficient video understanding with self-attention-based
    models. In general, most of these works extend the aforementioned design ideas
    (including both image-based and video-based backbones) in the context of ViTs,
    *e.g.*, performing spatial-temporal local self-attention [[178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180)], combining self-attention and convolution
    [[181](#bib.bib181)], and performing 1D temporal attention in (2+1)D designs [[182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Efficient Backbones for 3D Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The perception and understanding of 3D scenes is not only a key ability of human
    intelligence, but also an important task for computer vision which are ubiquitous
    in real-world applications. In this subsection, we will review the backbones designed
    for processing 3D information efficiently. In general, the works in this direction
    can be categorized by the forms of model inputs, *i.e.*, *3D point clouds* (Sec.
    2.4.1), *3D voxels* (Sec. 2.4.2) and *multi-view images* (Sec. 2.4.3).
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Point-based Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A fundamental type of 3D geometric data structure is the cloud of 3D points,
    where each point is represented by its three coordinates. PointNet [[186](#bib.bib186)]
    is the pioneering work that leveraging deep learning to process 3D point clouds.
    It adopts point-wise feature extraction with shared MLPs to maintain the permutation
    invariance. PointNet++ [[187](#bib.bib187)] improves PointNet by facilitating
    capturing local geometric structures. On top of them, a number of works focus
    on how to aggregating local information effectively without increasing computational
    cost significantly. Representative approaches include introducing graph neural
    networks [[188](#bib.bib188), [189](#bib.bib189)], projecting 3D points to regular
    grids to perform convolution [[190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193)], aggregating the features of adjacent points using the weights
    determined by the local geometric structure [[194](#bib.bib194), [195](#bib.bib195),
    [196](#bib.bib196)], and self-attention [[197](#bib.bib197), [198](#bib.bib198)].
    In particular, recent works have revealed that point-based models can achieve
    state-of-the-art computational efficiency with proper training and model scaling
    techniques [[199](#bib.bib199)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Voxel-based Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 3D point clouds can be further transformed to voxels, which are regular
    and can be directly processed with 3D convolution [[200](#bib.bib200)]. Typically,
    the 3D space is divided into cubic voxel grids, while the features of the points
    in each grid will be averaged. The side length of the grid is named as the voxel
    resolution. An important technique for processing voxels efficiently is sparse
    convolution [[201](#bib.bib201), [202](#bib.bib202), [203](#bib.bib203)], *i.e.*,
    only performing convolution on the voxels with 3D points in them. Many works design
    backbone networks with this mechanism conditioned on the vision task of interest
    [[204](#bib.bib204), [205](#bib.bib205), [206](#bib.bib206)] for an optimal efficiency-accuracy
    trade-off. In addition, the point-based and voxel-based models can be combined
    to reduce the memory and computational cost [[207](#bib.bib207)]. Some recent
    works have explored the automatic backbone design using NAS [[208](#bib.bib208)]
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3 Multi-view-based Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-view projective analysis is another effective solution for understanding
    3D shapes, where the 3D objects are projected into 2D images from varying visual
    angles and processed by 2D backbone networks [[209](#bib.bib209)]. This idea can
    be implemented for recognition [[210](#bib.bib210), [211](#bib.bib211)], retrieval
    [[212](#bib.bib212), [213](#bib.bib213)] and pose estimation [[214](#bib.bib214)].
    An important challenge for these methods is how to fuse the multi-view features.
    Existing works have proposed to leverage LSTM [[213](#bib.bib213)] or graph convolutional
    network [[210](#bib.bib210)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Dynamic Backbone Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although the advanced architectures introduced in Sec. [2](#S2 "2 Architecture
    Design of Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey") have achieved significant progress in improving the inference
    efficiency of deep models, they generally have an intrinsic limitation: the computational
    graphs are kept the same during inference when processing different inputs with
    varying complexity. Such a *static* inference paradigm inevitably brings redundant
    computation on some “easy” samples. To address this issue, dynamic neural networks
    [[33](#bib.bib33)] have attracted great research interest in recent years due
    to their favorable efficiency, representation power, and adaptiveness [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers have proposed various types of dynamic networks which can adapt
    their architectures/parameters to different inputs. Based on the granularity of
    adaptive inference, we categorize related works into *sample-wise* (Sec. [3.1](#S3.SS1
    "3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), *spatial-wise* (Sec. [3.2](#S3.SS2
    "3.2 Spatial-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), and *temporal-wise* (Sec. [3.3](#S3.SS3
    "3.3 Temporal-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")) dynamic networks. Compared to the
    previous work [[33](#bib.bib33)] which contains both vision and language models,
    we mainly focus on the computational efficient models for vision tasks in this
    survey. Moreover, more up-to-date works are included.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Sample-wise Dynamic Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most common adaptive inference paradigm is processing each input sample
    (*e.g.* an image) dynamically. There are mainly two lines of work in this direction:
    one aims at reducing the computation with decent network performance via dynamic
    *architectures*, and the other adjusts network *parameters* to boost the representation
    power with minor computational overhead. In this survey, we focus on the former
    line which typically reduces redundant computation for improving efficiency. Popular
    approaches include three types: 1) dynamic depth, 2) dynamic width, and 3) dynamic
    routing in a super network (SuperNet).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a42d226b13123857b6481412fa1a44f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Dynamic early exiting. When the prediction at an early exit satisfies
    some criterion (the green tick), the inference procedure terminates, and the later
    computation will be skipped.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Dynamic Depth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The inference procedure of a traditional (static) network can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}=f(\mathbf{x})=f^{L}\circ f^{L-1}\circ\dots\circ f^{1}(\mathbf{x}),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $f^{\ell},\ell=1,2,\dots,L$ is the $\ell$-th layer, and $L$ is the network
    depth. In contrast, networks with dynamic depth process each sample $\mathbf{x}_{i}$
    with an adaptive number of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}_{i}=f(\mathbf{x})=f^{L_{i}}\circ f^{L_{i}-1}\circ\dots\circ
    f^{1}(\mathbf{x}),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $1\leq L_{i}\leq L$ is decided based on $\mathbf{x}_{i}$ itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly two common implementations to realize dynamic depth. The first
    is *early exiting*, which means that the network predictions for some “easy” samples
    can be output at an intermediate layer without activating the deeper layers [[215](#bib.bib215),
    [216](#bib.bib216)] (Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")). Researchers have found that multiple classifiers in a deep
    model may interfere with each other and degrade the performance by forcing early
    layers to capture semantic-level features [[34](#bib.bib34)]. To address this
    issue, multi-scale feature representation is adopted [[34](#bib.bib34), [217](#bib.bib217)]
    to quickly produce coarse-scale features with rich semantic information. Instead
    of constructing intermediate exits in convolutional networks, the recent Dynamic
    Vision Transformer (DVT) [[218](#bib.bib218)] realizes early exiting in cascaded
    vision Transformers which process images with different token numbers. Dynamic
    Perceiver [[219](#bib.bib219)] proposes to integrate intermediate features and
    perform early-exit by introducing an addition attention-based path. Apart from
    architectural design, researchers have also proposed specialized techniques [[220](#bib.bib220),
    [221](#bib.bib221)] for training early-exiting models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8c9975d0efb635e6a5d6ba14e2604679.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Dynamic layer skipping. A gating module is used to decide whether
    to execute the block.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned early-exiting methods dynamically terminate the forward
    propagation at a certain layer. An alternative approach to dynamic depth is *skipping
    intermediate layers* in models with skip connection such as ResNets [[4](#bib.bib4)]
    and vision Transformers [[6](#bib.bib6)] (Figure [3](#S3.F3 "Figure 3 ‣ 3.1.1
    Dynamic Depth ‣ 3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")). Let $\mathbf{x}^{\ell}$
    and $f^{\ell}$ denote the feature and the computational unit at layer-$\ell$,
    a typical implementation of layer skipping is using a gating module $g^{\ell}(\cdot)$
    to dynamically decide whether to execute $f^{\ell}$ [[222](#bib.bib222), [223](#bib.bib223),
    [224](#bib.bib224)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}^{\ell+1}=\mathbf{x}^{\ell}+g^{\ell}(\mathbf{x})\cdot f^{\ell}(\mathbf{x}),g^{\ell}(\mathbf{x})\in{0,1}.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/39e08f3ac1c730a72080b38f4727296c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Dynamic channel skipping, which uses a gating module to decide the
    computation of convolution channels.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Dynamic Width
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Instead of skipping an entire layer, a less aggressive approach is adjusting
    the network *width* to different inputs. In this direction, the most popular implementation
    is dynamically *skipping the channels* in convolutional blocks via a gating module
    [[35](#bib.bib35), [225](#bib.bib225), [226](#bib.bib226), [227](#bib.bib227)]
    (Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Dynamic Depth ‣ 3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")). Specifically, a gating module is first executed before conducting
    a convolution operation. The output of this gating module is a $C$-dimensional
    binary vector that decides whether to compute each channel, where $C$ is the output
    channel number. This implementation is similar to that in the aforementioned layer-skipping
    scheme. The most prominent difference is that the output of the gating module
    in layer skipping is a scalar, and the gating module in channel-skipping is required
    to output a vector controlling the computation of different channels. Apart from
    convolution layers, the same idea can also be applied in vision Transformers to
    dynamically skip channels in multi-layer perceptron (MLP) blocks [[224](#bib.bib224)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Dynamic Routing in SuperNets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of skipping the computation of layers or channels in conventional network
    architectures, one can also realize data-dependent inference via *dynamic routing*
    in super networks (SuperNets). A SuperNet usually contains various inference paths,
    and routing nodes are responsible for allocating each sample to the appropriate
    path. Let $\mathbf{x}_{i}^{\ell}$ denote the $i$-th node in layer-$\ell$, a general
    formulation of the computation for obtaining node-$j$ in the next layer can be
    written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{j}^{\ell+1}=\sum_{i:\alpha_{i\rightarrow j}^{\ell}>0}\alpha_{i\rightarrow
    j}^{\ell}f_{i\rightarrow j}^{\ell}(\mathbf{x}_{i}^{\ell}),$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{i\rightarrow j}^{\ell}$ is the transformation between node $i$ and
    $j$, and $\alpha_{i\rightarrow j}^{\ell}$ is the weight for this path which is
    calculated based on $\mathbf{x}_{i}^{\ell}$. If $\alpha_{i\rightarrow j}^{\ell}=0$,
    the transformation $f_{i\rightarrow j}^{\ell}$ can be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: Extensive works have proposed different forms of SuperNets, such as tree structures
    [[36](#bib.bib36), [228](#bib.bib228), [229](#bib.bib229)], dynamic mixture-of-experts
    [[230](#bib.bib230), [231](#bib.bib231)], and more general architectures [[232](#bib.bib232),
    [233](#bib.bib233)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Spatial-wise Dynamic Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It has been found that different spatial locations in an image contribute unequally
    to the performance of vision tasks [[234](#bib.bib234)]. However, most existing
    deep models process different spatial locations with the same computation, leading
    to redundant computation on less important regions. To this end, spatial-wise
    dynamic networks are proposed to exploit the spatial redundancy in image data
    to achieve an improved efficiency. Based on the granularity of adaptive inference,
    we categorize relative works into pixel level, region level, and resolution level.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Pixel-level Dynamic Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A typical approach to spatial-wise adaptive inference is dynamically deciding
    whether to compute each pixel in a convolution block based on a binary mask [[235](#bib.bib235),
    [236](#bib.bib236), [237](#bib.bib237)]. This form is similar to that in layer
    skipping and channel skipping (Sec. [3.1](#S3.SS1 "3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")), except that the gating module is required to output a spatial
    mask. Each element of this spatial mask determines the computation of a feature
    pixel. In this way, the mask generators learn to locate the most discriminative
    regions in image features, and redundant computation on less informative pixels
    can be skipped.'
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of such pixel-level dynamic computation is that the acceleration
    is currently not supported by most deep learning libraries. The memory access
    cost can be heavier than static convolutions, and the computation parallelism
    is reduced due to sparse convolution. As a result, although the computation can
    be significantly reduced, the practical efficiency of these methods usually lags
    behind their theoretical efficiency. To this end, researchers have also proposed
    “coarse-grained” spatial-wise dynamic networks [[238](#bib.bib238), [39](#bib.bib39)],
    which means that an element of a spatial mask can decide a patch rather than a
    pixel. In this way, more contiguous memory access is realized for realistic speedup.
    Moreover, the scheduling strategies are also proven to have a considerable effect
    on the inference latency [[39](#bib.bib39)]. It is also promising to co-design
    algorithm, scheduling, and hardware devices to better harvest the theoretical
    efficiency of spatial-wise dynamic networks.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from skipping the computation of certain pixels, another line of work
    breaks the static reception field of traditional convolution and proposes deformable
    convolution [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241)]. Specifically,
    a lightweight module is used to learn the offsets for each feature pixel, and
    the convolution neighbors are sampled from arbitrary locations based on the predicted
    offsets. This idea has also been implemented in vision Transformers to enhance
    the performance of the local attention mechanism [[242](#bib.bib242)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Region-level Dynamic Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of flexibly deciding which feature pixels to compute, another line of
    work aims at locating important regions (patches) in input images and cropping
    these patches for recognition tasks. For example, image recognition can be formulated
    as a sequential decision problem, in which an RNN is adopted to make predictions
    based on the cropped image patches [[243](#bib.bib243), [244](#bib.bib244)]. A
    multi-scale CNN with multiple sub-networks could also be used to perform the classification
    task based on cropped salient image patches [[245](#bib.bib245)]. A lightweight
    module is placed between every two sub-networks to decide the coordinate and size
    of the salient patch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along this direction, the recent glance-and-focus network (GFNet) [[246](#bib.bib246),
    [247](#bib.bib247)] proposes a general framework for region-level dynamic inference
    which is compatible with various visual backbones. It first “glances” a low-resolution
    input image, and then repeatedly “focus” on salient regions using reinforcement
    learning (RL) [[248](#bib.bib248)]. Moreover, early exiting (Sec. [3.1](#S3.SS1
    "3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")) is allowed, which means that the
    step number of “focus” can be dynamically adjusted for different input images.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Resolution-level Dynamic Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most existing vision models process different images with the same resolution.
    However, the input complexity could vary, and not all images require a high-resolution
    representation. Ideally, low-resolution representations should be sufficient for
    those “easy” samples with large objects and canonical features. The early work
    [[249](#bib.bib249)] proposes to adaptively zoom input images in the face detection
    task. The recent resolution adaptive network (RANet) [[217](#bib.bib217)] builds
    a multi-scale architecture, in which inputs are first processed with a low resolution
    and a small sub-network. Large sub-networks and high-resolution representations
    are conditionally activated based on early predictions. Instead of using a specialized
    structure, dynamic resolution network [[250](#bib.bib250)] rescales each image
    with the resolution predicted by a small model and feeds the rescaled image to
    common CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Note that different spatial locations are still processed equally in the aforementioned
    methods. We categorize the relative works in this section since they mainly utilize
    the spatial redundancy of image inputs for efficient inference.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Temporal-wise Dynamic Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As video data can be viewed as a sequence of image data, adaptive computation
    could also be performed along the temporal dimension due to the considerable redundancy
    in video recognition tasks. Representative works can generally be divided into
    two lines: one processes video with recurrent models and dynamically save computation
    at certain time steps; the other aims at sampling key frames/clips and allocating
    the computation to these sampled frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Dynamic Recurrent Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different video frames are unequally informative. To this end, extensive studies
    propose to dynamically activate computation when updating the hidden state in
    recurrent models. For example, LiteEval [[251](#bib.bib251)] establishes two different
    sized LSTM [[252](#bib.bib252)]. In each time step, a gating module is used to
    decide which LSTM should be executed for processing the current frame. AdaFuse
    [[253](#bib.bib253)] dynamically skips the computation of some convolution channels,
    and these channels are filled with the hidden state from the previous step. Moreover,
    the numerical precision [[254](#bib.bib254)] and image resolution [[255](#bib.bib255)]
    of different frames can also be dynamically decided.
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned works generally require a ConvNet for encoding each input
    frame before updating the hidden state. A more flexible solution is allowing the
    network to learn “where to see”. In other words, networks can directly jump to
    an arbitrary temporal location in the video [[256](#bib.bib256), [257](#bib.bib257),
    [37](#bib.bib37)] or perform early exiting [[258](#bib.bib258), [259](#bib.bib259),
    [260](#bib.bib260)] instead of “watch” the entire video frame by frame.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Dynamic Key Frame Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative to skipping computation in recurrent networks is sampling key
    frames and then feeding the sampled frames rather than the whole video to a standard
    model. Reinforcement learning is a popular technique for training frame samplers
    [[261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263)].
  prefs: []
  type: TYPE_NORMAL
- en: A recent trend is simultaneously achieving dynamic inference from multiple perspectives.
    For example, AdaFocus and its variants [[38](#bib.bib38), [264](#bib.bib264),
    [265](#bib.bib265), [266](#bib.bib266)] makes use of both spatial and temporal
    redundancy in video data. Dynamic architecture with 3D convolution [[267](#bib.bib267)]
    is also an interesting topic.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Efficient Models for Downstream Computer Vision Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we assume that a light-weighted backbone network has already
    been obtained, and discuss how to design task-specific heads or algorithms on
    top of them. The general aim is to facilitate accomplishing real-world computer
    vision tasks efficiently or even in real time. To this end, we will focus on three
    representative tasks, namely *object detection* (Sec. [4.1](#S4.SS1 "4.1 Object
    Detection ‣ 4 Efficient Models for Downstream Computer Vision Tasks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), *semantic segmentation* (Sec.
    [4.2](#S4.SS2 "4.2 Semantic Segmentation ‣ 4 Efficient Models for Downstream Computer
    Vision Tasks ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")),
    and *instance segmentation* (Sec. [4.3](#S4.SS3 "4.3 Instance Segmentation ‣ 4
    Efficient Models for Downstream Computer Vision Tasks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), all of which have a strong need
    for accurate and real-time applications. Note that most of other more complex
    computer vision tasks (*e.g.* visual object tracking) are mainly based on the
    three tasks we consider.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Object detection aims to answer two fundamental questions in computer vision:
    what visual objects are contained in the images, and where are them [[8](#bib.bib8)]?
    The classification and localization results obtained by object detection usually
    serve as the basis of other vision tasks, *e.g.*, instance segmentation, image
    captioning, and object tracking. The algorithms for object detection can be roughly
    categorized into *two-stage* (Sec. 4.1.1) and *one-stage* (Sec. 4.1.2). In the
    following, we will discuss them respectively from the lens of computational efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Two-stage Detectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Object detection with deep learning starts from the two stage paradigm. The
    pioneer work, RCNN [[268](#bib.bib268), [269](#bib.bib269)], proposes to first
    crop a set of object proposals from the images, and classify them with deep networks.
    On top of it, SPPNet [[270](#bib.bib270)] avoids repeatedly inferring the backbones
    by adaptively pooling the features of the regions of interest. Fast RCNN [[271](#bib.bib271)]
    simultaneously train a detector and a bounding box regressor in the same network,
    leading to more than 200 times of speedup than RCNN. Faster R-CNN [[272](#bib.bib272),
    [273](#bib.bib273)] and its improvements [[274](#bib.bib274), [275](#bib.bib275)]
    introduce a region proposal network that cheaply generates object proposals from
    the features, yielding the first nearly real-time deep learning detector. The
    feature pyramid networks further propose to leverage the feature maps at varying
    scales to detect the object with different sizes respectively, which improves
    the detection accuracy significantly without sacrificing the efficiency [[276](#bib.bib276)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 One-stage Detectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The major motivation behind the two-stage detects is the “coarse-to-fine” refining,
    *i.e.*, first obtaining the coarse proposals, and then refining the localization
    and discrimination results on top of these proposals, such that an excellent detection
    performance can be achieved. Despite the aforementioned techniques proposed to
    improve the efficiency of this procedure, the speed and the complexity of two-stage
    detectors are usually not applicable to real-time applications. In contrast, the
    one-stage detectors directly output the detection results in a single step, yielding
    much faster inference speed with a decent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Bounding-box-based Methods. The first deep-learning-based one-stage detector
    is YOLO [[40](#bib.bib40)]. YOLO divides the image into grid regions and simultaneously
    predicts the bounding boxes and the classification results conditioned on each
    region. The subsequent works of YOLO [[277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279), [280](#bib.bib280), [43](#bib.bib43)] focus on further improving
    the localization performance or classification accuracy without affecting the
    practical speed. The latest version, YOLOv7 [[43](#bib.bib43)], achieves a state-of-the-art
    effectiveness-efficiency trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to YOLO, SSD [[41](#bib.bib41)] improves the accuracy of one-stage
    detectors by detecting the objects at different scales on different layers of
    the network. RetinaNet [[281](#bib.bib281)] proposes a focal loss to encourage
    the model to focus more on the difficult, misclassified examples, which boosting
    the accuracy of one-stage detectors effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Point-based Methods. The aforementioned detection methods mostly learn to
    produce the ground-truth bounding boxes on top of pre-defined anchor boxes. Despite
    the effectiveness, this paradigm suffers from a lot of design hyper-parameters
    and an imbalance between positive/negative boxes during training. To address this
    issue, CornerNet [[282](#bib.bib282)] proposes to directly predict the top-left
    corner and bottom-right corner of candidate boxes. Many subsequent works extend
    this point-based setting. For example, FCOS [[42](#bib.bib42)] predicts the distances
    from each location in feature maps to the four sides of the bounding box. ExtremeNet
    [[283](#bib.bib283)] learns to detect the extreme points the center of bounding
    boxes. CenterNet [[284](#bib.bib284)] further considers each object to be a single
    center point and regresses all the attributes (2D/3D size, orientation, depth,
    locations, etc.) based on this point.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Transformer-based Methods. In recent years, N. Carion *et al.* propose an
    end-to-end Transformer-based detection network, DETR [[285](#bib.bib285)]. DETR
    views detection as a set prediction problem, where the results are obtained based
    on several object queries. Deformable DETR [[286](#bib.bib286)] addresses the
    long convergence issue of DETR by introducing a deformable mechanism to self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Semantic Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aim of semantic segmentation is to predict the semantic label of each pixels
    [[9](#bib.bib9), [287](#bib.bib287)], *e.g.*, if a pixel belongs to a car, a bike,
    etc. Here we summarize existing efficient semantic segmentation methodologies
    based on their paradigms *i.e.*, *encoder-decoder* (Sec. 4.2.1), *multi-branch*
    (Sec. 4.2.2) and others (Sec. 4.2.3).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Encoder-decoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A popular approach is to first extract the low-resolution discriminative representations
    with a multi-stage backbone network, up-sample the deep features to the input
    resolution with a decoder, and then produce the pixel-wise predictions. This procedure
    is named as “encoder-decoder” [[288](#bib.bib288), [289](#bib.bib289)]. To improve
    the efficiency of this paradigm, many works propose to design light-weighted decoders.
    Representative methods include introducing split-transform-merge architectures
    [[290](#bib.bib290), [291](#bib.bib291), [292](#bib.bib292), [73](#bib.bib73),
    [293](#bib.bib293)] (Eq. (LABEL:eq:split)), developing efficient approximations
    of the computationally intensive dilated convolution [[294](#bib.bib294), [295](#bib.bib295),
    [296](#bib.bib296)], and introducing dense connections [[296](#bib.bib296), [297](#bib.bib297)].
    In addition, it is efficient to simultaneously feed the low-level and high-level
    features into the decoder, *i.e.*, comprehensively leveraging both of them improves
    the accuracy without introducing notable computational overhead [[298](#bib.bib298),
    [299](#bib.bib299), [300](#bib.bib300), [301](#bib.bib301), [297](#bib.bib297)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Multi-branch Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another popular efficient paradigm is designing multi-branch architectures.
    Typically, the model consist of two types of paths: 1) context paths with low-resolution
    feature maps and large receptive fields, aiming to extract discriminative information;
    and 2) spatial paths that preserve the low-level spatial information. These paths
    are fused in a parallel [[44](#bib.bib44), [298](#bib.bib298), [302](#bib.bib302),
    [303](#bib.bib303), [304](#bib.bib304)] or cascade [[305](#bib.bib305)] fashion,
    yielding high-resolution but semantically rich deep representations for segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Others
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, some new ideas have been proposed to facilitate efficient semantic
    segmentation. For example, processing deep features with self-attention layers
    [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)], designing segmentation
    models with NAS [[309](#bib.bib309), [310](#bib.bib310), [311](#bib.bib311)],
    adjusting the architecture of the decoder conditioned on the inputs [[233](#bib.bib233)].
    More recently, a considerable number of papers seek to design efficient semantic
    segmentation models on top of ViTs [[312](#bib.bib312), [313](#bib.bib313), [314](#bib.bib314),
    [315](#bib.bib315), [316](#bib.bib316)]. These works mainly focus on achieving
    a state-of-the-art performance with as less computational cost as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Instance Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instance segmentation can be seen as a combination of object detection and semantic
    segmentation, where the model needs to detect the instances of objects, demarcate
    their boundaries and recognize their categories [[9](#bib.bib9), [317](#bib.bib317)].
    Existing works in this direction can be categorized into *two-stage* (Sec. 4.3.1)
    and *End-to-end* (Sec. 4.3.2).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Two-stage Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the lens of efficiency, a notable milestone of deep-learning-based instance
    segmentation is the proposing of Mask R-CNN [[318](#bib.bib318)]. Mask R-CNN is
    developed by introducing mask segmentation branches on the basis of Faster R-CNN
    [[272](#bib.bib272)]. It enjoys high computational efficiency by directly obtaining
    the regions of interest from the feature maps. In contrast, MaskLab [[319](#bib.bib319)]
    improved Faster R-CNN by adding the semantic segmentation and direction prediction
    paths. To improve the accuracy of Mask R-CNN, MS R-CNN [[320](#bib.bib320)] predicts
    the quality of the predicted instance masks and prioritizes more accurate mask
    predictions during validation. PANet [[321](#bib.bib321)] introduces a path augmentation
    mechanism to facilitate the bottom-up information interaction of feature maps.
    HTC [[322](#bib.bib322)] proposes a hybrid task cascade framework to learn more
    discriminative features progressively while integrating complementary features
    in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 End-to-end Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another liner of works focus on realizing efficient end-to-end instance segmentation.
    SOLO [[45](#bib.bib45), [46](#bib.bib46)] achieves this by introducing the “instance
    categories”, which assigns categories to each pixel within an instance according
    to the instance’s location and size, thus converting instance segmentation into
    a pure dense classification problem. YOLACT [[323](#bib.bib323)] and BlendMask
    [[324](#bib.bib324)] propose to first generate a set of prototype masks, and then
    combines them with per-instance mask coefficients or attention scores. Inspired
    by SSD [[41](#bib.bib41)] and RetinaNet [[281](#bib.bib281)], TensorMask [[325](#bib.bib325)]
    build an efficient sliding-window-based instance segmentation framework.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Model Compression Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep networks necessitate substantial resources, including energy, processing
    capacity, and storage. These resource requirements diminish the suitability of
    deep networks for resource-constrained devices [[326](#bib.bib326)]. Furthermore,
    the extensive resource requirements of deep networks become a bottleneck for real-time
    inference and executing deep networks on browser-based applications. To address
    these drawbacks of deep networks, various model compression techniques have been
    proposed in existing literature. Several comprehensive reviews on model compression
    techniques exist [[327](#bib.bib327), [328](#bib.bib328)]. These reviews categorize
    model compression techniques, discuss challenges, provide overviews, solutions,
    and future directions of model compression techniques. We adopt their classification
    structure but place a greater emphasis on vision-related works. Specifically,
    we categorize existing research into network pruning [[329](#bib.bib329), [330](#bib.bib330)],
    network quantization [[331](#bib.bib331), [330](#bib.bib330)], low-rank decomposition
    [[332](#bib.bib332), [333](#bib.bib333)], knowledge distillation [[51](#bib.bib51),
    [334](#bib.bib334), [335](#bib.bib335)], and other techniques [[336](#bib.bib336),
    [337](#bib.bib337)]. For readers interested in a particular category, we recommend
    consulting these more targeted reviews [[329](#bib.bib329), [330](#bib.bib330),
    [331](#bib.bib331), [52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0aaa8618fecc579b6e81920b46a8dee2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The steps of network pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Network Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Network pruning is one of the most prevalent techniques for reducing the size
    of a deep learning model by eliminating inadequate components, such as channels,
    filters, neurons, or layers, resulting in a light-weighted model. Network pruning
    techniques can be categorized into four types: channel pruning, filter pruning,
    connection pruning, and layer pruning. These techniques help decrease the storage
    and computation requirements of deep networks. A typical pruning algorithm consists
    of two stages: evaluating and pruning unimportant parameters, followed by fine-tuning
    the pruned model to restore accuracy. The steps and categories are illustrated
    in Figure [5](#S5.F5 "Figure 5 ‣ 5 Model Compression Techniques ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: In deep networks, the inputs provided to each layer are channeled. Channel pruning
    involves removing unimportant channels to reduce computation and storage requirements.
    Various channel pruning schemes have been proposed [[338](#bib.bib338), [47](#bib.bib47),
    [339](#bib.bib339)]. Convolutional operations in ConvNets incorporate a large
    number of filters to enhance performance. Increases in filter quantities result
    in a significant growth in the number of floating-point operations. Filter pruning
    eliminates unimportant filters, thus reducing computation [[340](#bib.bib340),
    [341](#bib.bib341), [332](#bib.bib332)]. The number of input and output connections
    to a layer in deep networks determines the number of parameters. These parameters
    can be used to estimate the storage and computation requirements of deep networks.
    Connection pruning is a direct approach to reduce parameters by removing unimportant
    connections [[342](#bib.bib342), [343](#bib.bib343), [344](#bib.bib344)]. Layer
    pruning involves selecting and deleting certain unimportant layers from the network,
    leading to ultra-high compression of the deep network. This is particularly useful
    for deploying deep networks on resource-constrained computing devices, where ultra-high
    compression is necessary. Some layer pruning approaches have been proposed to
    substantially reduce both storage and computation requirements [[53](#bib.bib53),
    [345](#bib.bib345)]. However, layer pruning may result in a higher accuracy compromise
    due to the structural deterioration of deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Network Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network quantization aims to compress the original network by reducing the storage
    requirements of weights. It can be categorized into linear quantization and nonlinear
    quantization. Linear quantization focuses on minimizing the number of bits needed
    to represent each weight, while nonlinear quantization involves dividing weights
    into several groups, with each group sharing a single weight.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Linear Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Utilizing 32-bit floating-point numbers to represent weights consumes a substantial
    amount of resources. Consequently, linear quantization employs low-bit number
    representation to approximate each weight. Suyog *et al.* contend that the weights
    of deep networks can be represented by 16-bit fixed-point numbers without significantly
    reducing classification accuracy [[346](#bib.bib346)]. Some studies further compress
    ConvNets to 8-bit [[347](#bib.bib347), [348](#bib.bib348)]. In the extreme case
    of a 1-bit representation for each weight, binary weight neural networks emerge.
    The primary concept is to directly learn binary weights or activation during model
    training. Several works directly train ConvNets with binary weights, including
    BinaryConnect [[49](#bib.bib49)], BinaryNet[[349](#bib.bib349)], and XNOR [[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Nonlinear Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nonlinear quantization entails dividing weights into several groups, with each
    group sharing a single weight. Gong *et al.* initially employ the k-means algorithm
    to cluster weight parameters and replace the parameter values with the clustering
    center values, substantially reducing the network’s storage space [[350](#bib.bib350)].
    Wu *et al.* further quantize convolution filters, fully connected layers, and
    other parameters [[351](#bib.bib351)]. Chen *et al.* randomly assign weights to
    hash buckets, with each hash bucket sharing a single weight [[352](#bib.bib352)].
    Han *et al.* combine network pruning, parameter quantization, and Huffman coding
    to achieve significant reductions in storage and memory [[353](#bib.bib353)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knowledge distillation (KD) [[51](#bib.bib51)] is a widely adopted technique
    for transferring “dark knowledge” from a high-capacity model (teacher) to a more
    compact model (student) in order to achieve various types of efficiency. The two
    primary aspects of KD are knowledge representation and distillation schemes. In
    this section, we concentrate on existing research in these two technical areas
    and further summarize the theoretical exploration and application progress of
    KD in computer vision, as illustrated in Figure [6](#S5.F6 "Figure 6 ‣ 5.3 Knowledge
    Distillation ‣ 5 Model Compression Techniques ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42a002b2cc4b48a6adb3a2d2b186ec3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Knowledge distillation. The section mainly contains knowledge representation,
    distillation schemes, theory and applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Knowledge Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Drawing on [[52](#bib.bib52)], we examine different forms of knowledge in the
    following categories: response-based knowledge, feature-based knowledge, and relation-based
    knowledge. Response-based knowledge typically refers to the neural response of
    the teacher model’s final output layer, with the main idea being to directly emulate
    the teacher model’s final prediction. The most prevalent response-based knowledge
    for image classification is soft targets [[51](#bib.bib51)]. In object detection
    tasks, the response may include logits along with the bounding box offset [[354](#bib.bib354)].
    For semantic landmark localization tasks, such as human pose estimation, the teacher
    model’s response may consist of a heatmap for each landmark [[355](#bib.bib355)].'
  prefs: []
  type: TYPE_NORMAL
- en: Feature-based knowledge pertains to the feature representation derived from
    intermediate layers. Fitnets [[334](#bib.bib334)] are the first to introduce intermediate
    representations, which subsequently inspire the development of various methods
    [[356](#bib.bib356), [357](#bib.bib357), [358](#bib.bib358), [359](#bib.bib359),
    [360](#bib.bib360), [361](#bib.bib361), [362](#bib.bib362), [363](#bib.bib363)].
    Relation-based knowledge further investigates the relationships between different
    feature layers [[364](#bib.bib364), [365](#bib.bib365), [366](#bib.bib366)] or
    data samples [[367](#bib.bib367), [368](#bib.bib368), [369](#bib.bib369), [335](#bib.bib335)].
    For instance, Yim *et al.* [[364](#bib.bib364)] propose calculating the relations
    between pairs of feature maps using the Gram matrix, while Liu *et al.* [[367](#bib.bib367)]
    suggest transferring the instance relationship graph, which defines instance features
    and relationships as vertices and edges, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Distillation Schemes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The learning schemes of knowledge distillation can be classified into three
    main categories based on the synchronization of the teacher model’s update with
    the student model: offline distillation, online distillation, and self-distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: In offline distillation, the teacher model is usually assumed to be pre-trained.
    The primary focus of offline methods is to enhance various aspects of knowledge
    transfer, including knowledge representation and the design of loss functions.
    Vanilla knowledge distillation [[51](#bib.bib51)] serves as a classic example
    of offline distillation methods. Most prior knowledge distillation methods operate
    in an offline manner.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where a high-capacity, high-performance teacher model is unavailable,
    online distillation provides an alternative. In this approach, both the teacher
    model and the student model are updated simultaneously, allowing for an end-to-end
    trainable knowledge distillation framework. Deep mutual learning [[370](#bib.bib370)]
    introduced a method for training multiple neural networks collaboratively, where
    any given network can serve as the student model while the others act as teachers.
    Numerous online knowledge distillation methods have been proposed [[371](#bib.bib371),
    [372](#bib.bib372), [373](#bib.bib373)], with multi-branch architecture [[371](#bib.bib371)]
    and ensemble techniques [[372](#bib.bib372), [374](#bib.bib374)] being widely
    adopted.
  prefs: []
  type: TYPE_NORMAL
- en: Self-distillation refers to a learning process in which the student model acquires
    knowledge independently, without the presence of teacher models, whether pre-trained
    or virtual. Several studies have explored this idea in various contexts. For instance,
    Zhang *et al.* [[375](#bib.bib375)] propose a method for distilling knowledge
    from deeper layers to shallower ones for image classification tasks. Similarly,
    Hou *et al.* [[376](#bib.bib376)] employ attention maps from deeper layers as
    distillation targets for lower layers in object detection tasks. In contrast,
    Yang *et al.* [[377](#bib.bib377)] introduce snapshot distillation, where checkpoints
    from earlier epochs are considered as teachers to distill knowledge for the models
    in later epochs. Additionally, Wang *et al.* [[362](#bib.bib362)] suggest constraining
    the outputs of the backbone network using target class activation maps.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Theory and Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A wide range of knowledge distillation methods has been extensively employed
    in vision applications. Initially, most knowledge distillation methods were developed
    for image classification [[51](#bib.bib51), [364](#bib.bib364), [378](#bib.bib378),
    [379](#bib.bib379), [380](#bib.bib380)] and later extended to other vision tasks,
    including face recognition [[381](#bib.bib381), [361](#bib.bib361)], action recognition
    [[382](#bib.bib382), [383](#bib.bib383)], object detection [[354](#bib.bib354),
    [384](#bib.bib384), [385](#bib.bib385)], semantic segmentation [[386](#bib.bib386),
    [387](#bib.bib387), [388](#bib.bib388), [389](#bib.bib389)], depth estimation
    [[390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392), [393](#bib.bib393),
    [394](#bib.bib394), [395](#bib.bib395)], image retrieval [[396](#bib.bib396),
    [397](#bib.bib397)], video captioning [[398](#bib.bib398), [399](#bib.bib399),
    [400](#bib.bib400)], and video classification [[401](#bib.bib401), [402](#bib.bib402)],
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the significant practical success, relatively few works have focused
    on the theoretical or empirical understanding of knowledge distillation [[403](#bib.bib403),
    [404](#bib.bib404), [405](#bib.bib405), [379](#bib.bib379)]. Hinton *et al.* [[51](#bib.bib51)]
    suggest that the success of KD could be attributed to learning similarities between
    categories. Yuan *et al.* [[403](#bib.bib403)] posited that dark knowledge not
    only encompasses category similarities but also imposes regularization on student
    training. They indicate that KD is a learned label smoothing regularization (LSR).
    Tang *et al.* [[404](#bib.bib404)] propose approach where, in addition to regularization
    and class relationships, another type of knowledge, instance-specific knowledge,
    is also used by the teacher to rescale the student model’s per-instance gradients.
    Chen *et al.* [[405](#bib.bib405)] quantify the extraction of visual concepts
    from the intermediate layers of a deep learning model to explain knowledge distillation.
    Wang *et al.* [[379](#bib.bib379)] connect KD with the information bottleneck
    and empirically validate that preserving more mutual information between feature
    representation and input is more important than improving the teacher model’s
    accuracy. Overall, theoretical research remains limited compared to the diverse
    and numerous applications.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Low-rank Factorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolution kernels can be viewed as 3D tensors. Ideas based on tensor decomposition
    are derived from the intuition that there is structural sparsity in the 3D tensor.
    In the case of fully connected layers, they can be viewed as 2D matrices (or 3D
    tensors), and low-rankness can also be helpful. The key idea of low-rank factorization
    is to find an approximate low-rank tensor that is close to the real tensor and
    easy to decompose. Low-rank factorization is beneficial for both tensors and matrices.
  prefs: []
  type: TYPE_NORMAL
- en: There are several typical low-rank methods for compressing 3D convolutional
    layers. Lebedev *et al.* [[406](#bib.bib406)] propose Canonical Polyadic (CP)
    decomposition for kernel tensors. They use nonlinear least squares to compute
    the CP decomposition for a better low-rank approximation. Since low-rank tensor
    decomposition is a non-convex problem and generally difficult to compute, Jaderberg
    *et al.* use iterative schemes to obtain an approximate local solution [[333](#bib.bib333)].
    Then, Tai *et al.* find that the particular form of low-rank decomposition in
    [[333](#bib.bib333)] has an exact closed-form solution, which is the global optimum,
    and present a method for training low-rank constrained ConvNets from scratch [[407](#bib.bib407)].
  prefs: []
  type: TYPE_NORMAL
- en: Many classical works have exploited low-rankness in fully connected layers.
    Denil *et al.* reduce the number of dynamic parameters in deep models using the
    low-rank method [[408](#bib.bib408)]. Zhang *et al.* introduce a Tucker decomposition
    model to compress weight tensors in fully connected layers [[409](#bib.bib409)].
    Lu *et al.* adopt truncated singular value decomposition to decompose the fully
    connected layer for designing compact multi-task deep learning architectures [[410](#bib.bib410)].
    Sainath *et al.* explore a low-rank matrix factorization of the final weight layer
    in deep networks for acoustic modeling [[411](#bib.bib411)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Hybrid Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apart from the four categories of mainstream techniques mentioned above, there
    are other techniques for network compression. Some studies have attempted to integrate
    orthogonal techniques to achieve more significant performance [[353](#bib.bib353),
    [412](#bib.bib412), [413](#bib.bib413)]. Some works have designed compact networks
    [[26](#bib.bib26), [64](#bib.bib64), [89](#bib.bib89)] or efficient convolutions
    [[86](#bib.bib86), [72](#bib.bib72)], which have been discussed in Sec. [2](#S2
    "2 Architecture Design of Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Efficient Deployment on Hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The aforementioned works mostly design network architectures based on their
    theoretical computation (*e.g.* floating operations, FLOPs). However, there is
    often a gap between theoretical computation and practical latency on hardware
    devices [[27](#bib.bib27), [64](#bib.bib64)]. Realistic efficiency can be influenced
    by other factors such as hardware properties and scheduling strategies. Along
    this direction, we review relative works from the following perspectives: 1) hardware-aware
    neural architecture search (Sec. [6.1](#S6.SS1 "6.1 Hardware-aware Model Design
    ‣ 6 Efficient Deployment on Hardware ‣ Computation-efficient Deep Learning for
    Computer Vision: A Survey")); 2) acceleration software libraries and hardware
    design (Sec. [6.2](#S6.SS2 "6.2 Acceleration Tools ‣ 6 Efficient Deployment on
    Hardware ‣ Computation-efficient Deep Learning for Computer Vision: A Survey"));
    and 3) algorithm-software codesign techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Hardware-aware Model Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the practical latency of models can be influenced by many factors other than
    theoretical computation, the commonly used FLOPs is an inaccurate proxy for network
    efficiency. Ideally, one should develop efficient models based on specific hardware
    properties. However, hand-designing networks for different hardware devices can
    be laborious. Therefore, automatically *searching* for efficient architectures
    is emerging as a promising direction. Compared to the traditional NAS methods
    [[31](#bib.bib31), [414](#bib.bib414)], this line of works can generate appropriate
    models which satisfy different hardware constraints and gain realistic efficiency
    in practice. For example, ProxylessNAS [[54](#bib.bib54)] establishes a latency
    prediction function based on realistic tests on targeted hardware, and the predicted
    latency is then directly used as a regularization item in the NAS objective. A
    similar idea is also implemented by MnasNet [[53](#bib.bib53)] to search for efficient
    models on mobile devices. The following works FBNet [[159](#bib.bib159)], FBNet-v2
    [[415](#bib.bib415)] and OFA [[416](#bib.bib416)] have improved NAS techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the traditional static models, the hardware-aware design paradigm
    has also been applied to develop *spatial-wise dynamic networks* (Sec. [3.2](#S3.SS2
    "3.2 Spatial-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")) [[39](#bib.bib39)].'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we mainly give a brief introduction of basic ideas in this work due
    to the page limit. For more detailed techniques we refer the readers to the survey
    [[417](#bib.bib417)] which specifically focuses on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Acceleration Tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to architectural design, the efficient deployment of algorithms
    on hardwares also requires acceleration software libraries or specific hardware
    accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Software Libraries. Extensive efforts have been made to accelerate model
    inference on different hardware platforms. For example, NVIDIA TensorRT [[57](#bib.bib57)]
    is widely used to deploy models for optimized inference on GPUs. NNPACK (https://github.com/Maratyszcza/NNPACK.),
    CoreML [[58](#bib.bib58)] and TinyEngine [[418](#bib.bib418)] are representative
    tools on multi-core CPUs, Apple silicons, and microcontrollers (MCUs), respectively.
    Cross-platform tools such as Tencent TNN (https://github.com/Tencent/TNN). and
    Apache TVM [[59](#bib.bib59)] have also emerged as popular development tools.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Hardware Accelerators. Apart from adapting neural architectures to given
    hardware devices, another line of works studies accelerators from the hardware
    perspective to enable fast inference of deep models. For example, DianNao [[419](#bib.bib419)]
    focuses on memory behavior and proposes an accelerator that simultaneously improves
    the inference speed and energy consumption of deep models. An FPGA-based accelerator
    is proposed quantitatively analyze the throughput of CNNs with the help of the
    classical roofline model [[420](#bib.bib420)]. In addition to the regular deep
    networks, researchers have also proposed accelerators to improve the inference
    efficiency of spatially sparse convolution [[421](#bib.bib421), [422](#bib.bib422)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Algorithm-Hardware Co-design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The aforementioned methods typically improve the inference efficiency from
    the perspective of either algorithm or hardware. Ideally, one should expect algorithms
    and hardware can “cooperate” with each other to further push forward the Pareto
    frontier between accuracy and efficiency trade-off. Along this direction, extensive
    efforts have been made based on the highly flexible and versatile Field Programmable
    Gate Arrays (FPGA) platform, and NAS techniques (Sec. [6.1](#S6.SS1 "6.1 Hardware-aware
    Model Design ‣ 6 Efficient Deployment on Hardware ‣ Computation-efficient Deep
    Learning for Computer Vision: A Survey")) are widely used to search for hardware-friendly
    network structures [[423](#bib.bib423), [55](#bib.bib55), [424](#bib.bib424),
    [56](#bib.bib56), [425](#bib.bib425)]. The recent MCUNet series [[418](#bib.bib418),
    [426](#bib.bib426), [427](#bib.bib427)] has enabled both inference and training
    on MCUs based on algorithm-hardware co-design with the help of their proposed
    tiny-Engine tool (Sec. [6.2](#S6.SS2 "6.2 Acceleration Tools ‣ 6 Efficient Deployment
    on Hardware ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: The co-designing method has also been applied to the field of dynamic neural
    networks, especially for efficient spatially adaptive convolution [[428](#bib.bib428),
    [429](#bib.bib429), [430](#bib.bib430)] and attention [[431](#bib.bib431)] operations.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the significant advances in the field of computationally efficient deep
    learning in recent years, numerous open challenges warrant further research. In
    this section, we summarize these challenges and discuss potential future directions.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Designing General-purpose Backbones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The efficient extraction of discriminative representations from raw inputs has
    been established as a critical cornerstone for practical deep learning applications,
    as demonstrated in the existing literature. Light-weighted backbone networks are
    commonly employed to achieve this goal. As a result, a significant challenge lies
    in the design of efficient, general-purpose backbones. Potential avenues of investigation
    in this area encompass enhancing current convolution and self-attention operators
    via manual design [[26](#bib.bib26), [7](#bib.bib7)], employing automated architecture
    search methodologies [[24](#bib.bib24)], and amalgamating these approaches to
    create comprehensive efficient modules [[90](#bib.bib90)]. Specifically, the exploration
    of innovative information aggregation methods beyond convolution and self-attention
    appears promising, for instance, clustering algorithms [[432](#bib.bib432)], LSTM
    [[433](#bib.bib433)], and graph convolution [[434](#bib.bib434)]. Moreover, an
    emerging area of interest involves enabling backbone networks to accommodate multi-modal
    inputs (*e.g.*, text, images, and videos) and execute multiple visual tasks (*e.g.*,
    retrieval, classification, and visual question answering) [[435](#bib.bib435),
    [436](#bib.bib436)]. Consequently, the development of mobile-level multi-modal
    and multi-task visual foundation models could present an intriguing direction
    for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Developing Task-specialized Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the architectural advancements in backbone models, tailoring
    deep learning methodologies to specific computer vision tasks of interest has
    been demonstrated as crucial. Two research challenges of particular significance
    in this domain can be identified. Firstly, the exploitation of representations
    extracted by backbones to efficiently obtain task-specific features is essential,
    for example, multi-scale features for object detection and multi-path fused features
    for semantic segmentation. A potential solution to this challenge could involve
    designing specialized, efficient decoders (*e.g.*, utilizing NAS [[437](#bib.bib437),
    [311](#bib.bib311)]). Secondly, it is important to streamline the multi-stage
    design of visual tasks (*e.g.*, two-stage object detection [[273](#bib.bib273)]
    and instance segmentation [[318](#bib.bib318)] algorithms) to achieve end-to-end
    paradigms with minimal performance compromises. Additionally, the removal of time-consuming
    components, such as non-maximum suppression (NMS) [[8](#bib.bib8)], is crucial.
    A promising area for future research may involve the development of an efficient,
    unified, and end-to-end learnable interface for a majority of prevalent computer
    vision tasks [[438](#bib.bib438)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Deep Networks for Edge Computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practical applications, extant research predominantly focuses on conventional
    hardware, such as GPUs and CPUs. However, within the realm of edge computing,
    there is an increasing demand for the deployment of deep learning models on Internet
    of Things (IoT) devices and microcontrollers. These diminutive devices are characterized
    by their minimal size, low power consumption, affordability, and ubiquity [[418](#bib.bib418)].
    The development of deep learning algorithms specifically adapted for such devices
    represents an exigent research direction. MCUNets [[418](#bib.bib418), [426](#bib.bib426),
    [427](#bib.bib427)] have provided an initial exploration by optimizing the design,
    inference, and training of ConvNets for these devices. Another prospective concept
    involves the creation of spiking neural networks [[439](#bib.bib439)], which,
    when co-designed with hardware, can yield energy-efficient solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Leveraging Large-scale Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contemporary large visual backbone models have exhibited remarkable scalability
    in response to the increasing volumes of training data [[6](#bib.bib6)], that
    is, the model’s performance consistently enhances as more training data becomes
    accessible. However, it is generally arduous for computationally efficient models
    with a reduced number of parameters to capitalize on this high-data regime to
    the same extent as their larger counterparts. For example, the improvements attained
    by pre-training light-weighted models on expansive ImageNet-22K/JFT datasets are
    typically inferior to those observed in larger models [[6](#bib.bib6), [7](#bib.bib7),
    [74](#bib.bib74)]. This challenge is similarly experienced by self-supervised
    learning algorithms, where the methods effective for larger models frequently
    produce limited gains for smaller models [[440](#bib.bib440), [441](#bib.bib441)].
    As a result, a propitious avenue of research involves the exploration of effective
    scalable supervised and unsupervised learning algorithms for light-weighted models,
    allowing them to reap the benefits of an unlimited amount of data without incurring
    the expense of acquiring annotations. Some recent works on novel training algorithms
    have started to preliminarily explore this direction [[442](#bib.bib442), [82](#bib.bib82),
    [443](#bib.bib443), [444](#bib.bib444), [445](#bib.bib445)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Practical Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While numerous extant studies have attained low theoretical computational costs,
    they may be hindered by the restricted practical efficiency. For example, certain
    irregular network architectures discovered through NAS may display considerable
    latency on GPUs/CPUs, and the models employing group or depth-wise convolution
    may exhibit reduced gains in actual speedup relative to their theoretical computational
    efficiency. To tackle this challenge, researchers might consider integrating the
    speed on practical hardwares as an objective in architecture design [[53](#bib.bib53),
    [24](#bib.bib24)] or utilizing efficient implementation software [[57](#bib.bib57),
    [59](#bib.bib59)]. From a hardware design standpoint, one potential direction
    involves the creation of model-specialized hardware platforms [[423](#bib.bib423),
    [55](#bib.bib55), [424](#bib.bib424), [56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Model Compression Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network compression algorithms, encompassing network pruning, quantization,
    and knowledge distillation, have exhibited a robust capacity to diminish the inference
    costs associated with deep networks. However, several avenues of investigation
    remain unexplored. For instance, while the overarching concept of model compression
    is not confined to a particular vision task, a majority of algorithms predominantly
    concentrate on image classification, rendering their extension to other tasks
    non-trivial. A significant research direction entails the development of general-purpose,
    task-agnostic model compression techniques. Furthermore, strategies such as network
    pruning may yield irregular architectural topologies, potentially impairing the
    practical efficiency of deep learning models. Consequently, the examination of
    practically efficient compression methodologies constitutes a propitious area
    for future research.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported in part by the National Key R&D Program of China (2021ZD0140407),
    the National Natural Science Foundation of China (62022048, 62276150), Guoqiang
    Institute of Tsinghua University and Beijing Academy of Artificial Intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for
    large-scale image recognition. In ICLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In CVPR, pages 1–9, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In CVPR, pages 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Q
    Weinberger. Convolutional networks with dense connectivity. TPAMI, 44(12):8704–8716,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. In ICLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
    and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted
    windows. In ICCV, pages 10012–10022, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object
    detection in 20 years: A survey. Proceedings of the IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtarnavaz,
    and Demetri Terzopoulos. Image segmentation using deep learning: A survey. TPAMI,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Tianfei Zhou, Fatih Porikli, David J Crandall, Luc Van Gool, and Wenguan
    Wang. A survey on deep learning technique for video segmentation. TPAMI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Yu Kong and Yun Fu. Human action recognition and prediction: A survey.
    IJCV, 130(5):1366–1401, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Zehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang,
    and Jun Liu. Human action recognition from various data modalities: A review.
    TPAMI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun.
    Deep learning for 3d point clouds: A survey. TPAMI, 43(12):4338–4364, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jiasi Chen and Xukan Ran. Deep learning with edge computing: A review.
    Proceedings of the IEEE, 107(8):1655–1674, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yunbin Deng. Deep learning on mobile devices: a review. In Mobile Multimedia/Image
    Processing, Security, and Applications 2019, volume 10993, pages 52–66\. SPIE,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Chunlei Chen, Peng Zhang, Huixiang Zhang, Jiangyan Dai, Yugen Yi, Huihui
    Zhang, and Yonghui Zhang. Deep learning on computational-resource-limited platforms:
    a survey. Mobile Information Systems, 2020(4):1–19, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and Victor Hugo C
    de Albuquerque. Deep learning for safe autonomous driving: Current challenges
    and future directions. IEEE Transactions on Intelligent Transportation Systems,
    22(7):4316–4336, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner,
    Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai
    Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.
    A survey of deep learning techniques for autonomous driving. Journal of Field
    Robotics, 37(3):362–386, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Chaoyun Zhang, Paul Patras, and Hamed Haddadi. Deep learning in mobile
    and wireless networking: A survey. IEEE Communications surveys & tutorials, 21(3):2224–2287,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, and Mohsen Guizani. Deep
    learning for iot big data and streaming analytics: A survey. IEEE Communications
    Surveys & Tutorials, 20(4):2923–2960, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] He Li, Kaoru Ota, and Mianxiong Dong. Learning iot in edge: Deep learning
    for the internet of things with edge computing. IEEE network, 32(1):96–101, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Adrian Carrio, Carlos Sampedro, Alejandro Rodriguez-Ramos, and Pascual
    Campoy. A review of deep learning methods and applications for unmanned aerial
    vehicles. Journal of Sensors, 2017, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all:
    Train one network and specialize it for efficient deployment. In ICLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey
    on green deep learning. arXiv preprint arXiv:2111.05193, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510–4520,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing
    Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for
    mobilenetv3. In CVPR, pages 1314–1324, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional
    neural networks. In ICML, pages 6105–6114\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and
    Piotr Dollár. Designing network design spaces. In CVPR, pages 10428–10436, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Barret Zoph and Quoc Le. Neural architecture search with reinforcement
    learning. In ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture
    search. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang.
    Dynamic neural networks: A survey. TPAMI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten,
    and Kilian Q Weinberger. Multi-scale dense networks for resource efficient image
    classification. In ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning.
    NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis
    DeCoste, Wei Di, and Yizhou Yu. Hd-cnn: hierarchical deep convolutional neural
    networks for large scale visual recognition. In ICCV, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry S Davis.
    Adaframe: Adaptive frame selection for fast video recognition. In CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, and Gao
    Huang. Adaptive focus for efficient video recognition. In ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Yizeng Han, Zhihang Yuan, Yifan Pu, Chenhao Xue, Shiji Song, Guangyu Sun,
    and Gao Huang. Latency-aware spatial-wise dynamic networks. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In CVPR, pages 779–788, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV,
    pages 21–37\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional
    one-stage object detection. In ICCV, pages 9627–9636, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable
    bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv
    preprint arXiv:2207.02696, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong
    Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation.
    In ECCV, pages 325–341, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. Solo:
    Segmenting objects by locations. In ECCV, pages 649–665\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2:
    Dynamic and fast instance segmentation. In NeurIPS, volume 33, pages 17721–17732,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
    Zhang. Learning efficient convolutional networks through network slimming. In
    ICCV, pages 2736–2744, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
    sparse, trainable neural networks. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect:
    Training deep neural networks with binary weights during propagations. NeurIPS,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net:
    Imagenet classification using binary convolutional neural networks. In ECCV, pages
    525–542\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge
    in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge
    distillation: A survey. IJCV, 129(6):1789–1819, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
    Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for
    mobile. In CVPR, pages 2820–2828, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture
    search on target task and hardware. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow,
    Wen-mei Hwu, and Deming Chen. Fpga/dnn co-design: An efficient design methodology
    for iot intelligence on the edge. In DAC, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Weiwen Jiang, Lei Yang, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Shouzhen
    Gu, Sakyasingha Dasgupta, Yiyu Shi, and Jingtong Hu. Hardware/software co-exploration
    of neural architectures. IEEE Transactions on Computer-Aided Design of Integrated
    Circuits and Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Han Vanholder. Efficient inference with tensorrt. In GPU Technology Conference,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Mohit Thakkar and Mohit Thakkar. Introduction to core ml framework. Beginning
    Machine Learning in iOS: CoreML Framework, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q Yan,
    Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Tvm:
    end-to-end optimization stack for deep learning. arXiv preprint arXiv:1802.04799,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Yanjiao Chen, Baolin Zheng, Zihan Zhang, Qian Wang, Chao Shen, and Qian
    Zhang. Deep learning on mobile and embedded devices: State-of-the-art, challenges,
    and future directions. ACM Computing Surveys (CSUR), 53(4):1–37, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
    Aggregated residual transformations for deep neural networks. In CVPR, pages 1492–1500,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang,
    and Philip Torr. Res2net: A new multi-scale backbone architecture. TPAMI, 43(2):652–662,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An
    extremely efficient convolutional neural network for mobile devices. In CVPR,
    pages 6848–6856, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2:
    Practical guidelines for efficient cnn architecture design. In ECCV, pages 116–131,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
    Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation
    through attention. In ICML, pages 10347–10357, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.
    In CVPR, pages 510–519, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang,
    Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks.
    In CVPR, pages 2736–2746, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. Interleaved group
    convolutions. In ICCV, pages 4373–4382, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, and
    Guo-Jun Qi. Interleaved structured sparse convolutional neural networks. In CVPR,
    pages 8847–8856, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Ke Sun, Mingjie Li, Dong Liu, and Jingdong Wang. Igcv3: Interleaved low-rank
    group convolutions for efficient deep neural networks. In BMVC, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    NeurIPS, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] François Chollet. Xception: Deep learning with depthwise separable convolutions.
    In CVPR, pages 1251–1258, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi.
    Espnetv2: A light-weight, power efficient, and general purpose convolutional neural
    network. In CVPR, pages 9190–9200, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
    and Saining Xie. A convnet for the 2020s. In CVPR, pages 11976–11986, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan,
    and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In ICCV,
    pages 22–31, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and
    Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In CVPR,
    pages 12175–12185, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In ICML, pages 448–456\.
    PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
    Wojna. Rethinking the inception architecture for computer vision. In CVPR, pages
    2818–2826, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In AAAI, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng
    YAN. Inception transformer. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and
    Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated
    convolutions. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training.
    In ICML, pages 10096–10106\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
    Rethinking bottleneck structure for efficient mobile network design. In ECCV,
    pages 680–697\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very
    deep networks. NeurIPS, 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
    Densely connected convolutional networks. In CVPR, pages 4700–4708, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger.
    Condensenet: An efficient densenet using learned group convolutions. In CVPR,
    pages 2752–2761, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang,
    and Qi Tian. Condensenet v2: Sparse feature reactivation for deep networks. In
    CVPR, pages 3569–3578, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, and
    Qi Tian. Ghostnets on heterogeneous devices via cheap operations. IJCV, 130(4):1050–1069,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu.
    Ghostnet: More features from cheap operations. In CVPR, pages 1580–1589, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, and Yunhe Wang.
    Ghostnetv2: Enhance cheap operation with long-range attention. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus
    Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy
    in convolutional neural networks with octave convolution. In ICCV, pages 3435–3444,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with
    hilo attention. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation
    learning for human pose estimation. In CVPR, pages 5693–5703, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang
    Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution
    representation learning for visual recognition. TPAMI, 43(10):3349–3364, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and
    Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict.
    NeurIPS, 34:7281–7293, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang,
    Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone
    for dense prediction without convolutions. In ICCV, pages 568–578, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang,
    Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision
    transformer. Computational Visual Media, 8(3):415–424, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang.
    Shunted self-attention via multi-scale token aggregation. In CVPR, pages 10853–10862,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin
    Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention
    in vision transformers. NeurIPS, 34:9355–9366, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia
    Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity
    and resolution. In CVPR, pages 12009–12019, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake
    Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient
    visual backbones. In CVPR, pages 12894–12904, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan,
    and Jianfeng Gao. Focal self-attention for local-global interactions in vision
    transformers. arXiv preprint arXiv:2107.00641, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan,
    Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone
    with cross-shaped windows. In CVPR, pages 12124–12134, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo:
    Vision outlooker for visual recognition. TPAMI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, and
    Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer.
    In ECCV, pages 107–124\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar,
    Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In ECCV, pages
    459–479\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Stéphane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio
    Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional
    inductive biases. In ICML, pages 2286–2296\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Kehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, and Jie Chen.
    Locality guidance for improving vision transformers on tiny datasets. In ECCV,
    pages 110–127\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and
    Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating
    messenger tokens. In CVPR, pages 12063–12072, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
    Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training
    vision transformers from scratch on imagenet. In ICCV, pages 558–567, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
    Transformer in transformer. NeurIPS, 34:15908–15919, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
    Transformers are rnns: Fast autoregressive transformers with linear attention.
    In ICML, pages 5156–5165\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking
    attention with performers. In ICLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn
    Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating
    self-attention. In AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao,
    Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear
    complexity. In NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Haoran You, Yunyang Xiong, Xiaoliang Dai, Bichen Wu, Peizhao Zhang, Haoqi
    Fan, Peter Vajda, and Yingyan Lin. Castling-vit: Compressing self-attention via
    switching towards linear-angular attention during vision transformer inference.
    In CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient
    architecture search by network transformation. In AAAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten
    transformer: Vision transformer using focused linear attention. In ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional
    image transformers. In ICCV, pages 9981–9990, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang. Mpvit:
    Multi-path vision transformer for dense prediction. In CVPR, pages 7287–7296,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L Yuille, and Wei
    Shen. Glance-and-gaze vision transformer. NeurIPS, 34:12992–13003, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
    Incorporating convolution designs into visual transformers. In ICCV, pages 579–588,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit:
    Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying
    convolution and attention for all data sizes. In NeurIPS, pages 3965–3977, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and
    Ross Girshick. Early convolutions help transformers see better. In NeurIPS, pages
    30392–30400, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand
    Joulin, Hervé Jégou, and Matthijs Douze. Levit: a vision transformer in convnet’s
    clothing for faster inference. In ICCV, pages 12259–12269, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose,
    and mobile-friendly vision transformer. In ICLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and
    Qi Tian. Visformer: The vision-friendly transformer. In ICCV, pages 589–598, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe,
    and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In ICCV,
    pages 11936–11945, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer
    advanced by exploring intrinsic inductive bias. In NeurIPS, volume 34, pages 28522–28535,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vitaev2: Vision
    transformer advanced by exploring inductive bias for image recognition and beyond.
    IJCV, pages 1–22, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan,
    and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In CVPR, pages
    5270–5279, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian
    Cheng, and Jingdong Wang. Mixformer: Mixing features across windows and dimensions.
    In CVPR, pages 5249–5259, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Zhiliang Peng, Zonghao Guo, Wei Huang, Yaowei Wang, Lingxi Xie, Jianbin
    Jiao, Qi Tian, and Qixiang Ye. Conformer: Local features coupling global representations
    for recognition and detection. TPAMI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin
    Jiao, and Qixiang Ye. Conformer: Local features coupling global representations
    for visual recognition. In ICCV, pages 367–376, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr
    Dollár. On network design spaces for visual recognition. In ICCV, pages 1882–1890,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and accurate model
    scaling. In CVPR, pages 924–932, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, and Tong Zhang.
    Model rubik’s cube: Twisting resolution, depth and width for tinynets. NeurIPS,
    33:19353–19364, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing
    neural network architectures using reinforcement learning. In ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning
    transferable architectures for scalable image recognition. In CVPR, pages 8697–8710,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon
    Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image
    classifiers. In ICML, pages 2902–2911\. PMLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Lingxi Xie and Alan Yuille. Genetic cnn. In ICCV, pages 1379–1388, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized
    evolution for image classifier architecture search. In AAAI, pages 4780–4789,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient
    architecture search for convolutional neural networks. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level
    network transformation for efficient architecture search. In ICML, pages 678–687\.
    PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and
    Koray Kavukcuoglu. Hierarchical representations for efficient architecture search.
    In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient
    neural architecture search via parameters sharing. In ICML, pages 4095–4104\.
    PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan,
    and Quoc Le. Understanding and simplifying one-shot architecture search. In ICML,
    pages 550–559\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic
    neural architecture search. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in
    four gpu hours. In CVPR, pages 1761–1770, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox,
    and Frank Hutter. Understanding and robustifying differentiable architecture search.
    In ICLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and
    Qi Tian. Network adjustment: Channel and block search guided by resource utilization
    ratio. IJCV, 130(3):820–835, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable
    architecture search: Bridging the depth gap between search and evaluation. In
    ICCV, pages 1294–1303, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, and Changshui
    Zhang. Greedynas: Towards fast one-shot nas with greedy supernet. In CVPR, pages
    1999–2008, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Andrew Brock, Theo Lim, JM Ritchie, and Nick Weston. Smash: One-shot
    model architecture search through hypernetworks. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for
    neural architecture search. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei,
    and Jian Sun. Single path one-shot neural architecture search with uniform sampling.
    In ECCV, pages 544–560\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans,
    Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling
    up neural architecture search with big single-stage models. In ECCV, pages 702–717\.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming
    Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware
    efficient convnet design via differentiable neural architecture search. In CVPR,
    pages 10734–10742, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar
    Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV,
    pages 4489–4497, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a
    new model and the kinetics dataset. In CVPR, pages 6299–6308, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient
    convolutional network for online video understanding. In ECCV, pages 695–712,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
    Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video
    classification. In ECCV, pages 305–321, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification
    with channel-separated convolutional networks. In ICCV, pages 5552–5561, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video
    recognition. In CVPR, pages 203–213, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
    Paluri. A closer look at spatiotemporal convolutions for action recognition. In
    CVPR, pages 6450–6459, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi. Human action recognition
    using factorized spatio-temporal convolutional networks. In ICCV, pages 4597–4605,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation
    with pseudo-3d residual networks. In ICCV, pages 5533–5541, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal
    adaptive module for video recognition. In ICCV, pages 13708–13718, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal
    relational reasoning in videos. In ECCV, pages 803–818, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm:
    Spatiotemporal and motion encoding for action recognition. In ICCV, pages 2000–2009,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang.
    Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages
    909–918, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient
    video understanding. In ICCV, pages 7083–7093, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Hao Zhang, Yanbin Hao, and Chong-Wah Ngo. Token shift transformer for
    video classification. In ACM MM, pages 917–925, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift
    networks for video action recognition. In CVPR, pages 1102–1111, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast
    networks for video recognition. In ICCV, pages 6202–6211, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference
    networks for efficient action recognition. In CVPR, pages 1895–1904, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić,
    and Cordelia Schmid. Vivit: A video vision transformer. In ICCV, pages 6836–6846,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez,
    and Georgios Tzimiropoulos. Space-time mixing attention for video transformer.
    NeurIPS, 34:19594–19607, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and
    Han Hu. Video swin transformer. In CVPR, pages 3202–3211, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li,
    and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation
    learning. In ICLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun,
    and Cordelia Schmid. Multiview transformers for video recognition. In CVPR, pages
    3333–3343, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze,
    Christoph Feichtenhofer, Andrea Vedaldi, and João F Henriques. Keeping your eye
    on the ball: Trajectory attention in video transformers. NeurIPS, 34:12493–12506,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention
    all you need for video understanding? In ICML, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer
    network. In ICCV, pages 3163–3172, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep
    learning on point sets for 3d classification and segmentation. In CVPR, pages
    652–660, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 30,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph
    attention convolution for point cloud semantic segmentation. In CVPR, pages 10296–10305,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein,
    and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions
    On Graphics, 38(5):1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep
    learning on point sets with parameterized convolutional filters. In ECCV, pages
    87–102, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent
    convolutions for dense prediction in 3d. In CVPR, pages 3887–3896, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui,
    François Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution
    for point clouds. In ICCV, pages 6411–6420, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen.
    Pointcnn: Convolution on x-transformed points. NeurIPS, 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape
    convolutional neural network for point cloud analysis. In CVPR, pages 8895–8904,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional
    networks on 3d point clouds. In CVPR, pages 9621–9630, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at
    local aggregation operators in point cloud analysis. In ECCV, pages 326–342\.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu,
    Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation.
    In CVPR, pages 8500–8509, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun.
    Point transformer. In ICCV, pages 16259–16268, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader
    Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++
    with improved training and scaling strategies. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural
    network for real-time object recognition. In IROS, pages 922–928\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic
    segmentation with submanifold sparse convolutional networks. In CVPR, pages 9224–9232,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal
    convnets: Minkowski convolutional neural networks. In CVPR, pages 3075–3084, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Focal
    sparse convolutional networks for 3d object detection. In CVPR, pages 5428–5437,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object
    detection. In CVPR, pages 10529–10538, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping
    Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction
    with local vector representation for 3d object detection. IJCV, pages 1–21, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced
    grouping and sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for
    efficient 3d deep learning. In NeurIPS, volume 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang,
    and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution.
    In ECCV, pages 685–702\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller.
    Multi-view convolutional neural networks for 3d shape recognition. In ICCV, pages
    945–953, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional
    network for 3d shape analysis. In CVPR, pages 1850–1859, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view
    transformation network for 3d shape recognition. In ICCV, pages 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki.
    Gift: A real-time and scalable 3d shape search engine. In CVPR, pages 5023–5032,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, and Yue Gao. Mlvcnn:
    Multi-loop-view convolutional neural network for 3d shape retrieval. In AAAI,
    pages 8513–8520, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet:
    Joint object categorization and pose estimation using multiviews from unsupervised
    viewpoints. In CVPR, pages 5010–5019, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet:
    Fast inference via early exiting from deep neural networks. In ICPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive
    neural networks for efficient inference. In ICML, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang.
    Resolution adaptive networks for efficient inference. In CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all
    images are worth 16x16 words: Dynamic transformers for efficient image recognition.
    NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu,
    Chao Deng, Junlan Feng, Shiji Song, and Gao Huang. Dynamic perceiver for efficient
    visual recognition. In ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, and Gao Huang. Improved
    techniques for training adaptive deep networks. In ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao,
    Wenhui Huang, Chao Deng, and Gao Huang. Learning to weight samples for dynamic
    early-exiting networks. In ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez.
    Skipnet: Learning dynamic routing in convolutional networks. In ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Andreas Veit and Serge Belongie. Convolutional networks with adaptive
    inference graphs. In ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang
    Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image
    recognition. In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping
    for learning conditional channel gated networks. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Charles Herrmann, Richard Strong Bowen, and Ramin Zabih. Channel selection
    using gumbel softmax. In ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li, and
    Xiaojun Chang. Dynamic slimmable network. In CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi,
    and Aditya Nori. Adaptive neural trees. In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul
    Mazumder. The tree ensemble layer: Differentiability meets conditional computation.
    In ICML, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv:
    Conditionally parameterized convolutions for efficient inference. In NeurIPS,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan,
    Zidong Wang, Shiji Song, and Gao Huang. Adaptive rotated convolution for rotated
    object detection. In ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, and Min Sun.
    Instanas: Instance-aware neural architecture search. In AAAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang,
    and Jian Sun. Learning dynamic routing for semantic segmentation. In CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.
    Learning deep features for discriminative localization. In CVPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less:
    A more complicated network with less inference complexity. In CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting
    spatial sparsity for faster inference. In CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially
    adaptive inference with stochastic feature sampling and interpolation. In ECCV,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun
    Jiang. Spatially adaptive feature refinement for efficient inference. TIP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and
    Yichen Wei. Deformable convolutional networks. In ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets
    v2: More deformable, better results. In CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu,
    Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale
    vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision
    transformer with deformable attention. In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of
    visual attention. NeurIPS, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, and Wei Xu. Dynamic
    computational time for visual attention. In ICCVW, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent
    attention convolutional neural network for fine-grained image recognition. In
    CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang.
    Glance and focus: a dynamic approach to reducing spatial redundancy in image classification.
    In NeurIPS, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Gao Huang, Yulin Wang, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei
    Qi, and Shiji Song. Glance and focus networks for dynamic visual recognition.
    TPAMI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement
    learning: A survey. Journal of artificial intelligence research, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. Scale-aware
    face detection. In CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan,
    and Yunhe Wang. Dynamic resolution network. NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S Davis. Liteeval:
    A coarse-to-fine framework for resource efficient video recognition. NeurIPS,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid
    Karlinsky, Kate Saenko, Aude Oliva, and Rogerio Feris. Adafuse: Adaptive temporal
    fusion network for efficient action recognition. In ICLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Ximeng Sun, Rameswar Panda, Chun-Fu Richard Chen, Aude Oliva, Rogerio
    Feris, and Kate Saenko. Dynamic network quantization for efficient video inference.
    In ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid
    Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris. Ar-net: Adaptive frame
    resolution for efficient action recognition. In ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end
    learning of action detection from frame glimpses in videos. In CVPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Humam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem. Action search:
    Spotting actions in videos and its application to temporal action localization.
    In ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge, and Yi Yang.
    Watching a small portion could be as good as watching all: Towards efficient video
    classification. In IJCAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, and Shilei
    Wen. Dynamic inference: A new approach toward efficient video action recognition.
    In CVPRW, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Amir Ghodrati, Babak Ehteshami Bejnordi, and Amirhossein Habibian. Frameexit:
    Conditional early exiting for efficient video recognition. In CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive
    reinforcement learning for skeleton-based action recognition. In CVPR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, and Shilei Wen. Multi-agent
    reinforcement learning based frame sampling for effective untrimmed video recognition.
    In ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, and Limin Wang. Dynamic sampling
    networks for efficient action recognition in videos. TIP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov,
    Nikita Orlov, Humphrey Shi, and Gao Huang. Adafocus v2: End-to-end training of
    spatial dynamic networks for video recognition. In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] Yulin Wang, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita
    Orlov, Shiji Song, Humphrey Shi, and Gao Huang. Adafocusv3: On unified spatial-temporal
    dynamic video recognition. In ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] Ziwei Zheng, Le Yang, Yulin Wang, Miao Zhang, Lijun He, Gao Huang, and
    Fan Li. Dynamic spatial focus for efficient compressed video action recognition.
    IEEE TCSVT, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, and Larry S Davis. 2d or
    not 2d? adaptive 3d convolution selection for efficient video recognition. In
    CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    CVPR, pages 580–587, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based
    convolutional networks for accurate object detection and segmentation. TPAMI,
    38(1):142–158, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid
    pooling in deep convolutional networks for visual recognition. TPAMI, 37(9):1904–1916,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Ross Girshick. Fast r-cnn. In ICCV, pages 1440–1448, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. NeurIPS, 28,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. TPAMI, 39(06):1137–1149,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection
    via region-based fully convolutional networks. NeurIPS, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian
    Sun. Light-head r-cnn: In defense of two-stage object detector. arXiv preprint
    arXiv:1711.07264, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
    and Serge Belongie. Feature pyramid networks for object detection. In CVPR, pages
    2117–2125, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4:
    Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In
    CVPR, pages 7263–7271, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. You only learn one
    representation: Unified network for multiple tasks. arXiv preprint arXiv:2105.04206,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
    Focal loss for dense object detection. In ICCV, pages 2980–2988, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints.
    In ECCV, pages 734–750, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl. Bottom-up object
    detection by grouping extreme and center points. In CVPR, pages 850–859, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points.
    arXiv preprint arXiv:1904.07850, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
    Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers.
    In ECCV, pages 213–229\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
    Deformable detr: Deformable transformers for end-to-end object detection. In ICLR,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Christopher J Holder and Muhammad Shafique. On efficient real-time semantic
    segmentation: a survey. arXiv preprint arXiv:2206.08605, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya
    Jia. Pyramid scene parsing network. In CVPR, pages 2881–2890, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep
    convolutional encoder-decoder architecture for image segmentation. TPAMI, 39(12):2481–2495,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello.
    Enet: A deep neural network architecture for real-time semantic segmentation.
    arXiv preprint arXiv:1606.02147, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Abhishek Chaurasia and Eugenio Culurciello. Linknet: Exploiting encoder
    representations for efficient semantic segmentation. In 2017 IEEE visual communications
    and image processing (VCIP), pages 1–4\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh
    Hajishirzi. Espnet: Efficient spatial pyramid of dilated convolutions for semantic
    segmentation. In ECCV, pages 552–568, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng,
    and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic
    segmentation. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] Eduardo Romera, José M Alvarez, Luis M Bergasa, and Roberto Arroyo. Erfnet:
    Efficient residual factorized convnet for real-time semantic segmentation. IEEE
    Transactions on Intelligent Transportation Systems, 19(1):263–272, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Gen Li and Joongkyu Kim. Dabnet: Depth-wise asymmetric bottleneck for
    real-time semantic segmentation. In BMVC, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, and Jing-Jhih Lin. Efficient
    dense modules of asymmetric convolution for real-time semantic segmentation. In
    ACM MM Asia, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Ivan Krešo, Josip Krapac, and Siniša Šegvić. Efficient ladder-style densenets
    for semantic segmentation of large images. IEEE Transactions on Intelligent Transportation
    Systems, 22(8):4951–4961, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and
    Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image
    segmentation. In ECCV, pages 801–818, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Rudra PK Poudel, Stephan Liwicki, and Roberto Cipolla. Fast-scnn: Fast
    semantic segmentation network. In BMVC, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] Juntang Zhuang, Junlin Yang, Lin Gu, and Nicha Dvornek. Shelfnet for
    fast semantic segmentation. In ICCVW, pages 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] Marin Orsic, Ivan Kreso, Petra Bevandic, and Sinisa Segvic. In defense
    of pre-trained imagenet architectures for real-time semantic segmentation of road-driving
    images. In CVPR, pages 12607–12616, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Rudra PK Poudel, Ujwal Bonde, Stephan Liwicki, and Christopher Zach.
    Contextnet: Exploring context and detail for semantic segmentation in real-time.
    In BMVC, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong
    Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic
    segmentation. IJCV, 129:3051–3068, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng
    Luo, and Xiaolin Wei. Rethinking bisenet for real-time semantic segmentation.
    In CVPR, pages 9716–9725, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya
    Jia. Icnet for real-time semantic segmentation on high-resolution images. In ECCV,
    pages 405–420, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Yu Wang, Quan Zhou, Jia Liu, Jian Xiong, Guangwei Gao, Xiaofu Wu, and
    Longin Jan Latecki. Lednet: A lightweight encoder-decoder network for real-time
    semantic segmentation. In ICIP, pages 1860–1864\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Hanchao Li, Pengfei Xiong, Haoqiang Fan, and Jian Sun. Dfanet: Deep feature
    aggregation for real-time semantic segmentation. In CVPR, pages 9522–9531, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin,
    Kate Saenko, and Stan Sclaroff. Real-time semantic segmentation with fast attention.
    IEEE Robotics and Automation Letters, 6(1):263–270, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] Albert Shaw, Daniel Hunter, Forrest Landola, and Sammy Sidhu. Squeezenas:
    Fast neural architecture search for faster semantic segmentation. In ICCVW, pages
    0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua,
    Alan L Yuille, and Li Fei-Fei. Auto-deeplab: Hierarchical neural architecture
    search for semantic image segmentation. In CVPR, pages 82–92, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang
    Wang. Fasterseg: Searching for faster real-time semantic segmentation. In ICLR,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification
    is not all you need for semantic segmentation. NeurIPS, 34:17864–17875, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao
    Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic
    segmentation from a sequence-to-sequence perspective with transformers. In CVPR,
    pages 6881–6890, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez,
    and Ping Luo. Segformer: Simple and efficient design for semantic segmentation
    with transformers. NeurIPS, 34:12077–12090, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Yifan Zhang, Bo Pang, and Cewu Lu. Semantic segmentation by early region
    proxy. In CVPR, pages 1258–1268, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and
    Rohit Girdhar. Masked-attention mask transformer for universal image segmentation.
    In CVPR, pages 1290–1299, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] Huiyan Zhang, Hao Sun, Wengang Ao, and Georgi Dimirovski. A survey on
    instance segmentation: Recent advances and challenges. Int. J. Innov. Comput.
    Inf. Control, 17:1041–1053, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In ICCV, pages 2961–2969, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff,
    Peng Wang, and Hartwig Adam. Masklab: Instance segmentation by refining object
    detection with semantic and direction features. In CVPR, pages 4013–4022, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang
    Wang. Mask scoring r-cnn. In CVPR, pages 6409–6418, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation
    network for instance segmentation. In CVPR, pages 8759–8768, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang
    Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade
    for instance segmentation. In CVPR, pages 4974–4983, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time
    instance segmentation. In ICCV, pages 9157–9166, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, and Youliang
    Yan. Blendmask: Top-down meets bottom-up for instance segmentation. In CVPR, pages
    8573–8581, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Xinlei Chen, Ross Girshick, Kaiming He, and Piotr Dollár. Tensormask:
    A foundation for dense object segmentation. In ICCV, pages 2061–2069, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] Ayten Ozge Akmandor, YIN Hongxu, and Niraj K Jha. Smart, secure, yet
    energy-efficient, internet-of-things sensors. IEEE Transactions on Multi-Scale
    Computing Systems, 4(4):914–930, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] Rahul Mishra, Hari Prabhat Gupta, and Tanima Dutta. A survey on deep
    neural network compression: Challenges, overview, and solutions. arXiv preprint
    arXiv:2010.03954, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression
    and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] Sheng Xu, Anran Huang, Lei Chen, and Baochang Zhang. Convolutional neural
    network pruning: A survey. In 2020 39th Chinese Control Conference (CCC), pages
    7458–7463\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang.
    Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing,
    461:370–403, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
    Exploiting linear structure within convolutional networks for efficient evaluation.
    NeurIPS, 27, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional
    neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,
    Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng
    Liu, Zhaoning Zhang, and Yu Liu. Correlation congruence for knowledge distillation.
    In ICCV, pages 5007–5016, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Jacob Søgaard Larsen and Line Clemmensen. Weight sharing and deep learning
    for spectral data. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), pages 4227–4231\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, and Minje
    Park. Pvanet: Deep but lightweight neural networks for real-time object detection.
    arXiv preprint arXiv:1608.08021, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating
    very deep neural networks. In ICCV, pages 1389–1397, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and
    Qi Tian. Variational convolutional neural network pruning. In CVPR, pages 2780–2789,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning
    method for deep neural network compression. In ICCV, pages 5058–5066, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] Loc N Huynh, Youngki Lee, and Rajesh Krishna Balan. Deepmon: Mobile gpu-based
    deep learning framework for continuous vision applications. In Proceedings of
    the 15th Annual International Conference on Mobile Systems, Applications, and
    Services, pages 82–95, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz,
    and William J Dally. Eie: Efficient inference engine on compressed deep neural
    network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression
    for deep learning. NeurIPS, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler, and William J Dally.
    Scnn: An accelerator for compressed-sparse convolutional neural networks. ACM
    SIGARCH computer architecture news, 45(2):27–40, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zipping via layer-wise
    neuron sharing. NeurIPS, 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
    Deep learning with limited numerical precision. In ICML, pages 1737–1746\. PMLR,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented
    approximation of convolutional neural networks. arXiv preprint arXiv:1604.03168,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Manu Mathew, Kumar Desappan, Pramod Kumar Swami, and Soyeb Nagori. Sparse,
    quantized, full frame cnn for low power embedded devices. In CVPR Workshops, pages
    11–19, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Binarized neural networks: Training deep neural networks with weights
    and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep
    convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized
    convolutional neural networks for mobile devices. In CVPR, pages 4820–4828, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin
    Chen. Compressing neural networks with the hashing trick. In ICML, pages 2285–2294\.
    PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. arXiv
    preprint arXiv:1510.00149, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker.
    Learning efficient object detection models with knowledge distillation. NeurIPS,
    30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] Feng Zhang, Xiatian Zhu, and Mao Ye. Fast human pose estimation. In CVPR,
    pages 3517–3526, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention:
    Improving the performance of convolutional neural networks via attention transfer.
    In ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Jonghwan Mun, Kimin Lee, Jinwoo Shin, and Bohyung Han. Learning to specialize
    with knowledge distillation for visual question answering. NeurIPS, 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network:
    Network compression via factor transfer. NeurIPS, 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie
    Yan, and Xiaolin Hu. Knowledge distillation via route constrained optimization.
    In ICCV, pages 1345–1354, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] Kunran Xu, Lai Rui, Yishi Li, and Lin Gu. Feature normalized knowledge
    distillation for image classification. In ECCV, pages 664–680\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] Xiaobo Wang, Tianyu Fu, Shengcai Liao, Shuo Wang, Zhen Lei, and Tao Mei.
    Exclusivity-consistency regularized knowledge distillation for face recognition.
    In ECCV, pages 325–342\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, and Gao
    Huang. Towards learning spatially discriminative feature representations. In ICCV,
    pages 1326–1335, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Chaofei Wang, Shaowei Zhang, Shiji Song, and Gao Huang. Learn from the
    past: Experience ensemble knowledge distillation. In ICPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift from knowledge
    distillation: Fast optimization, network minimization and transfer learning. In
    CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised knowledge
    distillation using singular value decomposition. In ECCV, pages 335–350, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] Nikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. Heterogeneous
    knowledge distillation using information flow modeling. In CVPR, pages 2339–2348,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] Yufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li,
    and Yunqiang Duan. Knowledge distillation via instance relationship graph. In
    CVPR, pages 7096–7104, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation.
    In ICCV, pages 1365–1374, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge
    distillation. In CVPR, pages 3967–3976, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu. Deep mutual
    learning. In CVPR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] Xiatian Zhu, Shaogang Gong, et al. Knowledge distillation by on-the-fly
    native ensemble. NeurIPS, 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin
    Hu, and Ping Luo. Online knowledge distillation via collaborative learning. In
    CVPR, pages 11020–11029, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] Devesh Walawalkar, Zhiqiang Shen, and Marios Savvides. Online ensemble
    model compression using knowledge distillation. In ECCV, pages 18–35\. Springer,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online
    knowledge distillation with diverse peers. In AAAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and
    Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural
    networks via self distillation. In ICCV, pages 3713–3722, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy. Learning lightweight
    lane detection cnns by self attention distillation. In ICCV, pages 1013–1021,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation:
    Teacher-student optimization in one generation. In CVPR, pages 2859–2868, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] Zhimao Peng, Zechao Li, Junge Zhang, Yan Li, Guo-Jun Qi, and Jinhui Tang.
    Few-shot image recognition with knowledge transfer. In ICCV, pages 441–449, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] Chaofei Wang, Qisen Yang, Rui Huang, Shiji Song, and Gao Huang. Efficient
    knowledge distillation from model checkpoints. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] Chaofei Wang, Ke Yang, Shaowei Zhang, Gao Huang, and Shiji Song. Tc3kd:
    Knowledge distillation via teacher-student cooperative curriculum customization.
    Neurocomputing, 508:284–292, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] Yushu Feng, Huan Wang, Haoji Roland Hu, Lu Yu, Wei Wang, and Shiyan Wang.
    Triplet distillation for deep face recognition. In ICIP, pages 808–812\. IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] Nuno C Garcia, Pietro Morerio, and Vittorio Murino. Modality distillation
    with multiple stream networks for action recognition. In ECCV, pages 103–118,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul Sukthankar.
    D3d: Distilled 3d networks for video action recognition. In WACV, pages 625–634,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao
    Mei. Relation distillation networks for video object detection. In ICCV, pages
    7023–7032, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, and
    Erjin Zhou. General instance distillation for object detection. In CVPR, pages
    7842–7851, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong
    Wang. Structured knowledge distillation for semantic segmentation. In CVPR, pages
    2604–2613, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] Jianbo Jiao, Yunchao Wei, Zequn Jie, Honghui Shi, Rynson WH Lau, and
    Thomas S Huang. Geometry-aware distillation for indoor semantic segmentation.
    In CVPR, pages 2869–2878, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and Yongchao Xu. Intra-class
    feature variation distillation for semantic segmentation. In ECCV, pages 346–362\.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, and Chen Change Loy.
    Inter-region affinity distillation for road marking segmentation. In CVPR, pages
    12486–12495, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning
    monocular depth by distilling cross-domain stereo networks. In ECCV, pages 484–500,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net: Multi-tasks
    guided prediction-and-distillation network for simultaneous depth estimation and
    scene parsing. In CVPR, pages 675–684, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning
    monocular depth estimation infusing traditional stereo knowledge. In CVPR, pages
    9799–9809, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, and Elisa Ricci. Refine
    and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised
    monocular depth estimation. In CVPR, pages 9768–9777, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] Mihai Pirvu, Victor Robu, Vlad Licaret, Dragos Costea, Alina Marcu, Emil
    Slusanschi, Rahul Sukthankar, and Marius Leordeanu. Depth distillation: unsupervised
    metric depth estimation for uavs by finding consensus between kinematics, optical
    flow and deep learning. In CVPR, pages 3215–3223, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] Yiran Wang, Xingyi Li, Min Shi, Ke Xian, and Zhiguo Cao. Knowledge distillation
    for fast and accurate monocular depth estimation on mobile devices. In CVPR, pages
    2457–2465, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] Wei Chen, Yu Liu, Nan Pu, Weiping Wang, Li Liu, and Michael S Lew. Feature
    estimations based correlation distillation for incremental image retrieval. TMM,
    24:1844–1856, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] Young Kyun Jang, Geonmo Gu, Byungsoo Ko, Isaac Kang, and Nam Ik Cho.
    Deep hash distillation for image retrieval. In ECCV, pages 354–371\. Springer,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu,
    and Zheng-Jun Zha. Object relational graph with teacher-recommended learning for
    video captioning. In CVPR, pages 13278–13288, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan
    Adeli, and Juan Carlos Niebles. Spatio-temporal graph for video captioning with
    knowledge distillation. In CVPR, pages 10870–10879, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] Jingjing Dong, Zhenzhen Hu, and Yuanen Zhou. Revisiting knowledge distillation
    for image captioning. In CAAI International Conference on Artificial Intelligence,
    pages 613–625\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] Chenrui Zhang and Yuxin Peng. Better and faster: Knowledge transfer from
    multiple self-supervised learning tasks via graph distillation for video classification.
    In IJCAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] Shweta Bhardwaj, Mukundhan Srinivasan, and Mitesh M Khapra. Efficient
    video classification using fewer frames. In CVPR, pages 354–363, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisit
    knowledge distillation: a teacher-free framework. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi,
    and Sagar Jain. Understanding and improving knowledge distillation. arXiv preprint
    arXiv:2002.03532, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Xu Cheng, Zhefan Rao, Yilan Chen, and Quanshi Zhang. Explaining knowledge
    distillation by quantifying the knowledge. In CVPR, pages 12925–12935, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] V Lebedev, Y Ganin, M Rakhuba, I Oseledets, and V Lempitsky. Speeding-up
    convolutional neural networks using fine-tuned cp-decomposition. In ICLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, and E Weinan. Convolutional
    neural networks with low-rank regularization. In ICLR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando
    De Freitas. Predicting parameters in deep learning. NeurIPS, 26, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] Qingchen Zhang, Laurence T Yang, Xingang Liu, Zhikui Chen, and Peng Li.
    A tucker deep computation model for mobile multimedia feature learning. ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM), 13(3s):1–18,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and
    Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with applications
    in person attribute classification. In CVPR, pages 5334–5343, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
    Ramabhadran. Low-rank matrix factorization for deep neural network training with
    high-dimensional output targets. In 2013 IEEE international conference on acoustics,
    speech and signal processing, pages 6655–6659\. IEEE, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] Frederick Tung and Greg Mori. Clip-q: Deep network compression learning
    by in-parallel pruning-quantization. In CVPR, pages 7873–7882, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via
    distillation and quantization. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture
    search. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining
    Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, et al. Fbnetv2: Differentiable neural
    architecture search for spatial and channel dimensions. In CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all:
    Train one network and specialize it for efficient deployment. In ICLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] Krishna Teja Chitty-Venkata and Arun K Somani. Neural architecture search
    survey: A hardware perspective. ACM Computing Surveys, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet:
    Tiny deep learning on iot devices. NeurIPS, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
    and Olivier Temam. Diannao: A small-footprint high-throughput accelerator for
    ubiquitous machine-learning. ACM SIGARCH Computer Architecture News, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful
    visual performance model for multicore architectures. Communications of the ACM,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
    Jerger, and Andreas Moshovos. Cnvlutin: Ineffectual-neuron-free deep neural network
    computing. ACM SIGARCH Computer Architecture News, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] Liqiang Lu, Jiaming Xie, Ruirui Huang, Jiansong Zhang, Wei Lin, and Yun
    Liang. An efficient hardware accelerator for sparse convolutional neural networks
    on fpgas. In FCCM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] Xinyi Zhang, Weiwen Jiang, Yiyu Shi, and Jingtong Hu. When neural architecture
    search meets hardware implementation: from hardware awareness to co-design. In
    ISVLSI, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] Weiwen Jiang, Lei Yang, Sakyasingha Dasgupta, Jingtong Hu, and Yiyu Shi.
    Standing on the shoulders of giants: Hardware and neural architecture co-search
    with hot start. IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] Mohamed S Abdelfattah, Łukasz Dudziak, Thomas Chau, Royson Lee, Hyeji
    Kim, and Nicholas D Lane. Best of both worlds: Automl codesign of a cnn and its
    hardware accelerator. In DAC, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song Han. Mcunetv2: Memory-efficient
    patch-based inference for tiny deep learning. In NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song
    Han. On-device training under 256kb memory. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, and G Edward Suh.
    Boosting the performance of cnn accelerators with dynamic fine-grained channel
    gating. In MICRO, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng
    Jing, and Xiaoyao Liang. Drq: dynamic region-based quantization for deep neural
    network acceleration. In ISCA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian
    Verhelst. Processor architecture optimization for spatially dynamic neural networks.
    In VLSI-SoC, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] Zhe Zhou, Junlin Liu, Zhenyu Gu, and Guangyu Sun. Energon: Toward efficient
    acceleration of transformers using dynamic sparse attention. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu, and Yun Fu.
    Image as set of points. In ICLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] Yuki Tatsunami and Masato Taki. Sequencer: Deep LSTM for image classification.
    In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[434] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and Enhua Wu. Vision GNN:
    An image is worth graph of nodes. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[435] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang
    Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al.
    Image as a foreign language: Beit pretraining for all vision and vision-language
    tasks. arXiv preprint arXiv:2208.10442, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[436] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng
    Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new
    foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[437] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable
    feature pyramid architecture for object detection. In CVPR, pages 7036–7045, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[438] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and
    Geoffrey Hinton. A unified sequence interface for vision tasks. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[439] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothée
    Masquelier, and Anthony Maida. Deep learning in spiking neural networks. Neural
    networks, 111:47–63, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[440] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum
    contrast for unsupervised visual representation learning. In CVPR, pages 9729–9738,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[441] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
    Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000–16009,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[442] Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting
    locally supervised learning: an alternative to end-to-end training. In ICLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[443] Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang,
    and Yi Yang. Automated progressive learning for efficient training of vision transformers.
    In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[444] Zanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang, Yue Cao, and Gao Huang.
    Deep incubation: Training large models by divide-and-conquering. In ICCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[445] Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and
    Gao Huang. Efficienttrain: Exploring generalized curriculum learning for training
    visual backbones. In ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
