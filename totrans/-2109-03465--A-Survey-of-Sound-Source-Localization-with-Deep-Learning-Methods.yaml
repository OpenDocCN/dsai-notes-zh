- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:51:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:51:40'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.03465] A Survey of Sound Source Localization with Deep Learning Methods'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.03465] 深度学习方法的声源定位综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.03465](https://ar5iv.labs.arxiv.org/html/2109.03465)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2109.03465](https://ar5iv.labs.arxiv.org/html/2109.03465)
- en: A Survey of Sound Source Localization with Deep Learning Methods
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法的声源定位综述
- en: Pierre-Amaury Grumiaux
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 皮埃尔-阿莫里·格吕缪
- en: Nantes Université, École Centrale Nantes, CNRS, LS2N
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Nantes Université, École Centrale Nantes, CNRS, LS2N
- en: 2 chemin de la Houssinière
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2 chemin de la Houssinière
- en: F-44332 Nantes, France
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: F-44332 Nantes, 法国
- en: pierreamaury.grumiaux@gmail.com & Srđan Kitić
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: pierreamaury.grumiaux@gmail.com & Srđan Kitić
- en: Orange Labs
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Orange Labs
- en: 4 Rue du Clos Courtel
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 4 Rue du Clos Courtel
- en: 35510 Cesson-Sévigné, France
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 35510 Cesson-Sévigné, 法国
- en: srdan.kitic@orange.com & Laurent Girin
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: srdan.kitic@orange.com & Laurent Girin
- en: Univ. Grenoble Alpes, Grenoble-INP, GIPSA-lab
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Univ. Grenoble Alpes, Grenoble-INP, GIPSA-lab
- en: 11 Rue des Mathématiques
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 11 Rue des Mathématiques
- en: 38400 Saint-Martin-d’Hères, France
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 38400 Saint-Martin-d’Hères, 法国
- en: laurent.girin@grenoble-inp.fr & Alexandre Guérin
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: laurent.girin@grenoble-inp.fr & Alexandre Guérin
- en: Orange Labs
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Orange Labs
- en: 4 Rue du Clos Courtel
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 4 Rue du Clos Courtel
- en: 35510 Cesson-Sévigné, France
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 35510 Cesson-Sévigné, 法国
- en: alexandre.guerin@orange.com During the writing of this paper, Pierre-Amaury
    Grumiaux was also at Orange Labs, 4 Rue du Clos Courtel, F-35510 Cesson-Sévigné,
    France, and at Univ. Grenoble Alpes, Grenoble-INP, CNRS, GIPSA-lab, 11 Rue des
    Mathématiques, F-38400 Saint-Martin-d’Hères, France.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: alexandre.guerin@orange.com 在撰写本文期间，皮埃尔-阿莫里·格吕缪还在 Orange Labs，4 Rue du Clos
    Courtel, F-35510 Cesson-Sévigné, 法国，以及 Univ. Grenoble Alpes, Grenoble-INP, CNRS,
    GIPSA-lab, 11 Rue des Mathématiques, F-38400 Saint-Martin-d’Hères, 法国。
- en: Abstract
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This article is a survey of deep learning methods for single and multiple sound
    source localization, with a focus on sound source localization in indoor environments,
    where reverberation and diffuse noise are present. We provide an extensive topography
    of the neural network-based sound source localization literature in this context,
    organized according to the neural network architecture, the type of input features,
    the output strategy (classification or regression), the types of data used for
    model training and evaluation, and the model training strategy. Tables summarizing
    the literature survey are provided at the end of the paper, allowing a quick search
    of methods with a given set of target characteristics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文综述了单声源和多声源定位的深度学习方法，重点讨论了在存在混响和扩散噪声的室内环境中的声源定位。我们提供了在这一背景下基于神经网络的声源定位文献的广泛拓扑，按神经网络架构、输入特征类型、输出策略（分类或回归）、用于模型训练和评估的数据类型以及模型训练策略进行组织。文献综述的表格在本文末尾提供，便于快速查找具有特定目标特征的方法。
- en: 1 Introduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Sound source localization (SSL) is the problem of estimating the position of
    one or several sound sources relative to some arbitrary reference position, which
    is generally the position of the recording microphone array, based on the recorded
    multichannel acoustic signals. In most practical cases, SSL is simplified to the
    estimation of the sources’ direction of arrival (DoA), *i.e.*, it focuses on the
    estimation of azimuth and elevation angles, without estimating the distance to
    the microphone array (therefore, unless otherwise specified, in this article we
    use the terms “SSL” and “DoA estimation” interchangeably). SSL has numerous practical
    applications – for instance, in source separation, e.g., (Chazan et al., [2019](#bib.bib47)),
    automatic speech recognition (ASR), e.g., (Lee et al., [2016](#bib.bib176)), speech
    enhancement, e.g., (Xenaki et al., [2018](#bib.bib357)), human-robot interaction,
    e.g., (Li et al., [2016b](#bib.bib186)), noise control, e.g., (Chiariotti et al.,
    [2019](#bib.bib48)), and room acoustic analysis, e.g., (Amengual Garí et al.,
    [2017](#bib.bib8)). As detailed in the following, in this paper, we focus on sound
    sources in the audible range (typically speech and audio signals) in indoor (office
    or domestic) environments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 声源定位（SSL）是指基于录制的多通道声学信号，估计一个或多个声源相对于某个任意参考位置（通常是录音麦克风阵列的位置）的过程。在大多数实际情况下，SSL
    被简化为声源到达方向（DoA）的估计，*即*，它专注于估计方位角和仰角，而不估计到麦克风阵列的距离（因此，除非另有说明，在本文中我们将“SSL”和“DoA
    估计”交替使用）。SSL 有许多实际应用——例如，在源分离中，如（Chazan 等，[2019](#bib.bib47)）、自动语音识别（ASR）中，如（Lee
    等，[2016](#bib.bib176)）、语音增强中，如（Xenaki 等，[2018](#bib.bib357)）、人机交互中，如（Li 等，[2016b](#bib.bib186)）、噪声控制中，如（Chiariotti
    等，[2019](#bib.bib48)）和房间声学分析中，如（Amengual Garí 等，[2017](#bib.bib8)）。如以下详细介绍，本文专注于室内（办公室或家庭）环境中的可听范围内的声源（通常是语音和音频信号）。
- en: Although SSL is a longstanding and widely researched topic (Gerzon, [1992](#bib.bib97);
    DiBiase et al., [2001](#bib.bib70); Argentieri et al., [2015](#bib.bib11); Cobos
    et al., [2017](#bib.bib54); Benesty et al., [2008](#bib.bib18); Knapp and Carter,
    [1976](#bib.bib153); Brandstein and Ward, [2001](#bib.bib31); Nehorai and Paldi,
    [1994](#bib.bib219); Hickling et al., [1993](#bib.bib126)), it remains a very
    challenging problem to date. Traditional SSL methods are based on signal/channel
    models and signal processing (SP) techniques. Although they have shown notable
    advances in the domain over the years, they are known to perform poorly in difficult
    yet common scenarios where noise, reverberation, and several simultaneously emitting
    sound sources may be present (Blandin et al., [2012](#bib.bib27); Evers et al.,
    [2020](#bib.bib83)). In the last decade, the potential of data-driven deep learning
    (DL) techniques for addressing such difficult scenarios has received an increasing
    interest. As a result, an increasing number of SSL systems based on deep neural
    networks (DNNs) have been proposed in the recent years. Most of the reported works
    have indicated the superiority of DNN-based SSL methods over conventional (i.e.,
    SP-based) SSL methods. For example, Chakrabarty and Habets ([2017a](#bib.bib41))
    showed that, in low signal-to-noise ratio conditions, using a CNN led to a two-fold
    increase in overall DoA classification accuracy compared to using the conventional
    method called steered response power with phase transform (SRP-PHAT) (see Section
    [3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). In Perotin et al. ([2018b](#bib.bib246)), the authors
    were able to obtain a $25\%$ increase of DoA classification accuracy when using
    a convolutional recurrent neural network (CRNN) over a method based on independent
    component analysis (ICA). Finally Adavanne et al. ([2018](#bib.bib1)) proved that
    employing a CRNN can reduce the average angular error by $50\%$ in reverberant
    conditions compared to the conventional MUSIC algorithm (see Section [3](#S3 "3
    Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 SSL 是一个长期以来被广泛研究的课题（Gerzon, [1992](#bib.bib97)；DiBiase et al., [2001](#bib.bib70)；Argentieri
    et al., [2015](#bib.bib11)；Cobos et al., [2017](#bib.bib54)；Benesty et al., [2008](#bib.bib18)；Knapp
    和 Carter, [1976](#bib.bib153)；Brandstein 和 Ward, [2001](#bib.bib31)；Nehorai 和
    Paldi, [1994](#bib.bib219)；Hickling et al., [1993](#bib.bib126)），但至今仍然是一个非常具有挑战性的问题。传统的
    SSL 方法基于信号/通道模型和信号处理（SP）技术。尽管这些方法在这些年中取得了显著进展，但在噪声、混响和多个同时发射的声源等困难且常见的场景下，它们的表现较差（Blandin
    et al., [2012](#bib.bib27)；Evers et al., [2020](#bib.bib83)）。在过去十年中，数据驱动的深度学习（DL）技术在处理这些困难场景方面引起了越来越多的关注。因此，近年来提出了越来越多基于深度神经网络（DNN）的
    SSL 系统。大多数报道的研究表明，基于 DNN 的 SSL 方法优于传统的（即，基于 SP 的）SSL 方法。例如，Chakrabarty 和 Habets
    ([2017a](#bib.bib41)) 证明，在低信噪比条件下，使用 CNN 比使用传统的称为导向响应功率与相位变换（SRP-PHAT）的方法提高了两倍的
    DoA 分类准确率（见第 [3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") 节）。在 Perotin et al. ([2018b](#bib.bib246)) 的研究中，作者能够在使用卷积递归神经网络（CRNN）时，比使用基于独立成分分析（ICA）的方法提高
    $25\%$ 的 DoA 分类准确率。最后，Adavanne et al. ([2018](#bib.bib1)) 证明，采用 CRNN 可以在混响条件下将平均角度误差减少
    $50\%$，相比于传统的 MUSIC 算法（见第 [3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound
    Source Localization with Deep Learning Methods") 节）。
- en: This kind of results has further motivated the expansion of scientific papers
    on DL applied to SSL. In the meantime, there has been no comprehensive survey
    of the existing approaches, which would be very useful for researchers and practitioners
    in the domain. Although we can find reviews mostly focused on conventional methods,
    e.g., (Argentieri et al., [2015](#bib.bib11); Cobos et al., [2017](#bib.bib54);
    Evers et al., [2020](#bib.bib83); Gannot et al., [2019](#bib.bib91)), to the best
    of our knowledge only a very few have explicitly targeted SSL with DL methods.
    Ahmad et al. ([2021](#bib.bib6)) presented a short survey of several existing
    DL models and datasets for SSL before proposing a DL architecture of their own.
    Bianco et al. ([2019](#bib.bib24)) and Purwins et al. ([2019](#bib.bib261)) presented
    an interesting overview of machine learning applied to various problems in audio
    and acoustics. Nevertheless, only a short portion of each of these two reviews
    is dedicated to SSL with DNNs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结果进一步激发了关于深度学习应用于声源定位的科学论文的扩展。同时，现有方法尚未有全面的综述，这对领域中的研究人员和从业者非常有用。尽管我们可以找到主要关注传统方法的综述，例如（Argentieri
    et al., [2015](#bib.bib11); Cobos et al., [2017](#bib.bib54); Evers et al., [2020](#bib.bib83);
    Gannot et al., [2019](#bib.bib91)），但据我们所知，只有极少数专门针对深度学习方法的声源定位。Ahmad et al. ([2021](#bib.bib6))
    在提出他们自己的深度学习架构之前，简要综述了几种现有的深度学习模型和数据集。Bianco et al. ([2019](#bib.bib24)) 和 Purwins
    et al. ([2019](#bib.bib261)) 对应用于音频和声学的各种问题的机器学习进行了有趣的概述。然而，这两篇综述中仅有短短一部分专门讨论了深度神经网络（DNNs）在声源定位中的应用。
- en: 1.1 Aim of the paper
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 论文的目的
- en: The goal of this paper is to fill this gap, and to provide a thorough survey
    of the SSL literature using DL techniques. More precisely, we examined and review
    156 papers published from 2011 to 2021\. We classify and discuss the different
    approaches in terms of characteristics of the employed methods and addressed configurations
    (e.g., single-source vs. multi-source localization setup or neural network architecture;
    the exact list is given in Section [1.3](#S1.SS3 "1.3 Outline of the paper ‣ 1
    Introduction ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    In other words, we present a taxonomy of the DL-based SSL literature published
    in the last decade. At the end of the paper, we present a summary of this survey
    in the form of four tables (one for the period 2011–2018, and one for each of
    the years 2019, 2020 and 2021). All of the methods that we reviewed are reported
    in these tables with a summary of their characteristics presented in different
    columns. This enables the reader to rapidly select the subset of methods having
    a given set of characteristics, if they are interested in that particular type
    of method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是填补这一空白，并使用深度学习技术对声源定位（SSL）文献进行彻底的综述。更具体地说，我们检查和回顾了2011年至2021年间发表的156篇论文。我们根据所用方法的特点和所涉及的配置（例如，单源与多源定位设置或神经网络架构；详细列表见第[1.3节](#S1.SS3
    "1.3 论文大纲 ‣ 1 引言 ‣ 基于深度学习的方法的声源定位综述")）对不同的方法进行分类和讨论。换句话说，我们展示了过去十年中基于深度学习的声源定位文献的分类法。在论文的末尾，我们以四个表格的形式总结了这次综述（一个针对2011–2018年，另外三个分别针对2019年、2020年和2021年）。我们回顾的所有方法都在这些表格中列出，并以不同的列展示其特点。这使读者能够快速选择具有特定特点的子集方法，如果他们对那种特定类型的方法感兴趣的话。
- en: Note that in this survey paper, we do not aim to evaluate and compare the performance
    of the different systems. Due to the large number of DNN-based SSL papers and
    the diversity of configurations, such a contribution would be very difficult and
    cumbersome (albeit very useful), especially because the discussed systems are
    often trained and evaluated on different datasets. As we will see later, listing
    and commenting on these different datasets is, however, part of our survey effort.
    Note also that we do not consider SSL systems that exploit other modalities in
    addition to sound, e.g., audio-visual systems (Ban et al., [2018](#bib.bib16);
    Wu et al., [2021a](#bib.bib352); Masuyama et al., [2020](#bib.bib208)). Finally,
    we do consider DL-based methods for joint sound event localization and detection
    (SELD), which is a combination of sound event detection (SED; here detection actually
    means classification) and SSL, and in that case, we focus on the localization
    task. In particular, we include in the review the SELD methods presented to the
    DCASE Challenge (and/or to the corresponding DCASE Workshop) in 2019, 2020 and
    2021 (see the DCASE website at [https://dcase.community/](https://dcase.community/)).
    One of the task of this challenge is precisely dedicated to SELD, which has contributed
    to make the DL-based SSL (and SED) problem a popular research topic over the recent
    years.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这篇综述论文中，我们的目标并不是评估和比较不同系统的性能。由于基于深度神经网络的自监督学习论文数量庞大且配置多样，这样的贡献将是非常困难且繁琐的（尽管非常有用），特别是因为讨论的系统通常是在不同的数据集上进行训练和评估的。如我们后续所见，列举并评论这些不同的数据集仍然是我们综述工作的一部分。另请注意，我们不考虑除了声音之外还利用其他模态的自监督学习系统，例如视听系统（Ban
    et al., [2018](#bib.bib16); Wu et al., [2021a](#bib.bib352); Masuyama et al.,
    [2020](#bib.bib208)）。最后，我们考虑基于深度学习的方法用于联合声音事件定位和检测（SELD），这是一种声音事件检测（SED；这里的检测实际上是指分类）和自监督学习的结合，在这种情况下，我们重点关注定位任务。特别地，我们在综述中包括了提交至
    DCASE 挑战（和/或相应的 DCASE 研讨会）的 SELD 方法，这些挑战分别在 2019、2020 和 2021 年举办（请参见 DCASE 网站
    [https://dcase.community/](https://dcase.community/)）。这个挑战的一个任务专门用于 SELD，这有助于使基于深度学习的自监督学习（和
    SED）问题在近年来成为一个热门研究话题。
- en: 1.2 General principle of DL-based SSL
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 基于深度学习的自监督学习（DL-based SSL）的基本原理
- en: '![Refer to caption](img/43b7e23b12a5b7a471da79629c99c253.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43b7e23b12a5b7a471da79629c99c253.png)'
- en: 'Figure 1: General pipeline of a DL-based SSL system.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于深度学习的自监督学习系统的一般流程。
- en: The general principle of DL-based SSL methods and systems can be schematized
    with a simple pipeline, as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1.2 General
    principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"). A multichannel input signal recorded with a microphone
    array is processed by a feature extraction module to provide input features. These
    input features are fed into a DNN, which delivers an estimate of the source location
    or DoA. As discussed later in the paper, a recent trend is to skip the feature
    extraction module to directly feed the network with multichannel raw data. In
    any case, the two fundamental reasons behind the design of such SSL are detailed
    below.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的自监督学习方法和系统的一般原理可以用一个简单的流程图示化，如图 [1](#S1.F1 "图 1 ‣ 1.2 基于深度学习的自监督学习的基本原理
    ‣ 1 引言 ‣ 深度学习方法下的声音源定位综述") 所示。一个通过麦克风阵列录制的多通道输入信号会被特征提取模块处理，以提供输入特征。这些输入特征被输入到深度神经网络中，该网络提供源位置或到达角度（DoA）的估计。正如本文后续讨论的那样，最近的趋势是跳过特征提取模块，直接将多通道原始数据输入网络。无论如何，这种自监督学习设计的两个基本原因如下所述。
- en: 'First, multichannel signals recorded with an array of $I$ microphones distributed
    in space contain information about the location of the source(s). Indeed, when
    the microphones are close to each other compared to their distance to the source(s),
    the microphone signal waveforms, although appearing similar from a distance, exhibit
    more or less notable and complex differences in terms of delay and amplitude,
    depending on the experimental setup. These interchannel differences are due to
    distinct propagation paths from the source to the different microphones, for both
    the direct path (line of sight between source and microphone) and the numerous
    reflections that compose the reverberation in an indoor environment. In other
    words, a source signal $s_{j}(t)$ is convolved with different room impulse responses
    (RIRs) $a_{i,j}(t)$, which depend on the source position, microphone position
    and directivity ($i$ denotes the microphone index in the array), and acoustic
    environment configuration (e.g., room shape):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，用空间中分布的 $I$ 个麦克风阵列录制的多通道信号包含了关于声源位置的信息。实际上，当麦克风相互靠近，与它们与声源之间的距离相比时，从远处看虽然麦克风信号波形看起来相似，但根据实验设置，在延迟和幅度方面会出现更多或更少显著和复杂的差异。这些通道间差异是由于从源到不同麦克风的传播路径不同，无论是直接路径（源与麦克风之间的视线）还是组成室内环境混响的众多反射。换句话说，源信号
    $s_{j}(t)$ 与不同的房间脉冲响应（RIRs） $a_{i,j}(t)$ 进行卷积，这些响应取决于源位置、麦克风位置和指向性（$i$ 表示阵列中的麦克风索引），以及声学环境配置（例如房间形状）：
- en: '|  | $\displaystyle x_{i}(t)=a_{i,j}(t)\star s_{j}(t)+n_{i}(t)=\sum_{\tau=0}^{T-1}a_{i,j}(\tau)s_{j}(t-\tau)+n_{i}(t),$
    |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{i}(t)=a_{i,j}(t)\star s_{j}(t)+n_{i}(t)=\sum_{\tau=0}^{T-1}a_{i,j}(\tau)s_{j}(t-\tau)+n_{i}(t),$
    |  | (1) |'
- en: 'where $x_{i}(n)$ denotes the resulting recorded signal at microphone $i$, $n_{i}(t)$
    is the noise signal at microphone $i$ (diffuse, “background” noise and possibly
    some sensor noise), and $\star$ denotes the convolution (note that we work with
    digital signals and $t$ and $\tau$ are discrete time indexes; $T$ is the effective
    length of the RIR). Therefore, the recorded signal contains information on the
    relative source-to-microphone array position. The microphone signals are often
    expressed in the time-frequency (TF) domain, using the short-term Fourier transform
    (STFT), where the convolution in Eq. ([1](#S1.E1 "In 1.2 General principle of
    DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")) is assumed to transform into a product between the STFT of
    the source signal $S_{j}(f,n)$ and the acoustic transfer function (ATF) $A_{i,j}(f)$,
    which is the (discrete) Fourier transform of the corresponding RIR and is thus
    encoding the source spatial information ($f$ denotes the frequency bin, and $n$
    is the STFT frame index) (Gannot et al., [2017](#bib.bib90); Vincent et al., [2018](#bib.bib341)):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}(n)$ 表示麦克风 $i$ 处的录制信号，$n_{i}(t)$ 是麦克风 $i$ 处的噪声信号（扩散的、“背景”噪声以及可能的一些传感器噪声），$\star$
    表示卷积（注意我们处理的是数字信号，$t$ 和 $\tau$ 是离散时间索引；$T$ 是 RIR 的有效长度）。因此，录制的信号包含了源与麦克风阵列之间相对位置的信息。麦克风信号通常在时间-频率（TF）域中表示，使用短时傅里叶变换（STFT），其中方程
    ([1](#S1.E1 "In 1.2 General principle of DL-based SSL ‣ 1 Introduction ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")) 中的卷积假定转化为源信号 $S_{j}(f,n)$
    和声学传递函数（ATF） $A_{i,j}(f)$ 之间的乘积，而 ATF 是相应 RIR 的（离散）傅里叶变换，因此编码了源的空间信息（$f$ 表示频率
    bin，$n$ 是 STFT 帧索引）（Gannot 等， [2017](#bib.bib90)；Vincent 等， [2018](#bib.bib341)）：
- en: '|  | $X_{i}(f,n)=A_{i,j}(f)S_{j}(f,n)+N_{i}(f,n).$ |  | (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $X_{i}(f,n)=A_{i,j}(f)S_{j}(f,n)+N_{i}(f,n).$ |  | (2) |'
- en: 'When several, say $J$, sources are present, the recorded signal is the sum
    of their contribution (plus the noise):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个，例如 $J$ 个声源时，录制信号是它们贡献的总和（加上噪声）：
- en: '|  | $x_{i}(t)=\sum_{j=1}^{J}a_{i,j}(t)\star s_{j}(t)+n_{i}(t).$ |  | (3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{i}(t)=\sum_{j=1}^{J}a_{i,j}(t)\star s_{j}(t)+n_{i}(t).$ |  | (3) |'
- en: 'This latter equation is often reformulated in the TF domain in matrix form:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程通常在 TF 域中以矩阵形式重新表述：
- en: '|  | $\mathbf{X}(f,n)=\mathbf{A}(f)\mathbf{S}(f,n)+\mathbf{N}(f,n),$ |  | (4)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}(f,n)=\mathbf{A}(f)\mathbf{S}(f,n)+\mathbf{N}(f,n),$ |  | (4)
    |'
- en: where $\mathbf{X}(f,n)=[X_{1}(f,n),...,X_{I}(f,n)]^{\top}$ is the microphone
    signal vector, $\mathbf{A}(f)$ is the matrix gathering the ATFs, $\mathbf{S}(f,n)=[S_{1}(f,n),...,S_{J}(f,n)]^{\top}$
    is the source signal vector, and $\mathbf{N}(f,n)=[N_{1}(f,n),...,N_{I}(f,n)]^{\top}$
    is the noise vector. In that multi-source case, the difficulty of the SSL problem
    is that the contributions of the different sources generally overlap in time.
    SSL then requires to proceed to some kind of source clustering, which is generally
    easier to proceed in the frequency or time-frequency domain due to the natural
    sparsity of audio sources in that domain (Rickard, [2002](#bib.bib268)). In this
    paper, we do not describe the foundations of source-to-microphone propagation
    in more detail. They can be found in several references on general acoustics,
    e.g., (Jacobsen and Juhl, [2013](#bib.bib137); Rossing, [2007](#bib.bib275)),
    room acoustics, e.g., (Kuttruff, [2016](#bib.bib166)), array signal processing,
    e.g., (Brandstein and Ward, [2001](#bib.bib31); Benesty et al., [2008](#bib.bib18);
    Jarrett et al., [2017](#bib.bib140); Rafaely, [2019](#bib.bib263)), speech enhancement
    and audio source separation, e.g., (Gannot et al., [2017](#bib.bib90); Vincent
    et al., [2018](#bib.bib341)), and many papers on conventional SSL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{X}(f,n)=[X_{1}(f,n),...,X_{I}(f,n)]^{\top}$ 是麦克风信号向量，$\mathbf{A}(f)$
    是汇聚 ATFs 的矩阵，$\mathbf{S}(f,n)=[S_{1}(f,n),...,S_{J}(f,n)]^{\top}$ 是源信号向量，而 $\mathbf{N}(f,n)=[N_{1}(f,n),...,N_{I}(f,n)]^{\top}$
    是噪声向量。在多源情况下，SSL 问题的难点在于不同源的贡献通常在时间上重叠。因此，SSL 需要进行某种形式的源聚类，这在频域或时频域中通常更容易进行，因为在这些领域音频源的自然稀疏性（Rickard,
    [2002](#bib.bib268)）。在本文中，我们不对源到麦克风传播的基础进行更详细的描述。有关详细信息，请参考一些关于一般声学的文献，例如，（Jacobsen
    和 Juhl, [2013](#bib.bib137)；Rossing, [2007](#bib.bib275)），房间声学，例如，（Kuttruff, [2016](#bib.bib166)），阵列信号处理，例如，（Brandstein
    和 Ward, [2001](#bib.bib31)；Benesty 等, [2008](#bib.bib18)；Jarrett 等, [2017](#bib.bib140)；Rafaely,
    [2019](#bib.bib263)），语音增强和音频源分离，例如，（Gannot 等, [2017](#bib.bib90)；Vincent 等, [2018](#bib.bib341)），以及许多关于传统
    SSL 的论文。
- en: The second reason for designing DNN-based SSL systems is that even if the relationship
    between the information contained in the multichannel signal and the location
    of the source(s) is generally complex (especially in a multisource reverberant
    and noisy configuration, see Eqs. ([3](#S1.E3 "In 1.2 General principle of DL-based
    SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")) and ([4](#S1.E4 "In 1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"))), DNNs are
    powerful models that are able to automatically identify and exploit this relationship,
    given that they are provided with a sufficiently large number of representative
    training examples. This ability of data-driven DL methods to replace conventional
    methods based on a signal/channel model and SP techniques —or at least a part
    of them, since the feature extractor module can be based on conventional processing—
    makes them attractive for addressing problems such as SSL. While some conventional
    methods can adapt to the observed signals, e.g., Dvorkind and Gannot ([2005](#bib.bib74));
    Li et al. ([2016c](#bib.bib187)); Laufer-Goldshtein et al. ([2020](#bib.bib171));
    Li et al. ([2016b](#bib.bib186)), they are all intrinsically based on certain
    (more or less plausible) modeling assumptions, which can limit their effectiveness
    when exposed to the complexity of real-world acoustics. Deep learning models do
    not *explicitly* impose any such assumptions, and instead they efficiently adapt
    to the presented training data. This is, however, also the major drawback of the
    DNN-based approaches, as they are less generic than traditional methods. A deep
    model designed for and trained in a given configuration (e.g., a given microphone
    array geometry) will not provide satisfying localization results if the setup
    changes (Liu et al., [2018](#bib.bib195); Le Moing et al., [2021](#bib.bib173)),
    unless some relevant adaptation method can be used, which is still an open problem
    in DL in general.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 设计基于 DNN 的 SSL 系统的第二个原因是，即使多通道信号中包含的信息与源位置之间的关系通常很复杂（特别是在多源混响和噪声环境下，参见 Eqs. ([3](#S1.E3
    "在 1.2 基于 DL 的 SSL 一般原理 ‣ 1 引言 ‣ 基于深度学习方法的声音源定位综述")) 和 ([4](#S1.E4 "在 1.2 基于 DL
    的 SSL 一般原理 ‣ 1 引言 ‣ 基于深度学习方法的声音源定位综述"))），DNN 是强大的模型，能够自动识别并利用这种关系，只要提供足够多的具有代表性的训练样本。数据驱动的
    DL 方法能够替代基于信号/通道模型和 SP 技术的传统方法——或至少其中的一部分，因为特征提取模块可以基于传统处理——这使得它们在解决如 SSL 等问题时具有吸引力。虽然一些传统方法可以适应观测到的信号，例如
    Dvorkind 和 Gannot ([2005](#bib.bib74)); Li 等 ([2016c](#bib.bib187)); Laufer-Goldshtein
    等 ([2020](#bib.bib171)); Li 等 ([2016b](#bib.bib186))，但它们都本质上基于某些（或多或少合理的）建模假设，这可能会限制它们在面对现实世界声学复杂性时的有效性。深度学习模型并不*明确*施加任何这样的假设，而是有效地适应所呈现的训练数据。然而，这也是基于
    DNN 的方法的主要缺陷，因为它们不如传统方法通用。为特定配置（例如，特定的麦克风阵列几何）设计和训练的深度模型，如果设置发生变化（Liu 等，[2018](#bib.bib195);
    Le Moing 等，[2021](#bib.bib173)），将无法提供令人满意的定位结果，除非可以使用一些相关的适应方法，这在 DL 中仍然是一个未解问题。
- en: 1.3 Outline of the paper
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 论文概述
- en: 'The remainder of the paper is organized as follows. In Section [2](#S2 "2 Acoustic
    environment and sound source configurations ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), we specify the context and scope of the survey in
    terms of the considered acoustic environment and sound source configurations.
    In Section [3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), we briefly present the most common conventional
    SSL methods, for two reasons: first, they are often used as a baseline for the
    evaluation of DL-based methods; and second, we will see that several types of
    features extracted by conventional methods can be used in DL-based methods. Section [4](#S4
    "4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") aims to classify the different neural network architectures
    used for SSL. Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") presents the various types of input features used
    for SSL with neural networks. In Section [6](#S6 "6 Output strategies ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"), we explain the two
    output strategies employed in DL-based SSL: classification and regression. We
    then discuss in Section [7](#S7 "7 Data ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") the datasets used for training and evaluating the
    models. In Section [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), learning paradigms such as supervised or semi-supervised
    learning are discussed from the SSL perspective. Section [9](#S9 "9 Conclusions
    and perspectives ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    provides the two summary tables and concludes the paper. Note that, due to the
    large number of acronyms used in this survey paper, we provide a list of these
    acronyms in Table [1](#S1.T1 "Table 1 ‣ 1.3 Outline of the paper ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。在第[2](#S2 "2 Acoustic environment and sound source configurations
    ‣ A Survey of Sound Source Localization with Deep Learning Methods")节，我们指定了调查的背景和范围，包括考虑的声学环境和声源配置。在第[3](#S3
    "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")节，我们简要介绍了最常见的传统SSL方法，原因有二：首先，它们常作为DL方法评估的基准；其次，我们将看到传统方法提取的几种特征可以用于DL方法。第[4](#S4
    "4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")节旨在分类用于SSL的不同神经网络架构。第[5](#S5 "5 Input features ‣ A
    Survey of Sound Source Localization with Deep Learning Methods")节介绍了用于SSL的各种输入特征。在第[6](#S6
    "6 Output strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")节，我们解释了DL-based SSL中使用的两种输出策略：分类和回归。随后，在第[7](#S7 "7 Data ‣ A Survey of
    Sound Source Localization with Deep Learning Methods")节讨论了用于训练和评估模型的数据集。在第[8](#S8
    "8 Learning strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")节，从SSL的角度讨论了监督学习或半监督学习等学习范式。第[9](#S9 "9 Conclusions and perspectives
    ‣ A Survey of Sound Source Localization with Deep Learning Methods")节提供了两个总结表并结束论文。注意，由于本调查论文中使用了大量缩略语，我们在第[1](#S1.T1
    "Table 1 ‣ 1.3 Outline of the paper ‣ 1 Introduction ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")表中提供了这些缩略语的列表。
- en: 'Table 1: Table of acronyms.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：缩略语表。
- en: '| ACCDOA | activity-coupled Cartesian direction of arrival |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ACCDOA | 活动耦合笛卡尔到达方向 |'
- en: '| AE | autoencoder |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| AE | 自编码器 |'
- en: '| ATF | acoustic transfer function |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ATF | 声学传递函数 |'
- en: '| ASR | automatic speech recognition |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ASR | 自动语音识别 |'
- en: '| BGRU | bidirectional gated recurrent unit |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| BGRU | 双向门控递归单元 |'
- en: '| BIR | binaural impulse response |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| BIR | 双耳脉冲响应 |'
- en: '| BRIR | binaural room impulse response |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| BRIR | 双耳房间脉冲响应 |'
- en: '| CC | cross-correlation |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| CC | 交叉相关 |'
- en: '| CNN | convolutional neural network |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 |'
- en: '| CRNN | convolutional recurrent neural network |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| CRNN | 卷积递归神经网络 |'
- en: '| CPS | cross power spectrum |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| CPS | 交叉功率谱 |'
- en: '| DCASE | Detection and Classification |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| DCASE | 检测与分类 |'
- en: '|  | of Acoustic Scenes and Events |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 声学场景与事件 |'
- en: '| DIRHA | distant-speech interaction |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| DIRHA | 远程语音交互 |'
- en: '|  | for robust home applications |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | 用于鲁棒家用应用 |'
- en: '| DL | deep learning |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| DL | 深度学习 |'
- en: '| DNN | deep neural network |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 |'
- en: '| DoA | direction of arrival |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| DoA | 到达方向 |'
- en: '| DP-RTF | direct-path relative transfer function |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| DP-RTF | 直接路径相对传递函数 |'
- en: '| DRR | direct-to-reverberant ratio |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| DRR | 直接到混响比 |'
- en: '| EM | expectation maximization |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| EM | 期望最大化 |'
- en: '| ESPRIT | Estimation of Signal Parameters |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ESPRIT | 信号参数估计 |'
- en: '|  | via Rotational Invariance Techniques |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | 通过旋转不变性技术 |'
- en: '| EVD | eigenvalue decomposition |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| EVD | 特征值分解 |'
- en: '| FFNN | feed-forward neural network |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| FFNN | 前馈神经网络 |'
- en: '| FOA | first-order Ambisonics |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| FOA | 一阶Ambisonics |'
- en: '| GAN | generative adversarial network |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| GAN | 生成对抗网络 |'
- en: '| GCC | generalized cross-correlation |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| GCC | 广义互相关 |'
- en: '| GLU | gated linear unit |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GLU | 门控线性单元 |'
- en: '| GMM | Gaussian mixture models |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| GMM | 高斯混合模型 |'
- en: '| GMR | Gaussian mixture regression |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GMR | 高斯混合回归 |'
- en: '| GPU | graphical processing unit |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 图形处理单元 |'
- en: '| GRU | gated recurrent unit |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GRU | 门控递归单元 |'
- en: '| HATS | head-and-torso simulator |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| HATS | 头部和躯干模拟器 |'
- en: '| HOA | higher-order Ambisonics |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| HOA | 高阶Ambisonics |'
- en: '| HRTF | head-related transfer function |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| HRTF | 与头部相关的传递函数 |'
- en: '| ICA | independent component analysis |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ICA | 独立成分分析 |'
- en: '| ILD | interaural level difference |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ILD | 耳间水平差 |'
- en: '| IPD | interaural phase difference |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| IPD | 耳间相位差 |'
- en: '| ITD | interaural time difference |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ITD | 耳间时间差 |'
- en: '| ISM | image source method |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ISM | 图像源方法 |'
- en: '| LSTM | long short-term memory |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆 |'
- en: '| MHSA | multihead self-attention |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MHSA | 多头自注意力 |'
- en: '| MLP | multiLayer Perceptron |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 多层感知器 |'
- en: '| MOT | multi-object tracking |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MOT | 多目标跟踪 |'
- en: '| MUSIC | MUltiple SIgnal Classification |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MUSIC | 多信号分类 |'
- en: '| NLP | natural language processing |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| NLP | 自然语言处理 |'
- en: '| NoS | number of sources |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| NoS | 源的数量 |'
- en: '| PHAT | PHAse Transform |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| PHAT | 相位变换 |'
- en: '| RIR | room impulse response |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| RIR | 房间脉冲响应 |'
- en: '| RNN | recurrent neural network |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 循环神经网络 |'
- en: '| RTF | relative transfer function |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| RTF | 相对传递函数 |'
- en: '| SA | self-attention |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SA | 自注意力 |'
- en: '| SCM | spatial covariance matrix |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SCM | 空间协方差矩阵 |'
- en: '| SED | sound event detection |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SED | 声音事件检测 |'
- en: '| SELD | sound event localization and detection |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SELD | 声音事件定位与检测 |'
- en: '| SH | spherical harmonics |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SH | 球面谐波 |'
- en: '| SMIR | spherical microphone impulse response |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SMIR | 球面麦克风脉冲响应 |'
- en: '| SMN | sequence matching network |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SMN | 序列匹配网络 |'
- en: '| SP | signal processing |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SP | 信号处理 |'
- en: '| SPS | spatial pseudo-spectrum |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SPS | 空间伪谱 |'
- en: '| SRP | steered power response |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SRP | 引导功率响应 |'
- en: '| SSL | sound source localization |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| SSL | 声源定位 |'
- en: '| STFT | short-term Fourier transform |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| STFT | 短时傅里叶变换 |'
- en: '| TCN | temporal convolutional network |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| TCN | 时间卷积网络 |'
- en: '| TDoA | time difference of arrival |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| TDoA | 到达时间差 |'
- en: '| TF | time-frequency |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| TF | 时频 |'
- en: '| VAD | voice activity detection |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| VAD | 语音活动检测 |'
- en: '| VAE | variational autoencoder |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| VAE | 变分自编码器 |'
- en: '| WDO | W-disjoint orthogonality |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| WDO | W-不相交正交性 |'
- en: 2 Acoustic environment and sound source configurations
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 声学环境和声音源配置
- en: SSL has been applied in different configurations, depending on the application.
    In this section, we specify the scope of our survey in terms of acoustic environment
    (noisy, reverberant, or even multi-room) and the nature of the considered sound
    sources (their type, number, and static/mobile status).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: SSL 已经在不同配置中得到应用，具体取决于应用场景。在本节中，我们指定了我们调查的范围，包括声学环境（噪声、混响或甚至多房间）以及所考虑声音源的性质（它们的类型、数量以及静态/移动状态）。
- en: 2.1 Acoustic environments
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 声学环境
- en: 'In this paper, we focus on SSL in an indoor environment, *i.e.*, when the microphone
    array and the sound source(s) are present in a closed room, generally of moderate
    size, typically an office room or a domestic environment. This implies reverberation:
    in addition to the direct source-to-microphone propagation path, the recorded
    sound contains many other multi-path components of the same source. All of these
    components form the room impulse response (RIR), which is defined for each source
    position and microphone array position (including orientation) and for a given
    room configuration.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们关注于室内环境中的SSL，即当麦克风阵列和声音源存在于一个封闭的房间内，通常是中等大小的办公室或家庭环境时。这意味着存在混响：除了直接的源到麦克风的传播路径，录制的声音还包含了许多其他来自同一源的多路径成分。所有这些成分形成了房间脉冲响应（RIR），它针对每个源位置和麦克风阵列位置（包括方向）以及给定的房间配置进行定义。
- en: 'In a general manner, the presence of reverberation is seen as a notable perturbation
    that makes SSL more difficult compared to the simpler (but somewhat unrealistic)
    anechoic case, which assumes the absence of reverberation, as is obtained in the
    free field propagation setup. Another important adverse factor to take into account
    in SSL is noise. On the one hand, noise can come from interfering sound sources
    in the surrounding environment: TV, background music, pets, street noise passing
    through open or closed windows, etc. Often, noise is considered as diffuse, *i.e.*,
    it does not originate from a clear direction. On the other hand, the imperfections
    of the recording devices are another source of noise that are generally considered
    as artifacts.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，混响的存在被视为一个显著的扰动，使得SSL（声源定位）比起假设没有混响的简单（但有些不现实的）无混响情况更为复杂，这种无混响情况通常在自由场传播设置中获得。另一个在SSL中需要考虑的重要不利因素是噪声。一方面，噪声可能来自周围环境中的干扰声源：电视、背景音乐、宠物、街道噪声穿过开窗或闭窗等。噪声通常被认为是扩散的，*即*，它没有来自明确的方向。另一方面，录音设备的缺陷是另一个噪声来源，这些噪声通常被视为伪影。
- en: Early works on using neural networks for DoA estimation most often considered
    direct-path propagation only (the anechoic setting), e.g., (Rastogi et al., [1987](#bib.bib265);
    Goryn and Kaveh, [1988](#bib.bib102); Jha et al., [1988](#bib.bib144); Jha and
    Durrani, [1989](#bib.bib142), [1991](#bib.bib143); Falong et al., [1993](#bib.bib85);
    Yang et al., [1994](#bib.bib370); Southall et al., [1995](#bib.bib301); El Zooghby
    et al., [2000](#bib.bib76)), though a model of the acoustical environment was
    used to generate simulated data to train the neural network of Datum et al. ([1996](#bib.bib63)).
    Most of these works are from the pre-deep-learning era, using “shallow” neural
    networks with only one or two hidden layers Goodfellow et al. ([2016](#bib.bib101)).
    We do not detail these works in our survey, although we acknowledge them as pioneering
    contributions to the neural network-based DoA estimation problem. A few more recent
    works based on more “modern” neural network architectures also focused on anechoic
    propagation only, or did not consider sound sources in the audible bandwidth (Liu
    et al., [2018](#bib.bib195); Ünlerşen and Yaldiz, [2016](#bib.bib387); Bialer
    et al., [2019](#bib.bib22); Elbir, [2020](#bib.bib77); Choi and Chang, [2020](#bib.bib50)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于使用神经网络进行方向估计（DoA）的研究大多只考虑了直接路径传播（即无混响设置），例如（Rastogi et al., [1987](#bib.bib265);
    Goryn and Kaveh, [1988](#bib.bib102); Jha et al., [1988](#bib.bib144); Jha and
    Durrani, [1989](#bib.bib142), [1991](#bib.bib143); Falong et al., [1993](#bib.bib85);
    Yang et al., [1994](#bib.bib370); Southall et al., [1995](#bib.bib301); El Zooghby
    et al., [2000](#bib.bib76))，尽管Datum et al. ([1996](#bib.bib63))使用了声学环境模型来生成模拟数据以训练神经网络。这些工作大多来自深度学习之前的时代，使用了只有一层或两层隐藏层的“浅层”神经网络（Goodfellow
    et al., [2016](#bib.bib101)）。虽然我们在综述中没有详细介绍这些工作，但我们承认它们是神经网络基础的方向估计问题的开创性贡献。基于更“现代”神经网络架构的一些较新研究也只关注无混响传播，或未考虑可听带宽中的声源（Liu
    et al., [2018](#bib.bib195); Ünlerşen and Yaldiz, [2016](#bib.bib387); Bialer
    et al., [2019](#bib.bib22); Elbir, [2020](#bib.bib77); Choi and Chang, [2020](#bib.bib50)）。
- en: 2.2 Source types
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 源类型
- en: In the SSL literature, a great proportion of systems focuses on localizing speech
    sources because of their importance in related tasks such as speech enhancement
    or speech recognition. Examples of speaker localization systems can be found in
    papers by Chakrabarty and Habets ([2019b](#bib.bib44)); Grumiaux et al. ([2021b](#bib.bib106));
    He et al. ([2021a](#bib.bib121)); Hao et al. ([2020](#bib.bib116)). In such systems,
    the neural networks are trained to estimate the DoA of speech sources so that
    they are somewhat specialized in this type of source. Other systems, in particular
    those participating in the DCASE Challenge, consider a variety of sound source
    types (Politis et al., [2020b](#bib.bib254)). Depending on the challenge task
    and its corresponding dataset, these methods are capable of localizing alarms,
    crying babies, crashes, barking dogs, female/male screams, female/male speech,
    footsteps, knockings on doors, ringings, phones, and piano sounds. Note that the
    localization of such sources, even if they overlap in time, is not necessarily
    a more difficult problem than localization of several overlapping speakers, since
    the former usually have distinct spectral characteristics that neural models may
    exploit for better detection and localization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在SSL文献中，许多系统专注于定位语音源，因为它们在与语音增强或语音识别等相关任务中的重要性。关于扬声器定位系统的例子可以在Chakrabarty和Habets（[2019b](#bib.bib44)）；Grumiaux等人（[2021b](#bib.bib106)）；He等人（[2021a](#bib.bib121)）；Hao等人（[2020](#bib.bib116)）的论文中找到。在这些系统中，神经网络被训练来估计语音源的DoA，以便它们在这类源上有一定的专长。其他系统，特别是参与DCASE挑战赛的系统，考虑了各种声音源类型（Politis等人，[2020b](#bib.bib254)）。根据挑战任务及其相应的数据集，这些方法能够定位警报声、婴儿哭声、撞击声、狗吠声、男性/女性尖叫声、男性/女性语音、脚步声、敲门声、铃声、电话铃声和钢琴声。请注意，即使这些源在时间上重叠，它们的定位问题也不一定比多个重叠的说话者更难，因为前者通常具有明显的谱特征，神经模型可以利用这些特征来提高检测和定位的准确性。
- en: 2.3 Number of sources
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 源数量
- en: The number of sources (NoS) in a recorded mixture signal is an important parameter
    for SSL. In the SSL literature, the NoS might be considered as known (as a working
    hypothesis). Alternatively, it can be estimated along with the source location,
    in which case the SSL problem is a combination of detection and localization.
    Examples of conventional (non-deep) SSL works including NoS estimation can be
    found in papers by Arberet et al. ([2009](#bib.bib10)); Landschoot and Xiang ([2019](#bib.bib169)).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在录制混合信号中的源数量（NoS）是SSL的一个重要参数。在SSL文献中，NoS可能被视为已知（作为工作假设）。或者，它可以与源位置一起估计，在这种情况下，SSL问题是检测和定位的结合。关于包含NoS估计的传统（非深度）SSL工作的例子可以在Arberet等人的论文中找到（[2009](#bib.bib10)）；Landschoot和Xiang（[2019](#bib.bib169)）。
- en: Many DNN-based works have considered only one source to localize, as it is the
    simplest scenario to address, e.g., (Perotin et al., [2018b](#bib.bib246); Bologni
    et al., [2021](#bib.bib29); Liu et al., [2021](#bib.bib193)). We refer to this
    scenario as single-source SSL. In this case, the networks are trained and evaluated
    on datasets with only at most one active source (a source is said to be active
    when emitting sound and inactive otherwise). In terms of NoS, we thus have here
    either 1 or 0 active source. The activity of the source in the processed signal,
    which generally contains background noise, can be artificially controlled, *i.e.*,
    the knowledge of source activity is a working hypothesis. This is a reasonable
    approach at training time when using synthetic data, but it is quite unrealistic
    at test time on real-world data. Alternatively, the source activity can be estimated,
    which is a more realistic approach at test time. In the latter case, there are
    two ways of dealing with the source activity detection problem. The first is to
    employ a source detection algorithm beforehand and then apply the SSL method only
    on the signal portions with an active source. For example, a voice activity detection
    (VAD) technique has been used in the SSL systems of Kim and Hahn ([2018](#bib.bib147));
    Chang et al. ([2018](#bib.bib45)); Sehgal and Kehtarnavaz ([2018](#bib.bib292));
    Li et al. ([2016d](#bib.bib188)). The other way is to detect the activity of the
    source at the same time as the localization algorithm. For example, an additional
    neuron was added by Yalta et al. ([2017](#bib.bib366)) to the output layer of
    their DNN, which outputted $1$ when no source was active (in that case, all other
    localization neurons were trained to output $0$), and $0$ otherwise.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于DNN的工作只考虑了一种来源进行定位，因为这是最简单的解决方案，例如（Perotin等人，[2018b](#bib.bib246); Bologni等人，[2021](#bib.bib29);
    Liu 等人，[2021](#bib.bib193)）。我们将这种情况称为单源SSL。在这种情况下，网络在仅有最多一个活动源的数据集上进行训练和评估（源在发出声音时被称为活动，否则为非活动）。在NoS方面，我们在这里要么有1个活动源，要么没有活动源。在处理的信号中，源的活动性通常包含背景噪音，可以人为控制，即，源活动的知识是一个工作假设。这是一个合理的方法，当使用合成数据进行训练时，但在真实世界的测试数据中，这种方法是相当不现实的。或者，可以估计源的活动性，这是在测试时更现实的方法。在后一种情况下，处理源活动检测问题有两种方法。第一种方法是在实施SSL方法之前使用源检测算法，然后仅对具有活动源的信号部分应用SSL方法。例如，Kim和Hahn的SSL系统中使用了语音活动检测（VAD）技术（[2018](#bib.bib147)）；Chang等人（[2018](#bib.bib45)）；Sehgal和Kehtarnavaz（[2018](#bib.bib292)）；Li等人（[2016d](#bib.bib188)）。另一种方法是在与定位算法同时检测源的活动性。例如，Yalta等人在他们的DNN的输出层中添加了一个额外的神经元，当没有源活动时输出$1$（在这种情况下，所有其他定位神经元被训练为输出$0$），否则输出$0$（[2017](#bib.bib366)）。
- en: Multi-source localization is a much more difficult problem than single-source
    SSL. Current state-of-the-art DL-based methods address multi-source SSL in adverse
    environments. In this survey, we consider as multi-source localization the scenario
    in which several sources overlap in time (*i.e.*, they are simultaneously emitting),
    regardless of their type (e.g., there could be several speakers or several distinct
    sound events). The specific case of a multi-speaker conversation with or without
    speech overlap is strongly connected to the speaker diarization problem (“who
    speaks when?”) (Tranter and Reynolds, [2006](#bib.bib324); Anguera et al., [2012](#bib.bib9);
    Park et al., [2021b](#bib.bib241)). Speaker localization, diarization, and (speech)
    source separation are intrinsically connected problems, as the information retrieved
    from solving each of them can be useful for addressing the others (Vincent et al.,
    [2018](#bib.bib341); Kounades-Bastian et al., [2017](#bib.bib156); Jenrungrot
    et al., [2020](#bib.bib141)). An investigation of these connections is beyond
    the scope of this survey.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 多源定位比单源SSL是一个更困难的问题。当前最先进的基于DL的方法在恶劣环境中解决多源SSL问题。在本调查中，我们将多源定位定义为多个源在时间上重叠（即它们同时发射），而不考虑它们的类型（例如，可能有多个发言者或几种不同的声音事件）。一个特定的多发言者对话的情况是否存在语音重叠与发言者辨识问题（“谁在何时说话？”）密切相关（Tranter和Reynolds，[2006](#bib.bib324)；Anguera等人，[2012](#bib.bib9)；Park等人，[2021b](#bib.bib241)）。说话者定位，辨识和（语音）源分离是内在相关的问题，因为解决每个问题所取得的信息对其他问题的解决方案也是有用的（Vincent等人，[2018](#bib.bib341)；Kounades-Bastian等人，[2017](#bib.bib156)；Jenrungrot等人，[2020](#bib.bib141)）。对这些联系的调查不在本调查的范围之内。
- en: In the multi-source scenario, the source detection problem transposes to a source
    counting problem, but the same considerations as in the single-source scenario
    hold. In some works, the knowledge of the NoS is a working hypothesis, e.g., (Grumiaux
    et al., [2021b](#bib.bib106); Ma et al., [2015](#bib.bib201); He et al., [2019a](#bib.bib120);
    Perotin et al., [2019b](#bib.bib248); Fahim et al., [2020](#bib.bib84); Bohlender
    et al., [2021](#bib.bib28); Grumiaux et al., [2021a](#bib.bib105)) and the sources’
    DoA can be directly estimated. If the NoS is unknown, one can apply a source counting
    system beforehand, e.g., with a dedicated DNN (Grumiaux et al., [2020](#bib.bib104)).
    For example, Tian ([2020](#bib.bib323)) trained a separate neural network to estimate
    the NoS in the recorded mixture signal, after which he used this information along
    with the output of the DoA estimation neural network. Alternatively, the NoS can
    be estimated alongside the DoAs, as in the single-source scenario, based on the
    SSL network output. When using a classification paradigm, the network output generally
    predicts the probability of the presence of a source within each discretized region
    of the space (see Section [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")). One can thus set a threshold on this
    estimated probability, which implicitly provides source counting.¹¹1Note that
    this problem is common to DL-based multi-source SSL methods and conventional methods
    for which a source activity profile is estimated and peak-picking algorithms are
    typically used to select the active sources. Otherwise, the ground-truth or estimated
    NoS is typically used to select the corresponding number of classes having the
    highest probability.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在多源场景中，源检测问题转化为源计数问题，但与单源场景中的考虑相同。在一些工作中，NoS 的知识是一个工作假设，例如，（Grumiaux 等人，[2021b](#bib.bib106)；Ma
    等人，[2015](#bib.bib201)；He 等人，[2019a](#bib.bib120)；Perotin 等人，[2019b](#bib.bib248)；Fahim
    等人，[2020](#bib.bib84)；Bohlender 等人，[2021](#bib.bib28)；Grumiaux 等人，[2021a](#bib.bib105)），源的
    DoA 可以直接估计。如果 NoS 未知，可以事先应用源计数系统，例如，使用专用的 DNN（Grumiaux 等人，[2020](#bib.bib104)）。例如，Tian（[2020](#bib.bib323)）训练了一个单独的神经网络来估计录音混合信号中的
    NoS，然后将这些信息与 DoA 估计神经网络的输出一起使用。或者，NoS 可以与 DoAs 一起估计，就像在单源场景中一样，基于 SSL 网络输出。当使用分类范式时，网络输出通常预测每个离散化空间区域内存在源的概率（见第
    [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source Localization with Deep
    Learning Methods") 节）。因此，可以对这个估计概率设置一个阈值，这隐含地提供了源计数。¹¹1注意，这个问题在基于深度学习（DL）的多源 SSL
    方法和传统方法中是常见的，其中估计源活动概况并通常使用峰值选择算法来选择活跃的源。否则，通常使用真实值或估计的 NoS 来选择具有最高概率的对应类别数量。
- en: 'Finally, several DNN-based systems were purposefully designed to estimate the
    NoS alongside the DoAs. For example, the method proposed by Nguyen et al. ([2020a](#bib.bib221))
    uses a neural architecture with two output branches: the first branch is used
    to estimate the NoS (up to four sources; the problem is formulated as a classification
    task), while the second branch is used to classify the azimuth into several regions.
    In the same spirit, we can mention the numerous systems presented at the DCASE
    Challenge, in which the SED task, jointly conducted with SSL, intrinsically provides
    an estimate of the NoS. Note that many DCASE Challenge candidate systems will
    be reviewed in the core of this survey.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，几种基于深度神经网络（DNN）的方法被专门设计用来估计噪声源（NoS）以及到达角（DoAs）。例如，Nguyen 等人提出的方法（[2020a](#bib.bib221)）使用了一个具有两个输出分支的神经网络架构：第一个分支用于估计
    NoS（最多四个源；问题被表述为分类任务），而第二个分支则用于将方位角分类到多个区域中。同样地，我们可以提到在 DCASE 挑战赛中展示的众多系统，其中声源定位（SED）任务与声源分离（SSL）联合进行，自然提供了
    NoS 的估计。注意，许多 DCASE 挑战赛候选系统将在本调查的核心部分进行回顾。
- en: 2.4 Moving sources
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 移动源
- en: Source tracking is the problem of estimating the evolution of the sources’ position(s)
    over time, especially when the sources are mobile. In this survey paper, we do
    not address the problem of tracking on its own, which is usually done in a separate
    algorithm using the sequence of DoA estimates obtained by applying SSL on successive
    time windows (Vo et al., [2015](#bib.bib342)). Still, several DL-based SSL systems
    have been shown to produce more accurate localization of moving sources when they
    were trained on a dataset that includes this type of source (Adavanne et al.,
    [2019c](#bib.bib4); Diaz-Guerra et al., [2021a](#bib.bib68); Guirguis et al.,
    [2020](#bib.bib107); He et al., [2021b](#bib.bib122)). In other cases, as the
    number of real-world datasets with moving sources is limited and the simulation
    of signals with moving sources is cumbersome, a number of systems trained on static
    sources have been shown to retain fair to good performance for moving sources,
    e.g., (Opochinsky et al., [2021](#bib.bib232); Grumiaux et al., [2021a](#bib.bib105);
    Sundar et al., [2020](#bib.bib309)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 源追踪是估计源位置随时间演变的问题，特别是当源是移动的。在这篇综述论文中，我们没有单独讨论追踪问题，这通常在使用应用 SSL 处理的连续时间窗口获得的
    DoA 估计序列的单独算法中完成（Vo 等人，[2015](#bib.bib342)）。尽管如此，已经有几种基于深度学习的 SSL 系统被证明能够在训练数据集中包括这种类型的源时更准确地定位移动源（Adavanne
    等人，[2019c](#bib.bib4)；Diaz-Guerra 等人，[2021a](#bib.bib68)；Guirguis 等人，[2020](#bib.bib107)；He
    等人，[2021b](#bib.bib122)）。在其他情况下，由于实际移动源数据集数量有限且模拟移动源信号繁琐，许多在静态源上训练的系统在处理移动源时仍保持良好表现，例如（Opochinsky
    等人，[2021](#bib.bib232)；Grumiaux 等人，[2021a](#bib.bib105)；Sundar 等人，[2020](#bib.bib309)）。
- en: 3 Conventional SSL methods
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 传统 SSL 方法
- en: 'Before the advent of DL, a set of signal processing techniques were developed
    to address SSL. A detailed review of these techniques was made by DiBiase et al.
    ([2001](#bib.bib70)). A review in the specific robotics context was made by Argentieri
    et al. ([2015](#bib.bib11)). In this section, we briefly present the most common
    conventional SSL methods. As briefly stated in the introduction, the reason for
    this is twofold: first, conventional SSL methods are often used as baselines for
    DL-based methods; and second, many DL-based SSL methods use input features extracted
    with conventional methods (see Section [5](#S5 "5 Input features ‣ A Survey of
    Sound Source Localization with Deep Learning Methods")).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习出现之前，一套信号处理技术被开发出来以解决 SSL 问题。DiBiase 等人对这些技术进行了详细回顾（[2001](#bib.bib70)）。Argentieri
    等人对特定机器人领域的回顾（[2015](#bib.bib11)）。在本节中，我们简要介绍了最常见的传统 SSL 方法。正如引言中简要提到的，原因有二：首先，传统
    SSL 方法通常用作基于深度学习的方法的基准；其次，许多基于深度学习的 SSL 方法使用了传统方法提取的输入特征（见第 [5](#S5 "5 Input features
    ‣ A Survey of Sound Source Localization with Deep Learning Methods) 节）。
- en: 'When the geometry of the microphone array is known, DoA estimation can be performed
    by estimating the time-difference of arrival (TDoA) of the sources between the
    microphones (Xu et al., [2013](#bib.bib361)). The generalized cross-correlation
    with phase transform (GCC-PHAT) is one of the most employed method when dealing
    with a 2-microphone array (Knapp and Carter, [1976](#bib.bib153)). It is computed
    as the inverse Fourier transform of a weighted version of the cross-power spectrum
    (CPS) between the signals of the two microphones:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当麦克风阵列的几何结构已知时，可以通过估计麦克风之间源的到达时间差（TDoA）来进行 DoA 估计（Xu 等人，[2013](#bib.bib361)）。广义互相关相位变换（GCC-PHAT）是处理
    2 个麦克风阵列时最常用的方法之一（Knapp 和 Carter，[1976](#bib.bib153)）。它被计算为两个麦克风信号之间交叉功率谱（CPS）加权版本的逆傅里叶变换：
- en: '|  | $r_{1,2}(\tau)=\sum_{f=0}^{F-1}\frac{X_{1}(f)X_{2}(f)^{*}}{&#124;X_{1}(f)X_{2}(f)^{*}&#124;}e^{j2\pi\frac{f\tau}{N}},$
    |  | (5) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{1,2}(\tau)=\sum_{f=0}^{F-1}\frac{X_{1}(f)X_{2}(f)^{*}}{&#124;X_{1}(f)X_{2}(f)^{*}&#124;}e^{j2\pi\frac{f\tau}{N}},$
    |  | (5) |'
- en: 'where $X_{i}(f)$ are the $N$-point Fourier transform of the microphone signals
    $x_{i}(t)$, and $X_{1}(f)X_{2}(f)^{*}$ is the CPS (^∗ denotes the complex conjugate).
    The TDoA estimate is then obtained by finding the time delay between the microphone
    signals that maximizes the GCC-PHAT function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X_{i}(f)$ 是麦克风信号 $x_{i}(t)$ 的 $N$ 点傅里叶变换，$X_{1}(f)X_{2}(f)^{*}$ 是 CPS（^∗
    表示共轭复数）。然后通过找到使 GCC-PHAT 函数最大化的麦克风信号之间的时间延迟来获得 TDoA 估计：
- en: '|  | $\hat{\tau}=\arg\max_{\tau}r_{1,2}(\tau).$ |  | (6) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\tau}=\arg\max_{\tau}r_{1,2}(\tau).$ |  | (6) |'
- en: The GCC approach has been extended to arrays with more than two microphones,
    showing in particular that the localization could be improved by taking advantage
    of the multiple microphone pairs (DiBiase et al., [2001](#bib.bib70); Benesty
    et al., [2008](#bib.bib18)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: GCC方法已经扩展到多个麦克风的阵列，特别是显示了通过利用多个麦克风对可以提高定位精度（DiBiase等，[2001](#bib.bib70)；Benesty等，[2008](#bib.bib18)）。
- en: 'Building an acoustic power map $P(\mathbf{x})$, with $\mathbf{x}$ the spatial
    coordinates, usually a regular grid, is another way to retrieve the DoA of one
    or multiple sources, as local maxima of this map mainly correspond to the sources’
    DoA. The Steered-Response Power (SRP) map has been extensively used: it consists
    in pointing delay and sum beamformers towards each of the candidate grid positions
    and measuring the energy that arises from these directions. Its PHAT version,
    which reveals more robust to reverberation, is certainly the most popular. Practically,
    it can be derived from the average of the GCC-PHAT computed on all microphone
    pairs (DiBiase et al., [2001](#bib.bib70)):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 构建声学功率图 $P(\mathbf{x})$，其中 $\mathbf{x}$ 为空间坐标，通常为规则网格，是获取一个或多个源的方向估计的另一种方法，因为该图的局部最大值主要对应于源的方向估计。Steered-Response
    Power (SRP) 图谱被广泛使用：它通过将延迟和求和波束形成器指向每个候选网格位置，并测量从这些方向产生的能量来实现。其PHAT版本，因对混响更为鲁棒，毫无疑问是最受欢迎的。实际上，它可以通过计算所有麦克风对的GCC-PHAT的平均值来得到（DiBiase等，[2001](#bib.bib70)）：
- en: '|  | $P(\mathbf{x})=\sum_{m_{1}=1}^{M}\sum_{m_{2}=m_{1}+1}^{M}r_{1,2}(\tau_{m_{1},m_{2}}(\mathbf{x})),$
    |  | (7) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(\mathbf{x})=\sum_{m_{1}=1}^{M}\sum_{m_{2}=m_{1}+1}^{M}r_{1,2}(\tau_{m_{1},m_{2}}(\mathbf{x})),$
    |  | (7) |'
- en: where $\tau_{m_{1},m_{2}}(\mathbf{x})$ is the delay between the microphones
    $m_{1}$ and $m_{2}$ associated to the spatial position $\mathbf{x}$.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau_{m_{1},m_{2}}(\mathbf{x})$ 是与空间位置 $\mathbf{x}$ 相关的麦克风 $m_{1}$ 和 $m_{2}$
    之间的延迟。
- en: An alternative to building the SRP-based acoustic map – which happens to be
    computationally expensive as it usually amounts to a grid search – is localization
    by exploiting the sound intensity. The use of sound intensity for source localization
    has a long history, e.g., (Nehorai and Paldi, [1994](#bib.bib219); Hickling et al.,
    [1993](#bib.bib126); Jarrett et al., [2010](#bib.bib139); Tervo, [2009](#bib.bib320);
    Raangs and Druyvesteyn, [2002](#bib.bib262); Basten et al., [2008](#bib.bib17)).
    In favorable acoustic conditions, sound intensity is parallel to the direction
    of the propagating sound wave (see Section [5.5](#S5.SS5 "5.5 Intensity-based
    features ‣ 5 Input features ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")), and hence the DoA can be efficiently estimated. Unfortunately,
    its accuracy quickly degrades in the presence of acoustic reflections (Daniel
    and Kitić, [2020](#bib.bib62)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 构建基于SRP的声学图谱的一个替代方法是利用声强进行定位——这种方法通常计算开销较大，因为它通常需要进行网格搜索。利用声强进行源定位有着悠久的历史，例如，（Nehorai和Paldi，[1994](#bib.bib219)；Hickling等，[1993](#bib.bib126)；Jarrett等，[2010](#bib.bib139)；Tervo，[2009](#bib.bib320)；Raangs和Druyvesteyn，[2002](#bib.bib262)；Basten等，[2008](#bib.bib17)）。在有利的声学条件下，声强与传播声波的方向平行（见第[5.5](#S5.SS5
    "5.5 Intensity-based features ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods)节），因此，方向估计（DoA）可以高效地估算。不幸的是，声强的准确性在声学反射存在时迅速下降（Daniel和Kitić，[2020](#bib.bib62)）。
- en: 'Subspace methods are another classical family of localization algorithms. These
    methods rely on the computation of the (time-averaged) CPS matrix $\mathbf{R}(f)$
    defined by:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 子空间方法是另一类经典的定位算法。这些方法依赖于（时间平均）CPS矩阵 $\mathbf{R}(f)$ 的计算，该矩阵定义为：
- en: '|  | $\mathbf{R}(f)=\sum_{n=1}^{N}\mathbf{X}(f,n)\mathbf{X}(f,n)^{H},$ |  |
    (8) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{R}(f)=\sum_{n=1}^{N}\mathbf{X}(f,n)\mathbf{X}(f,n)^{H},$ |  |
    (8) |'
- en: where $\mathbf{X}(f,n)$ is the STFT (or more generally a local discrete Fourier
    transform) of the multichannel signal vector defined in ([4](#S1.E4 "In 1.2 General
    principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) (^H denotes the Hermitian operator), and its eigenvalue
    decomposition (EVD). Assuming that the target source signals and noise are uncorrelated,
    the multiple signal classification (MUSIC) method (Schmidt, [1986](#bib.bib288))
    applies EVD to estimate the signal and noise subspaces. After Eq. ([4](#S1.E4
    "In 1.2 General principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound
    Source Localization with Deep Learning Methods")), the signal subspace bases are
    assumed to correspond to the columns of the mixing matrix $\mathbf{A}(f)$, which
    are the multichannel ATFs of the sources (often referred to as *steering vectors*
    in this context). The signal or noise subspace bases are then used to probe a
    given direction for the presence of a source, i.e. apply *spatial filtering* or
    *beamforming* (Van Veen and Buckley, [1988](#bib.bib328); Benesty et al., [2008](#bib.bib18)).
    This time-demanding search can be relaxed using the Estimation of Signal Parameters
    via Rotational Invariance Technique (ESPRIT) algorithm (Roy and Kailath, [1989](#bib.bib276)),
    which exploits the structure of the source subspace to directly infer the source
    DoA. However, this often comes at the cost of producing less accurate predictions
    than MUSIC (Mabande et al., [2011](#bib.bib203)). MUSIC and ESPRIT assume narrowband
    signals, although wideband extensions have been proposed, e.g., (Dmochowski et al.,
    [2007](#bib.bib71); Hogg et al., [2021](#bib.bib130)). Subspace methods are robust
    to noise and can produce highly accurate estimates, but they are sensitive to
    reverberation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{X}(f,n)$是多通道信号向量的STFT（或更一般地，局部离散傅里叶变换）（[4](#S1.E4 "在1.2基于DL的SSL的一般原理
    ‣ 1介绍 ‣ 基于深度学习方法的声源定位调查") "In 1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"）的特征值分解。假设目标源信号和噪声不相关，则多信号分类（MUSIC）方法（Schmidt，[1986](#bib.bib288)）将特征值分解应用于估计信号和噪声子空间。在方程式([4](#S1.E4
    "在1.2基于DL的SSL的一般原理 ‣ 1介绍 ‣ 基于深度学习方法的声源定位调查") "In 1.2 General principle of DL-based
    SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")之后，假设信号子空间基底对应于混合矩阵$\mathbf{A}(f)$的列，这些列是源的多通道ATF（在这种情况下通常称为*指向向量*）。然后，信号或噪声子空间基底用于检测给定方向上是否存在源，即应用*空间滤波*或*波束成形*（Van Veen和Buckley，[1988](#bib.bib328);
    Benesty等，[2008](#bib.bib18)）。使用估计信号参数通过旋转不变性技术（ESPRIT）算法（Roy和Kailath，[1989](#bib.bib276)）可以放宽这一耗时的搜索，该算法利用源子空间的结构直接推断出源的DoA。然而，与MUSIC相比，这通常会产生更不准确的预测（Mabande等，[2011](#bib.bib203)）。MUSIC和ESPRIT都假设是窄带信号，尽管已经提出了宽带扩展，例如（Dmochowski等，[2007](#bib.bib71);
    Hogg等，[2021](#bib.bib130)）。子空间方法对噪声具有鲁棒性，并且可以产生高度准确的估计，但是它们对混响非常敏感。
- en: Methods based on probabilistic generative mixture models have been proposed
    by, e.g., Roman and Wang ([2008](#bib.bib272)); Mandel et al. ([2009](#bib.bib205));
    May et al. ([2011](#bib.bib209)); Woodruff and Wang ([2012](#bib.bib351)); Schwartz
    and Gannot ([2013](#bib.bib289)); Dorfan and Gannot ([2015](#bib.bib72)); Li et al.
    ([2017](#bib.bib189)). Typically, the models are variants of Gaussian mixture
    models (GMMs), with one Gaussian component per source to be localized or per candidate
    source position. In a very few papers (e.g., (May et al., [2011](#bib.bib209))),
    the model is trained offline with a dedicated training dataset. But most often,
    the model parameters are directly estimated “at test time,” that is using the
    multichannel signal containing the sources to localize. This is done by maximizing
    the data likelihood function with histogram-based or expectation-maximization
    (EM) algorithms exploiting the sparsity of sound sources in the time-frequency
    (TF) domain (Rickard, [2002](#bib.bib268)), which can be computationally intensive.
    A GMM variant functioning directly in regression mode, *i.e.*, a form of Gaussian
    mixture regression (GMR), was proposed for single-source localization by Deleforge
    and Horaud ([2012](#bib.bib65)) and later extended to multi-source localization
    (and possibly separation) (Deleforge et al., [2013](#bib.bib66), [2015](#bib.bib67)).
    The GMR is locally linear but globally non-linear and the estimation of the model
    parameters is done offline on training data. Hence the spirit is close to DNN-based
    SSL. White noise signals convolved with synthetic RIRs were used for training.
    The method was shown to generalize well to speech signals, which are sparser than
    noise in the TF domain, thanks to the use of a latent variable modeling the signal
    activity in each TF bin.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概率生成混合模型的方法已由例如 Roman 和 Wang ([2008](#bib.bib272))；Mandel 等 ([2009](#bib.bib205))；May
    等 ([2011](#bib.bib209))；Woodruff 和 Wang ([2012](#bib.bib351))；Schwartz 和 Gannot
    ([2013](#bib.bib289))；Dorfan 和 Gannot ([2015](#bib.bib72))；Li 等 ([2017](#bib.bib189))
    提出。通常，这些模型是高斯混合模型（GMM）的变体，每个源位置或候选源位置对应一个高斯成分。在极少数的文献中（例如，（May 等， [2011](#bib.bib209)）），该模型使用专门的训练数据集进行离线训练。但大多数情况下，模型参数是直接在“测试时”估计的，即使用包含源的多通道信号进行定位。这是通过最大化数据似然函数来实现的，采用基于直方图或期望最大化（EM）算法，利用时频（TF）域中声音源的稀疏性（Rickard,
    [2002](#bib.bib268)），这一过程可能计算量很大。Deleforge 和 Horaud ([2012](#bib.bib65)) 提出了一个直接在回归模式下运行的
    GMM 变体，即高斯混合回归（GMR）形式，用于单源定位，并且后来扩展到多源定位（以及可能的分离）（Deleforge 等，[2013](#bib.bib66)，[2015](#bib.bib67)）。GMR
    在局部是线性的，但在全局上是非线性的，模型参数的估计是在训练数据上离线完成的。因此，它的精神接近于基于 DNN 的 SSL。使用与合成 RIRs 卷积的白噪声信号进行训练。该方法显示出在语音信号上的良好泛化能力，因为语音信号在
    TF 域中比噪声更稀疏，这得益于使用了一个潜在变量来建模每个 TF 框中的信号活动。
- en: 'Mixture models are strongly connected to Bayesian inference, which considers
    the posterior distribution of model parameters given the observed data (hence
    involving both the likelihood function and a prior distribution of the model parameters).
    Escolano et al. ([2014](#bib.bib81)) considered applying Bayesian inference on
    a Laplacian source mixture model, using GCC-PHAT features in a two-microphone
    array set-up. Interestingly, they used two levels of Bayesian inference: one for
    the estimation of the NoS (which is an hyper-parameter of the model), using Bayesian
    model selection, and one for the estimation of the model parameters (and thus
    the corresponding source DoAs), using posterior distribution evaluation. In this
    work, the evaluation of the involved distributions was done with sampling techniques,
    e.g., Markov Chain Monte Carlo (MCMC) methods. The same methodology was further
    applied by Bush and Xiang ([2018](#bib.bib33)) with a coprime array consisting
    of two superimposed, spatially undersampled, uniform linear arrays (Vaidyanathan
    and Pal, [2010](#bib.bib326)), and by Landschoot and Xiang ([2019](#bib.bib169))
    in the spherical harmonics domain using a spherical microphone array (see Section [5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型与贝叶斯推断有很强的关联，贝叶斯推断考虑了在给定观察数据的情况下模型参数的后验分布（因此涉及到似然函数和模型参数的先验分布）。Escolano
    等人 ([2014](#bib.bib81)) 考虑了在拉普拉斯源混合模型上应用贝叶斯推断，使用 GCC-PHAT 特征在双麦克风阵列设置中。有趣的是，他们使用了两级贝叶斯推断：一个用于估计
    NoS（这是模型的超参数），使用贝叶斯模型选择；另一个用于估计模型参数（从而估计相应的源方向角），使用后验分布评估。在这项工作中，涉及的分布的评估是通过采样技术完成的，例如马尔科夫链蒙特卡洛（MCMC）方法。同样的方法论被
    Bush 和 Xiang ([2018](#bib.bib33)) 应用在由两个叠加的、空间下采样的均匀线性阵列（Vaidyanathan 和 Pal，[2010](#bib.bib326)）组成的互质阵列中，以及被
    Landschoot 和 Xiang ([2019](#bib.bib169)) 在球面谐波领域使用球面麦克风阵列中（见第 [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods) 节")。
- en: Compressive sensing and sparse recovery methods are widely used in acoustics
    for different purposes Gerstoft et al. ([2018](#bib.bib96)); Xenaki et al. ([2014](#bib.bib356)),
    including SSL Yang et al. ([2018](#bib.bib372)). The main premise is that many
    high-dimensional signals admit a low-dimensional representation, which can be
    viewed through, e.g., *sparse synthesis* Candes et al. ([2006](#bib.bib35)) or
    *sparse analysis* Nam et al. ([2013](#bib.bib215)) model. Concerning the SSL problem,
    the sparsity assumption is usually assumed in the spatial (or spatial beam) domain,
    e.g., Chardon and Daudet ([2012](#bib.bib46)); Noohi et al. ([2013](#bib.bib228));
    Fortunati et al. ([2014](#bib.bib87)); Kitić et al. ([2014](#bib.bib152)); Gerstoft
    et al. ([2016](#bib.bib95)), and the resulting problem is addressed by convex
    optimization, greedy or Bayesian methods, e.g., Foucart and Rauhut ([2013](#bib.bib88));
    Gerstoft et al. ([2018](#bib.bib96)). This concept has lead to prominent localization
    methods achieving remarkable performance. Nonetheless, despite their strong theoretical
    guarantees, compressive sensing methods suffer from two drawbacks. For one, it
    is usually required that the sources coincide with points of some pre-defined
    grid, although grid-free methods have been proposed in some specific cases, e.g.,
    Xenaki and Gerstoft ([2015](#bib.bib355)); Yang and Xie ([2015](#bib.bib371)).
    The second issue is shared with other conventional methods, *i.e.*, the strong
    modeling assumptions reflected in, for example, the known structure of the (sub-Gaussian)
    dictionary matrix. Dictionary learning techniques have been proposed to alleviate
    the latter problem to some extent, e.g., Wang et al. ([2018](#bib.bib346)); Hahmann
    et al. ([2021b](#bib.bib114)); Zea and Laudato ([2021](#bib.bib377)). Sparse Bayesian
    learning (SBL) is a combination of the Bayesian framework with the principles
    of sparse representations and compressed sensing. It usually involves using sparse
    arrays such as the coprime array mentioned above and nested arrays (Pal and Vaidyanathan,
    [2010](#bib.bib234)). SBL has been used for SSL by, e.g., Nannuru et al. ([2018](#bib.bib216));
    Zhang et al. ([2014](#bib.bib383)); Liu et al. ([2012](#bib.bib194)); Gerstoft
    et al. ([2016](#bib.bib95)); Xenaki et al. ([2018](#bib.bib357)); Ping et al.
    ([2020](#bib.bib252)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩感知和稀疏恢复方法在声学中用于不同的目的（Gerstoft et al., [2018](#bib.bib96); Xenaki et al., [2014](#bib.bib356)），包括SSL（Yang
    et al., [2018](#bib.bib372)）。其主要前提是，许多高维信号可以承认低维表示，这可以通过例如*稀疏合成*（Candes et al.,
    [2006](#bib.bib35)）或*稀疏分析*（Nam et al., [2013](#bib.bib215)）模型来查看。关于SSL问题，通常在空间（或空间束）域中假设稀疏性，例如Chardon和Daudet（[2012](#bib.bib46)）；Noohi
    et al.（[2013](#bib.bib228)）；Fortunati et al.（[2014](#bib.bib87)）；Kitić et al.（[2014](#bib.bib152)）；Gerstoft
    et al.（[2016](#bib.bib95)），并通过凸优化、贪婪或贝叶斯方法来解决所得问题，例如Foucart和Rauhut（[2013](#bib.bib88)）；Gerstoft
    et al.（[2018](#bib.bib96)）。这一概念导致了一些杰出的定位方法，取得了显著的性能。然而，尽管具有强大的理论保证，压缩感知方法仍然存在两个缺点。其一，通常要求源信号与某些预定义网格的点一致，尽管在某些特定情况下已提出无网格方法，例如Xenaki和Gerstoft（[2015](#bib.bib355)）；Yang和Xie（[2015](#bib.bib371)）。第二个问题与其他传统方法相同，即强建模假设，例如（亚高斯）字典矩阵的已知结构。已提出字典学习技术以在某种程度上缓解这一问题，例如Wang
    et al.（[2018](#bib.bib346)）；Hahmann et al.（[2021b](#bib.bib114)）；Zea和Laudato（[2021](#bib.bib377)）。稀疏贝叶斯学习（SBL）是将贝叶斯框架与稀疏表示和压缩感知原则相结合的一种方法。它通常涉及使用稀疏数组，例如上述提到的互质数组和嵌套数组（Pal和Vaidyanathan，[2010](#bib.bib234)）。SBL已被用于SSL，例如Nannuru
    et al.（[2018](#bib.bib216)）；Zhang et al.（[2014](#bib.bib383)）；Liu et al.（[2012](#bib.bib194)）；Gerstoft
    et al.（[2016](#bib.bib95)）；Xenaki et al.（[2018](#bib.bib357)）；Ping et al.（[2020](#bib.bib252)）。
- en: Finally, ICA is a class of algorithms aimed at retrieving the different source
    signals comprising a mixture by assuming and exploiting their mutual statistical
    independence. ICA has most often been used in audio processing for blind source
    separation, but it has also proven to be useful for multi-source SSL (Sawada et al.,
    [2003](#bib.bib286)). As briefly stated before, in the multi-source scenario,
    SSL is closely related to the source separation problem, since localization can
    help separation, and separation can help localization (Gannot et al., [2017](#bib.bib90);
    Vincent et al., [2018](#bib.bib341)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，ICA是一类算法，旨在通过假设和利用源信号之间的统计独立性来提取混合信号中的不同源信号。ICA最常用于音频处理中的盲源分离，但它在多源SSL中也被证明是有用的（Sawada
    et al., [2003](#bib.bib286)）。如前所述，在多源场景中，SSL与源分离问题密切相关，因为定位可以帮助分离，而分离可以帮助定位（Gannot
    et al., [2017](#bib.bib90); Vincent et al., [2018](#bib.bib341)）。
- en: 4 Neural network architectures for SSL
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 神经网络架构用于SSL
- en: In this section, we discuss the neural network architectures that have been
    proposed in the literature to address the SSL problem. However, we do not present
    the basics of these neural networks since they have been extensively described
    in the general DL literature, e.g., (LeCun et al., [2015](#bib.bib175); Goodfellow
    et al., [2016](#bib.bib101); Chollet, [2017](#bib.bib51)). The design of DNNs
    for a given application often requires investigating (and possibly combining)
    different architectures and tuning their hyperparameters. This was the case for
    SSL over the last decade, and the evolution of DL-based SSL techniques has followed
    the general evolution of DNNs toward more and more complex architectures or new
    efficient models adopted by the DL and SP communities at large, i.e., largely
    beyond the SSL problem (e.g., attention models). In other words, the DNN architectures
    used in SSL are often inherited from other works in other (connected or more distant)
    domains, simply because they were shown to work well on audio signals or other
    types or signals. In the same spirit, different models are often combined (in
    parallel and/or sequentially).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们讨论了文献中提出的用于解决SSL问题的神经网络架构。然而，我们不会介绍这些神经网络的基础知识，因为它们在一般的深度学习（DL）文献中已有广泛描述，例如（LeCun
    等，[2015](#bib.bib175); Goodfellow 等，[2016](#bib.bib101); Chollet，[2017](#bib.bib51)）。对于特定应用，深度神经网络（DNN）的设计通常需要研究（并可能结合）不同的架构，并调整它们的超参数。这在过去十年中的SSL领域也是如此，基于DL的SSL技术的发展跟随了DNNs向越来越复杂的架构或由DL和SP社区广泛采用的新型高效模型的发展，即大大超出了SSL问题的范围（例如，注意力模型）。换句话说，SSL中使用的DNN架构通常继承自其他（相关或更远）领域的工作，因为它们在音频信号或其他类型的信号上表现良好。在同样的精神下，不同的模型通常被结合（以并行和/或顺序的方式）。
- en: 'We have thus organized the presentation according to the type of layers used
    in the networks, with a progressive and “inclusive” approach in terms of complexity:
    a network within a given category can contain layers from another previously presented
    category. We thus first present systems based on feedforward neural networks (FFNNs).
    We then focus on convolutional neural networks (CNNs) and recurrent neural networks
    (RNNs), which generally incorporate some feedforward layers. Next, we review architectures
    combining CNNs with RNNs, namely convolutional recurrent neural networks (CRNNs).
    Then, we focus on neural networks with residual connections and with attention
    mechanisms. Finally, we present SSL systems with an encoder-decoder architecture.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们根据网络中使用的层类型组织了展示内容，采用了在复杂性方面渐进和“包容性”的方法：一个特定类别的网络可以包含来自之前展示的其他类别的层。我们首先介绍基于前馈神经网络（FFNNs）的系统。然后，我们重点介绍卷积神经网络（CNNs）和递归神经网络（RNNs），它们通常包含一些前馈层。接下来，我们回顾结合CNNs和RNNs的架构，即卷积递归神经网络（CRNNs）。然后，我们关注具有残差连接和注意力机制的神经网络。最后，我们介绍具有编码器-解码器架构的SSL系统。
- en: 4.1 Feedforward neural networks
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 前馈神经网络
- en: '![Refer to caption](img/f2cb33b1c190e84d0bd100d52db28aae.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f2cb33b1c190e84d0bd100d52db28aae.png)'
- en: 'Figure 2: The MLP architecture used by Takeda *et al.* in several papers (Takeda
    and Komatani, [2016b](#bib.bib316), [a](#bib.bib315), [2017](#bib.bib317); Takeda
    et al., [2018](#bib.bib318)). Multiple subband feedforward layers, indexed by
    $w$, are trained to extract features from the CPS matrix eigenvectors $\mathbf{e}_{w,i}$,
    which are used as directional activation functions. The obtained subband vectors
    $\mathbf{X}_{2,w}$ are integrated across subbands progressively via other feedforward
    layers, giving $\mathbf{X}_{3,w}$ and then $\mathbf{X}_{4}$. The output layer
    finally classifies its input in one of the candidate DoAs (the entries of the
    vector $\mathbf{X}_{5}$). Note: Reprinted from (Takeda and Komatani, [2016a](#bib.bib315));
    copyright by IEEE; reprinted with permission.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Takeda *等人* 在几篇论文中使用的MLP架构（Takeda 和 Komatani，[2016b](#bib.bib316)，[a](#bib.bib315)，[2017](#bib.bib317);
    Takeda 等人，[2018](#bib.bib318)）。多个子带前馈层，以 $w$ 为索引，被训练来从CPS矩阵特征向量 $\mathbf{e}_{w,i}$
    中提取特征，这些特征向量被用作方向激活函数。获得的子带向量 $\mathbf{X}_{2,w}$ 通过其他前馈层逐步整合到子带中，得到 $\mathbf{X}_{3,w}$，然后是
    $\mathbf{X}_{4}$。输出层最终将其输入分类为候选的到达角（向量 $\mathbf{X}_{5}$ 的条目）。注意：转载自（Takeda 和 Komatani，[2016a](#bib.bib315)）；版权归IEEE所有；经许可转载。
- en: 'The FFNN was the first and simplest type of artificial neural network to be
    designed. In such a network, data move in one direction from the input layer to
    the output layer, possibly via a series of hidden layers (Goodfellow et al., [2016](#bib.bib101);
    LeCun et al., [2015](#bib.bib175)). Non-linear activation functions are usually
    used after each layer (possibly except for the output layer). While this definition
    of FFNN is very general and may include architectures such as CNNs (discussed
    in the next subsection), here we mainly focus on architectures made of fully-connected
    layers known as Perceptron and Multi-Layer Perceptron (MLP) (Goodfellow et al.,
    [2016](#bib.bib101); LeCun et al., [2015](#bib.bib175)). A Perceptron has no hidden
    layer, while the notion of MLP is a bit ambiguous: some authors state that an
    MLP has one hidden layer, while others allow more hidden layers. In this paper,
    we call an MLP an FFNN with one or more hidden layers.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: FFNN 是第一个也是最简单的人工神经网络类型。在这样的网络中，数据从输入层到输出层沿一个方向移动，可能通过一系列隐藏层（Goodfellow et al.,
    [2016](#bib.bib101); LeCun et al., [2015](#bib.bib175)）。通常在每一层之后使用非线性激活函数（输出层可能除外）。尽管这种
    FFNN 的定义非常通用，可能包括诸如 CNNs（将在下一小节讨论）的架构，但在这里我们主要关注由完全连接层构成的架构，即感知机和多层感知机（MLP）（Goodfellow
    et al., [2016](#bib.bib101); LeCun et al., [2015](#bib.bib175)）。感知机没有隐藏层，而 MLP
    的概念有些模糊：一些作者认为 MLP 有一个隐藏层，而其他作者允许有更多隐藏层。在本文中，我们称一个具有一个或多个隐藏层的 FFNN 为 MLP。
- en: A few pioneering SSL methods using shallow neural networks (Perceptron or 1-hidden
    layer MLP) and applied in “unrealistic” setups (e.g., assuming direct-path sound
    propagation only) have been briefly mentioned in Section [2.1](#S2.SS1 "2.1 Acoustic
    environments ‣ 2 Acoustic environment and sound source configurations ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"). One of the first uses
    of an MLP for SSL was proposed by Kim and Ling ([2011](#bib.bib149)), who actually
    considered several MLPs. One network estimates the NoS, after which a distinct
    network is used for SSL for each considered NoS. The authors evaluated their method
    on reverberant data even though they assumed an anechoic setting. Tsuzuki et al.
    ([2013](#bib.bib325)) proposed using a complex-valued MLP in order to process
    complex two-microphone-based features, which led to better results than using
    a real-valued MLP. Youssef et al. ([2013](#bib.bib375)) also used an MLP to estimate
    the azimuth of a sound source from a binaural recording made with a robot head.
    The interaural time difference (ITD) and the interaural level difference (ILD)
    values (see Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) were separately fed into the input layer and were
    each processed by a specific set of neurons. A single-hidden-layer MLP was used
    by Xiao et al. ([2015](#bib.bib359)), taking GCC-PHAT-based features as inputs
    and tackling SSL as a classification problem (see Section [8](#S8 "8 Learning
    strategies ‣ A Survey of Sound Source Localization with Deep Learning Methods")),
    which showed an improvement over conventional methods on simulated and real data.
    A similar approach was proposed by Vesperini et al. ([2016](#bib.bib339)), but
    the localization was done by regression in the horizontal plane.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一些先驱性的 SSL 方法使用浅层神经网络（感知机或一个隐藏层的 MLP）并应用于“非现实”设置（例如，仅假设直接路径声音传播）在第[2.1](#S2.SS1
    "2.1 Acoustic environments ‣ 2 Acoustic environment and sound source configurations
    ‣ A Survey of Sound Source Localization with Deep Learning Methods")节中被简要提及。Kim
    和 Ling ([2011](#bib.bib149)) 提出了第一个使用 MLP 进行 SSL 的方法，他们实际上考虑了几种 MLP。一个网络估计 NoS，然后使用一个独立的网络对每个考虑的
    NoS 进行 SSL。作者在回声数据上评估了他们的方法，即使他们假设了一个无回声的环境。Tsuzuki 等人 ([2013](#bib.bib325)) 提出了使用复数值
    MLP 来处理基于两个麦克风的复杂特征，这比使用实值 MLP 得到了更好的结果。Youssef 等人 ([2013](#bib.bib375)) 还使用 MLP
    从机器人头部的双耳录音中估计声音源的方位角。双耳时间差（ITD）和双耳水平差（ILD）值（参见第[5](#S5 "5 Input features ‣ A
    Survey of Sound Source Localization with Deep Learning Methods")节）被分别输入到输入层，并由特定的神经元集合处理。Xiao
    等人 ([2015](#bib.bib359)) 使用了一个单隐藏层 MLP，将基于 GCC-PHAT 的特征作为输入，并将 SSL 视为分类问题（参见第[8](#S8
    "8 Learning strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")节），在模拟和真实数据上显示了相对于传统方法的改进。Vesperini 等人 ([2016](#bib.bib339)) 提出了类似的方法，但定位是在水平面上通过回归进行的。
- en: Naturally, MLPs with deeper architecture (*i.e.*, more hidden layers) have also
    been investigated for SSL. Roden et al. ([2015](#bib.bib271)) compared the performance
    of an MLP with two hidden layers and different input types, the number of hidden
    neurons being linked to the type of input features (see Section [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    for more details). Yiwere and Rhee ([2017](#bib.bib374)) used an MLP with three
    hidden layers (tested with different numbers of neurons) to output source azimuth
    and distance estimates. An MLP with four hidden layers was tested by He et al.
    ([2018a](#bib.bib118)) for multi-source localization and speech/non-speech classification,
    showing similar results as a 4-layer CNN (see Section [4.2](#S4.SS2 "4.2 Convolutional
    neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，具有更深架构的 MLP（*即* 更多隐藏层）也已被用于 SSL 研究。Roden 等人 ([2015](#bib.bib271)) 比较了具有两个隐藏层的
    MLP 和不同输入类型的性能，隐藏神经元的数量与输入特征的类型相关（详见第 [5](#S5 "5 Input features ‣ A Survey of
    Sound Source Localization with Deep Learning Methods") 节）。Yiwere 和 Rhee ([2017](#bib.bib374))
    使用了一个具有三个隐藏层的 MLP（用不同数量的神经元进行测试）来输出源的方位和距离估计。He 等人 ([2018a](#bib.bib118)) 测试了一个具有四个隐藏层的
    MLP 用于多源定位和语音/非语音分类，结果与一个 4 层 CNN 相似（见第 [4.2](#S4.SS2 "4.2 Convolutional neural
    networks ‣ 4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") 节）。
- en: Ma et al. ([2015](#bib.bib201)) proposed using a different MLP for different
    frequency sub-bands, with each MLP having eight hidden layers. This idea is based
    on the assumption that, in the presence of multiple sources, each frequency band
    is mostly dominated by a single source, which enables the training to be done
    exclusively on single-source data. The output of each sub-band MLP corresponds
    to a probability distribution on azimuth regions, and the final azimuth estimations
    are obtained by integrating the probability values over the frequency bands. Another
    system in the same vein was proposed by Takeda *et al.* in several papers (Takeda
    and Komatani, [2016b](#bib.bib316), [a](#bib.bib315), [2017](#bib.bib317); Takeda
    et al., [2018](#bib.bib318)). In these works, the eigenvectors of the recorded
    signal interchannel correlation matrix were separately fed per frequency band
    into parallel branches of the network, particularly into specific fully-connected
    layers. Then, several additional fully-connected layers progressively integrated
    the frequency-dependent outputs (see Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Feedforward
    neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")). The authors showed that this specific
    architecture outperforms a more conventional 7-layer MLP and the classical MUSIC
    algorithm on anechoic and reverberant single- and multi-source signals. Opochinsky
    et al. ([2019](#bib.bib231)) proposed a small 3-layer MLP to estimate the azimuth
    of a single source using the relative transfer function (RTF, see Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Relative transfer function (RTF) ‣ 5.1 Inter-channel features ‣ 5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    of the signal. Their approach is weakly supervised since one part of the loss
    function is computed without the ground truth DoA labels (see Section [8](#S8
    "8 Learning strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Ma 等人 ([2015](#bib.bib201)) 提出了对不同频率子带使用不同的多层感知机（MLP），每个 MLP 具有八个隐藏层。这一想法基于这样的假设：在多个来源存在的情况下，每个频率带主要被单一来源主导，这使得训练可以仅在单一来源数据上进行。每个子带
    MLP 的输出对应于方位区域上的概率分布，最终的方位估计通过在频率带上积分概率值来获得。Takeda *等人* 在几篇论文中提出了类似的系统（Takeda
    和 Komatani, [2016b](#bib.bib316), [a](#bib.bib315), [2017](#bib.bib317); Takeda
    等人, [2018](#bib.bib318)）。在这些工作中，记录信号的通道间相关矩阵的特征向量被分别输入到网络的平行分支中，特别是输入到特定的全连接层中。然后，几个附加的全连接层逐步整合了频率依赖的输出（见图
    [2](#S4.F2 "Figure 2 ‣ 4.1 Feedforward neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")）。作者展示了这种特定架构在无回声和混响单一及多源信号上优于传统的
    7 层 MLP 和经典的 MUSIC 算法。Opochinsky 等人 ([2019](#bib.bib231)) 提出了一个小型的 3 层 MLP 来估计单一来源的方位，使用信号的相对传递函数（RTF，见第
    [5.1.1](#S5.SS1.SSS1 "5.1.1 Relative transfer function (RTF) ‣ 5.1 Inter-channel
    features ‣ 5 Input features ‣ A Survey of Sound Source Localization with Deep
    Learning Methods") 节）。他们的方法是弱监督的，因为损失函数的一部分在没有真实的 DoA 标签的情况下计算（见第 [8](#S8 "8 Learning
    strategies ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    节）。
- en: An indirect use of an MLP was explored by Pak and Shin ([2019](#bib.bib233)),
    who used a 3-layer MLP to enhance the interaural phase difference (IPD) (see Section [5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    of the input signal, which was then used for DoA estimation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Pak 和 Shin ([2019](#bib.bib233)) 探索了 MLP 的间接使用，他们使用了一个 3 层的 MLP 来增强输入信号的耳间相位差（IPD）（见第
    [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning
    Methods) 节"），然后用于 DoA 估计。
- en: 4.2 Convolutional neural networks
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 卷积神经网络
- en: CNNs are a popular class of DNNs widely used for pattern recognition due to
    their property of being translation equivariant (Cohen et al., [2019](#bib.bib56);
    Goodfellow et al., [2016](#bib.bib101)). They have been successfully applied to
    various tasks, such as image classification, e.g., (Krizhevsky et al., [2017](#bib.bib164)),
    natural language processing (NLP), e.g., (Kim, [2014](#bib.bib148)) or automatic
    speech recognition, e.g., (Waibel et al., [1989](#bib.bib344)). CNNs have also
    been used for SSL, as detailed below.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是一种广泛用于模式识别的 DNN 类，由于其平移等变性（Cohen 等，[2019](#bib.bib56); Goodfellow 等，[2016](#bib.bib101)），它们被广泛应用于各种任务，如图像分类（例如，（Krizhevsky
    等，[2017](#bib.bib164)）），自然语言处理（NLP），例如，（Kim，[2014](#bib.bib148)）或自动语音识别（例如，（Waibel
    等，[1989](#bib.bib344)））。CNN 也被用于 SSL，如下所述。
- en: 'To our knowledge, Hirvonen ([2015](#bib.bib128)) was the first to use a CNN
    for SSL. He employed this architecture to classify an audio signal containing
    one speech or musical source into one of eight spatial regions (see Fig. [3](#S4.F3
    "Figure 3 ‣ 4.2 Convolutional neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    This CNN is composed of four convolutional layers to extract feature maps from
    multichannel magnitude spectrograms (see Section [5](#S5 "5 Input features ‣ A
    Survey of Sound Source Localization with Deep Learning Methods")), followed by
    four fully-connected layers for classification. Classical pooling is not used
    because, according to the author, it does not seem relevant for audio representations.
    Instead, a 4-tap stride with a 2-tap overlap is used to reduce the number of parameters.
    This approach shows good performance on single-source signals and is capable of
    adapting to different configurations without hand-engineering. However, two topical
    issues of such a system were pointed out by the author: the robustness of the
    network with respect to a shift in source location, and the difficulty of interpreting
    the hidden features.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，Hirvonen ([2015](#bib.bib128)) 是第一个将 CNN 应用于 SSL 的研究者。他采用这种架构将包含一个语音或音乐源的音频信号分类到八个空间区域之一（见图
    [3](#S4.F3 "Figure 3 ‣ 4.2 Convolutional neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods)）。这个
    CNN 由四个卷积层组成，用于从多通道幅度谱图中提取特征图（见第 [5](#S5 "5 Input features ‣ A Survey of Sound
    Source Localization with Deep Learning Methods) 节"），然后通过四个全连接层进行分类。由于作者认为经典池化与音频表示无关，因此未使用池化。相反，使用了
    4-步长和 2-重叠的方式来减少参数数量。这种方法在单源信号上表现良好，并能够适应不同的配置而无需手工调整。然而，作者指出了该系统的两个热点问题：网络对源位置变化的鲁棒性和隐藏特征的解释难度。
- en: '![Refer to caption](img/522bceba108f1b7791eea3261f505adf.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/522bceba108f1b7791eea3261f505adf.png)'
- en: 'Figure 3: The CNN architecture proposed by Hirvonen ([2015](#bib.bib128)) for
    SSL. The input is an 8-channel signal. For each short-term frame, the 8 magnitude
    spectra (of $128$ frequency bins) are concatenated to form a $1024\times 1$ tensor,
    which is fed into a series of four convolutional layers with $500$ or $600$ learnable
    kernels. The extracted features then pass through several feedforward layers containing
    $500$ or $300$ neurons. The output layer contains $8$ neurons (or $16$ if the
    source type is also considered) and estimates the probability of a source being
    present in eight candidate DoAs using a softmax activation function. Note: Reprinted
    from (Hirvonen, [2015](#bib.bib128)); copyright by the author; reprinted with
    permission.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Hirvonen ([2015](#bib.bib128)) 为 SSL 提出的 CNN 架构。输入为 8 通道信号。对于每个短时帧，将 8 个幅度谱（$128$
    频率 bin）串联形成一个 $1024\times 1$ 张量，该张量被输入到一系列具有 $500$ 或 $600$ 可学习内核的卷积层中。提取的特征随后通过几个前馈层，这些层包含
    $500$ 或 $300$ 个神经元。输出层包含 $8$ 个神经元（如果还考虑源类型则为 $16$），并使用 softmax 激活函数估计源存在于八个候选
    DoA 中的概率。注：转载自（Hirvonen，[2015](#bib.bib128)）；版权归作者所有；经许可转载。
- en: Chakrabarty and Habets also designed a CNN to predict the azimuth of one (Chakrabarty
    and Habets, [2017a](#bib.bib41)) or two (Chakrabarty and Habets, [2019b](#bib.bib44),
    [2017b](#bib.bib42)) speakers in reverberant environments. The input features
    are the multichannel short-time Fourier transform (STFT) phase spectrograms (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). In (Chakrabarty and Habets, [2017a](#bib.bib41)), they
    proposed using three successive convolutional layers with $64$ filters of size
    $2\times 2$ to consider neighboring frequency bands and microphones. In (Chakrabarty
    and Habets, [2017b](#bib.bib42)), they reduced the filter size to $2\times 1$
    ($1$ in the frequency axis) because of the W-disjoint orthogonality (WDO) assumption
    for speech signals, which assumes that several speakers are not simultaneously
    active in a same TF bin (Rickard, [2002](#bib.bib268)). In (Chakrabarty and Habets,
    [2019b](#bib.bib44)), they demonstrated that for an $M$-microphone array, the
    optimal number of convolutional layers for exploiting phase correlations between
    the neighboring microphones is $M-1$.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Chakrabarty 和 Habets 还设计了一个卷积神经网络（CNN），用于预测在混响环境中一个（Chakrabarty 和 Habets, [2017a](#bib.bib41)）或两个（Chakrabarty
    和 Habets, [2019b](#bib.bib44), [2017b](#bib.bib42)）说话人的方位。输入特征是多通道短时傅里叶变换（STFT）相位频谱图（见第[5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")节）。在（Chakrabarty
    和 Habets, [2017a](#bib.bib41)）中，他们提出使用三层连续卷积层，每层有 $64$ 个 $2\times 2$ 大小的滤波器，以考虑相邻频带和麦克风。在（Chakrabarty
    和 Habets, [2017b](#bib.bib42)）中，他们将滤波器大小减少到 $2\times 1$ （频率轴上为 $1$），这是由于语音信号的
    W-离散正交性（WDO）假设，该假设认为多个说话者不会在同一时间频率（TF）窗内同时活跃（Rickard, [2002](#bib.bib268)）。在（Chakrabarty
    和 Habets, [2019b](#bib.bib44)）中，他们展示了对于一个 $M$-麦克风阵列，利用相邻麦克风之间的相位相关性的最优卷积层数为 $M-1$。
- en: He et al. ([2018a](#bib.bib118)) compared a 4-layer MLP and a 4-layer CNN for
    the multi-speaker detection and localization task. The results showed similar
    accuracy for both architectures. A deeper architecture was proposed by Yalta et al.
    ([2017](#bib.bib366)), with 11 to 20 convolutional layers depending on the experiments.
    These deeper CNNs showed robustness against noise compared to MUSIC, as well as
    smaller training time, but this was partly due to the presence of residual blocks
    (see Section [4.5](#S4.SS5 "4.5 Residual neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    A similar architecture was presented by He et al. ([2018b](#bib.bib119)), with
    many convolutional layers and some residual blocks, although with a specific multi-task
    configuration. The end of the network was split into two convolutional branches,
    one for azimuth estimation, and the other for speech/non-speech signal classification.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: He 等人 ([2018a](#bib.bib118)) 比较了 4 层 MLP 和 4 层 CNN 在多说话人检测和定位任务中的表现。结果显示，两种架构的准确性相似。Yalta
    等人 ([2017](#bib.bib366)) 提出了更深的架构，根据实验不同，卷积层数量从 11 层到 20 层不等。这些更深的 CNN 相比于 MUSIC
    在噪声方面表现出了鲁棒性，同时训练时间也较短，但这部分是由于残差块的存在（见第[4.5](#S4.SS5 "4.5 Residual neural networks
    ‣ 4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")节）。He 等人 ([2018b](#bib.bib119)) 提出了类似的架构，具有许多卷积层和一些残差块，但配置为特定的多任务处理。网络的末端分为两个卷积分支，一个用于方位估计，另一个用于语音/非语音信号分类。
- en: 'While most localization systems aim to estimate the azimuth or both the azimuth
    and elevation, Thuillier et al. ([2018](#bib.bib322)) investigated the estimation
    of only the elevation angle using a CNN with binaural input features: the ipsilateral
    and contralateral head-related transfer function (HRTF) magnitude responses (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). Vera-Diaz et al. ([2018](#bib.bib336)) chose to apply
    a CNN directly on raw multichannel waveforms, assembled side by side as an image,
    to predict the Cartesian coordinates $(x,y,z)$ of a single static or moving speaker.
    The successive convolutional layers contain around a hundred filters from size
    $7\times 7$ for the first layers to $3\times 3$ for the last layer. Ma and Liu
    ([2018](#bib.bib202)) also used a CNN to perform regression, but they used the
    CPS matrix as an input feature (see Section [5](#S5 "5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")). To estimate both the
    azimuth and elevation, Nguyen et al. ([2018](#bib.bib220)) used a relatively small
    CNN (two convolutional layers) in regression mode, with binaural input features.
    A similar approach was considered by Sivasankaran et al. ([2018](#bib.bib299))
    for speaker localization based on a CNN. They showed that injecting a speaker
    identifier, particularly a mask estimated for the speaker uttering a given keyword,
    alongside the binaural features at the input layer improved the DoA estimation.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数定位系统旨在估计方位角或方位角和俯仰角，但Thuillier等人 ([2018](#bib.bib322)) 研究了仅使用卷积神经网络（CNN）和双耳输入特征来估计俯仰角：同侧和对侧头相关传递函数（HRTF）幅度响应（见[第5节](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")）。Vera-Diaz等人
    ([2018](#bib.bib336)) 选择将CNN直接应用于原始多通道波形，这些波形被并排组装成图像，以预测单个静态或移动扬声器的笛卡尔坐标 $(x,y,z)$。连续的卷积层包含大约一百个滤波器，从第一个层的大小
    $7\times 7$ 到最后一层的 $3\times 3$。Ma和Liu ([2018](#bib.bib202)) 也使用CNN进行回归，但他们使用CPS矩阵作为输入特征（见[第5节](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")）。为了估计方位角和俯仰角，Nguyen等人
    ([2018](#bib.bib220)) 使用了相对较小的CNN（两个卷积层）进行回归模式，并使用双耳输入特征。Sivasankaran等人 ([2018](#bib.bib299))
    考虑了一种类似的方法来进行基于CNN的扬声器定位。他们展示了在输入层注入扬声器标识符，特别是估计的用于说出特定关键词的扬声器的掩码，与双耳特征一起使用，可以改善到达方向（DoA）估计。
- en: A joint VAD and DoA estimation CNN was developed by Vecchiotti et al. ([2018](#bib.bib333)).
    They showed that both problems can be handled jointly in a multi-room environment
    using the same architecture, although considering separate input features (GCC-PHAT
    and log-mel-spectrograms) in two separate input branches. These branches are then
    concatenated in a further layer. Vecchiotti et al. ([2019b](#bib.bib335)) extended
    this work by exploring several variant architectures and experimental configurations,
    and Vecchiotti et al. ([2019a](#bib.bib334)) developed an end-to-end auditory-inspired
    system based on a CNN, with Gammatone filter layers included in the neural architecture.
    A method based on mask estimation was proposed by Zhang et al. ([2019b](#bib.bib381)),
    in which a TF mask was estimated and used to either clean or be appended to the
    input features, facilitating the DoA estimation by a CNN.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Vecchiotti等人 ([2018](#bib.bib333)) 开发了一个联合语音活动检测（VAD）和DoA估计的CNN。他们展示了在多房间环境中可以使用相同的架构来同时处理这两个问题，尽管在两个独立的输入分支中考虑了不同的输入特征（GCC-PHAT和对数梅尔谱）。这些分支随后在进一步的层中进行拼接。Vecchiotti等人
    ([2019b](#bib.bib335)) 通过探索几种变体架构和实验配置扩展了这项工作，而Vecchiotti等人 ([2019a](#bib.bib334))
    开发了一种基于CNN的端到端听觉启发系统，其中包括Gammatone滤波器层在神经网络架构中。Zhang等人 ([2019b](#bib.bib381))
    提出了基于掩码估计的方法，其中估计了TF掩码并用于清理或附加到输入特征上，从而通过CNN促进DoA估计。
- en: Nguyen et al. ([2020a](#bib.bib221)) presented a multi-task CNN containing 10
    convolutional layers with average pooling, inferring both the NoS and the sources’
    DoA. They evaluated their network on signals with up to four sources, showing
    very good performance on both simulated and real environments. A small 3-layer
    CNN was employed by Varanasi et al. ([2020](#bib.bib329)) to infer both azimuth
    and elevation using signals decomposed with third-order spherical harmonics (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). The authors tried several combinations of input features,
    including using only the magnitude and/or the phase of the spherical harmonic
    decomposition.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Nguyen 等人 ([2020a](#bib.bib221)) 提出了一个包含 10 层卷积层和平均池化的多任务 CNN，用于推断 NoS 和声源的
    DoA。他们在最多四个声源的信号上评估了他们的网络，在模拟和实际环境中表现非常好。Varanasi 等人 ([2020](#bib.bib329)) 使用一个小型的
    3 层 CNN 来推断方位角和仰角，使用了第三阶球面谐波分解的信号（见第 [5](#S5 "5 Input features ‣ A Survey of Sound
    Source Localization with Deep Learning Methods) 节"）。作者尝试了几种输入特征的组合，包括仅使用球面谐波分解的幅度和/或相位。
- en: In the context of hearing aids, a CNN was applied to both VAD and DoA estimation
    by Varzandeh et al. ([2020](#bib.bib331)). This system is based on two input features,
    GCC-PHAT and periodicity degree, both fed separately into two convolutional branches.
    These two branches are then concatenated in a further layer, which is followed
    by feedforward layers. Fahim et al. ([2020](#bib.bib84)) applied an 8-layer CNN
    to the so-called modal coherence of first-order Ambisonics input features (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")) for the localization of multiple sources in a reverberant
    environment. They proposed a new method to train a multi-source DoA estimation
    network with only single-source training data, showing an improvement over the
    system of Chakrabarty and Habets ([2019b](#bib.bib44)), especially for signals
    with three speakers. Hao et al. ([2020](#bib.bib116)) investigated a real-time
    implementation of SSL using a CNN with a relatively small architecture (three
    convolutional layers).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在助听器的背景下，Varzandeh 等人 ([2020](#bib.bib331)) 将 CNN 应用于 VAD 和 DoA 估计。该系统基于两个输入特征，GCC-PHAT
    和周期性度，分别送入两个卷积分支。这两个分支在进一步的层中连接，然后是前馈层。Fahim 等人 ([2020](#bib.bib84)) 将 8 层 CNN
    应用于所谓的第一阶 Ambisonics 输入特征的模态一致性（见第 [5](#S5 "5 Input features ‣ A Survey of Sound
    Source Localization with Deep Learning Methods) 节")，以在混响环境中定位多个声源。他们提出了一种新的方法，用单声道训练数据训练多声道
    DoA 估计网络，相比 Chakrabarty 和 Habets ([2019b](#bib.bib44)) 的系统有所改进，尤其是对于三个扬声器的信号。Hao
    等人 ([2020](#bib.bib116)) 研究了使用相对较小架构（三个卷积层）的 CNN 实时实现 SSL。
- en: Krause et al. ([2020a](#bib.bib160)) investigated the use of several types of
    convolution. They reported that networks using 3D convolutions (on the time, frequency,
    and channel axes) achieved better localization accuracy compared to those based
    on 2D convolutions, complex convolutions, and depth-wise separable convolutions
    (all of them on the time and frequency axes), but with a high computational cost.
    They also showed that the use of depth-wise separable convolutions leads to a
    good trade-off between accuracy and model complexity (to our knowledge, they were
    the first to explore this type of convolutions).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Krause 等人 ([2020a](#bib.bib160)) 研究了几种类型的卷积。他们报告称，使用 3D 卷积（在时间、频率和通道轴上）的网络相比基于
    2D 卷积、复杂卷积和深度可分离卷积（所有这些都在时间和频率轴上）取得了更好的定位准确性，但计算成本较高。他们还展示了使用深度可分离卷积在准确性和模型复杂性之间达到了良好的折衷（据我们所知，他们是首个探索这种卷积类型的研究者）。
- en: Bologni et al. ([2021](#bib.bib29)) proposed a neural network architecture including
    a set of 2D convolutional layers for frame-wise feature extraction, followed by
    several 1D convolutional layers in the time dimension for temporal aggregation.
    Diaz-Guerra et al. ([2021a](#bib.bib68)) applied 3D convolutional layers on SRP-PHAT
    power maps computed for both azimuth and elevation estimation. They also used
    a couple of 1D causal convolutional layers at the end of the network to perform
    single-source tracking. Their whole architecture was designed to function in fully
    causal mode so that it can be adapted for real-time applications. Wu et al. ([2021b](#bib.bib353))
    proposed using a supervised image mapping approach inspired from computer vision
    works and referred to as image translation. The used a CNN (completed with residual
    layers, see Section [4.5](#S4.SS5 "4.5 Residual neural networks ‣ 4 Neural network
    architectures for SSL ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")) to map an input 2D image (DoA features extracted by conventional beamforming
    and reshaped as a function of Cartesian coordinates $(x,y)$) into an output 2D
    image of the target source position (in which the pixel intensity is decreasing
    rapidly with the distance to the source), from which the source location is obtained.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Bologni 等人（[2021](#bib.bib29)）提出了一种神经网络架构，包括一组用于帧级特征提取的 2D 卷积层，之后是几个用于时间聚合的
    1D 卷积层。Diaz-Guerra 等人（[2021a](#bib.bib68)）在 SRP-PHAT 功率图上应用了 3D 卷积层，计算方位角和高度角估计。他们还在网络末端使用了几个
    1D 因果卷积层来执行单一来源跟踪。他们的整个架构被设计为在完全因果模式下运行，以便可以适应实时应用。Wu 等人（[2021b](#bib.bib353)）提出了使用一种受到计算机视觉工作启发的监督图像映射方法，称为图像翻译。他们使用了一种
    CNN（补充了残差层，见第[4.5节](#S4.SS5 "4.5 Residual neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")）将输入的
    2D 图像（通过传统波束形成提取的 DoA 特征，并根据笛卡尔坐标 $(x,y)$ 进行重塑）映射到目标源位置的输出 2D 图像（在该图像中，像素强度随源距离的增加而迅速减少），从中获得源的位置。
- en: As mentioned in the introduction, the DCASE Challenge includes a SELD task,
    and CNNs have also been used in some of the challenge candidate systems (Politis
    et al., [2020b](#bib.bib254)). Chytas and Potamianos ([2019](#bib.bib52)) used
    convolutional layers with hundreds of filters of size $4\times 10$ for azimuth
    and elevation estimation in a regression mode. Kong et al. ([2019](#bib.bib155))
    compared different numbers of convolutional layers for SELD, while an 8-layer
    CNN was proposed by Noh et al. ([2019](#bib.bib226)) to improve the results over
    the baseline.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如引言中提到的，DCASE 挑战包括一个 SELD 任务，CNN 也被用于一些挑战候选系统（Politis 等人，[2020b](#bib.bib254)）。Chytas
    和 Potamianos（[2019](#bib.bib52)）使用了具有数百个 $4\times 10$ 大小滤波器的卷积层进行回归模式下的方位角和高度角估计。Kong
    等人（[2019](#bib.bib155)）比较了不同数量的卷积层用于 SELD，而 Noh 等人（[2019](#bib.bib226)）提出了一种 8
    层 CNN 来改善基线结果。
- en: An indirect use of a CNN was proposed by Salvati et al. ([2018](#bib.bib283)).
    They trained the neural network to estimate a weight for each of the narrow-band
    SRP components fed at the input layer in order to compute a weighted combination
    of these components. In their experiments, they showed on a few test examples
    that this allowed for a better fusion of the narrow-band components and reduced
    the effects of noise and reverberation, leading to better localization accuracy.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Salvati 等人（[2018](#bib.bib283)）提出了一种间接使用卷积神经网络（CNN）的方法。他们训练了神经网络以估计输入层每个窄带 SRP
    组件的权重，从而计算这些组件的加权组合。在他们的实验中，他们在几个测试示例上展示了这种方法可以更好地融合窄带组件，并减少噪声和混响的影响，从而提高定位准确性。
- en: In the DoA estimation literature, a few works have explored the use of dilated
    convolutions in DNNs. Dilated convolutions, also known as atrous convolutions,
    are a type of convolutional layer in which the convolution kernel is wider than
    the classical one but zeros are inserted so that the number of parameters remains
    the same. Formally, a 1D dilated convolution with a dilation factor $l$ is defined
    by
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DoA 估计文献中，一些研究探索了在 DNN 中使用扩张卷积。扩张卷积，也称为 atrous 卷积，是一种卷积层，其卷积核比经典卷积核宽，但通过插入零来保持参数数量不变。形式上，具有扩张因子
    $l$ 的 1D 扩张卷积定义为
- en: '|  | $(x*k)(n)=\sum_{i}x(n-li)k(i),$ |  | (9) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $(x*k)(n)=\sum_{i}x(n-li)k(i),$ |  | (9) |'
- en: where $x$ is the input and $k$ the convolution kernel. The conventional linear
    convolution is obtained with $l=1$. This definition extends to multidimensional
    convolution.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是输入，$k$ 是卷积核。常规线性卷积在 $l=1$ 时获得。这个定义可以扩展到多维卷积。
- en: Chakrabarty and Habets ([2019a](#bib.bib43)) demonstrate that incorporating
    dilated convolutions with gradually increasing dilation factors reduces the optimal
    number of convolutional layers of their original CNN architecture (Chakrabarty
    and Habets, [2019b](#bib.bib44)) (discussed previously in this section). This
    leads to an architecture with similar SSL performance and lower computational
    cost.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Chakrabarty 和 Habets（[2019a](#bib.bib43)）展示了将膨胀卷积与逐渐增加的膨胀因子结合使用可以减少其原始 CNN 架构（Chakrabarty
    和 Habets，[2019b](#bib.bib44)）的卷积层的最佳数量（在本节中已讨论）。这导致了一个具有类似 SSL 性能和较低计算成本的架构。
- en: 4.3 Recurrent neural networks
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 递归神经网络
- en: RNNs are neural networks designed for modeling temporal sequences of data (LeCun
    et al., [2015](#bib.bib175); Goodfellow et al., [2016](#bib.bib101)). Particular
    types of RNNs include long short-term memory (LSTM) cells (Hochreiter and Schmidhuber,
    [1997](#bib.bib129)) and gated recurrent units (GRUs) (Cho et al., [2014](#bib.bib49)).
    These two types of RNNs have become very popular thanks to their capability to
    circumvent the training difficulties that regular RNNs face, in particular the
    vanishing and exploding gradient problems (LeCun et al., [2015](#bib.bib175);
    Goodfellow et al., [2016](#bib.bib101)).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 是用于建模数据时间序列的神经网络（LeCun 等，[2015](#bib.bib175)；Goodfellow 等，[2016](#bib.bib101)）。特定类型的
    RNNs 包括长短期记忆（LSTM）单元（Hochreiter 和 Schmidhuber，[1997](#bib.bib129)）和门控循环单元（GRUs）（Cho
    等，[2014](#bib.bib49)）。这两种类型的 RNNs 已经变得非常流行，因为它们能够规避常规 RNNs 面临的训练困难，特别是梯度消失和梯度爆炸问题（LeCun
    等，[2015](#bib.bib175)；Goodfellow 等，[2016](#bib.bib101)）。
- en: There are few published works on SSL using only RNNs, as recurrent layers are
    often combined with convolutional layers (see Section [4.4](#S4.SS4 "4.4 Convolutional
    recurrent neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")). Nguyen et al. ([2021a](#bib.bib224))
    used an RNN to align SED and DoA predictions, which were obtained separately for
    each possible sound event type. The RNN was ultimately used to determine which
    SED prediction matched which DoA estimation. A bidirectional LSTM network was
    used by Wang et al. ([2019](#bib.bib349)) to estimate a TF mask to enhance the
    signal, further facilitating DoA estimation by conventional methods such as SRP
    or subspace methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 关于仅使用 RNNs 的 SSL 的公开工作很少，因为递归层通常与卷积层结合使用（见第 [4.4](#S4.SS4 "4.4 Convolutional
    recurrent neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey
    of Sound Source Localization with Deep Learning Methods") 节）。Nguyen 等人（[2021a](#bib.bib224)）使用
    RNN 来对齐 SED 和 DoA 预测，这些预测是为每种可能的声音事件类型单独获得的。RNN 最终被用来确定哪个 SED 预测与哪个 DoA 估计匹配。Wang
    等人（[2019](#bib.bib349)）使用双向 LSTM 网络来估计 TF 掩膜以增强信号，并进一步通过传统方法（如 SRP 或子空间方法）促进 DoA
    估计。
- en: 4.4 Convolutional recurrent neural networks
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 卷积递归神经网络
- en: 'CRNNs are neural networks containing one or more convolutional layers and one
    or more recurrent layers. CRNNs have been regularly exploited for SSL since 2018
    because of the respective capabilities of these layers: The convolutional layers
    have proven to be suitable for extracting relevant features for SSL, and the recurrent
    layers are well designed for integrating the information over time.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: CRNNs 是包含一个或多个卷积层和一个或多个递归层的神经网络。自 2018 年以来，CRNNs 因其各层的能力而被广泛用于 SSL：卷积层已被证明适合提取
    SSL 所需的相关特征，而递归层则设计良好，适合于在时间上整合信息。
- en: In the series of papers (Adavanne et al., [2019c](#bib.bib4), [2018](#bib.bib1),
    [a](#bib.bib2)), Adavanne *et al.* used a CRNN for SELD, in a multi-task configuration,
    with first-order Ambisonics (FOA) input features (see Section [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    In (Adavanne et al., [2018](#bib.bib1)), their architecture contained a series
    of successive convolutional layers, each followed by a max-pooling layer and two
    bidirectional GRU (BGRU) layers. Then, a feedforward layer provided an estimation
    of the spatial pseudo-spectrum (SPS) provided by the MUSIC algorithm (Schmidt,
    [1986](#bib.bib288)), acting as an intermediary output (see Fig. [4](#S4.F4 "Figure
    4 ‣ 4.4 Convolutional recurrent neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    This SPS was then fed into the second part of the neural network, which was composed
    of two convolutional layers, a dense layer, two BGRU layers, and a final feedforward
    layer for azimuth and elevation estimation by classification. The use of an intermediary
    SPS output has been proposed to help the neural network learn a representation
    that has proven to be useful for SSL using traditional methods.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在一系列论文中（Adavanne 等人，[2019c](#bib.bib4)，[2018](#bib.bib1)，[a](#bib.bib2)），Adavanne
    *et al.* 使用了 CRNN 进行 SELD，并采用了多任务配置，输入特征为一阶 Ambisonics (FOA)（见第 [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    节）。在（Adavanne 等人，[2018](#bib.bib1)）中，他们的架构包含了一系列连续的卷积层，每个卷积层后跟一个最大池化层和两个双向 GRU
    (BGRU) 层。然后，一个前馈层提供了由 MUSIC 算法（Schmidt，[1986](#bib.bib288)）生成的空间伪谱 (SPS) 的估计，作为中间输出（见图
    [4](#S4.F4 "Figure 4 ‣ 4.4 Convolutional recurrent neural networks ‣ 4 Neural
    network architectures for SSL ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")）。该 SPS 随后被输入到神经网络的第二部分，该部分由两个卷积层、一个密集层、两个 BGRU 层和一个最终的前馈层组成，用于通过分类估计方位角和俯仰角。使用中间
    SPS 输出的方式被提出以帮助神经网络学习一种已经被证明对使用传统方法进行 SSL 有用的表示。
- en: In (Adavanne et al., [2019a](#bib.bib2)) and (Adavanne et al., [2019c](#bib.bib4)),
    this intermediary output was no longer used. Instead, the DoA was directly estimated
    using a block of convolutional layers, a block of BGRU layers, and a feedforward
    layer. This system is able to localize and detect several sound events even if
    they overlap in time, provided they are of different types (e.g., speech and car,
    see the discussion in Section [2.2](#S2.SS2 "2.2 Source types ‣ 2 Acoustic environment
    and sound source configurations ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")). This CRNN was the baseline system for Task 3 of the DCASE
    Challenge in 2019 and 2020\. Therefore, it has inspired many other works, and
    many DCASE Challenge candidate systems were built on the system of Adavanne et al.
    ([2019a](#bib.bib2)) with various modifications and improvements.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Adavanne 等人，[2019a](#bib.bib2)）和（Adavanne 等人，[2019c](#bib.bib4)）中，这种中间输出不再使用。相反，使用一组卷积层、一组
    BGRU 层和一个前馈层直接估计 DoA。该系统能够本地化和检测多个声音事件，即使它们在时间上重叠，只要它们是不同类型的（例如，语音和汽车，见第 [2.2](#S2.SS2
    "2.2 Source types ‣ 2 Acoustic environment and sound source configurations ‣ A
    Survey of Sound Source Localization with Deep Learning Methods") 节的讨论）。该 CRNN
    是 2019 年和 2020 年 DCASE 挑战任务 3 的基线系统。因此，它激发了许多其他工作，许多 DCASE 挑战候选系统基于 Adavanne 等人（[2019a](#bib.bib2)）的系统，并进行了各种修改和改进。
- en: For example, Lin and Wang ([2019](#bib.bib192)) added Gaussian noise to the
    input spectrograms to train the network to be more robust to noise. Lu ([2019](#bib.bib196))
    integrated some additional convolutional layers and replaced the BGRU layers with
    bidirectional LSTM layers. Leung and Ren ([2019](#bib.bib182)) used the same architecture
    with all combinations of cross-channel power spectra, whereas the replacement
    of input features with group delays was tested by Nustede and Anemüller ([2019](#bib.bib230)).
    GCC-PHAT features were added as input features by Maruri et al. ([2019](#bib.bib207)).
    Zhang et al. ([2019a](#bib.bib380)) used data augmentation during training and
    averaged the output of the network for a more stable DoA estimation. Xue et al.
    ([2019](#bib.bib364)) sent the input features separately into different branches
    of convolutional layers, log-mel, and constant Q-transform features on the one
    hand, and phase spectrograms and CPS features on the other hand (see Section [5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    Cao et al. ([2019b](#bib.bib37)) concatenated the log-mel spectrogram and GCC-PHAT
    features and fed them into two separate CRNNs for SED and DoA estimation (they
    also incorporated the intensity vector in (Cao et al., [2019a](#bib.bib36))).
    In contrast to the baseline of Adavanne et al. ([2019a](#bib.bib2)), more convolutional
    layers and one single BGRU layer were used. The convolutional part of the DoA
    network was transferred from the SED CRNN, which was followed by fine-tuning of
    the DoA branch, labelling this method as two-stage. This led to a notable improvement
    in localization performance over the DCASE Challenge baseline of Adavanne et al.
    ([2019a](#bib.bib2)). Small changes to this baseline were also tested by Pratik
    et al. ([2019](#bib.bib258)), such as the use of Bark-scale spectrograms as input
    features, the modification of the activation function or pooling layers, and the
    use of data augmentation, resulting in noticeable improvements for some experiments.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Lin 和 Wang ([2019](#bib.bib192)) 向输入频谱图中添加了高斯噪声，以训练网络对噪声更具鲁棒性。Lu ([2019](#bib.bib196))
    集成了一些额外的卷积层，并将 BGRU 层替换为双向 LSTM 层。Leung 和 Ren ([2019](#bib.bib182)) 使用了相同的架构，并结合了所有交叉通道功率谱，而
    Nustede 和 Anemüller ([2019](#bib.bib230)) 则测试了用组延迟替换输入特征。Maruri 等 ([2019](#bib.bib207))
    将 GCC-PHAT 特征作为输入特征添加。Zhang 等 ([2019a](#bib.bib380)) 在训练过程中使用了数据增强，并对网络输出进行了平均，以实现更稳定的
    DoA 估计。Xue 等 ([2019](#bib.bib364)) 将输入特征分别送入卷积层的不同分支，一方面是 log-mel 和恒定 Q 变换特征，另一方面是相位频谱图和
    CPS 特征（见第 [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods") 节）。Cao 等 ([2019b](#bib.bib37)) 将 log-mel 频谱图和 GCC-PHAT
    特征拼接，并将它们分别输入到两个独立的 CRNN 中用于 SED 和 DoA 估计（他们还将强度向量并入了 (Cao 等， [2019a](#bib.bib36)））。与
    Adavanne 等 ([2019a](#bib.bib2)) 的基线相比，使用了更多的卷积层和一个单独的 BGRU 层。DoA 网络的卷积部分从 SED
    CRNN 转移而来，随后对 DoA 分支进行了微调，将该方法标记为两阶段。这导致在 Adavanne 等 ([2019a](#bib.bib2)) 的 DCASE
    挑战基线上的定位性能显著提升。Pratik 等 ([2019](#bib.bib258)) 也测试了对该基线的小改动，例如使用 Bark 规模频谱图作为输入特征、修改激活函数或池化层以及使用数据增强，结果在一些实验中取得了显著改进。
- en: 'The same baseline neural architecture of Adavanne et al. ([2019a](#bib.bib2))
    was used by Kapka and Lewandowski ([2019](#bib.bib145)), with one separate (but
    identical, except for the output layer) CRNN instance for each subtask: source
    counting (up to two sources), DoA estimation of source 1 (if applicable), DoA
    estimation of source 2 (if applicable), and sound type classification. The authors
    showed that their method was more efficient than the baseline. Krause and Kowalczyk
    ([2019](#bib.bib159)) explored different manners of splitting the SED and DoA
    estimation tasks in a CRNN. While some configurations showed an improvement in
    SED, the localization accuracy was below the baseline for the reported experiments.
    Park et al. ([2019b](#bib.bib238)) investigated a combination of a gated linear
    unit (GLU, a convolutional block with a gated mechanism) and a trellis network
    (containing convolutional and recurrent layers, see the paper by Bai et al. ([2019](#bib.bib15))
    for details), yielding better results than the baseline. The authors extended
    this work for the DCASE 2020 Challenge by improving the overall architecture and
    investigating other loss functions (Park et al., [2020](#bib.bib239)). A non-direct
    DoA estimation scheme was also derived by Grondin et al. ([2019](#bib.bib103)),
    who estimated the TDoA using a CRNN, from which they inferred the DoA.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Kapka和Lewandowski（[2019](#bib.bib145)）使用了Adavanne等人（[2019a](#bib.bib2)）的相同基线神经架构，每个子任务有一个独立（但相同，除了输出层）CRNN实例：源计数（最多两个源）、源1的DoA估计（如适用）、源2的DoA估计（如适用）和声音类型分类。作者展示了他们的方法比基线更高效。Krause和Kowalczyk（[2019](#bib.bib159)）探索了在CRNN中分割SED和DoA估计任务的不同方式。虽然一些配置在SED上显示出改进，但在报告的实验中，定位准确性低于基线。Park等人（[2019b](#bib.bib238)）研究了门控线性单元（GLU，一种具有门控机制的卷积块）和格雷斯网络（包含卷积和递归层，详细信息见Bai等人（[2019](#bib.bib15)）的论文）的组合，获得了比基线更好的结果。作者通过改善整体架构和调查其他损失函数（Park等人，[2020](#bib.bib239)）扩展了这项工作以应对DCASE
    2020挑战。Grondin等人（[2019](#bib.bib103)）还提出了一种非直接DoA估计方案，他们使用CRNN估计TDoA，并从中推断DoA。
- en: 'We also found propositions of CRNN-based systems in the 2020 edition of the
    DCASE Challenge. Singla et al. ([2020](#bib.bib298)) used the same CRNN as in
    the baseline of Adavanne et al. ([2019a](#bib.bib2)), except that they did not
    use two separated output branches for SED and DoA estimation. Instead, they concatenated
    the SED output with the output of the previous layer to estimate the DoA. Song
    ([2020](#bib.bib300)) used separated neural networks similar to the one of Adavanne
    et al. ([2019a](#bib.bib2)) to address NoS estimation and DoA estimation in a
    sequential way. Multiple CRNNs were trained by Tian ([2020](#bib.bib323)): one
    to estimate the NoS (up to two sources), another to estimate the DoA assuming
    one active source, and another (same as the baseline) to estimate the DoAs of
    two simultaneously active sources. Cao et al. ([2020](#bib.bib38)) designed an
    end-to-end CRNN architecture to detect and estimate the DoA of possibly two instances
    of the same sound event. The addition of one-dimensional convolutional filters
    was investigated by Ronchini et al. ([2020](#bib.bib273)) to exploit the information
    along the feature axes. Sampathkumar and Kowerko ([2020](#bib.bib284)) augmented
    the baseline system of Adavanne et al. ([2019a](#bib.bib2)) by providing the network
    with more input features (log-mel spectrograms, GCC-PHAT, and intensity vector,
    see Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在2020年版的DCASE挑战中发现了基于CRNN系统的提案。Singla等人（[2020](#bib.bib298)）使用了与Adavanne等人（[2019a](#bib.bib2)）基线相同的CRNN，只是他们没有为SED和DoA估计使用两个分开的输出分支。而是将SED输出与前一层的输出连接起来以估计DoA。Song（[2020](#bib.bib300)）使用了类似于Adavanne等人（[2019a](#bib.bib2)）的分开神经网络，以顺序方式处理NoS估计和DoA估计。Tian（[2020](#bib.bib323)）训练了多个CRNN：一个用于估计NoS（最多两个源），另一个用于估计假设只有一个活动源的DoA，另一个（与基线相同）用于估计两个同时活动源的DoAs。Cao等人（[2020](#bib.bib38)）设计了一种端到端CRNN架构，用于检测和估计可能的两个相同声音事件的DoA。Ronchini等人（[2020](#bib.bib273)）调查了添加一维卷积滤波器以利用特征轴上的信息。Sampathkumar和Kowerko（[2020](#bib.bib284)）通过为网络提供更多输入特征（对数-梅尔频谱图、GCC-PHAT和强度向量，见第[5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods)节"）来增强Adavanne等人（[2019a](#bib.bib2)）的基线系统。
- en: '![Refer to caption](img/750355ca73afdb23856e07cfdf9780a9.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/750355ca73afdb23856e07cfdf9780a9.png)'
- en: 'Figure 4: The CRNN architecture of Adavanne et al. ([2018](#bib.bib1), [2019c](#bib.bib4),
    [2019a](#bib.bib2)), which has inspired numerous SELD systems. The input is the
    multichannel STFT-domain FOA magnitude and phase spectrogram. First, features
    are extracted by four successive convolutional layers with $64$ $3\times 3$ kernels,
    each followed by a max-pooling layer. Then two BGRU layers with $64$ units each
    and tanh activations are used to capture the temporal evolution of the extracted
    features. An intermediate SPS output is then computed using a time distributed
    feedforward layer (i.e., this layer is computed separately on each vector of the
    temporal axis). Then, two $16$-kernel convolution layers followed by a $32$-unit
    time distributed feedfoward layer and two $16$-units BGRU layers process the estimated
    SPS. A final $432$-unit time distributed feedforward layer with sigmoid activation
    function is employed to infer the DoA. Note: Reprinted from (Adavanne et al.,
    [2018](#bib.bib1)); copyright by IEEE; reprinted with permission.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Adavanne等人（[2018](#bib.bib1), [2019c](#bib.bib4), [2019a](#bib.bib2)）的CRNN架构，启发了众多SELD系统。输入是多通道STFT域FOA幅度和相位谱图。首先，通过四个连续的卷积层提取特征，每个卷积层有$64$个$3\times
    3$的卷积核，后跟一个最大池化层。然后使用两个BGRU层，每个层有$64$个单元和tanh激活函数，以捕捉提取特征的时间演变。接着，使用时间分布的前馈层计算中间SPS输出（即，此层在时间轴上的每个向量上分别计算）。然后，两个$16$-核卷积层，后跟一个$32$-单元的时间分布前馈层和两个$16$-单元的BGRU层处理估计的SPS。最后，使用具有sigmoid激活函数的$432$-单元时间分布前馈层来推断DoA。注：转载自（Adavanne等人，[2018](#bib.bib1)）；版权归IEEE所有；经许可转载。
- en: Independently of the DCASE Challenge, the CRNN of Adavanne et al. ([2019a](#bib.bib2))
    was adapted by Comminiello et al. ([2019](#bib.bib59)) to receive quaternion FOA
    input features, which slightly improved the CRNN performance. Perotin *et al.*
    proposed using a CRNN with bidirectional LSTM layers on the FOA pseudo-intensity
    vector to localize one (Perotin et al., [2018b](#bib.bib246)) or two (Perotin
    et al., [2019b](#bib.bib248)) speakers. They showed that this architecture achieves
    very good performance in simulated and real reverberant environments with static
    speakers (both types of input features are discussed in Section [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    This work was extended by Grumiaux et al. ([2021a](#bib.bib105)), who obtained
    a substantial improvement in performance over the CRNN of Perotin et al. ([2019b](#bib.bib248))
    by adding more convolutional layers with less max-pooling, to localize up to three
    simultaneous speakers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 不依赖于DCASE挑战，Adavanne等人（[2019a](#bib.bib2)）的CRNN被Comminiello等人（[2019](#bib.bib59)）改编以接收四元数FOA输入特征，这略微提高了CRNN的性能。Perotin
    *等人* 提出了使用具有双向LSTM层的CRNN，在FOA伪强度向量上定位一个（Perotin等人，[2018b](#bib.bib246)）或两个（Perotin等人，[2019b](#bib.bib248)）扬声器。他们展示了这种架构在模拟和真实混响环境中的静态扬声器上表现非常好（两种输入特征的讨论见[5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")节）。Grumiaux等人（[2021a](#bib.bib105)）在此工作的基础上进行了扩展，通过添加更多卷积层并减少最大池化，显著提高了相较于Perotin等人（[2019b](#bib.bib248)）的CRNN的性能，能够定位最多三个同时发言的扬声器。
- en: 'Non-square convolutional filters and a unidirectional LSTM layer were used
    in the CRNN architecture of Li et al. ([2018](#bib.bib184)). Xue et al. ([2020](#bib.bib365))
    presented a CRNN with two types of input features: the phase of the CPS and the
    signal waveforms. The former was first processed by a series of convolutional
    layers before being concatenated with the latter. Another improvement of the network
    of Adavanne et al. ([2019a](#bib.bib2)) was proposed by Komatsu et al. ([2020](#bib.bib154)),
    who replaced the classical convolutional blocks with GLUs, based on the hypothesis
    that GLUs are better suited for extracting relevant features from phase spectrograms.
    This has led to a notable improvement of localization performance compared to
    the baseline of Adavanne et al. ([2019a](#bib.bib2)). Bohlender et al. ([2021](#bib.bib28))
    proposed an extension of the system of Chakrabarty and Habets ([2019b](#bib.bib44)),
    in which LSTMs and temporal convolutional networks (TCNs) replaced the last dense
    layer of the former architecture. A TCN was made of successive 1D dilated causal
    convolutional layers with increasing dilated factors (Lea et al., [2017](#bib.bib174)).
    The authors showed that taking the temporal context into account with such temporal
    layers actually improves the localization accuracy.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 ([2018](#bib.bib184)) 在其 CRNN 架构中使用了非方形卷积滤波器和单向 LSTM 层。Xue 等人 ([2020](#bib.bib365))
    提出了一个具有两种输入特征的 CRNN：CPS 的相位和信号波形。前者首先经过一系列卷积层处理，然后与后者拼接。Adavanne 等人 ([2019a](#bib.bib2))
    的网络的另一个改进是由 Komatsu 等人 ([2020](#bib.bib154)) 提出的，他们用 GLUs 替代了经典的卷积块，基于 GLUs 更适合从相位谱图中提取相关特征的假设。与
    Adavanne 等人 ([2019a](#bib.bib2)) 的基线相比，这导致了定位性能的显著提高。Bohlender 等人 ([2021](#bib.bib28))
    提出了 Chakrabarty 和 Habets ([2019b](#bib.bib44)) 系统的扩展，其中 LSTMs 和时间卷积网络（TCNs）替代了原架构的最后一层密集层。TCN
    由具有递增扩展因子的连续 1D 膨胀因果卷积层组成 (Lea 等人，[2017](#bib.bib174))。作者展示了使用这样的时间层考虑时间上下文实际上提高了定位准确性。
- en: 'Finally, we can mention the original approach of Nguyen et al. ([2020c](#bib.bib223))
    in which a two-step hybrid approach with two CRNNs is used: In the first step,
    a first CRNN is used for SED and a single-source histogram-based (conventional)
    method is used for DoA estimation. In the second step, a second CRNN-based network,
    referred to as sequence matching network (SMN), is used to match the estimated
    sequences from the SED and DoA branches. This approach is motivated by the fact
    that overlapping sounds often have different onsets and offsets, and by matching
    the outputs of the two branches, an estimated DoA can be associated with the corresponding
    sound class. This approach was extended to localize moving sources in the framework
    of the DCASE 2020 Challenge (Nguyen et al., [2020b](#bib.bib222)), by adapting
    the resolution of the azimuth and elevation histograms and by using an ensemble
    of SMNs.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以提到 Nguyen 等人 ([2020c](#bib.bib223)) 的原始方法，其中使用了两个 CRNN 的两步混合方法：第一步，使用第一个
    CRNN 进行 SED，并使用单源直方图（传统）方法进行 DoA 估计。第二步，使用基于第二个 CRNN 的网络，称为序列匹配网络（SMN），来匹配来自 SED
    和 DoA 分支的估计序列。这种方法的动机是重叠声音通常具有不同的开始和结束，通过匹配两个分支的输出，可以将估计的 DoA 与相应的声音类别关联。该方法在
    DCASE 2020 挑战 (Nguyen 等人，[2020b](#bib.bib222)) 的框架中扩展到了移动源定位，通过调整方位和高度直方图的分辨率以及使用
    SMNs 的集成。
- en: 4.5 Residual neural networks
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 残差神经网络
- en: Residual neural networks were originally introduced by He et al. ([2016](#bib.bib117)),
    who pointed out that designing very deep networks can lead the gradients to explode
    or vanish due to the non-linear activation functions, as well as the degradation
    of the overall performance. Residual connections are designed to enable a feature
    to bypass a layer block in parallel to the conventional process through this layer
    block. This allows the gradients to flow directly through the network, usually
    leading to a better training.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 残差神经网络最初由 He 等人 ([2016](#bib.bib117)) 引入，他们指出设计非常深的网络可能会导致梯度爆炸或消失，这是由于非线性激活函数以及整体性能的退化。残差连接旨在使特征能够绕过层块并与常规过程并行进行。这使得梯度能够直接流经网络，通常导致更好的训练效果。
- en: To our knowledge, the first use of a network with residual connections for SSL
    was proposed by Yalta et al. ([2017](#bib.bib366)). As illustrated in Fig. [5](#S4.F5
    "Figure 5 ‣ 4.5 Residual neural networks ‣ 4 Neural network architectures for
    SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods"), this
    network includes three residual blocks, which are stacks of layers with one of
    the layers having residual connections with another layer deeper in the stack.
    Each of these blocks is made of three convolutional layers, the first and last
    of which are designed with $1\times 1$ filters, with the middle layer designed
    with $3\times 3$ filters. A residual connection is used between the input and
    output of each residual block. The same type of residual block was used for SSL
    by He et al. ([2019a](#bib.bib120), [2018b](#bib.bib119)) in parallel to sound
    classification as speech or non-speech. Suvorov et al. ([2018](#bib.bib310)) used
    a series of 1D convolutional layers with several residual connections for single-source
    localization, directly from the multichannel waveform.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，Yalta等人（[2017](#bib.bib366)）首次提出了使用残差连接的网络用于SSL。如图[5](#S4.F5 "Figure 5
    ‣ 4.5 Residual neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")所示，该网络包括三个残差块，每个块都是由多个层堆叠而成，其中一个层与更深层的另一个层具有残差连接。这些块中的每一个由三层卷积层组成，第一层和最后一层设计为$1\times
    1$滤波器，中间层设计为$3\times 3$滤波器。在每个残差块的输入和输出之间使用了残差连接。He等人（[2019a](#bib.bib120)，[2018b](#bib.bib119)）在进行声音分类（语音或非语音）时也使用了相同类型的残差块用于SSL。Suvorov等人（[2018](#bib.bib310)）使用了一系列1D卷积层和多个残差连接用于单一源定位，直接从多通道波形中获取。
- en: Pujol et al. ([2019](#bib.bib259), [2021](#bib.bib260)) integrated residual
    connections alongside 1D dilated convolutional layers with increasing dilation
    factors. They used the multichannel waveform as the network input. After the input
    layer, the architecture was divided into several subnetworks containing the dilated
    convolutional layers, which functioned as filter banks. Ranjan et al. ([2019](#bib.bib264))
    combined a modified version of the original ResNet architecture (He et al., [2016](#bib.bib117))
    with recurrent layers for SELD. This was shown to reduce the DoA error by more
    than 20° compared to the baseline of Adavanne et al. ([2019a](#bib.bib2)). Similarly,
    Bai et al. ([2021](#bib.bib14)) also used the ResNet model of (He et al., [2016](#bib.bib117))
    followed by two GRU layers and two fully-connected layers for SELD. Kujawski et al.
    ([2019](#bib.bib165)) also adopted the original ResNet architecture and applied
    it to the single-source localization problem.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Pujol等人（[2019](#bib.bib259)，[2021](#bib.bib260)）将残差连接与逐渐增大的1D膨胀卷积层结合使用。他们使用多通道波形作为网络输入。在输入层之后，架构被划分为几个子网络，其中包含作为滤波器组的膨胀卷积层。Ranjan等人（[2019](#bib.bib264)）将原始ResNet架构（He等人，[2016](#bib.bib117)）的修改版本与递归层结合用于SELD。研究表明，与Adavanne等人（[2019a](#bib.bib2)）的基线相比，这可以将DoA误差减少20°以上。同样，Bai等人（[2021](#bib.bib14)）也使用了（He等人，[2016](#bib.bib117)）的ResNet模型，后接两个GRU层和两个全连接层用于SELD。Kujawski等人（[2019](#bib.bib165)）也采用了原始的ResNet架构，并将其应用于单一源定位问题。
- en: Another interesting architecture containing residual connections was proposed
    by Naranjo-Alcazar et al. ([2020](#bib.bib217)) for the DCASE 2020 Challenge.
    Before the recurrent layers (consisting of two BGRUs), three residual blocks successively
    processed the input features. These residual blocks contained two residual convolutional
    layers, followed by a squeeze-excitation module (Hu et al., [2020](#bib.bib131)).
    These modules aim to improve the modeling of interdependencies between input feature
    channels compared to classical convolutional layers. Similar squeeze-excitation
    mechanisms were used by Sundar et al. ([2020](#bib.bib309)) for multi-source localization.
    Another combination of a residual network with squeeze-excitation blocks was reported
    by Huang and Perez ([2021](#bib.bib132)), who implemented it in the framework
    of a sample-level CNN (i.e., a CNN applied on the time-domain signal samples)
    (Lee et al., [2017](#bib.bib177)). The resulting blocks are further followed by
    two Conformer blocks (see the next subsection). The motivation for combining these
    different models was their observed effectiveness in other audio processing tasks
    such as SED.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Naranjo-Alcazar 等人 ([2020](#bib.bib217)) 为 DCASE 2020 挑战赛提出了另一种有趣的包含残差连接的架构。在递归层（由两个
    BGRUs 组成）之前，三个残差块依次处理输入特征。这些残差块包含两个残差卷积层，之后是一个 squeeze-excitation 模块（Hu 等人，[2020](#bib.bib131)）。这些模块旨在提高对输入特征通道之间相互依赖关系的建模能力，相较于传统的卷积层。类似的
    squeeze-excitation 机制被 Sundar 等人 ([2020](#bib.bib309)) 用于多源定位。Huang 和 Perez ([2021](#bib.bib132))
    报告了另一种将残差网络与 squeeze-excitation 块相结合的方法，他们在样本级 CNN 框架中实现了这一方法（即，应用于时间域信号样本的 CNN）（Lee
    等人，[2017](#bib.bib177)）。这些块进一步跟随两个 Conformer 块（见下一个小节）。结合这些不同模型的动机是它们在其他音频处理任务中（如
    SED）的有效性观察结果。
- en: Shimada et al. ([2020b](#bib.bib294)) and Shimada et al. ([2020a](#bib.bib293))
    adapted the MMDenseLSTM architecture, originally proposed by Takahashi et al.
    ([2018](#bib.bib314)) for sound source separation, to the SELD problem. This architecture
    consists of a series of blocks made of convolutions and recurrent layers with
    residual connections. Their system showed very good performance among the other
    participants to the DCASE 2020 Challenge. Wang et al. ([2020](#bib.bib347)) used
    an ensemble learning approach in which several variants of residual neural networks
    and recurrent layers were trained to estimate the DoA, achieving the best performance
    of the DCASE 2020 Challenge.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Shimada 等人 ([2020b](#bib.bib294)) 和 Shimada 等人 ([2020a](#bib.bib293)) 将 Takahashi
    等人 ([2018](#bib.bib314)) 原本为声音源分离提出的 MMDenseLSTM 架构适应到了 SELD 问题中。这种架构由一系列包含卷积和递归层的块组成，并带有残差连接。他们的系统在
    DCASE 2020 挑战赛中表现非常优秀。Wang 等人 ([2020](#bib.bib347)) 使用了一种集成学习方法，其中训练了多个残差神经网络和递归层的变体来估计
    DoA，实现了 DCASE 2020 挑战赛的最佳表现。
- en: '![Refer to caption](img/0cd56483e01ba69c030571705cfe7565.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0cd56483e01ba69c030571705cfe7565.png)'
- en: 'Figure 5: The residual neural network architecture used by Yalta et al. ([2017](#bib.bib366)).
    Three residual blocks are employed in this network, which are each composed of
    two convolutional layers with $32$ $1\times 1$ filters with another convolutional
    layer with $32$ $3\times 3$ filters in-between. For all three residual blocks,
    the input is added to the output with a residual connection, showed with a dashed
    arrow in this diagram. The authors show that the use of residual connections not
    only reduces the learning cost, but also improves the model performance. Note:
    Reprinted from (Yalta et al., [2017](#bib.bib366)); under Creative Commons Attribution-NoDerivatives
    4.0 International License.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: Yalta 等人 ([2017](#bib.bib366)) 使用的残差神经网络架构。在该网络中使用了三个残差块，每个块由两个 $32$ 个
    $1\times 1$ 过滤器的卷积层和一个包含 $32$ 个 $3\times 3$ 过滤器的卷积层组成。对于所有三个残差块，输入与输出通过残差连接相加，如图中虚线箭头所示。作者展示了残差连接的使用不仅减少了学习成本，而且提高了模型性能。注：转载自
    (Yalta 等人，[2017](#bib.bib366))；根据知识共享署名-禁止演绎 4.0 国际许可证。'
- en: Guirguis et al. ([2020](#bib.bib107)) designed a neural network with a TCN in
    addition to classical 2D convolutions and residual connections. Instead of using
    recurrent layers as usually considered, the architecture was composed of TCN blocks
    that were made of several residual blocks, including a 1D dilated convolutional
    layer with an increasing dilated factor. The authors showed that replacing recurrent
    layers with TCNs made the hardware implementation of the network more efficient
    while slightly improving the SELD performance compared to the baseline of Adavanne
    et al. ([2019a](#bib.bib2)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Guirguis等人（[2020](#bib.bib107)）设计了一个神经网络，其中除了经典的2D卷积和残差连接外，还加入了TCN。该架构并没有使用通常考虑的循环层，而是由TCN块组成，这些块由几个残差块组成，包括一个具有增加的扩张因子的1D扩张卷积层。作者表明，用TCN替换循环层使得网络的硬件实现更加有效，同时与Adavanne等人（[2019a](#bib.bib2)）的基线相比，稍微改善了SELD的性能。
- en: Yasuda et al. ([2020](#bib.bib373)) exploited a CRNN with residual connections
    in an indirect way for DoA estimation using an FOA pseudo-intensity vector input
    (see Section [5.5](#S5.SS5 "5.5 Intensity-based features ‣ 5 Input features ‣
    A Survey of Sound Source Localization with Deep Learning Methods")). A CRNN was
    first used to remove the reverberant part of the FOA pseudo-intensity vector,
    after which another CRNN was used to estimate a TF mask, which was applied to
    attenuate TF bins with a large amount of noise. The source DoA was finally estimated
    directly from the dereverberated and denoised pseudo-intensity vector.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Yasuda等人（[2020](#bib.bib373)）以间接方式利用带有残差连接的CRNN来进行方位角估计，使用FOA伪强度向量输入（见第[5.5](#S5.SS5
    "5.5 基于强度的特征 ‣ 5 输入特征 ‣ 深度学习方法中声源定位调查"）节）。首先使用CRNN来消除FOA伪强度向量的混响部分，然后使用另一个CRNN来估计TF掩膜，该掩膜应用于衰减具有大量噪声的TF频率。源方位角最终直接从去混响和去噪声的伪强度向量中估计出来。
- en: 4.6 Attention-based neural networks
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 基于注意力机制的神经网络
- en: An attention mechanism is a method that allows a neural network to put emphasis
    on vectors of a temporal sequence that are more relevant for a given task. Originally,
    attention was proposed by Bahdanau et al. ([2016](#bib.bib13)) to improve sequence-to-sequence
    models such as RNNs for machine translation. The general principle is to allocate
    a different weight to the vectors of the input sequence when using a combination
    of these vectors for estimating a vector of the output sequence. The model is
    trained to compute the optimal weights that reflect both the link between vectors
    of the input sequence (self-attention) and the relevance of the input vectors
    to explain each output vector (attention at the decoder). This pioneering work
    has inspired the now popular Transformer architecture proposed by Vaswani et al.
    ([2017](#bib.bib332)), which greatly improved the machine translation performance.
    In the Transformer, RNNs are removed, i.e. they are totally replaced by attention
    models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是一种允许神经网络专注于时间序列中对于给定任务更相关的向量的方法。最初，Bahdanau等人（[2016](#bib.bib13)）提出了注意力机制，用于改善诸如RNN在机器翻译等序列到序列模型。其一般原则是在使用这些向量的组合来估计输出序列的一个向量时，为输入序列的向量分配不同的权重。该模型被训练为计算反映输入序列向量之间的关系（自注意力）和解释每个输出向量的输入向量的相关性（解码器的注意力）的最佳权重。这项开创性工作启发了现在流行的Transformer架构，由Vaswani等人（[2017](#bib.bib332)）提出，并大大改善了机器翻译的性能。在Transformer中，RNN被移除，即它们完全被注意力模型取代。
- en: Attention models are now used in an increasing number of DL applications, including
    SSL. Phan et al. ([2020a](#bib.bib250), [b](#bib.bib251)) submitted an attention-based
    neural system for the DCASE 2020 Challenge. Their architecture was made of several
    convolutional layers, followed by a BGRU, after which a self-attention layer was
    used to infer the activity and the DoA of several distinct sound events at each
    timestep. Schymura et al. ([2020](#bib.bib290)) added an attention mechanism after
    the recurrent layers of a CRNN to output an estimation of the sound source activity
    and its azimuth/elevation. Compared to the baseline of Adavanne et al. ([2019a](#bib.bib2)),
    the addition of attention demonstrated a better use of temporal information for
    SELD. An extension of the system of Chakrabarty and Habets ([2019b](#bib.bib44))
    based on attention mechanisms has been proposed by Mack et al. ([2020](#bib.bib204)).
    Attention is employed to estimate binary masks to focus on frequency bins where
    the target source is dominant. The first attention stage appears right after the
    input layer (analogously to (Chakrabarty and Habets, [2019b](#bib.bib44)), their
    network uses phase spectrograms as inputs), while the second attention stage takes
    place after new features have been extracted using convolutional layers. Adavanne
    et al. ([2021](#bib.bib5)) used a self-attention layer after a GRU in order to
    estimate the association matrix which matches predictions and references. This
    solves the optimal assignment problem and resulted in large improvements in terms
    of localization error.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模型现在在越来越多的深度学习应用中使用，包括SSL。Phan等人（[2020a](#bib.bib250)，[b](#bib.bib251)）提交了一种基于注意力的神经系统用于DCASE
    2020挑战。他们的架构由若干卷积层组成，之后是一个BGRU，然后使用自注意力层来推断每个时间步的活动和多个不同声音事件的方向。Schymura等人（[2020](#bib.bib290)）在CRNN的递归层之后添加了一个注意力机制，以输出声音源活动及其方位/俯仰的估计。与Adavanne等人（[2019a](#bib.bib2)）的基线相比，添加注意力机制显示了在SELD中对时间信息的更好利用。Mack等人（[2020](#bib.bib204)）提出了一种基于注意力机制的Chakrabarty和Habets（[2019b](#bib.bib44)）系统的扩展。注意力被用来估计二进制掩码，专注于目标源占优势的频率区间。第一个注意力阶段出现在输入层之后（类似于（Chakrabarty和Habets，[2019b](#bib.bib44)），他们的网络使用相位谱图作为输入），而第二个注意力阶段则在使用卷积层提取新特征之后进行。Adavanne等人（[2021](#bib.bib5)）在GRU之后使用了自注意力层，以估计匹配预测和参考的关联矩阵。这解决了最优分配问题，并在定位误差方面取得了显著改进。
- en: Multi-head self-attention (MHSA), which is the parallel use of several Transformer-type
    attention models (Vaswani et al., [2017](#bib.bib332)), has also inspired SSL
    methods. In the DCASE 2021 Challenge, Emmanuel et al. ([2021](#bib.bib78)) employed
    a MHSA layer right after several convolution modules tailored to learn varying
    spectral characteristics. Yalta et al. ([2021](#bib.bib367)) proposed using the
    whole encoder part of the Transformer architecture, in addition to several convolutional
    layers, to extract features from the input data. Wang et al. ([2021](#bib.bib348))
    adapted the Conformer architecture, originally designed by Gulati et al. ([2020](#bib.bib109))
    for automatic speech recognition, to SSL. This architecture is composed of a feature
    extraction module based on ResNet and a MHSA module that learns local and global
    context representations. The authors demonstrated the benefit of using a specific
    data augmentation technique on this model. Zhang et al. ([2021](#bib.bib384))
    also employed this architecture in the DCASE 2021 Challenge. As briefly mentioned
    in the previous subsection, Conformer blocks were also used in the architecture
    proposed by Huang and Perez ([2021](#bib.bib132)), where they followed a sample-level
    CNN with residual connections and squeeze-excitation. A Conformer block was also
    used in the architecture proposed for SELD by Rho et al. ([2021](#bib.bib267)),
    after convolutional and fully-connected layers and before BGRU layers. Cao et al.
    ([2021](#bib.bib39)) positioned an 8-head attention layer after a series of convolutional
    layers to track the source location predictions over time for different sources
    (up to two sources in their experiments). Schymura et al. ([2021](#bib.bib291))
    used three 4-head self-attention encoders along the time axis after a series of
    convolutional layers before estimating the activity and location of several sound
    events (see Fig. [6](#S4.F6 "Figure 6 ‣ 4.6 Attention-based neural networks ‣
    4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")). This neural architecture showed an improvement
    over the DCASE Challenge baseline of Adavanne et al. ([2019a](#bib.bib2)). In
    the same line, Xinghao et al. ([2021](#bib.bib360)) replaced the conventional
    convolutional layers of the baseline with a combination of adaptive convolutional
    layers (using dilated convolutions with different dilation factors) and attention
    blocks. Another example of MHSA-based Transformer model for SSL can be found in
    the work of Park et al. ([2021a](#bib.bib240)). In this work, a pretrained model
    is fine-tuned with transfer learning. The output sequence corresponding to each
    3s-sequence of input data is averaged to provide one DoA estimation. Sudarsanam
    et al. ([2021](#bib.bib306)) enriched the CRNN baseline of Adavanne et al. ([2019a](#bib.bib2))
    with a set of several MHSA blocks followed by fully-connected layers. They provided
    an analysis of the influence of the number and dimension of the MHSA blocks (the
    optimal number was found to be 2) and the number of heads (optimal was 8), as
    well as the effect of positional embedding, normalization layers and residual
    connections. Grumiaux et al. ([2021b](#bib.bib106)) showed that replacing the
    recurrent layers of a CRNN with self-attention encoders yielded a notable reduction
    in the computation time. Moreover, the use of MHSA slightly improved localization
    performance upon the baseline CRNN architecture of Perotin et al. ([2019b](#bib.bib248))
    for the considered multiple speaker localization task.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力（MHSA），即并行使用多个Transformer类型的注意力模型（Vaswani等，[2017](#bib.bib332)），也启发了SSL方法。在DCASE
    2021挑战赛中，Emmanuel等人（[2021](#bib.bib78)）在几个卷积模块之后使用了MHSA层，以学习不同的光谱特征。Yalta等人（[2021](#bib.bib367)）提出在使用几个卷积层之外，还利用Transformer架构的整个编码器部分，从输入数据中提取特征。Wang等人（[2021](#bib.bib348)）将Conformer架构（最初由Gulati等人（[2020](#bib.bib109)）为自动语音识别设计）调整为SSL。这种架构包括一个基于ResNet的特征提取模块和一个学习局部及全局上下文表示的MHSA模块。作者展示了在该模型上使用特定数据增强技术的好处。Zhang等人（[2021](#bib.bib384)）在DCASE
    2021挑战赛中也使用了这种架构。如前一小节简要提到的，Conformer块也被用于Huang和Perez（[2021](#bib.bib132)）提出的架构中，他们在残差连接和挤压激励后的样本级CNN之后使用了Conformer块。Rho等人（[2021](#bib.bib267)）提出的用于SELD的架构中也使用了Conformer块，位于卷积和全连接层之后、BGRU层之前。Cao等人（[2021](#bib.bib39)）在一系列卷积层之后设置了一个8头注意力层，以跟踪不同源（在他们的实验中最多两个源）的源位置预测。Schymura等人（[2021](#bib.bib291)）在一系列卷积层之后，沿时间轴使用了三个4头自注意力编码器，在估计多个声音事件的活动和位置之前（见图[6](#S4.F6
    "Figure 6 ‣ 4.6 Attention-based neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")）。这种神经架构在Adavanne等人（[2019a](#bib.bib2)）的DCASE挑战基线之上显示了改进。类似地，Xinghao等人（[2021](#bib.bib360)）用自适应卷积层（使用不同膨胀因子的膨胀卷积）和注意力块的组合替代了基线中的传统卷积层。Park等人（[2021a](#bib.bib240)）的工作中也可以找到基于MHSA的Transformer模型用于SSL。在这项工作中，预训练模型通过迁移学习进行了微调。每个3秒输入数据序列对应的输出序列被平均，以提供一个DoA估计。Sudarsanam等人（[2021](#bib.bib306)）通过一组MHSA块和全连接层增强了Adavanne等人（[2019a](#bib.bib2)）的CRNN基线。他们分析了MHSA块的数量和维度（发现最佳数量为2）以及头数（最佳为8），以及位置嵌入、归一化层和残差连接的影响。Grumiaux等人（[2021b](#bib.bib106)）展示了用自注意力编码器替换CRNN的递归层显著减少了计算时间。此外，MHSA的使用在考虑的多说话人定位任务中略微提升了Perotin等人（[2019b](#bib.bib248)）的CRNN架构的定位性能。
- en: Finally, we can mention the use of cross-modal attention (CMA) models for SSL
    by Lee et al. ([2021a](#bib.bib178)). A CMA model is the generalization of self-attention
    with two data streams in place of one, which is used in the Transformer decoder
    (Vaswani et al., [2017](#bib.bib332)). Lee et al. ([2021a](#bib.bib178)) used
    two separate SED and DoA estimation CNN blocks to separately produce SED and DoA
    embeddings (this comes in contrast with most DCASE candidate systems where the
    first blocks are shared between SED and DoA estimation.) Then these embeddings
    are merged, first with a weighted linear combination and then with a second, more
    complex, alignment process using two mirrored CMA models. Finally, the SED and
    DoA outputs of the CMA modules are each sent to three parallel fully-connected
    networks for final estimation (this is because in the DCASE 2021 Challenge SELD
    Task, up to three sources can be simultaneously active).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以提到 Lee 等人（[2021a](#bib.bib178)）使用的跨模态注意力（CMA）模型用于 SSL。CMA 模型是自注意力的推广，具有两个数据流而不是一个，这在
    Transformer 解码器中使用（Vaswani 等人，[2017](#bib.bib332)）。Lee 等人（[2021a](#bib.bib178)）使用两个独立的
    SED 和 DoA 估计 CNN 模块分别生成 SED 和 DoA 嵌入（这与大多数 DCASE 候选系统相反，其中第一个模块在 SED 和 DoA 估计之间共享）。然后，这些嵌入被合并，首先通过加权线性组合，然后通过使用两个镜像
    CMA 模型的第二个更复杂的对齐过程。最后，CMA 模块的 SED 和 DoA 输出分别发送到三个并行的全连接网络进行最终估计（这是因为在 DCASE 2021
    挑战 SELD 任务中，最多可以同时激活三个源）。
- en: In a general manner, it appears that attention modules, and MHSA in particular,
    have a tendency to replace the recurrent units in the recent SSL DNNs, following
    the “Attention is all you need” seminal line of Vaswani et al. ([2017](#bib.bib332)).
    This is because compared to RNNs, attention modules can model longer-term dependencies
    at a lower computational cost and can highly benefit from parallel computations,
    especially at training time. This tendency is also observed in other application
    domains, as we will discuss in Section [9](#S9 "9 Conclusions and perspectives
    ‣ A Survey of Sound Source Localization with Deep Learning Methods").
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，注意力模块，特别是 MHSA，似乎有取代最近 SSL DNN 中递归单元的趋势，沿袭了 Vaswani 等人（[2017](#bib.bib332)）的“Attention
    is all you need”开创性观点。这是因为与 RNN 相比，注意力模块可以以更低的计算成本建模长期依赖，并且在训练时可以高度受益于并行计算。这一趋势在其他应用领域也有观察到，我们将在第
    [9](#S9 "9 结论与展望 ‣ 深度学习方法中的声源定位") 节中讨论。
- en: '![Refer to caption](img/605570d53ad3f9758574a3c94c970a8a.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/605570d53ad3f9758574a3c94c970a8a.png)'
- en: 'Figure 6: The self-attention-based neural network architecture of Schymura
    et al. ([2021](#bib.bib291)). The input is the multi-channel spectrogram shaped
    as a $K\times L\times 2C$ tensor, with $K$ the number of frequency bins, $L$ the
    number of frames, and $C$ the number of channels. A feature extraction is first
    done with convolutional layers (not detailed in the figure) to produce $\mathbf{z}_{k,n}$
    to which is attached a positional encoding vector $\mathbf{p}_{E}$. Then, a Transformer
    encoder computes a new representation of shape $K\times D_{E}$, which is used
    to compute the source activity $\gamma_{k,n}$ and the mean $\mathbf{\hat{x}}_{k,n}$
    of the multivariate Gaussian distributions representing the target sources’ location
    (the corresponding covariance matrix $\mathbf{\hat{\Sigma}}_{k,n}$ is computed
    via a parallel (simpler) mechanism.) Note: Reprinted from (Schymura et al., [2021](#bib.bib291));
    copyright by the authors; reprinted with permission.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Schymura 等人（[2021](#bib.bib291)）的基于自注意力的神经网络架构。输入是形状为 $K\times L\times 2C$
    的多通道谱图，其中 $K$ 是频率箱的数量，$L$ 是帧的数量，$C$ 是通道的数量。首先通过卷积层（图中未详细说明）进行特征提取，生成 $\mathbf{z}_{k,n}$，并附加一个位置编码向量
    $\mathbf{p}_{E}$。然后，Transformer 编码器计算出形状为 $K\times D_{E}$ 的新表示，用于计算源活动 $\gamma_{k,n}$
    和多变量高斯分布的均值 $\mathbf{\hat{x}}_{k,n}$，这些高斯分布代表目标源的位置（相应的协方差矩阵 $\mathbf{\hat{\Sigma}}_{k,n}$
    是通过一个平行的（更简单的）机制计算的）。注意：转载自（Schymura 等人，[2021](#bib.bib291)）；版权归作者所有；经许可转载。
- en: 4.7 Encoder-decoder neural networks
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 编码器-解码器神经网络
- en: 'An encoder-decoder network is an architecture made of two building blocks:
    an encoder, which is fed by the input features and outputs a specific representation
    of the input data, and a decoder, which transforms the new data representation
    from the encoder into the desired output data. Architectures following this principle
    have been largely explored in the DL literature due to their capacity to provide
    compact data representations in an unsupervised manner (Goodfellow et al., [2016](#bib.bib101)).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器网络是由两个构建块组成的架构：一个编码器，它接收输入特征并输出输入数据的特定表示；一个解码器，它将编码器的新数据表示转换为期望的输出数据。遵循这一原则的架构在深度学习文献中得到了广泛探索，因为它们能够以无监督的方式提供紧凑的数据表示（Goodfellow
    等人，[2016](#bib.bib101)）。
- en: 4.7.1 Autoencoder
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.1 自编码器
- en: An autoencoder (AE) is an encoder-decoder neural network that is trained to
    output a copy of its input. Often, the dimension of the encoder’s last layer output
    is small compared to the dimension of the data. This layer is then known as the
    bottleneck layer and it provides a compressed encoding of the input data. Originally,
    AEs were made of feed-forward layers, but this term is also contemporaneously
    used to designate AE networks with other types of layers, such as convolutional
    or recurrent layers. To the best of our knowledge, the first use of an AE for
    DoA estimation was reported by Zermini et al. ([2016](#bib.bib378)). They used
    a simple AE to estimate TF masks for each possible DoA, which were then used for
    source separation. An interesting AE-based method was presented by Huang et al.
    ([2020](#bib.bib135)), in which an ensemble of AEs were trained to reproduce the
    multichannel input signal at the output, with one AE per candidate source position.
    Since the common latent information among the different channels is the dry signal,
    each encoder approximately deconvolves the signal from a given microphone. These
    dry signal estimates should be similar provided that the source is indeed at the
    assumed position; hence, the localization is performed by finding the AE with
    the most consistent latent representation. However, it is not clear whether this
    model can generalize well to unseen source positions and acoustic conditions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AE）是一种编码器-解码器神经网络，经过训练以输出输入的副本。通常，编码器最后一层的输出维度相比于数据的维度较小。这一层被称为瓶颈层，它提供了输入数据的压缩编码。最初，AE由前馈层组成，但这个术语现在也用来指代具有其他类型层（如卷积层或递归层）的AE网络。据我们所知，Zermini
    等人 ([2016](#bib.bib378)) 首次使用AE进行方向估计。他们使用了一个简单的AE来估计每个可能的方向的TF掩膜，这些掩膜随后用于源分离。Huang
    等人 ([2020](#bib.bib135)) 提出了一个有趣的AE方法，其中训练了一个AE集合以在输出端重现多通道输入信号，每个候选源位置一个AE。由于不同通道间的共同潜在信息是干信号，每个编码器大致从给定的麦克风中解卷积信号。假设源确实在假定位置，这些干信号估计应该是相似的；因此，通过找到具有最一致潜在表示的AE来执行定位。然而，尚不清楚这个模型是否能很好地推广到未见过的源位置和声学条件。
- en: Le Moing et al. ([2020](#bib.bib172)) presented an AE with a large number of
    convolutional layers (and transposed convolutional layers, which are layers of
    the decoder that process the inverse operation of the corresponding convolutional
    layer at the encoder), which estimates the potential source activity of each subregion
    in the $(x,y)$ plane divided in a grid, making it possible to locate multiple
    sources. They evaluated several types of outputs (binary, Gaussian-based, and
    binary followed by regression refinement), each of which showed promising results
    on the simulated and real data. An extension of this work was presented in (Le Moing
    et al., [2021](#bib.bib173)), in which they proposed using adversarial training
    (see Section [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) to improve network performance on real data, as
    well as on microphone arrays unseen in the training set, in an unsupervised training
    scheme. To do this, they introduced a novel explicit transformation layer that
    helped the network to be invariant to the microphone array layout. Another encoder-decoder
    architecture was proposed by He et al. ([2021b](#bib.bib122)), in which a multichannel
    waveform was fed into a filter bank with learnable parameters, after which a 1D
    convolutional encoder-decoder network processed the filter bank output. The output
    of the last decoder was then fed separately into two branches, one for SED and
    the other for DoA estimation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Le Moing 等人（[2020](#bib.bib172)）提出了一种具有大量卷积层（以及转置卷积层，即处理与编码器中相应卷积层的逆操作的解码器层）的自编码器（AE），该自编码器可以估计
    $(x,y)$ 平面中每个子区域的潜在源活动，从而实现多个源的定位。他们评估了几种类型的输出（包括二值型、高斯型以及二值型后跟回归精炼），每种输出在模拟数据和真实数据上均显示出有前景的结果。这项工作的扩展在（Le
    Moing 等人，[2021](#bib.bib173)）中提出，其中他们建议使用对抗训练（参见第[8](#S8 "8 Learning strategies
    ‣ A Survey of Sound Source Localization with Deep Learning Methods)节"）来提高网络在真实数据上的性能，以及在训练集中未见过的麦克风阵列上，通过无监督训练方案实现。为此，他们引入了一种新型显式变换层，帮助网络对麦克风阵列布局保持不变。He
    等人（[2021b](#bib.bib122)）提出了另一种编码器-解码器架构，其中多通道波形被送入一个具有可学习参数的滤波器组，之后，1D卷积编码器-解码器网络处理滤波器组输出。最后解码器的输出被分别送入两个分支，一个用于声音事件检测（SED），另一个用于到达角（DoA）估计。
- en: An encoder-decoder structure with one encoder followed by two separate decoders
    was proposed by Wu et al. ([2021c](#bib.bib354)). Signals recorded from several
    microphone arrays were first transformed in the short-term Fourier transform domain
    (see Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) and then stacked in a 4D-tensor (whose dimensions
    were time, frequency, microphone array and microphone). This tensor was then sent
    to the encoder block, which was made of a series of convolutional layers followed
    by several residual blocks. The output of the encoder was then fed into two separate
    decoders, the first of which was trained to output a probability of source presence
    for each candidate $(x,y)$ region, while the second was trained in the same way
    but with a range compensation to make the network more robust. The same general
    encoder-decoder line was adopted in the 2D image mapping approach proposed by
    Wu et al. ([2021b](#bib.bib353)). Note that here, the network is composed of convolutional
    layers at the encoder and transposed convolutional layers at the decoder, which
    is typical for image mapping applications in computer vision.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人（[2021c](#bib.bib354)）提出了一种编码器-解码器结构，其中一个编码器后接两个独立的解码器。首先，将从多个麦克风阵列记录的信号转换到短时傅里叶变换域（参见第[5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods)节"），然后堆叠成一个
    4D 张量（其维度为时间、频率、麦克风阵列和麦克风）。该张量被送入由一系列卷积层和若干残差块组成的编码器块。编码器的输出被送入两个独立的解码器，第一个解码器训练以输出每个候选
    $(x,y)$ 区域的源存在概率，而第二个解码器则以相同的方式进行训练，但加入了范围补偿以增强网络的鲁棒性。Wu 等人（[2021b](#bib.bib353)）提出的
    2D 图像映射方法采用了相同的编码器-解码器结构。请注意，这里网络由编码器中的卷积层和解码器中的转置卷积层组成，这在计算机视觉中的图像映射应用中是典型的。
- en: An indirect use of an AE was proposed by Vera-Diaz et al. ([2020](#bib.bib337)),
    who used convolutional and transposed convolutional layers to estimate the TDoA
    from GCC-based input features. The main idea was to rely on the encoder-decoder
    capacity to reduce the dimension of the input data so that the bottleneck representation
    forced the decoder to output a smoother version of the TDoA. This technique was
    shown to outperform the classical GCC-PHAT method in the reported experiments.
    This work was extended in the presence of two sources (Vera-Diaz et al., [2021](#bib.bib338)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Vera-Diaz 等人（[2020](#bib.bib337)）提出了一种间接使用 AE 的方法，他们利用卷积层和转置卷积层从基于 GCC 的输入特征中估计
    TDoA。主要思想是依赖编码器-解码器的能力来减少输入数据的维度，使瓶颈表示迫使解码器输出更平滑的 TDoA 版本。这项技术在报告的实验中被证明优于传统的
    GCC-PHAT 方法。这项工作在存在两个源的情况下得到了扩展（Vera-Diaz 等人，[2021](#bib.bib338)）。
- en: 4.7.2 Variational autoencoder
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.2 变分自编码器
- en: A variational autoencoder (VAE) is a generative model that was originally proposed
    by Kingma and Welling ([2014](#bib.bib150)) and Rezende et al. ([2014](#bib.bib266))
    and is now very popular in the DL community. A VAE can be seen as a probabilistic
    version of an AE. Unlike a classical AE, a VAE learns a probability distribution
    of the data at the output of the decoder and also models the probability distribution
    of the so-called latent vector at the bottleneck layer, which makes the VAE strongly
    connected to the concept of unsupervised representation learning (Bengio et al.,
    [2013](#bib.bib20)). New data can thus be obtained with the decoder by sampling
    these distributions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）是一种生成模型，最初由 Kingma 和 Welling（[2014](#bib.bib150)）以及 Rezende 等人（[2014](#bib.bib266)）提出，现在在深度学习（DL）社区中非常流行。VAE
    可以看作是自编码器（AE）的概率版本。与传统的 AE 不同，VAE 在解码器的输出端学习数据的概率分布，同时也建模了所谓的潜在向量在瓶颈层的概率分布，这使得
    VAE 与无监督表示学习（Bengio 等人，[2013](#bib.bib20)）的概念紧密相关。因此，可以通过采样这些分布来使用解码器获得新数据。
- en: To our knowledge, Bianco et al. ([2020](#bib.bib25)) were the first to apply
    a VAE for SSL. Their VAE, made of convolutional layers, was trained to generate
    the phase of inter-microphone RTFs (see Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Relative
    transfer function (RTF) ‣ 5.1 Inter-channel features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")), jointly with a classifier
    that estimates the speaker’s DoA from the RTF phases. The interest of using a
    VAE is that this generative model, originally designed for unsupervised training,
    is here trained in a semi-supervised configuration using a large dataset of unlabeled
    RTF data together with a limited set of labeled data (RTF values + corresponding
    DoA labels). In such a limited labeled dataset configuration, this model was shown
    to outperform an SRP-PHAT-based method as well as a supervised CNN in reverberant
    scenarios. This semi-supervised (or weakly supervised) approach is further discussed
    in Section [9.1](#S9.SS1 "9.1 Adaptation to (limited sets of) real-world data
    ‣ 9 Conclusions and perspectives ‣ A Survey of Sound Source Localization with
    Deep Learning Methods"). An extension of this work has been further proposed in
    (Bianco et al., [2021](#bib.bib26)), with refined network architectures and more
    realistic acoustic scenarii.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，Bianco 等人（[2020](#bib.bib25)）是首个将 VAE 应用于 SSL 的研究者。他们的 VAE 由卷积层组成，经过训练以生成微音频
    RTF 的相位（参见第 [5.1.1](#S5.SS1.SSS1 "5.1.1 相对传递函数 (RTF) ‣ 5.1 通道间特征 ‣ 5 输入特征 ‣ 关于深度学习方法的声源定位综述")
    节），并与一个从 RTF 相位中估计说话者方向（DoA）的分类器联合训练。使用 VAE 的好处在于，这种生成模型最初是为无监督训练设计的，但在这里使用大规模未标记
    RTF 数据集以及有限标记数据（RTF 值 + 相应的 DoA 标签）的半监督配置进行训练。在这样的有限标记数据集配置中，证明该模型在混响场景中优于基于 SRP-PHAT
    的方法和监督 CNN。这种半监督（或弱监督）方法在第 [9.1](#S9.SS1 "9.1 适应（有限集合的）现实世界数据 ‣ 9 结论和展望 ‣ 关于深度学习方法的声源定位综述")
    节中进一步讨论。这项工作的扩展已在（Bianco 等人，[2021](#bib.bib26)）中提出，改进了网络架构，并提供了更现实的声学场景。
- en: 4.7.3 U-Net architecture
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.3 U-Net 架构
- en: A U-Net architecture is a particular fully-convolutional neural network originally
    proposed by Ronneberger et al. ([2015](#bib.bib274)) for biomedical image segmentation.
    In U-net, the input features are decomposed into successive feature maps throughout
    the encoder layers and then recomposed into “symmetrical” feature maps throughout
    the decoder layers, similarly to CNNs. Having the same dimension for feature maps
    at the same level in the encoder and decoder enables one to propagate information
    directly from an encoder level to the corresponding level of the decoder via residual
    connections. This leads to the typical U-shape schematization (see Fig. [7](#S4.F7
    "Figure 7 ‣ 4.7.3 U-Net architecture ‣ 4.7 Encoder-decoder neural networks ‣ 4
    Neural network architectures for SSL ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 架构是一种特定的全卷积神经网络，最初由 Ronneberger 等人提出 ([2015](#bib.bib274))，用于生物医学图像分割。在
    U-Net 中，输入特征通过编码器层分解为连续的特征图，然后通过解码器层重新组合成“对称”的特征图，类似于 CNNs。在编码器和解码器的相同层级上具有相同维度的特征图，可以通过残差连接直接从编码器层向对应的解码器层传播信息。这导致了典型的
    U 形图示（见图 [7](#S4.F7 "图7 ‣ 4.7.3 U-Net 架构 ‣ 4.7 编码器-解码器神经网络 ‣ 4 用于 SSL 的神经网络架构
    ‣ 基于深度学习方法的声音源定位调查")）。
- en: '![Refer to caption](img/3ec9a4eb000934e66050352358d7fd32.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3ec9a4eb000934e66050352358d7fd32.png)'
- en: 'Figure 7: The U-Net network architecture of Chazan et al. ([2019](#bib.bib47)).
    The input matrix $\mathcal{R}$ contains angular features extracted from the RTFs
    (see Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Relative transfer function (RTF) ‣ 5.1
    Inter-channel features ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) ($l$, $k$, and $M$ denote the time index, the frequency
    bin, and the number of microphones, respectively). Several stages of encoders
    (in blue) and decoders (in green) are used. At each encoder (or decoder) stage,
    two or three convolutional layers with $3\times 3$ kernels are employed to compute
    a new representation which is used as the input of the next encoder (or decoder,
    respectively), except for the bottleneck stage from which the output is fed as
    input into the upper-stage decoder. Residual connections are used to concatenate
    one encoder output to the input of the same stage decoder, to alleviate the loss
    information problem. The output of this system consists of one TF mask $p_{l,k}(\theta)$
    per considered DoA $\theta$. Note: Reprinted from (Chazan et al., [2019](#bib.bib47));
    copyright by IEEE; reprinted with permission.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Chazan 等人提出的 U-Net 网络架构 ([2019](#bib.bib47))。输入矩阵 $\mathcal{R}$ 包含从 RTFs
    中提取的角度特征（见第[5.1.1](#S5.SS1.SSS1 "5.1.1 相关转移函数 (RTF) ‣ 5.1 通道间特征 ‣ 5 输入特征 ‣ 基于深度学习方法的声音源定位调查")节）。其中
    $l$、$k$ 和 $M$ 分别表示时间索引、频率 bin 和麦克风数量。使用了几个编码器阶段（蓝色）和解码器阶段（绿色）。在每个编码器（或解码器）阶段，使用两个或三个
    $3\times 3$ 的卷积层来计算新的表示，该表示作为下一个编码器（或解码器）的输入，除了瓶颈阶段，其输出被作为输入送入上级解码器。使用残差连接将一个编码器的输出与同阶段解码器的输入连接，以缓解信息丢失问题。该系统的输出是每个考虑的
    DoA $\theta$ 的一个 TF 掩模 $p_{l,k}(\theta)$。注意：转载自 (Chazan 等人, [2019](#bib.bib47));
    版权所有 © IEEE; 经许可转载。
- en: Regarding SSL and DoA estimation, several works have been inspired by the original
    U-Net paper. Chazan et al. ([2019](#bib.bib47)) employed such an architecture
    to estimate one TF mask per considered DoA (see Fig. [7](#S4.F7 "Figure 7 ‣ 4.7.3
    U-Net architecture ‣ 4.7 Encoder-decoder neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")),
    in which each TF bin was associated with a single particular DoA. This spectral
    mask was finally applied for source separation. This system was extended by Hammer
    et al. ([2021](#bib.bib115)) to account for multiple moving speakers. Another
    joint localization and separation system based on a U-Net architecture was proposed
    by Jenrungrot et al. ([2020](#bib.bib141)). In this system, a U-Net was trained
    based on 1D convolutional layers and GLUs. The input is the multichannel raw waveform
    accompanied by an angular window that helps the network to perform separation
    on a particular zone. If the output of the network on the window is empty, no
    source is detected, otherwise, one or more sources are detected and the process
    is repeated with a smaller angular window, until the angular window reaches $2$°.
    This system shows interesting results on both synthetic and real reverberant data
    containing up to eight speakers.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SSL和DoA估计，几个工作受到了原始U-Net论文的启发。Chazan等人([2019](#bib.bib47))采用这种架构来估计每个考虑的DoA一个TF掩膜（见图[7](#S4.F7
    "Figure 7 ‣ 4.7.3 U-Net architecture ‣ 4.7 Encoder-decoder neural networks ‣ 4
    Neural network architectures for SSL ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")），其中每个TF bin与一个特定的DoA相关联。最终应用该光谱掩膜进行源分离。Hammer等人([2021](#bib.bib115))将该系统扩展以考虑多个移动扬声器。Jenrungrot等人([2020](#bib.bib141))提出了一种基于U-Net架构的联合定位和分离系统。在该系统中，基于1D卷积层和GLU训练了U-Net。输入是多通道原始波形，伴随一个角度窗口，帮助网络在特定区域进行分离。如果网络在窗口上的输出为空，则没有检测到源；否则，检测到一个或多个源，并且该过程会在较小的角度窗口中重复，直到角度窗口达到$2$°。该系统在包含多达八个扬声器的合成和真实混响数据上显示了有趣的结果。
- en: For the DCASE 2020 Challenge, a U-Net with several BGRU layers in-between the
    convolutional blocks, was proposed for SELD by Patel et al. ([2020](#bib.bib242)).
    The last transposed convolutional layer of this U-Net outputs a single-channel
    feature map per sound event, corresponding to its activity and DoA for all frames.
    This showed an improvement over the baseline of Adavanne et al. ([2019a](#bib.bib2))
    in terms of DoA error. Comanducci et al. ([2020a](#bib.bib57)) used a U-Net architecture
    in the second part of their proposed neural network to estimate the source coordinates
    $x$ and $y$. The first part, composed of convolutional layers, learns to map GCC-PHAT
    features to the so-called ray space (where source positions correspond to linear
    patterns, *cf.* (Bianchi et al., [2016](#bib.bib23))), which is an intermediate
    representation used as the input of the U-Net architecture.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DCASE 2020挑战赛，Patel等人([2020](#bib.bib242))提出了一种在卷积块之间具有多个BGRU层的U-Net用于声源定位（SELD）。该U-Net的最后一个转置卷积层输出每个声音事件的单通道特征图，表示其活动和方向（DoA）在所有帧中的情况。这显示出相较于Adavanne等人([2019a](#bib.bib2))的基线在DoA误差方面的改进。Comanducci等人([2020a](#bib.bib57))在他们提出的神经网络的第二部分中使用了U-Net架构来估计源坐标$x$和$y$。第一部分由卷积层组成，学习将GCC-PHAT特征映射到所谓的射线空间（其中源位置对应于线性模式，*参见*（Bianchi等人，[2016](#bib.bib23)）），这是作为U-Net架构输入的中间表示。
- en: 5 Input features
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 输入特征
- en: In this section, we provide an overview of the variety of input feature types
    found in the DL-based SSL literature. Generally, the considered features can be
    low-level signal representations such as waveforms or spectrograms, hand-crafted
    features such as binaural features, or they can be borrowed from traditional SP
    methods such as MUSIC or GCC-PHAT.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了基于DL的SSL文献中输入特征类型的多样性概述。一般而言，考虑的特征可以是低级信号表示，如波形或谱图，手工制作的特征，如双耳特征，或者它们可以借用自传统的SP方法，如MUSIC或GCC-PHAT。
- en: Overwhelmingly, the input features for the SSL neural networks are based on
    some representation readily used in signal processing, often emphasizing spatial
    and/or TF information embedded in the signal. This seems to yield good results,
    despite the growing trend in other domains to learn the feature representation
    directly from raw data. One interpretation may be that the network architectures
    in SSL are usually of a relatively modest size, as compared to end-to-end models
    used in some other domains, e.g., NLP. A few publications have compared different
    types of input features for SSL, e.g., (Roden et al., [2015](#bib.bib271); Krause
    et al., [2020b](#bib.bib161)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数SSL神经网络的输入特征都基于信号处理中常用的一些表示，通常强调信号中嵌入的空间和/或TF信息。尽管其他领域中的学习特征表示直接从原始数据中学习的趋势日益增长，但这似乎仍能产生良好的结果。一个解释可能是SSL中的网络结构通常相对较小，相对其他领域使用端到端模型（如自然语言处理）的规模较小。一些出版物已经比较了SSL的不同类型输入特征，例如（Roden
    et al.，[2015](#bib.bib271); Krause et al.，[2020b](#bib.bib161)）。
- en: It is also quite common to provide the network with concatenated features of
    different nature (even if these carry redundant information), which usually has
    positive impact on performance. This can be attributed to the flexibility of the
    learning process, which seemingly adapts the network weights such that the pertinent
    information is efficiently “routed” from such an input to the upper layers of
    the network, where it is merged into an abstract, optimized feature representation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 提供网络使用不同性质的特征进行拼接是非常常见的（即使这些特征携带冗余信息），这通常对性能有积极影响。这可以归因于学习过程的灵活性，学习过程似乎会调整网络权重，使相关信息有效地“路由”从输入到网络的上层，然后融合成一个抽象的优化特征表示。
- en: 'We organized this section into the following feature categories: inter-channel,
    cross-correlation-based, spectrogram-based, Ambisonics, intensity-based, and finally
    the direct use of the multichannel waveforms. Note that, as stated above, different
    kinds of features are often combined at the input layer of SSL neural networks.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本节组织成以下特征类别：通道间、基于互相关、基于谱图、Ambisonics、基于强度，最后是直接使用多声道波形。请注意，如上所述，不同类型的特征通常会在SSL神经网络的输入层进行组合。
- en: 5.1 Inter-channel features
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 通道间特征
- en: 5.1.1 Relative transfer function (RTF)
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 相对传递函数（RTF）
- en: The RTF is a very general inter-channel feature that has been widely used for
    conventional (non-deep) SSL and other spatial audio processing such as source
    separation and beamforming (Gannot et al., [2017](#bib.bib90)) and acoustic echo
    cancellation (Valero and Habets, [2017](#bib.bib327)), and is now considered for
    DL-based SSL as well. The RTF is defined for a given sound source position and
    for a microphone pair as the ratio $H(f)=A_{2}(f)/A_{1}(f)$ of the source-to-microphone
    ATFs of the two microphones, $A_{2}(f)$ and $A_{1}(f)$ (here we are working in
    the frequency or STFT domain and we recall that an ATF is the discrete Fourier
    transform of the corresponding RIR). It is thus strongly dependent on the source
    DoA (for a given recording set-up). In a multichannel set-up with more than two
    microphones, we can define an RTF for each microphone pair. Often, one microphone
    is used as a reference microphone, and the ATFs of all other microphones are divided
    by the ATF of this reference microphone.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: RTF是一种非常通用的通道间特征，已被广泛应用于传统（非深度）SSL和其他空间音频处理，如源分离和波束成形（Gannot et al.，[2017](#bib.bib90)）以及声学回声消除（Valero
    and Habets，[2017](#bib.bib327)），现在也被认为适用于基于DL的SSL。RTF针对给定声源位置和麦克风对被定义为两个麦克风的声源到麦克风的ATF的比值
    $H(f)=A_{2}(f)/A_{1}(f)$ （这里我们是在频率或STFT域中工作，我们回忆一下ATF是对应的RIR的离散傅里叶变换）。因此，它对于源的DoA（对于给定的记录设置）非常依赖。在具有两个以上麦克风的多声道设置中，我们可以为每个麦克风对定义一个RTF。通常，一个麦克风被用作参考麦克风，并且所有其他麦克风的ATF都除以该参考麦克风的ATF。
- en: 'As an ATF ratio, an RTF is thus a vector with an entry defined for each frequency
    bin. If only one directional source is present in the recorded signals and if
    the (diffuse) background noise is negligible, Eq. ([2](#S1.E2 "In 1.2 General
    principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) shows that an RTF estimate can be obtained for each
    STFT frame (indexed by $n$), each frequency bin, and each microphone pair (indexed
    by $i$ and $k$) by taking the ratio between the STFT transforms of the recorded
    waveforms of the two considered channels, $X_{i}(f,n)$ and $X_{k}(f,n)$:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 ATF 比率，RTF 是一个每个频率单元都有定义的向量。如果录制信号中只有一个定向源且背景噪声（扩散噪声）可以忽略，则方程 ([2](#S1.E2
    "In 1.2 General principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound
    Source Localization with Deep Learning Methods")) 表明可以为每个 STFT 帧（由 $n$ 索引）、每个频率单元和每对麦克风（由
    $i$ 和 $k$ 索引）获得一个 RTF 估计，通过取两个考虑的通道 $X_{i}(f,n)$ 和 $X_{k}(f,n)$ 的录制波形的 STFT 变换之间的比值：
- en: '|  | $\hat{H}_{i,k}(f)=\frac{X_{k}(f,n)}{X_{i}(f,n)}\approx\frac{A_{k}(f)S(f,n)}{A_{i}(f)S(f,n)}=\frac{A_{k}(f)}{A_{i}(f)}=H_{i,k}(f),$
    |  | (10) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{H}_{i,k}(f)=\frac{X_{k}(f,n)}{X_{i}(f,n)}\approx\frac{A_{k}(f)S(f,n)}{A_{i}(f)S(f,n)}=\frac{A_{k}(f)}{A_{i}(f)}=H_{i,k}(f),$
    |  | (10) |'
- en: where $S(f,n)$ is the STFT of the source signal. In the case where a background/sensor
    noise is present, more sophisticated RTF estimation procedures must be used, e.g.,
    (Cohen, [2004](#bib.bib55); Markovich-Golan and Gannot, [2015](#bib.bib206); Li
    et al., [2015](#bib.bib185)). If multiple sources are present, things become more
    complicated, but using the natural sparsity of speech/audio signals in the TF
    domain, *i.e.*, only at most one source is assumed to be active in each TF bin
    (Rickard, [2002](#bib.bib268)), the same principle as for one active source can
    be applied separately in each TF bin. Therefore, a multiple set of estimated RTFs
    at different frequencies (and possibly at different time frames if the sources
    are static or not moving too fast) can be used for multi-source localization.
    The reader is referred to (Gannot et al., [2017](#bib.bib90)) and references therein
    for more information on the RTF estimation problem.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S(f,n)$ 是源信号的短时傅里叶变换。如果存在背景/传感器噪声，则必须使用更复杂的 RTF 估计程序，例如 (Cohen, [2004](#bib.bib55);
    Markovich-Golan 和 Gannot, [2015](#bib.bib206); Li 等, [2015](#bib.bib185))。如果存在多个源，情况会变得更加复杂，但通过利用语音/音频信号在
    TF 域的自然稀疏性，*即*，假设每个 TF 单元中最多只有一个源是活动的 (Rickard, [2002](#bib.bib268))，可以在每个 TF
    单元中单独应用与一个活动源相同的原则。因此，可以使用在不同频率（如果源是静态的或移动不太快，可能在不同时间帧）上的多组估计 RTF 进行多源定位。有关 RTF
    估计问题的更多信息，请参见 (Gannot 等, [2017](#bib.bib90)) 及其参考文献。
- en: An RTF is a complex-valued vector. In practice, an equivalent real-valued pair
    of vectors is often used. We can use either the real and imaginary parts or the
    modulus and argument. Often, the log-squared value of the interchannel power ratio
    is used, *i.e.*, the interchannel power ratio in dB, and the argument of the RTF
    estimate ideally corresponds to the difference of the ATF phases. Such RTF-based
    representations have been used in several DNN-based systems for SSL. For example,
    Chazan et al. ([2019](#bib.bib47)), Hammer et al. ([2021](#bib.bib115)), and Bianco
    et al. ([2020](#bib.bib25), [2021](#bib.bib26)) used as input features the arguments
    of the measured RTFs obtained from all microphone pairs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: RTF 是一个复值向量。在实践中，通常使用等效的实值向量对。我们可以使用实部和虚部或模和幅角。通常使用的是互通道功率比的对数平方值，*即*，互通道功率比的
    dB 值，而 RTF 估计的幅角理想上对应于 ATF 相位的差异。这种基于 RTF 的表示法已在几个基于 DNN 的 SSL 系统中使用。例如，Chazan
    等 ([2019](#bib.bib47))、Hammer 等 ([2021](#bib.bib115)) 和 Bianco 等 ([2020](#bib.bib25),
    [2021](#bib.bib26)) 使用了从所有麦克风对中获得的测量 RTF 的幅角作为输入特征。
- en: 5.1.2 Binaural features
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 双耳特征
- en: Binaural features have also been used extensively for SSL, in both conventional
    and deep systems (Argentieri et al., [2015](#bib.bib11)). These features correspond
    to a specific two-channel recording set-up, one which attempts to reproduce human
    hearing in the most realistic way possible. Toward this aim, a dummy head/body
    with in-ear microphones is used to mimic the source-to-human-ear propagation,
    and in particular the effects of the head and external ear (pinnae), which are
    important for source localization by the human perception system. In an anechoic
    binaural set-up environment, the (two-channel) source-to-microphone impulse response
    is referred to as the binaural impulse response (BIR). The frequency-domain representation
    of a BIR is the HRTF. Both BIR and HRTF are functions of the source DoA. To take
    into account the room acoustics in a real-world SSL application, BIRs are extended
    to binaural room impulse responses (BRIRs), which combine head/body effects and
    room effects (in particular reverberation, see further discussion on BRIR simulation
    in Section [7.1](#S7.SS1 "7.1 Synthetic data ‣ 7 Data ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 双耳特征在SSL中也被广泛使用，无论是在传统系统还是深度系统中（Argentieri等，[2015](#bib.bib11)）。这些特征对应于特定的双通道录音设置，该设置试图以尽可能真实的方式再现人类听觉。为此，使用带耳内麦克风的假头/身体来模拟源到人耳的传播，特别是头部和外耳（耳廓）的影响，这些对人类感知系统的源定位非常重要。在无响室的双耳设置环境中，（双通道）源到麦克风的脉冲响应称为双耳脉冲响应（BIR）。BIR的频域表示即为HRTF。BIR和HRTF都是源方向的函数。为了在现实世界SSL应用中考虑房间声学，BIR被扩展为双耳房间脉冲响应（BRIR），它结合了头部/身体效应和房间效应（特别是混响，详见第[7.1节](#S7.SS1
    "7.1 Synthetic data ‣ 7 Data ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")中的BRIR模拟讨论）。
- en: 'Several binaural features are derived from binaural recordings: The interaural
    level difference corresponds to the short-term log-power magnitude of the ratio
    between the two binaural channels in the STFT domain, $X_{2}(f,n)$ and $X_{1}(f,n)$:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一些双耳特征源自双耳录音：耳间级差对应于STFT域中两个双耳通道之间比率的短期对数功率幅度，即$X_{2}(f,n)$和$X_{1}(f,n)$。
- en: '|  | $ILD(f,n)=20\log_{10}\displaystyle\left\lvert\frac{X_{2}(f,n)}{X_{1}(f,n)}\right\rvert.$
    |  | (11) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $ILD(f,n)=20\log_{10}\displaystyle\left\lvert\frac{X_{2}(f,n)}{X_{1}(f,n)}\right\rvert.$
    |  | (11) |'
- en: 'The interaural phase difference is the argument of this ratio:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 耳间相位差是该比率的辩值。
- en: '|  | $IPD(f,n)=\angle\frac{X_{2}(f,n)}{X_{1}(f,n)},$ |  | (12) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $IPD(f,n)=\angle\frac{X_{2}(f,n)}{X_{1}(f,n)},$ |  | (12) |'
- en: and the interaural time difference is the delay that maximizes the cross-correlation
    between the two channels, similarly to the TDoA in ([6](#S3.E6 "In 3 Conventional
    SSL methods ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    Just like the RTF, these features are actually vectors with frequency-dependent
    entries. In fact, the ILD and IPD are strongly related (not to say similar) to
    the log-power and argument of the RTF, as shown by comparing ([11](#S5.E11 "In
    5.1.2 Binaural features ‣ 5.1 Inter-channel features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")) and ([12](#S5.E12 "In
    5.1.2 Binaural features ‣ 5.1 Inter-channel features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")) with ([10](#S5.E10
    "In 5.1.1 Relative transfer function (RTF) ‣ 5.1 Inter-channel features ‣ 5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")),
    the difference relying more on the set-up than on the features themselves. The
    RTF can be seen as a more general (multichannel) concept, whereas binaural features
    refer to the specific two-channel binaural setup. As for the RTF, the ILD, IPD,
    and ITD implicitly encode the position of a source. Again, when several sources
    are present, the sparsity of speech/audio signals in the TF domain allows ILD/IPD/ITD
    values to provide information on the position of several simultaneously active
    sources.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 而耳间时间差是最大化两个通道之间的互相关延迟，类似于（[6](#S3.E6 "在3节传统SSL方法 ‣ 深度学习方法的声音源定位调查")）中的TDoA。就像RTF一样，这些特征实际上是具有频率依赖条目的向量。实际上，ILD和IPD与RTF的对数功率和相位密切相关（甚至可以说相似），如比较（[11](#S5.E11
    "在5.1.2双耳特征 ‣ 5.1 通道间特征 ‣ 5 输入特征 ‣ 深度学习方法的声音源定位调查")）和（[12](#S5.E12 "在5.1.2双耳特征
    ‣ 5.1 通道间特征 ‣ 5 输入特征 ‣ 深度学习方法的声音源定位调查")）与（[10](#S5.E10 "在5.1.1相对传递函数 (RTF) ‣ 5.1
    通道间特征 ‣ 5 输入特征 ‣ 深度学习方法的声音源定位调查")）相比，差异更多依赖于设置而不是特征本身。RTF可以被看作是一个更通用的（多通道）概念，而双耳特征则指的是特定的双通道双耳设置。至于RTF，ILD、IPD和ITD隐式编码了声音源的位置。同样，当存在多个声音源时，TF域中语音/音频信号的稀疏性允许ILD/IPD/ITD值提供关于多个同时活动源位置的信息。
- en: Youssef et al. ([2013](#bib.bib375)) used ILD and ITD vectors fed separately
    into specific input branches of an MLP. Ma et al. ([2015](#bib.bib201)) and Yiwere
    and Rhee ([2017](#bib.bib374)) concatenated the cross-correlation of the two binaural
    channels with the ILD before feeding it into the input layer of their network.
    Ma et al. ([2015](#bib.bib201)) justify this choice with two arguments. The first
    one is to avoid the noise-sensitivity of the peak-picking operation for the computation
    of the ITD, the second one is because of the systematic changes in the cross-correlation
    function according to the source azimuth. Nguyen et al. ([2018](#bib.bib220))
    used the IPD as the argument of a unitary complex number that was decomposed into
    real and imaginary parts. These parts were concatenated to the ILD for several
    frequency bins and several time frames, leading to a 2D tensor that was then fed
    into a CNN. Pang et al. ([2019](#bib.bib235)) also used a CNN to process ILD and
    IPD features in the TF domain, but the ILD and IPD 2D-tensors were directly concatenated
    at the input of the CNN. A system relying only on the IPD was proposed by Pak
    and Shin ([2019](#bib.bib233)). An MLP was trained to output a clean version of
    the noisy input IPD in order to better retrieve the DoA using a conventional method.
    Sivasankaran et al. ([2018](#bib.bib299)) used as input features the concatenation
    of the cosine and sine of the IPDs for several frequency bins and time frames.
    This choice was based on a previous work that showed similar performance for this
    type of input feature compared to classical phase maps, but with a lower dimension.
    In an original way, Thuillier et al. ([2018](#bib.bib322)) employed unusual binaural
    features. They used the ipsilateral and contralateral spectra. These features
    were shown to be relevant for elevation estimation using a CNN. We finally found
    other DNN-based systems that used ILD, e.g., (Roden et al., [2015](#bib.bib271);
    Zermini et al., [2016](#bib.bib378)), ITD, e.g., (Roden et al., [2015](#bib.bib271)),
    or IPD, e.g., (Shimada et al., [2020b](#bib.bib294), [a](#bib.bib293); Zermini
    et al., [2016](#bib.bib378); Subramanian et al., [2021b](#bib.bib305)) in addition
    to other types of features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Youssef 等人 ([2013](#bib.bib375)) 使用了分别输入到 MLP 的 ILD 和 ITD 向量。Ma 等人 ([2015](#bib.bib201))
    和 Yiwere 与 Rhee ([2017](#bib.bib374)) 将两个双耳通道的交叉相关与 ILD 连接在一起，然后输入到网络的输入层。Ma 等人
    ([2015](#bib.bib201)) 用两个论点来解释这一选择。第一个是为了避免在计算 ITD 时峰值选择操作的噪声敏感性，第二个是由于交叉相关函数随源方位角的系统性变化。Nguyen
    等人 ([2018](#bib.bib220)) 将 IPD 作为一个单位复数的参数，该复数被分解为实部和虚部。这些部分与 ILD 在多个频率槽和时间帧中连接，形成一个
    2D 张量，然后输入到 CNN 中。Pang 等人 ([2019](#bib.bib235)) 也使用 CNN 来处理 TF 域中的 ILD 和 IPD 特征，但
    ILD 和 IPD 2D 张量直接在 CNN 输入处连接。Pak 和 Shin ([2019](#bib.bib233)) 提出了一个仅依赖于 IPD 的系统。一个
    MLP 被训练来输出噪声输入 IPD 的清晰版本，以便使用传统方法更好地检索 DoA。Sivasankaran 等人 ([2018](#bib.bib299))
    以几个频率槽和时间帧的 IPD 的余弦和正弦的连接作为输入特征。这一选择基于之前的工作，显示这种输入特征与经典相位图相比表现相似，但维度更低。Thuillier
    等人 ([2018](#bib.bib322)) 以一种独特的方式使用了不寻常的双耳特征。他们使用了同侧和对侧谱。这些特征被证明对使用 CNN 进行高程估计是相关的。我们最终发现其他基于
    DNN 的系统使用了 ILD，例如 (Roden 等人，[2015](#bib.bib271); Zermini 等人，[2016](#bib.bib378))，ITD，例如
    (Roden 等人，[2015](#bib.bib271))，或 IPD，例如 (Shimada 等人，[2020b](#bib.bib294), [a](#bib.bib293);
    Zermini 等人，[2016](#bib.bib378); Subramanian 等人，[2021b](#bib.bib305))，以及其他类型的特征。
- en: 5.2 Cross-correlation (CC)-based features
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于交叉相关（CC）的特征
- en: Another manner for extracting and exploiting inter-channel information that
    depends on source location is to use features based on the cross-correlation (CC)
    between the signals of different channels. In particular, as seen in Section [3](#S3
    "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep
    Learning Methods"), a variant of CC known as GCC-PHAT is a common feature used
    in classical localization methods (Knapp and Carter, [1976](#bib.bib153)). It
    is less sensitive to speech signal variations than standard CC, but it may be
    adversely affected by noise and reverberation (Blandin et al., [2012](#bib.bib27)).
    Therefore, it has been used within the framework of neural networks, which was
    revealed to be robust to this type of disturbance/artefact. In several systems,
    GCC-PHAT has been computed for each microphone pair and several time delays, all
    concatenated to form a 1D vector used as the input of an MLP, e.g., (Xiao et al.,
    [2015](#bib.bib359); Vesperini et al., [2016](#bib.bib339); He et al., [2018a](#bib.bib118)).
    Other architectures include convolutional layers to extract useful information
    from multi-frame GCC-PHAT features, e.g., (He et al., [2018a](#bib.bib118); Vecchiotti
    et al., [2018](#bib.bib333), [2019b](#bib.bib335); Noh et al., [2019](#bib.bib226);
    Lu, [2019](#bib.bib196); Maruri et al., [2019](#bib.bib207); Pratik et al., [2019](#bib.bib258);
    Song, [2020](#bib.bib300); Li et al., [2018](#bib.bib184); Comanducci et al.,
    [2020a](#bib.bib57)).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 提取和利用依赖于源位置的通道间信息的另一种方式是使用基于不同通道信号之间的互相关（CC）的特征。特别是，如第[3](#S3 "3 Conventional
    SSL methods ‣ A Survey of Sound Source Localization with Deep Learning Methods")节所示，CC的一个变体，称为GCC-PHAT，是经典定位方法中常用的特征（Knapp和Carter，[1976](#bib.bib153)）。它对语音信号变化的敏感性低于标准CC，但可能会受到噪声和混响的负面影响（Blandin等，[2012](#bib.bib27)）。因此，它已经在神经网络框架中使用，这被揭示出对这类干扰/伪影具有鲁棒性。在几个系统中，GCC-PHAT被计算用于每对麦克风和几个时间延迟，所有这些都被串联起来形成一个1D向量，用作MLP的输入，例如，（Xiao等，[2015](#bib.bib359)；Vesperini等，[2016](#bib.bib339)；He等，[2018a](#bib.bib118)）。其他架构包括卷积层，从多帧GCC-PHAT特征中提取有用的信息，例如，（He等，[2018a](#bib.bib118)；Vecchiotti等，[2018](#bib.bib333)，[2019b](#bib.bib335)；Noh等，[2019](#bib.bib226)；Lu，[2019](#bib.bib196)；Maruri等，[2019](#bib.bib207)；Pratik等，[2019](#bib.bib258)；Song，[2020](#bib.bib300)；Li等，[2018](#bib.bib184)；Comanducci等，[2020a](#bib.bib57)）。
- en: Some SSL systems rely on the CPS, which we already mentioned in Section [3](#S3
    "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep
    Learning Methods") and which is linked to the CC by a Fourier transform operation
    (in practice, short-term estimates of the CPS are obtained by multiplying the
    STFT of one channel with the conjugate STFT of the other channel). Leung and Ren
    ([2019](#bib.bib182)) and Xue et al. ([2020](#bib.bib365)) sent the CPS into a
    CRNN architecture to improve localization performance over the baseline of Adavanne
    et al. ([2019a](#bib.bib2)) (see Section [4](#S4 "4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    Grondin et al. ([2019](#bib.bib103)) also used the cross-spectrum for each microphone
    pair in the convolutional block of their architecture, whereas GCC-PHAT features
    were concatenated in a deeper layer. The CPS was also used by Ma and Liu ([2018](#bib.bib202))
    as an input feature. Acoustic imaging has traditionally shown some interest in
    the CPS feature to predict localization and sound pressure level of competing
    sources; coupled with different architectures, from the simple MLP (Castellini
    et al., [2021](#bib.bib40)) to the complex CNN DenseNet network (Xu et al., [2021a](#bib.bib362)),
    authors have shown that the use of DNN could outperform traditional deconvolution
    methods, either in performance or computation time.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一些SSL系统依赖于CPS，这在第[3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")节中已经提到，并且通过傅里叶变换操作与CC相关联（在实践中，CPS的短期估计是通过将一个通道的STFT与另一个通道的共轭STFT相乘获得的）。Leung和Ren（[2019](#bib.bib182)）以及Xue等（[2020](#bib.bib365)）将CPS输入到CRNN架构中，以提高相对于Adavanne等（[2019a](#bib.bib2)）（参见第[4](#S4
    "4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")节）的定位性能。Grondin等（[2019](#bib.bib103)）也在其架构的卷积块中使用了每对麦克风的交叉谱，而GCC-PHAT特征被串联在更深的层中。CPS也被Ma和Liu（[2018](#bib.bib202)）用作输入特征。声学成像传统上对CPS特征表现出一定的兴趣，以预测竞争源的定位和声压级；结合不同的架构，从简单的MLP（Castellini等，[2021](#bib.bib40)）到复杂的CNN
    DenseNet网络（Xu等，[2021a](#bib.bib362)），作者们已经证明，使用DNN可以超越传统的去卷积方法，无论是性能还是计算时间。
- en: Traditional localization methods, such as MUSIC (Schmidt, [1986](#bib.bib288))
    or ESPRIT (Roy and Kailath, [1989](#bib.bib276)), have been widely examined in
    the literature (see Section [3](#S3 "3 Conventional SSL methods ‣ A Survey of
    Sound Source Localization with Deep Learning Methods")). These methods are based
    on the eigen-decomposition of the CC matrix of a multichannel recording. Several
    DNN-based SSL systems (Takeda and Komatani, [2016b](#bib.bib316), [a](#bib.bib315),
    [2017](#bib.bib317); Takeda et al., [2018](#bib.bib318)) have been inspired by
    these methods and reuse such features as input for their neural networks. Nguyen
    et al. ([2020a](#bib.bib221)) computed the spatial pseudo-spectrum based on the
    MUSIC algorithm and then used it as input features for a CNN.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的定位方法，如MUSIC（Schmidt，[1986](#bib.bib288)）或ESPRIT（Roy和Kailath，[1989](#bib.bib276)），在文献中已经被广泛研究（见第[3](#S3
    "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")节）。这些方法基于多通道录音的CC矩阵的特征值分解。几种基于DNN的SSL系统（Takeda和Komatani，[2016b](#bib.bib316)，[a](#bib.bib315)，[2017](#bib.bib317)；Takeda等人，[2018](#bib.bib318)）受到这些方法的启发，并将这些特征作为神经网络的输入。Nguyen等人（[2020a](#bib.bib221)）计算了基于MUSIC算法的空间伪谱，然后将其作为CNN的输入特征。
- en: Power map methods, which were discussed in Section [3](#S3 "3 Conventional SSL
    methods ‣ A Survey of Sound Source Localization with Deep Learning Methods"),
    have also been used to derive input features for DNN-based SSL systems. Salvati
    et al. ([2018](#bib.bib283)) proposed calculating the narrowband normalized steered
    response power for a set of candidate TDoAs corresponding to an angular grid and
    feeding it into a convolutional layer. This led to a localization performance
    improvement compared to the traditional SRP-PHAT method. Such power maps were
    also used by Diaz-Guerra et al. ([2021a](#bib.bib68)) as inputs of 3D convolutional
    layers. In acoustic imaging, a SRP map is also a standard feature where finding
    the position and the acoustic level is the main goal. Some recent works used a
    CNN (Gonçalves Pinto et al., [2021](#bib.bib99)) or a U-Net (Lee et al., [2021b](#bib.bib179))
    to produce clean deconvolved maps, hence going beyond the intrisic resolution
    of the array.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 功率图方法，如第[3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")节中讨论的，也被用于为基于DNN的SSL系统导出输入特征。Salvati等人（[2018](#bib.bib283)）提出计算一组候选TDoA的窄带标准化引导响应功率，这些TDoA对应于一个角度网格，并将其输入到卷积层中。这种方法相较于传统的SRP-PHAT方法提高了定位性能。Diaz-Guerra等人（[2021a](#bib.bib68)）也使用了这种功率图作为3D卷积层的输入。在声学成像中，SRP图也是一个标准特征，其主要目标是找到位置和声学水平。一些近期的研究使用了CNN（Gonçalves
    Pinto等人，[2021](#bib.bib99)）或U-Net（Lee等人，[2021b](#bib.bib179)）来生成干净的去卷积图，从而超越了阵列的固有分辨率。
- en: 5.3 Spectrogram-based features
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于声谱图的特征
- en: Alternatively to inter-channel features or CC-based features which already encode
    relative information between channels, another approach is to provide an SSL system
    directly with “raw” multichannel information, *i.e.*, without any pre-processing
    in the channel dimension.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是直接向SSL系统提供“原始”的多通道信息，*即*，在通道维度上不进行任何预处理，这与已经编码了通道间相对信息的通道间特征或基于CC的特征不同。
- en: This does not prevent some pre-processing in the other dimensions and, from
    an historical perspective, we notice that many models in this line use spectral
    or spectro-temporal features instead of raw waveforms (see next subsection) as
    inputs. In practice, (multichannel) STFT spectrograms are typically used (Vincent
    et al., [2018](#bib.bib341)). These multichannel spectrograms are generally organized
    as 3D tensors, with one dimension for time (or frames), one for frequency (bins),
    and one for channel. The general spirit of DNN-based SSL methods is that the network
    should be able to “see” by itself and automatically extract and exploit the differences
    between TF spectrograms along the channel dimension while exploiting the “sparsity”
    of TF signal representation.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不妨碍在其他维度上进行一些预处理，从历史角度来看，我们注意到许多这种类型的模型使用了光谱或光谱时间特征而不是原始波形（见下节）作为输入。在实践中，通常使用（多通道）STFT声谱图（Vincent等人，[2018](#bib.bib341)）。这些多通道声谱图通常组织为3D张量，其中一个维度表示时间（或帧），一个维度表示频率（频段），一个维度表示通道。基于DNN的SSL方法的一般精神是，网络应该能够“自我观察”并自动提取和利用通道维度上TF声谱图之间的差异，同时利用TF信号表示的“稀疏性”。
- en: In several works, the individual spectral vectors from the different STFT frames
    were provided independently to the neural model, meaning that the network did
    not take into account their temporal correlation (and a localization result is
    generally obtained independently for each frame). Thus, in that case, the network
    input is a matrix of size $M\times K$, with $M$ being the number of microphones,
    and $K$ being the number of considered STFT frequency bins. Hirvonen ([2015](#bib.bib128))
    concatenated the log-spectra of eight channels for each individual analysis frame
    and sent it into a CNN as a 2D matrix. Chakrabarty and Habets ([2019b](#bib.bib44),
    [2017a](#bib.bib41), [2017b](#bib.bib42), [a](#bib.bib43)) and Mack et al. ([2020](#bib.bib204))
    used the multichannel phase spectrogram as input features, disregarding the magnitude
    information. This choice is motivated by the fact that it allows to easily generate
    a training dataset from white noise signals. As an extension of this work, phase
    maps were also exploited by Bohlender et al. ([2021](#bib.bib28)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在几项工作中，不同STFT帧的单独光谱向量被独立地提供给神经网络，这意味着网络没有考虑它们的时间相关性（一般情况下，定位结果是针对每一帧独立获得的）。因此，在这种情况下，网络输入是一个大小为
    $M\times K$ 的矩阵，其中 $M$ 是麦克风的数量，$K$ 是考虑的STFT频率bin的数量。Hirvonen ([2015](#bib.bib128))
    将每个单独分析帧的八个通道的对数光谱连接起来，并作为2D矩阵输入到CNN中。Chakrabarty 和 Habets ([2019b](#bib.bib44),
    [2017a](#bib.bib41), [2017b](#bib.bib42), [a](#bib.bib43)) 和 Mack 等 ([2020](#bib.bib204))
    使用了多通道相位光谱图作为输入特征，而忽略了幅度信息。这一选择的动机是它允许从白噪声信号中轻松生成训练数据集。作为这项工作的扩展，Bohlender 等 ([2021](#bib.bib28))
    也利用了相位图。
- en: When several consecutive frames are considered, the STFT coefficients for multiple
    timesteps and multiple frequency bins form a 2D matrix for each recording channel.
    Usually, these spectrograms are stacked together in a third dimension to form
    the 3D input tensor. Several systems considered only the magnitude spectrograms,
    e.g., (Yalta et al., [2017](#bib.bib366); Wang et al., [2019](#bib.bib349); Patel
    et al., [2020](#bib.bib242); Pertilä and Cakir, [2017](#bib.bib249)), while others
    considered only the phase spectrogram, e.g., (Zhang et al., [2019b](#bib.bib381);
    Subramanian et al., [2021b](#bib.bib305)). When considering both magnitude and
    phase, they can also be stacked in a third dimension (as well as channels). This
    representation has been employed in many DNN-based SSL systems, e.g., (He et al.,
    [2021a](#bib.bib121); Guirguis et al., [2020](#bib.bib107); Krause et al., [2020a](#bib.bib160);
    Lin and Wang, [2019](#bib.bib192); Maruri et al., [2019](#bib.bib207); Zhang et al.,
    [2019a](#bib.bib380); Kapka and Lewandowski, [2019](#bib.bib145); Krause and Kowalczyk,
    [2019](#bib.bib159); Schymura et al., [2021](#bib.bib291)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑多个连续帧时，多个时间步长和多个频率bin的STFT系数为每个录音通道形成一个2D矩阵。通常，这些光谱图在第三维度上堆叠在一起形成3D输入张量。一些系统只考虑幅度光谱图，例如
    (Yalta 等, [2017](#bib.bib366); Wang 等, [2019](#bib.bib349); Patel 等, [2020](#bib.bib242);
    Pertilä 和 Cakir, [2017](#bib.bib249))，而其他系统则只考虑相位光谱图，例如 (Zhang 等, [2019b](#bib.bib381);
    Subramanian 等, [2021b](#bib.bib305))。当同时考虑幅度和相位时，它们也可以在第三维度（以及通道）中堆叠。这种表示方法已经在许多基于DNN的SSL系统中使用，例如
    (He 等, [2021a](#bib.bib121); Guirguis 等, [2020](#bib.bib107); Krause 等, [2020a](#bib.bib160);
    Lin 和 Wang, [2019](#bib.bib192); Maruri 等, [2019](#bib.bib207); Zhang 等, [2019a](#bib.bib380);
    Kapka 和 Lewandowski, [2019](#bib.bib145); Krause 和 Kowalczyk, [2019](#bib.bib159);
    Schymura 等, [2021](#bib.bib291))。
- en: Yang et al. ([2021a](#bib.bib368)) dedicated different input branches of their
    CRNN to magnitude and phase features. Other authors have proposed to decompose
    the complex-valued spectrograms into real and imaginary parts, e.g., (Hao et al.,
    [2020](#bib.bib116); He et al., [2018b](#bib.bib119); Le Moing et al., [2020](#bib.bib172);
    Küçük et al., [2019](#bib.bib167)). Finally, Leung and Ren ([2019](#bib.bib182))
    tried several combinations of features computed from the complex multi-channel
    spectrogram, including the magnitude and phase, the real and imaginary parts and
    the CPS. They claim that providing this redundant information could help the neural
    network for better localization.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Yang 等 ([2021a](#bib.bib368)) 将其CRNN的不同输入分支专门用于幅度和相位特征。其他作者则提议将复值光谱图分解为实部和虚部，例如
    (Hao 等, [2020](#bib.bib116); He 等, [2018b](#bib.bib119); Le Moing 等, [2020](#bib.bib172);
    Küçük 等, [2019](#bib.bib167))。最后，Leung 和 Ren ([2019](#bib.bib182)) 尝试了从复值多通道光谱图计算的几种特征组合，包括幅度和相位、实部和虚部以及CPS。他们声称，提供这些冗余信息可以帮助神经网络更好地进行定位。
- en: While basic (STFT) spectrograms consider equally-spaced frequency bins, mel-scale
    spectrograms and Bark-scale spectrograms are represented with a non-linear sub-bands
    division, corresponding to a perceptual scale (low-frequency sub-bands have a
    higher resolution than high-frequency sub-bands) (Peeters, [2004](#bib.bib244)).
    Mel-spectrograms were preferred to STFT spectrograms in several SSL neural networks,
    e.g., (Vecchiotti et al., [2018](#bib.bib333); Kong et al., [2019](#bib.bib155);
    Cao et al., [2019a](#bib.bib36), [b](#bib.bib37); Ranjan et al., [2019](#bib.bib264)).
    The Bark scale was also explored for spectrograms in the SSL system of Pratik
    et al. ([2019](#bib.bib258)).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的（STFT）谱图考虑了等间距的频率分 bins，而 mel 频谱图和 Bark 频谱图则用非线性的子带划分来表示，符合感知尺度（低频子带的分辨率高于高频子带）（Peeters，[2004](#bib.bib244)）。在多个
    SSL 神经网络中，mel 频谱图被优于 STFT 频谱图，例如，（Vecchiotti 等，[2018](#bib.bib333)；Kong 等，[2019](#bib.bib155)；Cao
    等，[2019a](#bib.bib36)，[b](#bib.bib37)；Ranjan 等，[2019](#bib.bib264)）。Bark 频率尺度也在
    Pratik 等人 ([2019](#bib.bib258)) 的 SSL 系统中被探索。
- en: 5.4 Ambisonic signal representation
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 Ambisonic 信号表示
- en: In the SSL literature, numerous systems utilize the Ambisonics format, *i.e.*,
    the spherical harmonics (SH) decomposition coefficients (Jarrett et al., [2017](#bib.bib140)),
    to represent the input signal. Ambisonics is a multichannel format that is increasingly
    used due to its capability to represent the spatial properties of a sound field,
    while being agnostic to the microphone array configuration (Zotter and Frank,
    [2019](#bib.bib386)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SSL 文献中，许多系统利用 Ambisonics 格式，*即*，球面谐波（SH）分解系数（Jarrett 等，[2017](#bib.bib140)），来表示输入信号。Ambisonics
    是一种多通道格式，由于其能够表示声音场的空间属性，并且对麦克风阵列配置具有无关性，越来越受到使用（Zotter 和 Frank，[2019](#bib.bib386)）。
- en: 'The SH decomposition is done for the acoustic pressure measured on the surface
    of a sphere $\mathbb{S}^{2}$, concentric with the microphone array. For a fixed
    sound source in far field, the decomposition coefficient of order $\ell$ and degree
    $m\in[-\ell,\ell]$, in the STFT domain, is given as follows Jarrett et al. ([2017](#bib.bib140)):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: SH 分解是在与麦克风阵列同心的球面 $\mathbb{S}^{2}$ 上测得的声压进行的。对于远场的固定声源，STFT 领域中顺序 $\ell$ 和度
    $m\in[-\ell,\ell]$ 的分解系数由 Jarrett 等人 ([2017](#bib.bib140)) 给出。
- en: '|  | $B_{\ell,m}(f,n)=\int_{\Omega\in\mathbb{S}^{2}}X(f,n,\Omega)Y^{*}_{\ell,m}(\Omega)d\Omega,$
    |  | (13) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $B_{\ell,m}(f,n)=\int_{\Omega\in\mathbb{S}^{2}}X(f,n,\Omega)Y^{*}_{\ell,m}(\Omega)d\Omega,$
    |  | (13) |'
- en: where $X(f,n,\Omega)$ and $Y_{\ell,m}(\Omega)$ are the acoustic pressure and
    the SH function, at the direction $\Omega$, respectively. In practice, this integral
    is approximated by a quadrature rule, since the number of microphones consisting
    an array is finite. Such approximation implies that the pressure $X(f,n,\Omega)$
    is assumed to be an (almost) “order-limited” function on the sphere Rafaely ([2019](#bib.bib263)),
    meaning that $B_{\ell>L,m}(f,n)=0$, for some maximal order $L$ (that depends on
    the number of microphones in the array). Hence, for FOA ($L=1$), the Ambisonics
    representation ([13](#S5.E13 "In 5.4 Ambisonic signal representation ‣ 5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    counts only $4$ coefficients (channels) per TF bin. Alternatively, the Higher-Order
    Ambisonics (HOA), $L>1$, signals have more than four channels.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X(f,n,\Omega)$ 和 $Y_{\ell,m}(\Omega)$ 分别是在方向 $\Omega$ 上的声压和 SH 函数。在实践中，由于麦克风阵列的数量有限，这个积分通过求积规则进行近似。这样的近似意味着压力
    $X(f,n,\Omega)$ 被假设为一个（几乎）“有序限制”的球面函数 Rafaely ([2019](#bib.bib263))，这意味着 $B_{\ell>L,m}(f,n)=0$，对于某个最大顺序
    $L$（取决于阵列中的麦克风数量）。因此，对于 FOA ($L=1$)，Ambisonics 表示（[13](#S5.E13 "In 5.4 Ambisonic
    signal representation ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")）每个 TF bin 仅包含 $4$ 个系数（通道）。另外，更高阶的 Ambisonics（HOA），$L>1$，信号有超过四个通道。
- en: 'The plane wave, bearing an amplitude $S(f,n)$, and coming from a direction
    $\Omega$, admits a simple SH representation $B_{\ell,m}(f,n)=S(f,n)Y_{\ell,m}(\Omega)$
    Rafaely ([2019](#bib.bib263)). Therefore, as opposed to other types of microphone
    arrays, the Ambisonic channels are in phase, since the spatial response of each
    channel $Y_{\ell,m}(\Omega)$ is TF-independent.²²2In practice, the spatial response
    of Ambisonic microphones is approximately frequency-independent only within certain
    bandwidth (dictated by the HOA order), due to spatial aliasing in the high frequency
    range, and noise amplification at lower frequencies Zotter and Frank ([2019](#bib.bib386)).
    Analogous to ([3](#S1.E3 "In 1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods")), the multichannel
    Ambisonic spectrogram $\mathbf{B}(f,n)$, due to $J$ sources and reverberation,
    is given by the multivariate expression:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 平面波，具有幅度 $S(f,n)$，并来自方向 $\Omega$，有一个简单的 SH 表示 $B_{\ell,m}(f,n)=S(f,n)Y_{\ell,m}(\Omega)$
    Rafaely ([2019](#bib.bib263))。因此，与其他类型的麦克风阵列相比，Ambisonic 通道是同相的，因为每个通道的空间响应 $Y_{\ell,m}(\Omega)$
    是 TF-独立的²²2在实践中，由于高频范围的空间混叠和低频下的噪声放大，Ambisonic 麦克风的空间响应在一定带宽内（由 HOA 阶数决定）大致是频率独立的
    Zotter 和 Frank ([2019](#bib.bib386))。类似于 ([3](#S1.E3 "In 1.2 General principle
    of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with
    Deep Learning Methods"))，由于 $J$ 个源和混响，多通道 Ambisonic 频谱图 $\mathbf{B}(f,n)$ 由多变量表达式给出：
- en: '|  | $\mathbf{B}(f,n)=\sum\limits_{j=1}^{J}\sum\limits_{r=0}^{\infty}A_{jr}(f,n)S_{j}(f,n)\mathbf{Y}(\Omega_{jr})+\mathbf{N}(f,n),$
    |  | (14) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{B}(f,n)=\sum\limits_{j=1}^{J}\sum\limits_{r=0}^{\infty}A_{jr}(f,n)S_{j}(f,n)\mathbf{Y}(\Omega_{jr})+\mathbf{N}(f,n),$
    |  | (14) |'
- en: where $A_{jr}$ is the amplitude of the $r$^(th) reflection of the source $S_{j}$
    (with $r=0$ corresponding to the direct path), $\mathbf{Y}$ is the vector whose
    entries are appropriate spherical harmonics $Y_{\ell,m}$ for all considered Ambisonic
    orders, and $\mathbf{N}$ is the additive noise vector. Note that the complex-valued
    amplitudes $A_{jr}$ account for the attenuation and phase shift of a corresponding
    plane wave component.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A_{jr}$ 是源 $S_{j}$ 的 $r$^(th) 反射的幅度（$r=0$ 对应直接路径），$\mathbf{Y}$ 是一个向量，其元素是所有考虑的
    Ambisonic 阶数的适当球面谐波 $Y_{\ell,m}$，而 $\mathbf{N}$ 是附加噪声向量。请注意，复值幅度 $A_{jr}$ 考虑了相应平面波分量的衰减和相位偏移。
- en: The FOA spectrograms, decomposed into magnitude and phase components, have been
    used by, e.g., Adavanne et al. ([2019c](#bib.bib4)); Guirguis et al. ([2020](#bib.bib107));
    Adavanne et al. ([2018](#bib.bib1), [2019a](#bib.bib2)); Kapka and Lewandowski
    ([2019](#bib.bib145)) and Krause and Kowalczyk ([2019](#bib.bib159)). Varanasi
    et al. ([2020](#bib.bib329)) and Poschadel et al. ([2021a](#bib.bib256), [b](#bib.bib257))
    used third-order Ambisonics spectrograms. Poschadel et al. ([2021a](#bib.bib256),
    [b](#bib.bib257)) compared the performance of a CRNN with HOA spectrograms from
    order $1$ to $4$, showing that the higher the order, the better the localization
    accuracy of the network (but still below the performance of the so-called FOA
    pseudo-intensity features, which we will discuss in Section [5.5](#S5.SS5 "5.5
    Intensity-based features ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")). They used the phase and magnitude for both elevation
    and azimuth estimation. Another way of representing the Ambisonics format was
    proposed by Comminiello et al. ([2019](#bib.bib59)). Based on the FOA spectrograms,
    they proposed considering them as quaternion-based input features, which proved
    to be a suitable representation in previous works (Parcollet et al., [2018](#bib.bib236)).
    To cope with this type of input feature, a neural network was adapted from the
    one of Adavanne et al. ([2019a](#bib.bib2)), showing an improvement over the baseline.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: FOA 声谱图被分解为幅度和相位成分，已被如 Adavanne 等人（[2019c](#bib.bib4)）；Guirguis 等人（[2020](#bib.bib107)）；Adavanne
    等人（[2018](#bib.bib1)、[2019a](#bib.bib2)）；Kapka 和 Lewandowski（[2019](#bib.bib145)）以及
    Krause 和 Kowalczyk（[2019](#bib.bib159)）等使用。Varanasi 等人（[2020](#bib.bib329)）和 Poschadel
    等人（[2021a](#bib.bib256)、[b](#bib.bib257)）使用了三阶 Ambisonics 声谱图。Poschadel 等人（[2021a](#bib.bib256)、[b](#bib.bib257)）比较了
    CRNN 在 HOA 声谱图从 $1$ 阶到 $4$ 阶的表现，显示出阶数越高，网络的定位精度越好（但仍低于所谓的 FOA 伪强度特征的性能，我们将在第 [5.5](#S5.SS5
    "5.5 Intensity-based features ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") 节中讨论）。他们使用相位和幅度来进行高程和方位角估计。Comminiello 等人（[2019](#bib.bib59)）提出了另一种表示
    Ambisonics 格式的方法。他们基于 FOA 声谱图，建议将其作为基于四元数的输入特征，这在之前的研究中证明是一种合适的表示方法（Parcollet
    等人，[2018](#bib.bib236)）。为应对这种输入特征，调整了来自 Adavanne 等人（[2019a](#bib.bib2)）的神经网络，显示出相较于基线的改进。
- en: 5.5 Intensity-based features
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 基于强度的特征
- en: 'Sound intensity is an acoustic quantity defined as the product of sound pressure
    and particle velocity (Jacobsen and Juhl, [2013](#bib.bib137); Rossing, [2007](#bib.bib275)).
    In the frequency or TF domain, sound intensity is a complex vector whose real
    part (known as “active” intensity) is proportional to the gradient of the phase
    of sound pressure, *i.e.*, it is orthogonal to the wavefront. This is a useful
    property that has been extensively used for SSL, e.g., (Nehorai and Paldi, [1994](#bib.bib219);
    Hickling et al., [1993](#bib.bib126); Jarrett et al., [2010](#bib.bib139); Tervo,
    [2009](#bib.bib320); Evers et al., [2014](#bib.bib82); Kitić and Guérin, [2018](#bib.bib151);
    Pavlidi et al., [2015](#bib.bib243)). The imaginary part (“reactive” intensity)
    is related to oscillatory local energy transfers, and its physical interpretation
    is less obvious (Maysenhölder, [1993](#bib.bib210)). Hence, it has been largely
    ignored by the SSL community, even though it is relevant in room acoustics Nolan
    et al. ([2019](#bib.bib227)). While the pressure is directly measurable by regular
    microphones, particle velocity requires specific sensors, such as acoustic vector-sensors
    (Nehorai and Paldi, [1994](#bib.bib219); Jacobsen and Juhl, [2013](#bib.bib137)),
    e.g., the “Microflown” transducer de Bree ([2003](#bib.bib64)). Otherwise, it
    has to be approximated using the acoustic pressure measurements. Under certain
    conditions, particle velocity can be assumed to be proportional to the spatial
    gradient of sound pressure (Rossing, [2007](#bib.bib275); Merimaa, [2006](#bib.bib214)),
    which allows for the estimation by, e.g., the finite difference method (Tervo,
    [2009](#bib.bib320)) or using the FOA channels discussed in the previous section
    (Zotter and Frank, [2019](#bib.bib386)). The latter approximation is often called
    (FOA) complex *pseudo-intensity* vector Jarrett et al. ([2010](#bib.bib139)):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 声强是一个声学量，定义为声压和粒子速度的乘积（Jacobsen 和 Juhl，[2013](#bib.bib137)；Rossing，[2007](#bib.bib275)）。在频率域或时频域，声强是一个复向量，其实部（称为“主动”强度）与声压相位的梯度成正比，*即*，它与波前正交。这是一个有用的属性，已被广泛用于
    SSL，例如（Nehorai 和 Paldi，[1994](#bib.bib219)；Hickling 等，[1993](#bib.bib126)；Jarrett
    等，[2010](#bib.bib139)；Tervo，[2009](#bib.bib320)；Evers 等，[2014](#bib.bib82)；Kitić
    和 Guérin，[2018](#bib.bib151)；Pavlidi 等，[2015](#bib.bib243)）。虚部（“反应”强度）与振荡局部能量转移相关，其物理解释不太明显（Maysenhölder，[1993](#bib.bib210)）。因此，尽管在房间声学中它是相关的，但它在
    SSL 社区中被大多忽视（Nolan 等，[2019](#bib.bib227)）。虽然压力可以通过常规麦克风直接测量，粒子速度需要特定的传感器，如声学矢量传感器（Nehorai
    和 Paldi，[1994](#bib.bib219)；Jacobsen 和 Juhl，[2013](#bib.bib137)），例如“Microflown”传感器
    de Bree（[2003](#bib.bib64)）。否则，它必须通过声压测量进行近似。在某些条件下，粒子速度可以被假设为与声压的空间梯度成正比（Rossing，[2007](#bib.bib275)；Merimaa，[2006](#bib.bib214)），这允许通过，例如，有限差分法（Tervo，[2009](#bib.bib320)）或使用前一节讨论的
    FOA 通道（Zotter 和 Frank，[2019](#bib.bib386)）来进行估计。后者的近似通常称为（FOA）复 *伪强度* 向量 Jarrett
    等（[2010](#bib.bib139)）：
- en: '|  | $\mathbf{I}(f,n)=B_{0,0}(f,n)\mathbf{B}_{\ell=1,m}(f,n)^{*},$ |  | (15)
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}(f,n)=B_{0,0}(f,n)\mathbf{B}_{\ell=1,m}(f,n)^{*},$ |  | (15)
    |'
- en: where $\mathbf{B}_{\ell=1,m}^{*}(f,n)$ is the vector of first-order SH coefficients,
    excluding the zero-order $B_{0,0}(f,n)$. In free field conditions, assuming the
    presence of a single source at the TF bin $(f,n)$, the entries of $\Re\left(\mathbf{I}(f,n)\right)$
    are the Cartesian coordinates of a vector colinear with the DoA of the source
    (with $\Re$ denoting the real part of a complex value).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{B}_{\ell=1,m}^{*}(f,n)$ 是一阶 SH 系数的向量，排除零阶 $B_{0,0}(f,n)$。在自由场条件下，假设
    TF bin $(f,n)$ 处存在一个单一源，$\Re\left(\mathbf{I}(f,n)\right)$ 的条目是与源的到达方向共线的向量的笛卡尔坐标（其中
    $\Re$ 表示复值的实部）。
- en: The first use of an Ambisonics pseudo-intensity vector for DL-based SSL was
    reported by Perotin et al. ([2018b](#bib.bib246)), showing superiority in performance
    compared to the use of the raw Ambisonics waveforms and traditional Ambisonics-based
    methods. Interestingly, the authors demonstrated that using both active and reactive
    intensity improves SSL performance. Moreover, they normalized the intensity vector
    of each frequency band by its energy, which can be shown to yield features similar
    to RTFs in the spherical harmonics domain (Daniel and Kitić, [2020](#bib.bib62);
    Jarrett et al., [2017](#bib.bib140)). Yasuda et al. ([2020](#bib.bib373)) proposed
    using two CRNNs to refine the input FOA pseudo-intensity vector. The first CRNN
    is trained to estimate denoising and separation masks under the assumption that
    there are two active sources and that the WDO hypothesis holds. The second CRNN
    estimates another mask to remove the remaining unwanted components (e.g., reverberation).
    The two networks, hence, produce an estimate of the “clean” intensity vector for
    each active source (the NoS is estimated by their system as well). The pseudo-intensity
    vector has consequently been used in several other recent works, e.g., (Grumiaux
    et al., [2021b](#bib.bib106); Perotin et al., [2019b](#bib.bib248); Grumiaux et al.,
    [2021a](#bib.bib105); Nguyen et al., [2021a](#bib.bib224); Cao et al., [2019a](#bib.bib36);
    Park et al., [2020](#bib.bib239); Song, [2020](#bib.bib300); Cao et al., [2021](#bib.bib39);
    Tang et al., [2019](#bib.bib319); Perotin et al., [2019a](#bib.bib247)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Perotin等人（[2018b](#bib.bib246)）首次使用了基于DL的SSL的Ambisonics伪强度向量，并显示了相比于原始Ambisonics波形和传统Ambisonics方法的性能优势。有趣的是，作者们展示了同时使用主动和反应强度可以提高SSL性能。此外，他们通过能量归一化每个频段的强度向量，这可以证明在球谐域中产生类似于RTFs的特征（Daniel和Kitić，[2020](#bib.bib62)；Jarrett等人，[2017](#bib.bib140)）。Yasuda等人（[2020](#bib.bib373)）提出使用两个CRNN来精炼输入FOA伪强度向量。第一个CRNN在假设存在两个主动源并且WDO假设成立的情况下，训练以估计去噪和分离掩膜。第二个CRNN估计另一个掩膜以去除剩余的噪声成分（例如，混响）。因此，这两个网络产生了每个主动源的“干净”强度向量估计（NoS也由他们的系统估计）。伪强度向量因此被用于一些其他近期的研究中，例如（Grumiaux等人，[2021b](#bib.bib106)；Perotin等人，[2019b](#bib.bib248)；Grumiaux等人，[2021a](#bib.bib105)；Nguyen等人，[2021a](#bib.bib224)；Cao等人，[2019a](#bib.bib36)；Park等人，[2020](#bib.bib239)；Song，[2020](#bib.bib300)；Cao等人，[2021](#bib.bib39)；Tang等人，[2019](#bib.bib319)；Perotin等人，[2019a](#bib.bib247)）。
- en: Sound intensity was also explored by Liu et al. ([2021](#bib.bib193)) without
    the Ambisonics representation. The authors computed the instantaneous complex
    sound intensity using an average of the sound pressure across the four considered
    channels and two orthogonal particle velocity components using the differences
    in sound pressure for both microphone pairs. They kept only the real part of the
    estimated sound intensity (active intensity) and applied a PHAT weighting to improve
    the robustness against reverberation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Liu等人（[2021](#bib.bib193)）也探讨了声音强度，但未使用Ambisonics表示。作者计算了瞬时复合声音强度，使用了四个考虑通道的声音压力平均值和两个正交的粒子速度分量，利用了两个麦克风对之间的声音压力差异。他们只保留了估计声音强度的实部（主动强度），并应用了PHAT加权以提高对混响的鲁棒性。
- en: 5.6 Waveforms
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 波形
- en: Since 2018, several authors have proposed directly providing their neural network
    models with the raw multichannel recorded signal waveforms. This idea relies on
    the DNN’s capability to find the best representation for SSL without the need
    of hand-crafted features or pre-processing of any kind. This is in line with the
    general trend of DL to go toward an end-to-end approach that is observed in many
    other applications, including in speech/audio processing. Of course, this goes
    together with the always increasing size of networks, datasets and computational
    power.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年以来，几位作者提出直接将原始多通道录制信号波形提供给他们的神经网络模型。这一想法依赖于DNN寻找SSL最佳表示的能力，无需手工特征或任何形式的预处理。这与DL的总体趋势一致，即朝着在许多其他应用中，包括语音/音频处理，观察到的端到端方法发展。当然，这也伴随着网络、数据集和计算能力的不断增加。
- en: To our knowledge, Suvorov et al. ([2018](#bib.bib310)) were the first to apply
    this idea. They trained their neural network directly with the recorded eight-channel
    waveforms, stacking many 1D convolutional layers to extract high-level features
    for the final DoA classification. Vera-Diaz et al. ([2018](#bib.bib336)), Vecchiotti
    et al. ([2019a](#bib.bib334)), Chytas and Potamianos ([2019](#bib.bib52)), Cao
    et al. ([2020](#bib.bib38)), and Pujol et al. ([2019](#bib.bib259), [2021](#bib.bib260))
    sent the raw multichannel waveforms into 2D convolutional layers. Huang and Perez
    ([2021](#bib.bib132)) sent the raw multichannel waveforms (in microphone format
    and FOA format) into a 1D CNN with residual connections and squeeze-excitation
    blocks. Note that this model is used for SELD and the authors motivate the use
    of raw waveform inputs by the fact that “SED and DOA may have some common features
    that are better preserved in the raw audio [wave]form.” Huang et al. ([2020](#bib.bib135))
    sent the multichannel waveforms into an AE. Jenrungrot et al. ([2020](#bib.bib141))
    shifted the waveforms of each channel to make them temporally aligned according
    to the TDoA before being injected into the input layer of their network. In the
    same vein, Huang et al. ([2018](#bib.bib133), [2019](#bib.bib134)) proposed time-shifting
    the multichannel signal by calculating the time delay between the microphone position
    and the candidate source location, which requires scanning for all candidate locations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，Suvorov 等人 ([2018](#bib.bib310)) 是首次应用这一思想的。他们直接用录制的八通道波形训练神经网络，通过堆叠多个
    1D 卷积层来提取高级特征，以进行最终的方向角（DoA）分类。Vera-Diaz 等人 ([2018](#bib.bib336))，Vecchiotti 等人
    ([2019a](#bib.bib334))，Chytas 和 Potamianos ([2019](#bib.bib52))，Cao 等人 ([2020](#bib.bib38))
    和 Pujol 等人 ([2019](#bib.bib259), [2021](#bib.bib260)) 将原始多通道波形输入到 2D 卷积层中。Huang
    和 Perez ([2021](#bib.bib132)) 将原始多通道波形（以麦克风格式和 FOA 格式）输入到带有残差连接和压缩激励块的 1D CNN
    中。需要注意的是，这个模型用于 SELD（声源定位和分类），作者通过“SED 和 DOA 可能有一些在原始音频[波]形中更好保留的共同特征”这一事实来说明使用原始波形输入的理由。Huang
    等人 ([2020](#bib.bib135)) 将多通道波形输入到 AE 中。Jenrungrot 等人 ([2020](#bib.bib141)) 在将每个通道的波形注入到网络的输入层之前，先将它们按
    TDoA（时间差）进行时间对齐。类似地，Huang 等人 ([2018](#bib.bib133), [2019](#bib.bib134)) 提出了通过计算麦克风位置和候选源位置之间的时间延迟来对多通道信号进行时间偏移，这需要对所有候选位置进行扫描。
- en: A potential disadvantage of waveform-based features is that the architectures
    exploiting such data are often more complex, as one part of the network needs
    to be dedicated to feature extraction. Moreover, some papers have reported that
    learning the “optimal” feature representations from raw data becomes more difficult
    when noise is present in the input signals (Wichern et al., [2019](#bib.bib350))
    or may even harm generalization, in some cases (Sato et al., [2021](#bib.bib285)).
    However, it is interesting to mention that the visual inspection of the learned
    weights of the input layers of some end-to-end (waveform-based) neural networks
    has revealed that they resemble the filterbanks that are usually applied in the
    pre-processing stage of SSL (see Section [5.3](#S5.SS3 "5.3 Spectrogram-based
    features ‣ 5 Input features ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")) and other various classical speech/audio processing tasks
    (Sainath et al., [2017](#bib.bib280); Luo and Mesgarani, [2019](#bib.bib199)).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 基于波形的特征的一个潜在缺点是，利用这些数据的架构通常更复杂，因为网络的一部分需要专门用于特征提取。此外，一些论文报告了在输入信号中存在噪声时，从原始数据中学习“最佳”特征表示变得更加困难（Wichern
    等人，[2019](#bib.bib350)），甚至在某些情况下可能会损害泛化能力（Sato 等人，[2021](#bib.bib285)）。然而，有趣的是，某些端到端（基于波形）神经网络的输入层学习权重的视觉检查显示，它们类似于通常在
    SSL（见第 [5.3](#S5.SS3 "5.3 Spectrogram-based features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")）和其他各种经典语音/音频处理任务中应用的滤波器组（Sainath
    等人，[2017](#bib.bib280); Luo 和 Mesgarani，[2019](#bib.bib199)）。
- en: 5.7 Other types of features
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 其他类型的特征
- en: Varzandeh et al. ([2020](#bib.bib331)) have proposed unusual types of features
    that do not belong to one of the categories described above. Particularly, they
    have used a periodicity degree feature together with GCC-PHAT features in a CNN.
    The periodicity degree is computed for a given frame and period. It is equal to
    the ratio between the harmonic power signal for the given period and the total
    power signal. This conveys information about the harmonic content of the source
    signal to the CNN.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Varzandeh 等人（[2020](#bib.bib331)）提出了一些不属于上述类别的特征。特别是，他们在 CNN 中使用了周期度特征以及 GCC-PHAT
    特征。周期度特征是针对给定帧和周期计算的，它等于给定周期的谐波功率信号与总功率信号之间的比率。这将源信号的谐波内容传达给 CNN。
- en: 6 Output strategies
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 输出策略
- en: 'In this section, we discuss the different strategies proposed in the literature
    to obtain a final DoA estimate. We generally divide the strategies into two categories:
    classification and regression. When the SSL network is designed for the classification
    task, the source location search space is generally divided into several zones,
    corresponding to different classes, and the neural network outputs a probability
    value for each class. As for regression, the goal is to directly estimate (continuous)
    source position/direction values, which are usually either Cartesian coordinates
    $(x,y,z)$, or spherical coordinates $(\theta,\phi,r)$ (although the source-microphone
    distance $r$ is very rarely considered). However, the latter is an important factor
    as it can affect the estimation accuracy (for instance, due to the influence of
    the direct-to-reverberant ratio (DRR) Vincent et al. ([2018](#bib.bib341))). Therefore,
    in order to obtain a robust model, the training dataset needs to be sufficiently
    diverse such that the network is exposed to sources at different directions, but
    also at different source-microphone distances. In the last subsection, we report
    a few non-direct methods in which the neural network does not estimate the location
    of a source in its output layer. Instead, it either helps another (conventional)
    algorithm to finally retrieve the desired DoA, or the location estimate is a byproduct
    of some intermediate network layer. A reader particularly interested in the comparison
    between the classification and regression approaches may consult the papers of
    Tang et al. ([2019](#bib.bib319)) and Perotin et al. ([2019a](#bib.bib247)).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了文献中提出的不同策略，以获得最终的 DoA 估计。我们通常将这些策略分为两类：分类和回归。当 SSL 网络设计用于分类任务时，源位置搜索空间通常被划分为几个区域，对应于不同的类别，神经网络为每个类别输出一个概率值。至于回归，目标是直接估计（连续的）源位置/方向值，这些值通常是笛卡尔坐标
    $(x,y,z)$，或者球面坐标 $(\theta,\phi,r)$（尽管源-麦克风距离 $r$ 很少被考虑）。然而，后者是一个重要因素，因为它会影响估计的准确性（例如，由于直接声与混响声比（DRR）的影响
    Vincent 等人（[2018](#bib.bib341)））。因此，为了获得一个鲁棒的模型，训练数据集需要足够多样，以便网络能够接触到来自不同方向的源，但也要接触到不同源-麦克风距离的数据。在最后一节中，我们报告了一些非直接方法，其中神经网络不会在其输出层估计源的位置。相反，它要么帮助其他（传统的）算法最终检索所需的
    DoA，要么位置估计是某些中间网络层的副产品。对分类和回归方法比较特别感兴趣的读者可以查阅 Tang 等人（[2019](#bib.bib319)）和 Perotin
    等人（[2019a](#bib.bib247)）的论文。
- en: 6.1 DoA estimation via classification
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 通过分类进行 DoA 估计
- en: 'Many systems treat DoA estimation as a classification problem, *i.e.*, each
    class represents a certain zone in the considered search space. In other words,
    space is divided into several subregions, usually of similar size, and the neural
    network is trained to produce a probability of active source presence for each
    subregion. Such a classification problem is often addressed by using a feedforward
    layer as the last layer in the network, with as many neurons as the number of
    considered subregions. Two activation functions are generally associated with
    the final layer neurons: the softmax and sigmoid functions. Softmax ensures that
    the sum of all neuron outputs is $1$, so it is suitable for a single-source localization
    scenario. With a sigmoid, all neuron outputs are within $[0,1]$ independently
    from each other, which is suitable for multi-source localization. The last layer
    output is often referred to as the spatial (pseudo)-spectrum, whose peaks correspond
    to a high probability of source activity in the corresponding zone.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 许多系统将 DoA 估计视为分类问题，即每个类别表示在考虑的搜索空间中的某个区域。换句话说，空间被划分为若干子区域，通常大小相似，神经网络被训练以为每个子区域产生一个活跃源存在的概率。这样的分类问题通常通过在网络的最后一层使用前馈层来解决，最后一层的神经元数量与考虑的子区域数量相等。最终层的神经元一般与两种激活函数相关：softmax
    和 sigmoid 函数。Softmax 确保所有神经元输出的总和为 $1$，因此适合单源定位场景。使用 sigmoid 时，所有神经元输出独立地位于 $[0,1]$
    之内，这适合多源定位。最后一层的输出通常被称为空间（伪）谱，其峰值对应于相应区域内源活动的高概率。
- en: 'As already mentioned in Section [2.3](#S2.SS3 "2.3 Number of sources ‣ 2 Acoustic
    environment and sound source configurations ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), the final DoA estimate(s) is/are generally extracted
    using a peak picking algorithm: If the number of sources $J$ is known, the selection
    of the $J$ highest peaks gives the multi-source DoA estimation; if the NoS is
    unknown, usually the peaks above a certain user-defined threshold are selected,
    leading to a joint NoS and localization estimations. Some preprocessing, such
    as spatial spectrum smoothing or angular distance constraints, can be used for
    better DoA estimation. Hence, such a classification strategy can be readily used
    for single-source and/or multi-source localization, as the neural network is trained
    to estimate a probability of source activity in each zone, regardless of the NoS.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 [2.3](#S2.SS3 "2.3 Number of sources ‣ 2 Acoustic environment and sound source
    configurations ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    节中所述，最终的 DoA 估计值通常通过峰值提取算法得出：如果已知源的数量 $J$，则选择 $J$ 个最高的峰值来进行多源 DoA 估计；如果源的数量未知，通常选择超过某个用户定义阈值的峰值，从而得到联合的源数量和定位估计。一些预处理步骤，如空间谱平滑或角距离约束，可以用于更好的
    DoA 估计。因此，这种分类策略可以轻松用于单源和/或多源定位，因为神经网络被训练来估计每个区域的源活动概率，无论源的数量如何。
- en: 6.1.1 Spherical coordinates
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 球坐标系
- en: Regarding the quantization of the source location space, namely the localization
    grid, different approaches have been proposed. Most early works focused on estimating
    only the source’s azimuth $\theta$ relative to the microphone array position,
    dividing the $360$° azimuth space into $N_{\theta}$ regions of equal size, leading
    to a grid quantization step of $\frac{360}{N_{\theta}}$. Without being exhaustive,
    we found in the literature many different values for $N_{\theta}$, e.g., $N_{\theta}=7$
    (Roden et al., [2015](#bib.bib271)), $N_{\theta}=8$ (Hirvonen, [2015](#bib.bib128)),
    $N_{\theta}=20$ (Suvorov et al., [2018](#bib.bib310)), $N_{\theta}=37$ (Vecchiotti
    et al., [2019a](#bib.bib334)), $N_{\theta}=72$ (Ma et al., [2015](#bib.bib201)),
    and $N_{\theta}=360$ (Xiao et al., [2015](#bib.bib359)). Some other works did
    not consider the whole $360$° azimuth space. For example, Chazan et al. ([2019](#bib.bib47))
    focused on the region $[0,180]$ with $N_{\theta}=13$.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 关于源位置空间的量化，即定位网格，已经提出了不同的方法。大多数早期工作集中于仅估计源相对于麦克风阵列位置的方位角 $\theta$，将 $360$° 方位角空间划分为
    $N_{\theta}$ 个相等大小的区域，从而得到网格量化步长 $\frac{360}{N_{\theta}}$。文献中出现了许多不同的 $N_{\theta}$
    值，例如 $N_{\theta}=7$（Roden 等，[2015](#bib.bib271)），$N_{\theta}=8$（Hirvonen，[2015](#bib.bib128)），$N_{\theta}=20$（Suvorov
    等，[2018](#bib.bib310)），$N_{\theta}=37$（Vecchiotti 等，[2019a](#bib.bib334)），$N_{\theta}=72$（Ma
    等，[2015](#bib.bib201)），以及 $N_{\theta}=360$（Xiao 等，[2015](#bib.bib359)）。一些其他工作没有考虑整个
    $360$° 方位角空间。例如，Chazan 等（[2019](#bib.bib47)）集中于区域 $[0,180]$，并使用 $N_{\theta}=13$。
- en: Estimating the elevation $\phi$ alone has not been frequently investigated in
    the literature, probably because of the lack of interesting applications in indoor
    scenarios. To the best of our knowledge, only one paper focused on estimating
    the elevation alone (Thuillier et al., [2018](#bib.bib322)). The authors divided
    the whole elevation range into nine regions of equal size. The majority of recent
    SSL neural networks are trained to estimate both source azimuth and elevation,
    whenever the microphone array geometry makes it possible. To do this, several
    options have been proposed in the literature. One can use two separate output
    layers, each with the same number of neurons as the number of subregions in the
    corresponding dimension. For example, the output layer of the neural architecture
    proposed by Fahim et al. ([2020](#bib.bib84)) is divided into two branches with
    fully connected layers, one for azimuth estimation ($N_{\theta}$ neurons), and
    the other for elevation estimation ($N_{\phi}$ neurons). One can also have a single
    output layer where each neuron corresponds to a zone in the unit sphere, *i.e.*,
    a unique pair $(\theta,\phi)$, e.g., (Grumiaux et al., [2021b](#bib.bib106); Perotin
    et al., [2019b](#bib.bib248)). Finally, one can directly design two separate neural
    networks, with each estimating the azimuth or the elevation angle, e.g., (Varanasi
    et al., [2020](#bib.bib329)).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 仅估计俯仰角 $\phi$ 在文献中并未频繁研究，可能是因为在室内场景中缺乏有趣的应用。据我们了解，只有一篇论文专注于单独估计俯仰角（Thuillier
    等人，[2018](#bib.bib322)）。作者将整个俯仰角范围划分为九个相等大小的区域。最近的大多数 SSL 神经网络被训练来估计源的方位角和俯仰角，只要麦克风阵列几何形状使之成为可能。为此，文献中提出了几种方案。可以使用两个独立的输出层，每个层的神经元数量与相应维度中的子区域数量相同。例如，Fahim
    等人（[2020](#bib.bib84)）提出的神经网络架构的输出层被分为两个分支，一个用于方位角估计（$N_{\theta}$ 个神经元），另一个用于俯仰角估计（$N_{\phi}$
    个神经元）。也可以有一个单独的输出层，其中每个神经元对应于单位球面上的一个区域，即一个独特的对 $(\theta,\phi)$，例如（Grumiaux 等人，[2021b](#bib.bib106)；Perotin
    等人，[2019b](#bib.bib248)）。最后，还可以直接设计两个独立的神经网络，每个网络估计方位角或俯仰角，例如（Varanasi 等人，[2020](#bib.bib329)）。
- en: However, most of the neural networks following the classification strategy for
    joint azimuth and elevation estimation are designed so that the output corresponds
    to a 2D grid on the unit sphere. For example, Perotin et al. ([2018b](#bib.bib246),
    [2019b](#bib.bib248)) and Grumiaux et al. ([2021a](#bib.bib105)) used a quasi-uniform
    spherical grid with $429$ classes, each represented by a unique neuron in the
    output layer of their network. Adavanne et al. ([2018](#bib.bib1)) sampled the
    unit sphere in the whole azimuth axis but in the limited elevation range of $[-60\degree,60\degree]$,
    yielding an output vector corresponding to $432$ classes.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数采用分类策略进行联合方位角和俯仰角估计的神经网络都设计为输出对应于单位球面上的二维网格。例如，Perotin 等人（[2018b](#bib.bib246)，[2019b](#bib.bib248)）和
    Grumiaux 等人（[2021a](#bib.bib105)）使用了一个准均匀球面网格，共有 $429$ 类，每类由网络输出层中的一个独特神经元表示。Adavanne
    等人（[2018](#bib.bib1)）对单位球面进行采样，覆盖整个方位角轴，但俯仰角范围有限为 $[-60\degree,60\degree]$，产生了一个对应
    $432$ 类的输出向量。
- en: 'Distance estimation has barely been investigated in the SSL literature, highlighting
    the fact that it is a difficult problem. Roden et al. ([2015](#bib.bib271)) addressed
    the distance estimation along with azimuth or elevation prediction by dividing
    the distance range into five candidate classes. Yiwere and Rhee ([2017](#bib.bib374))
    quantized the distance range into four classes and estimated it along with three
    possible azimuth values. In the paper by Takeda and Komatani ([2016b](#bib.bib316)),
    the azimuth axis was classified with $I=72$ classes along with the distance and
    height of the source, but these last two quantities were classified into a very
    small set of possible pairs: $(30,30)$, $(90,30)$ and $(90,90)$ (in centimeters).
    Bologni et al. ([2021](#bib.bib29)) trained a CNN to classify a single-source
    signal into a 2D map representing the azimuth and distance dimensions.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 距离估计在SSL文献中几乎没有被研究，这突显了这是一个困难的问题。Roden 等人（[2015](#bib.bib271)）通过将距离范围划分为五个候选类别来处理距离估计以及方位角或俯仰角预测。Yiwere
    和 Rhee（[2017](#bib.bib374)）将距离范围量化为四个类别，并结合三个可能的方位角值进行估计。在 Takeda 和 Komatani（[2016b](#bib.bib316)）的论文中，方位角轴被分类为
    $I=72$ 类，结合了源的距离和高度，但后两者被分类为一小组可能的对：$(30,30)$，$(90,30)$ 和 $(90,90)$（以厘米为单位）。Bologni
    等人（[2021](#bib.bib29)）训练了一个 CNN，将单一源信号分类为表示方位角和距离维度的二维地图。
- en: 6.1.2 Cartesian coordinates
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 笛卡尔坐标
- en: A few works applied the classification paradigm to estimate the Cartesian coordinates.
    Le Moing et al. ([2021](#bib.bib173), [2020](#bib.bib172)) and Ma and Liu ([2018](#bib.bib202))
    divided the horizontal $(x,y)$ plane into small regions of the same size, with
    each being a class in the output layer. However, this representation suffers from
    a decreasing angular difference between the regions that are far from the microphone
    array, which is probably why regression is usually preferred for estimating Cartesian
    coordinates.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究将分类范式应用于估计笛卡尔坐标。Le Moing 等人 ([2021](#bib.bib173), [2020](#bib.bib172)) 和
    Ma 和 Liu ([2018](#bib.bib202)) 将水平 $(x,y)$ 平面划分为大小相同的小区域，每个区域在输出层中作为一个类别。然而，这种表示方式存在一个缺陷，即远离麦克风阵列的区域之间的角度差逐渐减小，这可能是为什么通常更倾向于使用回归来估计笛卡尔坐标的原因。
- en: 6.2 DoA estimation via regression
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 通过回归进行方向角估计
- en: In regression SSL networks, the source location estimate is directly given by
    the continuous value provided by one or several output neurons (whether we consider
    Cartesian or spherical coordinates, and how many source coordinates are of interest).
    This technique offers the advantage of a potentially more accurate DoA estimation
    since there is no quantization. Its drawback is twofold. First, the NoS needs
    to be known or assumed, as there is no way to estimate if a source is active or
    not based on a localization regression. Second, regression-based SSL usually faces
    the well-known source permutation problem (Subramanian et al., [2021b](#bib.bib305)),
    which occurs in the multi-source localization configuration and is common with
    DL-based source separation methods. Indeed, during the computation of the loss
    function at the training time, there is an ambiguity in the association between
    target and actual output – in other words, which estimate should be associated
    with which target? This issue also arises during the evaluation. One possible
    solution is to force the SSL network training to be permutation invariant (Subramanian
    et al., [2021b](#bib.bib305)), in line with what was proposed for audio source
    separation (Yu et al., [2017](#bib.bib376)).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归 SSL 网络中，源位置的估计由一个或多个输出神经元提供的连续值直接给出（无论我们考虑笛卡尔坐标还是球面坐标，以及感兴趣的源坐标数量）。这种技术的优点在于，可能提供更准确的方向角估计，因为没有量化的影响。其缺点有两个方面。首先，需要知道或假设源的数量，因为基于定位回归无法估计源是否处于活动状态。其次，基于回归的
    SSL 通常面临着众所周知的源排列问题（Subramanian 等人，[2021b](#bib.bib305)），这种问题发生在多源定位配置中，并且在基于深度学习的源分离方法中很常见。确实，在训练时计算损失函数的过程中，目标和实际输出之间存在关联的模糊性——换句话说，哪个估计应该与哪个目标关联？在评估过程中也会出现这个问题。一个可能的解决方案是强制
    SSL 网络训练具备排列不变性（Subramanian 等人，[2021b](#bib.bib305)），这与为音频源分离所提出的方法一致（Yu 等人，[2017](#bib.bib376)）。
- en: As for classification, when using regression, there is a variety of possibilities
    for the type of coordinates to be estimated. The choice among these possibilities
    is driven more by the context or the application than by design limitations, since
    regression generally requires only a few output neurons.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类类似，使用回归时，有多种可能的坐标类型可以估计。这些可能性中的选择更多地受到背景或应用的驱动，而不是设计限制，因为回归通常只需要少量的输出神经元。
- en: 6.2.1 Spherical coordinates
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 球面坐标
- en: Tsuzuki et al. ([2013](#bib.bib325)) proposed a complex-valued neural approach
    for SSL. The output of the network is a complex number of unit amplitude whose
    argument is an estimate of the azimuth of the source. A direct regression scheme
    was employed by Nguyen et al. ([2018](#bib.bib220)) with a two-neuron output layer
    that predicts the azimuth and elevation values in a single-source environment.
    The system of Opochinsky et al. ([2019](#bib.bib231)) performed only azimuth estimation.
    Regarding the DCASE 2019 Challenge (Politis et al., [2020b](#bib.bib254)), a certain
    number of candidate systems have used two neurons per event type to estimate the
    azimuth and elevation of the considered event, e.g., (Chytas and Potamianos, [2019](#bib.bib52);
    Cao et al., [2019a](#bib.bib36); Park et al., [2019b](#bib.bib238)), while the
    event activity was jointly estimated in order to extract (or not) the corresponding
    coordinates. Sudo et al. ([2019](#bib.bib307)) proposed representing the output
    as a quaternion including the cosinus and sinus of the azimuth and elevation angles,
    from which they retrieve the DoA angle values. This enables to tackle the problem
    of discontinuity at angle interval boundaries (for instance, at $-180\degree$
    and $180\degree$).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Tsuzuki等人（[2013](#bib.bib325)）提出了一种用于SSL的复数值神经网络方法。网络的输出是一个单位幅度的复数，其幅角是源的方位角的估计值。Nguyen等人（[2018](#bib.bib220)）采用了一种直接回归方案，使用一个两神经元的输出层来预测单源环境中的方位角和仰角值。Opochinsky等人（[2019](#bib.bib231)）的系统仅进行方位角估计。关于DCASE
    2019挑战赛（Politis等人，[2020b](#bib.bib254)），一些候选系统使用每种事件类型两个神经元来估计考虑事件的方位角和仰角，例如（Chytas和Potamianos，[2019](#bib.bib52)；Cao等人，[2019a](#bib.bib36)；Park等人，[2019b](#bib.bib238)），同时联合估计事件活动以提取（或不提取）相应的坐标。Sudo等人（[2019](#bib.bib307)）提出将输出表示为包含方位角和仰角的余弦和正弦的四元数，从中提取DoA角度值。这有助于解决角度区间边界（例如，$-180\degree$和$180\degree$）的间断性问题。
- en: 'In the system of Maruri et al. ([2019](#bib.bib207)), azimuth and elevation
    estimations were done separately in two network branches, each containing a specific
    dense layer. Sundar et al. ([2020](#bib.bib309)) proposed a regression method
    relying on a preceding classification step: dividing the azimuth space into $I$
    equal subregions, with the output of the neural network being made of $3I$ neurons.
    Assuming there is at most one active source per subregion, three neurons are associated
    with each of them: one neuron is trained to detect the presence of a source, while
    the other two neurons estimate the distance and azimuth of that source. The loss
    function for training is a weighted sum of categorical cross-entropy (for the
    classification task) and mean square error (for the regression task).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在Maruri等人（[2019](#bib.bib207)）的系统中，方位角和仰角的估计分别在两个网络分支中进行，每个分支包含一个特定的密集层。Sundar等人（[2020](#bib.bib309)）提出了一种依赖于先前分类步骤的回归方法：将方位角空间划分为$I$个相等的子区域，神经网络的输出由$3I$个神经元构成。假设每个子区域最多有一个活动源，每个子区域关联三个神经元：一个神经元被训练用来检测源的存在，另外两个神经元则估计源的距离和方位角。训练的损失函数是分类任务的加权交叉熵和回归任务的均方误差的加权和。
- en: 6.2.2 Cartesian coordinates
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 笛卡尔坐标
- en: Another way to predict the DoA with regression is to estimate the Cartesian
    coordinates of the source(s). Vesperini et al. ([2016](#bib.bib339)) designed
    their network output layer with only two neurons to estimate the coordinates $x$
    and $y$ in the horizontal plane, with an output range normalized within $[0,1]$,
    which represents the scaled version of the room size in each dimension. Following
    the same idea, Vecchiotti et al. ([2018](#bib.bib333), [2019b](#bib.bib335)) also
    used two neurons to estimate $(x,y)$ but added a third one to estimate the source
    activity.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用回归预测DoA的方法是估计源的笛卡尔坐标。Vesperini等人（[2016](#bib.bib339)）设计了一个仅有两个神经元的网络输出层，用于估计水平面上的坐标$x$和$y$，输出范围在$[0,1]$内归一化，代表每个维度的房间大小的缩放版本。根据相同的思路，Vecchiotti等人（[2018](#bib.bib333)，[2019b](#bib.bib335)）也使用了两个神经元来估计$(x,y)$，但添加了一个第三个神经元来估计源的活动。
- en: The estimation of the three Cartesian coordinates $(x,y,z)$ has been investigated
    in several systems. Vera-Diaz et al. ([2018](#bib.bib336)) and Krause et al. ([2020a](#bib.bib160))
    designed the output layer with three neurons to estimate the coordinates of a
    single source with regression. Adavanne et al. ([2019c](#bib.bib4), [a](#bib.bib2))
    chose the same strategy. However, they performed SELD for several types of event,
    and thus there are three output neurons to provide $(x,y,z)$ estimates for each
    event type, plus another output neuron to estimate whether or not this event is
    active. The hyperbolic tangent activation function is used for the localization
    neurons to keep the output values in the $[-1,1]$ range, leading to a DoA estimate
    on the unit sphere. The same strategy was followed in an extension of this work
    by Comminiello et al. ([2019](#bib.bib59)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对三维笛卡尔坐标 $(x,y,z)$ 的估计已在多个系统中进行过研究。Vera-Diaz 等人 ([2018](#bib.bib336)) 和 Krause
    等人 ([2020a](#bib.bib160)) 设计了具有三个神经元的输出层，用于通过回归估计单一源的坐标。Adavanne 等人 ([2019c](#bib.bib4),
    [a](#bib.bib2)) 采用了相同的策略。然而，他们执行了多种类型事件的 SELD，因此有三个输出神经元为每种事件类型提供 $(x,y,z)$ 估计，并且还有一个额外的输出神经元用于估计该事件是否处于活动状态。对定位神经元使用双曲正切激活函数，以保持输出值在
    $[-1,1]$ 范围内，从而得到单位球上的方向角估计。Comminiello 等人 ([2019](#bib.bib59)) 在这项工作的扩展中也采用了相同的策略。
- en: In Shimada et al. ([2020a](#bib.bib293)), the authors proposed the activity-coupled
    cartesian DoA (ACCDOA) representation which encodes the DoA with the source activity
    in a single vector, separately for each sound class to be localized. More specifically,
    the ACCDOA vector encodes the Cartesian coordinates $(x,y,z)$, is then normalized
    and then multiplied by the source activity ($\in[0,1]$). Using a threshold, the
    active sources can be detected using this vector norm, and their respective DoAs
    can be retrieved from the normalized Cartesian coordinates. This ACCDOA output
    representation has then been used in other works, e.g., (Shimada et al., [2020b](#bib.bib294);
    Sudarsanam et al., [2021](#bib.bib306); Shimada et al., [2021](#bib.bib295); Nguyen
    et al., [2021b](#bib.bib225); Emmanuel et al., [2021](#bib.bib78); Naranjo-Alcazar
    et al., [2021](#bib.bib218)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Shimada 等人 ([2020a](#bib.bib293)) 的研究中，作者提出了活动耦合的笛卡尔方向角（ACCDOA）表示法，该方法将源活动与方向角编码为一个单独的向量，分别针对每个要定位的声音类别。更具体地说，ACCDOA
    向量编码了笛卡尔坐标 $(x,y,z)$，然后进行归一化，再乘以源活动 ($\in[0,1]$)。通过使用阈值，可以通过该向量范数检测活动源，并从归一化的笛卡尔坐标中检索其各自的方向角。该
    ACCDOA 输出表示法已在其他研究中得到应用，例如，(Shimada 等人，[2020b](#bib.bib294); Sudarsanam 等人，[2021](#bib.bib306);
    Shimada 等人，[2021](#bib.bib295); Nguyen 等人，[2021b](#bib.bib225); Emmanuel 等人，[2021](#bib.bib78);
    Naranjo-Alcazar 等人，[2021](#bib.bib218))。
- en: 6.3 Non-direct DoA estimation
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 非直接方向角估计
- en: Neural networks have also been used in the regression mode to estimate intermediate
    quantities, which are then used by a non-neural algorithm to predict the final
    DoA.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络也被应用于回归模式，以估计中间量，这些中间量随后被非神经算法用于预测最终的方向角（DoA）。
- en: Pertilä and Cakir ([2017](#bib.bib249)) proposed using a CNN in the regression
    mode to estimate a TF mask. This mask was then applied to the noisy multichannel
    spectrogram to obtain an estimate of the clean multichannel spectrogram, and a
    classical SRP-PHAT method was next applied to retrieve the final DoA. Another
    TF mask estimation was done by Wang et al. ([2019](#bib.bib349)) using a bidirectional
    LSTM network to improve traditional DoA estimation methods, such as GCC-PHAT or
    MUSIC. Pak and Shin ([2019](#bib.bib233)) trained an MLP to remove unwanted artefacts
    of the IPD input features. The cleaned feature was then used to estimate the DoA
    with a non-neural method. Yasuda et al. ([2020](#bib.bib373)) proposed a method
    to filter out reverberation and other non-desired effects from the intensity vector
    by TF mask estimation. The filtered intensity vector led to a better DoA estimation
    than an intensity-based conventional method. Yang et al. ([2021a](#bib.bib368))
    used a two-stage neural network system to estimate the direct-path RTF (DP-RTF),
    that is, the part of the RTF that corresponds to the direct source-to-microphone
    propagation (Li et al., [2016c](#bib.bib187)). In (Yang et al., [2021a](#bib.bib368)),
    the source DoA is the direction parameter of a DP-RTF taken from a dictionary
    of pre-computed DP-RTFs, corresponding to the closest match with the network estimate.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Pertilä 和 Cakir（[2017](#bib.bib249)）建议使用 CNN 回归模式来估计 TF 掩模。然后将该掩模应用于带噪声的多通道谱图，以获得干净的多通道谱图估计，接着应用经典的
    SRP-PHAT 方法来检索最终的 DoA。Wang 等人（[2019](#bib.bib349)）使用双向 LSTM 网络进行另一种 TF 掩模估计，以改进传统的
    DoA 估计方法，如 GCC-PHAT 或 MUSIC。Pak 和 Shin（[2019](#bib.bib233)）训练了一个 MLP 来去除 IPD 输入特征中的不需要的伪影。然后使用清理后的特征通过非神经方法来估计
    DoA。Yasuda 等人（[2020](#bib.bib373)）提出了一种通过 TF 掩模估计来过滤混响和其他不希望的效应的方法。过滤后的强度向量比基于强度的传统方法导致更好的
    DoA 估计。Yang 等人（[2021a](#bib.bib368)）使用两阶段神经网络系统来估计直接路径 RTF（DP-RTF），即 RTF 中与直接源到麦克风传播相对应的部分（Li
    等人，[2016c](#bib.bib187)）。在（Yang 等人，[2021a](#bib.bib368)）中，源 DoA 是从预计算的 DP-RTFs
    字典中获得的 DP-RTF 的方向参数，对应于与网络估计最接近的匹配。
- en: Huang et al. ([2018](#bib.bib133), [2019](#bib.bib134)) employed neural networks
    on multichannel waveforms, shifted in time with a delay corresponding to a certain
    candidate source location, to estimate the original dry signal. Doing this for
    a set of candidate locations, they then calculated the sum of CC coefficients
    between the estimated dry source signals for all candidate source locations. The
    final estimated location was obtained as the one leading to the maximum sum.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Huang 等人（[2018](#bib.bib133)，[2019](#bib.bib134)）使用神经网络处理多通道波形，这些波形在时间上有所偏移，延迟时间对应于某个候选源位置，以估计原始干信号。对一组候选位置进行此操作后，他们计算了所有候选源位置的估计干源信号之间的
    CC 系数之和。最终估计的位置是导致最大和的位置。
- en: A joint localization and separation scheme was proposed by Jenrungrot et al.
    ([2020](#bib.bib141)). The neural network was trained to estimate the signal coming
    from a certain direction within a certain angular window, whose parameters were
    injected as an input to each layer. Thus, the network acted like a radar and scanned
    through all directions, then progressively reduced the angular window up to a
    desired angular resolution.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Jenrungrot 等人（[2020](#bib.bib141)）提出了一种联合定位和分离方案。神经网络被训练来估计来自某个方向的信号，该方向在某个角度窗口内，其参数被注入到每一层中。因此，网络像雷达一样扫描所有方向，然后逐步缩小角度窗口，直到达到所需的角度分辨率。
- en: Several works proposed employing neural networks for a better prediction of
    the TDoA, which is then used to determine the DoA as often done in traditional
    methods. Grondin et al. ([2019](#bib.bib103)) estimated the TDoA in the regression
    mode using a hyperbolic tangent activation function at the output layer. Vera-Diaz
    et al. ([2020](#bib.bib337)) used an AE to estimate a function from GCC-based
    features (similar to TDoA) that exhibited a clear peak corresponding to the estimated
    DoA. Their work was extended in the presence of two sources (Vera-Diaz et al.,
    [2021](#bib.bib338)). In Comanducci et al. ([2020b](#bib.bib58)), the authors
    employed a U-Net in a regression manner to clean GCC-based features from noise
    and reverberation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究建议使用神经网络来更好地预测 TDoA，然后利用这些预测来确定 DoA，如传统方法中常做的那样。Grondin 等人（[2019](#bib.bib103)）使用超曲正切激活函数在输出层的回归模式中估计
    TDoA。Vera-Diaz 等人（[2020](#bib.bib337)）使用 AE 从基于 GCC 的特征（类似于 TDoA）中估计一个函数，该函数展示了一个与估计的
    DoA 对应的明显峰值。他们的工作在存在两个源的情况下进行了扩展（Vera-Diaz 等人，[2021](#bib.bib338)）。在 Comanducci
    等人（[2020b](#bib.bib58)）的研究中，作者采用 U-Net 回归方式来清理来自噪声和混响的基于 GCC 的特征。
- en: Subramanian et al. ([2021a](#bib.bib304)) proposed a neural system based on
    a stacked localization network, parametric beamformers and a speech recognition
    network. Since each of these modules is differentiable, the system is trained
    in the end-to-end mode, using an ASR-specific cost function. Despite being optimized
    for the ASR, the trained system also exhibits a very good performance in terms
    of source separation and localization, whose predictions are the intermediate
    results, retrievable at the output of the corresponding processing modules.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Subramanian 等人（[2021a](#bib.bib304)）提出了一种基于堆叠定位网络、参数波束形成器和语音识别网络的神经系统。由于这些模块都是可微分的，该系统最终采用端到端模式进行训练，使用
    ASR 特定的成本函数。尽管该系统针对 ASR 进行了优化，但其在源分离和定位方面也表现出非常好的性能，其预测结果是中间结果，可以在相应处理模块的输出中检索到。
- en: 7 Data
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 数据
- en: In this section, we detail the different approaches taken to deal with data
    during model training or testing. Because we are dealing with indoor domestic/office
    environments, noise and reverberation are common in real-world signals. We successively
    inspect the use of synthetic and recorded datasets in DNN-based SSL.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了在模型训练或测试过程中处理数据的不同方法。由于我们处理的是室内家庭/办公室环境，噪声和混响在真实世界信号中很常见。我们依次检查了在
    DNN 基于 SSL 的应用中使用合成数据集和记录数据集的情况。
- en: 7.1 Synthetic data
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 合成数据
- en: A well-known limitation of supervised learning (see Section [8](#S8 "8 Learning
    strategies ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    for SSL is the lack of labeled training data. In a general manner, it is difficult
    to produce datasets of recorded signals with corresponding source position metadata
    in diverse spatial configurations (and possibly with diverse spectral content)
    that would be sufficiently large for efficient SSL neural model training. Therefore,
    one often has to simulate a large amount of data to obtain an efficient SSL system.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习（参见第 [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source Localization
    with Deep Learning Methods) 节") 在 SSL 中的一个著名局限性是缺乏标记的训练数据。一般而言，生产具有不同空间配置（可能还有不同光谱内容）的记录信号数据集是困难的，这些数据集需要足够大以便有效地训练
    SSL 神经模型。因此，人们通常需要模拟大量数据来获得一个高效的 SSL 系统。
- en: To generate realistic data, taking into account reverberation, one needs to
    simulate the room acoustics. This is usually done by synthesizing the RIR that
    models the sound propagation for a “virtual” source-microphone pair. This is done
    for all microphones of the array (and for a large number of source positions and
    microphone array positions, see below). Then, a “dry” (*i.e.*, clean reverberation-free
    monophonic) source signal is convolved with this RIR to obtain the simulated microphone
    signal (this is done for every channel of the microphone array). As already stated
    in Section [1.2](#S1.SS2 "1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"), the foundation
    of SSL relies on the fact that the relative location of a source with respect
    to the microphone array position is implicitly encoded in the (multichannel) RIR,
    and an SSL DNN learns to extract and exploit this information from examples. Therefore,
    such data generation has to be done with many different dry signals and for a
    large number of simulated RIRs with different source and microphone array positions.
    The latter must be representative of the configurations in which the SSL system
    will be used in practice. Moreover, other parameters, such as room dimensions
    and reverberation time, may have to be varied to take into account other factors
    of variations in SSL.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成逼真的数据，同时考虑混响，需要模拟房间声学。这通常通过合成 RIR 来完成，以模拟“虚拟”源-麦克风对的声音传播。这对阵列中的所有麦克风（以及大量源位置和麦克风阵列位置，见下文）进行。这时，将一个“干燥”（*即*，干净的无混响的单声道）源信号与这个
    RIR 卷积，以获得模拟的麦克风信号（这对于麦克风阵列的每个通道进行）。正如在第[1.2](#S1.SS2 "1.2 General principle of
    DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")节中已经陈述的那样，SSL 的基础是源相对于麦克风阵列位置的相对位置被隐式编码在（多通道）RIR 中，SSL DNN 学会从示例中提取和利用这些信息。因此，这种数据生成必须在许多不同的干燥信号和大量不同源和麦克风阵列位置的模拟
    RIR 上进行。后者必须代表 SSL 系统在实际应用中使用的配置。此外，其他参数，如房间尺寸和混响时间，可能也需要变化，以考虑 SSL 中其他因素的变化。
- en: One advantage of this approach is that many dry signal datasets exist, in particular
    for speech signals, e.g., (Garofolo et al., [1993b](#bib.bib93); Lamel et al.,
    [1991](#bib.bib168); Garofolo et al., [1993a](#bib.bib92)). Therefore, many SSL
    methods are trained with dry speech signals convolved with simulated RIRs. Chakrabarty
    and Habets ([2017a](#bib.bib41), [b](#bib.bib42)) used white noise as the dry
    signal for training and speech signals for testing. This approach is reminiscent
    of the work of Deleforge et al. ([2013](#bib.bib66), [2015](#bib.bib67)) based
    on a GMR and as already mentioned in Section [3](#S3 "3 Conventional SSL methods
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"). Using white
    noise as the dry signal enables the acquisition of training data that are “dense”
    in the TF domain. However, Vargas et al. ([2021](#bib.bib330)) showed that training
    on speech or music signals leads to better results than noise-based training,
    even when the signals are simulated with a generative adversarial network (GAN).
    Furthermore, the results of Krause et al. ([2021](#bib.bib162)) indicate that
    using speech, noise and sound events data altogether leads to better localization
    performance, even compared to “matched” training and test signals.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个优势是存在许多干燥信号数据集，特别是语音信号，例如，（Garofolo 等，[1993b](#bib.bib93)；Lamel 等，[1991](#bib.bib168)；Garofolo
    等，[1993a](#bib.bib92)）。因此，许多 SSL 方法是在与模拟 RIR 卷积的干燥语音信号上训练的。Chakrabarty 和 Habets
    （[2017a](#bib.bib41)，[b](#bib.bib42)）使用白噪声作为训练的干燥信号，使用语音信号进行测试。这种方法让人联想到 Deleforge
    等人的工作（[2013](#bib.bib66)，[2015](#bib.bib67)），基于 GMR，并且如在第[3](#S3 "3 Conventional
    SSL methods ‣ A Survey of Sound Source Localization with Deep Learning Methods")节中已经提到的那样。使用白噪声作为干燥信号可以获取在
    TF 域中“密集”的训练数据。然而，Vargas 等人（[2021](#bib.bib330)）表明，即使信号是用生成对抗网络（GAN）模拟的，训练语音或音乐信号比基于噪声的训练效果更好。此外，Krause
    等人（[2021](#bib.bib162)）的结果表明，使用语音、噪声和声音事件数据的结合可以获得更好的定位性能，甚至比“匹配”的训练和测试信号更佳。
- en: As for RIR simulation, there exist several methods (and variants thereof) and
    many acoustic simulation softwares. Detailing these methods and software implementations
    is out of the scope of this article, but an interested reader may consult appropriate
    references, e.g., (Rindel, [2000](#bib.bib270); Svensson and Kristiansen, [2002](#bib.bib311);
    Siltanen et al., [2010](#bib.bib297)). Let us only mention that the simulators
    based on the image source method (ISM) (Allen and Berkley, [1979](#bib.bib7))
    have been widely used in the SSL community, probably due to the fact that they
    offer a relatively good trade-off between the simulation fidelity, in particular
    regarding the “head” of an RIR, *i.e.*, the direct propagation and early reflections
    (Rindel, [2000](#bib.bib270)), and computational complexity. Among publicly available
    libraries, the RIR generator of Habets ([2006](#bib.bib110)), the related signal
    generator (Habets, [2022](#bib.bib111)), the Roomsim toolbox of Campbell et al.
    ([2005](#bib.bib34)) and its extension to mobile sources called Roomsimove Vincent
    and Campbell ([2008](#bib.bib340)), the Spherical Microphone Impulse Response
    (SMIR) generator of Jarrett et al. ([2012](#bib.bib138)), the Pyroomacoustics
    toolbox of Scheibler et al. ([2018](#bib.bib287)), and the Multichannel Room Acoustics
    Simulator (MCRoomSim) of Wabnitz et al. ([2010](#bib.bib343)), are very popular.
    Such libraries have been used by, e.g., Chakrabarty and Habets ([2019b](#bib.bib44));
    Perotin et al. ([2019b](#bib.bib248)); Grumiaux et al. ([2021a](#bib.bib105));
    Nguyen et al. ([2020a](#bib.bib221)); Varanasi et al. ([2020](#bib.bib329)); Salvati
    et al. ([2018](#bib.bib283)); Li et al. ([2018](#bib.bib184)); Bianco et al. ([2020](#bib.bib25)).
    An efficient open-source implementation of the ISM method, relying on Graphic
    Processing Unit (GPU) acceleration, has been recently presented by Diaz-Guerra
    et al. ([2021b](#bib.bib69)) and used in Diaz-Guerra et al. ([2021a](#bib.bib68))
    to simulate moving sources.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 RIR 模拟，存在几种方法（及其变体）和许多声学模拟软件。详细讨论这些方法和软件实现超出了本文的范围，但感兴趣的读者可以查阅相关参考资料，例如，（Rindel，[2000](#bib.bib270)；Svensson
    和 Kristiansen，[2002](#bib.bib311)；Siltanen 等，[2010](#bib.bib297)）。我们仅提到，基于图像源方法（ISM）（Allen
    和 Berkley，[1979](#bib.bib7)）的模拟器在 SSL 社区中被广泛使用，这可能是因为它们在模拟保真度（特别是 RIR 的“头部”，*即*
    直接传播和早期反射（Rindel，[2000](#bib.bib270)））与计算复杂性之间提供了相对较好的权衡。在公开可用的库中，Habets 的 RIR
    生成器（[2006](#bib.bib110)）、相关信号生成器（Habets，[2022](#bib.bib111)）、Campbell 等的 Roomsim
    工具箱（[2005](#bib.bib34)）及其扩展至移动源的 Roomsimove（Vincent 和 Campbell，[2008](#bib.bib340)）、Jarrett
    等的球形麦克风脉冲响应（SMIR）生成器（[2012](#bib.bib138)）、Scheibler 等的 Pyroomacoustics 工具箱（[2018](#bib.bib287)）以及
    Wabnitz 等的多通道房间声学模拟器（MCRoomSim）（[2010](#bib.bib343)）都非常受欢迎。这些库被例如 Chakrabarty
    和 Habets（[2019b](#bib.bib44)）；Perotin 等（[2019b](#bib.bib248)）；Grumiaux 等（[2021a](#bib.bib105)）；Nguyen
    等（[2020a](#bib.bib221)）；Varanasi 等（[2020](#bib.bib329)）；Salvati 等（[2018](#bib.bib283)）；Li
    等（[2018](#bib.bib184)）；Bianco 等（[2020](#bib.bib25)）使用。最近，Diaz-Guerra 等（[2021b](#bib.bib69)）提出了一种高效的开源
    ISM 方法实现，依赖于图形处理单元（GPU）加速，并在 Diaz-Guerra 等（[2021a](#bib.bib68)）中用于模拟移动源。
- en: Other improved models based on the ISM have also been used to simulate impulse
    responses, such as the one presented by Hirvonen ([2015](#bib.bib128)). This model
    relies on that of Lehmann and Johansson ([2010](#bib.bib181)), which adds a diffuse
    reverberation model to the original ISM method. Hübner et al. ([2021](#bib.bib136))
    proposed a low-complexity model-based training data generation method that includes
    a deterministic model for the direct path and a statistical model for late reverberation.
    It has been demonstrated that the SSL neural network, trained using the data generated
    by this method, achieves comparable localization performance as the same architecture
    trained on a dataset generated by the usual ISM. However, the proposed simulation
    method is computationally more efficient. An investigation of several simulation
    methods was done by Gelderblom et al. ([2021](#bib.bib94)), with extensions of
    ISM, namely ISM with directional sources, and ISM with a diffuse field due to
    scattering. Gelderblom et al. ([2021](#bib.bib94)) compared the simulation algorithms
    via the training of an MLP (in both regression and classification modes) and showed
    that ISM with scattering effects and directional sources leads to the best SSL
    performance. More sophisticated software, such as ICARE^® Bouatouch et al. ([2006](#bib.bib30)),
    often combine ISM with efficient ray-tracing and statistical methods, permitting
    simulation of more complicated room geometries and acoustic effects. Note, however,
    that none of the methods based on approximating the sound propagation by geometrical
    acoustics is capable of precisely simulating certain wave phenomena, such as diffraction
    Kuttruff ([2016](#bib.bib166)).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 ISM 的其他改进模型也被用于模拟脉冲响应，例如 Hirvonen ([2015](#bib.bib128)) 提出的模型。该模型依赖于 Lehmann
    和 Johansson ([2010](#bib.bib181)) 的模型，后者在原始 ISM 方法中增加了扩散混响模型。Hübner et al. ([2021](#bib.bib136))
    提出了一个低复杂度模型的训练数据生成方法，包括一个用于直接路径的确定性模型和一个用于晚期混响的统计模型。研究表明，使用该方法生成的数据训练的 SSL 神经网络，能够达到与在常规
    ISM 数据集上训练的相同架构相当的定位性能。然而，所提出的模拟方法在计算上更为高效。Gelderblom et al. ([2021](#bib.bib94))
    对几种模拟方法进行了研究，包括 ISM 的扩展，即带有定向声源的 ISM 和由于散射产生的扩散场 ISM。Gelderblom et al. ([2021](#bib.bib94))
    通过训练 MLP（回归和分类模式）比较了模拟算法，并显示出带有散射效应和定向声源的 ISM 导致了最佳的 SSL 性能。更复杂的软件，如 ICARE^® Bouatouch
    et al. ([2006](#bib.bib30))，通常将 ISM 与高效的光线追踪和统计方法相结合，允许模拟更复杂的房间几何和声学效果。然而，需要注意的是，基于几何声学的方法无法准确模拟某些波动现象，如衍射
    Kuttruff ([2016](#bib.bib166)).
- en: Training and testing binaural SSL systems requires either directly using signals
    recorded in a binaural setup (see next subsection) or using a dataset of two-channel
    BIRs and convolving these BIRs with (speech/audio) dry signals, just like for
    simulations in conventional set-up. Most of the time, the BIRs are recorded ones
    (see next subsection; there exist a few BIR simulators, but we will not detail
    this quite specific aspect here). To take into account the room acoustics in a
    real-world SSL application, BIR effects are often combined with RIR effects. This
    is not obtained by trivially cascading the BIR and RIR filters, since the BIR
    depends on the source DoA, meaning that one would have to integrate it with RIR
    components from many incoming directions Bernschütz ([2016](#bib.bib21)). However,
    such a process is included in several RIR simulators, which are able to produce
    the corresponding combined response, called the binaural room impulse response
    (BRIR), e.g., (Campbell et al., [2005](#bib.bib34)). Recall that BIRs are often
    manipulated in the frequency domain (referred as HRTFs), where they are a function
    of both frequency and source DoA.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试双耳 SSL 系统需要直接使用在双耳设置中录制的信号（见下一小节），或使用一个双通道 BIR 数据集，并将这些 BIR 与（语音/音频）干信号卷积，就像在常规设置中的模拟一样。大多数情况下，BIR
    是录制的（见下一小节；虽然存在一些 BIR 模拟器，但我们不会详细讨论这一特定方面）。为了考虑实际 SSL 应用中的房间声学效果，BIR 效果通常与 RIR
    效果结合。这不是通过简单地级联 BIR 和 RIR 滤波器来实现的，因为 BIR 取决于声源的方向角（DoA），这意味着需要将其与来自许多不同方向的 RIR
    组件集成 Bernschütz ([2016](#bib.bib21))。然而，这种过程被包含在几个 RIR 模拟器中，这些模拟器能够生成对应的组合响应，称为双耳房间脉冲响应（BRIR），例如
    (Campbell et al., [2005](#bib.bib34))。请注意，BIR 经常在频域中操作（称为 HRTFs），其中它们是频率和声源 DoA
    的函数。
- en: 7.2 Real data
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 真实数据
- en: Collecting real labeled data is crucial to assessing the robustness of an SSL
    neural network in a real-world environment. However, it is a cumbersome task.
    As of today, only a few datasets of such recordings exist. Among them, several
    impulse response datasets are publicly available and have been used to generate
    training and/or testing data.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 收集真实标记数据对于评估SSL神经网络在真实环境中的鲁棒性至关重要。然而，这是一项繁琐的任务。至今，仅存在少量此类录音的数据集。其中，几个脉冲响应数据集公开可用，并已用于生成训练和/或测试数据。
- en: The distant-speech interaction for robust home applications (DIRHA) simulated
    corpus presented by Cristoforetti et al. ([2014](#bib.bib61)) has been used to
    simulate microphone speech signals based on real RIRs, recorded in a multi-room
    environment (Vesperini et al., [2016](#bib.bib339); Vecchiotti et al., [2018](#bib.bib333)).
    Another database consisting of recorded RIRs from three rooms with different acoustic
    characteristics is publicly available (Hadad et al., [2014](#bib.bib112)), using
    three microphone array configurations to capture signals from several source azimuth
    positions in the range $[-90\degree,90\degree]$. The RIR dataset published by
    Fernandez-Grande et al. ([2021](#bib.bib86)) is intended to be used for DoA estimation,
    and contains measurements from a three-channel array. Other RIR datasets have
    been published by, e.g., Szöke et al. ([2019](#bib.bib312)), Eaton et al. ([2015](#bib.bib75)),
    Hahmann et al. ([2021a](#bib.bib113)), Koyama et al. ([2021](#bib.bib158)), Kristoffersen
    et al. ([2021](#bib.bib163)), and Riezu and Grande ([2021](#bib.bib269)). The
    last four ones were initially designed for sound field analysis and synthesis,
    and they contain measurements from single-channel microphones (*i.e.*, not microphone
    arrays). However, the acquired RIRs correspond to multiple positions within a
    room, and could be potentially used to emulate microphone arrays.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Cristoforetti等人（[2014](#bib.bib61)）提出的远场语音交互（DIRHA）模拟语料库已被用于基于实际RIRs模拟麦克风语音信号，这些RIRs录制在多房间环境中（Vesperini等人，[2016](#bib.bib339)；Vecchiotti等人，[2018](#bib.bib333)）。另一个公开可用的数据库包含来自三间具有不同声学特性的房间的录制RIRs（Hadad等人，[2014](#bib.bib112)），使用三种麦克风阵列配置来捕获来自不同源方位的信号，范围为$[-90\degree,90\degree]$。Fernandez-Grande等人（[2021](#bib.bib86)）发布的RIR数据集旨在用于DoA估计，并包含来自三通道阵列的测量。其他RIR数据集由例如Szöke等人（[2019](#bib.bib312)）、Eaton等人（[2015](#bib.bib75)）、Hahmann等人（[2021a](#bib.bib113)）、Koyama等人（[2021](#bib.bib158)）、Kristoffersen等人（[2021](#bib.bib163)）以及Riezu和Grande（[2021](#bib.bib269)）发布。最后四个数据集最初设计用于声音场分析和合成，包含来自单通道麦克风的测量（*即*，非麦克风阵列）。然而，获得的RIRs对应于房间内多个位置，可能用于模拟麦克风阵列。
- en: As for BIR dataset recordings, a physical head-and-torso simulator (HATS) (aka
    “dummy head”) is used, with ear microphones plugged into the dummy head ears.
    To isolate head and torso effects from other environment effects such as reverberation,
    binaural recordings are generally made in an anechoic room. For example, the dataset
    published by Thiemann and Van De Par ([2015](#bib.bib321)) was collected using
    four different dummy heads and used for SSL by Roden et al. ([2015](#bib.bib271)).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BIR数据集录音，使用物理头肩模拟器（HATS）（又称“假头”），耳麦插入假头耳朵中。为了将头部和躯干效果与其他环境效果如混响隔离，双耳录音通常在消声室内进行。例如，Thiemann和Van
    De Par（[2015](#bib.bib321)）发布的数据集使用了四个不同的假头，并由Roden等人（[2015](#bib.bib271)）用于SSL。
- en: The Surrey Binaural Room Impulse Responses database was published by Francombe
    ([2017](#bib.bib89)) and has been used for SSL by, e.g., Ma et al. ([2015](#bib.bib201))
    to synthesize signals for evaluating the proposed method. This database has been
    recorded using a HATS in four room configurations, with sound coming from loudspeakers.
    It thus combines binaural effects with room effects.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Surrey双耳室内脉冲响应数据库由Francombe发布（[2017](#bib.bib89)），并已被Ma等人（[2015](#bib.bib201)）用于SSL，以合成信号以评估所提出的方法。该数据库使用HATS在四种房间配置下录制，声音来自扬声器。它结合了双耳效果与房间效果。
- en: 'Several challenges have also been organized for some years, and evaluation
    datasets with real recordings have been constituted to assess the candidate systems.
    Datasets were created for the SELD task of the DCASE Challenge, in 2019 (Adavanne
    et al., [2019b](#bib.bib3)), 2020 (Politis et al., [2020a](#bib.bib253)), and
    2021 (Politis et al., [2021](#bib.bib255)). These datasets contains sound events
    in reverberant and noisy environments, synthesized from recordings of real RIRs.
    These data come in two four-microphone spatial audio formats: tetrahedral microphone
    array and FOA. The dataset comprises 12 sound event types, including, e.g., barking
    dog, female/male speech or ringing, with up to three simultaneous events overlapping.
    In the 2019 dataset, the sources are static, whereas they are both static and
    moving in the 2020 and 2021 datasets, with more diverse acoustic conditions. Finally,
    in the 2021 edition of the DCASE dataset, additional sound events have been added
    to the recordings to play the role of (directional) interferers (that are not
    bound to be classified). These datasets have been used in many SSL systems, e.g.,
    (Cao et al., [2019a](#bib.bib36); Park et al., [2019b](#bib.bib238); Grondin et al.,
    [2019](#bib.bib103); Cao et al., [2020](#bib.bib38); Naranjo-Alcazar et al., [2020](#bib.bib217);
    Shimada et al., [2020b](#bib.bib294); Wang et al., [2020](#bib.bib347); Mazzon
    et al., [2019](#bib.bib211)). Very recently, another SELD challenge focused on
    3D sound has been announced (Guizzo et al., [2021](#bib.bib108)), where a *pair*
    of FOA microphones was used to capture a large number of RIRs in an office room,
    from which the audio data were generated.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来还组织了多个挑战，并建立了真实录音的评估数据集，以评估候选系统。数据集是为 DCASE 挑战的 SELD 任务创建的，分别在 2019 年（Adavanne
    等，[2019b](#bib.bib3)）、2020 年（Politis 等，[2020a](#bib.bib253)）和 2021 年（Politis 等，[2021](#bib.bib255)）。这些数据集包含在混响和噪声环境中的声音事件，这些声音事件是从真实
    RIR 的录音合成的。这些数据有两种四麦克风空间音频格式：四面体麦克风阵列和 FOA。数据集包括 12 种声音事件类型，例如，狗叫声、女性/男性讲话或铃声，最多可以有三个同时发生的事件重叠。在
    2019 年的数据集中，源是静态的，而在 2020 年和 2021 年的数据集中，源既有静态的也有移动的，声学条件更加多样化。最后，在 2021 年版的 DCASE
    数据集中，录音中增加了额外的声音事件，以充当（方向性）干扰源（这些事件不需要进行分类）。这些数据集已被许多 SSL 系统使用，例如，（Cao 等，[2019a](#bib.bib36)；Park
    等，[2019b](#bib.bib238)；Grondin 等，[2019](#bib.bib103)；Cao 等，[2020](#bib.bib38)；Naranjo-Alcazar
    等，[2020](#bib.bib217)；Shimada 等，[2020b](#bib.bib294)；Wang 等，[2020](#bib.bib347)；Mazzon
    等，[2019](#bib.bib211)）。最近，又有一个 SELD 挑战专注于 3D 声音（Guizzo 等，[2021](#bib.bib108)），其中使用了一对
    FOA 麦克风在办公室里捕捉大量的 RIR，从而生成了音频数据。
- en: The acoustic source LOCAlization and TrAcking (LOCATA) challenge (Evers et al.,
    [2020](#bib.bib83)) has been one of the most comprehensive challenges targeting
    the localization of speech sources. The challenge tasks include single and multiple
    SSL, each of which in a setting where the sources and/or microphones are static
    or mobile. The recordings have been made using several types of microphone arrays,
    namely the planar array from Brutti et al. ([2010](#bib.bib32)), the em32 Eigenmike${}^{\text{\textregistered}}$
    spherical array, a hearing aid, and a set of microphones mounted on a robot head.
    The ground truth data include position information obtained through an optical
    tracking system, hand-labeled VAD metadata, and dry (or close-talking) source
    signals. This dataset has been used in a number of works to validate the effectiveness
    of a proposed method on “real-life” recordings, e.g., (Grumiaux et al., [2021a](#bib.bib105);
    Diaz-Guerra et al., [2021a](#bib.bib68); Sundar et al., [2020](#bib.bib309); Pak
    and Shin, [2019](#bib.bib233); Varanasi et al., [2020](#bib.bib329); Tang et al.,
    [2019](#bib.bib319); Yang et al., [2021b](#bib.bib369)).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 声源定位和跟踪（LOCATA）挑战（Evers 等，[2020](#bib.bib83)）是针对语音源定位的最全面的挑战之一。挑战任务包括单声道和多声道的
    SSL，每种任务都在源和/或麦克风静态或移动的环境中进行。录音使用了几种类型的麦克风阵列，包括 Brutti 等的平面阵列（[2010](#bib.bib32)）、em32
    Eigenmike${}^{\text{\textregistered}}$ 球形阵列、助听器和安装在机器人头部的一组麦克风。真实数据包括通过光学跟踪系统获得的位置数据、手工标注的
    VAD 元数据和干（或近距离讲话）源信号。这个数据集已在多个研究中用于验证提出方法在“现实生活”录音中的有效性，例如，（Grumiaux 等，[2021a](#bib.bib105)；Diaz-Guerra
    等，[2021a](#bib.bib68)；Sundar 等，[2020](#bib.bib309)；Pak 和 Shin，[2019](#bib.bib233)；Varanasi
    等，[2020](#bib.bib329)；Tang 等，[2019](#bib.bib319)；Yang 等，[2021b](#bib.bib369)）。
- en: A few audio-visual datasets have also been developed and are publicly available,
    in which the audio data are enriched with video information. This type of dataset
    is dedicated to the development and testing of audio-visual localization and tracking
    techniques, which are out of the scope of this survey paper. Among these corpora,
    the AV16.3 corpus (Lathoud et al., [2004](#bib.bib170)) and the CHIL database
    (Stiefelhagen et al., [2007](#bib.bib302)) have provided an evaluative basis for
    several (purely audio) SSL systems (Vera-Diaz et al., [2018](#bib.bib336), [2020](#bib.bib337),
    [2021](#bib.bib338)) by considering only the audio part of the audiovisual dataset.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一些音视频数据集也已开发并公开，这些数据集中音频数据与视频信息相结合。这类数据集专门用于音视频定位和跟踪技术的发展和测试，而这超出了本调查论文的范围。在这些语料库中，AV16.3
    语料库（Lathoud 等，[2004](#bib.bib170)）和 CHIL 数据库（Stiefelhagen 等，[2007](#bib.bib302)）为几个（纯音频）SSL
    系统（Vera-Diaz 等，[2018](#bib.bib336)，[2020](#bib.bib337)，[2021](#bib.bib338)）提供了评估基础，这些系统只考虑了音视频数据集中的音频部分。
- en: Finally, we also found a series of papers in which neural networks were tested
    using real data specifically recorded for the presented work in the researchers’
    own laboratories, e.g., (Chazan et al., [2019](#bib.bib47); Grumiaux et al., [2021b](#bib.bib106);
    He et al., [2021a](#bib.bib121); Perotin et al., [2018b](#bib.bib246); Grumiaux
    et al., [2021a](#bib.bib105); Nguyen et al., [2020a](#bib.bib221); He et al.,
    [2018a](#bib.bib118); Varanasi et al., [2020](#bib.bib329); Le Moing et al., [2020](#bib.bib172);
    Perotin et al., [2019a](#bib.bib247)).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还发现了一系列论文，其中神经网络使用了专门在研究人员自己实验室中录制的真实数据进行测试，例如（Chazan 等，[2019](#bib.bib47)；Grumiaux
    等，[2021b](#bib.bib106)；He 等，[2021a](#bib.bib121)；Perotin 等，[2018b](#bib.bib246)；Grumiaux
    等，[2021a](#bib.bib105)；Nguyen 等，[2020a](#bib.bib221)；He 等，[2018a](#bib.bib118)；Varanasi
    等，[2020](#bib.bib329)；Le Moing 等，[2020](#bib.bib172)；Perotin 等，[2019a](#bib.bib247)）。
- en: 7.3 Data augmentation techniques
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 数据增强技术
- en: To limit the massive use of simulated data, which can limit the robustness of
    the network on real-world data, and to overcome the limitation in the amount of
    real data, several authors have proposed resorting to data augmentation techniques.
    Without producing more recordings, data augmentation allows for the creation of
    additional training examples, often leading to improved network performance.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制模拟数据的大量使用，这可能会限制网络在现实世界数据上的鲁棒性，并且克服真实数据量的限制，几位作者提出了采用数据增强技术的方案。数据增强在不产生更多录音的情况下，可以创建额外的训练样本，通常会导致网络性能的提升。
- en: 'For the DCASE Challenge, many submitted systems were trained using data augmentation
    techniques on the train dataset. Mazzon et al. ([2019](#bib.bib211)) proposed
    and evaluated three techniques to augment the training data, taking advantage
    of the FOA representation used by their SSL neural network: swap or inversion
    of FOA channels, label-oriented rotation (the rotation is applied to result in
    the desired label), or channel-oriented rotation (the rotation is directly applied
    with a desired matrix). Interestingly, the channel-oriented rotation method gave
    the worst results in their experiments, while the other two methods showed an
    improvement in neural network performance. Zhang et al. ([2019a](#bib.bib380))
    applied the SpecAugment method of Park et al. ([2019a](#bib.bib237)), which led
    to new data examples by masking certain time frames or frequencies of a spectrogram,
    or both at the same time. This method was also employed by, e.g., Yalta et al.
    ([2021](#bib.bib367)); Shimada et al. ([2021](#bib.bib295)); Bai et al. ([2021](#bib.bib14));
    Krause et al. ([2021](#bib.bib162)). In the work of Pratik et al. ([2019](#bib.bib258)),
    new training material was created with the Mixup method of Zhang et al. ([2018](#bib.bib379)),
    which relies on convex combinations of an existing training data pair. Noh et al.
    ([2019](#bib.bib226)) used pitch shifting and block mixing data augmentation (Salamon
    and Bello, [2017](#bib.bib281)). The techniques of Mazzon et al. ([2019](#bib.bib211))
    and Zhang et al. ([2019a](#bib.bib380)) were employed by Shimada et al. ([2020b](#bib.bib294),
    [2021](#bib.bib295)) to create new mixtures, along with another data augmentation
    method proposed by Takahashi et al. ([2016](#bib.bib313)), which is based on random
    mixing of two training signals.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DCASE 挑战，许多提交的系统在训练数据集上使用了数据增强技术。Mazzon 等人 ([2019](#bib.bib211)) 提出了并评估了三种增强训练数据的技术，利用他们的
    SSL 神经网络所使用的 FOA 表示：FOA 通道的交换或反转、标签导向旋转（旋转以产生所需标签），或通道导向旋转（旋转直接应用于期望矩阵）。有趣的是，通道导向旋转方法在他们的实验中效果最差，而其他两种方法则提高了神经网络性能。Zhang
    等人 ([2019a](#bib.bib380)) 应用了 Park 等人 ([2019a](#bib.bib237)) 的 SpecAugment 方法，该方法通过掩蔽光谱图的某些时间帧或频率，或者同时掩蔽两者，生成新的数据示例。这种方法也被，例如，Yalta
    等人 ([2021](#bib.bib367))、Shimada 等人 ([2021](#bib.bib295))、Bai 等人 ([2021](#bib.bib14))、Krause
    等人 ([2021](#bib.bib162)) 所采用。在 Pratik 等人 ([2019](#bib.bib258)) 的工作中，使用 Zhang 等人
    ([2018](#bib.bib379)) 的 Mixup 方法创建了新的训练材料，该方法依赖于现有训练数据对的凸组合。Noh 等人 ([2019](#bib.bib226))
    使用了音高偏移和块混合数据增强 (Salamon 和 Bello，[2017](#bib.bib281))。Mazzon 等人 ([2019](#bib.bib211))
    和 Zhang 等人 ([2019a](#bib.bib380)) 的技术被 Shimada 等人 ([2020b](#bib.bib294)、[2021](#bib.bib295))
    采用，用于创建新的混合样本，以及 Takahashi 等人 ([2016](#bib.bib313)) 提出的另一种数据增强方法，该方法基于两个训练信号的随机混合。
- en: Wang et al. ([2021](#bib.bib348)) applied four new data augmentation techniques
    to the DCASE dataset (Politis et al., [2021](#bib.bib255)). The first one applies
    the benefit of the FOA format to changing the location of the sources by swapping
    audio channels. The second method is based on the extraction of spatial and spectral
    information on the sources, which are then modified and recombined to create new
    training examples. The third one relies on mixing multiple examples, resulting
    in new multi-source labelled mixtures. The fourth technique is based on random
    TF masking. The authors evaluated the benefits of these data augmentation methods
    both when used separately and when applied sequentially.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 ([2021](#bib.bib348)) 将四种新的数据增强技术应用于 DCASE 数据集 (Politis 等人，[2021](#bib.bib255))。第一种技术利用
    FOA 格式的优势，通过交换音频通道来改变源的位置。第二种方法基于提取源的空间和频谱信息，然后对其进行修改和重新组合以创建新的训练样本。第三种方法依赖于混合多个示例，生成新的多源标签混合样本。第四种技术基于随机
    TF 掩蔽。作者评估了这些数据增强方法单独使用和顺序应用时的效果。
- en: 8 Learning strategies
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 种学习策略
- en: In a general manner, when training a neural network to accomplish a certain
    task, one needs to choose a training paradigm that often depends on the type and
    amount of available data. In the DNN-based SSL literature, most of the systems
    rely on supervised learning, although several examples of semi-supervised and
    weakly supervised learning can also be found.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，在训练神经网络以完成特定任务时，需要选择一种训练范式，这通常取决于可用数据的类型和数量。在基于 DNN 的 SSL 文献中，大多数系统依赖于监督学习，尽管也可以找到一些半监督和弱监督学习的例子。
- en: 8.1 Supervised learning
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 监督学习
- en: When training a neural network with supervised learning, the training dataset
    must contain the output target (also known as the label, especially in the classification
    mode) for each corresponding input data. A cost function (or loss function) is
    used to quantify the error between the output target and the actual output of
    the neural network for a given input data, and training consists of minimizing
    the average loss function over the training dataset. We have seen in Section [6](#S6
    "6 Output strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods") that in a single-source SSL scenario with the classification paradigm,
    a softmax output function is generally used. In that case, the cost function is
    generally the categorical cross-entropy, e.g., (Perotin et al., [2018b](#bib.bib246);
    Yalta et al., [2017](#bib.bib366); Chakrabarty and Habets, [2017a](#bib.bib41)).
    When dealing with multiple sources, still with the classification paradigm, sigmoid
    activation functions and a binary cross-entropy loss function are used, e.g.,
    (Perotin et al., [2019b](#bib.bib248); Grumiaux et al., [2021a](#bib.bib105);
    Chakrabarty and Habets, [2017b](#bib.bib42)). With a regression scheme, the choice
    for the cost function is the mean square error in most systems, e.g., (He et al.,
    [2021a](#bib.bib121); Nguyen et al., [2018](#bib.bib220); Krause et al., [2020a](#bib.bib160);
    Salvati et al., [2018](#bib.bib283); Adavanne et al., [2019a](#bib.bib2); Shimada
    et al., [2020a](#bib.bib293); Pertilä and Cakir, [2017](#bib.bib249)). We also
    sometimes witness the use of other cost functions, such as the angular error (Perotin
    et al., [2019a](#bib.bib247)) and the $\ell_{1}$-norm (Jenrungrot et al., [2020](#bib.bib141)).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用监督学习训练神经网络时，训练数据集必须包含每个输入数据对应的输出目标（也称为标签，特别是在分类模式下）。成本函数（或损失函数）用于量化输出目标与神经网络对于给定输入数据的实际输出之间的误差，训练过程包括最小化训练数据集上的平均损失函数。我们在第[6节](#S6
    "6 Output strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")中看到，在分类范式的单一源自监督学习场景中，通常使用softmax输出函数。在这种情况下，成本函数通常是分类交叉熵，例如（Perotin
    et al., [2018b](#bib.bib246); Yalta et al., [2017](#bib.bib366); Chakrabarty and
    Habets, [2017a](#bib.bib41)）。在处理多源的情况下，仍然采用分类范式，使用sigmoid激活函数和二元交叉熵损失函数，例如（Perotin
    et al., [2019b](#bib.bib248); Grumiaux et al., [2021a](#bib.bib105); Chakrabarty
    and Habets, [2017b](#bib.bib42)）。在回归方案中，大多数系统选择的成本函数是均方误差，例如（He et al., [2021a](#bib.bib121);
    Nguyen et al., [2018](#bib.bib220); Krause et al., [2020a](#bib.bib160); Salvati
    et al., [2018](#bib.bib283); Adavanne et al., [2019a](#bib.bib2); Shimada et al.,
    [2020a](#bib.bib293); Pertilä and Cakir, [2017](#bib.bib249)）。我们有时也会看到其他成本函数的使用，例如角度误差（Perotin
    et al., [2019a](#bib.bib247)）和$\ell_{1}$-范数（Jenrungrot et al., [2020](#bib.bib141)）。
- en: The limitation of supervised training is that the training relies on a great
    amount of labeled training data, whereas only a few real-world datasets with limited
    size have been collected for SSL. These datasets are not sufficient for robust
    training with DL models. To cope with these issues, one can opt for a data simulation
    method, as seen in Section [7.1](#S7.SS1 "7.1 Synthetic data ‣ 7 Data ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"), or data augmentation
    techniques, as seen in Section [7.3](#S7.SS3 "7.3 Data augmentation techniques
    ‣ 7 Data ‣ A Survey of Sound Source Localization with Deep Learning Methods").
    Otherwise, alternative training strategies can be employed, such as semi-supervised
    and weakly supervised learning, as presented hereafter.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的局限性在于训练依赖大量的标记数据，而实际收集到的有限大小的真实世界数据集仅适用于自监督学习。这些数据集对于深度学习模型的稳健训练来说是不够的。为了解决这些问题，可以选择数据模拟方法，如第[7.1节](#S7.SS1
    "7.1 Synthetic data ‣ 7 Data ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")所示，或数据增强技术，如第[7.3节](#S7.SS3 "7.3 Data augmentation techniques
    ‣ 7 Data ‣ A Survey of Sound Source Localization with Deep Learning Methods")所示。否则，可以采用替代的训练策略，如半监督学习和弱监督学习，如下文所述。
- en: 8.2 Semi-supervised and weakly supervised learning
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 半监督学习和弱监督学习
- en: Unsupervised learning refers to model training with a dataset that does not
    contain labels. In the present SSL framework, this means that we would have a
    dataset of recorded acoustic signals without the knowledge of sources position/direction,
    and hence unsupervised learning alone is not applicable to SSL in practice. Semi-supervised
    learning refers to when part of the learning is done in a supervised manner, and
    another part is done in an unsupervised manner. Usually the network is pre-trained
    with labeled data training and refined (or fine-tuned) using unsupervised learning,
    *i.e.*, without resorting to labels. In the SSL literature, semi-supervised learning
    has been proposed to improve the performance of the neural network on conditions
    unseen during supervised training or on real data, compared to its performance
    when trained only in the supervised manner. It can be seen as an alternative manner
    to enrich a labeled training dataset of too limited size or conditions (see Section [7](#S7
    "7 Data ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习指的是使用不包含标签的数据集进行模型训练。在当前的SSL框架中，这意味着我们将有一个记录的声学信号数据集，而没有源位置/方向的知识，因此单独使用无监督学习在实践中不适用于SSL。半监督学习指的是学习的一部分以监督的方式进行，另一部分以无监督的方式进行。通常，网络会先使用带标签的数据进行预训练，然后使用无监督学习（即无需依赖标签）进行细化（或微调）。在SSL文献中，半监督学习被提出用于改善神经网络在监督训练期间未见过的条件或真实数据上的表现，相比于仅在监督方式下训练的表现。它可以被视为一种替代方式，用于丰富标签训练数据集的数量过于有限或条件不足（见第[7](#S7
    "7 Data ‣ A Survey of Sound Source Localization with Deep Learning Methods")节）。
- en: For example, Takeda and Komatani ([2017](#bib.bib317)) and Takeda et al. ([2018](#bib.bib318))
    adapted a pre-trained neural network to unseen conditions in a unsupervised way.
    For the cost function, the cross-entropy was modified to be computed only with
    the estimated output, so that the overall entropy was minimized. They also applied
    a parameter selection method dedicated to avoid overfitting, as well as early
    stopping. Bianco et al. ([2020](#bib.bib25)) combined supervised and unsupervised
    learning using a VAE-based system. A generative network was trained to infer the
    phase of RTFs, which were used as input features in a classifier network. The
    cost function directly encompasses a supervised term and an unsupervised term
    and, during the training, the examples can come with or without labels.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Takeda和Komatani（[2017](#bib.bib317)）以及Takeda等人（[2018](#bib.bib318)）以无监督的方式将预训练的神经网络适应于未见过的条件。在代价函数中，交叉熵被修改为仅用估计输出进行计算，从而最小化整体熵。他们还应用了一种专门的参数选择方法以避免过拟合，以及早期停止。Bianco等人（[2020](#bib.bib25)）结合了监督和无监督学习，使用了基于VAE的系统。一个生成网络被训练来推断RTF的相位，RTF作为输入特征用于分类网络。代价函数直接包含一个监督项和一个无监督项，在训练过程中，样本可以有标签也可以没有标签。
- en: Le Moing et al. ([2021](#bib.bib173)) proposed a semi-supervised approach to
    adapt the network to real-world data after it was trained with a simulated dataset.
    This strategy was implemented with adversarial training (Goodfellow et al., [2014](#bib.bib100)).
    In the present SSL context, a discriminator network was trained to label incoming
    data as synthetic or real, and the generator network learned to fool the discriminator.
    This enabled the adaptation of the DoA estimation network to infer from real data.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Le Moing等人（[2021](#bib.bib173)）提出了一种半监督方法，用于在网络使用模拟数据集训练后，将其适应到现实世界数据中。该策略通过对抗训练（Goodfellow等人，[2014](#bib.bib100)）实施。在当前的SSL背景下，训练了一个判别网络来将输入数据标记为合成数据或真实数据，而生成网络则学习如何欺骗判别器。这使得DoA估计网络能够从真实数据中进行推断。
- en: 'A different kind of training, named weakly supervised, was used by He et al.
    ([2019a](#bib.bib120), [2021a](#bib.bib121)). The authors fine-tuned a pre-trained
    neural network by adapting the cost function to account for weak labels, which
    is the NoS, presumably known. This helped to improve the network performance by
    reducing the amount of incoherent predictions. Weak supervision was also used
    by Opochinsky et al. ([2019](#bib.bib231)). Under the assumption that only a few
    training data come with labels, a triplet loss function is computed. For each
    training step, three examples are drawn: a query sample, acting as a usual example,
    a positive sample close to the query sample, and a negative sample from a more
    remote source position. The triplet loss (named so because of these three components)
    is then derived so that the network learns to infer the position of the positive
    sample closer to the query sample than the negative sample.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不同的训练方法，称为弱监督，已被He等人（[2019a](#bib.bib120)，[2021a](#bib.bib121)）使用。作者通过调整代价函数以适应弱标签（即NoS，假定已知）来微调一个预训练的神经网络。这有助于通过减少不一致的预测来提高网络性能。Opochinsky等人（[2019](#bib.bib231)）也使用了弱监督。在假设只有少量训练数据带有标签的情况下，计算一个三元组损失函数。在每次训练步骤中，抽取三个样本：一个作为常规样本的查询样本，一个靠近查询样本的正样本，以及一个来自更远源位置的负样本。然后，基于这三个组件得出三元组损失，使得网络学习将正样本的位置推断得更接近查询样本而非负样本。
- en: 9 Conclusions and perspectives
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论与展望
- en: 'In this paper, we have presented a comprehensive overview of the literature
    on SSL techniques based on DL methods from 2011 to 2021\. We attempted to categorize
    the many publications in this domain according to different characteristics of
    the methods in terms of source (mixture) configuration, neural network architecture,
    input data type, output strategy, training and test datasets, and learning strategy.
    Tables II–V summarize our survey: They gather the references of the reviewed DL-based
    SSL papers with the main characteristics of the proposed methods (the ones that
    were used in our taxonomy of the different methods) being reported into different
    columns. We believe these tables can be very useful for a quick search of methods
    with a given set of characteristics.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了2011年至2021年间基于DL方法的SSL技术文献的全面概述。我们尝试根据方法的不同特征（包括源（混合）配置、神经网络架构、输入数据类型、输出策略、训练和测试数据集以及学习策略）对该领域的众多出版物进行分类。表II–V总结了我们的综述：这些表格汇总了我们审阅的基于DL的SSL论文的参考文献，并将提出的方法的主要特征（即我们在不同方法分类法中使用的特征）报告在不同的列中。我们相信这些表格对于快速查找具有特定特征的方法非常有用。
- en: To conclude this survey paper, we can comment on some current trends and draw
    a series of perspectives on the future directions that would be interesting to
    investigate to improve the performance of SSL systems and gain a better understanding
    of their behavior. Note that some of these perspectives appeal to general methodological
    issues in deep learning that are common to many applications, and some others
    are more specific to SSL. Note also that this list of research directions is not
    meant to be exhaustive.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为总结这篇综述论文，我们可以评论一些当前的趋势，并对未来的研究方向提出一系列展望，这些方向可能对提升SSL系统的性能并更好地理解其行为具有兴趣。需要注意的是，这些展望中有些涉及到深度学习中的一般性方法论问题，这些问题在许多应用中都很常见，而其他一些则更具体于SSL。还需注意的是，这个研究方向的列表并不是详尽无遗的。
- en: 9.1 Adaptation to (limited sets of) real-world data
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 适应（有限集的）真实世界数据
- en: In a general manner, we observe a drop in performance when DNNs trained on simulated
    data are tested on real-world signals. This effect is well-known in the DL research
    in general, it is a particular case of the poor generalization capability of DNNs
    in the case of significant train-test data mismatch (LeCun et al., [2015](#bib.bib175);
    Goodfellow et al., [2016](#bib.bib101)). We recall that this problem remains particularly
    crucial in SSL due to the difficulty of developing massive labeled datasets (i.e.,
    with reliable annotations of ground-truth sources location) and the use of simulated
    training data. This is valid for training datasets generated using the usual “shoebox”
    acoustic simulations. Such geometry is rarely encountered is real-world environments.
    Moreover, the placement of the simulated microphone array is often unrealistic
    (e.g., it is floating in the air, whereas a practical recording device is often
    positioned on a table, leading to strong reflections).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们观察到当在模拟数据上训练的深度神经网络（DNNs）在真实世界信号上测试时，性能会下降。这个现象在深度学习研究中是众所周知的，它是DNNs在训练和测试数据显著不匹配时较差的泛化能力的一个特例（LeCun
    et al., [2015](#bib.bib175); Goodfellow et al., [2016](#bib.bib101)）。我们回顾到，由于开发大规模标注数据集（即具有可靠真实源位置注释的数据集）和使用模拟训练数据的难度，这个问题在自监督学习（SSL）中尤为关键。这适用于使用常见的“鞋盒”声学模拟生成的训练数据集。这样的几何形状在真实世界环境中很少遇到。此外，模拟麦克风阵列的放置通常不现实（例如，它漂浮在空中，而实际录音设备通常放置在桌子上，导致强烈的反射）。
- en: A first approach to tackle this problem is to consider more sophisticated room
    acoustics simulators, capable of taking into account more complex room geometries
    and acoustic phenomena, such as scattering or diffraction, see the related discussion
    in Section [7.1](#S7.SS1 "7.1 Synthetic data ‣ 7 Data ‣ A Survey of Sound Source
    Localization with Deep Learning Methods"). However, this presents the limitation
    of a heavier computation cost, which should be balanced with the amount of data
    to be generated. Another line of research is to progressively train the network
    with more and more realistic signals, e.g., first with signals generated with
    simulated SRIRs, then fine-tuning the network with signals generated with real
    SRIRs, then further fine-tuning it with recorded data. This is in line with the
    general methodology of domain adaptation (DA) (Kouw and Loog, [2019](#bib.bib157))
    and transfer learning (Bengio, [2012](#bib.bib19); Zhuang et al., [2020](#bib.bib385))
    used in many applications of DL, which aims at improving the performance of a
    network on a particular domain (in our case, real-world data) after it has been
    trained on another domain (here, simulated data). For SSL, the idea is to “optimize”
    the model to the target acoustic environment and/or sound sources. DA is a promising
    research field on its own, and it has only recently attracted the attention of
    the SSL community. To our best knowledge, the adversarial approach of Le Moing
    et al. ([2021](#bib.bib173)) and the entropy-based adaptation of Takeda and Komatani
    ([2017](#bib.bib317)) are the only representatives of DA for SSL.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的第一个方法是考虑更复杂的房间声学模拟器，能够考虑更复杂的房间几何形状和声学现象，如散射或衍射，请参见第[7.1](#S7.SS1 "7.1
    Synthetic data ‣ 7 Data ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")节的相关讨论。然而，这带来了更高的计算成本，这需要与生成的数据量进行平衡。另一条研究方向是逐步训练网络，使其处理越来越真实的信号，例如，首先使用模拟的SRIRs生成的信号进行训练，然后用真实SRIRs生成的信号微调网络，进一步用录制的数据进行微调。这符合领域适应（DA）（Kouw
    and Loog, [2019](#bib.bib157)）和迁移学习（Bengio, [2012](#bib.bib19); Zhuang et al.,
    [2020](#bib.bib385)）的通用方法，这些方法用于许多深度学习应用中，旨在在网络在另一个领域（这里是模拟数据）上训练之后，提高其在特定领域（在我们的案例中是现实世界数据）上的性能。对于SSL，想法是“优化”模型以适应目标声学环境和/或声源。领域适应本身是一个有前途的研究领域，最近才引起了SSL社区的关注。据我们所知，Le
    Moing et al. ([2021](#bib.bib173))的对抗性方法和Takeda和Komatani ([2017](#bib.bib317))的基于熵的适应是SSL领域领域适应的唯一代表。
- en: Another line of research would be to inspire from weakly-supervised SSL methods
    based on manifold learning (Laufer-Goldshtein et al., [2020](#bib.bib171)). The
    general principle is that the high-dimensional multichannel observed data live
    in a low-dimensional acoustic space, controlled by a limited number of latent
    variables (mainly, room dimensions, source and microphone positions, and reflection
    coefficients). This low-dimensional space, or manifold, can be identified using
    a large set of unlabeled data and unsupervised data dimension reduction techniques.
    Then a limited set of labeled data can be used to identify the relationship between
    observed data and source positions “in the manifold,” and thus estimate the source
    positions from new observed data (using, e.g., interpolation techniques). This
    principle was largely developed by Laufer-Goldshtein et al. ([2020](#bib.bib171)),
    who proposed several non-deep manifold identification techniques and corresponding
    SSL algorithms. The same principle can be applied with a DL approach, in particular
    with deep latent-variable generative models such as the VAE, in the line of the
    semi-supervised VAE-SSL model of Bianco et al. ([2020](#bib.bib25), [2021](#bib.bib26))
    already mentioned in Section [4.7.2](#S4.SS7.SSS2 "4.7.2 Variational autoencoder
    ‣ 4.7 Encoder-decoder neural networks ‣ 4 Neural network architectures for SSL
    ‣ A Survey of Sound Source Localization with Deep Learning Methods") (see also
    an example of weakly supervised VAE-based source-filter decomposition of speech
    signals by Sadok et al. ([2022](#bib.bib279))). To our knowledge, SSL based on
    “deep manifold learning” is still a largely under-considered and open topic in
    the literature, yet it offers a promising direction to deal with limited annotated
    datasets.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 另一条研究方向是从基于流形学习的弱监督SSL方法中获得灵感（Laufer-Goldshtein 等人，[2020](#bib.bib171)）。一般原则是高维多通道观察数据存在于低维声学空间中，由有限数量的潜变量（主要是房间尺寸、声源和麦克风位置以及反射系数）控制。这个低维空间或流形可以使用大量的未标记数据和无监督的数据维度减少技术来识别。然后，可以使用有限数量的标记数据来识别“在流形中”观察数据与源位置之间的关系，从而估计从新的观察数据中源的位置（例如，使用插值技术）。这一原则主要由Laufer-Goldshtein
    等人（[2020](#bib.bib171)）发展，他们提出了几种非深度流形识别技术和相应的SSL算法。相同的原则也可以应用于DL方法，特别是使用如VAE这样的深度潜变量生成模型，这在Bianco
    等人（[2020](#bib.bib25)，[2021](#bib.bib26)）的半监督VAE-SSL模型中已经提到（参见第[4.7.2节](#S4.SS7.SSS2
    "4.7.2 Variational autoencoder ‣ 4.7 Encoder-decoder neural networks ‣ 4 Neural
    network architectures for SSL ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")）。另见Sadok 等人（[2022](#bib.bib279)）的基于弱监督VAE的语音信号源-滤波器分解的例子。据我们所知，基于“深度流形学习”的SSL在文献中仍然是一个被忽视且开放的话题，但它提供了处理有限标注数据集的有希望的方向。
- en: 9.2 Flexibility of the trained models
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 训练模型的灵活性
- en: As opposed to conventional SP techniques, which can be parameterized to adapt
    to the changes in the system setup, DL methods for SSL generally assume identical
    setups for the training and the inference phase. Particularly, the number, geometrical
    arrangement and the directivity of the microphones composing an array, are usually
    assumed to be fixed. This is a serious disadvantage, since the network needs to
    be retrained for different microphone arrays, despite the fact that the task (SSL)
    remains the same. A partial remedy is to use array-agnostic inputs, such as Ambisonics,
    e.g., (Adavanne et al., [2018](#bib.bib1); Perotin et al., [2019b](#bib.bib248);
    Grumiaux et al., [2021a](#bib.bib105)), CPS eigenvectors, e.g., (Takeda and Komatani,
    [2016b](#bib.bib316)) or spatial pseudo-spectra, e.g., (Nguyen et al., [2020a](#bib.bib221);
    Wu et al., [2021b](#bib.bib353)). Another possibility is to adopt array-invariant
    techniques from end-to-end multichannel speech enhancement, e.g., Luo et al. ([2020](#bib.bib200)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的SP技术不同，后者可以参数化以适应系统设置的变化，DL方法通常假设训练和推理阶段的设置是相同的。特别是，组成阵列的麦克风的数量、几何排列和指向性通常被假定为固定的。这是一个严重的缺点，因为尽管任务（SSL）保持不变，网络仍需要为不同的麦克风阵列重新训练。一种部分补救措施是使用与阵列无关的输入，例如Ambisonics，例如（Adavanne
    等人，[2018](#bib.bib1)；Perotin 等人，[2019b](#bib.bib248)；Grumiaux 等人，[2021a](#bib.bib105)），CPS特征向量，例如（Takeda
    和 Komatani，[2016b](#bib.bib316)）或空间伪谱，例如（Nguyen 等人，[2020a](#bib.bib221)；Wu 等人，[2021b](#bib.bib353)）。另一种可能性是采用来自端到端多通道语音增强的阵列不变技术，例如Luo
    等人（[2020](#bib.bib200)）。
- en: Moreover, one could apply transfer learning and DA techniques, discussed previously,
    that could enable the models trained for a particular microphone array to adapt
    to another. Such techniques could not only be beneficial for the changes in microphone
    array setups, but also for the changes in the input signal parameters, such as
    the sampling rate, frame and overlap length, as well as the type of the STFT window
    function. A radical approach would be to make the method inherently independent
    to parameterization, by treating the input signal as a point cloud, as recently
    suggested by Subramani and Smaragdis ([2021](#bib.bib303)).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以应用之前讨论的迁移学习和DA技术，使得为特定麦克风阵列训练的模型能够适应其他麦克风阵列。这些技术不仅对麦克风阵列设置的变化有益，还对输入信号参数的变化有益，如采样率、帧和重叠长度以及STFT窗口函数的类型。一种激进的方法是使方法本质上独立于参数化，将输入信号视为点云，正如Subramani和Smaragdis（[2021](#bib.bib303)）最近建议的那样。
- en: 9.3 Multi-task learning
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 多任务学习
- en: 'Multi-task training is a general methodology to improve the performance of
    a DNN-based system on a given task by training the model to jointly and simultaneously
    tackle several other tasks (Zhang and Yang, [2021](#bib.bib382); Ruder, [2017](#bib.bib277)).
    It has been observed in practice that this often leads to better performance on
    the first target task. This principle is most often implemented in the following
    manner: An early part of the model (e.g., a common feature extraction module composed
    of several layers or several layer blocks) is common for the different tasks,
    then the model splits into different branches, each one specialized in one of
    the different tasks. The common part is assumed to allow the discovery of an efficient
    signal representation, and the fact that this representation is used for several
    downstream tasks somehow reinforce the efficiency of the representation extraction.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务训练是一种通用的方法，通过训练模型同时处理几个其他任务，从而提高基于DNN系统在给定任务上的性能（Zhang和Yang，[2021](#bib.bib382);
    Ruder，[2017](#bib.bib277)）。实际观察表明，这通常会导致在第一个目标任务上的更好表现。这个原则通常以以下方式实现：模型的早期部分（例如，由多个层或多个层块组成的公共特征提取模块）对不同任务是共用的，然后模型分裂成不同的分支，每个分支专注于不同的任务。假定公共部分允许发现高效的信号表示，而这个表示用于多个下游任务，某种程度上增强了表示提取的效率。
- en: This principle can be applied to SSL. In fact, it has already been extensively
    illustrated in this survey with the SELD Task of the DCASE Challenge (Politis
    et al., [2020b](#bib.bib254)) and the many candidates that have been proposed
    to this challenge (and that we have reported in this survey). The vast majority
    of the candidate DNNs follow the above architecture, with a common feature extraction
    module followed by two SED and SSL branches. In 2021, the ACCDOA representation
    was adopted by many researchers, see Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Cartesian
    coordinates ‣ 6.2 DoA estimation via regression ‣ 6 Output strategies ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"), and allowed for a joint
    SED and SSL process up to the very last model layer. We believe that combining
    the SSL task with other tasks (alternately to SED or in addition to it) such as
    source separation or ASR could lead to further advances. For example jointly proceeding
    to source counting in addition to SSL in the work of Grumiaux et al. ([2021a](#bib.bib105))
    was shown to improve the SSL performance (note that here source counting is explicit
    and high-resolution, i.e., it consists in estimating the number of active sources
    at the short-term frame level, whereas it is most often implicit and generally
    made on a much larger time scale in the SELD task of the DCASE Challenge). Other
    examples of multi-task learning for SSL can be found in the works of Wu et al.
    ([2021c](#bib.bib354), [b](#bib.bib353)). Combining SSL with source separation
    in a DL framework is further discussed below.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则可以应用于 SSL。事实上，这在本调查中已经通过 DCASE 挑战中的 SELD 任务（Politis 等人，[2020b](#bib.bib254)）和为这一挑战提出的众多候选方案（我们在本调查中报告的）得到了广泛的说明。绝大多数候选
    DNN 遵循上述架构，具有一个通用特征提取模块，后跟两个 SED 和 SSL 分支。在 2021 年，许多研究人员采用了 ACCDOA 表示方法，见第 [6.2.2](#S6.SS2.SSS2
    "6.2.2 Cartesian coordinates ‣ 6.2 DoA estimation via regression ‣ 6 Output strategies
    ‣ A Survey of Sound Source Localization with Deep Learning Methods") 节，并实现了 SED
    和 SSL 过程的联合处理，直到最后的模型层。我们相信，将 SSL 任务与其他任务（交替于 SED 或与之并行）如源分离或 ASR 结合可能会带来进一步的进展。例如，Grumiaux
    等人（[2021a](#bib.bib105)）的研究表明，除了 SSL 进行源计数能够提高 SSL 性能（请注意，这里源计数是显式和高分辨率的，即在短期帧级别估计活跃源的数量，而在
    DCASE 挑战的 SELD 任务中，它通常是隐式的，并且在较大的时间尺度上进行）。关于 SSL 的多任务学习的其他例子可以在 Wu 等人（[2021c](#bib.bib354),
    [b](#bib.bib353)）的工作中找到。将 SSL 与源分离结合在 DL 框架中进一步讨论如下。
- en: Somewhat different from multi-task approaches, the end-to-end *task-oriented*
    learning of the entire processing chain (stacked localization, DoA-parameterized
    beamforming and ASR blocks) of Subramanian et al. ([2021a](#bib.bib304)) represents
    a refreshing idea to address the lack of DoA-annotated data. For instance, by
    using pre-trained ASR blocks, and by “freezing” all but the localization part
    of the system during training, one could use the abundant labeled speech corpora
    as a proxy information for the localization task. Such systems could incorporate
    both neural network modules and processing blocks based on conventional SP, as
    discussed in the next subsection.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 与多任务方法有所不同，Subramanian 等人（[2021a](#bib.bib304)）提出的端到端的*任务导向*学习方法涵盖了整个处理链（堆叠的定位、DoA
    参数化波束形成和 ASR 模块），为解决 DoA 注释数据的缺乏提供了一种新颖的思路。例如，通过使用预训练的 ASR 模块，并在训练过程中“冻结”系统中的所有部分，只保留定位部分，可以利用大量标注的语音语料作为定位任务的替代信息。这种系统可以结合神经网络模块和基于传统
    SP 的处理模块，如下一小节所讨论的那样。
- en: 9.4 Combination of DL and conventional SP techniques
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4 DL 和传统 SP 技术的结合
- en: In this survey paper, we have seen how the DL-based data-driven approach to
    the SSL problem has somehow replaced the conventional SP approach over the last
    decade. Yet, conventional methods are able to “explicitly” exploit strong prior
    knowledge on the physical underlying processes via signal and propagation models,
    whereas the exploitation of the spatial information contained in the mixture signal
    is done mostly “implicitly” by DNNs. Therefore, a major perspective for SSL is
    to get the best of both worlds, i.e., the combination of DL with conventional
    multichannel SP techniques.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查论文中，我们看到基于 DL 的数据驱动方法在过去十年中在一定程度上取代了传统的 SP 方法。然而，传统方法能够“明确地”利用有关物理基础过程的强先验知识，通过信号和传播模型，而
    DNNs 大多通过“隐式”方式利用混合信号中包含的空间信息。因此，SSL 的一个主要前景是获取两全其美，即将 DL 与传统的多通道 SP 技术结合起来。
- en: This can be inspired by what has been done in, e.g., speech enhancement and
    speech/audio source separation. In the single-channel configuration, DL-based
    speech enhancement and separation are mostly based on the masking approach in
    the TF domain. Binary masks or soft masks (reminiscent of the well-known single-channel
    Wiener filter) are estimated with DNNs from the noisy signal and applied to it
    to obtain a cleaned version, see the review by Wang and Chen ([2018](#bib.bib345)).
    For multichannel speech enhancement and separation, a straightforward approach
    is to input the multichannel signal in the mask estimation network. However, more
    clever strategies can be elaborated. For example, Erdogan et al. ([2016](#bib.bib80)),
    Heymann et al. ([2016](#bib.bib124)) and Higuchi et al. ([2017](#bib.bib127))
    proposed combining the DNN-based single-channel masking with beamforming techniques
    (Van Veen and Buckley, [1988](#bib.bib328)). In these works, the TF-domain masks
    estimated by a DNN are used to select speech-dominant against noise-dominant TF
    points, which are then used to estimate speech and noise spatial covariance matrices,
    respectively, which are finally used to build beamforming filters. These papers
    report better ASR scores than with direct TF masking or basic beamforming applied
    separately. This approach was extended by Perotin et al. ([2018a](#bib.bib245))
    with an additional first stage of beamforming in the HOA domain to improve the
    mask estimation. A joint end-to-end optimization of the mask estimator, the beamformer,
    and possibly an ASR acoustic model, was considered in the TF domain by Meng et al.
    ([2017](#bib.bib213)) and Heymann et al. ([2017](#bib.bib125)), and in the time
    domain by Li et al. ([2016a](#bib.bib183)). Closer to source separation than to
    beamforming, Nugraha et al. ([2016](#bib.bib229)) combined a DNN trained to estimate
    a clean speech spectrogram from a noisy speech spectrogram with the source separation
    technique based on the spatial covariance matrix (SCM) model and Wiener filtering
    of Duong et al. ([2010](#bib.bib73)). Leglaive et al. ([2019](#bib.bib180)) proposed
    an unsupervised multichannel speech enhancement system combining a VAE for modeling
    the (single-channel) clean speech signal and the SCM model for modeling the spatial
    characteristics of the multi-channel signal.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以从例如语音增强和语音/音频源分离中得到启发。在单通道配置中，基于深度学习的语音增强和分离大多基于时频（TF）域中的掩蔽方法。通过深度神经网络（DNN）从噪声信号中估计二进制掩蔽或软掩蔽（类似于著名的单通道维纳滤波器），并将其应用于信号以获得清洁版本，参见
    Wang 和 Chen 的综述（[2018](#bib.bib345)）。对于多通道语音增强和分离，直接的方法是将多通道信号输入到掩蔽估计网络中。然而，也可以提出更巧妙的策略。例如，Erdogan
    等人（[2016](#bib.bib80)）、Heymann 等人（[2016](#bib.bib124)）和 Higuchi 等人（[2017](#bib.bib127)）提出将基于
    DNN 的单通道掩蔽与波束形成技术（Van Veen 和 Buckley，[1988](#bib.bib328)）相结合。在这些工作中，DNN 估计的 TF
    域掩蔽被用来选择以语音为主的 TF 点与以噪声为主的 TF 点，然后用来估计语音和噪声的空间协方差矩阵，最后用于构建波束形成滤波器。这些论文报告了比直接 TF
    掩蔽或单独应用基本波束形成更好的自动语音识别（ASR）得分。这种方法被 Perotin 等人（[2018a](#bib.bib245)）扩展，增加了一个 HOA
    域中的波束形成第一阶段，以改善掩蔽估计。在 TF 域中，Meng 等人（[2017](#bib.bib213)）和 Heymann 等人（[2017](#bib.bib125)）考虑了掩蔽估计器、波束形成器以及可能的
    ASR 声学模型的联合端到端优化，而 Li 等人（[2016a](#bib.bib183)）则在时域中进行了类似的考虑。更接近源分离而非波束形成，Nugraha
    等人（[2016](#bib.bib229)）将训练用于从噪声语音光谱图中估计清洁语音光谱图的 DNN 与基于空间协方差矩阵（SCM）模型和 Duong 等人（[2010](#bib.bib73)）的维纳滤波技术相结合。Leglaive
    等人（[2019](#bib.bib180)）提出了一种无监督的多通道语音增强系统，将用于建模（单通道）清洁语音信号的变分自编码器（VAE）与用于建模多通道信号空间特征的
    SCM 模型相结合。
- en: Although we can find many examples of combination of DL-based and SP-based approaches
    for beamforming and source separation, to our knowledge and as shown by our survey,
    this principle has been poorly applied to SSL so far. Yet, powerful deep models,
    and in particular deep generative models such as GANs (Goodfellow et al., [2014](#bib.bib100)),
    VAEs (Kingma and Welling, [2014](#bib.bib150)), and dynamical VAEs (Girin et al.,
    [2021](#bib.bib98)) are now avaible to model the temporal and/or spectral characteristics
    of sounds, and can be combined with SP-based models. Morevover, as already mentioned
    earlier in this survey, the connection between audio source separation, diarization,
    and SSL is strong, reciprocal (each task can help to solve the other ones), and
    is already exploited in many conventional systems (Vincent et al., [2018](#bib.bib341);
    Gannot et al., [2017](#bib.bib90)). Future works may thus consider jointly sound
    source localization, diarization and separation/enhancement in an hybrid approach
    combining powerful DL models and conventional SP techniques. General frameworks
    for the joint optimization of DNN parameters and “conventional” parameters are
    now established and can be exploited (Engel et al., [2020](#bib.bib79); Shlezinger
    et al., [2020](#bib.bib296)).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以找到许多基于深度学习（DL）和基于信号处理（SP）的方法结合用于波束形成和源分离的例子，但据我们所知，并且如我们调查所示，这一原则在自监督学习（SSL）中应用较少。然而，强大的深度模型，特别是深度生成模型，如GANs（Goodfellow等，[2014](#bib.bib100)），VAEs（Kingma和Welling，[2014](#bib.bib150)），以及动态VAEs（Girin等，[2021](#bib.bib98)）现在可以用来建模声音的时间和/或频谱特征，并且可以与基于SP的模型结合。此外，正如本调查中早前提到的，音频源分离、分段和SSL之间的联系是紧密且互惠的（每个任务可以帮助解决其他任务），并且已经在许多传统系统中得到应用（Vincent等，[2018](#bib.bib341)；Gannot等，[2017](#bib.bib90)）。因此，未来的工作可以考虑在结合强大的DL模型和传统SP技术的混合方法中联合处理声音源定位、分段和分离/增强。现在已经建立了DNN参数与“传统”参数的联合优化的一般框架，并可以加以利用（Engel等，[2020](#bib.bib79)；Shlezinger等，[2020](#bib.bib296)）。
- en: 9.5 Moving sources and deep tracking
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5 移动源和深度跟踪
- en: In this survey, we poorly considered the case of moving sound sources and the
    necessity to rely in this case on tracking algorithms. These algorithms take as
    input the results of SSL obtained individually on each time frame and connect
    them through time. This is generally based on the use of a model of the source
    dynamics. In the multi-source case, dynamical models are often combined with source
    appearance models (which would model the sound texture or the different speakers’
    voice in the case of audio signals), resulting in the formation of source tracks
    with a consistent source “identity” for each of these tracks. Tracking algorithms
    also estimate the tracks “birth” and “death,” i.e., the time at which the corresponding
    sources are activated or inactivated. Such multi-object tracking (MOT) algorithms
    have a long history and their detailed description is beyond the scope of this
    paper; for a good overview of this domain, see the review papers of Vo et al.
    ([2015](#bib.bib342)) and Luo et al. ([2021](#bib.bib198)).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们对移动声音源的情况和在这种情况下依赖跟踪算法的必要性考虑较少。这些算法以每个时间帧单独获得的SSL结果作为输入，并通过时间将它们连接起来。这通常基于源动态模型的使用。在多源情况下，动态模型通常与源外观模型（如果是音频信号，则建模声音纹理或不同说话者的声音）结合，形成具有一致源“身份”的源轨迹。跟踪算法还估计轨迹的“出生”和“死亡”，即相应源的激活或停用时间。这些多目标跟踪（MOT）算法有着悠久的历史，其详细描述超出了本文的范围；有关这一领域的良好概述，请参见Vo等（[2015](#bib.bib342)）和Luo等（[2021](#bib.bib198)）的综述文章。
- en: More recently, deep approaches to the MOT problem have emerged, an evolution
    mostly driven by the computer vision community (Ciaparrone et al., [2020](#bib.bib53)).
    For example, RNNs have been used in place of the traditional Kalman filter to
    model object dynamics for MOT in videos, e.g., (Babaee et al., [2018](#bib.bib12);
    Sadeghian et al., [2017](#bib.bib278); Liang and Zhou, [2018](#bib.bib190); Saleh
    et al., [2021](#bib.bib282); Xiang et al., [2019](#bib.bib358)). The current trend
    is to replace RNNs with Transformer-like models, as discussed at the end of Section [4.6](#S4.SS6
    "4.6 Attention-based neural networks ‣ 4 Neural network architectures for SSL
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"), e.g., (Xu
    et al., [2021b](#bib.bib363); Sun et al., [2020](#bib.bib308); Meinhardt et al.,
    [2021](#bib.bib212)). The combination of a deep appearance model (automatic speaker
    recognition, SED) with a deep dynamical model in a sound source tracking system
    is a largely open problem and certainly a key ingredient for future developments
    in robust multi-source acoustic scene analysis in adverse acoustic environments
    and complex scenarios. Given the problem of annotated data scarsity in SSL, DL-based
    sound source localization and tracking may inspire from the unsupervised deep
    approaches to the MOT problem recently proposed by, e.g., (Luiten et al., [2020](#bib.bib197);
    Karthik et al., [2020](#bib.bib146); He et al., [2019b](#bib.bib123); Crawford
    and Pineau, [2020](#bib.bib60); Lin et al., [2022](#bib.bib191)).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，针对MOT问题的深度方法已经出现，这一发展主要由计算机视觉社区推动（Ciaparrone等，[2020](#bib.bib53)）。例如，RNN已被用于替代传统的卡尔曼滤波器，以建模视频中的物体动态，例如，（Babaee等，[2018](#bib.bib12)；Sadeghian等，[2017](#bib.bib278)；Liang和Zhou，[2018](#bib.bib190)；Saleh等，[2021](#bib.bib282)；Xiang等，[2019](#bib.bib358)）。当前的趋势是用类似Transformer的模型替代RNN，如第[4.6](#S4.SS6
    "4.6 Attention-based neural networks ‣ 4 Neural network architectures for SSL
    ‣ A Survey of Sound Source Localization with Deep Learning Methods")节末讨论的那样，例如，（Xu等，[2021b](#bib.bib363)；Sun等，[2020](#bib.bib308)；Meinhardt等，[2021](#bib.bib212)）。在声音源跟踪系统中，将深度外观模型（自动说话人识别，SED）与深度动态模型相结合仍是一个基本未解的问题，确实是未来在不良声学环境和复杂场景中进行鲁棒多源声学场景分析的重要组成部分。鉴于SSL中标注数据稀缺的问题，基于深度学习的声音源定位和跟踪可能会受到最近提出的无监督深度方法的启发，例如，（Luiten等，[2020](#bib.bib197)；Karthik等，[2020](#bib.bib146)；He等，[2019b](#bib.bib123)；Crawford和Pineau，[2020](#bib.bib60)；Lin等，[2022](#bib.bib191)）。
- en: 'Table 2: Summary of DL-based SSL systems published from 2011 to 2018, organized
    in chronological then alphabetical order. Type: R = regression, C = classification.
    Learning: S = supervised, SS = semi-supervised, WS = weakly supervised. Sources:
    NoS = considered number of sources, Kno. indicates if the NoS is known or not
    before estimating the DoA (✓= yes, ✗= no), Mov. specifies if moving sources are
    considered. Data: SA = synthetic anechoic, RA = real anechoic, SR = synthetic
    reverberant, RR = real reverberant.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：2011年至2018年发布的基于深度学习的半监督学习系统的总结，按时间顺序然后按字母顺序组织。类型：R = 回归，C = 分类。学习：S = 有监督，SS
    = 半监督，WS = 弱监督。来源：NoS = 考虑的来源数量，Kno. 表示在估算DoA之前是否知道NoS（✓= 是，✗= 否），Mov. 指定是否考虑移动来源。数据：SA
    = 合成无回声，RA = 真实无回声，SR = 合成混响，RR = 真实混响。
- en: Author Year Architecture Type Learn- Input features Output Sources Data NoS
    Kno. Mov. Train Test ing SA RA SR RR SA RA SR RR Kim and Ling ([2011](#bib.bib149))
    2011 MLP R S Power of multiple beams $\theta$ 1-5 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Tsuzuki
    et al. ([2013](#bib.bib325)) 2013 MLP R S Time delay, phase delay, sound pressure
    diff. $\theta$ 1 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ Youssef et al. ([2013](#bib.bib375)) 2013
    MLP R S ILD, ITD $\theta$ 1 ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ Hirvonen ([2015](#bib.bib128))
    2015 CNN C S Magnitude spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Ma et al. ([2015](#bib.bib201))
    2015 MLP C S Binaural cross-correlation + ILD $\theta$ 1-3 ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓
    Roden et al. ([2015](#bib.bib271)) 2015 MLP C S ILD, ITD, binaural magnitude +
    phase spectrogr., $\theta$ / $\phi$ / $r$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ binaural real
    + imaginary spectrograms Xiao et al. ([2015](#bib.bib359)) 2015 MLP C S GCC-PHAT
    $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Takeda and Komatani ([2016b](#bib.bib316)) 2016
    MLP C S Complex eigenvectors from correlation matrix $\theta$, $z$, $r$ 0-1 ✓
    ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Takeda and Komatani ([2016a](#bib.bib315)) 2016 MLP C S Complex
    eigenvectors from correlation matrix $\theta$ 0-2 ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Vesperini
    et al. ([2016](#bib.bib339)) 2016 MLP R S GCC-PHAT $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗
    ✓ ✓ Zermini et al. ([2016](#bib.bib378)) 2016 AE C S Mixing vector + ILD + IPD
    $\theta$ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Chakrabarty and Habets ([2017a](#bib.bib41)) 2017
    CNN C S Phase map $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Chakrabarty and Habets ([2017b](#bib.bib42))
    2017 CNN C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pertilä and Cakir ([2017](#bib.bib249))
    2017 CNN R S Magnitude spectrograms TF Mask 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Takeda and Komatani
    ([2017](#bib.bib317)) 2017 MLP C SS Complex eigenvectors from correlation matrix
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Yalta et al. ([2017](#bib.bib366)) 2017
    Res. CNN C S Magnitude spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yiwere and
    Rhee ([2017](#bib.bib374)) 2017 MLP C S Binaural cross-correlation + ILD $\theta$,
    $d$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Adavanne et al. ([2018](#bib.bib1)) 2018 CRNN C S Magnitude
    + phase spectrograms SPS, $\theta$, $\phi$ $\infty$ ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ He et al.
    ([2018a](#bib.bib118)) 2018 MLP, CNN C S GCC-PHAT $\theta$ 0-2 ✗/✓ ✗ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ He et al. ([2018b](#bib.bib119)) 2018 Res. CNN C S Real + imaginary spectrograms
    $\theta$ $\infty$ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Huang et al. ([2018](#bib.bib133)) 2018
    DNN R S Waveforms dry signal 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Li et al. ([2018](#bib.bib184))
    2018 CRNN C S GCC-PHAT $\theta$ 1 ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Ma and Liu ([2018](#bib.bib202))
    2018 CNN C S CPS $x$, $y$ 3 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Nguyen et al. ([2018](#bib.bib220))
    2018 CNN R S ILD + IPD $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Perotin et al. ([2018b](#bib.bib246))
    2018 CRNN C S Intensity $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Salvati et al.
    ([2018](#bib.bib283)) 2018 CNN C/R S Narrowband SRP components SRP weights 1 ✓
    ✗ ? ✗ ✗ ✓ ✓ Sivasankaran et al. ([2018](#bib.bib299)) 2018 CNN C S IPD $\theta$
    1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Suvorov et al. ([2018](#bib.bib310)) 2018 Res. CNN C S Waveforms
    $\theta$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Takeda et al. ([2018](#bib.bib318)) 2018 MLP C
    SS Complex eigenvectors from correlation matrix $\theta$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓
    Thuillier et al. ([2018](#bib.bib322)) 2018 CNN C S Ipsilateral + contralateral
    ear input signal $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vecchiotti et al. ([2018](#bib.bib333))
    2018 CNN R S GCC-PHAT + mel spectrograms $x$, $y$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vera-Diaz
    et al. ([2018](#bib.bib336)) 2018 CNN R S Waveforms $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✓
    ✓ ✗ ✗ ✗ ✓
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 作者 年份 架构 类型 学习- 输入特征 输出 来源 数据 NoS 知识 运动 训练 测试 采样 SA RA SR RR SA RA SR RR Kim
    和 Ling ([2011](#bib.bib149)) 2011 MLP R S 多束的功率 $\theta$ 1-5 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗
    ✓ Tsuzuki 等 ([2013](#bib.bib325)) 2013 MLP R S 时间延迟、相位延迟、声压差 $\theta$ 1 ✗ ✗ ✓
    ✗ ✗ ✗ ✗ ✓ ✗ ✗ Youssef 等 ([2013](#bib.bib375)) 2013 MLP R S ILD、ITD $\theta$ 1
    ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ Hirvonen ([2015](#bib.bib128)) 2015 CNN C S 幅度谱图 $\theta$
    1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Ma 等 ([2015](#bib.bib201)) 2015 MLP C S 双耳互相关 + ILD $\theta$
    1-3 ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ Roden 等 ([2015](#bib.bib271)) 2015 MLP C S ILD、ITD、双耳幅度
    + 相位谱图，$\theta$ / $\phi$ / $r$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ 双耳实部 + 虚部谱图 Xiao 等 ([2015](#bib.bib359))
    2015 MLP C S GCC-PHAT $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Takeda 和 Komatani ([2016b](#bib.bib316))
    2016 MLP C S 相关矩阵中的复特征向量 $\theta$、$z$、$r$ 0-1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Takeda 和 Komatani
    ([2016a](#bib.bib315)) 2016 MLP C S 相关矩阵中的复特征向量 $\theta$ 0-2 ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗
    ✓ Vesperini 等 ([2016](#bib.bib339)) 2016 MLP R S GCC-PHAT $x$、$y$ 1 ✓ ✗ ✗ ✗ ✓
    ✓ ✗ ✗ ✓ ✓ Zermini 等 ([2016](#bib.bib378)) 2016 AE C S 混合向量 + ILD + IPD $\theta$
    ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Chakrabarty 和 Habets ([2017a](#bib.bib41)) 2017 CNN C S 相位图
    $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Chakrabarty 和 Habets ([2017b](#bib.bib42)) 2017
    CNN C S 相位图 $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pertilä 和 Cakir ([2017](#bib.bib249))
    2017 CNN R S 幅度谱图 TF 掩模 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Takeda 和 Komatani ([2017](#bib.bib317))
    2017 MLP C SS 相关矩阵中的复特征向量 $\theta$、$\phi$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Yalta 等 ([2017](#bib.bib366))
    2017 Res. CNN C S 幅度谱图 $\theta$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yiwere 和 Rhee ([2017](#bib.bib374))
    2017 MLP C S 双耳互相关 + ILD $\theta$、$d$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Adavanne 等 ([2018](#bib.bib1))
    2018 CRNN C S 幅度 + 相位谱图 SPS、$\theta$、$\phi$ $\infty$ ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ He 等
    ([2018a](#bib.bib118)) 2018 MLP、CNN C S GCC-PHAT $\theta$ 0-2 ✗/✓ ✗ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ He 等 ([2018b](#bib.bib119)) 2018 Res. CNN C S 实部 + 虚部谱图 $\theta$ $\infty$
    ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Huang 等 ([2018](#bib.bib133)) 2018 DNN R S 波形干信号 1 ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ ✗ Li 等 ([2018](#bib.bib184)) 2018 CRNN C S GCC-PHAT $\theta$ 1 ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ ✗ Ma 和 Liu ([2018](#bib.bib202)) 2018 CNN C S CPS $x$、$y$ 3 ✓ ✗ ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ Nguyen 等 ([2018](#bib.bib220)) 2018 CNN R S ILD + IPD $\theta$、$\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Perotin 等 ([2018b](#bib.bib246)) 2018 CRNN C S 强度 $\theta$、$\phi$
    1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Salvati 等 ([2018](#bib.bib283)) 2018 CNN C/R S 窄带 SRP 组件
    SRP 权重 1 ✓ ✗ ? ✗ ✗ ✓ ✓ Sivasankaran 等 ([2018](#bib.bib299)) 2018 CNN C S IPD $\theta$
    1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Suvorov 等 ([2018](#bib.bib310)) 2018 Res. CNN C S 波形 $\theta$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Takeda 等 ([2018](#bib.bib318)) 2018 MLP C SS 相关矩阵中的复特征向量
    $\theta$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Thuillier 等 ([2018](#bib.bib322)) 2018 CNN C S
    同侧 + 对侧耳输入信号 $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vecchiotti 等 ([2018](#bib.bib333)) 2018
    CNN R S GCC-PHAT + mel 谱图 $x$、$y$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vera-Diaz 等 ([2018](#bib.bib336))
    2018 CNN R S 波形 $x$、$y$、$z$ 1 ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✓
- en: 'Table 3: Summary of DL-based SSL systems published in 2019, organized in alphabetical
    order. See Table I’s caption for acronyms specification.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：2019 年发布的基于 DL 的 SSL 系统总结，按字母顺序排列。有关缩写的说明，请参见表 I 的说明。
- en: Author Year Architecture Type Learn- Input features Output Sources Data NoS
    Kno. Mov. Train Test ing SA RA SR RR SA RA SR RR Adavanne et al. ([2019a](#bib.bib2))
    2019 CRNN R S FOA magnitude + phase spectrograms $x$, $y$, $z$ 1 ✓ ✗ ✓ ✓ ✓ ✓ ✓
    ✓ ✓ ✓ Adavanne et al. ([2019c](#bib.bib4)) 2019 CRNN R S FOA magnitude + phase
    spectrograms $x$, $y$, $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Cao et al. ([2019a](#bib.bib36))
    2019 CRNN R S Log-Mel spectrogr. + GCC-PHAT + intensity $\theta$, $\phi$ 1 ✓ ✗
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Cao et al. ([2019b](#bib.bib37)) 2019 CRNN R S Log-Mel spectrogr.
    + GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Chakrabarty and Habets ([2019a](#bib.bib43))
    2019 CNN C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Chakrabarty and Habets ([2019b](#bib.bib44))
    2019 CNN C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Chazan et al. ([2019](#bib.bib47))
    2019 U-net C S Phase map of the RTF between each mic pair $\theta$ $\infty$ ✗
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Chytas and Potamianos ([2019](#bib.bib52)) 2019 CNN R S Waveforms
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Comminiello et al. ([2019](#bib.bib59))
    2019 CRNN R S Quaternion FOA $x$, $y$, $z$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Grondin et al.
    ([2019](#bib.bib103)) 2019 CRNN R S CPS + GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ He et al. ([2019a](#bib.bib120)) 2019 Res. CNN C WS Real + imaginary
    spectrograms $\theta$ 1-2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Huang et al. ([2019](#bib.bib134))
    2019 CNN R S Waveforms dry signal 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Kapka and Lewandowski
    ([2019](#bib.bib145)) 2019 CRNN R S Magnitude + phase spectrograms $x$, $y$, $z$
    1-2 ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Kong et al. ([2019](#bib.bib155)) 2019 CNN R S Log-Mel
    magnitude FOA spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Krause and Kowalczyk
    ([2019](#bib.bib159)) 2019 CRNN R S Magnitude / phase spectrograms $\theta$, $\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Küçük et al. ([2019](#bib.bib167)) 2019 CNN C S Real + imaginary
    spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Kujawski et al. ([2019](#bib.bib165))
    2019 Res. CNN R S Beamforming map $x$, $y$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Leung and Ren
    ([2019](#bib.bib182)) 2019 CRNN R S CPS + real/imag. spectro + mag./phase spectro
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Lin and Wang ([2019](#bib.bib192)) 2019
    CRNN C S Magnitude and phase spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Lu ([2019](#bib.bib196)) 2019 CRNN R S GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ Maruri et al. ([2019](#bib.bib207)) 2019 CRNN R S GCC-PHAT + magnitude
    + phase spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Mazzon et al. ([2019](#bib.bib211))
    2019 CRNN R S Mel-spectrograms + GCC-PHAT/intensity $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ Noh et al. ([2019](#bib.bib226)) 2019 CNN C S GCC-PHAT $\theta$, $\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nustede and Anemüller ([2019](#bib.bib230)) 2019 CRNN R
    S Group delays $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Opochinsky et al. ([2019](#bib.bib231))
    2019 MLP R WS RTFs $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pak and Shin ([2019](#bib.bib233))
    2019 MLP R S IPD (clean) IPD ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pang et al. ([2019](#bib.bib235))
    2019 CNN R S ILD + IPD $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park et al. ([2019b](#bib.bib238))
    2019 CRNN R S Log-Mel spectrograms + intensity $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓
    ✗ ✗ ✗ ✓ Perotin et al. ([2019b](#bib.bib248)) 2019 CRNN C S FOA pseudo-intensity
    $\theta$, $\phi$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Perotin et al. ([2019a](#bib.bib247)) 2019
    CRNN C/R S FOA pseudo-intensity $\theta$, $\phi$ / $x$, $y$, $z$ 1 ✓ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ ✓ Pratik et al. ([2019](#bib.bib258)) 2019 CRNN R S GCC-PHAT + Mel/Bark
    spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Pujol et al. ([2019](#bib.bib259))
    2019 Res. CNN R S Waveforms $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ Ranjan et al. ([2019](#bib.bib264))
    2019 Res. CRNN C S Log-Mel spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓
    Sudo et al. ([2019](#bib.bib307)) 2019 CRNN R S cos(IPD), sin(IPD) $cos(\theta)$,
    $sin(\theta)$, 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ $cos(\phi)$, $sin(\phi)$ Tang et al. ([2019](#bib.bib319))
    2019 CRNN C/R S FOA pseudo-intensity $\theta$, $\phi$ / $x$, $y$, $z$ 1 ✓ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✗ ✓ Vecchiotti et al. ([2019b](#bib.bib335)) 2019 CNN R S GCC-PHAT +
    Mel-spectrograms $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Vecchiotti et al. ([2019a](#bib.bib334))
    2019 CNN C S Waveforms $\theta$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✓ Wang et al. ([2019](#bib.bib349))
    2019 RNN R S Magnitude spectrograms TF Mask 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Xue et al. ([2019](#bib.bib364))
    2019 CRNN R S Log-Mel spectr. + CQT + phase spectrogr. + CPS $\theta$, $\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Zhang et al. ([2019a](#bib.bib380)) 2019 CRNN R S Magnitude
    and phase spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Zhang et al. ([2019b](#bib.bib381))
    2019 CNN C S Phase spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 作者 年份 架构 类型 学习 输入特征 输出 数据 源 NoS 知识点 运动 训练 测试 SA RA SR RR SA RA SR RR Adavanne
    等 ([2019a](#bib.bib2)) 2019 CRNN R S FOA 幅度 + 相位声谱图 $x$, $y$, $z$ 1 ✓ ✗ ✓ ✓ ✓
    ✓ ✓ ✓ ✓ ✓ Adavanne 等 ([2019c](#bib.bib4)) 2019 CRNN R S FOA 幅度 + 相位声谱图 $x$, $y$,
    $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Cao 等 ([2019a](#bib.bib36)) 2019 CRNN R S Log-Mel 声谱图
    + GCC-PHAT + 强度 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Cao 等 ([2019b](#bib.bib37))
    2019 CRNN R S Log-Mel 声谱图 + GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Chakrabarty
    和 Habets ([2019a](#bib.bib43)) 2019 CNN C S 相位图 $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗
    Chakrabarty 和 Habets ([2019b](#bib.bib44)) 2019 CNN C S 相位图 $\theta$ 2 ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ ✓ Chazan 等 ([2019](#bib.bib47)) 2019 U-net C S 每对麦克风间的 RTF 相位图 $\theta$
    $\infty$ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Chytas 和 Potamianos ([2019](#bib.bib52)) 2019 CNN
    R S 波形 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Comminiello 等 ([2019](#bib.bib59))
    2019 CRNN R S 四元 FOA $x$, $y$, $z$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Grondin 等 ([2019](#bib.bib103))
    2019 CRNN R S CPS + GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ He 等 ([2019a](#bib.bib120))
    2019 Res. CNN C WS 真实 + 虚数声谱图 $\theta$ 1-2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Huang 等 ([2019](#bib.bib134))
    2019 CNN R S 波形 干信号 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Kapka 和 Lewandowski ([2019](#bib.bib145))
    2019 CRNN R S 幅度 + 相位声谱图 $x$, $y$, $z$ 1-2 ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Kong 等 ([2019](#bib.bib155))
    2019 CNN R S Log-Mel 幅度 FOA 声谱图 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Krause
    和 Kowalczyk ([2019](#bib.bib159)) 2019 CRNN R S 幅度 / 相位声谱图 $\theta$, $\phi$ 1
    ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Küçük 等 ([2019](#bib.bib167)) 2019 CNN C S 真实 + 虚数声谱图 $\theta$
    1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Kujawski 等 ([2019](#bib.bib165)) 2019 Res. CNN R S 波束形成图
    $x$, $y$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Leung 和 Ren ([2019](#bib.bib182)) 2019 CRNN R S
    CPS + 真实/虚数声谱图 + 幅度/相位声谱图 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Lin 和 Wang ([2019](#bib.bib192))
    2019 CRNN C S 幅度和相位声谱图 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Lu ([2019](#bib.bib196))
    2019 CRNN R S GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Maruri 等 ([2019](#bib.bib207))
    2019 CRNN R S GCC-PHAT + 幅度 + 相位声谱图 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Mazzon
    等 ([2019](#bib.bib211)) 2019 CRNN R S Mel-声谱图 + GCC-PHAT/强度 $\theta$, $\phi$ 1
    ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Noh 等 ([2019](#bib.bib226)) 2019 CNN C S GCC-PHAT $\theta$,
    $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nustede 和 Anemüller ([2019](#bib.bib230)) 2019 CRNN
    R S 群延迟 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Opochinsky 等 ([2019](#bib.bib231))
    2019 MLP R WS RTFs $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pak 和 Shin ([2019](#bib.bib233))
    2019 MLP R S IPD (清晰) IPD ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pang 等 ([2019](#bib.bib235)) 2019
    CNN R S ILD + IPD $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park 等 ([2019b](#bib.bib238))
    2019 CRNN R S Log-Mel 声谱图 + 强度 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Perotin
    等 ([2019b](#bib.bib248)) 2019 CRNN C S FOA 伪强度 $\theta$, $\phi$ 2 ✓ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ ✓ Perotin 等 ([2019a](#bib.bib247)) 2019 CRNN C/R S FOA 伪强度 $\theta$, $\phi$
    / $x$, $y$, $z$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Pratik 等 ([2019](#bib.bib258)) 2019 CRNN
    R S GCC-PHAT + Mel/Bark 声谱图 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Pujol 等 ([2019](#bib.bib259))
    2019 Res. CNN R S 波形 $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ Ranjan 等 ([2019](#bib.bib264))
    2019 Res. CRNN C S Log-Mel 声谱图 $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Sudo 等 ([2019](#bib.bib307))
    2019 CRNN R S cos(IPD), sin(IPD) $cos(\theta)$, $sin(\theta)$, 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ $cos(\phi)$, $sin(\phi)$ Tang 等 ([2019](#bib.bib319)) 2019 CRNN C/R S FOA
    伪强度 $\theta$, $\phi$ / $x$, $y$, $z$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Vecchiotti 等 ([2019b](#bib.bib335))
    2019 CNN R S GCC-PHAT + Mel-声谱图 $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Vecchiotti 等 ([2019a](#bib.bib334))
    2019 CNN C S 波形 $\theta$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✓ Wang 等 ([2019](#bib.bib349)) 2019
    RNN R S
- en: 'Table 4: Summary of DL-based SSL systems published in 2020, organized in alphabetical
    order. See Table I’s caption for acronyms specification.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：2020年发布的基于深度学习的自监督学习系统总结，按字母顺序排列。有关缩略语的说明请参见表 I 的标题。
- en: Author Year Architecture Type Learn. Input features Output Sources Data NoS
    Kno. Mov. Train Test SA RA SR RR SA RA SR RR Bianco et al. ([2020](#bib.bib25))
    2020 VAE C SS RTFs $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Cao et al. ([2020](#bib.bib38))
    2020 CRNN R S FOA waveforms $\theta$, $\phi$ 0-2 ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Comanducci
    et al. ([2020a](#bib.bib57)) 2020 CNN/U-Net C S GCC-PHAT $x$, $y$ 1 ✓ ✗ ✓ ✗ ✓
    ✗ ✗ ✗ ✓ ✓ Comanducci et al. ([2020b](#bib.bib58)) 2020 U-Net R S GCC Clean GCC
    1 ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Fahim et al. ([2020](#bib.bib84)) 2020 CNN C S FOA modal
    coherence $\theta$, $\phi$ 1-7 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Hao et al. ([2020](#bib.bib116))
    2020 CNN C S Real + imaginary spectrograms + spectral flux $\theta$ 1 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ Huang et al. ([2020](#bib.bib135)) 2020 AE R S Waveforms $\theta$
    1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Hübner et al. ([2021](#bib.bib136)) 2020 CNN C S Phase map
    $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Jenrungrot et al. ([2020](#bib.bib141)) 2020 U-Net
    R S Waveforms $\theta$ 0-8 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Mack et al. ([2020](#bib.bib204))
    2020 CNN + attention C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Le Moing et al.
    ([2020](#bib.bib172)) 2020 AE C,R S Real + imaginary spectrograms $x$, $y$ 1-3
    ✗ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Le Moing et al. ([2021](#bib.bib173)) 2020 AE C SS Real +
    imaginary spectrograms $x$, $y$ 1-3 ✗ ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Naranjo-Alcazar et al.
    ([2020](#bib.bib217)) 2020 Res. CRNN R S Log-Mel magnitude spectrograms + GCC-PHAT
    $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen et al. ([2020a](#bib.bib221)) 2020
    CNN C S Spatial pseudo-spectrum $\theta$ 0-4 ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Nguyen et al.
    ([2020b](#bib.bib222)) 2020 CRNN R S DoAs from histogram-based method $\theta$,
    $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen et al. ([2020c](#bib.bib223)) 2020 CRNN R
    S DoAs from histogram-based method $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park
    et al. ([2020](#bib.bib239)) 2020 CRNN R S Log-Mel energy + intensity $\theta$,
    $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Patel et al. ([2020](#bib.bib242)) 2020 U-Net R S
    Mel-spectrograms $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Phan et al. ([2020a](#bib.bib250))
    2020 CRNN + SA R S FOA log-Mel spectrograms + active/reactive $x$, $y$, $z$ 1
    ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ intensity, or GCC-PHAT Phan et al. ([2020b](#bib.bib251))
    2020 CRNN + SA R S FOA log-Mel spectrograms + active/reactive $x$, $y$, $z$ 1
    ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ intensity, or GCC-PHAT Ronchini et al. ([2020](#bib.bib273))
    2020 CRNN R S FOA log-Mel spectrograms + log-Mel intensity $x$, $y$, $z$ 1 ✓ ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Sampathkumar and Kowerko ([2020](#bib.bib284)) 2020 CRNN R S MIC
    + FOA Mel spectrograms + active intensity $\theta$, $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ + GCC-PHAT Shimada et al. ([2020a](#bib.bib293)) 2020 Res. CRNN R S FOA magnitude
    spectrograms + IPD ACCDOA 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Shimada et al. ([2020b](#bib.bib294))
    2020 Res. CRNN R S FOA magnitude spectrograms + IPD ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Singla et al. ([2020](#bib.bib298)) 2020 CRNN R S FOA log-Mel spectrograms +
    log-Mel intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Song ([2020](#bib.bib300))
    2020 CRNN R S GCC-PHAT + FOA active intensity $x$, $y$, $z$ 1 ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗
    ✗ ✓ Sundar et al. ([2020](#bib.bib309)) 2020 Res. CNN C/R S Waveforms $d$, $\theta$
    1-3 ✗ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ Tian ([2020](#bib.bib323)) 2020 CRNN ? S Ambisonics ?
    ? ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Varanasi et al. ([2020](#bib.bib329)) 2020 CNN C S 3rd spherical
    harmonics (phase or phase+magnitude) $\theta$, $\phi$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Varzandeh
    et al. ([2020](#bib.bib331)) 2020 CNN C S GCC-PHAT + periodicity degree $\theta$
    0-1 ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ Vera-Diaz et al. ([2020](#bib.bib337)) 2020 AE R S GCC-PHAT
    time-delay 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wang et al. ([2020](#bib.bib347)) 2020 Res. CRNN
    R S FOA pseudo-intensity + FOA log-Mel spectrograms $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ + GCC-PHAT Xue et al. ([2020](#bib.bib365)) 2020 CRNN C S CPS + waveforms
    + beamforming output $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yasuda et al. ([2020](#bib.bib373))
    2020 Res. CRNN R S FOA log-Mel spectrograms + intensity denoised IV 2 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 作者 年份 架构 类型 学习 输入特征 输出 来源 数据 NoS 知识 运动 训练 测试 SA RA SR RR SA RA SR RR Bianco
    等人 ([2020](#bib.bib25)) 2020 VAE C SS RTFs $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Cao
    等人 ([2020](#bib.bib38)) 2020 CRNN R S FOA 波形 $\theta$, $\phi$ 0-2 ✗ ✓ ✗ ✗ ✗ ✓
    ✗ ✗ ✗ ✓ Comanducci 等人 ([2020a](#bib.bib57)) 2020 CNN/U-Net C S GCC-PHAT $x$, $y$
    1 ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Comanducci 等人 ([2020b](#bib.bib58)) 2020 U-Net R S GCC Clean
    GCC 1 ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Fahim 等人 ([2020](#bib.bib84)) 2020 CNN C S FOA 模态一致性
    $\theta$, $\phi$ 1-7 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Hao 等人 ([2020](#bib.bib116)) 2020 CNN
    C S 实际 + 虚拟频谱图 + 频谱流量 $\theta$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Huang 等人 ([2020](#bib.bib135))
    2020 AE R S 波形 $\theta$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Hübner 等人 ([2021](#bib.bib136))
    2020 CNN C S 相位图 $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Jenrungrot 等人 ([2020](#bib.bib141))
    2020 U-Net R S 波形 $\theta$ 0-8 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Mack 等人 ([2020](#bib.bib204))
    2020 CNN + 注意力 C S 相位图 $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Le Moing 等人 ([2020](#bib.bib172))
    2020 AE C,R S 实际 + 虚拟频谱图 $x$, $y$ 1-3 ✗ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Le Moing 等人 ([2021](#bib.bib173))
    2020 AE C SS 实际 + 虚拟频谱图 $x$, $y$ 1-3 ✗ ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Naranjo-Alcazar 等人 ([2020](#bib.bib217))
    2020 Res. CRNN R S 对数-梅尔幅度频谱图 + GCC-PHAT $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen
    等人 ([2020a](#bib.bib221)) 2020 CNN C S 空间伪谱 $\theta$ 0-4 ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Nguyen
    等人 ([2020b](#bib.bib222)) 2020 CRNN R S 从基于直方图的方法获得的方向角 $\theta$, $\phi$ 1 ✓ ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen 等人 ([2020c](#bib.bib223)) 2020 CRNN R S 从基于直方图的方法获得的方向角
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park 等人 ([2020](#bib.bib239)) 2020 CRNN
    R S 对数-梅尔能量 + 强度 $\theta$, $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Patel 等人 ([2020](#bib.bib242))
    2020 U-Net R S 梅尔频谱图 $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Phan 等人 ([2020a](#bib.bib250))
    2020 CRNN + SA R S FOA 对数-梅尔频谱图 + 主动/反应性 $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ 强度，或
    GCC-PHAT Phan 等人 ([2020b](#bib.bib251)) 2020 CRNN + SA R S FOA 对数-梅尔频谱图 + 主动/反应性
    $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ 强度，或 GCC-PHAT Ronchini 等人 ([2020](#bib.bib273))
    2020 CRNN R S FOA 对数-梅尔频谱图 + 对数-梅尔强度 $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Sampathkumar
    和 Kowerko ([2020](#bib.bib284)) 2020 CRNN R S MIC + FOA 梅尔频谱图 + 主动强度 $\theta$,
    $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ + GCC-PHAT Shimada 等人 ([2020a](#bib.bib293)) 2020
    Res. CRNN R S FOA 幅度频谱图 + IPD ACCDOA 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Shimada 等人 ([2020b](#bib.bib294))
    2020 Res. CRNN R S FOA 幅度频谱图 + IPD ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Singla 等人 ([2020](#bib.bib298))
    2020 CRNN R S FOA 对数-梅尔频谱图 + 对数-梅尔强度 $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Song
    ([2020](#bib.bib300)) 2020 CRNN R S GCC-PHAT + FOA 主动强度 $x$, $y$, $z$ 1 ✗ ✓ ✗
    ✗ ✗ ✓ ✗ ✗ ✗ ✓ Sundar 等人 ([2020](#bib.bib309)) 2020 Res. CNN C/R S 波形 $d$, $\theta$
    1-3 ✗ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ Tian ([2020](#bib.bib323)) 2020 CRNN ? S Ambisonics ?
    ? ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Varanasi 等人 ([2020](#bib.bib329)) 2020 CNN C S 第三阶球谐函数（相位或相位+幅度）
    $\theta$, $\phi$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Varzandeh 等人 ([2020](#bib.bib331)) 2020
    CNN C S GCC-PHAT + 周期性度 $\theta$ 0-1 ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ Vera-Diaz 等人 ([2020](#bib.bib337))
    2020 AE R S GCC-PHAT 时间延迟 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wang 等人 ([2020](#bib.bib347))
    2020 Res. CRNN R S FOA 伪强度 + FOA 对数-梅尔频谱图 $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓
    + GCC-PHAT Xue 等人 ([2020](#bib.bib365)) 2020 CRNN C S CPS + 波形 + 波束形成输出 $\theta$,
    $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yasuda 等人 ([2020](#bib.bib373)) 2020 Res. CRNN R
    S FOA 对数-梅尔频谱图 + 强度去噪 IV 2 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓
- en: 'Table 5: Summary of DL-based SSL systems published in 2021, organized in alphabetical
    order. See Table I’s caption for acronyms specification.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：2021年发布的基于深度学习的自监督学习系统的总结，按字母顺序排列。有关缩写的说明，请参见表I的说明。
- en: Author Year Architecture Type Learn. Input features Output Sources Data NoS
    Kno. Mov. Train Test SA RA SR RR SA RA SR RR Adavanne et al. ([2021](#bib.bib5))
    2021 CRNN + SA R S FOA Mel spectrograms + intensity + GCC-PHAT x,y,z 2 ✓ ✓ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ Bai et al. ([2021](#bib.bib14)) 2021 Res. CRNN R S Log-Mel spectrograms
    + intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Bianco et al. ([2021](#bib.bib26))
    2021 VAE C SS RTF $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Bohlender et al. ([2021](#bib.bib28))
    2021 CNN/CRNN C S Phase map $\theta$ 1-3 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Bologni et al. ([2021](#bib.bib29))
    2021 CNN C S Waveforms $\theta$, $d$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Cao et al. ([2021](#bib.bib39))
    2021 SA R S Log-Mel spectrograms + intensity $x$, $y$, $z$ 0-2 ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗
    ✗ ✓ Castellini et al. ([2021](#bib.bib40)) 2021 MLP R S real + imaginary CPS $x$,
    $y$ 1-3 ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Diaz-Guerra et al. ([2021a](#bib.bib68)) 2021 CNN
    R S SRP-PHAT power map $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Emmanuel et al. ([2021](#bib.bib78))
    2021 CNN + SA R S Log-spectrograms + intensity ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Gelderblom
    et al. ([2021](#bib.bib94)) 2021 MLP C/R S GCC-PHAT $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✗ ✓ Gonçalves Pinto et al. ([2021](#bib.bib99)) 2021 CNN R S Magnitude CPS $x$,
    $y$ 1-10 ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Grumiaux et al. ([2021a](#bib.bib105)) 2021 CRNN
    C S Intensity $\theta$, $\phi$ 1-3 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Grumiaux et al. ([2021b](#bib.bib106))
    2021 CNN + SA C S Intensity $\theta$, $\phi$ 1-3 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Guirguis
    et al. ([2020](#bib.bib107)) 2021 TCN R S Magnitude + phase spectrograms $x$,
    $y$, $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Hammer et al. ([2021](#bib.bib115)) 2021 U-net
    C S Phase map of the RTF between each mic pair $\theta$ $\infty$ ✗ ✓ ✗ ✗ ✓ ✗ ✗
    ✗ ✗ ✓ He et al. ([2021a](#bib.bib121)) 2021 Res. CNN C WS Magnitude + phase spectrograms
    $\theta$ 1-4 ✓/✗ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✓ He et al. ([2021b](#bib.bib122)) 2021 CNN R
    S Waveforms $x$, $y$, $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Huang and Perez ([2021](#bib.bib132))
    2021 Res. CNN + SA R S Waveforms ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Komatsu et al. ([2020](#bib.bib154))
    2021 CRNN R S FOA magnitude + phase spectrograms $\theta$, $\phi$ 1 ✓ ✓ ✓ ✓ ✓
    ✓ ✓ ✓ ✓ ✓ Krause et al. ([2020a](#bib.bib160)) 2021 CNN R S Magnitude + phase
    spectrograms $x$, $y$, $z$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Krause et al. ([2020b](#bib.bib161))
    2021 CRNN R S Misc. $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Lee et al. ([2021b](#bib.bib179))
    2021 U-Net R S SRP power map $x$,$y$ 1-3 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ Lee et al. ([2021a](#bib.bib178))
    2021 CNN + attention C S Log-Mel spectrograms + intensity $\theta$ 1 ✓ ✓ ✗ ✗ ✓
    ✗ ✗ ✗ ✓ ✓ Liu et al. ([2021](#bib.bib193)) 2021 CNN C S Intensity $\theta$ 1 ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Naranjo-Alcazar et al. ([2021](#bib.bib218)) 2021 Res. CRNN
    R S Log-Mel spectrograms + GCC-PHAT ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen et al.
    ([2021a](#bib.bib224)) 2021 CRNN C S Intensity/GCC-PHAT $\theta$, $\phi$ 1 ✓ ✓
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Nguyen et al. ([2021b](#bib.bib225)) 2021 CNN + RNN/SA R S Log-spectrograms
    + DRR + SCM eigenvectors ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park et al. ([2021a](#bib.bib240))
    2021 SA R S log-Mel spectrograms + intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Poschadel et al. ([2021a](#bib.bib256)) 2021 CRNN C S HOA magnitude + phase
    spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Poschadel et al. ([2021b](#bib.bib257))
    2021 CRNN C S HOA magnitude + phase spectrograms $\theta$, $\phi$ 2-3 ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ ✓ Pujol et al. ([2021](#bib.bib260)) 2021 Res. CNN R S Waveforms $\theta$,
    $\phi$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✓ ✓ ✓ Rho et al. ([2021](#bib.bib267)) 2021 CRNN + SA R
    S Log-Mel spectrograms + intensity $\theta$, $\phi$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Schymura
    et al. ([2021](#bib.bib291)) 2021 CNN + SA R S Magnitude + phase spectrograms
    $\theta$, $\phi$ 1 ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✓ Schymura et al. ([2020](#bib.bib290)) 2021
    CNN + AE + attent. R S FOA magnitude + phase spectrograms $\theta$, $\phi$ 1 ✓
    ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Shimada et al. ([2021](#bib.bib295)) 2021 Res. CRNN + SA R S
    IPD ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Subramanian et al. ([2021a](#bib.bib304)) 2021
    CRNN C/R S Phase spectrogram $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Subramanian et al.
    ([2021b](#bib.bib305)) 2021 CRNN C Phase spectrograms, IPD $\theta$ 2 ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ ✗ Sudarsanam et al. ([2021](#bib.bib306)) 2021 SA R S Log-Mel spectrograms
    + intensity ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vargas et al. ([2021](#bib.bib330)) 2021
    CNN C S Phase map $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Vera-Diaz et al. ([2021](#bib.bib338))
    2021 AE R S GCC-PHAT time-delay 2 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wang et al. ([2021](#bib.bib348))
    2021 SA R S Mel-spectr. + intensity/Mel-spectr. + GCC-PHAT $x$, $y$, $z$ 1 ✓ ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wu et al. ([2021c](#bib.bib354)) 2021 AE R S Likelihood surface
    $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Wu et al. ([2021b](#bib.bib353)) 2021 CNN AE R
    S Beamforming heatmap image $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Xinghao et al. ([2021](#bib.bib360))
    2021 CNN + SA R S Log-Mel spectrograms + intensity ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Xu et al. ([2021a](#bib.bib362)) 2021 DenseNet R S Real CPS $x$, $y$ 6-25 ✗/✓
    ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ ✗ Yalta et al. ([2021](#bib.bib367)) 2021 SA R S Log-Mel spectrograms
    + intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yang et al. ([2021a](#bib.bib368))
    2021 CRNN C S Log-magnitude and phase spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ ✗ Yang et al. ([2021b](#bib.bib369)) 2021 CRNN C S Log-magnitude and phase spectrograms
    $\theta$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Zhang et al. ([2021](#bib.bib384)) 2021 CNN + SA
    R S Log-spectrograms + intensity + GCC-PHAT $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was funded by the French Association for Technological Research (ANRT
    CIFRE contract 2019/0533) and partially funded by the Multidisciplinary Institute
    in Artificial Intelligence MIAI@Grenoble-Alpes (ANR-19-P3IA-0003)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作由法国技术研究协会（ANRT CIFRE 合同 2019/0533）资助，并由人工智能多学科研究所 MIAI@Grenoble-Alpes（ANR-19-P3IA-0003）部分资助。
- en: References
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adavanne et al. [2018] S. Adavanne, A. Politis, and T. Virtanen. Direction of
    arrival estimation for multiple sound sources using convolutional recurrent neural
    network. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, Rome, Italy, Sept.
    2018.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adavanne 等 [2018] S. Adavanne, A. Politis, 和 T. Virtanen. 使用卷积递归神经网络进行多声源的到达方向估计。在
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, 罗马, 意大利, 2018年9月。
- en: Adavanne et al. [2019a] S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen.
    Sound event localization and detection of overlapping sources using convolutional
    recurrent neural networks. *IEEE J. Sel. Topics Signal Process.*, 13(1):34–48,
    Mar. 2019a.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adavanne 等 [2019a] S. Adavanne, A. Politis, J. Nikunen, 和 T. Virtanen. 使用卷积递归神经网络的重叠源声音事件定位和检测。*IEEE
    J. Sel. Topics Signal Process.*, 13(1):34–48, 2019年3月。
- en: Adavanne et al. [2019b] S. Adavanne, A. Politis, and T. Virtanen. A multi-room
    reverberant dataset for sound event localization and detection. In *Proc. Detection
    and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*, New
    York, NY, October 2019b.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adavanne 等 [2019b] S. Adavanne, A. Politis, 和 T. Virtanen. 用于声音事件定位和检测的多房间混响数据集。在
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*, 纽约, NY, 2019年10月。
- en: Adavanne et al. [2019c] S. Adavanne, A. Politis, and T. Virtanen. Localization,
    detection and tracking of multiple moving sound sources with a convolutional recurrent
    neural network. In *Proc. Detection and Classification of Acoustic Scenes and
    Events Workshop (DCASE Workshop)*, New York, NY, Apr. 2019c.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adavanne 等 [2019c] S. Adavanne, A. Politis, 和 T. Virtanen. 使用卷积递归神经网络对多个移动声音源进行定位、检测和跟踪。在
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*, 纽约, NY, 2019年4月。
- en: Adavanne et al. [2021] S. Adavanne, A. Politis, and T. Virtanen. Differentiable
    tracking-based training of deep learning sound source localizers. In *Proc. IEEE
    Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 211–215, New Paltz,
    NY, Oct. 2021.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adavanne 等 [2021] S. Adavanne, A. Politis, 和 T. Virtanen. 基于可微分跟踪的深度学习声音源定位器训练。在
    *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, 页码 211–215,
    New Paltz, NY, 2021年10月。
- en: Ahmad et al. [2021] M. Ahmad, M. Muaz, and M. Adeel. A survey of deep neural
    network in acoustic direction finding. In *Proc. IEEE Int. Conf. Digital Futures
    Transf. Technol. (ICoDT2)*, Islamabad, Pakistan, May 2021.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等 [2021] M. Ahmad, M. Muaz, 和 M. Adeel. 关于声学方向发现的深度神经网络综述。在 *Proc. IEEE
    Int. Conf. Digital Futures Transf. Technol. (ICoDT2)*, 伊斯兰堡, 巴基斯坦, 2021年5月。
- en: Allen and Berkley [1979] J. B. Allen and D. A. Berkley. Image method for efficiently
    simulating small-room acoustics. *J. Acoust. Soc. Am.*, 65(4):943–950, 1979.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allen 和 Berkley [1979] J. B. Allen 和 D. A. Berkley. 高效模拟小房间声学的图像方法。*J. Acoust.
    Soc. Am.*, 65(4):943–950, 1979.
- en: Amengual Garí et al. [2017] S. V. Amengual Garí, W. Lachenmayr, and E. Mommertz.
    Spatial analysis and auralization of room acoustics using a tetrahedral microphone.
    *J. Acoust. Soc. Am.*, 141(4):EL369–EL374, 2017.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amengual Garí 等 [2017] S. V. Amengual Garí, W. Lachenmayr, 和 E. Mommertz. 使用四面体麦克风进行房间声学的空间分析和听觉化。*J.
    Acoust. Soc. Am.*, 141(4):EL369–EL374, 2017.
- en: 'Anguera et al. [2012] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
    and O. Vinyals. Speaker diarization: A review of recent research. *IEEE Trans.
    Audio, Speech, Lang. Process.*, 20(2):356–370, 2012.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anguera 等 [2012] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
    和 O. Vinyals. 说话人分离：近期研究综述。*IEEE Trans. Audio, Speech, Lang. Process.*, 20(2):356–370,
    2012。
- en: Arberet et al. [2009] S. Arberet, R. Gribonval, and F. Bimbot. A robust method
    to count and locate audio sources in a multichannel underdetermined mixture. *IEEE
    Trans. Signal Process.*, 58(1):121–133, 2009.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arberet 等 [2009] S. Arberet, R. Gribonval, 和 F. Bimbot. 一种用于计数和定位多通道不足混合中的音频源的鲁棒方法。*IEEE
    Trans. Signal Process.*, 58(1):121–133, 2009.
- en: 'Argentieri et al. [2015] S. Argentieri, P. Danes, and P. Souères. A survey
    on sound source localization in robotics: From binaural to array processing methods.
    *Computer Speech Lang.*, 34(1):87–112, 2015.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argentieri 等 [2015] S. Argentieri, P. Danes, 和 P. Souères. 关于机器人中声音源定位的综述：从双耳处理到阵列处理方法。*Computer
    Speech Lang.*, 34(1):87–112, 2015.
- en: Babaee et al. [2018] M. Babaee, Z. Li, and G. Rigoll. Occlusion handling in
    tracking multiple people using RNN. In *Proc. IEEE Int. Conf. Image Process. (ICIP)*,
    pages 2715–2719, Athens, Greece, 2018.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babaee 等人 [2018] M. Babaee, Z. Li 和 G. Rigoll. 使用 RNN 处理多人的遮挡问题. 在 *IEEE 国际图像处理会议
    (ICIP)* 上, 第2715–2719页, 希腊雅典, 2018年。
- en: Bahdanau et al. [2016] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation
    by jointly learning to align and translate, May 2016. arXiv:1409.0473.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等人 [2016] D. Bahdanau, K. Cho 和 Y. Bengio. 神经机器翻译通过联合学习对齐和翻译, 2016年5月.
    arXiv:1409.0473.
- en: 'Bai et al. [2021] J. Bai, Z. Pu, and J. Chen. DCASE 2021 Task 3: SELD system
    based on Resnet and random segment augmentation. Technical report, November 2021.
    DCASE 2021 Challenge.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等人 [2021] J. Bai, Z. Pu 和 J. Chen. DCASE 2021 任务3: 基于 Resnet 和随机片段增强的 SELD
    系统. 技术报告, 2021年11月. DCASE 2021 挑战。'
- en: Bai et al. [2019] S. Bai, J. Z. Kolter, and V. Koltun. Trellis networks for
    sequence modeling, Mar. 2019. arXiv:1810.06682.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2019] S. Bai, J. Z. Kolter 和 V. Koltun. 用于序列建模的格网网络, 2019年3月. arXiv:1810.06682.
- en: Ban et al. [2018] Y. Ban, X. Li, X. Alameda-Pineda, L. Girin, and R. Horaud.
    Accounting for room acoustics in audio-visual multi-speaker tracking. In *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 6553–6557, Alberta,
    Canada, Apr. 2018.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ban 等人 [2018] Y. Ban, X. Li, X. Alameda-Pineda, L. Girin 和 R. Horaud. 考虑房间声学的视听多说话者跟踪.
    在 *IEEE 国际声学、语音和信号处理会议 (ICASSP)* 上, 第6553–6557页, 加拿大阿尔伯塔, 2018年4月。
- en: 'Basten et al. [2008] T. Basten, H. de Bree, and S. Sadasivan. Acoustic eyes:
    a novel sound source localization and monitoring technique with 3D sound probes.
    In *Proc. Int. Conf. Noise Vibration Engin. (ISMA)*, Leuven, Belgium, 2008.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Basten 等人 [2008] T. Basten, H. de Bree 和 S. Sadasivan. 声学眼睛: 一种新型的声音源定位和监测技术，使用
    3D 声音探头. 在 *国际噪声与振动工程会议 (ISMA)* 上, 比利时鲁汶, 2008年。'
- en: Benesty et al. [2008] J. Benesty, J. Chen, and Y. Huang. *Microphone Array Signal
    Processing*. Springer Science & Business Media, 2008.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benesty 等人 [2008] J. Benesty, J. Chen 和 Y. Huang. *麦克风阵列信号处理*. Springer 科学与商业媒体,
    2008年。
- en: Bengio [2012] Y. Bengio. Deep learning of representations for unsupervised and
    transfer learning. In *Proc. ICML Workshop Unsupervised & Transfer Learn.*, pages
    17–36, Bellevue, Washington, 2012.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio [2012] Y. Bengio. 深度学习表示用于无监督学习和迁移学习. 在 *ICML 无监督与迁移学习研讨会* 上, 第17–36页,
    华盛顿州贝尔维尤, 2012年。
- en: 'Bengio et al. [2013] Y. Bengio, A. Courville, and P. Vincent. Representation
    learning: A review and new perspectives. *IEEE Trans. Pattern Anal. Mach. Intell.*,
    35(8):1798–1828, 2013.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bengio 等人 [2013] Y. Bengio, A. Courville 和 P. Vincent. 表示学习: 回顾与新视角. *IEEE
    模式分析与机器智能学报*, 35(8):1798–1828, 2013年。'
- en: Bernschütz [2016] B. Bernschütz. *Microphone arrays and sound field decomposition
    for dynamic binaural recording*. PhD thesis, Technische Universitaet Berlin (Germany),
    2016.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernschütz [2016] B. Bernschütz. *麦克风阵列和声音场分解用于动态双耳录音*. 博士学位论文, 柏林工业大学 (德国),
    2016年。
- en: Bialer et al. [2019] O. Bialer, N. Garnett, and T. Tirer. Performance advantages
    of deep neural networks for angle of arrival estimation. In *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*, pages 3907–3911, Brighton, UK, 2019.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bialer 等人 [2019] O. Bialer, N. Garnett 和 T. Tirer. 深度神经网络在到达角度估计中的性能优势. 在 *IEEE
    国际声学、语音和信号处理会议 (ICASSP)* 上, 第3907–3911页, 英国布莱顿, 2019年。
- en: 'Bianchi et al. [2016] L. Bianchi, F. Antonacci, A. Sarti, and S. Tubaro. The
    ray space transform: a new framework for wave field processing. *IEEE Trans. Signal
    Process.*, 64(21):5696–5706, Nov. 2016.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bianchi 等人 [2016] L. Bianchi, F. Antonacci, A. Sarti 和 S. Tubaro. 光线空间变换: 一种新的波场处理框架.
    *IEEE 信号处理学报*, 64(21):5696–5706, 2016年11月。'
- en: 'Bianco et al. [2019] M. J. Bianco, P. Gerstoft, J. Traer, E. Ozanich, M. A.
    Roch, S. Gannot, and C.-A. Deledalle. Machine learning in acoustics: Theory and
    applications. *J. Acoust. Soc. Am.*, 146(5):3590–3628, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bianco 等人 [2019] M. J. Bianco, P. Gerstoft, J. Traer, E. Ozanich, M. A. Roch,
    S. Gannot 和 C.-A. Deledalle. 声学中的机器学习: 理论与应用. *美国声学学会杂志*, 146(5):3590–3628, 2019年。'
- en: Bianco et al. [2020] M. J. Bianco, S. Gannot, and P. Gerstoft. Semi-supervised
    source localization with deep generative modeling. In *Proc. IEEE Int. Workshop
    Mach. Learn. Signal Process. (MLSP)*, Eespo, Finland (virtual conference), 2020.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianco 等人 [2020] M. J. Bianco, S. Gannot 和 P. Gerstoft. 使用深度生成建模的半监督源定位. 在 *IEEE
    国际机器学习信号处理研讨会 (MLSP)* 上, 芬兰埃斯波 (虚拟会议), 2020年。
- en: Bianco et al. [2021] M. J. Bianco, S. Gannot, E. Fernandez-Grande, and P. Gerstoft.
    Semi-supervised source localization in reverberant environments with deep generative
    modeling. *IEEE Access*, 9:84956–84970, 2021.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianco 等人 [2021] M. J. Bianco, S. Gannot, E. Fernandez-Grande 和 P. Gerstoft.
    在混响环境中利用深度生成建模进行半监督源定位. *IEEE Access*, 9:84956–84970, 2021年。
- en: Blandin et al. [2012] C. Blandin, A. Ozerov, and E. Vincent. Multi-source TDOA
    estimation in reverberant audio using angular spectra and clustering. *Signal
    Process.*, 92(8):1950–1960, 2012.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blandin等人 [2012] C. Blandin, A. Ozerov, 和 E. Vincent. 使用角谱和聚类在混响音频中进行多源TDOA估计。*信号处理*,
    92(8):1950–1960, 2012。
- en: Bohlender et al. [2021] A. Bohlender, A. Spriet, W. Tirry, and N. Madhu. Exploiting
    temporal context in CNN based multisource DoA estimation. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 29:1594–1608, 2021.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bohlender等人 [2021] A. Bohlender, A. Spriet, W. Tirry, 和 N. Madhu. 利用时间上下文进行基于CNN的多源方向估计。*IEEE/ACM
    音频、语音与语言处理汇刊*, 29:1594–1608, 2021。
- en: Bologni et al. [2021] G. Bologni, R. Heusdens, and J. Martinez. Acoustic reflectors
    localization from stereo recordings using neural networks. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 1–5, Toronto, Canada (virtual
    conference), June 2021.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bologni等人 [2021] G. Bologni, R. Heusdens, 和 J. Martinez. 使用神经网络从立体录音中定位声学反射器。*IEEE国际声学、语音与信号处理会议（ICASSP）*会议录,
    第1–5页, 加拿大多伦多（虚拟会议）, 2021年6月。
- en: Bouatouch et al. [2006] K. Bouatouch, O. Deille, J. Maillard, J. Martin, and
    N. Noé. Real time acoustic rendering of complex environments including diffraction
    and curved surfaces. In *Proc. Audio Engin. Soc. (AES) Conv.*, Paris, France,
    2006.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bouatouch等人 [2006] K. Bouatouch, O. Deille, J. Maillard, J. Martin, 和 N. Noé.
    实时声学渲染复杂环境，包括衍射和曲面。*音频工程学会（AES）会议论文集*, 巴黎，法国, 2006。
- en: 'Brandstein and Ward [2001] M. Brandstein and D. Ward. *Microphone Arrays: Signal
    Processing Techniques and Applications*. Springer Science & Business Media, 2001.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandstein和Ward [2001] M. Brandstein 和 D. Ward. *麦克风阵列：信号处理技术与应用*. Springer科学与商业媒体,
    2001。
- en: Brutti et al. [2010] A. Brutti, L. Cristoforetti, W. Kellermann, L. Marquardt,
    and M. Omologo. WOZ acoustic data collection for interactive TV. *Lang. Resources
    Eval.*, 44(3):205–219, 2010.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brutti等人 [2010] A. Brutti, L. Cristoforetti, W. Kellermann, L. Marquardt, 和
    M. Omologo. 用于互动电视的WOZ声学数据收集。*语言资源评估*, 44(3):205–219, 2010。
- en: Bush and Xiang [2018] D. Bush and N. Xiang. A model-based Bayesian framework
    for sound source enumeration and direction of arrival estimation using a coprime
    microphone array. *J. Acoust. Soc. Am.*, 143(6):3934–3945, 2018.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bush和Xiang [2018] D. Bush 和 N. Xiang. 基于模型的贝叶斯框架用于使用互质麦克风阵列进行声源计数和到达方向估计。*美国声学学会期刊*,
    143(6):3934–3945, 2018。
- en: Campbell et al. [2005] D. Campbell, K. Palomaki, and G. Brown. A Matlab simulation
    of shoebox room acoustics for use in research and teaching. *Computing Inform.
    Syst.*, 9(3):48, 2005.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campbell等人 [2005] D. Campbell, K. Palomaki, 和 G. Brown. Matlab模拟用于研究和教学的鞋盒房间声学。*计算信息系统*,
    9(3):48, 2005。
- en: 'Candes et al. [2006] E. J. Candes, J. K. Romberg, and T. Tao. Stable signal
    recovery from incomplete and inaccurate measurements. *Communications on Pure
    and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical
    Sciences*, 59(8):1207–1223, 2006.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Candes等人 [2006] E. J. Candes, J. K. Romberg, 和 T. Tao. 从不完整和不准确的测量中稳定地恢复信号。*纯粹与应用数学通讯：由Courant数学科学研究所发布的期刊*,
    59(8):1207–1223, 2006。
- en: Cao et al. [2019a] Y. Cao, T. Iqbal, Q. Kong, M. B. Galindo, W. Wang, and M. D.
    Plumbley. Two-stage sound event localization and detection using intensity vector
    and generalized cross-correlation. Technical report, 2019a. DCASE 2019 Challenge.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人 [2019a] Y. Cao, T. Iqbal, Q. Kong, M. B. Galindo, W. Wang, 和 M. D. Plumbley.
    使用强度向量和广义互相关的两阶段声音事件定位和检测。技术报告, 2019a。DCASE 2019挑战。
- en: Cao et al. [2019b] Y. Cao, Q. Kong, T. Iqbal, F. An, W. Wang, and M. Plumbley.
    Polyphonic sound event detection and localization using a two-stage strategy.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, New York, NY, October 2019b.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人 [2019b] Y. Cao, Q. Kong, T. Iqbal, F. An, W. Wang, 和 M. Plumbley. 使用两阶段策略的多音声音事件检测和定位。*声学场景和事件检测与分类研讨会（DCASE
    Workshop）*会议录, 纽约, NY, 2019年10月。
- en: Cao et al. [2020] Y. Cao, T. Iqbal, Q. Kong, Y. Zhong, W. Wang, and M. D. Plumbley.
    Event-independent network for polyphonic sound event localization and detection.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, Tokyo, Japan, 2020.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人 [2020] Y. Cao, T. Iqbal, Q. Kong, Y. Zhong, W. Wang, 和 M. D. Plumbley.
    用于多音声音事件定位和检测的事件无关网络。*声学场景和事件检测与分类研讨会（DCASE Workshop）*会议录, 东京, 日本, 2020年。
- en: Cao et al. [2021] Y. Cao, T. Iqbal, Q. Kong, F. An, W. Wang, and M. D. Plumbley.
    An improved event-independent network for polyphonic sound event localization
    and detection. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    Toronto, Canada (virtual conference), 2021.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 [2021] Y. Cao, T. Iqbal, Q. Kong, F. An, W. Wang 和 M. D. Plumbley. 改进的事件独立网络用于多音频事件定位和检测。发表于
    *IEEE 国际声学、语音与信号处理会议（ICASSP）*，加拿大多伦多（虚拟会议），2021。
- en: Castellini et al. [2021] P. Castellini, N. Giulietti, N. Falcionelli, A. F.
    Dragoni, and P. Chiariotti. A neural network based microphone array approach to
    grid-less noise source localization. *Applied Acoustics*, 177:107947, 2021. ISSN
    0003-682X.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castellini 等 [2021] P. Castellini, N. Giulietti, N. Falcionelli, A. F. Dragoni
    和 P. Chiariotti. 基于神经网络的麦克风阵列方法用于无网格噪声源定位。*应用声学*，177:107947，2021。ISSN 0003-682X。
- en: Chakrabarty and Habets [2017a] S. Chakrabarty and E. A. P. Habets. Broadband
    DoA estimation using convolutional neural networks trained with noise signals.
    In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 136–140,
    New Paltz, NY, 2017a.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 和 Habets [2017a] S. Chakrabarty 和 E. A. P. Habets. 使用噪声信号训练的卷积神经网络进行宽带到达角估计。发表于
    *IEEE 应用信号处理与音频声学研讨会（WASPAA）*，第 136–140 页，美国纽约州新帕尔茨，2017a。
- en: Chakrabarty and Habets [2017b] S. Chakrabarty and E. A. P. Habets. Multi-speaker
    localization using convolutional neural network trained with noise, 2017b. arXiv:1712.04276.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 和 Habets [2017b] S. Chakrabarty 和 E. A. P. Habets. 使用噪声训练的卷积神经网络进行多扬声器定位，2017b。arXiv:1712.04276。
- en: Chakrabarty and Habets [2019a] S. Chakrabarty and E. A. P. Habets. Multi-scale
    aggregation of phase information for reducing computational cost of CNN based
    DoA estimation. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, A Coruña, Spain,
    2019a.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 和 Habets [2019a] S. Chakrabarty 和 E. A. P. Habets. 多尺度相位信息聚合以降低基于
    CNN 的到达角估计的计算成本。发表于 *欧洲信号处理会议（EUSIPCO）*，西班牙拉科鲁尼亚，2019a。
- en: Chakrabarty and Habets [2019b] S. Chakrabarty and E. A. P. Habets. Multi-speaker
    DoA estimation using deep convolutional networks trained with noise signals. *IEEE
    J. Sel. Topics Signal Process.*, 13(1):8–21, 2019b. ISSN 1932-4553, 1941-0484.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty 和 Habets [2019b] S. Chakrabarty 和 E. A. P. Habets. 使用噪声信号训练的深度卷积网络进行多扬声器到达角估计。*IEEE
    选择主题信号处理期刊*，13(1):8–21，2019b。ISSN 1932-4553，1941-0484。
- en: Chang et al. [2018] S.-Y. Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi,
    A. van den Oord, and O. Vinyals. Temporal modeling using dilated convolution and
    gating for voice-activity-detection. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 5549–5553, Calgary, Canada, Apr. 2018.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等 [2018] S.-Y. Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi, A.
    van den Oord 和 O. Vinyals. 使用膨胀卷积和门控的时间建模用于语音活动检测。发表于 *IEEE 国际声学、语音与信号处理会议（ICASSP）*，第
    5549–5553 页，加拿大卡尔加里，2018 年 4 月。
- en: Chardon and Daudet [2012] G. Chardon and L. Daudet. Narrowband source localization
    in an unknown reverberant environment using wavefield sparse decomposition. In
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 9–12,
    Kyoto, Japan, 2012.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chardon 和 Daudet [2012] G. Chardon 和 L. Daudet. 在未知混响环境中使用波场稀疏分解进行窄带源定位。发表于
    *IEEE 国际声学、语音与信号处理会议（ICASSP）*，第 9–12 页，京都，日本，2012。
- en: Chazan et al. [2019] S. E. Chazan, H. Hammer, G. Hazan, J. Goldberger, and S. Gannot.
    Multi-microphone speaker separation based on deep DoA estimation. In *Proc. Europ.
    Signal Process. Conf. (EUSIPCO)*, A Coruña, Spain, Sept. 2019.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chazan 等 [2019] S. E. Chazan, H. Hammer, G. Hazan, J. Goldberger 和 S. Gannot.
    基于深度到达角估计的多麦克风扬声器分离。发表于 *欧洲信号处理会议（EUSIPCO）*，西班牙拉科鲁尼亚，2019 年 9 月。
- en: Chiariotti et al. [2019] P. Chiariotti, M. Martarelli, and P. Castellini. Acoustic
    beamforming for noise source localization – reviews, methodology and applications.
    *Mech. Systems Signal Process.*, 120:422–448, 2019.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiariotti 等 [2019] P. Chiariotti, M. Martarelli 和 P. Castellini. 用于噪声源定位的声学波束形成——综述、方法和应用。*机械系统与信号处理*，120:422–448，2019。
- en: Cho et al. [2014] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,
    H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder
    for statistical machine translation, Sept. 2014. arXiv:1406.1078.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等 [2014] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,
    H. Schwenk 和 Y. Bengio. 使用 RNN 编码器-解码器进行统计机器翻译的短语表示学习，2014 年 9 月。arXiv:1406.1078。
- en: Choi and Chang [2020] J. Choi and J.-H. Chang. Convolutional neural network-based
    DoA estimation using stereo microphones for drone. In *Int. Conf. Electron., Inform.,
    Comm. (ICEIC)*, pages 1–5, Barcelona, Spain, Jan. 2020.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 和 Chang [2020] J. Choi 和 J.-H. Chang. 基于卷积神经网络的到达角估计，用于无人机的立体声麦克风。发表于 *电子信息通信国际会议（ICEIC）*，第
    1–5 页，西班牙巴塞罗那，2020 年 1 月。
- en: Chollet [2017] F. Chollet. *Deep Learning with Python*. Simon and Schuster,
    2017.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet [2017] F. Chollet. *Python 深度学习*。Simon and Schuster，2017年。
- en: Chytas and Potamianos [2019] S. P. Chytas and G. Potamianos. Hierarchical detection
    of sound events and their localization using convolutional neural networks with
    adaptive thresholds. In *Proc. Detection and Classification of Acoustic Scenes
    and Events Workshop (DCASE Workshop)*, New York, NY, 2019.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chytas 和 Potamianos [2019] S. P. Chytas 和 G. Potamianos. 使用具有自适应阈值的卷积神经网络进行声音事件的层次检测及其定位。在
    *声音场景与事件检测与分类研讨会（DCASE研讨会）* 上，纽约，NY，2019年。
- en: 'Ciaparrone et al. [2020] G. Ciaparrone, F. Luque Sánchez, S. Tabik, L. Troiano,
    R. Tagliaferri, and F. Herrera. Deep learning in video multi-object tracking:
    A survey. *Neurocomp.*, 381:61–88, 2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciaparrone 等人 [2020] G. Ciaparrone, F. Luque Sánchez, S. Tabik, L. Troiano,
    R. Tagliaferri, 和 F. Herrera. 视频多目标跟踪中的深度学习：综述。 *神经计算*，381:61–88，2020年。
- en: Cobos et al. [2017] M. Cobos, F. Antonacci, A. Alexandridis, A. Mouchtaris,
    and B. Lee. A survey of sound source localization methods in wireless acoustic
    sensor networks. *Wireless Comm. Mobile Computing*, 2017, 2017.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobos 等人 [2017] M. Cobos, F. Antonacci, A. Alexandridis, A. Mouchtaris, 和 B. Lee.
    无线声学传感网络中声音源定位方法的综述。 *无线通信与移动计算*，2017年，2017年。
- en: Cohen [2004] I. Cohen. Relative transfer function identification using speech
    signals. *IEEE Trans. Speech Audio Process.*, 12(5):451–459, 2004.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen [2004] I. Cohen. 使用语音信号的相对传递函数识别。 *IEEE 语音与音频处理汇刊*，12(5):451–459，2004年。
- en: Cohen et al. [2019] T. Cohen, M. Weiler, B. Kicanaoglu, and M. Welling. Gauge
    equivariant convolutional networks and the icosahedral cnn. In *International
    conference on Machine learning*, pages 1321–1330\. PMLR, 2019.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等人 [2019] T. Cohen, M. Weiler, B. Kicanaoglu, 和 M. Welling. 量规等变卷积网络和二十面体卷积神经网络。在
    *国际机器学习会议* 上，页码 1321–1330，PMLR，2019年。
- en: Comanducci et al. [2020a] L. Comanducci, F. Borra, P. Bestagini, F. Antonacci,
    S. Tubaro, and A. Sarti. Source localization using distributed microphones in
    reverberant environments based on deep learning and ray space transform. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 28:2238–2251, 2020a.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comanducci 等人 [2020a] L. Comanducci, F. Borra, P. Bestagini, F. Antonacci, S. Tubaro,
    和 A. Sarti. 基于深度学习和射线空间变换的混响环境中使用分布式麦克风进行源定位。 *IEEE/ACM 音频、语音与语言处理汇刊*，28:2238–2251，2020年。
- en: Comanducci et al. [2020b] L. Comanducci, M. Cobos, F. Antonacci, and A. Sarti.
    Time difference of arrival estimation from frequency-sliding generalized cross-correlations
    using convolutional neural networks. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 4945–4949, Barcelona, Spain (virtual conference),
    May 2020b.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comanducci 等人 [2020b] L. Comanducci, M. Cobos, F. Antonacci, 和 A. Sarti. 使用卷积神经网络从频率滑动广义互相关估计到达时间差。在
    *IEEE国际声学、语音和信号处理会议（ICASSP）* 上，页码 4945–4949，西班牙巴塞罗那（虚拟会议），2020年5月。
- en: Comminiello et al. [2019] D. Comminiello, M. Lella, S. Scardapane, and A. Uncini.
    Quaternion convolutional neural networks for detection and localization of 3D
    sound events. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    Brighton, UK, 2019.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comminiello 等人 [2019] D. Comminiello, M. Lella, S. Scardapane, 和 A. Uncini.
    用于检测和定位三维声音事件的四元数卷积神经网络。在 *IEEE国际声学、语音和信号处理会议（ICASSP）* 上，英国布莱顿，2019年。
- en: Crawford and Pineau [2020] E. Crawford and J. Pineau. Exploiting spatial invariance
    for scalable unsupervised object tracking. In *Proc. AAAI Conf. Artif. Intell.*,
    New York, NY, 2020.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crawford 和 Pineau [2020] E. Crawford 和 J. Pineau. 利用空间不变性进行可扩展的无监督目标跟踪。在 *AAAI
    人工智能会议* 上，纽约，NY，2020年。
- en: Cristoforetti et al. [2014] L. Cristoforetti, M. Ravanelli, M. Omologo, A. Sosi,
    and A. Abad. The DIRHA simulated corpus. In *Int. Conf. Lang. Resources Eval.
    (LREC)*, pages 2629–2634, Reykjavik, Iceland, 2014.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cristoforetti 等人 [2014] L. Cristoforetti, M. Ravanelli, M. Omologo, A. Sosi,
    和 A. Abad. DIRHA 模拟语料库。在 *国际语言资源评价会议（LREC）* 上，页码 2629–2634，冰岛雷克雅未克，2014年。
- en: Daniel and Kitić [2020] J. Daniel and S. Kitić. Time-domain velocity vector
    for retracing the multipath propagation. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 421–425, Barcelona, Spain (virtual conference),
    2020.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daniel 和 Kitić [2020] J. Daniel 和 S. Kitić. 用于重现多路径传播的时间域速度向量。在 *IEEE国际声学、语音和信号处理会议（ICASSP）*
    上，页码 421–425，西班牙巴塞罗那（虚拟会议），2020年。
- en: Datum et al. [1996] M. S. Datum, F. Palmieri, and A. Moiseff. An artificial
    neural network for sound localization using binaural cues. *J. Acoust. Soc. Am.*,
    100(1):372–383, 1996.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Datum 等人 [1996] M. S. Datum, F. Palmieri, 和 A. Moiseff. 用于声音定位的人工神经网络，使用双耳线索。
    *声学学会杂志*，100(1):372–383，1996年。
- en: de Bree [2003] H.-E. de Bree. An overview of microflown technologies. *Acta
    Acustica united with Acustica*, 89(1):163–172, 2003.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Bree [2003] H.-E. de Bree. 微型流技术概述。*《声学学报与声学》*, 89(1):163–172, 2003.
- en: Deleforge and Horaud [2012] A. Deleforge and R. Horaud. 2D sound-source localization
    on the binaural manifold. In *Proc. IEEE Int. Workshop Mach. Learn. Signal Process.
    (MLSP)*, pages 1–6, Santander, Spain, 2012.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deleforge 和 Horaud [2012] A. Deleforge 和 R. Horaud. 在双耳流形上的 2D 声源定位。见 *《IEEE
    国际机器学习信号处理研讨会论文集 (MLSP)》*, 页码 1–6, 西班牙桑坦德, 2012.
- en: Deleforge et al. [2013] A. Deleforge, F. Forbes, and R. Horaud. Variational
    EM for binaural sound-source separation and localization. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, Vancouver, Canada, 2013.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deleforge 等 [2013] A. Deleforge, F. Forbes, 和 R. Horaud. 用于双耳声音源分离和定位的变分 EM。见
    *《IEEE 国际声学、语音与信号处理会议论文集 (ICASSP)》*, 加拿大温哥华, 2013.
- en: Deleforge et al. [2015] A. Deleforge, R. Horaud, Y. Y. Schechner, and L. Girin.
    Co-localization of audio sources in images using binaural features and locally-linear
    regression. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 23(4):718–731, 2015.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deleforge 等 [2015] A. Deleforge, R. Horaud, Y. Y. Schechner, 和 L. Girin. 使用双耳特征和局部线性回归的图像中音频源的共同定位。*《IEEE/ACM
    音频、语音与语言处理汇刊》*, 23(4):718–731, 2015.
- en: Diaz-Guerra et al. [2021a] D. Diaz-Guerra, A. Miguel, and J. R. Beltran. Robust
    sound source tracking using SRP-PHAT and 3D convolutional neural networks. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 29:300–311, 2021a.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diaz-Guerra 等 [2021a] D. Diaz-Guerra, A. Miguel, 和 J. R. Beltran. 使用 SRP-PHAT
    和 3D 卷积神经网络的鲁棒声音源追踪。*《IEEE/ACM 音频、语音与语言处理汇刊》*, 29:300–311, 2021a.
- en: 'Diaz-Guerra et al. [2021b] D. Diaz-Guerra, A. Miguel, and J. R. Beltran. gpuRIR:
    A python library for room impulse response simulation with GPU acceleration. *Multimedia
    Tools Applic.*, 80(4):5653–5671, 2021b.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diaz-Guerra 等 [2021b] D. Diaz-Guerra, A. Miguel, 和 J. R. Beltran. gpuRIR：一个用于房间冲击响应模拟的
    Python 库，具有 GPU 加速。*《多媒体工具与应用》*, 80(4):5653–5671, 2021b.
- en: 'DiBiase et al. [2001] J. H. DiBiase, H. F. Silverman, and M. S. Brandstein.
    Robust localization in reverberant rooms. In M. Brandstein and D. Ward, editors,
    *Microphone Arrays: Signal Processing Techniques and Applications*, pages 157–180.
    Springer, Berlin, 2001. ISBN 978-3-662-04619-7.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DiBiase 等 [2001] J. H. DiBiase, H. F. Silverman, 和 M. S. Brandstein. 在混响房间中的鲁棒定位。见
    M. Brandstein 和 D. Ward, 编辑, *《麦克风阵列：信号处理技术与应用》*, 页码 157–180. Springer, Berlin,
    2001. ISBN 978-3-662-04619-7.
- en: 'Dmochowski et al. [2007] J. P. Dmochowski, J. Benesty, and S. Affes. Broadband
    MUSIC: Opportunities and challenges for multiple source localization. In *Proc.
    IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 18–21, New
    Paltz, NY, 2007.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dmochowski 等 [2007] J. P. Dmochowski, J. Benesty, 和 S. Affes. 宽带 MUSIC：多源定位的机会与挑战。见
    *《IEEE 应用信号处理与音频声学研讨会论文集 (WASPAA)》*, 页码 18–21, New Paltz, NY, 2007.
- en: Dorfan and Gannot [2015] Y. Dorfan and S. Gannot. Tree-based recursive expectation-maximization
    algorithm for localization of acoustic sources. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 23(10):1692–1703, 2015.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorfan 和 Gannot [2015] Y. Dorfan 和 S. Gannot. 基于树的递归期望最大化算法用于声源定位。*《IEEE/ACM
    音频、语音与语言处理汇刊》*, 23(10):1692–1703, 2015.
- en: Duong et al. [2010] N. Q. Duong, E. Vincent, and R. Gribonval. Under-determined
    reverberant audio source separation using a full-rank spatial covariance model.
    *IEEE Trans. Audio, Speech, Lang. Process.*, 18(7):1830–1840, 2010.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duong 等 [2010] N. Q. Duong, E. Vincent, 和 R. Gribonval. 使用满秩空间协方差模型的欠定混响音频源分离。*《IEEE
    音频、语音与语言处理汇刊》*, 18(7):1830–1840, 2010.
- en: Dvorkind and Gannot [2005] T. G. Dvorkind and S. Gannot. Time difference of
    arrival estimation of speech source in a noisy and reverberant environment. *Signal
    Process.*, 85(1):177–204, 2005.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvorkind 和 Gannot [2005] T. G. Dvorkind 和 S. Gannot. 在嘈杂和混响环境中语音源的到达时间差估计。*《信号处理》*,
    85(1):177–204, 2005.
- en: Eaton et al. [2015] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor.
    The ACE challenge — Corpus description and performance evaluation. In *Proc. IEEE
    Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 1–5, New Paltz,
    NY, Oct. 2015.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eaton 等 [2015] J. Eaton, N. D. Gaubitch, A. H. Moore, 和 P. A. Naylor. ACE 挑战——语料库描述和性能评估。见
    *《IEEE 应用信号处理与音频声学研讨会论文集 (WASPAA)》*, 页码 1–5, New Paltz, NY, 2015年10月.
- en: El Zooghby et al. [2000] A. El Zooghby, C. Christodoulou, and M. Georgiopoulos.
    A neural network-based smart antenna for multiple source tracking. *IEEE Trans.
    Antennas Propag.*, 48(5):768–776, 2000.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El Zooghby 等 [2000] A. El Zooghby, C. Christodoulou, 和 M. Georgiopoulos. 基于神经网络的智能天线用于多源追踪。*《IEEE
    天线与传播汇刊》*, 48(5):768–776, 2000.
- en: 'Elbir [2020] A. M. Elbir. DeepMUSIC: multiple signal classification via deep
    learning. *IEEE Sensors Lett.*, 4(4):1–4, Apr. 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elbir [2020] A. M. Elbir. DeepMUSIC: 通过深度学习的多信号分类。*IEEE Sensors Lett.*, 4(4):1–4,
    2020年4月。'
- en: Emmanuel et al. [2021] P. Emmanuel, N. Parrish, and M. Horton. Multi-scale network
    for sound event localization and detection. Technical report, 2021. DCASE 2021
    Challenge.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Emmanuel et al. [2021] P. Emmanuel, N. Parrish, 和 M. Horton. 多尺度网络用于声音事件定位和检测。技术报告，2021年。DCASE
    2021 挑战。
- en: 'Engel et al. [2020] J. Engel, L. Hantrakul, C. Gu, and A. Roberts. DDSP: Differentiable
    digital signal processing, 2020. arXiv:2001.04643.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engel et al. [2020] J. Engel, L. Hantrakul, C. Gu, 和 A. Roberts. DDSP：可微分数字信号处理，2020年。arXiv:2001.04643。
- en: Erdogan et al. [2016] H. Erdogan, J. R. Hershey, S. Watanabe, M. Mandel, and
    J. Le Roux. Improved MVDR beamforming using single-channel mask prediction networks.
    In *Proc. Interspeech Conf.*, San Francisco, CA, 2016.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erdogan et al. [2016] H. Erdogan, J. R. Hershey, S. Watanabe, M. Mandel, 和 J. Le Roux.
    使用单通道掩蔽预测网络的改进 MVDR 波束形成。在 *Proc. Interspeech Conf.*，加利福尼亚州旧金山，2016年。
- en: Escolano et al. [2014] J. Escolano, N. Xiang, J. M. Perez-Lorenzo, M. Cobos,
    and J. J. Lopez. A Bayesian direction-of-arrival model for an undetermined number
    of sources using a two-microphone array. *J. Acoust. Soc. Am.*, 135(2):742–753,
    2014.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Escolano et al. [2014] J. Escolano, N. Xiang, J. M. Perez-Lorenzo, M. Cobos,
    和 J. J. Lopez. 使用双麦克风阵列的贝叶斯到达方向模型。*J. Acoust. Soc. Am.*, 135(2):742–753, 2014年。
- en: Evers et al. [2014] C. Evers, A. H. Moore, and P. A. Naylor. Multiple source
    localisation in the spherical harmonic domain. In *Proc. IEEE Int. Workshop Acoustic
    Signal Enhanc. (IWAENC)*, pages 258–262, Antibes, France, 2014.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evers et al. [2014] C. Evers, A. H. Moore, 和 P. A. Naylor. 球面谐波域中的多源定位。在 *Proc.
    IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*，第258–262页，法国安提布，2014年。
- en: 'Evers et al. [2020] C. Evers, H. W. Löllmann, H. Mellmann, A. Schmidt, H. Barfuss,
    P. A. Naylor, and W. Kellermann. The LOCATA challenge: Acoustic source localization
    and tracking. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 28:1620–1643, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evers et al. [2020] C. Evers, H. W. Löllmann, H. Mellmann, A. Schmidt, H. Barfuss,
    P. A. Naylor, 和 W. Kellermann. LOCATA 挑战：声源定位和跟踪。*IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 28:1620–1643, 2020年。
- en: Fahim et al. [2020] A. Fahim, P. N. Samarasinghe, and T. D. Abhayapala. Multi-source
    DoA estimation through pattern recognition of the modal coherence of a reverberant
    soundfield. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 28:605–618, 2020.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fahim et al. [2020] A. Fahim, P. N. Samarasinghe, 和 T. D. Abhayapala. 通过模式识别的多源方向估计。*IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 28:605–618, 2020年。
- en: Falong et al. [1993] L. Falong, J. Hongbing, and Z. Xiaopeng. The ML bearing
    estimation by using neural networks. *J. Electronics (China)*, 10(1):1–8, 1993.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falong et al. [1993] L. Falong, J. Hongbing, 和 Z. Xiaopeng. 使用神经网络的最大似然轴承估计。*J.
    Electronics (China)*, 10(1):1–8, 1993年。
- en: Fernandez-Grande et al. [2021] E. Fernandez-Grande, M. J. Bianco, S. Gannot,
    and P. Gerstoft. DTU three-channel room impulse response dataset for direction
    of arrival estimation 2020, 2021. URL [https://dx.doi.org/10.21227/c5cn-jv76](https://dx.doi.org/10.21227/c5cn-jv76).
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandez-Grande et al. [2021] E. Fernandez-Grande, M. J. Bianco, S. Gannot,
    和 P. Gerstoft. DTU 三通道房间脉冲响应数据集，用于到达方向估计 2020，2021年。网址 [https://dx.doi.org/10.21227/c5cn-jv76](https://dx.doi.org/10.21227/c5cn-jv76)。
- en: Fortunati et al. [2014] S. Fortunati, R. Grasso, F. Gini, M. S. Greco, and K. LePage.
    Single-snapshot DOA estimation by using compressed sensing. *EURASIP J. Advances
    Signal Process.*, 2014(1):1–17, 2014.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortunati et al. [2014] S. Fortunati, R. Grasso, F. Gini, M. S. Greco, 和 K. LePage.
    使用压缩感知的单快照方向估计。*EURASIP J. Advances Signal Process.*, 2014(1):1–17, 2014年。
- en: Foucart and Rauhut [2013] S. Foucart and H. Rauhut. An invitation to compressive
    sensing. In *A mathematical introduction to compressive sensing*, pages 1–39\.
    Springer, 2013.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foucart and Rauhut [2013] S. Foucart 和 H. Rauhut. 压缩感知入门。在 *A mathematical introduction
    to compressive sensing*，第1–39页。Springer，2013年。
- en: Francombe [2017] J. Francombe. IoSR listening room multichannel BRIR dataset,
    2017. University of Surrey.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Francombe [2017] J. Francombe. IoSR 听音室多通道 BRIR 数据集，2017年。萨里大学。
- en: Gannot et al. [2017] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov.
    A consolidated perspective on multimicrophone speech enhancement and source separation.
    *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 25(4):692–730, 2017.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gannot et al. [2017] S. Gannot, E. Vincent, S. Markovich-Golan, 和 A. Ozerov.
    对多麦克风语音增强和源分离的综合观点。*IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 25(4):692–730,
    2017年。
- en: Gannot et al. [2019] S. Gannot, M. Haardt, W. Kellermann, and P. Willett. Introduction
    to the issue on acoustic source localization and tracking in dynamic real-life
    scenes. *IEEE J. Sel. Topics Signal Process.*, 13(1):3–7, 2019.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gannot et al. [2019] S. Gannot, M. Haardt, W. Kellermann, 和 P. Willett. 引入动态现实场景中声源的定位和跟踪。
    *IEEE信号处理专题杂志*, 13(1):3–7, 2019年.
- en: Garofolo et al. [1993a] J. Garofolo, D. Graff, D. Paul, and D. Pallett. CSR-I
    (WSJ0) Sennheiser LDC93S6B. *Linguistic Data Consortium, Philadelphia,*, 1993a.
    URL [https://catalog.ldc.upenn.edu/LDC93S6B](https://catalog.ldc.upenn.edu/LDC93S6B).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garofolo et al. [1993a] J. Garofolo, D. Graff, D. Paul, 和 D. Pallett. CSR-I
    (WSJ0) Sennheiser LDC93S6B.*语言数据联盟, 费城*, 1993a. URL [https://catalog.ldc.upenn.edu/LDC93S6B](https://catalog.ldc.upenn.edu/LDC93S6B).
- en: Garofolo et al. [1993b] J. S. Garofolo, L. Lamel, W. M. Fisher, J. G. Fiscus,
    D. S. Pallett, N. L. Dahlgren, and V. Zue. TIMIT Acoustic-Phonetic Continuous
    Speech Corpus. *Linguistic Data Consortium, Philadelphia,*, 1993b. URL [https://catalog.ldc.upenn.edu/LDC93s1](https://catalog.ldc.upenn.edu/LDC93s1).
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garofolo et al. [1993b] J. S. Garofolo, L. Lamel, W. M. Fisher, J. G. Fiscus,
    D. S. Pallett, N. L. Dahlgren, 和 V. Zue. TIMIT声学语音语料库. *语言数据联盟, 费城*, 1993b. URL
    [https://catalog.ldc.upenn.edu/LDC93s1](https://catalog.ldc.upenn.edu/LDC93s1).
- en: Gelderblom et al. [2021] F. B. Gelderblom, Y. Liu, J. Kvam, and T. A. Myrvoll.
    Synthetic data for DNN-based DoA estimation of indoor speech. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, Toronto, Canada (virtual conference),
    2021.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gelderblom et al. [2021] F. B. Gelderblom, Y. Liu, J. Kvam, 和 T. A. Myrvoll.
    室内语音的DNN-DOA估计的合成数据。 In *Proc. IEEE Int. Conf. 声学、语音、信号处理(ICASSP)*, 多伦多, 加拿大 (虚拟会议),
    2021年.
- en: Gerstoft et al. [2016] P. Gerstoft, C. F. Mecklenbräuker, A. Xenaki, and S. Nannuru.
    Multisnapshot sparse Bayesian learning for DOA. *IEEE Signal Process. Lett.*,
    23(10):1469–1473, 2016.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerstoft et al. [2016] P. Gerstoft, C. F. Mecklenbräuker, A. Xenaki, 和 S. Nannuru.
    DOA的多快照稀疏贝叶斯学习。 *IEEE信号处理信件*, 23(10):1469–1473, 2016.
- en: Gerstoft et al. [2018] P. Gerstoft, C. F. Mecklenbräuker, W. Seong, and M. Bianco.
    Introduction to compressive sensing in acoustics. *J. Acoust. Soc. Am.*, 143(6):3731–3736,
    2018.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerstoft et al. [2018] P. Gerstoft, C. F. Mecklenbräuker, W. Seong, 和 M. Bianco.
    引入声学压缩感知。 *声学学会杂志*, 143(6):3731–3736, 2018.
- en: Gerzon [1992] M. A. Gerzon. General metatheory of auditory localisation. In
    *Proc. Audio Engin. Soc. (AES) Conv.*, Vienna, Austria, march 1992.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerzon [1992] M. A. Gerzon. 声学定位的通用元理论. In *Proc. 音频工程协会（AES）大会*, 奥地利维也纳, 1992年3月.
- en: 'Girin et al. [2021] L. Girin, S. Leglaive, X. Bie, J. Diard, T. Hueber, and
    X. Alameda-Pineda. Dynamical variational autoencoders: A comprehensive review.
    *Foundations Trends Mach. Learn.*, 15(1–2):1–175, 2021.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Girin et al. [2021] L. Girin, S. Leglaive, X. Bie, J. Diard, T. Hueber, 和 X. Alameda-Pineda.
    动态变分自动编码器: 详细评论。 *机器学习基础和趋势*, 15(1–2):1–175, 2021年。'
- en: Gonçalves Pinto et al. [2021] W. Gonçalves Pinto, M. Bauerheim, and H. Parisot-Dupuis.
    Deconvoluting acoustic beamforming maps with a deep neural network. In *Proc.
    Inter-Noise Conf.*, pages 5397–5408, virtual conference, Aug. 2021.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonçalves Pinto et al. [2021] W. Gonçalves Pinto, M. Bauerheim, 和 H. Parisot-Dupuis.
    利用深度神经网络解卷积声学波束成像图。 In *Proc. 降噪大会*, 5397–5408页, 虚拟会议, 2021年8月.
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Nets. In *Proc.
    Advances Neural Inform. Process. Syst. (NIPS)*, Montréal, Canada, 2014.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, 和 Y. Bengio. 生成对抗网络。 In *Proc. NIPS Advances Neural Inform.
    Process. Syst.*, 加拿大蒙特利尔, 2014年.
- en: Goodfellow et al. [2016] I. Goodfellow, Y. Bengio, and A. Courville. *Deep Learning*.
    MIT Press, 2016.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2016] I. Goodfellow, Y. Bengio, 和 A. Courville. *深度学习*. MIT出版社,
    2016.
- en: Goryn and Kaveh [1988] D. Goryn and M. Kaveh. Neural networks for narrowband
    and wideband direction finding. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 2164–2167, New-York, NY, 1988.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goryn and Kaveh [1988] D. Goryn 和 M. Kaveh. 窄带和宽带定向导航的神经网络。 In *Proc. IEEE Int.
    Conf. 声学、语音、信号处理(ICASSP)*, 纽约, 纽约州, 1988年.
- en: Grondin et al. [2019] F. Grondin, I. Sobieraj, M. Plumbley, and J. Glass. Sound
    event localization and detection using CRNN on pairs of microphones. In *Proc.
    Detection and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*,
    pages 84–88, New York University, NY, USA, 2019.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grondin et al. [2019] F. Grondin, I. Sobieraj, M. Plumbley, 和 J. Glass. 使用CRNN对麦克风对定位和检测声音事件。
    In *Proc. 声学场景和事件检测分类研讨会(DCASE Workshop)*, 84–88页, 美国纽约大学, 纽约, 2019年.
- en: Grumiaux et al. [2020] P.-A. Grumiaux, S. Kitic, L. Girin, and A. Guerin. High-resolution
    speaker counting in reverberant rooms using CRNN with Ambisonics features. In
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, Amsterdam, The Netherlands (virtual
    conference), 2020.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grumiaux et al. [2020] P.-A. Grumiaux, S. Kitic, L. Girin, 和 A. Guerin. 使用 CRNN
    和 Ambisonics 特征在混响房间中进行高分辨率扬声器计数。见 *Proc. Europ. Signal Process. Conf. (EUSIPCO)*,
    Amsterdam, 荷兰 (虚拟会议), 2020。
- en: Grumiaux et al. [2021a] P.-A. Grumiaux, S. Kitic, L. Girin, and A. Guérin. Improved
    feature extraction for CRNN-based multiple sound source localization. In *Proc.
    Europ. Signal Process. Conf. (EUSIPCO)*, Dublin, Ireland (virtual conference),
    2021a.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grumiaux et al. [2021a] P.-A. Grumiaux, S. Kitic, L. Girin, 和 A. Guérin. 改进的特征提取用于基于
    CRNN 的多声源定位。见 *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, Dublin, 爱尔兰 (虚拟会议),
    2021a。
- en: 'Grumiaux et al. [2021b] P.-A. Grumiaux, S. Kitic, P. Srivastava, L. Girin,
    and A. Guérin. SALADnet: Self-attentive multisource localization in the Ambisonics
    domain. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    New Paltz, NY (virtual conference), 2021b.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grumiaux et al. [2021b] P.-A. Grumiaux, S. Kitic, P. Srivastava, L. Girin,
    和 A. Guérin. SALADnet: 自注意力多源定位在 Ambisonics 领域的应用。见 *Proc. IEEE Workshop Appl.
    Signal Process. Audio Acoust. (WASPAA)*, New Paltz, NY (虚拟会议), 2021b。'
- en: 'Guirguis et al. [2020] K. Guirguis, C. Schorn, A. Guntoro, S. Abdulatif, and
    B. Yang. SELD-TCN: sound event localization & detection via temporal convolutional
    networks. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 16–20, Amsterdam,
    The Netherlands (virtual conference), 2020.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guirguis et al. [2020] K. Guirguis, C. Schorn, A. Guntoro, S. Abdulatif, 和
    B. Yang. SELD-TCN: 通过时间卷积网络进行声音事件定位与检测。见 *Proc. Europ. Signal Process. Conf. (EUSIPCO)*,
    页 16–20, Amsterdam, 荷兰 (虚拟会议), 2020。'
- en: 'Guizzo et al. [2021] E. Guizzo, R. F. Gramaccioni, S. Jamili, C. Marinoni,
    E. Massaro, C. Medaglia, G. Nachira, L. Nucciarelli, L. Paglialunga, M. Pennese,
    et al. L3DAS21 Challenge: Machine Learning for 3D Audio Signal Processing, 2021.
    arXiv:2104.05499.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guizzo et al. [2021] E. Guizzo, R. F. Gramaccioni, S. Jamili, C. Marinoni,
    E. Massaro, C. Medaglia, G. Nachira, L. Nucciarelli, L. Paglialunga, M. Pennese,
    等. L3DAS21 挑战: 用于 3D 音频信号处理的机器学习, 2021. arXiv:2104.05499。'
- en: 'Gulati et al. [2020] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu,
    W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: convolution-augmented
    Transformer for speech recognition. In *Proc. Interspeech Conf.*, pages 5036–5040,
    Shanghai, China (virtual conference), Oct. 2020.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gulati et al. [2020] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J.
    Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, 和 R. Pang. Conformer: 卷积增强的 Transformer
    用于语音识别。见 *Proc. Interspeech Conf.*, 页 5036–5040, 上海, 中国 (虚拟会议), 2020年10月。'
- en: Habets [2006] E. A. P. Habets. Room impulse response generator. Technical report,
    Technische Universiteit Eindhoven, 2006.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Habets [2006] E. A. P. Habets. 房间脉冲响应生成器。技术报告，Eindhoven 理工大学，2006年。
- en: Habets [2022] E. A. P. Habets. Signal generator, 2022. URL [https://github.com/ehabets/Signal-Generator/](https://github.com/ehabets/Signal-Generator/).
    (Last viewed March 31, 2022).
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Habets [2022] E. A. P. Habets. 信号生成器, 2022. URL [https://github.com/ehabets/Signal-Generator/](https://github.com/ehabets/Signal-Generator/).
    (最后查看时间 2022年3月31日)。
- en: Hadad et al. [2014] E. Hadad, F. Heese, P. Vary, and S. Gannot. Multichannel
    audio database in various acoustic environments. In *Proc. IEEE Int. Workshop
    Acoustic Signal Enhanc. (IWAENC)*, pages 313–317, Antibes, France, Sept. 2014.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadad et al. [2014] E. Hadad, F. Heese, P. Vary, 和 S. Gannot. 不同声学环境中的多通道音频数据库。见
    *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*, 页 313–317, Antibes,
    法国, 2014年9月。
- en: Hahmann et al. [2021a] M. Hahmann, S. Verburg, and E. Fernandez-Grande. Acoustic
    frequency responses of an empty cuboid room. 2021a. URL [https://data.dtu.dk/articles/dataset/Acoustic_frequency_responses_of_an_empty_cuboid_room/13315289](https://data.dtu.dk/articles/dataset/Acoustic_frequency_responses_of_an_empty_cuboid_room/13315289).
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahmann et al. [2021a] M. Hahmann, S. Verburg, 和 E. Fernandez-Grande. 空的立方体房间的声学频率响应。2021a.
    URL [https://data.dtu.dk/articles/dataset/Acoustic_frequency_responses_of_an_empty_cuboid_room/13315289](https://data.dtu.dk/articles/dataset/Acoustic_frequency_responses_of_an_empty_cuboid_room/13315289)。
- en: Hahmann et al. [2021b] M. Hahmann, S. A. Verburg, and E. Fernandez-Grande. Spatial
    reconstruction of sound fields using local and data-driven functions. *J. Acoust.
    Soc. Am.*, 150(6):4417–4428, 2021b.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahmann et al. [2021b] M. Hahmann, S. A. Verburg, 和 E. Fernandez-Grande. 使用局部和数据驱动函数的声音场空间重建。*J.
    Acoust. Soc. Am.*, 150(6):4417–4428, 2021b。
- en: Hammer et al. [2021] H. Hammer, S. E. Chazan, J. Goldberger, and S. Gannot.
    Dynamically localizing multiple speakers based on the time-frequency domain. *EURASIP
    J. Audio, Speech, Music Process.*, 2021(1):1–10, 2021.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hammer et al. [2021] H. Hammer, S. E. Chazan, J. Goldberger, 和 S. Gannot. 基于时间频率域动态定位多个扬声器。*EURASIP
    J. Audio, Speech, Music Process.*, 2021(1):1–10, 2021。
- en: Hao et al. [2020] Y. Hao, A. Küçük, A. Ganguly, and I. M. S. Panahi. Spectral
    flux-based convolutional neural network architecture for speech source localization
    and its real-time implementation. *IEEE Access*, 8:197047–197058, 2020.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2020] Y. Hao, A. Küçük, A. Ganguly, 和 I. M. S. Panahi. 基于光谱流量的卷积神经网络架构用于语音源定位及其实时实现。*IEEE
    Access*，8:197047–197058，2020年。
- en: He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
    for image recognition. In *Proc. IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*,
    pages 770–778, Las Vegas, NV, 2016.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2016] K. He, X. Zhang, S. Ren, 和 J. Sun. 用于图像识别的深度残差学习。见于*Proc. IEEE
    Conf. Computer Vision Pattern Recogn. (CVPR)*，页面770–778，美国拉斯维加斯，2016年。
- en: He et al. [2018a] W. He, P. Motlicek, and J.-M. Odobez. Deep neural networks
    for multiple speaker detection and localization. In *IEEE Int. Conf. Robotics
    Autom. (ICRA)*, pages 74–79, Brisbane, Australia, 2018a.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2018a] W. He, P. Motlicek, 和 J.-M. Odobez. 用于多讲者检测和定位的深度神经网络。见于*IEEE
    Int. Conf. Robotics Autom. (ICRA)*，页面74–79，澳大利亚布里斯班，2018年。
- en: He et al. [2018b] W. He, P. Motlicek, and J.-M. Odobez. Joint localization and
    classification of multiple sound sources using a multi-task neural network. In
    *Proc. Interspeech Conf.*, pages 312–316, Hyderabad, India, 2018b.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2018b] W. He, P. Motlicek, 和 J.-M. Odobez. 使用多任务神经网络进行多声源的联合定位和分类。见于*Proc.
    Interspeech Conf.*，页面312–316，印度海得拉巴，2018年。
- en: He et al. [2019a] W. He, P. Motlicek, and J.-M. Odobez. Adaptation of multiple
    sound source localization neural networks with weak supervision and domain-adversarial
    training. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 770–774, Brighton, UK, May 2019a.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2019a] W. He, P. Motlicek, 和 J.-M. Odobez. 通过弱监督和领域对抗训练来适应多声源定位神经网络。见于*Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，页面770–774，英国布莱顿，2019年5月。
- en: He et al. [2021a] W. He, P. Motlicek, and J.-M. Odobez. Neural network adaptation
    and data augmentation for multi-speaker direction-of-arrival estimation. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 29:1303–1317, 2021a.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2021a] W. He, P. Motlicek, 和 J.-M. Odobez. 神经网络适应和数据增强用于多讲者到达方向估计。*IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*，29:1303–1317，2021年。
- en: 'He et al. [2021b] Y. He, N. Trigoni, and A. Markham. SoundDet: polyphonic moving
    sound event detection and localization from raw waveform. In *Proc. Int. Conf.
    Mach. Learn. (ICML)*, virtual conference, June 2021b.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2021b] Y. He, N. Trigoni, 和 A. Markham. SoundDet：从原始波形中进行多声部移动声音事件检测和定位。见于*Proc.
    Int. Conf. Mach. Learn. (ICML)*，虚拟会议，2021年6月。
- en: 'He et al. [2019b] Z. He, J. Li, D. Liu, H. He, and D. Barber. Tracking by animation:
    Unsupervised learning of multi-object attentive trackers. In *Proc. IEEE Conf.
    Computer Vision Pattern Recogn. (CVPR)*, pages 1318–1327, Long Beach, CA, 2019b.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2019b] Z. He, J. Li, D. Liu, H. He, 和 D. Barber. 通过动画进行跟踪：多目标关注跟踪器的无监督学习。见于*Proc.
    IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*，页面1318–1327，美国长滩，2019年。
- en: Heymann et al. [2016] J. Heymann, L. Drude, and R. Haeb-Umbach. Neural network
    based spectral mask estimation for acoustic beamforming. In *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*, Shanghai, China, 2016.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heymann et al. [2016] J. Heymann, L. Drude, 和 R. Haeb-Umbach. 基于神经网络的光谱掩蔽估计用于声学波束形成。见于*Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，中国上海，2016年。
- en: 'Heymann et al. [2017] J. Heymann, L. Drude, C. Boeddeker, P. Hanebrink, and
    R. Haeb-Umbach. Beamnet: End-to-end training of a beamformer-supported multi-channel
    ASR system. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    New Orleans, LA, 2017.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heymann et al. [2017] J. Heymann, L. Drude, C. Boeddeker, P. Hanebrink, 和 R.
    Haeb-Umbach. Beamnet：支持波束形成的多通道自动语音识别系统的端到端训练。见于*Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*，美国新奥尔良，2017年。
- en: Hickling et al. [1993] R. Hickling, W. Wei, and R. Raspet. Finding the direction
    of a sound source using a vector sound-intensity probe. *J. Acoust. Soc. Am.*,
    94(4):2408–2412, 1993.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hickling et al. [1993] R. Hickling, W. Wei, 和 R. Raspet. 使用矢量声强探头寻找声源的方向。*J.
    Acoust. Soc. Am.*，94(4):2408–2412，1993年。
- en: Higuchi et al. [2017] T. Higuchi, K. Kinoshita, M. Delcroix, K. Zmolkova, and
    T. Nakatani. Deep clustering-based beamforming for separation with unknown number
    of sources. In *Proc. Interspeech Conf.*, Stockholm, Sweden, 2017.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Higuchi et al. [2017] T. Higuchi, K. Kinoshita, M. Delcroix, K. Zmolkova, 和
    T. Nakatani. 基于深度聚类的波束形成用于未知数量声源的分离。见于*Proc. Interspeech Conf.*，瑞典斯德哥尔摩，2017年。
- en: Hirvonen [2015] T. Hirvonen. Classification of spatial audio location and content
    using convolutional neural networks. In *Audio Eng. Soc. Conv.*, Warsaw, Poland,
    2015.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirvonen [2015] T. Hirvonen. 使用卷积神经网络进行空间音频位置和内容的分类。见于*Audio Eng. Soc. Conv.*，波兰华沙，2015年。
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term
    memory. *Neural Comp.*, 9(8):1735–1780, Nov. 1997.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] S. Hochreiter 和 J. Schmidhuber. 长短期记忆。 *Neural
    Comp.*, 9(8):1735–1780, 1997年11月。
- en: Hogg et al. [2021] A. O. Hogg, V. W. Neo, S. Weiss, C. Evers, and P. A. Naylor.
    A polynomial eigenvalue decomposition MUSIC approach for broadband sound source
    localization. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    New Paltz, NY (virtual conference), 2021.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hogg 等人 [2021] A. O. Hogg, V. W. Neo, S. Weiss, C. Evers, 和 P. A. Naylor. 一种多项式特征值分解
    MUSIC 方法用于宽带声源定位。在 *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*，美国纽约州新帕尔茨（虚拟会议），2021年。
- en: Hu et al. [2020] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu. Squeeze-and-excitation
    networks. *IEEE Trans. Pattern Anal. Mach. Intell.*, 42(8):2011–2023, Aug. 2020.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2020] J. Hu, L. Shen, S. Albanie, G. Sun, 和 E. Wu. 挤压与激励网络。 *IEEE Trans.
    Pattern Anal. Mach. Intell.*, 42(8):2011–2023, 2020年8月。
- en: 'Huang and Perez [2021] D. Huang and R. Perez. SSELDNET: A fully end-to-end
    sample-level framework for sound event localization and detection. Technical report,
    November 2021. DCASE 2021 Challenge.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Perez [2021] D. Huang 和 R. Perez. SSELDNET：一个完全端到端的样本级框架，用于声音事件定位和检测。技术报告，2021年11月。DCASE
    2021挑战赛。
- en: Huang et al. [2018] Y. Huang, X. Wu, and T. Qu. DNN-based sound source localization
    method with microphone array. In *Proc. Int. Conf. Inform., Electron. Comm. Eng.
    (IECE)*, Beijing, China, Dec. 2018.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2018] Y. Huang, X. Wu, 和 T. Qu. 基于深度神经网络的麦克风阵列声源定位方法。在 *Proc. Int.
    Conf. Inform., Electron. Comm. Eng. (IECE)*，中国北京，2018年12月。
- en: Huang et al. [2019] Y. Huang, X. Wu, and T. Qu. A time-domain end-to-end method
    for sound source localization using multi-task learning. In *Proc. IEEE Int. Conf.
    Inform. Comm. Signal Process. (ICSP)*, pages 52–56, Weihai, China, Sept. 2019.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2019] Y. Huang, X. Wu, 和 T. Qu. 一种基于时间域的端到端声源定位方法，利用多任务学习。在 *Proc.
    IEEE Int. Conf. Inform. Comm. Signal Process. (ICSP)*，第52–56页，中国威海，2019年9月。
- en: Huang et al. [2020] Y. Huang, X. Wu, and T. Qu. A time-domain unsupervised learning
    based sound source localization method. In *Int. Conf. Inform. Comm. Signal Process.*,
    pages 26–32, Shanghai, China, Sept. 2020.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2020] Y. Huang, X. Wu, 和 T. Qu. 基于时间域的无监督学习声源定位方法。在 *Int. Conf. Inform.
    Comm. Signal Process.*，第26–32页，中国上海，2020年9月。
- en: Hübner et al. [2021] F. Hübner, W. Mack, and E. A. P. Habets. Efficient training
    data generation for phase-based DoA estimation. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, Toronto, Canada (virtual conference), 2021.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hübner 等人 [2021] F. Hübner, W. Mack, 和 E. A. P. Habets. 基于相位的到达角估计的高效训练数据生成。在
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，加拿大多伦多（虚拟会议），2021年。
- en: Jacobsen and Juhl [2013] F. Jacobsen and P. M. Juhl. *Fundamentals of General
    Linear Acoustics*. John Wiley & Sons, 2013.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacobsen 和 Juhl [2013] F. Jacobsen 和 P. M. Juhl. *普通线性声学基础*。John Wiley & Sons，2013年。
- en: 'Jarrett et al. [2012] D. Jarrett, E. Habets, M. Thomas, and P. Naylor. Rigid
    sphere room impulse response simulation: Algorithm and applications. *J. Acoust.
    Soc. Am.*, 132(3):1462–1472, 2012.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jarrett 等人 [2012] D. Jarrett, E. Habets, M. Thomas, 和 P. Naylor. 硬球房间脉冲响应模拟：算法与应用。
    *J. Acoust. Soc. Am.*, 132(3):1462–1472, 2012年。
- en: Jarrett et al. [2010] D. P. Jarrett, E. A. Habets, and P. A. Naylor. 3D source
    localization in the spherical harmonic domain using a pseudointensity vector.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 442–446, Aalborg, Denmark,
    2010.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jarrett 等人 [2010] D. P. Jarrett, E. A. Habets, 和 P. A. Naylor. 使用伪强度向量在球谐域中的3D源定位。在
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，第442–446页，丹麦奥尔堡，2010年。
- en: Jarrett et al. [2017] D. P. Jarrett, E. A. Habets, and P. A. Naylor. *Theory
    and Applications of Spherical Microphone Array Processing*. Springer, 2017.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jarrett 等人 [2017] D. P. Jarrett, E. A. Habets, 和 P. A. Naylor. *球形麦克风阵列处理的理论与应用*。Springer，2017年。
- en: 'Jenrungrot et al. [2020] T. Jenrungrot, V. Jayaram, S. Seitz, and I. Kemelmacher-Shlizerman.
    The cone of silence: speech separation by localization, 2020. arXiv:2010.06007.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenrungrot 等人 [2020] T. Jenrungrot, V. Jayaram, S. Seitz, 和 I. Kemelmacher-Shlizerman.
    沉默的圆锥：通过定位进行语音分离，2020年。arXiv:2010.06007。
- en: Jha and Durrani [1989] S. Jha and T. Durrani. Bearing estimation using neural
    optimisation methods. In *Proc. IEE Int. Conf. Artif. Neural Networks*, pages
    129–133, London, UK, 1989.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jha 和 Durrani [1989] S. Jha 和 T. Durrani. 使用神经优化方法进行方位估计。在 *Proc. IEE Int. Conf.
    Artif. Neural Networks*，第129–133页，英国伦敦，1989年。
- en: Jha and Durrani [1991] S. Jha and T. Durrani. Direction of arrival estimation
    using artificial neural networks. *IEEE Trans. Systems, Man, Cybern.*, 21(5):1192–1201,
    1991.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jha 和 Durrani [1991] S. Jha 和 T. Durrani. 使用人工神经网络进行到达方向估计。 *IEEE Trans. Systems,
    Man, Cybern.*, 21(5):1192–1201, 1991年。
- en: Jha et al. [1988] S. Jha, R. Chapman, and T. Durrani. Bearing estimation using
    neural networks. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 2156–2159, New-York, NY, 1988.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jha 等 [1988] S. Jha, R. Chapman 和 T. Durrani. 使用神经网络的方向估计。见 *IEEE 国际声学、语音和信号处理会议
    (ICASSP)*，第2156–2159页，美国纽约，1988年。
- en: Kapka and Lewandowski [2019] S. Kapka and M. Lewandowski. Sound source detection,
    localization and classification using consecutive ensemble of CRNN models. In
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*, New York, NY, 2019.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapka 和 Lewandowski [2019] S. Kapka 和 M. Lewandowski. 使用连续 CRNN 模型集成的声音源检测、定位和分类。见
    *声音场景和事件检测与分类研讨会 (DCASE Workshop)*，美国纽约，2019年。
- en: Karthik et al. [2020] S. Karthik, A. Prabhu, and V. Gandhi. Simple unsupervised
    multi-object tracking, 2020. arXiv:2006.02609.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karthik 等 [2020] S. Karthik, A. Prabhu 和 V. Gandhi. 简单的无监督多目标跟踪，2020年。arXiv:2006.02609。
- en: Kim and Hahn [2018] J. Kim and M. Hahn. Voice activity detection using an adaptive
    context attention model. *IEEE Signal Process. Lett.*, 25(8):1181–1185, Aug. 2018.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Hahn [2018] J. Kim 和 M. Hahn. 使用自适应上下文注意力模型的语音活动检测。*IEEE 信号处理快报*，25(8):1181–1185，2018年8月。
- en: Kim [2014] Y. Kim. Convolutional neural networks for sentence classification,
    2014. arXiv:1408.5882.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim [2014] Y. Kim. 用于句子分类的卷积神经网络，2014年。arXiv:1408.5882。
- en: Kim and Ling [2011] Y. Kim and H. Ling. Direction of arrival estimation of humans
    with a small sensor array using an artificial neural network. *Prog. Electromagn.
    Research*, 27:127–149, 2011.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Ling [2011] Y. Kim 和 H. Ling. 使用人工神经网络的小型传感器阵列的人体到达方向估计。*电磁学研究进展*，27:127–149，2011年。
- en: Kingma and Welling [2014] D. P. Kingma and M. Welling. Auto-encoding variational
    Bayes. In *Proc. Int. Conf. Learning Repres. (ICLR)*, Banff, Canada, 2014.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling [2014] D. P. Kingma 和 M. Welling. 自编码变分贝叶斯。见 *国际学习表示大会 (ICLR)*，加拿大班夫，2014年。
- en: 'Kitić and Guérin [2018] S. Kitić and A. Guérin. TRAMP: Tracking by a Real-time
    AMbisonic-based Particle filter. In *IEEE-AASP Challenge on Acoustic Source Localization
    and Tracking (LOCATA)*, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitić 和 Guérin [2018] S. Kitić 和 A. Guérin. TRAMP：基于实时 AMbisonic 的粒子滤波跟踪。见 *IEEE-AASP
    声源定位与跟踪挑战赛 (LOCATA)*，2018年。
- en: 'Kitić et al. [2014] S. Kitić, N. Bertin, and R. Gribonval. Hearing behind walls:
    localizing sources in the room next door with cosparsity. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 3087–3091, Florence, Italy,
    2014\. IEEE.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitić 等 [2014] S. Kitić, N. Bertin 和 R. Gribonval. 透过墙壁听声音：利用共稀疏性定位隔壁房间的源。见
    *IEEE 国际声学、语音和信号处理会议 (ICASSP)*，第3087–3091页，意大利佛罗伦萨，2014年。IEEE。
- en: Knapp and Carter [1976] C. Knapp and G. Carter. The generalized correlation
    method for estimation of time delay. *IEEE Trans. Acoust., Speech, Signal Process.*,
    24(4):320–327, Aug. 1976.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knapp 和 Carter [1976] C. Knapp 和 G. Carter. 时间延迟估计的广义相关方法。*IEEE 声学、语音和信号处理汇刊*，24(4):320–327，1976年8月。
- en: Komatsu et al. [2020] T. Komatsu, M. Togami, and T. Takahashi. Sound event localization
    and detection using convolutional recurrent neural networks and gated linear units.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 41–45, Amsterdam, The
    Netherlands (virtual conference), 2020.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komatsu 等 [2020] T. Komatsu, M. Togami 和 T. Takahashi. 使用卷积递归神经网络和门控线性单元的声音事件定位和检测。见
    *欧洲信号处理会议 (EUSIPCO)*，第41–45页，荷兰阿姆斯特丹（虚拟会议），2020年。
- en: Kong et al. [2019] Q. Kong, Y. Cao, T. Iqbal, W. Wang, and M. D. Plumbley. Cross-task
    learning for audio tagging, sound event detection and spatial localization. Technical
    report, 2019. DCASE 2019 Challenge.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等 [2019] Q. Kong, Y. Cao, T. Iqbal, W. Wang 和 M. D. Plumbley. 跨任务学习用于音频标记、声音事件检测和空间定位。技术报告，2019年。DCASE
    2019 挑战。
- en: Kounades-Bastian et al. [2017] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda,
    S. Gannot, and R. Horaud. An EM algorithm for joint source separation and diarisation
    of multichannel convolutive speech mixtures. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, pages 16–20, New Orleans, LA, 2017.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kounades-Bastian 等 [2017] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda,
    S. Gannot 和 R. Horaud. 用于多通道卷积语音混合源分离和分段的 EM 算法。见 *IEEE 国际声学、语音和信号处理会议 (ICASSP)*，第16–20页，美国路易斯安那州新奥尔良，2017年。
- en: Kouw and Loog [2019] W. M. Kouw and M. Loog. A review of domain adaptation without
    target labels. *IEEE transactions on pattern analysis and machine intelligence*,
    43(3):766–785, 2019.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kouw 和 Loog [2019] W. M. Kouw 和 M. Loog. 无目标标签的领域适应综述。*IEEE 模式分析与机器智能汇刊*，43(3):766–785，2019年。
- en: 'Koyama et al. [2021] S. Koyama, T. Nishida, K. Kimura, T. Abe, N. Ueno, and
    J. Brunnström. MeshRIR: A dataset of room impulse responses on meshed grid points
    for evaluating sound field analysis and synthesis methods. In *Proc. IEEE Workshop
    Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 1–5, New Paltz, NY (virtual
    conference), 2021.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koyama 等 [2021] S. Koyama, T. Nishida, K. Kimura, T. Abe, N. Ueno 和 J. Brunnström.
    MeshRIR：一个用于评估声场分析和合成方法的网格点房间冲激响应数据集。在 *Proc. IEEE Workshop Appl. Signal Process.
    Audio Acoust. (WASPAA)*，页码 1–5，美国纽约州新帕尔茨（虚拟会议），2021。
- en: Krause and Kowalczyk [2019] D. Krause and K. Kowalczyk. Arborescent neural network
    architectures for sound event detection and localization. Technical report, 2019.
    DCASE 2019 Challenge.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause 和 Kowalczyk [2019] D. Krause 和 K. Kowalczyk. 用于声音事件检测和定位的树状神经网络架构。技术报告，2019。DCASE
    2019 挑战。
- en: Krause et al. [2020a] D. Krause, A. Politis, and K. Kowalczyk. Comparison of
    convolution types in CNN-based feature extraction for sound source localization.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 820–824, Amsterdam, The
    Netherlands (virtual conference), 2020a.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause 等 [2020a] D. Krause, A. Politis 和 K. Kowalczyk. CNN 基于特征提取的卷积类型比较，用于声音源定位。在
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，页码 820–824，荷兰阿姆斯特丹（虚拟会议），2020a。
- en: Krause et al. [2020b] D. Krause, A. Politis, and K. Kowalczyk. Feature overview
    for joint modeling of sound event detection and localization using a microphone
    array. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 31–35, Amsterdam,
    The Netherlands (virtual conference), 2020b.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause 等 [2020b] D. Krause, A. Politis 和 K. Kowalczyk. 使用麦克风阵列的声音事件检测和定位的联合建模特征概述。在
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，页码 31–35，荷兰阿姆斯特丹（虚拟会议），2020b。
- en: Krause et al. [2021] D. Krause, A. Politis, and K. Kowalczyk. Data diversity
    for improving DNN-based localization of concurrent sound events. In *Proc. Europ.
    Signal Process. Conf. (EUSIPCO)*, pages 236–240, Dublin, Ireland (virtual conference),
    2021.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause 等 [2021] D. Krause, A. Politis 和 K. Kowalczyk. 改善基于 DNN 的并发声音事件定位的数据多样性。在
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，页码 236–240，爱尔兰都柏林（虚拟会议），2021。
- en: 'Kristoffersen et al. [2021] M. S. Kristoffersen, M. B. Møller, P. Martínez-Nuevo,
    and J. Østergaard. Deep sound field reconstruction in real rooms: Introducing
    the ISOBEL sound field dataset, 2021. arXiv:2102.06455.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kristoffersen 等 [2021] M. S. Kristoffersen, M. B. Møller, P. Martínez-Nuevo
    和 J. Østergaard. 真实房间中的深度声场重建：介绍 ISOBEL 声场数据集，2021。arXiv:2102.06455。
- en: Krizhevsky et al. [2017] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet
    classification with deep convolutional neural networks. *Comm. ACM*, 60(6):84–90,
    2017.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 [2017] A. Krizhevsky, I. Sutskever 和 G. E. Hinton. 使用深度卷积神经网络的
    ImageNet 分类。*Comm. ACM*，60(6):84–90，2017。
- en: Kujawski et al. [2019] A. Kujawski, G. Herold, and E. Sarradj. A deep learning
    method for grid-free localization and quantification of sound sources. *J. Acoust.
    Soc. Am.*, 146(3):EL225–EL231, Sept. 2019.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kujawski 等 [2019] A. Kujawski, G. Herold 和 E. Sarradj. 一种用于无网格定位和量化声音源的深度学习方法。*J.
    Acoust. Soc. Am.*，146(3):EL225–EL231，2019年9月。
- en: Kuttruff [2016] H. Kuttruff. *Room Acoustics*. CRC Press, 2016.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuttruff [2016] H. Kuttruff. *Room Acoustics*。CRC Press，2016。
- en: Küçük et al. [2019] A. Küçük, A. Ganguly, Y. Hao, and I. M. S. Panahi. Real-time
    convolutional neural network-based speech source localization on smartphone. *IEEE
    Access*, 7:169969–169978, 2019.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Küçük 等 [2019] A. Küçük, A. Ganguly, Y. Hao 和 I. M. S. Panahi. 基于卷积神经网络的智能手机实时语音源定位。*IEEE
    Access*，7:169969–169978，2019。
- en: Lamel et al. [1991] L. Lamel, J.-L. Gauvain, and M. Eskenazi. BREF, a large
    vocabulary spoken corpus for French. In *Proc. Europ. Conf. Speech Comm. Technol.
    (Eurospeech)*, pages 4–7, Genove, Italy, 1991.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lamel 等 [1991] L. Lamel, J.-L. Gauvain 和 M. Eskenazi. BREF，一个大型法语口语语料库。在 *Proc.
    Europ. Conf. Speech Comm. Technol. (Eurospeech)*，页码 4–7，意大利热那亚，1991。
- en: Landschoot and Xiang [2019] C. R. Landschoot and N. Xiang. Model-based Bayesian
    direction of arrival analysis for sound sources using a spherical microphone array.
    *J. Acoust. Soc. Am.*, 146(6):4936–4946, 2019.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landschoot 和 Xiang [2019] C. R. Landschoot 和 N. Xiang. 使用球形麦克风阵列的基于模型的贝叶斯到达方向分析。*J.
    Acoust. Soc. Am.*，146(6):4936–4946，2019。
- en: 'Lathoud et al. [2004] G. Lathoud, J.-M. Odobez, and D. Gatica-Perez. AV16.3:
    an audio-visual corpus for speaker localization and tracking. In *Proc. Int. Workshop
    Mach. Learn. Multimodal Interact.*, pages 182–195, Martigny, Switzerland, 2004.
    ISBN 978-3-540-24509-4 978-3-540-30568-2.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lathoud 等 [2004] G. Lathoud, J.-M. Odobez 和 D. Gatica-Perez. AV16.3：用于说话人定位和跟踪的视听语料库。在
    *Proc. Int. Workshop Mach. Learn. Multimodal Interact.*，页码 182–195，瑞士马蒂尼，2004。ISBN
    978-3-540-24509-4 978-3-540-30568-2。
- en: Laufer-Goldshtein et al. [2020] B. Laufer-Goldshtein, R. Talmon, and S. Gannot.
    Data-driven multi-microphone speaker localization on manifolds. *Found. and Trends
    in Signal Process.*, 14(1–2):1–161, 2020.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laufer-Goldshtein et al. [2020] B. Laufer-Goldshtein, R. Talmon, 和 S. Gannot.
    数据驱动的多麦克风扬声器定位方法。*Found. and Trends in Signal Process.*，14(1–2):1–161, 2020。
- en: Le Moing et al. [2020] G. Le Moing, P. Vinayavekhin, T. Inoue, J. Vongkulbhisal,
    A. Munawar, R. Tachibana, and D. J. Agravante. Learning multiple sound source
    2D localization. In *Proc. IEEE Int. Workshop Multimedia Signal Process. (MMSP)*,
    Tampere, Finland (virtual conference), 2020.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le Moing et al. [2020] G. Le Moing, P. Vinayavekhin, T. Inoue, J. Vongkulbhisal,
    A. Munawar, R. Tachibana, 和 D. J. Agravante. 学习多声源的二维定位。见于 *Proc. IEEE Int. Workshop
    Multimedia Signal Process. (MMSP)*，坦佩雷，芬兰（虚拟会议），2020。
- en: Le Moing et al. [2021] G. Le Moing, P. Vinayavekhin, D. J. Agravante, T. Inoue,
    J. Vongkulbhisal, A. Munawar, and R. Tachibana. Data-efficient framework for real-world
    multiple sound source 2D localization. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, Toronto, Canada (virtual conference), 2021.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le Moing et al. [2021] G. Le Moing, P. Vinayavekhin, D. J. Agravante, T. Inoue,
    J. Vongkulbhisal, A. Munawar, 和 R. Tachibana. 面向现实世界的多声源二维定位的数据高效框架。见于 *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，多伦多，加拿大（虚拟会议），2021。
- en: Lea et al. [2017] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager.
    Temporal convolutional networks for action segmentation and detection. In *Proc.
    IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*, pages 1003–1012, Honolulu,
    HI, July 2017.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lea et al. [2017] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, 和 G. D. Hager. 用于动作分割和检测的时间卷积网络。见于
    *Proc. IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*，第1003–1012页，檀香山，夏威夷，2017年7月。
- en: LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. *Nature*,
    521(7553):436–444, 2015.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. [2015] Y. LeCun, Y. Bengio, 和 G. Hinton. 深度学习。*Nature*，521(7553):436–444,
    2015。
- en: Lee et al. [2016] H. Lee, J. Cho, M. Kim, and H. Park. DNN-based feature enhancement
    using DoA-constrained ICA for robust speech recognition. *IEEE Signal Process.
    Lett.*, 23(8):1091–1095, Aug. 2016.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2016] H. Lee, J. Cho, M. Kim, 和 H. Park. 基于DNN的特征增强，使用DoA约束的ICA进行鲁棒语音识别。*IEEE
    Signal Process. Lett.*，23(8):1091–1095, 2016年8月。
- en: Lee et al. [2017] J. Lee, J. Park, K. L. Kim, and J. Nam. Sample-level deep
    convolutional neural networks for music auto-tagging using raw waveforms, 2017.
    arXiv:1703.01789.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2017] J. Lee, J. Park, K. L. Kim, 和 J. Nam. 基于样本级深度卷积神经网络的音乐自动标签技术，2017。arXiv:1703.01789。
- en: Lee et al. [2021a] S.-H. Lee, J.-W. Hwang, S.-B. Seo, and H.-M. Park. Sound
    event localization and detection using cross-modal attention and parameter sharing
    for DCASE2021 challenge. Technical report, November 2021a. DCASE 2021 Challenge.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2021a] S.-H. Lee, J.-W. Hwang, S.-B. Seo, 和 H.-M. Park. 使用跨模态注意力和参数共享进行声音事件定位和检测，以应对DCASE2021挑战。技术报告，2021年11月。DCASE
    2021挑战。
- en: Lee et al. [2021b] S. Y. Lee, J. Chang, and S. Lee. Deep learning-based method
    for multiple sound source localization with high resolution and accuracy. *Mech.
    Syst. Signal Process.*, 161:107959, 2021b.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2021b] S. Y. Lee, J. Chang, 和 S. Lee. 基于深度学习的多声源高分辨率和高精度定位方法。*Mech.
    Syst. Signal Process.*，161:107959, 2021b。
- en: Leglaive et al. [2019] S. Leglaive, L. Girin, and R. Horaud. Semi-supervised
    multichannel speech enhancement with variational autoencoders and non-negative
    matrix factorization. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, pages 101–105, Brighton, UK, 2019.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leglaive et al. [2019] S. Leglaive, L. Girin, 和 R. Horaud. 使用变分自编码器和非负矩阵分解的半监督多通道语音增强。见于
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，第101–105页，布赖顿，英国，2019。
- en: Lehmann and Johansson [2010] E. A. Lehmann and A. M. Johansson. Diffuse reverberation
    model for efficient image-source simulation of room impulse responses. *IEEE Trans.
    Audio, Speech, Lang. Process.*, 18(6):1429–1439, Aug. 2010.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehmann 和 Johansson [2010] E. A. Lehmann 和 A. M. Johansson. 用于高效图像源模拟的扩散混响模型。*IEEE
    Trans. Audio, Speech, Lang. Process.*，18(6):1429–1439, 2010年8月。
- en: Leung and Ren [2019] S. Leung and Y. Ren. Spectrum combination and convolutional
    recurrent neural networks for joint localization and detection of sound events.
    Technical report, June 2019. DCASE 2019 Challenge.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leung 和 Ren [2019] S. Leung 和 Y. Ren. 频谱组合和卷积递归神经网络用于声音事件的联合定位和检测。技术报告，2019年6月。DCASE
    2019挑战。
- en: Li et al. [2016a] B. Li, T. N. Sainath, R. J. Weiss, K. W. Wilson, and M. Bacchiani.
    Neural network adaptive beamforming for robust multichannel speech recognition.
    In *Proc. Interspeech Conf.*, San Francisco, CA, 2016a.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016a] B. Li, T. N. Sainath, R. J. Weiss, K. W. Wilson, 和 M. Bacchiani.
    用于鲁棒多通道语音识别的神经网络自适应波束形成。见于 *Proc. Interspeech Conf.*，旧金山，加州，2016a。
- en: Li et al. [2018] Q. Li, X. Zhang, and H. Li. Online direction of arrival estimation
    based on deep learning. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, pages 2616–2620, Calgary, Canada, Apr. 2018.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2018] Q. Li, X. Zhang, 和 H. Li. 基于深度学习的在线到达方向估计。发表于 *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*，第 2616–2620 页，加拿大卡尔加里，2018 年 4 月。
- en: Li et al. [2015] X. Li, L. Girin, R. Horaud, and S. Gannot. Estimation of relative
    transfer function in the presence of stationary noise based on segmental power
    spectral density matrix subtraction. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 320–324, Brisbane, Australia, 2015.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2015] X. Li, L. Girin, R. Horaud, 和 S. Gannot. 基于分段功率谱密度矩阵减法的静态噪声下相对传递函数估计。发表于
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，第 320–324 页，澳大利亚布里斯班，2015。
- en: Li et al. [2016b] X. Li, L. Girin, F. Badeig, and R. Horaud. Reverberant sound
    localization with a robot head based on direct-path relative transfer function.
    In *Proc. IEEE/RSJ Int. Conf. Intell. Robots Systems (IROS)*, pages 2819–2826,
    Daejeon, Korea, 2016b.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2016b] X. Li, L. Girin, F. Badeig, 和 R. Horaud. 基于直达路径相对传递函数的混响声定位。发表于
    *Proc. IEEE/RSJ Int. Conf. Intell. Robots Systems (IROS)*，第 2819–2826 页，韩国大田，2016b。
- en: Li et al. [2016c] X. Li, L. Girin, R. Horaud, and S. Gannot. Estimation of the
    direct-path relative transfer function for supervised sound source localization.
    *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 24(11):2171–2186, 2016c.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2016c] X. Li, L. Girin, R. Horaud, 和 S. Gannot. 监督下声音源定位的直达路径相对传递函数估计。*IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*，24(11):2171–2186，2016c。
- en: Li et al. [2016d] X. Li, R. Horaud, L. Girin, and S. Gannot. Voice activity
    detection based on statistical likelihood ratio with adaptive thresholding. In
    *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*, pages 1–5, Xi’an,
    China, 2016d.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2016d] X. Li, R. Horaud, L. Girin, 和 S. Gannot. 基于统计似然比和自适应阈值的语音活动检测。发表于
    *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*，第 1–5 页，中国西安，2016d。
- en: Li et al. [2017] X. Li, L. Girin, R. Horaud, and S. Gannot. Multiple-speaker
    localization based on direct-path features and likelihood maximization with spatial
    sparsity regularization. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 25(10):1997–2012,
    2017.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2017] X. Li, L. Girin, R. Horaud, 和 S. Gannot. 基于直达路径特征和具有空间稀疏性正则化的似然最大化的多扬声器定位。*IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*，25(10):1997–2012，2017。
- en: Liang and Zhou [2018] Y. Liang and Y. Zhou. Lstm multiple object tracker combining
    multiple cues. In *Proc. IEEE Int. Conf. Image Process. (ICIP)*, pages 2351–2355,
    Athens, Greece, 2018.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 和 Zhou [2018] Y. Liang 和 Y. Zhou. 结合多重线索的 LSTM 多目标跟踪器。发表于 *Proc. IEEE
    Int. Conf. Image Process. (ICIP)*，第 2351–2355 页，希腊雅典，2018。
- en: Lin et al. [2022] X. Lin, L. Girin, and X. Alameda-Pineda. Unsupervised multiple-object
    tracking with a dynamical variational autoencoder, 2022. arXiv:2202.09315.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2022] X. Lin, L. Girin, 和 X. Alameda-Pineda. 基于动态变分自编码器的无监督多目标跟踪，2022。arXiv:2202.09315。
- en: Lin and Wang [2019] Y. Lin and Z. Wang. A report on sound event localization
    and detection. Technical report, 2019. DCASE 2019 Challenge.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 和 Wang [2019] Y. Lin 和 Z. Wang. 关于声音事件定位和检测的报告。技术报告，2019。DCASE 2019 挑战。
- en: Liu et al. [2021] N. Liu, H. Chen, K. Songgong, and Y. Li. Deep learning assisted
    sound source localization using two orthogonal first-order differential microphone
    arrays. *J. Acoust. Soc. Am.*, 149(2):1069–1084, Feb. 2021.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2021] N. Liu, H. Chen, K. Songgong, 和 Y. Li. 使用两个正交一阶微分麦克风阵列的深度学习辅助声音源定位。*J.
    Acoust. Soc. Am.*，149(2):1069–1084，2021 年 2 月。
- en: Liu et al. [2012] Z.-M. Liu, Z.-T. Huang, and Y.-Y. Zhou. An efficient maximum
    likelihood method for direction-of-arrival estimation via sparse Bayesian learning.
    *IEEE Trans. Wireless Comm.*, 11(10):1–11, 2012.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2012] Z.-M. Liu, Z.-T. Huang, 和 Y.-Y. Zhou. 基于稀疏贝叶斯学习的到达方向估计的高效最大似然方法。*IEEE
    Trans. Wireless Comm.*，11(10):1–11，2012。
- en: Liu et al. [2018] Z.-M. Liu, C. Zhang, and P. S. Yu. Direction-of-arrival estimation
    based on deep neural networks with robustness to array imperfections. *IEEE Trans.
    Antennas Propag.*, 66(12):7315–7327, 2018.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2018] Z.-M. Liu, C. Zhang, 和 P. S. Yu. 基于深度神经网络且对阵列缺陷具有鲁棒性的到达方向估计。*IEEE
    Trans. Antennas Propag.*，66(12):7315–7327，2018。
- en: Lu [2019] Z. Lu. Sound event detection and localization based on CNN and LSTM.
    Technical report, 2019. DCASE 2019 Challenge.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu [2019] Z. Lu. 基于 CNN 和 LSTM 的声音事件检测和定位。技术报告，2019。DCASE 2019 挑战。
- en: 'Luiten et al. [2020] J. Luiten, I. E. Zulfikar, and B. Leibe. UnOVOST: Unsupervised
    offline video object segmentation and tracking. In *IEEE Winter Conf. Appl. Comput.
    Vis. (WACV)*, pages 1989–1998, Snowmass Village, CO, 2020.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2021] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, and T.-K. Kim.
    Multiple object tracking: A literature review. *Artif. Intell.*, 293:103448, 2021.
    ISSN 0004-3702.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo and Mesgarani [2019] Y. Luo and N. Mesgarani. Conv-TASnet: Surpassing ideal
    time–frequency magnitude masking for speech separation. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 27(8):1256–1266, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2020] Y. Luo, Z. Chen, N. Mesgarani, and T. Yoshioka. End-to-end
    microphone permutation and number invariant multi-channel speech separation. In
    *ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, pages 6394–6398\. IEEE, 2020.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2015] N. Ma, G. Brown, and T. May. Exploiting deep neural networks
    and head movements for binaural localisation of multiple speakers in reverberant
    conditions. In *Proc. Interspeech Conf.*, pages 160–164, Dresden, Germany, 2015.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma and Liu [2018] W. Ma and X. Liu. Phased microphone array for sound source
    localization with deep learning. *Aerospace Syst.*, 2(2):71–81, 2018.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mabande et al. [2011] E. Mabande, H. Sun, K. Kowalczyk, and W. Kellermann. Comparison
    of subspace-based and steered beamformer-based reflection localization methods.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 146–150, Barcelona, Spain,
    2011.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mack et al. [2020] W. Mack, U. Bharadwaj, S. Chakrabarty, and E. A. P. Habets.
    Signal-aware broadband DoA estimation using attention mechanisms. In *Proc. IEEE
    Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 4930–4934, Barcelona,
    Spain (virtual conference), May 2020.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mandel et al. [2009] M. I. Mandel, R. J. Weiss, and D. P. Ellis. Model-based
    expectation-maximization source separation and localization. *IEEE Trans. Audio,
    Speech, Lang. Process.*, 18(2):382–394, 2009.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markovich-Golan and Gannot [2015] S. Markovich-Golan and S. Gannot. Performance
    analysis of the covariance subtraction method for relative transfer function estimation
    and comparison to the covariance whitening method. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, Brisbane, Australia, 2015.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maruri et al. [2019] H. A. C. Maruri, P. L. Meyer, J. Huang, J. A. d. H. Ontiveros,
    and H. Lu. GCC-PHAT cross-correlation audio features for simultaneous sound event
    localization and detection (SELD) in multiple rooms. Technical report, 2019. DCASE
    2019 Challenge.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masuyama et al. [2020] Y. Masuyama, Y. Bando, K. Yatabe, Y. Sasaki, M. Onishi,
    and Y. Oikawa. Self-supervised neural audio-visual sound source localization via
    probabilistic spatial modeling. In *Proc. IEEE/RSJ Int. Conf. Intell. Robots Systems
    (IROS)*, pages 4848–4854, Las Vegas, NV, 2020.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May et al. [2011] T. May, S. Van De Par, and A. Kohlrausch. A probabilistic
    model for robust localization based on a binaural auditory front-end. *IEEE Trans.
    Audio, Speech, Lang. Process.*, 19(1):1–13, 2011.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: May 等 [2011] T. May, S. Van De Par 和 A. Kohlrausch. 基于双耳听觉前端的稳健定位的概率模型。*IEEE
    Trans. Audio, Speech, Lang. Process.*, 19(1):1–13, 2011。
- en: Maysenhölder [1993] W. Maysenhölder. The reactive intensity of general time-harmonic
    structure-borne sound fields. In *Proc. Int. Congress Intensity Techniques*, pages
    63–70, 1993.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maysenhölder [1993] W. Maysenhölder. 一般时间谐波结构传播声场的反应强度。在 *Proc. Int. Congress
    Intensity Techniques*，第63–70页，1993年。
- en: Mazzon et al. [2019] L. Mazzon, Y. Koizumi, M. Yasuda, and N. Harada. First
    order Ambisonics domain spatial augmentation for DNN-based direction of arrival
    estimation. In *Proc. Detection and Classification of Acoustic Scenes and Events
    Workshop (DCASE Workshop)*, New York, NY, 2019.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mazzon 等 [2019] L. Mazzon, Y. Koizumi, M. Yasuda 和 N. Harada. 基于 DNN 的到达方向估计的一级
    Ambisonics 域空间增强。在 *Proc. Detection and Classification of Acoustic Scenes and
    Events Workshop (DCASE Workshop)*，美国纽约，2019年。
- en: 'Meinhardt et al. [2021] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer.
    Trackformer: Multi-object tracking with transformers, 2021. arXiv:2101.02702.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Meinhardt 等 [2021] T. Meinhardt, A. Kirillov, L. Leal-Taixe 和 C. Feichtenhofer.
    Trackformer: 多目标跟踪与变换器，2021。arXiv:2101.02702。'
- en: Meng et al. [2017] Z. Meng, S. Watanabe, J. R. Hershey, and H. Erdogan. Deep
    long short-term memory adaptive beamforming networks for multichannel robust speech
    recognition. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    New Orleans, LA, 2017.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等 [2017] Z. Meng, S. Watanabe, J. R. Hershey 和 H. Erdogan. 用于多通道稳健语音识别的深度长短期记忆自适应波束形成网络。在
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，美国新奥尔良，2017年。
- en: 'Merimaa [2006] J. Merimaa. *Analysis, synthesis, and perception of spatial
    sound: binaural localization modeling and multichannel loudspeaker reproduction*.
    PhD thesis, Helsinki Univ. Technol., 2006.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merimaa [2006] J. Merimaa. *空间声音的分析、合成和感知：耳声定位建模和多声道扬声器重现*。博士论文，赫尔辛基理工大学，2006年。
- en: Nam et al. [2013] S. Nam, M. E. Davies, M. Elad, and R. Gribonval. The cosparse
    analysis model and algorithms. *Applied Computational Harmonic Anal.*, 34(1):30–56,
    2013.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nam 等 [2013] S. Nam, M. E. Davies, M. Elad 和 R. Gribonval. 稀疏分析模型和算法。*Applied
    Computational Harmonic Anal.*, 34(1):30–56, 2013。
- en: Nannuru et al. [2018] S. Nannuru, A. Koochakzadeh, K. L. Gemba, P. Pal, and
    P. Gerstoft. Sparse Bayesian learning for beamforming using sparse linear arrays.
    *J. Acoust. Soc. Am.*, 144(5):2719–2729, 2018.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nannuru 等 [2018] S. Nannuru, A. Koochakzadeh, K. L. Gemba, P. Pal 和 P. Gerstoft.
    使用稀疏线性阵列的稀疏贝叶斯学习进行波束形成。*J. Acoust. Soc. Am.*, 144(5):2719–2729, 2018。
- en: Naranjo-Alcazar et al. [2020] J. Naranjo-Alcazar, S. Perez-Castanos, J. Ferrandis,
    P. Zuccarello, and M. Cobos. Sound event localization and detection using squeeze-excitation
    residual CNNs. Technical report, June 2020. DCASE 2020 Challenge.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naranjo-Alcazar 等 [2020] J. Naranjo-Alcazar, S. Perez-Castanos, J. Ferrandis,
    P. Zuccarello 和 M. Cobos. 使用 squeeze-excitation 残差 CNNs 的声音事件定位和检测。技术报告，2020年6月。DCASE
    2020 挑战。
- en: Naranjo-Alcazar et al. [2021] J. Naranjo-Alcazar, S. Perez-Castanos, M. Cobos,
    F. J. Ferri, and P. Zuccarello. Sound event localisation and detection using squeeze-excitation
    residual CNNs. Technical report, November 2021. DCASE 2021 Challenge.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naranjo-Alcazar 等 [2021] J. Naranjo-Alcazar, S. Perez-Castanos, M. Cobos, F.
    J. Ferri 和 P. Zuccarello. 使用 squeeze-excitation 残差 CNNs 的声音事件定位和检测。技术报告，2021年11月。DCASE
    2021 挑战。
- en: Nehorai and Paldi [1994] A. Nehorai and E. Paldi. Acoustic vector-sensor array
    processing. *IEEE Trans. Signal Process.*, 42(9):2481–2491, 1994.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nehorai 和 Paldi [1994] A. Nehorai 和 E. Paldi. 声学矢量传感器阵列处理。*IEEE Trans. Signal
    Process.*, 42(9):2481–2491, 1994。
- en: Nguyen et al. [2018] Q. Nguyen, L. Girin, G. Bailly, F. Elisei, and D.-C. Nguyen.
    Autonomous sensorimotor learning for sound source localization by a humanoid robot.
    In *IEEE/RSJ IROS Workshop Crossmodal Learn. Intell. Robotics*, Madrid, Spain,
    2018.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等 [2018] Q. Nguyen, L. Girin, G. Bailly, F. Elisei 和 D.-C. Nguyen. 通过类人机器人进行声音源定位的自主感觉运动学习。在
    *IEEE/RSJ IROS Workshop Crossmodal Learn. Intell. Robotics*，西班牙马德里，2018年。
- en: Nguyen et al. [2020a] T. N. T. Nguyen, W.-S. Gan, R. Ranjan, and D. L. Jones.
    Robust source counting and DoA estimation using spatial pseudo-spectrum and convolutional
    neural network. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 28:2626–2637,
    2020a.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等 [2020a] T. N. T. Nguyen, W.-S. Gan, R. Ranjan 和 D. L. Jones. 使用空间伪谱和卷积神经网络进行稳健的源计数和方向估计。*IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 28:2626–2637, 2020a。
- en: Nguyen et al. [2020b] T. N. T. Nguyen, D. L. Jones, and W. S. Gan. Ensemble
    of sequence matching networks for dynamic sound event localization, detection,
    and tracking. In *Proc. Detection and Classification of Acoustic Scenes and Events
    Workshop (DCASE Workshop)*, Tokyo, Japan, November 2020b.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. [2020b] T. N. T. Nguyen, D. L. Jones, 和 W. S. Gan. 动态声音事件定位、检测和跟踪的序列匹配网络集成。见
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*，日本东京，2020年11月。
- en: Nguyen et al. [2020c] T. N. T. Nguyen, D. L. Jones, and W.-S. Gan. A sequence
    matching network for polyphonic sound event localization and detection. In *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 71–75, Barcelona,
    Spain (virtual conference), 2020c.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. [2020c] T. N. T. Nguyen, D. L. Jones, 和 W.-S. Gan. 用于多音声音事件定位和检测的序列匹配网络。见
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，页码 71–75，西班牙巴塞罗那（虚拟会议），2020年。
- en: Nguyen et al. [2021a] T. N. T. Nguyen, N. K. Nguyen, H. Phan, L. Pham, K. Ooi,
    D. L. Jones, and W.-S. Gan. A general network architecture for sound event localization
    and detection using transfer learning and recurrent neural network. In *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 935–939, Toronto,
    Canada (virtual conference), June 2021a.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. [2021a] T. N. T. Nguyen, N. K. Nguyen, H. Phan, L. Pham, K. Ooi,
    D. L. Jones, 和 W.-S. Gan. 使用迁移学习和递归神经网络的声音事件定位和检测的通用网络架构。见 *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*，页码 935–939，加拿大多伦多（虚拟会议），2021年6月。
- en: 'Nguyen et al. [2021b] T. N. T. Nguyen, K. Watcharasupat, N. K. Nguyen, D. L.
    Jones, and W. S. Gan. DCASE 2021 Task 3: spectrotemporally-aligned features for
    polyphonic sound event localization and detection. Technical report, 2021b. DCASE
    2021 Challenge.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen et al. [2021b] T. N. T. Nguyen, K. Watcharasupat, N. K. Nguyen, D. L.
    Jones, 和 W. S. Gan. DCASE 2021任务3: 用于多音声音事件定位和检测的频谱时间对齐特征。技术报告，2021年。DCASE 2021挑战。'
- en: Noh et al. [2019] K. Noh, J.-H. Choi, D. Jeon, and J.-H. Chang. Three-stage
    approach for sound event localization and detection. Technical report, 2019. DCASE
    2019 Challenge.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noh et al. [2019] K. Noh, J.-H. Choi, D. Jeon, 和 J.-H. Chang. 三阶段声音事件定位和检测方法。技术报告，2019年。DCASE
    2019挑战。
- en: Nolan et al. [2019] M. Nolan, S. A. Verburg, J. Brunskog, and E. Fernandez-Grande.
    Experimental characterization of the sound field in a reverberation room. *J.
    Acoust. Soc. Am.*, 145(4):2237–2246, 2019.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nolan et al. [2019] M. Nolan, S. A. Verburg, J. Brunskog, 和 E. Fernandez-Grande.
    在混响室中声场的实验特性描述。*J. Acoust. Soc. Am.*，145(4):2237–2246，2019年。
- en: Noohi et al. [2013] T. Noohi, N. Epain, and C. T. Jin. Direction of arrival
    estimation for spherical microphone arrays by combination of independent component
    analysis and sparse recovery. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 346–349, Vancouver, Canada, 2013.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noohi et al. [2013] T. Noohi, N. Epain, 和 C. T. Jin. 通过独立成分分析和稀疏恢复组合估计球形麦克风阵列的到达方向。见
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，页码 346–349，加拿大温哥华，2013年。
- en: Nugraha et al. [2016] A. A. Nugraha, A. Liutkus, and E. Vincent. Multichannel
    audio source separation with deep neural networks. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 24(9):1652–1664, 2016.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nugraha et al. [2016] A. A. Nugraha, A. Liutkus, 和 E. Vincent. 使用深度神经网络的多通道音频源分离。*IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*，24(9):1652–1664，2016年。
- en: Nustede and Anemüller [2019] E. J. Nustede and J. Anemüller. Group delay features
    for sound event detection and localization. Technical report, 2019. DCASE 2019
    Challenge.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nustede and Anemüller [2019] E. J. Nustede 和 J. Anemüller. 用于声音事件检测和定位的群延迟特征。技术报告，2019年。DCASE
    2019挑战。
- en: Opochinsky et al. [2019] R. Opochinsky, B. Laufer-Goldshtein, S. Gannot, and
    G. Chechik. Deep ranking-based sound source localization. In *Proc. IEEE Workshop
    Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 283–287, New Paltz, NY, 2019.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Opochinsky et al. [2019] R. Opochinsky, B. Laufer-Goldshtein, S. Gannot, 和 G.
    Chechik. 基于深度排序的声音源定位。见 *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.
    (WASPAA)*，页码 283–287，纽约新帕尔茨，2019年。
- en: Opochinsky et al. [2021] R. Opochinsky, G. Chechik, and S. Gannot. Deep ranking-based
    DoA tracking algorithm. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages
    1020–1024, Dublin, Ireland (virtual conference), 2021.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Opochinsky et al. [2021] R. Opochinsky, G. Chechik, 和 S. Gannot. 基于深度排序的到达方向跟踪算法。见
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，页码 1020–1024，爱尔兰都柏林（虚拟会议），2021年。
- en: Pak and Shin [2019] J. Pak and J. W. Shin. Sound localization based on phase
    difference enhancement using deep neural networks. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 27(8):1335–1345, 2019.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pak and Shin [2019] J. Pak 和 J. W. Shin. 基于相位差增强的声音定位，使用深度神经网络。*IEEE/ACM Trans.
    Audio, Speech, Lang. Process.*，27(8):1335–1345，2019年。
- en: 'Pal and Vaidyanathan [2010] P. Pal and P. P. Vaidyanathan. Nested arrays: A
    novel approach to array processing with enhanced degrees of freedom. *IEEE Trans.
    Signal Process.*, 58(8):4167–4181, 2010.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pal 和 Vaidyanathan [2010] P. Pal 和 P. P. Vaidyanathan。嵌套数组：一种具有增强自由度的数组处理新方法。*IEEE
    Trans. Signal Process.*，58(8)：4167–4181，2010。
- en: Pang et al. [2019] C. Pang, H. Liu, and X. Li. Multitask learning of time-frequency
    CNN for sound source localization. *IEEE Access*, 7:40725–40737, 2019.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 等人 [2019] C. Pang, H. Liu, 和 X. Li。时间频率 CNN 的多任务学习用于声音源定位。*IEEE Access*，7：40725–40737，2019。
- en: Parcollet et al. [2018] T. Parcollet, Y. Zhang, M. Morchid, C. Trabelsi, G. Linarès,
    R. De Mori, and Y. Bengio. Quaternion Convolutional Neural Networks for End-to-End
    Automatic Speech Recognition, June 2018. arXiv:1806.07789.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parcollet 等人 [2018] T. Parcollet, Y. Zhang, M. Morchid, C. Trabelsi, G. Linarès,
    R. De Mori, 和 Y. Bengio。用于端到端自动语音识别的四元数卷积神经网络，2018 年 6 月。arXiv:1806.07789。
- en: 'Park et al. [2019a] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
    Cubuk, and Q. V. Le. SpecAugment: a simple data augmentation method for automatic
    speech recognition. In *Proc. Interspeech Conf.*, pages 2613–2617, Graz, Austria,
    Sept. 2019a.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2019a] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
    和 Q. V. Le。SpecAugment：一种简单的数据增强方法，用于自动语音识别。见 *Proc. Interspeech Conf.*，第 2613–2617
    页，奥地利格拉茨，2019a 年 9 月。
- en: Park et al. [2019b] S. Park, W. Lim, S. Suh, and Y. Jeong. TrellisNet-based
    architecture for sound event localization and detection with reassembly learning.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, New York, NY, October 2019b.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2019b] S. Park, W. Lim, S. Suh, 和 Y. Jeong。基于 TrellisNet 的声音事件定位和检测架构，结合重新组合学习。见
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*，纽约，2020 年 10 月。
- en: Park et al. [2020] S. Park, S. Suh, and Y. Jeong. Sound event localization and
    detection with various loss functions. Technical report, 2020. DCASE 2020 Challenge.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2020] S. Park, S. Suh, 和 Y. Jeong。利用各种损失函数进行声音事件定位和检测。技术报告，2020 年。DCASE
    2020 挑战。
- en: 'Park et al. [2021a] S. Park, Y. Jeong, and T. Lee. Many-to-many audio spectrogram
    transformer: Transformer for sound event localization and detection. In *Proc.
    Detection and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*,
    pages 105–109, Barcelona, Spain, November 2021a. ISBN 978-84-09-36072-7.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2021a] S. Park, Y. Jeong, 和 T. Lee。多对多音频谱图变换器：用于声音事件定位和检测的变换器。见 *Proc.
    Detection and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*，第
    105–109 页，西班牙巴萨罗那，2021 年 11 月。ISBN 978-84-09-36072-7。
- en: 'Park et al. [2021b] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe,
    and S. Narayanan. A review of speaker diarization: recent advances with deep learning,
    June 2021b. arXiv:2101.09624.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2021b] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe,
    和 S. Narayanan。讲者分离的综述：深度学习的最新进展，2021 年 6 月。arXiv:2101.09624。
- en: Patel et al. [2020] S. J. Patel, M. Zawodniok, and J. Benesty. A single stage
    fully convolutional neural network for sound source localization and detection.
    Technical report, 2020. DCASE 2020 Challenge.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel 等人 [2020] S. J. Patel, M. Zawodniok, 和 J. Benesty。用于声音源定位和检测的单阶段全卷积神经网络。技术报告，2020
    年。DCASE 2020 挑战。
- en: Pavlidi et al. [2015] D. Pavlidi, S. Delikaris-Manias, V. Pulkki, and A. Mouchtaris.
    3D localization of multiple sound sources with intensity vector estimates in single
    source zones. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 1556–1560,
    Nice, France, 2015.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlidi 等人 [2015] D. Pavlidi, S. Delikaris-Manias, V. Pulkki, 和 A. Mouchtaris。在单一源区的强度向量估计下，多重声音源的
    3D 定位。见 *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，第 1556–1560 页，法国尼斯，2015
    年。
- en: Peeters [2004] G. Peeters. A large set of audio features for sound description
    (similarity and classification) in the CUIDADO project. *CUIDADO Project Report
    54.0*, 2004.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peeters [2004] G. Peeters。CUIDADO 项目中的大量音频特征集用于声音描述（相似性和分类）。*CUIDADO Project
    Report 54.0*，2004 年。
- en: Perotin et al. [2018a] L. Perotin, R. Serizel, E. Vincent, and A. Guérin. Multichannel
    speech separation with recurrent neural networks from high-order ambisonics recordings.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, Calgary,
    Canada, 2018a.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perotin 等人 [2018a] L. Perotin, R. Serizel, E. Vincent, 和 A. Guérin。基于高阶 Ambisonics
    录音的多通道语音分离。见 *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，加拿大卡尔加里，2018
    年。
- en: Perotin et al. [2018b] L. Perotin, R. Serizel, E. Vincent, and A. Guérin. CRNN-based
    joint azimuth and elevation localization with the Ambisonics intensity vector.
    In *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*, pages 241–245,
    Tokyo, Japan, Sept. 2018b.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perotin 等人 [2018b] L. Perotin, R. Serizel, E. Vincent, 和 A. Guérin。基于 CRNN 的联合方位角和高度定位，结合
    Ambisonics 强度向量。见 *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*，第
    241–245 页，日本东京，2018 年 9 月。
- en: Perotin et al. [2019a] L. Perotin, A. Défossez, E. Vincent, R. Serizel, and
    A. Guérin. Regression versus classification for neural network based audio source
    localization. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    New Paltz, NY, 2019a.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perotin et al. [2019a] L. Perotin, A. Défossez, E. Vincent, R. Serizel, 和 A.
    Guérin. 神经网络基于音频源定位的回归与分类。见 *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust.
    (WASPAA)*，美国纽约州新帕尔茨，2019 年。
- en: Perotin et al. [2019b] L. Perotin, R. Serizel, E. Vincent, and A. Guérin. CRNN-based
    multiple DoA estimation using acoustic intensity features for Ambisonics recordings.
    *IEEE J. Sel. Topics Signal Process.*, 13(1):22–33, Mar. 2019b.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perotin et al. [2019b] L. Perotin, R. Serizel, E. Vincent, 和 A. Guérin. 基于 CRNN
    的多重到达方向估计，使用声学强度特征用于 Ambisonics 录音。*IEEE J. Sel. Topics Signal Process.*，13(1):22–33，2019
    年 3 月。
- en: Pertilä and Cakir [2017] P. Pertilä and E. Cakir. Robust direction estimation
    with convolutional neural networks based steered response power. In *Proc. IEEE
    Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 6125–6129, New Orleans,
    LA, Mar. 2017.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pertilä 和 Cakir [2017] P. Pertilä 和 E. Cakir. 基于卷积神经网络的稳健方向估计，采用引导响应功率。见 *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*，第 6125–6129 页，美国路易斯安那州新奥尔良，2017
    年 3 月。
- en: Phan et al. [2020a] H. Phan, L. Pham, P. Koch, N. Q. K. Duong, I. McLoughlin,
    and A. Mertins. Audio event detection and localization with multitask regression
    network. Technical report, 2020a. DCASE 2020 Challenge.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phan et al. [2020a] H. Phan, L. Pham, P. Koch, N. Q. K. Duong, I. McLoughlin,
    和 A. Mertins. 使用多任务回归网络的音频事件检测和定位。技术报告，2020 年。DCASE 2020 挑战。
- en: Phan et al. [2020b] H. Phan, L. Pham, P. Koch, N. Q. K. Duong, I. McLoughlin,
    and A. Mertins. On multitask loss function for audio event detection and localization.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, pages 160–164, Tokyo, Japan, 2020b.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phan et al. [2020b] H. Phan, L. Pham, P. Koch, N. Q. K. Duong, I. McLoughlin,
    和 A. Mertins. 关于音频事件检测和定位的多任务损失函数。见 *Proc. Detection and Classification of Acoustic
    Scenes and Events Workshop (DCASE Workshop)*，第 160–164 页，日本东京，2020 年。
- en: Ping et al. [2020] G. Ping, E. Fernandez-Grande, P. Gerstoft, and Z. Chu. Three-dimensional
    source localization using sparse Bayesian learning on a spherical microphone array.
    *J. Acoust. Soc. Am.*, 147(6):3895–3904, 2020.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ping et al. [2020] G. Ping, E. Fernandez-Grande, P. Gerstoft, 和 Z. Chu. 使用稀疏贝叶斯学习在球形麦克风阵列上进行三维源定位。*J.
    Acoust. Soc. Am.*，147(6):3895–3904，2020 年。
- en: Politis et al. [2020a] A. Politis, S. Adavanne, and T. Virtanen. A dataset of
    reverberant spatial sound scenes with moving sources for sound event localization
    and detection. In *Proc. Detection and Classification of Acoustic Scenes and Events
    Workshop (DCASE Workshop)*, pages 165–169, Tokyo, Japan, November 2020a.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Politis et al. [2020a] A. Politis, S. Adavanne, 和 T. Virtanen. 一个包含移动源的混响空间声音场数据集，用于声音事件定位和检测。见
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*，第 165–169 页，日本东京，2020 年 11 月。
- en: Politis et al. [2020b] A. Politis, A. Mesaros, S. Adavanne, T. Heittola, and
    T. Virtanen. Overview and evaluation of sound event localization and detection
    in DCASE 2019. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 29:684–698, Sept.
    2020b.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Politis et al. [2020b] A. Politis, A. Mesaros, S. Adavanne, T. Heittola, 和 T.
    Virtanen. DCASE 2019 中声音事件定位和检测的概述与评估。*IEEE/ACM Trans. Audio, Speech, Lang. Process.*，29:684–698，2020
    年 9 月。
- en: Politis et al. [2021] A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava,
    and T. Virtanen. A dataset of dynamic reverberant sound scenes with directional
    interferers for sound event localization and detection. In *Proc. Detection and
    Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*, Barcelona,
    Spain, June 2021.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Politis et al. [2021] A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava,
    和 T. Virtanen. 一个包含定向干扰源的动态混响声音场数据集，用于声音事件定位和检测。见 *Proc. Detection and Classification
    of Acoustic Scenes and Events Workshop (DCASE Workshop)*，西班牙巴塞罗那，2021 年 6 月。
- en: Poschadel et al. [2021a] N. Poschadel, R. Hupke, S. Preihs, and J. Peissig.
    Direction of arrival estimation of noisy speech using convolutional recurrent
    neural networks with higher-order Ambisonics signals. In *Proc. Europ. Signal
    Process. Conf. (EUSIPCO)*, Dublin, Ireland (virtual conference), Mar. 2021a.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poschadel et al. [2021a] N. Poschadel, R. Hupke, S. Preihs, 和 J. Peissig. 使用卷积递归神经网络与高阶
    Ambisonics 信号对噪声语音进行到达方向估计。见 *Proc. Europ. Signal Process. Conf. (EUSIPCO)*，爱尔兰都柏林（虚拟会议），2021
    年 3 月。
- en: Poschadel et al. [2021b] N. Poschadel, S. Preihs, and J. Peissig. Multi-source
    direction of arrival estimation of noisy speech using convolutional recurrent
    neural networks with higher-order ambisonics signals. In *Proc. Europ. Signal
    Process. Conf. (EUSIPCO)*, pages 1015–1019, Dublin, Ireland (virtual conference),
    2021b.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pratik et al. [2019] P. Pratik, W. J. Jee, S. Nagisetty, R. Mars, and C. Lim.
    Sound event localization and detection using CRNN architecture with Mixup for
    model generalization. In *Proc. Detection and Classification of Acoustic Scenes
    and Events Workshop (DCASE Workshop)*, New York, NY, October 2019.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pujol et al. [2019] H. Pujol, E. Bavu, and A. Garcia. Source localization in
    reverberant rooms using deep learning and microphone arrays. In *Proc. Int. Congr.
    Acoust. (ICA)*, Aachen, Germany, 2019.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pujol et al. [2021] H. Pujol, E. Bavu, and A. Garcia. BeamLearning: an end-to-end
    deep learning approach for the angular localization of sound sources using raw
    multichannel acoustic pressure data. *J. Acoust. Soc. Am.*, 149(6):4248–4263,
    Apr. 2021.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Purwins et al. [2019] H. Purwins, B. Li, T. Virtanen, J. Schlüter, S.-Y. Chang,
    and T. Sainath. Deep learning for audio signal processing. *IEEE J. Sel. Topics
    Signal Process.*, 13(2):206–219, 2019.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raangs and Druyvesteyn [2002] R. Raangs and E. Druyvesteyn. Sound source localization
    using sound intensity measured by a three dimensional PU-probe. In *Proc. Audio
    Engin. Soc. (AES) Conv.*, Munich, Germany, 2002.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rafaely [2019] B. Rafaely. *Fundamentals of Spherical Array Processing*. Springer,
    2019.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranjan et al. [2019] R. Ranjan, S. Jayabalan, T. N. T. Nguyen, and W.-S. Lim.
    Sound events detection and direction of arrival estimation using residual net
    and recurrent neural networks. In *Proc. Detection and Classification of Acoustic
    Scenes and Events Workshop (DCASE Workshop)*, New York, NY, 2019.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rastogi et al. [1987] R. Rastogi, P. Gupta, and R. Kumaresan. Array signal processing
    with interconnected neuron-like elements. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 2328–2331, Dallas, TX, 1987.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende et al. [2014] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
    backpropagation and approximate inference in deep generative models. In *Proc.
    Int. Conf. Mach. Learn. (ICML)*, Beijing, China, 2014.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rho et al. [2021] D. Rho, S. Lee, J. Park, T. Kim, J. Chang, and J. Ko. A combination
    of various neural networks for sound event localization and detection. Technical
    report, November 2021. DCASE 2021 Challenge.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rickard [2002] S. Rickard. On the approximate W-disjoint orthogonality of speech.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 529–532,
    Orlando, Florida, 2002.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riezu and Grande [2021] S. A. V. Riezu and E. F. Grande. Room Impulse Response
    Dataset - ACT, DTU Elektro (011, IEC; plane, sphere). 4 2021. URL [https://data.dtu.dk/articles/dataset/Room_Impulse_Response_Dataset_-_ACT_DTU_Elektro_011_IEC_plane_sphere_/14320166](https://data.dtu.dk/articles/dataset/Room_Impulse_Response_Dataset_-_ACT_DTU_Elektro_011_IEC_plane_sphere_/14320166).
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riezu 和 Grande [2021] S. A. V. Riezu 和 E. F. Grande。房间脉冲响应数据集 - ACT，DTU Elektro
    (011，IEC; 平面，球体)。2021年4月。网址 [https://data.dtu.dk/articles/dataset/Room_Impulse_Response_Dataset_-_ACT_DTU_Elektro_011_IEC_plane_sphere_/14320166](https://data.dtu.dk/articles/dataset/Room_Impulse_Response_Dataset_-_ACT_DTU_Elektro_011_IEC_plane_sphere_/14320166)。
- en: Rindel [2000] J. H. Rindel. The use of computer modeling in room acoustics.
    *J. Vibroengineer.*, 3(4):219–224, 2000.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rindel [2000] J. H. Rindel。计算机建模在房间声学中的应用。*振动工程学报*，3(4):219–224，2000年。
- en: Roden et al. [2015] R. Roden, N. Moritz, S. Gerlach, S. Weinzierl, and S. Goetze.
    On sound source localization of speech signals using deep neural networks. In
    *Proc. Deutsche Jahrestagung Akustik (DAGA)*, Nuremberg, Germany, 2015. ISBN 978-3-939296-08-9.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roden 等 [2015] R. Roden, N. Moritz, S. Gerlach, S. Weinzierl 和 S. Goetze。基于深度神经网络的语音信号声源定位。在
    *德国声学年会 (DAGA)*，德国纽伦堡，2015年。ISBN 978-3-939296-08-9。
- en: Roman and Wang [2008] N. Roman and D. Wang. Binaural tracking of multiple moving
    sources. *IEEE Trans. Audio, Speech, Lang. Process.*, 16(4):728–739, 2008.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roman 和 Wang [2008] N. Roman 和 D. Wang。多声道跟踪移动源。*IEEE 音频、语音和语言处理学报*，16(4):728–739，2008年。
- en: Ronchini et al. [2020] F. Ronchini, D. Arteaga, and A. Pérez-López. Sound event
    localization and detection based on CRNN using rectangular filters and channel
    rotation data augmentation. In *Proc. Detection and Classification of Acoustic
    Scenes and Events Workshop (DCASE Workshop)*, Tokyo, Japan, Oct. 2020.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronchini 等 [2020] F. Ronchini, D. Arteaga, 和 A. Pérez-López。基于 CRNN 的声音事件定位与检测，使用矩形滤波器和通道旋转数据增强。在
    *声学场景和事件检测与分类研讨会 (DCASE Workshop)*，日本东京，2020年10月。
- en: 'Ronneberger et al. [2015] O. Ronneberger, P. Fischer, and T. Brox. U-Net: convolutional
    networks for biomedical image segmentation. In *Int. Conf. Medical Image Comput.
    Computer-Assisted Interv. (MICCAI)*, pages 234–241, Munich, Germany, May 2015.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等 [2015] O. Ronneberger, P. Fischer 和 T. Brox。U-Net: 用于生物医学图像分割的卷积网络。在
    *国际医学图像计算与计算机辅助干预会议 (MICCAI)*，第234–241页，德国慕尼黑，2015年5月。'
- en: Rossing [2007] T. D. Rossing. *Springer Handbook of Acoustics*. Springer, 2007.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rossing [2007] T. D. Rossing. *斯普林格声学手册*。斯普林格，2007年。
- en: 'Roy and Kailath [1989] R. Roy and T. Kailath. ESPRIT: Estimation of signal
    parameters via rotational invariance techniques. *IEEE Trans. Acoust., Speech,
    Signal Process.*, 37(7):984–995, July 1989.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 和 Kailath [1989] R. Roy 和 T. Kailath。ESPRIT：通过旋转不变性技术估计信号参数。*IEEE 声学、语音和信号处理学报*，37(7):984–995，1989年7月。
- en: Ruder [2017] S. Ruder. An overview of multi-task learning in deep neural networks.
    *arXiv preprint arXiv:1706.05098*, 2017.
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder [2017] S. Ruder。深度神经网络中的多任务学习概述。*arXiv 预印本 arXiv:1706.05098*，2017年。
- en: 'Sadeghian et al. [2017] A. Sadeghian, A. Alahi, and S. Savarese. Tracking the
    untrackable: Learning to track multiple cues with long-term dependencies. In *Proc.
    IEEE Int. Conf. Computer Vision (ICCV)*, pages 300–311, Venice, Italy, 2017.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sadeghian 等 [2017] A. Sadeghian, A. Alahi 和 S. Savarese。跟踪不可跟踪的目标：学习跟踪具有长期依赖的多个线索。在
    *IEEE 国际计算机视觉会议 (ICCV)*，第300–311页，意大利威尼斯，2017年。
- en: Sadok et al. [2022] S. Sadok, S. Leglaive, L. Girin, X. Alameda-Pineda, and
    R. Séguier. Learning and controlling the source-filter representation of speech
    with a variational autoencoder, 2022. arXiv:2204.07075.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sadok 等 [2022] S. Sadok, S. Leglaive, L. Girin, X. Alameda-Pineda 和 R. Séguier。利用变分自编码器学习和控制语音的源-滤波器表示，2022年。arXiv:2204.07075。
- en: Sainath et al. [2017] T. N. Sainath, R. J. Weiss, K. W. Wilson, B. Li, A. Narayanan,
    E. Variani, M. Bacchiani, I. Shafran, A. Senior, et al. Multichannel signal processing
    with deep neural networks for automatic speech recognition. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 25(5):965–979, 2017.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sainath 等 [2017] T. N. Sainath, R. J. Weiss, K. W. Wilson, B. Li, A. Narayanan,
    E. Variani, M. Bacchiani, I. Shafran, A. Senior 等。使用深度神经网络进行多通道信号处理以实现自动语音识别。*IEEE/ACM
    音频、语音和语言处理学报*，25(5):965–979，2017年。
- en: Salamon and Bello [2017] J. Salamon and J. P. Bello. Deep convolutional neural
    networks and data augmentation for environmental sound classification. *IEEE Signal
    Process. Lett.*, 24(3):279–283, Mar. 2017.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salamon 和 Bello [2017] J. Salamon 和 J. P. Bello。用于环境声音分类的深度卷积神经网络和数据增强。*IEEE
    信号处理快报*，24(3):279–283，2017年3月。
- en: Saleh et al. [2021] F. Saleh, S. Aliakbarian, H. Rezatofighi, M. Salzmann, and
    S. Gould. Probabilistic tracklet scoring and inpainting for multiple object tracking.
    In *Proc. IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*, pages 14329–14339,
    virtual conference, 2021.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saleh 等人 [2021] F. Saleh, S. Aliakbarian, H. Rezatofighi, M. Salzmann, 和 S.
    Gould. 用于多目标跟踪的概率轨迹评分和修补. 见 *Proc. IEEE Conf. Computer Vision Pattern Recogn.
    (CVPR)*, 页 14329–14339, 虚拟会议, 2021年。
- en: Salvati et al. [2018] D. Salvati, C. Drioli, and G. L. Foresti. Exploiting CNNs
    for improving acoustic source localization in noisy and reverberant conditions.
    *IEEE Trans. Emerg. Topics Comput. Intell.*, 2(2):103–116, Apr. 2018.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salvati 等人 [2018] D. Salvati, C. Drioli, 和 G. L. Foresti. 利用 CNN 在噪声和混响条件下改进声源定位.
    *IEEE Trans. Emerg. Topics Comput. Intell.*, 2(2):103–116, 2018年4月。
- en: Sampathkumar and Kowerko [2020] A. Sampathkumar and D. Kowerko. Sound event
    detection and localization using CRNN models. Technical report, 2020. DCASE 2020
    Challenge.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sampathkumar 和 Kowerko [2020] A. Sampathkumar 和 D. Kowerko. 使用 CRNN 模型进行声音事件检测和定位.
    技术报告, 2020年. DCASE 2020 挑战。
- en: Sato et al. [2021] I. Sato, G. Liu, K. Ishikawa, T. Suzuki, and M. Tanaka. Does
    end-to-end trained deep model always perform better than non-end-to-end counterpart?
    *Electronic Imaging*, 2021(10):240–1, 2021.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sato 等人 [2021] I. Sato, G. Liu, K. Ishikawa, T. Suzuki, 和 M. Tanaka. 端到端训练的深度模型是否总是优于非端到端模型？
    *Electronic Imaging*, 2021(10):240–1, 2021年。
- en: Sawada et al. [2003] H. Sawada, R. Mukai, and S. Makino. Direction of arrival
    estimation for multiple source signals using independent component analysis. In
    *IEEE Int. Symp. Signal Process. Applic.*, pages 411–414, Paris, France, 2003.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sawada 等人 [2003] H. Sawada, R. Mukai, 和 S. Makino. 使用独立成分分析对多个源信号进行到达方向估计. 见
    *IEEE Int. Symp. Signal Process. Applic.*, 页 411–414, 法国巴黎, 2003年。
- en: 'Scheibler et al. [2018] R. Scheibler, E. Bezzam, and I. Dokmanić. Pyroomacoustics:
    a Python package for audio room simulation and array processing algorithms. In
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 351–355,
    Calgary, Canada, Apr. 2018.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scheibler 等人 [2018] R. Scheibler, E. Bezzam, 和 I. Dokmanić. Pyroomacoustics:
    用于音频房间模拟和阵列处理算法的 Python 包. 见 *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, 页 351–355, 加拿大卡尔加里, 2018年4月。'
- en: Schmidt [1986] R. Schmidt. Multiple emitter location and signal parameter estimation.
    *IEEE Trans. Antennas Propag.*, 34(3):276–280, Mar. 1986.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidt [1986] R. Schmidt. 多发射器位置和信号参数估计. *IEEE Trans. Antennas Propag.*, 34(3):276–280,
    1986年3月。
- en: Schwartz and Gannot [2013] O. Schwartz and S. Gannot. Speaker tracking using
    recursive EM algorithms. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 22(2):392–402,
    2013.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwartz 和 Gannot [2013] O. Schwartz 和 S. Gannot. 使用递归 EM 算法进行说话人跟踪. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 22(2):392–402, 2013年。
- en: Schymura et al. [2020] C. Schymura, T. Ochiai, M. Delcroix, K. Kinoshita, T. Nakatani,
    S. Araki, and D. Kolossa. Exploiting attention-based sequence-to-sequence architectures
    for sound event localization. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*,
    Amsterdam, The Netherlands (virtual conference), 2020.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schymura 等人 [2020] C. Schymura, T. Ochiai, M. Delcroix, K. Kinoshita, T. Nakatani,
    S. Araki, 和 D. Kolossa. 利用基于注意力的序列到序列架构进行声音事件定位. 见 *Proc. Europ. Signal Process.
    Conf. (EUSIPCO)*, 荷兰阿姆斯特丹（虚拟会议）, 2020年。
- en: 'Schymura et al. [2021] C. Schymura, B. Bönninghoff, T. Ochiai, M. Delcroix,
    K. Kinoshita, T. Nakatani, S. Araki, and D. Kolossa. PILOT: introducing Transformers
    for probabilistic sound event localization. In *Proc. Interspeech Conf.*, Brno,
    Czech Republic, June 2021.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schymura 等人 [2021] C. Schymura, B. Bönninghoff, T. Ochiai, M. Delcroix, K.
    Kinoshita, T. Nakatani, S. Araki, 和 D. Kolossa. PILOT: 引入 Transformers 用于概率声音事件定位.
    见 *Proc. Interspeech Conf.*, 捷克共和国布尔诺, 2021年6月。'
- en: Sehgal and Kehtarnavaz [2018] A. Sehgal and N. Kehtarnavaz. A convolutional
    neural network smartphone app for real-time voice activity detection. *IEEE Access*,
    6:9017–9026, 2018.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sehgal 和 Kehtarnavaz [2018] A. Sehgal 和 N. Kehtarnavaz. 用于实时语音活动检测的卷积神经网络智能手机应用.
    *IEEE Access*, 6:9017–9026, 2018年。
- en: 'Shimada et al. [2020a] K. Shimada, Y. Koyama, N. Takahashi, S. Takahashi, and
    Y. Mitsufuji. ACCDOA: activity-coupled cartesian direction of arrival representation
    for sound event localization and detection. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, Barcelona, Spain (virtual conference), 2020a.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shimada 等人 [2020a] K. Shimada, Y. Koyama, N. Takahashi, S. Takahashi, 和 Y.
    Mitsufuji. ACCDOA: 结合活动的笛卡尔到达方向表示用于声音事件定位和检测. 见 *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, 西班牙巴塞罗那（虚拟会议）, 2020年。'
- en: Shimada et al. [2020b] K. Shimada, N. Takahashi, S. Takahashi, and Y. Mitsufuji.
    Sound event localization and detection using activity-coupled cartesian DoA vector
    and RD3net. Technical report, June 2020b. DCASE 2020 Challenge.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimada et al. [2021] K. Shimada, N. Takahashi, Y. Koyama, S. Takahashi, E. Tsunoo,
    M. Takahashi, and Y. Mitsufuji. Ensemble of accdoa- and einv2-based systems with
    d3nets and impulse response simulation for sound event localization and detection.
    Technical report, 2021. DCASE 2021 Challenge.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shlezinger et al. [2020] N. Shlezinger, J. Whang, Y. C. Eldar, and A. G. Dimakis.
    Model-based deep learning, 2020. arXiv:2012.08405.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siltanen et al. [2010] S. Siltanen, T. Lokki, and L. Savioja. Rays or waves?
    understanding the strengths and weaknesses of computational room acoustics modeling
    techniques. In *Proc. Int. Symp. Room Acoust. (ISRA)*, Melbourne, Australia, 2010.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singla et al. [2020] R. Singla, S. Tiwari, and R. Sharma. A sequential system
    for sound event detection and localization using CRNN. Technical report, 2020.
    DCASE 2020 Challenge.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sivasankaran et al. [2018] S. Sivasankaran, E. Vincent, and D. Fohr. Keyword-based
    speaker localization: localizing a target speaker in a multi-speaker environment.
    In *Proc. Interspeech Conf.*, Hyderabad, India, 2018.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song [2020] J.-m. Song. Localization and detection for moving sound sources
    using consecutive ensembles of 2D-CRNN. Technical report, 2020. DCASE 2020 Challenge.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Southall et al. [1995] H. Southall, J. Simmers, and T. O’Donnell. Direction
    finding in phased arrays with a neural network beamformer. *IEEE Trans. Antennas
    Propag.*, 43(12):1369–1374, 1995.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiefelhagen et al. [2007] R. Stiefelhagen, K. Bernardin, R. Bowers, R. T. Rose,
    M. Michel, and J. Garofolo. The CLEAR 2007 evaluation. In *Proc. Multimodal Technol.
    Percept. Humans*, pages 3–34, Baltimore, MD, 2007.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subramani and Smaragdis [2021] K. Subramani and P. Smaragdis. Point cloud audio
    processing. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    pages 31–35, New Paltz, NY (virtual conference), 2021.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subramanian et al. [2021a] A. S. Subramanian, C. Weng, S. Watanabe, M. Yu,
    Y. Xu, S.-X. Zhang, and D. Yu. Directional ASR: A new paradigm for E2E multi-speaker
    speech recognition with source localization. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, pages 8433–8437, Toronto, Canada (virtual conference),
    2021a.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subramanian et al. [2021b] A. S. Subramanian, C. Weng, S. Watanabe, M. Yu, and
    D. Yu. Deep learning based multi-source localization with source splitting and
    its effectiveness in multi-talker speech recognition, Feb. 2021b. arXiv:2102.07955.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudarsanam et al. [2021] P. A. Sudarsanam, A. Politis, and K. Drossos. Assessment
    of self-attention on learned features for sound event localization and detection.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, pages 100–104, Barcelona, Spain, November 2021. ISBN 978-84-09-36072-7.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudo et al. [2019] Y. Sudo, K. Itoyama, K. Nishida, and K. Nakadai. Improvement
    of DOA estimation by using quaternion output in sound event localization and detection.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, New York, NY, October 2019.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2020] P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang,
    and P. Luo. Transtrack: Multiple object tracking with transformer, 2020. arXiv:2012.15460.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundar et al. [2020] H. Sundar, W. Wang, M. Sun, and C. Wang. Raw waveform based
    end-to-end deep convolutional network for spatial localization of multiple acoustic
    sources. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 4642–4646, Barcelona, Spain (virtual conference), May 2020.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suvorov et al. [2018] D. Suvorov, G. Dong, and R. Zhukov. Deep residual network
    for sound source localization in the time domain, Aug. 2018. arXiv:1808.06429.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Svensson and Kristiansen [2002] P. Svensson and U. R. Kristiansen. Computational
    modelling and simulation of acoustic spaces. In *Proc. Audio Eng. Soc. Conf.*,
    Espoo, Finland, 2002.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szöke et al. [2019] I. Szöke, M. Skácel, L. Mošner, J. Paliesek, and J. Černocký.
    Building and evaluation of a real room impulse response dataset. *IEEE J. Sel.
    Topics Signal Process.*, 13(4):863–876, Aug. 2019.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takahashi et al. [2016] N. Takahashi, M. Gygli, B. Pfister, and L. V. Gool.
    Deep convolutional neural networks and data augmentation for acoustic event recognition.
    In *Proc. Interspeech Conf.*, pages 2982–2986, San Francisco, CA, Sept. 2016.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takahashi et al. [2018] N. Takahashi, N. Goswami, and Y. Mitsufuji. MMDenseLSTM:
    An efficient combination of convolutional and recurrent neural networks for audio
    source separation. In *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*,
    Tokyo, Japan, May 2018.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda and Komatani [2016a] R. Takeda and K. Komatani. Discriminative multiple
    sound source localization based on deep neural networks using independent location
    model. In *IEEE Spoken Language Technol. Workshop*, pages 603–609, San Juan, Portugal,
    2016a.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda and Komatani [2016b] R. Takeda and K. Komatani. Sound source localization
    based on deep neural networks with directional activate function exploiting phase
    information. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 405–409, Shanghai, China, 2016b.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda and Komatani [2017] R. Takeda and K. Komatani. Unsupervised adaptation
    of deep neural networks for sound source localization using entropy minimization.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 2217–2221,
    New-Orleans, LA, 2017.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda et al. [2018] R. Takeda, Y. Kudo, K. Takashima, Y. Kitamura, and K. Komatani.
    Unsupervised adaptation of neural networks for discriminative sound source localization
    with eliminative constraint. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 3514–3518, Calgary, Canada, 2018.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2019] Z. Tang, J. D. Kanu, K. Hogan, and D. Manocha. Regression
    and classification for direction-of-arrival estimation with convolutional recurrent
    neural networks. In *Proc. Interspeech Conf.*, pages 654–658, Graz, Austria, 2019.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tervo [2009] S. Tervo. Direction estimation based on sound intensity vectors.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 700–704, Glasgow, Scotland,
    2009.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thiemann and Van De Par [2015] J. Thiemann and S. Van De Par. Multiple model
    high-spatial resolution HRTF measurements. In *Proc. Deutsche Jahrestagung Akustik
    (DAGA)*, Nuremberg, Germany, 2015.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thuillier et al. [2018] E. Thuillier, H. Gamper, and I. J. Tashev. Spatial audio
    feature discovery with convolutional neural networks. In *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*, pages 6797–6801, Calgary, Canada,
    2018.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian [2020] C. Tian. Multiple CRNN for SELD. Technical report, 2020. DCASE 2020
    Challenge.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tranter and Reynolds [2006] S. E. Tranter and D. A. Reynolds. An overview of
    automatic speaker diarization systems. *IEEE Trans. Audio, Speech, Lang. Process.*,
    14(5):1557–1565, 2006.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsuzuki et al. [2013] H. Tsuzuki, M. Kugler, S. Kuroyanagi, and A. Iwata. An
    approach for sound source localization by complex-valued neural network. *IEICE
    Trans. Inform. Syst.*, 96(10):2257–2265, 2013.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaidyanathan and Pal [2010] P. P. Vaidyanathan and P. Pal. Sparse sensing with
    co-prime samplers and arrays. *IEEE Trans. Signal Process.*, 59(2):573–586, 2010.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valero and Habets [2017] M. L. Valero and E. A. Habets. Multi-microphone acoustic
    echo cancellation using relative echo transfer functions. In *Proc. IEEE Workshop
    Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 229–233, New Paltz, NY, 2017.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van Veen and Buckley [1988] B. D. Van Veen and K. M. Buckley. Beamforming:
    A versatile approach to spatial filtering. *IEEE Acoust., Speech, Signal Process.
    Magazine*, 5(2):4–24, 1988.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varanasi et al. [2020] V. Varanasi, H. Gupta, and R. M. Hegde. A deep learning
    framework for robust DoA estimation using spherical harmonic decomposition. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 28:1248–1259, 2020.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vargas et al. [2021] E. Vargas, J. R. Hopgood, K. Brown, and K. Subr. On improved
    training of CNN for acoustic source localisation. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 29:720–732, 2021.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varzandeh et al. [2020] R. Varzandeh, K. Adiloğlu, S. Doclo, and V. Hohmann.
    Exploiting periodicity features for joint detection and DoA estimation of speech
    sources using convolutional neural networks. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, pages 566–570, Barcelona, Spain (virtual conference),
    May 2020.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, Dec. 2017.
    arXiv:1706.03762.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecchiotti et al. [2018] P. Vecchiotti, E. Principi, S. Squartini, and F. Piazza.
    Deep neural networks for joint voice activity detection and speaker localization.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 1567–1571, Roma, Italy,
    2018.
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecchiotti et al. [2019a] P. Vecchiotti, N. Ma, S. Squartini, and G. J. Brown.
    End-to-end binaural sound localisation from the raw waveform. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 451–455, Brighton, UK,
    2019a.
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecchiotti et al. [2019b] P. Vecchiotti, G. Pepe, E. Principi, and S. Squartini.
    Detection of activity and position of speakers by using deep neural networks and
    acoustic data augmentation. *Expert Syst. with Applic.*, 134:53–65, 2019b.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vera-Diaz et al. [2018] J. M. Vera-Diaz, D. Pizarro, and J. Macias-Guarasa.
    Towards end-to-end acoustic localization using deep learning: from audio signal
    to source position coordinates. *Sensors*, 18(10):3418, 2018. ISSN 1424-8220.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vera-Diaz et al. [2020] J. M. Vera-Diaz, D. Pizarro, and J. Macias-Guarasa.
    Towards domain independence in CNN-based acoustic localization using deep cross
    correlations. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 226–230,
    Amsterdam, The Netherlands (virtual conference), 2020.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vera-Diaz et al. [2021] J. M. Vera-Diaz, D. Pizarro, and J. Macias-Guarasa.
    Acoustic source localization with deep generalized cross correlations. *Signal
    Process.*, 187:108169, Oct. 2021.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vesperini et al. [2016] F. Vesperini, P. Vecchiotti, E. Principi, S. Squartini,
    and F. Piazza. A neural network based algorithm for speaker localization in a
    multi-room environment. In *IEEE Int. Workshop Machine Learning for Signal Process.*,
    pages 1–6, Salerno, Italy, 2016.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent and Campbell [2008] E. Vincent and D. R. Campbell. Roomsimove. *GNU
    Public License*, 2008. URL [http://homepages.loria.fr/evincent/software/Roomsimove_1](http://homepages.loria.fr/evincent/software/Roomsimove_1).
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent et al. [2018] E. Vincent, T. Virtanen, and S. Gannot. *Audio Source
    Separation and Speech Enhancement*. John Wiley & Sons, 2018.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vo et al. [2015] B.-n. Vo, M. Mallick, Y. Bar-shalom, S. Coraluppi, R. Osborne,
    R. Mahler, and B.-t. Vo. Multitarget tracking. In *Wiley Encyclopedia of Electrical
    and Electronics Engineering*. 2015. ISBN 978-0-471-34608-1.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wabnitz et al. [2010] A. Wabnitz, N. Epain, C. Jin, and A. Van Schaik. Room
    acoustics simulation for multichannel microphone arrays. In *Proc. Int. Symp.
    Room Acoust. (ISRA)*, pages 1–6, Melbourne, Australia, 2010.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waibel et al. [1989] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang.
    Phoneme recognition using time-delay neural networks. *IEEE Trans. Acoust., Speech,
    Signal Process.*, 37(3):328–339, 1989.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Chen [2018] D. Wang and J. Chen. Supervised speech separation based
    on deep learning: An overview. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*,
    26(10):1702–1726, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] L. Wang, Y. Liu, L. Zhao, Q. Wang, X. Zeng, and K. Chen.
    Acoustic source localization in strong reverberant environment by parametric Bayesian
    dictionary learning. *Signal Process.*, 143:232–240, 2018.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] Q. Wang, H. Wu, Z. Jing, F. Ma, Y. Fang, Y. Wang, T. Chen,
    J. Pan, J. Du, and C.-H. Lee. The USTC-IFLYTEK system for sound event localization
    and detection of DCASE 2020 challenge. Technical report, 2020. DCASE 2020 Challenge.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2021] Q. Wang, J. Du, H.-X. Wu, J. Pan, F. Ma, and C.-H. Lee. A
    four-stage data augmentation approach to ResNet-Conformer based acoustic modeling
    for sound event localization and detection, Jan. 2021. arXiv:2101.02919.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019] Z. Wang, X. Zhang, and D. Wang. Robust speaker localization
    guided by deep learning-based time-frequency masking. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 27(1):178–188, Jan. 2019.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wichern et al. [2019] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn,
    D. Crow, E. Manilow, and J. L. Roux. Wham!: Extending speech separation to noisy
    environments. In *Proc. Interspeech Conf.*, Graz, Austria, 2019.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Woodruff and Wang [2012] J. Woodruff and D. Wang. Binaural localization of multiple
    sources in reverberant and noisy environments. *IEEE Trans. Audio, Speech, Lang.
    Process.*, 20(5):1503–1512, 2012.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021a] X. Wu, Z. Wu, L. Ju, and S. Wang. Binaural audio-visual localization.
    In *Proc. AAAI Conf. Artif. Intell.*, pages 2961–2968, virtual conference, 2021a.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021b] Y. Wu, R. Ayyalasomayajula, M. J. Bianco, D. Bharadia, and
    P. Gerstoft. Sound source localization based on multi-task learning and image
    translation network. *J. Acoust. Soc. Am.*, 150(5):3374–3386, 2021b.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2021c] Y. Wu, R. Ayyalasomayajula, M. J. Bianco, D. Bharadia, and
    P. Gerstoft. SSLIDE: sound source localization for indoors based on deep learning.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, Toronto,
    Canada (virtual conference), 2021c.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xenaki and Gerstoft [2015] A. Xenaki and P. Gerstoft. Grid-free compressive
    beamforming. *J. Acoust. Soc. Am.*, 137(4):1923–1935, 2015.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xenaki et al. [2014] A. Xenaki, P. Gerstoft, and K. Mosegaard. Compressive beamforming.
    *J. Acoust. Soc. Am.*, 136(1):260–271, 2014.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xenaki et al. [2018] A. Xenaki, J. Bünsow Boldt, and M. Græsbøll Christensen.
    Sound source localization and speech enhancement with sparse Bayesian learning
    beamforming. *J. Acoust. Soc. Am.*, 143(6):3912–3921, June 2018.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. [2019] J. Xiang, G. Zhang, and J. Hou. Online multi-object tracking
    based on feature representation and Bayesian filtering within a deep learning
    architecture. *IEEE Access*, 7:27923–27935, 2019.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. [2015] X. Xiao, S. Zhao, X. Zhong, D. L. Jones, E. S. Chng, and
    H. Li. A learning-based approach to direction of arrival estimation in noisy and
    reverberant environments. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, pages 2814–2818, Brisbane, Australia, 2015.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xinghao et al. [2021] S. Xinghao, Y. Hu, X. Zhu, and L. He. Sound event localization
    and detection based on adaptive hybrid convolution and multi-scale feature extractor.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, pages 130–134, Barcelona, Spain, November 2021. ISBN 978-84-09-36072-7.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2013] B. Xu, G. Sun, R. Yu, and Z. Yang. High-accuracy TDOA-based
    localization without time synchronization. *IEEE Trans. Parallel Distrib. Syst.*,
    24(8):1567–1576, Aug. 2013.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2021a] P. Xu, E. J. G. Arcondoulis, and Y. Liu. Acoustic source imaging
    using densely connected convolutional networks. *Mech. Syst. Signal Process.*,
    151:107370, 2021a.
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021b] Y. Xu, Y. Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-Pineda.
    Transcenter: Transformers with dense queries for multiple-object tracking, 2021b.
    arXiv:2103.15145.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. [2019] W. Xue, T. Ying, Z. Chao, and D. Guohong. Multi-beam and multi-task
    learning for joint sound event detection and localization. Technical report, 2019.
    DCASE 2019 Challenge.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. [2020] W. Xue, Y. Tong, C. Zhang, G. Ding, X. He, and B. Zhou. Sound
    event localization and detection based on multiple DoA beamforming and multi-task
    learning. In *Proc. Interspeech Conf.*, Shanghai, China (virtual conference),
    Oct. 2020.
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yalta et al. [2017] N. Yalta, K. Nakadai, and T. Ogata. Sound source localization
    using deep learning models. *J. Robotics Mechatron.*, 29(1):37–48, 2017.
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yalta et al. [2021] N. Yalta, Y. Sumiyoshi, and Y. Kawaguchi. The Hitachi DCASE
    2021 Task 3 system: handling directive interference with self attention layers.
    Technical report, 2021. DCASE 2021 Challenge.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021a] B. Yang, X. Li, and H. Liu. Supervised direct-path relative
    transfer function learning for binaural sound source localization. In *Proc. IEEE
    Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 825–829, Toronto,
    Canada (virtual conference), 2021a.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021b] B. Yang, H. Liu, and X. Li. Learning deep direct-path relative
    transfer function for binaural sound source localization. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 29:3491–3503, 2021b.
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [1994] W.-H. Yang, K.-K. Chan, and P.-R. Chang. Complex-valued neural
    network for direction of arrival estimation. *Electronics Lett.*, 30(7):574–575,
    1994.
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Xie [2015] Z. Yang and L. Xie. Enhancing sparsity and resolution via
    reweighted atomic norm minimization. *IEEE Trans. Signal Process.*, 64(4):995–1006,
    2015.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2018] Z. Yang, J. Li, P. Stoica, and L. Xie. Sparse methods for
    direction-of-arrival estimation. *Academic Press Library in Signal Process.*,
    7:509–581, 2018.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasuda et al. [2020] M. Yasuda, Y. Koizumi, S. Saito, H. Uematsu, and K. Imoto.
    Sound event localization based on sound intensity vector refined by DNN-based
    denoising and source separation. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 651–655, Barcelona, Spain (virtual conference), May
    2020.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yiwere and Rhee [2017] M. Yiwere and E. J. Rhee. Distance estimation and localization
    of sound sources in reverberant conditions using deep neural networks. *Int. J.
    Eng. Research Applic.*, 12(22):12384–12389, 2017.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youssef et al. [2013] K. Youssef, S. Argentieri, and J. Zarader. A learning-based
    approach to robust binaural sound localization. In *Proc. IEEE/RSJ Int. Conf.
    Intell. Robots Systems (IROS)*, pages 2927–2932, Tokyo, Japan, 2013.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. [2017] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen. Permutation invariant
    training of deep models for speaker-independent multi-talker speech separation.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 241–245,
    New Orleans, LA, Mar. 2017.
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zea and Laudato [2021] E. Zea and M. Laudato. On the representation of wavefronts
    localized in space-time and wavenumber-frequency domains. *JASA Express Letters*,
    1(5):054801, 2021.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zermini et al. [2016] A. Zermini, Y. Yu, Y. Xu, W. Wang, and M. D. Plumbley.
    Deep neural network based audio source separation. In *IMA Int. Conf. Math. Signal
    Process.*, Birmingham, UK, 2016.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2018] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. Mixup:
    beyond empirical risk minimization, Apr. 2018. arXiv:1710.09412.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019a] J. Zhang, W. Ding, and L. He. Data augmentation and priori
    knowledge-based regularization for sound event localization and detection. Technical
    report, 2019a. DCASE 2019 Challenge.
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019b] W. Zhang, Y. Zhou, and Y. Qian. Robust DoA estimation based
    on convolutional neural network and time-frequency masking. In *Proc. Interspeech
    Conf.*, pages 2703–2707, Graz, Austria, Sept. 2019b.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Yang [2021] Y. Zhang and Q. Yang. A survey on multi-task learning.
    *IEEE Transactions on Knowledge and Data Engineering*, 2021.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2014] Y. Zhang, Z. Ye, X. Xu, and N. Hu. Off-grid DOA estimation
    using array covariance matrix and block-sparse Bayesian learning. *Signal Process.*,
    98:197–201, 2014.
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2021] Y. Zhang, S. Wang, Z. Li, K. Guo, S. Chen, and Y. Pang.
    Data augmentation and class-based ensembled CNN-Conformer networks for sound event
    localization and detection. Technical report, 2021. DCASE 2021 Challenge.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. [2020] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong,
    and Q. He. A comprehensive survey on transfer learning. *Proceedings of the IEEE*,
    109(1):43–76, 2020.
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zotter and Frank [2019] F. Zotter and M. Frank. *Ambisonics: A Practical 3D
    Audio Theory for Recording, Studio Production, Sound Reinforcement, and Virtual
    Reality*. Springer Nature, 2019.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ünlerşen and Yaldiz [2016] M. F. Ünlerşen and E. Yaldiz. Direction of arrival
    estimation by using artificial neural networks. In *Proc. Euro. Modelling Symp.*,
    pages 242–245, Pisa, Italy, Nov. 2016.
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ünlerşen和Yaldiz [2016] M. F. Ünlerşen 和 E. Yaldiz. 使用人工神经网络的到达方向估计。见于 *Proc.
    Euro. Modelling Symp.*，第242–245页，意大利比萨，2016年11月。
