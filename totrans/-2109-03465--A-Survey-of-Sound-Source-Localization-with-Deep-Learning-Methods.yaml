- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:51:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2109.03465] A Survey of Sound Source Localization with Deep Learning Methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.03465](https://ar5iv.labs.arxiv.org/html/2109.03465)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Sound Source Localization with Deep Learning Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pierre-Amaury Grumiaux
  prefs: []
  type: TYPE_NORMAL
- en: Nantes Université, École Centrale Nantes, CNRS, LS2N
  prefs: []
  type: TYPE_NORMAL
- en: 2 chemin de la Houssinière
  prefs: []
  type: TYPE_NORMAL
- en: F-44332 Nantes, France
  prefs: []
  type: TYPE_NORMAL
- en: pierreamaury.grumiaux@gmail.com & Srđan Kitić
  prefs: []
  type: TYPE_NORMAL
- en: Orange Labs
  prefs: []
  type: TYPE_NORMAL
- en: 4 Rue du Clos Courtel
  prefs: []
  type: TYPE_NORMAL
- en: 35510 Cesson-Sévigné, France
  prefs: []
  type: TYPE_NORMAL
- en: srdan.kitic@orange.com & Laurent Girin
  prefs: []
  type: TYPE_NORMAL
- en: Univ. Grenoble Alpes, Grenoble-INP, GIPSA-lab
  prefs: []
  type: TYPE_NORMAL
- en: 11 Rue des Mathématiques
  prefs: []
  type: TYPE_NORMAL
- en: 38400 Saint-Martin-d’Hères, France
  prefs: []
  type: TYPE_NORMAL
- en: laurent.girin@grenoble-inp.fr & Alexandre Guérin
  prefs: []
  type: TYPE_NORMAL
- en: Orange Labs
  prefs: []
  type: TYPE_NORMAL
- en: 4 Rue du Clos Courtel
  prefs: []
  type: TYPE_NORMAL
- en: 35510 Cesson-Sévigné, France
  prefs: []
  type: TYPE_NORMAL
- en: alexandre.guerin@orange.com During the writing of this paper, Pierre-Amaury
    Grumiaux was also at Orange Labs, 4 Rue du Clos Courtel, F-35510 Cesson-Sévigné,
    France, and at Univ. Grenoble Alpes, Grenoble-INP, CNRS, GIPSA-lab, 11 Rue des
    Mathématiques, F-38400 Saint-Martin-d’Hères, France.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This article is a survey of deep learning methods for single and multiple sound
    source localization, with a focus on sound source localization in indoor environments,
    where reverberation and diffuse noise are present. We provide an extensive topography
    of the neural network-based sound source localization literature in this context,
    organized according to the neural network architecture, the type of input features,
    the output strategy (classification or regression), the types of data used for
    model training and evaluation, and the model training strategy. Tables summarizing
    the literature survey are provided at the end of the paper, allowing a quick search
    of methods with a given set of target characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sound source localization (SSL) is the problem of estimating the position of
    one or several sound sources relative to some arbitrary reference position, which
    is generally the position of the recording microphone array, based on the recorded
    multichannel acoustic signals. In most practical cases, SSL is simplified to the
    estimation of the sources’ direction of arrival (DoA), *i.e.*, it focuses on the
    estimation of azimuth and elevation angles, without estimating the distance to
    the microphone array (therefore, unless otherwise specified, in this article we
    use the terms “SSL” and “DoA estimation” interchangeably). SSL has numerous practical
    applications – for instance, in source separation, e.g., (Chazan et al., [2019](#bib.bib47)),
    automatic speech recognition (ASR), e.g., (Lee et al., [2016](#bib.bib176)), speech
    enhancement, e.g., (Xenaki et al., [2018](#bib.bib357)), human-robot interaction,
    e.g., (Li et al., [2016b](#bib.bib186)), noise control, e.g., (Chiariotti et al.,
    [2019](#bib.bib48)), and room acoustic analysis, e.g., (Amengual Garí et al.,
    [2017](#bib.bib8)). As detailed in the following, in this paper, we focus on sound
    sources in the audible range (typically speech and audio signals) in indoor (office
    or domestic) environments.
  prefs: []
  type: TYPE_NORMAL
- en: Although SSL is a longstanding and widely researched topic (Gerzon, [1992](#bib.bib97);
    DiBiase et al., [2001](#bib.bib70); Argentieri et al., [2015](#bib.bib11); Cobos
    et al., [2017](#bib.bib54); Benesty et al., [2008](#bib.bib18); Knapp and Carter,
    [1976](#bib.bib153); Brandstein and Ward, [2001](#bib.bib31); Nehorai and Paldi,
    [1994](#bib.bib219); Hickling et al., [1993](#bib.bib126)), it remains a very
    challenging problem to date. Traditional SSL methods are based on signal/channel
    models and signal processing (SP) techniques. Although they have shown notable
    advances in the domain over the years, they are known to perform poorly in difficult
    yet common scenarios where noise, reverberation, and several simultaneously emitting
    sound sources may be present (Blandin et al., [2012](#bib.bib27); Evers et al.,
    [2020](#bib.bib83)). In the last decade, the potential of data-driven deep learning
    (DL) techniques for addressing such difficult scenarios has received an increasing
    interest. As a result, an increasing number of SSL systems based on deep neural
    networks (DNNs) have been proposed in the recent years. Most of the reported works
    have indicated the superiority of DNN-based SSL methods over conventional (i.e.,
    SP-based) SSL methods. For example, Chakrabarty and Habets ([2017a](#bib.bib41))
    showed that, in low signal-to-noise ratio conditions, using a CNN led to a two-fold
    increase in overall DoA classification accuracy compared to using the conventional
    method called steered response power with phase transform (SRP-PHAT) (see Section
    [3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). In Perotin et al. ([2018b](#bib.bib246)), the authors
    were able to obtain a $25\%$ increase of DoA classification accuracy when using
    a convolutional recurrent neural network (CRNN) over a method based on independent
    component analysis (ICA). Finally Adavanne et al. ([2018](#bib.bib1)) proved that
    employing a CRNN can reduce the average angular error by $50\%$ in reverberant
    conditions compared to the conventional MUSIC algorithm (see Section [3](#S3 "3
    Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")).
  prefs: []
  type: TYPE_NORMAL
- en: This kind of results has further motivated the expansion of scientific papers
    on DL applied to SSL. In the meantime, there has been no comprehensive survey
    of the existing approaches, which would be very useful for researchers and practitioners
    in the domain. Although we can find reviews mostly focused on conventional methods,
    e.g., (Argentieri et al., [2015](#bib.bib11); Cobos et al., [2017](#bib.bib54);
    Evers et al., [2020](#bib.bib83); Gannot et al., [2019](#bib.bib91)), to the best
    of our knowledge only a very few have explicitly targeted SSL with DL methods.
    Ahmad et al. ([2021](#bib.bib6)) presented a short survey of several existing
    DL models and datasets for SSL before proposing a DL architecture of their own.
    Bianco et al. ([2019](#bib.bib24)) and Purwins et al. ([2019](#bib.bib261)) presented
    an interesting overview of machine learning applied to various problems in audio
    and acoustics. Nevertheless, only a short portion of each of these two reviews
    is dedicated to SSL with DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Aim of the paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of this paper is to fill this gap, and to provide a thorough survey
    of the SSL literature using DL techniques. More precisely, we examined and review
    156 papers published from 2011 to 2021\. We classify and discuss the different
    approaches in terms of characteristics of the employed methods and addressed configurations
    (e.g., single-source vs. multi-source localization setup or neural network architecture;
    the exact list is given in Section [1.3](#S1.SS3 "1.3 Outline of the paper ‣ 1
    Introduction ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    In other words, we present a taxonomy of the DL-based SSL literature published
    in the last decade. At the end of the paper, we present a summary of this survey
    in the form of four tables (one for the period 2011–2018, and one for each of
    the years 2019, 2020 and 2021). All of the methods that we reviewed are reported
    in these tables with a summary of their characteristics presented in different
    columns. This enables the reader to rapidly select the subset of methods having
    a given set of characteristics, if they are interested in that particular type
    of method.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this survey paper, we do not aim to evaluate and compare the performance
    of the different systems. Due to the large number of DNN-based SSL papers and
    the diversity of configurations, such a contribution would be very difficult and
    cumbersome (albeit very useful), especially because the discussed systems are
    often trained and evaluated on different datasets. As we will see later, listing
    and commenting on these different datasets is, however, part of our survey effort.
    Note also that we do not consider SSL systems that exploit other modalities in
    addition to sound, e.g., audio-visual systems (Ban et al., [2018](#bib.bib16);
    Wu et al., [2021a](#bib.bib352); Masuyama et al., [2020](#bib.bib208)). Finally,
    we do consider DL-based methods for joint sound event localization and detection
    (SELD), which is a combination of sound event detection (SED; here detection actually
    means classification) and SSL, and in that case, we focus on the localization
    task. In particular, we include in the review the SELD methods presented to the
    DCASE Challenge (and/or to the corresponding DCASE Workshop) in 2019, 2020 and
    2021 (see the DCASE website at [https://dcase.community/](https://dcase.community/)).
    One of the task of this challenge is precisely dedicated to SELD, which has contributed
    to make the DL-based SSL (and SED) problem a popular research topic over the recent
    years.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 General principle of DL-based SSL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43b7e23b12a5b7a471da79629c99c253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: General pipeline of a DL-based SSL system.'
  prefs: []
  type: TYPE_NORMAL
- en: The general principle of DL-based SSL methods and systems can be schematized
    with a simple pipeline, as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1.2 General
    principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"). A multichannel input signal recorded with a microphone
    array is processed by a feature extraction module to provide input features. These
    input features are fed into a DNN, which delivers an estimate of the source location
    or DoA. As discussed later in the paper, a recent trend is to skip the feature
    extraction module to directly feed the network with multichannel raw data. In
    any case, the two fundamental reasons behind the design of such SSL are detailed
    below.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, multichannel signals recorded with an array of $I$ microphones distributed
    in space contain information about the location of the source(s). Indeed, when
    the microphones are close to each other compared to their distance to the source(s),
    the microphone signal waveforms, although appearing similar from a distance, exhibit
    more or less notable and complex differences in terms of delay and amplitude,
    depending on the experimental setup. These interchannel differences are due to
    distinct propagation paths from the source to the different microphones, for both
    the direct path (line of sight between source and microphone) and the numerous
    reflections that compose the reverberation in an indoor environment. In other
    words, a source signal $s_{j}(t)$ is convolved with different room impulse responses
    (RIRs) $a_{i,j}(t)$, which depend on the source position, microphone position
    and directivity ($i$ denotes the microphone index in the array), and acoustic
    environment configuration (e.g., room shape):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle x_{i}(t)=a_{i,j}(t)\star s_{j}(t)+n_{i}(t)=\sum_{\tau=0}^{T-1}a_{i,j}(\tau)s_{j}(t-\tau)+n_{i}(t),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $x_{i}(n)$ denotes the resulting recorded signal at microphone $i$, $n_{i}(t)$
    is the noise signal at microphone $i$ (diffuse, “background” noise and possibly
    some sensor noise), and $\star$ denotes the convolution (note that we work with
    digital signals and $t$ and $\tau$ are discrete time indexes; $T$ is the effective
    length of the RIR). Therefore, the recorded signal contains information on the
    relative source-to-microphone array position. The microphone signals are often
    expressed in the time-frequency (TF) domain, using the short-term Fourier transform
    (STFT), where the convolution in Eq. ([1](#S1.E1 "In 1.2 General principle of
    DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")) is assumed to transform into a product between the STFT of
    the source signal $S_{j}(f,n)$ and the acoustic transfer function (ATF) $A_{i,j}(f)$,
    which is the (discrete) Fourier transform of the corresponding RIR and is thus
    encoding the source spatial information ($f$ denotes the frequency bin, and $n$
    is the STFT frame index) (Gannot et al., [2017](#bib.bib90); Vincent et al., [2018](#bib.bib341)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X_{i}(f,n)=A_{i,j}(f)S_{j}(f,n)+N_{i}(f,n).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'When several, say $J$, sources are present, the recorded signal is the sum
    of their contribution (plus the noise):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{i}(t)=\sum_{j=1}^{J}a_{i,j}(t)\star s_{j}(t)+n_{i}(t).$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'This latter equation is often reformulated in the TF domain in matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{X}(f,n)=\mathbf{A}(f)\mathbf{S}(f,n)+\mathbf{N}(f,n),$ |  | (4)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{X}(f,n)=[X_{1}(f,n),...,X_{I}(f,n)]^{\top}$ is the microphone
    signal vector, $\mathbf{A}(f)$ is the matrix gathering the ATFs, $\mathbf{S}(f,n)=[S_{1}(f,n),...,S_{J}(f,n)]^{\top}$
    is the source signal vector, and $\mathbf{N}(f,n)=[N_{1}(f,n),...,N_{I}(f,n)]^{\top}$
    is the noise vector. In that multi-source case, the difficulty of the SSL problem
    is that the contributions of the different sources generally overlap in time.
    SSL then requires to proceed to some kind of source clustering, which is generally
    easier to proceed in the frequency or time-frequency domain due to the natural
    sparsity of audio sources in that domain (Rickard, [2002](#bib.bib268)). In this
    paper, we do not describe the foundations of source-to-microphone propagation
    in more detail. They can be found in several references on general acoustics,
    e.g., (Jacobsen and Juhl, [2013](#bib.bib137); Rossing, [2007](#bib.bib275)),
    room acoustics, e.g., (Kuttruff, [2016](#bib.bib166)), array signal processing,
    e.g., (Brandstein and Ward, [2001](#bib.bib31); Benesty et al., [2008](#bib.bib18);
    Jarrett et al., [2017](#bib.bib140); Rafaely, [2019](#bib.bib263)), speech enhancement
    and audio source separation, e.g., (Gannot et al., [2017](#bib.bib90); Vincent
    et al., [2018](#bib.bib341)), and many papers on conventional SSL.
  prefs: []
  type: TYPE_NORMAL
- en: The second reason for designing DNN-based SSL systems is that even if the relationship
    between the information contained in the multichannel signal and the location
    of the source(s) is generally complex (especially in a multisource reverberant
    and noisy configuration, see Eqs. ([3](#S1.E3 "In 1.2 General principle of DL-based
    SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")) and ([4](#S1.E4 "In 1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"))), DNNs are
    powerful models that are able to automatically identify and exploit this relationship,
    given that they are provided with a sufficiently large number of representative
    training examples. This ability of data-driven DL methods to replace conventional
    methods based on a signal/channel model and SP techniques —or at least a part
    of them, since the feature extractor module can be based on conventional processing—
    makes them attractive for addressing problems such as SSL. While some conventional
    methods can adapt to the observed signals, e.g., Dvorkind and Gannot ([2005](#bib.bib74));
    Li et al. ([2016c](#bib.bib187)); Laufer-Goldshtein et al. ([2020](#bib.bib171));
    Li et al. ([2016b](#bib.bib186)), they are all intrinsically based on certain
    (more or less plausible) modeling assumptions, which can limit their effectiveness
    when exposed to the complexity of real-world acoustics. Deep learning models do
    not *explicitly* impose any such assumptions, and instead they efficiently adapt
    to the presented training data. This is, however, also the major drawback of the
    DNN-based approaches, as they are less generic than traditional methods. A deep
    model designed for and trained in a given configuration (e.g., a given microphone
    array geometry) will not provide satisfying localization results if the setup
    changes (Liu et al., [2018](#bib.bib195); Le Moing et al., [2021](#bib.bib173)),
    unless some relevant adaptation method can be used, which is still an open problem
    in DL in general.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Outline of the paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The remainder of the paper is organized as follows. In Section [2](#S2 "2 Acoustic
    environment and sound source configurations ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), we specify the context and scope of the survey in
    terms of the considered acoustic environment and sound source configurations.
    In Section [3](#S3 "3 Conventional SSL methods ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), we briefly present the most common conventional
    SSL methods, for two reasons: first, they are often used as a baseline for the
    evaluation of DL-based methods; and second, we will see that several types of
    features extracted by conventional methods can be used in DL-based methods. Section [4](#S4
    "4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") aims to classify the different neural network architectures
    used for SSL. Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") presents the various types of input features used
    for SSL with neural networks. In Section [6](#S6 "6 Output strategies ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"), we explain the two
    output strategies employed in DL-based SSL: classification and regression. We
    then discuss in Section [7](#S7 "7 Data ‣ A Survey of Sound Source Localization
    with Deep Learning Methods") the datasets used for training and evaluating the
    models. In Section [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), learning paradigms such as supervised or semi-supervised
    learning are discussed from the SSL perspective. Section [9](#S9 "9 Conclusions
    and perspectives ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    provides the two summary tables and concludes the paper. Note that, due to the
    large number of acronyms used in this survey paper, we provide a list of these
    acronyms in Table [1](#S1.T1 "Table 1 ‣ 1.3 Outline of the paper ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Table of acronyms.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ACCDOA | activity-coupled Cartesian direction of arrival |'
  prefs: []
  type: TYPE_TB
- en: '| AE | autoencoder |'
  prefs: []
  type: TYPE_TB
- en: '| ATF | acoustic transfer function |'
  prefs: []
  type: TYPE_TB
- en: '| ASR | automatic speech recognition |'
  prefs: []
  type: TYPE_TB
- en: '| BGRU | bidirectional gated recurrent unit |'
  prefs: []
  type: TYPE_TB
- en: '| BIR | binaural impulse response |'
  prefs: []
  type: TYPE_TB
- en: '| BRIR | binaural room impulse response |'
  prefs: []
  type: TYPE_TB
- en: '| CC | cross-correlation |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | convolutional neural network |'
  prefs: []
  type: TYPE_TB
- en: '| CRNN | convolutional recurrent neural network |'
  prefs: []
  type: TYPE_TB
- en: '| CPS | cross power spectrum |'
  prefs: []
  type: TYPE_TB
- en: '| DCASE | Detection and Classification |'
  prefs: []
  type: TYPE_TB
- en: '|  | of Acoustic Scenes and Events |'
  prefs: []
  type: TYPE_TB
- en: '| DIRHA | distant-speech interaction |'
  prefs: []
  type: TYPE_TB
- en: '|  | for robust home applications |'
  prefs: []
  type: TYPE_TB
- en: '| DL | deep learning |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | deep neural network |'
  prefs: []
  type: TYPE_TB
- en: '| DoA | direction of arrival |'
  prefs: []
  type: TYPE_TB
- en: '| DP-RTF | direct-path relative transfer function |'
  prefs: []
  type: TYPE_TB
- en: '| DRR | direct-to-reverberant ratio |'
  prefs: []
  type: TYPE_TB
- en: '| EM | expectation maximization |'
  prefs: []
  type: TYPE_TB
- en: '| ESPRIT | Estimation of Signal Parameters |'
  prefs: []
  type: TYPE_TB
- en: '|  | via Rotational Invariance Techniques |'
  prefs: []
  type: TYPE_TB
- en: '| EVD | eigenvalue decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| FFNN | feed-forward neural network |'
  prefs: []
  type: TYPE_TB
- en: '| FOA | first-order Ambisonics |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | generative adversarial network |'
  prefs: []
  type: TYPE_TB
- en: '| GCC | generalized cross-correlation |'
  prefs: []
  type: TYPE_TB
- en: '| GLU | gated linear unit |'
  prefs: []
  type: TYPE_TB
- en: '| GMM | Gaussian mixture models |'
  prefs: []
  type: TYPE_TB
- en: '| GMR | Gaussian mixture regression |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | graphical processing unit |'
  prefs: []
  type: TYPE_TB
- en: '| GRU | gated recurrent unit |'
  prefs: []
  type: TYPE_TB
- en: '| HATS | head-and-torso simulator |'
  prefs: []
  type: TYPE_TB
- en: '| HOA | higher-order Ambisonics |'
  prefs: []
  type: TYPE_TB
- en: '| HRTF | head-related transfer function |'
  prefs: []
  type: TYPE_TB
- en: '| ICA | independent component analysis |'
  prefs: []
  type: TYPE_TB
- en: '| ILD | interaural level difference |'
  prefs: []
  type: TYPE_TB
- en: '| IPD | interaural phase difference |'
  prefs: []
  type: TYPE_TB
- en: '| ITD | interaural time difference |'
  prefs: []
  type: TYPE_TB
- en: '| ISM | image source method |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | long short-term memory |'
  prefs: []
  type: TYPE_TB
- en: '| MHSA | multihead self-attention |'
  prefs: []
  type: TYPE_TB
- en: '| MLP | multiLayer Perceptron |'
  prefs: []
  type: TYPE_TB
- en: '| MOT | multi-object tracking |'
  prefs: []
  type: TYPE_TB
- en: '| MUSIC | MUltiple SIgnal Classification |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | natural language processing |'
  prefs: []
  type: TYPE_TB
- en: '| NoS | number of sources |'
  prefs: []
  type: TYPE_TB
- en: '| PHAT | PHAse Transform |'
  prefs: []
  type: TYPE_TB
- en: '| RIR | room impulse response |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | recurrent neural network |'
  prefs: []
  type: TYPE_TB
- en: '| RTF | relative transfer function |'
  prefs: []
  type: TYPE_TB
- en: '| SA | self-attention |'
  prefs: []
  type: TYPE_TB
- en: '| SCM | spatial covariance matrix |'
  prefs: []
  type: TYPE_TB
- en: '| SED | sound event detection |'
  prefs: []
  type: TYPE_TB
- en: '| SELD | sound event localization and detection |'
  prefs: []
  type: TYPE_TB
- en: '| SH | spherical harmonics |'
  prefs: []
  type: TYPE_TB
- en: '| SMIR | spherical microphone impulse response |'
  prefs: []
  type: TYPE_TB
- en: '| SMN | sequence matching network |'
  prefs: []
  type: TYPE_TB
- en: '| SP | signal processing |'
  prefs: []
  type: TYPE_TB
- en: '| SPS | spatial pseudo-spectrum |'
  prefs: []
  type: TYPE_TB
- en: '| SRP | steered power response |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | sound source localization |'
  prefs: []
  type: TYPE_TB
- en: '| STFT | short-term Fourier transform |'
  prefs: []
  type: TYPE_TB
- en: '| TCN | temporal convolutional network |'
  prefs: []
  type: TYPE_TB
- en: '| TDoA | time difference of arrival |'
  prefs: []
  type: TYPE_TB
- en: '| TF | time-frequency |'
  prefs: []
  type: TYPE_TB
- en: '| VAD | voice activity detection |'
  prefs: []
  type: TYPE_TB
- en: '| VAE | variational autoencoder |'
  prefs: []
  type: TYPE_TB
- en: '| WDO | W-disjoint orthogonality |'
  prefs: []
  type: TYPE_TB
- en: 2 Acoustic environment and sound source configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SSL has been applied in different configurations, depending on the application.
    In this section, we specify the scope of our survey in terms of acoustic environment
    (noisy, reverberant, or even multi-room) and the nature of the considered sound
    sources (their type, number, and static/mobile status).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Acoustic environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this paper, we focus on SSL in an indoor environment, *i.e.*, when the microphone
    array and the sound source(s) are present in a closed room, generally of moderate
    size, typically an office room or a domestic environment. This implies reverberation:
    in addition to the direct source-to-microphone propagation path, the recorded
    sound contains many other multi-path components of the same source. All of these
    components form the room impulse response (RIR), which is defined for each source
    position and microphone array position (including orientation) and for a given
    room configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a general manner, the presence of reverberation is seen as a notable perturbation
    that makes SSL more difficult compared to the simpler (but somewhat unrealistic)
    anechoic case, which assumes the absence of reverberation, as is obtained in the
    free field propagation setup. Another important adverse factor to take into account
    in SSL is noise. On the one hand, noise can come from interfering sound sources
    in the surrounding environment: TV, background music, pets, street noise passing
    through open or closed windows, etc. Often, noise is considered as diffuse, *i.e.*,
    it does not originate from a clear direction. On the other hand, the imperfections
    of the recording devices are another source of noise that are generally considered
    as artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: Early works on using neural networks for DoA estimation most often considered
    direct-path propagation only (the anechoic setting), e.g., (Rastogi et al., [1987](#bib.bib265);
    Goryn and Kaveh, [1988](#bib.bib102); Jha et al., [1988](#bib.bib144); Jha and
    Durrani, [1989](#bib.bib142), [1991](#bib.bib143); Falong et al., [1993](#bib.bib85);
    Yang et al., [1994](#bib.bib370); Southall et al., [1995](#bib.bib301); El Zooghby
    et al., [2000](#bib.bib76)), though a model of the acoustical environment was
    used to generate simulated data to train the neural network of Datum et al. ([1996](#bib.bib63)).
    Most of these works are from the pre-deep-learning era, using “shallow” neural
    networks with only one or two hidden layers Goodfellow et al. ([2016](#bib.bib101)).
    We do not detail these works in our survey, although we acknowledge them as pioneering
    contributions to the neural network-based DoA estimation problem. A few more recent
    works based on more “modern” neural network architectures also focused on anechoic
    propagation only, or did not consider sound sources in the audible bandwidth (Liu
    et al., [2018](#bib.bib195); Ünlerşen and Yaldiz, [2016](#bib.bib387); Bialer
    et al., [2019](#bib.bib22); Elbir, [2020](#bib.bib77); Choi and Chang, [2020](#bib.bib50)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Source types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the SSL literature, a great proportion of systems focuses on localizing speech
    sources because of their importance in related tasks such as speech enhancement
    or speech recognition. Examples of speaker localization systems can be found in
    papers by Chakrabarty and Habets ([2019b](#bib.bib44)); Grumiaux et al. ([2021b](#bib.bib106));
    He et al. ([2021a](#bib.bib121)); Hao et al. ([2020](#bib.bib116)). In such systems,
    the neural networks are trained to estimate the DoA of speech sources so that
    they are somewhat specialized in this type of source. Other systems, in particular
    those participating in the DCASE Challenge, consider a variety of sound source
    types (Politis et al., [2020b](#bib.bib254)). Depending on the challenge task
    and its corresponding dataset, these methods are capable of localizing alarms,
    crying babies, crashes, barking dogs, female/male screams, female/male speech,
    footsteps, knockings on doors, ringings, phones, and piano sounds. Note that the
    localization of such sources, even if they overlap in time, is not necessarily
    a more difficult problem than localization of several overlapping speakers, since
    the former usually have distinct spectral characteristics that neural models may
    exploit for better detection and localization.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Number of sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of sources (NoS) in a recorded mixture signal is an important parameter
    for SSL. In the SSL literature, the NoS might be considered as known (as a working
    hypothesis). Alternatively, it can be estimated along with the source location,
    in which case the SSL problem is a combination of detection and localization.
    Examples of conventional (non-deep) SSL works including NoS estimation can be
    found in papers by Arberet et al. ([2009](#bib.bib10)); Landschoot and Xiang ([2019](#bib.bib169)).
  prefs: []
  type: TYPE_NORMAL
- en: Many DNN-based works have considered only one source to localize, as it is the
    simplest scenario to address, e.g., (Perotin et al., [2018b](#bib.bib246); Bologni
    et al., [2021](#bib.bib29); Liu et al., [2021](#bib.bib193)). We refer to this
    scenario as single-source SSL. In this case, the networks are trained and evaluated
    on datasets with only at most one active source (a source is said to be active
    when emitting sound and inactive otherwise). In terms of NoS, we thus have here
    either 1 or 0 active source. The activity of the source in the processed signal,
    which generally contains background noise, can be artificially controlled, *i.e.*,
    the knowledge of source activity is a working hypothesis. This is a reasonable
    approach at training time when using synthetic data, but it is quite unrealistic
    at test time on real-world data. Alternatively, the source activity can be estimated,
    which is a more realistic approach at test time. In the latter case, there are
    two ways of dealing with the source activity detection problem. The first is to
    employ a source detection algorithm beforehand and then apply the SSL method only
    on the signal portions with an active source. For example, a voice activity detection
    (VAD) technique has been used in the SSL systems of Kim and Hahn ([2018](#bib.bib147));
    Chang et al. ([2018](#bib.bib45)); Sehgal and Kehtarnavaz ([2018](#bib.bib292));
    Li et al. ([2016d](#bib.bib188)). The other way is to detect the activity of the
    source at the same time as the localization algorithm. For example, an additional
    neuron was added by Yalta et al. ([2017](#bib.bib366)) to the output layer of
    their DNN, which outputted $1$ when no source was active (in that case, all other
    localization neurons were trained to output $0$), and $0$ otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-source localization is a much more difficult problem than single-source
    SSL. Current state-of-the-art DL-based methods address multi-source SSL in adverse
    environments. In this survey, we consider as multi-source localization the scenario
    in which several sources overlap in time (*i.e.*, they are simultaneously emitting),
    regardless of their type (e.g., there could be several speakers or several distinct
    sound events). The specific case of a multi-speaker conversation with or without
    speech overlap is strongly connected to the speaker diarization problem (“who
    speaks when?”) (Tranter and Reynolds, [2006](#bib.bib324); Anguera et al., [2012](#bib.bib9);
    Park et al., [2021b](#bib.bib241)). Speaker localization, diarization, and (speech)
    source separation are intrinsically connected problems, as the information retrieved
    from solving each of them can be useful for addressing the others (Vincent et al.,
    [2018](#bib.bib341); Kounades-Bastian et al., [2017](#bib.bib156); Jenrungrot
    et al., [2020](#bib.bib141)). An investigation of these connections is beyond
    the scope of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: In the multi-source scenario, the source detection problem transposes to a source
    counting problem, but the same considerations as in the single-source scenario
    hold. In some works, the knowledge of the NoS is a working hypothesis, e.g., (Grumiaux
    et al., [2021b](#bib.bib106); Ma et al., [2015](#bib.bib201); He et al., [2019a](#bib.bib120);
    Perotin et al., [2019b](#bib.bib248); Fahim et al., [2020](#bib.bib84); Bohlender
    et al., [2021](#bib.bib28); Grumiaux et al., [2021a](#bib.bib105)) and the sources’
    DoA can be directly estimated. If the NoS is unknown, one can apply a source counting
    system beforehand, e.g., with a dedicated DNN (Grumiaux et al., [2020](#bib.bib104)).
    For example, Tian ([2020](#bib.bib323)) trained a separate neural network to estimate
    the NoS in the recorded mixture signal, after which he used this information along
    with the output of the DoA estimation neural network. Alternatively, the NoS can
    be estimated alongside the DoAs, as in the single-source scenario, based on the
    SSL network output. When using a classification paradigm, the network output generally
    predicts the probability of the presence of a source within each discretized region
    of the space (see Section [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")). One can thus set a threshold on this
    estimated probability, which implicitly provides source counting.¹¹1Note that
    this problem is common to DL-based multi-source SSL methods and conventional methods
    for which a source activity profile is estimated and peak-picking algorithms are
    typically used to select the active sources. Otherwise, the ground-truth or estimated
    NoS is typically used to select the corresponding number of classes having the
    highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, several DNN-based systems were purposefully designed to estimate the
    NoS alongside the DoAs. For example, the method proposed by Nguyen et al. ([2020a](#bib.bib221))
    uses a neural architecture with two output branches: the first branch is used
    to estimate the NoS (up to four sources; the problem is formulated as a classification
    task), while the second branch is used to classify the azimuth into several regions.
    In the same spirit, we can mention the numerous systems presented at the DCASE
    Challenge, in which the SED task, jointly conducted with SSL, intrinsically provides
    an estimate of the NoS. Note that many DCASE Challenge candidate systems will
    be reviewed in the core of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Moving sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Source tracking is the problem of estimating the evolution of the sources’ position(s)
    over time, especially when the sources are mobile. In this survey paper, we do
    not address the problem of tracking on its own, which is usually done in a separate
    algorithm using the sequence of DoA estimates obtained by applying SSL on successive
    time windows (Vo et al., [2015](#bib.bib342)). Still, several DL-based SSL systems
    have been shown to produce more accurate localization of moving sources when they
    were trained on a dataset that includes this type of source (Adavanne et al.,
    [2019c](#bib.bib4); Diaz-Guerra et al., [2021a](#bib.bib68); Guirguis et al.,
    [2020](#bib.bib107); He et al., [2021b](#bib.bib122)). In other cases, as the
    number of real-world datasets with moving sources is limited and the simulation
    of signals with moving sources is cumbersome, a number of systems trained on static
    sources have been shown to retain fair to good performance for moving sources,
    e.g., (Opochinsky et al., [2021](#bib.bib232); Grumiaux et al., [2021a](#bib.bib105);
    Sundar et al., [2020](#bib.bib309)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Conventional SSL methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before the advent of DL, a set of signal processing techniques were developed
    to address SSL. A detailed review of these techniques was made by DiBiase et al.
    ([2001](#bib.bib70)). A review in the specific robotics context was made by Argentieri
    et al. ([2015](#bib.bib11)). In this section, we briefly present the most common
    conventional SSL methods. As briefly stated in the introduction, the reason for
    this is twofold: first, conventional SSL methods are often used as baselines for
    DL-based methods; and second, many DL-based SSL methods use input features extracted
    with conventional methods (see Section [5](#S5 "5 Input features ‣ A Survey of
    Sound Source Localization with Deep Learning Methods")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the geometry of the microphone array is known, DoA estimation can be performed
    by estimating the time-difference of arrival (TDoA) of the sources between the
    microphones (Xu et al., [2013](#bib.bib361)). The generalized cross-correlation
    with phase transform (GCC-PHAT) is one of the most employed method when dealing
    with a 2-microphone array (Knapp and Carter, [1976](#bib.bib153)). It is computed
    as the inverse Fourier transform of a weighted version of the cross-power spectrum
    (CPS) between the signals of the two microphones:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r_{1,2}(\tau)=\sum_{f=0}^{F-1}\frac{X_{1}(f)X_{2}(f)^{*}}{&#124;X_{1}(f)X_{2}(f)^{*}&#124;}e^{j2\pi\frac{f\tau}{N}},$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $X_{i}(f)$ are the $N$-point Fourier transform of the microphone signals
    $x_{i}(t)$, and $X_{1}(f)X_{2}(f)^{*}$ is the CPS (^∗ denotes the complex conjugate).
    The TDoA estimate is then obtained by finding the time delay between the microphone
    signals that maximizes the GCC-PHAT function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\tau}=\arg\max_{\tau}r_{1,2}(\tau).$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: The GCC approach has been extended to arrays with more than two microphones,
    showing in particular that the localization could be improved by taking advantage
    of the multiple microphone pairs (DiBiase et al., [2001](#bib.bib70); Benesty
    et al., [2008](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Building an acoustic power map $P(\mathbf{x})$, with $\mathbf{x}$ the spatial
    coordinates, usually a regular grid, is another way to retrieve the DoA of one
    or multiple sources, as local maxima of this map mainly correspond to the sources’
    DoA. The Steered-Response Power (SRP) map has been extensively used: it consists
    in pointing delay and sum beamformers towards each of the candidate grid positions
    and measuring the energy that arises from these directions. Its PHAT version,
    which reveals more robust to reverberation, is certainly the most popular. Practically,
    it can be derived from the average of the GCC-PHAT computed on all microphone
    pairs (DiBiase et al., [2001](#bib.bib70)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(\mathbf{x})=\sum_{m_{1}=1}^{M}\sum_{m_{2}=m_{1}+1}^{M}r_{1,2}(\tau_{m_{1},m_{2}}(\mathbf{x})),$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\tau_{m_{1},m_{2}}(\mathbf{x})$ is the delay between the microphones
    $m_{1}$ and $m_{2}$ associated to the spatial position $\mathbf{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to building the SRP-based acoustic map – which happens to be
    computationally expensive as it usually amounts to a grid search – is localization
    by exploiting the sound intensity. The use of sound intensity for source localization
    has a long history, e.g., (Nehorai and Paldi, [1994](#bib.bib219); Hickling et al.,
    [1993](#bib.bib126); Jarrett et al., [2010](#bib.bib139); Tervo, [2009](#bib.bib320);
    Raangs and Druyvesteyn, [2002](#bib.bib262); Basten et al., [2008](#bib.bib17)).
    In favorable acoustic conditions, sound intensity is parallel to the direction
    of the propagating sound wave (see Section [5.5](#S5.SS5 "5.5 Intensity-based
    features ‣ 5 Input features ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")), and hence the DoA can be efficiently estimated. Unfortunately,
    its accuracy quickly degrades in the presence of acoustic reflections (Daniel
    and Kitić, [2020](#bib.bib62)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Subspace methods are another classical family of localization algorithms. These
    methods rely on the computation of the (time-averaged) CPS matrix $\mathbf{R}(f)$
    defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{R}(f)=\sum_{n=1}^{N}\mathbf{X}(f,n)\mathbf{X}(f,n)^{H},$ |  |
    (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{X}(f,n)$ is the STFT (or more generally a local discrete Fourier
    transform) of the multichannel signal vector defined in ([4](#S1.E4 "In 1.2 General
    principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) (^H denotes the Hermitian operator), and its eigenvalue
    decomposition (EVD). Assuming that the target source signals and noise are uncorrelated,
    the multiple signal classification (MUSIC) method (Schmidt, [1986](#bib.bib288))
    applies EVD to estimate the signal and noise subspaces. After Eq. ([4](#S1.E4
    "In 1.2 General principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound
    Source Localization with Deep Learning Methods")), the signal subspace bases are
    assumed to correspond to the columns of the mixing matrix $\mathbf{A}(f)$, which
    are the multichannel ATFs of the sources (often referred to as *steering vectors*
    in this context). The signal or noise subspace bases are then used to probe a
    given direction for the presence of a source, i.e. apply *spatial filtering* or
    *beamforming* (Van Veen and Buckley, [1988](#bib.bib328); Benesty et al., [2008](#bib.bib18)).
    This time-demanding search can be relaxed using the Estimation of Signal Parameters
    via Rotational Invariance Technique (ESPRIT) algorithm (Roy and Kailath, [1989](#bib.bib276)),
    which exploits the structure of the source subspace to directly infer the source
    DoA. However, this often comes at the cost of producing less accurate predictions
    than MUSIC (Mabande et al., [2011](#bib.bib203)). MUSIC and ESPRIT assume narrowband
    signals, although wideband extensions have been proposed, e.g., (Dmochowski et al.,
    [2007](#bib.bib71); Hogg et al., [2021](#bib.bib130)). Subspace methods are robust
    to noise and can produce highly accurate estimates, but they are sensitive to
    reverberation.
  prefs: []
  type: TYPE_NORMAL
- en: Methods based on probabilistic generative mixture models have been proposed
    by, e.g., Roman and Wang ([2008](#bib.bib272)); Mandel et al. ([2009](#bib.bib205));
    May et al. ([2011](#bib.bib209)); Woodruff and Wang ([2012](#bib.bib351)); Schwartz
    and Gannot ([2013](#bib.bib289)); Dorfan and Gannot ([2015](#bib.bib72)); Li et al.
    ([2017](#bib.bib189)). Typically, the models are variants of Gaussian mixture
    models (GMMs), with one Gaussian component per source to be localized or per candidate
    source position. In a very few papers (e.g., (May et al., [2011](#bib.bib209))),
    the model is trained offline with a dedicated training dataset. But most often,
    the model parameters are directly estimated “at test time,” that is using the
    multichannel signal containing the sources to localize. This is done by maximizing
    the data likelihood function with histogram-based or expectation-maximization
    (EM) algorithms exploiting the sparsity of sound sources in the time-frequency
    (TF) domain (Rickard, [2002](#bib.bib268)), which can be computationally intensive.
    A GMM variant functioning directly in regression mode, *i.e.*, a form of Gaussian
    mixture regression (GMR), was proposed for single-source localization by Deleforge
    and Horaud ([2012](#bib.bib65)) and later extended to multi-source localization
    (and possibly separation) (Deleforge et al., [2013](#bib.bib66), [2015](#bib.bib67)).
    The GMR is locally linear but globally non-linear and the estimation of the model
    parameters is done offline on training data. Hence the spirit is close to DNN-based
    SSL. White noise signals convolved with synthetic RIRs were used for training.
    The method was shown to generalize well to speech signals, which are sparser than
    noise in the TF domain, thanks to the use of a latent variable modeling the signal
    activity in each TF bin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixture models are strongly connected to Bayesian inference, which considers
    the posterior distribution of model parameters given the observed data (hence
    involving both the likelihood function and a prior distribution of the model parameters).
    Escolano et al. ([2014](#bib.bib81)) considered applying Bayesian inference on
    a Laplacian source mixture model, using GCC-PHAT features in a two-microphone
    array set-up. Interestingly, they used two levels of Bayesian inference: one for
    the estimation of the NoS (which is an hyper-parameter of the model), using Bayesian
    model selection, and one for the estimation of the model parameters (and thus
    the corresponding source DoAs), using posterior distribution evaluation. In this
    work, the evaluation of the involved distributions was done with sampling techniques,
    e.g., Markov Chain Monte Carlo (MCMC) methods. The same methodology was further
    applied by Bush and Xiang ([2018](#bib.bib33)) with a coprime array consisting
    of two superimposed, spatially undersampled, uniform linear arrays (Vaidyanathan
    and Pal, [2010](#bib.bib326)), and by Landschoot and Xiang ([2019](#bib.bib169))
    in the spherical harmonics domain using a spherical microphone array (see Section [5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).'
  prefs: []
  type: TYPE_NORMAL
- en: Compressive sensing and sparse recovery methods are widely used in acoustics
    for different purposes Gerstoft et al. ([2018](#bib.bib96)); Xenaki et al. ([2014](#bib.bib356)),
    including SSL Yang et al. ([2018](#bib.bib372)). The main premise is that many
    high-dimensional signals admit a low-dimensional representation, which can be
    viewed through, e.g., *sparse synthesis* Candes et al. ([2006](#bib.bib35)) or
    *sparse analysis* Nam et al. ([2013](#bib.bib215)) model. Concerning the SSL problem,
    the sparsity assumption is usually assumed in the spatial (or spatial beam) domain,
    e.g., Chardon and Daudet ([2012](#bib.bib46)); Noohi et al. ([2013](#bib.bib228));
    Fortunati et al. ([2014](#bib.bib87)); Kitić et al. ([2014](#bib.bib152)); Gerstoft
    et al. ([2016](#bib.bib95)), and the resulting problem is addressed by convex
    optimization, greedy or Bayesian methods, e.g., Foucart and Rauhut ([2013](#bib.bib88));
    Gerstoft et al. ([2018](#bib.bib96)). This concept has lead to prominent localization
    methods achieving remarkable performance. Nonetheless, despite their strong theoretical
    guarantees, compressive sensing methods suffer from two drawbacks. For one, it
    is usually required that the sources coincide with points of some pre-defined
    grid, although grid-free methods have been proposed in some specific cases, e.g.,
    Xenaki and Gerstoft ([2015](#bib.bib355)); Yang and Xie ([2015](#bib.bib371)).
    The second issue is shared with other conventional methods, *i.e.*, the strong
    modeling assumptions reflected in, for example, the known structure of the (sub-Gaussian)
    dictionary matrix. Dictionary learning techniques have been proposed to alleviate
    the latter problem to some extent, e.g., Wang et al. ([2018](#bib.bib346)); Hahmann
    et al. ([2021b](#bib.bib114)); Zea and Laudato ([2021](#bib.bib377)). Sparse Bayesian
    learning (SBL) is a combination of the Bayesian framework with the principles
    of sparse representations and compressed sensing. It usually involves using sparse
    arrays such as the coprime array mentioned above and nested arrays (Pal and Vaidyanathan,
    [2010](#bib.bib234)). SBL has been used for SSL by, e.g., Nannuru et al. ([2018](#bib.bib216));
    Zhang et al. ([2014](#bib.bib383)); Liu et al. ([2012](#bib.bib194)); Gerstoft
    et al. ([2016](#bib.bib95)); Xenaki et al. ([2018](#bib.bib357)); Ping et al.
    ([2020](#bib.bib252)).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ICA is a class of algorithms aimed at retrieving the different source
    signals comprising a mixture by assuming and exploiting their mutual statistical
    independence. ICA has most often been used in audio processing for blind source
    separation, but it has also proven to be useful for multi-source SSL (Sawada et al.,
    [2003](#bib.bib286)). As briefly stated before, in the multi-source scenario,
    SSL is closely related to the source separation problem, since localization can
    help separation, and separation can help localization (Gannot et al., [2017](#bib.bib90);
    Vincent et al., [2018](#bib.bib341)).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Neural network architectures for SSL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the neural network architectures that have been
    proposed in the literature to address the SSL problem. However, we do not present
    the basics of these neural networks since they have been extensively described
    in the general DL literature, e.g., (LeCun et al., [2015](#bib.bib175); Goodfellow
    et al., [2016](#bib.bib101); Chollet, [2017](#bib.bib51)). The design of DNNs
    for a given application often requires investigating (and possibly combining)
    different architectures and tuning their hyperparameters. This was the case for
    SSL over the last decade, and the evolution of DL-based SSL techniques has followed
    the general evolution of DNNs toward more and more complex architectures or new
    efficient models adopted by the DL and SP communities at large, i.e., largely
    beyond the SSL problem (e.g., attention models). In other words, the DNN architectures
    used in SSL are often inherited from other works in other (connected or more distant)
    domains, simply because they were shown to work well on audio signals or other
    types or signals. In the same spirit, different models are often combined (in
    parallel and/or sequentially).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have thus organized the presentation according to the type of layers used
    in the networks, with a progressive and “inclusive” approach in terms of complexity:
    a network within a given category can contain layers from another previously presented
    category. We thus first present systems based on feedforward neural networks (FFNNs).
    We then focus on convolutional neural networks (CNNs) and recurrent neural networks
    (RNNs), which generally incorporate some feedforward layers. Next, we review architectures
    combining CNNs with RNNs, namely convolutional recurrent neural networks (CRNNs).
    Then, we focus on neural networks with residual connections and with attention
    mechanisms. Finally, we present SSL systems with an encoder-decoder architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Feedforward neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2cb33b1c190e84d0bd100d52db28aae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The MLP architecture used by Takeda *et al.* in several papers (Takeda
    and Komatani, [2016b](#bib.bib316), [a](#bib.bib315), [2017](#bib.bib317); Takeda
    et al., [2018](#bib.bib318)). Multiple subband feedforward layers, indexed by
    $w$, are trained to extract features from the CPS matrix eigenvectors $\mathbf{e}_{w,i}$,
    which are used as directional activation functions. The obtained subband vectors
    $\mathbf{X}_{2,w}$ are integrated across subbands progressively via other feedforward
    layers, giving $\mathbf{X}_{3,w}$ and then $\mathbf{X}_{4}$. The output layer
    finally classifies its input in one of the candidate DoAs (the entries of the
    vector $\mathbf{X}_{5}$). Note: Reprinted from (Takeda and Komatani, [2016a](#bib.bib315));
    copyright by IEEE; reprinted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The FFNN was the first and simplest type of artificial neural network to be
    designed. In such a network, data move in one direction from the input layer to
    the output layer, possibly via a series of hidden layers (Goodfellow et al., [2016](#bib.bib101);
    LeCun et al., [2015](#bib.bib175)). Non-linear activation functions are usually
    used after each layer (possibly except for the output layer). While this definition
    of FFNN is very general and may include architectures such as CNNs (discussed
    in the next subsection), here we mainly focus on architectures made of fully-connected
    layers known as Perceptron and Multi-Layer Perceptron (MLP) (Goodfellow et al.,
    [2016](#bib.bib101); LeCun et al., [2015](#bib.bib175)). A Perceptron has no hidden
    layer, while the notion of MLP is a bit ambiguous: some authors state that an
    MLP has one hidden layer, while others allow more hidden layers. In this paper,
    we call an MLP an FFNN with one or more hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: A few pioneering SSL methods using shallow neural networks (Perceptron or 1-hidden
    layer MLP) and applied in “unrealistic” setups (e.g., assuming direct-path sound
    propagation only) have been briefly mentioned in Section [2.1](#S2.SS1 "2.1 Acoustic
    environments ‣ 2 Acoustic environment and sound source configurations ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"). One of the first uses
    of an MLP for SSL was proposed by Kim and Ling ([2011](#bib.bib149)), who actually
    considered several MLPs. One network estimates the NoS, after which a distinct
    network is used for SSL for each considered NoS. The authors evaluated their method
    on reverberant data even though they assumed an anechoic setting. Tsuzuki et al.
    ([2013](#bib.bib325)) proposed using a complex-valued MLP in order to process
    complex two-microphone-based features, which led to better results than using
    a real-valued MLP. Youssef et al. ([2013](#bib.bib375)) also used an MLP to estimate
    the azimuth of a sound source from a binaural recording made with a robot head.
    The interaural time difference (ITD) and the interaural level difference (ILD)
    values (see Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) were separately fed into the input layer and were
    each processed by a specific set of neurons. A single-hidden-layer MLP was used
    by Xiao et al. ([2015](#bib.bib359)), taking GCC-PHAT-based features as inputs
    and tackling SSL as a classification problem (see Section [8](#S8 "8 Learning
    strategies ‣ A Survey of Sound Source Localization with Deep Learning Methods")),
    which showed an improvement over conventional methods on simulated and real data.
    A similar approach was proposed by Vesperini et al. ([2016](#bib.bib339)), but
    the localization was done by regression in the horizontal plane.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, MLPs with deeper architecture (*i.e.*, more hidden layers) have also
    been investigated for SSL. Roden et al. ([2015](#bib.bib271)) compared the performance
    of an MLP with two hidden layers and different input types, the number of hidden
    neurons being linked to the type of input features (see Section [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")
    for more details). Yiwere and Rhee ([2017](#bib.bib374)) used an MLP with three
    hidden layers (tested with different numbers of neurons) to output source azimuth
    and distance estimates. An MLP with four hidden layers was tested by He et al.
    ([2018a](#bib.bib118)) for multi-source localization and speech/non-speech classification,
    showing similar results as a 4-layer CNN (see Section [4.2](#S4.SS2 "4.2 Convolutional
    neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")).
  prefs: []
  type: TYPE_NORMAL
- en: Ma et al. ([2015](#bib.bib201)) proposed using a different MLP for different
    frequency sub-bands, with each MLP having eight hidden layers. This idea is based
    on the assumption that, in the presence of multiple sources, each frequency band
    is mostly dominated by a single source, which enables the training to be done
    exclusively on single-source data. The output of each sub-band MLP corresponds
    to a probability distribution on azimuth regions, and the final azimuth estimations
    are obtained by integrating the probability values over the frequency bands. Another
    system in the same vein was proposed by Takeda *et al.* in several papers (Takeda
    and Komatani, [2016b](#bib.bib316), [a](#bib.bib315), [2017](#bib.bib317); Takeda
    et al., [2018](#bib.bib318)). In these works, the eigenvectors of the recorded
    signal interchannel correlation matrix were separately fed per frequency band
    into parallel branches of the network, particularly into specific fully-connected
    layers. Then, several additional fully-connected layers progressively integrated
    the frequency-dependent outputs (see Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Feedforward
    neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")). The authors showed that this specific
    architecture outperforms a more conventional 7-layer MLP and the classical MUSIC
    algorithm on anechoic and reverberant single- and multi-source signals. Opochinsky
    et al. ([2019](#bib.bib231)) proposed a small 3-layer MLP to estimate the azimuth
    of a single source using the relative transfer function (RTF, see Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Relative transfer function (RTF) ‣ 5.1 Inter-channel features ‣ 5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    of the signal. Their approach is weakly supervised since one part of the loss
    function is computed without the ground truth DoA labels (see Section [8](#S8
    "8 Learning strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")).
  prefs: []
  type: TYPE_NORMAL
- en: An indirect use of an MLP was explored by Pak and Shin ([2019](#bib.bib233)),
    who used a 3-layer MLP to enhance the interaural phase difference (IPD) (see Section [5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    of the input signal, which was then used for DoA estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Convolutional neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs are a popular class of DNNs widely used for pattern recognition due to
    their property of being translation equivariant (Cohen et al., [2019](#bib.bib56);
    Goodfellow et al., [2016](#bib.bib101)). They have been successfully applied to
    various tasks, such as image classification, e.g., (Krizhevsky et al., [2017](#bib.bib164)),
    natural language processing (NLP), e.g., (Kim, [2014](#bib.bib148)) or automatic
    speech recognition, e.g., (Waibel et al., [1989](#bib.bib344)). CNNs have also
    been used for SSL, as detailed below.
  prefs: []
  type: TYPE_NORMAL
- en: 'To our knowledge, Hirvonen ([2015](#bib.bib128)) was the first to use a CNN
    for SSL. He employed this architecture to classify an audio signal containing
    one speech or musical source into one of eight spatial regions (see Fig. [3](#S4.F3
    "Figure 3 ‣ 4.2 Convolutional neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    This CNN is composed of four convolutional layers to extract feature maps from
    multichannel magnitude spectrograms (see Section [5](#S5 "5 Input features ‣ A
    Survey of Sound Source Localization with Deep Learning Methods")), followed by
    four fully-connected layers for classification. Classical pooling is not used
    because, according to the author, it does not seem relevant for audio representations.
    Instead, a 4-tap stride with a 2-tap overlap is used to reduce the number of parameters.
    This approach shows good performance on single-source signals and is capable of
    adapting to different configurations without hand-engineering. However, two topical
    issues of such a system were pointed out by the author: the robustness of the
    network with respect to a shift in source location, and the difficulty of interpreting
    the hidden features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/522bceba108f1b7791eea3261f505adf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The CNN architecture proposed by Hirvonen ([2015](#bib.bib128)) for
    SSL. The input is an 8-channel signal. For each short-term frame, the 8 magnitude
    spectra (of $128$ frequency bins) are concatenated to form a $1024\times 1$ tensor,
    which is fed into a series of four convolutional layers with $500$ or $600$ learnable
    kernels. The extracted features then pass through several feedforward layers containing
    $500$ or $300$ neurons. The output layer contains $8$ neurons (or $16$ if the
    source type is also considered) and estimates the probability of a source being
    present in eight candidate DoAs using a softmax activation function. Note: Reprinted
    from (Hirvonen, [2015](#bib.bib128)); copyright by the author; reprinted with
    permission.'
  prefs: []
  type: TYPE_NORMAL
- en: Chakrabarty and Habets also designed a CNN to predict the azimuth of one (Chakrabarty
    and Habets, [2017a](#bib.bib41)) or two (Chakrabarty and Habets, [2019b](#bib.bib44),
    [2017b](#bib.bib42)) speakers in reverberant environments. The input features
    are the multichannel short-time Fourier transform (STFT) phase spectrograms (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). In (Chakrabarty and Habets, [2017a](#bib.bib41)), they
    proposed using three successive convolutional layers with $64$ filters of size
    $2\times 2$ to consider neighboring frequency bands and microphones. In (Chakrabarty
    and Habets, [2017b](#bib.bib42)), they reduced the filter size to $2\times 1$
    ($1$ in the frequency axis) because of the W-disjoint orthogonality (WDO) assumption
    for speech signals, which assumes that several speakers are not simultaneously
    active in a same TF bin (Rickard, [2002](#bib.bib268)). In (Chakrabarty and Habets,
    [2019b](#bib.bib44)), they demonstrated that for an $M$-microphone array, the
    optimal number of convolutional layers for exploiting phase correlations between
    the neighboring microphones is $M-1$.
  prefs: []
  type: TYPE_NORMAL
- en: He et al. ([2018a](#bib.bib118)) compared a 4-layer MLP and a 4-layer CNN for
    the multi-speaker detection and localization task. The results showed similar
    accuracy for both architectures. A deeper architecture was proposed by Yalta et al.
    ([2017](#bib.bib366)), with 11 to 20 convolutional layers depending on the experiments.
    These deeper CNNs showed robustness against noise compared to MUSIC, as well as
    smaller training time, but this was partly due to the presence of residual blocks
    (see Section [4.5](#S4.SS5 "4.5 Residual neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    A similar architecture was presented by He et al. ([2018b](#bib.bib119)), with
    many convolutional layers and some residual blocks, although with a specific multi-task
    configuration. The end of the network was split into two convolutional branches,
    one for azimuth estimation, and the other for speech/non-speech signal classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'While most localization systems aim to estimate the azimuth or both the azimuth
    and elevation, Thuillier et al. ([2018](#bib.bib322)) investigated the estimation
    of only the elevation angle using a CNN with binaural input features: the ipsilateral
    and contralateral head-related transfer function (HRTF) magnitude responses (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). Vera-Diaz et al. ([2018](#bib.bib336)) chose to apply
    a CNN directly on raw multichannel waveforms, assembled side by side as an image,
    to predict the Cartesian coordinates $(x,y,z)$ of a single static or moving speaker.
    The successive convolutional layers contain around a hundred filters from size
    $7\times 7$ for the first layers to $3\times 3$ for the last layer. Ma and Liu
    ([2018](#bib.bib202)) also used a CNN to perform regression, but they used the
    CPS matrix as an input feature (see Section [5](#S5 "5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")). To estimate both the
    azimuth and elevation, Nguyen et al. ([2018](#bib.bib220)) used a relatively small
    CNN (two convolutional layers) in regression mode, with binaural input features.
    A similar approach was considered by Sivasankaran et al. ([2018](#bib.bib299))
    for speaker localization based on a CNN. They showed that injecting a speaker
    identifier, particularly a mask estimated for the speaker uttering a given keyword,
    alongside the binaural features at the input layer improved the DoA estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: A joint VAD and DoA estimation CNN was developed by Vecchiotti et al. ([2018](#bib.bib333)).
    They showed that both problems can be handled jointly in a multi-room environment
    using the same architecture, although considering separate input features (GCC-PHAT
    and log-mel-spectrograms) in two separate input branches. These branches are then
    concatenated in a further layer. Vecchiotti et al. ([2019b](#bib.bib335)) extended
    this work by exploring several variant architectures and experimental configurations,
    and Vecchiotti et al. ([2019a](#bib.bib334)) developed an end-to-end auditory-inspired
    system based on a CNN, with Gammatone filter layers included in the neural architecture.
    A method based on mask estimation was proposed by Zhang et al. ([2019b](#bib.bib381)),
    in which a TF mask was estimated and used to either clean or be appended to the
    input features, facilitating the DoA estimation by a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Nguyen et al. ([2020a](#bib.bib221)) presented a multi-task CNN containing 10
    convolutional layers with average pooling, inferring both the NoS and the sources’
    DoA. They evaluated their network on signals with up to four sources, showing
    very good performance on both simulated and real environments. A small 3-layer
    CNN was employed by Varanasi et al. ([2020](#bib.bib329)) to infer both azimuth
    and elevation using signals decomposed with third-order spherical harmonics (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")). The authors tried several combinations of input features,
    including using only the magnitude and/or the phase of the spherical harmonic
    decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of hearing aids, a CNN was applied to both VAD and DoA estimation
    by Varzandeh et al. ([2020](#bib.bib331)). This system is based on two input features,
    GCC-PHAT and periodicity degree, both fed separately into two convolutional branches.
    These two branches are then concatenated in a further layer, which is followed
    by feedforward layers. Fahim et al. ([2020](#bib.bib84)) applied an 8-layer CNN
    to the so-called modal coherence of first-order Ambisonics input features (see
    Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")) for the localization of multiple sources in a reverberant
    environment. They proposed a new method to train a multi-source DoA estimation
    network with only single-source training data, showing an improvement over the
    system of Chakrabarty and Habets ([2019b](#bib.bib44)), especially for signals
    with three speakers. Hao et al. ([2020](#bib.bib116)) investigated a real-time
    implementation of SSL using a CNN with a relatively small architecture (three
    convolutional layers).
  prefs: []
  type: TYPE_NORMAL
- en: Krause et al. ([2020a](#bib.bib160)) investigated the use of several types of
    convolution. They reported that networks using 3D convolutions (on the time, frequency,
    and channel axes) achieved better localization accuracy compared to those based
    on 2D convolutions, complex convolutions, and depth-wise separable convolutions
    (all of them on the time and frequency axes), but with a high computational cost.
    They also showed that the use of depth-wise separable convolutions leads to a
    good trade-off between accuracy and model complexity (to our knowledge, they were
    the first to explore this type of convolutions).
  prefs: []
  type: TYPE_NORMAL
- en: Bologni et al. ([2021](#bib.bib29)) proposed a neural network architecture including
    a set of 2D convolutional layers for frame-wise feature extraction, followed by
    several 1D convolutional layers in the time dimension for temporal aggregation.
    Diaz-Guerra et al. ([2021a](#bib.bib68)) applied 3D convolutional layers on SRP-PHAT
    power maps computed for both azimuth and elevation estimation. They also used
    a couple of 1D causal convolutional layers at the end of the network to perform
    single-source tracking. Their whole architecture was designed to function in fully
    causal mode so that it can be adapted for real-time applications. Wu et al. ([2021b](#bib.bib353))
    proposed using a supervised image mapping approach inspired from computer vision
    works and referred to as image translation. The used a CNN (completed with residual
    layers, see Section [4.5](#S4.SS5 "4.5 Residual neural networks ‣ 4 Neural network
    architectures for SSL ‣ A Survey of Sound Source Localization with Deep Learning
    Methods")) to map an input 2D image (DoA features extracted by conventional beamforming
    and reshaped as a function of Cartesian coordinates $(x,y)$) into an output 2D
    image of the target source position (in which the pixel intensity is decreasing
    rapidly with the distance to the source), from which the source location is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the introduction, the DCASE Challenge includes a SELD task,
    and CNNs have also been used in some of the challenge candidate systems (Politis
    et al., [2020b](#bib.bib254)). Chytas and Potamianos ([2019](#bib.bib52)) used
    convolutional layers with hundreds of filters of size $4\times 10$ for azimuth
    and elevation estimation in a regression mode. Kong et al. ([2019](#bib.bib155))
    compared different numbers of convolutional layers for SELD, while an 8-layer
    CNN was proposed by Noh et al. ([2019](#bib.bib226)) to improve the results over
    the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: An indirect use of a CNN was proposed by Salvati et al. ([2018](#bib.bib283)).
    They trained the neural network to estimate a weight for each of the narrow-band
    SRP components fed at the input layer in order to compute a weighted combination
    of these components. In their experiments, they showed on a few test examples
    that this allowed for a better fusion of the narrow-band components and reduced
    the effects of noise and reverberation, leading to better localization accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the DoA estimation literature, a few works have explored the use of dilated
    convolutions in DNNs. Dilated convolutions, also known as atrous convolutions,
    are a type of convolutional layer in which the convolution kernel is wider than
    the classical one but zeros are inserted so that the number of parameters remains
    the same. Formally, a 1D dilated convolution with a dilation factor $l$ is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(x*k)(n)=\sum_{i}x(n-li)k(i),$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $x$ is the input and $k$ the convolution kernel. The conventional linear
    convolution is obtained with $l=1$. This definition extends to multidimensional
    convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Chakrabarty and Habets ([2019a](#bib.bib43)) demonstrate that incorporating
    dilated convolutions with gradually increasing dilation factors reduces the optimal
    number of convolutional layers of their original CNN architecture (Chakrabarty
    and Habets, [2019b](#bib.bib44)) (discussed previously in this section). This
    leads to an architecture with similar SSL performance and lower computational
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Recurrent neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs are neural networks designed for modeling temporal sequences of data (LeCun
    et al., [2015](#bib.bib175); Goodfellow et al., [2016](#bib.bib101)). Particular
    types of RNNs include long short-term memory (LSTM) cells (Hochreiter and Schmidhuber,
    [1997](#bib.bib129)) and gated recurrent units (GRUs) (Cho et al., [2014](#bib.bib49)).
    These two types of RNNs have become very popular thanks to their capability to
    circumvent the training difficulties that regular RNNs face, in particular the
    vanishing and exploding gradient problems (LeCun et al., [2015](#bib.bib175);
    Goodfellow et al., [2016](#bib.bib101)).
  prefs: []
  type: TYPE_NORMAL
- en: There are few published works on SSL using only RNNs, as recurrent layers are
    often combined with convolutional layers (see Section [4.4](#S4.SS4 "4.4 Convolutional
    recurrent neural networks ‣ 4 Neural network architectures for SSL ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")). Nguyen et al. ([2021a](#bib.bib224))
    used an RNN to align SED and DoA predictions, which were obtained separately for
    each possible sound event type. The RNN was ultimately used to determine which
    SED prediction matched which DoA estimation. A bidirectional LSTM network was
    used by Wang et al. ([2019](#bib.bib349)) to estimate a TF mask to enhance the
    signal, further facilitating DoA estimation by conventional methods such as SRP
    or subspace methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Convolutional recurrent neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CRNNs are neural networks containing one or more convolutional layers and one
    or more recurrent layers. CRNNs have been regularly exploited for SSL since 2018
    because of the respective capabilities of these layers: The convolutional layers
    have proven to be suitable for extracting relevant features for SSL, and the recurrent
    layers are well designed for integrating the information over time.'
  prefs: []
  type: TYPE_NORMAL
- en: In the series of papers (Adavanne et al., [2019c](#bib.bib4), [2018](#bib.bib1),
    [a](#bib.bib2)), Adavanne *et al.* used a CRNN for SELD, in a multi-task configuration,
    with first-order Ambisonics (FOA) input features (see Section [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    In (Adavanne et al., [2018](#bib.bib1)), their architecture contained a series
    of successive convolutional layers, each followed by a max-pooling layer and two
    bidirectional GRU (BGRU) layers. Then, a feedforward layer provided an estimation
    of the spatial pseudo-spectrum (SPS) provided by the MUSIC algorithm (Schmidt,
    [1986](#bib.bib288)), acting as an intermediary output (see Fig. [4](#S4.F4 "Figure
    4 ‣ 4.4 Convolutional recurrent neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    This SPS was then fed into the second part of the neural network, which was composed
    of two convolutional layers, a dense layer, two BGRU layers, and a final feedforward
    layer for azimuth and elevation estimation by classification. The use of an intermediary
    SPS output has been proposed to help the neural network learn a representation
    that has proven to be useful for SSL using traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: In (Adavanne et al., [2019a](#bib.bib2)) and (Adavanne et al., [2019c](#bib.bib4)),
    this intermediary output was no longer used. Instead, the DoA was directly estimated
    using a block of convolutional layers, a block of BGRU layers, and a feedforward
    layer. This system is able to localize and detect several sound events even if
    they overlap in time, provided they are of different types (e.g., speech and car,
    see the discussion in Section [2.2](#S2.SS2 "2.2 Source types ‣ 2 Acoustic environment
    and sound source configurations ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")). This CRNN was the baseline system for Task 3 of the DCASE
    Challenge in 2019 and 2020\. Therefore, it has inspired many other works, and
    many DCASE Challenge candidate systems were built on the system of Adavanne et al.
    ([2019a](#bib.bib2)) with various modifications and improvements.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Lin and Wang ([2019](#bib.bib192)) added Gaussian noise to the
    input spectrograms to train the network to be more robust to noise. Lu ([2019](#bib.bib196))
    integrated some additional convolutional layers and replaced the BGRU layers with
    bidirectional LSTM layers. Leung and Ren ([2019](#bib.bib182)) used the same architecture
    with all combinations of cross-channel power spectra, whereas the replacement
    of input features with group delays was tested by Nustede and Anemüller ([2019](#bib.bib230)).
    GCC-PHAT features were added as input features by Maruri et al. ([2019](#bib.bib207)).
    Zhang et al. ([2019a](#bib.bib380)) used data augmentation during training and
    averaged the output of the network for a more stable DoA estimation. Xue et al.
    ([2019](#bib.bib364)) sent the input features separately into different branches
    of convolutional layers, log-mel, and constant Q-transform features on the one
    hand, and phase spectrograms and CPS features on the other hand (see Section [5](#S5
    "5 Input features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    Cao et al. ([2019b](#bib.bib37)) concatenated the log-mel spectrogram and GCC-PHAT
    features and fed them into two separate CRNNs for SED and DoA estimation (they
    also incorporated the intensity vector in (Cao et al., [2019a](#bib.bib36))).
    In contrast to the baseline of Adavanne et al. ([2019a](#bib.bib2)), more convolutional
    layers and one single BGRU layer were used. The convolutional part of the DoA
    network was transferred from the SED CRNN, which was followed by fine-tuning of
    the DoA branch, labelling this method as two-stage. This led to a notable improvement
    in localization performance over the DCASE Challenge baseline of Adavanne et al.
    ([2019a](#bib.bib2)). Small changes to this baseline were also tested by Pratik
    et al. ([2019](#bib.bib258)), such as the use of Bark-scale spectrograms as input
    features, the modification of the activation function or pooling layers, and the
    use of data augmentation, resulting in noticeable improvements for some experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same baseline neural architecture of Adavanne et al. ([2019a](#bib.bib2))
    was used by Kapka and Lewandowski ([2019](#bib.bib145)), with one separate (but
    identical, except for the output layer) CRNN instance for each subtask: source
    counting (up to two sources), DoA estimation of source 1 (if applicable), DoA
    estimation of source 2 (if applicable), and sound type classification. The authors
    showed that their method was more efficient than the baseline. Krause and Kowalczyk
    ([2019](#bib.bib159)) explored different manners of splitting the SED and DoA
    estimation tasks in a CRNN. While some configurations showed an improvement in
    SED, the localization accuracy was below the baseline for the reported experiments.
    Park et al. ([2019b](#bib.bib238)) investigated a combination of a gated linear
    unit (GLU, a convolutional block with a gated mechanism) and a trellis network
    (containing convolutional and recurrent layers, see the paper by Bai et al. ([2019](#bib.bib15))
    for details), yielding better results than the baseline. The authors extended
    this work for the DCASE 2020 Challenge by improving the overall architecture and
    investigating other loss functions (Park et al., [2020](#bib.bib239)). A non-direct
    DoA estimation scheme was also derived by Grondin et al. ([2019](#bib.bib103)),
    who estimated the TDoA using a CRNN, from which they inferred the DoA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also found propositions of CRNN-based systems in the 2020 edition of the
    DCASE Challenge. Singla et al. ([2020](#bib.bib298)) used the same CRNN as in
    the baseline of Adavanne et al. ([2019a](#bib.bib2)), except that they did not
    use two separated output branches for SED and DoA estimation. Instead, they concatenated
    the SED output with the output of the previous layer to estimate the DoA. Song
    ([2020](#bib.bib300)) used separated neural networks similar to the one of Adavanne
    et al. ([2019a](#bib.bib2)) to address NoS estimation and DoA estimation in a
    sequential way. Multiple CRNNs were trained by Tian ([2020](#bib.bib323)): one
    to estimate the NoS (up to two sources), another to estimate the DoA assuming
    one active source, and another (same as the baseline) to estimate the DoAs of
    two simultaneously active sources. Cao et al. ([2020](#bib.bib38)) designed an
    end-to-end CRNN architecture to detect and estimate the DoA of possibly two instances
    of the same sound event. The addition of one-dimensional convolutional filters
    was investigated by Ronchini et al. ([2020](#bib.bib273)) to exploit the information
    along the feature axes. Sampathkumar and Kowerko ([2020](#bib.bib284)) augmented
    the baseline system of Adavanne et al. ([2019a](#bib.bib2)) by providing the network
    with more input features (log-mel spectrograms, GCC-PHAT, and intensity vector,
    see Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/750355ca73afdb23856e07cfdf9780a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The CRNN architecture of Adavanne et al. ([2018](#bib.bib1), [2019c](#bib.bib4),
    [2019a](#bib.bib2)), which has inspired numerous SELD systems. The input is the
    multichannel STFT-domain FOA magnitude and phase spectrogram. First, features
    are extracted by four successive convolutional layers with $64$ $3\times 3$ kernels,
    each followed by a max-pooling layer. Then two BGRU layers with $64$ units each
    and tanh activations are used to capture the temporal evolution of the extracted
    features. An intermediate SPS output is then computed using a time distributed
    feedforward layer (i.e., this layer is computed separately on each vector of the
    temporal axis). Then, two $16$-kernel convolution layers followed by a $32$-unit
    time distributed feedfoward layer and two $16$-units BGRU layers process the estimated
    SPS. A final $432$-unit time distributed feedforward layer with sigmoid activation
    function is employed to infer the DoA. Note: Reprinted from (Adavanne et al.,
    [2018](#bib.bib1)); copyright by IEEE; reprinted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: Independently of the DCASE Challenge, the CRNN of Adavanne et al. ([2019a](#bib.bib2))
    was adapted by Comminiello et al. ([2019](#bib.bib59)) to receive quaternion FOA
    input features, which slightly improved the CRNN performance. Perotin *et al.*
    proposed using a CRNN with bidirectional LSTM layers on the FOA pseudo-intensity
    vector to localize one (Perotin et al., [2018b](#bib.bib246)) or two (Perotin
    et al., [2019b](#bib.bib248)) speakers. They showed that this architecture achieves
    very good performance in simulated and real reverberant environments with static
    speakers (both types of input features are discussed in Section [5](#S5 "5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    This work was extended by Grumiaux et al. ([2021a](#bib.bib105)), who obtained
    a substantial improvement in performance over the CRNN of Perotin et al. ([2019b](#bib.bib248))
    by adding more convolutional layers with less max-pooling, to localize up to three
    simultaneous speakers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-square convolutional filters and a unidirectional LSTM layer were used
    in the CRNN architecture of Li et al. ([2018](#bib.bib184)). Xue et al. ([2020](#bib.bib365))
    presented a CRNN with two types of input features: the phase of the CPS and the
    signal waveforms. The former was first processed by a series of convolutional
    layers before being concatenated with the latter. Another improvement of the network
    of Adavanne et al. ([2019a](#bib.bib2)) was proposed by Komatsu et al. ([2020](#bib.bib154)),
    who replaced the classical convolutional blocks with GLUs, based on the hypothesis
    that GLUs are better suited for extracting relevant features from phase spectrograms.
    This has led to a notable improvement of localization performance compared to
    the baseline of Adavanne et al. ([2019a](#bib.bib2)). Bohlender et al. ([2021](#bib.bib28))
    proposed an extension of the system of Chakrabarty and Habets ([2019b](#bib.bib44)),
    in which LSTMs and temporal convolutional networks (TCNs) replaced the last dense
    layer of the former architecture. A TCN was made of successive 1D dilated causal
    convolutional layers with increasing dilated factors (Lea et al., [2017](#bib.bib174)).
    The authors showed that taking the temporal context into account with such temporal
    layers actually improves the localization accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can mention the original approach of Nguyen et al. ([2020c](#bib.bib223))
    in which a two-step hybrid approach with two CRNNs is used: In the first step,
    a first CRNN is used for SED and a single-source histogram-based (conventional)
    method is used for DoA estimation. In the second step, a second CRNN-based network,
    referred to as sequence matching network (SMN), is used to match the estimated
    sequences from the SED and DoA branches. This approach is motivated by the fact
    that overlapping sounds often have different onsets and offsets, and by matching
    the outputs of the two branches, an estimated DoA can be associated with the corresponding
    sound class. This approach was extended to localize moving sources in the framework
    of the DCASE 2020 Challenge (Nguyen et al., [2020b](#bib.bib222)), by adapting
    the resolution of the azimuth and elevation histograms and by using an ensemble
    of SMNs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Residual neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Residual neural networks were originally introduced by He et al. ([2016](#bib.bib117)),
    who pointed out that designing very deep networks can lead the gradients to explode
    or vanish due to the non-linear activation functions, as well as the degradation
    of the overall performance. Residual connections are designed to enable a feature
    to bypass a layer block in parallel to the conventional process through this layer
    block. This allows the gradients to flow directly through the network, usually
    leading to a better training.
  prefs: []
  type: TYPE_NORMAL
- en: To our knowledge, the first use of a network with residual connections for SSL
    was proposed by Yalta et al. ([2017](#bib.bib366)). As illustrated in Fig. [5](#S4.F5
    "Figure 5 ‣ 4.5 Residual neural networks ‣ 4 Neural network architectures for
    SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods"), this
    network includes three residual blocks, which are stacks of layers with one of
    the layers having residual connections with another layer deeper in the stack.
    Each of these blocks is made of three convolutional layers, the first and last
    of which are designed with $1\times 1$ filters, with the middle layer designed
    with $3\times 3$ filters. A residual connection is used between the input and
    output of each residual block. The same type of residual block was used for SSL
    by He et al. ([2019a](#bib.bib120), [2018b](#bib.bib119)) in parallel to sound
    classification as speech or non-speech. Suvorov et al. ([2018](#bib.bib310)) used
    a series of 1D convolutional layers with several residual connections for single-source
    localization, directly from the multichannel waveform.
  prefs: []
  type: TYPE_NORMAL
- en: Pujol et al. ([2019](#bib.bib259), [2021](#bib.bib260)) integrated residual
    connections alongside 1D dilated convolutional layers with increasing dilation
    factors. They used the multichannel waveform as the network input. After the input
    layer, the architecture was divided into several subnetworks containing the dilated
    convolutional layers, which functioned as filter banks. Ranjan et al. ([2019](#bib.bib264))
    combined a modified version of the original ResNet architecture (He et al., [2016](#bib.bib117))
    with recurrent layers for SELD. This was shown to reduce the DoA error by more
    than 20° compared to the baseline of Adavanne et al. ([2019a](#bib.bib2)). Similarly,
    Bai et al. ([2021](#bib.bib14)) also used the ResNet model of (He et al., [2016](#bib.bib117))
    followed by two GRU layers and two fully-connected layers for SELD. Kujawski et al.
    ([2019](#bib.bib165)) also adopted the original ResNet architecture and applied
    it to the single-source localization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting architecture containing residual connections was proposed
    by Naranjo-Alcazar et al. ([2020](#bib.bib217)) for the DCASE 2020 Challenge.
    Before the recurrent layers (consisting of two BGRUs), three residual blocks successively
    processed the input features. These residual blocks contained two residual convolutional
    layers, followed by a squeeze-excitation module (Hu et al., [2020](#bib.bib131)).
    These modules aim to improve the modeling of interdependencies between input feature
    channels compared to classical convolutional layers. Similar squeeze-excitation
    mechanisms were used by Sundar et al. ([2020](#bib.bib309)) for multi-source localization.
    Another combination of a residual network with squeeze-excitation blocks was reported
    by Huang and Perez ([2021](#bib.bib132)), who implemented it in the framework
    of a sample-level CNN (i.e., a CNN applied on the time-domain signal samples)
    (Lee et al., [2017](#bib.bib177)). The resulting blocks are further followed by
    two Conformer blocks (see the next subsection). The motivation for combining these
    different models was their observed effectiveness in other audio processing tasks
    such as SED.
  prefs: []
  type: TYPE_NORMAL
- en: Shimada et al. ([2020b](#bib.bib294)) and Shimada et al. ([2020a](#bib.bib293))
    adapted the MMDenseLSTM architecture, originally proposed by Takahashi et al.
    ([2018](#bib.bib314)) for sound source separation, to the SELD problem. This architecture
    consists of a series of blocks made of convolutions and recurrent layers with
    residual connections. Their system showed very good performance among the other
    participants to the DCASE 2020 Challenge. Wang et al. ([2020](#bib.bib347)) used
    an ensemble learning approach in which several variants of residual neural networks
    and recurrent layers were trained to estimate the DoA, achieving the best performance
    of the DCASE 2020 Challenge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0cd56483e01ba69c030571705cfe7565.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The residual neural network architecture used by Yalta et al. ([2017](#bib.bib366)).
    Three residual blocks are employed in this network, which are each composed of
    two convolutional layers with $32$ $1\times 1$ filters with another convolutional
    layer with $32$ $3\times 3$ filters in-between. For all three residual blocks,
    the input is added to the output with a residual connection, showed with a dashed
    arrow in this diagram. The authors show that the use of residual connections not
    only reduces the learning cost, but also improves the model performance. Note:
    Reprinted from (Yalta et al., [2017](#bib.bib366)); under Creative Commons Attribution-NoDerivatives
    4.0 International License.'
  prefs: []
  type: TYPE_NORMAL
- en: Guirguis et al. ([2020](#bib.bib107)) designed a neural network with a TCN in
    addition to classical 2D convolutions and residual connections. Instead of using
    recurrent layers as usually considered, the architecture was composed of TCN blocks
    that were made of several residual blocks, including a 1D dilated convolutional
    layer with an increasing dilated factor. The authors showed that replacing recurrent
    layers with TCNs made the hardware implementation of the network more efficient
    while slightly improving the SELD performance compared to the baseline of Adavanne
    et al. ([2019a](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: Yasuda et al. ([2020](#bib.bib373)) exploited a CRNN with residual connections
    in an indirect way for DoA estimation using an FOA pseudo-intensity vector input
    (see Section [5.5](#S5.SS5 "5.5 Intensity-based features ‣ 5 Input features ‣
    A Survey of Sound Source Localization with Deep Learning Methods")). A CRNN was
    first used to remove the reverberant part of the FOA pseudo-intensity vector,
    after which another CRNN was used to estimate a TF mask, which was applied to
    attenuate TF bins with a large amount of noise. The source DoA was finally estimated
    directly from the dereverberated and denoised pseudo-intensity vector.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Attention-based neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An attention mechanism is a method that allows a neural network to put emphasis
    on vectors of a temporal sequence that are more relevant for a given task. Originally,
    attention was proposed by Bahdanau et al. ([2016](#bib.bib13)) to improve sequence-to-sequence
    models such as RNNs for machine translation. The general principle is to allocate
    a different weight to the vectors of the input sequence when using a combination
    of these vectors for estimating a vector of the output sequence. The model is
    trained to compute the optimal weights that reflect both the link between vectors
    of the input sequence (self-attention) and the relevance of the input vectors
    to explain each output vector (attention at the decoder). This pioneering work
    has inspired the now popular Transformer architecture proposed by Vaswani et al.
    ([2017](#bib.bib332)), which greatly improved the machine translation performance.
    In the Transformer, RNNs are removed, i.e. they are totally replaced by attention
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Attention models are now used in an increasing number of DL applications, including
    SSL. Phan et al. ([2020a](#bib.bib250), [b](#bib.bib251)) submitted an attention-based
    neural system for the DCASE 2020 Challenge. Their architecture was made of several
    convolutional layers, followed by a BGRU, after which a self-attention layer was
    used to infer the activity and the DoA of several distinct sound events at each
    timestep. Schymura et al. ([2020](#bib.bib290)) added an attention mechanism after
    the recurrent layers of a CRNN to output an estimation of the sound source activity
    and its azimuth/elevation. Compared to the baseline of Adavanne et al. ([2019a](#bib.bib2)),
    the addition of attention demonstrated a better use of temporal information for
    SELD. An extension of the system of Chakrabarty and Habets ([2019b](#bib.bib44))
    based on attention mechanisms has been proposed by Mack et al. ([2020](#bib.bib204)).
    Attention is employed to estimate binary masks to focus on frequency bins where
    the target source is dominant. The first attention stage appears right after the
    input layer (analogously to (Chakrabarty and Habets, [2019b](#bib.bib44)), their
    network uses phase spectrograms as inputs), while the second attention stage takes
    place after new features have been extracted using convolutional layers. Adavanne
    et al. ([2021](#bib.bib5)) used a self-attention layer after a GRU in order to
    estimate the association matrix which matches predictions and references. This
    solves the optimal assignment problem and resulted in large improvements in terms
    of localization error.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head self-attention (MHSA), which is the parallel use of several Transformer-type
    attention models (Vaswani et al., [2017](#bib.bib332)), has also inspired SSL
    methods. In the DCASE 2021 Challenge, Emmanuel et al. ([2021](#bib.bib78)) employed
    a MHSA layer right after several convolution modules tailored to learn varying
    spectral characteristics. Yalta et al. ([2021](#bib.bib367)) proposed using the
    whole encoder part of the Transformer architecture, in addition to several convolutional
    layers, to extract features from the input data. Wang et al. ([2021](#bib.bib348))
    adapted the Conformer architecture, originally designed by Gulati et al. ([2020](#bib.bib109))
    for automatic speech recognition, to SSL. This architecture is composed of a feature
    extraction module based on ResNet and a MHSA module that learns local and global
    context representations. The authors demonstrated the benefit of using a specific
    data augmentation technique on this model. Zhang et al. ([2021](#bib.bib384))
    also employed this architecture in the DCASE 2021 Challenge. As briefly mentioned
    in the previous subsection, Conformer blocks were also used in the architecture
    proposed by Huang and Perez ([2021](#bib.bib132)), where they followed a sample-level
    CNN with residual connections and squeeze-excitation. A Conformer block was also
    used in the architecture proposed for SELD by Rho et al. ([2021](#bib.bib267)),
    after convolutional and fully-connected layers and before BGRU layers. Cao et al.
    ([2021](#bib.bib39)) positioned an 8-head attention layer after a series of convolutional
    layers to track the source location predictions over time for different sources
    (up to two sources in their experiments). Schymura et al. ([2021](#bib.bib291))
    used three 4-head self-attention encoders along the time axis after a series of
    convolutional layers before estimating the activity and location of several sound
    events (see Fig. [6](#S4.F6 "Figure 6 ‣ 4.6 Attention-based neural networks ‣
    4 Neural network architectures for SSL ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")). This neural architecture showed an improvement
    over the DCASE Challenge baseline of Adavanne et al. ([2019a](#bib.bib2)). In
    the same line, Xinghao et al. ([2021](#bib.bib360)) replaced the conventional
    convolutional layers of the baseline with a combination of adaptive convolutional
    layers (using dilated convolutions with different dilation factors) and attention
    blocks. Another example of MHSA-based Transformer model for SSL can be found in
    the work of Park et al. ([2021a](#bib.bib240)). In this work, a pretrained model
    is fine-tuned with transfer learning. The output sequence corresponding to each
    3s-sequence of input data is averaged to provide one DoA estimation. Sudarsanam
    et al. ([2021](#bib.bib306)) enriched the CRNN baseline of Adavanne et al. ([2019a](#bib.bib2))
    with a set of several MHSA blocks followed by fully-connected layers. They provided
    an analysis of the influence of the number and dimension of the MHSA blocks (the
    optimal number was found to be 2) and the number of heads (optimal was 8), as
    well as the effect of positional embedding, normalization layers and residual
    connections. Grumiaux et al. ([2021b](#bib.bib106)) showed that replacing the
    recurrent layers of a CRNN with self-attention encoders yielded a notable reduction
    in the computation time. Moreover, the use of MHSA slightly improved localization
    performance upon the baseline CRNN architecture of Perotin et al. ([2019b](#bib.bib248))
    for the considered multiple speaker localization task.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can mention the use of cross-modal attention (CMA) models for SSL
    by Lee et al. ([2021a](#bib.bib178)). A CMA model is the generalization of self-attention
    with two data streams in place of one, which is used in the Transformer decoder
    (Vaswani et al., [2017](#bib.bib332)). Lee et al. ([2021a](#bib.bib178)) used
    two separate SED and DoA estimation CNN blocks to separately produce SED and DoA
    embeddings (this comes in contrast with most DCASE candidate systems where the
    first blocks are shared between SED and DoA estimation.) Then these embeddings
    are merged, first with a weighted linear combination and then with a second, more
    complex, alignment process using two mirrored CMA models. Finally, the SED and
    DoA outputs of the CMA modules are each sent to three parallel fully-connected
    networks for final estimation (this is because in the DCASE 2021 Challenge SELD
    Task, up to three sources can be simultaneously active).
  prefs: []
  type: TYPE_NORMAL
- en: In a general manner, it appears that attention modules, and MHSA in particular,
    have a tendency to replace the recurrent units in the recent SSL DNNs, following
    the “Attention is all you need” seminal line of Vaswani et al. ([2017](#bib.bib332)).
    This is because compared to RNNs, attention modules can model longer-term dependencies
    at a lower computational cost and can highly benefit from parallel computations,
    especially at training time. This tendency is also observed in other application
    domains, as we will discuss in Section [9](#S9 "9 Conclusions and perspectives
    ‣ A Survey of Sound Source Localization with Deep Learning Methods").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/605570d53ad3f9758574a3c94c970a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The self-attention-based neural network architecture of Schymura
    et al. ([2021](#bib.bib291)). The input is the multi-channel spectrogram shaped
    as a $K\times L\times 2C$ tensor, with $K$ the number of frequency bins, $L$ the
    number of frames, and $C$ the number of channels. A feature extraction is first
    done with convolutional layers (not detailed in the figure) to produce $\mathbf{z}_{k,n}$
    to which is attached a positional encoding vector $\mathbf{p}_{E}$. Then, a Transformer
    encoder computes a new representation of shape $K\times D_{E}$, which is used
    to compute the source activity $\gamma_{k,n}$ and the mean $\mathbf{\hat{x}}_{k,n}$
    of the multivariate Gaussian distributions representing the target sources’ location
    (the corresponding covariance matrix $\mathbf{\hat{\Sigma}}_{k,n}$ is computed
    via a parallel (simpler) mechanism.) Note: Reprinted from (Schymura et al., [2021](#bib.bib291));
    copyright by the authors; reprinted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Encoder-decoder neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An encoder-decoder network is an architecture made of two building blocks:
    an encoder, which is fed by the input features and outputs a specific representation
    of the input data, and a decoder, which transforms the new data representation
    from the encoder into the desired output data. Architectures following this principle
    have been largely explored in the DL literature due to their capacity to provide
    compact data representations in an unsupervised manner (Goodfellow et al., [2016](#bib.bib101)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.1 Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An autoencoder (AE) is an encoder-decoder neural network that is trained to
    output a copy of its input. Often, the dimension of the encoder’s last layer output
    is small compared to the dimension of the data. This layer is then known as the
    bottleneck layer and it provides a compressed encoding of the input data. Originally,
    AEs were made of feed-forward layers, but this term is also contemporaneously
    used to designate AE networks with other types of layers, such as convolutional
    or recurrent layers. To the best of our knowledge, the first use of an AE for
    DoA estimation was reported by Zermini et al. ([2016](#bib.bib378)). They used
    a simple AE to estimate TF masks for each possible DoA, which were then used for
    source separation. An interesting AE-based method was presented by Huang et al.
    ([2020](#bib.bib135)), in which an ensemble of AEs were trained to reproduce the
    multichannel input signal at the output, with one AE per candidate source position.
    Since the common latent information among the different channels is the dry signal,
    each encoder approximately deconvolves the signal from a given microphone. These
    dry signal estimates should be similar provided that the source is indeed at the
    assumed position; hence, the localization is performed by finding the AE with
    the most consistent latent representation. However, it is not clear whether this
    model can generalize well to unseen source positions and acoustic conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Le Moing et al. ([2020](#bib.bib172)) presented an AE with a large number of
    convolutional layers (and transposed convolutional layers, which are layers of
    the decoder that process the inverse operation of the corresponding convolutional
    layer at the encoder), which estimates the potential source activity of each subregion
    in the $(x,y)$ plane divided in a grid, making it possible to locate multiple
    sources. They evaluated several types of outputs (binary, Gaussian-based, and
    binary followed by regression refinement), each of which showed promising results
    on the simulated and real data. An extension of this work was presented in (Le Moing
    et al., [2021](#bib.bib173)), in which they proposed using adversarial training
    (see Section [8](#S8 "8 Learning strategies ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) to improve network performance on real data, as
    well as on microphone arrays unseen in the training set, in an unsupervised training
    scheme. To do this, they introduced a novel explicit transformation layer that
    helped the network to be invariant to the microphone array layout. Another encoder-decoder
    architecture was proposed by He et al. ([2021b](#bib.bib122)), in which a multichannel
    waveform was fed into a filter bank with learnable parameters, after which a 1D
    convolutional encoder-decoder network processed the filter bank output. The output
    of the last decoder was then fed separately into two branches, one for SED and
    the other for DoA estimation.
  prefs: []
  type: TYPE_NORMAL
- en: An encoder-decoder structure with one encoder followed by two separate decoders
    was proposed by Wu et al. ([2021c](#bib.bib354)). Signals recorded from several
    microphone arrays were first transformed in the short-term Fourier transform domain
    (see Section [5](#S5 "5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) and then stacked in a 4D-tensor (whose dimensions
    were time, frequency, microphone array and microphone). This tensor was then sent
    to the encoder block, which was made of a series of convolutional layers followed
    by several residual blocks. The output of the encoder was then fed into two separate
    decoders, the first of which was trained to output a probability of source presence
    for each candidate $(x,y)$ region, while the second was trained in the same way
    but with a range compensation to make the network more robust. The same general
    encoder-decoder line was adopted in the 2D image mapping approach proposed by
    Wu et al. ([2021b](#bib.bib353)). Note that here, the network is composed of convolutional
    layers at the encoder and transposed convolutional layers at the decoder, which
    is typical for image mapping applications in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: An indirect use of an AE was proposed by Vera-Diaz et al. ([2020](#bib.bib337)),
    who used convolutional and transposed convolutional layers to estimate the TDoA
    from GCC-based input features. The main idea was to rely on the encoder-decoder
    capacity to reduce the dimension of the input data so that the bottleneck representation
    forced the decoder to output a smoother version of the TDoA. This technique was
    shown to outperform the classical GCC-PHAT method in the reported experiments.
    This work was extended in the presence of two sources (Vera-Diaz et al., [2021](#bib.bib338)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.2 Variational autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A variational autoencoder (VAE) is a generative model that was originally proposed
    by Kingma and Welling ([2014](#bib.bib150)) and Rezende et al. ([2014](#bib.bib266))
    and is now very popular in the DL community. A VAE can be seen as a probabilistic
    version of an AE. Unlike a classical AE, a VAE learns a probability distribution
    of the data at the output of the decoder and also models the probability distribution
    of the so-called latent vector at the bottleneck layer, which makes the VAE strongly
    connected to the concept of unsupervised representation learning (Bengio et al.,
    [2013](#bib.bib20)). New data can thus be obtained with the decoder by sampling
    these distributions.
  prefs: []
  type: TYPE_NORMAL
- en: To our knowledge, Bianco et al. ([2020](#bib.bib25)) were the first to apply
    a VAE for SSL. Their VAE, made of convolutional layers, was trained to generate
    the phase of inter-microphone RTFs (see Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Relative
    transfer function (RTF) ‣ 5.1 Inter-channel features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")), jointly with a classifier
    that estimates the speaker’s DoA from the RTF phases. The interest of using a
    VAE is that this generative model, originally designed for unsupervised training,
    is here trained in a semi-supervised configuration using a large dataset of unlabeled
    RTF data together with a limited set of labeled data (RTF values + corresponding
    DoA labels). In such a limited labeled dataset configuration, this model was shown
    to outperform an SRP-PHAT-based method as well as a supervised CNN in reverberant
    scenarios. This semi-supervised (or weakly supervised) approach is further discussed
    in Section [9.1](#S9.SS1 "9.1 Adaptation to (limited sets of) real-world data
    ‣ 9 Conclusions and perspectives ‣ A Survey of Sound Source Localization with
    Deep Learning Methods"). An extension of this work has been further proposed in
    (Bianco et al., [2021](#bib.bib26)), with refined network architectures and more
    realistic acoustic scenarii.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.3 U-Net architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A U-Net architecture is a particular fully-convolutional neural network originally
    proposed by Ronneberger et al. ([2015](#bib.bib274)) for biomedical image segmentation.
    In U-net, the input features are decomposed into successive feature maps throughout
    the encoder layers and then recomposed into “symmetrical” feature maps throughout
    the decoder layers, similarly to CNNs. Having the same dimension for feature maps
    at the same level in the encoder and decoder enables one to propagate information
    directly from an encoder level to the corresponding level of the decoder via residual
    connections. This leads to the typical U-shape schematization (see Fig. [7](#S4.F7
    "Figure 7 ‣ 4.7.3 U-Net architecture ‣ 4.7 Encoder-decoder neural networks ‣ 4
    Neural network architectures for SSL ‣ A Survey of Sound Source Localization with
    Deep Learning Methods")).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ec9a4eb000934e66050352358d7fd32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The U-Net network architecture of Chazan et al. ([2019](#bib.bib47)).
    The input matrix $\mathcal{R}$ contains angular features extracted from the RTFs
    (see Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Relative transfer function (RTF) ‣ 5.1
    Inter-channel features ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) ($l$, $k$, and $M$ denote the time index, the frequency
    bin, and the number of microphones, respectively). Several stages of encoders
    (in blue) and decoders (in green) are used. At each encoder (or decoder) stage,
    two or three convolutional layers with $3\times 3$ kernels are employed to compute
    a new representation which is used as the input of the next encoder (or decoder,
    respectively), except for the bottleneck stage from which the output is fed as
    input into the upper-stage decoder. Residual connections are used to concatenate
    one encoder output to the input of the same stage decoder, to alleviate the loss
    information problem. The output of this system consists of one TF mask $p_{l,k}(\theta)$
    per considered DoA $\theta$. Note: Reprinted from (Chazan et al., [2019](#bib.bib47));
    copyright by IEEE; reprinted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding SSL and DoA estimation, several works have been inspired by the original
    U-Net paper. Chazan et al. ([2019](#bib.bib47)) employed such an architecture
    to estimate one TF mask per considered DoA (see Fig. [7](#S4.F7 "Figure 7 ‣ 4.7.3
    U-Net architecture ‣ 4.7 Encoder-decoder neural networks ‣ 4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")),
    in which each TF bin was associated with a single particular DoA. This spectral
    mask was finally applied for source separation. This system was extended by Hammer
    et al. ([2021](#bib.bib115)) to account for multiple moving speakers. Another
    joint localization and separation system based on a U-Net architecture was proposed
    by Jenrungrot et al. ([2020](#bib.bib141)). In this system, a U-Net was trained
    based on 1D convolutional layers and GLUs. The input is the multichannel raw waveform
    accompanied by an angular window that helps the network to perform separation
    on a particular zone. If the output of the network on the window is empty, no
    source is detected, otherwise, one or more sources are detected and the process
    is repeated with a smaller angular window, until the angular window reaches $2$°.
    This system shows interesting results on both synthetic and real reverberant data
    containing up to eight speakers.
  prefs: []
  type: TYPE_NORMAL
- en: For the DCASE 2020 Challenge, a U-Net with several BGRU layers in-between the
    convolutional blocks, was proposed for SELD by Patel et al. ([2020](#bib.bib242)).
    The last transposed convolutional layer of this U-Net outputs a single-channel
    feature map per sound event, corresponding to its activity and DoA for all frames.
    This showed an improvement over the baseline of Adavanne et al. ([2019a](#bib.bib2))
    in terms of DoA error. Comanducci et al. ([2020a](#bib.bib57)) used a U-Net architecture
    in the second part of their proposed neural network to estimate the source coordinates
    $x$ and $y$. The first part, composed of convolutional layers, learns to map GCC-PHAT
    features to the so-called ray space (where source positions correspond to linear
    patterns, *cf.* (Bianchi et al., [2016](#bib.bib23))), which is an intermediate
    representation used as the input of the U-Net architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Input features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide an overview of the variety of input feature types
    found in the DL-based SSL literature. Generally, the considered features can be
    low-level signal representations such as waveforms or spectrograms, hand-crafted
    features such as binaural features, or they can be borrowed from traditional SP
    methods such as MUSIC or GCC-PHAT.
  prefs: []
  type: TYPE_NORMAL
- en: Overwhelmingly, the input features for the SSL neural networks are based on
    some representation readily used in signal processing, often emphasizing spatial
    and/or TF information embedded in the signal. This seems to yield good results,
    despite the growing trend in other domains to learn the feature representation
    directly from raw data. One interpretation may be that the network architectures
    in SSL are usually of a relatively modest size, as compared to end-to-end models
    used in some other domains, e.g., NLP. A few publications have compared different
    types of input features for SSL, e.g., (Roden et al., [2015](#bib.bib271); Krause
    et al., [2020b](#bib.bib161)).
  prefs: []
  type: TYPE_NORMAL
- en: It is also quite common to provide the network with concatenated features of
    different nature (even if these carry redundant information), which usually has
    positive impact on performance. This can be attributed to the flexibility of the
    learning process, which seemingly adapts the network weights such that the pertinent
    information is efficiently “routed” from such an input to the upper layers of
    the network, where it is merged into an abstract, optimized feature representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We organized this section into the following feature categories: inter-channel,
    cross-correlation-based, spectrogram-based, Ambisonics, intensity-based, and finally
    the direct use of the multichannel waveforms. Note that, as stated above, different
    kinds of features are often combined at the input layer of SSL neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Inter-channel features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 Relative transfer function (RTF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The RTF is a very general inter-channel feature that has been widely used for
    conventional (non-deep) SSL and other spatial audio processing such as source
    separation and beamforming (Gannot et al., [2017](#bib.bib90)) and acoustic echo
    cancellation (Valero and Habets, [2017](#bib.bib327)), and is now considered for
    DL-based SSL as well. The RTF is defined for a given sound source position and
    for a microphone pair as the ratio $H(f)=A_{2}(f)/A_{1}(f)$ of the source-to-microphone
    ATFs of the two microphones, $A_{2}(f)$ and $A_{1}(f)$ (here we are working in
    the frequency or STFT domain and we recall that an ATF is the discrete Fourier
    transform of the corresponding RIR). It is thus strongly dependent on the source
    DoA (for a given recording set-up). In a multichannel set-up with more than two
    microphones, we can define an RTF for each microphone pair. Often, one microphone
    is used as a reference microphone, and the ATFs of all other microphones are divided
    by the ATF of this reference microphone.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an ATF ratio, an RTF is thus a vector with an entry defined for each frequency
    bin. If only one directional source is present in the recorded signals and if
    the (diffuse) background noise is negligible, Eq. ([2](#S1.E2 "In 1.2 General
    principle of DL-based SSL ‣ 1 Introduction ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")) shows that an RTF estimate can be obtained for each
    STFT frame (indexed by $n$), each frequency bin, and each microphone pair (indexed
    by $i$ and $k$) by taking the ratio between the STFT transforms of the recorded
    waveforms of the two considered channels, $X_{i}(f,n)$ and $X_{k}(f,n)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{H}_{i,k}(f)=\frac{X_{k}(f,n)}{X_{i}(f,n)}\approx\frac{A_{k}(f)S(f,n)}{A_{i}(f)S(f,n)}=\frac{A_{k}(f)}{A_{i}(f)}=H_{i,k}(f),$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $S(f,n)$ is the STFT of the source signal. In the case where a background/sensor
    noise is present, more sophisticated RTF estimation procedures must be used, e.g.,
    (Cohen, [2004](#bib.bib55); Markovich-Golan and Gannot, [2015](#bib.bib206); Li
    et al., [2015](#bib.bib185)). If multiple sources are present, things become more
    complicated, but using the natural sparsity of speech/audio signals in the TF
    domain, *i.e.*, only at most one source is assumed to be active in each TF bin
    (Rickard, [2002](#bib.bib268)), the same principle as for one active source can
    be applied separately in each TF bin. Therefore, a multiple set of estimated RTFs
    at different frequencies (and possibly at different time frames if the sources
    are static or not moving too fast) can be used for multi-source localization.
    The reader is referred to (Gannot et al., [2017](#bib.bib90)) and references therein
    for more information on the RTF estimation problem.
  prefs: []
  type: TYPE_NORMAL
- en: An RTF is a complex-valued vector. In practice, an equivalent real-valued pair
    of vectors is often used. We can use either the real and imaginary parts or the
    modulus and argument. Often, the log-squared value of the interchannel power ratio
    is used, *i.e.*, the interchannel power ratio in dB, and the argument of the RTF
    estimate ideally corresponds to the difference of the ATF phases. Such RTF-based
    representations have been used in several DNN-based systems for SSL. For example,
    Chazan et al. ([2019](#bib.bib47)), Hammer et al. ([2021](#bib.bib115)), and Bianco
    et al. ([2020](#bib.bib25), [2021](#bib.bib26)) used as input features the arguments
    of the measured RTFs obtained from all microphone pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Binaural features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Binaural features have also been used extensively for SSL, in both conventional
    and deep systems (Argentieri et al., [2015](#bib.bib11)). These features correspond
    to a specific two-channel recording set-up, one which attempts to reproduce human
    hearing in the most realistic way possible. Toward this aim, a dummy head/body
    with in-ear microphones is used to mimic the source-to-human-ear propagation,
    and in particular the effects of the head and external ear (pinnae), which are
    important for source localization by the human perception system. In an anechoic
    binaural set-up environment, the (two-channel) source-to-microphone impulse response
    is referred to as the binaural impulse response (BIR). The frequency-domain representation
    of a BIR is the HRTF. Both BIR and HRTF are functions of the source DoA. To take
    into account the room acoustics in a real-world SSL application, BIRs are extended
    to binaural room impulse responses (BRIRs), which combine head/body effects and
    room effects (in particular reverberation, see further discussion on BRIR simulation
    in Section [7.1](#S7.SS1 "7.1 Synthetic data ‣ 7 Data ‣ A Survey of Sound Source
    Localization with Deep Learning Methods")).
  prefs: []
  type: TYPE_NORMAL
- en: 'Several binaural features are derived from binaural recordings: The interaural
    level difference corresponds to the short-term log-power magnitude of the ratio
    between the two binaural channels in the STFT domain, $X_{2}(f,n)$ and $X_{1}(f,n)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ILD(f,n)=20\log_{10}\displaystyle\left\lvert\frac{X_{2}(f,n)}{X_{1}(f,n)}\right\rvert.$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'The interaural phase difference is the argument of this ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $IPD(f,n)=\angle\frac{X_{2}(f,n)}{X_{1}(f,n)},$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: and the interaural time difference is the delay that maximizes the cross-correlation
    between the two channels, similarly to the TDoA in ([6](#S3.E6 "In 3 Conventional
    SSL methods ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    Just like the RTF, these features are actually vectors with frequency-dependent
    entries. In fact, the ILD and IPD are strongly related (not to say similar) to
    the log-power and argument of the RTF, as shown by comparing ([11](#S5.E11 "In
    5.1.2 Binaural features ‣ 5.1 Inter-channel features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")) and ([12](#S5.E12 "In
    5.1.2 Binaural features ‣ 5.1 Inter-channel features ‣ 5 Input features ‣ A Survey
    of Sound Source Localization with Deep Learning Methods")) with ([10](#S5.E10
    "In 5.1.1 Relative transfer function (RTF) ‣ 5.1 Inter-channel features ‣ 5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods")),
    the difference relying more on the set-up than on the features themselves. The
    RTF can be seen as a more general (multichannel) concept, whereas binaural features
    refer to the specific two-channel binaural setup. As for the RTF, the ILD, IPD,
    and ITD implicitly encode the position of a source. Again, when several sources
    are present, the sparsity of speech/audio signals in the TF domain allows ILD/IPD/ITD
    values to provide information on the position of several simultaneously active
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Youssef et al. ([2013](#bib.bib375)) used ILD and ITD vectors fed separately
    into specific input branches of an MLP. Ma et al. ([2015](#bib.bib201)) and Yiwere
    and Rhee ([2017](#bib.bib374)) concatenated the cross-correlation of the two binaural
    channels with the ILD before feeding it into the input layer of their network.
    Ma et al. ([2015](#bib.bib201)) justify this choice with two arguments. The first
    one is to avoid the noise-sensitivity of the peak-picking operation for the computation
    of the ITD, the second one is because of the systematic changes in the cross-correlation
    function according to the source azimuth. Nguyen et al. ([2018](#bib.bib220))
    used the IPD as the argument of a unitary complex number that was decomposed into
    real and imaginary parts. These parts were concatenated to the ILD for several
    frequency bins and several time frames, leading to a 2D tensor that was then fed
    into a CNN. Pang et al. ([2019](#bib.bib235)) also used a CNN to process ILD and
    IPD features in the TF domain, but the ILD and IPD 2D-tensors were directly concatenated
    at the input of the CNN. A system relying only on the IPD was proposed by Pak
    and Shin ([2019](#bib.bib233)). An MLP was trained to output a clean version of
    the noisy input IPD in order to better retrieve the DoA using a conventional method.
    Sivasankaran et al. ([2018](#bib.bib299)) used as input features the concatenation
    of the cosine and sine of the IPDs for several frequency bins and time frames.
    This choice was based on a previous work that showed similar performance for this
    type of input feature compared to classical phase maps, but with a lower dimension.
    In an original way, Thuillier et al. ([2018](#bib.bib322)) employed unusual binaural
    features. They used the ipsilateral and contralateral spectra. These features
    were shown to be relevant for elevation estimation using a CNN. We finally found
    other DNN-based systems that used ILD, e.g., (Roden et al., [2015](#bib.bib271);
    Zermini et al., [2016](#bib.bib378)), ITD, e.g., (Roden et al., [2015](#bib.bib271)),
    or IPD, e.g., (Shimada et al., [2020b](#bib.bib294), [a](#bib.bib293); Zermini
    et al., [2016](#bib.bib378); Subramanian et al., [2021b](#bib.bib305)) in addition
    to other types of features.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Cross-correlation (CC)-based features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another manner for extracting and exploiting inter-channel information that
    depends on source location is to use features based on the cross-correlation (CC)
    between the signals of different channels. In particular, as seen in Section [3](#S3
    "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep
    Learning Methods"), a variant of CC known as GCC-PHAT is a common feature used
    in classical localization methods (Knapp and Carter, [1976](#bib.bib153)). It
    is less sensitive to speech signal variations than standard CC, but it may be
    adversely affected by noise and reverberation (Blandin et al., [2012](#bib.bib27)).
    Therefore, it has been used within the framework of neural networks, which was
    revealed to be robust to this type of disturbance/artefact. In several systems,
    GCC-PHAT has been computed for each microphone pair and several time delays, all
    concatenated to form a 1D vector used as the input of an MLP, e.g., (Xiao et al.,
    [2015](#bib.bib359); Vesperini et al., [2016](#bib.bib339); He et al., [2018a](#bib.bib118)).
    Other architectures include convolutional layers to extract useful information
    from multi-frame GCC-PHAT features, e.g., (He et al., [2018a](#bib.bib118); Vecchiotti
    et al., [2018](#bib.bib333), [2019b](#bib.bib335); Noh et al., [2019](#bib.bib226);
    Lu, [2019](#bib.bib196); Maruri et al., [2019](#bib.bib207); Pratik et al., [2019](#bib.bib258);
    Song, [2020](#bib.bib300); Li et al., [2018](#bib.bib184); Comanducci et al.,
    [2020a](#bib.bib57)).
  prefs: []
  type: TYPE_NORMAL
- en: Some SSL systems rely on the CPS, which we already mentioned in Section [3](#S3
    "3 Conventional SSL methods ‣ A Survey of Sound Source Localization with Deep
    Learning Methods") and which is linked to the CC by a Fourier transform operation
    (in practice, short-term estimates of the CPS are obtained by multiplying the
    STFT of one channel with the conjugate STFT of the other channel). Leung and Ren
    ([2019](#bib.bib182)) and Xue et al. ([2020](#bib.bib365)) sent the CPS into a
    CRNN architecture to improve localization performance over the baseline of Adavanne
    et al. ([2019a](#bib.bib2)) (see Section [4](#S4 "4 Neural network architectures
    for SSL ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
    Grondin et al. ([2019](#bib.bib103)) also used the cross-spectrum for each microphone
    pair in the convolutional block of their architecture, whereas GCC-PHAT features
    were concatenated in a deeper layer. The CPS was also used by Ma and Liu ([2018](#bib.bib202))
    as an input feature. Acoustic imaging has traditionally shown some interest in
    the CPS feature to predict localization and sound pressure level of competing
    sources; coupled with different architectures, from the simple MLP (Castellini
    et al., [2021](#bib.bib40)) to the complex CNN DenseNet network (Xu et al., [2021a](#bib.bib362)),
    authors have shown that the use of DNN could outperform traditional deconvolution
    methods, either in performance or computation time.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional localization methods, such as MUSIC (Schmidt, [1986](#bib.bib288))
    or ESPRIT (Roy and Kailath, [1989](#bib.bib276)), have been widely examined in
    the literature (see Section [3](#S3 "3 Conventional SSL methods ‣ A Survey of
    Sound Source Localization with Deep Learning Methods")). These methods are based
    on the eigen-decomposition of the CC matrix of a multichannel recording. Several
    DNN-based SSL systems (Takeda and Komatani, [2016b](#bib.bib316), [a](#bib.bib315),
    [2017](#bib.bib317); Takeda et al., [2018](#bib.bib318)) have been inspired by
    these methods and reuse such features as input for their neural networks. Nguyen
    et al. ([2020a](#bib.bib221)) computed the spatial pseudo-spectrum based on the
    MUSIC algorithm and then used it as input features for a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Power map methods, which were discussed in Section [3](#S3 "3 Conventional SSL
    methods ‣ A Survey of Sound Source Localization with Deep Learning Methods"),
    have also been used to derive input features for DNN-based SSL systems. Salvati
    et al. ([2018](#bib.bib283)) proposed calculating the narrowband normalized steered
    response power for a set of candidate TDoAs corresponding to an angular grid and
    feeding it into a convolutional layer. This led to a localization performance
    improvement compared to the traditional SRP-PHAT method. Such power maps were
    also used by Diaz-Guerra et al. ([2021a](#bib.bib68)) as inputs of 3D convolutional
    layers. In acoustic imaging, a SRP map is also a standard feature where finding
    the position and the acoustic level is the main goal. Some recent works used a
    CNN (Gonçalves Pinto et al., [2021](#bib.bib99)) or a U-Net (Lee et al., [2021b](#bib.bib179))
    to produce clean deconvolved maps, hence going beyond the intrisic resolution
    of the array.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Spectrogram-based features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alternatively to inter-channel features or CC-based features which already encode
    relative information between channels, another approach is to provide an SSL system
    directly with “raw” multichannel information, *i.e.*, without any pre-processing
    in the channel dimension.
  prefs: []
  type: TYPE_NORMAL
- en: This does not prevent some pre-processing in the other dimensions and, from
    an historical perspective, we notice that many models in this line use spectral
    or spectro-temporal features instead of raw waveforms (see next subsection) as
    inputs. In practice, (multichannel) STFT spectrograms are typically used (Vincent
    et al., [2018](#bib.bib341)). These multichannel spectrograms are generally organized
    as 3D tensors, with one dimension for time (or frames), one for frequency (bins),
    and one for channel. The general spirit of DNN-based SSL methods is that the network
    should be able to “see” by itself and automatically extract and exploit the differences
    between TF spectrograms along the channel dimension while exploiting the “sparsity”
    of TF signal representation.
  prefs: []
  type: TYPE_NORMAL
- en: In several works, the individual spectral vectors from the different STFT frames
    were provided independently to the neural model, meaning that the network did
    not take into account their temporal correlation (and a localization result is
    generally obtained independently for each frame). Thus, in that case, the network
    input is a matrix of size $M\times K$, with $M$ being the number of microphones,
    and $K$ being the number of considered STFT frequency bins. Hirvonen ([2015](#bib.bib128))
    concatenated the log-spectra of eight channels for each individual analysis frame
    and sent it into a CNN as a 2D matrix. Chakrabarty and Habets ([2019b](#bib.bib44),
    [2017a](#bib.bib41), [2017b](#bib.bib42), [a](#bib.bib43)) and Mack et al. ([2020](#bib.bib204))
    used the multichannel phase spectrogram as input features, disregarding the magnitude
    information. This choice is motivated by the fact that it allows to easily generate
    a training dataset from white noise signals. As an extension of this work, phase
    maps were also exploited by Bohlender et al. ([2021](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: When several consecutive frames are considered, the STFT coefficients for multiple
    timesteps and multiple frequency bins form a 2D matrix for each recording channel.
    Usually, these spectrograms are stacked together in a third dimension to form
    the 3D input tensor. Several systems considered only the magnitude spectrograms,
    e.g., (Yalta et al., [2017](#bib.bib366); Wang et al., [2019](#bib.bib349); Patel
    et al., [2020](#bib.bib242); Pertilä and Cakir, [2017](#bib.bib249)), while others
    considered only the phase spectrogram, e.g., (Zhang et al., [2019b](#bib.bib381);
    Subramanian et al., [2021b](#bib.bib305)). When considering both magnitude and
    phase, they can also be stacked in a third dimension (as well as channels). This
    representation has been employed in many DNN-based SSL systems, e.g., (He et al.,
    [2021a](#bib.bib121); Guirguis et al., [2020](#bib.bib107); Krause et al., [2020a](#bib.bib160);
    Lin and Wang, [2019](#bib.bib192); Maruri et al., [2019](#bib.bib207); Zhang et al.,
    [2019a](#bib.bib380); Kapka and Lewandowski, [2019](#bib.bib145); Krause and Kowalczyk,
    [2019](#bib.bib159); Schymura et al., [2021](#bib.bib291)).
  prefs: []
  type: TYPE_NORMAL
- en: Yang et al. ([2021a](#bib.bib368)) dedicated different input branches of their
    CRNN to magnitude and phase features. Other authors have proposed to decompose
    the complex-valued spectrograms into real and imaginary parts, e.g., (Hao et al.,
    [2020](#bib.bib116); He et al., [2018b](#bib.bib119); Le Moing et al., [2020](#bib.bib172);
    Küçük et al., [2019](#bib.bib167)). Finally, Leung and Ren ([2019](#bib.bib182))
    tried several combinations of features computed from the complex multi-channel
    spectrogram, including the magnitude and phase, the real and imaginary parts and
    the CPS. They claim that providing this redundant information could help the neural
    network for better localization.
  prefs: []
  type: TYPE_NORMAL
- en: While basic (STFT) spectrograms consider equally-spaced frequency bins, mel-scale
    spectrograms and Bark-scale spectrograms are represented with a non-linear sub-bands
    division, corresponding to a perceptual scale (low-frequency sub-bands have a
    higher resolution than high-frequency sub-bands) (Peeters, [2004](#bib.bib244)).
    Mel-spectrograms were preferred to STFT spectrograms in several SSL neural networks,
    e.g., (Vecchiotti et al., [2018](#bib.bib333); Kong et al., [2019](#bib.bib155);
    Cao et al., [2019a](#bib.bib36), [b](#bib.bib37); Ranjan et al., [2019](#bib.bib264)).
    The Bark scale was also explored for spectrograms in the SSL system of Pratik
    et al. ([2019](#bib.bib258)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Ambisonic signal representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the SSL literature, numerous systems utilize the Ambisonics format, *i.e.*,
    the spherical harmonics (SH) decomposition coefficients (Jarrett et al., [2017](#bib.bib140)),
    to represent the input signal. Ambisonics is a multichannel format that is increasingly
    used due to its capability to represent the spatial properties of a sound field,
    while being agnostic to the microphone array configuration (Zotter and Frank,
    [2019](#bib.bib386)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The SH decomposition is done for the acoustic pressure measured on the surface
    of a sphere $\mathbb{S}^{2}$, concentric with the microphone array. For a fixed
    sound source in far field, the decomposition coefficient of order $\ell$ and degree
    $m\in[-\ell,\ell]$, in the STFT domain, is given as follows Jarrett et al. ([2017](#bib.bib140)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $B_{\ell,m}(f,n)=\int_{\Omega\in\mathbb{S}^{2}}X(f,n,\Omega)Y^{*}_{\ell,m}(\Omega)d\Omega,$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $X(f,n,\Omega)$ and $Y_{\ell,m}(\Omega)$ are the acoustic pressure and
    the SH function, at the direction $\Omega$, respectively. In practice, this integral
    is approximated by a quadrature rule, since the number of microphones consisting
    an array is finite. Such approximation implies that the pressure $X(f,n,\Omega)$
    is assumed to be an (almost) “order-limited” function on the sphere Rafaely ([2019](#bib.bib263)),
    meaning that $B_{\ell>L,m}(f,n)=0$, for some maximal order $L$ (that depends on
    the number of microphones in the array). Hence, for FOA ($L=1$), the Ambisonics
    representation ([13](#S5.E13 "In 5.4 Ambisonic signal representation ‣ 5 Input
    features ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    counts only $4$ coefficients (channels) per TF bin. Alternatively, the Higher-Order
    Ambisonics (HOA), $L>1$, signals have more than four channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plane wave, bearing an amplitude $S(f,n)$, and coming from a direction
    $\Omega$, admits a simple SH representation $B_{\ell,m}(f,n)=S(f,n)Y_{\ell,m}(\Omega)$
    Rafaely ([2019](#bib.bib263)). Therefore, as opposed to other types of microphone
    arrays, the Ambisonic channels are in phase, since the spatial response of each
    channel $Y_{\ell,m}(\Omega)$ is TF-independent.²²2In practice, the spatial response
    of Ambisonic microphones is approximately frequency-independent only within certain
    bandwidth (dictated by the HOA order), due to spatial aliasing in the high frequency
    range, and noise amplification at lower frequencies Zotter and Frank ([2019](#bib.bib386)).
    Analogous to ([3](#S1.E3 "In 1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods")), the multichannel
    Ambisonic spectrogram $\mathbf{B}(f,n)$, due to $J$ sources and reverberation,
    is given by the multivariate expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{B}(f,n)=\sum\limits_{j=1}^{J}\sum\limits_{r=0}^{\infty}A_{jr}(f,n)S_{j}(f,n)\mathbf{Y}(\Omega_{jr})+\mathbf{N}(f,n),$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $A_{jr}$ is the amplitude of the $r$^(th) reflection of the source $S_{j}$
    (with $r=0$ corresponding to the direct path), $\mathbf{Y}$ is the vector whose
    entries are appropriate spherical harmonics $Y_{\ell,m}$ for all considered Ambisonic
    orders, and $\mathbf{N}$ is the additive noise vector. Note that the complex-valued
    amplitudes $A_{jr}$ account for the attenuation and phase shift of a corresponding
    plane wave component.
  prefs: []
  type: TYPE_NORMAL
- en: The FOA spectrograms, decomposed into magnitude and phase components, have been
    used by, e.g., Adavanne et al. ([2019c](#bib.bib4)); Guirguis et al. ([2020](#bib.bib107));
    Adavanne et al. ([2018](#bib.bib1), [2019a](#bib.bib2)); Kapka and Lewandowski
    ([2019](#bib.bib145)) and Krause and Kowalczyk ([2019](#bib.bib159)). Varanasi
    et al. ([2020](#bib.bib329)) and Poschadel et al. ([2021a](#bib.bib256), [b](#bib.bib257))
    used third-order Ambisonics spectrograms. Poschadel et al. ([2021a](#bib.bib256),
    [b](#bib.bib257)) compared the performance of a CRNN with HOA spectrograms from
    order $1$ to $4$, showing that the higher the order, the better the localization
    accuracy of the network (but still below the performance of the so-called FOA
    pseudo-intensity features, which we will discuss in Section [5.5](#S5.SS5 "5.5
    Intensity-based features ‣ 5 Input features ‣ A Survey of Sound Source Localization
    with Deep Learning Methods")). They used the phase and magnitude for both elevation
    and azimuth estimation. Another way of representing the Ambisonics format was
    proposed by Comminiello et al. ([2019](#bib.bib59)). Based on the FOA spectrograms,
    they proposed considering them as quaternion-based input features, which proved
    to be a suitable representation in previous works (Parcollet et al., [2018](#bib.bib236)).
    To cope with this type of input feature, a neural network was adapted from the
    one of Adavanne et al. ([2019a](#bib.bib2)), showing an improvement over the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Intensity-based features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sound intensity is an acoustic quantity defined as the product of sound pressure
    and particle velocity (Jacobsen and Juhl, [2013](#bib.bib137); Rossing, [2007](#bib.bib275)).
    In the frequency or TF domain, sound intensity is a complex vector whose real
    part (known as “active” intensity) is proportional to the gradient of the phase
    of sound pressure, *i.e.*, it is orthogonal to the wavefront. This is a useful
    property that has been extensively used for SSL, e.g., (Nehorai and Paldi, [1994](#bib.bib219);
    Hickling et al., [1993](#bib.bib126); Jarrett et al., [2010](#bib.bib139); Tervo,
    [2009](#bib.bib320); Evers et al., [2014](#bib.bib82); Kitić and Guérin, [2018](#bib.bib151);
    Pavlidi et al., [2015](#bib.bib243)). The imaginary part (“reactive” intensity)
    is related to oscillatory local energy transfers, and its physical interpretation
    is less obvious (Maysenhölder, [1993](#bib.bib210)). Hence, it has been largely
    ignored by the SSL community, even though it is relevant in room acoustics Nolan
    et al. ([2019](#bib.bib227)). While the pressure is directly measurable by regular
    microphones, particle velocity requires specific sensors, such as acoustic vector-sensors
    (Nehorai and Paldi, [1994](#bib.bib219); Jacobsen and Juhl, [2013](#bib.bib137)),
    e.g., the “Microflown” transducer de Bree ([2003](#bib.bib64)). Otherwise, it
    has to be approximated using the acoustic pressure measurements. Under certain
    conditions, particle velocity can be assumed to be proportional to the spatial
    gradient of sound pressure (Rossing, [2007](#bib.bib275); Merimaa, [2006](#bib.bib214)),
    which allows for the estimation by, e.g., the finite difference method (Tervo,
    [2009](#bib.bib320)) or using the FOA channels discussed in the previous section
    (Zotter and Frank, [2019](#bib.bib386)). The latter approximation is often called
    (FOA) complex *pseudo-intensity* vector Jarrett et al. ([2010](#bib.bib139)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}(f,n)=B_{0,0}(f,n)\mathbf{B}_{\ell=1,m}(f,n)^{*},$ |  | (15)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{B}_{\ell=1,m}^{*}(f,n)$ is the vector of first-order SH coefficients,
    excluding the zero-order $B_{0,0}(f,n)$. In free field conditions, assuming the
    presence of a single source at the TF bin $(f,n)$, the entries of $\Re\left(\mathbf{I}(f,n)\right)$
    are the Cartesian coordinates of a vector colinear with the DoA of the source
    (with $\Re$ denoting the real part of a complex value).
  prefs: []
  type: TYPE_NORMAL
- en: The first use of an Ambisonics pseudo-intensity vector for DL-based SSL was
    reported by Perotin et al. ([2018b](#bib.bib246)), showing superiority in performance
    compared to the use of the raw Ambisonics waveforms and traditional Ambisonics-based
    methods. Interestingly, the authors demonstrated that using both active and reactive
    intensity improves SSL performance. Moreover, they normalized the intensity vector
    of each frequency band by its energy, which can be shown to yield features similar
    to RTFs in the spherical harmonics domain (Daniel and Kitić, [2020](#bib.bib62);
    Jarrett et al., [2017](#bib.bib140)). Yasuda et al. ([2020](#bib.bib373)) proposed
    using two CRNNs to refine the input FOA pseudo-intensity vector. The first CRNN
    is trained to estimate denoising and separation masks under the assumption that
    there are two active sources and that the WDO hypothesis holds. The second CRNN
    estimates another mask to remove the remaining unwanted components (e.g., reverberation).
    The two networks, hence, produce an estimate of the “clean” intensity vector for
    each active source (the NoS is estimated by their system as well). The pseudo-intensity
    vector has consequently been used in several other recent works, e.g., (Grumiaux
    et al., [2021b](#bib.bib106); Perotin et al., [2019b](#bib.bib248); Grumiaux et al.,
    [2021a](#bib.bib105); Nguyen et al., [2021a](#bib.bib224); Cao et al., [2019a](#bib.bib36);
    Park et al., [2020](#bib.bib239); Song, [2020](#bib.bib300); Cao et al., [2021](#bib.bib39);
    Tang et al., [2019](#bib.bib319); Perotin et al., [2019a](#bib.bib247)).
  prefs: []
  type: TYPE_NORMAL
- en: Sound intensity was also explored by Liu et al. ([2021](#bib.bib193)) without
    the Ambisonics representation. The authors computed the instantaneous complex
    sound intensity using an average of the sound pressure across the four considered
    channels and two orthogonal particle velocity components using the differences
    in sound pressure for both microphone pairs. They kept only the real part of the
    estimated sound intensity (active intensity) and applied a PHAT weighting to improve
    the robustness against reverberation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Waveforms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since 2018, several authors have proposed directly providing their neural network
    models with the raw multichannel recorded signal waveforms. This idea relies on
    the DNN’s capability to find the best representation for SSL without the need
    of hand-crafted features or pre-processing of any kind. This is in line with the
    general trend of DL to go toward an end-to-end approach that is observed in many
    other applications, including in speech/audio processing. Of course, this goes
    together with the always increasing size of networks, datasets and computational
    power.
  prefs: []
  type: TYPE_NORMAL
- en: To our knowledge, Suvorov et al. ([2018](#bib.bib310)) were the first to apply
    this idea. They trained their neural network directly with the recorded eight-channel
    waveforms, stacking many 1D convolutional layers to extract high-level features
    for the final DoA classification. Vera-Diaz et al. ([2018](#bib.bib336)), Vecchiotti
    et al. ([2019a](#bib.bib334)), Chytas and Potamianos ([2019](#bib.bib52)), Cao
    et al. ([2020](#bib.bib38)), and Pujol et al. ([2019](#bib.bib259), [2021](#bib.bib260))
    sent the raw multichannel waveforms into 2D convolutional layers. Huang and Perez
    ([2021](#bib.bib132)) sent the raw multichannel waveforms (in microphone format
    and FOA format) into a 1D CNN with residual connections and squeeze-excitation
    blocks. Note that this model is used for SELD and the authors motivate the use
    of raw waveform inputs by the fact that “SED and DOA may have some common features
    that are better preserved in the raw audio [wave]form.” Huang et al. ([2020](#bib.bib135))
    sent the multichannel waveforms into an AE. Jenrungrot et al. ([2020](#bib.bib141))
    shifted the waveforms of each channel to make them temporally aligned according
    to the TDoA before being injected into the input layer of their network. In the
    same vein, Huang et al. ([2018](#bib.bib133), [2019](#bib.bib134)) proposed time-shifting
    the multichannel signal by calculating the time delay between the microphone position
    and the candidate source location, which requires scanning for all candidate locations.
  prefs: []
  type: TYPE_NORMAL
- en: A potential disadvantage of waveform-based features is that the architectures
    exploiting such data are often more complex, as one part of the network needs
    to be dedicated to feature extraction. Moreover, some papers have reported that
    learning the “optimal” feature representations from raw data becomes more difficult
    when noise is present in the input signals (Wichern et al., [2019](#bib.bib350))
    or may even harm generalization, in some cases (Sato et al., [2021](#bib.bib285)).
    However, it is interesting to mention that the visual inspection of the learned
    weights of the input layers of some end-to-end (waveform-based) neural networks
    has revealed that they resemble the filterbanks that are usually applied in the
    pre-processing stage of SSL (see Section [5.3](#S5.SS3 "5.3 Spectrogram-based
    features ‣ 5 Input features ‣ A Survey of Sound Source Localization with Deep
    Learning Methods")) and other various classical speech/audio processing tasks
    (Sainath et al., [2017](#bib.bib280); Luo and Mesgarani, [2019](#bib.bib199)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Other types of features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Varzandeh et al. ([2020](#bib.bib331)) have proposed unusual types of features
    that do not belong to one of the categories described above. Particularly, they
    have used a periodicity degree feature together with GCC-PHAT features in a CNN.
    The periodicity degree is computed for a given frame and period. It is equal to
    the ratio between the harmonic power signal for the given period and the total
    power signal. This conveys information about the harmonic content of the source
    signal to the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Output strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we discuss the different strategies proposed in the literature
    to obtain a final DoA estimate. We generally divide the strategies into two categories:
    classification and regression. When the SSL network is designed for the classification
    task, the source location search space is generally divided into several zones,
    corresponding to different classes, and the neural network outputs a probability
    value for each class. As for regression, the goal is to directly estimate (continuous)
    source position/direction values, which are usually either Cartesian coordinates
    $(x,y,z)$, or spherical coordinates $(\theta,\phi,r)$ (although the source-microphone
    distance $r$ is very rarely considered). However, the latter is an important factor
    as it can affect the estimation accuracy (for instance, due to the influence of
    the direct-to-reverberant ratio (DRR) Vincent et al. ([2018](#bib.bib341))). Therefore,
    in order to obtain a robust model, the training dataset needs to be sufficiently
    diverse such that the network is exposed to sources at different directions, but
    also at different source-microphone distances. In the last subsection, we report
    a few non-direct methods in which the neural network does not estimate the location
    of a source in its output layer. Instead, it either helps another (conventional)
    algorithm to finally retrieve the desired DoA, or the location estimate is a byproduct
    of some intermediate network layer. A reader particularly interested in the comparison
    between the classification and regression approaches may consult the papers of
    Tang et al. ([2019](#bib.bib319)) and Perotin et al. ([2019a](#bib.bib247)).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 DoA estimation via classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many systems treat DoA estimation as a classification problem, *i.e.*, each
    class represents a certain zone in the considered search space. In other words,
    space is divided into several subregions, usually of similar size, and the neural
    network is trained to produce a probability of active source presence for each
    subregion. Such a classification problem is often addressed by using a feedforward
    layer as the last layer in the network, with as many neurons as the number of
    considered subregions. Two activation functions are generally associated with
    the final layer neurons: the softmax and sigmoid functions. Softmax ensures that
    the sum of all neuron outputs is $1$, so it is suitable for a single-source localization
    scenario. With a sigmoid, all neuron outputs are within $[0,1]$ independently
    from each other, which is suitable for multi-source localization. The last layer
    output is often referred to as the spatial (pseudo)-spectrum, whose peaks correspond
    to a high probability of source activity in the corresponding zone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As already mentioned in Section [2.3](#S2.SS3 "2.3 Number of sources ‣ 2 Acoustic
    environment and sound source configurations ‣ A Survey of Sound Source Localization
    with Deep Learning Methods"), the final DoA estimate(s) is/are generally extracted
    using a peak picking algorithm: If the number of sources $J$ is known, the selection
    of the $J$ highest peaks gives the multi-source DoA estimation; if the NoS is
    unknown, usually the peaks above a certain user-defined threshold are selected,
    leading to a joint NoS and localization estimations. Some preprocessing, such
    as spatial spectrum smoothing or angular distance constraints, can be used for
    better DoA estimation. Hence, such a classification strategy can be readily used
    for single-source and/or multi-source localization, as the neural network is trained
    to estimate a probability of source activity in each zone, regardless of the NoS.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Spherical coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regarding the quantization of the source location space, namely the localization
    grid, different approaches have been proposed. Most early works focused on estimating
    only the source’s azimuth $\theta$ relative to the microphone array position,
    dividing the $360$° azimuth space into $N_{\theta}$ regions of equal size, leading
    to a grid quantization step of $\frac{360}{N_{\theta}}$. Without being exhaustive,
    we found in the literature many different values for $N_{\theta}$, e.g., $N_{\theta}=7$
    (Roden et al., [2015](#bib.bib271)), $N_{\theta}=8$ (Hirvonen, [2015](#bib.bib128)),
    $N_{\theta}=20$ (Suvorov et al., [2018](#bib.bib310)), $N_{\theta}=37$ (Vecchiotti
    et al., [2019a](#bib.bib334)), $N_{\theta}=72$ (Ma et al., [2015](#bib.bib201)),
    and $N_{\theta}=360$ (Xiao et al., [2015](#bib.bib359)). Some other works did
    not consider the whole $360$° azimuth space. For example, Chazan et al. ([2019](#bib.bib47))
    focused on the region $[0,180]$ with $N_{\theta}=13$.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the elevation $\phi$ alone has not been frequently investigated in
    the literature, probably because of the lack of interesting applications in indoor
    scenarios. To the best of our knowledge, only one paper focused on estimating
    the elevation alone (Thuillier et al., [2018](#bib.bib322)). The authors divided
    the whole elevation range into nine regions of equal size. The majority of recent
    SSL neural networks are trained to estimate both source azimuth and elevation,
    whenever the microphone array geometry makes it possible. To do this, several
    options have been proposed in the literature. One can use two separate output
    layers, each with the same number of neurons as the number of subregions in the
    corresponding dimension. For example, the output layer of the neural architecture
    proposed by Fahim et al. ([2020](#bib.bib84)) is divided into two branches with
    fully connected layers, one for azimuth estimation ($N_{\theta}$ neurons), and
    the other for elevation estimation ($N_{\phi}$ neurons). One can also have a single
    output layer where each neuron corresponds to a zone in the unit sphere, *i.e.*,
    a unique pair $(\theta,\phi)$, e.g., (Grumiaux et al., [2021b](#bib.bib106); Perotin
    et al., [2019b](#bib.bib248)). Finally, one can directly design two separate neural
    networks, with each estimating the azimuth or the elevation angle, e.g., (Varanasi
    et al., [2020](#bib.bib329)).
  prefs: []
  type: TYPE_NORMAL
- en: However, most of the neural networks following the classification strategy for
    joint azimuth and elevation estimation are designed so that the output corresponds
    to a 2D grid on the unit sphere. For example, Perotin et al. ([2018b](#bib.bib246),
    [2019b](#bib.bib248)) and Grumiaux et al. ([2021a](#bib.bib105)) used a quasi-uniform
    spherical grid with $429$ classes, each represented by a unique neuron in the
    output layer of their network. Adavanne et al. ([2018](#bib.bib1)) sampled the
    unit sphere in the whole azimuth axis but in the limited elevation range of $[-60\degree,60\degree]$,
    yielding an output vector corresponding to $432$ classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance estimation has barely been investigated in the SSL literature, highlighting
    the fact that it is a difficult problem. Roden et al. ([2015](#bib.bib271)) addressed
    the distance estimation along with azimuth or elevation prediction by dividing
    the distance range into five candidate classes. Yiwere and Rhee ([2017](#bib.bib374))
    quantized the distance range into four classes and estimated it along with three
    possible azimuth values. In the paper by Takeda and Komatani ([2016b](#bib.bib316)),
    the azimuth axis was classified with $I=72$ classes along with the distance and
    height of the source, but these last two quantities were classified into a very
    small set of possible pairs: $(30,30)$, $(90,30)$ and $(90,90)$ (in centimeters).
    Bologni et al. ([2021](#bib.bib29)) trained a CNN to classify a single-source
    signal into a 2D map representing the azimuth and distance dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Cartesian coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A few works applied the classification paradigm to estimate the Cartesian coordinates.
    Le Moing et al. ([2021](#bib.bib173), [2020](#bib.bib172)) and Ma and Liu ([2018](#bib.bib202))
    divided the horizontal $(x,y)$ plane into small regions of the same size, with
    each being a class in the output layer. However, this representation suffers from
    a decreasing angular difference between the regions that are far from the microphone
    array, which is probably why regression is usually preferred for estimating Cartesian
    coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 DoA estimation via regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In regression SSL networks, the source location estimate is directly given by
    the continuous value provided by one or several output neurons (whether we consider
    Cartesian or spherical coordinates, and how many source coordinates are of interest).
    This technique offers the advantage of a potentially more accurate DoA estimation
    since there is no quantization. Its drawback is twofold. First, the NoS needs
    to be known or assumed, as there is no way to estimate if a source is active or
    not based on a localization regression. Second, regression-based SSL usually faces
    the well-known source permutation problem (Subramanian et al., [2021b](#bib.bib305)),
    which occurs in the multi-source localization configuration and is common with
    DL-based source separation methods. Indeed, during the computation of the loss
    function at the training time, there is an ambiguity in the association between
    target and actual output – in other words, which estimate should be associated
    with which target? This issue also arises during the evaluation. One possible
    solution is to force the SSL network training to be permutation invariant (Subramanian
    et al., [2021b](#bib.bib305)), in line with what was proposed for audio source
    separation (Yu et al., [2017](#bib.bib376)).
  prefs: []
  type: TYPE_NORMAL
- en: As for classification, when using regression, there is a variety of possibilities
    for the type of coordinates to be estimated. The choice among these possibilities
    is driven more by the context or the application than by design limitations, since
    regression generally requires only a few output neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Spherical coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tsuzuki et al. ([2013](#bib.bib325)) proposed a complex-valued neural approach
    for SSL. The output of the network is a complex number of unit amplitude whose
    argument is an estimate of the azimuth of the source. A direct regression scheme
    was employed by Nguyen et al. ([2018](#bib.bib220)) with a two-neuron output layer
    that predicts the azimuth and elevation values in a single-source environment.
    The system of Opochinsky et al. ([2019](#bib.bib231)) performed only azimuth estimation.
    Regarding the DCASE 2019 Challenge (Politis et al., [2020b](#bib.bib254)), a certain
    number of candidate systems have used two neurons per event type to estimate the
    azimuth and elevation of the considered event, e.g., (Chytas and Potamianos, [2019](#bib.bib52);
    Cao et al., [2019a](#bib.bib36); Park et al., [2019b](#bib.bib238)), while the
    event activity was jointly estimated in order to extract (or not) the corresponding
    coordinates. Sudo et al. ([2019](#bib.bib307)) proposed representing the output
    as a quaternion including the cosinus and sinus of the azimuth and elevation angles,
    from which they retrieve the DoA angle values. This enables to tackle the problem
    of discontinuity at angle interval boundaries (for instance, at $-180\degree$
    and $180\degree$).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the system of Maruri et al. ([2019](#bib.bib207)), azimuth and elevation
    estimations were done separately in two network branches, each containing a specific
    dense layer. Sundar et al. ([2020](#bib.bib309)) proposed a regression method
    relying on a preceding classification step: dividing the azimuth space into $I$
    equal subregions, with the output of the neural network being made of $3I$ neurons.
    Assuming there is at most one active source per subregion, three neurons are associated
    with each of them: one neuron is trained to detect the presence of a source, while
    the other two neurons estimate the distance and azimuth of that source. The loss
    function for training is a weighted sum of categorical cross-entropy (for the
    classification task) and mean square error (for the regression task).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Cartesian coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to predict the DoA with regression is to estimate the Cartesian
    coordinates of the source(s). Vesperini et al. ([2016](#bib.bib339)) designed
    their network output layer with only two neurons to estimate the coordinates $x$
    and $y$ in the horizontal plane, with an output range normalized within $[0,1]$,
    which represents the scaled version of the room size in each dimension. Following
    the same idea, Vecchiotti et al. ([2018](#bib.bib333), [2019b](#bib.bib335)) also
    used two neurons to estimate $(x,y)$ but added a third one to estimate the source
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: The estimation of the three Cartesian coordinates $(x,y,z)$ has been investigated
    in several systems. Vera-Diaz et al. ([2018](#bib.bib336)) and Krause et al. ([2020a](#bib.bib160))
    designed the output layer with three neurons to estimate the coordinates of a
    single source with regression. Adavanne et al. ([2019c](#bib.bib4), [a](#bib.bib2))
    chose the same strategy. However, they performed SELD for several types of event,
    and thus there are three output neurons to provide $(x,y,z)$ estimates for each
    event type, plus another output neuron to estimate whether or not this event is
    active. The hyperbolic tangent activation function is used for the localization
    neurons to keep the output values in the $[-1,1]$ range, leading to a DoA estimate
    on the unit sphere. The same strategy was followed in an extension of this work
    by Comminiello et al. ([2019](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: In Shimada et al. ([2020a](#bib.bib293)), the authors proposed the activity-coupled
    cartesian DoA (ACCDOA) representation which encodes the DoA with the source activity
    in a single vector, separately for each sound class to be localized. More specifically,
    the ACCDOA vector encodes the Cartesian coordinates $(x,y,z)$, is then normalized
    and then multiplied by the source activity ($\in[0,1]$). Using a threshold, the
    active sources can be detected using this vector norm, and their respective DoAs
    can be retrieved from the normalized Cartesian coordinates. This ACCDOA output
    representation has then been used in other works, e.g., (Shimada et al., [2020b](#bib.bib294);
    Sudarsanam et al., [2021](#bib.bib306); Shimada et al., [2021](#bib.bib295); Nguyen
    et al., [2021b](#bib.bib225); Emmanuel et al., [2021](#bib.bib78); Naranjo-Alcazar
    et al., [2021](#bib.bib218)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Non-direct DoA estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural networks have also been used in the regression mode to estimate intermediate
    quantities, which are then used by a non-neural algorithm to predict the final
    DoA.
  prefs: []
  type: TYPE_NORMAL
- en: Pertilä and Cakir ([2017](#bib.bib249)) proposed using a CNN in the regression
    mode to estimate a TF mask. This mask was then applied to the noisy multichannel
    spectrogram to obtain an estimate of the clean multichannel spectrogram, and a
    classical SRP-PHAT method was next applied to retrieve the final DoA. Another
    TF mask estimation was done by Wang et al. ([2019](#bib.bib349)) using a bidirectional
    LSTM network to improve traditional DoA estimation methods, such as GCC-PHAT or
    MUSIC. Pak and Shin ([2019](#bib.bib233)) trained an MLP to remove unwanted artefacts
    of the IPD input features. The cleaned feature was then used to estimate the DoA
    with a non-neural method. Yasuda et al. ([2020](#bib.bib373)) proposed a method
    to filter out reverberation and other non-desired effects from the intensity vector
    by TF mask estimation. The filtered intensity vector led to a better DoA estimation
    than an intensity-based conventional method. Yang et al. ([2021a](#bib.bib368))
    used a two-stage neural network system to estimate the direct-path RTF (DP-RTF),
    that is, the part of the RTF that corresponds to the direct source-to-microphone
    propagation (Li et al., [2016c](#bib.bib187)). In (Yang et al., [2021a](#bib.bib368)),
    the source DoA is the direction parameter of a DP-RTF taken from a dictionary
    of pre-computed DP-RTFs, corresponding to the closest match with the network estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Huang et al. ([2018](#bib.bib133), [2019](#bib.bib134)) employed neural networks
    on multichannel waveforms, shifted in time with a delay corresponding to a certain
    candidate source location, to estimate the original dry signal. Doing this for
    a set of candidate locations, they then calculated the sum of CC coefficients
    between the estimated dry source signals for all candidate source locations. The
    final estimated location was obtained as the one leading to the maximum sum.
  prefs: []
  type: TYPE_NORMAL
- en: A joint localization and separation scheme was proposed by Jenrungrot et al.
    ([2020](#bib.bib141)). The neural network was trained to estimate the signal coming
    from a certain direction within a certain angular window, whose parameters were
    injected as an input to each layer. Thus, the network acted like a radar and scanned
    through all directions, then progressively reduced the angular window up to a
    desired angular resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Several works proposed employing neural networks for a better prediction of
    the TDoA, which is then used to determine the DoA as often done in traditional
    methods. Grondin et al. ([2019](#bib.bib103)) estimated the TDoA in the regression
    mode using a hyperbolic tangent activation function at the output layer. Vera-Diaz
    et al. ([2020](#bib.bib337)) used an AE to estimate a function from GCC-based
    features (similar to TDoA) that exhibited a clear peak corresponding to the estimated
    DoA. Their work was extended in the presence of two sources (Vera-Diaz et al.,
    [2021](#bib.bib338)). In Comanducci et al. ([2020b](#bib.bib58)), the authors
    employed a U-Net in a regression manner to clean GCC-based features from noise
    and reverberation.
  prefs: []
  type: TYPE_NORMAL
- en: Subramanian et al. ([2021a](#bib.bib304)) proposed a neural system based on
    a stacked localization network, parametric beamformers and a speech recognition
    network. Since each of these modules is differentiable, the system is trained
    in the end-to-end mode, using an ASR-specific cost function. Despite being optimized
    for the ASR, the trained system also exhibits a very good performance in terms
    of source separation and localization, whose predictions are the intermediate
    results, retrievable at the output of the corresponding processing modules.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we detail the different approaches taken to deal with data
    during model training or testing. Because we are dealing with indoor domestic/office
    environments, noise and reverberation are common in real-world signals. We successively
    inspect the use of synthetic and recorded datasets in DNN-based SSL.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Synthetic data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A well-known limitation of supervised learning (see Section [8](#S8 "8 Learning
    strategies ‣ A Survey of Sound Source Localization with Deep Learning Methods"))
    for SSL is the lack of labeled training data. In a general manner, it is difficult
    to produce datasets of recorded signals with corresponding source position metadata
    in diverse spatial configurations (and possibly with diverse spectral content)
    that would be sufficiently large for efficient SSL neural model training. Therefore,
    one often has to simulate a large amount of data to obtain an efficient SSL system.
  prefs: []
  type: TYPE_NORMAL
- en: To generate realistic data, taking into account reverberation, one needs to
    simulate the room acoustics. This is usually done by synthesizing the RIR that
    models the sound propagation for a “virtual” source-microphone pair. This is done
    for all microphones of the array (and for a large number of source positions and
    microphone array positions, see below). Then, a “dry” (*i.e.*, clean reverberation-free
    monophonic) source signal is convolved with this RIR to obtain the simulated microphone
    signal (this is done for every channel of the microphone array). As already stated
    in Section [1.2](#S1.SS2 "1.2 General principle of DL-based SSL ‣ 1 Introduction
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"), the foundation
    of SSL relies on the fact that the relative location of a source with respect
    to the microphone array position is implicitly encoded in the (multichannel) RIR,
    and an SSL DNN learns to extract and exploit this information from examples. Therefore,
    such data generation has to be done with many different dry signals and for a
    large number of simulated RIRs with different source and microphone array positions.
    The latter must be representative of the configurations in which the SSL system
    will be used in practice. Moreover, other parameters, such as room dimensions
    and reverberation time, may have to be varied to take into account other factors
    of variations in SSL.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of this approach is that many dry signal datasets exist, in particular
    for speech signals, e.g., (Garofolo et al., [1993b](#bib.bib93); Lamel et al.,
    [1991](#bib.bib168); Garofolo et al., [1993a](#bib.bib92)). Therefore, many SSL
    methods are trained with dry speech signals convolved with simulated RIRs. Chakrabarty
    and Habets ([2017a](#bib.bib41), [b](#bib.bib42)) used white noise as the dry
    signal for training and speech signals for testing. This approach is reminiscent
    of the work of Deleforge et al. ([2013](#bib.bib66), [2015](#bib.bib67)) based
    on a GMR and as already mentioned in Section [3](#S3 "3 Conventional SSL methods
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"). Using white
    noise as the dry signal enables the acquisition of training data that are “dense”
    in the TF domain. However, Vargas et al. ([2021](#bib.bib330)) showed that training
    on speech or music signals leads to better results than noise-based training,
    even when the signals are simulated with a generative adversarial network (GAN).
    Furthermore, the results of Krause et al. ([2021](#bib.bib162)) indicate that
    using speech, noise and sound events data altogether leads to better localization
    performance, even compared to “matched” training and test signals.
  prefs: []
  type: TYPE_NORMAL
- en: As for RIR simulation, there exist several methods (and variants thereof) and
    many acoustic simulation softwares. Detailing these methods and software implementations
    is out of the scope of this article, but an interested reader may consult appropriate
    references, e.g., (Rindel, [2000](#bib.bib270); Svensson and Kristiansen, [2002](#bib.bib311);
    Siltanen et al., [2010](#bib.bib297)). Let us only mention that the simulators
    based on the image source method (ISM) (Allen and Berkley, [1979](#bib.bib7))
    have been widely used in the SSL community, probably due to the fact that they
    offer a relatively good trade-off between the simulation fidelity, in particular
    regarding the “head” of an RIR, *i.e.*, the direct propagation and early reflections
    (Rindel, [2000](#bib.bib270)), and computational complexity. Among publicly available
    libraries, the RIR generator of Habets ([2006](#bib.bib110)), the related signal
    generator (Habets, [2022](#bib.bib111)), the Roomsim toolbox of Campbell et al.
    ([2005](#bib.bib34)) and its extension to mobile sources called Roomsimove Vincent
    and Campbell ([2008](#bib.bib340)), the Spherical Microphone Impulse Response
    (SMIR) generator of Jarrett et al. ([2012](#bib.bib138)), the Pyroomacoustics
    toolbox of Scheibler et al. ([2018](#bib.bib287)), and the Multichannel Room Acoustics
    Simulator (MCRoomSim) of Wabnitz et al. ([2010](#bib.bib343)), are very popular.
    Such libraries have been used by, e.g., Chakrabarty and Habets ([2019b](#bib.bib44));
    Perotin et al. ([2019b](#bib.bib248)); Grumiaux et al. ([2021a](#bib.bib105));
    Nguyen et al. ([2020a](#bib.bib221)); Varanasi et al. ([2020](#bib.bib329)); Salvati
    et al. ([2018](#bib.bib283)); Li et al. ([2018](#bib.bib184)); Bianco et al. ([2020](#bib.bib25)).
    An efficient open-source implementation of the ISM method, relying on Graphic
    Processing Unit (GPU) acceleration, has been recently presented by Diaz-Guerra
    et al. ([2021b](#bib.bib69)) and used in Diaz-Guerra et al. ([2021a](#bib.bib68))
    to simulate moving sources.
  prefs: []
  type: TYPE_NORMAL
- en: Other improved models based on the ISM have also been used to simulate impulse
    responses, such as the one presented by Hirvonen ([2015](#bib.bib128)). This model
    relies on that of Lehmann and Johansson ([2010](#bib.bib181)), which adds a diffuse
    reverberation model to the original ISM method. Hübner et al. ([2021](#bib.bib136))
    proposed a low-complexity model-based training data generation method that includes
    a deterministic model for the direct path and a statistical model for late reverberation.
    It has been demonstrated that the SSL neural network, trained using the data generated
    by this method, achieves comparable localization performance as the same architecture
    trained on a dataset generated by the usual ISM. However, the proposed simulation
    method is computationally more efficient. An investigation of several simulation
    methods was done by Gelderblom et al. ([2021](#bib.bib94)), with extensions of
    ISM, namely ISM with directional sources, and ISM with a diffuse field due to
    scattering. Gelderblom et al. ([2021](#bib.bib94)) compared the simulation algorithms
    via the training of an MLP (in both regression and classification modes) and showed
    that ISM with scattering effects and directional sources leads to the best SSL
    performance. More sophisticated software, such as ICARE^® Bouatouch et al. ([2006](#bib.bib30)),
    often combine ISM with efficient ray-tracing and statistical methods, permitting
    simulation of more complicated room geometries and acoustic effects. Note, however,
    that none of the methods based on approximating the sound propagation by geometrical
    acoustics is capable of precisely simulating certain wave phenomena, such as diffraction
    Kuttruff ([2016](#bib.bib166)).
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing binaural SSL systems requires either directly using signals
    recorded in a binaural setup (see next subsection) or using a dataset of two-channel
    BIRs and convolving these BIRs with (speech/audio) dry signals, just like for
    simulations in conventional set-up. Most of the time, the BIRs are recorded ones
    (see next subsection; there exist a few BIR simulators, but we will not detail
    this quite specific aspect here). To take into account the room acoustics in a
    real-world SSL application, BIR effects are often combined with RIR effects. This
    is not obtained by trivially cascading the BIR and RIR filters, since the BIR
    depends on the source DoA, meaning that one would have to integrate it with RIR
    components from many incoming directions Bernschütz ([2016](#bib.bib21)). However,
    such a process is included in several RIR simulators, which are able to produce
    the corresponding combined response, called the binaural room impulse response
    (BRIR), e.g., (Campbell et al., [2005](#bib.bib34)). Recall that BIRs are often
    manipulated in the frequency domain (referred as HRTFs), where they are a function
    of both frequency and source DoA.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Real data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Collecting real labeled data is crucial to assessing the robustness of an SSL
    neural network in a real-world environment. However, it is a cumbersome task.
    As of today, only a few datasets of such recordings exist. Among them, several
    impulse response datasets are publicly available and have been used to generate
    training and/or testing data.
  prefs: []
  type: TYPE_NORMAL
- en: The distant-speech interaction for robust home applications (DIRHA) simulated
    corpus presented by Cristoforetti et al. ([2014](#bib.bib61)) has been used to
    simulate microphone speech signals based on real RIRs, recorded in a multi-room
    environment (Vesperini et al., [2016](#bib.bib339); Vecchiotti et al., [2018](#bib.bib333)).
    Another database consisting of recorded RIRs from three rooms with different acoustic
    characteristics is publicly available (Hadad et al., [2014](#bib.bib112)), using
    three microphone array configurations to capture signals from several source azimuth
    positions in the range $[-90\degree,90\degree]$. The RIR dataset published by
    Fernandez-Grande et al. ([2021](#bib.bib86)) is intended to be used for DoA estimation,
    and contains measurements from a three-channel array. Other RIR datasets have
    been published by, e.g., Szöke et al. ([2019](#bib.bib312)), Eaton et al. ([2015](#bib.bib75)),
    Hahmann et al. ([2021a](#bib.bib113)), Koyama et al. ([2021](#bib.bib158)), Kristoffersen
    et al. ([2021](#bib.bib163)), and Riezu and Grande ([2021](#bib.bib269)). The
    last four ones were initially designed for sound field analysis and synthesis,
    and they contain measurements from single-channel microphones (*i.e.*, not microphone
    arrays). However, the acquired RIRs correspond to multiple positions within a
    room, and could be potentially used to emulate microphone arrays.
  prefs: []
  type: TYPE_NORMAL
- en: As for BIR dataset recordings, a physical head-and-torso simulator (HATS) (aka
    “dummy head”) is used, with ear microphones plugged into the dummy head ears.
    To isolate head and torso effects from other environment effects such as reverberation,
    binaural recordings are generally made in an anechoic room. For example, the dataset
    published by Thiemann and Van De Par ([2015](#bib.bib321)) was collected using
    four different dummy heads and used for SSL by Roden et al. ([2015](#bib.bib271)).
  prefs: []
  type: TYPE_NORMAL
- en: The Surrey Binaural Room Impulse Responses database was published by Francombe
    ([2017](#bib.bib89)) and has been used for SSL by, e.g., Ma et al. ([2015](#bib.bib201))
    to synthesize signals for evaluating the proposed method. This database has been
    recorded using a HATS in four room configurations, with sound coming from loudspeakers.
    It thus combines binaural effects with room effects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several challenges have also been organized for some years, and evaluation
    datasets with real recordings have been constituted to assess the candidate systems.
    Datasets were created for the SELD task of the DCASE Challenge, in 2019 (Adavanne
    et al., [2019b](#bib.bib3)), 2020 (Politis et al., [2020a](#bib.bib253)), and
    2021 (Politis et al., [2021](#bib.bib255)). These datasets contains sound events
    in reverberant and noisy environments, synthesized from recordings of real RIRs.
    These data come in two four-microphone spatial audio formats: tetrahedral microphone
    array and FOA. The dataset comprises 12 sound event types, including, e.g., barking
    dog, female/male speech or ringing, with up to three simultaneous events overlapping.
    In the 2019 dataset, the sources are static, whereas they are both static and
    moving in the 2020 and 2021 datasets, with more diverse acoustic conditions. Finally,
    in the 2021 edition of the DCASE dataset, additional sound events have been added
    to the recordings to play the role of (directional) interferers (that are not
    bound to be classified). These datasets have been used in many SSL systems, e.g.,
    (Cao et al., [2019a](#bib.bib36); Park et al., [2019b](#bib.bib238); Grondin et al.,
    [2019](#bib.bib103); Cao et al., [2020](#bib.bib38); Naranjo-Alcazar et al., [2020](#bib.bib217);
    Shimada et al., [2020b](#bib.bib294); Wang et al., [2020](#bib.bib347); Mazzon
    et al., [2019](#bib.bib211)). Very recently, another SELD challenge focused on
    3D sound has been announced (Guizzo et al., [2021](#bib.bib108)), where a *pair*
    of FOA microphones was used to capture a large number of RIRs in an office room,
    from which the audio data were generated.'
  prefs: []
  type: TYPE_NORMAL
- en: The acoustic source LOCAlization and TrAcking (LOCATA) challenge (Evers et al.,
    [2020](#bib.bib83)) has been one of the most comprehensive challenges targeting
    the localization of speech sources. The challenge tasks include single and multiple
    SSL, each of which in a setting where the sources and/or microphones are static
    or mobile. The recordings have been made using several types of microphone arrays,
    namely the planar array from Brutti et al. ([2010](#bib.bib32)), the em32 Eigenmike${}^{\text{\textregistered}}$
    spherical array, a hearing aid, and a set of microphones mounted on a robot head.
    The ground truth data include position information obtained through an optical
    tracking system, hand-labeled VAD metadata, and dry (or close-talking) source
    signals. This dataset has been used in a number of works to validate the effectiveness
    of a proposed method on “real-life” recordings, e.g., (Grumiaux et al., [2021a](#bib.bib105);
    Diaz-Guerra et al., [2021a](#bib.bib68); Sundar et al., [2020](#bib.bib309); Pak
    and Shin, [2019](#bib.bib233); Varanasi et al., [2020](#bib.bib329); Tang et al.,
    [2019](#bib.bib319); Yang et al., [2021b](#bib.bib369)).
  prefs: []
  type: TYPE_NORMAL
- en: A few audio-visual datasets have also been developed and are publicly available,
    in which the audio data are enriched with video information. This type of dataset
    is dedicated to the development and testing of audio-visual localization and tracking
    techniques, which are out of the scope of this survey paper. Among these corpora,
    the AV16.3 corpus (Lathoud et al., [2004](#bib.bib170)) and the CHIL database
    (Stiefelhagen et al., [2007](#bib.bib302)) have provided an evaluative basis for
    several (purely audio) SSL systems (Vera-Diaz et al., [2018](#bib.bib336), [2020](#bib.bib337),
    [2021](#bib.bib338)) by considering only the audio part of the audiovisual dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also found a series of papers in which neural networks were tested
    using real data specifically recorded for the presented work in the researchers’
    own laboratories, e.g., (Chazan et al., [2019](#bib.bib47); Grumiaux et al., [2021b](#bib.bib106);
    He et al., [2021a](#bib.bib121); Perotin et al., [2018b](#bib.bib246); Grumiaux
    et al., [2021a](#bib.bib105); Nguyen et al., [2020a](#bib.bib221); He et al.,
    [2018a](#bib.bib118); Varanasi et al., [2020](#bib.bib329); Le Moing et al., [2020](#bib.bib172);
    Perotin et al., [2019a](#bib.bib247)).
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Data augmentation techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To limit the massive use of simulated data, which can limit the robustness of
    the network on real-world data, and to overcome the limitation in the amount of
    real data, several authors have proposed resorting to data augmentation techniques.
    Without producing more recordings, data augmentation allows for the creation of
    additional training examples, often leading to improved network performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the DCASE Challenge, many submitted systems were trained using data augmentation
    techniques on the train dataset. Mazzon et al. ([2019](#bib.bib211)) proposed
    and evaluated three techniques to augment the training data, taking advantage
    of the FOA representation used by their SSL neural network: swap or inversion
    of FOA channels, label-oriented rotation (the rotation is applied to result in
    the desired label), or channel-oriented rotation (the rotation is directly applied
    with a desired matrix). Interestingly, the channel-oriented rotation method gave
    the worst results in their experiments, while the other two methods showed an
    improvement in neural network performance. Zhang et al. ([2019a](#bib.bib380))
    applied the SpecAugment method of Park et al. ([2019a](#bib.bib237)), which led
    to new data examples by masking certain time frames or frequencies of a spectrogram,
    or both at the same time. This method was also employed by, e.g., Yalta et al.
    ([2021](#bib.bib367)); Shimada et al. ([2021](#bib.bib295)); Bai et al. ([2021](#bib.bib14));
    Krause et al. ([2021](#bib.bib162)). In the work of Pratik et al. ([2019](#bib.bib258)),
    new training material was created with the Mixup method of Zhang et al. ([2018](#bib.bib379)),
    which relies on convex combinations of an existing training data pair. Noh et al.
    ([2019](#bib.bib226)) used pitch shifting and block mixing data augmentation (Salamon
    and Bello, [2017](#bib.bib281)). The techniques of Mazzon et al. ([2019](#bib.bib211))
    and Zhang et al. ([2019a](#bib.bib380)) were employed by Shimada et al. ([2020b](#bib.bib294),
    [2021](#bib.bib295)) to create new mixtures, along with another data augmentation
    method proposed by Takahashi et al. ([2016](#bib.bib313)), which is based on random
    mixing of two training signals.'
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. ([2021](#bib.bib348)) applied four new data augmentation techniques
    to the DCASE dataset (Politis et al., [2021](#bib.bib255)). The first one applies
    the benefit of the FOA format to changing the location of the sources by swapping
    audio channels. The second method is based on the extraction of spatial and spectral
    information on the sources, which are then modified and recombined to create new
    training examples. The third one relies on mixing multiple examples, resulting
    in new multi-source labelled mixtures. The fourth technique is based on random
    TF masking. The authors evaluated the benefits of these data augmentation methods
    both when used separately and when applied sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Learning strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a general manner, when training a neural network to accomplish a certain
    task, one needs to choose a training paradigm that often depends on the type and
    amount of available data. In the DNN-based SSL literature, most of the systems
    rely on supervised learning, although several examples of semi-supervised and
    weakly supervised learning can also be found.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training a neural network with supervised learning, the training dataset
    must contain the output target (also known as the label, especially in the classification
    mode) for each corresponding input data. A cost function (or loss function) is
    used to quantify the error between the output target and the actual output of
    the neural network for a given input data, and training consists of minimizing
    the average loss function over the training dataset. We have seen in Section [6](#S6
    "6 Output strategies ‣ A Survey of Sound Source Localization with Deep Learning
    Methods") that in a single-source SSL scenario with the classification paradigm,
    a softmax output function is generally used. In that case, the cost function is
    generally the categorical cross-entropy, e.g., (Perotin et al., [2018b](#bib.bib246);
    Yalta et al., [2017](#bib.bib366); Chakrabarty and Habets, [2017a](#bib.bib41)).
    When dealing with multiple sources, still with the classification paradigm, sigmoid
    activation functions and a binary cross-entropy loss function are used, e.g.,
    (Perotin et al., [2019b](#bib.bib248); Grumiaux et al., [2021a](#bib.bib105);
    Chakrabarty and Habets, [2017b](#bib.bib42)). With a regression scheme, the choice
    for the cost function is the mean square error in most systems, e.g., (He et al.,
    [2021a](#bib.bib121); Nguyen et al., [2018](#bib.bib220); Krause et al., [2020a](#bib.bib160);
    Salvati et al., [2018](#bib.bib283); Adavanne et al., [2019a](#bib.bib2); Shimada
    et al., [2020a](#bib.bib293); Pertilä and Cakir, [2017](#bib.bib249)). We also
    sometimes witness the use of other cost functions, such as the angular error (Perotin
    et al., [2019a](#bib.bib247)) and the $\ell_{1}$-norm (Jenrungrot et al., [2020](#bib.bib141)).
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of supervised training is that the training relies on a great
    amount of labeled training data, whereas only a few real-world datasets with limited
    size have been collected for SSL. These datasets are not sufficient for robust
    training with DL models. To cope with these issues, one can opt for a data simulation
    method, as seen in Section [7.1](#S7.SS1 "7.1 Synthetic data ‣ 7 Data ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"), or data augmentation
    techniques, as seen in Section [7.3](#S7.SS3 "7.3 Data augmentation techniques
    ‣ 7 Data ‣ A Survey of Sound Source Localization with Deep Learning Methods").
    Otherwise, alternative training strategies can be employed, such as semi-supervised
    and weakly supervised learning, as presented hereafter.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Semi-supervised and weakly supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning refers to model training with a dataset that does not
    contain labels. In the present SSL framework, this means that we would have a
    dataset of recorded acoustic signals without the knowledge of sources position/direction,
    and hence unsupervised learning alone is not applicable to SSL in practice. Semi-supervised
    learning refers to when part of the learning is done in a supervised manner, and
    another part is done in an unsupervised manner. Usually the network is pre-trained
    with labeled data training and refined (or fine-tuned) using unsupervised learning,
    *i.e.*, without resorting to labels. In the SSL literature, semi-supervised learning
    has been proposed to improve the performance of the neural network on conditions
    unseen during supervised training or on real data, compared to its performance
    when trained only in the supervised manner. It can be seen as an alternative manner
    to enrich a labeled training dataset of too limited size or conditions (see Section [7](#S7
    "7 Data ‣ A Survey of Sound Source Localization with Deep Learning Methods")).
  prefs: []
  type: TYPE_NORMAL
- en: For example, Takeda and Komatani ([2017](#bib.bib317)) and Takeda et al. ([2018](#bib.bib318))
    adapted a pre-trained neural network to unseen conditions in a unsupervised way.
    For the cost function, the cross-entropy was modified to be computed only with
    the estimated output, so that the overall entropy was minimized. They also applied
    a parameter selection method dedicated to avoid overfitting, as well as early
    stopping. Bianco et al. ([2020](#bib.bib25)) combined supervised and unsupervised
    learning using a VAE-based system. A generative network was trained to infer the
    phase of RTFs, which were used as input features in a classifier network. The
    cost function directly encompasses a supervised term and an unsupervised term
    and, during the training, the examples can come with or without labels.
  prefs: []
  type: TYPE_NORMAL
- en: Le Moing et al. ([2021](#bib.bib173)) proposed a semi-supervised approach to
    adapt the network to real-world data after it was trained with a simulated dataset.
    This strategy was implemented with adversarial training (Goodfellow et al., [2014](#bib.bib100)).
    In the present SSL context, a discriminator network was trained to label incoming
    data as synthetic or real, and the generator network learned to fool the discriminator.
    This enabled the adaptation of the DoA estimation network to infer from real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A different kind of training, named weakly supervised, was used by He et al.
    ([2019a](#bib.bib120), [2021a](#bib.bib121)). The authors fine-tuned a pre-trained
    neural network by adapting the cost function to account for weak labels, which
    is the NoS, presumably known. This helped to improve the network performance by
    reducing the amount of incoherent predictions. Weak supervision was also used
    by Opochinsky et al. ([2019](#bib.bib231)). Under the assumption that only a few
    training data come with labels, a triplet loss function is computed. For each
    training step, three examples are drawn: a query sample, acting as a usual example,
    a positive sample close to the query sample, and a negative sample from a more
    remote source position. The triplet loss (named so because of these three components)
    is then derived so that the network learns to infer the position of the positive
    sample closer to the query sample than the negative sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusions and perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we have presented a comprehensive overview of the literature
    on SSL techniques based on DL methods from 2011 to 2021\. We attempted to categorize
    the many publications in this domain according to different characteristics of
    the methods in terms of source (mixture) configuration, neural network architecture,
    input data type, output strategy, training and test datasets, and learning strategy.
    Tables II–V summarize our survey: They gather the references of the reviewed DL-based
    SSL papers with the main characteristics of the proposed methods (the ones that
    were used in our taxonomy of the different methods) being reported into different
    columns. We believe these tables can be very useful for a quick search of methods
    with a given set of characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this survey paper, we can comment on some current trends and draw
    a series of perspectives on the future directions that would be interesting to
    investigate to improve the performance of SSL systems and gain a better understanding
    of their behavior. Note that some of these perspectives appeal to general methodological
    issues in deep learning that are common to many applications, and some others
    are more specific to SSL. Note also that this list of research directions is not
    meant to be exhaustive.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Adaptation to (limited sets of) real-world data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a general manner, we observe a drop in performance when DNNs trained on simulated
    data are tested on real-world signals. This effect is well-known in the DL research
    in general, it is a particular case of the poor generalization capability of DNNs
    in the case of significant train-test data mismatch (LeCun et al., [2015](#bib.bib175);
    Goodfellow et al., [2016](#bib.bib101)). We recall that this problem remains particularly
    crucial in SSL due to the difficulty of developing massive labeled datasets (i.e.,
    with reliable annotations of ground-truth sources location) and the use of simulated
    training data. This is valid for training datasets generated using the usual “shoebox”
    acoustic simulations. Such geometry is rarely encountered is real-world environments.
    Moreover, the placement of the simulated microphone array is often unrealistic
    (e.g., it is floating in the air, whereas a practical recording device is often
    positioned on a table, leading to strong reflections).
  prefs: []
  type: TYPE_NORMAL
- en: A first approach to tackle this problem is to consider more sophisticated room
    acoustics simulators, capable of taking into account more complex room geometries
    and acoustic phenomena, such as scattering or diffraction, see the related discussion
    in Section [7.1](#S7.SS1 "7.1 Synthetic data ‣ 7 Data ‣ A Survey of Sound Source
    Localization with Deep Learning Methods"). However, this presents the limitation
    of a heavier computation cost, which should be balanced with the amount of data
    to be generated. Another line of research is to progressively train the network
    with more and more realistic signals, e.g., first with signals generated with
    simulated SRIRs, then fine-tuning the network with signals generated with real
    SRIRs, then further fine-tuning it with recorded data. This is in line with the
    general methodology of domain adaptation (DA) (Kouw and Loog, [2019](#bib.bib157))
    and transfer learning (Bengio, [2012](#bib.bib19); Zhuang et al., [2020](#bib.bib385))
    used in many applications of DL, which aims at improving the performance of a
    network on a particular domain (in our case, real-world data) after it has been
    trained on another domain (here, simulated data). For SSL, the idea is to “optimize”
    the model to the target acoustic environment and/or sound sources. DA is a promising
    research field on its own, and it has only recently attracted the attention of
    the SSL community. To our best knowledge, the adversarial approach of Le Moing
    et al. ([2021](#bib.bib173)) and the entropy-based adaptation of Takeda and Komatani
    ([2017](#bib.bib317)) are the only representatives of DA for SSL.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of research would be to inspire from weakly-supervised SSL methods
    based on manifold learning (Laufer-Goldshtein et al., [2020](#bib.bib171)). The
    general principle is that the high-dimensional multichannel observed data live
    in a low-dimensional acoustic space, controlled by a limited number of latent
    variables (mainly, room dimensions, source and microphone positions, and reflection
    coefficients). This low-dimensional space, or manifold, can be identified using
    a large set of unlabeled data and unsupervised data dimension reduction techniques.
    Then a limited set of labeled data can be used to identify the relationship between
    observed data and source positions “in the manifold,” and thus estimate the source
    positions from new observed data (using, e.g., interpolation techniques). This
    principle was largely developed by Laufer-Goldshtein et al. ([2020](#bib.bib171)),
    who proposed several non-deep manifold identification techniques and corresponding
    SSL algorithms. The same principle can be applied with a DL approach, in particular
    with deep latent-variable generative models such as the VAE, in the line of the
    semi-supervised VAE-SSL model of Bianco et al. ([2020](#bib.bib25), [2021](#bib.bib26))
    already mentioned in Section [4.7.2](#S4.SS7.SSS2 "4.7.2 Variational autoencoder
    ‣ 4.7 Encoder-decoder neural networks ‣ 4 Neural network architectures for SSL
    ‣ A Survey of Sound Source Localization with Deep Learning Methods") (see also
    an example of weakly supervised VAE-based source-filter decomposition of speech
    signals by Sadok et al. ([2022](#bib.bib279))). To our knowledge, SSL based on
    “deep manifold learning” is still a largely under-considered and open topic in
    the literature, yet it offers a promising direction to deal with limited annotated
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Flexibility of the trained models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As opposed to conventional SP techniques, which can be parameterized to adapt
    to the changes in the system setup, DL methods for SSL generally assume identical
    setups for the training and the inference phase. Particularly, the number, geometrical
    arrangement and the directivity of the microphones composing an array, are usually
    assumed to be fixed. This is a serious disadvantage, since the network needs to
    be retrained for different microphone arrays, despite the fact that the task (SSL)
    remains the same. A partial remedy is to use array-agnostic inputs, such as Ambisonics,
    e.g., (Adavanne et al., [2018](#bib.bib1); Perotin et al., [2019b](#bib.bib248);
    Grumiaux et al., [2021a](#bib.bib105)), CPS eigenvectors, e.g., (Takeda and Komatani,
    [2016b](#bib.bib316)) or spatial pseudo-spectra, e.g., (Nguyen et al., [2020a](#bib.bib221);
    Wu et al., [2021b](#bib.bib353)). Another possibility is to adopt array-invariant
    techniques from end-to-end multichannel speech enhancement, e.g., Luo et al. ([2020](#bib.bib200)).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, one could apply transfer learning and DA techniques, discussed previously,
    that could enable the models trained for a particular microphone array to adapt
    to another. Such techniques could not only be beneficial for the changes in microphone
    array setups, but also for the changes in the input signal parameters, such as
    the sampling rate, frame and overlap length, as well as the type of the STFT window
    function. A radical approach would be to make the method inherently independent
    to parameterization, by treating the input signal as a point cloud, as recently
    suggested by Subramani and Smaragdis ([2021](#bib.bib303)).
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Multi-task learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-task training is a general methodology to improve the performance of
    a DNN-based system on a given task by training the model to jointly and simultaneously
    tackle several other tasks (Zhang and Yang, [2021](#bib.bib382); Ruder, [2017](#bib.bib277)).
    It has been observed in practice that this often leads to better performance on
    the first target task. This principle is most often implemented in the following
    manner: An early part of the model (e.g., a common feature extraction module composed
    of several layers or several layer blocks) is common for the different tasks,
    then the model splits into different branches, each one specialized in one of
    the different tasks. The common part is assumed to allow the discovery of an efficient
    signal representation, and the fact that this representation is used for several
    downstream tasks somehow reinforce the efficiency of the representation extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: This principle can be applied to SSL. In fact, it has already been extensively
    illustrated in this survey with the SELD Task of the DCASE Challenge (Politis
    et al., [2020b](#bib.bib254)) and the many candidates that have been proposed
    to this challenge (and that we have reported in this survey). The vast majority
    of the candidate DNNs follow the above architecture, with a common feature extraction
    module followed by two SED and SSL branches. In 2021, the ACCDOA representation
    was adopted by many researchers, see Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Cartesian
    coordinates ‣ 6.2 DoA estimation via regression ‣ 6 Output strategies ‣ A Survey
    of Sound Source Localization with Deep Learning Methods"), and allowed for a joint
    SED and SSL process up to the very last model layer. We believe that combining
    the SSL task with other tasks (alternately to SED or in addition to it) such as
    source separation or ASR could lead to further advances. For example jointly proceeding
    to source counting in addition to SSL in the work of Grumiaux et al. ([2021a](#bib.bib105))
    was shown to improve the SSL performance (note that here source counting is explicit
    and high-resolution, i.e., it consists in estimating the number of active sources
    at the short-term frame level, whereas it is most often implicit and generally
    made on a much larger time scale in the SELD task of the DCASE Challenge). Other
    examples of multi-task learning for SSL can be found in the works of Wu et al.
    ([2021c](#bib.bib354), [b](#bib.bib353)). Combining SSL with source separation
    in a DL framework is further discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: Somewhat different from multi-task approaches, the end-to-end *task-oriented*
    learning of the entire processing chain (stacked localization, DoA-parameterized
    beamforming and ASR blocks) of Subramanian et al. ([2021a](#bib.bib304)) represents
    a refreshing idea to address the lack of DoA-annotated data. For instance, by
    using pre-trained ASR blocks, and by “freezing” all but the localization part
    of the system during training, one could use the abundant labeled speech corpora
    as a proxy information for the localization task. Such systems could incorporate
    both neural network modules and processing blocks based on conventional SP, as
    discussed in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Combination of DL and conventional SP techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this survey paper, we have seen how the DL-based data-driven approach to
    the SSL problem has somehow replaced the conventional SP approach over the last
    decade. Yet, conventional methods are able to “explicitly” exploit strong prior
    knowledge on the physical underlying processes via signal and propagation models,
    whereas the exploitation of the spatial information contained in the mixture signal
    is done mostly “implicitly” by DNNs. Therefore, a major perspective for SSL is
    to get the best of both worlds, i.e., the combination of DL with conventional
    multichannel SP techniques.
  prefs: []
  type: TYPE_NORMAL
- en: This can be inspired by what has been done in, e.g., speech enhancement and
    speech/audio source separation. In the single-channel configuration, DL-based
    speech enhancement and separation are mostly based on the masking approach in
    the TF domain. Binary masks or soft masks (reminiscent of the well-known single-channel
    Wiener filter) are estimated with DNNs from the noisy signal and applied to it
    to obtain a cleaned version, see the review by Wang and Chen ([2018](#bib.bib345)).
    For multichannel speech enhancement and separation, a straightforward approach
    is to input the multichannel signal in the mask estimation network. However, more
    clever strategies can be elaborated. For example, Erdogan et al. ([2016](#bib.bib80)),
    Heymann et al. ([2016](#bib.bib124)) and Higuchi et al. ([2017](#bib.bib127))
    proposed combining the DNN-based single-channel masking with beamforming techniques
    (Van Veen and Buckley, [1988](#bib.bib328)). In these works, the TF-domain masks
    estimated by a DNN are used to select speech-dominant against noise-dominant TF
    points, which are then used to estimate speech and noise spatial covariance matrices,
    respectively, which are finally used to build beamforming filters. These papers
    report better ASR scores than with direct TF masking or basic beamforming applied
    separately. This approach was extended by Perotin et al. ([2018a](#bib.bib245))
    with an additional first stage of beamforming in the HOA domain to improve the
    mask estimation. A joint end-to-end optimization of the mask estimator, the beamformer,
    and possibly an ASR acoustic model, was considered in the TF domain by Meng et al.
    ([2017](#bib.bib213)) and Heymann et al. ([2017](#bib.bib125)), and in the time
    domain by Li et al. ([2016a](#bib.bib183)). Closer to source separation than to
    beamforming, Nugraha et al. ([2016](#bib.bib229)) combined a DNN trained to estimate
    a clean speech spectrogram from a noisy speech spectrogram with the source separation
    technique based on the spatial covariance matrix (SCM) model and Wiener filtering
    of Duong et al. ([2010](#bib.bib73)). Leglaive et al. ([2019](#bib.bib180)) proposed
    an unsupervised multichannel speech enhancement system combining a VAE for modeling
    the (single-channel) clean speech signal and the SCM model for modeling the spatial
    characteristics of the multi-channel signal.
  prefs: []
  type: TYPE_NORMAL
- en: Although we can find many examples of combination of DL-based and SP-based approaches
    for beamforming and source separation, to our knowledge and as shown by our survey,
    this principle has been poorly applied to SSL so far. Yet, powerful deep models,
    and in particular deep generative models such as GANs (Goodfellow et al., [2014](#bib.bib100)),
    VAEs (Kingma and Welling, [2014](#bib.bib150)), and dynamical VAEs (Girin et al.,
    [2021](#bib.bib98)) are now avaible to model the temporal and/or spectral characteristics
    of sounds, and can be combined with SP-based models. Morevover, as already mentioned
    earlier in this survey, the connection between audio source separation, diarization,
    and SSL is strong, reciprocal (each task can help to solve the other ones), and
    is already exploited in many conventional systems (Vincent et al., [2018](#bib.bib341);
    Gannot et al., [2017](#bib.bib90)). Future works may thus consider jointly sound
    source localization, diarization and separation/enhancement in an hybrid approach
    combining powerful DL models and conventional SP techniques. General frameworks
    for the joint optimization of DNN parameters and “conventional” parameters are
    now established and can be exploited (Engel et al., [2020](#bib.bib79); Shlezinger
    et al., [2020](#bib.bib296)).
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Moving sources and deep tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this survey, we poorly considered the case of moving sound sources and the
    necessity to rely in this case on tracking algorithms. These algorithms take as
    input the results of SSL obtained individually on each time frame and connect
    them through time. This is generally based on the use of a model of the source
    dynamics. In the multi-source case, dynamical models are often combined with source
    appearance models (which would model the sound texture or the different speakers’
    voice in the case of audio signals), resulting in the formation of source tracks
    with a consistent source “identity” for each of these tracks. Tracking algorithms
    also estimate the tracks “birth” and “death,” i.e., the time at which the corresponding
    sources are activated or inactivated. Such multi-object tracking (MOT) algorithms
    have a long history and their detailed description is beyond the scope of this
    paper; for a good overview of this domain, see the review papers of Vo et al.
    ([2015](#bib.bib342)) and Luo et al. ([2021](#bib.bib198)).
  prefs: []
  type: TYPE_NORMAL
- en: More recently, deep approaches to the MOT problem have emerged, an evolution
    mostly driven by the computer vision community (Ciaparrone et al., [2020](#bib.bib53)).
    For example, RNNs have been used in place of the traditional Kalman filter to
    model object dynamics for MOT in videos, e.g., (Babaee et al., [2018](#bib.bib12);
    Sadeghian et al., [2017](#bib.bib278); Liang and Zhou, [2018](#bib.bib190); Saleh
    et al., [2021](#bib.bib282); Xiang et al., [2019](#bib.bib358)). The current trend
    is to replace RNNs with Transformer-like models, as discussed at the end of Section [4.6](#S4.SS6
    "4.6 Attention-based neural networks ‣ 4 Neural network architectures for SSL
    ‣ A Survey of Sound Source Localization with Deep Learning Methods"), e.g., (Xu
    et al., [2021b](#bib.bib363); Sun et al., [2020](#bib.bib308); Meinhardt et al.,
    [2021](#bib.bib212)). The combination of a deep appearance model (automatic speaker
    recognition, SED) with a deep dynamical model in a sound source tracking system
    is a largely open problem and certainly a key ingredient for future developments
    in robust multi-source acoustic scene analysis in adverse acoustic environments
    and complex scenarios. Given the problem of annotated data scarsity in SSL, DL-based
    sound source localization and tracking may inspire from the unsupervised deep
    approaches to the MOT problem recently proposed by, e.g., (Luiten et al., [2020](#bib.bib197);
    Karthik et al., [2020](#bib.bib146); He et al., [2019b](#bib.bib123); Crawford
    and Pineau, [2020](#bib.bib60); Lin et al., [2022](#bib.bib191)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of DL-based SSL systems published from 2011 to 2018, organized
    in chronological then alphabetical order. Type: R = regression, C = classification.
    Learning: S = supervised, SS = semi-supervised, WS = weakly supervised. Sources:
    NoS = considered number of sources, Kno. indicates if the NoS is known or not
    before estimating the DoA (✓= yes, ✗= no), Mov. specifies if moving sources are
    considered. Data: SA = synthetic anechoic, RA = real anechoic, SR = synthetic
    reverberant, RR = real reverberant.'
  prefs: []
  type: TYPE_NORMAL
- en: Author Year Architecture Type Learn- Input features Output Sources Data NoS
    Kno. Mov. Train Test ing SA RA SR RR SA RA SR RR Kim and Ling ([2011](#bib.bib149))
    2011 MLP R S Power of multiple beams $\theta$ 1-5 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Tsuzuki
    et al. ([2013](#bib.bib325)) 2013 MLP R S Time delay, phase delay, sound pressure
    diff. $\theta$ 1 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ Youssef et al. ([2013](#bib.bib375)) 2013
    MLP R S ILD, ITD $\theta$ 1 ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ Hirvonen ([2015](#bib.bib128))
    2015 CNN C S Magnitude spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Ma et al. ([2015](#bib.bib201))
    2015 MLP C S Binaural cross-correlation + ILD $\theta$ 1-3 ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓
    Roden et al. ([2015](#bib.bib271)) 2015 MLP C S ILD, ITD, binaural magnitude +
    phase spectrogr., $\theta$ / $\phi$ / $r$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ binaural real
    + imaginary spectrograms Xiao et al. ([2015](#bib.bib359)) 2015 MLP C S GCC-PHAT
    $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Takeda and Komatani ([2016b](#bib.bib316)) 2016
    MLP C S Complex eigenvectors from correlation matrix $\theta$, $z$, $r$ 0-1 ✓
    ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Takeda and Komatani ([2016a](#bib.bib315)) 2016 MLP C S Complex
    eigenvectors from correlation matrix $\theta$ 0-2 ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Vesperini
    et al. ([2016](#bib.bib339)) 2016 MLP R S GCC-PHAT $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗
    ✓ ✓ Zermini et al. ([2016](#bib.bib378)) 2016 AE C S Mixing vector + ILD + IPD
    $\theta$ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Chakrabarty and Habets ([2017a](#bib.bib41)) 2017
    CNN C S Phase map $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Chakrabarty and Habets ([2017b](#bib.bib42))
    2017 CNN C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pertilä and Cakir ([2017](#bib.bib249))
    2017 CNN R S Magnitude spectrograms TF Mask 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Takeda and Komatani
    ([2017](#bib.bib317)) 2017 MLP C SS Complex eigenvectors from correlation matrix
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ Yalta et al. ([2017](#bib.bib366)) 2017
    Res. CNN C S Magnitude spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yiwere and
    Rhee ([2017](#bib.bib374)) 2017 MLP C S Binaural cross-correlation + ILD $\theta$,
    $d$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Adavanne et al. ([2018](#bib.bib1)) 2018 CRNN C S Magnitude
    + phase spectrograms SPS, $\theta$, $\phi$ $\infty$ ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ He et al.
    ([2018a](#bib.bib118)) 2018 MLP, CNN C S GCC-PHAT $\theta$ 0-2 ✗/✓ ✗ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ He et al. ([2018b](#bib.bib119)) 2018 Res. CNN C S Real + imaginary spectrograms
    $\theta$ $\infty$ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Huang et al. ([2018](#bib.bib133)) 2018
    DNN R S Waveforms dry signal 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Li et al. ([2018](#bib.bib184))
    2018 CRNN C S GCC-PHAT $\theta$ 1 ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Ma and Liu ([2018](#bib.bib202))
    2018 CNN C S CPS $x$, $y$ 3 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Nguyen et al. ([2018](#bib.bib220))
    2018 CNN R S ILD + IPD $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Perotin et al. ([2018b](#bib.bib246))
    2018 CRNN C S Intensity $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Salvati et al.
    ([2018](#bib.bib283)) 2018 CNN C/R S Narrowband SRP components SRP weights 1 ✓
    ✗ ? ✗ ✗ ✓ ✓ Sivasankaran et al. ([2018](#bib.bib299)) 2018 CNN C S IPD $\theta$
    1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Suvorov et al. ([2018](#bib.bib310)) 2018 Res. CNN C S Waveforms
    $\theta$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Takeda et al. ([2018](#bib.bib318)) 2018 MLP C
    SS Complex eigenvectors from correlation matrix $\theta$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓
    Thuillier et al. ([2018](#bib.bib322)) 2018 CNN C S Ipsilateral + contralateral
    ear input signal $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vecchiotti et al. ([2018](#bib.bib333))
    2018 CNN R S GCC-PHAT + mel spectrograms $x$, $y$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vera-Diaz
    et al. ([2018](#bib.bib336)) 2018 CNN R S Waveforms $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✓
    ✓ ✗ ✗ ✗ ✓
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of DL-based SSL systems published in 2019, organized in alphabetical
    order. See Table I’s caption for acronyms specification.'
  prefs: []
  type: TYPE_NORMAL
- en: Author Year Architecture Type Learn- Input features Output Sources Data NoS
    Kno. Mov. Train Test ing SA RA SR RR SA RA SR RR Adavanne et al. ([2019a](#bib.bib2))
    2019 CRNN R S FOA magnitude + phase spectrograms $x$, $y$, $z$ 1 ✓ ✗ ✓ ✓ ✓ ✓ ✓
    ✓ ✓ ✓ Adavanne et al. ([2019c](#bib.bib4)) 2019 CRNN R S FOA magnitude + phase
    spectrograms $x$, $y$, $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Cao et al. ([2019a](#bib.bib36))
    2019 CRNN R S Log-Mel spectrogr. + GCC-PHAT + intensity $\theta$, $\phi$ 1 ✓ ✗
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Cao et al. ([2019b](#bib.bib37)) 2019 CRNN R S Log-Mel spectrogr.
    + GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Chakrabarty and Habets ([2019a](#bib.bib43))
    2019 CNN C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Chakrabarty and Habets ([2019b](#bib.bib44))
    2019 CNN C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Chazan et al. ([2019](#bib.bib47))
    2019 U-net C S Phase map of the RTF between each mic pair $\theta$ $\infty$ ✗
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Chytas and Potamianos ([2019](#bib.bib52)) 2019 CNN R S Waveforms
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Comminiello et al. ([2019](#bib.bib59))
    2019 CRNN R S Quaternion FOA $x$, $y$, $z$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Grondin et al.
    ([2019](#bib.bib103)) 2019 CRNN R S CPS + GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ He et al. ([2019a](#bib.bib120)) 2019 Res. CNN C WS Real + imaginary
    spectrograms $\theta$ 1-2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Huang et al. ([2019](#bib.bib134))
    2019 CNN R S Waveforms dry signal 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Kapka and Lewandowski
    ([2019](#bib.bib145)) 2019 CRNN R S Magnitude + phase spectrograms $x$, $y$, $z$
    1-2 ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Kong et al. ([2019](#bib.bib155)) 2019 CNN R S Log-Mel
    magnitude FOA spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Krause and Kowalczyk
    ([2019](#bib.bib159)) 2019 CRNN R S Magnitude / phase spectrograms $\theta$, $\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Küçük et al. ([2019](#bib.bib167)) 2019 CNN C S Real + imaginary
    spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Kujawski et al. ([2019](#bib.bib165))
    2019 Res. CNN R S Beamforming map $x$, $y$ 1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Leung and Ren
    ([2019](#bib.bib182)) 2019 CRNN R S CPS + real/imag. spectro + mag./phase spectro
    $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Lin and Wang ([2019](#bib.bib192)) 2019
    CRNN C S Magnitude and phase spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Lu ([2019](#bib.bib196)) 2019 CRNN R S GCC-PHAT $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ Maruri et al. ([2019](#bib.bib207)) 2019 CRNN R S GCC-PHAT + magnitude
    + phase spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Mazzon et al. ([2019](#bib.bib211))
    2019 CRNN R S Mel-spectrograms + GCC-PHAT/intensity $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ Noh et al. ([2019](#bib.bib226)) 2019 CNN C S GCC-PHAT $\theta$, $\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nustede and Anemüller ([2019](#bib.bib230)) 2019 CRNN R
    S Group delays $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Opochinsky et al. ([2019](#bib.bib231))
    2019 MLP R WS RTFs $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pak and Shin ([2019](#bib.bib233))
    2019 MLP R S IPD (clean) IPD ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Pang et al. ([2019](#bib.bib235))
    2019 CNN R S ILD + IPD $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park et al. ([2019b](#bib.bib238))
    2019 CRNN R S Log-Mel spectrograms + intensity $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓
    ✗ ✗ ✗ ✓ Perotin et al. ([2019b](#bib.bib248)) 2019 CRNN C S FOA pseudo-intensity
    $\theta$, $\phi$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Perotin et al. ([2019a](#bib.bib247)) 2019
    CRNN C/R S FOA pseudo-intensity $\theta$, $\phi$ / $x$, $y$, $z$ 1 ✓ ✗ ✗ ✗ ✓ ✗
    ✗ ✗ ✓ ✓ Pratik et al. ([2019](#bib.bib258)) 2019 CRNN R S GCC-PHAT + Mel/Bark
    spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Pujol et al. ([2019](#bib.bib259))
    2019 Res. CNN R S Waveforms $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✓ ✗ Ranjan et al. ([2019](#bib.bib264))
    2019 Res. CRNN C S Log-Mel spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓
    Sudo et al. ([2019](#bib.bib307)) 2019 CRNN R S cos(IPD), sin(IPD) $cos(\theta)$,
    $sin(\theta)$, 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ $cos(\phi)$, $sin(\phi)$ Tang et al. ([2019](#bib.bib319))
    2019 CRNN C/R S FOA pseudo-intensity $\theta$, $\phi$ / $x$, $y$, $z$ 1 ✓ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✗ ✓ Vecchiotti et al. ([2019b](#bib.bib335)) 2019 CNN R S GCC-PHAT +
    Mel-spectrograms $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Vecchiotti et al. ([2019a](#bib.bib334))
    2019 CNN C S Waveforms $\theta$ 1 ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✓ Wang et al. ([2019](#bib.bib349))
    2019 RNN R S Magnitude spectrograms TF Mask 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Xue et al. ([2019](#bib.bib364))
    2019 CRNN R S Log-Mel spectr. + CQT + phase spectrogr. + CPS $\theta$, $\phi$
    1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Zhang et al. ([2019a](#bib.bib380)) 2019 CRNN R S Magnitude
    and phase spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Zhang et al. ([2019b](#bib.bib381))
    2019 CNN C S Phase spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of DL-based SSL systems published in 2020, organized in alphabetical
    order. See Table I’s caption for acronyms specification.'
  prefs: []
  type: TYPE_NORMAL
- en: Author Year Architecture Type Learn. Input features Output Sources Data NoS
    Kno. Mov. Train Test SA RA SR RR SA RA SR RR Bianco et al. ([2020](#bib.bib25))
    2020 VAE C SS RTFs $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Cao et al. ([2020](#bib.bib38))
    2020 CRNN R S FOA waveforms $\theta$, $\phi$ 0-2 ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Comanducci
    et al. ([2020a](#bib.bib57)) 2020 CNN/U-Net C S GCC-PHAT $x$, $y$ 1 ✓ ✗ ✓ ✗ ✓
    ✗ ✗ ✗ ✓ ✓ Comanducci et al. ([2020b](#bib.bib58)) 2020 U-Net R S GCC Clean GCC
    1 ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Fahim et al. ([2020](#bib.bib84)) 2020 CNN C S FOA modal
    coherence $\theta$, $\phi$ 1-7 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Hao et al. ([2020](#bib.bib116))
    2020 CNN C S Real + imaginary spectrograms + spectral flux $\theta$ 1 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ Huang et al. ([2020](#bib.bib135)) 2020 AE R S Waveforms $\theta$
    1 ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Hübner et al. ([2021](#bib.bib136)) 2020 CNN C S Phase map
    $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Jenrungrot et al. ([2020](#bib.bib141)) 2020 U-Net
    R S Waveforms $\theta$ 0-8 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Mack et al. ([2020](#bib.bib204))
    2020 CNN + attention C S Phase map $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Le Moing et al.
    ([2020](#bib.bib172)) 2020 AE C,R S Real + imaginary spectrograms $x$, $y$ 1-3
    ✗ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Le Moing et al. ([2021](#bib.bib173)) 2020 AE C SS Real +
    imaginary spectrograms $x$, $y$ 1-3 ✗ ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Naranjo-Alcazar et al.
    ([2020](#bib.bib217)) 2020 Res. CRNN R S Log-Mel magnitude spectrograms + GCC-PHAT
    $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen et al. ([2020a](#bib.bib221)) 2020
    CNN C S Spatial pseudo-spectrum $\theta$ 0-4 ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Nguyen et al.
    ([2020b](#bib.bib222)) 2020 CRNN R S DoAs from histogram-based method $\theta$,
    $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen et al. ([2020c](#bib.bib223)) 2020 CRNN R
    S DoAs from histogram-based method $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park
    et al. ([2020](#bib.bib239)) 2020 CRNN R S Log-Mel energy + intensity $\theta$,
    $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Patel et al. ([2020](#bib.bib242)) 2020 U-Net R S
    Mel-spectrograms $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Phan et al. ([2020a](#bib.bib250))
    2020 CRNN + SA R S FOA log-Mel spectrograms + active/reactive $x$, $y$, $z$ 1
    ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ intensity, or GCC-PHAT Phan et al. ([2020b](#bib.bib251))
    2020 CRNN + SA R S FOA log-Mel spectrograms + active/reactive $x$, $y$, $z$ 1
    ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ intensity, or GCC-PHAT Ronchini et al. ([2020](#bib.bib273))
    2020 CRNN R S FOA log-Mel spectrograms + log-Mel intensity $x$, $y$, $z$ 1 ✓ ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Sampathkumar and Kowerko ([2020](#bib.bib284)) 2020 CRNN R S MIC
    + FOA Mel spectrograms + active intensity $\theta$, $\phi$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ + GCC-PHAT Shimada et al. ([2020a](#bib.bib293)) 2020 Res. CRNN R S FOA magnitude
    spectrograms + IPD ACCDOA 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Shimada et al. ([2020b](#bib.bib294))
    2020 Res. CRNN R S FOA magnitude spectrograms + IPD ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Singla et al. ([2020](#bib.bib298)) 2020 CRNN R S FOA log-Mel spectrograms +
    log-Mel intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Song ([2020](#bib.bib300))
    2020 CRNN R S GCC-PHAT + FOA active intensity $x$, $y$, $z$ 1 ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗
    ✗ ✓ Sundar et al. ([2020](#bib.bib309)) 2020 Res. CNN C/R S Waveforms $d$, $\theta$
    1-3 ✗ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ Tian ([2020](#bib.bib323)) 2020 CRNN ? S Ambisonics ?
    ? ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Varanasi et al. ([2020](#bib.bib329)) 2020 CNN C S 3rd spherical
    harmonics (phase or phase+magnitude) $\theta$, $\phi$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Varzandeh
    et al. ([2020](#bib.bib331)) 2020 CNN C S GCC-PHAT + periodicity degree $\theta$
    0-1 ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ Vera-Diaz et al. ([2020](#bib.bib337)) 2020 AE R S GCC-PHAT
    time-delay 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wang et al. ([2020](#bib.bib347)) 2020 Res. CRNN
    R S FOA pseudo-intensity + FOA log-Mel spectrograms $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ + GCC-PHAT Xue et al. ([2020](#bib.bib365)) 2020 CRNN C S CPS + waveforms
    + beamforming output $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yasuda et al. ([2020](#bib.bib373))
    2020 Res. CRNN R S FOA log-Mel spectrograms + intensity denoised IV 2 ✓ ✗ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of DL-based SSL systems published in 2021, organized in alphabetical
    order. See Table I’s caption for acronyms specification.'
  prefs: []
  type: TYPE_NORMAL
- en: Author Year Architecture Type Learn. Input features Output Sources Data NoS
    Kno. Mov. Train Test SA RA SR RR SA RA SR RR Adavanne et al. ([2021](#bib.bib5))
    2021 CRNN + SA R S FOA Mel spectrograms + intensity + GCC-PHAT x,y,z 2 ✓ ✓ ✗ ✗
    ✗ ✓ ✗ ✗ ✗ ✓ Bai et al. ([2021](#bib.bib14)) 2021 Res. CRNN R S Log-Mel spectrograms
    + intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Bianco et al. ([2021](#bib.bib26))
    2021 VAE C SS RTF $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Bohlender et al. ([2021](#bib.bib28))
    2021 CNN/CRNN C S Phase map $\theta$ 1-3 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Bologni et al. ([2021](#bib.bib29))
    2021 CNN C S Waveforms $\theta$, $d$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Cao et al. ([2021](#bib.bib39))
    2021 SA R S Log-Mel spectrograms + intensity $x$, $y$, $z$ 0-2 ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗
    ✗ ✓ Castellini et al. ([2021](#bib.bib40)) 2021 MLP R S real + imaginary CPS $x$,
    $y$ 1-3 ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Diaz-Guerra et al. ([2021a](#bib.bib68)) 2021 CNN
    R S SRP-PHAT power map $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Emmanuel et al. ([2021](#bib.bib78))
    2021 CNN + SA R S Log-spectrograms + intensity ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Gelderblom
    et al. ([2021](#bib.bib94)) 2021 MLP C/R S GCC-PHAT $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✗ ✓ Gonçalves Pinto et al. ([2021](#bib.bib99)) 2021 CNN R S Magnitude CPS $x$,
    $y$ 1-10 ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ Grumiaux et al. ([2021a](#bib.bib105)) 2021 CRNN
    C S Intensity $\theta$, $\phi$ 1-3 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Grumiaux et al. ([2021b](#bib.bib106))
    2021 CNN + SA C S Intensity $\theta$, $\phi$ 1-3 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Guirguis
    et al. ([2020](#bib.bib107)) 2021 TCN R S Magnitude + phase spectrograms $x$,
    $y$, $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Hammer et al. ([2021](#bib.bib115)) 2021 U-net
    C S Phase map of the RTF between each mic pair $\theta$ $\infty$ ✗ ✓ ✗ ✗ ✓ ✗ ✗
    ✗ ✗ ✓ He et al. ([2021a](#bib.bib121)) 2021 Res. CNN C WS Magnitude + phase spectrograms
    $\theta$ 1-4 ✓/✗ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✓ He et al. ([2021b](#bib.bib122)) 2021 CNN R
    S Waveforms $x$, $y$, $z$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Huang and Perez ([2021](#bib.bib132))
    2021 Res. CNN + SA R S Waveforms ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Komatsu et al. ([2020](#bib.bib154))
    2021 CRNN R S FOA magnitude + phase spectrograms $\theta$, $\phi$ 1 ✓ ✓ ✓ ✓ ✓
    ✓ ✓ ✓ ✓ ✓ Krause et al. ([2020a](#bib.bib160)) 2021 CNN R S Magnitude + phase
    spectrograms $x$, $y$, $z$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Krause et al. ([2020b](#bib.bib161))
    2021 CRNN R S Misc. $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Lee et al. ([2021b](#bib.bib179))
    2021 U-Net R S SRP power map $x$,$y$ 1-3 ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ Lee et al. ([2021a](#bib.bib178))
    2021 CNN + attention C S Log-Mel spectrograms + intensity $\theta$ 1 ✓ ✓ ✗ ✗ ✓
    ✗ ✗ ✗ ✓ ✓ Liu et al. ([2021](#bib.bib193)) 2021 CNN C S Intensity $\theta$ 1 ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Naranjo-Alcazar et al. ([2021](#bib.bib218)) 2021 Res. CRNN
    R S Log-Mel spectrograms + GCC-PHAT ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Nguyen et al.
    ([2021a](#bib.bib224)) 2021 CRNN C S Intensity/GCC-PHAT $\theta$, $\phi$ 1 ✓ ✓
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Nguyen et al. ([2021b](#bib.bib225)) 2021 CNN + RNN/SA R S Log-spectrograms
    + DRR + SCM eigenvectors ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Park et al. ([2021a](#bib.bib240))
    2021 SA R S log-Mel spectrograms + intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Poschadel et al. ([2021a](#bib.bib256)) 2021 CRNN C S HOA magnitude + phase
    spectrograms $\theta$, $\phi$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Poschadel et al. ([2021b](#bib.bib257))
    2021 CRNN C S HOA magnitude + phase spectrograms $\theta$, $\phi$ 2-3 ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ ✓ Pujol et al. ([2021](#bib.bib260)) 2021 Res. CNN R S Waveforms $\theta$,
    $\phi$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✓ ✓ ✓ Rho et al. ([2021](#bib.bib267)) 2021 CRNN + SA R
    S Log-Mel spectrograms + intensity $\theta$, $\phi$ 1 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Schymura
    et al. ([2021](#bib.bib291)) 2021 CNN + SA R S Magnitude + phase spectrograms
    $\theta$, $\phi$ 1 ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✓ Schymura et al. ([2020](#bib.bib290)) 2021
    CNN + AE + attent. R S FOA magnitude + phase spectrograms $\theta$, $\phi$ 1 ✓
    ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Shimada et al. ([2021](#bib.bib295)) 2021 Res. CRNN + SA R S
    IPD ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Subramanian et al. ([2021a](#bib.bib304)) 2021
    CRNN C/R S Phase spectrogram $\theta$ 2 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ Subramanian et al.
    ([2021b](#bib.bib305)) 2021 CRNN C Phase spectrograms, IPD $\theta$ 2 ✓ ✗ ✗ ✗
    ✓ ✗ ✗ ✗ ✓ ✗ Sudarsanam et al. ([2021](#bib.bib306)) 2021 SA R S Log-Mel spectrograms
    + intensity ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Vargas et al. ([2021](#bib.bib330)) 2021
    CNN C S Phase map $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Vera-Diaz et al. ([2021](#bib.bib338))
    2021 AE R S GCC-PHAT time-delay 2 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wang et al. ([2021](#bib.bib348))
    2021 SA R S Mel-spectr. + intensity/Mel-spectr. + GCC-PHAT $x$, $y$, $z$ 1 ✓ ✓
    ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Wu et al. ([2021c](#bib.bib354)) 2021 AE R S Likelihood surface
    $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Wu et al. ([2021b](#bib.bib353)) 2021 CNN AE R
    S Beamforming heatmap image $x$, $y$ 1 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓ Xinghao et al. ([2021](#bib.bib360))
    2021 CNN + SA R S Log-Mel spectrograms + intensity ACCDOA 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ Xu et al. ([2021a](#bib.bib362)) 2021 DenseNet R S Real CPS $x$, $y$ 6-25 ✗/✓
    ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ ✗ Yalta et al. ([2021](#bib.bib367)) 2021 SA R S Log-Mel spectrograms
    + intensity $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ Yang et al. ([2021a](#bib.bib368))
    2021 CRNN C S Log-magnitude and phase spectrograms $\theta$ 1 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓ ✗ Yang et al. ([2021b](#bib.bib369)) 2021 CRNN C S Log-magnitude and phase spectrograms
    $\theta$ 1 ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ Zhang et al. ([2021](#bib.bib384)) 2021 CNN + SA
    R S Log-spectrograms + intensity + GCC-PHAT $x$, $y$, $z$ 1 ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗
    ✓
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was funded by the French Association for Technological Research (ANRT
    CIFRE contract 2019/0533) and partially funded by the Multidisciplinary Institute
    in Artificial Intelligence MIAI@Grenoble-Alpes (ANR-19-P3IA-0003)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adavanne et al. [2018] S. Adavanne, A. Politis, and T. Virtanen. Direction of
    arrival estimation for multiple sound sources using convolutional recurrent neural
    network. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, Rome, Italy, Sept.
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adavanne et al. [2019a] S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen.
    Sound event localization and detection of overlapping sources using convolutional
    recurrent neural networks. *IEEE J. Sel. Topics Signal Process.*, 13(1):34–48,
    Mar. 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adavanne et al. [2019b] S. Adavanne, A. Politis, and T. Virtanen. A multi-room
    reverberant dataset for sound event localization and detection. In *Proc. Detection
    and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*, New
    York, NY, October 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adavanne et al. [2019c] S. Adavanne, A. Politis, and T. Virtanen. Localization,
    detection and tracking of multiple moving sound sources with a convolutional recurrent
    neural network. In *Proc. Detection and Classification of Acoustic Scenes and
    Events Workshop (DCASE Workshop)*, New York, NY, Apr. 2019c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adavanne et al. [2021] S. Adavanne, A. Politis, and T. Virtanen. Differentiable
    tracking-based training of deep learning sound source localizers. In *Proc. IEEE
    Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 211–215, New Paltz,
    NY, Oct. 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmad et al. [2021] M. Ahmad, M. Muaz, and M. Adeel. A survey of deep neural
    network in acoustic direction finding. In *Proc. IEEE Int. Conf. Digital Futures
    Transf. Technol. (ICoDT2)*, Islamabad, Pakistan, May 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allen and Berkley [1979] J. B. Allen and D. A. Berkley. Image method for efficiently
    simulating small-room acoustics. *J. Acoust. Soc. Am.*, 65(4):943–950, 1979.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amengual Garí et al. [2017] S. V. Amengual Garí, W. Lachenmayr, and E. Mommertz.
    Spatial analysis and auralization of room acoustics using a tetrahedral microphone.
    *J. Acoust. Soc. Am.*, 141(4):EL369–EL374, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anguera et al. [2012] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Friedland,
    and O. Vinyals. Speaker diarization: A review of recent research. *IEEE Trans.
    Audio, Speech, Lang. Process.*, 20(2):356–370, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arberet et al. [2009] S. Arberet, R. Gribonval, and F. Bimbot. A robust method
    to count and locate audio sources in a multichannel underdetermined mixture. *IEEE
    Trans. Signal Process.*, 58(1):121–133, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Argentieri et al. [2015] S. Argentieri, P. Danes, and P. Souères. A survey
    on sound source localization in robotics: From binaural to array processing methods.
    *Computer Speech Lang.*, 34(1):87–112, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Babaee et al. [2018] M. Babaee, Z. Li, and G. Rigoll. Occlusion handling in
    tracking multiple people using RNN. In *Proc. IEEE Int. Conf. Image Process. (ICIP)*,
    pages 2715–2719, Athens, Greece, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. [2016] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation
    by jointly learning to align and translate, May 2016. arXiv:1409.0473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. [2021] J. Bai, Z. Pu, and J. Chen. DCASE 2021 Task 3: SELD system
    based on Resnet and random segment augmentation. Technical report, November 2021.
    DCASE 2021 Challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2019] S. Bai, J. Z. Kolter, and V. Koltun. Trellis networks for
    sequence modeling, Mar. 2019. arXiv:1810.06682.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ban et al. [2018] Y. Ban, X. Li, X. Alameda-Pineda, L. Girin, and R. Horaud.
    Accounting for room acoustics in audio-visual multi-speaker tracking. In *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 6553–6557, Alberta,
    Canada, Apr. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basten et al. [2008] T. Basten, H. de Bree, and S. Sadasivan. Acoustic eyes:
    a novel sound source localization and monitoring technique with 3D sound probes.
    In *Proc. Int. Conf. Noise Vibration Engin. (ISMA)*, Leuven, Belgium, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benesty et al. [2008] J. Benesty, J. Chen, and Y. Huang. *Microphone Array Signal
    Processing*. Springer Science & Business Media, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio [2012] Y. Bengio. Deep learning of representations for unsupervised and
    transfer learning. In *Proc. ICML Workshop Unsupervised & Transfer Learn.*, pages
    17–36, Bellevue, Washington, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. [2013] Y. Bengio, A. Courville, and P. Vincent. Representation
    learning: A review and new perspectives. *IEEE Trans. Pattern Anal. Mach. Intell.*,
    35(8):1798–1828, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bernschütz [2016] B. Bernschütz. *Microphone arrays and sound field decomposition
    for dynamic binaural recording*. PhD thesis, Technische Universitaet Berlin (Germany),
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bialer et al. [2019] O. Bialer, N. Garnett, and T. Tirer. Performance advantages
    of deep neural networks for angle of arrival estimation. In *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*, pages 3907–3911, Brighton, UK, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bianchi et al. [2016] L. Bianchi, F. Antonacci, A. Sarti, and S. Tubaro. The
    ray space transform: a new framework for wave field processing. *IEEE Trans. Signal
    Process.*, 64(21):5696–5706, Nov. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bianco et al. [2019] M. J. Bianco, P. Gerstoft, J. Traer, E. Ozanich, M. A.
    Roch, S. Gannot, and C.-A. Deledalle. Machine learning in acoustics: Theory and
    applications. *J. Acoust. Soc. Am.*, 146(5):3590–3628, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianco et al. [2020] M. J. Bianco, S. Gannot, and P. Gerstoft. Semi-supervised
    source localization with deep generative modeling. In *Proc. IEEE Int. Workshop
    Mach. Learn. Signal Process. (MLSP)*, Eespo, Finland (virtual conference), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianco et al. [2021] M. J. Bianco, S. Gannot, E. Fernandez-Grande, and P. Gerstoft.
    Semi-supervised source localization in reverberant environments with deep generative
    modeling. *IEEE Access*, 9:84956–84970, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blandin et al. [2012] C. Blandin, A. Ozerov, and E. Vincent. Multi-source TDOA
    estimation in reverberant audio using angular spectra and clustering. *Signal
    Process.*, 92(8):1950–1960, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bohlender et al. [2021] A. Bohlender, A. Spriet, W. Tirry, and N. Madhu. Exploiting
    temporal context in CNN based multisource DoA estimation. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 29:1594–1608, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bologni et al. [2021] G. Bologni, R. Heusdens, and J. Martinez. Acoustic reflectors
    localization from stereo recordings using neural networks. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 1–5, Toronto, Canada (virtual
    conference), June 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bouatouch et al. [2006] K. Bouatouch, O. Deille, J. Maillard, J. Martin, and
    N. Noé. Real time acoustic rendering of complex environments including diffraction
    and curved surfaces. In *Proc. Audio Engin. Soc. (AES) Conv.*, Paris, France,
    2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brandstein and Ward [2001] M. Brandstein and D. Ward. *Microphone Arrays: Signal
    Processing Techniques and Applications*. Springer Science & Business Media, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brutti et al. [2010] A. Brutti, L. Cristoforetti, W. Kellermann, L. Marquardt,
    and M. Omologo. WOZ acoustic data collection for interactive TV. *Lang. Resources
    Eval.*, 44(3):205–219, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bush and Xiang [2018] D. Bush and N. Xiang. A model-based Bayesian framework
    for sound source enumeration and direction of arrival estimation using a coprime
    microphone array. *J. Acoust. Soc. Am.*, 143(6):3934–3945, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campbell et al. [2005] D. Campbell, K. Palomaki, and G. Brown. A Matlab simulation
    of shoebox room acoustics for use in research and teaching. *Computing Inform.
    Syst.*, 9(3):48, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Candes et al. [2006] E. J. Candes, J. K. Romberg, and T. Tao. Stable signal
    recovery from incomplete and inaccurate measurements. *Communications on Pure
    and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical
    Sciences*, 59(8):1207–1223, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2019a] Y. Cao, T. Iqbal, Q. Kong, M. B. Galindo, W. Wang, and M. D.
    Plumbley. Two-stage sound event localization and detection using intensity vector
    and generalized cross-correlation. Technical report, 2019a. DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2019b] Y. Cao, Q. Kong, T. Iqbal, F. An, W. Wang, and M. Plumbley.
    Polyphonic sound event detection and localization using a two-stage strategy.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, New York, NY, October 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2020] Y. Cao, T. Iqbal, Q. Kong, Y. Zhong, W. Wang, and M. D. Plumbley.
    Event-independent network for polyphonic sound event localization and detection.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, Tokyo, Japan, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2021] Y. Cao, T. Iqbal, Q. Kong, F. An, W. Wang, and M. D. Plumbley.
    An improved event-independent network for polyphonic sound event localization
    and detection. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    Toronto, Canada (virtual conference), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castellini et al. [2021] P. Castellini, N. Giulietti, N. Falcionelli, A. F.
    Dragoni, and P. Chiariotti. A neural network based microphone array approach to
    grid-less noise source localization. *Applied Acoustics*, 177:107947, 2021. ISSN
    0003-682X.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakrabarty and Habets [2017a] S. Chakrabarty and E. A. P. Habets. Broadband
    DoA estimation using convolutional neural networks trained with noise signals.
    In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 136–140,
    New Paltz, NY, 2017a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakrabarty and Habets [2017b] S. Chakrabarty and E. A. P. Habets. Multi-speaker
    localization using convolutional neural network trained with noise, 2017b. arXiv:1712.04276.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakrabarty and Habets [2019a] S. Chakrabarty and E. A. P. Habets. Multi-scale
    aggregation of phase information for reducing computational cost of CNN based
    DoA estimation. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, A Coruña, Spain,
    2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakrabarty and Habets [2019b] S. Chakrabarty and E. A. P. Habets. Multi-speaker
    DoA estimation using deep convolutional networks trained with noise signals. *IEEE
    J. Sel. Topics Signal Process.*, 13(1):8–21, 2019b. ISSN 1932-4553, 1941-0484.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. [2018] S.-Y. Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi,
    A. van den Oord, and O. Vinyals. Temporal modeling using dilated convolution and
    gating for voice-activity-detection. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 5549–5553, Calgary, Canada, Apr. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chardon and Daudet [2012] G. Chardon and L. Daudet. Narrowband source localization
    in an unknown reverberant environment using wavefield sparse decomposition. In
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 9–12,
    Kyoto, Japan, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chazan et al. [2019] S. E. Chazan, H. Hammer, G. Hazan, J. Goldberger, and S. Gannot.
    Multi-microphone speaker separation based on deep DoA estimation. In *Proc. Europ.
    Signal Process. Conf. (EUSIPCO)*, A Coruña, Spain, Sept. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiariotti et al. [2019] P. Chiariotti, M. Martarelli, and P. Castellini. Acoustic
    beamforming for noise source localization – reviews, methodology and applications.
    *Mech. Systems Signal Process.*, 120:422–448, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. [2014] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,
    H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder
    for statistical machine translation, Sept. 2014. arXiv:1406.1078.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi and Chang [2020] J. Choi and J.-H. Chang. Convolutional neural network-based
    DoA estimation using stereo microphones for drone. In *Int. Conf. Electron., Inform.,
    Comm. (ICEIC)*, pages 1–5, Barcelona, Spain, Jan. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet [2017] F. Chollet. *Deep Learning with Python*. Simon and Schuster,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chytas and Potamianos [2019] S. P. Chytas and G. Potamianos. Hierarchical detection
    of sound events and their localization using convolutional neural networks with
    adaptive thresholds. In *Proc. Detection and Classification of Acoustic Scenes
    and Events Workshop (DCASE Workshop)*, New York, NY, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ciaparrone et al. [2020] G. Ciaparrone, F. Luque Sánchez, S. Tabik, L. Troiano,
    R. Tagliaferri, and F. Herrera. Deep learning in video multi-object tracking:
    A survey. *Neurocomp.*, 381:61–88, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobos et al. [2017] M. Cobos, F. Antonacci, A. Alexandridis, A. Mouchtaris,
    and B. Lee. A survey of sound source localization methods in wireless acoustic
    sensor networks. *Wireless Comm. Mobile Computing*, 2017, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen [2004] I. Cohen. Relative transfer function identification using speech
    signals. *IEEE Trans. Speech Audio Process.*, 12(5):451–459, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen et al. [2019] T. Cohen, M. Weiler, B. Kicanaoglu, and M. Welling. Gauge
    equivariant convolutional networks and the icosahedral cnn. In *International
    conference on Machine learning*, pages 1321–1330\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comanducci et al. [2020a] L. Comanducci, F. Borra, P. Bestagini, F. Antonacci,
    S. Tubaro, and A. Sarti. Source localization using distributed microphones in
    reverberant environments based on deep learning and ray space transform. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 28:2238–2251, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comanducci et al. [2020b] L. Comanducci, M. Cobos, F. Antonacci, and A. Sarti.
    Time difference of arrival estimation from frequency-sliding generalized cross-correlations
    using convolutional neural networks. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 4945–4949, Barcelona, Spain (virtual conference),
    May 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comminiello et al. [2019] D. Comminiello, M. Lella, S. Scardapane, and A. Uncini.
    Quaternion convolutional neural networks for detection and localization of 3D
    sound events. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    Brighton, UK, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawford and Pineau [2020] E. Crawford and J. Pineau. Exploiting spatial invariance
    for scalable unsupervised object tracking. In *Proc. AAAI Conf. Artif. Intell.*,
    New York, NY, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cristoforetti et al. [2014] L. Cristoforetti, M. Ravanelli, M. Omologo, A. Sosi,
    and A. Abad. The DIRHA simulated corpus. In *Int. Conf. Lang. Resources Eval.
    (LREC)*, pages 2629–2634, Reykjavik, Iceland, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daniel and Kitić [2020] J. Daniel and S. Kitić. Time-domain velocity vector
    for retracing the multipath propagation. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 421–425, Barcelona, Spain (virtual conference),
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datum et al. [1996] M. S. Datum, F. Palmieri, and A. Moiseff. An artificial
    neural network for sound localization using binaural cues. *J. Acoust. Soc. Am.*,
    100(1):372–383, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Bree [2003] H.-E. de Bree. An overview of microflown technologies. *Acta
    Acustica united with Acustica*, 89(1):163–172, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleforge and Horaud [2012] A. Deleforge and R. Horaud. 2D sound-source localization
    on the binaural manifold. In *Proc. IEEE Int. Workshop Mach. Learn. Signal Process.
    (MLSP)*, pages 1–6, Santander, Spain, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleforge et al. [2013] A. Deleforge, F. Forbes, and R. Horaud. Variational
    EM for binaural sound-source separation and localization. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, Vancouver, Canada, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleforge et al. [2015] A. Deleforge, R. Horaud, Y. Y. Schechner, and L. Girin.
    Co-localization of audio sources in images using binaural features and locally-linear
    regression. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 23(4):718–731, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diaz-Guerra et al. [2021a] D. Diaz-Guerra, A. Miguel, and J. R. Beltran. Robust
    sound source tracking using SRP-PHAT and 3D convolutional neural networks. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 29:300–311, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diaz-Guerra et al. [2021b] D. Diaz-Guerra, A. Miguel, and J. R. Beltran. gpuRIR:
    A python library for room impulse response simulation with GPU acceleration. *Multimedia
    Tools Applic.*, 80(4):5653–5671, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DiBiase et al. [2001] J. H. DiBiase, H. F. Silverman, and M. S. Brandstein.
    Robust localization in reverberant rooms. In M. Brandstein and D. Ward, editors,
    *Microphone Arrays: Signal Processing Techniques and Applications*, pages 157–180.
    Springer, Berlin, 2001. ISBN 978-3-662-04619-7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dmochowski et al. [2007] J. P. Dmochowski, J. Benesty, and S. Affes. Broadband
    MUSIC: Opportunities and challenges for multiple source localization. In *Proc.
    IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 18–21, New
    Paltz, NY, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dorfan and Gannot [2015] Y. Dorfan and S. Gannot. Tree-based recursive expectation-maximization
    algorithm for localization of acoustic sources. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 23(10):1692–1703, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duong et al. [2010] N. Q. Duong, E. Vincent, and R. Gribonval. Under-determined
    reverberant audio source separation using a full-rank spatial covariance model.
    *IEEE Trans. Audio, Speech, Lang. Process.*, 18(7):1830–1840, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dvorkind and Gannot [2005] T. G. Dvorkind and S. Gannot. Time difference of
    arrival estimation of speech source in a noisy and reverberant environment. *Signal
    Process.*, 85(1):177–204, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eaton et al. [2015] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor.
    The ACE challenge — Corpus description and performance evaluation. In *Proc. IEEE
    Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 1–5, New Paltz,
    NY, Oct. 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: El Zooghby et al. [2000] A. El Zooghby, C. Christodoulou, and M. Georgiopoulos.
    A neural network-based smart antenna for multiple source tracking. *IEEE Trans.
    Antennas Propag.*, 48(5):768–776, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elbir [2020] A. M. Elbir. DeepMUSIC: multiple signal classification via deep
    learning. *IEEE Sensors Lett.*, 4(4):1–4, Apr. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emmanuel et al. [2021] P. Emmanuel, N. Parrish, and M. Horton. Multi-scale network
    for sound event localization and detection. Technical report, 2021. DCASE 2021
    Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engel et al. [2020] J. Engel, L. Hantrakul, C. Gu, and A. Roberts. DDSP: Differentiable
    digital signal processing, 2020. arXiv:2001.04643.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erdogan et al. [2016] H. Erdogan, J. R. Hershey, S. Watanabe, M. Mandel, and
    J. Le Roux. Improved MVDR beamforming using single-channel mask prediction networks.
    In *Proc. Interspeech Conf.*, San Francisco, CA, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Escolano et al. [2014] J. Escolano, N. Xiang, J. M. Perez-Lorenzo, M. Cobos,
    and J. J. Lopez. A Bayesian direction-of-arrival model for an undetermined number
    of sources using a two-microphone array. *J. Acoust. Soc. Am.*, 135(2):742–753,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evers et al. [2014] C. Evers, A. H. Moore, and P. A. Naylor. Multiple source
    localisation in the spherical harmonic domain. In *Proc. IEEE Int. Workshop Acoustic
    Signal Enhanc. (IWAENC)*, pages 258–262, Antibes, France, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evers et al. [2020] C. Evers, H. W. Löllmann, H. Mellmann, A. Schmidt, H. Barfuss,
    P. A. Naylor, and W. Kellermann. The LOCATA challenge: Acoustic source localization
    and tracking. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 28:1620–1643, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fahim et al. [2020] A. Fahim, P. N. Samarasinghe, and T. D. Abhayapala. Multi-source
    DoA estimation through pattern recognition of the modal coherence of a reverberant
    soundfield. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 28:605–618, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Falong et al. [1993] L. Falong, J. Hongbing, and Z. Xiaopeng. The ML bearing
    estimation by using neural networks. *J. Electronics (China)*, 10(1):1–8, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernandez-Grande et al. [2021] E. Fernandez-Grande, M. J. Bianco, S. Gannot,
    and P. Gerstoft. DTU three-channel room impulse response dataset for direction
    of arrival estimation 2020, 2021. URL [https://dx.doi.org/10.21227/c5cn-jv76](https://dx.doi.org/10.21227/c5cn-jv76).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunati et al. [2014] S. Fortunati, R. Grasso, F. Gini, M. S. Greco, and K. LePage.
    Single-snapshot DOA estimation by using compressed sensing. *EURASIP J. Advances
    Signal Process.*, 2014(1):1–17, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foucart and Rauhut [2013] S. Foucart and H. Rauhut. An invitation to compressive
    sensing. In *A mathematical introduction to compressive sensing*, pages 1–39\.
    Springer, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Francombe [2017] J. Francombe. IoSR listening room multichannel BRIR dataset,
    2017. University of Surrey.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gannot et al. [2017] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov.
    A consolidated perspective on multimicrophone speech enhancement and source separation.
    *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 25(4):692–730, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gannot et al. [2019] S. Gannot, M. Haardt, W. Kellermann, and P. Willett. Introduction
    to the issue on acoustic source localization and tracking in dynamic real-life
    scenes. *IEEE J. Sel. Topics Signal Process.*, 13(1):3–7, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garofolo et al. [1993a] J. Garofolo, D. Graff, D. Paul, and D. Pallett. CSR-I
    (WSJ0) Sennheiser LDC93S6B. *Linguistic Data Consortium, Philadelphia,*, 1993a.
    URL [https://catalog.ldc.upenn.edu/LDC93S6B](https://catalog.ldc.upenn.edu/LDC93S6B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garofolo et al. [1993b] J. S. Garofolo, L. Lamel, W. M. Fisher, J. G. Fiscus,
    D. S. Pallett, N. L. Dahlgren, and V. Zue. TIMIT Acoustic-Phonetic Continuous
    Speech Corpus. *Linguistic Data Consortium, Philadelphia,*, 1993b. URL [https://catalog.ldc.upenn.edu/LDC93s1](https://catalog.ldc.upenn.edu/LDC93s1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gelderblom et al. [2021] F. B. Gelderblom, Y. Liu, J. Kvam, and T. A. Myrvoll.
    Synthetic data for DNN-based DoA estimation of indoor speech. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, Toronto, Canada (virtual conference),
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerstoft et al. [2016] P. Gerstoft, C. F. Mecklenbräuker, A. Xenaki, and S. Nannuru.
    Multisnapshot sparse Bayesian learning for DOA. *IEEE Signal Process. Lett.*,
    23(10):1469–1473, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerstoft et al. [2018] P. Gerstoft, C. F. Mecklenbräuker, W. Seong, and M. Bianco.
    Introduction to compressive sensing in acoustics. *J. Acoust. Soc. Am.*, 143(6):3731–3736,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerzon [1992] M. A. Gerzon. General metatheory of auditory localisation. In
    *Proc. Audio Engin. Soc. (AES) Conv.*, Vienna, Austria, march 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girin et al. [2021] L. Girin, S. Leglaive, X. Bie, J. Diard, T. Hueber, and
    X. Alameda-Pineda. Dynamical variational autoencoders: A comprehensive review.
    *Foundations Trends Mach. Learn.*, 15(1–2):1–175, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gonçalves Pinto et al. [2021] W. Gonçalves Pinto, M. Bauerheim, and H. Parisot-Dupuis.
    Deconvoluting acoustic beamforming maps with a deep neural network. In *Proc.
    Inter-Noise Conf.*, pages 5397–5408, virtual conference, Aug. 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Nets. In *Proc.
    Advances Neural Inform. Process. Syst. (NIPS)*, Montréal, Canada, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2016] I. Goodfellow, Y. Bengio, and A. Courville. *Deep Learning*.
    MIT Press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goryn and Kaveh [1988] D. Goryn and M. Kaveh. Neural networks for narrowband
    and wideband direction finding. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 2164–2167, New-York, NY, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grondin et al. [2019] F. Grondin, I. Sobieraj, M. Plumbley, and J. Glass. Sound
    event localization and detection using CRNN on pairs of microphones. In *Proc.
    Detection and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*,
    pages 84–88, New York University, NY, USA, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grumiaux et al. [2020] P.-A. Grumiaux, S. Kitic, L. Girin, and A. Guerin. High-resolution
    speaker counting in reverberant rooms using CRNN with Ambisonics features. In
    *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, Amsterdam, The Netherlands (virtual
    conference), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grumiaux et al. [2021a] P.-A. Grumiaux, S. Kitic, L. Girin, and A. Guérin. Improved
    feature extraction for CRNN-based multiple sound source localization. In *Proc.
    Europ. Signal Process. Conf. (EUSIPCO)*, Dublin, Ireland (virtual conference),
    2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grumiaux et al. [2021b] P.-A. Grumiaux, S. Kitic, P. Srivastava, L. Girin,
    and A. Guérin. SALADnet: Self-attentive multisource localization in the Ambisonics
    domain. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    New Paltz, NY (virtual conference), 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guirguis et al. [2020] K. Guirguis, C. Schorn, A. Guntoro, S. Abdulatif, and
    B. Yang. SELD-TCN: sound event localization & detection via temporal convolutional
    networks. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 16–20, Amsterdam,
    The Netherlands (virtual conference), 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guizzo et al. [2021] E. Guizzo, R. F. Gramaccioni, S. Jamili, C. Marinoni,
    E. Massaro, C. Medaglia, G. Nachira, L. Nucciarelli, L. Paglialunga, M. Pennese,
    et al. L3DAS21 Challenge: Machine Learning for 3D Audio Signal Processing, 2021.
    arXiv:2104.05499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulati et al. [2020] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu,
    W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: convolution-augmented
    Transformer for speech recognition. In *Proc. Interspeech Conf.*, pages 5036–5040,
    Shanghai, China (virtual conference), Oct. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Habets [2006] E. A. P. Habets. Room impulse response generator. Technical report,
    Technische Universiteit Eindhoven, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Habets [2022] E. A. P. Habets. Signal generator, 2022. URL [https://github.com/ehabets/Signal-Generator/](https://github.com/ehabets/Signal-Generator/).
    (Last viewed March 31, 2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadad et al. [2014] E. Hadad, F. Heese, P. Vary, and S. Gannot. Multichannel
    audio database in various acoustic environments. In *Proc. IEEE Int. Workshop
    Acoustic Signal Enhanc. (IWAENC)*, pages 313–317, Antibes, France, Sept. 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hahmann et al. [2021a] M. Hahmann, S. Verburg, and E. Fernandez-Grande. Acoustic
    frequency responses of an empty cuboid room. 2021a. URL [https://data.dtu.dk/articles/dataset/Acoustic_frequency_responses_of_an_empty_cuboid_room/13315289](https://data.dtu.dk/articles/dataset/Acoustic_frequency_responses_of_an_empty_cuboid_room/13315289).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hahmann et al. [2021b] M. Hahmann, S. A. Verburg, and E. Fernandez-Grande. Spatial
    reconstruction of sound fields using local and data-driven functions. *J. Acoust.
    Soc. Am.*, 150(6):4417–4428, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hammer et al. [2021] H. Hammer, S. E. Chazan, J. Goldberger, and S. Gannot.
    Dynamically localizing multiple speakers based on the time-frequency domain. *EURASIP
    J. Audio, Speech, Music Process.*, 2021(1):1–10, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. [2020] Y. Hao, A. Küçük, A. Ganguly, and I. M. S. Panahi. Spectral
    flux-based convolutional neural network architecture for speech source localization
    and its real-time implementation. *IEEE Access*, 8:197047–197058, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
    for image recognition. In *Proc. IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*,
    pages 770–778, Las Vegas, NV, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2018a] W. He, P. Motlicek, and J.-M. Odobez. Deep neural networks
    for multiple speaker detection and localization. In *IEEE Int. Conf. Robotics
    Autom. (ICRA)*, pages 74–79, Brisbane, Australia, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2018b] W. He, P. Motlicek, and J.-M. Odobez. Joint localization and
    classification of multiple sound sources using a multi-task neural network. In
    *Proc. Interspeech Conf.*, pages 312–316, Hyderabad, India, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2019a] W. He, P. Motlicek, and J.-M. Odobez. Adaptation of multiple
    sound source localization neural networks with weak supervision and domain-adversarial
    training. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 770–774, Brighton, UK, May 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2021a] W. He, P. Motlicek, and J.-M. Odobez. Neural network adaptation
    and data augmentation for multi-speaker direction-of-arrival estimation. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 29:1303–1317, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2021b] Y. He, N. Trigoni, and A. Markham. SoundDet: polyphonic moving
    sound event detection and localization from raw waveform. In *Proc. Int. Conf.
    Mach. Learn. (ICML)*, virtual conference, June 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2019b] Z. He, J. Li, D. Liu, H. He, and D. Barber. Tracking by animation:
    Unsupervised learning of multi-object attentive trackers. In *Proc. IEEE Conf.
    Computer Vision Pattern Recogn. (CVPR)*, pages 1318–1327, Long Beach, CA, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heymann et al. [2016] J. Heymann, L. Drude, and R. Haeb-Umbach. Neural network
    based spectral mask estimation for acoustic beamforming. In *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*, Shanghai, China, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heymann et al. [2017] J. Heymann, L. Drude, C. Boeddeker, P. Hanebrink, and
    R. Haeb-Umbach. Beamnet: End-to-end training of a beamformer-supported multi-channel
    ASR system. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    New Orleans, LA, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hickling et al. [1993] R. Hickling, W. Wei, and R. Raspet. Finding the direction
    of a sound source using a vector sound-intensity probe. *J. Acoust. Soc. Am.*,
    94(4):2408–2412, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higuchi et al. [2017] T. Higuchi, K. Kinoshita, M. Delcroix, K. Zmolkova, and
    T. Nakatani. Deep clustering-based beamforming for separation with unknown number
    of sources. In *Proc. Interspeech Conf.*, Stockholm, Sweden, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirvonen [2015] T. Hirvonen. Classification of spatial audio location and content
    using convolutional neural networks. In *Audio Eng. Soc. Conv.*, Warsaw, Poland,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term
    memory. *Neural Comp.*, 9(8):1735–1780, Nov. 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hogg et al. [2021] A. O. Hogg, V. W. Neo, S. Weiss, C. Evers, and P. A. Naylor.
    A polynomial eigenvalue decomposition MUSIC approach for broadband sound source
    localization. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    New Paltz, NY (virtual conference), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2020] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu. Squeeze-and-excitation
    networks. *IEEE Trans. Pattern Anal. Mach. Intell.*, 42(8):2011–2023, Aug. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Perez [2021] D. Huang and R. Perez. SSELDNET: A fully end-to-end
    sample-level framework for sound event localization and detection. Technical report,
    November 2021. DCASE 2021 Challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2018] Y. Huang, X. Wu, and T. Qu. DNN-based sound source localization
    method with microphone array. In *Proc. Int. Conf. Inform., Electron. Comm. Eng.
    (IECE)*, Beijing, China, Dec. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2019] Y. Huang, X. Wu, and T. Qu. A time-domain end-to-end method
    for sound source localization using multi-task learning. In *Proc. IEEE Int. Conf.
    Inform. Comm. Signal Process. (ICSP)*, pages 52–56, Weihai, China, Sept. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2020] Y. Huang, X. Wu, and T. Qu. A time-domain unsupervised learning
    based sound source localization method. In *Int. Conf. Inform. Comm. Signal Process.*,
    pages 26–32, Shanghai, China, Sept. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hübner et al. [2021] F. Hübner, W. Mack, and E. A. P. Habets. Efficient training
    data generation for phase-based DoA estimation. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, Toronto, Canada (virtual conference), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobsen and Juhl [2013] F. Jacobsen and P. M. Juhl. *Fundamentals of General
    Linear Acoustics*. John Wiley & Sons, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jarrett et al. [2012] D. Jarrett, E. Habets, M. Thomas, and P. Naylor. Rigid
    sphere room impulse response simulation: Algorithm and applications. *J. Acoust.
    Soc. Am.*, 132(3):1462–1472, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jarrett et al. [2010] D. P. Jarrett, E. A. Habets, and P. A. Naylor. 3D source
    localization in the spherical harmonic domain using a pseudointensity vector.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 442–446, Aalborg, Denmark,
    2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jarrett et al. [2017] D. P. Jarrett, E. A. Habets, and P. A. Naylor. *Theory
    and Applications of Spherical Microphone Array Processing*. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jenrungrot et al. [2020] T. Jenrungrot, V. Jayaram, S. Seitz, and I. Kemelmacher-Shlizerman.
    The cone of silence: speech separation by localization, 2020. arXiv:2010.06007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jha and Durrani [1989] S. Jha and T. Durrani. Bearing estimation using neural
    optimisation methods. In *Proc. IEE Int. Conf. Artif. Neural Networks*, pages
    129–133, London, UK, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jha and Durrani [1991] S. Jha and T. Durrani. Direction of arrival estimation
    using artificial neural networks. *IEEE Trans. Systems, Man, Cybern.*, 21(5):1192–1201,
    1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jha et al. [1988] S. Jha, R. Chapman, and T. Durrani. Bearing estimation using
    neural networks. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 2156–2159, New-York, NY, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kapka and Lewandowski [2019] S. Kapka and M. Lewandowski. Sound source detection,
    localization and classification using consecutive ensemble of CRNN models. In
    *Proc. Detection and Classification of Acoustic Scenes and Events Workshop (DCASE
    Workshop)*, New York, NY, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karthik et al. [2020] S. Karthik, A. Prabhu, and V. Gandhi. Simple unsupervised
    multi-object tracking, 2020. arXiv:2006.02609.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim and Hahn [2018] J. Kim and M. Hahn. Voice activity detection using an adaptive
    context attention model. *IEEE Signal Process. Lett.*, 25(8):1181–1185, Aug. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim [2014] Y. Kim. Convolutional neural networks for sentence classification,
    2014. arXiv:1408.5882.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim and Ling [2011] Y. Kim and H. Ling. Direction of arrival estimation of humans
    with a small sensor array using an artificial neural network. *Prog. Electromagn.
    Research*, 27:127–149, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2014] D. P. Kingma and M. Welling. Auto-encoding variational
    Bayes. In *Proc. Int. Conf. Learning Repres. (ICLR)*, Banff, Canada, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitić and Guérin [2018] S. Kitić and A. Guérin. TRAMP: Tracking by a Real-time
    AMbisonic-based Particle filter. In *IEEE-AASP Challenge on Acoustic Source Localization
    and Tracking (LOCATA)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitić et al. [2014] S. Kitić, N. Bertin, and R. Gribonval. Hearing behind walls:
    localizing sources in the room next door with cosparsity. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 3087–3091, Florence, Italy,
    2014\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knapp and Carter [1976] C. Knapp and G. Carter. The generalized correlation
    method for estimation of time delay. *IEEE Trans. Acoust., Speech, Signal Process.*,
    24(4):320–327, Aug. 1976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Komatsu et al. [2020] T. Komatsu, M. Togami, and T. Takahashi. Sound event localization
    and detection using convolutional recurrent neural networks and gated linear units.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 41–45, Amsterdam, The
    Netherlands (virtual conference), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. [2019] Q. Kong, Y. Cao, T. Iqbal, W. Wang, and M. D. Plumbley. Cross-task
    learning for audio tagging, sound event detection and spatial localization. Technical
    report, 2019. DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kounades-Bastian et al. [2017] D. Kounades-Bastian, L. Girin, X. Alameda-Pineda,
    S. Gannot, and R. Horaud. An EM algorithm for joint source separation and diarisation
    of multichannel convolutive speech mixtures. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, pages 16–20, New Orleans, LA, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kouw and Loog [2019] W. M. Kouw and M. Loog. A review of domain adaptation without
    target labels. *IEEE transactions on pattern analysis and machine intelligence*,
    43(3):766–785, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koyama et al. [2021] S. Koyama, T. Nishida, K. Kimura, T. Abe, N. Ueno, and
    J. Brunnström. MeshRIR: A dataset of room impulse responses on meshed grid points
    for evaluating sound field analysis and synthesis methods. In *Proc. IEEE Workshop
    Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 1–5, New Paltz, NY (virtual
    conference), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krause and Kowalczyk [2019] D. Krause and K. Kowalczyk. Arborescent neural network
    architectures for sound event detection and localization. Technical report, 2019.
    DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krause et al. [2020a] D. Krause, A. Politis, and K. Kowalczyk. Comparison of
    convolution types in CNN-based feature extraction for sound source localization.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 820–824, Amsterdam, The
    Netherlands (virtual conference), 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krause et al. [2020b] D. Krause, A. Politis, and K. Kowalczyk. Feature overview
    for joint modeling of sound event detection and localization using a microphone
    array. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 31–35, Amsterdam,
    The Netherlands (virtual conference), 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krause et al. [2021] D. Krause, A. Politis, and K. Kowalczyk. Data diversity
    for improving DNN-based localization of concurrent sound events. In *Proc. Europ.
    Signal Process. Conf. (EUSIPCO)*, pages 236–240, Dublin, Ireland (virtual conference),
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kristoffersen et al. [2021] M. S. Kristoffersen, M. B. Møller, P. Martínez-Nuevo,
    and J. Østergaard. Deep sound field reconstruction in real rooms: Introducing
    the ISOBEL sound field dataset, 2021. arXiv:2102.06455.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2017] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet
    classification with deep convolutional neural networks. *Comm. ACM*, 60(6):84–90,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kujawski et al. [2019] A. Kujawski, G. Herold, and E. Sarradj. A deep learning
    method for grid-free localization and quantification of sound sources. *J. Acoust.
    Soc. Am.*, 146(3):EL225–EL231, Sept. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuttruff [2016] H. Kuttruff. *Room Acoustics*. CRC Press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Küçük et al. [2019] A. Küçük, A. Ganguly, Y. Hao, and I. M. S. Panahi. Real-time
    convolutional neural network-based speech source localization on smartphone. *IEEE
    Access*, 7:169969–169978, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lamel et al. [1991] L. Lamel, J.-L. Gauvain, and M. Eskenazi. BREF, a large
    vocabulary spoken corpus for French. In *Proc. Europ. Conf. Speech Comm. Technol.
    (Eurospeech)*, pages 4–7, Genove, Italy, 1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landschoot and Xiang [2019] C. R. Landschoot and N. Xiang. Model-based Bayesian
    direction of arrival analysis for sound sources using a spherical microphone array.
    *J. Acoust. Soc. Am.*, 146(6):4936–4946, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lathoud et al. [2004] G. Lathoud, J.-M. Odobez, and D. Gatica-Perez. AV16.3:
    an audio-visual corpus for speaker localization and tracking. In *Proc. Int. Workshop
    Mach. Learn. Multimodal Interact.*, pages 182–195, Martigny, Switzerland, 2004.
    ISBN 978-3-540-24509-4 978-3-540-30568-2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laufer-Goldshtein et al. [2020] B. Laufer-Goldshtein, R. Talmon, and S. Gannot.
    Data-driven multi-microphone speaker localization on manifolds. *Found. and Trends
    in Signal Process.*, 14(1–2):1–161, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le Moing et al. [2020] G. Le Moing, P. Vinayavekhin, T. Inoue, J. Vongkulbhisal,
    A. Munawar, R. Tachibana, and D. J. Agravante. Learning multiple sound source
    2D localization. In *Proc. IEEE Int. Workshop Multimedia Signal Process. (MMSP)*,
    Tampere, Finland (virtual conference), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le Moing et al. [2021] G. Le Moing, P. Vinayavekhin, D. J. Agravante, T. Inoue,
    J. Vongkulbhisal, A. Munawar, and R. Tachibana. Data-efficient framework for real-world
    multiple sound source 2D localization. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, Toronto, Canada (virtual conference), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lea et al. [2017] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager.
    Temporal convolutional networks for action segmentation and detection. In *Proc.
    IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*, pages 1003–1012, Honolulu,
    HI, July 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. *Nature*,
    521(7553):436–444, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2016] H. Lee, J. Cho, M. Kim, and H. Park. DNN-based feature enhancement
    using DoA-constrained ICA for robust speech recognition. *IEEE Signal Process.
    Lett.*, 23(8):1091–1095, Aug. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2017] J. Lee, J. Park, K. L. Kim, and J. Nam. Sample-level deep
    convolutional neural networks for music auto-tagging using raw waveforms, 2017.
    arXiv:1703.01789.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2021a] S.-H. Lee, J.-W. Hwang, S.-B. Seo, and H.-M. Park. Sound
    event localization and detection using cross-modal attention and parameter sharing
    for DCASE2021 challenge. Technical report, November 2021a. DCASE 2021 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2021b] S. Y. Lee, J. Chang, and S. Lee. Deep learning-based method
    for multiple sound source localization with high resolution and accuracy. *Mech.
    Syst. Signal Process.*, 161:107959, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leglaive et al. [2019] S. Leglaive, L. Girin, and R. Horaud. Semi-supervised
    multichannel speech enhancement with variational autoencoders and non-negative
    matrix factorization. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, pages 101–105, Brighton, UK, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lehmann and Johansson [2010] E. A. Lehmann and A. M. Johansson. Diffuse reverberation
    model for efficient image-source simulation of room impulse responses. *IEEE Trans.
    Audio, Speech, Lang. Process.*, 18(6):1429–1439, Aug. 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leung and Ren [2019] S. Leung and Y. Ren. Spectrum combination and convolutional
    recurrent neural networks for joint localization and detection of sound events.
    Technical report, June 2019. DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2016a] B. Li, T. N. Sainath, R. J. Weiss, K. W. Wilson, and M. Bacchiani.
    Neural network adaptive beamforming for robust multichannel speech recognition.
    In *Proc. Interspeech Conf.*, San Francisco, CA, 2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2018] Q. Li, X. Zhang, and H. Li. Online direction of arrival estimation
    based on deep learning. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, pages 2616–2620, Calgary, Canada, Apr. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2015] X. Li, L. Girin, R. Horaud, and S. Gannot. Estimation of relative
    transfer function in the presence of stationary noise based on segmental power
    spectral density matrix subtraction. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 320–324, Brisbane, Australia, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2016b] X. Li, L. Girin, F. Badeig, and R. Horaud. Reverberant sound
    localization with a robot head based on direct-path relative transfer function.
    In *Proc. IEEE/RSJ Int. Conf. Intell. Robots Systems (IROS)*, pages 2819–2826,
    Daejeon, Korea, 2016b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2016c] X. Li, L. Girin, R. Horaud, and S. Gannot. Estimation of the
    direct-path relative transfer function for supervised sound source localization.
    *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 24(11):2171–2186, 2016c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2016d] X. Li, R. Horaud, L. Girin, and S. Gannot. Voice activity
    detection based on statistical likelihood ratio with adaptive thresholding. In
    *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*, pages 1–5, Xi’an,
    China, 2016d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2017] X. Li, L. Girin, R. Horaud, and S. Gannot. Multiple-speaker
    localization based on direct-path features and likelihood maximization with spatial
    sparsity regularization. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 25(10):1997–2012,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang and Zhou [2018] Y. Liang and Y. Zhou. Lstm multiple object tracker combining
    multiple cues. In *Proc. IEEE Int. Conf. Image Process. (ICIP)*, pages 2351–2355,
    Athens, Greece, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2022] X. Lin, L. Girin, and X. Alameda-Pineda. Unsupervised multiple-object
    tracking with a dynamical variational autoencoder, 2022. arXiv:2202.09315.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin and Wang [2019] Y. Lin and Z. Wang. A report on sound event localization
    and detection. Technical report, 2019. DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021] N. Liu, H. Chen, K. Songgong, and Y. Li. Deep learning assisted
    sound source localization using two orthogonal first-order differential microphone
    arrays. *J. Acoust. Soc. Am.*, 149(2):1069–1084, Feb. 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2012] Z.-M. Liu, Z.-T. Huang, and Y.-Y. Zhou. An efficient maximum
    likelihood method for direction-of-arrival estimation via sparse Bayesian learning.
    *IEEE Trans. Wireless Comm.*, 11(10):1–11, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2018] Z.-M. Liu, C. Zhang, and P. S. Yu. Direction-of-arrival estimation
    based on deep neural networks with robustness to array imperfections. *IEEE Trans.
    Antennas Propag.*, 66(12):7315–7327, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu [2019] Z. Lu. Sound event detection and localization based on CNN and LSTM.
    Technical report, 2019. DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luiten et al. [2020] J. Luiten, I. E. Zulfikar, and B. Leibe. UnOVOST: Unsupervised
    offline video object segmentation and tracking. In *IEEE Winter Conf. Appl. Comput.
    Vis. (WACV)*, pages 1989–1998, Snowmass Village, CO, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2021] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, and T.-K. Kim.
    Multiple object tracking: A literature review. *Artif. Intell.*, 293:103448, 2021.
    ISSN 0004-3702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo and Mesgarani [2019] Y. Luo and N. Mesgarani. Conv-TASnet: Surpassing ideal
    time–frequency magnitude masking for speech separation. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 27(8):1256–1266, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2020] Y. Luo, Z. Chen, N. Mesgarani, and T. Yoshioka. End-to-end
    microphone permutation and number invariant multi-channel speech separation. In
    *ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, pages 6394–6398\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2015] N. Ma, G. Brown, and T. May. Exploiting deep neural networks
    and head movements for binaural localisation of multiple speakers in reverberant
    conditions. In *Proc. Interspeech Conf.*, pages 160–164, Dresden, Germany, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma and Liu [2018] W. Ma and X. Liu. Phased microphone array for sound source
    localization with deep learning. *Aerospace Syst.*, 2(2):71–81, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mabande et al. [2011] E. Mabande, H. Sun, K. Kowalczyk, and W. Kellermann. Comparison
    of subspace-based and steered beamformer-based reflection localization methods.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 146–150, Barcelona, Spain,
    2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mack et al. [2020] W. Mack, U. Bharadwaj, S. Chakrabarty, and E. A. P. Habets.
    Signal-aware broadband DoA estimation using attention mechanisms. In *Proc. IEEE
    Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 4930–4934, Barcelona,
    Spain (virtual conference), May 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mandel et al. [2009] M. I. Mandel, R. J. Weiss, and D. P. Ellis. Model-based
    expectation-maximization source separation and localization. *IEEE Trans. Audio,
    Speech, Lang. Process.*, 18(2):382–394, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markovich-Golan and Gannot [2015] S. Markovich-Golan and S. Gannot. Performance
    analysis of the covariance subtraction method for relative transfer function estimation
    and comparison to the covariance whitening method. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, Brisbane, Australia, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maruri et al. [2019] H. A. C. Maruri, P. L. Meyer, J. Huang, J. A. d. H. Ontiveros,
    and H. Lu. GCC-PHAT cross-correlation audio features for simultaneous sound event
    localization and detection (SELD) in multiple rooms. Technical report, 2019. DCASE
    2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masuyama et al. [2020] Y. Masuyama, Y. Bando, K. Yatabe, Y. Sasaki, M. Onishi,
    and Y. Oikawa. Self-supervised neural audio-visual sound source localization via
    probabilistic spatial modeling. In *Proc. IEEE/RSJ Int. Conf. Intell. Robots Systems
    (IROS)*, pages 4848–4854, Las Vegas, NV, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May et al. [2011] T. May, S. Van De Par, and A. Kohlrausch. A probabilistic
    model for robust localization based on a binaural auditory front-end. *IEEE Trans.
    Audio, Speech, Lang. Process.*, 19(1):1–13, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maysenhölder [1993] W. Maysenhölder. The reactive intensity of general time-harmonic
    structure-borne sound fields. In *Proc. Int. Congress Intensity Techniques*, pages
    63–70, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mazzon et al. [2019] L. Mazzon, Y. Koizumi, M. Yasuda, and N. Harada. First
    order Ambisonics domain spatial augmentation for DNN-based direction of arrival
    estimation. In *Proc. Detection and Classification of Acoustic Scenes and Events
    Workshop (DCASE Workshop)*, New York, NY, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meinhardt et al. [2021] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer.
    Trackformer: Multi-object tracking with transformers, 2021. arXiv:2101.02702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. [2017] Z. Meng, S. Watanabe, J. R. Hershey, and H. Erdogan. Deep
    long short-term memory adaptive beamforming networks for multichannel robust speech
    recognition. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    New Orleans, LA, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Merimaa [2006] J. Merimaa. *Analysis, synthesis, and perception of spatial
    sound: binaural localization modeling and multichannel loudspeaker reproduction*.
    PhD thesis, Helsinki Univ. Technol., 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nam et al. [2013] S. Nam, M. E. Davies, M. Elad, and R. Gribonval. The cosparse
    analysis model and algorithms. *Applied Computational Harmonic Anal.*, 34(1):30–56,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nannuru et al. [2018] S. Nannuru, A. Koochakzadeh, K. L. Gemba, P. Pal, and
    P. Gerstoft. Sparse Bayesian learning for beamforming using sparse linear arrays.
    *J. Acoust. Soc. Am.*, 144(5):2719–2729, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naranjo-Alcazar et al. [2020] J. Naranjo-Alcazar, S. Perez-Castanos, J. Ferrandis,
    P. Zuccarello, and M. Cobos. Sound event localization and detection using squeeze-excitation
    residual CNNs. Technical report, June 2020. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naranjo-Alcazar et al. [2021] J. Naranjo-Alcazar, S. Perez-Castanos, M. Cobos,
    F. J. Ferri, and P. Zuccarello. Sound event localisation and detection using squeeze-excitation
    residual CNNs. Technical report, November 2021. DCASE 2021 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nehorai and Paldi [1994] A. Nehorai and E. Paldi. Acoustic vector-sensor array
    processing. *IEEE Trans. Signal Process.*, 42(9):2481–2491, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. [2018] Q. Nguyen, L. Girin, G. Bailly, F. Elisei, and D.-C. Nguyen.
    Autonomous sensorimotor learning for sound source localization by a humanoid robot.
    In *IEEE/RSJ IROS Workshop Crossmodal Learn. Intell. Robotics*, Madrid, Spain,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. [2020a] T. N. T. Nguyen, W.-S. Gan, R. Ranjan, and D. L. Jones.
    Robust source counting and DoA estimation using spatial pseudo-spectrum and convolutional
    neural network. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 28:2626–2637,
    2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. [2020b] T. N. T. Nguyen, D. L. Jones, and W. S. Gan. Ensemble
    of sequence matching networks for dynamic sound event localization, detection,
    and tracking. In *Proc. Detection and Classification of Acoustic Scenes and Events
    Workshop (DCASE Workshop)*, Tokyo, Japan, November 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. [2020c] T. N. T. Nguyen, D. L. Jones, and W.-S. Gan. A sequence
    matching network for polyphonic sound event localization and detection. In *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 71–75, Barcelona,
    Spain (virtual conference), 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. [2021a] T. N. T. Nguyen, N. K. Nguyen, H. Phan, L. Pham, K. Ooi,
    D. L. Jones, and W.-S. Gan. A general network architecture for sound event localization
    and detection using transfer learning and recurrent neural network. In *Proc.
    IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 935–939, Toronto,
    Canada (virtual conference), June 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. [2021b] T. N. T. Nguyen, K. Watcharasupat, N. K. Nguyen, D. L.
    Jones, and W. S. Gan. DCASE 2021 Task 3: spectrotemporally-aligned features for
    polyphonic sound event localization and detection. Technical report, 2021b. DCASE
    2021 Challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noh et al. [2019] K. Noh, J.-H. Choi, D. Jeon, and J.-H. Chang. Three-stage
    approach for sound event localization and detection. Technical report, 2019. DCASE
    2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nolan et al. [2019] M. Nolan, S. A. Verburg, J. Brunskog, and E. Fernandez-Grande.
    Experimental characterization of the sound field in a reverberation room. *J.
    Acoust. Soc. Am.*, 145(4):2237–2246, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noohi et al. [2013] T. Noohi, N. Epain, and C. T. Jin. Direction of arrival
    estimation for spherical microphone arrays by combination of independent component
    analysis and sparse recovery. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 346–349, Vancouver, Canada, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nugraha et al. [2016] A. A. Nugraha, A. Liutkus, and E. Vincent. Multichannel
    audio source separation with deep neural networks. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 24(9):1652–1664, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nustede and Anemüller [2019] E. J. Nustede and J. Anemüller. Group delay features
    for sound event detection and localization. Technical report, 2019. DCASE 2019
    Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opochinsky et al. [2019] R. Opochinsky, B. Laufer-Goldshtein, S. Gannot, and
    G. Chechik. Deep ranking-based sound source localization. In *Proc. IEEE Workshop
    Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 283–287, New Paltz, NY, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opochinsky et al. [2021] R. Opochinsky, G. Chechik, and S. Gannot. Deep ranking-based
    DoA tracking algorithm. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages
    1020–1024, Dublin, Ireland (virtual conference), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pak and Shin [2019] J. Pak and J. W. Shin. Sound localization based on phase
    difference enhancement using deep neural networks. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 27(8):1335–1345, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pal and Vaidyanathan [2010] P. Pal and P. P. Vaidyanathan. Nested arrays: A
    novel approach to array processing with enhanced degrees of freedom. *IEEE Trans.
    Signal Process.*, 58(8):4167–4181, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. [2019] C. Pang, H. Liu, and X. Li. Multitask learning of time-frequency
    CNN for sound source localization. *IEEE Access*, 7:40725–40737, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parcollet et al. [2018] T. Parcollet, Y. Zhang, M. Morchid, C. Trabelsi, G. Linarès,
    R. De Mori, and Y. Bengio. Quaternion Convolutional Neural Networks for End-to-End
    Automatic Speech Recognition, June 2018. arXiv:1806.07789.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2019a] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
    Cubuk, and Q. V. Le. SpecAugment: a simple data augmentation method for automatic
    speech recognition. In *Proc. Interspeech Conf.*, pages 2613–2617, Graz, Austria,
    Sept. 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. [2019b] S. Park, W. Lim, S. Suh, and Y. Jeong. TrellisNet-based
    architecture for sound event localization and detection with reassembly learning.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, New York, NY, October 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. [2020] S. Park, S. Suh, and Y. Jeong. Sound event localization and
    detection with various loss functions. Technical report, 2020. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2021a] S. Park, Y. Jeong, and T. Lee. Many-to-many audio spectrogram
    transformer: Transformer for sound event localization and detection. In *Proc.
    Detection and Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*,
    pages 105–109, Barcelona, Spain, November 2021a. ISBN 978-84-09-36072-7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2021b] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe,
    and S. Narayanan. A review of speaker diarization: recent advances with deep learning,
    June 2021b. arXiv:2101.09624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patel et al. [2020] S. J. Patel, M. Zawodniok, and J. Benesty. A single stage
    fully convolutional neural network for sound source localization and detection.
    Technical report, 2020. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pavlidi et al. [2015] D. Pavlidi, S. Delikaris-Manias, V. Pulkki, and A. Mouchtaris.
    3D localization of multiple sound sources with intensity vector estimates in single
    source zones. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 1556–1560,
    Nice, France, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peeters [2004] G. Peeters. A large set of audio features for sound description
    (similarity and classification) in the CUIDADO project. *CUIDADO Project Report
    54.0*, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perotin et al. [2018a] L. Perotin, R. Serizel, E. Vincent, and A. Guérin. Multichannel
    speech separation with recurrent neural networks from high-order ambisonics recordings.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, Calgary,
    Canada, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perotin et al. [2018b] L. Perotin, R. Serizel, E. Vincent, and A. Guérin. CRNN-based
    joint azimuth and elevation localization with the Ambisonics intensity vector.
    In *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*, pages 241–245,
    Tokyo, Japan, Sept. 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perotin et al. [2019a] L. Perotin, A. Défossez, E. Vincent, R. Serizel, and
    A. Guérin. Regression versus classification for neural network based audio source
    localization. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    New Paltz, NY, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perotin et al. [2019b] L. Perotin, R. Serizel, E. Vincent, and A. Guérin. CRNN-based
    multiple DoA estimation using acoustic intensity features for Ambisonics recordings.
    *IEEE J. Sel. Topics Signal Process.*, 13(1):22–33, Mar. 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pertilä and Cakir [2017] P. Pertilä and E. Cakir. Robust direction estimation
    with convolutional neural networks based steered response power. In *Proc. IEEE
    Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 6125–6129, New Orleans,
    LA, Mar. 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phan et al. [2020a] H. Phan, L. Pham, P. Koch, N. Q. K. Duong, I. McLoughlin,
    and A. Mertins. Audio event detection and localization with multitask regression
    network. Technical report, 2020a. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phan et al. [2020b] H. Phan, L. Pham, P. Koch, N. Q. K. Duong, I. McLoughlin,
    and A. Mertins. On multitask loss function for audio event detection and localization.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, pages 160–164, Tokyo, Japan, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ping et al. [2020] G. Ping, E. Fernandez-Grande, P. Gerstoft, and Z. Chu. Three-dimensional
    source localization using sparse Bayesian learning on a spherical microphone array.
    *J. Acoust. Soc. Am.*, 147(6):3895–3904, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Politis et al. [2020a] A. Politis, S. Adavanne, and T. Virtanen. A dataset of
    reverberant spatial sound scenes with moving sources for sound event localization
    and detection. In *Proc. Detection and Classification of Acoustic Scenes and Events
    Workshop (DCASE Workshop)*, pages 165–169, Tokyo, Japan, November 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Politis et al. [2020b] A. Politis, A. Mesaros, S. Adavanne, T. Heittola, and
    T. Virtanen. Overview and evaluation of sound event localization and detection
    in DCASE 2019. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 29:684–698, Sept.
    2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Politis et al. [2021] A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava,
    and T. Virtanen. A dataset of dynamic reverberant sound scenes with directional
    interferers for sound event localization and detection. In *Proc. Detection and
    Classification of Acoustic Scenes and Events Workshop (DCASE Workshop)*, Barcelona,
    Spain, June 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poschadel et al. [2021a] N. Poschadel, R. Hupke, S. Preihs, and J. Peissig.
    Direction of arrival estimation of noisy speech using convolutional recurrent
    neural networks with higher-order Ambisonics signals. In *Proc. Europ. Signal
    Process. Conf. (EUSIPCO)*, Dublin, Ireland (virtual conference), Mar. 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poschadel et al. [2021b] N. Poschadel, S. Preihs, and J. Peissig. Multi-source
    direction of arrival estimation of noisy speech using convolutional recurrent
    neural networks with higher-order ambisonics signals. In *Proc. Europ. Signal
    Process. Conf. (EUSIPCO)*, pages 1015–1019, Dublin, Ireland (virtual conference),
    2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pratik et al. [2019] P. Pratik, W. J. Jee, S. Nagisetty, R. Mars, and C. Lim.
    Sound event localization and detection using CRNN architecture with Mixup for
    model generalization. In *Proc. Detection and Classification of Acoustic Scenes
    and Events Workshop (DCASE Workshop)*, New York, NY, October 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pujol et al. [2019] H. Pujol, E. Bavu, and A. Garcia. Source localization in
    reverberant rooms using deep learning and microphone arrays. In *Proc. Int. Congr.
    Acoust. (ICA)*, Aachen, Germany, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pujol et al. [2021] H. Pujol, E. Bavu, and A. Garcia. BeamLearning: an end-to-end
    deep learning approach for the angular localization of sound sources using raw
    multichannel acoustic pressure data. *J. Acoust. Soc. Am.*, 149(6):4248–4263,
    Apr. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Purwins et al. [2019] H. Purwins, B. Li, T. Virtanen, J. Schlüter, S.-Y. Chang,
    and T. Sainath. Deep learning for audio signal processing. *IEEE J. Sel. Topics
    Signal Process.*, 13(2):206–219, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raangs and Druyvesteyn [2002] R. Raangs and E. Druyvesteyn. Sound source localization
    using sound intensity measured by a three dimensional PU-probe. In *Proc. Audio
    Engin. Soc. (AES) Conv.*, Munich, Germany, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rafaely [2019] B. Rafaely. *Fundamentals of Spherical Array Processing*. Springer,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranjan et al. [2019] R. Ranjan, S. Jayabalan, T. N. T. Nguyen, and W.-S. Lim.
    Sound events detection and direction of arrival estimation using residual net
    and recurrent neural networks. In *Proc. Detection and Classification of Acoustic
    Scenes and Events Workshop (DCASE Workshop)*, New York, NY, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rastogi et al. [1987] R. Rastogi, P. Gupta, and R. Kumaresan. Array signal processing
    with interconnected neuron-like elements. In *Proc. IEEE Int. Conf. Acoust., Speech,
    Signal Process. (ICASSP)*, pages 2328–2331, Dallas, TX, 1987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende et al. [2014] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
    backpropagation and approximate inference in deep generative models. In *Proc.
    Int. Conf. Mach. Learn. (ICML)*, Beijing, China, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rho et al. [2021] D. Rho, S. Lee, J. Park, T. Kim, J. Chang, and J. Ko. A combination
    of various neural networks for sound event localization and detection. Technical
    report, November 2021. DCASE 2021 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rickard [2002] S. Rickard. On the approximate W-disjoint orthogonality of speech.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 529–532,
    Orlando, Florida, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riezu and Grande [2021] S. A. V. Riezu and E. F. Grande. Room Impulse Response
    Dataset - ACT, DTU Elektro (011, IEC; plane, sphere). 4 2021. URL [https://data.dtu.dk/articles/dataset/Room_Impulse_Response_Dataset_-_ACT_DTU_Elektro_011_IEC_plane_sphere_/14320166](https://data.dtu.dk/articles/dataset/Room_Impulse_Response_Dataset_-_ACT_DTU_Elektro_011_IEC_plane_sphere_/14320166).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rindel [2000] J. H. Rindel. The use of computer modeling in room acoustics.
    *J. Vibroengineer.*, 3(4):219–224, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roden et al. [2015] R. Roden, N. Moritz, S. Gerlach, S. Weinzierl, and S. Goetze.
    On sound source localization of speech signals using deep neural networks. In
    *Proc. Deutsche Jahrestagung Akustik (DAGA)*, Nuremberg, Germany, 2015. ISBN 978-3-939296-08-9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roman and Wang [2008] N. Roman and D. Wang. Binaural tracking of multiple moving
    sources. *IEEE Trans. Audio, Speech, Lang. Process.*, 16(4):728–739, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ronchini et al. [2020] F. Ronchini, D. Arteaga, and A. Pérez-López. Sound event
    localization and detection based on CRNN using rectangular filters and channel
    rotation data augmentation. In *Proc. Detection and Classification of Acoustic
    Scenes and Events Workshop (DCASE Workshop)*, Tokyo, Japan, Oct. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. [2015] O. Ronneberger, P. Fischer, and T. Brox. U-Net: convolutional
    networks for biomedical image segmentation. In *Int. Conf. Medical Image Comput.
    Computer-Assisted Interv. (MICCAI)*, pages 234–241, Munich, Germany, May 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rossing [2007] T. D. Rossing. *Springer Handbook of Acoustics*. Springer, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roy and Kailath [1989] R. Roy and T. Kailath. ESPRIT: Estimation of signal
    parameters via rotational invariance techniques. *IEEE Trans. Acoust., Speech,
    Signal Process.*, 37(7):984–995, July 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder [2017] S. Ruder. An overview of multi-task learning in deep neural networks.
    *arXiv preprint arXiv:1706.05098*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sadeghian et al. [2017] A. Sadeghian, A. Alahi, and S. Savarese. Tracking the
    untrackable: Learning to track multiple cues with long-term dependencies. In *Proc.
    IEEE Int. Conf. Computer Vision (ICCV)*, pages 300–311, Venice, Italy, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sadok et al. [2022] S. Sadok, S. Leglaive, L. Girin, X. Alameda-Pineda, and
    R. Séguier. Learning and controlling the source-filter representation of speech
    with a variational autoencoder, 2022. arXiv:2204.07075.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sainath et al. [2017] T. N. Sainath, R. J. Weiss, K. W. Wilson, B. Li, A. Narayanan,
    E. Variani, M. Bacchiani, I. Shafran, A. Senior, et al. Multichannel signal processing
    with deep neural networks for automatic speech recognition. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 25(5):965–979, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salamon and Bello [2017] J. Salamon and J. P. Bello. Deep convolutional neural
    networks and data augmentation for environmental sound classification. *IEEE Signal
    Process. Lett.*, 24(3):279–283, Mar. 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saleh et al. [2021] F. Saleh, S. Aliakbarian, H. Rezatofighi, M. Salzmann, and
    S. Gould. Probabilistic tracklet scoring and inpainting for multiple object tracking.
    In *Proc. IEEE Conf. Computer Vision Pattern Recogn. (CVPR)*, pages 14329–14339,
    virtual conference, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salvati et al. [2018] D. Salvati, C. Drioli, and G. L. Foresti. Exploiting CNNs
    for improving acoustic source localization in noisy and reverberant conditions.
    *IEEE Trans. Emerg. Topics Comput. Intell.*, 2(2):103–116, Apr. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampathkumar and Kowerko [2020] A. Sampathkumar and D. Kowerko. Sound event
    detection and localization using CRNN models. Technical report, 2020. DCASE 2020
    Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sato et al. [2021] I. Sato, G. Liu, K. Ishikawa, T. Suzuki, and M. Tanaka. Does
    end-to-end trained deep model always perform better than non-end-to-end counterpart?
    *Electronic Imaging*, 2021(10):240–1, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sawada et al. [2003] H. Sawada, R. Mukai, and S. Makino. Direction of arrival
    estimation for multiple source signals using independent component analysis. In
    *IEEE Int. Symp. Signal Process. Applic.*, pages 411–414, Paris, France, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scheibler et al. [2018] R. Scheibler, E. Bezzam, and I. Dokmanić. Pyroomacoustics:
    a Python package for audio room simulation and array processing algorithms. In
    *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 351–355,
    Calgary, Canada, Apr. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidt [1986] R. Schmidt. Multiple emitter location and signal parameter estimation.
    *IEEE Trans. Antennas Propag.*, 34(3):276–280, Mar. 1986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwartz and Gannot [2013] O. Schwartz and S. Gannot. Speaker tracking using
    recursive EM algorithms. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*, 22(2):392–402,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schymura et al. [2020] C. Schymura, T. Ochiai, M. Delcroix, K. Kinoshita, T. Nakatani,
    S. Araki, and D. Kolossa. Exploiting attention-based sequence-to-sequence architectures
    for sound event localization. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*,
    Amsterdam, The Netherlands (virtual conference), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schymura et al. [2021] C. Schymura, B. Bönninghoff, T. Ochiai, M. Delcroix,
    K. Kinoshita, T. Nakatani, S. Araki, and D. Kolossa. PILOT: introducing Transformers
    for probabilistic sound event localization. In *Proc. Interspeech Conf.*, Brno,
    Czech Republic, June 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sehgal and Kehtarnavaz [2018] A. Sehgal and N. Kehtarnavaz. A convolutional
    neural network smartphone app for real-time voice activity detection. *IEEE Access*,
    6:9017–9026, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shimada et al. [2020a] K. Shimada, Y. Koyama, N. Takahashi, S. Takahashi, and
    Y. Mitsufuji. ACCDOA: activity-coupled cartesian direction of arrival representation
    for sound event localization and detection. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, Barcelona, Spain (virtual conference), 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimada et al. [2020b] K. Shimada, N. Takahashi, S. Takahashi, and Y. Mitsufuji.
    Sound event localization and detection using activity-coupled cartesian DoA vector
    and RD3net. Technical report, June 2020b. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimada et al. [2021] K. Shimada, N. Takahashi, Y. Koyama, S. Takahashi, E. Tsunoo,
    M. Takahashi, and Y. Mitsufuji. Ensemble of accdoa- and einv2-based systems with
    d3nets and impulse response simulation for sound event localization and detection.
    Technical report, 2021. DCASE 2021 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shlezinger et al. [2020] N. Shlezinger, J. Whang, Y. C. Eldar, and A. G. Dimakis.
    Model-based deep learning, 2020. arXiv:2012.08405.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siltanen et al. [2010] S. Siltanen, T. Lokki, and L. Savioja. Rays or waves?
    understanding the strengths and weaknesses of computational room acoustics modeling
    techniques. In *Proc. Int. Symp. Room Acoust. (ISRA)*, Melbourne, Australia, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singla et al. [2020] R. Singla, S. Tiwari, and R. Sharma. A sequential system
    for sound event detection and localization using CRNN. Technical report, 2020.
    DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sivasankaran et al. [2018] S. Sivasankaran, E. Vincent, and D. Fohr. Keyword-based
    speaker localization: localizing a target speaker in a multi-speaker environment.
    In *Proc. Interspeech Conf.*, Hyderabad, India, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song [2020] J.-m. Song. Localization and detection for moving sound sources
    using consecutive ensembles of 2D-CRNN. Technical report, 2020. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Southall et al. [1995] H. Southall, J. Simmers, and T. O’Donnell. Direction
    finding in phased arrays with a neural network beamformer. *IEEE Trans. Antennas
    Propag.*, 43(12):1369–1374, 1995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiefelhagen et al. [2007] R. Stiefelhagen, K. Bernardin, R. Bowers, R. T. Rose,
    M. Michel, and J. Garofolo. The CLEAR 2007 evaluation. In *Proc. Multimodal Technol.
    Percept. Humans*, pages 3–34, Baltimore, MD, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subramani and Smaragdis [2021] K. Subramani and P. Smaragdis. Point cloud audio
    processing. In *Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)*,
    pages 31–35, New Paltz, NY (virtual conference), 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subramanian et al. [2021a] A. S. Subramanian, C. Weng, S. Watanabe, M. Yu,
    Y. Xu, S.-X. Zhang, and D. Yu. Directional ASR: A new paradigm for E2E multi-speaker
    speech recognition with source localization. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, pages 8433–8437, Toronto, Canada (virtual conference),
    2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subramanian et al. [2021b] A. S. Subramanian, C. Weng, S. Watanabe, M. Yu, and
    D. Yu. Deep learning based multi-source localization with source splitting and
    its effectiveness in multi-talker speech recognition, Feb. 2021b. arXiv:2102.07955.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudarsanam et al. [2021] P. A. Sudarsanam, A. Politis, and K. Drossos. Assessment
    of self-attention on learned features for sound event localization and detection.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, pages 100–104, Barcelona, Spain, November 2021. ISBN 978-84-09-36072-7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudo et al. [2019] Y. Sudo, K. Itoyama, K. Nishida, and K. Nakadai. Improvement
    of DOA estimation by using quaternion output in sound event localization and detection.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, New York, NY, October 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2020] P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang,
    and P. Luo. Transtrack: Multiple object tracking with transformer, 2020. arXiv:2012.15460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundar et al. [2020] H. Sundar, W. Wang, M. Sun, and C. Wang. Raw waveform based
    end-to-end deep convolutional network for spatial localization of multiple acoustic
    sources. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 4642–4646, Barcelona, Spain (virtual conference), May 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suvorov et al. [2018] D. Suvorov, G. Dong, and R. Zhukov. Deep residual network
    for sound source localization in the time domain, Aug. 2018. arXiv:1808.06429.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Svensson and Kristiansen [2002] P. Svensson and U. R. Kristiansen. Computational
    modelling and simulation of acoustic spaces. In *Proc. Audio Eng. Soc. Conf.*,
    Espoo, Finland, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szöke et al. [2019] I. Szöke, M. Skácel, L. Mošner, J. Paliesek, and J. Černocký.
    Building and evaluation of a real room impulse response dataset. *IEEE J. Sel.
    Topics Signal Process.*, 13(4):863–876, Aug. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takahashi et al. [2016] N. Takahashi, M. Gygli, B. Pfister, and L. V. Gool.
    Deep convolutional neural networks and data augmentation for acoustic event recognition.
    In *Proc. Interspeech Conf.*, pages 2982–2986, San Francisco, CA, Sept. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takahashi et al. [2018] N. Takahashi, N. Goswami, and Y. Mitsufuji. MMDenseLSTM:
    An efficient combination of convolutional and recurrent neural networks for audio
    source separation. In *Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)*,
    Tokyo, Japan, May 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda and Komatani [2016a] R. Takeda and K. Komatani. Discriminative multiple
    sound source localization based on deep neural networks using independent location
    model. In *IEEE Spoken Language Technol. Workshop*, pages 603–609, San Juan, Portugal,
    2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda and Komatani [2016b] R. Takeda and K. Komatani. Sound source localization
    based on deep neural networks with directional activate function exploiting phase
    information. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*,
    pages 405–409, Shanghai, China, 2016b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda and Komatani [2017] R. Takeda and K. Komatani. Unsupervised adaptation
    of deep neural networks for sound source localization using entropy minimization.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 2217–2221,
    New-Orleans, LA, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda et al. [2018] R. Takeda, Y. Kudo, K. Takashima, Y. Kitamura, and K. Komatani.
    Unsupervised adaptation of neural networks for discriminative sound source localization
    with eliminative constraint. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 3514–3518, Calgary, Canada, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2019] Z. Tang, J. D. Kanu, K. Hogan, and D. Manocha. Regression
    and classification for direction-of-arrival estimation with convolutional recurrent
    neural networks. In *Proc. Interspeech Conf.*, pages 654–658, Graz, Austria, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tervo [2009] S. Tervo. Direction estimation based on sound intensity vectors.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 700–704, Glasgow, Scotland,
    2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thiemann and Van De Par [2015] J. Thiemann and S. Van De Par. Multiple model
    high-spatial resolution HRTF measurements. In *Proc. Deutsche Jahrestagung Akustik
    (DAGA)*, Nuremberg, Germany, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thuillier et al. [2018] E. Thuillier, H. Gamper, and I. J. Tashev. Spatial audio
    feature discovery with convolutional neural networks. In *Proc. IEEE Int. Conf.
    Acoust., Speech, Signal Process. (ICASSP)*, pages 6797–6801, Calgary, Canada,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian [2020] C. Tian. Multiple CRNN for SELD. Technical report, 2020. DCASE 2020
    Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tranter and Reynolds [2006] S. E. Tranter and D. A. Reynolds. An overview of
    automatic speaker diarization systems. *IEEE Trans. Audio, Speech, Lang. Process.*,
    14(5):1557–1565, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsuzuki et al. [2013] H. Tsuzuki, M. Kugler, S. Kuroyanagi, and A. Iwata. An
    approach for sound source localization by complex-valued neural network. *IEICE
    Trans. Inform. Syst.*, 96(10):2257–2265, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaidyanathan and Pal [2010] P. P. Vaidyanathan and P. Pal. Sparse sensing with
    co-prime samplers and arrays. *IEEE Trans. Signal Process.*, 59(2):573–586, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valero and Habets [2017] M. L. Valero and E. A. Habets. Multi-microphone acoustic
    echo cancellation using relative echo transfer functions. In *Proc. IEEE Workshop
    Appl. Signal Process. Audio Acoust. (WASPAA)*, pages 229–233, New Paltz, NY, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van Veen and Buckley [1988] B. D. Van Veen and K. M. Buckley. Beamforming:
    A versatile approach to spatial filtering. *IEEE Acoust., Speech, Signal Process.
    Magazine*, 5(2):4–24, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varanasi et al. [2020] V. Varanasi, H. Gupta, and R. M. Hegde. A deep learning
    framework for robust DoA estimation using spherical harmonic decomposition. *IEEE/ACM
    Trans. Audio, Speech, Lang. Process.*, 28:1248–1259, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vargas et al. [2021] E. Vargas, J. R. Hopgood, K. Brown, and K. Subr. On improved
    training of CNN for acoustic source localisation. *IEEE/ACM Trans. Audio, Speech,
    Lang. Process.*, 29:720–732, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varzandeh et al. [2020] R. Varzandeh, K. Adiloğlu, S. Doclo, and V. Hohmann.
    Exploiting periodicity features for joint detection and DoA estimation of speech
    sources using convolutional neural networks. In *Proc. IEEE Int. Conf. Acoust.,
    Speech, Signal Process. (ICASSP)*, pages 566–570, Barcelona, Spain (virtual conference),
    May 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, Dec. 2017.
    arXiv:1706.03762.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecchiotti et al. [2018] P. Vecchiotti, E. Principi, S. Squartini, and F. Piazza.
    Deep neural networks for joint voice activity detection and speaker localization.
    In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 1567–1571, Roma, Italy,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecchiotti et al. [2019a] P. Vecchiotti, N. Ma, S. Squartini, and G. J. Brown.
    End-to-end binaural sound localisation from the raw waveform. In *Proc. IEEE Int.
    Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 451–455, Brighton, UK,
    2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecchiotti et al. [2019b] P. Vecchiotti, G. Pepe, E. Principi, and S. Squartini.
    Detection of activity and position of speakers by using deep neural networks and
    acoustic data augmentation. *Expert Syst. with Applic.*, 134:53–65, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vera-Diaz et al. [2018] J. M. Vera-Diaz, D. Pizarro, and J. Macias-Guarasa.
    Towards end-to-end acoustic localization using deep learning: from audio signal
    to source position coordinates. *Sensors*, 18(10):3418, 2018. ISSN 1424-8220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vera-Diaz et al. [2020] J. M. Vera-Diaz, D. Pizarro, and J. Macias-Guarasa.
    Towards domain independence in CNN-based acoustic localization using deep cross
    correlations. In *Proc. Europ. Signal Process. Conf. (EUSIPCO)*, pages 226–230,
    Amsterdam, The Netherlands (virtual conference), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vera-Diaz et al. [2021] J. M. Vera-Diaz, D. Pizarro, and J. Macias-Guarasa.
    Acoustic source localization with deep generalized cross correlations. *Signal
    Process.*, 187:108169, Oct. 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vesperini et al. [2016] F. Vesperini, P. Vecchiotti, E. Principi, S. Squartini,
    and F. Piazza. A neural network based algorithm for speaker localization in a
    multi-room environment. In *IEEE Int. Workshop Machine Learning for Signal Process.*,
    pages 1–6, Salerno, Italy, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent and Campbell [2008] E. Vincent and D. R. Campbell. Roomsimove. *GNU
    Public License*, 2008. URL [http://homepages.loria.fr/evincent/software/Roomsimove_1](http://homepages.loria.fr/evincent/software/Roomsimove_1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent et al. [2018] E. Vincent, T. Virtanen, and S. Gannot. *Audio Source
    Separation and Speech Enhancement*. John Wiley & Sons, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vo et al. [2015] B.-n. Vo, M. Mallick, Y. Bar-shalom, S. Coraluppi, R. Osborne,
    R. Mahler, and B.-t. Vo. Multitarget tracking. In *Wiley Encyclopedia of Electrical
    and Electronics Engineering*. 2015. ISBN 978-0-471-34608-1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wabnitz et al. [2010] A. Wabnitz, N. Epain, C. Jin, and A. Van Schaik. Room
    acoustics simulation for multichannel microphone arrays. In *Proc. Int. Symp.
    Room Acoust. (ISRA)*, pages 1–6, Melbourne, Australia, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waibel et al. [1989] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang.
    Phoneme recognition using time-delay neural networks. *IEEE Trans. Acoust., Speech,
    Signal Process.*, 37(3):328–339, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Chen [2018] D. Wang and J. Chen. Supervised speech separation based
    on deep learning: An overview. *IEEE/ACM Trans. Audio, Speech, Lang. Process.*,
    26(10):1702–1726, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] L. Wang, Y. Liu, L. Zhao, Q. Wang, X. Zeng, and K. Chen.
    Acoustic source localization in strong reverberant environment by parametric Bayesian
    dictionary learning. *Signal Process.*, 143:232–240, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] Q. Wang, H. Wu, Z. Jing, F. Ma, Y. Fang, Y. Wang, T. Chen,
    J. Pan, J. Du, and C.-H. Lee. The USTC-IFLYTEK system for sound event localization
    and detection of DCASE 2020 challenge. Technical report, 2020. DCASE 2020 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2021] Q. Wang, J. Du, H.-X. Wu, J. Pan, F. Ma, and C.-H. Lee. A
    four-stage data augmentation approach to ResNet-Conformer based acoustic modeling
    for sound event localization and detection, Jan. 2021. arXiv:2101.02919.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019] Z. Wang, X. Zhang, and D. Wang. Robust speaker localization
    guided by deep learning-based time-frequency masking. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 27(1):178–188, Jan. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wichern et al. [2019] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn,
    D. Crow, E. Manilow, and J. L. Roux. Wham!: Extending speech separation to noisy
    environments. In *Proc. Interspeech Conf.*, Graz, Austria, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Woodruff and Wang [2012] J. Woodruff and D. Wang. Binaural localization of multiple
    sources in reverberant and noisy environments. *IEEE Trans. Audio, Speech, Lang.
    Process.*, 20(5):1503–1512, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021a] X. Wu, Z. Wu, L. Ju, and S. Wang. Binaural audio-visual localization.
    In *Proc. AAAI Conf. Artif. Intell.*, pages 2961–2968, virtual conference, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021b] Y. Wu, R. Ayyalasomayajula, M. J. Bianco, D. Bharadia, and
    P. Gerstoft. Sound source localization based on multi-task learning and image
    translation network. *J. Acoust. Soc. Am.*, 150(5):3374–3386, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2021c] Y. Wu, R. Ayyalasomayajula, M. J. Bianco, D. Bharadia, and
    P. Gerstoft. SSLIDE: sound source localization for indoors based on deep learning.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, Toronto,
    Canada (virtual conference), 2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xenaki and Gerstoft [2015] A. Xenaki and P. Gerstoft. Grid-free compressive
    beamforming. *J. Acoust. Soc. Am.*, 137(4):1923–1935, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xenaki et al. [2014] A. Xenaki, P. Gerstoft, and K. Mosegaard. Compressive beamforming.
    *J. Acoust. Soc. Am.*, 136(1):260–271, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xenaki et al. [2018] A. Xenaki, J. Bünsow Boldt, and M. Græsbøll Christensen.
    Sound source localization and speech enhancement with sparse Bayesian learning
    beamforming. *J. Acoust. Soc. Am.*, 143(6):3912–3921, June 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. [2019] J. Xiang, G. Zhang, and J. Hou. Online multi-object tracking
    based on feature representation and Bayesian filtering within a deep learning
    architecture. *IEEE Access*, 7:27923–27935, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. [2015] X. Xiao, S. Zhao, X. Zhong, D. L. Jones, E. S. Chng, and
    H. Li. A learning-based approach to direction of arrival estimation in noisy and
    reverberant environments. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
    (ICASSP)*, pages 2814–2818, Brisbane, Australia, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xinghao et al. [2021] S. Xinghao, Y. Hu, X. Zhu, and L. He. Sound event localization
    and detection based on adaptive hybrid convolution and multi-scale feature extractor.
    In *Proc. Detection and Classification of Acoustic Scenes and Events Workshop
    (DCASE Workshop)*, pages 130–134, Barcelona, Spain, November 2021. ISBN 978-84-09-36072-7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2013] B. Xu, G. Sun, R. Yu, and Z. Yang. High-accuracy TDOA-based
    localization without time synchronization. *IEEE Trans. Parallel Distrib. Syst.*,
    24(8):1567–1576, Aug. 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2021a] P. Xu, E. J. G. Arcondoulis, and Y. Liu. Acoustic source imaging
    using densely connected convolutional networks. *Mech. Syst. Signal Process.*,
    151:107370, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021b] Y. Xu, Y. Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-Pineda.
    Transcenter: Transformers with dense queries for multiple-object tracking, 2021b.
    arXiv:2103.15145.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. [2019] W. Xue, T. Ying, Z. Chao, and D. Guohong. Multi-beam and multi-task
    learning for joint sound event detection and localization. Technical report, 2019.
    DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. [2020] W. Xue, Y. Tong, C. Zhang, G. Ding, X. He, and B. Zhou. Sound
    event localization and detection based on multiple DoA beamforming and multi-task
    learning. In *Proc. Interspeech Conf.*, Shanghai, China (virtual conference),
    Oct. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yalta et al. [2017] N. Yalta, K. Nakadai, and T. Ogata. Sound source localization
    using deep learning models. *J. Robotics Mechatron.*, 29(1):37–48, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yalta et al. [2021] N. Yalta, Y. Sumiyoshi, and Y. Kawaguchi. The Hitachi DCASE
    2021 Task 3 system: handling directive interference with self attention layers.
    Technical report, 2021. DCASE 2021 Challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021a] B. Yang, X. Li, and H. Liu. Supervised direct-path relative
    transfer function learning for binaural sound source localization. In *Proc. IEEE
    Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 825–829, Toronto,
    Canada (virtual conference), 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021b] B. Yang, H. Liu, and X. Li. Learning deep direct-path relative
    transfer function for binaural sound source localization. *IEEE/ACM Trans. Audio,
    Speech, Lang. Process.*, 29:3491–3503, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [1994] W.-H. Yang, K.-K. Chan, and P.-R. Chang. Complex-valued neural
    network for direction of arrival estimation. *Electronics Lett.*, 30(7):574–575,
    1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Xie [2015] Z. Yang and L. Xie. Enhancing sparsity and resolution via
    reweighted atomic norm minimization. *IEEE Trans. Signal Process.*, 64(4):995–1006,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2018] Z. Yang, J. Li, P. Stoica, and L. Xie. Sparse methods for
    direction-of-arrival estimation. *Academic Press Library in Signal Process.*,
    7:509–581, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasuda et al. [2020] M. Yasuda, Y. Koizumi, S. Saito, H. Uematsu, and K. Imoto.
    Sound event localization based on sound intensity vector refined by DNN-based
    denoising and source separation. In *Proc. IEEE Int. Conf. Acoust., Speech, Signal
    Process. (ICASSP)*, pages 651–655, Barcelona, Spain (virtual conference), May
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yiwere and Rhee [2017] M. Yiwere and E. J. Rhee. Distance estimation and localization
    of sound sources in reverberant conditions using deep neural networks. *Int. J.
    Eng. Research Applic.*, 12(22):12384–12389, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youssef et al. [2013] K. Youssef, S. Argentieri, and J. Zarader. A learning-based
    approach to robust binaural sound localization. In *Proc. IEEE/RSJ Int. Conf.
    Intell. Robots Systems (IROS)*, pages 2927–2932, Tokyo, Japan, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. [2017] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen. Permutation invariant
    training of deep models for speaker-independent multi-talker speech separation.
    In *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, pages 241–245,
    New Orleans, LA, Mar. 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zea and Laudato [2021] E. Zea and M. Laudato. On the representation of wavefronts
    localized in space-time and wavenumber-frequency domains. *JASA Express Letters*,
    1(5):054801, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zermini et al. [2016] A. Zermini, Y. Yu, Y. Xu, W. Wang, and M. D. Plumbley.
    Deep neural network based audio source separation. In *IMA Int. Conf. Math. Signal
    Process.*, Birmingham, UK, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2018] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. Mixup:
    beyond empirical risk minimization, Apr. 2018. arXiv:1710.09412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019a] J. Zhang, W. Ding, and L. He. Data augmentation and priori
    knowledge-based regularization for sound event localization and detection. Technical
    report, 2019a. DCASE 2019 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019b] W. Zhang, Y. Zhou, and Y. Qian. Robust DoA estimation based
    on convolutional neural network and time-frequency masking. In *Proc. Interspeech
    Conf.*, pages 2703–2707, Graz, Austria, Sept. 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Yang [2021] Y. Zhang and Q. Yang. A survey on multi-task learning.
    *IEEE Transactions on Knowledge and Data Engineering*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2014] Y. Zhang, Z. Ye, X. Xu, and N. Hu. Off-grid DOA estimation
    using array covariance matrix and block-sparse Bayesian learning. *Signal Process.*,
    98:197–201, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2021] Y. Zhang, S. Wang, Z. Li, K. Guo, S. Chen, and Y. Pang.
    Data augmentation and class-based ensembled CNN-Conformer networks for sound event
    localization and detection. Technical report, 2021. DCASE 2021 Challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. [2020] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong,
    and Q. He. A comprehensive survey on transfer learning. *Proceedings of the IEEE*,
    109(1):43–76, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zotter and Frank [2019] F. Zotter and M. Frank. *Ambisonics: A Practical 3D
    Audio Theory for Recording, Studio Production, Sound Reinforcement, and Virtual
    Reality*. Springer Nature, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ünlerşen and Yaldiz [2016] M. F. Ünlerşen and E. Yaldiz. Direction of arrival
    estimation by using artificial neural networks. In *Proc. Euro. Modelling Symp.*,
    pages 242–245, Pisa, Italy, Nov. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
