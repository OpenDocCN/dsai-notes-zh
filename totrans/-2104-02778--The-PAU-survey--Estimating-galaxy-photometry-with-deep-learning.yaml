- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:55:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2104.02778] The PAU survey: Estimating galaxy photometry with deep learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.02778](https://ar5iv.labs.arxiv.org/html/2104.02778)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The PAU survey: Estimating galaxy photometry with deep learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'L. Cabayol¹, M. Eriksen¹ , A. Amara⁴, J. Carretero¹³³footnotemark: 3, R. Casas^(2,3),
    F. J. Castander^(2,3), J. De. Vicente⁸, E. Fernández¹, J. García-Bellido⁵, E. Gaztanaga^(2,3),
    H. Hildebrandt⁶, R. Miquel^(1,7), C. Padilla¹, E. Sánchez⁸, S. Serrano², I. Sevilla-Noarbe⁸,
    P. Tallada-Crespí⁸³³footnotemark: 3'
  prefs: []
  type: TYPE_NORMAL
- en: ¹Institut de Física d’Altes Energies (IFAE), The Barcelona Institute of Science
    and Technology, 08193 Bellaterra (Barcelona), Spain
  prefs: []
  type: TYPE_NORMAL
- en: ²Institute of Space Sciences (ICE, CSIC), Campus UAB, Carrer de Can Magrans,
    s/n, 08193 Barcelona, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ³Institut d’Estudis Espacials de Catalunya (IEEC), 08193 Barcelona, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Institute of Cosmology & Gravitation, University of Portsmouth, Dennis Sciama
    Building, Burnaby Road, Portsmouth PO1 3FX, UK
  prefs: []
  type: TYPE_NORMAL
- en: ⁵Instituto de Fisica Teorica UAM/CSIC, Universidad Autonoma de Madrid, 28049
    Madrid, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ⁶Ruhr University Bochum, Faculty of Physics and Astronomy, Astronomical Institute
    (AIRUB),
  prefs: []
  type: TYPE_NORMAL
- en: German Centre for Cosmological Lensing, 44780 Bochum, Germany
  prefs: []
  type: TYPE_NORMAL
- en: ⁷Institució Catalana de Recerca i Estudis Avançats, E-08010 Barcelona, Spain
  prefs: []
  type: TYPE_NORMAL
- en: '⁸Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas (CIEMAT),
    Madrid, Spain E-mail:lcabayol@ifae.esE-mail: eriksen@pic.esAlso at Port d’Informació
    Científica (PIC), Campus UAB, C. Albareda s/n, 08193 Bellaterra (Cerdanyola del
    Vallès), Spain(Accepted XXX. Received YYY; in original form ZZZ)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the dramatic rise in high-quality galaxy data expected from Euclid and
    Vera C. Rubin Observatory, there will be increasing demand for fast high-precision
    methods for measuring galaxy fluxes. These will be essential for inferring the
    redshifts of the galaxies. In this paper, we introduce Lumos, a deep learning
    method to measure photometry from galaxy images. Lumos builds on BKGnet, an algorithm
    to predict the background and its associated error, and predicts the background-subtracted
    flux probability density function. We have developed Lumos for data from the Physics
    of the Accelerating Universe Survey (PAUS), an imaging survey using a 40 narrow-band
    filter camera (PAUCam). PAUCam images are affected by scattered light, displaying
    a background noise pattern that can be predicted and corrected for. On average,
    Lumos increases the SNR of the observations by a factor of 2 compared to an aperture
    photometry algorithm. It also incorporates other advantages like robustness towards
    distorting artefacts, e.g. cosmic rays or scattered light, the ability of deblending
    and less sensitivity to uncertainties in the galaxy profile parameters used to
    infer the photometry. Indeed, the number of flagged photometry outlier observations
    is reduced from 10% to 2%, comparing to aperture photometry. Furthermore, with
    Lumos photometry, the photo-z scatter is reduced by $\approx$10% with the Deepz
    machine learning photo-z code and the photo-z outlier rate by 20%. The photo-z
    improvement is lower than expected from the SNR increment, however currently the
    photometric calibration and outliers in the photometry seem to be its limiting
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'techniques: photometric – techniques: image processing – galaxies:photometry
    – cosmology: observations^†^†pubyear: ^†^†pagerange: The PAU survey: Estimating
    galaxy photometry with deep learning–[Data availability](#Sx2 "Data availability
    ‣ The PAU survey: Estimating galaxy photometry with deep learning")'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wide field galaxy surveys are a powerful tool for cosmology. Galaxy redshifts
    are the most fundamental property of any cosmological or galaxy evolution study.
    Spectroscopic surveys, e.g. the Sloan Digital Sky Survey (SDSS, Ahumada et al.,
    [2020](#bib.bib1)), measure very high precision redshifts, however these are only
    possible for on the order of a million objects (e.g. BOSS, Dawson et al., [2013](#bib.bib19)).
    In contrast, imaging surveys are $\approx$ 2 orders of magnitude ahead in terms
    of number of objects but these are observed with a lower spectral resolution,
    which makes the redshift measurements less precise. Current and past imaging surveys,
    e.g. The Kilo-Degree Survey (KiDS, de Jong et al., [2013](#bib.bib73)), The Hyper
    Supreme-Cam Subaru (HSC, Aihara et al., [2018](#bib.bib2)) or The Dark Energy
    Survey (DES, DES Collaboration et al., [2021](#bib.bib17)) have detected hundreds
    of millions of galaxies and oncoming surveys like Euclid (Laureijs et al., [2011](#bib.bib48))
    or Vera C. Rubin Observatory (LSST, Ivezić et al., [2019](#bib.bib38)) will increase
    this number to billions. Consequently, fast and precise methods to analyse and
    extract galaxy properties (e.g. flux, size, shape) are needed.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different algorithms to estimate galaxy photometry. One very
    widely used example is SExtractor (Bertin & Arnouts, [1996](#bib.bib8)), which
    applies aperture photometry (Ni et al., [2019](#bib.bib57)) inspired by the Kron
    ([1980](#bib.bib44)) first moments algorithm. This technique measures the flux
    of the targeted galaxies by placing an aperture around the source and measuring
    the light captured inside such aperture. Another technique is model fitting (Heasley,
    [1999](#bib.bib32)), which consists of fitting the galaxy image to a theoretical
    model and extract its photometry. This includes the GaaP (Kuijken, [2008](#bib.bib45))
    algorithm, which estimates the total flux by fitting the pixelated galaxy images
    to polar shapelets (Refregier, [2003](#bib.bib61); Massey & Refregier, [2005](#bib.bib54)),
    separating the galaxy image into components with explicit rotational symmetries.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other examples as e.g. ProFound (Robotham et al., [2018](#bib.bib62)),
    T-PHOT (Merlin et al., [2015](#bib.bib55)) and Tractor (Lang et al., [2016](#bib.bib47)),
    and each of them is adjusted to outperform the others on a particular data set.
    For instance, a photometry algorithm can be optimised to work very well on images
    with many blended galaxies (e.g. Boucaud et al., [2020](#bib.bib12)) while another
    can be intended to improve the photometry of very noisy galaxies. Therefore, depending
    on the type of data and the science goals, different methodologies are applied
    to improve the photometry estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Although all these algorithms have proven to work well, they also have their
    shortcomings. Aperture photometry works very well on clean images, but it is not
    robust towards distorting effects such as e.g. blended galaxies, variable background
    light and cosmic rays. On the other hand, model fitting is sensitive to model
    parametrisation. In contrast, this is not the case for machine learning techniques,
    which learn a model adapted to the data. Furthermore, deep learning has proven
    to be very powerful on image recognition and computer vision (e.g. Girshick, [2015](#bib.bib28);
    Zhao et al., [2019](#bib.bib71)), which makes it a robust tool to work on images
    with artifacts and variant effects. Also, the evaluation of a trained machine
    learning algorithm is very fast, which is relevant when dealing with very large
    amounts of data. For instance, Haigh et al. ([2021](#bib.bib30)) compares several
    source-extraction codes and conclude that currently there is no tool sufficiently
    fast and accurate to be well suited to large-scale automated segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has already been applied to different steps of astronomical imaging
    photometry, e.g. photometry of blended galaxies (Boucaud et al., [2020](#bib.bib12)),
    PSF simulation (Herbel et al., [2018](#bib.bib33)), cosmic ray rejection (Zhang
    & Bloom, [2020](#bib.bib70)) or source detection (Hausen & Robertson, [2020](#bib.bib31)).
    The power of deep learning techniques on object detection or image recognition
    tasks makes these steps of the data reduction, among others, very suitable candidates
    to apply machine learning. While addressing them with classical methods can be
    difficult and computationally expensive, deep learning is an effective tool to
    tackle the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we present Lumos¹¹1The code is available at https://github.com/PAU-survey/lumos
    under a GPL-3 license., a deep learning based algorithm to extract the photometry
    from astronomical images. It consists of a Convolutional Neural Network that works
    on input galaxy images and a Mixture density network that outputs the probability
    distribution of galaxy fluxes. Lumos builds on BKGnet (Cabayol-Garcia et al.,
    [2020](#bib.bib13)), a deep learning algorithm to predict and correct strongly
    varying astronomical backgrounds. Lumos estimates the probability distribution
    of the background subtracted galaxy flux, which requires the implicit estimation
    and subtraction of the background noise.
  prefs: []
  type: TYPE_NORMAL
- en: We have developed and tested Lumos using images from the Physics of the Accelerating
    Universe Survey (PAUS). PAUS is an imaging survey that measures high precision
    photo-z to faint magnitudes $i_{\rm AB}<22.5$, while covering a large area of
    sky (Martí et al., [2014](#bib.bib53)). This is possible thanks to the PAUCam
    instrument (Castander et al., [2012](#bib.bib16); Padilla et al., [2016](#bib.bib58),
    [2019](#bib.bib59)), a camera equipped with 40 narrow band filters covering the
    optical spectrum (Casas et al., [2016](#bib.bib15)). With BCNz2 (Eriksen et al.,
    [2019](#bib.bib24)), a template fitting algorithm, PAUS reaches a photo-z precision
    $\sigma(z)/(1+z)\approx 0.0035$ for the best 50% of the sample, compared to typical
    precision of 0.05 for broad band measurements. Similar results are obtained with
    Delight, a hybrid template-machine-learning photometric redshift algorithm that
    uses Gaussian processes (Soo et al., [2021](#bib.bib64)). Both methods are further
    improved with Deepz (Eriksen et al., [2020](#bib.bib25)), a deep learning algorithm
    to measure photo-zs that reduces the $\sigma_{\rm 68}$ scatter by 50% compared
    with the template fitting method, having the largest improvement at fainter galaxies.
    Furthermore, Alarcon et al. [2021](#bib.bib3) presents a high precision photo-z
    catalogue in the COSMOS field computed using a combination of PAUS NB and 26 broad,
    intermediate, and narrow bands covering the UV, visible and near infrared spectrum.
    Although Lumos has been developed for PAUS, it can be adapted to any imaging survey,
    like e.g. Euclid or Rubin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of this paper is as follows. In $\S$[2](#S2 "2 Data ‣ The PAU
    survey: Estimating galaxy photometry with deep learning"), we present the PAUS
    data used in this paper and the simulations we use for training. Section [3](#S3
    "3 Flux estimation methods ‣ The PAU survey: Estimating galaxy photometry with
    deep learning") presents different flux estimation alternatives that we have used
    to compare to Lumos performance. In §[4](#S4 "4 Lumos: Measuring fluxes with a
    CNN ‣ The PAU survey: Estimating galaxy photometry with deep learning"), we introduce
    Lumos, its architecture and the training procedure. Section [5](#S5 "5 Lumos flux
    measurements on simulations ‣ The PAU survey: Estimating galaxy photometry with
    deep learning") presents Lumos results on simulations, including validation of
    the flux probability distributions, a comparison with alternative flux estimation
    methods and deblending tests. Finally, $\S$[6](#S6 "6 Lumos photometry on PAUS
    data ‣ The PAU survey: Estimating galaxy photometry with deep learning") shows
    Lumos results on the PAUS data, including single exposure photometry, co-added
    fluxes and photometric redshifts obtained with Lumos photometry. Conclusions and
    discussion can be found in §[7](#S7 "7 Conclusions and discussion ‣ The PAU survey:
    Estimating galaxy photometry with deep learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present PAUS data ($\S$[2.1](#S2.SS1 "2.1 PAUS data ‣ 2
    Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")) and
    Teahupoo simulations ($\S$[2.2](#S2.SS2 "2.2 Teahupoo simulations ‣ 2 Data ‣ The
    PAU survey: Estimating galaxy photometry with deep learning")), the simulated
    galaxy images used throughout the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 PAUS data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PAUS data are taken with the William Herschel Telescope (WHT), at the Observatorio
    del Roque de los Muchachos in La Palma at the Canary Islands (Spain). Images are
    obtained with the PAUCam instrument (Castander et al., [2012](#bib.bib16); Padilla
    et al., [2019](#bib.bib59)), an optical camera equipped with 40 narrow band filters
    (NB), covering a wavelength range from 4500 to 8500Å (Casas et al., [2016](#bib.bib15)).
    The NB filters have 130Å full width half maximum (FWHM) and a separation between
    consecutive bands of 100Å. The camera has 18 red-sensitive fully depleted Hamamatsu
    CCD detectors (Casas et al., [2012](#bib.bib14)), although only the 8 central
    CCDs are currently used for NB imaging. Each CCD has 4096x2048 pixels with a pixel
    scale of 0.263 arcsec/pix. The NB filters are mounted in five trays with 8 filters
    per tray that can be exchanged and placed in front of the central CCDs. The NB
    filter set effectively measures a low resolution spectrum ($R\approx 50$).
  prefs: []
  type: TYPE_NORMAL
- en: PAUS has been observing since the 2015B semester and as of 2021A, PAUS has taken
    data for 160 nights. It partially covers the CFHTLS fields²²2http://www.cfht.hawaii.edu/Science/CFHTLS_Y_WIRCam
  prefs: []
  type: TYPE_NORMAL
- en: /cfhtlsdeepwidefields.html W1, W2 and W3 and the COSMOS field³³3http://cosmos.astro.caltech.edu/.
    Currently, PAUS data has a 40 narrow band coverage of 10 deg² in W1 and W2, 20
    deg² in W3 and the 2 deg² of the COSMOS field. The PAUS data are stored at the
    Port d’Informació Científica (PIC), where the data are processed and distributed
    (Tonello et al., [2019](#bib.bib68)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 COSMOS sample
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This paper focuses on data from the COSMOS field (Lilly et al., [2009](#bib.bib51)),
    which were taken in the semesters 2015B, 2016A, 2016B and 2017B. These are the
    data used in the BKGnet paper and also in all the photo-z studies published so
    far. The COSMOS field observations comprise a total of 9749 images, 243 images
    in each NB. While observing COSMOS, the PAU camera was modified to mitigate the
    effect of scattered light by introducing baffles on the edges of the NB filters
    of each filter tray. This camera intervention changed the noise patterns of PAUS
    images. Half of the images in COSMOS were taken before the camera modification
    and the other half, after. The baseline exposure times in the COSMOS field are
    70, 80, 90, 110 and 130 seconds from the bluest to the reddest filter trays. The
    complete photometry catalogue comprises 64,476 galaxies to $i_{\rm AB}$ < 23 in
    40 NB filters. This corresponds to $\approx$ 12,5 million galaxy observations
    ($\approx$ 5 observations per galaxy and NB filter).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Standard data reduction pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PAUS data reduction process consists of two pipelines: the Nightly and the
    MEMBA pipelines. The Nightly pipeline (Serrano et al. in prep.) is the first step
    and performs an instrumental de-trending processing, e.g. electronic or illumination
    biases. Electronic biases are corrected with an overscan subtraction, an observation
    with the shutter closed and zero exposure time. Vignetting of the telescope corrector,
    pixel-to-pixel variations or dead pixels are detected and corrected with dome
    flats, 10 second exposures of a uniformly and homogeneously illuminated screen.
    The Point Spread Function (PSF) is modeled with PSFex (Bertin, [2011](#bib.bib7))
    using star observations from the COSMOS Advanced Camera for Surveys (ACS, Leauthaud
    et al., [2007](#bib.bib49)). Cosmic rays are detected and masked from the image
    using a Laplacian edge detector (van Dokkum, [2001](#bib.bib74)). The astrometry
    of the narrow band images is calibrated using SCAMP (Bertin, [2006](#bib.bib6))
    with the Gaia DR2 stars (Gaia Collaboration et al., [2018](#bib.bib27)) as a reference
    catalogue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MEMBA pipeline (Serrano et al. in prep.) is the second step in the data
    reduction. It applies forced aperture photometry to targets selected from an external
    detection catalogue (see $\S$[3.2](#S3.SS2 "3.2 Aperture photometry ‣ 3 Flux estimation
    methods ‣ The PAU survey: Estimating galaxy photometry with deep learning") for
    more detailes). In the COSMOS field, the detection parent catalogue is provided
    by Ilbert et al. ([2008](#bib.bib35)) and the photometry calibration is relative
    to SDSS stars (Castander et al. in prep.). A brief description of the photometry
    calibration can be found in Eriksen et al. ([2019](#bib.bib24)).'
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning algorithm developed in this paper presents an alternative
    to MEMBA. It aims to be more robust in the presence of distorting effects as blending
    or scattered light. It also aims to be provide better photometry measurements
    of galaxies with errors in the parent catalogue galaxy profile parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Teahupoo simulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f57164438e9beb9b63f2700730cf40be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: From left to right, Teahupoo galaxy images with $i$-band magnitudes
    $18.5,20.3$ and $22.3$, in PAUS "NB685". These are simulated with a exposure time
    of 90 seconds, the baseline PAUS exposure time in the "NB685" filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this work, we have constructed the Teahupoo⁴⁴4Named after the favourite
    sandwich of the first author. simulations, a set of PAUS-like galaxy image simulations.
    Three examples of Teahupoo galaxies with $i$-band magnitudes $i_{\rm AB}=18.5,20.3$
    and 22.3 are shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.2 Teahupoo simulations ‣
    2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning"). Note
    that already at $i_{\rm AB}\approx 20$ it is hard to distinguish the galaxy from
    the background noise and with $i_{\rm AB}>22$, the galaxy signal is visually masked
    by background fluctuations.'
  prefs: []
  type: TYPE_NORMAL
- en: Teahupoo light profiles are modelled with a single Sérsic profile. Each galaxy
    is generated using
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I(R)=I_{\rm e}\exp{\left\{-(2n_{\rm s}-1/3)\left[\left(\frac{R}{R_{\rm
    50}}\right)^{\rm 1/n_{\rm s}}-1\right]\right\}},$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $I_{\rm e}$, $r_{\rm 50}$ and $n_{\rm s}$ are the the surface brightness,
    the half light radius and the Sérsic index, respectively. As the shape and the
    size of the galaxy are correlated, to ensure galaxies are realistic, we jointly
    sample the half light radius, the Sérsic index and the ellipticity from their
    distributions in the COSMOS field. Elliptical galaxies are simulated by elongating
    the half light radius according to the $b/a$ distribution in the COSMOS field.
    Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Teahupoo simulations ‣ 2 Data ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") shows the distributions of $r_{\rm
    50}$ (top left) and the Sérsic index (top right) of PAUS galaxies in the COSMOS
    field, which are provided by Ilbert et al. ([2009](#bib.bib36)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ded82d91a2a4c5e613b3a329d32f9337.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Distributions of the half light radius ($r_{\rm 50}$) (top left),
    Sérsic index ($n_{\rm s}$) (top right), the PSF FWHM (bottom left) and the $I_{\rm
    auto}$ magnitude of PAUS galaxies in the COSMOS field (bottom right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Teahupoo image simulations are 60x60 pixels image generated with Astropy (Astropy
    Collaboration et al., [2013](#bib.bib5); Price-Whelan et al., [2018](#bib.bib60)).
    Astropy methods evaluate the galaxy profiles (Eq. [1](#S2.E1 "In 2.2 Teahupoo
    simulations ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning")) at the center of each pixel instead of integrating along the pixel.
    This is problematic for small and steep galaxy profiles (high $n_{\rm s}$), where
    the flux changes significantly along the pixel. To correct for this effect, galaxies
    are initially drawn in a 600x600 grid with a later size reduction. Furthermore,
    drawing on a larger grid allows shifting the galaxy at a sub-pixel level from
    the center. Including sub-pixels shifts in Teahupoo galaxy images has also proven
    important to reduce the number of photometry outliers on real PAUCam galaxies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Teahupoo images use background cutouts from real PAUCam images. These cutouts
    can contain artifacts such as other galaxies, cosmic rays or crosstalk. This has
    proven very important for our network, as it learns how to make predictions when
    they are present (see e.g. $\S$[5.3](#S5.SS3 "5.3 Deblending with Lumos ‣ 5 Lumos
    flux measurements on simulations ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") and $\S$[6.1](#S6.SS1 "6.1 Single exposure measurements ‣
    6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")). The background light noise patterns across the CCD are
    narrow band dependent and changed when the PAUCam camera was modified. For this
    reason, the background stamps are taken from a PAUCam image observed with the
    same NB filter we are simulating and we track if it was observed before or after
    the camera intervention (see $\S$[4.1](#S4.SS1 "4.1 Input data ‣ 4 Lumos: Measuring
    fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    The galaxy signal is also wavelength dependent. Consequently, we independently
    sample the galaxy flux from forty flux distributions, one per NB filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simulated galaxy is convolved with the PSF as detected in the source image
    of the background cutout (Bertin, [2011](#bib.bib7)). The distribution of PSFs
    in the COSMOS field is also displayed in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Teahupoo
    simulations ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning") (bottom left panel). Using the same PSF for the galaxy and the background
    noise is crucial. Otherwise, the network could artificially learn that the galaxy
    has a different PSF than the background and use this to estimate the clean flux,
    which would not work on PAUCam data. Before combining the background cutout and
    the galaxy, we simulate photon shot noise on the galaxy. Note that other sources
    of additive noise, e.g. readout or electronic noise, are not required as the background
    cutout already includes them. This is another benefit of using real PAUCam background
    stamps, as simulating realistic noise is often hard and could easily lead to differences
    between simulations and data.'
  prefs: []
  type: TYPE_NORMAL
- en: As simulations are constructed from real PAUS flux measurements and PAUCam background
    cutouts, outlier measurements in any of these two might end up represented in
    the Teahupoo images. Some examples of this could be background images with spurious
    effects or outlier flux measurements at the catalogue level (i.e. artificially
    low or high fluxes). To reduce the number of affected Teahupoo galaxies, the flux
    distribution used to create them is clipped at 0 and 1000 e^-/s. This ensures
    that neither negative fluxes nor artificially bright examples are represented
    in the image simulations. Furthermore, we have also proceeded with a visual inspection
    of the PAUCam images. This filters out very poor observations, but cannot deal
    with local effects in regions of the CCD, e.g. saturated pixels, and therefore
    a few outliers will still leak into the Teahupoo catalogue.
  prefs: []
  type: TYPE_NORMAL
- en: The methodology used to generate Teahupoo images is very similar to that of
    the Balrog simulations (Suchyta et al., [2016](#bib.bib65); Eckert et al., [2020](#bib.bib23)).
    The main similarity between Balrog and Teahupoo is that both methodologies add
    the simulated galaxy on real survey images. In contrast, there are also differences
    as Balrog uses Galsim (Rowe et al., [2015](#bib.bib63)) to draw the simulated
    galaxies, while Teahupoo galaxies are built with Astropy. Also, Teahupoo galaxies
    are constructed in a super-resolution grid, which increases the resolution of
    small objects and allows to include sub-pixel shifts from the center of the stamp.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Comparison between PAUCam and Teahupoo galaxies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised machine learning algorithms require a training sample, i.e. a set
    of data with a known solution that is used to find the non-linear mapping from
    the input to the output of the network. Having a good and large training sample
    is a crucial part of the training and ideally, we would train Lumos on a sample
    of PAUCam images with known photometry. However, in absence of that, we are using
    Teahupoo images for training. These simulations need to be representative of the
    testing data, and small differences between PAUS and Teahupoo galaxies can lead
    to a degradation of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: To test the similarity between PAUCam and Teahupoo galaxies, we have generated
    a controlled sample of Teahupoo-PAUCam galaxy pairs. Given a PAUS galaxy, its
    simulated pair is constructed with a Sérsic modelling using the same profile parameters
    ($r_{\rm 50},n_{\rm s}$) and the same amount of light. The simulated background
    stamp is selected from a sourceless region in the same image as its real PAUCam
    pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Comparison between PAUCam and Teahupoo galaxies
    ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning") shows
    three Teahupoo -PAUCam galaxy pairs with $i$-band magnitudes of 18.6, 19.4 and
    21.0, respectively. The plot shows the pixel values along the central row of the
    image normalised with the mean background (excluding the source) of the stamp.
    Therefore, pixels without galaxy light contribution should be fluctuating around
    unity, while pixels with higher values will be showing the galaxy light profile.
    In general, PAUS and Teahupoo galaxies fit well up to background light and shot
    noise fluctuations. The first plot on the left-hand side is a clear example of
    this. In the middle galaxy, the two galaxies also match reasonably well, however
    it also exhibits a small shift between the galaxy peaks, possibly due to an astrometry
    inaccuracy. The right plot shows that the comparison on fainter sources is much
    harder, as fainter galaxies can barely be distinguished from background fluctuations
    (see also Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 Teahupoo simulations ‣ 2 Data ‣ The
    PAU survey: Estimating galaxy photometry with deep learning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several effects could bring variations between Teahupoo and PAUCam galaxies.
    For instance, inaccuracies in any step of the data reduction process as e.g. the
    photometric calibration, the astrometry or the PSF measurement will not be fully
    represented in the simulations. The PSF is a clear example, as currently a single
    PSF measurement is taken from each PAUCam image assuming the PSF is constant across
    the CCD, which could potentially yield to discrepancies between simulations and
    data. Additionally, if the real galaxy cannot be modelled with a single Sérsic
    modelling, that would also imply a difference between the two images. These discrepancies
    will propagate into larger errors in the flux estimation. Nonetheless, note that
    inaccuracies in the calibration, the modelling or the PSF would also affect the
    measurement with other flux estimation methods (e.g. aperture photometry or model
    fitting). Furthermore, Lumos uses both the galaxy image and the image of the modeled
    profile, which allows it to provide flux uncertainties that take into account
    discrepancies between the galaxy and the model. It also allows detecting inaccuracies
    in e.g. the astrometry or the PSF, which will also be considered in the uncertainty
    measurement (see last paragraph in §[6.1](#S6.SS1 "6.1 Single exposure measurements
    ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9dfca73193ea6cd9e2ade53c9096700.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparison between single pixel row light profile for pairs of PAUCam
    galaxies (solid black line) and Teahupoo simulated galaxies (dashed light blue).
    Teahupoo galaxies are constructed to exactly mimic its real PAUCam pair. The plot
    shows the pixel value along the central row of pixels (crossing the source) divided
    by the mean image background noise. Therefore, the central peak at x = 30 corresponds
    to the galaxy light contribution. From left to right, the galaxy magnitudes are
    $i_{\rm AB}$ = 18.6, 19.4 and 21.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Flux estimation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce other flux estimation methodologies that we will
    use to compare to Lumos performance. Particularly, we consider a profile fitting
    methodology (§[3.1](#S3.SS1 "3.1 Profile fitting ‣ 3 Flux estimation methods ‣
    The PAU survey: Estimating galaxy photometry with deep learning")), aperture photometry
    (§[3.2](#S3.SS2 "3.2 Aperture photometry ‣ 3 Flux estimation methods ‣ The PAU
    survey: Estimating galaxy photometry with deep learning")) and a linear weighted
    sum of the galaxy pixels (§[3.3](#S3.SS3 "3.3 Weighted pixel sum ‣ 3 Flux estimation
    methods ‣ The PAU survey: Estimating galaxy photometry with deep learning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Profile fitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a profile fitting approach, the background subtracted galaxy image is fitted
    to a theoretical galaxy model to infer the profile amplitude. Assuming that the
    galaxy can be modelled as $I(r)=I_{\rm e}R(r)$, where $I_{\rm e}$ is the profile
    amplitude and R($r_{\rm i}$) corresponds to a Sérsic light profile at pixel i,
    we can fit the image to the theoretical profile with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\chi^{2}=\sum_{\rm i}\frac{(f_{\rm i}-I(r_{\rm i}))^{2}}{\sigma_{\rm
    F,i}^{2}},$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where i sums over pixels, f is the background subtracted flux ($f\equiv F-B$,
    with F and B being the total flux and the background noise) and $I(r_{\rm i})$
    is the galaxy theoretical model in pixel i. Assuming Poisson errors, the previous
    equation becomes
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\chi^{2}(I_{e})=\sum_{\rm i}\frac{(f_{\rm i}-I_{e}R(r_{i}))^{2}}{I_{e}R(r_{i})+B},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where B is the mean background per pixel. The total flux is measured as the
    $I_{\rm e}$ minimising Equation [3](#S3.E3 "In 3.1 Profile fitting ‣ 3 Flux estimation
    methods ‣ The PAU survey: Estimating galaxy photometry with deep learning"). Note
    that the parameter $I_{\rm e}$ appears twice in the equation, which makes the
    closed form not feasible. Instead, we have minimised Equation [3](#S3.E3 "In 3.1
    Profile fitting ‣ 3 Flux estimation methods ‣ The PAU survey: Estimating galaxy
    photometry with deep learning") with a Nealder-Mead algorithm from SciPy (Jones
    et al., [2001](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Aperture photometry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Aperture photometry (Mighell, [1999](#bib.bib56)) is widely used in a large
    number of surveys e.g. DES (Drlica-Wagner et al., [2018](#bib.bib22)) or Pan-STARRS
    (Magnier et al., [2020](#bib.bib52)), and also in PAUS. This approach measures
    all the pixel contributions inside an aperture of radius R with subtraction of
    the background light, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f=\iint_{\rm R}{\rm d}A\,\left(S(r)-\bar{b}\right),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $S$ is the signal per pixel, $\bar{b}$ is the mean background per pixel,
    R the aperture radius and A is the aperture area.
  prefs: []
  type: TYPE_NORMAL
- en: 'In PAUS, the apertures are elliptical and their areas are set to target a fixed
    amount of galaxy light, in our case 62.5% of the flux. Therefore, obtaining the
    total flux requires scaling the measurement (Eq. [4](#S3.E4 "In 3.2 Aperture photometry
    ‣ 3 Flux estimation methods ‣ The PAU survey: Estimating galaxy photometry with
    deep learning")) by 1/0.625\. Given a target percentage of light, R is estimated
    using a simulated galaxy profile (Sérsic index, size, ellipticity) convolved with
    the image PSF. The background light is measured as the mean of the pixel values
    within an annulus of $R_{\rm in}=30$ pixels, $R_{\rm out}=45$ pixels centered
    at the targeted galaxy. The pixels within the ring are $\sigma$-clipped beforehand,
    to prevent from artifacts biasing the background measurement.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Weighted pixel sum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In aperture photometry, all pixels within the aperture contribute equally to
    the total flux, i.e. pixels at the galaxy border with low SNR and pixels at the
    galaxy center contribute the same. In terms of total SNR, this is not optimal,
    especially for small and faint galaxies where all the signal is distributed among
    very few pixels. Weighting differently each pixel contribution could increase
    the SNR of the measurements.
  prefs: []
  type: TYPE_NORMAL
- en: There are different choices of weights, with some providing a higher SNR than
    others. Indeed note that aperture photometry is just a simple case of pixel weighting,
    where pixels within the aperture have a unity weight and those outside do not
    contribute.
  prefs: []
  type: TYPE_NORMAL
- en: The weighting we are interested in is that giving the most optimal unbiased
    linear solution. This means the unbiased estimator providing the maximum SNR,
    which can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm SNR}=\frac{\sum_{\rm i}w_{\rm i}m_{\rm i}}{\sqrt{\sum_{\rm i}w_{\rm
    i}^{2}(m_{\rm i}+b_{\rm i})}},$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $b_{\rm i}$, $m_{\rm i}$ and $w_{\rm i}$ are the background mean value,
    the signal mean value and the optimal weight in pixel i, respectively. Maximising
    the SNR (Eq. [5](#S3.E5 "In 3.3 Weighted pixel sum ‣ 3 Flux estimation methods
    ‣ The PAU survey: Estimating galaxy photometry with deep learning")) as a function
    of the pixel weights $w_{i}$ leads to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{\rm x}=\frac{\sum_{\rm i}m_{\rm i}}{\sum_{\rm i}m_{\rm i}^{2}/(m_{\rm
    i}+b_{\rm i})}\frac{1}{1+b_{\rm x}/m_{\rm x}},$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $w_{\rm x}$ is the optimal weight of pixel x (see Appendix [A](#A1 "Appendix
    A Flux estimation methods: derivations ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") for a more detailed derivation). This is the linear estimator
    providing the most precise unbiased flux measurements. However, obtaining the
    optimal weights in Equation [6](#S3.E6 "In 3.3 Weighted pixel sum ‣ 3 Flux estimation
    methods ‣ The PAU survey: Estimating galaxy photometry with deep learning") requires
    a perfect knowledge of the galaxy light profile and consequently, uncertainties
    in the pixel signal and background noise degrade the precision of the flux measurements.
    Nonetheless, this methodology puts a limit on how well linear methods can measure
    galaxy fluxes, which can be used to benchmark the Lumos performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '4 Lumos: Measuring fluxes with a CNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we describe Lumos, our deep learning algorithm to measure
    the photometry of astronomical objects. A short introduction to the deep learning
    concepts needed to understand this section can be found in Appendix [B](#A2 "Appendix
    B Convolutional Neural Networks ‣ The PAU survey: Estimating galaxy photometry
    with deep learning"). We will discuss Lumos input data (§[4.1](#S4.SS1 "4.1 Input
    data ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy
    photometry with deep learning")), its architecture (§[4.2](#S4.SS2 "4.2 Lumos
    architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")) and the training procedure (§[4.3](#S4.SS3
    "4.3 Unsupervised transfer learning ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The
    PAU survey: Estimating galaxy photometry with deep learning"), §[4.4](#S4.SS4
    "4.4 Training procedure ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey:
    Estimating galaxy photometry with deep learning")). In sections §[4.2](#S4.SS2
    "4.2 Lumos architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey:
    Estimating galaxy photometry with deep learning"), §[4.3](#S4.SS3 "4.3 Unsupervised
    transfer learning ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating
    galaxy photometry with deep learning") and §[4.4](#S4.SS4 "4.4 Training procedure
    ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") we describe technical details of the the network’s architecture
    and the training procedure, which are not absolutely required to understand the
    main results of this work.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Input data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Lumos input consists of two types of data. The most important input includes
    two images of 60x60 pixels. The first one is made of an image cutout centered
    at the target galaxy and, given that PAUS pixel scale is 0.263, it covers 16x16
    arcsec of the night sky. Although most PAUS galaxies have a half light radius
    between 1 and 3 pixels, which would not require such a large cutout, in Cabayol-Garcia
    et al. ([2020](#bib.bib13)) we already showed that the network needs larger stamps
    to accurately model the background noise fluctuations and scattered light patterns.
    The second image contains the convolved galaxy profile drawn using the parameters
    from an external catalogue. We have tested other possibilities like, e.g. using
    the true galaxy profile, the PSF profile or both separately, obtaining the best
    results with the convolved galaxy profile. Note that in the training phase the
    input galaxy cutouts are Teahupoo image simulations (see §[2.2](#S2.SS2 "2.2 Teahupoo
    simulations ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning")), while in the testing phase these of real galaxy cutouts .'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the network can directly estimate the photometry using only images, we
    have found that additional information improves the results. This information
    is the second type of input and it currently includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The i-band magnitude of the target galaxy obtained from the external catalogue.
    This is not strictly needed as the network works without this information. However
    it helps providing better photometry uncertainties and so far Lumos requires other
    information (galaxy profile, coordinates) from an external catalogue anyway.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CCD coordinates. PAUCam images contain scattered light with a band dependent
    spatial pattern across the CCD. This makes the CCD position and the band relevant
    information for the network (see Figures 1 and 2 in Cabayol-Garcia et al. ([2020](#bib.bib13))).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The narrow band filter identification. The galaxy flux distribution is different
    for each narrow band filter. Furthermore, the scattered light pattern also depends
    on the NB filter (see Figure 2 in Cabayol-Garcia et al. ([2020](#bib.bib13))).
    Therefore, the band provides valuable extra knowledge of the expected flux and
    background noise pattern.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A camera intervention flag. The camera was modified while observing the COSMOS
    field (see §[2](#S2 "2 Data ‣ The PAU survey: Estimating galaxy photometry with
    deep learning")). Therefore, the network also benefits from knowing if an image
    was taken before or after the camera intervention. This information is combined
    with that of the NB filter and given as an 80x10 trainable matrix (see §[4.2](#S4.SS2
    "4.2 Lumos architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") for more details). In practice,
    the network effectively works for all intents and purposes as having 80 different
    filters instead of 40.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How these inputs are combined and given to the network is described in the next
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Lumos architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0629e4016a1567411ba29304ba67285b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Lumos architecture. Orange cubes correspond to convolutional layers,
    blue cubes are pooling layers and the purple ones are batch normalisation layers.
    The vectors between two CNN blocks corresponds to the dimension of the previous
    block’s output and the input dimension of the following. The CNN’s output is linearised
    (green stick), combined with external information (coordinates, NB filter and
    i-band magnitude) and input to a MDN. The MDN outputs a probability distribution
    of the total flux as a linear combination of five Gaussians.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Lumos architecture (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.2 Lumos architecture
    ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")) has two differentiated parts, a Convolutional Neural Network
    (CNN) and a Mixture Density Network (MDN). The CNN works directly on the input
    images and it is built up with five blocks of convolution-pooling-batch normalisation
    (see Appendix [B](#A2 "Appendix B Convolutional Neural Networks ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") for a more detailed explanation).
    These layers are represented in Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Lumos architecture
    ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") as orange, blue and purple stacked blocks, respectively.
    The CNN’s output is transformed into a 1D array including the galaxy image information
    and then combined with external information regarding its i-band magnitude, its
    position in the CCD, the NB filter it was observed with and the camera intervention
    flag (see §[4.1](#S4.SS1 "4.1 Input data ‣ 4 Lumos: Measuring fluxes with a CNN
    ‣ The PAU survey: Estimating galaxy photometry with deep learning")). For the
    band information, the network uses a 80x10 matrix, where each combination of band
    $\times$ camera intervention flag (before/after) is represented by 10 features
    to be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN’s output array combined with the galaxy information is the input of
    the MDN. MDNs (Bishop, [1994](#bib.bib10)) are a variant of neural networks that
    combine a feed-forward network and a mixture model. The MDN returns the probability
    distribution of the total flux prediction as a linear combination of k distributions.
    These distributions could be any sort of basis function, e.g. Gaussians or multiquadratic
    functions. However, in this work we have only considered Gaussian distributions,
    in such a way that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(f&#124;G)=\sum_{\rm i}^{\rm k}\alpha_{\rm i}N_{\rm i}(\mu_{\rm i},\sigma_{\rm
    i})\,,$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $f$ is the galaxy flux, G is the galaxy data, $N_{\rm i}(\mu_{\rm i},\sigma_{\rm
    i})$ is the i-th Gaussian outcome distribution and the parameters $\alpha_{i}$
    are the so-called mixing coefficients. These parameters give the relative contribution
    of each Gaussian component to the total probability distribution. Note that the
    set of mixing coefficients must sum to unity and therefore they can be understood
    as the prior probability of each Gaussian component.
  prefs: []
  type: TYPE_NORMAL
- en: The MDN in Lumos consists in 4 dense layers with parameters 5000:1000:100:15\.
    It outputs five mixing coefficients ($\alpha$) together with five pairs of $(\mu,\sigma)$
    parametrising the Gaussian components (N). This kind of network architecture was
    already used on PAUS data in Eriksen et al. ([2020](#bib.bib25)), where it was
    used to predict redshift probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of loss function is a crucial step in the construction of a neural
    network. Lumos combines two loss functions, the first as in Equation [8](#S4.E8
    "In 4.2 Lumos architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") and a modified version of it
    in Equation [10](#S4.E10 "In 4.2 Lumos architecture ‣ 4 Lumos: Measuring fluxes
    with a CNN ‣ The PAU survey: Estimating galaxy photometry with deep learning").'
  prefs: []
  type: TYPE_NORMAL
- en: A common loss function for a Gaussian MDN is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm\mathcal{L}_{\rm MDN}}=\sum_{\rm i=1}^{k}\left[\log(\alpha_{\rm i})-\frac{(f_{\rm
    i}-\bar{f})^{2}}{\sigma_{\rm i}^{2}}-2\log{(\sigma_{\rm i})}\right],$ |  | (8)
    |'
  prefs: []
  type: TYPE_TB
- en: which corresponds to the negative log likelihood of a Gaussian distribution
    with mean $\bar{f}$ and variance $\sigma_{\rm i}^{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: In Lumos, we have also considered a variation of the previous loss motivated
    by the fact that physical galaxies have positive flux values. This alternative
    loss also corresponds to the Gaussian negative log-likelihood, but integrated
    from 0 to $\infty$, which changes the PDF normalisation.
  prefs: []
  type: TYPE_NORMAL
- en: The logarithm of a Gaussian PDF ($G(x)$) integrated from 0 to $\infty$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log\left({G(x)}\right)=\log{\left(\frac{2}{\pi}\right)}+\log{\left(\exp\left(\frac{-\frac{1}{2}(x-\mu)^{2}}{\sigma^{2}}\right)\right)}+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\log{\left(\sigma{\rm Erf}\left(\frac{\mu}{\sqrt{2}\sigma}\right)+\sigma\right)}\>,$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: which leads to the following loss function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\rm\mathcal{L}_{\rm MDN}}=\sum_{\rm i=1}^{k}\log{(\alpha_{\rm
    i})}-\frac{(f_{\rm i}-\bar{f})^{2}}{\sigma_{\rm i}^{2}}-2\log{(\sigma_{\rm i})}-$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\log{\left({\rm Erf}\left(\frac{\bar{f}}{\sqrt{2}\sigma_{\rm
    i}}\right)+1\right)}\>,$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where again $\bar{f}$ is the true flux and $\sigma_{\rm i}$ is the flux uncertainty.
    Therefore, the truncation of the Gaussian distribution effectively corresponds
    to an additional term in the loss function. Lumos combines both loss functions
    and uses Equation [8](#S4.E8 "In 4.2 Lumos architecture ‣ 4 Lumos: Measuring fluxes
    with a CNN ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    on objects with $i_{\rm AB}<20.5$ and Equation [10](#S4.E10 "In 4.2 Lumos architecture
    ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") for the rest of the sample. Choosing a single loos function
    for all magnitudes also works, however combining the two improves the photometry
    for the faintest and the brightest galaxies.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Unsupervised transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning (Tan et al., [2018](#bib.bib66); Zhuang et al., [2019](#bib.bib72))
    is a deep learning technique that aims to adapt a model trained to make predictions
    on a particular task to work on a similar but different problem. One example is
    a classifier trained to distinguish between cats and dogs adapted to distinguish
    horses and zebras. Instead of training from scratch, the zebra-horse classifier
    takes the parameters of the cat-dog classifier as initial parameters, in such
    a way that the network has already learnt to extract shared features like e.g.
    detect edges or shapes. Such mutual features are commonly extracted in the shallower
    layers of the network, while deeper layers pick up more subtle data traits. For
    this reason, many times transfer learning only requires training deeper layers
    of the network, while shallower layers can be shared among the different networks.
  prefs: []
  type: TYPE_NORMAL
- en: The same idea can be applied to adapt models trained on simulations to perform
    well on data (Tercan et al., [2018](#bib.bib67); Eriksen et al., [2020](#bib.bib25)).
    To train a supervised network one needs data with a known solution (labeled data).
    Many times there are labeled data available, but not enough to train a network
    from scratch. A possible solution in such cases is to train the network on simulations
    and use a small labeled dataset to adapt the model to the data. This requires
    two consecutive trainings, one initial on simulations and once this finishes,
    an additional one on data with the network parameters from the training on simulations
    as a starting point. Domínguez Sánchez et al. [2019](#bib.bib20) also explores
    the possibility of using transfer learning to adapt a model trained on data from
    one survey to another one. Particularly, it adapts a morphology classifier trained
    on SDSS images to work on DES images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lumos is trained on Teahupoo image simulations (see §[2](#S2 "2 Data ‣ The
    PAU survey: Estimating galaxy photometry with deep learning") for more details)
    and we cannot apply supervised transfer learning as there are no data with known
    photometry. Instead, we will use the compatibility of independent observations
    of the same galaxy in the same NB filter to apply what we call unsupervised transfer
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: For that, we have collected a set of PAUCam observed galaxy pairs, with two
    independent images of the same galaxy observed with the same narrow band filter.
    Lumos should predict compatible flux PDFs for the two observations of the same
    object, learning to ignore differences in e.g. the background noise or the PSF.
    Therefore, after training Lumos on Teahupoo image simulations, we retrain it on
    the set of PAUCam galaxy pairs, forcing compatibility between the two flux measurements.
    With this procedure, we make sure that Lumos has seen real data before evaluating
    the network on the test sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unsupervised transfer learning loss function is constructed comparing the
    probability distribution of two observations. Before comparing the PDFs, these
    need to be calibrated with the image zero-point (see §[2.1](#S2.SS1 "2.1 PAUS
    data ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    for more details). The PDFs are parametrised with five Gaussian distributions
    (as provided by Lumos, see §[4.2](#S4.SS2 "4.2 Lumos architecture ‣ 4 Lumos: Measuring
    fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry with deep learning")),
    in such a way that each Gaussian component in the first observation is compared
    to all the components in the second observation. The negative log likelihood of
    the difference between the two predicted PDFs takes the form of'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\rm\mathcal{L}_{\rm UTL}}=\sum_{\rm i}\sum_{\rm j}\log{\alpha_{\rm
    i}}+\log{\beta_{\rm j}}-\frac{1}{2}\frac{(f_{i}-f_{j})^{2}}{\sigma_{\rm i}^{2}+\sigma_{\rm
    j}^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-\sqrt{\sigma_{f_{\rm i}}^{2}+\sigma_{f_{\rm j}}^{2}}-\log{\left[1+{\rm
    Erf}\left(\frac{f_{\rm i}}{\sqrt{2}\sigma_{\rm i}}\right)\right]}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-\log{\left[1+{\rm Erf}\left(\frac{f_{\rm j}}{\sqrt{2}\sigma_{\rm
    j}}\right)\right]}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+\log{\left[1+{\rm Erf}\left(\frac{f_{\rm i}/\sigma_{\rm
    i}^{2}+f_{\rm j}/\sigma_{\rm j}^{2}}{\sqrt{1/\sigma_{\rm i}^{2}+1/\sigma_{\rm
    j}^{2}}}\right)\right]},$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{\rm i}$ ($f_{\rm j}$) is the i-th (j-th) Gaussian component of the
    first (second) exposure, and similarly for $\sigma$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Training procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lumos is trained with 500,000 simulated images constructed combining simulated
    galaxies and real PAUCam backgrounds (see §[2.2](#S2.SS2 "2.2 Teahupoo simulations
    ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    The sample is split in 90% for training and 10% for validation. We also generate
    10,000 independent simulated galaxies for testing. Lumos is trained for 100 epochs
    with an initial learning rate of $10^{-4}$, which is reduced a factor of 10 every
    40 epochs. We use Adam (Kingma & Ba, [2015](#bib.bib42)) as optimisation algorithm.
    The training takes about 20 hours using an NVIDIA TITAN V GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The network is trained with a batch size of 500 galaxies, which is a relatively
    large batch size. As simulations are constructed from real PAUS flux measurements
    and PAUCam background cutouts, Teahupoo image simulations might contain some outliers.
    We have applied a filtering to reduce them (see the last paragraph in §[2.2](#S2.SS2
    "2.2 Teahupoo simulations ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")), however a few poor examples can still be part of Teahupoo
    images and these are very difficult to detect. Having a large batch size reduces
    their effect on the overall loss function and consequently in the training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the supervised training, we apply the unsupervised transfer learning.
    This part is trained with 20,000 galaxy pairs. A danger of applying unsupervised
    transfer learning is that nothing prevents the network from biasing the flux predictions,
    since Equation [11](#S4.E11 "In 4.3 Unsupervised transfer learning ‣ 4 Lumos:
    Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry with
    deep learning") does not directly constrain the flux prediction but the pairwise
    consistency. For this reason, we have also included a supervised training with
    20,000 simulations. The loss function ends up combining Equations [8](#S4.E8 "In
    4.2 Lumos architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey:
    Estimating galaxy photometry with deep learning"), [10](#S4.E10 "In 4.2 Lumos
    architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating
    galaxy photometry with deep learning") and [11](#S4.E11 "In 4.3 Unsupervised transfer
    learning ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating galaxy
    photometry with deep learning") and therefore becomes'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm\mathcal{L}}={\rm\mathcal{L}_{\rm MDN}}+{\rm\mathcal{L}_{\rm UTL}}.$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: These two losses are weighted equally in the loss function. Nevertheless, this
    is not strictly necessary and one could consider weighting them differently.
  prefs: []
  type: TYPE_NORMAL
- en: Lumos has already been trained on reliable simulations, therefore its parameters
    should be close to optimal before the unsupervised transfer learning training.
    Consequently, instead of training all the network parameters, the transfer learning
    only varies the parameters in the last linear layer, while all the others are
    frozen. The unsupervised transfer learning phase is trained for 100 epochs, with
    an initial learning rate of $10^{-5}$, which is reduced by a factor of 10 every
    50 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Lumos flux measurements on simulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we will test Lumos on Teahupoo galaxies. We will first validate
    the flux probability distributions (§[5.1](#S5.SS1 "5.1 Flux probability distributions
    ‣ 5 Lumos flux measurements on simulations ‣ The PAU survey: Estimating galaxy
    photometry with deep learning")), followed by a comparison with other flux estimation
    methods (§[5.2](#S5.SS2 "5.2 Comparison with different flux estimation methods
    ‣ 5 Lumos flux measurements on simulations ‣ The PAU survey: Estimating galaxy
    photometry with deep learning")). Finally we will test how well Lumos performs
    on blended galaxies (§[5.3](#S5.SS3 "5.3 Deblending with Lumos ‣ 5 Lumos flux
    measurements on simulations ‣ The PAU survey: Estimating galaxy photometry with
    deep learning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Flux probability distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most photometry algorithms only provide a flux measurement and its uncertainty.
    In contrast, Lumos provides the flux probability distributions as a linear combination
    of five Gaussians (see §[4.2](#S4.SS2 "4.2 Lumos architecture ‣ 4 Lumos: Measuring
    fluxes with a CNN ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    Figure [5](#S5.F5 "Figure 5 ‣ 5.1 Flux probability distributions ‣ 5 Lumos flux
    measurements on simulations ‣ The PAU survey: Estimating galaxy photometry with
    deep learning") shows the predicted flux PDFs for two PAUCam galaxies (solid lines)
    and two Teahupoo galaxies mimicking them (dashed lines) (see §[2.3](#S2.SS3 "2.3
    Comparison between PAUCam and Teahupoo galaxies ‣ 2 Data ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")). The flux probability distributions on
    data and on the simulations are very similar, providing additional confidence
    on the reliability of Teahupoo galaxies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, notice that the resulting PDFs are not Gaussian. This can especially
    be seen in the faintest galaxy, which displays secondary peaks on the left and
    right of the main one. This type of PDF is common in Lumos predictions, where
    fainter galaxies exhibit more non-gaussianities than brighter ones. In general,
    Lumos PDFs are more Gaussian at redder bands, where galaxies are also brighter.
    At the blue end, many PAUS galaxies have fluxes very close to 0 for which Lumos
    commonly provides very non-Gaussian PDFs (see §[6.3](#S6.SS3 "6.3 Coadded flux
    measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy
    photometry with deep learning") for further discussion).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5659cf8cdb401ed25a8bcceb851022b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Flux probability distributions provided by Lumos for two PAUCam galaxies
    (Gal1, Gal2, solid lines) and their Teahupoo imitations (dashed lines).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Probability Integral Transform (PIT) on simulations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Probability Integral Transform (PIT) (Dawid, [1984](#bib.bib18); Gneiting
    et al., [2005](#bib.bib29); Bordoloi et al., [2010](#bib.bib11)) tests the quality
    of the probability distribution and it is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm PIT}\equiv\int_{\rm-\infty}^{f^{*}}{\rm d}f\,\phi(f)\,$ |  | (13)
    |'
  prefs: []
  type: TYPE_TB
- en: where $f^{*}$ is the true flux value and $\phi(f)$ is the probability distribution.
    When $\phi(f)$ faithfully represents the true value, the PIT distribution is the
    uniform distribution U[0,1].
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [6](#S5.F6 "Figure 6 ‣ 5.1.1 Probability Integral Transform (PIT)
    on simulations ‣ 5.1 Flux probability distributions ‣ 5 Lumos flux measurements
    on simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning"),
    we have estimated the PIT value for 10,000 Teahupoo galaxies with known true flux.
    The plot shows two distributions, one including (solid blue line) and another
    not (dashed red line) including the CCD coordinates of galaxies in the training
    and test set. When the training does not include the coordinates, the PIT distribution
    displays two peaks of outliers at the first and last bin of the plot. These outliers
    correspond to galaxies with strongly varying background light that require accurate
    knowledge of the background noise patterns in the different NB filters.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, when the training includes the CCD coordinates, the PIT test displays
    a flat U[0,1], showing that Lumos provides robust flux probability distributions
    and that CCD coordinates are essential information for cutouts with varying backgrounds.
    This is consistent with the results in Figure 7 of Cabayol-Garcia et al. ([2020](#bib.bib13)),
    where the CCD coordinates proved essential to predict accurate backgrounds (solid
    black line).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eede65aa18900731ae15b70fa29a35a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: PIT distribution of the Lumos flux PDFs on a set of 10,000 Teahupoo
    galaxies. We have tested Lumos PDFs with (solid blue line) and without (red dashed
    line) including the CCD coordinates. If the CCD coordinates are not included,
    Lumos provides outliers (peaks at 0 and 1) corresponding to scattered light affected
    objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Single flux and flux uncertainty measurements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although the flux PDF provides more information, many applications require
    a single flux measurement and uncertainty. Flux point-like estimates can be computed
    with different statistical estimators as the mean, the median or the peak. For
    (almost) Gaussian PDFs, these estimators provide very similar flux measurements.
    However when the PDFs move away from gaussianity, these estimators can provide
    significant differences among them. As an example, in Lumos multiple peaked distributions
    tend to provide higher flux measurements with the median than with the peak. This
    is because these PDFs commonly belong to faint objects with the main peak very
    close to zero and secondary peaks and the tail moving towards higher fluxes (see
    Fig. [17](#S6.F17 "Figure 17 ‣ 6.3 Coadded flux measurements ‣ 6 Lumos photometry
    on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    for an example).'
  prefs: []
  type: TYPE_NORMAL
- en: For the flux uncertainty, the most straightforward estimator is the standard
    deviation. However, another possibility is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma_{\rm 68}\equiv\frac{1}{2}[q_{\rm 84}-q_{\rm 16}]\,,$ |  | (14)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $q_{\rm 16}$ ($q_{\rm 84}$) are the 16-th (84-th) quantiles. For Gaussian
    distributions, these two estimators coincide. However, in the case of non-Gaussian
    PDFs, $\sigma_{\rm 68}$ is more robust towards distributions with tails but provides
    higher uncertainties in the presence of multiple peaks (see §[6.3](#S6.SS3 "6.3
    Coadded flux measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") for further discussion).'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the median or $\sigma_{\rm 68}$ are more robust with noisy PDFs,
    they require the explicit PDF construction from the Gaussian parametrisation provided
    by Lumos. This is time consuming, since we already have more than 10 million galaxy
    exposures in the small COSMOS field. A fast alternative is to analytically determine
    the mean and the variance from the Gaussian component parameters. The mean flux
    ($f$) is estimated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f=\sum_{\rm i}\alpha_{\rm i}\textperiodcentered\mu_{\rm i}\,,$ |  | (15)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{\rm i}$ and $\mu_{\rm i}$ are the mixing coefficient and the
    expected value of the i-th Gaussian component, respectively. The associated variance
    ($\sigma_{\rm f}^{2}$) is then given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma^{2}_{\rm f}=\sum_{\rm i}\left[\alpha_{\rm i}\ \left(\sigma_{\rm
    i}^{2}+\ \mathinner{\!\left\lVert\mu_{\rm i}-\sum_{\rm j}\alpha_{\rm j}\mu_{\rm
    j})\right\rVert}^{2}\right)\right],$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma_{\rm i}^{2}$ is the variance of the $i$-th Gaussian component.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Comparison with different flux estimation methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While Lumos has proven to provide reliable flux probability distributions,
    other methodologies as model fitting or aperture photometry are also able to provide
    accurate flux estimates (e.g. Lang et al., [2016](#bib.bib47); Drlica-Wagner et al.,
    [2018](#bib.bib22); Kuijken et al., [2019](#bib.bib46)). In this section, we will
    use simulations to compare the performance of Lumos with a profile fitting method
    (§[3.1](#S3.SS1 "3.1 Profile fitting ‣ 3 Flux estimation methods ‣ The PAU survey:
    Estimating galaxy photometry with deep learning")), aperture photometry (§[3.2](#S3.SS2
    "3.2 Aperture photometry ‣ 3 Flux estimation methods ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")) and a linear weighted sum of pixels (§[3.3](#S3.SS3
    "3.3 Weighted pixel sum ‣ 3 Flux estimation methods ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")). To quantify the quality of the flux measurements,
    we will use'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\rm\textbf{Bias}\mathrel{\mathop{\mathchar 58\relax}}}$
    | $\displaystyle{\rm Median}\left[(f-f_{\rm 0})/f_{\rm 0}\right],$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dispersion: | $\displaystyle\sigma_{68}\left[(f-f_{\rm 0})/f_{\rm 0}\right],$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{\rm 0}$ is the ground truth flux.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Comparison with different flux estimation
    methods ‣ 5 Lumos flux measurements on simulations ‣ The PAU survey: Estimating
    galaxy photometry with deep learning") compares the bias (left panel) and the
    dispersion (right panel) in the flux predictions as a function of the $I_{\rm
    auto}$ magnitude for the four methods. The model fitting method (purple dashed
    dotted line) displays a systematic increment of the bias with magnitude, with
    a 20% bias at the faint end. PAUS galaxies are already hard to distinguish from
    background fluctuations for magnitudes $i_{\rm AB}>20$ (see Figure [2](#S2.F2
    "Figure 2 ‣ 2.2 Teahupoo simulations ‣ 2 Data ‣ The PAU survey: Estimating galaxy
    photometry with deep learning")), which could be severely complicate the fitting
    at fainter magnitudes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second method is the pixel weighted sum. It is unbiased for objects with
    $i<21$, but fainter objects are 10% biased. While the optimal weights (Eq. [6](#S3.E6
    "In 3.3 Weighted pixel sum ‣ 3 Flux estimation methods ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")) ensure unbiased flux estimates, these
    also require perfect knowledge of the galaxy profile and the background light.
    On simulations, the galaxy light distribution is known, however the background
    light is not. Therefore, at the faint end where the galaxy signal is comparable
    to the background fluctuations, the weights (Eq. [6](#S3.E6 "In 3.3 Weighted pixel
    sum ‣ 3 Flux estimation methods ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")) seem to be very sensitive to inaccuracies in the background
    noise. In contrast, aperture photometry and Lumos provide unbiased estimates to
    a 5% level up to magnitude 22.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of dispersion, Lumos is the most precise method with $\sigma_{\rm 68}=0.36$.
    This implies a 28% improvement with respect to the linear pixel weighting method,
    which is the second best method with $\sigma_{\rm 68}=0.47$. As expected, the
    optimal weighted sum is the most precise unbiased linear method, but it degrades
    at fainter magnitudes. Lumos overcomes the linear optimal weighting with a non-linear
    mapping and provides good flux estimates at all magnitudes.
  prefs: []
  type: TYPE_NORMAL
- en: The previous results combine measurements from all NB filters. Considering measurements
    from each NB filter independently, bluer bands show a higher dispersion, which
    is expected since their SNR is lower. Furthermore, the flux measurements are unbiased
    to a 3% level in all NB filters and these also show unbiased for all galaxy sizes
    ($r_{\rm 50}$ and PSFs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e154858a7c5b3ec35a0386c9e2e07da4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparison of the bias (left panel) and the dispersion (right panel)
    among the flux measurements with Lumos, aperture photometry, model fitting and
    optimal pixel weighting. These results include galaxy image simulations in the
    40 PAUS NB filters. The I-band magnitude corresponds to the AUTO magnitude as
    measured by the HST-ACS on the COSMOS field.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Deblending with Lumos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Blending is the superposition of galaxies with other astrophysical objects along
    the line of sight. It affects the photometric and shape measurements contributing
    to systematics in weak lensing studies (Arcelin et al., [2021](#bib.bib4)). Deblending
    will be a challenge for future ground-based photometric surveys such as Rubin
    (Ivezić et al., [2019](#bib.bib38)) or Euclid (Laureijs et al., [2011](#bib.bib48))
    and it has recently been approached with deep learning techniques (e.g. Arcelin
    et al., [2021](#bib.bib4); Boucaud et al., [2020](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we test if Lumos can extract the galaxy photometry when the
    target galaxy is blended with another object. Even though Lumos is not explicitly
    constructed to predict the photometry in the presence of other galaxies, the simulated
    training sample is built using background cutouts from PAUCam images (see §[2](#S2
    "2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    These image cutouts are centered at random CCD positions, without minding if there
    are any other artifacts nearby. Consequently, the training sample contains examples
    of blended galaxies to learn from. This is one benefit of machine learning algorithms.
    These are flexible enough to learn how to extract the photometry of blended sources
    by only including examples in the training sample, without explicitly constructing
    the algorithm for this task.'
  prefs: []
  type: TYPE_NORMAL
- en: We have generated 3600 60x60 pixel realisations of the same target Teahupoo
    galaxy, which is placed at the central pixel of the stamp. Each of these realisations
    also contains the same PAUS galaxy centered on a different pixel at a time. The
    PAUS galaxy moves across the stamp in steps of one pixel, in such a way that the
    realisation where it is located at the central pixel corresponds to a total blending
    with the Teahupoo galaxy. At the end of the day, with all the realisation the
    PAUS galaxy covers all the pixels in the stamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [8](#S5.F8 "Figure 8 ‣ 5.3 Deblending with Lumos ‣ 5 Lumos flux measurements
    on simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    shows the accuracy in the flux measurement as a function of distance to the overlapping
    source. The target galaxy is fainter than the overlapping one, with $i_{\rm AB}=22$
    and $i_{\rm AB}=20$, respectively. With aperture photometry (dashed red line),
    the flux is considerably biased for all distances R. This is expected since summing
    all contributions within the aperture does not differentiate if the light belongs
    to the target source or the overlapping one. Therefore, for $R<15$, the bias measurement
    is caused by light from the overlapping source accounted inside the aperture.
    At larger values of R, the background noise prediction is also affected by the
    overlapping source. In the PAUS aperture photometry pipeline, the background is
    estimated within a 15 pixel wide annulus, located at 30 pixels from the target
    source. When the overlapping source is very bright, as it happens in this example,
    it can affect the background prediction and the flux prediction at the same time,
    which degrades the performance even more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, Lumos (blue solid line) extracts better the photometry of
    blended galaxies. The relative bias of the measurement in Figure [8](#S5.F8 "Figure
    8 ‣ 5.3 Deblending with Lumos ‣ 5 Lumos flux measurements on simulations ‣ The
    PAU survey: Estimating galaxy photometry with deep learning") fluctuates around
    2-10% for different distances R. Unlike aperture photometry, Lumos is able to
    distinguish between the two galaxies and consider the overlapping one a source
    of noise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [9](#S5.F9 "Figure 9 ‣ 5.3 Deblending with Lumos ‣ 5 Lumos flux measurements
    on simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning"),
    we have explored more the Lumos deblending capability as a function of the magnitude
    and distance to the overlapping galaxy. For the top plot, we have simulated 20
    image cutouts with an $i_{\rm AB}\approx 21$ galaxy at the center (the same for
    the twenty realisations). In each of the cutouts, we have included a second source
    always located at five pixels from the center, but which varies brightness among
    realisations. The plot shows the photometry accuracy with Lumos (blue solid line)
    and aperture photometry (red dashed line) for different magnitudes of the overlapping
    sources. For this study, we have also applied a $\sigma$-clipping of the annulus
    so that the background measurement in the aperture photometry is more robust.
    As indicated in the previous test, Lumos shows more robust towards overlapping
    nearby sources, even when these are bright. In all cases, aperture photometry
    provides biased measurements due to the proximity of the source, which is always
    accounted within the aperture pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: For the bottom plot, we have proceeded similarly. We have generated ten realisations
    of the same galaxy with magnitude $i_{\rm AB}\approx 21$ and in each of the cutouts,
    we have included a second source with $i_{\rm AB}\approx 22$, but located at a
    different distance from the target source. The plot exhibits the photometry accuracy
    as a function of the distance between the target and the overlapping source. Again,
    the plot shows much better accuracy for Lumos than aperture photometry.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c35f2771737ba4df505ea6a6dc2f4961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Accuracy in the flux measurement when the target galaxy $(i_{\rm
    AB=22}$) is blended with another source $(i_{\rm AB=20}$) as a function of the
    relative distance between Teahupoo target galaxy and the ’blending’ PAUS galaxy
    (R). The solid blue line corresponds to Lumos, while the red dashed line is forced
    photometry.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d79e177cea9bcb0d3560891e270d84f2.png)![Refer to caption](img/6b52264c78dc789a8c7b81f92ec8323a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Accuracy in the flux predictions in the presence of overlapping sources
    for Lumos (solid blue line) and aperture photometry (dashed red line) as a function
    of *Top*: magnitude of the overlapping source. *Bottom* Distance in pixels between
    the target and the overlapping source.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Lumos photometry on PAUS data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we present the photometry extracted from PAUCam images in the
    COSMOS field with Lumos. First, we show single observation measurements (§[6.1](#S6.SS1
    "6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU
    survey: Estimating galaxy photometry with deep learning")) and compare them to
    SDSS measurements (§[6.2](#S6.SS2 "6.2 Comparison with SDSS spectroscopy ‣ 6 Lumos
    photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning")). We then discuss the co-added flux measurements (§[6.3](#S6.SS3 "6.3
    Coadded flux measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey:
    Estimating galaxy photometry with deep learning")) and show the photometric redshift
    results with Lumos photometry (§[6.4](#S6.SS4 "6.4 Photometric redshift estimates
    ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Single exposure measurements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PAUS has taken around 10,000 images in the COSMOS field (§[2.1](#S2.SS1 "2.1
    PAUS data ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")),
    which contain ten million galaxy observations. In this section we will only show
    the results on the spectroscopic sample with $i_{\rm AB}<22.5$, which are $\approx$
    3 million exposures from 15,000 galaxies. For the targeting, we use the Ilbert
    et al. ([2008](#bib.bib35)) catalogue, the same external catalogue that MEMBA
    (the PAUS aperture photometry algorithm, see §[2.1](#S2.SS1 "2.1 PAUS data ‣ 2
    Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")) uses.
    Given a PAUCam image, Lumos matches it with the detection catalogue and creates
    the cutouts around the sources. Each source is also matched to the external catalogue,
    which provides a value for the half light radius, the ellipticity and the Sérsic
    index. From these values, Lumos generates the modelled galaxy profiles (see §[4.1](#S4.SS1
    "4.1 Input data ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")). Then, it provides the flux probability
    distribution of the observed galaxies.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88f5b7455579b6b2b919b16726bd75cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Flux and flux uncertainty ratios between Lumos and MEMBA photometry
    in equally populated magnitude bins. The shaded areas correspond to the 16th and
    84th quantiles.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Flux and flux error measurements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we will use flux and flux error point estimates calculated
    as the mean and the variance of the Lumos flux PDFs (see §[5.1](#S5.SS1 "5.1 Flux
    probability distributions ‣ 5 Lumos flux measurements on simulations ‣ The PAU
    survey: Estimating galaxy photometry with deep learning")), which corresponds
    to the total galaxy flux. For MEMBA, we will use flux measurements from aperture
    photometry using the background subtraction from BKGnet, which has proven more
    accurate than that estimated with an annulus. Figure [10](#S6.F10 "Figure 10 ‣
    6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") shows the flux (dashed blue
    line) and flux error (solid red line) ratios between Lumos and MEMBA photometry
    in all NB filters. The shaded areas correspond to the 16th and 84th quantiles.
    For the full sample, the flux ratio between the two photometries is 0.99\. In
    magnitude bins, this ratio oscillates between 0.95 and 1.02, with Lumos measuring
    $\approx$ 4% less flux in the brightest ($i_{\rm AB}<18$) and the faintest ($i_{\rm
    AB}>22$) bins. At the faintest end, the spread in the flux ratio increases, which
    is natural since these galaxies are noisier. Studying each NB filter independently,
    all the ratios but those from the three bluest bands (’NB455’, ’NB465’ and ’NB475’)
    oscillate between 0.95 and 1.03\. The three bluest bands display a $\approx 0.9$
    ratio between MEMBA and Lumos, which is attributed to very faint galaxies with
    negative flux measurements in MEMBA, which are not allowed in Lumos. MEMBA has
    proven accurate enough to obtain very precise photo-zs, therefore measuring similar
    fluxes with MEMBA and Lumos is a good first test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Altogether, Lumos also provides 40% lower flux uncertainties than MEMBA displaying
    a lower error for 85% of the measurements. The ratio between Lumos and MEMBA flux
    uncertainties (Fig [10](#S6.F10 "Figure 10 ‣ 6.1 Single exposure measurements
    ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")) is not constant with magnitude. Lumos shows 60% lower errors
    for objects with $i_{\rm AB}>22$. This number monotonically decreases to e.g.
    30% at $i_{\rm AB}=21$ or 10% at $i_{\rm AB}=20.5$, while some of the brightest
    objects display lower errors with MEMBA. At the brightest end, one can note a
    large scatter of the error ratio. This is attributed to some very bright galaxies
    with a significantly large error with Lumos. However, note that aperture photometry
    provides a purely statistical error, while in Lumos any additional source of uncertainty,
    e.g. an artifact in the cutout, inaccuracies in the profile parameters used to
    infer the photometry ($n_{\rm s}$, $r_{\rm 50}$) or data reduction issues underrepresented
    in the training simulations will be also accounted in the error estimate. As Lumos
    is provided with both the galaxy image and the model, it is able to capture discrepancies
    or potential sources of inaccuracies and account for them in the flux uncertainty.
    Following this line of study, inaccuracies in the profile parameters would most
    likely have a larger impact on the photometry of large, bright and resolved galaxies,
    where e.g. slightly underestimating $r_{50}$ could easily lead to a quite biased
    flux measurement. This could also explain the increment of spread in the uncertainty
    ratio at the brightest end.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Colour histograms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assuming that galaxies have an underlying distribution of colours, the width
    of the colour histograms is an estimation of the uncertainty in the photometry
    measurements. The intrinsic width of the colour histogram is broaden by photometry
    uncertainties and consequently, the photometry providing narrower colour histograms
    is that with lowest uncertainties. Using colour histograms to compare photometries
    was used in Wright et al. ([2016](#bib.bib69)), where they presented and applied
    LAMBDAR to improve the Galaxy and Mass Assembly (GAMA, Driver et al., [2011](#bib.bib21))
    photometry .
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [11](#S6.F11 "Figure 11 ‣ 6.1.2 Colour histograms ‣ 6.1 Single exposure
    measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy
    photometry with deep learning") shows the NB785-NB795 colour distribution (more
    colour histograms can be found in Appendix [E](#A5 "Appendix E Colour histograms
    in the complete narrow band set ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")). By eye it can be already noted that Lumos provides a narrower
    colour distribution than MEMBA. We have estimated the width of such colour histograms
    with $\sigma_{\rm 68}$ (Eq. [14](#S5.E14 "In 5.1.2 Single flux and flux uncertainty
    measurements ‣ 5.1 Flux probability distributions ‣ 5 Lumos flux measurements
    on simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning"))
    and $\sigma_{\rm 95}$ (equivalent to $\sigma_{\rm 68}$ but considering the 2.5
    and 97.5 quantiles, i.e. the width accounts for 95% of the data). Concretely,
    MEMBA provides $\sigma_{\rm 68}$ = 0.26, while Lumos results in $\sigma_{\rm 68}$
    = 0.19, which corresponds to a 30% lower effective width. Considering $\sigma_{\rm
    95}$, Lumos reduces the width a factor of $\approx$3, from 0.74 to 0.41\. This
    also suggests that Lumos reduced the number of photometry outliers, which are
    not affecting $\sigma_{\rm 68}$ but enlarge $\sigma_{\rm 95}$. Such photometric
    outliers are located asymmetrically on the tails of the distribution, which triggers
    the skewness of the histograms and therefore a shift in the median of the MEMBA
    histogram with respect to that of Lumos. This can be noted in the NB785-NB795
    colour histogram, but also in other colour histograms in Figure [22](#A5.F22 "Figure
    22 ‣ Appendix E Colour histograms in the complete narrow band set ‣ The PAU survey:
    Estimating galaxy photometry with deep learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the other narrow bands also show narrower color histograms with Lumos (see
    Appendix [E](#A5 "Appendix E Colour histograms in the complete narrow band set
    ‣ The PAU survey: Estimating galaxy photometry with deep learning") for more details).
    Furthermore, the relative difference in $\sigma_{\rm 95}$ is systematically higher
    than with $\sigma_{\rm 68}$. This is likely related with exposures with noisy
    photometry and outliers, which lay in the tails of the colour histograms (see
    Appendix [E](#A5 "Appendix E Colour histograms in the complete narrow band set
    ‣ The PAU survey: Estimating galaxy photometry with deep learning") for more details).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49639ad1a256643df92e84fe32e56e14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: MEMBA (red dashed line) and Lumos (solid black line) colour histogram
    for NB colour (NB785-NB795). Note that the photometry with lower uncertainties
    is that displaying a narrower colour histogram, which in this case is the Lumos
    photometry'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Validation of the flux uncertainties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To test Lumos flux uncertainties and ensure that these are not artificially
    low, we have made use of PAUS taking multiple observations of the same galaxy
    in the same narrow band filter. Given two observations of the same galaxy in the
    same NB, their flux measurements and uncertainties must be compatible. This is
    formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D\equiv\frac{(f_{\rm 1}-f_{\rm 2})}{\sqrt{(\sigma^{2}_{\rm 1}+\sigma^{2}_{\rm
    2})}}\,,$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{\rm 1}$, $f_{\rm 2}$ are the flux estimates of two exposures of the
    same object and $\sigma_{\rm 1}$, $\sigma_{\rm 2}$ are their associated uncertainties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [12](#S6.F12 "Figure 12 ‣ 6.1.3 Validation of the flux uncertainties
    ‣ 6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU
    survey: Estimating galaxy photometry with deep learning") shows the width of the
    $D$ distribution in equally populated magnitude bins. If the photometry uncertainties
    are properly accounted for, the distribution of $D$ (Eq. [19](#S6.E19 "In 6.1.3
    Validation of the flux uncertainties ‣ 6.1 Single exposure measurements ‣ 6 Lumos
    photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning")) should be a Gaussian with unit standard deviation. To be less affected
    by outliers, we have estimated the width of D with $\sigma_{\rm 68}$ (Eq. [14](#S5.E14
    "In 5.1.2 Single flux and flux uncertainty measurements ‣ 5.1 Flux probability
    distributions ‣ 5 Lumos flux measurements on simulations ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")) and both Lumos and MEMBA display a quite
    constant unity $\sigma_{\rm 68}[D]$ along the tested magnitude range (solid black
    and red dashed lines, respectively). In the case of MEMBA, the background estimation
    with aperture photometry was providing 20% underestimated errors at the bright
    end. This trend was fixed using BKGnet (see Fig. 11 in Cabayol-Garcia et al. ([2020](#bib.bib13))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [13](#S6.F13 "Figure 13 ‣ 6.1.3 Validation of the flux uncertainties
    ‣ 6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU
    survey: Estimating galaxy photometry with deep learning") shows the distribution
    of the quantity defined in Equation [19](#S6.E19 "In 6.1.3 Validation of the flux
    uncertainties ‣ 6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS
    data ‣ The PAU survey: Estimating galaxy photometry with deep learning") with
    Lumos (solid black) and MEMBA (dashed red) photometries. As expected, both of
    them fit a Gaussian with zero mean and unit variance, however we can note a tail
    of outliers in the MEMBA photometry not present with Lumos. This can be connected
    to Lumos providing not purely statistical uncertainties. While inaccuracies in
    the profile parameters or contaminating effects at the image level are not considered
    in MEMBA flux errors, Lumos is flexible enough to provide an error estimate that
    already takes into account these effects. As an example, if the parent catalogue
    provides a 10% underestimated $r_{\rm 50}$ for a particular galaxy, with aperture
    photometry the aperture size and consequently the flux measurement will be also
    underestimated. However, the flux uncertainty will only account for the statistical
    variation in the pixels within the aperture, while the error in the galaxy profile
    will not be considered. In contrast, Lumos is provided with the galaxy and the
    galaxy modelled image and therefore, differences between these two are captured
    and accounted for in the flux uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24f6cc70f02a01f3fcb09e2d119cef74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The width of the distribution in Equation [19](#S6.E19 "In 6.1.3
    Validation of the flux uncertainties ‣ 6.1 Single exposure measurements ‣ 6 Lumos
    photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning") for Lumos and MEMBA flux predictions in equally populated magnitude
    bins. Robust uncertainties must provide a unity width (marked by the thick grey
    dotted line).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23a26f21243ba26a579e9411764fafb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Distribution of D (Eq. [19](#S6.E19 "In 6.1.3 Validation of the
    flux uncertainties ‣ 6.1 Single exposure measurements ‣ 6 Lumos photometry on
    PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep learning"))
    estimated with Lumos (solid black) and MEMBA (dashed red) photometries in logarithmic
    scale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [14](#S6.F14 "Figure 14 ‣ 6.1.3 Validation of the flux uncertainties
    ‣ 6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU
    survey: Estimating galaxy photometry with deep learning") compares the median
    SNR per narrow band in MEMBA and Lumos photometries for galaxies with $i_{\rm
    AB}<22.5$. The shaded areas correspond to the 16-th and 84-th quantiles of the
    SNR distribution. For the complete photometry catalogue, on average Lumos provides
    a 54% higher SNR. Furthermore, it gives a higher median SNR at all wavelengths,
    although the increment with respect to MEMBA is higher in bluer bands. For galaxies
    with $i_{\rm AB}>22$, the SNR is 2.5 times higher in Lumos. The ratio increases
    to 3 taking into account only the bluest narrow band ("NB455") and decreases to
    a factor of 2 for the reddest one ("NB845"). This is natural considering that
    Lumos gives the greatest improvement in terms of SNR for faint objects. Altogether,
    $\approx 85\%$ of the observations have higher SNR with Lumos photometry.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/698b985d62952bd53a8be81acc3f2f2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Median SNR per narrow band filter with Lumos and MEMBA flux measurements.
    Shaded areas are the 16-th and 84-th quantiles of the SNR distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Observation’s flagging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The MEMBA pipeline already provides an outlier flag for its measurements. This
    is a discrete value that flags objects with problematic image reductions, e.g.
    saturated pixels, crosstalk, cosmetics, distortion or undesirable artifacts near
    the target source such as scattered light, cosmic rays and blending. Lumos uses
    the reduced PAUCam images, which are affected by all these effects. However, as
    we already showed in Cabayol-Garcia et al. ([2020](#bib.bib13)) and earlier in
    this paper (§[5.3](#S5.SS3 "5.3 Deblending with Lumos ‣ 5 Lumos flux measurements
    on simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning")),
    Lumos deals with recurrent problems as scattered light or blending. Figure [15](#S6.F15
    "Figure 15 ‣ 6.1.4 Observation’s flagging ‣ 6.1 Single exposure measurements ‣
    6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") shows examples of scattered light (left panel) and cosmic
    ray (right panel) affected observations. For the former, MEMBA provides a flux
    of -43.87 $e^{\rm-}$/s, while Lumos measures 23.01 $e^{\rm-}$/s. In the cosmic
    ray example, MEMBA provides a calibrated flux of -211.21 $e^{\rm-}$/s, while with
    Lumos this is 110.76 $e^{\rm-}$/s. Other observations of the same galaxy in the
    same NB filter provide a mean flux 100.75 $e^{\rm-}$/s, which suggests that the
    Lumos measurement is closer to the correct flux.'
  prefs: []
  type: TYPE_NORMAL
- en: Currently with aperture photometry 10% of the observations are flagged. Within
    these flagged objects, 72% are observations affected by scattered light, 18% have
    image distortion effects and the rest is distributed among other minority effects
    such as e.g. crosstalk, cosmic rays or cosmetics. We have observed that Lumos
    predictions are only affected in the presence of image distortions, cosmetics
    and saturated pixels, which reduces the number of flagged observations from 10%
    to 2%. This reduction highlights that Lumos is more robust towards outliers in
    the photometry, which it is particularly interesting since the network is not
    explicitly trained to deal with artifacts as cosmic rays or crosstalk signals.
    However, by using real PAUCam background cutouts, we include examples of such
    effects in the training sample from which Lumos learns to make robust predictions.
    As a result, Lumos increases the size of the galaxy sample that is considered
    reliable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/909c87322175317554ce95ed73714c9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Observations affected by scattered light (left panel) and cosmic
    rays (right panel). While MEMBA provides outlier flux measurements for both observations,
    Lumos estimates a flux close to that measured in other exposures of the same object.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Comparison with SDSS spectroscopy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further validate the flux estimates, we have compared our measurements with
    synthetic PAUS photometry. These are constructed convolving SDSS galaxy spectra
    with the PAUCam filter throughput. Unfortunately, the synthetic PAUS data corresponds
    to a bright sample with a magnitude limit $i_{\rm AB}<20.5$, which only provides
    validation of bright sources. Comparing PAUS with PAUS synthetic data requires
    having spectra and PAUS photometry of the same galaxies and matching them by sky
    position (we have paired galaxies within 0.5 arcsec). It also requires scaling
    the synthetic PAUS fluxes with a multiplicative zero point (zp). The zero point
    is obtained by minimising the $\chi^{2}$ between PAUS observations and PAUS synthetic
    fluxes, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\chi^{2}=\sum_{\rm i}\frac{\left(f_{\rm SDSS,i}-{\rm zp}\cdot f_{\rm
    SDSS_{\rm PAUS,i}}\right)^{2}}{\sigma_{\rm SDSS,i}^{2}+{\rm zp}^{2}\cdot\sigma_{\rm
    SDSS_{\rm PAUS,i}}^{2}},$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: where SDSS is the observed SDSS photometry, ${\rm SDSS}_{\rm PAUS}$ is the SDSS-PAUS
    synthetic flux and the sum (i) is over the $gri$ bands. This minimisation provides
    a median of 1.64 with a $\sigma_{\rm 68}$ = 1.01\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [16](#S6.F16 "Figure 16 ‣ 6.2 Comparison with SDSS spectroscopy ‣ 6
    Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry with
    deep learning") shows that both MEMBA and Lumos agree well with SDSS-PAUS convolved
    flux measurements. MEMBA displays a lower spread than Lumos, with $\sigma_{\rm
    68}$ = 0.22 and 0.24, respectively. Nonetheless, Lumos shows a 5% bias while in
    MEMBA this goes to 10%. Furthermore, the number of observations at more than $5\sigma$
    from the mean of the distribution is reduced by 2 with Lumos, going from 3% to
    1.5%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/010cd148c258b9e4d51c98b12445e3ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Comparison between PAUS flux measurements and SDSS measurements
    convolved with PAUCam filters (PAUS synthetic fluxes). The solid black line corresponds
    to Lumos measurements and the red dashed line to MEMBA.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Coadded flux measurements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The co-added flux measurements are constructed combining individual observations
    of the same galaxy in the same narrow band. Co-adding exposures increases the
    SNR of the galaxy photometry and it is very helpful to reject wrong observations.
    Before co-adding individual observations, a zero-point calibration per image is
    required, which in our case is done relative to SDSS (see §[2.1](#S2.SS1 "2.1
    PAUS data ‣ 2 Data ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    for more details about PAUS data). One common way of co-adding flux measurements
    ($f_{\rm coadd}$), and the currently implemented in PAUS, is a weighted sum of
    the individual observations'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{\rm coadd}=\frac{\sum_{\rm i}f_{\rm i}/\sigma_{i}^{2}}{\sum_{\rm i}1/\sigma_{i}^{2}},$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where the weights are the inverse variance of the observations and $f_{\rm i}$
    and $\sigma_{i}^{2}$ are the flux measurement and its variance of the i-th observation,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lumos calibrates the Gaussian components individually with the photometric
    zero point in such a way that when the these are combined, they already provide
    a calibrated PDF for the flux observation. With Lumos, combining point like estimates
    with Equation [21](#S6.E21 "In 6.3 Coadded flux measurements ‣ 6 Lumos photometry
    on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    is still possible. Nevertheless, it can also generate co-added measurements combining
    the probability distribution of the individual galaxy observations. Figure [17](#S6.F17
    "Figure 17 ‣ 6.3 Coadded flux measurements ‣ 6 Lumos photometry on PAUS data ‣
    The PAU survey: Estimating galaxy photometry with deep learning") shows an example
    of the co-added flux PDF of a faint galaxy. The dashed coloured lines correspond
    to the individual observations while the black line is the co-added PDF. This
    example also shows the benefit of creating co-adds at a PDF level. Combining point-like
    values would only provide a flux measurement close to the co-added PDF peak. In
    contrast, combining the PDFs keeps the contributions from the secondary peaks
    and the tails and therefore, it contains more valuable information about the measurement
    than a single point-like estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, having the PDF allows the calculation of the flux and flux error
    using different statistical estimators, e.g. the median or the peak. From the
    co-added flux PDF, we estimate the mean, median, the peak, the variance, $\sigma_{\rm
    68}$ and $\sigma_{\rm 95}$. From the two latest quantities, we construct a measurement
    of the PDFs gaussianity,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\eta\equiv\sigma_{\rm 95}/\sigma_{\rm 68}-1\,,$ |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'which can be helpful to decide which are the best flux and flux error estimators.
    While for a sharp and peaked PDF ($\eta\gtrapprox 1$), the peak or the median
    would provide similar flux estimates, in non-gaussian PDFs as e.g. the galaxy
    in Figure [17](#S6.F17 "Figure 17 ‣ 6.3 Coadded flux measurements ‣ 6 Lumos photometry
    on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    these two estimators would give significantly different measurements. The PDF
    gaussianity also affects the flux uncertainty estimators. Broadly, galaxies with
    $\eta>1$ have $\sigma_{\rm 68}<\sigma_{\rm std}$, while this is the opposite for
    galaxies with $\eta<1$. Figure [17](#S6.F17 "Figure 17 ‣ 6.3 Coadded flux measurements
    ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") ($\eta=0.56$) is also an example of multiple peaked PDF where
    $\sigma_{\rm 68}$ > $\sigma_{\rm std}$ (see the top right box in the Figure for
    more details).'
  prefs: []
  type: TYPE_NORMAL
- en: We have tested applying different flux and flux error estimators based on the
    $\eta$ parameter. However, at the end of the day we have found that the peak of
    the flux PDF is the best estimator regardless of the PDF gaussianity and that
    using $\sigma_{\rm 68}$, $\sigma_{\rm std}$ or $\sigma_{\rm 95}/2$ does not lead
    to a significant difference.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b73715d36761806ff2e710c69677fada.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The co-added flux probability distribution (black solid line) constructed
    from its individual observations (colored dashed lines). The upper box displays
    the $\sigma_{\rm std}$, $\sigma_{\rm 68}$ and $\sigma_{\rm 95}$ of the co-added
    PDF.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Photometric redshift estimates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Accurate photo-z estimates are crucial for many science applications. Improving
    the photometry SNR is expected to improve the photo-z estimates. In this section,
    we have tested Lumos photometry with BCNz2 (Eriksen et al., [2019](#bib.bib24))
    and Deepz (Eriksen et al., [2020](#bib.bib25)): a template based method and a
    deep learning algorithm built specifically for estimating PAUS photo-zs. As there
    is not a sample of galaxies with known photometry, testing the photo-zs has also
    been particularly helpful to find and fix some issues in the Lumos photometry
    that were missed with other validation tests. One example of this are galaxies
    which were exhibiting an oscillating photometry. This kind of objects were detected
    as photo-z outliers and we could trace that these were triggered by galaxy images
    with sub-pixels shifts with respect to the center of the stamp.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [18](#S6.F18 "Figure 18 ‣ 6.4 Photometric redshift estimates ‣ 6 Lumos
    photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning") shows the photo-z dispersion with BCNz2 or Deepz using Lumos photometry
    to $i_{\rm AB}<22.5$. We have also included the photo-z result with the MEMBA
    forced aperture photometry as a comparison. With BCNz2, Lumos photometry reduces
    the photo-z scatter by 5-15% for galaxies with $i_{\rm AB}>20.5$. However, the
    right panel also shows a small degradation at the faintest galaxies with BCNz2
    on Lumos photometry (dashed blue line). This degradation is related with the galaxy
    redshift rather than to its brightness. At high redshift, photo-zs with Lumos
    photometry are statistically better, however there are some high redshfit outliers
    that increase $\sigma_{\rm 68}$. Rejecting galaxies with spectroscopic redshift
    ($z_{\rm s}$) $z_{\rm s}$ > 0.8, the faintest galaxies ($i_{\rm AB}>22$) have
    a 14% lower photo-z dispersion with Lumos than with MEMBA photometry.'
  prefs: []
  type: TYPE_NORMAL
- en: Photo-zs with Deepz are not showing this degradation at high redshift. For galaxies
    with $i_{\rm AB}>20.5$, the Deepz photo-zs are between 10% and 20% more precise
    with Lumos photometry. Furthermore, at $i_{\rm AB}>22$ and without any redshift
    cut, photo-zs are 15% better. This suggests that the minor degradation with BCNz2
    at high redshift is caused by the photo-z code.
  prefs: []
  type: TYPE_NORMAL
- en: With both photo-z codes, the performance is degrading at the brightest end with
    the Lumos photometry. This could potentially be triggered by differences between
    the Teahupoo image simulations and the data. Bright galaxies with higher SNR are
    more resolved. Therefore, discrepancies between the training simulations and the
    data are more evident and these could have a stronger effect on the network’s
    performance. Nevertheless, the fraction of objects affected by this effect is
    small and furthermore, these are the brightest galaxies, which are not those we
    are more interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/44e96da0f3b04e21c3827de6d1fc09ff.png)![Refer to caption](img/f714749c80a1d411cf6491f4b5d82403.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: *Left:* Photo-z precision with BCNz2 and Deepz using Lumos or MEMBA
    photometry. *Right*: Relative difference between photo-zs with Lumos or MEMBA
    photometry.'
  prefs: []
  type: TYPE_NORMAL
- en: In PAUS, a galaxy is considered an outlier if
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;z_{\rm p}-z_{\rm s}&#124;\ /\ (1+z_{\rm s})>0.02\,,$ |  | (23)
    |'
  prefs: []
  type: TYPE_TB
- en: where $z_{\rm p}$ is the photo-z and $z_{\rm s}$ is the spec-z. This outlier
    definition is very strict compared to broad band photometry, where a common outlier
    definition is $|z_{\rm p}-z_{\rm s}|>0.15\,(1+z_{\rm s})$, e.g. Ilbert et al.
    ([2006](#bib.bib34)); Bilicki et al. ([2018](#bib.bib9)). Lumos photometry reduces
    the outlier rate with both BCNz2 and Deepz. With BCNz2, the outlier rate is reduced
    by 5% in the complete catalogue. With Deepz, the improvement is greater, with
    20% less outliers. This number increases to 23% for objects with $i_{\rm AB}>22$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, we do not know about any photo-z code that could
    properly deal with flux PDFs and both BCNz2 and Deepz require point estimates
    for the flux and its uncertainty. Here, these quantities are estimated as the
    peak and $\sigma_{\rm 68}$ of the co-added flux PDF. We have also tested other
    quantities as e.g. the median and the standard deviation or choosing different
    estimators based on the $\eta$ parameters (Eq. [22](#S6.E22 "In 6.3 Coadded flux
    measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy
    photometry with deep learning")). However, the peak and $\sigma_{\rm 68}$ are
    those providing the best photo-z estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The photo-z improvement obtained with Lumos photometry is lower than expected
    considering the increment in the SNR. In Appendix [D](#A4 "Appendix D Photometric
    redshifts with BCNz2 on PAUS galaxy mocks ‣ The PAU survey: Estimating galaxy
    photometry with deep learning"), we have used PAUS simulated mocks to test BCNz2
    performance with SNR and its behaviour with artificially injected issues in the
    sample photometry. The results suggest that the photo-z improvement should be
    $\approx$90% greater than what we actually see on data if the sample had perfect
    photometry. However, errors in the zero-point calibration and outliers in the
    flux measurements rapidly degrade the photo-z performance, suggesting that currently
    these are potential limiting factors of the photo-z performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions and discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accurate galaxy photometry is a key ingredient for imaging surveys to obtain
    precise photometric redshifts. We have developed Lumos, a deep learning method
    to estimate the galaxy flux for astronomical images. Lumos is the evolution of
    BKGnet, a deep learning method that predicts the background light of astronomical
    images with strongly varying noise patterns. In contrast, Lumos predicts the background
    subtracted galaxy flux, which requires an intrinsic background noise measurement.
    The algorithm has been developed for PAUCam images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lumos is trained on Teahupoo galaxies, image simulations specially built for
    this work (see Fig. [2](#S2.F2 "Figure 2 ‣ 2.2 Teahupoo simulations ‣ 2 Data ‣
    The PAU survey: Estimating galaxy photometry with deep learning")). Teahupoo galaxy
    images use PAUCam image cutouts for the background noise. First, astronomical
    images contain distorting effects, e.g. scattered light or crosstalk, and artifacts
    like e.g. cosmic rays or blended galaxies, that trigger inaccuracies in the photometry.
    Including real PAUCam cutouts in our simulations ensures that Lumos has training
    examples to learn how to deal with such effects. Without explicitly developing
    Lumos to provide photometry in the presence of distorting artifacts, the network
    provides reliable flux measurements on PAUCam observations affected by scattered
    light, cosmic rays or other contaminating effects that require flagging with aperture
    photometry.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we have tested the Lumos deblending capability on simulations
    (see §[5.3](#S5.SS3 "5.3 Deblending with Lumos ‣ 5 Lumos flux measurements on
    simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    Without explicitly including blended galaxies in the training sample, Lumos is
    able to extract the target galaxy photometry much better than aperture photometry
    (Fig [8](#S5.F8 "Figure 8 ‣ 5.3 Deblending with Lumos ‣ 5 Lumos flux measurements
    on simulations ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    While aperture photometry provides a catastrophic flux measurement for blended
    sources, Lumos is able to provide a flux with 2-10% accuracy, depending on the
    distance in pixels to the overlapping source. This is particularly interesting
    since Lumos has not been written to deblend galaxies, however this came without
    additional cost by using deep learning and real PAUCam background noise patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lumos consists on a CNN followed by a MDN (Fig. [4](#S4.F4 "Figure 4 ‣ 4.2
    Lumos architecture ‣ 4 Lumos: Measuring fluxes with a CNN ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")). While most photometry algorithms provide
    a flux value and an associated uncertainty, Lumos outputs the flux probability
    distribution as a linear combination of five Gaussian distributions. Even if many
    science applications require photometry point estimates, having the PDF enables
    the generation of a co-added flux PDF (§[6.3](#S6.SS3 "6.3 Coadded flux measurements
    ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")). The co-added flux PDF keeps valuable information about
    the individual flux exposure distributions that would be missed by combining point-like
    estimates. While using the full PDF would require reworking of the pipelines using
    the photometry as an input, this can also be part of an end-to-end photometry
    machine learning pipeline that goes from images to photo-z estimates. The network
    could benefit from all the information available in the full PDF to provide more
    precise photo-z estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On PAUS observations, Lumos provides fluxes that differ less than 1% from the
    baseline aperture photometry measurements (see Fig [10](#S6.F10 "Figure 10 ‣ 6.1
    Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey:
    Estimating galaxy photometry with deep learning")). Concerning uncertainties,
    our photometry errors are 40% lower than with aperture photometry. This translates
    into between 1.5 and 3 times higher SNR in Lumos than in MEMBA, with the largest
    improvement at the faint end (Fig. [14](#S6.F14 "Figure 14 ‣ 6.1.3 Validation
    of the flux uncertainties ‣ 6.1 Single exposure measurements ‣ 6 Lumos photometry
    on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    We have run the BCNz2 and Deepz codes with Lumos photometry, resulting in a reduction
    of the photo-z scatter with both (Fig [18](#S6.F18 "Figure 18 ‣ 6.4 Photometric
    redshift estimates ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")). The photo-z improvement using Lumos photometry
    is greater with Deepz rather than with BCNz2, with an overall scatter reduction
    of 10% on the full catalogue and 13% for galaxies with $i_{\rm AB}>22$ and an
    outlier rate reduction of $\approx$ 20%. Nevertheless, Appendix [D](#A4 "Appendix
    D Photometric redshifts with BCNz2 on PAUS galaxy mocks ‣ The PAU survey: Estimating
    galaxy photometry with deep learning") shows that the photo-z improvement is limited
    by the photometric calibration and outliers in the sample. These outliers can
    have different natures as e.g. the Lumos photometry itself or problems in the
    reduced PAUCam images.'
  prefs: []
  type: TYPE_NORMAL
- en: Lumos obtains the largest improvement at fainter galaxies, showing less degradation
    than aperture photometry. Future imaging surveys like Euclid or Rubin will observe
    much deeper galaxies with very low SNR, where Lumos could be a very helpful tool
    to improve the photometry. Furthermore, Lumos has been shown to be robust for
    blended sources, which could be also beneficial for future deeper surveys where
    the number of blended galaxies will significantly increase.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have only tested the method on PAUCam images so far, we believe
    the methodology should readily apply to other imaging surveys. Training the network
    for other surveys can be addressed by training the network from scratch with simulated
    galaxies using real background cutouts from the targeted survey. Nevertheless,
    one potential difficulty of the method applied to deeper surveys is the the modelled
    galaxy profile input requirement. While PAUS galaxies are shallow and therefore,
    there exists previous observations, deeper surveys like Rubin will observe galaxies
    for which there is not previous knowledge. Not using the modelled profile in the
    training barely affects the overall predicted flux measurements, however this
    degrades the SNR by 15%.
  prefs: []
  type: TYPE_NORMAL
- en: Lumos supersedes BKGnet and provides a background subtracted flux measurements,
    which requires a measure of the background light contribution. Consequently, Lumos
    deals with potential correlations between the galaxy flux and the background light
    that are not easy to address analytically. Moreover, in this work we have combined
    two independent networks, Lumos and Deepz, which provides the greatest photo-z
    obtained. This motivates the construction of an end-to-end pipeline that supersedes
    Lumos providing the galaxy photometry and the photometric redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors thank Malgorzata Siudek, Pablo Renard, Giorgio Manzoni, Jacobo Asorey,
    Luca Tortorelli and Helena Sánchez-Domínguez for providing feedback on the paper.
  prefs: []
  type: TYPE_NORMAL
- en: The PAU Survey is partially supported by MINECO under grants CSD2007-00060,
    AYA2015-71825, ESP2017-89838, PGC2018-094773, PGC2018-102021, SEV-2016-0588, SEV-2016-0597,
    MDM-2015-0509, PID2019-111317GB-C31 and Juan de la Cierva fellowship and LACEGAL
    and EWC Marie Sklodowska-Curie grant No 734374 and no.776247 with ERDF funds from
    the EU Horizon 2020 Programme, some of which include ERDF funds from the European
    Union. IEEC and IFAE are partially funded by the CERCA and Beatriu de Pinos program
    of the Generalitat de Catalunya. Funding for PAUS has also been provided by Durham
    University (via the ERC StG DEGAS-259586), ETH Zurich, Leiden University (via
    ERC StG ADULT-279396 and Netherlands Organisation for Scientific Research (NWO)
    Vici grant 639.043.512), Bochum University (via a Heisenberg grant of the Deutsche
    Forschungsgemeinschaft (Hi 1495/5-1) as well as an ERC Consolidator Grant (No.
    770935)), University College London, Portsmouth support through the Royal Society
    Wolfson fellowship and from the European Union’s Horizon 2020 research and innovation
    programme under the grant agreement No 776247 EWC.
  prefs: []
  type: TYPE_NORMAL
- en: The PAU data center is hosted by the Port d’Informació Científica (PIC), maintained
    through a collaboration of CIEMAT and IFAE, with additional support from Universitat
    Autònoma de Barcelona and ERDF. We acknowledge the PIC services department team
    for their support and fruitful discussions. We gratefully acknowledge the support
    of NVIDIA Corporation with the donation of the Titan V GPU used for this research.
  prefs: []
  type: TYPE_NORMAL
- en: Data availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PAUS raw data is publically available through the ING group. A few reduced
    images are publically available at https://www.pausurvey.org.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ahumada et al. (2020) Ahumada R., et al., 2020, [Astrophys. J. Suppl.](http://dx.doi.org/10.3847/1538-4365/ab929e),
    249, 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aihara et al. (2018) Aihara H., et al., 2018, [PASJ](http://dx.doi.org/10.1093/pasj/psx081),
    [70, S8](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S...8A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alarcon et al. (2021) Alarcon A., et al., 2021, [MNRAS](http://dx.doi.org/10.1093/mnras/staa3659),
    [501, 6103](https://ui.adsabs.harvard.edu/abs/2021MNRAS.501.6103A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arcelin et al. (2021) Arcelin B., Doux C., Aubourg E., Roucelle C., LSST Dark
    Energy Science Collaboration 2021, [MNRAS](http://dx.doi.org/10.1093/mnras/staa3062),
    [500, 531](https://ui.adsabs.harvard.edu/abs/2021MNRAS.500..531A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astropy Collaboration et al. (2013) Astropy Collaboration et al., 2013, [A&A](http://dx.doi.org/10.1051/0004-6361/201322068),
    [558, A33](http://adsabs.harvard.edu/abs/2013A%26A...558A..33A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertin (2006) Bertin E., 2006, in Gabriel C., Arviset C., Ponz D., Enrique S.,
    eds, Astronomical Society of the Pacific Conference Series Vol. 351, Astronomical
    Data Analysis Software and Systems XV. p. 112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertin (2011) Bertin E., 2011, in Evans I. N., Accomazzi A., Mink D. J., Rots
    A. H., eds, Astronomical Society of the Pacific Conference Series Vol. 442, Astronomical
    Data Analysis Software and Systems XX. p. 435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [Astronomy and Astrophysics
    Supplement Series](http://dx.doi.org/10.1051/aas:1996164), [117, 393](https://ui.adsabs.harvard.edu/#abs/1996A&AS..117..393B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilicki et al. (2018) Bilicki M., et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201731942),
    [616, A69](https://ui.adsabs.harvard.edu/abs/2018A&A...616A..69B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bishop (1994) Bishop C. M., 1994, Technical report, Mixture density networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bordoloi et al. (2010) Bordoloi R., Lilly S. J., Amara A., 2010, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.16765.x),
    [406, 881](https://ui.adsabs.harvard.edu/abs/2010MNRAS.406..881B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boucaud et al. (2020) Boucaud A., et al., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/stz3056),
    [491, 2481](https://ui.adsabs.harvard.edu/abs/2020MNRAS.491.2481B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cabayol-Garcia et al. (2020) Cabayol-Garcia L., et al., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/stz3274),
    [491, 5392](https://ui.adsabs.harvard.edu/abs/2020MNRAS.491.5392C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casas et al. (2012) Casas R., et al., 2012, in High Energy, Optical, and Infrared
    Detectors for Astronomy V. p. 845326, [doi:10.1117/12.924640](http://dx.doi.org/10.1117/12.924640)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casas et al. (2016) Casas R., et al., 2016, in Ground-based and Airborne Instrumentation
    for Astronomy VI. p. 99084K, [doi:10.1117/12.2232422](http://dx.doi.org/10.1117/12.2232422)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castander et al. (2012) Castander F. J., et al., 2012, in Ground-based and Airborne
    Instrumentation for Astronomy IV. p. 84466D, [doi:10.1117/12.926234](http://dx.doi.org/10.1117/12.926234)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DES Collaboration et al. (2021) DES Collaboration et al., 2021, arXiv e-prints,
    [p. arXiv:2105.13549](https://ui.adsabs.harvard.edu/abs/2021arXiv210513549D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dawid (1984) Dawid A. P., 1984, Journal of the Royal Statistical Society. Series
    A (General), 147, 278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dawson et al. (2013) Dawson K. S., et al., 2013, [AJ](http://dx.doi.org/10.1088/0004-6256/145/1/10),
    [145, 10](https://ui.adsabs.harvard.edu/abs/2013AJ....145...10D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domínguez Sánchez et al. (2019) Domínguez Sánchez H., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/sty3497),
    [484, 93](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484...93D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver et al. (2011) Driver S. P., et al., 2011, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.18188.x),
    [413, 971](https://ui.adsabs.harvard.edu/abs/2011MNRAS.413..971D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drlica-Wagner et al. (2018) Drlica-Wagner A., et al., 2018, [ApJS](http://dx.doi.org/10.3847/1538-4365/aab4f5),
    [235, 33](https://ui.adsabs.harvard.edu/abs/2018ApJS..235...33D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eckert et al. (2020) Eckert K., et al., 2020, [Mon. Not. Roy. Astron. Soc.](http://dx.doi.org/10.1093/mnras/staa2133),
    497, 2529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eriksen et al. (2019) Eriksen M., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz204),
    [484, 4200](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.4200E)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eriksen et al. (2020) Eriksen M., et al., 2020, [Mon. Not. Roy. Astron. Soc.](http://dx.doi.org/10.1093/mnras/staa2265),
    497, 4565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fisher (1922) Fisher R. A., 1922, [doi:https://doi.org/10.1098/rsta.1922.0009](http://dx.doi.org/https://doi.org/10.1098/rsta.1922.0009),
    222, 309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaia Collaboration et al. (2018) Gaia Collaboration et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833051),
    [616, A1](https://ui.adsabs.harvard.edu/abs/2018A&A...616A...1G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick (2015) Girshick R., 2015, in 2015 IEEE International Conference on
    Computer Vision (ICCV). pp 1440–1448, [doi:10.1109/ICCV.2015.169](http://dx.doi.org/10.1109/ICCV.2015.169)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gneiting et al. (2005) Gneiting T., Raftery A. E., Westveld A. H., Goldman T.,
    2005, [Monthly Weather Review](http://dx.doi.org/10.1175/MWR2904.1), 133, 1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haigh et al. (2021) Haigh C., Chamba N., Venhola A., Peletier R., Doorenbos
    L., Watkins M., Wilkinson M. H. F., 2021, [A&A](http://dx.doi.org/10.1051/0004-6361/201936561),
    [645, A107](https://ui.adsabs.harvard.edu/abs/2021A&A...645A.107H)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hausen & Robertson (2020) Hausen R., Robertson B. E., 2020, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab8868),
    [248, 20](https://ui.adsabs.harvard.edu/abs/2020ApJS..248...20H)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heasley (1999) Heasley J. N., 1999, in Craine E. R., Crawford D. L., Tucker
    R. A., eds, Astronomical Society of the Pacific Conference Series Vol. 189, Precision
    CCD Photometry. p. 56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Herbel et al. (2018) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018, [J. Cosmology Astropart. Phys.](http://dx.doi.org/10.1088/1475-7516/2018/07/054),
    [2018, 054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilbert et al. (2006) Ilbert O., et al., 2006, [A&A](http://dx.doi.org/10.1051/0004-6361:20065138),
    [457, 841](https://ui.adsabs.harvard.edu/abs/2006A&A...457..841I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilbert et al. (2008) Ilbert O., et al., 2008, in Kodama T., Yamada T., Aoki
    K., eds, Astronomical Society of the Pacific Conference Series Vol. 399, Panoramic
    Views of Galaxy Formation and Evolution. p. 169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilbert et al. (2009) Ilbert O., et al., 2009, [ApJ](http://dx.doi.org/10.1088/0004-637X/690/2/1236),
    [690, 1236](https://ui.adsabs.harvard.edu/abs/2009ApJ...690.1236I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ioffe & Szegedy (2015) Ioffe S., Szegedy C., 2015, in Bach F., Blei D., eds,
    Proceedings of Machine Learning Research Vol. 37, Proceedings of the 32nd International
    Conference on Machine Learning. PMLR, Lille, France, pp 448–456, [http://proceedings.mlr.press/v37/ioffe15.html](http://proceedings.mlr.press/v37/ioffe15.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jones et al. (2001) Jones E., Oliphant T., Peterson P., et al., 2001, SciPy:
    Open source scientific tools for Python, [http://www.scipy.org/](http://www.scipy.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kelley (1960) Kelley H. J., 1960, Ars Journal, 30, 947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kennicutt (1998) Kennicutt Robert C. J., 1998, [ARA&A](http://dx.doi.org/10.1146/annurev.astro.36.1.189),
    [36, 189](https://ui.adsabs.harvard.edu/abs/1998ARA&A..36..189K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Ba (2015) Kingma D. P., Ba J., 2015, in Bengio Y., LeCun Y., eds, 3rd
    International Conference on Learning Representations, ICLR 2015, San Diego, CA,
    USA, May 7-9, 2015, Conference Track Proceedings. [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012, in
    Proceedings of the 25th International Conference on Neural Information Processing
    Systems - Volume 1. NIPS’12. Curran Associates Inc., USA, pp 1097–1105, [http://dl.acm.org/citation.cfm?id=2999134.2999257](http://dl.acm.org/citation.cfm?id=2999134.2999257)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kron (1980) Kron R. G., 1980, [ApJS](http://dx.doi.org/10.1086/190669), [43,
    305](https://ui.adsabs.harvard.edu/abs/1980ApJS...43..305K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuijken (2008) Kuijken K., 2008, [A&A](http://dx.doi.org/10.1051/0004-6361:20066601),
    [482, 1053](https://ui.adsabs.harvard.edu/abs/2008A&A...482.1053K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuijken et al. (2019) Kuijken K., et al., 2019, [A&A](http://dx.doi.org/10.1051/0004-6361/201834918),
    [625, A2](https://ui.adsabs.harvard.edu/abs/2019A&A...625A...2K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lang et al. (2016) Lang D., Hogg D. W., Schlegel D. J., 2016, [The Astronomical
    Journal](http://dx.doi.org/10.3847/0004-6256/151/2/36), 151, 36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laureijs et al. (2011) Laureijs R., et al., 2011, preprint (arXiv:1110.3193),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leauthaud et al. (2007) Leauthaud A., et al., 2007, [ApJS](http://dx.doi.org/10.1086/516598),
    [172, 219](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..219L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecun et al. (1998) Lecun Y., Bottou L., Bengio Y., Haffner P., 1998, [Proceedings
    of the IEEE](http://dx.doi.org/10.1109/5.726791), 86, 2278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lilly et al. (2009) Lilly S. J., et al., 2009, [ApJS](http://dx.doi.org/10.1088/0067-0049/184/2/218),
    [184, 218](https://ui.adsabs.harvard.edu/abs/2009ApJS..184..218L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magnier et al. (2020) Magnier E. A., et al., 2020, [ApJS](http://dx.doi.org/10.3847/1538-4365/abb82a),
    [251, 6](https://ui.adsabs.harvard.edu/abs/2020ApJS..251....6M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martí et al. (2014) Martí P., Miquel R., Castander F. J., Gaztañaga E., Eriksen
    M., Sánchez C., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu801), [442, 92](https://ui.adsabs.harvard.edu/#abs/2014MNRAS.442...92M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massey & Refregier (2005) Massey R., Refregier A., 2005, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2005.09453.x),
    [363, 197](https://ui.adsabs.harvard.edu/abs/2005MNRAS.363..197M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merlin et al. (2015) Merlin E., et al., 2015, [A&A](http://dx.doi.org/10.1051/0004-6361/201526471),
    [582, A15](https://ui.adsabs.harvard.edu/abs/2015A&A...582A..15M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mighell (1999) Mighell K. J., 1999, in Craine E. R., Crawford D. L., Tucker
    R. A., eds, Astronomical Society of the Pacific Conference Series Vol. 189, Precision
    CCD Photometry. p. 50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ni et al. (2019) Ni Q., Timlin J., Brandt W. N., Yang G., 2019, [Research Notes
    of the AAS](http://dx.doi.org/10.3847/2515-5172/aaf8af), 3, 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padilla et al. (2016) Padilla C., et al., 2016, in Ground-based and Airborne
    Instrumentation for Astronomy VI. p. 99080Z, [doi:10.1117/12.2231884](http://dx.doi.org/10.1117/12.2231884)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padilla et al. (2019) Padilla C., et al., 2019, [AJ](http://dx.doi.org/10.3847/1538-3881/ab0412),
    [157, 246](https://ui.adsabs.harvard.edu/abs/2019AJ....157..246P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Price-Whelan et al. (2018) Price-Whelan A. M., et al., 2018, [AJ](http://dx.doi.org/10.3847/1538-3881/aabc4f),
    [156, 123](https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refregier (2003) Refregier A., 2003, [Monthly Notices of the Royal Astronomical
    Society](http://dx.doi.org/10.1046/j.1365-8711.2003.05901.x), 338, 35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotham et al. (2018) Robotham A. S. G., Davies L. J. M., Driver S. P., Koushan
    S., Taranu D. S., Casura S., Liske J., 2018, [MNRAS](http://dx.doi.org/10.1093/mnras/sty440),
    [476, 3137](https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.3137R)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rowe et al. (2015) Rowe B. T. P., et al., 2015, [Astronomy and Computing](http://dx.doi.org/10.1016/j.ascom.2015.02.002),
    [10, 121](https://ui.adsabs.harvard.edu/abs/2015A&C....10..121R)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soo et al. (2021) Soo J. Y. H., et al., 2021, preprint (arXiv:2101.03723),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suchyta et al. (2016) Suchyta E., et al., 2016, [Mon. Not. Roy. Astron. Soc.](http://dx.doi.org/10.1093/mnras/stv2953),
    457, 786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2018) Tan C., Sun F., Kong T., Zhang W., Yang C., Liu C., 2018,
    [Lecture Notes in Computer Science](http://dx.doi.org/10.1007/978-3-030-01424-7_27),
    [p. arXiv:1808.01974](https://ui.adsabs.harvard.edu/abs/2018arXiv180801974T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tercan et al. (2018) Tercan H., Guajardo A., Heinisch J., Thiele T., Hopmann
    C., Meisen T., 2018, [Procedia CIRP](http://dx.doi.org/https://doi.org/10.1016/j.procir.2018.03.087),
    72, 185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonello et al. (2019) Tonello N., et al., 2019, [Astronomy and Computing](http://dx.doi.org/10.1016/j.ascom.2019.04.002),
    [27, 171](https://ui.adsabs.harvard.edu/abs/2019A&C....27..171T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wright et al. (2016) Wright A. H., et al., 2016, [MNRAS](http://dx.doi.org/10.1093/mnras/stw832),
    [460, 765](https://ui.adsabs.harvard.edu/abs/2016MNRAS.460..765W)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang & Bloom (2020) Zhang K., Bloom J. S., 2020, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab3fa6),
    [889, 24](https://ui.adsabs.harvard.edu/abs/2020ApJ...889...24Z)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Zhao Z., Zheng P., Xu S., Wu X., 2019, [IEEE Transactions
    on Neural Networks and Learning Systems](http://dx.doi.org/10.1109/TNNLS.2018.2876865),
    30, 3212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. (2019) Zhuang F., Qi Z., Duan K., Xi D., Zhu Y., Zhu H., Xiong
    H., He Q., 2019, preprint (arXiv:1911.02685),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Jong et al. (2013) de Jong J. T. A., et al., 2013, The Messenger, [154, 44](https://ui.adsabs.harvard.edu/abs/2013Msngr.154...44D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Dokkum (2001) van Dokkum P. G., 2001, [PASP](http://dx.doi.org/10.1086/323894),
    [113, 1420](https://ui.adsabs.harvard.edu/abs/2001PASP..113.1420V)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix A Flux estimation methods: derivations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This appendix derives the linear combination of pixel values giving and unbiased
    and optimal flux measurement (Eq. [6](#S3.E6 "In 3.3 Weighted pixel sum ‣ 3 Flux
    estimation methods ‣ The PAU survey: Estimating galaxy photometry with deep learning")).
    The SNR of the measurement when combining pixels with mean $m$ and weight $w$
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm SNR}=\frac{\sum_{\rm i}w_{\rm i}m_{\rm i}}{\sqrt{\sum_{\rm i}w_{\rm
    i}^{2}(m_{\rm i}+b_{\rm i})}},$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: where $b_{i}$ is the background mean value. The optimal SNR is found by requiring
    stationary derivatives for all weights independently, which results in
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{\rm x}=\lambda\frac{m_{\rm x}}{(m_{\rm x}+b_{\rm x})}$ |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is a constant. The flux measurement being unbiased means
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{\rm i}m_{\rm i}=\sum_{\rm i}w_{\rm i}m_{\rm i}.$ |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'Using this requirement, the pixel weights (Eq.[25](#A1.E25 "In Appendix A Flux
    estimation methods: derivations ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")) becomes'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{\rm x}=\frac{\sum_{\rm i}m_{\rm i}}{\sum_{\rm i}m_{\rm i}^{2}/(m_{\rm
    i}+b_{\rm i})}\frac{1}{1+b_{\rm x}/m_{\rm x}}$ |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: Notice that, given a pixel x, its weight $w_{\rm x}$ depends on the true flux
    ($m_{\rm x}$) and background ($b_{\rm x}$) on that concrete pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning methods are data analysis techniques where the algorithm learns
    from the data. In particular, one of the currently most popular classes of algorithms
    are neural networks (Lecun et al., [1998](#bib.bib50)), which are designed to
    recognise patterns, usually learned from training data (supervised method). They
    are mainly used for regression and classification problems. Deep learning is a
    subset of machine learning that refers to the development of neural network technology
    involving a large number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: Other terms that one needs to be familiar with are epoch and batch. An epoch
    is an iteration over the complete training dataset. However, it is common practice
    to give the data to the network in batches. Feeding the network in batches helps
    it learn faster as in every iteration over a batch, it updates all the weights.
    Then, instead of updating once per epoch, it updates as many times as there are
    batches, which speeds up the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning methods, and in general any supervised machine learning algorithm,
    model a problem by optimising a set of trainable parameters that fit the data.
    This is done in three stages: forward propagation, back propagation and weight
    optimisation. The network starts with the forward propagation. At this stage,
    the input data propagates through all the network layers to give a prediction
    for each of the input samples. After that, by comparing the prediction with the
    known true value (label), the network estimates a prediction error with a loss
    function. The ultimate goal of the training procedure is to minimise the loss
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: To minimise the loss, one performs a process named back propagation after evaluating
    the loss function. Back propagation consists of computing the contribution of
    each weight to the loss function (Kelley, [1960](#bib.bib40)). Such contributions
    are calculated using the chain rule. The weight optimisation consists in updating
    the network parameters using the derivatives estimated. This whole procedure takes
    place repeatedly, reducing the loss function after each iteration while adapting
    the parameters to the data. The amount of variation allowed per iteration is regulated
    by the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: After each layer there is an activation function, which are non-linear functions
    that map the outcome of a layer to the input of the following one. This is required
    to produce non-linearities in the model. An example of an activation function
    is the Rectified Linear Unit (ReLu) (Krizhevsky et al., [2012](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we use a Convolutional Neural Network (CNN; Lecun et al., [1998](#bib.bib50)).
    Our network contains four differentiated types of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolutional layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: This layer makes the network powerful in image and pattern recognition tasks.
    It has a filter, technically named kernel and is usually 2-dimensional, which
    contains a set of trainable weights used to convolve the image. The outcome of
    this layer is the input image convolved with the kernel. In a given convolutional
    layer, each of the outputs is a linear combination of the different convolutions.
    Each of these convolutions will generate a convolved image, which we refer to
    as channel. All of them together are the input of the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pooling layers*:'
  prefs: []
  type: TYPE_NORMAL
- en: This layer reduces the dimensionality of the set of convolved images. It applies
    some function (e.g. maximum, sum, mean) to a group of spatially connected pixels
    and reduces the dimensions of such group. For example, the Max-Pooling, which
    we use in this paper, takes a e.g. (2x2) group of pixels and converts them to
    their maximum. Although we use it to handle the amount of data generated after
    the convolutions, it also regularises the model to avoid learning from non-generalisable
    noise and details in the training data (also known as overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: '*Batch normalisation layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: In this layer the network normalises the output of a previous activation layer.
    It subtracts the mean and divides by the standard deviation. Batch normalisation
    helps to increase the stability of a neural network and avoids over-fitting problems
    (Ioffe & Szegedy, [2015](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Fully connected layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'These layers are usually the last layers of the network and its input is the
    linearised outcome of the previous ones (in our network: convolutions, poolings
    and batch normalisations). It applies a linear transformation from the input to
    the output.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Forecasting the effect of errors on profile parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithms described in §[3](#S3 "3 Flux estimation methods ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") need information about the galaxy
    profile properties, making the flux measurement accuracy sensitive to errors on
    these parameters. Here we will quantify the effect that errors in the input galaxy
    parameters have on the flux measurements using a Fisher forecast formalism (Fisher,
    [1922](#bib.bib26)) on Teahupoo galaxies. In particular, we will test if neural
    networks are more robust to errors in the input parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: A photometry algorithm ($\Phi$) that measures the flux ($\tilde{f}$)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tilde{f}=\Phi(I,f,r_{\rm 50},n_{\rm s},PSF,e,b)$ |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: is foremost dependent on the galaxy image ($I$), but also parameters such as
    the total flux ($f$), the half-light radius ($r_{\rm 50}$), the Sérsic index ($n_{\rm
    s}$), the PSF FWHM, the ellipticity ($e$) and the background light ($b$). This
    is because e.g. the aperture algorithm uses these quantities to scale the pixels
    and they are used to construct a profile for the profile fit. We can estimate
    the error on the flux from these parameters using a Fisher matrix formalism. The
    Fisher matrix is estimated by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{{FM}}_{\rm\mu\nu}=\frac{\partial\tilde{f}}{\partial\mu}\left(\sigma_{\tilde{f}}^{-2}\right)\frac{\partial\tilde{f}}{\partial\nu}\,,$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: 'where the indices $\mu$ and $\nu$ are the different parameters the total flux
    depend on (see Eq. [28](#A3.E28 "In Appendix C Forecasting the effect of errors
    on profile parameters ‣ The PAU survey: Estimating galaxy photometry with deep
    learning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance matrix of the flux measurements can be estimated as the inverse
    of the Fisher matrix. Figure [19](#A3.F19 "Figure 19 ‣ Appendix C Forecasting
    the effect of errors on profile parameters ‣ The PAU survey: Estimating galaxy
    photometry with deep learning") shows the correlation matrices of the parameters
    $Flux,r_{\rm 50},n_{\rm s},PSF,e$ and $b$ for the four flux estimation methods
    described in §[3](#S3 "3 Flux estimation methods ‣ The PAU survey: Estimating
    galaxy photometry with deep learning"). The correlation matrix differs for different
    galaxy types (e.g. different morphologies or brightness). Here we have constructed
    a galaxy with values of $r_{50}$, $n_{\rm s}$ and $PSF$ corresponding to the mean
    of their distribution. The model-fitting method (top left panel), together with
    the optimal weighting (bottom left panel) are those showing more correlation between
    the flux and the profile parameters. In contrast, the forced aperture photometry
    and Lumos show a lower correlation. The lower correlation makes these methods
    more robust since the effect of uncertainties in the parameters is also lower.'
  prefs: []
  type: TYPE_NORMAL
- en: All methods but Lumos show a high correlation with the background estimation.
    The background light is not an input parameter for Lumos, since it is intrinsically
    measured inside the method. This makes Lumos insensitive to external errors on
    this parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f4bfce5a261128f19874bb11f957e87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The correlation matrix for the parameters $f,r_{\rm 50},n_{\rm s},PSF,e$
    and $bkg$ for: *Top left:* The model fitting method, *Top right:* Forced aperture
    photometry, *Bottom left:* Optimal weighted pixel sum and *Bottom right:* Lumos.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, the correlation matrices in Figure [19](#A3.F19 "Figure 19 ‣
    Appendix C Forecasting the effect of errors on profile parameters ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") are for a particular common
    galaxy. To estimate the requirements on these parameters, we have studied how
    errors in the profile parameters propagate to errors in the flux measurement.
    For that, we have assumed a 10% prior error in each of the input parameters, such
    that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{{FM}}_{\rm comb}=\textbf{{FM}}+\textbf{{FM}}_{\rm priors},$ |  |
    (30) |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{{FM}}_{\rm priors}$ is a diagonal matrix including the inverse
    prior variance of each parameter. The variance on the flux parameter is then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{{$\sigma^{2}_{\rm f}$}}={\left(\textbf{{FM}}^{\rm-1}_{\rm comb}\right)}_{\rm
    ff}$ |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: where the matrix subscripts (ff) denotes selecting the row/column corresponding
    to the flux parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#A3.T1 "Table 1 ‣ Appendix C Forecasting the effect of errors on
    profile parameters ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    shows the percentage of error in the parameters that propagates to a 10% error
    in the flux measurements. While studying a particular parameter, we always assume
    that the rest are fixed. As the sensitivity to the parameters can vary among galaxy
    types, the results in Table [1](#A3.T1 "Table 1 ‣ Appendix C Forecasting the effect
    of errors on profile parameters ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") are averaged out of one hundred independent random galaxies.
    Lumos is the method that requires higher errors on the parameters to propagate
    to a 10% flux error, i.e. it shows as the most robust method. As expected from
    Figure [19](#A3.F19 "Figure 19 ‣ Appendix C Forecasting the effect of errors on
    profile parameters ‣ The PAU survey: Estimating galaxy photometry with deep learning"),
    the PSF is the parameter it is more sensitive to, followed by the half light radius.
    However, in both cases it is still less sensitive than the other methodologies.'
  prefs: []
  type: TYPE_NORMAL
- en: The robustness of Lumos most likely comes from having the galaxy image. Lumos
    is provided with the galaxy and the modelled profile images, which allows the
    comparison and detection of problematic profiles instead of blindly relying on
    the input parameters. Furthermore, small errors in such parameters are more subtle
    and difficult to detect when these are encoded in a profile rather than directly
    inputted into the method.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r_{\rm 50}$ | $n_{\rm s}$ | PSF | e | b |'
  prefs: []
  type: TYPE_TB
- en: '| Model-fitting | 4 | 4 | 4 | 5 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Forced photometry | 21 | 59 | 19 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Opt. weighted sum | 8 | 11 | 4 | 13 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Lumos | 28 | 80 | 14 | 83 | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Percentage of error in $r_{\rm 50}$, $n_{\rm s}$, PSF, ellipticity
    ($e$) and background noise ($b$) that propagates to a 10% error in the total flux.
    Note that when studying one of the parameters, the rest remain fixed. Also note
    that high errors indicate that the method is more robust, since it requires a
    large error in the parameter to propagate to a 10% flux error.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Photometric redshifts with BCNz2 on PAUS galaxy mocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will run BCNz2 on the PAUS galaxy mock. The purpose of this
    test is to have an idea of the photo-z improvement that we should expect on data.
  prefs: []
  type: TYPE_NORMAL
- en: We have generated PAUS photometry with the same pipeline as the Flagship simulations
    (Castander et al (in prep.)) containing  500K objects over 25 $deg^{2}$ with a
    redshift limit of 2.25\. Initially, galaxies are generated with rest-frame luminosity
    using abundance matching between the halo mass function and SDSS galaxies. Next,
    the galaxy redshift is estimated using evolutionary population synthesis models.
    Then, mock galaxies are matched to the COSMOS galaxies from Ilbert et al. ([2008](#bib.bib35))
    and extinction and an SED are assigned to each of them. The SED templates also
    take into account the emission lines. $H_{\rm\alpha}$ is computed form the ultra-violet
    following Kennicutt ([1998](#bib.bib41)). The other line fluxes are computed following
    observed relations. Finally, the SED is convolved with the filter transmission
    curves to produce the fluxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we have used PAUS narrow bands, the CFHT $u$-band and the Subaru
    $BVriz$ broad bands and the noise is included assuming Gaussian uncertainties.
    We have randomly selected 10,000 noiseless galaxies and included uncertainties
    to mimic the SNR provided by Lumos (Fig. [20](#A4.F20 "Figure 20 ‣ Appendix D
    Photometric redshifts with BCNz2 on PAUS galaxy mocks ‣ The PAU survey: Estimating
    galaxy photometry with deep learning")). For the broad bands, we have generated
    errors to simulate the SNR in Eriksen et al. ([2019](#bib.bib24)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [21](#A4.F21 "Figure 21 ‣ Appendix D Photometric redshifts with BCNz2
    on PAUS galaxy mocks ‣ The PAU survey: Estimating galaxy photometry with deep
    learning") shows that the photo-z on the PAUS-mock (black solid line) has on average
    90% less scatter than on data (blue solid line). This is a large number considering
    that the SNR on both samples is similar and consequently, a similar performance
    is expected. Therefore, the photo-zs appear limited by other factors than the
    photometry SNR.'
  prefs: []
  type: TYPE_NORMAL
- en: The purple solid line shows the effect of the photometric calibration. For each
    flux measurements, we have generated five individual observations resembling the
    number of PAUS observations in the COSMOS field from a Gaussian centered at the
    co-added flux. Each of these observations is randomly assigned with a photometric
    zero point and its uncertainty from the distribution of zero-point in the PAUS
    COSMOS data. The zero points are scattered with their uncertainty, sampling for
    every galaxy a new zero point from a Gaussian. The individual observations are
    combined back to a single flux error using the scattered zero points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The median zero point uncertainty in the PAUS data in COSMOS is $\approx$ 4%.
    Including this effect in the photo-zs (purple solid line in Fig. [21](#A4.F21
    "Figure 21 ‣ Appendix D Photometric redshifts with BCNz2 on PAUS galaxy mocks
    ‣ The PAU survey: Estimating galaxy photometry with deep learning")) degrades
    the precision by 40% with respect to perfect photometry (black solid line), from
    $\sigma_{\rm 68}$ = 0.0026 to $\sigma_{\rm 68}$ = 0.0041, with faint galaxies
    being more affected. Nevertheless, after including the calibration effect, the
    photo-z scatter is still significantly lower on the PAUS mock compared to the
    results on data. The coloured dotted lines in Figure [21](#A4.F21 "Figure 21 ‣
    Appendix D Photometric redshifts with BCNz2 on PAUS galaxy mocks ‣ The PAU survey:
    Estimating galaxy photometry with deep learning") shows the photo-z dispersion
    with further effects in the photometry that could potentially lead to a photo-z
    degradation. All the lines are built on the purple one, assuming the calibration
    effect.'
  prefs: []
  type: TYPE_NORMAL
- en: The green dotted line studies the effect of having a addition 20% error in 20%
    of the photometric zero points. Note that this additional error is not accounted
    for in the final photometric error and could potentially make a correct observation
    an outlier.This further reduced the photo-z precision to $\sigma_{\rm 68}$ = 0.0050,
    significantly affecting bright galaxies. Indeed, this effect is required to understand
    the photo-z degradation at the bright end on data with respect to simulations.
  prefs: []
  type: TYPE_NORMAL
- en: In the orange dotted line, we have artificially injected 1.5% of outliers in
    the PAUS mock fluxes. These outliers are directly affecting the co-added flux
    measurement and therefore, a 1.5% of affected fluxes corresponds to a higher percentage
    of affected galaxies. Particularly, 45% of the galaxies in the PAUS mock have
    at least one affected NB flux measurement and $\approx 10\%$ have more than one.
    Nevertheless, mind that not all the galaxies with affected photometries end up
    providing worse redshift estimates. Indeed, these outliers barely affect the bright
    end, where galaxies have high SNR and the photo-z algorithm deals well with an
    outlier in one of the bands. Contrary, at the faint end outliers increase the
    photo-z scatter by $\approx$2\. We have also tested the effect of other percentages
    of outliers finding that 1% was too few to explain the degradation on data and
    2%, too much. With 1.5% of outliers, the photo-z precision degrades by 80%, providing
    a $\sigma_{\rm 68}$ = 0.0074\. The dashed red line combines the two previous effects
    and gives a photo-z precision close to that obtained on data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/634f01ee1e1dbd51dc1b6c0aa266ea31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The SNR of the PAUS galaxy simulations used to run BCNz2. The solid
    red line corresponds to a PAUS mock with a SNR similar to that provided by Lumos.
    As a comparison, the dashed black line corresponds to the observed SNR on PAUS
    data with Lumos photometry.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45dc47befa7b4f83352134d153a91a4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Photo-z dispersion as a function of $i$-band magnitude using BCNz2
    for a galaxy mock with Lumos SNR (black solid line) and PAUS data with Lumos photometry
    (blue solid line). The purple line includes the photometric calibration on the
    PAUS mocks. Dotted lines include outliers and calibration errors in the PAUS mock
    photometry. The red dashed line combines the effect of the two dotted lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Colour histograms in the complete narrow band set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Colour histograms can be used to compare different photometries of the same
    images. Assuming an underlying galactic colour distribution, photometry uncertainties
    broaden such distribution and therefore the width of the colour histogram provides
    an idea of the photometry uncertainties. Consequently, the best photometry on
    a sample of galaxies is that providing narrower colour distributions. In Figure
    [11](#S6.F11 "Figure 11 ‣ 6.1.2 Colour histograms ‣ 6.1 Single exposure measurements
    ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") (§[6.1](#S6.SS1 "6.1 Single exposure measurements ‣ 6 Lumos
    photometry on PAUS data ‣ The PAU survey: Estimating galaxy photometry with deep
    learning")), we showed the NB785-NB795 histogram, which displayed a narrower distribution
    for Lumos than MEMBA. Here, we show the colour histograms results for the rest
    of the bands. Figure [22](#A5.F22 "Figure 22 ‣ Appendix E Colour histograms in
    the complete narrow band set ‣ The PAU survey: Estimating galaxy photometry with
    deep learning") shows the colour histogram of nine different narrow bands with
    the Lumos photometry (black solid line) and the MEMBA photometry (red dashed line).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, Figure [23](#A5.F23 "Figure 23 ‣ Appendix E Colour histograms
    in the complete narrow band set ‣ The PAU survey: Estimating galaxy photometry
    with deep learning") shows the relative difference in the effective width of the
    colour histograms with the photometries from Lumos and MEMBA. As in Section [6.1](#S6.SS1
    "6.1 Single exposure measurements ‣ 6 Lumos photometry on PAUS data ‣ The PAU
    survey: Estimating galaxy photometry with deep learning"), the effective widths
    are estimated with $\sigma_{\rm 68}$ (Eq. [14](#S5.E14 "In 5.1.2 Single flux and
    flux uncertainty measurements ‣ 5.1 Flux probability distributions ‣ 5 Lumos flux
    measurements on simulations ‣ The PAU survey: Estimating galaxy photometry with
    deep learning")) and $\sigma_{\rm 95}$, which is equivalent to $\sigma_{\rm 68}$
    but considering the 2.5 and 97.5 quantiles. Lumos provides narrower colour histograms
    in all the NB filters. The relative difference in $\sigma_{\rm 68}$ oscillates
    between 30% and 40% in all NBs but the bluest, where it is $\approx$15%. With
    $\sigma_{\rm 95}$, the relative effective width is lower at the first eight bluer
    bands, $\approx$30%, and increases to $\approx$70% for the rest of the bands.
    The relative difference in $\sigma_{\rm 95}$ is systematically larger than with
    $\sigma_{\rm 68}$, which is likely related with very noisy measurements. Photometry
    measurements in the tails of the colour histograms will not affect the $\sigma_{\rm
    68}$ measurement, however these will be accounted in $\sigma_{\rm 95}$. Consequently,
    Figure [23](#A5.F23 "Figure 23 ‣ Appendix E Colour histograms in the complete
    narrow band set ‣ The PAU survey: Estimating galaxy photometry with deep learning")
    would be showing that the number of outlier observations is lower in Lumos than
    in MEMBA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4295428047dcc365a6c4d0342a9a6fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Colour histograms for nine different narrow band filters using Lumos
    (solid black line) or MEMBA (dashed red line) photometries.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53e6e5a06320dd267b2d798d5c5ae98f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Relative difference between the effective width of the colour histograms
    of the PAUS photometry with Lumos and MEMBA. The effective width has been estimated
    with $\sigma_{\rm 68}$ (blue line) and $\sigma_{\rm 95}$ (red line).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Photometry and photo-z correlations with galaxy parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3fc0f800672c461b72d52ada3b3331d4.png)![Refer to caption](img/88c22b6378aea7d3a96ebe5158660d17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: *Left: Bias in the photometry measurements as a function of the
    galaxy size ($r_{\rm 50}$*), galaxy shape (Sérsic index), ellipticity (b/a) and
    PSF. *Right*: Precision in the photometry as a function of the same parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ffa5a20d33f51dcd8c1dc05d8f727b88.png)![Refer to caption](img/e80c459d6ec24e687b125e81731b7dce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: *Left*: Bias in the photo-z measurements as a function of the galaxy
    size ($r_{\rm 50}$'
  prefs: []
  type: TYPE_NORMAL
- en: '), galaxy shape (Sérsic index) and the spectroscopic redshift ($z_{\rm z}$).
    *Right*: Precision in the photo-z as a function of the same parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Comparison with different flux estimation
    methods ‣ 5 Lumos flux measurements on simulations ‣ The PAU survey: Estimating
    galaxy photometry with deep learning") only showed the bias and the precision
    of the photometry obtained with Lumos (blue solid line) as a function of magnitude
    on simulations. In this appendix, we are extending the exploration of the photometry
    (§[F.1](#A6.SS1 "F.1 Lumos photometry correlation with galaxy parameters ‣ Appendix
    F Photometry and photo-z correlations with galaxy parameters ‣ The PAU survey:
    Estimating galaxy photometry with deep learning")) and the photo-z (§[F.2](#A6.SS2
    "F.2 Photo-z correlation with galaxy parameters ‣ Appendix F Photometry and photo-z
    correlations with galaxy parameters ‣ The PAU survey: Estimating galaxy photometry
    with deep learning")) predictions as a function of other galaxy parameters as
    e.g. the galaxy size or ellipticity.'
  prefs: []
  type: TYPE_NORMAL
- en: F.1 Lumos photometry correlation with galaxy parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [24](#A6.F24 "Figure 24 ‣ Appendix F Photometry and photo-z correlations
    with galaxy parameters ‣ The PAU survey: Estimating galaxy photometry with deep
    learning") shows the bias and the precision of the Lumos photometry as a function
    of the galaxy size ($r_{\rm 50}$), the galaxy shape (Sérsic index, n), the galaxy
    ellipticity (b/a) and the PSF, binned in ten equally populated bins. The photometry
    does not show a significant bias with any of the galaxy parameters. Note that
    the galaxy bias increment for higher PSF values is expected since this parameter
    directly correlates with the quality of the data. The largest galaxies in the
    dataset tend to have slightly underestimated flux predictions ($\approx$ 1-2%).
    This could be a consequence of the fixed stamp size, which could lead to a small
    leak of light. Nevertheless, this should have a weaker effect on Lumos than on
    other non-trainable algorithms, since the network can learn to predict reasonable
    fluxes when the galaxy is partially outside the stamp.'
  prefs: []
  type: TYPE_NORMAL
- en: The photometry precision is better for larger galaxies. Furthermore, the precision
    is higher for larger Sérsic indices, which is expected since larger Sérsic indices
    correspond to bigger and brighter galaxies. In contrast, we do not see any correlation
    between the photometry precision and the galaxy ellipticity.
  prefs: []
  type: TYPE_NORMAL
- en: F.2 Photo-z correlation with galaxy parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [25](#A6.F25 "Figure 25 ‣ Appendix F Photometry and photo-z correlations
    with galaxy parameters ‣ The PAU survey: Estimating galaxy photometry with deep
    learning") explores the photo-z performance with the BCNz2 template fitting and
    the Deepz machine learning codes as a function of the galaxy size ($r_{\rm 50}$),
    the galaxy shape (Sérsic index) and the spectroscopic redshift. This is presented
    for both the Lumos and the MEMBA photometries. In this case, these quantities
    are binned in 10 equally spaced bins, so that that we can explore the photo-z
    performance on the edges of the training set distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the photo-z bias (first and second columns) is not affected neither
    by the size nor the shape of the galaxy with any of the codes or photometries.
    The photo-zs are also unbiased as a function of spectroscopic redshift, only presenting
    a $\approx$1% bias at high redshifts with the MEMBA photometry and the Deepz code
    (blue solid line). Using Deepz on the Lumos photometry also presents a $\approx$0.5%
    bias, while such biases disappear with the BCNz2 algorithm on both photometries.
    This suggests that it might be triggered by the photo-z method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The photo-z precision (third and forth rows) shows a similar trend with the
    galaxy size and shape using the MEMBA or Lumos photometries. Note that Lumos provides
    better photo-z precision for small galaxies, while MEMBA gives better photo-zs
    for larger galaxies. This is potentially related to discrepancies between the
    training image simulations and the data (see §[6.4](#S6.SS4 "6.4 Photometric redshift
    estimates ‣ 6 Lumos photometry on PAUS data ‣ The PAU survey: Estimating galaxy
    photometry with deep learning")). Such differences affect more large bright galaxies,
    as these are more resolved. A similar effect can be noted as a function of Sérsic
    index.'
  prefs: []
  type: TYPE_NORMAL
- en: The photo-z precision with spectroscopic redshift presents a similar trend for
    both photometries, exhibiting better photo-zs for $z_{\rm s}$>1 with both the
    BCNz2 and Deepz codes on the Lumos photometry. At high redshfits, the improvement
    with the Lumos photometry and the Deepz code is remarkable.
  prefs: []
  type: TYPE_NORMAL
