- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:31:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:31:30'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2407.08137] Abstract'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2407.08137] 摘要'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08137](https://ar5iv.labs.arxiv.org/html/2407.08137)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08137](https://ar5iv.labs.arxiv.org/html/2407.08137)
- en: Survey on Fundamental Deep Learning 3D Reconstruction Techniques Yonge Bai¹,
    LikHang Wong², TszYin Twan²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基础深度学习 3D 重建技术的调研 由 Yonge Bai¹、LikHang Wong²、TszYin Twan²
- en: ¹McMaster University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹麦克马斯特大学
- en: ²City University of Hong Kong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²香港城市大学
- en: baiy58@mcmaster.ca, klhwong3-c@my.cityu.edu.hk, tytwan2-c@my.cityu.edu.hk July
    10, 2024
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: baiy58@mcmaster.ca, klhwong3-c@my.cityu.edu.hk, tytwan2-c@my.cityu.edu.hk 2024年7月10日
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This survey aims to investigate fundamental deep learning (DL) based 3D reconstruction
    techniques that produce photo-realistic 3D models and scenes, highlighting Neural
    Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and 3D Gaussian Splatting.
    We dissect the underlying algorithms, evaluate their strengths and tradeoffs,
    and project future research trajectories in this rapidly evolving field. We provide
    a comprehensive overview of the fundamental in DL-driven 3D scene reconstruction,
    offering insights into their potential applications and limitations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本调研旨在探讨生成照片级真实感 3D 模型和场景的基础深度学习（DL）3D 重建技术，重点介绍神经辐射场（NeRFs）、潜在扩散模型（LDM）和 3D
    高斯溅射。我们剖析了其基础算法，评估其优缺点，并预测这一快速发展的领域的未来研究方向。我们提供了关于 DL 驱动的 3D 场景重建的全面概述，深入了解其潜在应用和局限性。
- en: Background
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: 3D reconstruction is a process aimed at creating volumetric surfaces from image
    and/or video data. This area of research has gained immense traction in recent
    months and finds applications in numerous domains, including virtual reality,
    augmented reality, autonomous driving, and robotics. Deep learning has emerged
    to the forefront of 3D reconstruction techniques and has demonstrated impressive
    results enhancing realism and accuracy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 重建是一个旨在从图像和/或视频数据中创建体积表面的过程。这一研究领域在最近几个月获得了极大的关注，并在多个领域中找到了应用，包括虚拟现实、增强现实、自动驾驶和机器人技术。深度学习已成为
    3D 重建技术的前沿，并展示了增强现实感和准确性的令人印象深刻的成果。
- en: Neural Radiance Fields
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经辐射场
- en: Neural Radiance Field (NeRF) is a method for novel view synthesis of complex
    scenes using a set of input perspectives and optimizes a model to approximate
    a continuous volumetric scene or surface[[9](#bib.bib9)]. The method represents
    the volume using a multilayer preceptron (MLP) whose input is a 5D vector $(x,y,z,\theta,\phi)$.
    $(x,y,z)$ representing the spatial location and $(\theta,\phi)$ representing the
    viewing direction, with an output of a 4D vector ($R,G,B,\sigma)$ representing
    the RGB color and a volume density. NeRFs achieved SOTA results on quantitative
    benchmarks as well as qualitative tests on neural rendering and view synthesis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经辐射场（NeRF）是一种用于复杂场景新视角合成的方法，它利用一组输入视角并优化模型以近似一个连续的体积场景或表面[[9](#bib.bib9)]。该方法使用多层感知器（MLP）表示体积，其输入是一个
    5D 向量 $(x,y,z,\theta,\phi)$。其中 $(x,y,z)$ 表示空间位置，$(\theta,\phi)$ 表示观察方向，输出是一个 4D
    向量 ($R,G,B,\sigma$)，表示 RGB 颜色和体积密度。NeRFs 在定量基准测试以及神经渲染和视图合成的定性测试中达到了 SOTA 结果。
- en: Prior Work
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 之前的工作
- en: NeRFs build upon prior work in RGB-alpha volume rendering for view-synthesis
    and the use of neural networks (NN) as implicit continuous shape representations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs 基于 RGB-alpha 体积渲染和神经网络（NN）作为隐式连续形状表示的先前工作。
- en: Volume Rendering for View-Synthesis
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用于视图合成的体积渲染
- en: This process involves, using a set of images to learn a 3D discrete volume representation,
    the model estimate the volume density and emitted color at each point in the 3D
    space, which is then used to synthesize images from various viewpoints. Prior
    methods include Soft 3D, which implements a soft 3D representation of the scene
    by using traditional stereo methods, this representation is used directly to model
    ray visibility and occlusion during view-synthesis [[12](#bib.bib12)]. Along with
    deep learning methods such as Neural Volumes which uses a an encoder-decoder network
    that transforms the input images into a 3D voxel grid, used to generate new views
    [[6](#bib.bib6)]. While these volumetric representation are easy to optimize by
    being trained on how well they render the ground truth views, but as the resolution
    or complexity of the scene increases the compute and memory needed to store these
    discretized representations become unpractical.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程涉及使用一组图像来学习 3D 离散体积表示，模型估计 3D 空间中每个点的体积密度和发射的颜色，然后用这些信息从不同的视角合成图像。之前的方法包括
    Soft 3D，它通过使用传统的立体视觉方法实现了场景的软 3D 表示，这种表示直接用于建模光线可见性和遮挡 [[12](#bib.bib12)]。还有深度学习方法，如
    Neural Volumes，它使用编码器-解码器网络将输入图像转换为 3D 体素网格，用于生成新视角 [[6](#bib.bib6)]。尽管这些体积表示通过训练来优化其对地面真实视图的渲染效果较为容易，但随着场景分辨率或复杂度的增加，存储这些离散化表示所需的计算和内存变得不切实际。
- en: Neural Networks as Shape Representations
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络作为形状表示
- en: This field of study aims to implicitly represent the 3D surface with a NN’s
    weights. In contrast to the volumetric approach this representation encodes a
    description of a 3D surface at infinite resolution without excessive memory footprint
    as described here[[8](#bib.bib8)]. The NN encodes the 3D surface by learning to
    map a point in space to a property of that point in the 3D space, for example
    occupancy [[8](#bib.bib8)] or signed distance fields [[11](#bib.bib11)]. While
    this approach saves significant memory it is harder to optimize, leading to poor
    synthetic views compared to the discrete representations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个研究领域旨在用神经网络的权重隐式表示 3D 表面。与体积方法相比，这种表示以无限分辨率编码 3D 表面的描述，而不会产生过多的内存占用，如此处所述
    [[8](#bib.bib8)]。神经网络通过学习将空间中的一个点映射到该点在 3D 空间中的一个属性（例如占用 [[8](#bib.bib8)] 或带符号距离场
    [[11](#bib.bib11)]）来编码 3D 表面。尽管这种方法节省了大量内存，但其优化难度较大，导致与离散表示相比生成的视图效果较差。
- en: 'Approach: NeRF'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：NeRF
- en: NeRFs combine these two approaches by representing the scene in the weights
    of an MLP but view synthesis is trained using the techniques in traditional volume
    rendering.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs 通过在 MLP 的权重中表示场景来结合这两种方法，但视图合成使用传统体积渲染中的技术进行训练。
- en: '![Refer to caption](img/0b471327a5afe49d6e657cab24050730.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b471327a5afe49d6e657cab24050730.png)'
- en: 'Figure 1: An overview of the neural radiance field scene representation and
    differentiable rendering procedure. Synthesize images by sampling 5D coordinates
    (location and viewing direction) along camera rays (a), feeding those locations
    into an MLP to predict a color and volume density (b), and using volume rendering
    techniques to composite these values into an image (c). This rendering function
    is differentiable, so we can optimize our scene representation by minimizing the
    residual between synthesized color and ground truth of the actual color(d).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：神经辐射场场景表示和可微分渲染过程的概述。通过在相机光线（a）上采样 5D 坐标（位置和视角），将这些位置输入到 MLP 中以预测颜色和体积密度（b），并使用体积渲染技术将这些值合成到图像中（c）。这个渲染函数是可微分的，因此我们可以通过最小化合成颜色与实际颜色之间的残差来优化我们的场景表示（d）。
- en: Neural Radiance Field Scene Representation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经辐射场场景表示
- en: The scene is represented by 5D vector comprised of $\mathbf{x}=(x,y,z)$ and
    $\mathbf{d}=(\theta,\phi)$. This continuous 5D scene representation is approximated
    by a MLP network $F_{\Theta}:(\mathbf{x},\mathbf{d})\to(\mathbf{c},\sigma)$, whose
    weights $\Theta$ are optimized to predict each 5D input’s $\mathbf{c}=(R,G,B)$
    representing RGB color and $\sigma$ representing density. Density can be thought
    of as occlusion, points with a high occlusion having a higher $\sigma$ value than
    points with lower occlusion.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 场景由 5D 向量表示，包括 $\mathbf{x}=(x,y,z)$ 和 $\mathbf{d}=(\theta,\phi)$。这个连续的 5D 场景表示由一个
    MLP 网络 $F_{\Theta}:(\mathbf{x},\mathbf{d})\to(\mathbf{c},\sigma)$ 近似，其权重 $\Theta$
    被优化以预测每个 5D 输入的 $\mathbf{c}=(R,G,B)$ 代表 RGB 颜色，和 $\sigma$ 代表密度。密度可以被视为遮挡，高遮挡的点具有比低遮挡点更高的
    $\sigma$ 值。
- en: 'The implicit representation is held consist by forcing the network to predict
    $\sigma$ only as a function of $\mathbf{x}$, as density should not change as a
    result of viewing angle. While $\mathbf{c}$ is trained as a function of both $\mathbf{x}$
    and $\mathbf{d}$. The MLP $F_{\Theta}$ has 9 fully-connected layers using ReLU
    activation functions and 256 channels per layer for the first 8 layers and 128
    channels for the last layer. $F_{\Theta}$ first processes $\mathbf{x}$ with the
    first 8 layers outputting $\sigma$ and a 256-dimensional feature vector $\mathbf{v}$.
    $\mathbf{v}$ is then concatenated with $\mathbf{d}$ and passed into the final
    layer that outputs $\mathbf{c}$. This process is shown in Figure [2](#Sx4.F2 "Figure
    2 ‣ Neural Radiance Field Scene Representation ‣ Approach: NeRF").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '隐式表示通过强制网络仅将$\sigma$作为$\mathbf{x}$的函数来保持一致，因为密度不应因视角变化而改变。而$\mathbf{c}$则作为$\mathbf{x}$和$\mathbf{d}$的函数进行训练。MLP
    $F_{\Theta}$具有9个全连接层，前8层每层使用ReLU激活函数和256个通道，最后一层使用128个通道。$F_{\Theta}$首先用前8层处理$\mathbf{x}$，输出$\sigma$和一个256维的特征向量$\mathbf{v}$。然后将$\mathbf{v}$与$\mathbf{d}$连接，并传递到最终层，输出$\mathbf{c}$。这一过程如图[2](#Sx4.F2
    "Figure 2 ‣ Neural Radiance Field Scene Representation ‣ Approach: NeRF")所示。'
- en: '![Refer to caption](img/53ff97fc2463572e853aa4342a577918.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53ff97fc2463572e853aa4342a577918.png)'
- en: 'Figure 2: An overview of the NeRF model. $\mathbf{x}$ is passed into the first
    8 layers, which output $\mathbf{v}$ and $\sigma$ a). $\mathbf{v}$ is concatenated
    with $\mathbf{d}$ and passed into the last layer, which outputs $\mathbf{c}$ b).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：NeRF模型概述。$\mathbf{x}$输入前8层，输出$\mathbf{v}$和$\sigma$ a)。$\mathbf{v}$与$\mathbf{d}$连接，传递到最后一层，输出$\mathbf{c}$
    b)。
- en: Volume Rendering with Radiance Fields
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于辐射场的体积渲染
- en: The color of any ray passing through the scene is rendered using principles
    from classical volume rendering.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 任何穿过场景的光线颜色都使用经典体积渲染的原理进行渲染。
- en: '|  | $\hat{C}(\mathbf{r})=\sum_{i=1}^{N}w_{i}c_{i},\text{ where }w_{i}=T_{i}\alpha_{i}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{C}(\mathbf{r})=\sum_{i=1}^{N}w_{i}c_{i},\text{ where }w_{i}=T_{i}\alpha_{i}$
    |  | (1) |'
- en: 'Equation ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields ‣ Approach:
    NeRF")) can be explained as the color $c_{i}$ of each point being weighted by
    $w_{i}$. $w_{i}$ is made up of $T_{i}=\exp(-\sum_{j=1}^{i-1}\sigma_{i}\delta_{i})$
    where $\sigma_{i}$ is the density and $\delta_{i}$ is the distance between adjacently
    sampled points. $T_{i}$ denotes the accumulated transmittance until point $i$
    which can be thought of as the amount of light blocked earlier along the ray,
    and $\alpha_{i}=1-\exp(-\sigma_{i}\delta_{i})$ denoting the opacity at point $i$.
    Thus the color predicted at point with higher transmittance and opacity (the beginning
    of surfaces) contribute more to final predicted color of ray $\mathbf{r}$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '方程 ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields ‣ Approach: NeRF"))
    可以解释为每个点的颜色$c_{i}$按$w_{i}$加权。$w_{i}$由$T_{i}=\exp(-\sum_{j=1}^{i-1}\sigma_{i}\delta_{i})$组成，其中$\sigma_{i}$是密度，$\delta_{i}$是相邻采样点之间的距离。$T_{i}$表示点$i$的累计透射率，可以认为是光在光线中早期被阻挡的量，而$\alpha_{i}=1-\exp(-\sigma_{i}\delta_{i})$表示点$i$的透明度。因此，具有更高透射率和透明度（即表面起始点）的点对光线$\mathbf{r}$的最终预测颜色贡献更多。'
- en: Optimizing a NeRF
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化NeRF
- en: The previous sections covered the core components to NeRFs but the original
    paper had two more techniques to achieve SOTA quality—positional encoding and
    hierarchical volume sampling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的部分涵盖了NeRF的核心组件，但原始论文还有两种技术实现SOTA质量——位置编码和分层体积采样。
- en: Positional Encoding
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'The authors found that directly feeding in $(x,y,z,\theta,\phi)$ to $F_{\Theta}$
    resulted in poor performance. As a result, they chose to map the inputs to a higher
    dimensional space using high frequency functions, this enabled the model to better
    fit data with high variations. Thus $F_{\Theta}$ is reformulated as a composition
    of two functions $F_{\Theta}=F_{\Theta}^{\prime}\circ\gamma$. $F_{\Theta}^{\prime}$
    being the original MLP and $\gamma$ defined as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发现直接将$(x,y,z,\theta,\phi)$输入$F_{\Theta}$会导致性能较差。因此，他们选择将输入映射到更高维空间，使用高频函数，这使得模型能够更好地拟合具有高变化的数据。因此，$F_{\Theta}$被重新表述为两个函数的组合$F_{\Theta}=F_{\Theta}^{\prime}\circ\gamma$。$F_{\Theta}^{\prime}$是原始MLP，$\gamma$定义为：
- en: '|  | <math   alttext="\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi x),&amp;\cos(2^{0}\pi
    x)\\ \vdots&amp;\vdots\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi x),&amp;\cos(2^{0}\pi
    x)\\ \vdots&amp;\vdots\\'
- en: \sin(2^{L-1}\pi x),&amp;\cos(2^{L-1}\pi x)\end{array}\right)" display="block"><semantics
    ><mrow  ><mrow ><mi  >γ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi  >x</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow
    ><mo  >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mrow ><mrow ><mi  >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><msup ><mn >2</mn><mn >0</mn></msup><mo lspace="0em" rspace="0em"  >​</mo><mi
    >π</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >x</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd ><mrow ><mi  >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow  ><msup ><mn >2</mn><mn >0</mn></msup><mo lspace="0em" rspace="0em"
    >​</mo><mi >π</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >x</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd  ><mi mathvariant="normal"  >⋮</mi></mtd><mtd
    ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr  ><mtd ><mrow ><mrow  ><mi >sin</mi><mo
    >⁡</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mn >2</mn><mrow ><mi
    >L</mi><mo >−</mo><mn >1</mn></mrow></msup><mo lspace="0em" rspace="0em"  >​</mo><mi
    >π</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >x</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd ><mrow ><mi  >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow  ><msup ><mn >2</mn><mrow ><mi >L</mi><mo >−</mo><mn >1</mn></mrow></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mi >π</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >x</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><ci >𝛾</ci><ci  >𝑥</ci></apply><matrix ><matrixrow ><apply  ><apply ><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn><cn type="integer"  >0</cn></apply><ci
    >𝜋</ci><ci >𝑥</ci></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><cn type="integer" >2</cn><cn type="integer" >0</cn></apply><ci
    >𝜋</ci><ci >𝑥</ci></apply></apply></matrixrow><matrixrow ><ci >⋮</ci><ci  >⋮</ci></matrixrow><matrixrow
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"
    >2</cn><apply ><ci >𝐿</ci><cn type="integer"  >1</cn></apply></apply><ci >𝜋</ci><ci
    >𝑥</ci></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn
    type="integer" >2</cn><apply ><ci >𝐿</ci><cn type="integer" >1</cn></apply></apply><ci
    >𝜋</ci><ci >𝑥</ci></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi
    x),&\cos(2^{0}\pi x)\\ \vdots&\vdots\\ \sin(2^{L-1}\pi x),&\cos(2^{L-1}\pi x)\end{array}\right)</annotation></semantics></math>
    |  | (2) |
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma(x)=\left(\begin{array}{cc}\sin(2^{0}\pi x),&\cos(2^{0}\pi x)\\ \vdots&\vdots\\
    \sin(2^{L-1}\pi x),&\cos(2^{L-1}\pi x)\end{array}\right)$
- en: $\gamma(\cdot)$ is applied to $(x,y,z)$ in $\mathbf{x}$ with $L=10$ and $(\theta,\phi)$
    with $L=4$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma(\cdot)$ 被应用于 $\mathbf{x}$ 中的 $(x,y,z)$，其中 $L=10$，以及 $(\theta,\phi)$
    中的 $L=4$。
- en: '$\gamma$ is a mapping from $\mathbb{R}$ to $\mathbb{R}^{2L}$ that significantly
    improves performance (Figure [3](#Sx4.F3 "Figure 3 ‣ Positional Encoding ‣ Optimizing
    a NeRF ‣ Approach: NeRF")).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma$ 是从 $\mathbb{R}$ 到 $\mathbb{R}^{2L}$ 的映射，显著提高了性能（图 [3](#Sx4.F3 "图3 ‣
    位置编码 ‣ 优化NeRF ‣ 方法：NeRF")）。
- en: '![Refer to caption](img/86cbd6695e1b64342591a54b9e1106a9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/86cbd6695e1b64342591a54b9e1106a9.png)'
- en: 'Figure 3: Visualizing how the model improves the positional encoding. Without
    it the model is unable to represent high variation geometries and textures resulting
    in an over smoothed, blurred appearance. Also how removing view dependency affect
    the models ability to render lighting and reflections.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：可视化模型如何改善位置编码。如果没有它，模型将无法表示高变化的几何形状和纹理，导致过度平滑、模糊的外观。同时，去除视图依赖性如何影响模型渲染光照和反射的能力。
- en: Hierarchical Volume Sampling
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分层体积采样
- en: 'Free space and occluded region contribute much less to the quality of the NeRF
    compared to areas at the beginning of a surface, but with uniform sampling, are
    sampled at the same rate. So the authors proposed a hierarchical representation
    that increases rendering efficiency and quality by allocating samples proportional
    to their expected effect shown in [4](#Sx4.F4 "Figure 4 ‣ Hierarchical Volume
    Sampling ‣ Optimizing a NeRF ‣ Approach: NeRF"). For example, if the object in
    question was a ball, there would be less samples taken in the open space in front
    of the ball and inside of the ball verses samples directly on the ball’s surface.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自由空间和遮挡区域对NeRF的质量贡献远小于表面起始区域，但在均匀采样下，它们的采样率是相同的。因此，作者提出了一种分层表示，通过将样本分配与其预期效果成比例，从而提高渲染效率和质量，如[4](#Sx4.F4
    "图4 ‣ 分层体积采样 ‣ 优化NeRF ‣ 方法：NeRF")所示。例如，如果对象是一个球，则在球前的开放空间和球内部的样本会比直接在球表面上的样本少。
- en: '![Refer to caption](img/7f621d39468278022b3ae5b5656b63a9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7f621d39468278022b3ae5b5656b63a9.png)'
- en: 'Figure 4: Illustrating hierarchical sampling, where samples are proportional
    to their contribution to the final volume render.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：说明分层采样，其中样本与其对最终体积渲染的贡献成正比。
- en: 'This is done by optimizing two networks. One ”coarse” and one ”fine”. The course
    network samples points uniformly along the ray, while the fine network is biases
    toward the relevant part of the volume by normalizing the per sample weights $w_{i}$
    described in equation ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields ‣
    Approach: NeRF")), this allows one to treat the weight of each point as a probability
    distribution which is sampled to train the fine network. This procedure allocates
    more samples to regions expected to contain visible content.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过优化两个网络来实现的。一个是“粗略”网络，一个是“精细”网络。粗略网络沿光线均匀采样点，而精细网络通过标准化每个样本的权重$w_{i}$（如方程([1](#Sx4.E1
    "在体积渲染与辐射场中 ‣ 方法：NeRF")）所述）倾向于体积的相关部分，这允许将每个点的权重视为概率分布，并进行采样以训练精细网络。该过程将更多样本分配到预计包含可见内容的区域。
- en: Limitations
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局限性
- en: 'While having groundbreaking abilities to render photorealistic 3D volumes from
    2D images, the original NeRF methodology suffered from several limitations. These
    limitations include:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有从2D图像渲染照片级真实3D体积的突破性能力，原始的NeRF方法仍存在若干局限性。这些局限性包括：
- en: Computational Efficiency
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算效率
- en: The optimization of a single scene took 100-300k iterations to converge of a
    single NVIDIA V100 GPU which corresponded to 1-2 days [[9](#bib.bib9)]. This poor
    computational efficiency is a product of dense sampling of rays for rendering.
    This dense sampling approach helped in capturing fine details and accurately representing
    complex scenes, but it significantly increases the computational load.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 单个场景的优化需要100-300k次迭代才能在单个NVIDIA V100 GPU上收敛，这大约对应1-2天[[9](#bib.bib9)]。这种低效的计算性能是由于渲染时对光线的密集采样造成的。这种密集采样方法有助于捕捉细节并准确表示复杂场景，但显著增加了计算负担。
- en: Lack of Generalizability
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 泛化能力不足
- en: NeRFs are inheritably inflexible due to the as models overfit to one scene.
    A NeRF cannot be adapted for novel scenes without complete retraining.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型对单一场景过拟合，NeRFs天生缺乏灵活性。NeRF无法在没有完全重新训练的情况下适应新场景。
- en: Difficulty of Editing
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编辑难度
- en: Modifying content in NeRFs such as moving or removing object is very difficult.
    Since the model represents the scene as a continuous function and does not store
    geometric information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在NeRFs中修改内容，如移动或删除对象，非常困难。因为模型将场景表示为连续函数，而不存储几何信息。
- en: Data requirements
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据要求
- en: NeRFs require a lot of data to produce high quality results show in the original
    paper. The synthetic 3D models as lego bulldozer and pirate ship took about 100
    image and the real life scenes such as the flower and conference room each requiring
    around 60 [[9](#bib.bib9)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs需要大量数据来产生高质量的结果，如原始论文所示。合成3D模型如乐高推土机和海盗船大约需要100张图像，而现实生活场景如花朵和会议室每个大约需要60张[[9](#bib.bib9)]。
- en: Transient Artifacts
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 瞬态伪影
- en: 'The original NeRFs assume that the world is geometrically, materially, and
    photometrically static. Therefore requiring that any two photographs taken at
    the same position and orientation must be identical [[7](#bib.bib7)] they do not
    have a way to adjust for transient occlusions or variable appearance which result
    in artifacts and noise when this assumption fails such as with real world images.
    This is clearly show in [5](#Sx4.F5 "Figure 5 ‣ Transient Artifacts ‣ Approach:
    NeRF").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 原始NeRFs假设世界在几何、材料和光度上是静态的。因此，要求在相同位置和方向拍摄的任何两张照片必须完全一致[[7](#bib.bib7)]，它们没有办法调整瞬态遮挡或变化的外观，这导致了当这一假设失败时，例如在现实世界图像中出现伪影和噪声。这在[5](#Sx4.F5
    "图5 ‣ 瞬态伪影 ‣ 方法：NeRF")中有清晰的展示。
- en: '![Refer to caption](img/20240280a95fa7d3cff863d3a09daa40.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/20240280a95fa7d3cff863d3a09daa40.png)'
- en: 'Figure 5: Comparison made in the paper NeRF in the Wild [[7](#bib.bib7)], where
    the original NeRF (left) noisy artifacts compared to NeRF-W (right).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：论文《NeRF in the Wild》[[7](#bib.bib7)]中的比较，其中原始NeRF（左）与NeRF-W（右）相比，出现了噪声伪影。
- en: Instant-NGP
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即时神经图形原语
- en: Overview
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Instant-NGP[[10](#bib.bib10)], proposed by Nvlabs, is a method that significantly
    reduce the computation demand of original NeRFs. It leverages multi-resolution
    hash grids to improve memory usage and optimizes 3D reconstruction performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Instant-NGP[[10](#bib.bib10)]，由Nvlabs提出，是一种显著降低原始NeRFs计算需求的方法。它利用多分辨率哈希网格来提高内存使用效率，并优化3D重建性能。
- en: Prior work
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先前工作
- en: Learnable positional encoding
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可学习的位置信息编码
- en: 'Learnable positional encoding refers to positional encodings that are parameterized
    for specific positions in a continuous 3D space. The positional encoding for a
    point $p$ in 3D space can be represented as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可学习的位置信息编码是指针对连续3D空间中特定位置参数化的位置信息编码。3D空间中一个点$p$的位置信息编码可以表示为：
- en: '|  | $\mathbf{pe}(p)=\sigma(\mathbf{W}\mathbf{p}+\mathbf{b})$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{pe}(p)=\sigma(\mathbf{W}\mathbf{p}+\mathbf{b})$ |  |'
- en: where $\mathbf{p}=(x,y,z)^{T}$ represents the coordinates of the position in
    3D space, $\mathbf{W}$ is a learnable weight matrix, $\mathbf{b}$ is a bias vector,
    and $\sigma$ denotes a non-linear activation function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{p}=(x,y,z)^{T}$表示3D空间中位置的坐标，$\mathbf{W}$是一个可学习的权重矩阵，$\mathbf{b}$是一个偏置向量，$\sigma$表示非线性激活函数。
- en: These positional encodings can then be integrated into a neural network model
    to facilitate the learning of spatial relationships.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些位置信息编码可以集成到神经网络模型中，以便学习空间关系。
- en: '![Refer to caption](img/e51d7086583d44a5682ce23d98b831d1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e51d7086583d44a5682ce23d98b831d1.png)'
- en: 'Figure 6: Experiment performed in the original paper. as the number of parameters
    used for learning the positional encoding increases, the image becomes clearer
    and sharper.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：原始论文中进行的实验。随着用于学习位置信息编码的参数数量增加，图像变得更清晰、更锐利。
- en: Algorithm
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: Multi-Resolution Hash Encoding
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多分辨率哈希编码
- en: '![Refer to caption](img/3e7c325913acb7910757dacfbc443679.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3e7c325913acb7910757dacfbc443679.png)'
- en: 'Figure 7: Illustration of the multiresolution hash encoding represented in
    2D in the original paper.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：原始论文中以2D形式表示的多分辨率哈希编码示意图。
- en: 'One of the key components of Instant-NGP (Instant Neural Graphics Primitives)
    is the Multi-Resolution Hash Encoding. Instead of learning the positional encoding
    for the entire 3D space, the 3D space is first scaled to fit within a normalized
    range of 0 to 1\. This normalized space is then replicated across multiple resolutions
    and each subdivided into grids of varying densities. This captures both coarse
    and fine details in the scene. Each level focuses on learning the positional encodings
    at the vertices of the grids. Mathematically, this can be expressed as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Instant-NGP（即时神经图形原语）的关键组件之一是多分辨率哈希编码。与其学习整个3D空间的位置信息编码，不如首先将3D空间缩放到0到1的标准化范围内。然后，将这个标准化空间复制到多个分辨率上，每个分辨率再细分成不同密度的网格。这种方法能够捕捉场景中的粗略和细致细节。每个级别专注于在网格顶点处学习位置信息编码。从数学上来看，这可以表达为以下形式：
- en: '|  | $\mathbf{p}_{scaled}=\mathbf{p}\cdot\mathbf{s}$ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}_{scaled}=\mathbf{p}\cdot\mathbf{s}$ |  |'
- en: 'where $\mathbf{p}$ represents the original coordinates in 3D space, and $\mathbf{s}$
    is a scaling factor that normalizes the space to the [0, 1] range. Following the
    scaling, the coordinates are hashed into a multi-resolution structure using a
    spatial hash function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '-   其中$\mathbf{p}$表示3D空间中的原始坐标，$\mathbf{s}$是将空间标准化到[0, 1]范围的缩放因子。缩放后，坐标通过空间哈希函数被哈希到多分辨率结构中：'
- en: '|  | $h(\mathbf{x})=\left(\bigoplus_{i=1}^{d}(x_{i}\cdot\pi_{i})\right)\mod
    T$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $h(\mathbf{x})=\left(\bigoplus_{i=1}^{d}(x_{i}\cdot\pi_{i})\right)\mod
    T$ |  |'
- en: Here, $d$ is the dimensionality of the space (e.g., 3 for 3D coordinates), $\mathbf{x}=(x_{1},x_{2},\dots,x_{d})$
    represents the scaled coordinates, $\bigoplus$ denotes the bit-wise XOR operation,
    $\pi_{i}$ are large prime numbers unique to each dimension, and $T$ is the size
    of the hash table. This function maps spatial coordinates to indices in the hash
    table, where the neural network’s parameters are stored or retrieved, linking
    specific spatial locations to neural network parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '-   这里，$d$是空间的维度（例如，对于3D坐标为3），$\mathbf{x}=(x_{1},x_{2},\dots,x_{d})$表示缩放坐标，$\bigoplus$表示按位异或操作，$\pi_{i}$是每个维度特有的大质数，$T$是哈希表的大小。该函数将空间坐标映射到哈希表中的索引，其中存储或检索神经网络的参数，将特定空间位置与神经网络参数关联。'
- en: In a nutshell, a hash table is assigned to each level of resolution. For each
    resolution, each vertex is mapped to a entry in the resolution’s hash table. Higher-resolution
    have larger hash tables compared to lower-resolutions. Every resolution’s hash
    table map each of it’s vertices to an individual set of parameters that learn
    their positional encodings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '-   简而言之，每个分辨率级别都有一个哈希表。对于每个分辨率，每个顶点都被映射到该分辨率哈希表中的一个条目。相比于低分辨率，高分辨率具有更大的哈希表。每个分辨率的哈希表将其每个顶点映射到一组单独的参数，这些参数学习它们的位置编码。'
- en: Learning positional encoding
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '-   学习位置编码'
- en: 'During training, when the model is exposed to images from different viewpoints,
    the NN adjusts the parameters stored in the hash table to minimize the difference
    between the rendered images and the actual training images. The loss $L$ can be
    expressed as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在训练过程中，当模型暴露于不同视角的图像时，神经网络会调整存储在哈希表中的参数，以最小化渲染图像与实际训练图像之间的差异。损失$L$可以表示为：'
- en: '|  | $L(\theta)=\frac{1}{2}\sum_{i=1}^{m}(R(x_{i},\theta)-y_{i})^{2}$ |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $L(\theta)=\frac{1}{2}\sum_{i=1}^{m}(R(x_{i},\theta)-y_{i})^{2}$ |  |'
- en: where $R(x_{i},\theta)$ is the rendered image based on parameters $\theta$ and
    view point $x_{i}$, $y_{i}$ is the corresponding actual image, and $m$ is the
    number of pixels or data points considered. We can then learn these parameters
    using different optimization techniques like gradient descent.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '-   其中$R(x_{i},\theta)$是基于参数$\theta$和视角$x_{i}$渲染的图像，$y_{i}$是相应的实际图像，$m$是考虑的像素或数据点数量。然后，我们可以使用不同的优化技术，如梯度下降，来学习这些参数。'
- en: Hash Collisions
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '-   哈希冲突'
- en: Hash collisions are avoided by assigning a hash table at each resolution that
    long enough to ensure one-to-one mapping from entries to positional encodings.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '-   通过为每个分辨率分配一个足够长的哈希表来避免哈希冲突，从而确保条目到位置编码的一对一映射。'
- en: Performance
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   性能'
- en: As shown in figure [8](#Sx5.F8 "Figure 8 ‣ Performance ‣ Instant-NGP"), Instant-NGP
    achieved a notable 20-60× speed improvement compared to compared to the original
    NeRFs while maintaining it’s quality.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '-   如图[8](#Sx5.F8 "Figure 8 ‣ Performance ‣ Instant-NGP")所示，Instant-NGP实现了相较于原始NeRFs显著的20-60倍速度提升，同时保持了其质量。'
- en: '![Refer to caption](img/416d3486eaa17a75360a764b027deedf.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/416d3486eaa17a75360a764b027deedf.png)'
- en: 'Figure 8: The figure adapted from the original paper compares the Peak Signal
    to Noise Ratio (PSNR) performance of various NeRF implementations, including the
    author’s multi-resolution hash encoding method, against other models that require
    hours of training. First row is the name of the object constructed.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '-   图8：从原始论文改编的图表比较了各种NeRF实现的峰值信噪比（PSNR）性能，包括作者的多分辨率哈希编码方法，与其他需要数小时训练的模型。第一行是构建对象的名称。'
- en: Limitations
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   局限性'
- en: Instant-NGP focuses on speeding up the computation and training processes of
    NeRFs. However, it still suffers from many of the same issues such as generalability
    different datasets or unseen scenarios. In the next section, we introduce LDM
    based techniques for 3D reconstruction to address the issue of generalizable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '-   Instant-NGP专注于加快NeRFs的计算和训练过程。然而，它仍然存在许多相同的问题，如对不同数据集或未见场景的泛化能力。在下一节中，我们将介绍基于LDM的3D重建技术，以解决泛化问题。'
- en: Latent-Diffusion-Model based 3D reconstruction
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   基于潜在扩散模型的3D重建'
- en: Background
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 背景
- en: Traditional 3D-Reconstruction algorithms rely heavily on the training data to
    capture all aspects of the volume. Humans, however, are able to estimate a 3D
    surface from a single image. This concept is the foundation of the Zero-1-to-3[[5](#bib.bib5)]
    framework developed out of Columbia University which introduces a diffusion-based
    3D reconstruction method. Zero-1-to-3 utilizes a LDM, originally designed for
    text-conditioned image generation, to generate new perspectives of an image based
    on a camera’s extrinsic parameters like rotation and translation. Zero-1-to-3
    leverages the geometric priors learned by large-scale LDMs, allowing the generation
    of novel views from a single image. Zero-1-to-3 demonstrates strong zero-shot
    generalization capabilities, outperforming prior models in both single-view 3D
    reconstruction and novel view synthesis tasks. See Figure [9](#Sx6.F9 "Figure
    9 ‣ Background ‣ Latent-Diffusion-Model based 3D reconstruction").
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 3D 重建算法严重依赖训练数据来捕捉体积的所有方面。然而，人类能够从单张图像中估计出 3D 表面。这一概念是哥伦比亚大学开发的 Zero-1-to-3[[5](#bib.bib5)]
    框架的基础，该框架引入了一种基于扩散的 3D 重建方法。Zero-1-to-3 利用 LDM，最初设计用于文本条件图像生成，根据相机的外部参数（如旋转和位移）生成图像的新视角。Zero-1-to-3
    利用大规模 LDM 训练得到的几何先验，允许从单张图像生成新视角。Zero-1-to-3 展现了强大的零样本泛化能力，在单视角 3D 重建和新视角合成任务中超越了之前的模型。请参见图
    [9](#Sx6.F9 "图 9 ‣ 背景 ‣ 基于潜在扩散模型的 3D 重建")。
- en: '![Refer to caption](img/a29a1d797ac0d6fbaaea703e935eeb8a.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a29a1d797ac0d6fbaaea703e935eeb8a.png)'
- en: 'Figure 9: High Level Picture of Zero-1-to-3 from the original paper'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：原始论文中的 Zero-1-to-3 高级示意图
- en: 'Prior Work: Denoising Diffusion Probabilistic Models'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先前的工作：去噪扩散概率模型
- en: Denoising Diffusion Probabilistic Models (DDPMs)[[1](#bib.bib1)] are a class
    of generative models that transform data by gradually adding noise over a sequence
    of steps, then learning to reverse this process to generate new samples from noise.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪扩散概率模型（DDPMs）[[1](#bib.bib1)] 是一类生成模型，通过逐步添加噪声来转化数据，然后学习逆向过程从噪声中生成新的样本。
- en: Forward Process
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前向过程
- en: 'The forward process in DDPM is a Markov chain that gradually adds Gaussian
    noise to the data over $T$ timesteps. The process can be mathematically described
    as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM中的前向过程是一个马尔可夫链，它在 $T$ 个时间步中逐渐向数据中添加高斯噪声。这个过程可以数学上描述为：
- en: '|  | $x_{t}=\sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon,\quad\epsilon\sim\mathcal{N}(0,I)$
    |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t}=\sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon,\quad\epsilon\sim\mathcal{N}(0,I)$
    |  |'
- en: where $x_{0}$ is the original data, $x_{t}$ is the data at timestep $t$, $\epsilon$
    is the noise, and $\alpha_{t}$ is the variance schedule parameters that determine
    how much noise is added at each step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$x_{0}$ 是原始数据，$x_{t}$ 是时间步 $t$ 的数据，$\epsilon$ 是噪声，$\alpha_{t}$ 是方差调度参数，用于确定每一步添加多少噪声。
- en: '![Refer to caption](img/fec1d74bc4a2d2fcbdbb096e79d0b5a6.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fec1d74bc4a2d2fcbdbb096e79d0b5a6.png)'
- en: 'Figure 10: Forward Process of DDPM. Adapted from [[4](#bib.bib4)].'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：DDPM 的前向过程。改编自 [[4](#bib.bib4)]。
- en: Reverse Process
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向过程
- en: 'The reverse process aims to reconstruct the original data from the noise by
    learning a parameterized model $p_{\theta}$. The reverse process is also a Markov
    chain, described as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 反向过程旨在通过学习一个参数化模型 $p_{\theta}$ 从噪声中重建原始数据。反向过程也是一个马尔可夫链，描述为：
- en: '|  | $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$ |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$ |  |'
- en: where $\epsilon_{\theta}(x_{t},t)$ is a neural network predicting the noise,
    $\sigma_{t}$ is the standard deviation of the reverse process noise, and $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$
    is the cumulative product of the $\alpha_{t}$ values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\epsilon_{\theta}(x_{t},t)$ 是一个预测噪声的神经网络，$\sigma_{t}$ 是反向过程噪声的标准差，$\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$
    是 $\alpha_{t}$ 值的累积乘积。
- en: Training
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training of DDPMs involves optimizing the parameters $\theta$ of the neural
    network to minimize the difference between the noise predicted by the model and
    the actual noise added during the forward process. The loss function is typically
    the mean squared error between these two noise terms:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM 的训练包括优化神经网络的参数 $\theta$，以最小化模型预测的噪声与前向过程实际添加的噪声之间的差异。损失函数通常是这两个噪声项之间的均方误差：
- en: '|  | $L(\theta)=\mathbb{E}_{t,x_{0},\epsilon}\left[\&#124;\epsilon-\epsilon_{\theta}(x_{t},t)\&#124;_{2}^{2}\right]$
    |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $L(\theta)=\mathbb{E}_{t,x_{0},\epsilon}\left[\| \epsilon - \epsilon_{\theta}(x_{t},t)
    \|_{2}^{2}\right]$ |  |'
- en: where $x_{t}$ is computed during the forward process and $\epsilon$ is the Gaussian
    noise added at each step.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{t}$ 在前向过程中计算，$\epsilon$ 是每步添加的高斯噪声。
- en: Sampling
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 采样
- en: To generate new samples, the reverse process is initialized with pure noise
    $x_{T}\sim\mathcal{N}(0,I)$ and iteratively applies the reverse steps to produce
    samples approximating the distribution of the original data $x_{0}$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为生成新样本，反向过程以纯噪声 $x_{T}\sim\mathcal{N}(0,I)$ 初始化，并通过迭代应用反向步骤来产生近似原始数据分布 $x_{0}$
    的样本。
- en: Algorithm 1 DDPM Sampling Algorithm
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 DDPM 采样算法
- en: '1:procedure DdpmSampling($\theta,T,\{\alpha_{t}\}$)2:     Input: Trained model
    parameters $\theta$, total timesteps $T$, noise schedule $\{\alpha_{t}\}$3:     Output:
    A sample approximating the data distribution4:     Initialize: Draw $x_{T}\sim\mathcal{N}(0,I)$
    Start with pure noise5:     for $t=T$ down to 1 do6:         Calculate $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$7:         Calculate
    $\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\cdot(1-\alpha_{t})$8:         Predict
    noise $\epsilon_{t}=\epsilon_{\theta}(x_{t},t)$9:         if $t>1$ then10:              $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$11:         else12:              $x_{0}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)$
    Final denoising step13:         end if14:     end for15:     Return $x_{0}$16:end procedure![Refer
    to caption](img/fde45972e6e7c59a5b529e8472b4cadb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '1:过程 DdpmSampling($\theta,T,\{\alpha_{t}\}$)2:     输入: 训练好的模型参数 $\theta$，总时间步
    $T$，噪声调度 $\{\alpha_{t}\}$3:     输出: 一个近似数据分布的样本4:     初始化: 生成 $x_{T}\sim\mathcal{N}(0,I)$
    从纯噪声开始5:     对 $t=T$ 直到 1 进行6:         计算 $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$7:         计算
    $\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\cdot(1-\alpha_{t})$8:         预测噪声
    $\epsilon_{t}=\epsilon_{\theta}(x_{t},t)$9:         如果 $t>1$ 则10:              $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$11:         否则12:              $x_{0}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)$
    最终去噪步骤13:         结束 if14:     结束 for15:     返回 $x_{0}$16:结束 过程![参见说明](img/fde45972e6e7c59a5b529e8472b4cadb.png)'
- en: 'Figure 11: Sampling Process. Adapted from [[4](#bib.bib4)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 采样过程。改编自 [[4](#bib.bib4)]。'
- en: Latent Diffusion Model in Zero-1-to-3
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零到三的潜在扩散模型
- en: Latent Diffusion Models[[13](#bib.bib13)] proposed in 2021 are a type of generative
    model that combines the strengths of diffusion models and Variationsal Autoencoders(VAEs).
    Traditional DDPMs operates in the image pixel space, which requires more computation.
    LDMs compress the full image data space into a latent space before the diffusion
    and denosing process, improving efficiency and scalability in generating high-quality
    images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型[[13](#bib.bib13)]于 2021 年提出，是一种生成模型，结合了扩散模型和变分自编码器（VAEs）的优点。传统的 DDPM
    在图像像素空间中操作，这需要更多的计算。LDM 在扩散和去噪过程之前将完整的图像数据空间压缩到潜在空间，从而提高生成高质量图像的效率和可扩展性。
- en: '![Refer to caption](img/fd190672eaf100bdf9ac10b48d174cdf.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd190672eaf100bdf9ac10b48d174cdf.png)'
- en: 'Figure 12: The architecture of Latent Diffusion Model in the original paper.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 原始论文中潜在扩散模型的架构。'
- en: Training Latent Diffusion Models $\epsilon_{\theta}$
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练潜在扩散模型 $\epsilon_{\theta}$
- en: LDM is trained in two main stages. First, a VAE is used to learn an encoding
    function $E(x)$ and a decoding function $D(z)$, where $x$ represents the high-resolution
    image and $z$ is it’s latent representation. The encoder compresses $x$ to $z$,
    and the decoder attempts to reconstruct $x$ from $z$.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LDM 的训练分为两个主要阶段。首先，使用 VAE 学习编码函数 $E(x)$ 和解码函数 $D(z)$，其中 $x$ 表示高分辨率图像，$z$ 是它的潜在表示。编码器将
    $x$ 压缩到 $z$，解码器尝试从 $z$ 重建 $x$。
- en: Training VAE
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练 VAE
- en: 'The VAE optimizes the parameters $\phi$ (encoder) and $\psi$ (decoder) by minimizing
    the reconstruction loss combined with the KL-divergence loss:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 通过最小化重建损失与 KL 散度损失的结合来优化参数 $\phi$（编码器）和 $\psi$（解码器）：
- en: '![Refer to caption](img/65b3721fa5e277712537d9c30756131b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65b3721fa5e277712537d9c30756131b.png)'
- en: 'Figure 13: Training VAE. Figure from Lightning AI'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: VAE 训练。图来自 Lightning AI'
- en: '|  | $\mathcal{L}_{VAE}(\phi,\psi)=\mathbb{E}_{q_{\phi}(z&#124;x)}[\log p_{\psi}(x&#124;z)]-D_{KL}(q_{\phi}(z&#124;x)\&#124;p(z))$
    |  | (3) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{VAE}(\phi,\psi)=\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\psi}(x|z)]-D_{KL}(q_{\phi}(z|x)\|p(z))$
    |  | (3) |'
- en: Training Attention-U-Net Denoiser
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练注意力 U-Net 去噪器
- en: 'In the second stage, an Attention-U-Net is trained as the denoising model in
    the latent space. This model learns a sequence of denoising steps that transform
    a sample from a noise distribution $p(z_{T})$ to the data distribution $p(z_{0})$
    over T timesteps. The U-Net model parameter $\theta$ are optimized by minimizing
    the expected reverse KL-divergence between the true data distribution and the
    model distribution as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，训练一个注意力 U-Net 作为潜在空间中的去噪模型。该模型学习一系列去噪步骤，将来自噪声分布 $p(z_{T})$ 的样本转变为数据分布
    $p(z_{0})$，并经过 T 个时间步。通过最小化真实数据分布与模型分布之间的预期反向 KL 散度来优化 U-Net 模型参数 $\theta$，公式如下：
- en: '|  | $\mathcal{L}(\theta)=\mathbb{E}_{z_{0},\epsilon\sim\mathcal{N}(0,I),t}\left[\&#124;\epsilon-\epsilon_{\theta}(z_{t},t)\&#124;^{2}\right]$
    |  | (4) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\theta)=\mathbb{E}_{z_{0},\epsilon\sim\mathcal{N}(0,I),t}\left[\&#124;\epsilon-\epsilon_{\theta}(z_{t},t)\&#124;^{2}\right]$
    |  | (4) |'
- en: where $z_{t}=\sqrt{\bar{\alpha}_{t}}z_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon$,
    and $\bar{\alpha}_{t}$ is the variance schedule. We use KL-divergence to calculate
    ”how different” the true data distribution and the model distribution are. We
    aim to make the latent space distribution of the model similar to that of the
    real world. This is the key to generating realistic images. Figure [14](#Sx6.F14
    "Figure 14 ‣ Training Attention-U-Net Denoiser ‣ Latent Diffusion Model in Zero-1-to-3
    ‣ Latent-Diffusion-Model based 3D reconstruction") shows example of minimizing
    two distributions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z_{t}=\sqrt{\bar{\alpha}_{t}}z_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon$，而
    $\bar{\alpha}_{t}$ 是方差调度。我们使用 KL 散度来计算真实数据分布与模型分布之间的“差异”。我们的目标是使模型的潜在空间分布类似于现实世界。这是生成真实图像的关键。图
    [14](#Sx6.F14 "图 14 ‣ 训练注意力 U-Net 去噪器 ‣ Zero-1-to-3 中的潜在扩散模型 ‣ 基于潜在扩散模型的 3D 重建")
    显示了最小化两个分布的示例。
- en: '![Refer to caption](img/460481baf7eb06b300b1e91a577e3c98.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/460481baf7eb06b300b1e91a577e3c98.png)'
- en: 'Figure 14: On the left (bad) example, the difference between $Q(x)$ distribution
    and $P(x)$ is not minimized. On the right (good) example, the difference between
    $Q(x)$ distribution and $P(x)$ is minimized. (from [[3](#bib.bib3)])'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：左侧（不佳）示例中，$Q(x)$ 分布与 $P(x)$ 之间的差异未被最小化。右侧（良好）示例中，$Q(x)$ 分布与 $P(x)$ 之间的差异已被最小化。（来自
    [[3](#bib.bib3)]）
- en: '![Refer to caption](img/7359350a51c2ef90b1274d4b52bdb839.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7359350a51c2ef90b1274d4b52bdb839.png)'
- en: 'Figure 15: Attention-U-Net Architecture in the original paper'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：原始论文中的注意力 U-Net 架构
- en: '![Refer to caption](img/5dfa0e92ea394081b28abc82f375d70d.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5dfa0e92ea394081b28abc82f375d70d.png)'
- en: 'Figure 16: Training Attention-U-Net Denoiser. Figure from Lightning AI'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：训练注意力 U-Net 去噪器。图源自 Lightning AI
- en: Conditioning on Camera Parameters
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于相机参数的条件设定
- en: The third stage in the Zero-1-to-3 framework focuses on the conditioning of
    the LDM based on camera extrinsic parameters such as rotation ($R$) and translation
    ($t$). This conditioning is critical for generating novel views of the object,
    which are essential for effective 3D reconstruction from a single image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Zero-1-to-3 框架的第三阶段专注于基于相机外部参数（如旋转 ($R$) 和平移 ($t$)）对 LDM 的条件设定。这种条件设定对于生成物体的新视角至关重要，这对于从单张图像中有效地进行
    3D 重建至关重要。
- en: Mechanism of Conditioning
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 条件设定机制
- en: 'In this stage, the previously trained latent representations are manipulated
    according to the desired camera transformations to simulate new perspectives.
    This process involves:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，之前训练的潜在表示根据所需的相机变换进行操作，以模拟新的视角。该过程包括：
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Camera Transformations: Adjusting the latent variables $z$ to reflect changes
    in viewpoint due to different rotations $R$ and translations $t$.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相机变换：调整潜在变量 $z$ 以反映由于不同旋转 $R$ 和平移 $t$ 引起的视点变化。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformation Implementation: This could be achieved either through a learned
    transformation model within the LDM framework or by applying predefined transformation
    matrices directly to the latent vectors.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变换实现：可以通过在 LDM 框架内学习的变换模型实现，也可以直接将预定义的变换矩阵应用于潜在向量。
- en: Training the Model for Conditional Output
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为条件输出训练模型
- en: 'The model is further trained to handle conditional outputs effectively, which
    involves:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 模型进一步训练以有效处理条件输出，这包括：
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Preparation: The official code used the RTMV dataset[[15](#bib.bib15)]
    where objects are captured from multiple viewpoints to pair latent representations
    with corresponding camera parameters.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据准备：官方代码使用了 RTMV 数据集[[15](#bib.bib15)]，该数据集从多个视角捕获物体，以将潜在表示与相应的相机参数配对。
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Adaptation: Extending the latent diffusion model training to not only
    generate images from the latent representation $z$ but also new perspectives of
    the images from it’s transformed latent representation $z^{\prime}$.'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型适应：扩展潜在扩散模型训练，不仅生成来自潜在表示 $z$ 的图像，还生成其变换后的潜在表示 $z^{\prime}$ 的新视角图像。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MSE Loss: We compute the MSE between the output image and real image with respect
    to $R$ and $t$. A ”near-view consistency loss” that calculate the MSE between
    the image rendered from a view and the image rendered from a nearby view is also
    used to maintain the consistency in 3D reconstruction.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MSE 损失：我们计算输出图像与真实图像之间的 MSE，针对 $R$ 和 $t$。还使用了“近视一致性损失”，该损失计算从一个视角渲染的图像与从附近视角渲染的图像之间的
    MSE，以保持 3D 重建中的一致性。
- en: Novel View generation
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 新视角生成
- en: 'To generate a novel view, the following transformation is applied to the latent
    space:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成新视角，对潜在空间应用以下变换：
- en: '|  | $z^{\prime}=f(z,R,t)$ |  | (5) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{\prime}=f(z,R,t)$ |  | (5) |'
- en: 'Where $f$ is the transformation function that modifies $z$ based on $R$ and
    $t$. The Zero-1-to-3 model generates the novel view as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 是基于 $R$ 和 $t$ 修改 $z$ 的变换函数。Zero-1-to-3 模型生成新视角如下：
- en: '|  | $x^{\prime}=D(\epsilon_{\theta}(z^{\prime}))$ |  | (6) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{\prime}=D(\epsilon_{\theta}(z^{\prime}))$ |  | (6) |'
- en: Where $x^{\prime}$ represents the image generated from the new perspective,
    and $D$ is the decoder part of the LDM that synthesizes the final image output
    from the transformed latent representation $z^{\prime}$.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x^{\prime}$ 表示从新视角生成的图像，$D$ 是 LDM 的解码器部分，它从变换后的潜在表示 $z^{\prime}$ 合成最终的图像输出。
- en: 3D reconstruction $\epsilon_{\theta}$
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3D 重建 $\epsilon_{\theta}$
- en: 'The 3D reconstruction is performed by first generating multiple views of the
    object using the above method for various $R$ and $t$ matrices. Each generated
    image $x^{\prime}$ provides a different perspective of the object. These images
    are then used to reconstruct the 3D model:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 重建是通过首先使用上述方法生成物体的多个视图来执行的，这些视图是通过不同的 $R$ 和 $t$ 矩阵生成的。每个生成的图像 $x^{\prime}$
    提供了物体的不同视角。这些图像随后被用来重建 3D 模型：
- en: '|  | $\text{3D Model}=\text{Integrate}(\{x^{\prime}\})$ |  | (7) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{3D Model}=\text{Integrate}(\{x^{\prime}\})$ |  | (7) |'
- en: The integration process typically involves techniques like volumetric fusion
    or multi-view stereo algorithms, which consolidate the information from different
    images to create a detailed 3D representation of the object, as shown in figure
    [17](#Sx6.F17 "Figure 17 ‣ 3D reconstruction ϵ_𝜃 ‣ Latent Diffusion Model in Zero-1-to-3
    ‣ Latent-Diffusion-Model based 3D reconstruction").
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 集成过程通常涉及体积融合或多视图立体算法等技术，这些技术整合来自不同图像的信息以创建物体的详细 3D 表示，如图 [17](#Sx6.F17 "Figure
    17 ‣ 3D reconstruction ϵ_𝜃 ‣ Latent Diffusion Model in Zero-1-to-3 ‣ Latent-Diffusion-Model
    based 3D reconstruction") 所示。
- en: '![Refer to caption](img/1a7baef9ae0cc4e9550967d80125c530.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1a7baef9ae0cc4e9550967d80125c530.png)'
- en: 'Figure 17: Demo of 3D reconstruction from Zero-1-to-3'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：Zero-1-to-3 的 3D 重建演示
- en: Limitations for diffusion-based and NeRF-based 3D reconstruction
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于扩散和 NeRF 的 3D 重建的局限性
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Flexibility and Real-Time 3D scene Rendering: Training the Zero-1-to-3 model
    for 3D scenes reconstruction typically require iterative denoising processes during
    sampling, which can be computationally intensive and slow. This makes them less
    suitable for applications requiring real-time rendering.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活性和实时 3D 场景渲染：训练 Zero-1-to-3 模型进行 3D 场景重建通常需要在采样过程中进行迭代去噪过程，这可能计算密集且速度较慢。这使得它们不太适合需要实时渲染的应用。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Implicit Representation Ambiguity: Both NeRF and Diffusion models represent
    the 3D object implicitly; they do not explicitly construct the 3D space. NeRFs
    utilize the weights of an MLP to represent a 3D scene and LDMs use the latent
    space for new perspectives generation for 3D reconstruction. While implicit representation
    saves significant space it may lead to ambiguities in interpreting the model.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐式表示模糊性：NeRF 和扩散模型都隐式地表示 3D 物体；它们不显式构建 3D 空间。NeRF 利用 MLP 的权重来表示 3D 场景，而 LDM
    则使用潜在空间生成 3D 重建的新视角。虽然隐式表示节省了大量空间，但可能会导致对模型解释的模糊性。
- en: 'Approach: 3D Gaussian Splatting'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法：3D 高斯散点法
- en: Background
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 背景
- en: Throughout the evolution of 3D scene reconstruction, explicit representations
    such as meshes and point clouds have always been preferred by developers and researchers
    due to their clearly defined structure, fast rendering, and ease of editing. NeRF-based
    methods have shifted towards continuous representation of 3D scenes. While the
    continuous nature of these methods helps optimization, the stochastic sampling
    required for rendering is costly and can result in noise [[2](#bib.bib2)]. On
    top of that the implicit representation’s lack of geometric information does not
    lend itself well to editing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D场景重建的发展过程中，由于网格和点云等显式表示具有明确的结构、快速渲染和易于编辑的特点，一直受到开发者和研究人员的青睐。基于NeRF的方法则转向3D场景的连续表示。虽然这些方法的连续性有助于优化，但渲染所需的随机采样代价高且可能导致噪声[[2](#bib.bib2)]。此外，隐式表示缺乏几何信息，不利于编辑。
- en: Overview
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: 3D Gaussian Splatting provides a high-quality novel view with real-time rendering
    for 3D scenes, these are achieved with the utilization of the Gaussian function
    to present a smooth and accurate texture using captured photos of a scene.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 高斯点云渲染提供了高质量的新视角和实时渲染的3D场景，这些都是通过利用高斯函数来呈现平滑且准确的纹理，利用拍摄到的场景照片实现的。
- en: '![Refer to caption](img/4233b37fbc835e72cbe140d357cd30d7.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4233b37fbc835e72cbe140d357cd30d7.png)'
- en: 'Figure 18: Overview of 3D Gaussian Splatting process.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18: 3D 高斯点云渲染过程概述。'
- en: To reconstruct a 3D model using 3D Gaussian splatting, first, a video of the
    object is taken from different angles, then converted into frames representing
    static scenes at different camera angles. Structure from Motion (SfM) with feature
    detection and matching techniques such as SIFT is then applied to these images
    to produce a sparse point cloud. The 3D data points of the point cloud are then
    represented by 3D Gaussians. With the optimization process, adaptive density control
    of Gaussians, and high-efficiency algorithm design, realistic views of the 3D
    model can be reconstructed with a high frame rate.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用3D高斯点云渲染重建3D模型，首先需要从不同角度拍摄物体的视频，然后将其转换为表示不同相机角度的静态场景的帧。然后对这些图像应用结构光束（SfM）及特征检测和匹配技术，如SIFT，以生成稀疏点云。点云的3D数据点随后由3D高斯函数表示。通过优化过程、高斯的自适应密度控制和高效算法设计，可以以高帧率重建3D模型的逼真视图。
- en: 'The algorithm of 3D Gaussian Splatting is demonstrated below, where it can
    be slit into 3 parts: initialization, optimization, and density control of Gaussians.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 高斯点云渲染算法如下面所示，它可以分为三个部分：初始化、优化和高斯的密度控制。
- en: Algorithm
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: Algorithm 2 Optimization and Densification
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 优化和密度调整
- en: '$w$, $h$: width and height of the training images'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '$w$, $h$: 训练图像的宽度和高度'
- en: '$M\leftarrow$ SfM Points $\triangleright$ Positions$i\leftarrow 0$ $\triangleright$
    Iteration Countwhile not converged do     $V,\hat{I}\leftarrow$ SampleTrainingView()
    $\triangleright$ Camera $V$ and Image     $I\leftarrow$ Rasterize($M$, $S$, $C$,
    $A$, $V$) $\triangleright$ Alg. [3](#alg3 "Algorithm 3 ‣ Tile-based rasterizer
    for real-time rendering ‣ Approach: 3D Gaussian Splatting")     $L\leftarrow Loss(I,\hat{I})$
    $\triangleright$ Loss     $M$, $S$, $C$, $A$ $\leftarrow$ Adam($\nabla L$) $\triangleright$
    Backprop & Step     if IsRefinementIteration($i$) then         for all Gaussians
    $(\mu,\Sigma,c,\alpha)$ in $(M,S,C,A)$ do              if $\alpha<\epsilon$ or
    IsTooLarge($\mu,\Sigma)$ then $\triangleright$ Pruning                  RemoveGaussian()              end if              if $\nabla_{p}L>\tau_{p}$ then
    $\triangleright$ Densification                  if $\|S\|>\tau_{S}$ then $\triangleright$
    Over-reconstruction                       SplitGaussian($\mu,\Sigma,c,\alpha$)                  else$\triangleright$
    Under-reconstruction                       CloneGaussian($\mu,\Sigma,c,\alpha$)                  end if              end if         end for     end if     $i\leftarrow
    i+1$end while'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: $M\leftarrow$ SfM 点 $\triangleright$ 位置 $i\leftarrow 0$ $\triangleright$ 迭代计数
    while not converged do     $V,\hat{I}\leftarrow$ SampleTrainingView() $\triangleright$
    相机 $V$ 和图像     $I\leftarrow$ Rasterize($M$, $S$, $C$, $A$, $V$) $\triangleright$
    算法 [3](#alg3 "算法 3 ‣ 基于瓦片的光栅化器用于实时渲染 ‣ 方法：3D 高斯散射")     $L\leftarrow Loss(I,\hat{I})$
    $\triangleright$ 损失     $M$, $S$, $C$, $A$ $\leftarrow$ Adam($\nabla L$) $\triangleright$
    反向传播与步骤     if IsRefinementIteration($i$) then         for all 高斯分布 $(\mu,\Sigma,c,\alpha)$
    在 $(M,S,C,A)$ do              if $\alpha<\epsilon$ 或 IsTooLarge($\mu,\Sigma)$ then
    $\triangleright$ 剪枝                  RemoveGaussian()              end if              if $\nabla_{p}L>\tau_{p}$ then
    $\triangleright$ 密度化                  if $\|S\|>\tau_{S}$ then $\triangleright$
    过度重建                       SplitGaussian($\mu,\Sigma,c,\alpha$)                  else$\triangleright$
    欠重建                       CloneGaussian($\mu,\Sigma,c,\alpha$)                  end if              end if         end for     end if     $i\leftarrow
    i+1$ end while
- en: Initialization
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始化
- en: 'Points in the sparse 3D data point cloud generated by SfM, are initialized
    to 3D Gaussians. The Gaussians are defined by the following variables: position
    $p$, world space 3D covariance matrix $\Sigma$, opacity $\alpha$, and spherical
    harmonics coefficient c (representation of RBG color), given formula:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由SfM生成的稀疏3D数据点云中的点被初始化为3D高斯分布。高斯分布由以下变量定义：位置$p$、世界空间3D协方差矩阵$\Sigma$、透明度$\alpha$和球面谐波系数$c$（RGB颜色的表示），给定公式：
- en: '|  | $G(x)~{}=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$ |  | (8) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $G(x)~{}=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$ |  | (8) |'
- en: Optimization
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化
- en: Initially, the Gaussians are sparse and not representative, but they are gradually
    optimized to better represent the scene. To do this, a random camera view $V$
    with it’s corresponding image $\hat{I}$ is chosen from the training set. A rasterized
    Gaussian image $I$ is generated by passing the Gaussian means, $\Sigma$, $c$,
    $\alpha$, and $V$ to a differentiable rasterizer function $Rasterize()$.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，高斯分布是稀疏的且不具有代表性，但它们会逐渐优化，以更好地表示场景。为此，从训练集中选择一个随机相机视角$V$及其对应的图像$\hat{I}$。通过将高斯均值、$\Sigma$、$c$、$\alpha$和$V$传递给可微分的光栅化函数$Rasterize()$，生成光栅化的高斯图像$I$。
- en: 'A loss function, shown in equation ([9](#Sx7.E9 "In Optimization ‣ Algorithm
    ‣ Approach: 3D Gaussian Splatting")), is used to compute the gradients of the
    two images $\hat{I}$ and $I$.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用损失函数（见方程（[9](#Sx7.E9 "在优化 ‣ 算法 ‣ 方法：3D高斯散射")）），计算两个图像$\hat{I}$和$I$的梯度。
- en: '|  | $\mathcal{L}=(1-\lambda)\mathcal{L}_{1}+\lambda\mathcal{L_{\textrm{D-SSIM}}}$
    |  | (9) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=(1-\lambda)\mathcal{L}_{1}+\lambda\mathcal{L_{\textrm{D-SSIM}}}$
    |  | (9) |'
- en: Here, $\mathcal{L}_{1}$ is the Mean Absolute Error of $\hat{I}$ and $I$, $\mathcal{L}_{D-SSMI}$
    is the Difference Structural Similarity Index based loss, and $\lambda$ is a pre-defined
    weight that adjusts the contribution of $\mathcal{L}_{1}$ and $\mathcal{L}_{D-SSMI}$
    to the final loss $\mathcal{L}$. The parameters of the Gaussians are adjusted
    accordingly with the Adam optimizer [[2](#bib.bib2)].
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathcal{L}_{1}$是$\hat{I}$和$I$的平均绝对误差，$\mathcal{L}_{D-SSMI}$是基于差异结构相似性指数的损失，$\lambda$是一个预定义的权重，用于调整$\mathcal{L}_{1}$和$\mathcal{L}_{D-SSMI}$对最终损失$\mathcal{L}$的贡献。高斯分布的参数会通过Adam优化器[[2](#bib.bib2)]进行相应调整。
- en: Adaptive control of Gaussians
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高斯分布的自适应控制
- en: 'After initialization, an adaptive approach is used to control the number and
    density of Guassians. Adaptive control refers to automatically adjusting the size
    and number of Gaussians to optimize the representation of the static 3D scene.
    The adaptive density control follows the following behaviors (see Fig. 12):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，使用自适应方法来控制高斯的数量和密度。自适应控制指的是自动调整高斯的大小和数量，以优化静态3D场景的表示。自适应密度控制遵循以下行为（见图12）：
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gaussian Removal: For every 100 iterations, if the Gaussians are too large
    in the 3D space or have opacity under a defined threshold of opacity $\epsilon_{\alpha}$(essentially
    transparent), they are removed.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高斯移除：每100次迭代，如果高斯在3D空间中过大或其不透明度低于定义的不透明度阈值$\epsilon_{\alpha}$（基本上是透明的），则将其移除。
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gaussian Duplication: For regions filled by gaussians that are greater than
    the defined threshold but is too small, the Gaussians are cloned and moved along
    their direction to cover the empty space. This behavior adaptively and gradually
    increases the number and volume of the Gaussians until the area is well-fitted.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高斯克隆：对于那些大于定义阈值但又过小的区域中的高斯，高斯会被克隆并沿其方向移动以覆盖空白区域。这种行为自适应地逐渐增加高斯的数量和体积，直到区域得到良好拟合。
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gaussian Split: For regions that are over-reconstructed by large Gaussians
    (variance is too high), they are split into smaller Gaussians by a factor $\phi$,
    the original paper used $\phi=1.6$.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高斯分裂：对于那些被大高斯（方差过高）重建过度的区域，它们会被分裂成较小的高斯，分裂因子为$\phi$，原始论文中使用了$\phi=1.6$。
- en: '![Refer to caption](img/4239a49b90764d069b404d1913e97e72.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4239a49b90764d069b404d1913e97e72.png)'
- en: 'Figure 19: Gaussians split in over-reconstructed areas while clone in under-reconstructed
    areas.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：高斯在重建过度的区域被分裂，而在重建不足的区域被克隆。
- en: Due to the splitting and duplicating, the number of Gaussians increases. To
    address this, every Gaussians’ opacity $\alpha$ close to zero every $N=3000$ iteration,
    after the optimization step then increases the $\alpha$ values for the Guassian
    where this is needed while allowing the unused ones to be removed.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分裂和克隆，高斯的数量增加。为了解决这个问题，每$N=3000$次迭代时将高斯的不透明度$\alpha$接近于零，优化步骤后再增加需要的高斯的$\alpha$值，同时允许移除未使用的高斯。
- en: Gradient computation of $\Sigma$ and $\Sigma^{\prime}$
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: $\Sigma$和$\Sigma^{\prime}$的梯度计算
- en: Each Gaussian is represented as an ellipsoid in the 3D space, modelled by a
    covariance matrices $\Sigma$.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每个高斯在3D空间中被表示为一个椭球体，由协方差矩阵$\Sigma$建模。
- en: '|  | $\Sigma=RSS^{T}R^{T}$ |  | (10) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Sigma=RSS^{T}R^{T}$ |  | (10) |'
- en: Where S is the scaling matrix and R is the rotation matrix.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 其中S是缩放矩阵，R是旋转矩阵。
- en: For each camera angle, 3D Gaussians are rasterized to the 2D screen. The 3D
    covariance matrix $\Sigma^{\prime}$ is derived using the viewing transformation
    matrix $W$ and Jacobian $J$, which approximates the projective transformation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个相机角度，3D高斯被光栅化到2D屏幕上。3D协方差矩阵$\Sigma^{\prime}$通过视图变换矩阵$W$和雅可比矩阵$J$推导出来，这近似于投影变换。
- en: '|  | $\Sigma^{\prime}=JW{\Sigma}W^{T}J^{T}$ |  | (11) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Sigma^{\prime}=JW{\Sigma}W^{T}J^{T}$ |  | (11) |'
- en: $\Sigma^{\prime}$, a 3x3 matrix, can be simplified by ignoring the elements
    in the third row and column while retaining the properties of their corresponding
    planar points representation [[19](#bib.bib19)].
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: $\Sigma^{\prime}$，一个3x3矩阵，可以通过忽略第三行和第三列中的元素来简化，同时保留其对应平面点表示的属性[[19](#bib.bib19)]。
- en: 'To compute the gradient of the 3D space covariance, the chain rule is applied
    to $\Sigma$ and $\Sigma^{\prime}$ with reference to their rotation $q$ and scaling
    $s$. This results in the expressions $\frac{d\Sigma^{\prime}}{ds}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{ds}$
    and $\frac{d\Sigma^{\prime}}{dq}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{dq}$.
    By substituting $U=JW$ into the equation for $\Sigma^{\prime}$, we get ${\Sigma^{\prime}}=U\Sigma
    U^{T}$. This equation allows the partial derivative of each element in $\frac{d\Sigma^{\prime}}{d\Sigma}$
    to be represented in terms of $U$:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算3D空间协方差的梯度，链式法则被应用于$\Sigma$和$\Sigma^{\prime}$，参考它们的旋转$q$和缩放$s$。这导致了表达式$\frac{d\Sigma^{\prime}}{ds}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{ds}$和$\frac{d\Sigma^{\prime}}{dq}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{dq}$。通过将$U=JW$代入$\Sigma^{\prime}$的方程中，我们得到${\Sigma^{\prime}}=U\Sigma
    U^{T}$。这个方程允许将$\frac{d\Sigma^{\prime}}{d\Sigma}$中每个元素的偏导数用$U$表示：
- en: <math   alttext="\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&amp;U_{1,i}U_{2,j}\\
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&amp;U_{1,i}U_{2,j}\\
- en: U_{1,j}U_{2,i}&amp;U_{2,i}U_{2,j}\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mfrac ><mrow ><mo rspace="0em" >∂</mo><msup ><mi mathvariant="normal"
    >Σ</mi><mo >′</mo></msup></mrow><mrow ><mo rspace="0em" >∂</mo><msub ><mi mathvariant="normal"
    >Σ</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></msub></mrow></mfrac><mo
    >=</mo><mrow ><mo >(</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mrow
    ><msub ><mi >U</mi><mrow ><mn >1</mn><mo >,</mo><mi >i</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >U</mi><mrow ><mn >1</mn><mo >,</mo><mi >j</mi></mrow></msub></mrow></mtd><mtd
    ><mrow ><msub ><mi >U</mi><mrow ><mn >1</mn><mo >,</mo><mi >i</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi
    >j</mi></mrow></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><msub ><mi >U</mi><mrow
    ><mn >1</mn><mo >,</mo><mi >j</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi >i</mi></mrow></msub></mrow></mtd><mtd
    ><mrow ><msub ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi >i</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi
    >j</mi></mrow></msub></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >Σ</ci><ci >′</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >Σ</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></apply></apply><matrix ><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑈</ci><list
    ><cn type="integer" >1</cn><ci >𝑖</ci></list></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑈</ci><list ><cn type="integer" >1</cn><ci >𝑗</ci></list></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑈</ci><list ><cn type="integer"
    >1</cn><ci >𝑖</ci></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑈</ci><list ><cn type="integer" >2</cn><ci >𝑗</ci></list></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑈</ci><list
    ><cn type="integer" >1</cn><ci >𝑗</ci></list></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑈</ci><list ><cn type="integer" >2</cn><ci >𝑖</ci></list></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑈</ci><list ><cn type="integer"  >2</cn><ci
    >𝑖</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑈</ci><list ><cn type="integer"  >2</cn><ci >𝑗</ci></list></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&U_{1,i}U_{2,j}\\
    U_{1,j}U_{2,i}&U_{2,i}U_{2,j}\end{smallmatrix}\right)</annotation></semantics></math>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&U_{1,i}U_{2,j}\\
    U_{1,j}U_{2,i}&U_{2,i}U_{2,j}\end{smallmatrix}\right)\)
- en: 'By substituting $M=RS$ into equation ([10](#Sx7.E10 "In Gradient computation
    of Σ and Σ'' ‣ Approach: 3D Gaussian Splatting")), it can then be rewritten as
    $\Sigma=MM^{T}$. Using the chain rule, we can derive $\frac{d\Sigma}{ds}=\frac{d\Sigma}{dM}\frac{dM}{ds}$
    and $\frac{d\Sigma}{dq}=\frac{d\Sigma}{dM}\frac{dM}{dq}$. This allows us to calculate
    the scaling gradient at position $(i,j)$ of the covariance matrix with:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 将$M=RS$代入方程（[10](#Sx7.E10 "在Σ和Σ'的梯度计算中 ‣ 方法：3D高斯点积"）），可以将其重写为$\Sigma=MM^{T}$。使用链式法则，我们可以推导出$\frac{d\Sigma}{ds}=\frac{d\Sigma}{dM}\frac{dM}{ds}$和$\frac{d\Sigma}{dq}=\frac{d\Sigma}{dM}\frac{dM}{dq}$。这使我们能够计算协方差矩阵在位置$(i,j)$的缩放梯度，计算公式为：
- en: '|  | $\frac{\partial M_{i,j}}{\partial s_{k}}=\left\{\begin{array}[]{lr}R_{i,k}&amp;\text{if
    }j=k\\ 0&amp;\text{otherwise}\end{array}\right.$ |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial M_{i,j}}{\partial s_{k}}=\left\{\begin{array}[]{lr}R_{i,k}&amp;\text{如果
    }j=k\\ 0&amp;\text{否则}\end{array}\right.$ |  |'
- en: 'For defining derivatives of $M$ with respect to rotation matrix $R$ in terms
    of quaternion $q$ components, the following formula demonstrating how quaternion
    components affect $R$ is involved:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何用四元数$q$的分量来定义旋转矩阵$R$的$M$的导数，涉及到以下公式，这个公式展示了四元数分量如何影响$R$：
- en: '|  | <math   alttext="R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&amp;(q_{i}q_{j}-q_{r}q_{k})&amp;(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&amp;\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&amp;(q_{j}q_{k}-q_{r}q_{i})\\'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&amp;(q_{i}q_{j}-q_{r}q_{k})&amp;(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&amp;\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&amp;(q_{j}q_{k}-q_{r}q_{i})\\'
- en: (q_{i}q_{k}-q_{r}q_{j})&amp;(q_{j}q_{k}+q_{r}q_{i})&amp;\frac{1}{2}-(q_{i}^{2}+q_{j}^{2})\end{pmatrix}"
    display="block"><semantics ><mrow  ><mrow ><mi >R</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow  ><mo stretchy="false"  >(</mo><mi >q</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow
    ><mn >2</mn><mo lspace="0em" rspace="0em" >​</mo><mrow  ><mo >(</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mstyle displaystyle="false"  ><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo >−</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msubsup ><mi >q</mi><mi >j</mi><mn >2</mn></msubsup><mo >+</mo><msubsup ><mi
    >q</mi><mi >k</mi><mn >2</mn></msubsup></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    ><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow><mo
    >−</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mtd><mtd
    ><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow><mo
    >+</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >j</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mo stretchy="false"  >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow><mo
    >+</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mtd><mtd
    ><mrow ><mstyle displaystyle="false" ><mfrac ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo
    >−</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi >q</mi><mi >i</mi><mn
    >2</mn></msubsup><mo >+</mo><msubsup ><mi >q</mi><mi >k</mi><mn >2</mn></msubsup></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mtd><mtd ><mrow ><mo stretchy="false"
    >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi >j</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow><mo >−</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><mrow ><msub ><mi >q</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow><mo >−</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd ><mrow ><mo stretchy="false" >(</mo><mrow
    ><mrow ><msub ><mi >q</mi><mi >j</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow><mo >+</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd ><mrow ><mstyle displaystyle="false"
    ><mfrac ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo >−</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msubsup ><mi >q</mi><mi >i</mi><mn >2</mn></msubsup><mo >+</mo><msubsup
    ><mi >q</mi><mi >j</mi><mn >2</mn></msubsup></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><ci >𝑅</ci><ci  >𝑞</ci></apply><apply ><cn type="integer" >2</cn><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply><cn
    type="integer"  >2</cn></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci
    >𝑗</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑟</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑘</ci></apply></apply></apply><apply ><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply><cn
    type="integer"  >2</cn></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci
    >𝑘</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑟</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑘</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑟</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply><cn
    type="integer"  >2</cn></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&(q_{i}q_{j}-q_{r}q_{k})&(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&(q_{j}q_{k}-q_{r}q_{i})\\
    (q_{i}q_{k}-q_{r}q_{j})&(q_{j}q_{k}+q_{r}q_{i})&\frac{1}{2}-(q_{i}^{2}+q_{j}^{2})\end{pmatrix}</annotation></semantics></math>
    |  | (12) |
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: \( R(q) = 2 \begin{pmatrix} \frac{1}{2} - (q_{j}^{2} + q_{k}^{2}) & (q_{i}q_{j}
    - q_{r}q_{k}) & (q_{i}q_{k} + q_{r}q_{j}) \\ (q_{i}q_{j} + q_{r}q_{k}) & \frac{1}{2}
    - (q_{i}^{2} + q_{k}^{2}) & (q_{j}q_{k} - q_{r}q_{i}) \\ (q_{i}q_{k} - q_{r}q_{j})
    & (q_{j}q_{k} + q_{r}q_{i}) & \frac{1}{2} - (q_{i}^{2} + q_{j}^{2}) \end{pmatrix}
    \)
- en: 'And therefore the gradient $\frac{\partial M}{\partial q_{x}}$ for 4 components
    of quaternion $r,i,j,k$ can be calculated as follow:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，四元数$r,i,j,k$的梯度$\frac{\partial M}{\partial q_{x}}$可以按如下方式计算：
- en: '|  |  | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&amp;-s_{y}q_{k}&amp;s_{z}q_{j}\\
    s_{x}q_{k}&amp;0&amp;-s_{z}q_{i}\\'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&amp;-s_{y}q_{k}&amp;s_{z}q_{j}\\
    s_{x}q_{k}&amp;0&amp;-s_{z}q_{i}\\'
- en: -s_{x}q_{j}&amp;s_{y}q_{i}&amp;0\end{smallmatrix}\right)," display="inline"><semantics
    ><mrow  ><mrow ><mstyle displaystyle="true" ><mfrac ><mrow ><mo rspace="0em"  >∂</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mrow
    ><mo >−</mo><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mrow></mtd><mtd ><mrow ><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi
    >q</mi><mi >j</mi></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><msub ><mi >s</mi><mi
    >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mtd><mtd
    ><mn >0</mn></mtd><mtd ><mrow  ><mo >−</mo><mrow ><msub ><mi >s</mi><mi >z</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mo >−</mo><mrow ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mn >0</mn></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow></mrow><mo >,</mo></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply ><ci >𝑀</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑟</ci></apply></apply></apply><apply ><cn type="integer" >2</cn><matrix
    ><matrixrow ><cn type="integer" >0</cn><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑦</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑘</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply><cn
    type="integer"  >0</cn><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑖</ci></apply></apply></apply></matrixrow><matrixrow ><apply ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply></apply><cn
    type="integer"  >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&-s_{y}q_{k}&s_{z}q_{j}\\
    s_{x}q_{k}&0&-s_{z}q_{i}\\ -s_{x}q_{j}&s_{y}q_{i}&0\end{smallmatrix}\right),</annotation></semantics></math>
    | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{i}}=2\left(\begin{smallmatrix}0&amp;s_{y}q_{j}&amp;s_{z}q_{k}\\
    s_{x}q_{j}&amp;-2s_{y}q_{i}&amp;-s_{z}q_{r}\\
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&-s_{y}q_{k}&s_{z}q_{j}\\
    s_{x}q_{k}&0&-s_{z}q_{i}\\ -s_{x}q_{j}&s_{y}q_{i}&0\end{smallmatrix}\right),\)
- en: s_{x}q_{k}&amp;s_{y}q_{r}&amp;-2s_{z}q_{i}\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mstyle displaystyle="true"  ><mfrac ><mrow ><mo rspace="0em" >∂</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em" >∂</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn  >2</mn><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mrow
    ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >q</mi><mi >j</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >z</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >j</mi></msub></mrow></mtd><mtd ><mrow ><mo >−</mo><mrow ><mn
    >2</mn><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >s</mi><mi >y</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><mo >−</mo><mrow ><msub ><mi >s</mi><mi >z</mi></msub><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >y</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mtd><mtd
    ><mrow ><mo >−</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >q</mi><mi >i</mi></msub></mrow></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝑀</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply></apply></apply><apply
    ><cn type="integer" >2</cn><matrix ><matrixrow ><cn type="integer" >0</cn><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑧</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply></apply><apply
    ><apply ><cn type="integer" >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑠</ci><ci >𝑦</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑟</ci></apply></apply></apply></matrixrow><matrixrow ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply></apply><apply
    ><apply ><cn type="integer" >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑖</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{i}}=2\left(\begin{smallmatrix}0&s_{y}q_{j}&s_{z}q_{k}\\
    s_{x}q_{j}&-2s_{y}q_{i}&-s_{z}q_{r}\\ s_{x}q_{k}&s_{y}q_{r}&-2s_{z}q_{i}\end{smallmatrix}\right)</annotation></semantics></math>
    |  | (13) |
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial M}{\partial q_{i}} = 2\left(\begin{smallmatrix}0 & s_{y}q_{j}
    & s_{z}q_{k}\\ s_{x}q_{j} & -2s_{y}q_{i} & -s_{z}q_{r}\\ s_{x}q_{k} & s_{y}q_{r}
    & -2s_{z}q_{i}\end{smallmatrix}\right)\)
- en: '|  |  | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&amp;s_{y}q_{i}&amp;s_{z}q_{r}\\
    s_{x}q_{i}&amp;0&amp;s_{z}q_{k}\\'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  | <math alttext="\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&amp;s_{y}q_{i}&amp;s_{z}q_{r}\\
    s_{x}q_{i}&amp;0&amp;s_{z}q_{k}\\'
- en: -s_{x}q_{r}&amp;s_{y}q_{k}&amp;-2s_{z}q_{j}\end{smallmatrix}\right)," display="inline"><semantics
    ><mrow  ><mrow ><mstyle displaystyle="true"  ><mfrac ><mrow ><mo rspace="0em"  >∂</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mrow ><mo >−</mo><mrow ><mn
    >2</mn><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >s</mi><mi >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >z</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mn >0</mn></mtd><mtd ><mrow  ><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >q</mi><mi >k</mi></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mo >−</mo><mrow
    ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >r</mi></msub></mrow></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >y</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mtd><mtd
    ><mrow ><mo >−</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >q</mi><mi >j</mi></msub></mrow></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply ><apply
    ><ci >𝑀</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply></apply></apply><apply ><cn type="integer"  >2</cn><matrix
    ><matrixrow ><apply ><apply ><cn type="integer"  >2</cn><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑥</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑦</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑖</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑟</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply></apply><cn
    type="integer"  >0</cn><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑘</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply><apply
    ><apply ><cn type="integer"  >2</cn><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&s_{y}q_{i}&s_{z}q_{r}\\
    s_{x}q_{i}&0&s_{z}q_{k}\\ -s_{x}q_{r}&s_{y}q_{k}&-2s_{z}q_{j}\end{smallmatrix}\right),</annotation></semantics></math>
    | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&amp;-s_{y}q_{r}&amp;s_{z}q_{i}\\
    s_{x}q_{r}&amp;-2s_{y}q_{k}&amp;s_{z}q_{j}\\
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \(\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&s_{y}q_{i}&s_{z}q_{r}\\
    s_{x}q_{i}&0&s_{z}q_{k}\\ -s_{x}q_{r}&s_{y}q_{k}&-2s_{z}q_{j}\end{smallmatrix}\right),\)
- en: s_{x}q_{i}&amp;s_{y}q_{j}&amp;0\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mstyle displaystyle="true"  ><mfrac ><mrow ><mo rspace="0em" >∂</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em" >∂</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo  >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mo >−</mo><mrow ><mn
    >2</mn><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >s</mi><mi >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><mo >−</mo><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><msub ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><msub ><mi
    >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >q</mi><mi
    >r</mi></msub></mrow></mtd><mtd ><mrow ><mo >−</mo><mrow ><mn >2</mn><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >z</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >y</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mtd><mtd
    ><mn >0</mn></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝑀</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply></apply><apply
    ><cn type="integer" >2</cn><matrix ><matrixrow ><apply ><apply ><cn type="integer"
    >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑘</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑧</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑟</ci></apply></apply><apply
    ><apply ><cn type="integer" >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑠</ci><ci >𝑦</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑞</ci><ci >𝑘</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑗</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑖</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑞</ci><ci >𝑗</ci></apply></apply><cn
    type="integer" >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&-s_{y}q_{r}&s_{z}q_{i}\\
    s_{x}q_{r}&-2s_{y}q_{k}&s_{z}q_{j}\\ s_{x}q_{i}&s_{y}q_{j}&0\end{smallmatrix}\right)</annotation></semantics></math>
    |  |
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&-s_{y}q_{r}&s_{z}q_{i}\\
    s_{x}q_{r}&-2s_{y}q_{k}&s_{z}q_{j}\\ s_{x}q_{i}&s_{y}q_{j}&0\end{smallmatrix}\right)\)
- en: Tile-based rasterizer for real-time rendering
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于实时渲染的基于瓦片的光栅化器
- en: A technique called Tile-based Rasterizer is used to quickly render the 3D model
    constructed by the Gaussians (see Fig 13). This approach first uses a given view
    angle $V$ of the camera and its position $p$ to filter out the Gaussians that
    are not contributing to the view frustum. In this way only the useful Gaussians
    are involved in the $\alpha$-blending, improving the rendering efficiency.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为基于瓦片的光栅化器的技术被用来快速渲染由高斯构建的3D模型（见图13）。这种方法首先使用给定的摄像机视角$V$和位置$p$来过滤掉不贡献于视锥体的高斯。这样，只有有用的高斯参与$\alpha$-混合，提高了渲染效率。
- en: '![Refer to caption](img/5ef26be5b9c73def8f5da0b6a46b7199.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ef26be5b9c73def8f5da0b6a46b7199.png)'
- en: 'Figure 20: Overview of Tile-based rasterize areas, where green Gaussians are
    contributing to the view frustum, and red Gaussians are not, given camera angle
    $V$.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20: 基于瓦片的光栅化区域概述，其中绿色高斯对视锥体有贡献，红色高斯没有，给定摄像机角度$V$。'
- en: Instead of sorting all the Gaussian individually for per-pixel $\alpha$-blending,
    tiles are created to divide the 2D screen into smaller 16x16 sections. An instance
    key is assigned to each of the Gaussians using their corresponding view-space
    depth and the tiles they reside with a tile ID. The Gaussians are then sorted
    according to their instance key using a GPU Radix sort. The Radix sort is capable
    of handling large sets of data in parallel with GPU threads. The result of the
    Gaussian sorting can also demonstrate the depth level of tiles.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与其对每个像素进行单独排序以实现$\alpha$-混合，不如创建瓦片将2D屏幕划分为更小的16x16区域。每个高斯使用其相应的视空间深度和它们所在的瓦片
    ID 分配一个实例键。然后，高斯根据其实例键使用GPU基数排序进行排序。基数排序能够在GPU线程中并行处理大规模数据。高斯排序的结果还可以展示瓦片的深度级别。
- en: After sorting, a thread block is assigned to each tile, and the Gaussians are
    loaded into the corresponding memory. $\alpha$-blending is then performed from
    front to back on the sorted list of Gaussians onto the 2D scene, using the cumulative
    color and opacity $\alpha$ of the Gaussians until each pixel reaches a target
    alpha saturation. This design maximizes computation efficiency by enabling parallelism
    of both tile rendering and Gaussian loading in the shared memory space.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 排序后，将每个线程块分配给每个瓦片，高斯分布被加载到相应的内存中。然后，在已排序的高斯列表上进行从前到后的$\alpha$-混合，使用高斯的累计颜色和不透明度$\alpha$，直到每个像素达到目标
    alpha 饱和度。这种设计通过在共享内存空间中实现瓦片渲染和高斯加载的并行性，从而最大化计算效率。
- en: Algorithm 3 GPU software rasterization of 3D Gaussians
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 GPU 软件光栅化 3D 高斯分布
- en: '$w$, $h$: width and height of the image to rasterize'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '$w$, $h$: 需要光栅化的图像的宽度和高度'
- en: '$M$, $S$: Gaussian means and covariances in world space'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '$M$, $S$: 世界空间中的高斯均值和协方差'
- en: '$C$, $A$: Gaussian colors and opacities'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '$C$, $A$: 高斯颜色和不透明度'
- en: '$V$: view configuration of current camera'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '$V$: 当前摄像机的视图配置'
- en: function Rasterize($w$, $h$, $M$, $S$, $C$, $A$, $V$)     CullGaussian($p$,
    $V$) $\triangleright$ Frustum Culling     $M^{\prime},S^{\prime}$ $\leftarrow$
    ScreenspaceGaussians($M$, $S$, $V$) $\triangleright$ Transform     $T$ $\leftarrow$
    CreateTiles($w$, $h$)     $L$, $K$ $\leftarrow$ DuplicateWithKeys($M^{\prime}$,
    $T$) $\triangleright$ Indices and Keys     SortByKeys($K$, $L$) $\triangleright$
    Globally Sort     $R$ $\leftarrow$ IdentifyTileRanges($T$, $K$)     $I\leftarrow\mathbf{0}$
    $\triangleright$ Init Canvas     for all Tiles $t$ in $I$ do         for all Pixels
    $i$ in $t$ do              $r\leftarrow$ GetTileRange($R$, $t$)              $I[i]\leftarrow$
    BlendInOrder($i$, $L$, $r$, $K$, $M^{\prime}$, $S^{\prime}$, $C$, $A$)         end for     end forreturn
    $I$end function
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: function Rasterize($w$, $h$, $M$, $S$, $C$, $A$, $V$)     CullGaussian($p$,
    $V$) $\triangleright$ 裁剪     $M^{\prime},S^{\prime}$ $\leftarrow$ ScreenspaceGaussians($M$,
    $S$, $V$) $\triangleright$ 转换     $T$ $\leftarrow$ CreateTiles($w$, $h$)     $L$,
    $K$ $\leftarrow$ DuplicateWithKeys($M^{\prime}$, $T$) $\triangleright$ 索引和键     SortByKeys($K$,
    $L$) $\triangleright$ 全局排序     $R$ $\leftarrow$ IdentifyTileRanges($T$, $K$)     $I\leftarrow\mathbf{0}$
    $\triangleright$ 初始化画布     for all Tiles $t$ in $I$ do         for all Pixels
    $i$ in $t$ do              $r\leftarrow$ GetTileRange($R$, $t$)              $I[i]\leftarrow$
    BlendInOrder($i$, $L$, $r$, $K$, $M^{\prime}$, $S^{\prime}$, $C$, $A$)         end
    for     end forreturn $I$end function
- en: Limitations
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制
- en: Large memory bandwidth
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大内存带宽
- en: To achieve real-time rendering with high frames per second, a parallel computing
    approach is used in the $Rasterize()$ function. This involves a large amount of
    dynamic data loading occurring in the shared memory of each tile during the $\alpha$-blending
    process. Therefore, a large GPU memory bandwidth is required to support the data
    traffic.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高帧率的实时渲染，$Rasterize()$ 函数中使用了并行计算方法。这涉及到在每个瓦片的共享内存中发生大量的动态数据加载，在 $\alpha$-混合过程中。因此，需要较大的
    GPU 内存带宽来支持数据流量。
- en: Robustness in Sparse Viewpoints
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏视角下的鲁棒性
- en: Gaussians are optimized by taking the gradient compared with the true camera
    view. However, viewpoints with few or no data points have less data to optimize
    the Guassians in their region resulting in artifacts and distortions.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯通过与真实相机视图比较梯度进行优化。然而，数据点较少或没有数据点的视角在其区域优化高斯时的数据较少，从而导致伪影和失真。
- en: Future Trends
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来趋势
- en: Semantic-Driven 3D Reconstruction
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义驱动的 3D 重建
- en: 'Many 3D reconstruction techniques focus on generating 3D models from images.
    Yet, the integration of text prompts as a guiding factor presents an exciting
    avenue for future research. For example, the method outlined in the paper ”Semantic-Driven
    3D Reconstruction from Single Images”[[18](#bib.bib18)] demonstrates how textual
    cues can significantly enhance both the precision and contextual relevance of
    reconstructed models. While, ”LGM: Large Multi-View Gaussian Model for High-Resolution
    3D Content Creation” [[14](#bib.bib14)] demonstrate impressive zero-shot 3D generations
    from only a text prompt.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 3D 重建技术专注于从图像中生成 3D 模型。然而，文本提示作为指导因素的整合呈现出未来研究的激动人心的方向。例如，论文《从单张图像进行语义驱动的
    3D 重建》中[[18](#bib.bib18)]展示了文本提示如何显著提升重建模型的精度和上下文相关性。而《LGM：高分辨率 3D 内容创建的大型多视角高斯模型》[[14](#bib.bib14)]展示了仅通过文本提示就能进行令人印象深刻的零-shot
    3D 生成。
- en: Dynamic 3D scene reconstruction
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态 3D 场景重建
- en: The previously mentioned approaches can only use captured information in static
    scenes to reproduce static 3D models, where any structure change of scene during
    information capturing will result in misinformation that leads to under-reconstruction
    in specific areas. To achieve dynamic 3D scene reconstruction, 4D Gaussian Splatting
    utilizes a set of canonical 3D Gaussians and transforming them through a deformation
    field at different times, resulting in producing dynamically changing 3D models
    that can represent the motion of objects over time [[16](#bib.bib16)].
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法仅能利用静态场景中的捕获信息重建静态 3D 模型，其中信息捕获过程中场景的任何结构变化都会导致误信息，从而在特定区域下重建不足。为了实现动态 3D
    场景重建，4D 高斯喷溅利用一组典型的 3D 高斯，并通过在不同时间的变形场进行转换，从而生成能够表示物体随时间变化的动态 3D 模型[[16](#bib.bib16)]。
- en: Single View 3D reconstruction
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单视角 3D 重建
- en: Building on the methodology introduced in Zero 1-to-3, an area that has gained
    significant traction is Single View 3D reconstruction. Leveraging diffusion models
    to generate 3D objects from a single image, [[14](#bib.bib14)] and [[17](#bib.bib17)]
    have demonstrated promising work in this domain.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基于《Zero 1-to-3》中介绍的方法，一个获得显著关注的领域是单视角 3D 重建。利用扩散模型从单张图像生成 3D 对象，[[14](#bib.bib14)]
    和 [[17](#bib.bib17)] 在这一领域展示了有前景的工作。
- en: References
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models, 2020.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Jonathan Ho, Ajay Jain, 和 Pieter Abbeel. 去噪扩散概率模型，2020年。'
- en: '[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis.
    3d gaussian splatting for real-time radiance field rendering, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, 和 George Drettakis.
    实时辐射场渲染的 3D 高斯喷溅，2023年。'
- en: '[3] Agustinus Kristiadi. Kl divergence: Forward vs reverse? https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/,
    2016. Accessed: 2024-04-22.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Agustinus Kristiadi. KL 散度：前向与反向？ https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/，2016年。访问时间：2024年4月22日。'
- en: '[4] Hung-yi Lee. Forward process of ddpm, April 2023. https://www.youtube.com/watch?v=ifCDXFdeaaM&t=608.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hung-yi Lee. ddpm 的前向过程，2023年4月。 https://www.youtube.com/watch?v=ifCDXFdeaaM&t=608.'
- en: '[5] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov,
    and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov,
    和 Carl Vondrick. Zero-1-to-3：零-shot 一张图像到 3D 对象，2023年。'
- en: '[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
    Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes
    from images. ACM Trans. Graph., 38(4):65:1–65:14, July 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
    Lehrmann, 和 Yaser Sheikh。神经体积：从图像中学习动态可渲染体积。《ACM Trans. Graph.》，38(4):65:1–65:14，2019年7月。'
- en: '[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron,
    Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields
    for unconstrained photo collections, 2021.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron,
    Alexey Dosovitskiy, 和 Daniel Duckworth。Nerf in the wild: 用于无约束照片集的神经辐射场，2021年。'
- en: '[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and
    Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space,
    2019.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, 和
    Andreas Geiger。占据网络：在函数空间中学习3D重建，2019年。'
- en: '[9] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron,
    Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields
    for view synthesis, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron,
    Ravi Ramamoorthi, 和 Ren Ng。Nerf: 通过神经辐射场表示场景用于视图合成，2020年。'
- en: '[10] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant
    neural graphics primitives with a multiresolution hash encoding. ACM Transactions
    on Graphics, 41(4):1–15, July 2022.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Thomas Müller, Alex Evans, Christoph Schied, 和 Alexander Keller。使用多分辨率哈希编码的即时神经图形原语。《ACM
    Transactions on Graphics》，41(4):1–15，2022年7月。'
- en: '[11] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and
    Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape
    representation, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, 和 Steven
    Lovegrove。Deepsdf: 用于形状表示的连续签名距离函数学习，2019年。'
- en: '[12] Eric Penner and Li Zhang. Soft 3d reconstruction for view synthesis. 36(6),
    2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Eric Penner 和 Li Zhang。用于视图合成的软3D重建。36(6)，2017年。'
- en: '[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn
    Ommer。基于潜在扩散模型的高分辨率图像合成，2022年。'
- en: '[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and
    Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content
    creation. arXiv preprint arXiv:2402.05054, 2024.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, 和
    Ziwei Liu。Lgm: 用于高分辨率3D内容创建的大型多视角高斯模型。arXiv预印本 arXiv:2402.05054，2024年。'
- en: '[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller,
    Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki
    Takikawa, and Stan Birchfield. Rtmv: A ray-traced multi-view synthetic dataset
    for novel view synthesis, 2022.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller,
    Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki
    Takikawa, 和 Stan Birchfield。Rtmv: 用于新视图合成的光线追踪多视角合成数据集，2022年。'
- en: '[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei,
    Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic
    scene rendering, 2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei,
    Wenyu Liu, Qi Tian, 和 Xinggang Wang。4D高斯点云用于实时动态场景渲染，2023年。'
- en: '[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying
    Shan. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view
    large reconstruction models, 2024.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, 和 Ying
    Shan。Instantmesh: 从单张图像中高效生成3D网格，配合稀疏视图的大规模重建模型，2024年。'
- en: '[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, and Chi-Keung Tang. Facednerf:
    Semantics-driven face reconstruction, prompt editing and relighting with diffusion
    models, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, 和 Chi-Keung Tang。Facednerf:
    基于语义的面部重建、提示编辑和扩散模型下的重光照，2023年。'
- en: '[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable
    point-based radiance fields for efficient view synthesis, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, 和 Felix Heide。用于高效视图合成的可微分点云辐射场，2023年。'
