- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2407.08137] Abstract'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08137](https://ar5iv.labs.arxiv.org/html/2407.08137)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Survey on Fundamental Deep Learning 3D Reconstruction Techniques Yonge Bai¹,
    LikHang Wong², TszYin Twan²
  prefs: []
  type: TYPE_NORMAL
- en: ¹McMaster University
  prefs: []
  type: TYPE_NORMAL
- en: ²City University of Hong Kong
  prefs: []
  type: TYPE_NORMAL
- en: baiy58@mcmaster.ca, klhwong3-c@my.cityu.edu.hk, tytwan2-c@my.cityu.edu.hk July
    10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey aims to investigate fundamental deep learning (DL) based 3D reconstruction
    techniques that produce photo-realistic 3D models and scenes, highlighting Neural
    Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and 3D Gaussian Splatting.
    We dissect the underlying algorithms, evaluate their strengths and tradeoffs,
    and project future research trajectories in this rapidly evolving field. We provide
    a comprehensive overview of the fundamental in DL-driven 3D scene reconstruction,
    offering insights into their potential applications and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3D reconstruction is a process aimed at creating volumetric surfaces from image
    and/or video data. This area of research has gained immense traction in recent
    months and finds applications in numerous domains, including virtual reality,
    augmented reality, autonomous driving, and robotics. Deep learning has emerged
    to the forefront of 3D reconstruction techniques and has demonstrated impressive
    results enhancing realism and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Radiance Fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural Radiance Field (NeRF) is a method for novel view synthesis of complex
    scenes using a set of input perspectives and optimizes a model to approximate
    a continuous volumetric scene or surface[[9](#bib.bib9)]. The method represents
    the volume using a multilayer preceptron (MLP) whose input is a 5D vector $(x,y,z,\theta,\phi)$.
    $(x,y,z)$ representing the spatial location and $(\theta,\phi)$ representing the
    viewing direction, with an output of a 4D vector ($R,G,B,\sigma)$ representing
    the RGB color and a volume density. NeRFs achieved SOTA results on quantitative
    benchmarks as well as qualitative tests on neural rendering and view synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: Prior Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NeRFs build upon prior work in RGB-alpha volume rendering for view-synthesis
    and the use of neural networks (NN) as implicit continuous shape representations.
  prefs: []
  type: TYPE_NORMAL
- en: Volume Rendering for View-Synthesis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This process involves, using a set of images to learn a 3D discrete volume representation,
    the model estimate the volume density and emitted color at each point in the 3D
    space, which is then used to synthesize images from various viewpoints. Prior
    methods include Soft 3D, which implements a soft 3D representation of the scene
    by using traditional stereo methods, this representation is used directly to model
    ray visibility and occlusion during view-synthesis [[12](#bib.bib12)]. Along with
    deep learning methods such as Neural Volumes which uses a an encoder-decoder network
    that transforms the input images into a 3D voxel grid, used to generate new views
    [[6](#bib.bib6)]. While these volumetric representation are easy to optimize by
    being trained on how well they render the ground truth views, but as the resolution
    or complexity of the scene increases the compute and memory needed to store these
    discretized representations become unpractical.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks as Shape Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This field of study aims to implicitly represent the 3D surface with a NN’s
    weights. In contrast to the volumetric approach this representation encodes a
    description of a 3D surface at infinite resolution without excessive memory footprint
    as described here[[8](#bib.bib8)]. The NN encodes the 3D surface by learning to
    map a point in space to a property of that point in the 3D space, for example
    occupancy [[8](#bib.bib8)] or signed distance fields [[11](#bib.bib11)]. While
    this approach saves significant memory it is harder to optimize, leading to poor
    synthetic views compared to the discrete representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach: NeRF'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NeRFs combine these two approaches by representing the scene in the weights
    of an MLP but view synthesis is trained using the techniques in traditional volume
    rendering.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b471327a5afe49d6e657cab24050730.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of the neural radiance field scene representation and
    differentiable rendering procedure. Synthesize images by sampling 5D coordinates
    (location and viewing direction) along camera rays (a), feeding those locations
    into an MLP to predict a color and volume density (b), and using volume rendering
    techniques to composite these values into an image (c). This rendering function
    is differentiable, so we can optimize our scene representation by minimizing the
    residual between synthesized color and ground truth of the actual color(d).'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Radiance Field Scene Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The scene is represented by 5D vector comprised of $\mathbf{x}=(x,y,z)$ and
    $\mathbf{d}=(\theta,\phi)$. This continuous 5D scene representation is approximated
    by a MLP network $F_{\Theta}:(\mathbf{x},\mathbf{d})\to(\mathbf{c},\sigma)$, whose
    weights $\Theta$ are optimized to predict each 5D input’s $\mathbf{c}=(R,G,B)$
    representing RGB color and $\sigma$ representing density. Density can be thought
    of as occlusion, points with a high occlusion having a higher $\sigma$ value than
    points with lower occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implicit representation is held consist by forcing the network to predict
    $\sigma$ only as a function of $\mathbf{x}$, as density should not change as a
    result of viewing angle. While $\mathbf{c}$ is trained as a function of both $\mathbf{x}$
    and $\mathbf{d}$. The MLP $F_{\Theta}$ has 9 fully-connected layers using ReLU
    activation functions and 256 channels per layer for the first 8 layers and 128
    channels for the last layer. $F_{\Theta}$ first processes $\mathbf{x}$ with the
    first 8 layers outputting $\sigma$ and a 256-dimensional feature vector $\mathbf{v}$.
    $\mathbf{v}$ is then concatenated with $\mathbf{d}$ and passed into the final
    layer that outputs $\mathbf{c}$. This process is shown in Figure [2](#Sx4.F2 "Figure
    2 ‣ Neural Radiance Field Scene Representation ‣ Approach: NeRF").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53ff97fc2463572e853aa4342a577918.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the NeRF model. $\mathbf{x}$ is passed into the first
    8 layers, which output $\mathbf{v}$ and $\sigma$ a). $\mathbf{v}$ is concatenated
    with $\mathbf{d}$ and passed into the last layer, which outputs $\mathbf{c}$ b).'
  prefs: []
  type: TYPE_NORMAL
- en: Volume Rendering with Radiance Fields
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The color of any ray passing through the scene is rendered using principles
    from classical volume rendering.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{C}(\mathbf{r})=\sum_{i=1}^{N}w_{i}c_{i},\text{ where }w_{i}=T_{i}\alpha_{i}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Equation ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields ‣ Approach:
    NeRF")) can be explained as the color $c_{i}$ of each point being weighted by
    $w_{i}$. $w_{i}$ is made up of $T_{i}=\exp(-\sum_{j=1}^{i-1}\sigma_{i}\delta_{i})$
    where $\sigma_{i}$ is the density and $\delta_{i}$ is the distance between adjacently
    sampled points. $T_{i}$ denotes the accumulated transmittance until point $i$
    which can be thought of as the amount of light blocked earlier along the ray,
    and $\alpha_{i}=1-\exp(-\sigma_{i}\delta_{i})$ denoting the opacity at point $i$.
    Thus the color predicted at point with higher transmittance and opacity (the beginning
    of surfaces) contribute more to final predicted color of ray $\mathbf{r}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a NeRF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous sections covered the core components to NeRFs but the original
    paper had two more techniques to achieve SOTA quality—positional encoding and
    hierarchical volume sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The authors found that directly feeding in $(x,y,z,\theta,\phi)$ to $F_{\Theta}$
    resulted in poor performance. As a result, they chose to map the inputs to a higher
    dimensional space using high frequency functions, this enabled the model to better
    fit data with high variations. Thus $F_{\Theta}$ is reformulated as a composition
    of two functions $F_{\Theta}=F_{\Theta}^{\prime}\circ\gamma$. $F_{\Theta}^{\prime}$
    being the original MLP and $\gamma$ defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi
    x),&amp;\cos(2^{0}\pi x)\\ \vdots&amp;\vdots\\'
  prefs: []
  type: TYPE_NORMAL
- en: \sin(2^{L-1}\pi x),&amp;\cos(2^{L-1}\pi x)\end{array}\right)" display="block"><semantics
    ><mrow  ><mrow 
    ><mi  >γ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false" 
    >(</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >=</mo><mrow 
    ><mo  >(</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd  ><mrow
     ><mrow 
    ><mi  >sin</mi><mo
     >⁡</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow
     ><msup
     ><mn
     >2</mn><mn
     >0</mn></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >π</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >x</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
     ><mrow 
    ><mi  >cos</mi><mo
     >⁡</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow  ><msup
     ><mn 
    >2</mn><mn 
    >0</mn></msup><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >π</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >x</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
     ><mtd  ><mi
    mathvariant="normal"  >⋮</mi></mtd><mtd
     ><mi mathvariant="normal" 
    >⋮</mi></mtd></mtr><mtr  ><mtd
     ><mrow 
    ><mrow  ><mi
     >sin</mi><mo 
    >⁡</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><msup 
    ><mn 
    >2</mn><mrow 
    ><mi 
    >L</mi><mo 
    >−</mo><mn 
    >1</mn></mrow></msup><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >π</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >x</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
     ><mrow 
    ><mi  >cos</mi><mo
     >⁡</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow  ><msup
     ><mn 
    >2</mn><mrow 
    ><mi 
    >L</mi><mo 
    >−</mo><mn 
    >1</mn></mrow></msup><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >π</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >x</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><mo
     >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    >𝛾</ci><ci  >𝑥</ci></apply><matrix
     ><matrixrow 
    ><apply  ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><cn
    type="integer"  >0</cn></apply><ci
     >𝜋</ci><ci
     >𝑥</ci></apply></apply><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><cn type="integer" 
    >2</cn><cn type="integer" 
    >0</cn></apply><ci 
    >𝜋</ci><ci 
    >𝑥</ci></apply></apply></matrixrow><matrixrow
     ><ci 
    >⋮</ci><ci  >⋮</ci></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><cn type="integer"
     >2</cn><apply
     ><ci
     >𝐿</ci><cn
    type="integer"  >1</cn></apply></apply><ci
     >𝜋</ci><ci
     >𝑥</ci></apply></apply><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><cn type="integer" 
    >2</cn><apply 
    ><ci 
    >𝐿</ci><cn type="integer" 
    >1</cn></apply></apply><ci 
    >𝜋</ci><ci 
    >𝑥</ci></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi
    x),&\cos(2^{0}\pi x)\\ \vdots&\vdots\\ \sin(2^{L-1}\pi x),&\cos(2^{L-1}\pi x)\end{array}\right)</annotation></semantics></math>
    |  | (2) |
  prefs: []
  type: TYPE_NORMAL
- en: $\gamma(\cdot)$ is applied to $(x,y,z)$ in $\mathbf{x}$ with $L=10$ and $(\theta,\phi)$
    with $L=4$.
  prefs: []
  type: TYPE_NORMAL
- en: '$\gamma$ is a mapping from $\mathbb{R}$ to $\mathbb{R}^{2L}$ that significantly
    improves performance (Figure [3](#Sx4.F3 "Figure 3 ‣ Positional Encoding ‣ Optimizing
    a NeRF ‣ Approach: NeRF")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86cbd6695e1b64342591a54b9e1106a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Visualizing how the model improves the positional encoding. Without
    it the model is unable to represent high variation geometries and textures resulting
    in an over smoothed, blurred appearance. Also how removing view dependency affect
    the models ability to render lighting and reflections.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Volume Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Free space and occluded region contribute much less to the quality of the NeRF
    compared to areas at the beginning of a surface, but with uniform sampling, are
    sampled at the same rate. So the authors proposed a hierarchical representation
    that increases rendering efficiency and quality by allocating samples proportional
    to their expected effect shown in [4](#Sx4.F4 "Figure 4 ‣ Hierarchical Volume
    Sampling ‣ Optimizing a NeRF ‣ Approach: NeRF"). For example, if the object in
    question was a ball, there would be less samples taken in the open space in front
    of the ball and inside of the ball verses samples directly on the ball’s surface.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f621d39468278022b3ae5b5656b63a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustrating hierarchical sampling, where samples are proportional
    to their contribution to the final volume render.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by optimizing two networks. One ”coarse” and one ”fine”. The course
    network samples points uniformly along the ray, while the fine network is biases
    toward the relevant part of the volume by normalizing the per sample weights $w_{i}$
    described in equation ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields ‣
    Approach: NeRF")), this allows one to treat the weight of each point as a probability
    distribution which is sampled to train the fine network. This procedure allocates
    more samples to regions expected to contain visible content.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While having groundbreaking abilities to render photorealistic 3D volumes from
    2D images, the original NeRF methodology suffered from several limitations. These
    limitations include:'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The optimization of a single scene took 100-300k iterations to converge of a
    single NVIDIA V100 GPU which corresponded to 1-2 days [[9](#bib.bib9)]. This poor
    computational efficiency is a product of dense sampling of rays for rendering.
    This dense sampling approach helped in capturing fine details and accurately representing
    complex scenes, but it significantly increases the computational load.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of Generalizability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NeRFs are inheritably inflexible due to the as models overfit to one scene.
    A NeRF cannot be adapted for novel scenes without complete retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Difficulty of Editing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modifying content in NeRFs such as moving or removing object is very difficult.
    Since the model represents the scene as a continuous function and does not store
    geometric information.
  prefs: []
  type: TYPE_NORMAL
- en: Data requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NeRFs require a lot of data to produce high quality results show in the original
    paper. The synthetic 3D models as lego bulldozer and pirate ship took about 100
    image and the real life scenes such as the flower and conference room each requiring
    around 60 [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: Transient Artifacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original NeRFs assume that the world is geometrically, materially, and
    photometrically static. Therefore requiring that any two photographs taken at
    the same position and orientation must be identical [[7](#bib.bib7)] they do not
    have a way to adjust for transient occlusions or variable appearance which result
    in artifacts and noise when this assumption fails such as with real world images.
    This is clearly show in [5](#Sx4.F5 "Figure 5 ‣ Transient Artifacts ‣ Approach:
    NeRF").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20240280a95fa7d3cff863d3a09daa40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison made in the paper NeRF in the Wild [[7](#bib.bib7)], where
    the original NeRF (left) noisy artifacts compared to NeRF-W (right).'
  prefs: []
  type: TYPE_NORMAL
- en: Instant-NGP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instant-NGP[[10](#bib.bib10)], proposed by Nvlabs, is a method that significantly
    reduce the computation demand of original NeRFs. It leverages multi-resolution
    hash grids to improve memory usage and optimizes 3D reconstruction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Prior work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learnable positional encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Learnable positional encoding refers to positional encodings that are parameterized
    for specific positions in a continuous 3D space. The positional encoding for a
    point $p$ in 3D space can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{pe}(p)=\sigma(\mathbf{W}\mathbf{p}+\mathbf{b})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{p}=(x,y,z)^{T}$ represents the coordinates of the position in
    3D space, $\mathbf{W}$ is a learnable weight matrix, $\mathbf{b}$ is a bias vector,
    and $\sigma$ denotes a non-linear activation function.
  prefs: []
  type: TYPE_NORMAL
- en: These positional encodings can then be integrated into a neural network model
    to facilitate the learning of spatial relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e51d7086583d44a5682ce23d98b831d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Experiment performed in the original paper. as the number of parameters
    used for learning the positional encoding increases, the image becomes clearer
    and sharper.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-Resolution Hash Encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e7c325913acb7910757dacfbc443679.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of the multiresolution hash encoding represented in
    2D in the original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the key components of Instant-NGP (Instant Neural Graphics Primitives)
    is the Multi-Resolution Hash Encoding. Instead of learning the positional encoding
    for the entire 3D space, the 3D space is first scaled to fit within a normalized
    range of 0 to 1\. This normalized space is then replicated across multiple resolutions
    and each subdivided into grids of varying densities. This captures both coarse
    and fine details in the scene. Each level focuses on learning the positional encodings
    at the vertices of the grids. Mathematically, this can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{p}_{scaled}=\mathbf{p}\cdot\mathbf{s}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{p}$ represents the original coordinates in 3D space, and $\mathbf{s}$
    is a scaling factor that normalizes the space to the [0, 1] range. Following the
    scaling, the coordinates are hashed into a multi-resolution structure using a
    spatial hash function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h(\mathbf{x})=\left(\bigoplus_{i=1}^{d}(x_{i}\cdot\pi_{i})\right)\mod
    T$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $d$ is the dimensionality of the space (e.g., 3 for 3D coordinates), $\mathbf{x}=(x_{1},x_{2},\dots,x_{d})$
    represents the scaled coordinates, $\bigoplus$ denotes the bit-wise XOR operation,
    $\pi_{i}$ are large prime numbers unique to each dimension, and $T$ is the size
    of the hash table. This function maps spatial coordinates to indices in the hash
    table, where the neural network’s parameters are stored or retrieved, linking
    specific spatial locations to neural network parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, a hash table is assigned to each level of resolution. For each
    resolution, each vertex is mapped to a entry in the resolution’s hash table. Higher-resolution
    have larger hash tables compared to lower-resolutions. Every resolution’s hash
    table map each of it’s vertices to an individual set of parameters that learn
    their positional encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Learning positional encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'During training, when the model is exposed to images from different viewpoints,
    the NN adjusts the parameters stored in the hash table to minimize the difference
    between the rendered images and the actual training images. The loss $L$ can be
    expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L(\theta)=\frac{1}{2}\sum_{i=1}^{m}(R(x_{i},\theta)-y_{i})^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $R(x_{i},\theta)$ is the rendered image based on parameters $\theta$ and
    view point $x_{i}$, $y_{i}$ is the corresponding actual image, and $m$ is the
    number of pixels or data points considered. We can then learn these parameters
    using different optimization techniques like gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Hash Collisions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hash collisions are avoided by assigning a hash table at each resolution that
    long enough to ensure one-to-one mapping from entries to positional encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in figure [8](#Sx5.F8 "Figure 8 ‣ Performance ‣ Instant-NGP"), Instant-NGP
    achieved a notable 20-60× speed improvement compared to compared to the original
    NeRFs while maintaining it’s quality.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/416d3486eaa17a75360a764b027deedf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The figure adapted from the original paper compares the Peak Signal
    to Noise Ratio (PSNR) performance of various NeRF implementations, including the
    author’s multi-resolution hash encoding method, against other models that require
    hours of training. First row is the name of the object constructed.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instant-NGP focuses on speeding up the computation and training processes of
    NeRFs. However, it still suffers from many of the same issues such as generalability
    different datasets or unseen scenarios. In the next section, we introduce LDM
    based techniques for 3D reconstruction to address the issue of generalizable.
  prefs: []
  type: TYPE_NORMAL
- en: Latent-Diffusion-Model based 3D reconstruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional 3D-Reconstruction algorithms rely heavily on the training data to
    capture all aspects of the volume. Humans, however, are able to estimate a 3D
    surface from a single image. This concept is the foundation of the Zero-1-to-3[[5](#bib.bib5)]
    framework developed out of Columbia University which introduces a diffusion-based
    3D reconstruction method. Zero-1-to-3 utilizes a LDM, originally designed for
    text-conditioned image generation, to generate new perspectives of an image based
    on a camera’s extrinsic parameters like rotation and translation. Zero-1-to-3
    leverages the geometric priors learned by large-scale LDMs, allowing the generation
    of novel views from a single image. Zero-1-to-3 demonstrates strong zero-shot
    generalization capabilities, outperforming prior models in both single-view 3D
    reconstruction and novel view synthesis tasks. See Figure [9](#Sx6.F9 "Figure
    9 ‣ Background ‣ Latent-Diffusion-Model based 3D reconstruction").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a29a1d797ac0d6fbaaea703e935eeb8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: High Level Picture of Zero-1-to-3 from the original paper'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior Work: Denoising Diffusion Probabilistic Models'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Denoising Diffusion Probabilistic Models (DDPMs)[[1](#bib.bib1)] are a class
    of generative models that transform data by gradually adding noise over a sequence
    of steps, then learning to reverse this process to generate new samples from noise.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The forward process in DDPM is a Markov chain that gradually adds Gaussian
    noise to the data over $T$ timesteps. The process can be mathematically described
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t}=\sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon,\quad\epsilon\sim\mathcal{N}(0,I)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $x_{0}$ is the original data, $x_{t}$ is the data at timestep $t$, $\epsilon$
    is the noise, and $\alpha_{t}$ is the variance schedule parameters that determine
    how much noise is added at each step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fec1d74bc4a2d2fcbdbb096e79d0b5a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Forward Process of DDPM. Adapted from [[4](#bib.bib4)].'
  prefs: []
  type: TYPE_NORMAL
- en: Reverse Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The reverse process aims to reconstruct the original data from the noise by
    learning a parameterized model $p_{\theta}$. The reverse process is also a Markov
    chain, described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon_{\theta}(x_{t},t)$ is a neural network predicting the noise,
    $\sigma_{t}$ is the standard deviation of the reverse process noise, and $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$
    is the cumulative product of the $\alpha_{t}$ values.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training of DDPMs involves optimizing the parameters $\theta$ of the neural
    network to minimize the difference between the noise predicted by the model and
    the actual noise added during the forward process. The loss function is typically
    the mean squared error between these two noise terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L(\theta)=\mathbb{E}_{t,x_{0},\epsilon}\left[\&#124;\epsilon-\epsilon_{\theta}(x_{t},t)\&#124;_{2}^{2}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $x_{t}$ is computed during the forward process and $\epsilon$ is the Gaussian
    noise added at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To generate new samples, the reverse process is initialized with pure noise
    $x_{T}\sim\mathcal{N}(0,I)$ and iteratively applies the reverse steps to produce
    samples approximating the distribution of the original data $x_{0}$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 DDPM Sampling Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:procedure DdpmSampling($\theta,T,\{\alpha_{t}\}$)2:     Input: Trained model
    parameters $\theta$, total timesteps $T$, noise schedule $\{\alpha_{t}\}$3:     Output:
    A sample approximating the data distribution4:     Initialize: Draw $x_{T}\sim\mathcal{N}(0,I)$
    Start with pure noise5:     for $t=T$ down to 1 do6:         Calculate $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$7:         Calculate
    $\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\cdot(1-\alpha_{t})$8:         Predict
    noise $\epsilon_{t}=\epsilon_{\theta}(x_{t},t)$9:         if $t>1$ then10:              $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$11:         else12:              $x_{0}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)$
    Final denoising step13:         end if14:     end for15:     Return $x_{0}$16:end procedure![Refer
    to caption](img/fde45972e6e7c59a5b529e8472b4cadb.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Sampling Process. Adapted from [[4](#bib.bib4)].'
  prefs: []
  type: TYPE_NORMAL
- en: Latent Diffusion Model in Zero-1-to-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Latent Diffusion Models[[13](#bib.bib13)] proposed in 2021 are a type of generative
    model that combines the strengths of diffusion models and Variationsal Autoencoders(VAEs).
    Traditional DDPMs operates in the image pixel space, which requires more computation.
    LDMs compress the full image data space into a latent space before the diffusion
    and denosing process, improving efficiency and scalability in generating high-quality
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd190672eaf100bdf9ac10b48d174cdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The architecture of Latent Diffusion Model in the original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Latent Diffusion Models $\epsilon_{\theta}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LDM is trained in two main stages. First, a VAE is used to learn an encoding
    function $E(x)$ and a decoding function $D(z)$, where $x$ represents the high-resolution
    image and $z$ is it’s latent representation. The encoder compresses $x$ to $z$,
    and the decoder attempts to reconstruct $x$ from $z$.
  prefs: []
  type: TYPE_NORMAL
- en: Training VAE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The VAE optimizes the parameters $\phi$ (encoder) and $\psi$ (decoder) by minimizing
    the reconstruction loss combined with the KL-divergence loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65b3721fa5e277712537d9c30756131b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Training VAE. Figure from Lightning AI'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{VAE}(\phi,\psi)=\mathbb{E}_{q_{\phi}(z&#124;x)}[\log p_{\psi}(x&#124;z)]-D_{KL}(q_{\phi}(z&#124;x)\&#124;p(z))$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Training Attention-U-Net Denoiser
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the second stage, an Attention-U-Net is trained as the denoising model in
    the latent space. This model learns a sequence of denoising steps that transform
    a sample from a noise distribution $p(z_{T})$ to the data distribution $p(z_{0})$
    over T timesteps. The U-Net model parameter $\theta$ are optimized by minimizing
    the expected reverse KL-divergence between the true data distribution and the
    model distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\theta)=\mathbb{E}_{z_{0},\epsilon\sim\mathcal{N}(0,I),t}\left[\&#124;\epsilon-\epsilon_{\theta}(z_{t},t)\&#124;^{2}\right]$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $z_{t}=\sqrt{\bar{\alpha}_{t}}z_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon$,
    and $\bar{\alpha}_{t}$ is the variance schedule. We use KL-divergence to calculate
    ”how different” the true data distribution and the model distribution are. We
    aim to make the latent space distribution of the model similar to that of the
    real world. This is the key to generating realistic images. Figure [14](#Sx6.F14
    "Figure 14 ‣ Training Attention-U-Net Denoiser ‣ Latent Diffusion Model in Zero-1-to-3
    ‣ Latent-Diffusion-Model based 3D reconstruction") shows example of minimizing
    two distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/460481baf7eb06b300b1e91a577e3c98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: On the left (bad) example, the difference between $Q(x)$ distribution
    and $P(x)$ is not minimized. On the right (good) example, the difference between
    $Q(x)$ distribution and $P(x)$ is minimized. (from [[3](#bib.bib3)])'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7359350a51c2ef90b1274d4b52bdb839.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Attention-U-Net Architecture in the original paper'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5dfa0e92ea394081b28abc82f375d70d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Training Attention-U-Net Denoiser. Figure from Lightning AI'
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning on Camera Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The third stage in the Zero-1-to-3 framework focuses on the conditioning of
    the LDM based on camera extrinsic parameters such as rotation ($R$) and translation
    ($t$). This conditioning is critical for generating novel views of the object,
    which are essential for effective 3D reconstruction from a single image.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanism of Conditioning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this stage, the previously trained latent representations are manipulated
    according to the desired camera transformations to simulate new perspectives.
    This process involves:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Camera Transformations: Adjusting the latent variables $z$ to reflect changes
    in viewpoint due to different rotations $R$ and translations $t$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformation Implementation: This could be achieved either through a learned
    transformation model within the LDM framework or by applying predefined transformation
    matrices directly to the latent vectors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training the Model for Conditional Output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The model is further trained to handle conditional outputs effectively, which
    involves:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Preparation: The official code used the RTMV dataset[[15](#bib.bib15)]
    where objects are captured from multiple viewpoints to pair latent representations
    with corresponding camera parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Adaptation: Extending the latent diffusion model training to not only
    generate images from the latent representation $z$ but also new perspectives of
    the images from it’s transformed latent representation $z^{\prime}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MSE Loss: We compute the MSE between the output image and real image with respect
    to $R$ and $t$. A ”near-view consistency loss” that calculate the MSE between
    the image rendered from a view and the image rendered from a nearby view is also
    used to maintain the consistency in 3D reconstruction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Novel View generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To generate a novel view, the following transformation is applied to the latent
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z^{\prime}=f(z,R,t)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Where $f$ is the transformation function that modifies $z$ based on $R$ and
    $t$. The Zero-1-to-3 model generates the novel view as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x^{\prime}=D(\epsilon_{\theta}(z^{\prime}))$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Where $x^{\prime}$ represents the image generated from the new perspective,
    and $D$ is the decoder part of the LDM that synthesizes the final image output
    from the transformed latent representation $z^{\prime}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3D reconstruction $\epsilon_{\theta}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 3D reconstruction is performed by first generating multiple views of the
    object using the above method for various $R$ and $t$ matrices. Each generated
    image $x^{\prime}$ provides a different perspective of the object. These images
    are then used to reconstruct the 3D model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{3D Model}=\text{Integrate}(\{x^{\prime}\})$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: The integration process typically involves techniques like volumetric fusion
    or multi-view stereo algorithms, which consolidate the information from different
    images to create a detailed 3D representation of the object, as shown in figure
    [17](#Sx6.F17 "Figure 17 ‣ 3D reconstruction ϵ_𝜃 ‣ Latent Diffusion Model in Zero-1-to-3
    ‣ Latent-Diffusion-Model based 3D reconstruction").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a7baef9ae0cc4e9550967d80125c530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Demo of 3D reconstruction from Zero-1-to-3'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations for diffusion-based and NeRF-based 3D reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flexibility and Real-Time 3D scene Rendering: Training the Zero-1-to-3 model
    for 3D scenes reconstruction typically require iterative denoising processes during
    sampling, which can be computationally intensive and slow. This makes them less
    suitable for applications requiring real-time rendering.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implicit Representation Ambiguity: Both NeRF and Diffusion models represent
    the 3D object implicitly; they do not explicitly construct the 3D space. NeRFs
    utilize the weights of an MLP to represent a 3D scene and LDMs use the latent
    space for new perspectives generation for 3D reconstruction. While implicit representation
    saves significant space it may lead to ambiguities in interpreting the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Approach: 3D Gaussian Splatting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout the evolution of 3D scene reconstruction, explicit representations
    such as meshes and point clouds have always been preferred by developers and researchers
    due to their clearly defined structure, fast rendering, and ease of editing. NeRF-based
    methods have shifted towards continuous representation of 3D scenes. While the
    continuous nature of these methods helps optimization, the stochastic sampling
    required for rendering is costly and can result in noise [[2](#bib.bib2)]. On
    top of that the implicit representation’s lack of geometric information does not
    lend itself well to editing.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3D Gaussian Splatting provides a high-quality novel view with real-time rendering
    for 3D scenes, these are achieved with the utilization of the Gaussian function
    to present a smooth and accurate texture using captured photos of a scene.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4233b37fbc835e72cbe140d357cd30d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Overview of 3D Gaussian Splatting process.'
  prefs: []
  type: TYPE_NORMAL
- en: To reconstruct a 3D model using 3D Gaussian splatting, first, a video of the
    object is taken from different angles, then converted into frames representing
    static scenes at different camera angles. Structure from Motion (SfM) with feature
    detection and matching techniques such as SIFT is then applied to these images
    to produce a sparse point cloud. The 3D data points of the point cloud are then
    represented by 3D Gaussians. With the optimization process, adaptive density control
    of Gaussians, and high-efficiency algorithm design, realistic views of the 3D
    model can be reconstructed with a high frame rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm of 3D Gaussian Splatting is demonstrated below, where it can
    be slit into 3 parts: initialization, optimization, and density control of Gaussians.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithm 2 Optimization and Densification
  prefs: []
  type: TYPE_NORMAL
- en: '$w$, $h$: width and height of the training images'
  prefs: []
  type: TYPE_NORMAL
- en: '$M\leftarrow$ SfM Points $\triangleright$ Positions$i\leftarrow 0$ $\triangleright$
    Iteration Countwhile not converged do     $V,\hat{I}\leftarrow$ SampleTrainingView()
    $\triangleright$ Camera $V$ and Image     $I\leftarrow$ Rasterize($M$, $S$, $C$,
    $A$, $V$) $\triangleright$ Alg. [3](#alg3 "Algorithm 3 ‣ Tile-based rasterizer
    for real-time rendering ‣ Approach: 3D Gaussian Splatting")     $L\leftarrow Loss(I,\hat{I})$
    $\triangleright$ Loss     $M$, $S$, $C$, $A$ $\leftarrow$ Adam($\nabla L$) $\triangleright$
    Backprop & Step     if IsRefinementIteration($i$) then         for all Gaussians
    $(\mu,\Sigma,c,\alpha)$ in $(M,S,C,A)$ do              if $\alpha<\epsilon$ or
    IsTooLarge($\mu,\Sigma)$ then $\triangleright$ Pruning                  RemoveGaussian()              end if              if $\nabla_{p}L>\tau_{p}$ then
    $\triangleright$ Densification                  if $\|S\|>\tau_{S}$ then $\triangleright$
    Over-reconstruction                       SplitGaussian($\mu,\Sigma,c,\alpha$)                  else$\triangleright$
    Under-reconstruction                       CloneGaussian($\mu,\Sigma,c,\alpha$)                  end if              end if         end for     end if     $i\leftarrow
    i+1$end while'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Points in the sparse 3D data point cloud generated by SfM, are initialized
    to 3D Gaussians. The Gaussians are defined by the following variables: position
    $p$, world space 3D covariance matrix $\Sigma$, opacity $\alpha$, and spherical
    harmonics coefficient c (representation of RBG color), given formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $G(x)~{}=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Initially, the Gaussians are sparse and not representative, but they are gradually
    optimized to better represent the scene. To do this, a random camera view $V$
    with it’s corresponding image $\hat{I}$ is chosen from the training set. A rasterized
    Gaussian image $I$ is generated by passing the Gaussian means, $\Sigma$, $c$,
    $\alpha$, and $V$ to a differentiable rasterizer function $Rasterize()$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A loss function, shown in equation ([9](#Sx7.E9 "In Optimization ‣ Algorithm
    ‣ Approach: 3D Gaussian Splatting")), is used to compute the gradients of the
    two images $\hat{I}$ and $I$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=(1-\lambda)\mathcal{L}_{1}+\lambda\mathcal{L_{\textrm{D-SSIM}}}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathcal{L}_{1}$ is the Mean Absolute Error of $\hat{I}$ and $I$, $\mathcal{L}_{D-SSMI}$
    is the Difference Structural Similarity Index based loss, and $\lambda$ is a pre-defined
    weight that adjusts the contribution of $\mathcal{L}_{1}$ and $\mathcal{L}_{D-SSMI}$
    to the final loss $\mathcal{L}$. The parameters of the Gaussians are adjusted
    accordingly with the Adam optimizer [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive control of Gaussians
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After initialization, an adaptive approach is used to control the number and
    density of Guassians. Adaptive control refers to automatically adjusting the size
    and number of Gaussians to optimize the representation of the static 3D scene.
    The adaptive density control follows the following behaviors (see Fig. 12):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gaussian Removal: For every 100 iterations, if the Gaussians are too large
    in the 3D space or have opacity under a defined threshold of opacity $\epsilon_{\alpha}$(essentially
    transparent), they are removed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gaussian Duplication: For regions filled by gaussians that are greater than
    the defined threshold but is too small, the Gaussians are cloned and moved along
    their direction to cover the empty space. This behavior adaptively and gradually
    increases the number and volume of the Gaussians until the area is well-fitted.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gaussian Split: For regions that are over-reconstructed by large Gaussians
    (variance is too high), they are split into smaller Gaussians by a factor $\phi$,
    the original paper used $\phi=1.6$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4239a49b90764d069b404d1913e97e72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Gaussians split in over-reconstructed areas while clone in under-reconstructed
    areas.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the splitting and duplicating, the number of Gaussians increases. To
    address this, every Gaussians’ opacity $\alpha$ close to zero every $N=3000$ iteration,
    after the optimization step then increases the $\alpha$ values for the Guassian
    where this is needed while allowing the unused ones to be removed.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient computation of $\Sigma$ and $\Sigma^{\prime}$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each Gaussian is represented as an ellipsoid in the 3D space, modelled by a
    covariance matrices $\Sigma$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Sigma=RSS^{T}R^{T}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Where S is the scaling matrix and R is the rotation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: For each camera angle, 3D Gaussians are rasterized to the 2D screen. The 3D
    covariance matrix $\Sigma^{\prime}$ is derived using the viewing transformation
    matrix $W$ and Jacobian $J$, which approximates the projective transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Sigma^{\prime}=JW{\Sigma}W^{T}J^{T}$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: $\Sigma^{\prime}$, a 3x3 matrix, can be simplified by ignoring the elements
    in the third row and column while retaining the properties of their corresponding
    planar points representation [[19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the gradient of the 3D space covariance, the chain rule is applied
    to $\Sigma$ and $\Sigma^{\prime}$ with reference to their rotation $q$ and scaling
    $s$. This results in the expressions $\frac{d\Sigma^{\prime}}{ds}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{ds}$
    and $\frac{d\Sigma^{\prime}}{dq}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{dq}$.
    By substituting $U=JW$ into the equation for $\Sigma^{\prime}$, we get ${\Sigma^{\prime}}=U\Sigma
    U^{T}$. This equation allows the partial derivative of each element in $\frac{d\Sigma^{\prime}}{d\Sigma}$
    to be represented in terms of $U$:'
  prefs: []
  type: TYPE_NORMAL
- en: <math   alttext="\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&amp;U_{1,i}U_{2,j}\\
  prefs: []
  type: TYPE_NORMAL
- en: U_{1,j}U_{2,i}&amp;U_{2,i}U_{2,j}\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mfrac
     ><mrow 
    ><mo rspace="0em" 
    >∂</mo><msup 
    ><mi mathvariant="normal" 
    >Σ</mi><mo 
    >′</mo></msup></mrow><mrow 
    ><mo rspace="0em" 
    >∂</mo><msub 
    ><mi mathvariant="normal" 
    >Σ</mi><mrow 
    ><mi 
    >i</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >j</mi></mrow></msub></mrow></mfrac><mo
     >=</mo><mrow
     ><mo 
    >(</mo><mtable columnspacing="5pt" rowspacing="0pt"
     ><mtr 
    ><mtd  ><mrow
     ><msub 
    ><mi 
    >U</mi><mrow 
    ><mn 
    >1</mn><mo 
    >,</mo><mi 
    >i</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >U</mi><mrow
     ><mn
     >1</mn><mo
     >,</mo><mi
     >j</mi></mrow></msub></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >U</mi><mrow 
    ><mn 
    >1</mn><mo 
    >,</mo><mi 
    >i</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >U</mi><mrow
     ><mn
     >2</mn><mo
     >,</mo><mi
     >j</mi></mrow></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >U</mi><mrow
     ><mn
     >1</mn><mo
     >,</mo><mi
     >j</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >U</mi><mrow
     ><mn
     >2</mn><mo
     >,</mo><mi
     >i</mi></mrow></msub></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >U</mi><mrow 
    ><mn 
    >2</mn><mo 
    >,</mo><mi 
    >i</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >U</mi><mrow
     ><mn
     >2</mn><mo
     >,</mo><mi
     >j</mi></mrow></msub></mrow></mtd></mtr></mtable><mo
     >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >Σ</ci><ci
     >′</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >Σ</ci><apply
     ><ci
     >𝑖</ci><ci
     >𝑗</ci></apply></apply></apply></apply><matrix
     ><matrixrow
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑈</ci><list 
    ><cn type="integer" 
    >1</cn><ci 
    >𝑖</ci></list></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑈</ci><list 
    ><cn type="integer" 
    >1</cn><ci 
    >𝑗</ci></list></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑈</ci><list 
    ><cn type="integer" 
    >1</cn><ci 
    >𝑖</ci></list></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑈</ci><list 
    ><cn type="integer" 
    >2</cn><ci 
    >𝑗</ci></list></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑈</ci><list 
    ><cn type="integer" 
    >1</cn><ci 
    >𝑗</ci></list></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑈</ci><list 
    ><cn type="integer" 
    >2</cn><ci 
    >𝑖</ci></list></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑈</ci><list
     ><cn
    type="integer"  >2</cn><ci
     >𝑖</ci></list></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑈</ci><list
     ><cn
    type="integer"  >2</cn><ci
     >𝑗</ci></list></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&U_{1,i}U_{2,j}\\
    U_{1,j}U_{2,i}&U_{2,i}U_{2,j}\end{smallmatrix}\right)</annotation></semantics></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'By substituting $M=RS$ into equation ([10](#Sx7.E10 "In Gradient computation
    of Σ and Σ'' ‣ Approach: 3D Gaussian Splatting")), it can then be rewritten as
    $\Sigma=MM^{T}$. Using the chain rule, we can derive $\frac{d\Sigma}{ds}=\frac{d\Sigma}{dM}\frac{dM}{ds}$
    and $\frac{d\Sigma}{dq}=\frac{d\Sigma}{dM}\frac{dM}{dq}$. This allows us to calculate
    the scaling gradient at position $(i,j)$ of the covariance matrix with:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial M_{i,j}}{\partial s_{k}}=\left\{\begin{array}[]{lr}R_{i,k}&amp;\text{if
    }j=k\\ 0&amp;\text{otherwise}\end{array}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'For defining derivatives of $M$ with respect to rotation matrix $R$ in terms
    of quaternion $q$ components, the following formula demonstrating how quaternion
    components affect $R$ is involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&amp;(q_{i}q_{j}-q_{r}q_{k})&amp;(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&amp;\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&amp;(q_{j}q_{k}-q_{r}q_{i})\\'
  prefs: []
  type: TYPE_NORMAL
- en: (q_{i}q_{k}-q_{r}q_{j})&amp;(q_{j}q_{k}+q_{r}q_{i})&amp;\frac{1}{2}-(q_{i}^{2}+q_{j}^{2})\end{pmatrix}"
    display="block"><semantics ><mrow  ><mrow
     ><mi 
    >R</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi
     >q</mi><mo stretchy="false" 
    >)</mo></mrow></mrow><mo  >=</mo><mrow
     ><mn 
    >2</mn><mo lspace="0em" rspace="0em" 
    >​</mo><mrow  ><mo
     >(</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd 
    ><mrow  ><mstyle
    displaystyle="false"  ><mfrac
     ><mn
     >1</mn><mn
     >2</mn></mfrac></mstyle><mo
     >−</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><msubsup
     ><mi
     >q</mi><mi
     >j</mi><mn
     >2</mn></msubsup><mo
     >+</mo><msubsup
     ><mi
     >q</mi><mi
     >k</mi><mn
     >2</mn></msubsup></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
     ><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><mrow 
    ><msub 
    ><mi 
    >q</mi><mi 
    >i</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow><mo
     >−</mo><mrow
     ><msub
     ><mi
     >q</mi><mi
     >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd
     ><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><mrow 
    ><msub 
    ><mi 
    >q</mi><mi 
    >i</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >q</mi><mi
     >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><mo
    stretchy="false"  >(</mo><mrow
     ><mrow
     ><msub
     ><mi
     >q</mi><mi
     >i</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >q</mi><mi
     >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd
     ><mrow 
    ><mstyle displaystyle="false" 
    ><mfrac 
    ><mn 
    >1</mn><mn 
    >2</mn></mfrac></mstyle><mo 
    >−</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><msubsup 
    ><mi 
    >q</mi><mi 
    >i</mi><mn 
    >2</mn></msubsup><mo 
    >+</mo><msubsup 
    ><mi 
    >q</mi><mi 
    >k</mi><mn 
    >2</mn></msubsup></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mtd><mtd
     ><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><mrow 
    ><msub 
    ><mi 
    >q</mi><mi 
    >j</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow><mo
     >−</mo><mrow
     ><msub
     ><mi
     >q</mi><mi
     >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><mo
    stretchy="false"  >(</mo><mrow
     ><mrow
     ><msub
     ><mi
     >q</mi><mi
     >i</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow><mo
     >−</mo><mrow
     ><msub
     ><mi
     >q</mi><mi
     >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd
     ><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><mrow 
    ><msub 
    ><mi 
    >q</mi><mi 
    >j</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >q</mi><mi
     >r</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd
     ><mrow 
    ><mstyle displaystyle="false" 
    ><mfrac 
    ><mn 
    >1</mn><mn 
    >2</mn></mfrac></mstyle><mo 
    >−</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><msubsup 
    ><mi 
    >q</mi><mi 
    >i</mi><mn 
    >2</mn></msubsup><mo 
    >+</mo><msubsup 
    ><mi 
    >q</mi><mi 
    >j</mi><mn 
    >2</mn></msubsup></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mtd></mtr></mtable><mo
     >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    >𝑅</ci><ci  >𝑞</ci></apply><apply
     ><cn type="integer" 
    >2</cn><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix
     ><matrixrow 
    ><apply  ><apply
     ><cn
    type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑖</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑗</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑟</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply></apply><apply
     ><apply
     ><cn
    type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑖</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑟</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑗</ci></apply></apply></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply></apply><apply
     ><apply
     ><cn
    type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&(q_{i}q_{j}-q_{r}q_{k})&(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&(q_{j}q_{k}-q_{r}q_{i})\\
    (q_{i}q_{k}-q_{r}q_{j})&(q_{j}q_{k}+q_{r}q_{i})&\frac{1}{2}-(q_{i}^{2}+q_{j}^{2})\end{pmatrix}</annotation></semantics></math>
    |  | (12) |
  prefs: []
  type: TYPE_NORMAL
- en: 'And therefore the gradient $\frac{\partial M}{\partial q_{x}}$ for 4 components
    of quaternion $r,i,j,k$ can be calculated as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | <math   alttext="\displaystyle\frac{\partial
    M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&amp;-s_{y}q_{k}&amp;s_{z}q_{j}\\
    s_{x}q_{k}&amp;0&amp;-s_{z}q_{i}\\'
  prefs: []
  type: TYPE_NORMAL
- en: -s_{x}q_{j}&amp;s_{y}q_{i}&amp;0\end{smallmatrix}\right)," display="inline"><semantics
    ><mrow  ><mrow
     ><mstyle displaystyle="true"
     ><mfrac
     ><mrow
     ><mo
    rspace="0em"  >∂</mo><mi
     >M</mi></mrow><mrow
     ><mo
    rspace="0em"  >∂</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mfrac></mstyle><mo
     >=</mo><mrow
     ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >​</mo><mrow
     ><mo 
    >(</mo><mtable columnspacing="5pt" rowspacing="0pt"
     ><mtr 
    ><mtd  ><mn
     >0</mn></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >z</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mtd><mtd
     ><mn 
    >0</mn></mtd><mtd 
    ><mrow  ><mo
     >−</mo><mrow
     ><msub
     ><mi
     >s</mi><mi
     >z</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><mo
     >−</mo><mrow
     ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mtd><mtd
     ><mn 
    >0</mn></mtd></mtr></mtable><mo 
    >)</mo></mrow></mrow></mrow><mo 
    >,</mo></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply
     ><apply
     ><ci
     >𝑀</ci></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply></apply></apply><apply
     ><cn type="integer"
     >2</cn><matrix
     ><matrixrow
     ><cn type="integer"
     >0</cn><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply></matrixrow><matrixrow
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply><cn
    type="integer"  >0</cn><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply></apply></matrixrow><matrixrow
     ><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply><cn
    type="integer"  >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial
    M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&-s_{y}q_{k}&s_{z}q_{j}\\ s_{x}q_{k}&0&-s_{z}q_{i}\\
    -s_{x}q_{j}&s_{y}q_{i}&0\end{smallmatrix}\right),</annotation></semantics></math>
    | <math   alttext="\displaystyle\frac{\partial
    M}{\partial q_{i}}=2\left(\begin{smallmatrix}0&amp;s_{y}q_{j}&amp;s_{z}q_{k}\\
    s_{x}q_{j}&amp;-2s_{y}q_{i}&amp;-s_{z}q_{r}\\
  prefs: []
  type: TYPE_NORMAL
- en: s_{x}q_{k}&amp;s_{y}q_{r}&amp;-2s_{z}q_{i}\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mstyle
    displaystyle="true"  ><mfrac
     ><mrow 
    ><mo rspace="0em" 
    >∂</mo><mi 
    >M</mi></mrow><mrow 
    ><mo rspace="0em" 
    >∂</mo><msub 
    ><mi 
    >q</mi><mi 
    >i</mi></msub></mrow></mfrac></mstyle><mo
     >=</mo><mrow 
    ><mn  >2</mn><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo 
    >(</mo><mtable columnspacing="5pt" rowspacing="0pt"
     ><mtr 
    ><mtd  ><mn
     >0</mn></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >z</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >s</mi><mi
     >y</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >z</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >s</mi><mi
     >z</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mrow></mtd></mtr></mtable><mo
     >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><ci 
    >𝑀</ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑖</ci></apply></apply></apply><apply 
    ><cn type="integer" 
    >2</cn><matrix 
    ><matrixrow 
    ><cn type="integer" 
    >0</cn><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑦</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑗</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑧</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑗</ci></apply></apply><apply 
    ><apply 
    ><cn type="integer" 
    >2</cn><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑦</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑖</ci></apply></apply></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑦</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑟</ci></apply></apply><apply 
    ><apply 
    ><cn type="integer" 
    >2</cn><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑧</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑖</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial
    M}{\partial q_{i}}=2\left(\begin{smallmatrix}0&s_{y}q_{j}&s_{z}q_{k}\\ s_{x}q_{j}&-2s_{y}q_{i}&-s_{z}q_{r}\\
    s_{x}q_{k}&s_{y}q_{r}&-2s_{z}q_{i}\end{smallmatrix}\right)</annotation></semantics></math>
    |  | (13) |
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | <math   alttext="\displaystyle\frac{\partial
    M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&amp;s_{y}q_{i}&amp;s_{z}q_{r}\\
    s_{x}q_{i}&amp;0&amp;s_{z}q_{k}\\'
  prefs: []
  type: TYPE_NORMAL
- en: -s_{x}q_{r}&amp;s_{y}q_{k}&amp;-2s_{z}q_{j}\end{smallmatrix}\right)," display="inline"><semantics
    ><mrow  ><mrow
     ><mstyle
    displaystyle="true"  ><mfrac
     ><mrow
     ><mo
    rspace="0em"  >∂</mo><mi
     >M</mi></mrow><mrow
     ><mo
    rspace="0em"  >∂</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mfrac></mstyle><mo
     >=</mo><mrow
     ><mn
     >2</mn><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo 
    >(</mo><mtable columnspacing="5pt" rowspacing="0pt"
     ><mtr 
    ><mtd  ><mrow
     ><mo
     >−</mo><mrow
     ><mn
     >2</mn><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >z</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mtd><mtd
     ><mn 
    >0</mn></mtd><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >z</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><mo
     >−</mo><mrow
     ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >s</mi><mi
     >z</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mrow></mtd></mtr></mtable><mo
     >)</mo></mrow></mrow></mrow><mo
     >,</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply 
    ><apply 
    ><ci 
    >𝑀</ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑗</ci></apply></apply></apply><apply
     ><cn
    type="integer"  >2</cn><matrix
     ><matrixrow
     ><apply
     ><apply
     ><cn
    type="integer"  >2</cn><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply></apply></matrixrow><matrixrow
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply><cn
    type="integer"  >0</cn><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply></matrixrow><matrixrow
     ><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑘</ci></apply></apply><apply
     ><apply
     ><cn
    type="integer"  >2</cn><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial
    M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&s_{y}q_{i}&s_{z}q_{r}\\
    s_{x}q_{i}&0&s_{z}q_{k}\\ -s_{x}q_{r}&s_{y}q_{k}&-2s_{z}q_{j}\end{smallmatrix}\right),</annotation></semantics></math>
    | <math   alttext="\displaystyle\frac{\partial
    M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&amp;-s_{y}q_{r}&amp;s_{z}q_{i}\\
    s_{x}q_{r}&amp;-2s_{y}q_{k}&amp;s_{z}q_{j}\\
  prefs: []
  type: TYPE_NORMAL
- en: s_{x}q_{i}&amp;s_{y}q_{j}&amp;0\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mstyle
    displaystyle="true"  ><mfrac
     ><mrow 
    ><mo rspace="0em" 
    >∂</mo><mi 
    >M</mi></mrow><mrow 
    ><mo rspace="0em" 
    >∂</mo><msub 
    ><mi 
    >q</mi><mi 
    >k</mi></msub></mrow></mfrac></mstyle><mo
     >=</mo><mrow
     ><mn 
    >2</mn><mo lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo  >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr
     ><mtd 
    ><mrow  ><mo
     >−</mo><mrow
     ><mn
     >2</mn><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >z</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >r</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><mo 
    >−</mo><mrow 
    ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >s</mi><mi
     >y</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >k</mi></msub></mrow></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >z</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >q</mi><mi
     >i</mi></msub></mrow></mtd><mtd
     ><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >y</mi></msub><mo lspace="0em" rspace="0em"
     >​</mo><msub
     ><mi
     >q</mi><mi
     >j</mi></msub></mrow></mtd><mtd
     ><mn 
    >0</mn></mtd></mtr></mtable><mo 
    >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><ci 
    >𝑀</ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply></apply><apply 
    ><cn type="integer" 
    >2</cn><matrix 
    ><matrixrow 
    ><apply 
    ><apply 
    ><cn type="integer" 
    >2</cn><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑟</ci></apply></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑖</ci></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑟</ci></apply></apply><apply 
    ><apply 
    ><cn type="integer" 
    >2</cn><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑦</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑘</ci></apply></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑧</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑗</ci></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑖</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑦</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑞</ci><ci 
    >𝑗</ci></apply></apply><cn type="integer"
     >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial
    M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&-s_{y}q_{r}&s_{z}q_{i}\\
    s_{x}q_{r}&-2s_{y}q_{k}&s_{z}q_{j}\\ s_{x}q_{i}&s_{y}q_{j}&0\end{smallmatrix}\right)</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: Tile-based rasterizer for real-time rendering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A technique called Tile-based Rasterizer is used to quickly render the 3D model
    constructed by the Gaussians (see Fig 13). This approach first uses a given view
    angle $V$ of the camera and its position $p$ to filter out the Gaussians that
    are not contributing to the view frustum. In this way only the useful Gaussians
    are involved in the $\alpha$-blending, improving the rendering efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ef26be5b9c73def8f5da0b6a46b7199.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Overview of Tile-based rasterize areas, where green Gaussians are
    contributing to the view frustum, and red Gaussians are not, given camera angle
    $V$.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of sorting all the Gaussian individually for per-pixel $\alpha$-blending,
    tiles are created to divide the 2D screen into smaller 16x16 sections. An instance
    key is assigned to each of the Gaussians using their corresponding view-space
    depth and the tiles they reside with a tile ID. The Gaussians are then sorted
    according to their instance key using a GPU Radix sort. The Radix sort is capable
    of handling large sets of data in parallel with GPU threads. The result of the
    Gaussian sorting can also demonstrate the depth level of tiles.
  prefs: []
  type: TYPE_NORMAL
- en: After sorting, a thread block is assigned to each tile, and the Gaussians are
    loaded into the corresponding memory. $\alpha$-blending is then performed from
    front to back on the sorted list of Gaussians onto the 2D scene, using the cumulative
    color and opacity $\alpha$ of the Gaussians until each pixel reaches a target
    alpha saturation. This design maximizes computation efficiency by enabling parallelism
    of both tile rendering and Gaussian loading in the shared memory space.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 GPU software rasterization of 3D Gaussians
  prefs: []
  type: TYPE_NORMAL
- en: '$w$, $h$: width and height of the image to rasterize'
  prefs: []
  type: TYPE_NORMAL
- en: '$M$, $S$: Gaussian means and covariances in world space'
  prefs: []
  type: TYPE_NORMAL
- en: '$C$, $A$: Gaussian colors and opacities'
  prefs: []
  type: TYPE_NORMAL
- en: '$V$: view configuration of current camera'
  prefs: []
  type: TYPE_NORMAL
- en: function Rasterize($w$, $h$, $M$, $S$, $C$, $A$, $V$)     CullGaussian($p$,
    $V$) $\triangleright$ Frustum Culling     $M^{\prime},S^{\prime}$ $\leftarrow$
    ScreenspaceGaussians($M$, $S$, $V$) $\triangleright$ Transform     $T$ $\leftarrow$
    CreateTiles($w$, $h$)     $L$, $K$ $\leftarrow$ DuplicateWithKeys($M^{\prime}$,
    $T$) $\triangleright$ Indices and Keys     SortByKeys($K$, $L$) $\triangleright$
    Globally Sort     $R$ $\leftarrow$ IdentifyTileRanges($T$, $K$)     $I\leftarrow\mathbf{0}$
    $\triangleright$ Init Canvas     for all Tiles $t$ in $I$ do         for all Pixels
    $i$ in $t$ do              $r\leftarrow$ GetTileRange($R$, $t$)              $I[i]\leftarrow$
    BlendInOrder($i$, $L$, $r$, $K$, $M^{\prime}$, $S^{\prime}$, $C$, $A$)         end for     end forreturn
    $I$end function
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large memory bandwidth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To achieve real-time rendering with high frames per second, a parallel computing
    approach is used in the $Rasterize()$ function. This involves a large amount of
    dynamic data loading occurring in the shared memory of each tile during the $\alpha$-blending
    process. Therefore, a large GPU memory bandwidth is required to support the data
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness in Sparse Viewpoints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gaussians are optimized by taking the gradient compared with the true camera
    view. However, viewpoints with few or no data points have less data to optimize
    the Guassians in their region resulting in artifacts and distortions.
  prefs: []
  type: TYPE_NORMAL
- en: Future Trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic-Driven 3D Reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many 3D reconstruction techniques focus on generating 3D models from images.
    Yet, the integration of text prompts as a guiding factor presents an exciting
    avenue for future research. For example, the method outlined in the paper ”Semantic-Driven
    3D Reconstruction from Single Images”[[18](#bib.bib18)] demonstrates how textual
    cues can significantly enhance both the precision and contextual relevance of
    reconstructed models. While, ”LGM: Large Multi-View Gaussian Model for High-Resolution
    3D Content Creation” [[14](#bib.bib14)] demonstrate impressive zero-shot 3D generations
    from only a text prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic 3D scene reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The previously mentioned approaches can only use captured information in static
    scenes to reproduce static 3D models, where any structure change of scene during
    information capturing will result in misinformation that leads to under-reconstruction
    in specific areas. To achieve dynamic 3D scene reconstruction, 4D Gaussian Splatting
    utilizes a set of canonical 3D Gaussians and transforming them through a deformation
    field at different times, resulting in producing dynamically changing 3D models
    that can represent the motion of objects over time [[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: Single View 3D reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Building on the methodology introduced in Zero 1-to-3, an area that has gained
    significant traction is Single View 3D reconstruction. Leveraging diffusion models
    to generate 3D objects from a single image, [[14](#bib.bib14)] and [[17](#bib.bib17)]
    have demonstrated promising work in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis.
    3d gaussian splatting for real-time radiance field rendering, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Agustinus Kristiadi. Kl divergence: Forward vs reverse? https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/,
    2016. Accessed: 2024-04-22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Hung-yi Lee. Forward process of ddpm, April 2023. https://www.youtube.com/watch?v=ifCDXFdeaaM&t=608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov,
    and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
    Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes
    from images. ACM Trans. Graph., 38(4):65:1–65:14, July 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron,
    Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields
    for unconstrained photo collections, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and
    Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron,
    Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields
    for view synthesis, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant
    neural graphics primitives with a multiresolution hash encoding. ACM Transactions
    on Graphics, 41(4):1–15, July 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and
    Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape
    representation, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Eric Penner and Li Zhang. Soft 3d reconstruction for view synthesis. 36(6),
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and
    Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content
    creation. arXiv preprint arXiv:2402.05054, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller,
    Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki
    Takikawa, and Stan Birchfield. Rtmv: A ray-traced multi-view synthetic dataset
    for novel view synthesis, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei,
    Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic
    scene rendering, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying
    Shan. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view
    large reconstruction models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, and Chi-Keung Tang. Facednerf:
    Semantics-driven face reconstruction, prompt editing and relighting with diffusion
    models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable
    point-based radiance fields for efficient view synthesis, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
