- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:50:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.04764] Deep learning-based person re-identification methods: A survey
    and outlook of recent works'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.04764] 基于深度学习的个人重新识别方法：近期工作的调查与展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.04764](https://ar5iv.labs.arxiv.org/html/2110.04764)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.04764](https://ar5iv.labs.arxiv.org/html/2110.04764)
- en: '[orcid=0000-0003-1616-8054]'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0003-1616-8054]'
- en: \credit
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Conceptualization, Methodology, Investigation, Writing-original draft, Revision
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 概念化，方法论，调查，撰写-原稿，修订
- en: '[orcid=0000-0002-5664-1558] \cormark[1] \creditonceptualization, Writing -
    Review & Editing, Supervision, Resources \cortext[1]Corresponding author'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0002-5664-1558] \cormark[1] \credit概念化，撰写-审阅与编辑，监督，资源 \cortext[1]通讯作者'
- en: \credit
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Writing-Review, Funding Acquisition
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 撰写-审阅，资金获取
- en: \credit
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Data curation, Visualization, Investigation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理，可视化，调查
- en: \credit
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Writing-Review, Data curation
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 撰写-审阅，数据整理
- en: \credit
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Writing-Review, Investigation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 撰写-审阅，调查
- en: \credit
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Writing-Review, Data curation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 撰写-审阅，数据整理
- en: \credit
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Data Curation, Visualization, Validation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理，可视化，验证
- en: 'Deep learning-based person re-identification methods: A survey and outlook
    of recent works'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的个人重新识别方法：近期工作的调查与展望
- en: Zhangqiang Ming mingzhangqiang@stu.scu.edu.cn organization=College of Computer
    Science, addressline=Sichuan University, city=Chengdu, postcode=610065, country=China
       Min Zhu zhumin@scu.edu.cn    Xiangkun Wang    Jiamin Zhu    Junlong Cheng   
    Chengrui Gao    Yong Yang    Xiaoyong Wei
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 张强 Ming mingzhangqiang@stu.scu.edu.cn 组织=计算机科学学院，地址=四川大学，城市=成都，邮政编码=610065，国家=中国
       朱敏 zhumin@scu.edu.cn    王向坤    朱佳敏    程军龙    高成睿    杨勇    魏晓勇
- en: Abstract
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, with the increasing demand for public safety and the rapid
    development of intelligent surveillance networks, person re-identification (Re-ID)
    has become one of the hot research topics in the computer vision field. The main
    research goal of person Re-ID is to retrieve persons with the same identity from
    different cameras. However, traditional person Re-ID methods require manual marking
    of person targets, which consumes a lot of labor cost. With the widespread application
    of deep neural networks, many deep learning-based person Re-ID methods have emerged.
    Therefore, this paper is to facilitate researchers to understand the latest research
    results and the future trends in the field. Firstly, we summarize the studies
    of several recently published person Re-ID surveys and complement the latest research
    methods to systematically classify deep learning-based person Re-ID methods. Secondly,
    we propose a multi-dimensional taxonomy that classifies current deep learning-based
    person Re-ID methods into four categories according to metric and representation
    learning, including methods for deep metric learning, local feature learning,
    generative adversarial learning and sequence feature learning. Furthermore, we
    subdivide the above four categories according to their methodologies and motivations,
    discussing the advantages and limitations of part subcategories. Finally, we discuss
    some challenges and possible research directions for person Re-ID.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着对公共安全需求的增加以及智能监控网络的快速发展，个人重新识别（Re-ID）已成为计算机视觉领域的热门研究主题。个人 Re-ID 的主要研究目标是从不同的摄像头中检索具有相同身份的人员。然而，传统的个人
    Re-ID 方法需要人工标记目标，这消耗了大量的劳动力成本。随着深度神经网络的广泛应用，许多基于深度学习的个人 Re-ID 方法应运而生。因此，本文旨在帮助研究人员了解该领域的最新研究成果和未来趋势。首先，我们总结了几篇最近发布的个人
    Re-ID 调查研究，并补充了最新的研究方法，以系统地分类基于深度学习的个人 Re-ID 方法。其次，我们提出了一种多维分类法，根据度量和表示学习将当前基于深度学习的个人
    Re-ID 方法分为四类，包括深度度量学习、局部特征学习、生成对抗学习和序列特征学习的方法。此外，我们根据这些方法的技术和动机对上述四类进行细分，讨论了部分子类别的优点和局限性。最后，我们讨论了个人
    Re-ID 的一些挑战和可能的研究方向。
- en: 'keywords:'
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Person re-identification\sepDeep metric learning\sepLocal feature learning\sepGenerative
    adversarial learning\sepSequence feature learning\sep{highlights}
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 个人重新识别\sep深度度量学习\sep局部特征学习\sep生成对抗学习\sep序列特征学习\sep{亮点}
- en: The main contributions of person Re-ID surveys are summarized and discussed
    in recent years.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，个人重新识别（Re-ID）调查的主要贡献被总结和讨论。
- en: A metric and representation learning-based taxonomy is provided for recent person
    Re-ID methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为近期的个人 Re-ID 方法提供了一种基于度量和表示学习的分类法。
- en: The above main categories are subdivided based on their methodologies and motivations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上述主要类别根据其方法和动机进行了细分。
- en: The advantages and limitations of part subcategories are summarized.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 部分子类别的优缺点进行了总结。
- en: Furthermore, some challenges and possible research directions for person Re-ID
    are discussed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还讨论了个体重新识别（Re-ID）的一些挑战和可能的研究方向。
- en: 1 Introduction
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In recent years, with the rapid development of intelligent surveillance devices
    and the increasing demand for public safety, a large number of cameras have been
    deployed in public places such as airports, communities, streets and campuses.
    These camera networks typically span large geographic areas with non-overlapping
    coverage and generate a large amount of surveillance video every day. We use this
    video data to analyze the activity patterns and behavioral characteristics of
    pedestrians in the real world for applications such as target detection, multi-camera
    target tracking and crowd behavior analysis. Person Re-ID can be traced back to
    the problem of multi-target multi-camera tracking (MTMCT tracking) [[1](#bib.bib1)],
    which aims to determine whether pedestrians captured by different cameras or pedestrian
    images from different video clips of the same camera are the same pedestrian [[2](#bib.bib2)].
    [Figure 1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") illustrates an example of a surveillance
    area monitored by multiple cameras with non-overlapping fields of view.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着智能监控设备的快速发展和对公共安全的需求增加，公共场所如机场、社区、街道和校园中部署了大量摄像头。这些摄像头网络通常覆盖大范围的地理区域，且覆盖区域不重叠，每天生成大量监控视频。我们利用这些视频数据分析现实世界中行人的活动模式和行为特征，用于目标检测、多摄像头目标跟踪和人群行为分析等应用。个人
    Re-ID 可以追溯到多目标多摄像头跟踪（MTMCT 跟踪）的问题 [[1](#bib.bib1)]，其目标是确定不同摄像头捕捉到的行人或同一摄像头的不同视频片段中的行人是否为同一行人
    [[2](#bib.bib2)]。 [图 1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ 基于深度学习的个人重识别方法：近期工作的调查和展望")
    展示了由多个摄像头监控的具有非重叠视野的监控区域的示例。
- en: '![Refer to caption](img/b4ee804aa14fafa2377ccd5d1c59169e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b4ee804aa14fafa2377ccd5d1c59169e.png)'
- en: 'Fig. 1: Multi-camera surveillance network illustration of person Re-ID.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：个人 Re-ID 的多摄像头监控网络示意图。
- en: '[Figure 2](#S1.F2 "Fig. 2 ‣ 1 Introduction ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the complete flow of the
    person Re-ID system, which mainly consists of two stages: pedestrian detection
    and re-identification [[3](#bib.bib3)]. For pedestrian detection, many algorithms
    with high detection accuracy have emerged, such as YOLO [[4](#bib.bib4)], SSD
    [[5](#bib.bib5)] and Fast R-CNN [[6](#bib.bib6)]. Person Re-ID constructs a large
    image dataset (Gallery) from the detected pedestrian images and retrieves matching
    pedestrian images from it using probe images (Probe), so person Re-ID can also
    be regarded as an image retrieval task [[7](#bib.bib7)]. The key of person Re-ID
    is to learn discriminative features of pedestrians to distinguish between pedestrian
    images with the same identity and those with different identities. However, the
    difficulty of learning discriminative features of pedestrians is increased by
    the variation of view, pose, illumination and resolution in different cameras
    in the real world where pedestrians may appear in multiple cameras in multiple
    regions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](#S1.F2 "Fig. 2 ‣ 1 Introduction ‣ 基于深度学习的个人重识别方法：近期工作的调查和展望") 显示了个人 Re-ID
    系统的完整流程，该流程主要包括两个阶段：行人检测和重新识别 [[3](#bib.bib3)]。对于行人检测，已经出现了许多具有高检测精度的算法，如 YOLO
    [[4](#bib.bib4)]、SSD [[5](#bib.bib5)] 和 Fast R-CNN [[6](#bib.bib6)]。个人 Re-ID 从检测到的行人图像中构建了一个大型图像数据集（Gallery），并使用探测图像（Probe）从中检索匹配的行人图像，因此个人
    Re-ID 也可以被视为一个图像检索任务 [[7](#bib.bib7)]。个人 Re-ID 的关键是学习行人的区分特征，以区分具有相同身份和不同身份的行人图像。然而，由于真实世界中行人在多个区域的多个摄像头中出现，视角、姿势、光照和分辨率的变化增加了学习行人区分特征的难度。'
- en: '![Refer to caption](img/0676647c7b4f0eae962069f4029ccef6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0676647c7b4f0eae962069f4029ccef6.png)'
- en: 'Fig. 2: Flowchart of person re-identification system.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：个人重新识别系统的流程图。
- en: '![Refer to caption](img/a6931c8ed180aab61b1644c687eea8a0.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6931c8ed180aab61b1644c687eea8a0.png)'
- en: 'Fig. 3: The number of person Re-ID papers on top conferences and journals over
    the years.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：近年来顶级会议和期刊上个人 Re-ID 论文的数量。
- en: 'Traditional person Re-ID methods mainly used manual extraction of fixed discriminative
    features [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)] or learned better similarity measures [[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)], which were error-prone and time-consuming,
    and greatly affected the accuracy and real-time performance of pedestrian Re-ID
    tasks. In 2014, deep learning was first used in the person Re-ID field [[17](#bib.bib17),
    [18](#bib.bib18)]. [Figure 3](#S1.F3 "Fig. 3 ‣ 1 Introduction ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works") illustrates
    that there has been a significant increase in the proportion of collected person
    Re-ID papers over the years. Some researchers designed different loss functions
    to optimize the learning of discriminative features by network models [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)]. Other
    researchers extracted more robust features of pedestrians by introducing local
    feature learning [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] or using attention mechanisms to focus on key information of
    body parts [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)]. Ngo et al. [[35](#bib.bib35), [36](#bib.bib36)]
    explored the method of high-level feature extraction aimed to explore context-based
    concept fusion by modeling inter-concept relationships, which were not modeled
    based on semantic reasoning. [[37](#bib.bib37)]. Several works enhanced the final
    feature representation by combining global and local features of pedestrians [[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]. Due to the good performance of GAN in generating images and
    learning features, generative adversarial learning was widely used for person
    Re-ID tasks [[45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)]. To alleviate the shortage of information
    in single-frame images, some researchers used the complementary spatial and temporal
    cues of video sequences to effectively fuse more information in the video sequences
    [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61)]. Recently, graph convolutional network-based methods [[60](#bib.bib60),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)] also emerged
    to learn more discriminative and robust features by modeling graph relationships
    on pedestrian images. Some researchers [[66](#bib.bib66), [67](#bib.bib67)] improved
    the robustness of the person Re-ID model by exploiting the information of a person’s
    3D shape. These methods are numerous and have different emphases. To give researchers
    a quick overview of the current state of development and valuable research directions
    in the field of person Re-ID, we conduct an in-depth survey of deep learning-based
    person Re-ID methods and summarize the relevant research results in recent years.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的行人重识别方法主要使用手动提取固定的区分特征[[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)]或学习更好的相似度度量[[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]，这些方法容易出错且耗时，严重影响了行人重识别任务的准确性和实时性。2014年，深度学习首次应用于行人重识别领域[[17](#bib.bib17),
    [18](#bib.bib18)]。[Figure 3](#S1.F3 "Fig. 3 ‣ 1 Introduction ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works")展示了近年来收集的行人重识别论文比例的显著增加。一些研究者设计了不同的损失函数，以优化网络模型对区分特征的学习[[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)]。其他研究者通过引入局部特征学习[[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]或使用注意力机制来关注身体部位的关键信息[[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]，以提取更鲁棒的行人特征。Ngo等人[[35](#bib.bib35),
    [36](#bib.bib36)]探索了高层次特征提取的方法，旨在通过建模概念间的关系来探索基于上下文的概念融合，而不是基于语义推理[[37](#bib.bib37)]。一些工作通过结合行人的全局和局部特征来增强最终的特征表示[[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]。由于GAN在生成图像和学习特征方面表现良好，生成对抗学习被广泛应用于行人重识别任务[[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]。为了缓解单帧图像信息不足的问题，一些研究者利用视频序列的空间和时间互补线索来有效地融合更多信息[[56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61)]。最近，基于图卷积网络的方法[[60](#bib.bib60),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]也出现了，通过在行人图像上建模图关系来学习更具区分性和鲁棒性的特征。一些研究者[[66](#bib.bib66),
    [67](#bib.bib67)]通过利用人的3D形状信息来提高行人重识别模型的鲁棒性。这些方法种类繁多，侧重点各异。为了让研究人员快速了解行人重识别领域的最新发展状态和有价值的研究方向，我们对基于深度学习的行人重识别方法进行了深入的调查，并总结了近年来的相关研究成果。'
- en: 'Table 1: Comparison of several person Re-ID surveys in recent years.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：近年来几项行人再识别调查的比较。
- en: '| Survey | Reference | Contribution |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 参考文献 | 贡献 |'
- en: '| A survey of approaches and trends in person re-identification[[68](#bib.bib68)]
    | IVC’14 | 1\. Explored the problem of person Re-ID including system-level challenges,
    descriptor issues and correspondence issues; 2\. Summarized the person Re-ID methods
    before 2016 including Contextual methods, Non-contextual methods and Active methods.
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 行人再识别方法及趋势的调查[[68](#bib.bib68)] | IVC’14 | 1\. 探讨了行人再识别问题，包括系统级挑战、描述符问题和对应问题；
    2\. 总结了2016年前的行人再识别方法，包括上下文方法、非上下文方法和主动方法。 |'
- en: '| Person Re-identification Past, Present and Future[[3](#bib.bib3)] | arXiv’16
    | 1\. Introduced the history of person re-ID and its relationship between image
    classification and instance retrieval; 2\. Surveyed a broad selection of the hand-crafted
    systems and the large-scale methods in both image- and video-based re-ID. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 行人再识别的过去、现在与未来[[3](#bib.bib3)] | arXiv’16 | 1\. 介绍了行人再识别的历史及其与图像分类和实例检索的关系；
    2\. 调查了图像和视频基础上广泛选择的手工制作系统和大规模方法。 |'
- en: '| A study on deep convolutional neural network based approaches for person
    re-identification[[69](#bib.bib69)] | PRMI’17 | 1\. Introduced image-based and
    video-based person Re-ID methods; 2\. Discussed some important but undeveloped
    issues and future research directions. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度卷积神经网络的行人再识别方法研究[[69](#bib.bib69)] | PRMI’17 | 1\. 介绍了基于图像和视频的行人再识别方法；
    2\. 讨论了一些重要但尚未开发的问题以及未来的研究方向。 |'
- en: '| Survey on person re-identification based on deep learning[[70](#bib.bib70)]
    | CAAI’18 | 1\. Some deep learning-based person Re-ID methods such as CNN-based,
    GAN-based and Hybrid-based methods are presented; 2\. Proposed the direction of
    further research. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度学习的行人再识别调查[[70](#bib.bib70)] | CAAI’18 | 1\. 介绍了一些基于深度学习的行人再识别方法，如基于
    CNN 的方法、基于 GAN 的方法和混合型方法； 2\. 提出了进一步研究的方向。 |'
- en: '| A systematic evaluation and benchmark for person re-identification: Features,
    metrics, and datasets[[71](#bib.bib71)] | TPAMI’19 | 1\. Presented an extensive
    review and performance evaluation of single-shot and multi-shot Re-ID algorithms.
    2\. Introduced the most recent advances in both feature extraction and metric
    learning. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 行人再识别的系统评估与基准：特征、度量标准和数据集[[71](#bib.bib71)] | TPAMI’19 | 1\. 提供了单次和多次行人再识别算法的广泛综述和性能评估。
    2\. 介绍了在特征提取和度量学习方面的最新进展。 |'
- en: '| Beyond intra-modality discrepancy: A comprehensive survey of heterogeneous
    person re-identification[[72](#bib.bib72)] | arXiv’19 | 1\. Surveyed the models
    that have been widely employed in heterogeneous person Re-ID. 2\. Considered four
    cross-modality application scenarios: Low-resolution (LR), Infrared (IR), Sketch,
    and Text. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 超越模态内差异：异质性行人再识别的综合调查[[72](#bib.bib72)] | arXiv’19 | 1\. 调查了在异质性行人再识别中广泛应用的模型。
    2\. 考虑了四种跨模态应用场景：低分辨率（LR）、红外（IR）、草图和文本。 |'
- en: '| Deep learning-based methods for person re-identification: A comprehensive
    review[[73](#bib.bib73)] | NC’19 | 1\. Reviewed six types of methods of person
    Re-ID based on deep learning, including identification deep model, verification
    deep model, distance metric-based deep model, part-based deep model, video-based
    deep model and data augmentation-based deep model. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度学习的行人再识别方法：全面回顾[[73](#bib.bib73)] | NC’19 | 1\. 回顾了六种基于深度学习的行人再识别方法，包括识别深度模型、验证深度模型、距离度量深度模型、部件深度模型、视频深度模型和数据增强深度模型。
    |'
- en: '| A Brief Survey of Deep Learning Techniques for Person Re-identification[[74](#bib.bib74)]
    | ICETCE’20 | 1\. Presented the issues of person Re-ID and the approaches used
    to solve these issues. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 行人再识别的深度学习技术简要调查[[74](#bib.bib74)] | ICETCE’20 | 1\. 提出了行人再识别的问题及解决这些问题的方法。
    |'
- en: '| A Survey of Open-World Person Re-Identification[[75](#bib.bib75)] | TCSVT’20
    | 1\. Analyzed the discrepancies between closed-word and open-world scenarios.
    2\. Described the developments of open-set Re-ID works and their limitations.
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 开放世界行人再识别的调查[[75](#bib.bib75)] | TCSVT’20 | 1\. 分析了封闭世界与开放世界场景之间的差异。 2\.
    描述了开放集再识别工作的进展及其局限性。 |'
- en: '| Person search: New paradigm of person re-identification: A survey and outlook
    of recent works[[76](#bib.bib76)] | IVC’20 | 1\. Discussed about feature representation
    learning and deep metric learning with novel loss functions. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 人物搜索：人物再识别的新范式：对近期工作的调查和展望[[76](#bib.bib76)] | IVC’20 | 1\. 讨论了特征表示学习和具有新颖损失函数的深度度量学习。
    |'
- en: '| Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?[[77](#bib.bib77)] | arXiv’20 | 1\. Surveyed state-of-the-art DNN
    models being used for person Re-ID task. 2\. Discussed their limitations that
    can work as guidelines for future research. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 关于基于深度学习的可靠人物再识别模型的调查：我们已经到达终点了吗？[[77](#bib.bib77)] | arXiv’20 | 1\. 调查了用于人物再识别任务的最先进DNN模型。2\.
    讨论了这些模型的局限性，这些局限性可以作为未来研究的指导。 |'
- en: '| Person re-identification based on metric learning: a survey[[77](#bib.bib77)]
    | MTA’21 | 1\. Summarized the research progress of person Re-ID methods based
    on metric learning. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 基于度量学习的人物再识别：一项调查[[77](#bib.bib77)] | MTA’21 | 1\. 总结了基于度量学习的人物再识别方法的研究进展。
    |'
- en: '| Deep Learning for Person Re-identification: A Survey and Outlook[[78](#bib.bib78)]
    | TPAMI’21 | 1\. Reviewed for closed-world person Re-ID from three different perspectives,
    including deep feature representation learning, deep metric learning and ranking
    optimization. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习在人物再识别中的应用：调查与展望[[78](#bib.bib78)] | TPAMI’21 | 1\. 从三个不同的角度回顾了封闭世界人物再识别，包括深度特征表示学习、深度度量学习和排名优化。
    |'
- en: '| Survey on Unsupervised Techniques for Person Re-Identification[[79](#bib.bib79)]
    | CDS’21 | 1\. Reviewed the state-of-the-art unsupervised techniques of person
    Re-ID. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 关于无监督技术的人物再识别调查[[79](#bib.bib79)] | CDS’21 | 1\. 回顾了最先进的无监督人物再识别技术。 |'
- en: '| SSS-PR: A short survey of surveys in person re-identification[[80](#bib.bib80)]
    | PRL’21 | 1\. Proposed a multi-dimensional taxonomy to categorize the most relevant
    researches according to different perspectives. 2\. Filled the gap between the
    recently published surveys. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SSS-PR：人物再识别领域调查的简要调查[[80](#bib.bib80)] | PRL’21 | 1\. 提出了一个多维分类法，根据不同的视角对最相关的研究进行分类。2\.
    填补了最近发布的调查之间的空白。 |'
- en: '| Cross-Domain Person Re-identification: A Review[[81](#bib.bib81)] | AIC’21
    | 1\. Reviewed methods of cross-domain person Re-ID.2\. Compared the performance
    of these methods on public datasets. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域人物再识别：一项综述[[81](#bib.bib81)] | AIC’21 | 1\. 回顾了跨领域人物再识别的方法。2\. 比较了这些方法在公共数据集上的表现。
    |'
- en: 'Prior to this survey, some researchers [[68](#bib.bib68), [3](#bib.bib3), [69](#bib.bib69),
    [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] also reviewed the person
    Re-ID field. In [Table 1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works"), we summarize
    the major contributions of these reviews. Some of these surveys [[3](#bib.bib3),
    [69](#bib.bib69)] summarized image-based and video-based person Re-ID methods.
    Other surveys [[70](#bib.bib70), [73](#bib.bib73), [74](#bib.bib74), [78](#bib.bib78),
    [77](#bib.bib77), [79](#bib.bib79)] summarized the deep learning-based person
    Re-ID methods in different dimensions, which developed rapidly after 2014 and
    became the main research means. Recently, Wang et al. [[81](#bib.bib81)] outlined
    methods of cross-domain person Re-ID and compared the performance of these methods
    on public datasets. Yaghoubi et al. [[80](#bib.bib80)] proposed a multi-dimensional
    taxonomy to categorize the most relevant researches according to different perspectives.
    Zhou et al. [[82](#bib.bib82)] provided a review to summarize the developments
    in domain generalization for computer vision over the past decade. Behera et al.
    [[83](#bib.bib83)] reviewed traditional and deep learning person Re-ID methods
    in both contextual and non-contextual dimensions. Wu et al. [[84](#bib.bib84)]
    proposed new taxonomies for the two components of feature extraction and metric
    learning on person Re-ID. Behera et al. [[85](#bib.bib85)] conceptualized an overview
    of interpreting various futuristic cues on the IoT platform for achieving person
    Re-ID.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在本次调查之前，一些研究者[[68](#bib.bib68), [3](#bib.bib3), [69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75),
    [76](#bib.bib76), [77](#bib.bib77), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81)] 也回顾了人物重识别领域。在[表1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Deep learning-based person re-identification methods: A survey and outlook of
    recent works")中，我们总结了这些综述的主要贡献。其中一些综述[[3](#bib.bib3), [69](#bib.bib69)] 总结了基于图像和视频的人物重识别方法。其他综述[[70](#bib.bib70),
    [73](#bib.bib73), [74](#bib.bib74), [78](#bib.bib78), [77](#bib.bib77), [79](#bib.bib79)]
    从不同维度总结了基于深度学习的人物重识别方法，这些方法在2014年后迅速发展，成为主要的研究手段。最近，Wang等[[81](#bib.bib81)] 概述了跨域人物重识别的方法，并比较了这些方法在公共数据集上的性能。Yaghoubi等[[80](#bib.bib80)]
    提出了一个多维分类法，根据不同视角对最相关的研究进行分类。Zhou等[[82](#bib.bib82)] 提供了一项综述，总结了过去十年计算机视觉领域的领域泛化发展。Behera等[[83](#bib.bib83)]
    回顾了传统和深度学习的人物重识别方法，包括上下文和非上下文维度。Wu等[[84](#bib.bib84)] 提出了人物重识别中特征提取和度量学习两个组件的新分类法。Behera等[[85](#bib.bib85)]
    对在物联网平台上解释各种未来线索以实现人物重识别进行了概述。'
- en: 'However, there are still some improvements to be made in these surveys, which
    lack the systematic classification and analysis of deep learning-based person
    Re-ID methods, also miss many discussions parts for person Re-ID. In this paper,
    compared to the above review, we focus more on metric learning and representation
    learning of deep learning methods in person Re-ID tasks and complement the latest
    research methods of recent years. We present an in-depth and comprehensive review
    of existing deep learning-based methods and discuss their advantages and limitations.
    We classify deep learning-based person Re-ID methods in terms of metric and representation
    learning dimensions, including four categories: deep metric learning, local feature
    learning, generative adversarial learning and sequence feature learning.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些调查仍有一些改进的空间，它们缺乏对基于深度学习的人物重识别（Re-ID）方法的系统分类和分析，并且遗漏了许多人脸重识别的讨论部分。本文相较于上述综述，更加关注深度学习方法在人脸重识别任务中的度量学习和表示学习，并补充了近年来的最新研究方法。我们提供了对现有深度学习方法的深入和全面的回顾，讨论了它们的优点和局限性。我们从度量和表示学习的维度对基于深度学习的人物重识别方法进行了分类，包括四个类别：深度度量学习、局部特征学习、生成对抗学习和序列特征学习。
- en: 'Deep metric learning focused on designing better loss functions for model training.
    Common loss functions for person Re-ID included: classification loss, verification
    loss, contrastive loss, triplet loss and quadruplet loss. Representation learning
    focused on developing feature construction strategies [[86](#bib.bib86), [78](#bib.bib78)].
    Therefore, we discussed the common feature learning strategies in recent person
    Re-ID methods, which were mainly in three categories: 1) Local feature learning,
    it learned part-level local features to formulate a combined representation for
    each person image; 2) Generative adversarial learning, it learned the image specific
    style representation or disentangled representation to achieve image-image style
    transfer or extract invariant features; 3) Sequence feature learning, it learned
    video sequence representation using multiple image frames and temporal information.
    In addition, we subdivided the above four categories based on their methodologies
    and motivations. This classification has a clear structure, which comprehensively
    reflects the most common deep metric learning methods and various representation
    learning methods in Re-ID tasks. Therefore, it is suitable for researchers to
    explore person Re-ID for practical needs. Furthermore, we attempt to discuss several
    challenges and research directions.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the main contributions of our work are summarized as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize the studies of several recently published person Re-ID surveys
    and complement the latest research methods to systematically classify deep learning-based
    person Re-ID methods.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We comprehensively review the research methods of recent deep learning-based
    person Re-ID. Then, we propose a multi-dimensional taxonomy that classifies these
    methods into four categories according to metric and representation learning,
    including deep metric learning, local feature learning, generative adversarial
    learning, and sequence feature learning.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We subdivide the above four categories based on their methodologies and motivations,
    discussing the advantages and limitations of part subcategories. This classification
    is more suitable for researchers to explore these methods from their practical
    needs.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize some existing challenges in the person Re-ID field and consider
    that there is still enough necessity to research it. Furthermore, we discuss seven
    possible research directions for person Re-ID researchers.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remaining parts of this survey are structured as follows. In [section 2](#S2
    "2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), we discuss the common datasets
    and evaluation metrics used for person Re-ID benchmarks. In [section 3](#S3 "3
    Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), we comprehensively review current
    deep learning-based methods for person Re-ID and divide them into four categories
    according to metric learning and representation learning. Furthermore, we subdivide
    the above four categories based on their methodologies and motivations, discussing
    the advantages and limitations of part subcategories. Finally, in [section 4](#S4
    "4 Conclusion and future directions ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), we summarize this paper and discuss
    the current challenges and future directions in the person Re-ID field.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 2 Datasets and Evaluation metrics
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present common datasets for evaluating existing deep learning-based
    person Re-ID methods in both image and video dimensions. In addition, we briefly
    describe common evaluation metrics for person Re-ID.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Datasets
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, many methods have emerged to improve the performance of person
    Re-ID. However, the uncertainty of the real world brings about problems including
    occlusion, lighting changes, camera view switching, pose changes and similar clothing
    still cannot be well solved. These challenges make many algorithms still not available
    for real-world applications. Therefore, it is crucial to explore large-scale person
    Re-ID datasets covering more real scenes. As deep learning-based feature extraction
    methods gradually replace traditional manual feature extraction methods, deep
    neural networks require a large amount of training data, which has led to the
    rapid development of large-scale datasets. The types of datasets and annotation
    methods vary greatly between datasets. Usually, the datasets used for person Re-ID
    can be divided into two categories, namely image-based and video-based datasets.
    The following subsection will introduce two types of commonly used datasets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Image-based person Re-ID datasets
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VIPeR [[87](#bib.bib87)] dataset is the first proposed small person Re-ID dataset.
    The VIPeR contains two viewpoint cameras, each of which captures only one image.
    The VIPeR uses manually labeled pedestrians and contains 1,264 images for a total
    of 632 different pedestrians. Each image is cropped and scaled to a size of 128x48\.
    The VIPeR dataset, which features multiple views, poses, and lighting variations,
    has been tested by many researchers, but it remains one of the most challenging
    person Re-ID datasets.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: CUHK01 [[88](#bib.bib88)] dataset has 971 persons and 3,884 manually cropped
    images, with each person also having at least two images captured in two disjoint
    camera views. In the CUHK01 dataset, camera A has more variations of viewpoints
    and poses, and camera B mainly includes images of the frontal view and the back
    view.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CUHK01 [[88](#bib.bib88)] 数据集包含 971 人和 3,884 张手动裁剪的图像，每个人在两个不同的摄像头视角下至少有两张图像。在
    CUHK01 数据集中，摄像头 A 的视角和姿态变化更多，而摄像头 B 主要包括正面视图和背面视图的图像。
- en: CUHK02 [[89](#bib.bib89)] dataset has 1,816 persons and 7,264 manually cropped
    images. The CUHK02 contains five pairs of camera views(ten camera views)), with
    each person also having at least two images captured in two disjoint camera views.
    Compared to the CUHK01 [[88](#bib.bib88)] dataset, the CUHK02 dataset has many
    identities and camera views and can obtain more configurations(which are the combinations
    of viewpoints, poses, image resolutions, lightings and photometric settings) of
    pedestrian images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: CUHK02 [[89](#bib.bib89)] 数据集包含 1,816 人和 7,264 张手动裁剪的图像。CUHK02 包含五对摄像头视角（十个摄像头视角），每个人在两个不同的摄像头视角下至少有两张图像。与
    CUHK01 [[88](#bib.bib88)] 数据集相比，CUHK02 数据集具有更多的身份和摄像头视角，并且能够获取更多的配置（即视角、姿态、图像分辨率、光照和光度设置的组合）来进行行人图像的捕捉。
- en: CUHK03 [[14](#bib.bib14)] dataset belongs to the large-scale person Re-ID dataset
    and is collected at the Chinese University of Hong Kong. The CUHK03 is acquired
    by 10 (5 pairs) cameras and provided with a manual marker and a deformable part
    model (DPM) detector [[90](#bib.bib90)] together to detect pedestrian bounding
    boxes. The CUHK03 contains 1,360 different pedestrians with a total of 13,164
    images, each of variable size. The CUHK03 improves on the CUHK01 [[88](#bib.bib88)]
    and CUHK02 [[89](#bib.bib89)] by increasing the number of cameras and captured
    images, thus capturing pedestrian images from more viewpoints. The CUHK03 dataset
    uses the pedestrian detection algorithm DPM to annotate pedestrians, making it
    more compatible with person Re-ID in the real world than using individual manual
    annotations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: CUHK03 [[14](#bib.bib14)] 数据集属于大规模人物重新识别数据集，收集于香港中文大学。CUHK03 是通过 10 个（5 对）摄像头采集的，并配备了一个手动标记器和一个可变形部件模型（DPM）检测器
    [[90](#bib.bib90)] 用于检测行人边界框。CUHK03 包含 1,360 个不同的行人，共有 13,164 张图像，每张图像的大小各不相同。CUHK03
    相比于 CUHK01 [[88](#bib.bib88)] 和 CUHK02 [[89](#bib.bib89)] 通过增加摄像头和捕获图像的数量，从而从更多的视角捕捉到行人图像。CUHK03
    数据集使用行人检测算法 DPM 对行人进行标注，使其在实际世界中与人物重新识别的兼容性更高，而不是使用单独的手动标注。
- en: Market-1501 [[91](#bib.bib91)] dataset is a large-scale person Re-ID dataset
    published in 2015, which was acquired using five high-resolution cameras and one
    low-resolution camera in front of a supermarket at Tsinghua University. The Market-1501
    uses a pedestrian detector DPM to automatically detect pedestrian bounding boxes.
    It contains 1,501 different pedestrians with a total of 32,668 images, each with
    a size of 128x64\. Compared with CUHK03, Market-1501 has more annotated images
    and contains 2793+500k interfering factors, and it is closer to the real world.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Market-1501 [[91](#bib.bib91)] 数据集是一个大规模人物重新识别数据集，于 2015 年发布，该数据集使用五个高分辨率摄像头和一个低分辨率摄像头在清华大学超市前采集。Market-1501
    使用行人检测器 DPM 自动检测行人边界框。它包含 1,501 个不同的行人，共有 32,668 张图像，每张图像的大小为 128x64。与 CUHK03
    相比，Market-1501 具有更多的标注图像，并包含 2793+500k 的干扰因素，更接近实际世界。
- en: DukeMTMC-reID [[92](#bib.bib92)] dataset belongs to a subset of the MTMCT dataset
    DukeMTMC [[93](#bib.bib93)]. The DukeMTMC-reID dataset is collected at Duke University
    using eight static HD cameras. It contains 16,522 training images (from 702 persons),
    2,228 query images (from other 702 persons), and a search gallery (Gallery) of
    17,661 images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: DukeMTMC-reID [[92](#bib.bib92)] 数据集属于 MTMCT 数据集 DukeMTMC [[93](#bib.bib93)]
    的一个子集。DukeMTMC-reID 数据集在杜克大学使用八个静态高清摄像头收集。它包含 16,522 张训练图像（来自 702 人），2,228 张查询图像（来自其他
    702 人），以及一个包含 17,661 张图像的搜索库（Gallery）。
- en: MSMT17 [[45](#bib.bib45)] dataset is a large-scale person Re-ID dataset published
    in 2018 and is captured at the campus by fifteen cameras. The MSMT17 uses the
    pedestrian detector Faster R-CNN [[94](#bib.bib94)] to automatically detect pedestrian-labeled
    frames. It contains 4,101 different pedestrian information with a total of 126,441
    images, which is one of the large datasets of pedestrian and annotated images
    in the current person Re-ID task. The MSMT17 dataset can cover more scenes than
    earlier datasets where a single scene and no significant light changes existed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MSMT17 [[45](#bib.bib45)] 数据集是一个大规模的人物重识别数据集，于 2018 年发布，并在校园内由 15 个摄像头捕捉。MSMT17
    使用行人检测器 Faster R-CNN [[94](#bib.bib94)] 自动检测行人标注帧。它包含 4,101 个不同的行人信息，总计 126,441
    张图像，是当前人物重识别任务中最大的行人和标注图像数据集之一。MSMT17 数据集可以覆盖比早期数据集更多的场景，其中早期数据集仅包含单一场景且光照变化不显著。
- en: Airport [[71](#bib.bib71)] dataset is constructed using video data from the
    six cameras installed at the postcentral security checkpoint at a commercial airport
    within the United States. The Airport dataset consists of 9,651 identities, 31,238
    distractors, a total of 39,902 images, and each one is cropped and scaled to a
    size of 128x64\. The Airport uses pre-detected bounding boxes generated using
    aggregated channel features(ACF) [[95](#bib.bib95)] detector, which can accurately
    reflect real-world person Re-ID issues.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Airport [[71](#bib.bib71)] 数据集是使用美国一个商业机场的中央安全检查站安装的六个摄像头的视频数据构建的。Airport 数据集包含
    9,651 个身份、31,238 个干扰项，总计 39,902 张图像，每张图像都被裁剪并缩放至 128x64。Airport 使用预检测的边界框，这些边界框是使用聚合通道特征（ACF）
    [[95](#bib.bib95)] 检测器生成的，能够准确反映现实世界的人物重识别问题。
- en: '[Table 2](#S2.T2 "Table 2 ‣ 2.1.1 Image-based person Re-ID datasets ‣ 2.1 Datasets
    ‣ 2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the details of the above
    datasets. Most of the earlier image-based person Re-ID datasets (VIPeR, CUHK01,
    CUHK02, CUHK03, Market-1501) have the following limitations: (1) covering a single
    scene; (2) short time span without significant illumination variations; (3) expensive
    manual annotation or outdated automatic annotation with DPM detection. The CUHK03
    contains viewpoint variations, detection errors, occlusions images. The Market-1501
    contains viewpoint variations, detection errors and low-resolution images. But
    their simulation of the real world is relatively weak. The DukeMTMC-reID dataset
    contains more challenging attributes include viewpoint variations, illumination
    variations, detection errors, occlusions, and background clutter. The MSMT17 dataset
    collects images captured by 15 cameras for both indoor and outdoor scenes. Therefore,
    it presents complex scene transformations and backgrounds. The videos cover a
    long time, thus presenting complex lighting variations. The Airport dataset contains
    a large number of annotated identities and bounding boxes. To our best knowledge,
    MSMT17 and Airport are the largest and most challenging public datasets for person
    Re-ID.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#S2.T2 "表 2 ‣ 2.1.1 基于图像的人物重识别数据集 ‣ 2.1 数据集 ‣ 2 数据集与评估指标 ‣ 基于深度学习的人物重识别方法：近期工作的调查与展望")
    显示了上述数据集的详细信息。早期的大多数基于图像的人物重识别数据集（VIPeR、CUHK01、CUHK02、CUHK03、Market-1501）存在以下限制：（1）覆盖单一场景；（2）时间跨度短且光照变化不显著；（3）昂贵的人工标注或过时的自动标注与
    DPM 检测。CUHK03 包含视角变化、检测错误和遮挡图像。Market-1501 包含视角变化、检测错误和低分辨率图像。但它们对现实世界的模拟相对较弱。DukeMTMC-reID
    数据集包含更多具有挑战性的属性，包括视角变化、光照变化、检测错误、遮挡和背景杂乱。MSMT17 数据集收集了由 15 个摄像头捕捉的室内和室外场景图像。因此，它呈现了复杂的场景变换和背景。这些视频覆盖了较长时间，从而呈现复杂的光照变化。Airport
    数据集包含大量的标注身份和边界框。据我们所知，MSMT17 和 Airport 是目前最大的和最具挑战性的公开人物重识别数据集。'
- en: 'Table 2: Typical image-based person Re-ID datasets.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：典型的基于图像的人物重识别数据集。
- en: '| Data type | Datasets | Years | ID | Boxes | Cameras | Labeled | Evaluation
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 数据集 | 年份 | 身份 | 边界框 | 摄像头 | 标注 | 评估 |'
- en: '| Image | ViPeR[[87](#bib.bib87)] | 2007 | 632 | 1,264 | 2 | Handcrafted |
    CMC |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | ViPeR[[87](#bib.bib87)] | 2007 | 632 | 1,264 | 2 | 手工制作 | CMC |'
- en: '| CUHK01[[88](#bib.bib88)] | 2012 | 971 | 3,884 | 2 | Handcrafted | CMC |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| CUHK01[[88](#bib.bib88)] | 2012 | 971 | 3,884 | 2 | 手工制作 | CMC |'
- en: '| CUHK02[[89](#bib.bib89)] | 2013 | 1,816 | 7,264 | 10 | Handcrafted | CMC
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| CUHK02[[89](#bib.bib89)] | 2013 | 1,816 | 7,264 | 10 | 手工制作 | CMC |'
- en: '| CUHK03[[14](#bib.bib14)] | 2014 | 1,360 | 13,164 | 10 | DPM+Handcrafted |
    CMC+mAP |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| CUHK03[[14](#bib.bib14)] | 2014 | 1,360 | 13,164 | 10 | DPM+手工制作 | CMC+mAP
    |'
- en: '| Market-1501[[91](#bib.bib91)] | 2015 | 1,501 | 32,217 | 6 | DPM+Handcrafted
    | CMC+mAP |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Market-1501[[91](#bib.bib91)] | 2015 | 1,501 | 32,217 | 6 | DPM+手工制作 | CMC+mAP
    |'
- en: '| DukeMTMC-reID[[52](#bib.bib52)] | 2017 | 1,812 | 36,441 | 8 | Handcrafted
    | CMC+mAP |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DukeMTMC-reID[[52](#bib.bib52)] | 2017 | 1,812 | 36,441 | 8 | 手工制作 | CMC+mAP
    |'
- en: '|  | MSMT17[[45](#bib.bib45)] | 2018 | 4,101 | 126,441 | 15 | Faster RCNN |
    CMC+mAP |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | MSMT17[[45](#bib.bib45)] | 2018 | 4,101 | 126,441 | 15 | Faster RCNN |
    CMC+mAP |'
- en: '|  | Airport[[71](#bib.bib71)] | 2019 | 9,651 | 39,902 | 6 | ACF | CMC+mAP
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | Airport[[71](#bib.bib71)] | 2019 | 9,651 | 39,902 | 6 | ACF | CMC+mAP
    |'
- en: 'Table 3: Typical video-based person Re-ID datasets.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：典型的基于视频的人物再识别数据集。
- en: '| Data type | Datasets | Years | ID | Tracks | Cameras | Labeled | Evaluation
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 数据集 | 年份 | ID | 轨迹 | 摄像机 | 标注 | 评估 |'
- en: '| Video | PRID-2011[[96](#bib.bib96)] | 2011 | 934 | 400 | 2 | Handcrafted
    | CMC |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 视频 | PRID-2011[[96](#bib.bib96)] | 2011 | 934 | 400 | 2 | 手工制作 | CMC |'
- en: '| iLIDS-VID [[97](#bib.bib97)] | 2014 | 300 | 600 | 2 | Handcrafted | CMC |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| iLIDS-VID [[97](#bib.bib97)] | 2014 | 300 | 600 | 2 | 手工制作 | CMC |'
- en: '| MARS[[98](#bib.bib98)] | 2016 | 1,261 | 20,715 | 6 | DPM+GMMCP | CMC+mAP
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MARS[[98](#bib.bib98)] | 2016 | 1,261 | 20,715 | 6 | DPM+GMMCP | CMC+mAP
    |'
- en: '| DukeMTMC-V[[19](#bib.bib19)] | 2018 | 1,812 | 4,832 | 8 | DPM | CMC+mAP |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| DukeMTMC-V[[19](#bib.bib19)] | 2018 | 1,812 | 4,832 | 8 | DPM | CMC+mAP |'
- en: '| LPW[[99](#bib.bib99)] | 2018 | 2,731 | 7,694 | 11 | DPM+NN+Handcrafted |
    CMC+mAP |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| LPW[[99](#bib.bib99)] | 2018 | 2,731 | 7,694 | 11 | DPM+NN+手工制作 | CMC+mAP
    |'
- en: '![Refer to caption](img/8ade1bffbd56b00b40d85acde4da30b6.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ade1bffbd56b00b40d85acde4da30b6.png)'
- en: (a) Market-1501[[91](#bib.bib91)]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Market-1501[[91](#bib.bib91)]
- en: '![Refer to caption](img/e6bdf6fd85b62604d50867800fff608a.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e6bdf6fd85b62604d50867800fff608a.png)'
- en: (b) DukeMTMC-reID[[92](#bib.bib92)]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DukeMTMC-reID[[92](#bib.bib92)]
- en: '![Refer to caption](img/81bf19b2d94876538f94837b97bda478.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/81bf19b2d94876538f94837b97bda478.png)'
- en: (c) MSMT17[[8](#bib.bib8)]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (c) MSMT17[[8](#bib.bib8)]
- en: 'Fig. 4: Sampled person images of person Re-ID datasets.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于视频的人物再识别数据集的样本行人图像。
- en: 2.1.2 Video-based person Re-ID datasets
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 基于视频的人物再识别数据集
- en: PRID2011 [[96](#bib.bib96)] dataset is a video-based person Re-ID dataset proposed
    in 2011 and is obtained from two non-overlapping camera acquisitions and contains
    a total of 24,541 images of 934 different pedestrians with manually labeled pedestrian
    bounding boxes. The resolution size of each image is 128×64.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PRID2011 [[96](#bib.bib96)] 数据集是2011年提出的基于视频的人物再识别数据集，来自两台不重叠的摄像机采集，总计24,541张934名不同行人的图像，行人边界框为人工标注。每张图像的分辨率为128×64。
- en: iLIDS-VID [[97](#bib.bib97)] dataset is acquired by two cameras at the airport
    and contains 300 pedestrians with 600 tracks totaling 42,495 pedestrian images.
    The iLIDS-VID uses manual annotation of pedestrian bounding boxes.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: iLIDS-VID [[97](#bib.bib97)] 数据集由机场的两台摄像机获得，包含300名行人，600条轨迹，总计42,495张行人图像。iLIDS-VID
    使用人工标注的行人边界框。
- en: MARS [[98](#bib.bib98)] dataset is the first large-scale video-based person
    re-identification dataset proposed in 2016, which contains 1,261 different pedestrians
    and approximately 20,000 video sequences of pedestrian trajectories acquired from
    six different cameras. The DPM detector and the generalized maximum multi-cornered
    problem (GMMCP) tracker [[100](#bib.bib100)] were used for pedestrian detection
    and trajectory tracking of MARS, respectively. The MARS dataset contains 3,248
    interfering trajectories and is fixed with 631 and 630 different pedestrians to
    divide the training and test sets, respectively. It can be considered as an extension
    of Market-1501.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: MARS [[98](#bib.bib98)] 数据集是2016年提出的第一个大规模视频基人物再识别数据集，包含1,261名不同的行人和约20,000个行人轨迹视频序列，来自六台不同的摄像机。MARS使用DPM检测器和广义最大多角点问题（GMMCP）跟踪器[[100](#bib.bib100)]分别用于行人检测和轨迹跟踪。MARS数据集包含3,248条干扰轨迹，固定为631名和630名不同的行人分别用于训练集和测试集。它可以视为Market-1501的扩展。
- en: DukeMTMC-VideoReID [[19](#bib.bib19)] dataset belongs to a subset of the MTMCT
    dataset DukeMTMC [[93](#bib.bib93)] for video-based person Re-ID. This dataset
    contains 1,812 different pedestrians, 4,832 pedestrian trajectories totaling 815,420
    images, in which 408 pedestrians as interference terms, 702 pedestrians for training,
    702 pedestrians for testing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: DukeMTMC-VideoReID [[19](#bib.bib19)] 数据集属于MTMCT数据集DukeMTMC [[93](#bib.bib93)]
    的一个子集，用于基于视频的人物再识别。该数据集包含1,812名不同的行人，4,832条行人轨迹，总计815,420张图像，其中408名行人为干扰项，702名行人用于训练，702名行人用于测试。
- en: LPW(Labeled Person in the Wild) [[99](#bib.bib99)] dataset is a largescale video
    sequence-based person Re-ID dataset that collects three different crowded scenes
    containing 2,731 different pedestrians and 7,694 pedestrian trajectories with
    more than 590,000 images. The LPW dataset is collected in crowded scenes and has
    more occlusions, providing more realistic and challenging benchmarks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: LPW（在野外标记的行人）[[99](#bib.bib99)] 数据集是一个大规模的视频序列行人重识别数据集，收集了包含2,731名不同行人和7,694条行人轨迹（超过590,000张图像）的三种不同拥挤场景。LPW
    数据集采集于拥挤场景，具有更多遮挡，提供了更具现实性和挑战性的基准。
- en: '[Table 3](#S2.T3 "Table 3 ‣ 2.1.1 Image-based person Re-ID datasets ‣ 2.1 Datasets
    ‣ 2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the details of video-based
    person Re-ID datasets. PRID2011 and iLIDS-VID use only two cameras to capture
    video and are labeled with fewer identities. That means other identities are only
    single camera’s frame segments and the lighting and shooting angles of these identities
    in this dataset may not change much. MARS and DukeMTMC-ViedeReID are large-scale
    video-based person Re-ID datasets. Their bounding boxes and tracks are automatically
    generated and contain several natural detection or tracking errors, and each tag
    may have multiple tracks. LPW is one of the most challenging video-based person
    Re-ID datasets available and closer to the real world, distinguishes from existing
    datasets in three aspects: more identities and tracks, automatically detected
    bounding boxes, and far more crowded scenes with a larger time span.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3](#S2.T3 "表 3 ‣ 2.1.1 基于图像的行人重识别数据集 ‣ 2.1 数据集 ‣ 2 数据集和评估指标 ‣ 基于深度学习的行人重识别方法：近期工作的调查与展望")
    展示了基于视频的行人重识别数据集的详细信息。PRID2011 和 iLIDS-VID 仅使用两个摄像头捕捉视频，并标注了较少的身份。这意味着其他身份只有单摄像头的帧段，这些身份在数据集中可能在光照和拍摄角度上变化不大。MARS
    和 DukeMTMC-ViedeReID 是大规模基于视频的行人重识别数据集。它们的边界框和轨迹是自动生成的，包含了几种自然检测或跟踪错误，每个标签可能有多个轨迹。LPW
    是目前最具挑战性的基于视频的行人重识别数据集之一，与现有数据集相比，主要在以下三个方面有所区别：更多的身份和轨迹、自动检测的边界框，以及更多的拥挤场景和更大的时间跨度。'
- en: '[Figure 4](#S2.F4 "Fig. 4 ‣ 2.1.1 Image-based person Re-ID datasets ‣ 2.1 Datasets
    ‣ 2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows some sample images of a
    partial person Re-ID dataset. We can see that with the development of large-scale
    person Re-ID datasets, the number of pedestrian IDs and the number of labeled
    frames or trajectories in the datasets are increasing, and the scenarios covered
    by the datasets are getting richer. These datasets use a combination of deep learning
    detectors and manual annotation to detect pedestrian bounding boxes, making the
    latest datasets closer and closer to the real world, thus enhancing the robustness
    of person Re-ID models. In addition, almost all mainstream person Re-ID datasets
    are evaluated using mean average precision (mAP) and cumulative matching characteristics
    (CMC) curves for performance evaluation.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4](#S2.F4 "图 4 ‣ 2.1.1 基于图像的行人重识别数据集 ‣ 2.1 数据集 ‣ 2 数据集和评估指标 ‣ 基于深度学习的行人重识别方法：近期工作的调查与展望")
    展示了部分行人重识别数据集的样本图像。我们可以看到，随着大规模行人重识别数据集的发展，数据集中的行人 ID 数量和标记的帧或轨迹数量都在增加，数据集覆盖的场景也越来越丰富。这些数据集结合了深度学习检测器和人工标注来检测行人边界框，使得最新的数据集越来越接近真实世界，从而增强了行人重识别模型的鲁棒性。此外，几乎所有主流的行人重识别数据集都是使用平均精度均值（mAP）和累积匹配特征（CMC）曲线进行性能评估的。'
- en: 2.2 Evaluation metrics
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 评估指标
- en: The commonly used evaluation metrics of person Re-ID algorithms are cumulative
    matching characteristics (CMC) curves and mean average precision (mAP).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 行人重识别算法常用的评估指标是累积匹配特征（CMC）曲线和平均精度均值（mAP）。
- en: 'In pattern recognition systems, CMC curves are important evaluation metrics
    in the fields of face, fingerprint, iris detection and person Re-ID, which can
    comprehensively assess the merits of model algorithms. Furthermore, CMC curves
    are considered to be a comprehensive reflection of the performance of the person
    Re-ID classifier. Before calculating the CMC curves, the probability ${Acc_{k}}$
    that the top-k retrieved images (top-k) in the gallery contain the correct query
    result is obtained by ranking the similarity between the query target and the
    target image to be queried, which is calculated as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="{Acc_{k}}=\begin{array}[]{l}\begin{cases}1&amp;\text{if
    top-k rank gallery samples}\\ &amp;\text{ contain query identity.}\\'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{otherwise}\end{cases}\end{array}" display="block"><semantics ><mrow
    ><mrow  ><mi >A</mi><mo lspace="0em" rspace="0em" >​</mo><mi >c</mi><mo lspace="0em"
    rspace="0em" >​</mo><msub  ><mi >c</mi><mi >k</mi></msub></mrow><mo  >=</mo><mtable
    displaystyle="true"  ><mtr ><mtd  columnalign="left" ><mrow  ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><mn
    >1</mn></mtd><mtd columnalign="left"  ><mtext >if top-k rank gallery samples</mtext></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mtext > contain query identity.</mtext></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mn >0</mn></mtd><mtd columnalign="left"  ><mtext >otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >𝐴</ci><ci  >𝑐</ci><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci  >𝑐</ci><ci >𝑘</ci></apply></apply><matrix
    ><matrixrow ><apply  ><csymbol cd="latexml"  >cases</csymbol><cn type="integer"  >1</cn><ci
    ><mtext >if top-k rank gallery samples</mtext></ci><ci  ><mtext >otherwise</mtext></ci><ci
    ><mtext > contain query identity.</mtext></ci><cn type="integer" >0</cn><ci ><mtext
    >otherwise</mtext></ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >{Acc_{k}}=\begin{array}[]{l}\begin{cases}1&\text{if
    top-k rank gallery samples}\\ &\text{ contain query identity.}\\ 0&\text{otherwise}\end{cases}\end{array}</annotation></semantics></math>
    |  | (1) |
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: CMC curves are calculated by adding up the ${Acc_{k}}$ of each query image and
    dividing it by the total number of query images, which are usually expressed as
    Rank-k. For example, Rank-1 accuracy indicates the probability of correctly matching
    to the first target in the matching list.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: A single evaluation metric often cannot comprehensively evaluate the comprehensive
    performance of the person Re-ID algorithm. The mAP can reflect the extent to which
    all images with correct queries are at the front of the result queue in the query
    results. Considering both the average precision (AP) and precision-recall curve
    (PR) of the query process [[91](#bib.bib91)], instead of just focusing on the
    hit rate, which can measure the performance of person Re-ID algorithms more comprehensively.
    The algorithm is usually necessary to be evaluated separately for CMC curves and
    mAP in person Re-ID tasks. Zheng et.al. [[8](#bib.bib8)] proposed the Re-ranking
    method, which can re-rank the query results and further improve the effectiveness
    of Rank-k and mAP accuracy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 单一评价指标通常无法全面评估人物重识别算法的综合性能。mAP 能反映所有正确查询图像在查询结果中的前置程度。考虑查询过程的平均精度（AP）和精确度-召回曲线（PR）[[91](#bib.bib91)]，而不仅仅关注命中率，可以更全面地衡量人物重识别算法的性能。算法通常需要分别评估CMC曲线和mAP在人物重识别任务中的表现。郑等人[[8](#bib.bib8)]提出了重排序方法，该方法可以重新排序查询结果，进一步提高Rank-k和mAP的准确性。
- en: 3 Deep Learning Based Re-ID Method
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的人物重识别方法
- en: 'In this section, we classify deep learning-based person Re-ID methods into
    four categories with classification structure is shown in [Figure 5](#S3.F5 "Fig.
    5 ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), including methods for depth metric
    learning, local feature learning, generative adversarial learning and sequences
    feature learning. In addition, we subdivide the above four categories according
    to their methodologies and motivations, discussing and comparing the advantages
    and limitations of part subcategories.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将基于深度学习的人物重识别方法分为四类，分类结构见[图 5](#S3.F5 "Fig. 5 ‣ 3 Deep Learning Based
    Re-ID Method ‣ Deep learning-based person re-identification methods: A survey
    and outlook of recent works")，包括深度度量学习、局部特征学习、生成对抗学习和序列特征学习的方法。此外，我们还根据其方法论和动机对上述四类进行细分，讨论和比较各部分子类别的优缺点。'
- en: '![Refer to caption](img/c86c28b91a6bbec30943bd2b8dd039f0.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c86c28b91a6bbec30943bd2b8dd039f0.png)'
- en: 'Fig. 5: Classification structure of deep learning-based person re-identification
    methods.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于深度学习的人物重识别方法的分类结构。
- en: 3.1 Deep metric learning
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度度量学习
- en: 'Deep metric learning (DML) is one of the metric learning (ML) methods that
    aims to learn the similarity or dissimilarity between two pedestrian objects.
    The main goal of DML is to learn a mapping from the original image to the feature
    embedding (FE) such that the same pedestrians have smaller distances using a distance
    function on the feature space and different pedestrians feature farther apart
    from each other [[101](#bib.bib101), [102](#bib.bib102)]. With the rise of deep
    neural networks(DNNs), DML has been widely used in computational vision, such
    as face recognition, image retrieval, and person Re-ID. DML is mainly used to
    constrain the learning of discriminative features by designing loss functions
    for network models [[78](#bib.bib78)]. In this paper, we focus on loss functions
    commonly used in person Re-ID tasks, including classification loss [[19](#bib.bib19),
    [20](#bib.bib20), [47](#bib.bib47), [51](#bib.bib51), [103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105)], verification loss [[20](#bib.bib20), [106](#bib.bib106),
    [107](#bib.bib107), [78](#bib.bib78)], contrastive loss [[21](#bib.bib21), [108](#bib.bib108),
    [109](#bib.bib109)], triplet loss [[22](#bib.bib22), [110](#bib.bib110), [111](#bib.bib111)]
    and quadruplet loss [[23](#bib.bib23)]. An illustration of five loss functions
    is shown in [Figure 6](#S3.F6 "Fig. 6 ‣ 3.1 Deep metric learning ‣ 3 Deep Learning
    Based Re-ID Method ‣ Deep learning-based person re-identification methods: A survey
    and outlook of recent works"). These deep metric learning methods enable models
    to learn discriminative features automatically, which can solve the problem of
    manually designing features that consume a lot of labor costs.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 深度度量学习（DML）是度量学习（ML）方法之一，旨在学习两个行人对象之间的相似性或差异性。DML的主要目标是学习从原始图像到特征嵌入（FE）的映射，使得相同的行人在特征空间上具有较小的距离，而不同的行人则相距较远[[101](#bib.bib101),
    [102](#bib.bib102)]。随着深度神经网络（DNNs）的兴起，DML已广泛应用于计算机视觉领域，如人脸识别、图像检索和行人重识别（Re-ID）。DML主要通过为网络模型设计损失函数来约束特征的学习[[78](#bib.bib78)]。本文重点讨论在行人Re-ID任务中常用的损失函数，包括分类损失[[19](#bib.bib19),
    [20](#bib.bib20), [47](#bib.bib47), [51](#bib.bib51), [103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105)]，验证损失[[20](#bib.bib20), [106](#bib.bib106), [107](#bib.bib107),
    [78](#bib.bib78)]，对比损失[[21](#bib.bib21), [108](#bib.bib108), [109](#bib.bib109)]，三元组损失[[22](#bib.bib22),
    [110](#bib.bib110), [111](#bib.bib111)]和四元组损失[[23](#bib.bib23)]。五种损失函数的示意图见[图6](#S3.F6
    "图6 ‣ 3.1 深度度量学习 ‣ 3 基于深度学习的Re-ID方法 ‣ 基于深度学习的行人重识别方法：最近工作的调查与展望")。这些深度度量学习方法使模型能够自动学习区分性特征，从而解决了手动设计特征带来的大量劳动成本问题。
- en: '![Refer to caption](img/7a02fc5d1f329c3329b7622070d43cd4.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7a02fc5d1f329c3329b7622070d43cd4.png)'
- en: 'Fig. 6: An illustration shows a variety of existing deep metric learning-based
    person Re-ID losses. (a) Classification loss. (b) Verification loss. (c) Contrastive
    loss. (d) Triplet loss, where $A$, $P$, $N$ indicate anchor, positive and negative
    samples, respectively. (e) Quadruplet loss, where $N1$ and $N2$ are different
    negative samples.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：示意图展示了多种基于深度度量学习的行人Re-ID损失。 (a) 分类损失。 (b) 验证损失。 (c) 对比损失。 (d) 三元组损失，其中$A$、$P$、$N$分别表示锚点、正样本和负样本。
    (e) 四元组损失，其中$N1$和$N2$是不同的负样本。
- en: 3.1.1 Classification loss
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 分类损失
- en: 'Zheng et al. [[3](#bib.bib3), [112](#bib.bib112)] treated the training process
    of person Re-ID as a multi-classification task for images and proposed an ID-discriminative
    embedding (IDE) network. The IDE treats each pedestrian as a different class and
    uses the ID of the pedestrian as a classification label to train a deep neural
    network, so the classification loss is also called ID loss. The training network
    for classification loss usually inputs a picture and connects a fully connected
    layer (FC) for classification at the end of the network, and then maps the feature
    vectors of the image onto the probability space by the softmax activation function.
    The cross-entropy loss for multi-classification of the person Re-ID task can be
    expressed as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng等人[[3](#bib.bib3), [112](#bib.bib112)]将行人Re-ID的训练过程视为图像的多分类任务，并提出了ID区分嵌入（IDE）网络。IDE将每个行人视为不同的类别，并使用行人的ID作为分类标签来训练深度神经网络，因此分类损失也称为ID损失。分类损失的训练网络通常输入一张图片，并在网络末尾连接一个全连接层（FC）进行分类，然后通过softmax激活函数将图像的特征向量映射到概率空间。行人Re-ID任务的多分类交叉熵损失可以表示为：
- en: '|  | $\displaystyle\mathcal{L}_{id}=-\sum_{a=1}^{K}q\left(x_{a}\right)\log{}p\left(y_{a}&#124;x_{a}\right)$
    |  | (2) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{id}=-\sum_{a=1}^{K}q\left(x_{a}\right)\log{}p\left(y_{a}&#124;x_{a}\right)$
    |  | (2) |'
- en: Where $K$ represents the number of training sample ID categories per batch,
    $q(x_{a})$ denotes the label of sample image $x_{a}$. If $x_{a}$ is identified
    as $y_{a}$, then $q(x_{a})=1$, otherwise $q(x_{a})=0$. $p(y_{a}|x_{a})$ is the
    probability that picture $x_{a}$ is predicted as category $y_{a}$ using the softmax
    activation function. Classification loss is widely used as a depth metric learning
    for person Re-ID methods because of its advantages such as easy training of models
    and mining hard samples [[19](#bib.bib19), [20](#bib.bib20), [47](#bib.bib47),
    [51](#bib.bib51), [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105)].
    However, using ID information alone is not enough to learn a model with sufficient
    generalization ability. Therefore, ID loss usually needs to be combined with other
    losses to constrain the training of the model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$K$表示每批次的训练样本ID类别数，$q(x_{a})$表示样本图像$x_{a}$的标签。如果$x_{a}$被识别为$y_{a}$，则$q(x_{a})=1$，否则$q(x_{a})=0$。$p(y_{a}|x_{a})$是使用softmax激活函数预测图像$x_{a}$为类别$y_{a}$的概率。分类损失由于模型训练简单且能挖掘困难样本，被广泛用于行人Re-ID方法的深度度量学习[[19](#bib.bib19),
    [20](#bib.bib20), [47](#bib.bib47), [51](#bib.bib51), [103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105)]。然而，仅使用ID信息不足以学习出具有足够泛化能力的模型。因此，ID损失通常需要与其他损失结合以约束模型的训练。
- en: 3.1.2 Verification loss
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 验证损失
- en: 'Person Re-ID can also be treated as a validation problem, and validation loss
    is proposed to guide the training of the model. In contrast to classification
    loss, the network trained by verification loss requires two images as input, and
    a binary loss is computed by fusing the feature information of the two images,
    which in turn determines whether the input two images are the same pedestrian
    [[20](#bib.bib20), [106](#bib.bib106), [107](#bib.bib107), [78](#bib.bib78)].
    The expression of the cross-entropy validation loss function is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 行人Re-ID也可以视为一个验证问题，提出了验证损失以指导模型训练。与分类损失相比，由验证损失训练的网络需要两张图像作为输入，通过融合这两张图像的特征信息计算二元损失，从而判断输入的两张图像是否为同一行人[[20](#bib.bib20),
    [106](#bib.bib106), [107](#bib.bib107), [78](#bib.bib78)]。交叉熵验证损失函数的表达式如下：
- en: '|  | $\displaystyle\mathcal{L}_{v}=-y_{ab}\log{}p\left(y_{ab}&#124;f_{ab}\right)-$
    |  | (3) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{v}=-y_{ab}\log{}p\left(y_{ab}&#124;f_{ab}\right)-$
    |  | (3) |'
- en: '|  | $\displaystyle(1-y_{ab})\log{}\left(1-p(y_{ab}&#124;f_{ab})\right)$ |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(1-y_{ab})\log{}\left(1-p(y_{ab}&#124;f_{ab})\right)$ |  |'
- en: Supposing the network inputs two images $x_{a}$ and $x_{b}$, we get the feature
    vectors $f_{a}$ and $f_{b}$ of these two images respectively, and calculate the
    difference feature $f_{ab}={(f_{a}-f_{b})^{2}}$ of the two feature vectors. We
    use the softmax activation function to calculate the probability $p$ that the
    image pairs $x_{a}$ and $x_{b}$ have the same pedestrian ID, where $y_{ab}$ is
    the pedestrian ID label of the two images. When the images $x_{a}$ and $x_{b}$
    have the same ID, $y_{ab}=1$, otherwise, $y_{ab}=0$.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设网络输入两张图像$x_{a}$和$x_{b}$，我们分别获取这两张图像的特征向量$f_{a}$和$f_{b}$，并计算这两个特征向量的差异特征$f_{ab}={(f_{a}-f_{b})^{2}}$。我们使用softmax激活函数来计算图像对$x_{a}$和$x_{b}$具有相同行人ID的概率$p$，其中$y_{ab}$是这两张图像的行人ID标签。当图像$x_{a}$和$x_{b}$具有相同ID时，$y_{ab}=1$，否则$y_{ab}=0$。
- en: The verification loss is less efficient in recognition because it can only input
    a pair of images to judge the similarity when tested while ignoring the relationship
    between the image pair and other images in the dataset. For this reason, researchers
    considered combining classification and validation networks [[106](#bib.bib106),
    [107](#bib.bib107)], and the combined loss can be expressed as $\mathcal{L}=\mathcal{L}_{id}+\mathcal{L}_{v}$.
    The hybrid loss can combine the advantages of classification loss and verification
    loss, which can predict the identity ID of pedestrians and perform similarity
    metrics simultaneously, thus improving the accuracy of person Re-ID.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 验证损失在识别中的效率较低，因为它只能输入一对图像来判断相似性，而忽略了图像对与数据集中其他图像之间的关系。因此，研究人员考虑将分类和验证网络结合起来[[106](#bib.bib106),
    [107](#bib.bib107)]，组合损失可以表示为$\mathcal{L}=\mathcal{L}_{id}+\mathcal{L}_{v}$。混合损失可以结合分类损失和验证损失的优点，既可以预测行人的身份ID，又可以同时进行相似性度量，从而提高行人Re-ID的准确性。
- en: 3.1.3 Contrastive loss
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 对比损失
- en: 'Contrastive loss, which mainly constrains the similarity or dissimilarity between
    pairs of data, is generally used for model training of twin networks (Siamese
    Network) in person Re-ID tasks [[21](#bib.bib21), [108](#bib.bib108), [109](#bib.bib109)].
    Its function can be expressed as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失主要约束数据对之间的相似性或不相似性，通常用于行人 Re-ID 任务中的双胞胎网络（Siamese Network）的模型训练 [[21](#bib.bib21),
    [108](#bib.bib108), [109](#bib.bib109)]。其函数可以表示为：
- en: '|  | $\displaystyle\mathcal{L}_{c}=yd\left(x_{a}-x_{b}\right)^{2}+\left(1-y\right)\left[m-d\left(x_{a}-x_{b}\right)^{2}\right]_{+}$
    |  | (4) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{c}=yd\left(x_{a}-x_{b}\right)^{2}+\left(1-y\right)\left[m-d\left(x_{a}-x_{b}\right)^{2}\right]_{+}$
    |  | (4) |'
- en: Where $\left[z\right]_{+}=max(0,z)$, $x_{a}$ and $x_{b}$ are two images input
    to the twin network at the same time. $d(x_{a},x_{b})$ usually indicates the euclidean
    distance (similarity) of the two images. $m$ is the set training threshold, and
    $y$ is the label of whether each pair of training images matches. When $y=1$,
    it means that the input images $x_{a}$ and $x_{b}$ belong to the pedestrians with
    the same ID (positive sample pair). When $y=0$, it means that the input images
    $x_{a}$ and $x_{b}$ belong to pedestrians with different IDs (negative sample
    pair). $\mathcal{L}_{c}$ reflects well the matching degree of sample pairs, which
    is often used to train models for person Re-ID feature extraction and often works
    together with classification loss combinations for training networks [[47](#bib.bib47)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left[z\right]_{+}=max(0,z)$，$x_{a}$ 和 $x_{b}$ 是同时输入到双胞胎网络的两幅图像。$d(x_{a},x_{b})$
    通常表示两幅图像的欧氏距离（相似度）。$m$ 是设定的训练阈值，$y$ 是每对训练图像是否匹配的标签。当 $y=1$ 时，表示输入图像 $x_{a}$ 和
    $x_{b}$ 属于相同 ID 的行人（正样本对）。当 $y=0$ 时，表示输入图像 $x_{a}$ 和 $x_{b}$ 属于不同 ID 的行人（负样本对）。$\mathcal{L}_{c}$
    很好地反映了样本对的匹配程度，它通常用于训练模型以进行行人 Re-ID 特征提取，并且通常与分类损失组合一起用于训练网络 [[47](#bib.bib47)]。
- en: 3.1.4 Triplet loss
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 三元组损失
- en: 'Triplet loss is one of the most widely used depth metric losses in person Re-ID
    tasks, and it aims to minimize the intra-class distance and maximize the inter-inter-class
    distance of samples. With the development of deep neural networks, a large number
    of variants based on triplet loss have emerged [[22](#bib.bib22), [110](#bib.bib110),
    [111](#bib.bib111), [113](#bib.bib113)]. The triplet loss function can be expressed
    as:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组损失是行人 Re-ID 任务中最广泛使用的深度度量损失之一，其目标是最小化类内距离并最大化类间距离。随着深度神经网络的发展，基于三元组损失的变体大量出现
    [[22](#bib.bib22), [110](#bib.bib110), [111](#bib.bib111), [113](#bib.bib113)]。三元组损失函数可以表示为：
- en: '|  | $\displaystyle\mathcal{L}_{trip}=\left[m+d(x_{a},x_{p})-d(x_{a},x_{n})\right]_{+}$
    |  | (5) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{trip}=\left[m+d(x_{a},x_{p})-d(x_{a},x_{n})\right]_{+}$
    |  | (5) |'
- en: Different from contrast loss, the input of triplet loss is a triplet consisting
    of three images. Each triplet contains a pair of positive samples and a negative
    sample, where $x_{a}$ is the Anchor image, $x_{p}$ is the Positive image, and
    $x_{n}$ is the Negative image, and the pedestrians of $x_{a}$ and $x_{p}$ have
    the same ID. The pedestrians of $x_{a}$ and $x_{n}$ have different IDs. By model
    training, the distance between $x_{a}$ and $x_{p}$ in the Euclidean space is made
    closer than the distance between $x_{n}$ and $x_{a}$. To improve the performance
    of models, some deep learning-based person Re-ID methods use a combination of
    classification loss and triplet loss [[114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118)]. Experiments have
    shown that combining these two losses facilitates the model to learn discriminative
    features. Traditional triplet loss randomly selects three images from the training
    set during training, which may result in a simple combination of samples and lacks
    the training of hard sample combinations and makes the training model less generalizable.
    For this reason, some researchers considered improving triplet loss for mining
    hard samples [[22](#bib.bib22), [119](#bib.bib119), [120](#bib.bib120)].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Quadruplet loss
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another improvement for the triplet loss is to add a negative sample picture
    $X_{n2}$ to form a quadruplet loss [[23](#bib.bib23)], where negative sample $X_{n1}$
    and negative sample $X_{n2}$ have different pedestrian IDs. The expression of
    the quadruplet loss function is:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{quad}=\left[m_{1}+d(x_{a},x_{p})-d(x_{a},x_{n1})\right]_{+}+$
    |  | (6) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left[m_{2}+d(x_{a},x_{p})-d(x_{n1},x_{n2})\right]_{+}$
    |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: Where $m_{1}$ and $m_{2}$ are custom training thresholds. The positive and negative
    sample pairs have the same anchored image $x_{a}$. The first term of $\mathcal{L}_{quad}$
    is identical to the triplet loss function, which is used to constrain the relative
    distance between positive and negative sample pairs. The traditional triplet loss
    function often increases the inter-class distance of negatives sample pairs, which
    affects the feature learning of image $x_{a}$. For this reason, $\mathcal{L}_{quad}$
    introduces a second term to constrain the absolute distance between positive and
    negative sample pairs. The positive and negative sample pairs in the second term
    have different anchor images, which can effectively reduce the intra-class distance
    of positive sample pairs while increasing the distance of negative sample pairs
    between classes. In order to make the first term play a dominant role, it is usually
    important to ensure that ${m}_{1}>{m}_{2}$ during the training process. However,
    most person Re-ID methods using triplet loss drive focused more on differentiating
    appearance differences and cannot effectively learn fine-grained features. To
    address this issue, Yan et al. [[121](#bib.bib121)] introduce a novel pairwise
    loss function that enables Re-ID models to learn the fine-grained features by
    adaptively enforcing an exponential penalization.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Local feature learning
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the features extracted from pedestrian images for classification, person
    Re-ID methods can be classified into global feature learning-based methods and
    local feature learning-based methods. The global feature learning methods usually
    extract one feature of the pedestrian image [[122](#bib.bib122), [123](#bib.bib123),
    [124](#bib.bib124)], and it is difficult for this method to capture the detailed
    information of the pedestrian. Therefore, how to extract discriminative local
    features of pedestrians with subtle differences becomes a problem for researchers
    to focus on.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The local feature learning-based methods aim at learning pedestrian discriminative
    features and ensuring the alignment of each local feature. Manual annotation or
    neural networks are usually used to automatically focus on certain local regions
    with key information and extract the distinguishing features from these regions.
    Commonly used local feature learning methods are predefined stripe segmentation
    [[24](#bib.bib24), [25](#bib.bib25), [27](#bib.bib27), [108](#bib.bib108), [125](#bib.bib125),
    [126](#bib.bib126)], multi-scale fusion [[127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131)], soft attention [[26](#bib.bib26),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [132](#bib.bib132), [133](#bib.bib133)], pedestrian semantic
    extraction [[114](#bib.bib114), [27](#bib.bib27), [28](#bib.bib28), [31](#bib.bib31),
    [134](#bib.bib134)] and global-local feature learning [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)].
    These methods can alleviate the problems of occlusion, boundary detection errors,
    view, and pose variations.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Predefined stripe segmentation
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main idea of the method based on predefined stripe segmentation is to strip
    the learned features according to some predefined division rules, which must ensure
    that the partitions are spatially aligned. Liu et al. [[125](#bib.bib125)] proposed
    an attribute and appearance-based contextual attention network, where the appearance
    network learns spatial features from the whole body, horizontal and vertical parts
    of the pedestrian. Varior et al. [[108](#bib.bib108)] divided the pedestrian image
    into several strips uniformly and extracted local features from each strip image
    block. Sun et al. [[24](#bib.bib24)] considered the content consistency within
    each stripe to propose a local convolutional baseline (PCB). The PCB uses a uniform
    feature partitioning strategy to learn local features and outputs convolutional
    features consisting of multiple stripes to enhance the consistency of each partition’s
    feature content, thus ensuring stripes are spatially aligned.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Although the above methods can extract discriminative features for the striped
    areas, it may lead to incorrect retrieval results as the model is unable to distinguish
    between the obscured and unobscured areas. To relieve occlusion, Sun et al. [[25](#bib.bib25)]
    proposed a visibility-aware local model based on PCB to ensure that local features
    are spatially aligned and avoid interference due to pedestrian occlusion by learning
    common region features that are visible in both images. Fu et al. [[126](#bib.bib126)]
    horizontally slice the deep feature maps into multiple spatial strips using various
    pyramid scales and used global average pooling and maximum pooling to obtain discriminative
    features for each strip, which was named horizontal pyramid pooling (HPP). HPP
    can ignore that interference information, mainly coming from similar clothing
    or background.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Multi-scale fusion
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Small-scale feature maps have a strong ability to represent spatial geometric
    information and can obtain detailed information of the image. Large-scale feature
    maps are good at characterizing semantic information and can get contour information
    of images. Extracting pedestrian features at multiple scales for fusion can obtain
    rich pedestrian feature representation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[127](#bib.bib127)] proposed a multi-scale triple convolutional
    neural network, which can capture pedestrian appearance features at different
    scales. Since pedestrian features learned at different scales differ or conflict,
    the direct merging of features at multiple scales may not achieve the best fusion
    effect. Therefore, researchers have started to focus on the complementary advantages
    of cross-scale implicit associations. Chen et al. [[128](#bib.bib128)] studied
    the problem of person Re-ID multi-scale feature learning and proposed a deep pyramidal
    feature learning deep neural network framework, which can overcome the differences
    in cross-scale feature learning while learning multi-scale complementary features.
    Zhou et al. [[135](#bib.bib135)] presented a Re-ID CNN termed Omni-scale network
    (OSNet) to learn features that not only captured different spatial scales but
    also encapsulated a synergistic combination of multiple scales. In traditional
    person Re-ID datasets, OSNet achieved state-of-the-art performance, despite being
    much smaller than existing Re-ID models. The challenge of large intra-class variation
    and small inter-class variation often arises in cross-camera person Re-ID tasks.
    For example, cross-camera viewpoint changes can obscure parts of the person with
    discriminative features, or pedestrians wearing similar clothes appear across
    cameras, which makes the matching of the same person incorrect.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Soft attention
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of attention is to find the areas that have a greater impact on the
    feature map and to focus the model on discriminative local parts of body appearances
    to correct misalignment, eliminate background perturbance. Due to the good performance
    of the attention mechanism in the computer vision field, it is often used as a
    local feature learning in the person Re-ID tasks. Most of the current attention-based
    person Re-ID methods tend to use soft attention which can be divided into spatial
    attention, channel attention, mixed attention, non-local attention and position
    attention. Liu et al. [[29](#bib.bib29)] proposed an attention-based deep neural
    network capable of capturing multiple attention features from the underlying to
    the semantic layer to learn fine-grained integrated features of pedestrians. Li
    et al. [[30](#bib.bib30)] used balanced attention convolutional neural networks
    to maximize the complementary information of attention features at different scales
    to solve the person Re-ID challenge for arbitrary unaligned images. To obtain
    the local fine-grained features of a person, Ning et al. [[136](#bib.bib136)]
    proposed a multi-branch attention network with diversity loss, and the local features
    were obtained via adaptive filtering by removing interference information.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The above spatial attention-based methods tend to focus only on the local discriminative
    features of pedestrians but ignore the impact of feature diversity on pedestrian
    retrieval. Chen et al. [[26](#bib.bib26)] proposed an attentional diversity network
    that used complementary channel attention module (CAM) and position attention
    module (PAM) to learn the characteristics of pedestrian diversity. Considering
    that features extracted from first-order attention such as spatial attention and
    channel attention are not discriminative in complex camera view and pose change
    scenarios [[32](#bib.bib32)]. Chen et al. [[33](#bib.bib33)] proposed a higher-order
    attention module, which modelled the complex higher-order information in the attention
    mechanism to mine discriminative attention features among pedestrians.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Semantic extraction
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some researchers used deep neural networks to extract semantic information such
    as body parts or body postures instead of bounding boxes to extract local features
    from pedestrian body parts to improve the performance of person Re-ID. Zhao et
    al. [[28](#bib.bib28)] considered the application of body structure information
    to the person Re-ID task and proposed a novel CNN, called SpindleNet. Specifically,
    firstly, SpindleNet used a body part generation network to locate 14 key points
    of body parts to extract 7 body regions of the pedestrian. Secondly, SpindleNet
    captured semantic features from different body regions using a convolutional neural
    network. Finally, SpindleNet used a tree fusion network with competing strategies
    to merge the semantic features from different body regions. SpindleNet can align
    the features of body parts over the whole image and can better highlight local
    detail information.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, not only do the body parts contain discriminative features but also
    non-body parts may contain certain key features, such as the pedestrian’s distinguishing
    backpack or handbag. Therefore, some researchers considered the alignment of non-body
    parts. Guo et al. [[114](#bib.bib114)] proposed a dual part-aligned representation
    scheme that captured distinguishing information beyond body parts using an attention
    mechanism to update the representation by exploiting the complementary information
    from both the accurate human parts and the coarse non-human parts. Miao et al.
    [[27](#bib.bib27)] proposed a pose-guided feature alignment scheme to distinguish
    information from occlusion noise by pedestrian pose bounding markers, thus aligning
    the query image and the non-occluded area of the queried image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Global-local feature learning
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Local feature learning can capture detailed information about a region of the
    pedestrian, but the reliability of local features can be affected by variations
    in pose and occlusion. Therefore, some researchers often combine fine-grained
    local features with coarse-grained global features to enhance the final feature
    representation. Wang et al. [[38](#bib.bib38)] proposed a multi-granularity feature
    learning strategy with global and local information, including one branch for
    global feature learning and two branches for local feature learning. Ming et al.
    [[137](#bib.bib137)] designed a global-local dynamic feature alignment network
    (GLDFA-Net) framework, which contained both global and local branches. The local
    sliding alignment(LSA) strategy was introduced into the local branch of GLDFA-Net
    to guide the computation of distance metrics, which can further improve the accuracy
    of the testing phase. To mitigate the impact of imprecise bounding boxes on pedestrian
    matching, Zheng et al. [[43](#bib.bib43)] proposed a coarse-grained to fine-grained
    pyramid model that integrates not only local and global information of pedestrians
    but also progressive cues from coarse to fine-grained. The model can match pedestrian
    images of different scales and retrieve pedestrian images with the same local
    identity even when the images are not aligned.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Comparison and discussion
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of experimental results of local feature learning based
    methods. $*$ represents the use of multiple deep learning methods. The bold and
    underlined numbers represent the top two results.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Methods | CUHK03 | Market-1501 | DukeMTMC-reID | Reference |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 | mAP | R-1 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| Predefined stripe segmentation | PCB+RPP[[24](#bib.bib24)] | - | - | 81.6
    | 93.8 | 69.2 | 83.3 | ECCV’18 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| CA3Net[[125](#bib.bib125)] | - | - | 80.0 | 93.2 | 70.2 | 84.6 | ACMMM’18
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| PGFA[[27](#bib.bib27)]* | - | - | 76.8 | 91.2 | 65.5 | 82.6 | ICCV’19 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| VPM[[25](#bib.bib25)] | - | - | 80.8 | 93.0 | 72.6 | 83.6 | CVPR’19 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| HPM[[126](#bib.bib126)] | 57.5 | 63.9 | 82.7 | 94.2 | 74.3 | 86.6 | AAAI’19
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| Multi-scale fusion | DPFL[[128](#bib.bib128)] | 40.5 | 43.0 | 72.6 | 88.6
    | 60.6 | 79.2 | ICCV’17 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| MSCAN[[130](#bib.bib130)] | - | 74.2 | 66.7 | 86.8 | - | - | CVPR’17 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| OSNet[[129](#bib.bib129)] | 67.8 | 72.3 | 84.9 | 94.8 | 73.5 | 88.6 | ICCV’19
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| CAM[[131](#bib.bib131)] | 64.2 | 66.6 | 84.5 | 94.7 | 72.9 | 85.8 | CVPR’19
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| Soft attention | HydraPlus[[29](#bib.bib29)] | - | 91.8 | - | 76.9 | - |
    - | ICCV’17 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| HA-CNN[[30](#bib.bib30)] | 44.4 | 41.0 | 75.7 | 91.2 | 63.8 | 80.5 | CVPR’18
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| ABD-net[[26](#bib.bib26)] | - | - | 88.3 | 95.6 | 78.6 | 89.0 | ICCV’19 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| HOA[[33](#bib.bib33)] | 72.4 | 77.2 | 85.0 | 95.1 | 77.2 | 89.1 | ICCV’19
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| AANet[[34](#bib.bib34)] | - | - | 83.4 | 93.9 | 74.3 | 87.7 | CVPR’19 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '|  | HLGAT[[132](#bib.bib132)] | 80.6 | 83.5 | 93.4 | 97.5 | 87.3 | 92.7 |
    CVPR’21 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '|  | PAT[[138](#bib.bib138)] | - | - | 88.0 | 95.4 | 78.2 | 88.8 | CVPR’21
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| Semantic extraction | Spindle[[28](#bib.bib28)] | - | 88.5 | - | 76.9 | -
    | - | CVPR’17 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| SPReID[[134](#bib.bib134)] | - | 94.3 | 83.4 | 93.7 | 73.3 | 85.9 | CVPR’18
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| P2-Net[[114](#bib.bib114)] | 73.6 | 78.3 | 85.6 | 95.2 | 73.1 | 86.5 | ICCV’19
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| Global-Local feature learning | GLAD[[39](#bib.bib39)] | - | 85.0 | 73.9
    | 89.9 | - | - | ACMMM’17 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| PDC[[40](#bib.bib40)] | - | 88.7 | 63.4 | 84.1 | - | - | ICCV’17 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| Pyramid[[43](#bib.bib43)] | 76.9 | 78.9 | 88.2 | 95.7 | 79.0 | 89.0 | CVPR’19
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| RGA[[42](#bib.bib42)] | 77.4 | 81.1 | 88.4 | 96.1 | - | - | CVPR’20 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| SCSN[[41](#bib.bib41)] | 84.0 | 86.8 | 88.5 | 95.7 | 79.0 | 91.0 | CVPR’20
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '[Table 4](#S3.T4 "Table 4 ‣ 3.2.6 Comparison and discussion ‣ 3.2 Local feature
    learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the experimental results
    of the local feature learning methods on CUHK03, Market1501 and DukeMTMC-reID
    datasets. These results are all experimental results without Re-ranking [[8](#bib.bib8)].
    In a general view, the experimental performance of the semantic extraction and
    global-local feature learning methods is significantly higher than that of the
    methods with predefined stripe segmentation, multi-scale fusion and part attention.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In general, the predefined stripe segmentation method is simple and easy to
    implement, but it is hard segmentation and requires high image alignment. With
    the change of real scene camera view and pedestrian pose, the hard segmentation
    strategy cannot solve the problem of unaligned pedestrians well. The multi-scale
    fusion method can learn the deeper cues of pedestrian images, but there will be
    redundancy and conflicting features at different scales. The attention focuses
    only on the local features of key parts of pedestrians, and easily ignores the
    distinguishing features of non-focus regions. The semantic extraction method can
    precisely locate the local features of pedestrians by learning the structural
    information of pedestrian pose, but it requires the additional computation of
    pedestrian pose models. The global-local feature learning method can effectively
    utilize the complementary advantages of global features and local features and
    is one of the common methods used by researchers to improve model performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Generative adversarial learning
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 2014, Goodfellow et al. [[139](#bib.bib139)] first proposed generative adversarial
    networks (GAN) and rapidly developed in recent years. A large number of variants
    and applications of GAN emerged [[45](#bib.bib45), [47](#bib.bib47), [51](#bib.bib51),
    [140](#bib.bib140), [54](#bib.bib54), [55](#bib.bib55), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143)]. Image generation, as one of the significant applications
    of GAN, was widely used in the field of person Re-ID. [Figure 7](#S3.F7 "Fig.
    7 ‣ 3.3 Generative adversarial learning ‣ 3 Deep Learning Based Re-ID Method ‣
    Deep learning-based person re-identification methods: A survey and outlook of
    recent works") shows the workflow diagram of GAN used to generate the image. In
    the training phase, the generator ${G}_{AB}$ converts image $A$ into image $B$
    with random noise, the generator ${G}_{BA}$ converts image $B$ into image $A$,
    and the discriminator ${D}_{B}$ determines whether the generated image $B$ approximates
    the original image $B$ style (Real or Fake). The generator and discriminator keep
    adversarial until convergence by minimizing the discriminator loss and ${L}_{2}$
    loss [[2](#bib.bib2)].'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Some researchers used GAN to transform the style of images or unify different
    image styles to mitigate image style differences between various datasets or within
    the same dataset [[45](#bib.bib45), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [144](#bib.bib144), [51](#bib.bib51), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147)]. Some works used GAN to synthesize pedestrian images with
    a different pose, appearance, lighting, and resolution for expanding the dataset
    to improve the generalization ability of the model [[52](#bib.bib52), [53](#bib.bib53),
    [148](#bib.bib148), [149](#bib.bib149), [140](#bib.bib140), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152), [54](#bib.bib54), [153](#bib.bib153)].
    Some researchers also used GAN to learn features that are not noise-related but
    identity-related to improve the accuracy of feature matching [[46](#bib.bib46),
    [55](#bib.bib55), [141](#bib.bib141), [154](#bib.bib154)]. These methods can alleviate
    the small number of training samples, resolution, illumination, view, and pose
    variation. Based on the characteristics and application scenarios of GAN, we classify
    the generative adversarial learning-based person Re-ID methods into three categories:
    image-image style transfer, data enhancement, and invariant feature learning.
    For image-image style transfer methods, GAN learned the background, resolution,
    lighting and other features of an image and transferred these features to other
    images to give other images a different style. For data enhancement methods, the
    diversity of samples that can be generated by GAN to expand the dataset was used
    to reinforce the final feature representation. For invariant feature learning,
    GAN was used for disentangled representation learning, which can learn identity-related
    but noise-independent features (e.g., pose, lighting, resolution, etc.).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c1472463149f6b076dd3aa0beb31e71.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: Workflow diagram of GAN for image generation.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Image-image style transfer
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Domain gaps usually exist between different datasets in the person Re-ID task
    [[45](#bib.bib45)]. When training and testing on various datasets separately,
    the performance of the model is severely degraded, which hinders the effective
    generalization of the model to new test sets [[155](#bib.bib155), [156](#bib.bib156)].
    A common strategy to solve such as domain gap is to use GAN to perform style transformation
    across data domains. Since CycleGAN [[142](#bib.bib142)] implemented conversion
    of any two image styles, researchers considered improving on this to achieve adaptive
    pedestrian style conversion between different datasets to reduce or eliminate
    domain aberrations. Inspired by CycleGAN, Wei et al. [[45](#bib.bib45)] proposed
    a person transfer generative adversarial network (PTGAN) to transfer the pedestrians
    in the source domain to the target dataset while preserving the identity of the
    pedestrians in the source domain so that the pedestrians in the source domain
    have the background and lighting styles of the target domain. The pedestrians
    in the source domain are transferred to the target dataset so that the pedestrians
    in the source domain have the background and lighting patterns of the target domain.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Deng et al. [[47](#bib.bib47)] used twin networks and CycleGAN to form a similarity
    preserving generative adversarial network (SPGAN) to migrate labeled pedestrians
    from the source domain to the target domain in an unsupervised manner. Liu et
    al. [[48](#bib.bib48)] proposed an adaptive transfer network (ATNet). ATNet used
    three CycleGANs to implement the style of camera view, lighting, resolution and
    adaptively assign weights to each CycleGAN according to the degree of influence
    of different factors. Zhong et al. [[50](#bib.bib50)] proposed hetero-homogeneous
    learning (HHL) method that considers not only the domain differences between various
    datasets but also the effect of style differences of cameras within the target
    domain on the cross-domain adaptation person Re-ID performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Zhong et al. [[51](#bib.bib51)] introduced camera style(CamStyle) to solve the
    problem of style variation among different cameras within the same dataset. CamStyle
    used CycleGAN to migrate the labeled training data to various cameras so that
    the synthesized samples had the styles of different cameras while retaining the
    pedestrian labels. In addition, CamStyle can also smooth out style differences
    between various cameras in the same dataset.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Data enhancement
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike style transformation that uses GAN to reduce domain gaps, the data augmentation-based
    methods start with the training of the model and improve the generalization ability
    of the model by increasing the diversity of the training data. Zheng et al. [[52](#bib.bib52)]
    were the first to use deep convolutional generative adversarial networks (DCGAN)
    [[157](#bib.bib157)] to generate sample data. Huang et al. [[53](#bib.bib53)]
    proposed a multi-pseudo regularized label (MpRL), which assigned an appropriate
    virtual label to each generated sample to establish the correspondence between
    the real image and the generated image. MpRL effectively distinguished various
    generated data and achieved good recognition results on datasets such as Market-1501,
    DukeMTMC-Reid and CUHK03.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[140](#bib.bib140)] introduced pedestrian pose information to assist
    GAN in generating samples. The GAN was used to generate sample images with both
    the pose structure of pedestrians in MARS data and the appearance of pedestrians
    in the existing dataset. Qian et al. [[150](#bib.bib150)] used pose normalization
    GAN (PN-GAN) to generate pedestrian images with uniform body pose. To alleviate
    the problem that the pose of pedestrian images generated by earlier methods is
    prone to large deviations, Zhu et al. [[151](#bib.bib151)] trained a discriminator
    using a multilayer cascaded attention network. The discriminator can efficiently
    optimize the pose transformation of pedestrians using pose and appearance features
    so that the generated pedestrian images have the better pose and appearance consistency
    with the input images.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison of experimental results of different GAN-based methods
    on CUHK03, Market1501 and DukeMTMC-reID datasets.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Methods | CUHK03 | Market-1501 | DukeMTMC-reID | Reference |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 | mAP | R-1 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| Image-image style transfer | IDE[[3](#bib.bib3)]+CameraStyle[[51](#bib.bib51)]
    | - | - | 68.7 | 88.1 | 57.6 | 78.3 | CVPR’18 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| IDE[[3](#bib.bib3)]+UnityStyle[[145](#bib.bib145)] | - | - | 89.3 | 93.2
    | 65.2 | 82.1 | CVPR’20 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| Data enhancement | LSRO[[52](#bib.bib52)] | 87.4 | 84.6 | 66.1 | 84.0 | 47.1
    | 67.7 | ICCV’17 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| PNGAN[[150](#bib.bib150)] | - | 79.8 | 72.6 | 89.4 | 53.2 | 73.6 | ECCV’18
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| PT[[140](#bib.bib140)] | 42.0 | 45.1 | 68.9 | 87.6 | 56.9 | 78.5 | CVPR’18
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| MpRL[[53](#bib.bib53)] | 87.5 | 85.4 | 67.5 | 85.8 | 58.6 | 78.8 | TIP’19
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| DG-Net[[54](#bib.bib54)] | - | - | 86.0 | 94.8 | 74.8 | 86.6 | CVPR’19 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| Invariant feature learning | FD-GAN[[55](#bib.bib55)] | 91.3 | 92.6 | 77.7
    | 90.5 | 64.5 | 80.0 | NIPS’18 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| RAIN[[141](#bib.bib141)] | - | 78.9 | - | - | - | - | AAAI’19 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| CAD-Net[[154](#bib.bib154)] | - | 82.1 | - | 83.7 | - | 75.6 | ICCV’19 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| DI-REID[[46](#bib.bib46)] | - | 85.7 | - | - | - | - | CVPR’20 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: 3.3.3 Invariant feature learning
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GAN can be used for feature learning in addition to synthesizing images. Generally,
    the person Re-ID task in the real-world consists of high-level and low-level vision
    variations [[46](#bib.bib46)]. The former mainly includes changes in pedestrian
    occlusion, pose, and camera view, and the latter mainly includes changes in resolution,
    illumination, and weather. The images obtained from low-level vision changes are
    usually called degraded images. These vision changes may lead to the loss of discriminative
    feature information, which may cause feature mismatch and significantly degrade
    the retrieval performance [[158](#bib.bib158)].
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: For pose changes in high-level vision, Ge et al. [[55](#bib.bib55)] proposed
    feature distilling generative adversarial network (FD-GAN) to learn features related
    to pedestrian identity instead of pose for pedestrians. The method requires no
    additional computational cost or auxiliary attitude information and has advanced
    experimental results on the Market-1501, CUHK03 and DukeMTMC-reID.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Several researchers considered using GAN to learn common invariant features
    of low- and high-resolution pedestrian images. Chen et al. [[141](#bib.bib141)]
    proposed an end-to-end resolution adaptation and re-identification network (RAIN)
    that learned and aligned invariant features of pedestrian images of different
    resolutions by adding adversarial losses to low- and high-resolution image features.
    Li et al. [[154](#bib.bib154)] proposed adversarial learning strategies for cross
    resolution, which not only learned invariant features of pedestrian images of
    different resolutions but also recovered the lost fine-grained detail information
    of low-resolution images using super-resolution (SR).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparison of experimental results of different GAN-based cross-domain
    methods.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | D → M | M→ D | Reference |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| LOMO[[159](#bib.bib159)] | 8.0 | 27.2 | 4.8 | 12.3 | CVPR’15 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| Bow[[91](#bib.bib91)] | 14.8 | 35.8 | 8.3 | 17.1 | ICCV’15 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| UMDL[[160](#bib.bib160)] | 12.4 | 34.5 | 7.3 | 18.5 | CVPR’16 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| CAMEL[[16](#bib.bib16)] | 26.3 | 54.5 | - | - | ICCV’17 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| PUL[[161](#bib.bib161)] | 20.5 | 45.5 | 16.4 | 30.0 | MM’18 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| CycleGAN[[142](#bib.bib142)] | 19.1 | 45.6 | 19.6 | 38.1 | CVPR’17 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| PTGAN[[45](#bib.bib45)] | - | 38.6 | 27.4 | - | CVPR’18 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| SPGAN[[47](#bib.bib47)] | 22.8 | 51.5 | 22.3 | 41.4 | CVPR’18 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| HHL[[50](#bib.bib50)] | 31.4 | 62.2 | 27.2 | 46.9 | ECCV’18 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| ATNet[[48](#bib.bib48)] | 25.6 | 55.7 | 24.9 | 45.1 | CVPR’19 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| CR-GAN[[49](#bib.bib49)] | 29.6 | 59.6 | 30.0 | 52.2 | ICCV’19 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| DG-Net++[[162](#bib.bib162)] | 61.7 | 82.1 | 63.8 | 78.9 | ECCV’20 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| GCL[[153](#bib.bib153)] | 75.4 | 90.5 | 67.6 | 81.9 | CVPR’21 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: 3.3.4 Comparison and discussion
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Table 5](#S3.T5 "Table 5 ‣ 3.3.2 Data enhancement ‣ 3.3 Generative adversarial
    learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the experimental results
    of GAN-based methods on CUHK03, Market1501 and DukeMTMC-reID datasets. These results
    are all experimental results without Re-ranking. Both IDE [[3](#bib.bib3)] + CameraStyle
    [[51](#bib.bib51)] and IDE [[3](#bib.bib3)] + UnityGAN [[145](#bib.bib145)] methods
    use GAN to generate pedestrian images with different camera styles within the
    same dataset with great experimental performance obtained in the Market-1501 and
    DukeMTMC-reID datasets. FD-GAN [[55](#bib.bib55)] has the highest performance
    in CUKH03, and it learns features related to pedestrian identity and poses independently,
    effectively reducing the influence of pose on the accuracy of person Re-ID.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Market-1501 (M) and DukeMTMC-reID (D) datasets as the source and
    target domains, respectively. [Table 6](#S3.T6 "Table 6 ‣ 3.3.3 Invariant feature
    learning ‣ 3.3 Generative adversarial learning ‣ 3 Deep Learning Based Re-ID Method
    ‣ Deep learning-based person re-identification methods: A survey and outlook of
    recent works") compares traditional manual feature extraction methods (LOMO [[159](#bib.bib159)]
    and Bow [[91](#bib.bib91)]), traditional unsupervised methods (UMDL [[160](#bib.bib160)],
    CAMEL [[16](#bib.bib16)] and PUL [[161](#bib.bib161)]), and GAN-based cross-domain
    style transfer methods (CycleGAN(base) [[142](#bib.bib142)], PTGAN [[47](#bib.bib47)],
    SPGAN [[47](#bib.bib47)], HHL [[50](#bib.bib50)], ATNet [[48](#bib.bib48)] and
    CR-GAN [[49](#bib.bib49)]). From the experimental results, the cross-domain style
    transformation method is significantly better than the traditional unsupervised
    learning and manual feature learning methods.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: In general, the method of image-to-image style transformation smooths the style
    variation of pedestrian images in different domains. Such methods can obtain a
    large number of automatically labeled synthetic images with the target domain
    style, which can be used together with the original images to enhance the training
    set and mitigate the domain gaps between different datasets. The problem with
    these methods is that the synthetic images contain noise, which may conflict with
    the source domain images when used for model training and affect the learning
    of discriminative features by the model. The method of generating diverse pedestrian
    images using GAN alleviates the problem of insufficient available training data
    to a certain extent. The method of image synthesis without auxiliary information
    guidance cannot generate high-quality images with sufficient distinguishing information.
    Auxiliary information-guided image synthesis methods require complex network structures
    to learn various pedestrian poses, which adds additional training costs. Invariant
    feature learning methods can alleviate the problem of unaligned pedestrian features
    and improve the accuracy of person Re-ID by learning features related to pedestrian
    identity but not to pose, resolution and illumination.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Sequence feature learning
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There have been many researchers who have used the rich information contained
    in video sequences for person Re-ID. These sequence feature learning-based methods
    take short videos as input and use both spatial and temporal complementary cues,
    which can alleviate the limitations of appearance-based features. Most of these
    methods use optical flow information [[56](#bib.bib56), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165), [166](#bib.bib166)], 3-dimensional convolutional neural networks
    (3DCNNs) [[57](#bib.bib57), [167](#bib.bib167)], recurrent neural networks(RNN)
    or long short term memory(LSTM) [[164](#bib.bib164), [165](#bib.bib165), [168](#bib.bib168),
    [169](#bib.bib169)], spatial-temporal attention [[58](#bib.bib58), [59](#bib.bib59),
    [166](#bib.bib166), [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173)] or graph convolutional networks (GCN) [[62](#bib.bib62), [61](#bib.bib61),
    [60](#bib.bib60), [174](#bib.bib174)] to model the spatial-temporal information
    of video sequences. These methods can mitigate occlusions, resolution changes,
    illumination changes, view and pose variations, etc.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Optical flow
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The optical flow method uses the change of pixels in the video sequence in the
    time domain and the correlation of the spatial-temporal context of adjacent frames
    to obtain the correspondence between the previous frame and the current frame.
    This method can obtain the motion information of the target between adjacent frames.
    Chung et al. [[56](#bib.bib56)] proposed a dual-stream convolutional neural network
    (DSCNN), where each stream is a siamese network. DSCNN can model both RGB images
    and optical flow, learning spatial and temporal information separately, allowing
    each siamese network to extract the best feature representation. Liu et al. [[163](#bib.bib163)]
    proposed an accumulative context network (AMOC), which consists of two input sequences,
    feeding the original RGB image and the optical flow image containing motion information,
    respectively. AMOC was used to improve the accuracy of person Re-ID by learning
    the discriminative cumulative motion context information of video sequences. The
    optical flow method was often used in combination with other methods such as McLaughlin
    et al. [[165](#bib.bib165)] who used optical flow information and RGB colors of
    images to capture motion and appearance information, combined with RNN to extract
    complete pedestrian appearance features of video sequences.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 3D convolutional neural network
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Three-dimensional convolutional neural networks (3DC-NN) are capable of capturing
    temporal and spatial feature information in videos. Recently, some researchers
    have applied 3DCNN to video-based person Re-ID with good results. Liao et al.
    [[57](#bib.bib57)] proposed a video person Re-ID method based on a combination
    of 3DCNN and non-local attention. 3DCNN used 3D convolution on video sequences
    to extract aggregated representations of spatial and temporal features and used
    non-local spatial-temporal attention to solve the alignment problem of deformed
    images. Although 3DCNN exhibited better performance, the stacked 3D convolution
    led to significant growth of parameters. Too many parameters not only made 3DCNN
    computationally expensive but also led to difficulties in model training and optimization.
    This made 3DCNN not readily applicable on video sequence-based person Re-ID, where
    the training set was commonly small and person ID annotation was expensive. To
    explore rich temporal cues for person Re-ID while mitigating the shortcomings
    of existing 3DCNN models, Li et al. [[167](#bib.bib167)] proposed a dual-stream
    multiscale 3D convolutional neural network (M3DCNN) for extracting spatial-temporal
    cues for video-based person Re-ID. M3DCNN was also more efficient and easier to
    optimize than the existing 3DCNN.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 RNN or LSTM
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RNNs or LSTM can extract temporal features and are often applied in video-based
    person Re-ID tasks. McLaughlin et al. [[165](#bib.bib165)] proposed a novel recursive
    convolutional network(RCN), which used CNN to extract spatial features of video
    frames and RNNs to extract temporal features of video sequences. Yan et al. [[169](#bib.bib169)]
    used a recurrent feature aggregation network based on LSTM, which obtained cumulative
    discriminative features from the first LSTM node to the deepest LSTM node and
    effectively alleviated interference caused by occlusion, background clutter and
    detection failure. Chen et al. [[164](#bib.bib164)] decomposed a video sequence
    into multiple segments and used LSTM to learn the segments where the probe images
    are located in temporal and spatial features. This method reduces the variation
    of identical pedestrians in the sample and facilitates the learning of similarity
    features. Both types of methods mentioned above process each video frame independently.
    The features extracted by LSTM are generally affected by the length of the video
    sequence. The RNN only establishes temporal associations on high-level features
    and thus cannot capture the temporal cues of local details of the image [[167](#bib.bib167)].
    Therefore, there is still a need to explore a more efficient method for extracting
    spatial-temporal features.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Spatial-temporal attention
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention mechanism can selectively focus on the useful local information
    and has good performance in solving the problems of camera view switching, lighting
    changes and occlusion in person Re-ID tasks. Recently, several researchers have
    used attentional mechanisms to solve video-based person Re-ID tasks in both temporal
    and spatial dimensions. To solve the problem of unalignment and occlusion caused
    by changes in pedestrian body pose and camera view in video sequences, Li et al.
    [[170](#bib.bib170)] proposed a spatial-temporal attention model, the core idea
    of which is to use multiple spatial attention to extract features of key body
    parts and use temporal attention to compute the combined feature representations
    extracted by each spatial attention model. This method can better mine the potential
    distinguishing feature representations in video sequences. Similarly, Fu et al.
    [[58](#bib.bib58)] proposed a spatial-temporal attention framework that can fully
    utilize the distinguishing features of each pedestrian in both temporal and spatial
    dimensions through video frame selection, local feature mining, and feature fusion.
    The approach can well address challenges such as pedestrian pose variation and
    partial occlusion. Xu et al. [[166](#bib.bib166)] proposed a joint temporal and
    spatial attention pooling network to learn the feature representations of video
    sequences through the interdependence between video sequences.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Graph convolutional networks
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In recent years, graph convolutional networks (GCNs) have been widely used
    for person Re-ID tasks due to their powerful automatic relational modeling capabilities
    [[175](#bib.bib175)], and a large number of variant networks have emerged [[60](#bib.bib60),
    [63](#bib.bib63), [64](#bib.bib64), [62](#bib.bib62), [65](#bib.bib65)]. Yang
    et al. [[60](#bib.bib60)] proposed a unified spatial-temporal graph convolutional
    neural network that modelled video sequences in three dimensions: temporal, spatial
    and appearance, and to mine more discriminative and robust information. Wu et
    al. [[62](#bib.bib62)] proposed an adaptive graph representation learning scheme
    for video person Re-ID using pose alignment connections and feature similarity
    connections to construct adaptive structure-aware adjacency graphs.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Yan et al. [[63](#bib.bib63)] proposed a framework for pedestrian retrieval
    based on contextual graphical convolutional networks. Since image appearance features
    are not sufficient to distinguish different people, the authors use contextual
    information to extend instance-level features to improve the discriminative power
    of the features and the robustness of person retrieval. Shen et al. [[64](#bib.bib64)]
    proposed a similarity-guided graph neural network that represents pairwise relationships
    between probe-gallery image pairs (nodes) by creating a graph and using this relationship
    to enhance the learning of discriminative features. This updated probe-gallery
    image is used to predict the relational features for accurate similarity estimation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The last group of [Table 7](#S3.T7 "Table 7 ‣ 3.4.6 Comparison and discussion
    ‣ 3.4 Sequence feature learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works") shows
    the experimental results of the GCN-based sequence feature learning methods on
    MARS, DukeMTMC-VideoReID iLIDS-VID and PRID-2011 datasets. From the results in
    the above table, the experimental performance of the GCN-based methods is significantly
    better than the other sequence feature learning methods. In particular, CTL [[174](#bib.bib174)]
    achieved Rank-1 accuracy of 91.4% and mAP of 86.7% on MARS. CTL utilized a CNN
    backbone and a key-points estimator to extract semantic local features from the
    human body at multiple granularities as graph nodes. CTL effectively mined comprehensive
    cues complementary to appearance information to enhance the representation capability.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.6 Comparison and discussion
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 7: Comparison of experimental results of video sequence feature learning-based
    methods.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Methods | MARS | DukeMTMC-VideoReID | iLIDS-VID | PRID-2011
    | Reference |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 | R-1 | R-5 | R-1 | R-5 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| Optical flow | RCN[[165](#bib.bib165)]* | - | - | - | - | 58.0 | 84.0 | 70.0
    | 90.0 | CVPR’16 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| TSSCNN[[56](#bib.bib56)] | - | - | - | - | 60.0 | 86.0 | 78.0 | 94.0 | ICCV’17
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| ASTPN[[166](#bib.bib166)] | - | 44.0 | - | - | 62.0 | 86.0 | 70.0 | 90.0
    | ICCV’17 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| AMOC[[163](#bib.bib163)] | 52.9 | 68.3 | - | - | 68.7 | 94.3 | 83.7 | 98.3
    | TCSVT’18 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| CSACSE[[164](#bib.bib164)]* | 69.4 | 81.2 | - | - | 79.8 | 91.8 | 81.2 |
    92.1 | CVPR’18 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| CSACSE+OF[[164](#bib.bib164)]* | 76.1 | 86.3 | - | - | 85.4 | 96.7 | 93.0
    | 99.3 | CVPR’18 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| 3DCNN | 3DCNN+NLA[[57](#bib.bib57)] | 77.0 | 84.3 | - | - | 81.3 | - | 91.2
    | - | ACCV’18 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| M3D[[167](#bib.bib167)] | 74.1 | 84.4 | - | - | 74.0 | 94.3 | 94.4 | 100.0
    | AAAI’19 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| RNN or LSTM | RFA[[169](#bib.bib169)] | - | - | - | - | 49.3 | 76.8 | 58.2
    | 85.8 | ECCV’16 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| SFT[[168](#bib.bib168)] | 50.7 | 70.6 | - | - | 55.2 | 86.5 | 79.4 | 94.4
    | CVPR’17 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| Spatial-temporal attention | DRSA[[170](#bib.bib170)] | 65.8 | 82.3 | - |
    - | 80.2 | - | 93.2 | - | CVPR’18 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| GLTR[[172](#bib.bib172)] | 78.5 | 87.0 | 93.7 | 96.3 | 86.0 | 98.0 | 95.5
    | 100.0 | ICCV’19 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| VRSTC[[59](#bib.bib59)] | 82.3 | 88.5 | 93.5 | 95.0 | 83.4 | 95.5 | - | -
    | CVPR’19 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| STA[[58](#bib.bib58)] | 80.8 | 86.3 | 94.9 | 96.2 | - | - | - | - | AAAI’19
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| MG-RAFA[[171](#bib.bib171)] | 85.6 | 88.8 | - | - | 88.6 | 98.0 | 95.9 |
    99.7 | CVPR’20 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '|  | BiCnet-TKS[[173](#bib.bib173)] | 86.0 | 90.2 | 96.1 | 96.3 | 75.1 | 84.6
    | - | - | CVPR’21 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| GCN | AdaptiveGraph[[62](#bib.bib62)] | 81.9 | 89.5 | 95.4 | 97.0 | 84.5
    | 96.7 | 94.6 | 99.1 | TIP’20 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| MGH[[61](#bib.bib61)] | 85.8 | 90.0 | - | - | 85.6 | 97.1 | 94.8 | 99.3 |
    CVPR’20 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| STGCN[[60](#bib.bib60)] | 83.7 | 89.9 | 95.7 | 97.3 | - | - | - | - | CVPR’20
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '|  | CTL[[174](#bib.bib174)] | 86.7 | 91.4 | - | - | - | 89.7 | - | - | CVPR’21
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '[Table 7](#S3.T7 "Table 7 ‣ 3.4.6 Comparison and discussion ‣ 3.4 Sequence
    feature learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person
    re-identification methods: A survey and outlook of recent works") shows the experimental
    results of sequence feature learning-based methods on MARS, DukeMTMC-VideoReID,
    iLIDS-VID and PRID-2011 datasets. Some researchers used GCN to model the Spatio-temporal
    relationships of video sequences and achieved good results. Compared with optical
    flow, 3DCNN, and RNN or LSTM methods, spatial-temporal attention-based methods
    can obtain better experimental performance. AdaptiveGraph [[62](#bib.bib62)],
    MGH [[61](#bib.bib61)] and STGCN [[60](#bib.bib60)] were able to obtain high experimental
    results on the above datasets, with some improvement in accuracy compared to the
    previous types of methods.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The core idea of the sequence feature learning-based methods is to fuse more
    spatial-temporal information from multiple dimensions to mitigate the effects
    of a range of problems such as occlusion, illumination, and viewpoint changes
    in the person Re-ID tasks. Although Optical flow can provide contextual information
    of video sequence frames, it only represents the local dynamics of adjacent images,
    which may introduce noise due to spatial misalignment. The process of computing
    optical flow is time-consuming. 3DCNN can capture both temporal and spatial feature
    information in video sequences. Although 3DCNN can achieve better performance,
    these methods are computationally time-consuming and hard to optimize. RNN or
    LSTM can extract temporal features of video sequences and has been popular among
    researchers for some time. In the task of person Re-ID, the ability of RNN or
    LSTM have limited features for temporal information extraction and suffer from
    difficulties in model training due to the complex network structure [[176](#bib.bib176)].
    Although the introduction of temporal attention and spatial attention can alleviate
    the problem of switching between different camera views in a row, lighting changes
    and occlusion, the accuracy of person Re-ID is affected because of the temporal
    relationship between body parts in different frames is not fully considered [[60](#bib.bib60)].
    Most of the current person Re-ID studies are image-based, and a large number of
    methods and datasets closer to the real world have emerged. Compared to image-based
    methods, sequence feature learning-based methods still hold great research promise.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion and future directions
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents a comprehensive survey with in-depth discussion for deep
    learning-based person Re-ID methods in recent years. Firstly, we summarize the
    main contributions of several recently published person Re-ID surveys and discuss
    the common datasets used for person Re-ID benchmarks. Secondly, we comprehensively
    review the current deep learning-based methods, these methods are classified into
    four major categories according to metric learning and representation learning,
    including deep metric learning, local feature learning, generative adversarial
    learning and sequence feature learning. We subdivide the above four categories
    according to their methodologies and motivations, analyzing and discussing the
    advantages and limitations of each subcategory of the method. This classification
    is more suitable for researchers to explore these methods from their practical
    needs.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Although existing deep learning-based methods have achieved good results in
    person Re-ID tasks, they still face many challenges. Most of the datasets currently
    applied for person Re-ID training are processed visible images or videos, but
    real-world data often exhibit a combination of multiple modalities. Although semi-supervised
    and unsupervised methods can alleviate the problem of high labeling costs, they
    still do not perform as well as supervised methods. There are domain differences
    in pedestrian images captured by different cameras, and models trained on one
    dataset can experience severe performance degradation when tested on another dataset.
    Because people may change their clothes or different people may wear very similar
    clothes, the appearance features of pedestrians will become unreliable for person
    Re-ID. In addition, how to improve the speed and accuracy of model retrieval is
    critical for real-world model deployment. The increase of privacy scenarios fundamentally
    limits the traditional centralized person Re-ID methods. The detection and re-identification
    modules of most person Re-ID systems are separated from each other, making it
    difficult to expand to real-world applications.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: In summary, there are still many challenges to be explored and researched in
    deep learning-based person Re-ID methods. The following subsections present potential
    solutions to address the above existing challenges, as well as prospects of future
    research directions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: (1) Cross-modal person Re-ID. Most existing Re-ID methods evaluate their performance
    on publicly available datasets, which are obtained based on image or video processing.
    However, the acquisition of real-world data is diverse and the data may appear
    as a combination of different modalities (visible, infrared, depth map and text
    descriptions, etc.). For example, in the absence of sufficient visibility information
    like images or videos, text descriptions can provide unique attributes aiding
    information for person Re-ID. Several research works[[161](#bib.bib161), [177](#bib.bib177)]
    learned discriminative cross-modal visual-textual features for better similarity
    evaluation in description-based person Re-ID. Because it was difficult for visible-light
    cameras to capture valid appearance information in dark environments, some researchers
    [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180), [181](#bib.bib181),
    [182](#bib.bib182)] used thermal infrared images to learn rich visual representations
    for cross-modality matching. Existing works mainly focus on alleviating the modality
    discrepancy by aligning the distributions of features from different modalities.
    Meanwhile, how to combine various modal complementary information is also worth
    studying in the future.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: (2) High-performance semi-supervised and unsupervised person Re-ID. Because
    it was expensive to annotate person images across multiple cameras, some researchers
    [[183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185), [186](#bib.bib186),
    [187](#bib.bib187), [188](#bib.bib188)] focused on semi-supervised and unsupervised
    methods for person Re-ID. These methods aimed to learn discriminative features
    from unlabeled or minimally labeled images of people. Compared to supervised learning,
    semi-supervised and unsupervised methods alleviated the need for expensive data
    annotation and showed great potential to facilitate person Re-ID to practical
    applications. Some semi-supervised person Re-ID methods [[189](#bib.bib189), [190](#bib.bib190)]
    made use of image clustering or tracklet clustering in the target domain to adapt
    the model to the new domain. Some unsupervised person Re-ID methods [[183](#bib.bib183),
    [184](#bib.bib184), [185](#bib.bib185)] used soft labels or multi-labels to learn
    discriminative embedding features. Although lacking realistic label learning discriminative
    features, the performance of person Re-ID methods in semi-supervised and unsupervised
    scenarios was still inferior to that of supervised methods, they still maintained
    significant research value and significance in improving the generalization ability
    of the model [[2](#bib.bib2)]. In future research, better clustering or label
    assignment strategies should be considered to improve the performance of person
    Re-ID.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: (3) Domain adaptation person Re-ID. The background, resolution, and illumination
    environment of different cameras in the real world vary greatly, which interfered
    with the learning of distinguishing features of pedestrians and affected the performance
    of person Re-ID. Some researchers [[45](#bib.bib45), [47](#bib.bib47), [49](#bib.bib49),
    [48](#bib.bib48)] transferred images with identity labels from the source to the
    target domain to learn discriminative models, but they largely ignored the unlabeled
    samples and the substantial sample distributions in target domains. Some researchers
    [[155](#bib.bib155), [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193),
    [194](#bib.bib194)] used clustering or graph matching methods to predict the pseudo-labels
    in the target domain for discriminative model learning, but they still faced the
    challenge of accurately predicting hard samples labels. Domain adaption is crucial
    for person Re-ID models learned in unknown domains. Therefore, it remains one
    of the important research directions for the future.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: (4) Person Re-ID in the 3D space. In the real world, the spatial location of
    cameras is uncertain and a new camera may be temporarily inserted into an existing
    camera network. Considering people may change their clothes or different people
    may wear very similar clothes, pedestrian appearance features will become unreliable
    for Re-ID [[67](#bib.bib67)]. The 3D structure does not rely on the appearance
    information of 2D images can effectively alleviate this limitation. However, the
    acquisition of 3D point cloud data of pedestrians requires additional auxiliary
    models. Some researchers [[67](#bib.bib67), [66](#bib.bib66)] extracted 3D shape
    embeddings directly from 2D images, obtained more robust structural and appearance
    information by aligning 2D and 3D local features, or planed 3D models back to
    2D images for representation learning in 2D space for data augmentation purposes.
    Although the above studies achieved good experimental results, the 2D data space
    inherently limited the model to understand the 3D geometric information of people.
    Therefore, further exploration of person Re-ID methods in 3D space is still an
    important research direction in the future.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: (5) Fast person Re-ID. Most current methods to person Re-ID focus mainly on
    prior knowledge or designing complex network architectures to learn robust identity
    invariant feature representations. These methods use complex network models to
    extract high-dimensional features to improve model performance. However, the above
    methods use Euclidean distance to calculate the similarity of features and obtain
    the rank list by fast sorting, which will increase with the retrieval time as
    the size of the gallery library increases. This retrieval method will be very
    time-consuming, making the model unsuitable for real-world applications. Therefore,
    some researchers [[195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197)]
    considered introducing hashing to improve the retrieval speed. Faster person Re-ID
    retrieval from coarse-to-fine (CtF) [[196](#bib.bib196)] can be achieved by supplementing
    long and short hash codes to get faster and better accuracy. Zhao et al. [[197](#bib.bib197)]
    proposed saliency-guided iterative asymmetric mutual hashing (SIAMH) to achieve
    high-quality hash code generation and fast feature extraction. However, how to
    design a specific retrieval strategy to reduce the information redundancy among
    models and improve retrieval speed and accuracy still needs further research.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: (6) Decentralised learning person Re-ID. Most of the existing person Re-ID methods
    use a centralized learning paradigm, which requires collecting all training data
    from different camera views or domains for centralized training. Although these
    supervised or unsupervised methods have made significant progress, centralized
    person Re-ID learning ignores images of people that contain large amounts of personal
    and private information that may not be allowed to be shared into a central data
    set. As privacy scenarios increase, it can fundamentally limit the centralized
    learning person Re-ID methods in the real world. Several recent works [[198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)] attempted to address
    the above problem through decentralised learning. These methods either built a
    globally generalised model server through federated learning, which did not require
    access to local training data and shared of cross-domain data, or selectively
    performed knowledge aggregation to optimize the trade-off between model personalisation
    and generalization in decentralised person Re-ID. In future work, how to ensure
    understanding cross-domain data heterogeneity while learning a global generalised
    model remains challenging.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '(7) End-to-end person Re-ID system. As is shown in [Figure 2](#S1.F2 "Fig.
    2 ‣ 1 Introduction ‣ Deep learning-based person re-identification methods: A survey
    and outlook of recent works") in the introduction of this paper, person detection
    and re-identification in most current person Re-ID systems are two independent
    modules. The person Re-ID task uses the correct pedestrian already detected by
    default, but some practical open-world applications require end-to-end person
    search from the raw images or videos [[202](#bib.bib202)]. The two-stage end-to-end
    person re-identification framework was one of the most common Re-ID systems that
    systematically evaluated the advantages and limitations of combining different
    detectors and Re-ID models [[112](#bib.bib112)]. Munjal et al. [[203](#bib.bib203)]
    proposed a query-guided end-to-end person search network (QEEPS) to join person
    detection and re-identification. In addition, the end-to-end person Re-ID was
    also widely used in multi-target multi-camera tracking (MTMC tracking) [[110](#bib.bib110),
    [204](#bib.bib204), [205](#bib.bib205)]. Person Re-ID algorithms rely not only
    on accurate person detection algorithms but also on detected unlabeled pedestrians,
    which remains a current challenge. Therefore, how to effectively combine person
    detection and re-identification to design an end-to-end person Re-ID system is
    also a direction that researchers need to pay attention to in the future.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Declaration of Competing Interest
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors declare that they have no known competing financial interests or
    personal relationships that could have appeared to influence the work reported
    in this paper.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors wish to thank Jizhuo Li, Xiao Pang, Jiamin Zhu, Fuqiu Chen, Yi Zhou
    and Cheng Zhang. This work was supported by the National Key Research and Development
    Project of China (No. JG2018190), and in part by the National Natural Science
    Foundation of China (No. 61872256).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Wang [2013] X. Wang, Intelligent multi-camera video surveillance: A review,
    Pattern recognition letters. 34 (2013) 3–19.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2019] H. Luo, W. Jiang, X. Fan, S. Zhang, A survey on deep learning
    based person re-identification, Acta Automatica Sinica. 45 (2019) 2032–2049.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2016] L. Zheng, Y. Yang, A. G. Hauptmann, Person re-identification:
    Past, present and future, 2016, https://arxiv.org/pdf/1610.02984.pdf. [a͡rXiv
    preprint arXiv:1610.02984](http://arxiv.org/abs/1610.02984).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi [2018] J. Redmon, A. Farhadi, Yolov3: An incremental improvement,
    2018, https://arxiv.org/pdf/1804.02767.pdf. [a͡rXiv preprint arXiv:1804.02767](http://arxiv.org/abs/1804.02767).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu,
    A. Berg, Ssd: Single shot multibox detector, Proceedings of the European Conference
    on Computer Vision, 2016, pp. 21–37.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick [2015] R. Girshick, Fast r-cnn, Proceedings of the IEEE International
    Conference on Computer Vision, 2015, pp. 1440–1448.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. [2020] L. Qi, P. Yu, Y. Gao, Research on weak-supervised person re-identification,
    Journal of Software. 9 (2020) 2883–2902.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. [2017] Z. Zhong, L. Zheng, S. Cao, D.and Li, Re-ranking person
    re-identification with k-reciprocal encoding, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2017, pp. 3652–3661.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2013] R. Zhao, W. Ouyang, X. Wang, Person re-identification by
    salience matching, Proceedings of the IEEE International Conference on Computer
    Vision, 2013, pp. 2528–2535.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martinel et al. [2015] N. Martinel, C. Micheloni, G. Foresti, Saliency weighted
    features for person re-identification, Proceedings of the European Conference
    on Computer Vision, 2015, pp. 191–208.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. [2017] L. An, X. Chen, S. Liu, Y. Lei, S. Yang, Integrating appearance
    features and soft biometrics for person re-identification, Multimedia Tools and
    Applications. 76 (2017) 12117–12131.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2017] H. Hu, W. Fang, G. Zeng, Z. Hu, B. Li, A person re-identification
    algorithm based on pyramid color topology feature, Multimedia Tools and Applications.
    76 (2017) 26633–26646.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dikmen et al. [2011] M. Dikmen, E. Akbas, T. Huang, N. Ahuja, Pedestrian recognition
    with a learned metric, Proceedings of the Asian Conference on Computer Vision,
    2011, pp. 501–512.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2014] W. Li, R. Zhao, T. Xiao, X. Wang, Deepreid: Deep filter pairing
    neural network for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2014, pp. 152–159.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2015] Y. Chen, W. Zheng, J. Lai, Mirror representation for modeling
    view-specific transform in person re-identification, Proceedings of the International
    Joint Conference on Artificial Intelligence, 2015, pp. 3402–3408.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016] X. Wang, W. Zheng, X. Li, J. Zhang, Cross-scenario transfer
    person reidentification, IEEE Transactions on Circuits and Systems for Video Technology.
    26 (2016) 1447–1460.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. [2014] D. Yi, Z. Lei, S. Liao, S. Z. Li, Deep metric learning for
    person re-identification, Proceedings of the International Conference on Pattern
    Recognition, 2014, pp. 34–39.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2014] W. Li, R. Zhao, T. Xiao, X. Wang, Deepreid: Deep filter pairing
    neural network for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2014, pp. 152–159.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2018] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, Y. Yang, Exploit
    the unknown gradually: One-shot video-based person re-identification by stepwise
    learning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 5177–5186.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] M. Zheng, S. Karanam, Z. Wu, R. Radke, Re-identification
    with consistent attentive siamese networks, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 5735–5744.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varior et al. [2016] R. Varior, M. Haloi, G. Wang, Gated siamese convolutional
    neural network architecture for human re-identification, Proceedings of the European
    Conference on Computer Vision, 2016, pp. 791–808.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermans et al. [2017] A. Hermans, L. Beyer, B. Leibe, In defense of the triplet
    loss for person re-identification, 2017, https://arxiv.org/pdf/1703.07737.pdf.
    [a͡rXiv preprint arXiv:1703.07737](http://arxiv.org/abs/1703.07737).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2017] W. Chen, X. Chen, J. Zhang, K. Huang, Beyond triplet loss:
    A deep quadruplet network for person re-identification, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 403–412.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2018] Y. Sun, L. Zheng, Y. Yang, Q. Tian, S. Wang, Beyond part
    models: Person retrieval with refined part pooling (and a strong convolutional
    baseline), Proceedings of the European conference on computer vision, 2018, pp.
    480–496.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2019] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, J. Sun, Perceive
    where to focus: Learning visibility-aware part-level features for partial person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 393–402.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2019] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Wang,
    Abd-net: Attentive but diverse person re-identification, Proceedings of the IEEE
    International Conference on Computer Vision, 2019, pp. 8350–8360.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. [2019] J. Miao, Y. Wu, P. Liu, Y. Ding, Y. Yang, Pose-guided feature
    alignment for occluded person re-identification, Proceedings of the IEEEF International
    Conference on Computer Vision, 2019, pp. 542–551.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2017] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Tang,
    Spindle net: Person re-identification with human body region guided feature decomposition
    and fusion, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2017, pp. 1077–1085.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, X. Wang,
    Hydraplus-net: Attentive deep features for pedestrian analysis, Proceedings of
    the IEEE international conference on computer vision, 2017, pp. 350–359.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2018] W. Li, X. Zhu, S. Gong, Harmonious attention network for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2018, pp. 2285–2294.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2017] L. Zhao, X. Li, Y. Zhuang, J. Wang, Deeply-learned part-aligned
    representations for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 3239–3248.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2018] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7132–7141.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] B. Chen, W. Deng, J. Hu, Mixed high-order attention network
    for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 371–381.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. [2019] C. P. Tay, S. Roy, K. H. Yap, Aanet: Attribute attention
    network for person re-identifications, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2019, pp. 7127–7136.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ngo et al. [2005] C. W. Ngo, Z. Pan, X. Wei, X. Wu, H. K. Tan, W. Zhao, Motion
    driven approaches to shot boundary detection, low-level feature extraction and
    bbc rushes characterization at trecvid 2005, trecvid workshop participants notebook
    papers, 2005.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ngo et al. [2008] C. W. Ngo, Y. G. Jiang, X. Y. Wei, W. Zhao, F. Wang, X. Wu,
    H. K. Tan, Beyond semantic search: What you observe may not be what you think,
    trecvid workshop participants notebook papers, 2008.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei and Yang [2012] X. Y. Wei, Z. Q. Yang, Mining in-class social networks for
    large-scale pedagogical analysis, Proceedings of the ACM international conference
    on Multimedia, 2012, pp. 639–648.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] G. Wang, Y. Yuan, X. Chen, J. Li, X. Zhou, Learning discriminative
    features with multiple granularities for person re-identification, Proceedings
    of the ACM International Conference on Multimedia, 2018, pp. 274–282.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2017] L. Wei, S. Zhang, H. Yao, W. Gao, Q. Tian, Glad: Global-local-alignment
    descriptor for pedestrian retrieval, Proceedings of the ACM international conference
    on Multimedia, 2017, pp. 420–428.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. [2017] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, Q. Tian, Pose-driven
    deep convolutional model for person re-identification, Proceedings of the IEEE
    International Conference on Computer Vision, 2017, pp. 3980–3989.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] X. Chen, C. Fu, Y. Zhao, F. Zheng, J. Song, R. Ji, Y. Yang,
    Salience-guided cascaded suppression network for person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 3297–3307.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Z. Zhang, C. Lan, W. Zeng, X. Jin, Z. Chen, Relation-aware
    global attention for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2020, pp. 3183–3192.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] F. Zheng, C. Deng, X. Sun, X. Jiang, X. Guo, Z. Yu, R. Ji,
    Pyramidal person re-identification via multi-loss dynamic training, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 8506–8514.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. [2019] H. Yao, S. Zhang, R. Hong, Y. Zhang, C. Xu, Q. Tian, Deep
    representation learning with part loss for person re-identification, IEEE Transactions
    on Image Processing. 28 (2019) 2860–2871.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2018] L. Wei, S. Zhang, W. Gao, Q. Tian, Person transfer gan to
    bridge domain gap for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 79–88.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2020] Y. Huang, Z. J. Zha, X. Fu, R. Hong, L. Li, Real-world person
    re-identification via degradation invariance learning, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2020, pp. 14084–14094.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2017] W. Deng, Z. Liang, G. Kang, Y. Yi, J. Jiao, Image-image domain
    adaptation with preserved self-similarity and domain-dissimilarity for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2017, pp. 994–1003.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] J. Liu, Z. Zha, D. Chen, R. Hong, M. Wang, Adaptive transfer
    network for cross-domain person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 7202–7211.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Y. Chen, X. Zhu, S. Gong, Instance-guided context rendering
    for cross-domain person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2020, pp. 232–242.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. [2018a] Z. Zhong, L. Zheng, S. Li, Y. Yang, Generalizing a person
    retrieval model hetero and homogeneously, Proceedings of the European Conference
    on Computer Vision, 2018a, pp. 172–188.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. [2018b] Z. Zhong, Z. Liang, Z. Zheng, S. Li, Y. Yi, Camera style
    adaptation for person re-identification, Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, 2018b, pp. 5157–5166.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] Z. Zheng, Z. Liang, Y. Yi, Unlabeled samples generated by
    gan improve the person re-identification baseline in vitro, Proceedings of the
    IEEE International Conference on Computer Vision, 2017, pp. 618–626.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2019] Y. Huang, J. Xu, Q. Wu, Z. Zheng, Z. Zhang, J. Zhang, Multi-pseudo
    regularized label for generated data in person re-identification, IEEE Transactions
    on Image Processing. 28 (2019) 1391–1403.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y. Yang, J. Kautz, Joint
    discriminative and generative learning for person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 2138–2147.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. [2018] Y. Ge, Z. Li, H. Zhao, G. Yin, S. Yi, X. Wang, H. Li, Fd-gan:
    Pose-guided feature distilling gan for robust person re-identification, 2018,
    https://arxiv.org/pdf/1810.02936.pdf. [a͡rXiv preprint arXiv:1810.02936](http://arxiv.org/abs/1810.02936).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2017] D. Chung, K. Tahboub, E. Delp, A two stream siamese convolutional
    neural network for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 1983–1991.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao et al. [2019] X. Liao, L. He, Z. Yang, C. Zhang, Video-based person re-identification
    via 3d convolutional networks and non-local attention, Proceedings of the Asian
    Conference on Computer Vision, 2019, pp. 620–634.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2019] Y. Fu, X. Wang, Y. Wei, T. Huang, Sta: Spatial-temporal attention
    for large-scale video-based person re-identification, Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 33, 2019, pp. 8287–8294.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2019] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, X. Chen, Vrstc:
    Occlusion-free video person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 7183–7192.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2020] J. Yang, W. Zheng, Q. Yang, Y. Chen, Q. Tian, Spatial-temporal
    graph convolutional network for video-based person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 3289–3299.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2020] Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, Y. Tai, L. Shao,
    Learning multi-granular hypergraphs for video-based person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2020, pp. 2899–2908.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2020] Y. Wu, O. Bourahla, X. Li, F. Wu, Q. Tian, X. Zhou, Adaptive
    graph representation learning for video person re-identification, IEEE Transactions
    on Image Processing. 29 (2020) 8821–8830.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2019] Y. Yan, Q. Zhang, B. Ni, W. Zhang, M. Xu, X. Yang, Learning
    context graph for person search, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2019, pp. 2158–2167.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. [2018] Y. Shen, H. Li, S. Yi, D. Chen, X. Wang, Person re-identification
    with deep similarity-guided graph neural network, Proceedings of the European
    Conference on Computer Vision, 2018, pp. 486–504.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2021] Z. Bai, Z. Wang, J. Wang, D. Hu, E. Ding, Unsupervised multi-source
    domain adaptation for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 12914–12923.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2020] Z. Zheng, N. Zheng, Y. Yang, Parameter-efficient person
    re-identification in the 3d space, 2020, https://arxiv.org/pdf/2006.04569.pdf.
    [a͡rXiv preprint arXiv:2006.04569](http://arxiv.org/abs/2006.04569).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] J. Chen, X. Jiang, F. Wang, J. Zhang, F. Zheng, X. Sun, W. S.
    Zheng, Learning 3d shape feature for texture-insensitive person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 8146–8155.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bedagkar-Gala and Shah [2014] A. Bedagkar-Gala, S. K. Shah, A survey of approaches
    and trends in person re-identification, Image and Vision Computing. 32 (2014)
    270–286.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chahar and Nain [2017] H. Chahar, N. Nain, A study on deep convolutional neural
    network based approaches for person re-identification, Pattern Recognition and
    Machine Intelligence. (2017) 543–548.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] K. Wang, H. Wang, M. Liu, X. Xing, T. Han, Survey on person
    re-identification based on deep learning, CAAI Transactions on Intelligence Technology.
    3 (2018) 219–227.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'karanam et al. [2019] S. karanam, M. Gou, Z. Wu, A. Rates-Borras, O. Camps,
    R. Radke, A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets, IEEE Transactions on Pattern Analysis and Machine
    Intelligence. 41 (2019) 523–536.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Z. Wang, Z. Wang, Y. Zheng, Y. Wu, W. Zeng, S. Satoh, Beyond
    intra-modality: A survey of heterogeneous person re-identification, 2020, https://arxiv.org/pdf/1905.10048.pdf.
    [a͡rXiv preprint arXiv:1905.10048](http://arxiv.org/abs/1905.10048).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2019] D. Wu, S. Zheng, X. Zhang, C. Yuan, F. Cheng, Y. Zhao, Y. Lin,
    Z. Zhao, Y. Jiang, D. Huang, Deep learning-based methods for person re-identification:
    A comprehensive review, Neurocomputing. 337 (2019) 354–371.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathur et al. [2020] N. Mathur, S. Mathur, D. Mathur, P. Dadheech, A brief
    survey of deep learning techniques for person re-identification, Proceedings of
    International Conference on Emerging Technologies in Computer Engineering: Machine
    Learning and Internet of Things, 2020, pp. 129–138.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. [2020] Q. Leng, M. Ye, Q. Tian, A survey of open-world person re-identification,
    IEEE Transactions on Circuits and Systems for Video Technology. 30 (2020) 1092–1108.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam [2020] K. Islam, Person search: New paradigm of person re-identification:
    A survey and outlook of recent works, Image and Vision Computing. 101 (2020) 103970.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lavi et al. [2020] B. Lavi, I. Ullah, M. Fatan, A. Rocha, Survey on reliable
    deep learning-based person re-identification models: Are we there yet?, 2020,
    https://arxiv.org/pdf/2005.00355.pdf. [a͡rXiv preprint arXiv:2005.00355](http://arxiv.org/abs/2005.00355).'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2021] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, S. C. Hoi, Deep
    learning for person re-identification: A survey and outlook, IEEE Transactions
    on Pattern Analysis and Machine Intelligence. (2021) 1–1.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021] C. Yang, F. Qi, H. Jia, Survey on unsupervised techniques
    for person re-identification, International Conference on Computing and Data Science,
    2021, pp. 161–164.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yaghoubi et al. [2021] E. Yaghoubi, A. Kumar, H. Proença, Sss-pr: A short survey
    of surveys in person re-identification, Pattern Recognition Letters. 143 (2021)
    50–57.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2021] Y. Wang, S. Yang, S. Liu, Z. Zhang, Cross-domain person
    re-identification: A review, Artificial Intelligence in China, 2021, pp. 153–160.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2021] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain generalization:
    A survey, 2021, https://arxiv.org/pdf/2103.02503.pdf. [a͡rXiv preprint arXiv:2103.02503](http://arxiv.org/abs/2103.02503).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behera et al. [2020] N. K. S. Behera, P. K. Sa, S. Bakshi, Person re-identification
    for smart cities: State-of-the-art and the path ahead, Pattern Recognition Letters.
    138 (2020) 282–289.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] W. Wu, D. Tao, H. Li, Z. Yang, J. Cheng, Deep features for
    person re-identification on metric learning, Pattern Recognition. 110 (2021) 107424.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behera et al. [2021] N. K. S. Behera, T. K. Behera, M. Nappi, S. Bakshi, P. K.
    Sa, Futuristic person re-identification over internet of biometrics things (iobt):
    Technical potential versus practical reality, Pattern Recognition Letters. 151
    (2021) 163–171.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. [2013] Y. Bengio, A. Courville, P. Vincent, Representation learning:
    A review and new perspectives, IEEE Transactions on Pattern Analysis and Machine
    Intelligence 35 (2013) 1798–1828.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gray et al. [2007] D. Gray, S. Brennan, H. Tao, Evaluating appearance models
    for recognition, reacquisition, and tracking, IEEE international workshop on performance
    evaluation for tracking and surveillance, 2007, pp. 1–7.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2012] W. Li, R. Zhao, X. Wang, Human reidentification with transferred
    metric learning, Proceedings of the Asian conference on Computer Vision, 2012,
    pp. 31–44.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Wang [2013] W. Li, X. Wang, Locally aligned feature transforms across
    views, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2013, pp. 3594–3601.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Felzenszwalb et al. [2010] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan,
    Object detection with discriminatively trained part-based models, IEEE Transactions
    on Pattern Analysis and Machine Intelligence. 32 (2010) 1627–1645.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2015] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, Q. Tian,
    Scalable person re-identification: A benchmark, Proceedings of the IEEE International
    Conference on Computer Vision, 2015, pp. 1116–1124.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] Z. Zheng, L. Zheng, Y. Yang, Unlabeled samples generated
    by gan improve the person re-identification baseline in vitro, Proceedings of
    the IEEE International Conference on Computer Vision, 2017, pp. 3774–3782.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ristani et al. [2016] E. Ristani, F. Solera, R. Zou, R. Cucchiara, C. Tomasi,
    Performance measures and a data set for multi target multi camera tracking, Proceedings
    of the European conference on computer vision, 2016, pp. 17–35.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards
    real-time object detection with region proposal networks, Advances in Neural Information
    Processing Systems. 39 (2015) 1137–1149.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dollar et al. [2014] P. Dollar, R. Appel, S. Belongie, P. Perona, Fast feature
    pyramids for object detection, IEEE Transactions on Pattern Analysis and Machine
    Intelligence. 36 (2014) 1532–1545.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirzer et al. [2011] M. Hirzer, C. Beleznai, P. Roth, H. Bischof, Person re-identification
    by descriptive and discriminative classification, Image Analysis. (2011) 91–102.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2014] T. Wang, S. Gong, X. Zhu, S. Wang, Person re-identification
    by video ranking, In Proceedings of the European Conference on Computer Vision,
    2014, pp. 688–703.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2016] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, Q. Tian,
    Mars: A video benchmark for large-scale person re-identification, Proceedings
    of the European Conference on Computer Vision, 2016, pp. 868–884.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2018] G. Song, B. Leng, Y. Liu, C. Hetang, S. Cai, Region-based
    quality estimation network for large-scale person re-identification, Proceedings
    of the AAAI Conference on Artificial Intelligence, 2018, pp. 7347–7354.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dehghan et al. [2015] A. Dehghan, S. Modiri Assari, M. Shah, Gmmcp tracker:
    Globally optimal generalized maximum multi clique problem for multiple object
    tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2015, pp. 4091–4099.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh Song et al. [2016] H. Oh Song, Y. Xiang, S. Jegelka, S. Savarese, Deep metric
    learning via lifted structured feature embedding, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2016, pp. 4004–4012.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2018] Y. Duan, J. Lu, J. Feng, J. Zhou, Deep localized metric learning,
    IEEE Transactions on Circuits and Systems for Video Technology. 28 (2018) 2644–2656.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2018] H. Huang, D. Li, Z. Zhang, X. Chen, K. Huang, Adversarially
    occluded samples for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 5098–5107.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. [2019] Z. Zhong, L. Zheng, Z. Luo, S. Li, Y. Yang, Invariance
    matters: Exemplar memory for domain adaptive person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 598–607.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2019] C. Luo, Y. Chen, N. Wang, Z. Zhang, Spectral feature transformation
    for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 4976–4985.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] Z. Zheng, L. Zheng, Y. Yang, A discriminatively learned
    cnn embedding for person reidentification, ACM Transactions on Multimedia Computing,
    Communications, and Applications. 14 (2017) 1–20.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] H. Chen, Y. Wang, Y. Shi, K. Yan, M. Geng, Y. Tian, T. Xiang,
    Deep transfer learning for person re-identification, Proceedings of the IEEE International
    Conference on Multimedia Big Data, 2018, pp. 1–5.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varior et al. [2016] R. R. Varior, B. Shuai, J. Lu, D. Xu, G. Wang, A siamese
    long short-term memory architecture for human re-identification, Proceedings of
    the European conference on computer vision, 2016, pp. 135–153.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] Y. Wang, Z. Chen, F. Wu, G. Wang, Person re-identification
    with cascaded pairwise convolutions, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 1470–1478.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ristani and Tomasi [2018] E. Ristani, C. Tomasi, Features for multi-target multi-camera
    tracking and re-identification, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 6036–6046.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2018] C. Song, Y. Huang, W. Ouyang, L. Wang, Mask-guided contrastive
    attention model for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 1179–1188.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] L. Zheng, H. Zhang, S. Sun, M. Chandraker, Y. Yang, Q. Tian,
    Person re-identification in the wild, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2017, pp. 1367–1376.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2021] H. Li, G. Wu, W. S. Zheng, Combined depth space based architecture
    search for person re-identification, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 6729–6738.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2019] J. Guo, Y. Yuan, L. Huang, C. Zhang, J. Yao, K. Han, Beyond
    human parts: Dual part-aligned representations for person re-identification, Proceedings
    of the IEEE International Conference on Computer Vision, 2019, pp. 3642–3651.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] G. Chen, C. Lin, L. Ren, J. Lu, J. Zhou, Self-critical attention
    learning for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 9637–9646.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] Z. Liu, J. Wang, S. Gong, H. Lu, D. Tao, Deep reinforcement
    active learning for human-in-the-loop person re-identification, Proceedings of
    the IEEE International Conference on Computer Vision, 2019, pp. 6122–6131.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2019] J. Song, Y. Yang, Y. Song, T. Xiang, T. Hospedales, Generalizable
    person re-identification by domain-invariant mapping network, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 719–728.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2019] S. Zhou, F. Wang, Z. Huang, J. Wang, Discriminative feature
    learning with consistent attention regularization for person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.
    8040–8049.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2016] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, S. Z.
    Li, Embedding deep metric for person re-identification: A study against large
    variations, Proceedings of the European Conference on Computer Vision, 2016, pp.
    732–748.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishchuk et al. [2017] A. Mishchuk, D. Mishkin, F. Radenovic, J. Matas, Working
    hard to know your neighbor’s margins: Local descriptor learning loss, Proceedings
    of the International Conference on Neural Information Processing Systems, 2017,
    pp. 4829–4840.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2021] C. Yan, G. Pang, X. Bai, C. Liu, N. Xin, L. Gu, J. Zhou,
    Beyond triplet loss: Person re-identification with fine-grained difference-aware
    pairwise loss, IEEE Transactions on Multimedia. (2021) 1–1.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2016] L. Wu, C. Shen, A. Hengel, Personnet: Person re-identification
    with deep convolutional neural networks, 2016, https://arxiv.org/pdf/1601.07255.pdf.
    [a͡rXiv preprint arXiv:1601.07255](http://arxiv.org/abs/1601.07255).'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016] F. Wang, W. Zuo, L. Lin, D. Zhang, L. Zhang, Joint learning
    of single-image and cross-image representations for person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2016, pp. 1288–1296.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2017] X. Qian, Y. Fu, Y. Jiang, T. Xiang, X. Xue, Multi-scale deep
    learning architectures for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 5399–5408.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2018] J. Liu, Z. Zha, H. Xie, Z. Xiong, Y. Zhang, Ca3net: Contextual-attentional
    attribute-appearance network for person re-identification, Proceedings of the
    ACM international conference on Multimedia, 2018, pp. 737–745.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2019] Y. Fu, Y. Wei, Y. Zhou, H. Shi, G. Huang, X. Wang, Z. Yao,
    T. Huang, Horizontal pyramid matching for person re-identification, Proceedings
    of the AAAI conference on artificial intelligence, volume 33, 2019, pp. 8295–8302.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2016] J. Liu, Z. Zha, Q. Tian, D. Liu, T. Yao, Q. Ling, T. Mei,
    Multi-scale triplet cnn for person re-identification, Proceedings of the ACM international
    conference on Multimedia, 2016, pp. 192–196.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2017] Y. Chen, X. Zhu, S. Gong, Person re-identification by deep
    learning multi-scale representations, Proceedings of the IEEE International Conference
    on Computer Vision Workshops, 2017, pp. 2590–2600.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2019] K. Zhou, Y. Yang, A. Cavallaro, T. Xiang, Omni-scale feature
    learning for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 3702–3712.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2017] D. Li, X. Chen, Z. Zhang, K. Huang, Learning deep context-aware
    features over body and latent parts for person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 384–393.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2019] W. Yang, H. Huang, Z. Zhang, X. Chen, K. Huang, S. Zhang,
    Towards rich feature discovery with class activation maps augmentation for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 1389–1398.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2021] Z. Zhang, H. Zhang, S. Liu, Person re-identification using
    heterogeneous local graph attention networks, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 12136–12145.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2021] X. Ning, K. Gong, W. Li, L. Zhang, Jwsaa: Joint weak saliency
    and attention aware for person re-identification, Neurocomputing. 453 (2021) 801–811.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalayeh et al. [2018] M. M. Kalayeh, E. Basaran, M. Gökmen, M. E. Kamasak, M. Shah,
    Human semantic parsing for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 1062–1071.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2021] K. Zhou, Y. Yang, A. Cavallaro, T. Xiang, Learning generalisable
    omni-scale representations for person re-identification, IEEE Transactions on
    Pattern Analysis and Machine Intelligence. (2021) 1–1.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ning et al. [2021] X. Ning, K. Gong, W. Li, L. Zhang, X. Bai, S. Tian, Feature
    refinement and filter network for person re-identification, IEEE Transactions
    on Circuits and Systems for Video Technology. 31 (2021) 3391–3402.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ming et al. [2021] Y. Ming, Z. aand Yang, X. Wei, J. Yan, X. Wang, F. Wang,
    M. Zhu, Global-local dynamic feature alignment network for person re-identification,
    2021, https://arxiv.org/pdf/2109.05759.pdf. [a͡rXiv preprint arXiv:2109.05759](http://arxiv.org/abs/2109.05759).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Y. Li, J. He, T. Zhang, X. Liu, Y. Zhang, F. Wu, Diverse part
    discovery: Occluded person re-identification with part-aware transformer, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 2898–2907.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, Proceedings of
    the International Conference on Neural Information Processing Systems, volume 2,
    2014, p. 2672–2680.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2018] J. Liu, B. Ni, Y. Yan, P. Zhou, S. Cheng, J. Hu, Pose transferrable
    person re-identification, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2018, pp. 4099–4108.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] Y. C. Chen, Y. J. Li, Y. C. F. Du, X.and Wang, Learning resolution-invariant
    deep representations for person re-identification, Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 33, 2019, pp. 8215–8222.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] J. Zhu, T. Park, P. Isola, A. Efros, Unpaired image-to-image
    translation using cycle-consistent adversarial networks, Proceedings of the IEEE
    International Conference on Computer Vision, 2017, pp. 2223–2232.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2018] Y. Choi, M. Choi, M. Kim, J. W. Ha, S. Kim, J. Choo, Stargan:
    Unified generative adversarial networks for multi-domain image-to-image translation,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 8789–8797.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. [2018] W. Liang, G. Wang, J. Lai, J. Zhu, M2m-gan: Many-to-many
    generative adversarial transfer learning for person re-identification, 2018, https://arxiv.org/pdf/1811.03768.pdf.
    [a͡rXiv preprint arXiv:1811.03768](http://arxiv.org/abs/1811.03768).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2020] C. Liu, X. Chang, Y. D. Shen, Unity style transfer for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2020, pp. 6887–6896.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019] Z. Wang, Z. Wang, Y. Zheng, Y. Y. Chuang, S. I. Satoh, Learning
    to reduce dual-level discrepancy for infra-red-visible person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2019, pp. 618–626.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bak et al. [2018] S. Bak, P. Carr, J. F. Lalonde, Domain adaptation through
    synthesis for unsupervised person re-identification, Proceedings of the European
    Conference on Computer Vision, 2018, pp. 189–205.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2019] C. Dai, H. Wang, T. Ni, S. Chen, Person re-identification
    based on deep convolutional generative adversarial network and expanded neighbor
    reranking, Journal of Computer Research and Development. 56 (2019) 1632–1641.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2020] W. X. Yang, Y. Yan, S. Chen, X. K. Zhang, H. Z. Wang, Multi-scale
    generative adversarial network for person re-identification under occlusion, Journal
    of Computer Research and Development. 31 (2020) 1943–1958.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2018] X. Qian, Y. Fu, T. Xiang, W. Wang, J. Qiu, Y. Wu, X. Xue,
    Pose-normalized image generation for person re-identification, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 650–667.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2019] Z. Zhu, T. Huang, B. Shi, M. Yu, B. Wang, X. Bai, Progressive
    pose attention transfer for person image generation, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 2347–2356.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2018] L. Ma, Q. Sun, S. Georgoulis, L. V. Gool, B. Schiele, M. Fritz,
    Disentangled person image generation, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 99–108.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] H. Chen, Y. Wang, B. Lagadec, A. Dantcheva, F. Bremond, Joint
    generative and contrastive learning for unsupervised person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 2004–2013.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Y. J. Li, Y. C. Chen, Y. Y. Lin, X. Du, Y. C. F. Wang, Recover
    and identify: A generative dual model for cross-resolution person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.
    8090–8099.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. [2018] H. Fan, L. Zheng, C. Yan, Y. Yang, Unsupervised person re-identification:
    Clustering and fine-tuning, ACM Transactions on Multimedia Computing, Communications,
    and Applications. 14 (2018) 1–18.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torralba and Efros [2011] A. Torralba, A. A. Efros, Unbiased look at dataset
    bias, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2011, pp. 1521–1528.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] J. Liu, Z. Zha, R. Hong, M. Wang, Y. Zhang, Deep adversarial
    graph attention convolution network for text-based person search, Proceedings
    of the ACM International Conference on Multimedia, 2019, pp. 665–673.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. [2019] S. Mao, S. Zhang, M. Yang, Resolution-invariant person re-identification,
    2019, https://arxiv.org/pdf/1906.09748.pdf. [a͡rXiv preprint arXiv:1906.09748](http://arxiv.org/abs/1906.09748).
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao et al. [2015] S. Liao, H. Yang, X. Zhu, S. Z. Li, Person re-identification
    by local maximal occurrence representation and metric learning, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2197–2206.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. [2016] P. Peng, X. Tao, Y. Wang, M. Pontil, Y. Tian, Unsupervised
    cross-dataset transfer learning for person re-identification, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1306–1315.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zha et al. [2020] Z. Zha, J. Liu, D. Chen, F. Wu, Adversarial attribute-text
    embedding for person search with natural language query, IEEE Transactions on
    Multimedia. 22 (2020) 1836–1846.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. [2020] Y. Zou, X. Yang, Z. Yu, B. Kumar, J. Kautz, Joint disentangling
    and adaptation for cross-domain person re-identification, Proceedings of the European
    Conference on Computer Vision, 2020, pp. 87–104.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2018] H. Liu, Z. Jie, K. Jayashree, M. Qi, J. Jiang, S. Yan, J. Feng,
    Video-based person re-identification with accumulative motion context, IEEE Transactions
    on Circuits and Systems for Video Technology. 28 (2018) 2788–2802.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] D. Chen, H. Li, T. Xiao, S. Yi, X. Wang, Video person re-identification
    with competitive snippet-similarity aggregation and co-attentive snippet embedding,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 1169–1178.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McLaughlin et al. [2016] N. McLaughlin, J. del Rincon, P. Miller, Recurrent
    convolutional network for video-based person re-identification, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1325–1334.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2017] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, P. Zhou, Jointly
    attentive spatial-temporal pooling networks for video-based person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2017, pp.
    4733–4742.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2019] J. Li, S. Zhang, T. Huang, Multi-scale 3d convolution network
    for video based person re-identification, Proceedings of the AAAI Conference on
    Artificial Intelligence, volume 33, 2019, pp. 8618–8625.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2017] Z. Zhou, Y. Huang, W. Wang, L. Wang, T. Tan, See the forest
    for the trees: Joint spatial and temporal recurrent neural networks for video-based
    person re-identification, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2017, pp. 4747–4756.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2016] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, X. Yang, Person re-identification
    via recurrent feature aggregation, Proceedings of the European Conference on Computer
    Vision, 2016, pp. 701–716.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2018] S. Li, S. Bak, P. Carr, X. Wang, Diversity regularized spatiotemporal
    attention for video-based person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 369–378.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Z. Zhang, C. Lan, W. Zeng, Z. Chen, Multi-granularity reference-aided
    attentive feature aggregation for video-based person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 10407–10416.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2019] J. Li, J. Wang, Q. Tian, W. Gao, S. Zhang, Global-local temporal
    representations for video person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2019, pp. 3958–3967.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2021] R. Hou, H. Chang, B. Ma, R. Huang, S. Shan, Bicnet-tks: Learning
    efficient spatial-temporal representation for video person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 2014–2023.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021] J. Liu, Z. J. Zha, W. Wu, K. Zheng, Q. Sun, Spatial-temporal
    correlation and topology learning for person re-identification in videos, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 4370–4379.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling [2017] T. Kipf, M. Welling, Semi-supervised classification
    with graph convolutional networks, 2017, https://arxiv.org/pdf/1609.02907.pdf.
    [a͡rXiv preprint arXiv:1609.02907](http://arxiv.org/abs/1609.02907).
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Nevatia [2018] J. Gao, R. Nevatia, Revisiting temporal modeling for
    video-based person reid, 2018, https://arxiv.org/pdf/1805.02104.pdf. [a͡rXiv preprint
    arXiv:1805.02104](http://arxiv.org/abs/1805.02104).
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu et al. [2020] K. Niu, Y. Huang, W. Ouyang, L. Wang, Improving description-based
    person re-identification by multi-granularity image-text alignments, IEEE Transactions
    on Image Processing. 29 (2020) 5542–5556.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2020] S. Choi, S. Lee, T. Kim, Y.and Kim, C. Kim, Hi-cmd: Hierarchical
    cross-modality disentanglement for visible-infrared person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2020, pp. 10257–10266.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2020] M. Ye, J. Shen, D. J. Crandall, L. Shao, J. Luo, Dynamic dual-attentive
    aggregation learning for visible-infrared person re-identification, Proceedings
    of the European Conference on Computer Vision, 2020, pp. 229–247.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] G. A. Wang, T. Zhang, Y. Yang, J. Cheng, J. Chang, X. Liang,
    Z. G. Hou, Cross-modality paired-images generation for rgb-infrared person re-identification,
    Proceedings of the AAAI Conference on Artificial Intelligence, 2020, pp. 12144–12151.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2020] D. Li, X. Wei, X. Hong, Y. Gong, Infrared-visible cross-modal
    person re-identification with an x modality, Proceedings of the AAAI Conference
    on Artificial Intelligence, 2020, pp. 4610–4617.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] Q. Wu, P. Dai, J. Chen, C. W. Lin, Y. Wu, F. Huang, R. Ji,
    Discover cross-modality nuances for visible-infrared person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 4330–4339.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. [2019] H. Yu, W. Zheng, A. Wu, X. Guo, S. Gong, J. Lai, Unsupervised
    person re-identification by soft multilabel learning, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2019, pp. 2148–2157.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2020] Y. Lin, L. Xie, Y. Wu, C. Yan, Q. Tian, Unsupervised person
    re-identification via softened similarity learning, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2020, pp. 3390–3399.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Zhang [2020] D. Wang, S. Zhang, Unsupervised person re-identification
    via multi-label classification, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2020, pp. 10981–10990.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2019] Q. Yang, H. Yu, A. Wu, W. Zheng, Patch-based discriminative
    feature learning for unsupervised person re-identification, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3633–3642.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xin et al. [2019] X. Xin, J. Wang, R. Xie, S. Zhou, W. Huang, N. Zheng, Semi-supervised
    person re-identification using multi-view clustering, Pattern Recognition. 88
    (2019) 285–297.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. [2020] L. Qi, L. Wang, J. Huo, Y. Shi, Y. Gao, Progressive cross-camera
    soft-label learning for semi-supervised person re-identification, IEEE Transactions
    on Circuits and Systems for Video Technology. 30 (2020) 2815–2829.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2019] H. Tang, Y. Zhao, H. Lu, Unsupervised person re-identification
    with iterative self-supervised domain adaptation, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, 2019, pp. 1536–1543.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2020] G. Wu, X. Zhu, S. Gong, Tracklet self-supervised learning for
    unsupervised person re-identification, Proceedings of the AAAI Conference on Artificial
    Intelligence, 2020, pp. 12362–12369.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2019] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, T. S. Huang, Self-similarity
    grouping: A simple unsupervised cross domain adaptation approach for person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.
    6112–6121.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. [2020] Y. Zhai, S. Lu, Q. Ye, X. Shan, J. Chen, R. Ji, Y. Tian,
    Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2020, pp. 9021–9030.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019] X. Zhang, J. Cao, C. Shen, M. You, Self-training with progressive
    augmentation for unsupervised cross-domain person re-identification, Proceedings
    of the IEEE International Conference on Computer Vision, 2019, pp. 8222–8231.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2017] M. Ye, A. J. Ma, L. Zheng, J. Li, P. C. Yuen, Dynamic label
    graph matching for unsupervised video re-identification, Proceedings of the IEEE
    International Conference on Computer Vision, 2017, pp. 5142–5150.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2017] J. Chen, Y. Wang, J. Qin, L. Liu, L. Shao, Fast person re-identification
    via cross-camera semantic binary transformation, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2017, pp. 3873–3882.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] G. A. Wang, S. Gong, J. Cheng, Z. Hou, Faster person re-identification,
    Proceedings of the European Conference on Computer Vision, 2020, pp. 275–292.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2021] C. Zhao, Y. Tu, Z. Lai, F. Shen, H. T. Shen, D. Miao, Salience-guided
    iterative asymmetric mutual hashing for fast person re-identification, IEEE Transactions
    on Image Processing. 30 (2021) 7776–7789.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Gong [2020] G. Wu, S. Gong, Decentralised learning from independent multi-domain
    labels for person re-identification, 2020, https://arxiv.org/pdf/2006.04150.pdf.
    [a͡rXiv preprint arXiv:2006.04150](http://arxiv.org/abs/2006.04150).
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2021] S. Sun, G. Wu, S. Gong, Decentralised person re-identification
    with selective knowledge aggregation, 2021, https://arxiv.org/pdf/2110.11384.pdf.
    [a͡rXiv preprint arXiv:2110.11384](http://arxiv.org/abs/2110.11384).
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. [2020] W. Zhuang, Y. Wen, X. Zhang, X. Gan, D. Yin, D. Zhou, S. Yi,
    Performance optimization of federated person re-identification via benchmark analysis,
    Proceedings of the ACM International Conference on Multimedia, 2020, pp. 955–963.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bedogni et al. [2021] L. Bedogni, S. K. Rumi, F. D. Salim, Modelling memory
    for individual re-identification in decentralised mobile contact tracing applications,
    Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,
    2021, pp. 1–21.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. [2017] T. Xiao, S. Li, B. Wang, L. Lin, X. Wang, Joint detection
    and identification feature learning for person search, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 3415–3424.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munjal et al. [2019] B. Munjal, S. Amin, F. Tombari, F. Galasso, Query-guided
    end-to-end person search, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2019, pp. 811–820.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2017] S. Tang, M. Andriluka, B. Andres, B. Schiele, Multiple people
    tracking by lifted multicut and person re-identification, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 3539–3548.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. [2019] Y. Hou, L. Zheng, Z. Wang, S. Wang, Locality aware appearance
    metric for multi-target multi-camera tracking, 2019, https://arxiv.org/pdf/1911.12037.pdf.
    [a͡rXiv preprint arXiv:1911.12037](http://arxiv.org/abs/1911.12037).
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
