- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:50:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2110.04764] Deep learning-based person re-identification methods: A survey
    and outlook of recent works'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.04764](https://ar5iv.labs.arxiv.org/html/2110.04764)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[orcid=0000-0003-1616-8054]'
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Conceptualization, Methodology, Investigation, Writing-original draft, Revision
  prefs: []
  type: TYPE_NORMAL
- en: '[orcid=0000-0002-5664-1558] \cormark[1] \creditonceptualization, Writing -
    Review & Editing, Supervision, Resources \cortext[1]Corresponding author'
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Writing-Review, Funding Acquisition
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Data curation, Visualization, Investigation
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Writing-Review, Data curation
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Writing-Review, Investigation
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Writing-Review, Data curation
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Data Curation, Visualization, Validation
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning-based person re-identification methods: A survey and outlook
    of recent works'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zhangqiang Ming mingzhangqiang@stu.scu.edu.cn organization=College of Computer
    Science, addressline=Sichuan University, city=Chengdu, postcode=610065, country=China
       Min Zhu zhumin@scu.edu.cn    Xiangkun Wang    Jiamin Zhu    Junlong Cheng   
    Chengrui Gao    Yong Yang    Xiaoyong Wei
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, with the increasing demand for public safety and the rapid
    development of intelligent surveillance networks, person re-identification (Re-ID)
    has become one of the hot research topics in the computer vision field. The main
    research goal of person Re-ID is to retrieve persons with the same identity from
    different cameras. However, traditional person Re-ID methods require manual marking
    of person targets, which consumes a lot of labor cost. With the widespread application
    of deep neural networks, many deep learning-based person Re-ID methods have emerged.
    Therefore, this paper is to facilitate researchers to understand the latest research
    results and the future trends in the field. Firstly, we summarize the studies
    of several recently published person Re-ID surveys and complement the latest research
    methods to systematically classify deep learning-based person Re-ID methods. Secondly,
    we propose a multi-dimensional taxonomy that classifies current deep learning-based
    person Re-ID methods into four categories according to metric and representation
    learning, including methods for deep metric learning, local feature learning,
    generative adversarial learning and sequence feature learning. Furthermore, we
    subdivide the above four categories according to their methodologies and motivations,
    discussing the advantages and limitations of part subcategories. Finally, we discuss
    some challenges and possible research directions for person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Person re-identification\sepDeep metric learning\sepLocal feature learning\sepGenerative
    adversarial learning\sepSequence feature learning\sep{highlights}
  prefs: []
  type: TYPE_NORMAL
- en: The main contributions of person Re-ID surveys are summarized and discussed
    in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: A metric and representation learning-based taxonomy is provided for recent person
    Re-ID methods.
  prefs: []
  type: TYPE_NORMAL
- en: The above main categories are subdivided based on their methodologies and motivations.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages and limitations of part subcategories are summarized.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, some challenges and possible research directions for person Re-ID
    are discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, with the rapid development of intelligent surveillance devices
    and the increasing demand for public safety, a large number of cameras have been
    deployed in public places such as airports, communities, streets and campuses.
    These camera networks typically span large geographic areas with non-overlapping
    coverage and generate a large amount of surveillance video every day. We use this
    video data to analyze the activity patterns and behavioral characteristics of
    pedestrians in the real world for applications such as target detection, multi-camera
    target tracking and crowd behavior analysis. Person Re-ID can be traced back to
    the problem of multi-target multi-camera tracking (MTMCT tracking) [[1](#bib.bib1)],
    which aims to determine whether pedestrians captured by different cameras or pedestrian
    images from different video clips of the same camera are the same pedestrian [[2](#bib.bib2)].
    [Figure 1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") illustrates an example of a surveillance
    area monitored by multiple cameras with non-overlapping fields of view.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4ee804aa14fafa2377ccd5d1c59169e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Multi-camera surveillance network illustration of person Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2](#S1.F2 "Fig. 2 ‣ 1 Introduction ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the complete flow of the
    person Re-ID system, which mainly consists of two stages: pedestrian detection
    and re-identification [[3](#bib.bib3)]. For pedestrian detection, many algorithms
    with high detection accuracy have emerged, such as YOLO [[4](#bib.bib4)], SSD
    [[5](#bib.bib5)] and Fast R-CNN [[6](#bib.bib6)]. Person Re-ID constructs a large
    image dataset (Gallery) from the detected pedestrian images and retrieves matching
    pedestrian images from it using probe images (Probe), so person Re-ID can also
    be regarded as an image retrieval task [[7](#bib.bib7)]. The key of person Re-ID
    is to learn discriminative features of pedestrians to distinguish between pedestrian
    images with the same identity and those with different identities. However, the
    difficulty of learning discriminative features of pedestrians is increased by
    the variation of view, pose, illumination and resolution in different cameras
    in the real world where pedestrians may appear in multiple cameras in multiple
    regions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0676647c7b4f0eae962069f4029ccef6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Flowchart of person re-identification system.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6931c8ed180aab61b1644c687eea8a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: The number of person Re-ID papers on top conferences and journals over
    the years.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional person Re-ID methods mainly used manual extraction of fixed discriminative
    features [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)] or learned better similarity measures [[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)], which were error-prone and time-consuming,
    and greatly affected the accuracy and real-time performance of pedestrian Re-ID
    tasks. In 2014, deep learning was first used in the person Re-ID field [[17](#bib.bib17),
    [18](#bib.bib18)]. [Figure 3](#S1.F3 "Fig. 3 ‣ 1 Introduction ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works") illustrates
    that there has been a significant increase in the proportion of collected person
    Re-ID papers over the years. Some researchers designed different loss functions
    to optimize the learning of discriminative features by network models [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)]. Other
    researchers extracted more robust features of pedestrians by introducing local
    feature learning [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] or using attention mechanisms to focus on key information of
    body parts [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)]. Ngo et al. [[35](#bib.bib35), [36](#bib.bib36)]
    explored the method of high-level feature extraction aimed to explore context-based
    concept fusion by modeling inter-concept relationships, which were not modeled
    based on semantic reasoning. [[37](#bib.bib37)]. Several works enhanced the final
    feature representation by combining global and local features of pedestrians [[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]. Due to the good performance of GAN in generating images and
    learning features, generative adversarial learning was widely used for person
    Re-ID tasks [[45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)]. To alleviate the shortage of information
    in single-frame images, some researchers used the complementary spatial and temporal
    cues of video sequences to effectively fuse more information in the video sequences
    [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61)]. Recently, graph convolutional network-based methods [[60](#bib.bib60),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)] also emerged
    to learn more discriminative and robust features by modeling graph relationships
    on pedestrian images. Some researchers [[66](#bib.bib66), [67](#bib.bib67)] improved
    the robustness of the person Re-ID model by exploiting the information of a person’s
    3D shape. These methods are numerous and have different emphases. To give researchers
    a quick overview of the current state of development and valuable research directions
    in the field of person Re-ID, we conduct an in-depth survey of deep learning-based
    person Re-ID methods and summarize the relevant research results in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of several person Re-ID surveys in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Reference | Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| A survey of approaches and trends in person re-identification[[68](#bib.bib68)]
    | IVC’14 | 1\. Explored the problem of person Re-ID including system-level challenges,
    descriptor issues and correspondence issues; 2\. Summarized the person Re-ID methods
    before 2016 including Contextual methods, Non-contextual methods and Active methods.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Person Re-identification Past, Present and Future[[3](#bib.bib3)] | arXiv’16
    | 1\. Introduced the history of person re-ID and its relationship between image
    classification and instance retrieval; 2\. Surveyed a broad selection of the hand-crafted
    systems and the large-scale methods in both image- and video-based re-ID. |'
  prefs: []
  type: TYPE_TB
- en: '| A study on deep convolutional neural network based approaches for person
    re-identification[[69](#bib.bib69)] | PRMI’17 | 1\. Introduced image-based and
    video-based person Re-ID methods; 2\. Discussed some important but undeveloped
    issues and future research directions. |'
  prefs: []
  type: TYPE_TB
- en: '| Survey on person re-identification based on deep learning[[70](#bib.bib70)]
    | CAAI’18 | 1\. Some deep learning-based person Re-ID methods such as CNN-based,
    GAN-based and Hybrid-based methods are presented; 2\. Proposed the direction of
    further research. |'
  prefs: []
  type: TYPE_TB
- en: '| A systematic evaluation and benchmark for person re-identification: Features,
    metrics, and datasets[[71](#bib.bib71)] | TPAMI’19 | 1\. Presented an extensive
    review and performance evaluation of single-shot and multi-shot Re-ID algorithms.
    2\. Introduced the most recent advances in both feature extraction and metric
    learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Beyond intra-modality discrepancy: A comprehensive survey of heterogeneous
    person re-identification[[72](#bib.bib72)] | arXiv’19 | 1\. Surveyed the models
    that have been widely employed in heterogeneous person Re-ID. 2\. Considered four
    cross-modality application scenarios: Low-resolution (LR), Infrared (IR), Sketch,
    and Text. |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning-based methods for person re-identification: A comprehensive
    review[[73](#bib.bib73)] | NC’19 | 1\. Reviewed six types of methods of person
    Re-ID based on deep learning, including identification deep model, verification
    deep model, distance metric-based deep model, part-based deep model, video-based
    deep model and data augmentation-based deep model. |'
  prefs: []
  type: TYPE_TB
- en: '| A Brief Survey of Deep Learning Techniques for Person Re-identification[[74](#bib.bib74)]
    | ICETCE’20 | 1\. Presented the issues of person Re-ID and the approaches used
    to solve these issues. |'
  prefs: []
  type: TYPE_TB
- en: '| A Survey of Open-World Person Re-Identification[[75](#bib.bib75)] | TCSVT’20
    | 1\. Analyzed the discrepancies between closed-word and open-world scenarios.
    2\. Described the developments of open-set Re-ID works and their limitations.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Person search: New paradigm of person re-identification: A survey and outlook
    of recent works[[76](#bib.bib76)] | IVC’20 | 1\. Discussed about feature representation
    learning and deep metric learning with novel loss functions. |'
  prefs: []
  type: TYPE_TB
- en: '| Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?[[77](#bib.bib77)] | arXiv’20 | 1\. Surveyed state-of-the-art DNN
    models being used for person Re-ID task. 2\. Discussed their limitations that
    can work as guidelines for future research. |'
  prefs: []
  type: TYPE_TB
- en: '| Person re-identification based on metric learning: a survey[[77](#bib.bib77)]
    | MTA’21 | 1\. Summarized the research progress of person Re-ID methods based
    on metric learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Learning for Person Re-identification: A Survey and Outlook[[78](#bib.bib78)]
    | TPAMI’21 | 1\. Reviewed for closed-world person Re-ID from three different perspectives,
    including deep feature representation learning, deep metric learning and ranking
    optimization. |'
  prefs: []
  type: TYPE_TB
- en: '| Survey on Unsupervised Techniques for Person Re-Identification[[79](#bib.bib79)]
    | CDS’21 | 1\. Reviewed the state-of-the-art unsupervised techniques of person
    Re-ID. |'
  prefs: []
  type: TYPE_TB
- en: '| SSS-PR: A short survey of surveys in person re-identification[[80](#bib.bib80)]
    | PRL’21 | 1\. Proposed a multi-dimensional taxonomy to categorize the most relevant
    researches according to different perspectives. 2\. Filled the gap between the
    recently published surveys. |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-Domain Person Re-identification: A Review[[81](#bib.bib81)] | AIC’21
    | 1\. Reviewed methods of cross-domain person Re-ID.2\. Compared the performance
    of these methods on public datasets. |'
  prefs: []
  type: TYPE_TB
- en: 'Prior to this survey, some researchers [[68](#bib.bib68), [3](#bib.bib3), [69](#bib.bib69),
    [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] also reviewed the person
    Re-ID field. In [Table 1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works"), we summarize
    the major contributions of these reviews. Some of these surveys [[3](#bib.bib3),
    [69](#bib.bib69)] summarized image-based and video-based person Re-ID methods.
    Other surveys [[70](#bib.bib70), [73](#bib.bib73), [74](#bib.bib74), [78](#bib.bib78),
    [77](#bib.bib77), [79](#bib.bib79)] summarized the deep learning-based person
    Re-ID methods in different dimensions, which developed rapidly after 2014 and
    became the main research means. Recently, Wang et al. [[81](#bib.bib81)] outlined
    methods of cross-domain person Re-ID and compared the performance of these methods
    on public datasets. Yaghoubi et al. [[80](#bib.bib80)] proposed a multi-dimensional
    taxonomy to categorize the most relevant researches according to different perspectives.
    Zhou et al. [[82](#bib.bib82)] provided a review to summarize the developments
    in domain generalization for computer vision over the past decade. Behera et al.
    [[83](#bib.bib83)] reviewed traditional and deep learning person Re-ID methods
    in both contextual and non-contextual dimensions. Wu et al. [[84](#bib.bib84)]
    proposed new taxonomies for the two components of feature extraction and metric
    learning on person Re-ID. Behera et al. [[85](#bib.bib85)] conceptualized an overview
    of interpreting various futuristic cues on the IoT platform for achieving person
    Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are still some improvements to be made in these surveys, which
    lack the systematic classification and analysis of deep learning-based person
    Re-ID methods, also miss many discussions parts for person Re-ID. In this paper,
    compared to the above review, we focus more on metric learning and representation
    learning of deep learning methods in person Re-ID tasks and complement the latest
    research methods of recent years. We present an in-depth and comprehensive review
    of existing deep learning-based methods and discuss their advantages and limitations.
    We classify deep learning-based person Re-ID methods in terms of metric and representation
    learning dimensions, including four categories: deep metric learning, local feature
    learning, generative adversarial learning and sequence feature learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep metric learning focused on designing better loss functions for model training.
    Common loss functions for person Re-ID included: classification loss, verification
    loss, contrastive loss, triplet loss and quadruplet loss. Representation learning
    focused on developing feature construction strategies [[86](#bib.bib86), [78](#bib.bib78)].
    Therefore, we discussed the common feature learning strategies in recent person
    Re-ID methods, which were mainly in three categories: 1) Local feature learning,
    it learned part-level local features to formulate a combined representation for
    each person image; 2) Generative adversarial learning, it learned the image specific
    style representation or disentangled representation to achieve image-image style
    transfer or extract invariant features; 3) Sequence feature learning, it learned
    video sequence representation using multiple image frames and temporal information.
    In addition, we subdivided the above four categories based on their methodologies
    and motivations. This classification has a clear structure, which comprehensively
    reflects the most common deep metric learning methods and various representation
    learning methods in Re-ID tasks. Therefore, it is suitable for researchers to
    explore person Re-ID for practical needs. Furthermore, we attempt to discuss several
    challenges and research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the main contributions of our work are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize the studies of several recently published person Re-ID surveys
    and complement the latest research methods to systematically classify deep learning-based
    person Re-ID methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We comprehensively review the research methods of recent deep learning-based
    person Re-ID. Then, we propose a multi-dimensional taxonomy that classifies these
    methods into four categories according to metric and representation learning,
    including deep metric learning, local feature learning, generative adversarial
    learning, and sequence feature learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We subdivide the above four categories based on their methodologies and motivations,
    discussing the advantages and limitations of part subcategories. This classification
    is more suitable for researchers to explore these methods from their practical
    needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize some existing challenges in the person Re-ID field and consider
    that there is still enough necessity to research it. Furthermore, we discuss seven
    possible research directions for person Re-ID researchers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remaining parts of this survey are structured as follows. In [section 2](#S2
    "2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), we discuss the common datasets
    and evaluation metrics used for person Re-ID benchmarks. In [section 3](#S3 "3
    Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), we comprehensively review current
    deep learning-based methods for person Re-ID and divide them into four categories
    according to metric learning and representation learning. Furthermore, we subdivide
    the above four categories based on their methodologies and motivations, discussing
    the advantages and limitations of part subcategories. Finally, in [section 4](#S4
    "4 Conclusion and future directions ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), we summarize this paper and discuss
    the current challenges and future directions in the person Re-ID field.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Datasets and Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present common datasets for evaluating existing deep learning-based
    person Re-ID methods in both image and video dimensions. In addition, we briefly
    describe common evaluation metrics for person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, many methods have emerged to improve the performance of person
    Re-ID. However, the uncertainty of the real world brings about problems including
    occlusion, lighting changes, camera view switching, pose changes and similar clothing
    still cannot be well solved. These challenges make many algorithms still not available
    for real-world applications. Therefore, it is crucial to explore large-scale person
    Re-ID datasets covering more real scenes. As deep learning-based feature extraction
    methods gradually replace traditional manual feature extraction methods, deep
    neural networks require a large amount of training data, which has led to the
    rapid development of large-scale datasets. The types of datasets and annotation
    methods vary greatly between datasets. Usually, the datasets used for person Re-ID
    can be divided into two categories, namely image-based and video-based datasets.
    The following subsection will introduce two types of commonly used datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Image-based person Re-ID datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VIPeR [[87](#bib.bib87)] dataset is the first proposed small person Re-ID dataset.
    The VIPeR contains two viewpoint cameras, each of which captures only one image.
    The VIPeR uses manually labeled pedestrians and contains 1,264 images for a total
    of 632 different pedestrians. Each image is cropped and scaled to a size of 128x48\.
    The VIPeR dataset, which features multiple views, poses, and lighting variations,
    has been tested by many researchers, but it remains one of the most challenging
    person Re-ID datasets.
  prefs: []
  type: TYPE_NORMAL
- en: CUHK01 [[88](#bib.bib88)] dataset has 971 persons and 3,884 manually cropped
    images, with each person also having at least two images captured in two disjoint
    camera views. In the CUHK01 dataset, camera A has more variations of viewpoints
    and poses, and camera B mainly includes images of the frontal view and the back
    view.
  prefs: []
  type: TYPE_NORMAL
- en: CUHK02 [[89](#bib.bib89)] dataset has 1,816 persons and 7,264 manually cropped
    images. The CUHK02 contains five pairs of camera views(ten camera views)), with
    each person also having at least two images captured in two disjoint camera views.
    Compared to the CUHK01 [[88](#bib.bib88)] dataset, the CUHK02 dataset has many
    identities and camera views and can obtain more configurations(which are the combinations
    of viewpoints, poses, image resolutions, lightings and photometric settings) of
    pedestrian images.
  prefs: []
  type: TYPE_NORMAL
- en: CUHK03 [[14](#bib.bib14)] dataset belongs to the large-scale person Re-ID dataset
    and is collected at the Chinese University of Hong Kong. The CUHK03 is acquired
    by 10 (5 pairs) cameras and provided with a manual marker and a deformable part
    model (DPM) detector [[90](#bib.bib90)] together to detect pedestrian bounding
    boxes. The CUHK03 contains 1,360 different pedestrians with a total of 13,164
    images, each of variable size. The CUHK03 improves on the CUHK01 [[88](#bib.bib88)]
    and CUHK02 [[89](#bib.bib89)] by increasing the number of cameras and captured
    images, thus capturing pedestrian images from more viewpoints. The CUHK03 dataset
    uses the pedestrian detection algorithm DPM to annotate pedestrians, making it
    more compatible with person Re-ID in the real world than using individual manual
    annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Market-1501 [[91](#bib.bib91)] dataset is a large-scale person Re-ID dataset
    published in 2015, which was acquired using five high-resolution cameras and one
    low-resolution camera in front of a supermarket at Tsinghua University. The Market-1501
    uses a pedestrian detector DPM to automatically detect pedestrian bounding boxes.
    It contains 1,501 different pedestrians with a total of 32,668 images, each with
    a size of 128x64\. Compared with CUHK03, Market-1501 has more annotated images
    and contains 2793+500k interfering factors, and it is closer to the real world.
  prefs: []
  type: TYPE_NORMAL
- en: DukeMTMC-reID [[92](#bib.bib92)] dataset belongs to a subset of the MTMCT dataset
    DukeMTMC [[93](#bib.bib93)]. The DukeMTMC-reID dataset is collected at Duke University
    using eight static HD cameras. It contains 16,522 training images (from 702 persons),
    2,228 query images (from other 702 persons), and a search gallery (Gallery) of
    17,661 images.
  prefs: []
  type: TYPE_NORMAL
- en: MSMT17 [[45](#bib.bib45)] dataset is a large-scale person Re-ID dataset published
    in 2018 and is captured at the campus by fifteen cameras. The MSMT17 uses the
    pedestrian detector Faster R-CNN [[94](#bib.bib94)] to automatically detect pedestrian-labeled
    frames. It contains 4,101 different pedestrian information with a total of 126,441
    images, which is one of the large datasets of pedestrian and annotated images
    in the current person Re-ID task. The MSMT17 dataset can cover more scenes than
    earlier datasets where a single scene and no significant light changes existed.
  prefs: []
  type: TYPE_NORMAL
- en: Airport [[71](#bib.bib71)] dataset is constructed using video data from the
    six cameras installed at the postcentral security checkpoint at a commercial airport
    within the United States. The Airport dataset consists of 9,651 identities, 31,238
    distractors, a total of 39,902 images, and each one is cropped and scaled to a
    size of 128x64\. The Airport uses pre-detected bounding boxes generated using
    aggregated channel features(ACF) [[95](#bib.bib95)] detector, which can accurately
    reflect real-world person Re-ID issues.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2](#S2.T2 "Table 2 ‣ 2.1.1 Image-based person Re-ID datasets ‣ 2.1 Datasets
    ‣ 2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the details of the above
    datasets. Most of the earlier image-based person Re-ID datasets (VIPeR, CUHK01,
    CUHK02, CUHK03, Market-1501) have the following limitations: (1) covering a single
    scene; (2) short time span without significant illumination variations; (3) expensive
    manual annotation or outdated automatic annotation with DPM detection. The CUHK03
    contains viewpoint variations, detection errors, occlusions images. The Market-1501
    contains viewpoint variations, detection errors and low-resolution images. But
    their simulation of the real world is relatively weak. The DukeMTMC-reID dataset
    contains more challenging attributes include viewpoint variations, illumination
    variations, detection errors, occlusions, and background clutter. The MSMT17 dataset
    collects images captured by 15 cameras for both indoor and outdoor scenes. Therefore,
    it presents complex scene transformations and backgrounds. The videos cover a
    long time, thus presenting complex lighting variations. The Airport dataset contains
    a large number of annotated identities and bounding boxes. To our best knowledge,
    MSMT17 and Airport are the largest and most challenging public datasets for person
    Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Typical image-based person Re-ID datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data type | Datasets | Years | ID | Boxes | Cameras | Labeled | Evaluation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image | ViPeR[[87](#bib.bib87)] | 2007 | 632 | 1,264 | 2 | Handcrafted |
    CMC |'
  prefs: []
  type: TYPE_TB
- en: '| CUHK01[[88](#bib.bib88)] | 2012 | 971 | 3,884 | 2 | Handcrafted | CMC |'
  prefs: []
  type: TYPE_TB
- en: '| CUHK02[[89](#bib.bib89)] | 2013 | 1,816 | 7,264 | 10 | Handcrafted | CMC
    |'
  prefs: []
  type: TYPE_TB
- en: '| CUHK03[[14](#bib.bib14)] | 2014 | 1,360 | 13,164 | 10 | DPM+Handcrafted |
    CMC+mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Market-1501[[91](#bib.bib91)] | 2015 | 1,501 | 32,217 | 6 | DPM+Handcrafted
    | CMC+mAP |'
  prefs: []
  type: TYPE_TB
- en: '| DukeMTMC-reID[[52](#bib.bib52)] | 2017 | 1,812 | 36,441 | 8 | Handcrafted
    | CMC+mAP |'
  prefs: []
  type: TYPE_TB
- en: '|  | MSMT17[[45](#bib.bib45)] | 2018 | 4,101 | 126,441 | 15 | Faster RCNN |
    CMC+mAP |'
  prefs: []
  type: TYPE_TB
- en: '|  | Airport[[71](#bib.bib71)] | 2019 | 9,651 | 39,902 | 6 | ACF | CMC+mAP
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Typical video-based person Re-ID datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data type | Datasets | Years | ID | Tracks | Cameras | Labeled | Evaluation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Video | PRID-2011[[96](#bib.bib96)] | 2011 | 934 | 400 | 2 | Handcrafted
    | CMC |'
  prefs: []
  type: TYPE_TB
- en: '| iLIDS-VID [[97](#bib.bib97)] | 2014 | 300 | 600 | 2 | Handcrafted | CMC |'
  prefs: []
  type: TYPE_TB
- en: '| MARS[[98](#bib.bib98)] | 2016 | 1,261 | 20,715 | 6 | DPM+GMMCP | CMC+mAP
    |'
  prefs: []
  type: TYPE_TB
- en: '| DukeMTMC-V[[19](#bib.bib19)] | 2018 | 1,812 | 4,832 | 8 | DPM | CMC+mAP |'
  prefs: []
  type: TYPE_TB
- en: '| LPW[[99](#bib.bib99)] | 2018 | 2,731 | 7,694 | 11 | DPM+NN+Handcrafted |
    CMC+mAP |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/8ade1bffbd56b00b40d85acde4da30b6.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Market-1501[[91](#bib.bib91)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6bdf6fd85b62604d50867800fff608a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) DukeMTMC-reID[[92](#bib.bib92)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81bf19b2d94876538f94837b97bda478.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) MSMT17[[8](#bib.bib8)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 4: Sampled person images of person Re-ID datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Video-based person Re-ID datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PRID2011 [[96](#bib.bib96)] dataset is a video-based person Re-ID dataset proposed
    in 2011 and is obtained from two non-overlapping camera acquisitions and contains
    a total of 24,541 images of 934 different pedestrians with manually labeled pedestrian
    bounding boxes. The resolution size of each image is 128×64.
  prefs: []
  type: TYPE_NORMAL
- en: iLIDS-VID [[97](#bib.bib97)] dataset is acquired by two cameras at the airport
    and contains 300 pedestrians with 600 tracks totaling 42,495 pedestrian images.
    The iLIDS-VID uses manual annotation of pedestrian bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: MARS [[98](#bib.bib98)] dataset is the first large-scale video-based person
    re-identification dataset proposed in 2016, which contains 1,261 different pedestrians
    and approximately 20,000 video sequences of pedestrian trajectories acquired from
    six different cameras. The DPM detector and the generalized maximum multi-cornered
    problem (GMMCP) tracker [[100](#bib.bib100)] were used for pedestrian detection
    and trajectory tracking of MARS, respectively. The MARS dataset contains 3,248
    interfering trajectories and is fixed with 631 and 630 different pedestrians to
    divide the training and test sets, respectively. It can be considered as an extension
    of Market-1501.
  prefs: []
  type: TYPE_NORMAL
- en: DukeMTMC-VideoReID [[19](#bib.bib19)] dataset belongs to a subset of the MTMCT
    dataset DukeMTMC [[93](#bib.bib93)] for video-based person Re-ID. This dataset
    contains 1,812 different pedestrians, 4,832 pedestrian trajectories totaling 815,420
    images, in which 408 pedestrians as interference terms, 702 pedestrians for training,
    702 pedestrians for testing.
  prefs: []
  type: TYPE_NORMAL
- en: LPW(Labeled Person in the Wild) [[99](#bib.bib99)] dataset is a largescale video
    sequence-based person Re-ID dataset that collects three different crowded scenes
    containing 2,731 different pedestrians and 7,694 pedestrian trajectories with
    more than 590,000 images. The LPW dataset is collected in crowded scenes and has
    more occlusions, providing more realistic and challenging benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3](#S2.T3 "Table 3 ‣ 2.1.1 Image-based person Re-ID datasets ‣ 2.1 Datasets
    ‣ 2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the details of video-based
    person Re-ID datasets. PRID2011 and iLIDS-VID use only two cameras to capture
    video and are labeled with fewer identities. That means other identities are only
    single camera’s frame segments and the lighting and shooting angles of these identities
    in this dataset may not change much. MARS and DukeMTMC-ViedeReID are large-scale
    video-based person Re-ID datasets. Their bounding boxes and tracks are automatically
    generated and contain several natural detection or tracking errors, and each tag
    may have multiple tracks. LPW is one of the most challenging video-based person
    Re-ID datasets available and closer to the real world, distinguishes from existing
    datasets in three aspects: more identities and tracks, automatically detected
    bounding boxes, and far more crowded scenes with a larger time span.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4](#S2.F4 "Fig. 4 ‣ 2.1.1 Image-based person Re-ID datasets ‣ 2.1 Datasets
    ‣ 2 Datasets and Evaluation metrics ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows some sample images of a
    partial person Re-ID dataset. We can see that with the development of large-scale
    person Re-ID datasets, the number of pedestrian IDs and the number of labeled
    frames or trajectories in the datasets are increasing, and the scenarios covered
    by the datasets are getting richer. These datasets use a combination of deep learning
    detectors and manual annotation to detect pedestrian bounding boxes, making the
    latest datasets closer and closer to the real world, thus enhancing the robustness
    of person Re-ID models. In addition, almost all mainstream person Re-ID datasets
    are evaluated using mean average precision (mAP) and cumulative matching characteristics
    (CMC) curves for performance evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The commonly used evaluation metrics of person Re-ID algorithms are cumulative
    matching characteristics (CMC) curves and mean average precision (mAP).
  prefs: []
  type: TYPE_NORMAL
- en: 'In pattern recognition systems, CMC curves are important evaluation metrics
    in the fields of face, fingerprint, iris detection and person Re-ID, which can
    comprehensively assess the merits of model algorithms. Furthermore, CMC curves
    are considered to be a comprehensive reflection of the performance of the person
    Re-ID classifier. Before calculating the CMC curves, the probability ${Acc_{k}}$
    that the top-k retrieved images (top-k) in the gallery contain the correct query
    result is obtained by ranking the similarity between the query target and the
    target image to be queried, which is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="{Acc_{k}}=\begin{array}[]{l}\begin{cases}1&amp;\text{if
    top-k rank gallery samples}\\ &amp;\text{ contain query identity.}\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{otherwise}\end{cases}\end{array}" display="block"><semantics ><mrow
     ><mrow  ><mi
     >A</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi 
    >c</mi><mo lspace="0em" rspace="0em" 
    >​</mo><msub  ><mi
     >c</mi><mi 
    >k</mi></msub></mrow><mo  >=</mo><mtable
    displaystyle="true"  ><mtr 
    ><mtd class="ltx_align_left" columnalign="left" 
    ><mrow  ><mo
     >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mn
     >1</mn></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mtext
     >if
    top-k rank gallery samples</mtext></mtd></mtr><mtr 
    ><mtd class="ltx_align_left" columnalign="left"
     ><mtext 
    > contain query identity.</mtext></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mn
     >0</mn></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mtext
     >otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    >𝐴</ci><ci  >𝑐</ci><apply
     ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci  >𝑐</ci><ci
     >𝑘</ci></apply></apply><matrix
     ><matrixrow 
    ><apply  ><csymbol
    cd="latexml"  >cases</csymbol><cn
    type="integer"  >1</cn><ci
     ><mtext
     >if
    top-k rank gallery samples</mtext></ci><ci  ><mtext
    class="ltx_mathvariant_italic"  >otherwise</mtext></ci><ci
     ><mtext
     > contain
    query identity.</mtext></ci><cn type="integer" 
    >0</cn><ci 
    ><mtext 
    >otherwise</mtext></ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >{Acc_{k}}=\begin{array}[]{l}\begin{cases}1&\text{if
    top-k rank gallery samples}\\ &\text{ contain query identity.}\\ 0&\text{otherwise}\end{cases}\end{array}</annotation></semantics></math>
    |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: CMC curves are calculated by adding up the ${Acc_{k}}$ of each query image and
    dividing it by the total number of query images, which are usually expressed as
    Rank-k. For example, Rank-1 accuracy indicates the probability of correctly matching
    to the first target in the matching list.
  prefs: []
  type: TYPE_NORMAL
- en: A single evaluation metric often cannot comprehensively evaluate the comprehensive
    performance of the person Re-ID algorithm. The mAP can reflect the extent to which
    all images with correct queries are at the front of the result queue in the query
    results. Considering both the average precision (AP) and precision-recall curve
    (PR) of the query process [[91](#bib.bib91)], instead of just focusing on the
    hit rate, which can measure the performance of person Re-ID algorithms more comprehensively.
    The algorithm is usually necessary to be evaluated separately for CMC curves and
    mAP in person Re-ID tasks. Zheng et.al. [[8](#bib.bib8)] proposed the Re-ranking
    method, which can re-rank the query results and further improve the effectiveness
    of Rank-k and mAP accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Based Re-ID Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we classify deep learning-based person Re-ID methods into
    four categories with classification structure is shown in [Figure 5](#S3.F5 "Fig.
    5 ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works"), including methods for depth metric
    learning, local feature learning, generative adversarial learning and sequences
    feature learning. In addition, we subdivide the above four categories according
    to their methodologies and motivations, discussing and comparing the advantages
    and limitations of part subcategories.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c86c28b91a6bbec30943bd2b8dd039f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Classification structure of deep learning-based person re-identification
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Deep metric learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep metric learning (DML) is one of the metric learning (ML) methods that
    aims to learn the similarity or dissimilarity between two pedestrian objects.
    The main goal of DML is to learn a mapping from the original image to the feature
    embedding (FE) such that the same pedestrians have smaller distances using a distance
    function on the feature space and different pedestrians feature farther apart
    from each other [[101](#bib.bib101), [102](#bib.bib102)]. With the rise of deep
    neural networks(DNNs), DML has been widely used in computational vision, such
    as face recognition, image retrieval, and person Re-ID. DML is mainly used to
    constrain the learning of discriminative features by designing loss functions
    for network models [[78](#bib.bib78)]. In this paper, we focus on loss functions
    commonly used in person Re-ID tasks, including classification loss [[19](#bib.bib19),
    [20](#bib.bib20), [47](#bib.bib47), [51](#bib.bib51), [103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105)], verification loss [[20](#bib.bib20), [106](#bib.bib106),
    [107](#bib.bib107), [78](#bib.bib78)], contrastive loss [[21](#bib.bib21), [108](#bib.bib108),
    [109](#bib.bib109)], triplet loss [[22](#bib.bib22), [110](#bib.bib110), [111](#bib.bib111)]
    and quadruplet loss [[23](#bib.bib23)]. An illustration of five loss functions
    is shown in [Figure 6](#S3.F6 "Fig. 6 ‣ 3.1 Deep metric learning ‣ 3 Deep Learning
    Based Re-ID Method ‣ Deep learning-based person re-identification methods: A survey
    and outlook of recent works"). These deep metric learning methods enable models
    to learn discriminative features automatically, which can solve the problem of
    manually designing features that consume a lot of labor costs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a02fc5d1f329c3329b7622070d43cd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: An illustration shows a variety of existing deep metric learning-based
    person Re-ID losses. (a) Classification loss. (b) Verification loss. (c) Contrastive
    loss. (d) Triplet loss, where $A$, $P$, $N$ indicate anchor, positive and negative
    samples, respectively. (e) Quadruplet loss, where $N1$ and $N2$ are different
    negative samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Classification loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Zheng et al. [[3](#bib.bib3), [112](#bib.bib112)] treated the training process
    of person Re-ID as a multi-classification task for images and proposed an ID-discriminative
    embedding (IDE) network. The IDE treats each pedestrian as a different class and
    uses the ID of the pedestrian as a classification label to train a deep neural
    network, so the classification loss is also called ID loss. The training network
    for classification loss usually inputs a picture and connects a fully connected
    layer (FC) for classification at the end of the network, and then maps the feature
    vectors of the image onto the probability space by the softmax activation function.
    The cross-entropy loss for multi-classification of the person Re-ID task can be
    expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{id}=-\sum_{a=1}^{K}q\left(x_{a}\right)\log{}p\left(y_{a}&#124;x_{a}\right)$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Where $K$ represents the number of training sample ID categories per batch,
    $q(x_{a})$ denotes the label of sample image $x_{a}$. If $x_{a}$ is identified
    as $y_{a}$, then $q(x_{a})=1$, otherwise $q(x_{a})=0$. $p(y_{a}|x_{a})$ is the
    probability that picture $x_{a}$ is predicted as category $y_{a}$ using the softmax
    activation function. Classification loss is widely used as a depth metric learning
    for person Re-ID methods because of its advantages such as easy training of models
    and mining hard samples [[19](#bib.bib19), [20](#bib.bib20), [47](#bib.bib47),
    [51](#bib.bib51), [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105)].
    However, using ID information alone is not enough to learn a model with sufficient
    generalization ability. Therefore, ID loss usually needs to be combined with other
    losses to constrain the training of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Verification loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Person Re-ID can also be treated as a validation problem, and validation loss
    is proposed to guide the training of the model. In contrast to classification
    loss, the network trained by verification loss requires two images as input, and
    a binary loss is computed by fusing the feature information of the two images,
    which in turn determines whether the input two images are the same pedestrian
    [[20](#bib.bib20), [106](#bib.bib106), [107](#bib.bib107), [78](#bib.bib78)].
    The expression of the cross-entropy validation loss function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{v}=-y_{ab}\log{}p\left(y_{ab}&#124;f_{ab}\right)-$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle(1-y_{ab})\log{}\left(1-p(y_{ab}&#124;f_{ab})\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Supposing the network inputs two images $x_{a}$ and $x_{b}$, we get the feature
    vectors $f_{a}$ and $f_{b}$ of these two images respectively, and calculate the
    difference feature $f_{ab}={(f_{a}-f_{b})^{2}}$ of the two feature vectors. We
    use the softmax activation function to calculate the probability $p$ that the
    image pairs $x_{a}$ and $x_{b}$ have the same pedestrian ID, where $y_{ab}$ is
    the pedestrian ID label of the two images. When the images $x_{a}$ and $x_{b}$
    have the same ID, $y_{ab}=1$, otherwise, $y_{ab}=0$.
  prefs: []
  type: TYPE_NORMAL
- en: The verification loss is less efficient in recognition because it can only input
    a pair of images to judge the similarity when tested while ignoring the relationship
    between the image pair and other images in the dataset. For this reason, researchers
    considered combining classification and validation networks [[106](#bib.bib106),
    [107](#bib.bib107)], and the combined loss can be expressed as $\mathcal{L}=\mathcal{L}_{id}+\mathcal{L}_{v}$.
    The hybrid loss can combine the advantages of classification loss and verification
    loss, which can predict the identity ID of pedestrians and perform similarity
    metrics simultaneously, thus improving the accuracy of person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Contrastive loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Contrastive loss, which mainly constrains the similarity or dissimilarity between
    pairs of data, is generally used for model training of twin networks (Siamese
    Network) in person Re-ID tasks [[21](#bib.bib21), [108](#bib.bib108), [109](#bib.bib109)].
    Its function can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{c}=yd\left(x_{a}-x_{b}\right)^{2}+\left(1-y\right)\left[m-d\left(x_{a}-x_{b}\right)^{2}\right]_{+}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Where $\left[z\right]_{+}=max(0,z)$, $x_{a}$ and $x_{b}$ are two images input
    to the twin network at the same time. $d(x_{a},x_{b})$ usually indicates the euclidean
    distance (similarity) of the two images. $m$ is the set training threshold, and
    $y$ is the label of whether each pair of training images matches. When $y=1$,
    it means that the input images $x_{a}$ and $x_{b}$ belong to the pedestrians with
    the same ID (positive sample pair). When $y=0$, it means that the input images
    $x_{a}$ and $x_{b}$ belong to pedestrians with different IDs (negative sample
    pair). $\mathcal{L}_{c}$ reflects well the matching degree of sample pairs, which
    is often used to train models for person Re-ID feature extraction and often works
    together with classification loss combinations for training networks [[47](#bib.bib47)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Triplet loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Triplet loss is one of the most widely used depth metric losses in person Re-ID
    tasks, and it aims to minimize the intra-class distance and maximize the inter-inter-class
    distance of samples. With the development of deep neural networks, a large number
    of variants based on triplet loss have emerged [[22](#bib.bib22), [110](#bib.bib110),
    [111](#bib.bib111), [113](#bib.bib113)]. The triplet loss function can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{trip}=\left[m+d(x_{a},x_{p})-d(x_{a},x_{n})\right]_{+}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Different from contrast loss, the input of triplet loss is a triplet consisting
    of three images. Each triplet contains a pair of positive samples and a negative
    sample, where $x_{a}$ is the Anchor image, $x_{p}$ is the Positive image, and
    $x_{n}$ is the Negative image, and the pedestrians of $x_{a}$ and $x_{p}$ have
    the same ID. The pedestrians of $x_{a}$ and $x_{n}$ have different IDs. By model
    training, the distance between $x_{a}$ and $x_{p}$ in the Euclidean space is made
    closer than the distance between $x_{n}$ and $x_{a}$. To improve the performance
    of models, some deep learning-based person Re-ID methods use a combination of
    classification loss and triplet loss [[114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118)]. Experiments have
    shown that combining these two losses facilitates the model to learn discriminative
    features. Traditional triplet loss randomly selects three images from the training
    set during training, which may result in a simple combination of samples and lacks
    the training of hard sample combinations and makes the training model less generalizable.
    For this reason, some researchers considered improving triplet loss for mining
    hard samples [[22](#bib.bib22), [119](#bib.bib119), [120](#bib.bib120)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Quadruplet loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another improvement for the triplet loss is to add a negative sample picture
    $X_{n2}$ to form a quadruplet loss [[23](#bib.bib23)], where negative sample $X_{n1}$
    and negative sample $X_{n2}$ have different pedestrian IDs. The expression of
    the quadruplet loss function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{quad}=\left[m_{1}+d(x_{a},x_{p})-d(x_{a},x_{n1})\right]_{+}+$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left[m_{2}+d(x_{a},x_{p})-d(x_{n1},x_{n2})\right]_{+}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Where $m_{1}$ and $m_{2}$ are custom training thresholds. The positive and negative
    sample pairs have the same anchored image $x_{a}$. The first term of $\mathcal{L}_{quad}$
    is identical to the triplet loss function, which is used to constrain the relative
    distance between positive and negative sample pairs. The traditional triplet loss
    function often increases the inter-class distance of negatives sample pairs, which
    affects the feature learning of image $x_{a}$. For this reason, $\mathcal{L}_{quad}$
    introduces a second term to constrain the absolute distance between positive and
    negative sample pairs. The positive and negative sample pairs in the second term
    have different anchor images, which can effectively reduce the intra-class distance
    of positive sample pairs while increasing the distance of negative sample pairs
    between classes. In order to make the first term play a dominant role, it is usually
    important to ensure that ${m}_{1}>{m}_{2}$ during the training process. However,
    most person Re-ID methods using triplet loss drive focused more on differentiating
    appearance differences and cannot effectively learn fine-grained features. To
    address this issue, Yan et al. [[121](#bib.bib121)] introduce a novel pairwise
    loss function that enables Re-ID models to learn the fine-grained features by
    adaptively enforcing an exponential penalization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Local feature learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the features extracted from pedestrian images for classification, person
    Re-ID methods can be classified into global feature learning-based methods and
    local feature learning-based methods. The global feature learning methods usually
    extract one feature of the pedestrian image [[122](#bib.bib122), [123](#bib.bib123),
    [124](#bib.bib124)], and it is difficult for this method to capture the detailed
    information of the pedestrian. Therefore, how to extract discriminative local
    features of pedestrians with subtle differences becomes a problem for researchers
    to focus on.
  prefs: []
  type: TYPE_NORMAL
- en: The local feature learning-based methods aim at learning pedestrian discriminative
    features and ensuring the alignment of each local feature. Manual annotation or
    neural networks are usually used to automatically focus on certain local regions
    with key information and extract the distinguishing features from these regions.
    Commonly used local feature learning methods are predefined stripe segmentation
    [[24](#bib.bib24), [25](#bib.bib25), [27](#bib.bib27), [108](#bib.bib108), [125](#bib.bib125),
    [126](#bib.bib126)], multi-scale fusion [[127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131)], soft attention [[26](#bib.bib26),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [132](#bib.bib132), [133](#bib.bib133)], pedestrian semantic
    extraction [[114](#bib.bib114), [27](#bib.bib27), [28](#bib.bib28), [31](#bib.bib31),
    [134](#bib.bib134)] and global-local feature learning [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)].
    These methods can alleviate the problems of occlusion, boundary detection errors,
    view, and pose variations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Predefined stripe segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main idea of the method based on predefined stripe segmentation is to strip
    the learned features according to some predefined division rules, which must ensure
    that the partitions are spatially aligned. Liu et al. [[125](#bib.bib125)] proposed
    an attribute and appearance-based contextual attention network, where the appearance
    network learns spatial features from the whole body, horizontal and vertical parts
    of the pedestrian. Varior et al. [[108](#bib.bib108)] divided the pedestrian image
    into several strips uniformly and extracted local features from each strip image
    block. Sun et al. [[24](#bib.bib24)] considered the content consistency within
    each stripe to propose a local convolutional baseline (PCB). The PCB uses a uniform
    feature partitioning strategy to learn local features and outputs convolutional
    features consisting of multiple stripes to enhance the consistency of each partition’s
    feature content, thus ensuring stripes are spatially aligned.
  prefs: []
  type: TYPE_NORMAL
- en: Although the above methods can extract discriminative features for the striped
    areas, it may lead to incorrect retrieval results as the model is unable to distinguish
    between the obscured and unobscured areas. To relieve occlusion, Sun et al. [[25](#bib.bib25)]
    proposed a visibility-aware local model based on PCB to ensure that local features
    are spatially aligned and avoid interference due to pedestrian occlusion by learning
    common region features that are visible in both images. Fu et al. [[126](#bib.bib126)]
    horizontally slice the deep feature maps into multiple spatial strips using various
    pyramid scales and used global average pooling and maximum pooling to obtain discriminative
    features for each strip, which was named horizontal pyramid pooling (HPP). HPP
    can ignore that interference information, mainly coming from similar clothing
    or background.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Multi-scale fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Small-scale feature maps have a strong ability to represent spatial geometric
    information and can obtain detailed information of the image. Large-scale feature
    maps are good at characterizing semantic information and can get contour information
    of images. Extracting pedestrian features at multiple scales for fusion can obtain
    rich pedestrian feature representation.
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[127](#bib.bib127)] proposed a multi-scale triple convolutional
    neural network, which can capture pedestrian appearance features at different
    scales. Since pedestrian features learned at different scales differ or conflict,
    the direct merging of features at multiple scales may not achieve the best fusion
    effect. Therefore, researchers have started to focus on the complementary advantages
    of cross-scale implicit associations. Chen et al. [[128](#bib.bib128)] studied
    the problem of person Re-ID multi-scale feature learning and proposed a deep pyramidal
    feature learning deep neural network framework, which can overcome the differences
    in cross-scale feature learning while learning multi-scale complementary features.
    Zhou et al. [[135](#bib.bib135)] presented a Re-ID CNN termed Omni-scale network
    (OSNet) to learn features that not only captured different spatial scales but
    also encapsulated a synergistic combination of multiple scales. In traditional
    person Re-ID datasets, OSNet achieved state-of-the-art performance, despite being
    much smaller than existing Re-ID models. The challenge of large intra-class variation
    and small inter-class variation often arises in cross-camera person Re-ID tasks.
    For example, cross-camera viewpoint changes can obscure parts of the person with
    discriminative features, or pedestrians wearing similar clothes appear across
    cameras, which makes the matching of the same person incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Soft attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of attention is to find the areas that have a greater impact on the
    feature map and to focus the model on discriminative local parts of body appearances
    to correct misalignment, eliminate background perturbance. Due to the good performance
    of the attention mechanism in the computer vision field, it is often used as a
    local feature learning in the person Re-ID tasks. Most of the current attention-based
    person Re-ID methods tend to use soft attention which can be divided into spatial
    attention, channel attention, mixed attention, non-local attention and position
    attention. Liu et al. [[29](#bib.bib29)] proposed an attention-based deep neural
    network capable of capturing multiple attention features from the underlying to
    the semantic layer to learn fine-grained integrated features of pedestrians. Li
    et al. [[30](#bib.bib30)] used balanced attention convolutional neural networks
    to maximize the complementary information of attention features at different scales
    to solve the person Re-ID challenge for arbitrary unaligned images. To obtain
    the local fine-grained features of a person, Ning et al. [[136](#bib.bib136)]
    proposed a multi-branch attention network with diversity loss, and the local features
    were obtained via adaptive filtering by removing interference information.
  prefs: []
  type: TYPE_NORMAL
- en: The above spatial attention-based methods tend to focus only on the local discriminative
    features of pedestrians but ignore the impact of feature diversity on pedestrian
    retrieval. Chen et al. [[26](#bib.bib26)] proposed an attentional diversity network
    that used complementary channel attention module (CAM) and position attention
    module (PAM) to learn the characteristics of pedestrian diversity. Considering
    that features extracted from first-order attention such as spatial attention and
    channel attention are not discriminative in complex camera view and pose change
    scenarios [[32](#bib.bib32)]. Chen et al. [[33](#bib.bib33)] proposed a higher-order
    attention module, which modelled the complex higher-order information in the attention
    mechanism to mine discriminative attention features among pedestrians.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Semantic extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some researchers used deep neural networks to extract semantic information such
    as body parts or body postures instead of bounding boxes to extract local features
    from pedestrian body parts to improve the performance of person Re-ID. Zhao et
    al. [[28](#bib.bib28)] considered the application of body structure information
    to the person Re-ID task and proposed a novel CNN, called SpindleNet. Specifically,
    firstly, SpindleNet used a body part generation network to locate 14 key points
    of body parts to extract 7 body regions of the pedestrian. Secondly, SpindleNet
    captured semantic features from different body regions using a convolutional neural
    network. Finally, SpindleNet used a tree fusion network with competing strategies
    to merge the semantic features from different body regions. SpindleNet can align
    the features of body parts over the whole image and can better highlight local
    detail information.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, not only do the body parts contain discriminative features but also
    non-body parts may contain certain key features, such as the pedestrian’s distinguishing
    backpack or handbag. Therefore, some researchers considered the alignment of non-body
    parts. Guo et al. [[114](#bib.bib114)] proposed a dual part-aligned representation
    scheme that captured distinguishing information beyond body parts using an attention
    mechanism to update the representation by exploiting the complementary information
    from both the accurate human parts and the coarse non-human parts. Miao et al.
    [[27](#bib.bib27)] proposed a pose-guided feature alignment scheme to distinguish
    information from occlusion noise by pedestrian pose bounding markers, thus aligning
    the query image and the non-occluded area of the queried image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Global-local feature learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Local feature learning can capture detailed information about a region of the
    pedestrian, but the reliability of local features can be affected by variations
    in pose and occlusion. Therefore, some researchers often combine fine-grained
    local features with coarse-grained global features to enhance the final feature
    representation. Wang et al. [[38](#bib.bib38)] proposed a multi-granularity feature
    learning strategy with global and local information, including one branch for
    global feature learning and two branches for local feature learning. Ming et al.
    [[137](#bib.bib137)] designed a global-local dynamic feature alignment network
    (GLDFA-Net) framework, which contained both global and local branches. The local
    sliding alignment(LSA) strategy was introduced into the local branch of GLDFA-Net
    to guide the computation of distance metrics, which can further improve the accuracy
    of the testing phase. To mitigate the impact of imprecise bounding boxes on pedestrian
    matching, Zheng et al. [[43](#bib.bib43)] proposed a coarse-grained to fine-grained
    pyramid model that integrates not only local and global information of pedestrians
    but also progressive cues from coarse to fine-grained. The model can match pedestrian
    images of different scales and retrieve pedestrian images with the same local
    identity even when the images are not aligned.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Comparison and discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of experimental results of local feature learning based
    methods. $*$ represents the use of multiple deep learning methods. The bold and
    underlined numbers represent the top two results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Methods | CUHK03 | Market-1501 | DukeMTMC-reID | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 | mAP | R-1 |'
  prefs: []
  type: TYPE_TB
- en: '| Predefined stripe segmentation | PCB+RPP[[24](#bib.bib24)] | - | - | 81.6
    | 93.8 | 69.2 | 83.3 | ECCV’18 |'
  prefs: []
  type: TYPE_TB
- en: '| CA3Net[[125](#bib.bib125)] | - | - | 80.0 | 93.2 | 70.2 | 84.6 | ACMMM’18
    |'
  prefs: []
  type: TYPE_TB
- en: '| PGFA[[27](#bib.bib27)]* | - | - | 76.8 | 91.2 | 65.5 | 82.6 | ICCV’19 |'
  prefs: []
  type: TYPE_TB
- en: '| VPM[[25](#bib.bib25)] | - | - | 80.8 | 93.0 | 72.6 | 83.6 | CVPR’19 |'
  prefs: []
  type: TYPE_TB
- en: '| HPM[[126](#bib.bib126)] | 57.5 | 63.9 | 82.7 | 94.2 | 74.3 | 86.6 | AAAI’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-scale fusion | DPFL[[128](#bib.bib128)] | 40.5 | 43.0 | 72.6 | 88.6
    | 60.6 | 79.2 | ICCV’17 |'
  prefs: []
  type: TYPE_TB
- en: '| MSCAN[[130](#bib.bib130)] | - | 74.2 | 66.7 | 86.8 | - | - | CVPR’17 |'
  prefs: []
  type: TYPE_TB
- en: '| OSNet[[129](#bib.bib129)] | 67.8 | 72.3 | 84.9 | 94.8 | 73.5 | 88.6 | ICCV’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| CAM[[131](#bib.bib131)] | 64.2 | 66.6 | 84.5 | 94.7 | 72.9 | 85.8 | CVPR’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| Soft attention | HydraPlus[[29](#bib.bib29)] | - | 91.8 | - | 76.9 | - |
    - | ICCV’17 |'
  prefs: []
  type: TYPE_TB
- en: '| HA-CNN[[30](#bib.bib30)] | 44.4 | 41.0 | 75.7 | 91.2 | 63.8 | 80.5 | CVPR’18
    |'
  prefs: []
  type: TYPE_TB
- en: '| ABD-net[[26](#bib.bib26)] | - | - | 88.3 | 95.6 | 78.6 | 89.0 | ICCV’19 |'
  prefs: []
  type: TYPE_TB
- en: '| HOA[[33](#bib.bib33)] | 72.4 | 77.2 | 85.0 | 95.1 | 77.2 | 89.1 | ICCV’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| AANet[[34](#bib.bib34)] | - | - | 83.4 | 93.9 | 74.3 | 87.7 | CVPR’19 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HLGAT[[132](#bib.bib132)] | 80.6 | 83.5 | 93.4 | 97.5 | 87.3 | 92.7 |
    CVPR’21 |'
  prefs: []
  type: TYPE_TB
- en: '|  | PAT[[138](#bib.bib138)] | - | - | 88.0 | 95.4 | 78.2 | 88.8 | CVPR’21
    |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic extraction | Spindle[[28](#bib.bib28)] | - | 88.5 | - | 76.9 | -
    | - | CVPR’17 |'
  prefs: []
  type: TYPE_TB
- en: '| SPReID[[134](#bib.bib134)] | - | 94.3 | 83.4 | 93.7 | 73.3 | 85.9 | CVPR’18
    |'
  prefs: []
  type: TYPE_TB
- en: '| P2-Net[[114](#bib.bib114)] | 73.6 | 78.3 | 85.6 | 95.2 | 73.1 | 86.5 | ICCV’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| Global-Local feature learning | GLAD[[39](#bib.bib39)] | - | 85.0 | 73.9
    | 89.9 | - | - | ACMMM’17 |'
  prefs: []
  type: TYPE_TB
- en: '| PDC[[40](#bib.bib40)] | - | 88.7 | 63.4 | 84.1 | - | - | ICCV’17 |'
  prefs: []
  type: TYPE_TB
- en: '| Pyramid[[43](#bib.bib43)] | 76.9 | 78.9 | 88.2 | 95.7 | 79.0 | 89.0 | CVPR’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| RGA[[42](#bib.bib42)] | 77.4 | 81.1 | 88.4 | 96.1 | - | - | CVPR’20 |'
  prefs: []
  type: TYPE_TB
- en: '| SCSN[[41](#bib.bib41)] | 84.0 | 86.8 | 88.5 | 95.7 | 79.0 | 91.0 | CVPR’20
    |'
  prefs: []
  type: TYPE_TB
- en: '[Table 4](#S3.T4 "Table 4 ‣ 3.2.6 Comparison and discussion ‣ 3.2 Local feature
    learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the experimental results
    of the local feature learning methods on CUHK03, Market1501 and DukeMTMC-reID
    datasets. These results are all experimental results without Re-ranking [[8](#bib.bib8)].
    In a general view, the experimental performance of the semantic extraction and
    global-local feature learning methods is significantly higher than that of the
    methods with predefined stripe segmentation, multi-scale fusion and part attention.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the predefined stripe segmentation method is simple and easy to
    implement, but it is hard segmentation and requires high image alignment. With
    the change of real scene camera view and pedestrian pose, the hard segmentation
    strategy cannot solve the problem of unaligned pedestrians well. The multi-scale
    fusion method can learn the deeper cues of pedestrian images, but there will be
    redundancy and conflicting features at different scales. The attention focuses
    only on the local features of key parts of pedestrians, and easily ignores the
    distinguishing features of non-focus regions. The semantic extraction method can
    precisely locate the local features of pedestrians by learning the structural
    information of pedestrian pose, but it requires the additional computation of
    pedestrian pose models. The global-local feature learning method can effectively
    utilize the complementary advantages of global features and local features and
    is one of the common methods used by researchers to improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Generative adversarial learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 2014, Goodfellow et al. [[139](#bib.bib139)] first proposed generative adversarial
    networks (GAN) and rapidly developed in recent years. A large number of variants
    and applications of GAN emerged [[45](#bib.bib45), [47](#bib.bib47), [51](#bib.bib51),
    [140](#bib.bib140), [54](#bib.bib54), [55](#bib.bib55), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143)]. Image generation, as one of the significant applications
    of GAN, was widely used in the field of person Re-ID. [Figure 7](#S3.F7 "Fig.
    7 ‣ 3.3 Generative adversarial learning ‣ 3 Deep Learning Based Re-ID Method ‣
    Deep learning-based person re-identification methods: A survey and outlook of
    recent works") shows the workflow diagram of GAN used to generate the image. In
    the training phase, the generator ${G}_{AB}$ converts image $A$ into image $B$
    with random noise, the generator ${G}_{BA}$ converts image $B$ into image $A$,
    and the discriminator ${D}_{B}$ determines whether the generated image $B$ approximates
    the original image $B$ style (Real or Fake). The generator and discriminator keep
    adversarial until convergence by minimizing the discriminator loss and ${L}_{2}$
    loss [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some researchers used GAN to transform the style of images or unify different
    image styles to mitigate image style differences between various datasets or within
    the same dataset [[45](#bib.bib45), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [144](#bib.bib144), [51](#bib.bib51), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147)]. Some works used GAN to synthesize pedestrian images with
    a different pose, appearance, lighting, and resolution for expanding the dataset
    to improve the generalization ability of the model [[52](#bib.bib52), [53](#bib.bib53),
    [148](#bib.bib148), [149](#bib.bib149), [140](#bib.bib140), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152), [54](#bib.bib54), [153](#bib.bib153)].
    Some researchers also used GAN to learn features that are not noise-related but
    identity-related to improve the accuracy of feature matching [[46](#bib.bib46),
    [55](#bib.bib55), [141](#bib.bib141), [154](#bib.bib154)]. These methods can alleviate
    the small number of training samples, resolution, illumination, view, and pose
    variation. Based on the characteristics and application scenarios of GAN, we classify
    the generative adversarial learning-based person Re-ID methods into three categories:
    image-image style transfer, data enhancement, and invariant feature learning.
    For image-image style transfer methods, GAN learned the background, resolution,
    lighting and other features of an image and transferred these features to other
    images to give other images a different style. For data enhancement methods, the
    diversity of samples that can be generated by GAN to expand the dataset was used
    to reinforce the final feature representation. For invariant feature learning,
    GAN was used for disentangled representation learning, which can learn identity-related
    but noise-independent features (e.g., pose, lighting, resolution, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c1472463149f6b076dd3aa0beb31e71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: Workflow diagram of GAN for image generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Image-image style transfer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Domain gaps usually exist between different datasets in the person Re-ID task
    [[45](#bib.bib45)]. When training and testing on various datasets separately,
    the performance of the model is severely degraded, which hinders the effective
    generalization of the model to new test sets [[155](#bib.bib155), [156](#bib.bib156)].
    A common strategy to solve such as domain gap is to use GAN to perform style transformation
    across data domains. Since CycleGAN [[142](#bib.bib142)] implemented conversion
    of any two image styles, researchers considered improving on this to achieve adaptive
    pedestrian style conversion between different datasets to reduce or eliminate
    domain aberrations. Inspired by CycleGAN, Wei et al. [[45](#bib.bib45)] proposed
    a person transfer generative adversarial network (PTGAN) to transfer the pedestrians
    in the source domain to the target dataset while preserving the identity of the
    pedestrians in the source domain so that the pedestrians in the source domain
    have the background and lighting styles of the target domain. The pedestrians
    in the source domain are transferred to the target dataset so that the pedestrians
    in the source domain have the background and lighting patterns of the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: Deng et al. [[47](#bib.bib47)] used twin networks and CycleGAN to form a similarity
    preserving generative adversarial network (SPGAN) to migrate labeled pedestrians
    from the source domain to the target domain in an unsupervised manner. Liu et
    al. [[48](#bib.bib48)] proposed an adaptive transfer network (ATNet). ATNet used
    three CycleGANs to implement the style of camera view, lighting, resolution and
    adaptively assign weights to each CycleGAN according to the degree of influence
    of different factors. Zhong et al. [[50](#bib.bib50)] proposed hetero-homogeneous
    learning (HHL) method that considers not only the domain differences between various
    datasets but also the effect of style differences of cameras within the target
    domain on the cross-domain adaptation person Re-ID performance.
  prefs: []
  type: TYPE_NORMAL
- en: Zhong et al. [[51](#bib.bib51)] introduced camera style(CamStyle) to solve the
    problem of style variation among different cameras within the same dataset. CamStyle
    used CycleGAN to migrate the labeled training data to various cameras so that
    the synthesized samples had the styles of different cameras while retaining the
    pedestrian labels. In addition, CamStyle can also smooth out style differences
    between various cameras in the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Data enhancement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike style transformation that uses GAN to reduce domain gaps, the data augmentation-based
    methods start with the training of the model and improve the generalization ability
    of the model by increasing the diversity of the training data. Zheng et al. [[52](#bib.bib52)]
    were the first to use deep convolutional generative adversarial networks (DCGAN)
    [[157](#bib.bib157)] to generate sample data. Huang et al. [[53](#bib.bib53)]
    proposed a multi-pseudo regularized label (MpRL), which assigned an appropriate
    virtual label to each generated sample to establish the correspondence between
    the real image and the generated image. MpRL effectively distinguished various
    generated data and achieved good recognition results on datasets such as Market-1501,
    DukeMTMC-Reid and CUHK03.
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[140](#bib.bib140)] introduced pedestrian pose information to assist
    GAN in generating samples. The GAN was used to generate sample images with both
    the pose structure of pedestrians in MARS data and the appearance of pedestrians
    in the existing dataset. Qian et al. [[150](#bib.bib150)] used pose normalization
    GAN (PN-GAN) to generate pedestrian images with uniform body pose. To alleviate
    the problem that the pose of pedestrian images generated by earlier methods is
    prone to large deviations, Zhu et al. [[151](#bib.bib151)] trained a discriminator
    using a multilayer cascaded attention network. The discriminator can efficiently
    optimize the pose transformation of pedestrians using pose and appearance features
    so that the generated pedestrian images have the better pose and appearance consistency
    with the input images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison of experimental results of different GAN-based methods
    on CUHK03, Market1501 and DukeMTMC-reID datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Methods | CUHK03 | Market-1501 | DukeMTMC-reID | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 | mAP | R-1 |'
  prefs: []
  type: TYPE_TB
- en: '| Image-image style transfer | IDE[[3](#bib.bib3)]+CameraStyle[[51](#bib.bib51)]
    | - | - | 68.7 | 88.1 | 57.6 | 78.3 | CVPR’18 |'
  prefs: []
  type: TYPE_TB
- en: '| IDE[[3](#bib.bib3)]+UnityStyle[[145](#bib.bib145)] | - | - | 89.3 | 93.2
    | 65.2 | 82.1 | CVPR’20 |'
  prefs: []
  type: TYPE_TB
- en: '| Data enhancement | LSRO[[52](#bib.bib52)] | 87.4 | 84.6 | 66.1 | 84.0 | 47.1
    | 67.7 | ICCV’17 |'
  prefs: []
  type: TYPE_TB
- en: '| PNGAN[[150](#bib.bib150)] | - | 79.8 | 72.6 | 89.4 | 53.2 | 73.6 | ECCV’18
    |'
  prefs: []
  type: TYPE_TB
- en: '| PT[[140](#bib.bib140)] | 42.0 | 45.1 | 68.9 | 87.6 | 56.9 | 78.5 | CVPR’18
    |'
  prefs: []
  type: TYPE_TB
- en: '| MpRL[[53](#bib.bib53)] | 87.5 | 85.4 | 67.5 | 85.8 | 58.6 | 78.8 | TIP’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| DG-Net[[54](#bib.bib54)] | - | - | 86.0 | 94.8 | 74.8 | 86.6 | CVPR’19 |'
  prefs: []
  type: TYPE_TB
- en: '| Invariant feature learning | FD-GAN[[55](#bib.bib55)] | 91.3 | 92.6 | 77.7
    | 90.5 | 64.5 | 80.0 | NIPS’18 |'
  prefs: []
  type: TYPE_TB
- en: '| RAIN[[141](#bib.bib141)] | - | 78.9 | - | - | - | - | AAAI’19 |'
  prefs: []
  type: TYPE_TB
- en: '| CAD-Net[[154](#bib.bib154)] | - | 82.1 | - | 83.7 | - | 75.6 | ICCV’19 |'
  prefs: []
  type: TYPE_TB
- en: '| DI-REID[[46](#bib.bib46)] | - | 85.7 | - | - | - | - | CVPR’20 |'
  prefs: []
  type: TYPE_TB
- en: 3.3.3 Invariant feature learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GAN can be used for feature learning in addition to synthesizing images. Generally,
    the person Re-ID task in the real-world consists of high-level and low-level vision
    variations [[46](#bib.bib46)]. The former mainly includes changes in pedestrian
    occlusion, pose, and camera view, and the latter mainly includes changes in resolution,
    illumination, and weather. The images obtained from low-level vision changes are
    usually called degraded images. These vision changes may lead to the loss of discriminative
    feature information, which may cause feature mismatch and significantly degrade
    the retrieval performance [[158](#bib.bib158)].
  prefs: []
  type: TYPE_NORMAL
- en: For pose changes in high-level vision, Ge et al. [[55](#bib.bib55)] proposed
    feature distilling generative adversarial network (FD-GAN) to learn features related
    to pedestrian identity instead of pose for pedestrians. The method requires no
    additional computational cost or auxiliary attitude information and has advanced
    experimental results on the Market-1501, CUHK03 and DukeMTMC-reID.
  prefs: []
  type: TYPE_NORMAL
- en: Several researchers considered using GAN to learn common invariant features
    of low- and high-resolution pedestrian images. Chen et al. [[141](#bib.bib141)]
    proposed an end-to-end resolution adaptation and re-identification network (RAIN)
    that learned and aligned invariant features of pedestrian images of different
    resolutions by adding adversarial losses to low- and high-resolution image features.
    Li et al. [[154](#bib.bib154)] proposed adversarial learning strategies for cross
    resolution, which not only learned invariant features of pedestrian images of
    different resolutions but also recovered the lost fine-grained detail information
    of low-resolution images using super-resolution (SR).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparison of experimental results of different GAN-based cross-domain
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | D → M | M→ D | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 |'
  prefs: []
  type: TYPE_TB
- en: '| LOMO[[159](#bib.bib159)] | 8.0 | 27.2 | 4.8 | 12.3 | CVPR’15 |'
  prefs: []
  type: TYPE_TB
- en: '| Bow[[91](#bib.bib91)] | 14.8 | 35.8 | 8.3 | 17.1 | ICCV’15 |'
  prefs: []
  type: TYPE_TB
- en: '| UMDL[[160](#bib.bib160)] | 12.4 | 34.5 | 7.3 | 18.5 | CVPR’16 |'
  prefs: []
  type: TYPE_TB
- en: '| CAMEL[[16](#bib.bib16)] | 26.3 | 54.5 | - | - | ICCV’17 |'
  prefs: []
  type: TYPE_TB
- en: '| PUL[[161](#bib.bib161)] | 20.5 | 45.5 | 16.4 | 30.0 | MM’18 |'
  prefs: []
  type: TYPE_TB
- en: '| CycleGAN[[142](#bib.bib142)] | 19.1 | 45.6 | 19.6 | 38.1 | CVPR’17 |'
  prefs: []
  type: TYPE_TB
- en: '| PTGAN[[45](#bib.bib45)] | - | 38.6 | 27.4 | - | CVPR’18 |'
  prefs: []
  type: TYPE_TB
- en: '| SPGAN[[47](#bib.bib47)] | 22.8 | 51.5 | 22.3 | 41.4 | CVPR’18 |'
  prefs: []
  type: TYPE_TB
- en: '| HHL[[50](#bib.bib50)] | 31.4 | 62.2 | 27.2 | 46.9 | ECCV’18 |'
  prefs: []
  type: TYPE_TB
- en: '| ATNet[[48](#bib.bib48)] | 25.6 | 55.7 | 24.9 | 45.1 | CVPR’19 |'
  prefs: []
  type: TYPE_TB
- en: '| CR-GAN[[49](#bib.bib49)] | 29.6 | 59.6 | 30.0 | 52.2 | ICCV’19 |'
  prefs: []
  type: TYPE_TB
- en: '| DG-Net++[[162](#bib.bib162)] | 61.7 | 82.1 | 63.8 | 78.9 | ECCV’20 |'
  prefs: []
  type: TYPE_TB
- en: '| GCL[[153](#bib.bib153)] | 75.4 | 90.5 | 67.6 | 81.9 | CVPR’21 |'
  prefs: []
  type: TYPE_TB
- en: 3.3.4 Comparison and discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Table 5](#S3.T5 "Table 5 ‣ 3.3.2 Data enhancement ‣ 3.3 Generative adversarial
    learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person re-identification
    methods: A survey and outlook of recent works") shows the experimental results
    of GAN-based methods on CUHK03, Market1501 and DukeMTMC-reID datasets. These results
    are all experimental results without Re-ranking. Both IDE [[3](#bib.bib3)] + CameraStyle
    [[51](#bib.bib51)] and IDE [[3](#bib.bib3)] + UnityGAN [[145](#bib.bib145)] methods
    use GAN to generate pedestrian images with different camera styles within the
    same dataset with great experimental performance obtained in the Market-1501 and
    DukeMTMC-reID datasets. FD-GAN [[55](#bib.bib55)] has the highest performance
    in CUKH03, and it learns features related to pedestrian identity and poses independently,
    effectively reducing the influence of pose on the accuracy of person Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Market-1501 (M) and DukeMTMC-reID (D) datasets as the source and
    target domains, respectively. [Table 6](#S3.T6 "Table 6 ‣ 3.3.3 Invariant feature
    learning ‣ 3.3 Generative adversarial learning ‣ 3 Deep Learning Based Re-ID Method
    ‣ Deep learning-based person re-identification methods: A survey and outlook of
    recent works") compares traditional manual feature extraction methods (LOMO [[159](#bib.bib159)]
    and Bow [[91](#bib.bib91)]), traditional unsupervised methods (UMDL [[160](#bib.bib160)],
    CAMEL [[16](#bib.bib16)] and PUL [[161](#bib.bib161)]), and GAN-based cross-domain
    style transfer methods (CycleGAN(base) [[142](#bib.bib142)], PTGAN [[47](#bib.bib47)],
    SPGAN [[47](#bib.bib47)], HHL [[50](#bib.bib50)], ATNet [[48](#bib.bib48)] and
    CR-GAN [[49](#bib.bib49)]). From the experimental results, the cross-domain style
    transformation method is significantly better than the traditional unsupervised
    learning and manual feature learning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the method of image-to-image style transformation smooths the style
    variation of pedestrian images in different domains. Such methods can obtain a
    large number of automatically labeled synthetic images with the target domain
    style, which can be used together with the original images to enhance the training
    set and mitigate the domain gaps between different datasets. The problem with
    these methods is that the synthetic images contain noise, which may conflict with
    the source domain images when used for model training and affect the learning
    of discriminative features by the model. The method of generating diverse pedestrian
    images using GAN alleviates the problem of insufficient available training data
    to a certain extent. The method of image synthesis without auxiliary information
    guidance cannot generate high-quality images with sufficient distinguishing information.
    Auxiliary information-guided image synthesis methods require complex network structures
    to learn various pedestrian poses, which adds additional training costs. Invariant
    feature learning methods can alleviate the problem of unaligned pedestrian features
    and improve the accuracy of person Re-ID by learning features related to pedestrian
    identity but not to pose, resolution and illumination.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Sequence feature learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There have been many researchers who have used the rich information contained
    in video sequences for person Re-ID. These sequence feature learning-based methods
    take short videos as input and use both spatial and temporal complementary cues,
    which can alleviate the limitations of appearance-based features. Most of these
    methods use optical flow information [[56](#bib.bib56), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165), [166](#bib.bib166)], 3-dimensional convolutional neural networks
    (3DCNNs) [[57](#bib.bib57), [167](#bib.bib167)], recurrent neural networks(RNN)
    or long short term memory(LSTM) [[164](#bib.bib164), [165](#bib.bib165), [168](#bib.bib168),
    [169](#bib.bib169)], spatial-temporal attention [[58](#bib.bib58), [59](#bib.bib59),
    [166](#bib.bib166), [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173)] or graph convolutional networks (GCN) [[62](#bib.bib62), [61](#bib.bib61),
    [60](#bib.bib60), [174](#bib.bib174)] to model the spatial-temporal information
    of video sequences. These methods can mitigate occlusions, resolution changes,
    illumination changes, view and pose variations, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Optical flow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The optical flow method uses the change of pixels in the video sequence in the
    time domain and the correlation of the spatial-temporal context of adjacent frames
    to obtain the correspondence between the previous frame and the current frame.
    This method can obtain the motion information of the target between adjacent frames.
    Chung et al. [[56](#bib.bib56)] proposed a dual-stream convolutional neural network
    (DSCNN), where each stream is a siamese network. DSCNN can model both RGB images
    and optical flow, learning spatial and temporal information separately, allowing
    each siamese network to extract the best feature representation. Liu et al. [[163](#bib.bib163)]
    proposed an accumulative context network (AMOC), which consists of two input sequences,
    feeding the original RGB image and the optical flow image containing motion information,
    respectively. AMOC was used to improve the accuracy of person Re-ID by learning
    the discriminative cumulative motion context information of video sequences. The
    optical flow method was often used in combination with other methods such as McLaughlin
    et al. [[165](#bib.bib165)] who used optical flow information and RGB colors of
    images to capture motion and appearance information, combined with RNN to extract
    complete pedestrian appearance features of video sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 3D convolutional neural network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Three-dimensional convolutional neural networks (3DC-NN) are capable of capturing
    temporal and spatial feature information in videos. Recently, some researchers
    have applied 3DCNN to video-based person Re-ID with good results. Liao et al.
    [[57](#bib.bib57)] proposed a video person Re-ID method based on a combination
    of 3DCNN and non-local attention. 3DCNN used 3D convolution on video sequences
    to extract aggregated representations of spatial and temporal features and used
    non-local spatial-temporal attention to solve the alignment problem of deformed
    images. Although 3DCNN exhibited better performance, the stacked 3D convolution
    led to significant growth of parameters. Too many parameters not only made 3DCNN
    computationally expensive but also led to difficulties in model training and optimization.
    This made 3DCNN not readily applicable on video sequence-based person Re-ID, where
    the training set was commonly small and person ID annotation was expensive. To
    explore rich temporal cues for person Re-ID while mitigating the shortcomings
    of existing 3DCNN models, Li et al. [[167](#bib.bib167)] proposed a dual-stream
    multiscale 3D convolutional neural network (M3DCNN) for extracting spatial-temporal
    cues for video-based person Re-ID. M3DCNN was also more efficient and easier to
    optimize than the existing 3DCNN.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 RNN or LSTM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RNNs or LSTM can extract temporal features and are often applied in video-based
    person Re-ID tasks. McLaughlin et al. [[165](#bib.bib165)] proposed a novel recursive
    convolutional network(RCN), which used CNN to extract spatial features of video
    frames and RNNs to extract temporal features of video sequences. Yan et al. [[169](#bib.bib169)]
    used a recurrent feature aggregation network based on LSTM, which obtained cumulative
    discriminative features from the first LSTM node to the deepest LSTM node and
    effectively alleviated interference caused by occlusion, background clutter and
    detection failure. Chen et al. [[164](#bib.bib164)] decomposed a video sequence
    into multiple segments and used LSTM to learn the segments where the probe images
    are located in temporal and spatial features. This method reduces the variation
    of identical pedestrians in the sample and facilitates the learning of similarity
    features. Both types of methods mentioned above process each video frame independently.
    The features extracted by LSTM are generally affected by the length of the video
    sequence. The RNN only establishes temporal associations on high-level features
    and thus cannot capture the temporal cues of local details of the image [[167](#bib.bib167)].
    Therefore, there is still a need to explore a more efficient method for extracting
    spatial-temporal features.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Spatial-temporal attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention mechanism can selectively focus on the useful local information
    and has good performance in solving the problems of camera view switching, lighting
    changes and occlusion in person Re-ID tasks. Recently, several researchers have
    used attentional mechanisms to solve video-based person Re-ID tasks in both temporal
    and spatial dimensions. To solve the problem of unalignment and occlusion caused
    by changes in pedestrian body pose and camera view in video sequences, Li et al.
    [[170](#bib.bib170)] proposed a spatial-temporal attention model, the core idea
    of which is to use multiple spatial attention to extract features of key body
    parts and use temporal attention to compute the combined feature representations
    extracted by each spatial attention model. This method can better mine the potential
    distinguishing feature representations in video sequences. Similarly, Fu et al.
    [[58](#bib.bib58)] proposed a spatial-temporal attention framework that can fully
    utilize the distinguishing features of each pedestrian in both temporal and spatial
    dimensions through video frame selection, local feature mining, and feature fusion.
    The approach can well address challenges such as pedestrian pose variation and
    partial occlusion. Xu et al. [[166](#bib.bib166)] proposed a joint temporal and
    spatial attention pooling network to learn the feature representations of video
    sequences through the interdependence between video sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Graph convolutional networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In recent years, graph convolutional networks (GCNs) have been widely used
    for person Re-ID tasks due to their powerful automatic relational modeling capabilities
    [[175](#bib.bib175)], and a large number of variant networks have emerged [[60](#bib.bib60),
    [63](#bib.bib63), [64](#bib.bib64), [62](#bib.bib62), [65](#bib.bib65)]. Yang
    et al. [[60](#bib.bib60)] proposed a unified spatial-temporal graph convolutional
    neural network that modelled video sequences in three dimensions: temporal, spatial
    and appearance, and to mine more discriminative and robust information. Wu et
    al. [[62](#bib.bib62)] proposed an adaptive graph representation learning scheme
    for video person Re-ID using pose alignment connections and feature similarity
    connections to construct adaptive structure-aware adjacency graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Yan et al. [[63](#bib.bib63)] proposed a framework for pedestrian retrieval
    based on contextual graphical convolutional networks. Since image appearance features
    are not sufficient to distinguish different people, the authors use contextual
    information to extend instance-level features to improve the discriminative power
    of the features and the robustness of person retrieval. Shen et al. [[64](#bib.bib64)]
    proposed a similarity-guided graph neural network that represents pairwise relationships
    between probe-gallery image pairs (nodes) by creating a graph and using this relationship
    to enhance the learning of discriminative features. This updated probe-gallery
    image is used to predict the relational features for accurate similarity estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last group of [Table 7](#S3.T7 "Table 7 ‣ 3.4.6 Comparison and discussion
    ‣ 3.4 Sequence feature learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based
    person re-identification methods: A survey and outlook of recent works") shows
    the experimental results of the GCN-based sequence feature learning methods on
    MARS, DukeMTMC-VideoReID iLIDS-VID and PRID-2011 datasets. From the results in
    the above table, the experimental performance of the GCN-based methods is significantly
    better than the other sequence feature learning methods. In particular, CTL [[174](#bib.bib174)]
    achieved Rank-1 accuracy of 91.4% and mAP of 86.7% on MARS. CTL utilized a CNN
    backbone and a key-points estimator to extract semantic local features from the
    human body at multiple granularities as graph nodes. CTL effectively mined comprehensive
    cues complementary to appearance information to enhance the representation capability.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.6 Comparison and discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 7: Comparison of experimental results of video sequence feature learning-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Methods | MARS | DukeMTMC-VideoReID | iLIDS-VID | PRID-2011
    | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | R-1 | mAP | R-1 | R-1 | R-5 | R-1 | R-5 |'
  prefs: []
  type: TYPE_TB
- en: '| Optical flow | RCN[[165](#bib.bib165)]* | - | - | - | - | 58.0 | 84.0 | 70.0
    | 90.0 | CVPR’16 |'
  prefs: []
  type: TYPE_TB
- en: '| TSSCNN[[56](#bib.bib56)] | - | - | - | - | 60.0 | 86.0 | 78.0 | 94.0 | ICCV’17
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASTPN[[166](#bib.bib166)] | - | 44.0 | - | - | 62.0 | 86.0 | 70.0 | 90.0
    | ICCV’17 |'
  prefs: []
  type: TYPE_TB
- en: '| AMOC[[163](#bib.bib163)] | 52.9 | 68.3 | - | - | 68.7 | 94.3 | 83.7 | 98.3
    | TCSVT’18 |'
  prefs: []
  type: TYPE_TB
- en: '| CSACSE[[164](#bib.bib164)]* | 69.4 | 81.2 | - | - | 79.8 | 91.8 | 81.2 |
    92.1 | CVPR’18 |'
  prefs: []
  type: TYPE_TB
- en: '| CSACSE+OF[[164](#bib.bib164)]* | 76.1 | 86.3 | - | - | 85.4 | 96.7 | 93.0
    | 99.3 | CVPR’18 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DCNN | 3DCNN+NLA[[57](#bib.bib57)] | 77.0 | 84.3 | - | - | 81.3 | - | 91.2
    | - | ACCV’18 |'
  prefs: []
  type: TYPE_TB
- en: '| M3D[[167](#bib.bib167)] | 74.1 | 84.4 | - | - | 74.0 | 94.3 | 94.4 | 100.0
    | AAAI’19 |'
  prefs: []
  type: TYPE_TB
- en: '| RNN or LSTM | RFA[[169](#bib.bib169)] | - | - | - | - | 49.3 | 76.8 | 58.2
    | 85.8 | ECCV’16 |'
  prefs: []
  type: TYPE_TB
- en: '| SFT[[168](#bib.bib168)] | 50.7 | 70.6 | - | - | 55.2 | 86.5 | 79.4 | 94.4
    | CVPR’17 |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial-temporal attention | DRSA[[170](#bib.bib170)] | 65.8 | 82.3 | - |
    - | 80.2 | - | 93.2 | - | CVPR’18 |'
  prefs: []
  type: TYPE_TB
- en: '| GLTR[[172](#bib.bib172)] | 78.5 | 87.0 | 93.7 | 96.3 | 86.0 | 98.0 | 95.5
    | 100.0 | ICCV’19 |'
  prefs: []
  type: TYPE_TB
- en: '| VRSTC[[59](#bib.bib59)] | 82.3 | 88.5 | 93.5 | 95.0 | 83.4 | 95.5 | - | -
    | CVPR’19 |'
  prefs: []
  type: TYPE_TB
- en: '| STA[[58](#bib.bib58)] | 80.8 | 86.3 | 94.9 | 96.2 | - | - | - | - | AAAI’19
    |'
  prefs: []
  type: TYPE_TB
- en: '| MG-RAFA[[171](#bib.bib171)] | 85.6 | 88.8 | - | - | 88.6 | 98.0 | 95.9 |
    99.7 | CVPR’20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BiCnet-TKS[[173](#bib.bib173)] | 86.0 | 90.2 | 96.1 | 96.3 | 75.1 | 84.6
    | - | - | CVPR’21 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | AdaptiveGraph[[62](#bib.bib62)] | 81.9 | 89.5 | 95.4 | 97.0 | 84.5
    | 96.7 | 94.6 | 99.1 | TIP’20 |'
  prefs: []
  type: TYPE_TB
- en: '| MGH[[61](#bib.bib61)] | 85.8 | 90.0 | - | - | 85.6 | 97.1 | 94.8 | 99.3 |
    CVPR’20 |'
  prefs: []
  type: TYPE_TB
- en: '| STGCN[[60](#bib.bib60)] | 83.7 | 89.9 | 95.7 | 97.3 | - | - | - | - | CVPR’20
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | CTL[[174](#bib.bib174)] | 86.7 | 91.4 | - | - | - | 89.7 | - | - | CVPR’21
    |'
  prefs: []
  type: TYPE_TB
- en: '[Table 7](#S3.T7 "Table 7 ‣ 3.4.6 Comparison and discussion ‣ 3.4 Sequence
    feature learning ‣ 3 Deep Learning Based Re-ID Method ‣ Deep learning-based person
    re-identification methods: A survey and outlook of recent works") shows the experimental
    results of sequence feature learning-based methods on MARS, DukeMTMC-VideoReID,
    iLIDS-VID and PRID-2011 datasets. Some researchers used GCN to model the Spatio-temporal
    relationships of video sequences and achieved good results. Compared with optical
    flow, 3DCNN, and RNN or LSTM methods, spatial-temporal attention-based methods
    can obtain better experimental performance. AdaptiveGraph [[62](#bib.bib62)],
    MGH [[61](#bib.bib61)] and STGCN [[60](#bib.bib60)] were able to obtain high experimental
    results on the above datasets, with some improvement in accuracy compared to the
    previous types of methods.'
  prefs: []
  type: TYPE_NORMAL
- en: The core idea of the sequence feature learning-based methods is to fuse more
    spatial-temporal information from multiple dimensions to mitigate the effects
    of a range of problems such as occlusion, illumination, and viewpoint changes
    in the person Re-ID tasks. Although Optical flow can provide contextual information
    of video sequence frames, it only represents the local dynamics of adjacent images,
    which may introduce noise due to spatial misalignment. The process of computing
    optical flow is time-consuming. 3DCNN can capture both temporal and spatial feature
    information in video sequences. Although 3DCNN can achieve better performance,
    these methods are computationally time-consuming and hard to optimize. RNN or
    LSTM can extract temporal features of video sequences and has been popular among
    researchers for some time. In the task of person Re-ID, the ability of RNN or
    LSTM have limited features for temporal information extraction and suffer from
    difficulties in model training due to the complex network structure [[176](#bib.bib176)].
    Although the introduction of temporal attention and spatial attention can alleviate
    the problem of switching between different camera views in a row, lighting changes
    and occlusion, the accuracy of person Re-ID is affected because of the temporal
    relationship between body parts in different frames is not fully considered [[60](#bib.bib60)].
    Most of the current person Re-ID studies are image-based, and a large number of
    methods and datasets closer to the real world have emerged. Compared to image-based
    methods, sequence feature learning-based methods still hold great research promise.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion and future directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents a comprehensive survey with in-depth discussion for deep
    learning-based person Re-ID methods in recent years. Firstly, we summarize the
    main contributions of several recently published person Re-ID surveys and discuss
    the common datasets used for person Re-ID benchmarks. Secondly, we comprehensively
    review the current deep learning-based methods, these methods are classified into
    four major categories according to metric learning and representation learning,
    including deep metric learning, local feature learning, generative adversarial
    learning and sequence feature learning. We subdivide the above four categories
    according to their methodologies and motivations, analyzing and discussing the
    advantages and limitations of each subcategory of the method. This classification
    is more suitable for researchers to explore these methods from their practical
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Although existing deep learning-based methods have achieved good results in
    person Re-ID tasks, they still face many challenges. Most of the datasets currently
    applied for person Re-ID training are processed visible images or videos, but
    real-world data often exhibit a combination of multiple modalities. Although semi-supervised
    and unsupervised methods can alleviate the problem of high labeling costs, they
    still do not perform as well as supervised methods. There are domain differences
    in pedestrian images captured by different cameras, and models trained on one
    dataset can experience severe performance degradation when tested on another dataset.
    Because people may change their clothes or different people may wear very similar
    clothes, the appearance features of pedestrians will become unreliable for person
    Re-ID. In addition, how to improve the speed and accuracy of model retrieval is
    critical for real-world model deployment. The increase of privacy scenarios fundamentally
    limits the traditional centralized person Re-ID methods. The detection and re-identification
    modules of most person Re-ID systems are separated from each other, making it
    difficult to expand to real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, there are still many challenges to be explored and researched in
    deep learning-based person Re-ID methods. The following subsections present potential
    solutions to address the above existing challenges, as well as prospects of future
    research directions.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Cross-modal person Re-ID. Most existing Re-ID methods evaluate their performance
    on publicly available datasets, which are obtained based on image or video processing.
    However, the acquisition of real-world data is diverse and the data may appear
    as a combination of different modalities (visible, infrared, depth map and text
    descriptions, etc.). For example, in the absence of sufficient visibility information
    like images or videos, text descriptions can provide unique attributes aiding
    information for person Re-ID. Several research works[[161](#bib.bib161), [177](#bib.bib177)]
    learned discriminative cross-modal visual-textual features for better similarity
    evaluation in description-based person Re-ID. Because it was difficult for visible-light
    cameras to capture valid appearance information in dark environments, some researchers
    [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180), [181](#bib.bib181),
    [182](#bib.bib182)] used thermal infrared images to learn rich visual representations
    for cross-modality matching. Existing works mainly focus on alleviating the modality
    discrepancy by aligning the distributions of features from different modalities.
    Meanwhile, how to combine various modal complementary information is also worth
    studying in the future.
  prefs: []
  type: TYPE_NORMAL
- en: (2) High-performance semi-supervised and unsupervised person Re-ID. Because
    it was expensive to annotate person images across multiple cameras, some researchers
    [[183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185), [186](#bib.bib186),
    [187](#bib.bib187), [188](#bib.bib188)] focused on semi-supervised and unsupervised
    methods for person Re-ID. These methods aimed to learn discriminative features
    from unlabeled or minimally labeled images of people. Compared to supervised learning,
    semi-supervised and unsupervised methods alleviated the need for expensive data
    annotation and showed great potential to facilitate person Re-ID to practical
    applications. Some semi-supervised person Re-ID methods [[189](#bib.bib189), [190](#bib.bib190)]
    made use of image clustering or tracklet clustering in the target domain to adapt
    the model to the new domain. Some unsupervised person Re-ID methods [[183](#bib.bib183),
    [184](#bib.bib184), [185](#bib.bib185)] used soft labels or multi-labels to learn
    discriminative embedding features. Although lacking realistic label learning discriminative
    features, the performance of person Re-ID methods in semi-supervised and unsupervised
    scenarios was still inferior to that of supervised methods, they still maintained
    significant research value and significance in improving the generalization ability
    of the model [[2](#bib.bib2)]. In future research, better clustering or label
    assignment strategies should be considered to improve the performance of person
    Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Domain adaptation person Re-ID. The background, resolution, and illumination
    environment of different cameras in the real world vary greatly, which interfered
    with the learning of distinguishing features of pedestrians and affected the performance
    of person Re-ID. Some researchers [[45](#bib.bib45), [47](#bib.bib47), [49](#bib.bib49),
    [48](#bib.bib48)] transferred images with identity labels from the source to the
    target domain to learn discriminative models, but they largely ignored the unlabeled
    samples and the substantial sample distributions in target domains. Some researchers
    [[155](#bib.bib155), [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193),
    [194](#bib.bib194)] used clustering or graph matching methods to predict the pseudo-labels
    in the target domain for discriminative model learning, but they still faced the
    challenge of accurately predicting hard samples labels. Domain adaption is crucial
    for person Re-ID models learned in unknown domains. Therefore, it remains one
    of the important research directions for the future.
  prefs: []
  type: TYPE_NORMAL
- en: (4) Person Re-ID in the 3D space. In the real world, the spatial location of
    cameras is uncertain and a new camera may be temporarily inserted into an existing
    camera network. Considering people may change their clothes or different people
    may wear very similar clothes, pedestrian appearance features will become unreliable
    for Re-ID [[67](#bib.bib67)]. The 3D structure does not rely on the appearance
    information of 2D images can effectively alleviate this limitation. However, the
    acquisition of 3D point cloud data of pedestrians requires additional auxiliary
    models. Some researchers [[67](#bib.bib67), [66](#bib.bib66)] extracted 3D shape
    embeddings directly from 2D images, obtained more robust structural and appearance
    information by aligning 2D and 3D local features, or planed 3D models back to
    2D images for representation learning in 2D space for data augmentation purposes.
    Although the above studies achieved good experimental results, the 2D data space
    inherently limited the model to understand the 3D geometric information of people.
    Therefore, further exploration of person Re-ID methods in 3D space is still an
    important research direction in the future.
  prefs: []
  type: TYPE_NORMAL
- en: (5) Fast person Re-ID. Most current methods to person Re-ID focus mainly on
    prior knowledge or designing complex network architectures to learn robust identity
    invariant feature representations. These methods use complex network models to
    extract high-dimensional features to improve model performance. However, the above
    methods use Euclidean distance to calculate the similarity of features and obtain
    the rank list by fast sorting, which will increase with the retrieval time as
    the size of the gallery library increases. This retrieval method will be very
    time-consuming, making the model unsuitable for real-world applications. Therefore,
    some researchers [[195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197)]
    considered introducing hashing to improve the retrieval speed. Faster person Re-ID
    retrieval from coarse-to-fine (CtF) [[196](#bib.bib196)] can be achieved by supplementing
    long and short hash codes to get faster and better accuracy. Zhao et al. [[197](#bib.bib197)]
    proposed saliency-guided iterative asymmetric mutual hashing (SIAMH) to achieve
    high-quality hash code generation and fast feature extraction. However, how to
    design a specific retrieval strategy to reduce the information redundancy among
    models and improve retrieval speed and accuracy still needs further research.
  prefs: []
  type: TYPE_NORMAL
- en: (6) Decentralised learning person Re-ID. Most of the existing person Re-ID methods
    use a centralized learning paradigm, which requires collecting all training data
    from different camera views or domains for centralized training. Although these
    supervised or unsupervised methods have made significant progress, centralized
    person Re-ID learning ignores images of people that contain large amounts of personal
    and private information that may not be allowed to be shared into a central data
    set. As privacy scenarios increase, it can fundamentally limit the centralized
    learning person Re-ID methods in the real world. Several recent works [[198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)] attempted to address
    the above problem through decentralised learning. These methods either built a
    globally generalised model server through federated learning, which did not require
    access to local training data and shared of cross-domain data, or selectively
    performed knowledge aggregation to optimize the trade-off between model personalisation
    and generalization in decentralised person Re-ID. In future work, how to ensure
    understanding cross-domain data heterogeneity while learning a global generalised
    model remains challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '(7) End-to-end person Re-ID system. As is shown in [Figure 2](#S1.F2 "Fig.
    2 ‣ 1 Introduction ‣ Deep learning-based person re-identification methods: A survey
    and outlook of recent works") in the introduction of this paper, person detection
    and re-identification in most current person Re-ID systems are two independent
    modules. The person Re-ID task uses the correct pedestrian already detected by
    default, but some practical open-world applications require end-to-end person
    search from the raw images or videos [[202](#bib.bib202)]. The two-stage end-to-end
    person re-identification framework was one of the most common Re-ID systems that
    systematically evaluated the advantages and limitations of combining different
    detectors and Re-ID models [[112](#bib.bib112)]. Munjal et al. [[203](#bib.bib203)]
    proposed a query-guided end-to-end person search network (QEEPS) to join person
    detection and re-identification. In addition, the end-to-end person Re-ID was
    also widely used in multi-target multi-camera tracking (MTMC tracking) [[110](#bib.bib110),
    [204](#bib.bib204), [205](#bib.bib205)]. Person Re-ID algorithms rely not only
    on accurate person detection algorithms but also on detected unlabeled pedestrians,
    which remains a current challenge. Therefore, how to effectively combine person
    detection and re-identification to design an end-to-end person Re-ID system is
    also a direction that researchers need to pay attention to in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: Declaration of Competing Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors declare that they have no known competing financial interests or
    personal relationships that could have appeared to influence the work reported
    in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors wish to thank Jizhuo Li, Xiao Pang, Jiamin Zhu, Fuqiu Chen, Yi Zhou
    and Cheng Zhang. This work was supported by the National Key Research and Development
    Project of China (No. JG2018190), and in part by the National Natural Science
    Foundation of China (No. 61872256).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Wang [2013] X. Wang, Intelligent multi-camera video surveillance: A review,
    Pattern recognition letters. 34 (2013) 3–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2019] H. Luo, W. Jiang, X. Fan, S. Zhang, A survey on deep learning
    based person re-identification, Acta Automatica Sinica. 45 (2019) 2032–2049.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2016] L. Zheng, Y. Yang, A. G. Hauptmann, Person re-identification:
    Past, present and future, 2016, https://arxiv.org/pdf/1610.02984.pdf. [a͡rXiv
    preprint arXiv:1610.02984](http://arxiv.org/abs/1610.02984).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi [2018] J. Redmon, A. Farhadi, Yolov3: An incremental improvement,
    2018, https://arxiv.org/pdf/1804.02767.pdf. [a͡rXiv preprint arXiv:1804.02767](http://arxiv.org/abs/1804.02767).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu,
    A. Berg, Ssd: Single shot multibox detector, Proceedings of the European Conference
    on Computer Vision, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick [2015] R. Girshick, Fast r-cnn, Proceedings of the IEEE International
    Conference on Computer Vision, 2015, pp. 1440–1448.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. [2020] L. Qi, P. Yu, Y. Gao, Research on weak-supervised person re-identification,
    Journal of Software. 9 (2020) 2883–2902.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. [2017] Z. Zhong, L. Zheng, S. Cao, D.and Li, Re-ranking person
    re-identification with k-reciprocal encoding, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2017, pp. 3652–3661.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2013] R. Zhao, W. Ouyang, X. Wang, Person re-identification by
    salience matching, Proceedings of the IEEE International Conference on Computer
    Vision, 2013, pp. 2528–2535.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martinel et al. [2015] N. Martinel, C. Micheloni, G. Foresti, Saliency weighted
    features for person re-identification, Proceedings of the European Conference
    on Computer Vision, 2015, pp. 191–208.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. [2017] L. An, X. Chen, S. Liu, Y. Lei, S. Yang, Integrating appearance
    features and soft biometrics for person re-identification, Multimedia Tools and
    Applications. 76 (2017) 12117–12131.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2017] H. Hu, W. Fang, G. Zeng, Z. Hu, B. Li, A person re-identification
    algorithm based on pyramid color topology feature, Multimedia Tools and Applications.
    76 (2017) 26633–26646.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dikmen et al. [2011] M. Dikmen, E. Akbas, T. Huang, N. Ahuja, Pedestrian recognition
    with a learned metric, Proceedings of the Asian Conference on Computer Vision,
    2011, pp. 501–512.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2014] W. Li, R. Zhao, T. Xiao, X. Wang, Deepreid: Deep filter pairing
    neural network for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2014, pp. 152–159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2015] Y. Chen, W. Zheng, J. Lai, Mirror representation for modeling
    view-specific transform in person re-identification, Proceedings of the International
    Joint Conference on Artificial Intelligence, 2015, pp. 3402–3408.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016] X. Wang, W. Zheng, X. Li, J. Zhang, Cross-scenario transfer
    person reidentification, IEEE Transactions on Circuits and Systems for Video Technology.
    26 (2016) 1447–1460.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. [2014] D. Yi, Z. Lei, S. Liao, S. Z. Li, Deep metric learning for
    person re-identification, Proceedings of the International Conference on Pattern
    Recognition, 2014, pp. 34–39.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2014] W. Li, R. Zhao, T. Xiao, X. Wang, Deepreid: Deep filter pairing
    neural network for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2014, pp. 152–159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2018] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, Y. Yang, Exploit
    the unknown gradually: One-shot video-based person re-identification by stepwise
    learning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 5177–5186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] M. Zheng, S. Karanam, Z. Wu, R. Radke, Re-identification
    with consistent attentive siamese networks, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 5735–5744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varior et al. [2016] R. Varior, M. Haloi, G. Wang, Gated siamese convolutional
    neural network architecture for human re-identification, Proceedings of the European
    Conference on Computer Vision, 2016, pp. 791–808.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermans et al. [2017] A. Hermans, L. Beyer, B. Leibe, In defense of the triplet
    loss for person re-identification, 2017, https://arxiv.org/pdf/1703.07737.pdf.
    [a͡rXiv preprint arXiv:1703.07737](http://arxiv.org/abs/1703.07737).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2017] W. Chen, X. Chen, J. Zhang, K. Huang, Beyond triplet loss:
    A deep quadruplet network for person re-identification, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 403–412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2018] Y. Sun, L. Zheng, Y. Yang, Q. Tian, S. Wang, Beyond part
    models: Person retrieval with refined part pooling (and a strong convolutional
    baseline), Proceedings of the European conference on computer vision, 2018, pp.
    480–496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2019] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, J. Sun, Perceive
    where to focus: Learning visibility-aware part-level features for partial person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 393–402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2019] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Wang,
    Abd-net: Attentive but diverse person re-identification, Proceedings of the IEEE
    International Conference on Computer Vision, 2019, pp. 8350–8360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. [2019] J. Miao, Y. Wu, P. Liu, Y. Ding, Y. Yang, Pose-guided feature
    alignment for occluded person re-identification, Proceedings of the IEEEF International
    Conference on Computer Vision, 2019, pp. 542–551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2017] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Tang,
    Spindle net: Person re-identification with human body region guided feature decomposition
    and fusion, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2017, pp. 1077–1085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, X. Wang,
    Hydraplus-net: Attentive deep features for pedestrian analysis, Proceedings of
    the IEEE international conference on computer vision, 2017, pp. 350–359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2018] W. Li, X. Zhu, S. Gong, Harmonious attention network for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2018, pp. 2285–2294.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2017] L. Zhao, X. Li, Y. Zhuang, J. Wang, Deeply-learned part-aligned
    representations for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 3239–3248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2018] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7132–7141.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] B. Chen, W. Deng, J. Hu, Mixed high-order attention network
    for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 371–381.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. [2019] C. P. Tay, S. Roy, K. H. Yap, Aanet: Attribute attention
    network for person re-identifications, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2019, pp. 7127–7136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ngo et al. [2005] C. W. Ngo, Z. Pan, X. Wei, X. Wu, H. K. Tan, W. Zhao, Motion
    driven approaches to shot boundary detection, low-level feature extraction and
    bbc rushes characterization at trecvid 2005, trecvid workshop participants notebook
    papers, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ngo et al. [2008] C. W. Ngo, Y. G. Jiang, X. Y. Wei, W. Zhao, F. Wang, X. Wu,
    H. K. Tan, Beyond semantic search: What you observe may not be what you think,
    trecvid workshop participants notebook papers, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei and Yang [2012] X. Y. Wei, Z. Q. Yang, Mining in-class social networks for
    large-scale pedagogical analysis, Proceedings of the ACM international conference
    on Multimedia, 2012, pp. 639–648.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] G. Wang, Y. Yuan, X. Chen, J. Li, X. Zhou, Learning discriminative
    features with multiple granularities for person re-identification, Proceedings
    of the ACM International Conference on Multimedia, 2018, pp. 274–282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2017] L. Wei, S. Zhang, H. Yao, W. Gao, Q. Tian, Glad: Global-local-alignment
    descriptor for pedestrian retrieval, Proceedings of the ACM international conference
    on Multimedia, 2017, pp. 420–428.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. [2017] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, Q. Tian, Pose-driven
    deep convolutional model for person re-identification, Proceedings of the IEEE
    International Conference on Computer Vision, 2017, pp. 3980–3989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] X. Chen, C. Fu, Y. Zhao, F. Zheng, J. Song, R. Ji, Y. Yang,
    Salience-guided cascaded suppression network for person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 3297–3307.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Z. Zhang, C. Lan, W. Zeng, X. Jin, Z. Chen, Relation-aware
    global attention for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2020, pp. 3183–3192.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] F. Zheng, C. Deng, X. Sun, X. Jiang, X. Guo, Z. Yu, R. Ji,
    Pyramidal person re-identification via multi-loss dynamic training, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 8506–8514.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. [2019] H. Yao, S. Zhang, R. Hong, Y. Zhang, C. Xu, Q. Tian, Deep
    representation learning with part loss for person re-identification, IEEE Transactions
    on Image Processing. 28 (2019) 2860–2871.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2018] L. Wei, S. Zhang, W. Gao, Q. Tian, Person transfer gan to
    bridge domain gap for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 79–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2020] Y. Huang, Z. J. Zha, X. Fu, R. Hong, L. Li, Real-world person
    re-identification via degradation invariance learning, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2020, pp. 14084–14094.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2017] W. Deng, Z. Liang, G. Kang, Y. Yi, J. Jiao, Image-image domain
    adaptation with preserved self-similarity and domain-dissimilarity for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2017, pp. 994–1003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] J. Liu, Z. Zha, D. Chen, R. Hong, M. Wang, Adaptive transfer
    network for cross-domain person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 7202–7211.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Y. Chen, X. Zhu, S. Gong, Instance-guided context rendering
    for cross-domain person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2020, pp. 232–242.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. [2018a] Z. Zhong, L. Zheng, S. Li, Y. Yang, Generalizing a person
    retrieval model hetero and homogeneously, Proceedings of the European Conference
    on Computer Vision, 2018a, pp. 172–188.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. [2018b] Z. Zhong, Z. Liang, Z. Zheng, S. Li, Y. Yi, Camera style
    adaptation for person re-identification, Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, 2018b, pp. 5157–5166.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] Z. Zheng, Z. Liang, Y. Yi, Unlabeled samples generated by
    gan improve the person re-identification baseline in vitro, Proceedings of the
    IEEE International Conference on Computer Vision, 2017, pp. 618–626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2019] Y. Huang, J. Xu, Q. Wu, Z. Zheng, Z. Zhang, J. Zhang, Multi-pseudo
    regularized label for generated data in person re-identification, IEEE Transactions
    on Image Processing. 28 (2019) 1391–1403.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y. Yang, J. Kautz, Joint
    discriminative and generative learning for person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 2138–2147.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. [2018] Y. Ge, Z. Li, H. Zhao, G. Yin, S. Yi, X. Wang, H. Li, Fd-gan:
    Pose-guided feature distilling gan for robust person re-identification, 2018,
    https://arxiv.org/pdf/1810.02936.pdf. [a͡rXiv preprint arXiv:1810.02936](http://arxiv.org/abs/1810.02936).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2017] D. Chung, K. Tahboub, E. Delp, A two stream siamese convolutional
    neural network for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 1983–1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao et al. [2019] X. Liao, L. He, Z. Yang, C. Zhang, Video-based person re-identification
    via 3d convolutional networks and non-local attention, Proceedings of the Asian
    Conference on Computer Vision, 2019, pp. 620–634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2019] Y. Fu, X. Wang, Y. Wei, T. Huang, Sta: Spatial-temporal attention
    for large-scale video-based person re-identification, Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 33, 2019, pp. 8287–8294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2019] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, X. Chen, Vrstc:
    Occlusion-free video person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 7183–7192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2020] J. Yang, W. Zheng, Q. Yang, Y. Chen, Q. Tian, Spatial-temporal
    graph convolutional network for video-based person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 3289–3299.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2020] Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, Y. Tai, L. Shao,
    Learning multi-granular hypergraphs for video-based person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2020, pp. 2899–2908.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2020] Y. Wu, O. Bourahla, X. Li, F. Wu, Q. Tian, X. Zhou, Adaptive
    graph representation learning for video person re-identification, IEEE Transactions
    on Image Processing. 29 (2020) 8821–8830.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2019] Y. Yan, Q. Zhang, B. Ni, W. Zhang, M. Xu, X. Yang, Learning
    context graph for person search, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2019, pp. 2158–2167.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. [2018] Y. Shen, H. Li, S. Yi, D. Chen, X. Wang, Person re-identification
    with deep similarity-guided graph neural network, Proceedings of the European
    Conference on Computer Vision, 2018, pp. 486–504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2021] Z. Bai, Z. Wang, J. Wang, D. Hu, E. Ding, Unsupervised multi-source
    domain adaptation for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 12914–12923.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2020] Z. Zheng, N. Zheng, Y. Yang, Parameter-efficient person
    re-identification in the 3d space, 2020, https://arxiv.org/pdf/2006.04569.pdf.
    [a͡rXiv preprint arXiv:2006.04569](http://arxiv.org/abs/2006.04569).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] J. Chen, X. Jiang, F. Wang, J. Zhang, F. Zheng, X. Sun, W. S.
    Zheng, Learning 3d shape feature for texture-insensitive person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 8146–8155.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bedagkar-Gala and Shah [2014] A. Bedagkar-Gala, S. K. Shah, A survey of approaches
    and trends in person re-identification, Image and Vision Computing. 32 (2014)
    270–286.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chahar and Nain [2017] H. Chahar, N. Nain, A study on deep convolutional neural
    network based approaches for person re-identification, Pattern Recognition and
    Machine Intelligence. (2017) 543–548.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] K. Wang, H. Wang, M. Liu, X. Xing, T. Han, Survey on person
    re-identification based on deep learning, CAAI Transactions on Intelligence Technology.
    3 (2018) 219–227.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'karanam et al. [2019] S. karanam, M. Gou, Z. Wu, A. Rates-Borras, O. Camps,
    R. Radke, A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets, IEEE Transactions on Pattern Analysis and Machine
    Intelligence. 41 (2019) 523–536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Z. Wang, Z. Wang, Y. Zheng, Y. Wu, W. Zeng, S. Satoh, Beyond
    intra-modality: A survey of heterogeneous person re-identification, 2020, https://arxiv.org/pdf/1905.10048.pdf.
    [a͡rXiv preprint arXiv:1905.10048](http://arxiv.org/abs/1905.10048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2019] D. Wu, S. Zheng, X. Zhang, C. Yuan, F. Cheng, Y. Zhao, Y. Lin,
    Z. Zhao, Y. Jiang, D. Huang, Deep learning-based methods for person re-identification:
    A comprehensive review, Neurocomputing. 337 (2019) 354–371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathur et al. [2020] N. Mathur, S. Mathur, D. Mathur, P. Dadheech, A brief
    survey of deep learning techniques for person re-identification, Proceedings of
    International Conference on Emerging Technologies in Computer Engineering: Machine
    Learning and Internet of Things, 2020, pp. 129–138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. [2020] Q. Leng, M. Ye, Q. Tian, A survey of open-world person re-identification,
    IEEE Transactions on Circuits and Systems for Video Technology. 30 (2020) 1092–1108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam [2020] K. Islam, Person search: New paradigm of person re-identification:
    A survey and outlook of recent works, Image and Vision Computing. 101 (2020) 103970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lavi et al. [2020] B. Lavi, I. Ullah, M. Fatan, A. Rocha, Survey on reliable
    deep learning-based person re-identification models: Are we there yet?, 2020,
    https://arxiv.org/pdf/2005.00355.pdf. [a͡rXiv preprint arXiv:2005.00355](http://arxiv.org/abs/2005.00355).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2021] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, S. C. Hoi, Deep
    learning for person re-identification: A survey and outlook, IEEE Transactions
    on Pattern Analysis and Machine Intelligence. (2021) 1–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021] C. Yang, F. Qi, H. Jia, Survey on unsupervised techniques
    for person re-identification, International Conference on Computing and Data Science,
    2021, pp. 161–164.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yaghoubi et al. [2021] E. Yaghoubi, A. Kumar, H. Proença, Sss-pr: A short survey
    of surveys in person re-identification, Pattern Recognition Letters. 143 (2021)
    50–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2021] Y. Wang, S. Yang, S. Liu, Z. Zhang, Cross-domain person
    re-identification: A review, Artificial Intelligence in China, 2021, pp. 153–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2021] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain generalization:
    A survey, 2021, https://arxiv.org/pdf/2103.02503.pdf. [a͡rXiv preprint arXiv:2103.02503](http://arxiv.org/abs/2103.02503).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behera et al. [2020] N. K. S. Behera, P. K. Sa, S. Bakshi, Person re-identification
    for smart cities: State-of-the-art and the path ahead, Pattern Recognition Letters.
    138 (2020) 282–289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] W. Wu, D. Tao, H. Li, Z. Yang, J. Cheng, Deep features for
    person re-identification on metric learning, Pattern Recognition. 110 (2021) 107424.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behera et al. [2021] N. K. S. Behera, T. K. Behera, M. Nappi, S. Bakshi, P. K.
    Sa, Futuristic person re-identification over internet of biometrics things (iobt):
    Technical potential versus practical reality, Pattern Recognition Letters. 151
    (2021) 163–171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. [2013] Y. Bengio, A. Courville, P. Vincent, Representation learning:
    A review and new perspectives, IEEE Transactions on Pattern Analysis and Machine
    Intelligence 35 (2013) 1798–1828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gray et al. [2007] D. Gray, S. Brennan, H. Tao, Evaluating appearance models
    for recognition, reacquisition, and tracking, IEEE international workshop on performance
    evaluation for tracking and surveillance, 2007, pp. 1–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2012] W. Li, R. Zhao, X. Wang, Human reidentification with transferred
    metric learning, Proceedings of the Asian conference on Computer Vision, 2012,
    pp. 31–44.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Wang [2013] W. Li, X. Wang, Locally aligned feature transforms across
    views, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2013, pp. 3594–3601.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Felzenszwalb et al. [2010] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan,
    Object detection with discriminatively trained part-based models, IEEE Transactions
    on Pattern Analysis and Machine Intelligence. 32 (2010) 1627–1645.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2015] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, Q. Tian,
    Scalable person re-identification: A benchmark, Proceedings of the IEEE International
    Conference on Computer Vision, 2015, pp. 1116–1124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] Z. Zheng, L. Zheng, Y. Yang, Unlabeled samples generated
    by gan improve the person re-identification baseline in vitro, Proceedings of
    the IEEE International Conference on Computer Vision, 2017, pp. 3774–3782.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ristani et al. [2016] E. Ristani, F. Solera, R. Zou, R. Cucchiara, C. Tomasi,
    Performance measures and a data set for multi target multi camera tracking, Proceedings
    of the European conference on computer vision, 2016, pp. 17–35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards
    real-time object detection with region proposal networks, Advances in Neural Information
    Processing Systems. 39 (2015) 1137–1149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dollar et al. [2014] P. Dollar, R. Appel, S. Belongie, P. Perona, Fast feature
    pyramids for object detection, IEEE Transactions on Pattern Analysis and Machine
    Intelligence. 36 (2014) 1532–1545.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirzer et al. [2011] M. Hirzer, C. Beleznai, P. Roth, H. Bischof, Person re-identification
    by descriptive and discriminative classification, Image Analysis. (2011) 91–102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2014] T. Wang, S. Gong, X. Zhu, S. Wang, Person re-identification
    by video ranking, In Proceedings of the European Conference on Computer Vision,
    2014, pp. 688–703.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2016] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, Q. Tian,
    Mars: A video benchmark for large-scale person re-identification, Proceedings
    of the European Conference on Computer Vision, 2016, pp. 868–884.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2018] G. Song, B. Leng, Y. Liu, C. Hetang, S. Cai, Region-based
    quality estimation network for large-scale person re-identification, Proceedings
    of the AAAI Conference on Artificial Intelligence, 2018, pp. 7347–7354.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dehghan et al. [2015] A. Dehghan, S. Modiri Assari, M. Shah, Gmmcp tracker:
    Globally optimal generalized maximum multi clique problem for multiple object
    tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2015, pp. 4091–4099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh Song et al. [2016] H. Oh Song, Y. Xiang, S. Jegelka, S. Savarese, Deep metric
    learning via lifted structured feature embedding, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2016, pp. 4004–4012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2018] Y. Duan, J. Lu, J. Feng, J. Zhou, Deep localized metric learning,
    IEEE Transactions on Circuits and Systems for Video Technology. 28 (2018) 2644–2656.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2018] H. Huang, D. Li, Z. Zhang, X. Chen, K. Huang, Adversarially
    occluded samples for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 5098–5107.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. [2019] Z. Zhong, L. Zheng, Z. Luo, S. Li, Y. Yang, Invariance
    matters: Exemplar memory for domain adaptive person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 598–607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2019] C. Luo, Y. Chen, N. Wang, Z. Zhang, Spectral feature transformation
    for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 4976–4985.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] Z. Zheng, L. Zheng, Y. Yang, A discriminatively learned
    cnn embedding for person reidentification, ACM Transactions on Multimedia Computing,
    Communications, and Applications. 14 (2017) 1–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] H. Chen, Y. Wang, Y. Shi, K. Yan, M. Geng, Y. Tian, T. Xiang,
    Deep transfer learning for person re-identification, Proceedings of the IEEE International
    Conference on Multimedia Big Data, 2018, pp. 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varior et al. [2016] R. R. Varior, B. Shuai, J. Lu, D. Xu, G. Wang, A siamese
    long short-term memory architecture for human re-identification, Proceedings of
    the European conference on computer vision, 2016, pp. 135–153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] Y. Wang, Z. Chen, F. Wu, G. Wang, Person re-identification
    with cascaded pairwise convolutions, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 1470–1478.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ristani and Tomasi [2018] E. Ristani, C. Tomasi, Features for multi-target multi-camera
    tracking and re-identification, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 6036–6046.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2018] C. Song, Y. Huang, W. Ouyang, L. Wang, Mask-guided contrastive
    attention model for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 1179–1188.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] L. Zheng, H. Zhang, S. Sun, M. Chandraker, Y. Yang, Q. Tian,
    Person re-identification in the wild, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2017, pp. 1367–1376.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2021] H. Li, G. Wu, W. S. Zheng, Combined depth space based architecture
    search for person re-identification, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 6729–6738.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2019] J. Guo, Y. Yuan, L. Huang, C. Zhang, J. Yao, K. Han, Beyond
    human parts: Dual part-aligned representations for person re-identification, Proceedings
    of the IEEE International Conference on Computer Vision, 2019, pp. 3642–3651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] G. Chen, C. Lin, L. Ren, J. Lu, J. Zhou, Self-critical attention
    learning for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 9637–9646.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] Z. Liu, J. Wang, S. Gong, H. Lu, D. Tao, Deep reinforcement
    active learning for human-in-the-loop person re-identification, Proceedings of
    the IEEE International Conference on Computer Vision, 2019, pp. 6122–6131.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2019] J. Song, Y. Yang, Y. Song, T. Xiang, T. Hospedales, Generalizable
    person re-identification by domain-invariant mapping network, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 719–728.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2019] S. Zhou, F. Wang, Z. Huang, J. Wang, Discriminative feature
    learning with consistent attention regularization for person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.
    8040–8049.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2016] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, S. Z.
    Li, Embedding deep metric for person re-identification: A study against large
    variations, Proceedings of the European Conference on Computer Vision, 2016, pp.
    732–748.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishchuk et al. [2017] A. Mishchuk, D. Mishkin, F. Radenovic, J. Matas, Working
    hard to know your neighbor’s margins: Local descriptor learning loss, Proceedings
    of the International Conference on Neural Information Processing Systems, 2017,
    pp. 4829–4840.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2021] C. Yan, G. Pang, X. Bai, C. Liu, N. Xin, L. Gu, J. Zhou,
    Beyond triplet loss: Person re-identification with fine-grained difference-aware
    pairwise loss, IEEE Transactions on Multimedia. (2021) 1–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2016] L. Wu, C. Shen, A. Hengel, Personnet: Person re-identification
    with deep convolutional neural networks, 2016, https://arxiv.org/pdf/1601.07255.pdf.
    [a͡rXiv preprint arXiv:1601.07255](http://arxiv.org/abs/1601.07255).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016] F. Wang, W. Zuo, L. Lin, D. Zhang, L. Zhang, Joint learning
    of single-image and cross-image representations for person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2016, pp. 1288–1296.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2017] X. Qian, Y. Fu, Y. Jiang, T. Xiang, X. Xue, Multi-scale deep
    learning architectures for person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 5399–5408.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2018] J. Liu, Z. Zha, H. Xie, Z. Xiong, Y. Zhang, Ca3net: Contextual-attentional
    attribute-appearance network for person re-identification, Proceedings of the
    ACM international conference on Multimedia, 2018, pp. 737–745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2019] Y. Fu, Y. Wei, Y. Zhou, H. Shi, G. Huang, X. Wang, Z. Yao,
    T. Huang, Horizontal pyramid matching for person re-identification, Proceedings
    of the AAAI conference on artificial intelligence, volume 33, 2019, pp. 8295–8302.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2016] J. Liu, Z. Zha, Q. Tian, D. Liu, T. Yao, Q. Ling, T. Mei,
    Multi-scale triplet cnn for person re-identification, Proceedings of the ACM international
    conference on Multimedia, 2016, pp. 192–196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2017] Y. Chen, X. Zhu, S. Gong, Person re-identification by deep
    learning multi-scale representations, Proceedings of the IEEE International Conference
    on Computer Vision Workshops, 2017, pp. 2590–2600.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2019] K. Zhou, Y. Yang, A. Cavallaro, T. Xiang, Omni-scale feature
    learning for person re-identification, Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 3702–3712.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2017] D. Li, X. Chen, Z. Zhang, K. Huang, Learning deep context-aware
    features over body and latent parts for person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 384–393.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2019] W. Yang, H. Huang, Z. Zhang, X. Chen, K. Huang, S. Zhang,
    Towards rich feature discovery with class activation maps augmentation for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 1389–1398.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2021] Z. Zhang, H. Zhang, S. Liu, Person re-identification using
    heterogeneous local graph attention networks, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 12136–12145.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2021] X. Ning, K. Gong, W. Li, L. Zhang, Jwsaa: Joint weak saliency
    and attention aware for person re-identification, Neurocomputing. 453 (2021) 801–811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalayeh et al. [2018] M. M. Kalayeh, E. Basaran, M. Gökmen, M. E. Kamasak, M. Shah,
    Human semantic parsing for person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 1062–1071.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2021] K. Zhou, Y. Yang, A. Cavallaro, T. Xiang, Learning generalisable
    omni-scale representations for person re-identification, IEEE Transactions on
    Pattern Analysis and Machine Intelligence. (2021) 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ning et al. [2021] X. Ning, K. Gong, W. Li, L. Zhang, X. Bai, S. Tian, Feature
    refinement and filter network for person re-identification, IEEE Transactions
    on Circuits and Systems for Video Technology. 31 (2021) 3391–3402.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ming et al. [2021] Y. Ming, Z. aand Yang, X. Wei, J. Yan, X. Wang, F. Wang,
    M. Zhu, Global-local dynamic feature alignment network for person re-identification,
    2021, https://arxiv.org/pdf/2109.05759.pdf. [a͡rXiv preprint arXiv:2109.05759](http://arxiv.org/abs/2109.05759).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Y. Li, J. He, T. Zhang, X. Liu, Y. Zhang, F. Wu, Diverse part
    discovery: Occluded person re-identification with part-aware transformer, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 2898–2907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, Proceedings of
    the International Conference on Neural Information Processing Systems, volume 2,
    2014, p. 2672–2680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2018] J. Liu, B. Ni, Y. Yan, P. Zhou, S. Cheng, J. Hu, Pose transferrable
    person re-identification, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2018, pp. 4099–4108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] Y. C. Chen, Y. J. Li, Y. C. F. Du, X.and Wang, Learning resolution-invariant
    deep representations for person re-identification, Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 33, 2019, pp. 8215–8222.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] J. Zhu, T. Park, P. Isola, A. Efros, Unpaired image-to-image
    translation using cycle-consistent adversarial networks, Proceedings of the IEEE
    International Conference on Computer Vision, 2017, pp. 2223–2232.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2018] Y. Choi, M. Choi, M. Kim, J. W. Ha, S. Kim, J. Choo, Stargan:
    Unified generative adversarial networks for multi-domain image-to-image translation,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 8789–8797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. [2018] W. Liang, G. Wang, J. Lai, J. Zhu, M2m-gan: Many-to-many
    generative adversarial transfer learning for person re-identification, 2018, https://arxiv.org/pdf/1811.03768.pdf.
    [a͡rXiv preprint arXiv:1811.03768](http://arxiv.org/abs/1811.03768).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2020] C. Liu, X. Chang, Y. D. Shen, Unity style transfer for person
    re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2020, pp. 6887–6896.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019] Z. Wang, Z. Wang, Y. Zheng, Y. Y. Chuang, S. I. Satoh, Learning
    to reduce dual-level discrepancy for infra-red-visible person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2019, pp. 618–626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bak et al. [2018] S. Bak, P. Carr, J. F. Lalonde, Domain adaptation through
    synthesis for unsupervised person re-identification, Proceedings of the European
    Conference on Computer Vision, 2018, pp. 189–205.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2019] C. Dai, H. Wang, T. Ni, S. Chen, Person re-identification
    based on deep convolutional generative adversarial network and expanded neighbor
    reranking, Journal of Computer Research and Development. 56 (2019) 1632–1641.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2020] W. X. Yang, Y. Yan, S. Chen, X. K. Zhang, H. Z. Wang, Multi-scale
    generative adversarial network for person re-identification under occlusion, Journal
    of Computer Research and Development. 31 (2020) 1943–1958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2018] X. Qian, Y. Fu, T. Xiang, W. Wang, J. Qiu, Y. Wu, X. Xue,
    Pose-normalized image generation for person re-identification, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 650–667.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2019] Z. Zhu, T. Huang, B. Shi, M. Yu, B. Wang, X. Bai, Progressive
    pose attention transfer for person image generation, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 2347–2356.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2018] L. Ma, Q. Sun, S. Georgoulis, L. V. Gool, B. Schiele, M. Fritz,
    Disentangled person image generation, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 99–108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] H. Chen, Y. Wang, B. Lagadec, A. Dantcheva, F. Bremond, Joint
    generative and contrastive learning for unsupervised person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 2004–2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Y. J. Li, Y. C. Chen, Y. Y. Lin, X. Du, Y. C. F. Wang, Recover
    and identify: A generative dual model for cross-resolution person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.
    8090–8099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. [2018] H. Fan, L. Zheng, C. Yan, Y. Yang, Unsupervised person re-identification:
    Clustering and fine-tuning, ACM Transactions on Multimedia Computing, Communications,
    and Applications. 14 (2018) 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torralba and Efros [2011] A. Torralba, A. A. Efros, Unbiased look at dataset
    bias, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2011, pp. 1521–1528.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] J. Liu, Z. Zha, R. Hong, M. Wang, Y. Zhang, Deep adversarial
    graph attention convolution network for text-based person search, Proceedings
    of the ACM International Conference on Multimedia, 2019, pp. 665–673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. [2019] S. Mao, S. Zhang, M. Yang, Resolution-invariant person re-identification,
    2019, https://arxiv.org/pdf/1906.09748.pdf. [a͡rXiv preprint arXiv:1906.09748](http://arxiv.org/abs/1906.09748).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao et al. [2015] S. Liao, H. Yang, X. Zhu, S. Z. Li, Person re-identification
    by local maximal occurrence representation and metric learning, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2197–2206.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. [2016] P. Peng, X. Tao, Y. Wang, M. Pontil, Y. Tian, Unsupervised
    cross-dataset transfer learning for person re-identification, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1306–1315.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zha et al. [2020] Z. Zha, J. Liu, D. Chen, F. Wu, Adversarial attribute-text
    embedding for person search with natural language query, IEEE Transactions on
    Multimedia. 22 (2020) 1836–1846.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. [2020] Y. Zou, X. Yang, Z. Yu, B. Kumar, J. Kautz, Joint disentangling
    and adaptation for cross-domain person re-identification, Proceedings of the European
    Conference on Computer Vision, 2020, pp. 87–104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2018] H. Liu, Z. Jie, K. Jayashree, M. Qi, J. Jiang, S. Yan, J. Feng,
    Video-based person re-identification with accumulative motion context, IEEE Transactions
    on Circuits and Systems for Video Technology. 28 (2018) 2788–2802.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] D. Chen, H. Li, T. Xiao, S. Yi, X. Wang, Video person re-identification
    with competitive snippet-similarity aggregation and co-attentive snippet embedding,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 1169–1178.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McLaughlin et al. [2016] N. McLaughlin, J. del Rincon, P. Miller, Recurrent
    convolutional network for video-based person re-identification, Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1325–1334.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2017] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, P. Zhou, Jointly
    attentive spatial-temporal pooling networks for video-based person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2017, pp.
    4733–4742.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2019] J. Li, S. Zhang, T. Huang, Multi-scale 3d convolution network
    for video based person re-identification, Proceedings of the AAAI Conference on
    Artificial Intelligence, volume 33, 2019, pp. 8618–8625.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2017] Z. Zhou, Y. Huang, W. Wang, L. Wang, T. Tan, See the forest
    for the trees: Joint spatial and temporal recurrent neural networks for video-based
    person re-identification, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2017, pp. 4747–4756.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2016] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, X. Yang, Person re-identification
    via recurrent feature aggregation, Proceedings of the European Conference on Computer
    Vision, 2016, pp. 701–716.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2018] S. Li, S. Bak, P. Carr, X. Wang, Diversity regularized spatiotemporal
    attention for video-based person re-identification, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 369–378.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Z. Zhang, C. Lan, W. Zeng, Z. Chen, Multi-granularity reference-aided
    attentive feature aggregation for video-based person re-identification, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 10407–10416.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2019] J. Li, J. Wang, Q. Tian, W. Gao, S. Zhang, Global-local temporal
    representations for video person re-identification, Proceedings of the IEEE International
    Conference on Computer Vision, 2019, pp. 3958–3967.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2021] R. Hou, H. Chang, B. Ma, R. Huang, S. Shan, Bicnet-tks: Learning
    efficient spatial-temporal representation for video person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 2014–2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021] J. Liu, Z. J. Zha, W. Wu, K. Zheng, Q. Sun, Spatial-temporal
    correlation and topology learning for person re-identification in videos, Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 4370–4379.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling [2017] T. Kipf, M. Welling, Semi-supervised classification
    with graph convolutional networks, 2017, https://arxiv.org/pdf/1609.02907.pdf.
    [a͡rXiv preprint arXiv:1609.02907](http://arxiv.org/abs/1609.02907).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Nevatia [2018] J. Gao, R. Nevatia, Revisiting temporal modeling for
    video-based person reid, 2018, https://arxiv.org/pdf/1805.02104.pdf. [a͡rXiv preprint
    arXiv:1805.02104](http://arxiv.org/abs/1805.02104).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu et al. [2020] K. Niu, Y. Huang, W. Ouyang, L. Wang, Improving description-based
    person re-identification by multi-granularity image-text alignments, IEEE Transactions
    on Image Processing. 29 (2020) 5542–5556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2020] S. Choi, S. Lee, T. Kim, Y.and Kim, C. Kim, Hi-cmd: Hierarchical
    cross-modality disentanglement for visible-infrared person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2020, pp. 10257–10266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2020] M. Ye, J. Shen, D. J. Crandall, L. Shao, J. Luo, Dynamic dual-attentive
    aggregation learning for visible-infrared person re-identification, Proceedings
    of the European Conference on Computer Vision, 2020, pp. 229–247.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] G. A. Wang, T. Zhang, Y. Yang, J. Cheng, J. Chang, X. Liang,
    Z. G. Hou, Cross-modality paired-images generation for rgb-infrared person re-identification,
    Proceedings of the AAAI Conference on Artificial Intelligence, 2020, pp. 12144–12151.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2020] D. Li, X. Wei, X. Hong, Y. Gong, Infrared-visible cross-modal
    person re-identification with an x modality, Proceedings of the AAAI Conference
    on Artificial Intelligence, 2020, pp. 4610–4617.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] Q. Wu, P. Dai, J. Chen, C. W. Lin, Y. Wu, F. Huang, R. Ji,
    Discover cross-modality nuances for visible-infrared person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2021, pp. 4330–4339.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. [2019] H. Yu, W. Zheng, A. Wu, X. Guo, S. Gong, J. Lai, Unsupervised
    person re-identification by soft multilabel learning, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2019, pp. 2148–2157.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2020] Y. Lin, L. Xie, Y. Wu, C. Yan, Q. Tian, Unsupervised person
    re-identification via softened similarity learning, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2020, pp. 3390–3399.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Zhang [2020] D. Wang, S. Zhang, Unsupervised person re-identification
    via multi-label classification, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2020, pp. 10981–10990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2019] Q. Yang, H. Yu, A. Wu, W. Zheng, Patch-based discriminative
    feature learning for unsupervised person re-identification, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3633–3642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xin et al. [2019] X. Xin, J. Wang, R. Xie, S. Zhou, W. Huang, N. Zheng, Semi-supervised
    person re-identification using multi-view clustering, Pattern Recognition. 88
    (2019) 285–297.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. [2020] L. Qi, L. Wang, J. Huo, Y. Shi, Y. Gao, Progressive cross-camera
    soft-label learning for semi-supervised person re-identification, IEEE Transactions
    on Circuits and Systems for Video Technology. 30 (2020) 2815–2829.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2019] H. Tang, Y. Zhao, H. Lu, Unsupervised person re-identification
    with iterative self-supervised domain adaptation, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, 2019, pp. 1536–1543.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2020] G. Wu, X. Zhu, S. Gong, Tracklet self-supervised learning for
    unsupervised person re-identification, Proceedings of the AAAI Conference on Artificial
    Intelligence, 2020, pp. 12362–12369.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2019] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, T. S. Huang, Self-similarity
    grouping: A simple unsupervised cross domain adaptation approach for person re-identification,
    Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.
    6112–6121.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. [2020] Y. Zhai, S. Lu, Q. Ye, X. Shan, J. Chen, R. Ji, Y. Tian,
    Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification,
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2020, pp. 9021–9030.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019] X. Zhang, J. Cao, C. Shen, M. You, Self-training with progressive
    augmentation for unsupervised cross-domain person re-identification, Proceedings
    of the IEEE International Conference on Computer Vision, 2019, pp. 8222–8231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2017] M. Ye, A. J. Ma, L. Zheng, J. Li, P. C. Yuen, Dynamic label
    graph matching for unsupervised video re-identification, Proceedings of the IEEE
    International Conference on Computer Vision, 2017, pp. 5142–5150.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2017] J. Chen, Y. Wang, J. Qin, L. Liu, L. Shao, Fast person re-identification
    via cross-camera semantic binary transformation, Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, 2017, pp. 3873–3882.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] G. A. Wang, S. Gong, J. Cheng, Z. Hou, Faster person re-identification,
    Proceedings of the European Conference on Computer Vision, 2020, pp. 275–292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2021] C. Zhao, Y. Tu, Z. Lai, F. Shen, H. T. Shen, D. Miao, Salience-guided
    iterative asymmetric mutual hashing for fast person re-identification, IEEE Transactions
    on Image Processing. 30 (2021) 7776–7789.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Gong [2020] G. Wu, S. Gong, Decentralised learning from independent multi-domain
    labels for person re-identification, 2020, https://arxiv.org/pdf/2006.04150.pdf.
    [a͡rXiv preprint arXiv:2006.04150](http://arxiv.org/abs/2006.04150).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2021] S. Sun, G. Wu, S. Gong, Decentralised person re-identification
    with selective knowledge aggregation, 2021, https://arxiv.org/pdf/2110.11384.pdf.
    [a͡rXiv preprint arXiv:2110.11384](http://arxiv.org/abs/2110.11384).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. [2020] W. Zhuang, Y. Wen, X. Zhang, X. Gan, D. Yin, D. Zhou, S. Yi,
    Performance optimization of federated person re-identification via benchmark analysis,
    Proceedings of the ACM International Conference on Multimedia, 2020, pp. 955–963.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bedogni et al. [2021] L. Bedogni, S. K. Rumi, F. D. Salim, Modelling memory
    for individual re-identification in decentralised mobile contact tracing applications,
    Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,
    2021, pp. 1–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. [2017] T. Xiao, S. Li, B. Wang, L. Lin, X. Wang, Joint detection
    and identification feature learning for person search, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 3415–3424.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munjal et al. [2019] B. Munjal, S. Amin, F. Tombari, F. Galasso, Query-guided
    end-to-end person search, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2019, pp. 811–820.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2017] S. Tang, M. Andriluka, B. Andres, B. Schiele, Multiple people
    tracking by lifted multicut and person re-identification, Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 3539–3548.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. [2019] Y. Hou, L. Zheng, Z. Wang, S. Wang, Locality aware appearance
    metric for multi-target multi-camera tracking, 2019, https://arxiv.org/pdf/1911.12037.pdf.
    [a͡rXiv preprint arXiv:1911.12037](http://arxiv.org/abs/1911.12037).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
