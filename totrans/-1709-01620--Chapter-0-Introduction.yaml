- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:08:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1709.01620] Chapter 0 Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1709.01620](https://ar5iv.labs.arxiv.org/html/1709.01620)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep Learning Techniques for Music Generation – A Survey Jean-Pierre Briot^(∗,)¹¹1Also
    Visiting Professor at UNIRIO (Universidade Federal do Estado do Rio de Janeiro)
    and Permanent Visiting Professor at PUC-Rio (Pontifícia Universidade Católica
    do Rio de Janeiro), Rio de Janeiro, Brazil., Gaëtan Hadjeres^† and François-David
    Pachet^‡ ^∗ Sorbonne Université, CNRS, LIP6, F-75005 Paris, France
  prefs: []
  type: TYPE_NORMAL
- en: ^† Sony Computer Science Laboratories, CSL-Paris, F-75005 Paris, France
  prefs: []
  type: TYPE_NORMAL
- en: ^‡ Spotify Creator Technology Research Lab, CTRL, F-75008 Paris, France
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper is a survey and an analysis of different ways of using deep learning
    (deep artificial neural networks) to generate musical content. We propose a methodology
    based on five dimensions for our analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objective
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What musical content is to be generated?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: melody, polyphony, accompaniment or counterpoint.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For what destination and for what use?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: To be performed by a human(s) (in the case of a musical score), or by a machine
    (in the case of an audio file).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the concepts to be manipulated?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: waveform, spectrogram, note, chord, meter and beat.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What format is to be used?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: MIDI, piano roll or text.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How will the representation be encoded?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: scalar, one-hot or many-hot.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What type(s) of deep neural network is (are) to be used?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: feedforward network, recurrent network, autoencoder or generative
    adversarial networks.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenge
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the limitations and open challenges?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: variability, interactivity and creativity.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we model and control the process of generation?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: single-step feedforward, iterative feedforward, sampling or input
    manipulation.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: For each dimension, we conduct a comparative analysis of various models and
    techniques and we propose some tentative multidimensional typology. This typology
    is bottom-up, based on the analysis of many existing deep-learning based systems
    for music generation selected from the relevant literature. These systems are
    described and are used to exemplify the various choices of objective, representation,
    architecture, challenge and strategy. The last section includes some discussion
    and some prospects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supplementary material is provided at the following companion web site:'
  prefs: []
  type: TYPE_NORMAL
- en: www.briot.info/dlt4mg/
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper is a simplified (weak DRM²²2In addition to including high quality
    color figures, the book includes: a table of contents, a list of tables, a list
    of figures, a table of acronyms, a glossary and an index.) version of the following
    book briot:dlt4mg:book:2019: Jean-Pierre Briot, Gaëtan Hadjeres and François-David
    Pachet, Deep Learning Techniques for Music Generation, Computational Synthesis
    and Creative Systems, Springer, 2019. Hardcover ISBN: 978-3-319-70162-2. eBook
    ISBN: 978-3-319-70163-9. Series ISSN: 2509-6575.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 0 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Chapter 1 Introduction provides motivation for this book. It includes a short
    summary of the history of computer music and music generation (including previous
    uses of artificial neural networks as well as other models such as grammars, rules
    and Markov chains) up to the recent rise of deep learning. It also describes the
    organization and the public of the book as well as related work.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has recently become a fast growing domain and is now used routinely
    for classification and prediction tasks, such as image recognition, voice recognition
    or translation. It became popular in 2012, when a deep learning architecture significantly
    outperformed standard techniques relying on handcrafted features in an image classification
    competition, see more details in Section LABEL:section:architectures:history.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may explain this success and reemergence of artificial neural network techniques
    by the combination of:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: availability of massive data;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: availability of efficient and affordable computing power¹¹1Notably, thanks to
    graphics processing units (GPU), initially designed for video games, which have
    now one of their biggest markets in data science and deep learning applications.;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'technical advances, such as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: pre-training, which resolved initially inefficient training of neural networks
    with many layers hinton:fast:algorithm:2006²²2Although nowadays it has being replaced
    by other techniques, such as batch normalization ioffe:batch:normalization:arxiv:2015
    and deep residual learning he:resnet:deep:residual:arxiv:2015.;
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: convolutions, which provide motif translation invariance le:cun:convolutional:handbook:1998;
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM (long short-term memory), which resolved initially inefficient training
    of recurrent neural networks hochreiter:lstm:1997.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There is no consensual definition for deep learning. It is a repertoire of
    machine learning (ML) techniques, based on artificial neural networks. The key
    aspect and common ground is the term deep. This means that there are multiple
    layers processing multiple hierarchical levels of abstractions, which are automatically
    extracted from data³³3That said, although deep learning will automatically extract
    significant features from the data, manual choices of input representation, e.g.,
    spectrum vs raw wave signal for audio, may be very significant for the accuracy
    of the learning and for the quality of the generated content, see Section LABEL:section:representation:feature:extraction..
    Thus a deep architecture can manage and decompose complex representations in terms
    of simpler representations. The technical foundation is mostly artificial neural
    networks, as we will see in Chapter LABEL:section:chapter:architecture, with many
    extensions, such as: convolutional networks, recurrent networks, autoencoders,
    and restricted Boltzmann machines. For more information about the history and
    various facets of deep learning, see, e.g., a recent comprehensive book on the
    domain goodfellow:deep:learning:book:2016.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Driving applications of deep learning are traditional machine learning tasks⁴⁴4Tasks
    in machine learning are types of problems and may also be described in terms of
    how the machine learning system should process an example (goodfellow:deep:learning:book:2016,
    Section 5.1.1). Examples are: classification, regression and anomaly detection.:
    classification (for instance, identification of images) and prediction⁵⁵5As a
    testimony of the initial DNA of neural networks: linear regression and logistic
    regression, see Section LABEL:section:architecture:linear:regression. (for instance,
    of the weather) and also more recent ones such as translation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But a growing area of application of deep learning techniques is the generation
    of content. Content can be of various kinds: images, text and music, the latter
    being the focus of our analysis. The motivation is in using now widely available
    various corpora to automatically learn musical styles and to generate new musical
    content based on this.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1 Computer-Based Music Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first music generated by computer appeared in 1957. It was a 17 seconds
    long melody named “The Silver Scale” by its author Newman Guttman and was generated
    by a software for sound synthesis named Music I, developed by Mathews at Bell
    Laboratories. The same year, “The Illiac Suite” was the first score composed by
    a computer lejaren:illiac:book:1959. It was named after the ILLIAC I computer
    at the University of Illinois at Urbana-Champaign (UIUC) in the United States.
    The human “meta-composers” were Lejaren A. Hiller and Leonard M. Isaacson, both
    musicians and scientists. It was an early example of algorithmic composition,
    making use of stochastic models (Markov chains) for generation as well as rules
    to filter generated material according to desired properties.
  prefs: []
  type: TYPE_NORMAL
- en: In the domain of sound synthesis, a landmark was the release in 1983 by Yamaha
    of the DX 7 synthesizer, building on groundwork by Chowning on a model of synthesis
    based on frequency modulation (FM). The same year, the MIDI⁶⁶6Musical instrument
    digital interface, to be introduced in Section LABEL:section:representation:midi.
    interface was launched, as a way to interoperate various software and instruments
    (including the Yamaha DX 7 synthesizer). Another landmark was the development
    by Puckette at IRCAM of the Max/MSP real-time interactive processing environment,
    used for real-time synthesis and for interactive performances.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding algorithmic composition, in the early 1960s Iannis Xenakis explored
    the idea of stochastic composition⁷⁷7One of the first documented case of stochastic
    music, long before computers, is the Musikalisches Wurfelspiel (Dice Music), attributed
    to Wolfgang Amadeus Mozart. It was designed for using dice to generate music by
    concatenating randomly selected predefined music segments composed in a given
    style (Austrian waltz in a given key). xenakis:formalized:music:book:1963, in
    his composition named “Atrées” in 1962. The idea involved using computer fast
    computations to calculate various possibilities from a set of probabilities designed
    by the composer in order to generate samples of musical pieces to be selected.
    In another approach following the initial direction of “The Illiac Suite”, grammars
    and rules were used to specify the style of a given corpus or more generally tonal
    music theory. An example is the generation in the 1980s by Ebcioğlu’s composition
    software named CHORAL of a four-part chorale in the style of Johann Sebastian
    Bach, according to over 350 handcrafted rules ebcioglu:expert:system:bach:cmj:1988.
    In the late 1980s David Cope’s system named Experiments in Musical Intelligence
    (EMI) extended that approach with the capacity to learn from a corpus of scores
    of a composer to create its own grammar and database of rules cope:algorithmic:composer:book:2000.
  prefs: []
  type: TYPE_NORMAL
- en: Since then, computer music has continued developing for the general public,
    if we consider, for instance, the GarageBand music composition and production
    application for Apple platforms (computers, tablets and cellphones), as an offspring
    of the initial Cubase sequencer software, released by Steinberg in 1989.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the history and principles of computer music in general,
    see, for example, the book by Roads roads:computer:music:tutorial:book:1996. For
    more details about the history and principles of algorithmic composition, see,
    for example, maurer:brief:history:algorithmic:composition:1999 and the books by
    Cope cope:algorithmic:composer:book:2000 or Dean and McLean dean:oxford:handbook:2018.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Autonomy versus Assistance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When talking about computer-based music generation, there is actually some ambiguity
    about whether the objective is
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to design and construct autonomous music-making systems – two recent examples
    being the deep-learning based Amper™ and Jukedeck systems/companies aimed at the
    creation of original music for commercials and documentary; or
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to design and construct computer-based environments to assist human musicians
    (composers, arrangers, producers, etc.) – two examples being the FlowComposer
    environment developed at Sony CSL-Paris papadopoulos:flow:composer:cp:2016 (introduced
    in Section LABEL:section:systems:flow:composer) and the OpenMusic environment
    developed at IRCAM assayag:openmusic:cmj:1999.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The quest for autonomous music-making systems may be an interesting perspective
    for exploring the process of composition⁸⁸8As Richard Feynman coined it: “What
    I cannot create, I do not understand.” and it also serves as an evaluation method.
    An example of a musical Turing test⁹⁹9Initially codified in 1950 by Alan Turing
    and named by him the “imitation game” turing:test:mind:1950, the “Turing test”
    is a test of the ability for a machine to exhibit intelligent behavior equivalent
    to (and more precisely, indistinguishable from) the behavior of a human. In his
    imaginary experimental setting, Turing proposed the test to be a natural language
    conversation between a human (the evaluator) and a hidden actor (another human
    or a machine). If the evaluator cannot reliably tell the machine from the human,
    the machine is said to have passed the test. will be introduced in Section LABEL:section:experiment:deep:bach.
    It consists in presenting to various members of the public (from beginners to
    experts) chorales composed by J. S. Bach or generated by a deep learning system
    and played by human musicians^(10)^(10)10This is to avoid the bias (synthetic
    flavor) of a computer rendered generated music.. As we will see in the following,
    deep learning techniques turn out to be very efficient at succeeding in such tests,
    due to their capacity to learn musical style from a given corpus and to generate
    new music that fits into this style. That said, we consider that such a test is
    more a means than an end.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A broader perspective is in assisting human musicians during the various steps
    of music creation: composition, arranging, orchestration, production, etc. Indeed,
    to compose or to improvise^(11)^(11)11Improvisation is a form of real time composition.,
    a musician rarely creates new music from scratch. S/he reuses and adapts, consciously
    or unconsciously, features from various music that s/he already knows or has heard,
    while following some principles and guidelines, such as theories about harmony
    and scales. A computer-based musician assistant may act during different stages
    of the composition, to initiate, suggest, provoke and/or complement the inspiration
    of the human composer.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, as we will see, the majority of current deep-learning based systems
    for generating music are still focused on autonomous generation, although more
    and more systems are addressing the issue of human-level control and interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Symbolic versus Sub-Symbolic AI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Artificial Intelligence (AI) is often divided into two main streams^(12)^(12)12With
    some precaution, as this division is not that strict.:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: symbolic AI – dealing with high-level symbolic representations (e.g., chords,
    harmony…) and processes (harmonization, analysis…); and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sub-symbolic AI – dealing with low-level representations (e.g., sound, timbre…)
    and processes (pitch recognition, classification…).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Examples of symbolic models used for music are rule-based systems or grammars
    to represent harmony. Examples of sub-symbolic models used for music are machine
    learning algorithms for automatically learning musical styles from a corpus of
    musical pieces. These models can then be used in a generative and interactive
    manner, to help musicians in creating new music, by taking advantage of this added
    “intelligent” memory (associative, inductive and generative) to suggest proposals,
    sketches, extrapolations, mappings, etc. This is now feasible because of the growing
    availability of music in various forms, e.g., sound, scores and MIDI files, which
    can be automatically processed by computers.
  prefs: []
  type: TYPE_NORMAL
- en: A recent example of an integrated music composition environment is FlowComposer
    papadopoulos:flow:composer:cp:2016, which we will introduce in Section LABEL:section:systems:flow:composer.
    It offers various symbolic and sub-symbolic techniques, e.g., Markov chains for
    modeling style, a constraint solving module for expressing constraints, a rule-based
    module to produce harmonic analysis; and an audio mapping module to produce rendering.
    Another example of an integrated music composition environment is OpenMusic assayag:openmusic:cmj:1999.
  prefs: []
  type: TYPE_NORMAL
- en: However, a deeper integration of sub-symbolic techniques, such as deep learning,
    with symbolic techniques, such as constraints and reasoning, is still an open
    issue^(13)^(13)13The general objective of integrating sub-symbolic and symbolic
    levels into a complete AI system is among the “Holy Grails” of AI., although some
    partial integrations in restricted contexts already exist (see, for example, Markov
    constraints in pachet:markov:constraints:constraints:2011; barbieri:lyrics:style:2012
    and an example of use for FlowComposer in Section LABEL:section:systems:flow:composer).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The motivation for using deep learning (and more generally machine learning
    techniques) to generate musical content is its generality. As opposed to handcrafted
    models, such as grammar-based steedman:generative:grammar:blues:mp:1984 or rule-based
    music generation systems ebcioglu:expert:system:bach:cmj:1988, a machine learning-based
    generation system can be agnostic, as it learns a model from an arbitrary corpus
    of music. As a result, the same system may be used for various musical genres.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, as more large scale musical datasets are made available, a machine
    learning-based generation system will be able to automatically learn a musical
    style from a corpus and to generate new musical content. As stated by Fiebrink
    and Caramiaux fiebrink:ml:creative:tool:arxiv:2016, some benefits are
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it can make creation feasible when the desired application is too complex to
    be described by analytical formulations or manual brute force design, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning algorithms are often less brittle than manually designed rule sets
    and learned rules are more likely to generalize accurately to new contexts in
    which inputs may change.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moreover, as opposed to structured representations like rules and grammars,
    deep learning is good at processing raw unstructured data, from which its hierarchy
    of layers will extract higher level representations adapted to the task.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Present and Future
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we will see, the research domain in deep learning-based music generation
    has turned hot recently, building on initial work using artificial neural networks
    to generate music (e.g., the pioneering experiments by Todd in 1989 todd:connectionist:composition:1989
    and the CONCERT system developed by Mozer in 1994 mozer:composition:prediction:1994),
    while creating an active stream of new ideas and challenges made possible thanks
    to the progress of deep learning. Let us also note the growing interest by some
    private big actors of digital media in the computer-aided generation of artistic
    content, with the creation by Google in June 2016 of the Magenta research project
    google:magenta:project:web and the creation by Spotify in September 2017 of the
    Creator Technology Research Lab (CTRL) spotify:ctrl:2017. This is likely to contribute
    to blurring the line between music creation and music consumption through the
    personalization of musical content amato:ai:media:arxiv:2019.
  prefs: []
  type: TYPE_NORMAL
- en: 2 This Book
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The lack (to our knowledge) of a comprehensive survey and analysis of this active
    research domain motivated the writing of this book, built in a bottom-up way from
    the analysis of numerous recent research works. The objective is to provide a
    comprehensive description of the issues and techniques for using deep learning
    to generate music, illustrated through the analysis of various architectures,
    systems and experiments presented in the literature. We also propose a conceptual
    framework and typology aimed at a better understanding of the design decisions
    for current as well as future systems.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Other Books and Sources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To our knowledge, there are only a few partial attempts at analyzing the use
    of deep learning for generating music. In briot:dlt4mg:arxiv:2017, a very preliminary
    version of this work, Briot et al. proposed a first survey of various systems
    through a multicriteria analysis (considering as dimensions the objective, representation,
    architecture and strategy). We have extended and consolidated this study by integrating
    as an additional dimension the challenge (after having analyzed it in briot:mgbdl:cd:ncaa:2018).
  prefs: []
  type: TYPE_NORMAL
- en: In graves:generating:sequences:rnn:arxiv:2014, Graves presented an analysis
    focusing on recurrent neural networks and text generation. In humphrey:feature:learning:music:2013,
    Humphrey et al. presented another analysis, sharing some issues about music representation
    (see Section LABEL:section:representation) but dedicated to music information
    retrieval (MIR) tasks, such as chord recognition, genre recognition and mood estimation.
    On MIR applications of deep learning, see also the recent tutorial paper by Choi
    et al. choi:tutorial:deep:learning:mir:arxiv:2017.
  prefs: []
  type: TYPE_NORMAL
- en: One could also consult the proceedings of some recently created international
    workshops on the topic, such as
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Workshop on Constructive Machine Learning (CML 2016), held during the 30th
    Annual Conference on Neural Information Processing Systems (NIPS 2016) cml:nips:2016;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Workshop on Deep Learning for Music (DLM), held during the International
    Joint Conference on Neural Networks (IJCNN 2017) dl4m:ijcnn:2017, followed by
    a special journal issue herremans:deep:music:ncaa:2019; and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: on the deep challenge of creativity, the related Series of International Conferences
    on Computational Creativity (ICCC) iacc:iccc:web.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a more general survey of computer-based techniques to generate music, the
    reader can refer to general books such as
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roads’ book about computer music roads:computer:music:tutorial:book:1996;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cope’s cope:algorithmic:composer:book:2000, Dean and McLean’s dean:oxford:handbook:2018
    and/or Nierhaus’ books nierhaus:algorithmic:composition:book:2009 about algorithmic
    composition;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a recent survey about AI methods in algorithmic composition fernandez:ai:methods:algorithmic:composition:survey:jair:2013;
    and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cope’s book about models of musical creativity cope:musical:creativity:book:2005.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: About machine learning in general, some examples of textbooks are
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the textbook by Mitchell mitchell:ml:book:1997;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a nice introduction and summary by Domingos domingos:things:know:2012; and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a recent, complete and comprehensive book about deep learning by Goodfellow
    et al. goodfellow:deep:learning:book:2016.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Other Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have to remember that there are various other models and techniques for using
    computers to generate music, such as rules, grammars, automata, Markov models
    and graphical models. These models are either manually defined by experts or are
    automatically learnt from examples by using various machine learning techniques.
    They will not be addressed in this book as we are concerned here with deep learning
    techniques. However, in the following section we make a quick comparison of deep
    learning and Markov models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning versus Markov Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning models are not the only models able to learn musical style from
    examples. Markov chain models are also widely used, see, for example, pachet:continuator:ieee:cga:2004.
    A quick comparison (inspired by the analysis of Mozer in mozer:composition:prediction:1994^(14)^(14)14Note
    that he made his analysis in in 1994, long before the deep learning wave.) of
    the pros (+) and cons (–) of deep neural network models and Markov chain models
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[–]'
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Markov models are conceptually simple.
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Markov models have a simple implementation and a simple learning algorithm,
    as the model is a transition probability table^(15)^(15)15Statistics are collected
    from the dataset of examples in order to compute the probabilities..
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs: []
  type: TYPE_NORMAL
- en: Neural network models are conceptually simple but the optimized implementations
    of current deep network architectures may be complex and need a lot of tuning.
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs: []
  type: TYPE_NORMAL
- en: Order 1 Markov models (that is, considering only the previous state) do not
    capture long-term temporal structures.
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs: []
  type: TYPE_NORMAL
- en: Order n Markov models (considering n previous states) are possible but require
    an explosive training set size^(16)^(16)16See the discussion in (mozer:composition:prediction:1994,
    page 249). and can lead to plagiarism^(17)^(17)17By recopying too long sequences
    from the corpus. Some promising solution is to consider a variable order Markov
    model and to constrain the generation (through min order and max order constraints)
    on some sweet spot between junk and plagiarism papadopoulos:maxorder:universality:book:2016..
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can capture various types of relations, contexts and regularities.
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks can learn long-term and high-order dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Markov models can learn from a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks need a lot of examples in order to be able to learn well.
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs: []
  type: TYPE_NORMAL
- en: Markov models do not generalize very well.
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks generalize better through the use of distributed representations
    hinton:boltzmann:machines:pdp:1986.
  prefs: []
  type: TYPE_NORMAL
- en: +
  prefs: []
  type: TYPE_NORMAL
- en: Markov models are operational models (automata) on which some control on the
    generation could be attached^(18)^(18)18Examples are Markov constraints pachet:markov:constraints:constraints:2011
    and factor graphs pachet:variations:structured:ismir:2017..
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks are generative models with a distributed representation and therefore
    with no direct control to be attached^(19)^(19)19This issue as well as some possible
    solutions will be discussed in Section LABEL:section:challenges:strategies:control:dimensions:strategies..
  prefs: []
  type: TYPE_NORMAL
- en: As deep learning implementations are now mature and a large number of examples
    are available, deep learning-based models are in high demand for their characteristics.
    That said, other models (such as Markov chains, graphical models, etc.) are still
    useful and used and the choice of a model and its tuning depends on the characteristics
    of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Requisites and Roadmap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This book does not require prior knowledge about deep learning and neural networks
    nor music.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 Introduction (this chapter) introduces the purpose and rationale of
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter [1](#Ch1 "Chapter 1 Method") Method introduces the method of analysis
    (conceptual framework) and the five dimensions at its basis (objective, representation,
    architecture, challenge and strategy), dimensions that we discuss within the next
    four chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:objective Objective concerns the different types
    of musical content that we want to generate (such as a melody or an accompaniment
    to an existing melody)^(20)^(20)20Our proposed typology of possible objectives
    will turn out to be useful for our analysis because, as we will see, different
    objectives can lead to different architectures and strategies., as well as their
    expected use (by a human and/or a machine).
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:representation Representation provides an analysis
    of the different types of representation and techniques for encoding musical content
    (such as notes, durations or chords) for a deep learning architecture. This chapter
    may be skipped by a reader already expert in computer music, although some of
    the encoding strategies are specific to neural networks and deep learning architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:architecture Architecture summarizes the most
    common deep learning architectures (such as feedforward, recurrent or autoencoder)
    used for the generation of music. This includes a short reminder of the very basics
    of a simple neural network. This chapter may be skipped by a reader already expert
    in artificial neural networks and deep learning architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:challenges:strategies Challenge and Strategy provides
    an analysis of the various challenges that occur when applying deep learning techniques
    to music generation, as well as various strategies for addressing them. We will
    ground our study in the analysis of various systems and experiments surveyed from
    the literature. This chapter is the core of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:analysis Analysis summarizes the survey and analysis
    conducted in Chapter LABEL:section:chapter:challenges:strategies through some
    tables as a way to identify the design decisions and their interrelations for
    the different systems surveyed^(21)^(21)21And hopefully also for the future ones.
    If we draw the analogy (at some meta-level) with the expected ability for a model
    learnt from a corpus by a machine to be able to generalize to future examples
    (see Section LABEL:section:architecture:training:overfitting), we hope that the
    conceptual framework presented in this book, (manually) inducted from a corpus
    of scientific and technical literature about deep-learning-based music generation
    systems, will also be able to help in the design and the understanding of future
    systems..
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:discussion:conclusion Discussion and Conclusion
    revisits some of the open issues that were touched in during the analysis of challenges
    and strategies presented in Chapter LABEL:section:chapter:challenges:strategies,
    before concluding this book.
  prefs: []
  type: TYPE_NORMAL
- en: A table of contents, a table of acronyms, a list of references, a glossary and
    an index complete this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supplementary material is provided at the following companion web site:'
  prefs: []
  type: TYPE_NORMAL
- en: www.briot.info/dlt4mg/
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This book does not intend to be a general introduction to deep learning – a
    recent and broad spectrum book on this topic is goodfellow:deep:learning:book:2016.
    We do not intend to get into all technical details of implementation, like engineering
    and tuning, as well as theory^(22)^(22)22For instance, we will not develop the
    probability theory and information theory frameworks for formalizing and interpreting
    the behavior of neural networks and deep learning. However, Section LABEL:section:architecture:neural:network:entropy
    will introduce the intuition behind the notions of entropy and cross-entropy,
    used for measuring the progress made during learning., as we wish to focus on
    the conceptual level, whilst providing a sufficient degree of precision. Also,
    although having a clear pedagogical objective, we do not provide some end-to-end
    tutorial with all the steps and details on how to implement and tune a complete
    deep learning-based music generation system.
  prefs: []
  type: TYPE_NORMAL
- en: Last, as this book is about a very active domain and as our survey and analysis
    is based on existing systems, our analysis is obviously not exhaustive. We have
    tried to select the most representative proposals and experiments, while new proposals
    are being presented at the time of our writing. Therefore, we encourage readers
    and colleagues to provide any feedback and suggestions for improving this survey
    and analysis which is a still ongoing project.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
