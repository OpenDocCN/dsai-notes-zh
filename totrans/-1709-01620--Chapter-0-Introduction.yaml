- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:08:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[1709.01620] Chapter 0 Introduction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1709.01620](https://ar5iv.labs.arxiv.org/html/1709.01620)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep Learning Techniques for Music Generation – A Survey Jean-Pierre Briot^(∗,)¹¹1Also
    Visiting Professor at UNIRIO (Universidade Federal do Estado do Rio de Janeiro)
    and Permanent Visiting Professor at PUC-Rio (Pontifícia Universidade Católica
    do Rio de Janeiro), Rio de Janeiro, Brazil., Gaëtan Hadjeres^† and François-David
    Pachet^‡ ^∗ Sorbonne Université, CNRS, LIP6, F-75005 Paris, France
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: ^† Sony Computer Science Laboratories, CSL-Paris, F-75005 Paris, France
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ^‡ Spotify Creator Technology Research Lab, CTRL, F-75008 Paris, France
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper is a survey and an analysis of different ways of using deep learning
    (deep artificial neural networks) to generate musical content. We propose a methodology
    based on five dimensions for our analysis:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objective
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What musical content is to be generated?
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: melody, polyphony, accompaniment or counterpoint.'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For what destination and for what use?
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: To be performed by a human(s) (in the case of a musical score), or by a machine
    (in the case of an audio file).
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representation
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the concepts to be manipulated?
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: waveform, spectrogram, note, chord, meter and beat.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What format is to be used?
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: MIDI, piano roll or text.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How will the representation be encoded?
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: scalar, one-hot or many-hot.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What type(s) of deep neural network is (are) to be used?
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: feedforward network, recurrent network, autoencoder or generative
    adversarial networks.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenge
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the limitations and open challenges?
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: variability, interactivity and creativity.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategy
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we model and control the process of generation?
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examples are: single-step feedforward, iterative feedforward, sampling or input
    manipulation.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: For each dimension, we conduct a comparative analysis of various models and
    techniques and we propose some tentative multidimensional typology. This typology
    is bottom-up, based on the analysis of many existing deep-learning based systems
    for music generation selected from the relevant literature. These systems are
    described and are used to exemplify the various choices of objective, representation,
    architecture, challenge and strategy. The last section includes some discussion
    and some prospects.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Supplementary material is provided at the following companion web site:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: www.briot.info/dlt4mg/
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper is a simplified (weak DRM²²2In addition to including high quality
    color figures, the book includes: a table of contents, a list of tables, a list
    of figures, a table of acronyms, a glossary and an index.) version of the following
    book briot:dlt4mg:book:2019: Jean-Pierre Briot, Gaëtan Hadjeres and François-David
    Pachet, Deep Learning Techniques for Music Generation, Computational Synthesis
    and Creative Systems, Springer, 2019. Hardcover ISBN: 978-3-319-70162-2. eBook
    ISBN: 978-3-319-70163-9. Series ISSN: 2509-6575.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 0 Introduction
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Chapter 1 Introduction provides motivation for this book. It includes a short
    summary of the history of computer music and music generation (including previous
    uses of artificial neural networks as well as other models such as grammars, rules
    and Markov chains) up to the recent rise of deep learning. It also describes the
    organization and the public of the book as well as related work.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has recently become a fast growing domain and is now used routinely
    for classification and prediction tasks, such as image recognition, voice recognition
    or translation. It became popular in 2012, when a deep learning architecture significantly
    outperformed standard techniques relying on handcrafted features in an image classification
    competition, see more details in Section LABEL:section:architectures:history.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'We may explain this success and reemergence of artificial neural network techniques
    by the combination of:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: availability of massive data;
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: availability of efficient and affordable computing power¹¹1Notably, thanks to
    graphics processing units (GPU), initially designed for video games, which have
    now one of their biggest markets in data science and deep learning applications.;
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'technical advances, such as:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: pre-training, which resolved initially inefficient training of neural networks
    with many layers hinton:fast:algorithm:2006²²2Although nowadays it has being replaced
    by other techniques, such as batch normalization ioffe:batch:normalization:arxiv:2015
    and deep residual learning he:resnet:deep:residual:arxiv:2015.;
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: convolutions, which provide motif translation invariance le:cun:convolutional:handbook:1998;
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM (long short-term memory), which resolved initially inefficient training
    of recurrent neural networks hochreiter:lstm:1997.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There is no consensual definition for deep learning. It is a repertoire of
    machine learning (ML) techniques, based on artificial neural networks. The key
    aspect and common ground is the term deep. This means that there are multiple
    layers processing multiple hierarchical levels of abstractions, which are automatically
    extracted from data³³3That said, although deep learning will automatically extract
    significant features from the data, manual choices of input representation, e.g.,
    spectrum vs raw wave signal for audio, may be very significant for the accuracy
    of the learning and for the quality of the generated content, see Section LABEL:section:representation:feature:extraction..
    Thus a deep architecture can manage and decompose complex representations in terms
    of simpler representations. The technical foundation is mostly artificial neural
    networks, as we will see in Chapter LABEL:section:chapter:architecture, with many
    extensions, such as: convolutional networks, recurrent networks, autoencoders,
    and restricted Boltzmann machines. For more information about the history and
    various facets of deep learning, see, e.g., a recent comprehensive book on the
    domain goodfellow:deep:learning:book:2016.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习没有统一的定义。它是一种基于人工神经网络的机器学习（ML）技术的综合体。关键方面和共同点是“深度”这个术语。这意味着有多个层次处理多个层次的抽象，这些抽象是从数据中自动提取的³³3虽然深度学习会自动提取数据中的重要特征，但输入表示的手动选择，例如音频的频谱与原始波形信号，对于学习的准确性和生成内容的质量可能非常重要，请参见第LABEL:section:representation:feature:extraction节。因此，深度架构可以管理和分解复杂的表示，转化为更简单的表示。其技术基础主要是人工神经网络，如我们将在第LABEL:section:chapter:architecture章中看到的，还有许多扩展，如：卷积网络、递归网络、自编码器和限制玻尔兹曼机。有关深度学习历史和各种方面的更多信息，请参见例如，最近的一本全面介绍该领域的书籍goodfellow:deep:learning:book:2016。
- en: 'Driving applications of deep learning are traditional machine learning tasks⁴⁴4Tasks
    in machine learning are types of problems and may also be described in terms of
    how the machine learning system should process an example (goodfellow:deep:learning:book:2016,
    Section 5.1.1). Examples are: classification, regression and anomaly detection.:
    classification (for instance, identification of images) and prediction⁵⁵5As a
    testimony of the initial DNA of neural networks: linear regression and logistic
    regression, see Section LABEL:section:architecture:linear:regression. (for instance,
    of the weather) and also more recent ones such as translation.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的驱动应用是传统的机器学习任务⁴⁴4机器学习中的任务是问题类型，也可以描述为机器学习系统应该如何处理一个示例（goodfellow:deep:learning:book:2016，第5.1.1节）。示例包括：分类、回归和异常检测：分类（例如，图像识别）和预测⁵⁵5作为神经网络初始DNA的见证：线性回归和逻辑回归，请参见第LABEL:section:architecture:linear:regression节。（例如，天气预测）以及更近期的应用，如翻译。
- en: 'But a growing area of application of deep learning techniques is the generation
    of content. Content can be of various kinds: images, text and music, the latter
    being the focus of our analysis. The motivation is in using now widely available
    various corpora to automatically learn musical styles and to generate new musical
    content based on this.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但深度学习技术的一个增长领域是内容生成。内容可以是多种类型的：图像、文本和音乐，后者是我们分析的重点。动机在于利用现在广泛可用的各种语料库，自动学习音乐风格，并基于此生成新的音乐内容。
- en: 1 Motivation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 动机
- en: 1 Computer-Based Music Systems
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1 基于计算机的音乐系统
- en: The first music generated by computer appeared in 1957. It was a 17 seconds
    long melody named “The Silver Scale” by its author Newman Guttman and was generated
    by a software for sound synthesis named Music I, developed by Mathews at Bell
    Laboratories. The same year, “The Illiac Suite” was the first score composed by
    a computer lejaren:illiac:book:1959. It was named after the ILLIAC I computer
    at the University of Illinois at Urbana-Champaign (UIUC) in the United States.
    The human “meta-composers” were Lejaren A. Hiller and Leonard M. Isaacson, both
    musicians and scientists. It was an early example of algorithmic composition,
    making use of stochastic models (Markov chains) for generation as well as rules
    to filter generated material according to desired properties.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In the domain of sound synthesis, a landmark was the release in 1983 by Yamaha
    of the DX 7 synthesizer, building on groundwork by Chowning on a model of synthesis
    based on frequency modulation (FM). The same year, the MIDI⁶⁶6Musical instrument
    digital interface, to be introduced in Section LABEL:section:representation:midi.
    interface was launched, as a way to interoperate various software and instruments
    (including the Yamaha DX 7 synthesizer). Another landmark was the development
    by Puckette at IRCAM of the Max/MSP real-time interactive processing environment,
    used for real-time synthesis and for interactive performances.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Regarding algorithmic composition, in the early 1960s Iannis Xenakis explored
    the idea of stochastic composition⁷⁷7One of the first documented case of stochastic
    music, long before computers, is the Musikalisches Wurfelspiel (Dice Music), attributed
    to Wolfgang Amadeus Mozart. It was designed for using dice to generate music by
    concatenating randomly selected predefined music segments composed in a given
    style (Austrian waltz in a given key). xenakis:formalized:music:book:1963, in
    his composition named “Atrées” in 1962. The idea involved using computer fast
    computations to calculate various possibilities from a set of probabilities designed
    by the composer in order to generate samples of musical pieces to be selected.
    In another approach following the initial direction of “The Illiac Suite”, grammars
    and rules were used to specify the style of a given corpus or more generally tonal
    music theory. An example is the generation in the 1980s by Ebcioğlu’s composition
    software named CHORAL of a four-part chorale in the style of Johann Sebastian
    Bach, according to over 350 handcrafted rules ebcioglu:expert:system:bach:cmj:1988.
    In the late 1980s David Cope’s system named Experiments in Musical Intelligence
    (EMI) extended that approach with the capacity to learn from a corpus of scores
    of a composer to create its own grammar and database of rules cope:algorithmic:composer:book:2000.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Since then, computer music has continued developing for the general public,
    if we consider, for instance, the GarageBand music composition and production
    application for Apple platforms (computers, tablets and cellphones), as an offspring
    of the initial Cubase sequencer software, released by Steinberg in 1989.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the history and principles of computer music in general,
    see, for example, the book by Roads roads:computer:music:tutorial:book:1996. For
    more details about the history and principles of algorithmic composition, see,
    for example, maurer:brief:history:algorithmic:composition:1999 and the books by
    Cope cope:algorithmic:composer:book:2000 or Dean and McLean dean:oxford:handbook:2018.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 2 Autonomy versus Assistance
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When talking about computer-based music generation, there is actually some ambiguity
    about whether the objective is
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to design and construct autonomous music-making systems – two recent examples
    being the deep-learning based Amper™ and Jukedeck systems/companies aimed at the
    creation of original music for commercials and documentary; or
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to design and construct computer-based environments to assist human musicians
    (composers, arrangers, producers, etc.) – two examples being the FlowComposer
    environment developed at Sony CSL-Paris papadopoulos:flow:composer:cp:2016 (introduced
    in Section LABEL:section:systems:flow:composer) and the OpenMusic environment
    developed at IRCAM assayag:openmusic:cmj:1999.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The quest for autonomous music-making systems may be an interesting perspective
    for exploring the process of composition⁸⁸8As Richard Feynman coined it: “What
    I cannot create, I do not understand.” and it also serves as an evaluation method.
    An example of a musical Turing test⁹⁹9Initially codified in 1950 by Alan Turing
    and named by him the “imitation game” turing:test:mind:1950, the “Turing test”
    is a test of the ability for a machine to exhibit intelligent behavior equivalent
    to (and more precisely, indistinguishable from) the behavior of a human. In his
    imaginary experimental setting, Turing proposed the test to be a natural language
    conversation between a human (the evaluator) and a hidden actor (another human
    or a machine). If the evaluator cannot reliably tell the machine from the human,
    the machine is said to have passed the test. will be introduced in Section LABEL:section:experiment:deep:bach.
    It consists in presenting to various members of the public (from beginners to
    experts) chorales composed by J. S. Bach or generated by a deep learning system
    and played by human musicians^(10)^(10)10This is to avoid the bias (synthetic
    flavor) of a computer rendered generated music.. As we will see in the following,
    deep learning techniques turn out to be very efficient at succeeding in such tests,
    due to their capacity to learn musical style from a given corpus and to generate
    new music that fits into this style. That said, we consider that such a test is
    more a means than an end.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'A broader perspective is in assisting human musicians during the various steps
    of music creation: composition, arranging, orchestration, production, etc. Indeed,
    to compose or to improvise^(11)^(11)11Improvisation is a form of real time composition.,
    a musician rarely creates new music from scratch. S/he reuses and adapts, consciously
    or unconsciously, features from various music that s/he already knows or has heard,
    while following some principles and guidelines, such as theories about harmony
    and scales. A computer-based musician assistant may act during different stages
    of the composition, to initiate, suggest, provoke and/or complement the inspiration
    of the human composer.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: That said, as we will see, the majority of current deep-learning based systems
    for generating music are still focused on autonomous generation, although more
    and more systems are addressing the issue of human-level control and interaction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 3 Symbolic versus Sub-Symbolic AI
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Artificial Intelligence (AI) is often divided into two main streams^(12)^(12)12With
    some precaution, as this division is not that strict.:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: symbolic AI – dealing with high-level symbolic representations (e.g., chords,
    harmony…) and processes (harmonization, analysis…); and
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sub-symbolic AI – dealing with low-level representations (e.g., sound, timbre…)
    and processes (pitch recognition, classification…).
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Examples of symbolic models used for music are rule-based systems or grammars
    to represent harmony. Examples of sub-symbolic models used for music are machine
    learning algorithms for automatically learning musical styles from a corpus of
    musical pieces. These models can then be used in a generative and interactive
    manner, to help musicians in creating new music, by taking advantage of this added
    “intelligent” memory (associative, inductive and generative) to suggest proposals,
    sketches, extrapolations, mappings, etc. This is now feasible because of the growing
    availability of music in various forms, e.g., sound, scores and MIDI files, which
    can be automatically processed by computers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: A recent example of an integrated music composition environment is FlowComposer
    papadopoulos:flow:composer:cp:2016, which we will introduce in Section LABEL:section:systems:flow:composer.
    It offers various symbolic and sub-symbolic techniques, e.g., Markov chains for
    modeling style, a constraint solving module for expressing constraints, a rule-based
    module to produce harmonic analysis; and an audio mapping module to produce rendering.
    Another example of an integrated music composition environment is OpenMusic assayag:openmusic:cmj:1999.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: However, a deeper integration of sub-symbolic techniques, such as deep learning,
    with symbolic techniques, such as constraints and reasoning, is still an open
    issue^(13)^(13)13The general objective of integrating sub-symbolic and symbolic
    levels into a complete AI system is among the “Holy Grails” of AI., although some
    partial integrations in restricted contexts already exist (see, for example, Markov
    constraints in pachet:markov:constraints:constraints:2011; barbieri:lyrics:style:2012
    and an example of use for FlowComposer in Section LABEL:section:systems:flow:composer).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The motivation for using deep learning (and more generally machine learning
    techniques) to generate musical content is its generality. As opposed to handcrafted
    models, such as grammar-based steedman:generative:grammar:blues:mp:1984 or rule-based
    music generation systems ebcioglu:expert:system:bach:cmj:1988, a machine learning-based
    generation system can be agnostic, as it learns a model from an arbitrary corpus
    of music. As a result, the same system may be used for various musical genres.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, as more large scale musical datasets are made available, a machine
    learning-based generation system will be able to automatically learn a musical
    style from a corpus and to generate new musical content. As stated by Fiebrink
    and Caramiaux fiebrink:ml:creative:tool:arxiv:2016, some benefits are
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it can make creation feasible when the desired application is too complex to
    be described by analytical formulations or manual brute force design, and
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning algorithms are often less brittle than manually designed rule sets
    and learned rules are more likely to generalize accurately to new contexts in
    which inputs may change.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moreover, as opposed to structured representations like rules and grammars,
    deep learning is good at processing raw unstructured data, from which its hierarchy
    of layers will extract higher level representations adapted to the task.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 5 Present and Future
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we will see, the research domain in deep learning-based music generation
    has turned hot recently, building on initial work using artificial neural networks
    to generate music (e.g., the pioneering experiments by Todd in 1989 todd:connectionist:composition:1989
    and the CONCERT system developed by Mozer in 1994 mozer:composition:prediction:1994),
    while creating an active stream of new ideas and challenges made possible thanks
    to the progress of deep learning. Let us also note the growing interest by some
    private big actors of digital media in the computer-aided generation of artistic
    content, with the creation by Google in June 2016 of the Magenta research project
    google:magenta:project:web and the creation by Spotify in September 2017 of the
    Creator Technology Research Lab (CTRL) spotify:ctrl:2017. This is likely to contribute
    to blurring the line between music creation and music consumption through the
    personalization of musical content amato:ai:media:arxiv:2019.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 2 This Book
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The lack (to our knowledge) of a comprehensive survey and analysis of this active
    research domain motivated the writing of this book, built in a bottom-up way from
    the analysis of numerous recent research works. The objective is to provide a
    comprehensive description of the issues and techniques for using deep learning
    to generate music, illustrated through the analysis of various architectures,
    systems and experiments presented in the literature. We also propose a conceptual
    framework and typology aimed at a better understanding of the design decisions
    for current as well as future systems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 1 Other Books and Sources
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To our knowledge, there are only a few partial attempts at analyzing the use
    of deep learning for generating music. In briot:dlt4mg:arxiv:2017, a very preliminary
    version of this work, Briot et al. proposed a first survey of various systems
    through a multicriteria analysis (considering as dimensions the objective, representation,
    architecture and strategy). We have extended and consolidated this study by integrating
    as an additional dimension the challenge (after having analyzed it in briot:mgbdl:cd:ncaa:2018).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: In graves:generating:sequences:rnn:arxiv:2014, Graves presented an analysis
    focusing on recurrent neural networks and text generation. In humphrey:feature:learning:music:2013,
    Humphrey et al. presented another analysis, sharing some issues about music representation
    (see Section LABEL:section:representation) but dedicated to music information
    retrieval (MIR) tasks, such as chord recognition, genre recognition and mood estimation.
    On MIR applications of deep learning, see also the recent tutorial paper by Choi
    et al. choi:tutorial:deep:learning:mir:arxiv:2017.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: One could also consult the proceedings of some recently created international
    workshops on the topic, such as
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Workshop on Constructive Machine Learning (CML 2016), held during the 30th
    Annual Conference on Neural Information Processing Systems (NIPS 2016) cml:nips:2016;
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Workshop on Deep Learning for Music (DLM), held during the International
    Joint Conference on Neural Networks (IJCNN 2017) dl4m:ijcnn:2017, followed by
    a special journal issue herremans:deep:music:ncaa:2019; and
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: on the deep challenge of creativity, the related Series of International Conferences
    on Computational Creativity (ICCC) iacc:iccc:web.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a more general survey of computer-based techniques to generate music, the
    reader can refer to general books such as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roads’ book about computer music roads:computer:music:tutorial:book:1996;
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cope’s cope:algorithmic:composer:book:2000, Dean and McLean’s dean:oxford:handbook:2018
    and/or Nierhaus’ books nierhaus:algorithmic:composition:book:2009 about algorithmic
    composition;
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a recent survey about AI methods in algorithmic composition fernandez:ai:methods:algorithmic:composition:survey:jair:2013;
    and
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cope’s book about models of musical creativity cope:musical:creativity:book:2005.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: About machine learning in general, some examples of textbooks are
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the textbook by Mitchell mitchell:ml:book:1997;
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a nice introduction and summary by Domingos domingos:things:know:2012; and
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a recent, complete and comprehensive book about deep learning by Goodfellow
    et al. goodfellow:deep:learning:book:2016.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Other Models
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have to remember that there are various other models and techniques for using
    computers to generate music, such as rules, grammars, automata, Markov models
    and graphical models. These models are either manually defined by experts or are
    automatically learnt from examples by using various machine learning techniques.
    They will not be addressed in this book as we are concerned here with deep learning
    techniques. However, in the following section we make a quick comparison of deep
    learning and Markov models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning versus Markov Models
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning models are not the only models able to learn musical style from
    examples. Markov chain models are also widely used, see, for example, pachet:continuator:ieee:cga:2004.
    A quick comparison (inspired by the analysis of Mozer in mozer:composition:prediction:1994^(14)^(14)14Note
    that he made his analysis in in 1994, long before the deep learning wave.) of
    the pros (+) and cons (–) of deep neural network models and Markov chain models
    is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[–]'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Markov models are conceptually simple.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Markov models have a simple implementation and a simple learning algorithm,
    as the model is a transition probability table^(15)^(15)15Statistics are collected
    from the dataset of examples in order to compute the probabilities..
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: –
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Neural network models are conceptually simple but the optimized implementations
    of current deep network architectures may be complex and need a lot of tuning.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: –
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Order 1 Markov models (that is, considering only the previous state) do not
    capture long-term temporal structures.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: –
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Order n Markov models (considering n previous states) are possible but require
    an explosive training set size^(16)^(16)16See the discussion in (mozer:composition:prediction:1994,
    page 249). and can lead to plagiarism^(17)^(17)17By recopying too long sequences
    from the corpus. Some promising solution is to consider a variable order Markov
    model and to constrain the generation (through min order and max order constraints)
    on some sweet spot between junk and plagiarism papadopoulos:maxorder:universality:book:2016..
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can capture various types of relations, contexts and regularities.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks can learn long-term and high-order dependencies.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Markov models can learn from a few examples.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: –
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks need a lot of examples in order to be able to learn well.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: –
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Markov models do not generalize very well.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks generalize better through the use of distributed representations
    hinton:boltzmann:machines:pdp:1986.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: +
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Markov models are operational models (automata) on which some control on the
    generation could be attached^(18)^(18)18Examples are Markov constraints pachet:markov:constraints:constraints:2011
    and factor graphs pachet:variations:structured:ismir:2017..
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: –
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks are generative models with a distributed representation and therefore
    with no direct control to be attached^(19)^(19)19This issue as well as some possible
    solutions will be discussed in Section LABEL:section:challenges:strategies:control:dimensions:strategies..
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: As deep learning implementations are now mature and a large number of examples
    are available, deep learning-based models are in high demand for their characteristics.
    That said, other models (such as Markov chains, graphical models, etc.) are still
    useful and used and the choice of a model and its tuning depends on the characteristics
    of the problem.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 4 Requisites and Roadmap
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This book does not require prior knowledge about deep learning and neural networks
    nor music.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 Introduction (this chapter) introduces the purpose and rationale of
    the book.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Chapter [1](#Ch1 "Chapter 1 Method") Method introduces the method of analysis
    (conceptual framework) and the five dimensions at its basis (objective, representation,
    architecture, challenge and strategy), dimensions that we discuss within the next
    four chapters.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:objective Objective concerns the different types
    of musical content that we want to generate (such as a melody or an accompaniment
    to an existing melody)^(20)^(20)20Our proposed typology of possible objectives
    will turn out to be useful for our analysis because, as we will see, different
    objectives can lead to different architectures and strategies., as well as their
    expected use (by a human and/or a machine).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:representation Representation provides an analysis
    of the different types of representation and techniques for encoding musical content
    (such as notes, durations or chords) for a deep learning architecture. This chapter
    may be skipped by a reader already expert in computer music, although some of
    the encoding strategies are specific to neural networks and deep learning architectures.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:architecture Architecture summarizes the most
    common deep learning architectures (such as feedforward, recurrent or autoencoder)
    used for the generation of music. This includes a short reminder of the very basics
    of a simple neural network. This chapter may be skipped by a reader already expert
    in artificial neural networks and deep learning architectures.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:challenges:strategies Challenge and Strategy provides
    an analysis of the various challenges that occur when applying deep learning techniques
    to music generation, as well as various strategies for addressing them. We will
    ground our study in the analysis of various systems and experiments surveyed from
    the literature. This chapter is the core of the book.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:analysis Analysis summarizes the survey and analysis
    conducted in Chapter LABEL:section:chapter:challenges:strategies through some
    tables as a way to identify the design decisions and their interrelations for
    the different systems surveyed^(21)^(21)21And hopefully also for the future ones.
    If we draw the analogy (at some meta-level) with the expected ability for a model
    learnt from a corpus by a machine to be able to generalize to future examples
    (see Section LABEL:section:architecture:training:overfitting), we hope that the
    conceptual framework presented in this book, (manually) inducted from a corpus
    of scientific and technical literature about deep-learning-based music generation
    systems, will also be able to help in the design and the understanding of future
    systems..
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Chapter LABEL:section:chapter:discussion:conclusion Discussion and Conclusion
    revisits some of the open issues that were touched in during the analysis of challenges
    and strategies presented in Chapter LABEL:section:chapter:challenges:strategies,
    before concluding this book.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A table of contents, a table of acronyms, a list of references, a glossary and
    an index complete this book.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Supplementary material is provided at the following companion web site:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: www.briot.info/dlt4mg/
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limits
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This book does not intend to be a general introduction to deep learning – a
    recent and broad spectrum book on this topic is goodfellow:deep:learning:book:2016.
    We do not intend to get into all technical details of implementation, like engineering
    and tuning, as well as theory^(22)^(22)22For instance, we will not develop the
    probability theory and information theory frameworks for formalizing and interpreting
    the behavior of neural networks and deep learning. However, Section LABEL:section:architecture:neural:network:entropy
    will introduce the intuition behind the notions of entropy and cross-entropy,
    used for measuring the progress made during learning., as we wish to focus on
    the conceptual level, whilst providing a sufficient degree of precision. Also,
    although having a clear pedagogical objective, we do not provide some end-to-end
    tutorial with all the steps and details on how to implement and tune a complete
    deep learning-based music generation system.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Last, as this book is about a very active domain and as our survey and analysis
    is based on existing systems, our analysis is obviously not exhaustive. We have
    tried to select the most representative proposals and experiments, while new proposals
    are being presented at the time of our writing. Therefore, we encourage readers
    and colleagues to provide any feedback and suggestions for improving this survey
    and analysis which is a still ongoing project.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 Method
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
