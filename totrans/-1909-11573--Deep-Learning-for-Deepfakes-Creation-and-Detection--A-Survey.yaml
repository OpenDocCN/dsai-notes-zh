- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:04:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1909.11573] Deep Learning for Deepfakes Creation and Detection: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.11573](https://ar5iv.labs.arxiv.org/html/1909.11573)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Deepfakes Creation and Detection: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanh Thi Nguyen Quoc Viet Hung Nguyen Dung Tien Nguyen Duc Thanh Nguyen Thien Huynh-The
    Saeid Nahavandi Thanh Tam Nguyen Quoc-Viet Pham Cuong M. Nguyen
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning has been successfully applied to solve various complex problems
    ranging from big data analytics to computer vision and human-level control. Deep
    learning advances however have also been employed to create software that can
    cause threats to privacy, democracy and national security. One of those deep learning-powered
    applications recently emerged is deepfake. Deepfake algorithms can create fake
    images and videos that humans cannot distinguish them from authentic ones. The
    proposal of technologies that can automatically detect and assess the integrity
    of digital visual media is therefore indispensable. This paper presents a survey
    of algorithms used to create deepfakes and, more importantly, methods proposed
    to detect deepfakes in the literature to date. We present extensive discussions
    on challenges, research trends and directions related to deepfake technologies.
    By reviewing the background of deepfakes and state-of-the-art deepfake detection
    methods, this study provides a comprehensive overview of deepfake techniques and
    facilitates the development of new and more robust methods to deal with the increasingly
    challenging deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: deepfakes , face manipulation , artificial intelligence , deep learning , autoencoders
    , GAN , forensics , survey\affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst1]organization=School of Information Technology, Deakin University,state=Victoria,
    country=Australia'
  prefs: []
  type: TYPE_NORMAL
- en: \affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst2]organization=School of Information and Communication Technology, Griffith
    University, state=Queensland, country=Australia'
  prefs: []
  type: TYPE_NORMAL
- en: \affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst3]organization=ICT Convergence Research Center, Kumoh National Institute
    of Technology, state=Gyeongbuk, country=Republic of Korea'
  prefs: []
  type: TYPE_NORMAL
- en: \affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst4]organization=Institute for Intelligent Systems Research and Innovation,
    Deakin University, state=Victoria, country=Australia'
  prefs: []
  type: TYPE_NORMAL
- en: \affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst5]organization=Faculty of Information Technology, Ho Chi Minh City University
    of Technology (HUTECH), state=Ho Chi Minh City, country=Vietnam'
  prefs: []
  type: TYPE_NORMAL
- en: \affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst6]organization=Korean Southeast Center for the 4th Industrial Revolution
    Leader Education, Pusan National University, state=Busan, country=Republic of
    Korea'
  prefs: []
  type: TYPE_NORMAL
- en: \affiliation
  prefs: []
  type: TYPE_NORMAL
- en: '[inst7]organization=LAMIH UMR CNRS 8201, Universite Polytechnique Hauts-de-France,
    state=Valenciennes, country=France'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a narrow definition, deepfakes (stemming from “deep learning” and “fake”)
    are created by techniques that can superimpose face images of a target person
    onto a video of a source person to make a video of the target person doing or
    saying things the source person does. This constitutes a category of deepfakes,
    namely *face-swap*. In a broader definition, deepfakes are artificial intelligence-synthesized
    content that can also fall into two other categories, i.e., *lip-sync* and *puppet-master*.
    Lip-sync deepfakes refer to videos that are modified to make the mouth movements
    consistent with an audio recording. Puppet-master deepfakes include videos of
    a target person (puppet) who is animated following the facial expressions, eye
    and head movements of another person (master) sitting in front of a camera [[1](#bib.bib1)].
  prefs: []
  type: TYPE_NORMAL
- en: While some deepfakes can be created by traditional visual effects or computer-graphics
    approaches, the recent common underlying mechanism for deepfake creation is deep
    learning models such as autoencoders and generative adversarial networks (GANs),
    which have been applied widely in the computer vision domain [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]. These models are used to examine facial expressions and movements
    of a person and synthesize facial images of another person making analogous expressions
    and movements [[9](#bib.bib9)]. Deepfake methods normally require a large amount
    of image and video data to train models to create photo-realistic images and videos.
    As public figures such as celebrities and politicians may have a large number
    of videos and images available online, they are initial targets of deepfakes.
    Deepfakes were used to swap faces of celebrities or politicians to bodies in porn
    images and videos. The first deepfake video emerged in 2017 where face of a celebrity
    was swapped to the face of a porn actor. It is threatening to world security when
    deepfake methods can be employed to create videos of world leaders with fake speeches
    for falsification purposes [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)].
    Deepfakes therefore can be abused to cause political or religion tensions between
    countries, to fool public and affect results in election campaigns, or create
    chaos in financial markets by creating fake news [[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)]. It can be even used to generate fake satellite images of the
    Earth to contain objects that do not really exist to confuse military analysts,
    e.g., creating a fake bridge across a river although there is no such a bridge
    in reality. This can mislead a troop who have been guided to cross the bridge
    in a battle [[16](#bib.bib16), [17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: As the democratization of creating realistic digital humans has positive implications,
    there is also positive use of deepfakes such as their applications in visual effects,
    digital avatars, snapchat filters, creating voices of those who have lost theirs
    or updating episodes of movies without reshooting them [[18](#bib.bib18)]. Deepfakes
    can have creative or productive impacts in photography, video games, virtual reality,
    movie productions, and entertainment, e.g., realistic video dubbing of foreign
    films, education through the reanimation of historical figures, virtually trying
    on clothes while shopping, and so on [[19](#bib.bib19), [20](#bib.bib20)]. However,
    the number of malicious uses of deepfakes largely dominates that of the positive
    ones. The development of advanced deep neural networks and the availability of
    large amount of data have made the forged images and videos almost indistinguishable
    to humans and even to sophisticated computer algorithms. The process of creating
    those manipulated images and videos is also much simpler today as it needs as
    little as an identity photo or a short video of a target individual. Less and
    less effort is required to produce a stunningly convincing tempered footage. Recent
    advances can even create a deepfake with just a still image [[21](#bib.bib21)].
    Deepfakes therefore can be a threat affecting not only public figures but also
    ordinary people. For example, a voice deepfake was used to scam a CEO out of $243,000
    [[22](#bib.bib22)]. A recent release of a software called DeepNude shows more
    disturbing threats as it can transform a person to a non-consensual porn [[23](#bib.bib23)].
    Likewise, the Chinese app Zao has gone viral lately as less-skilled users can
    swap their faces onto bodies of movie stars and insert themselves into well-known
    movies and TV clips [[24](#bib.bib24)]. These forms of falsification create a
    huge threat to violation of privacy and identity, and affect many aspects of human
    lives.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/128d6827701974df74f2d217f70fd0ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Number of papers related to deepfakes in years from 2016 to 2021,
    obtained from https://app.dimensions.ai at the end of 2021 with the search keyword
    “deepfake” applied to full text of scholarly papers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the truth in digital domain therefore has become increasingly critical.
    It is even more challenging when dealing with deepfakes as they are majorly used
    to serve malicious purposes and almost anyone can create deepfakes these days
    using existing deepfake tools. Thus far, there have been numerous methods proposed
    to detect deepfakes [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]. Most of them are based on deep learning, and thus a battle
    between malicious and positive uses of deep learning methods has been arising.
    To address the threat of face-swapping technology or deepfakes, the United States
    Defense Advanced Research Projects Agency (DARPA) initiated a research scheme
    in media forensics (named Media Forensics or MediFor) to accelerate the development
    of fake digital visual media detection methods [[30](#bib.bib30)]. Recently, Facebook
    Inc. teaming up with Microsoft Corp and the Partnership on AI coalition have launched
    the Deepfake Detection Challenge to catalyse more research and development in
    detecting and preventing deepfakes from being used to mislead viewers [[31](#bib.bib31)].
    Data obtained from https://app.dimensions.ai at the end of 2021 show that the
    number of deepfake papers has increased significantly in recent years (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Deepfakes Creation and Detection:
    A Survey")). Although the obtained numbers of deepfake papers may be lower than
    actual numbers but the research trend of this topic is obviously increasing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There have been existing survey papers about creating and detecting deepfakes,
    presented in [[32](#bib.bib32), [20](#bib.bib20), [19](#bib.bib19)]. For example,
    Mirsky and Lee [[19](#bib.bib19)] focused on reenactment approaches (i.e., to
    change a target’s expression, mouth, pose, gaze or body), and replacement approaches
    (i.e., to replace a target’s face by swap or transfer methods). Verdoliva [[20](#bib.bib20)]
    separated detection approaches into conventional methods (e.g., blind methods
    without using any external data for training, one-class sensor-based and model-based
    methods, and supervised methods with handcrafted features) and deep learning-based
    approaches (e.g., CNN models). Tolosana et al. [[32](#bib.bib32)] categorized
    both creation and detection methods based on the way deepfakes are created, including
    entire face synthesis, identity swap, attribute manipulation, and expression swap.
    On the other hand, we carry out the survey with a different perspective and taxonomy.
    We categorize the deepfake detection methods based on the data type, i.e., images
    or videos, as presented in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning
    for Deepfakes Creation and Detection: A Survey"). With fake image detection methods,
    we focus on the features that are used, i.e., whether they are handcrafted features
    or deep features. With fake video detection methods, two main subcategories are
    identified based on whether the method uses temporal features across frames or
    visual artifacts within a video frame. We also discuss extensively the challenges,
    research trends and directions on deepfake detection and multimedia forensics
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42bdb910e34de12c9daaa0a9c3e53d65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Categories of reviewed papers relevant to deepfake detection methods
    where we divide papers into two major groups, i.e., fake image detection and face
    video detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deepfake Creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deepfakes have become popular due to the quality of tampered videos and also
    the easy-to-use ability of their applications to a wide range of users with various
    computer skills from professional to novice. These applications are mostly developed
    based on deep learning techniques. Deep learning is well known for its capability
    of representing complex and high-dimensional data. One variant of the deep networks
    with that capability is deep autoencoders, which have been widely applied for
    dimensionality reduction and image compression [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35)]. The first attempt of deepfake creation was FakeApp, developed
    by a Reddit user using autoencoder-decoder pairing structure [[36](#bib.bib36),
    [37](#bib.bib37)]. In that method, the autoencoder extracts latent features of
    face images and the decoder is used to reconstruct the face images. To swap faces
    between source images and target images, there is a need of two encoder-decoder
    pairs where each pair is used to train on an image set, and the encoder’s parameters
    are shared between two network pairs. In other words, two pairs have the same
    encoder network. This strategy enables the common encoder to find and learn the
    similarity between two sets of face images, which are relatively unchallenging
    because faces normally have similar features such as eyes, nose, mouth positions.
    Fig. [3](#S2.F3 "Figure 3 ‣ 2 Deepfake Creation ‣ Deep Learning for Deepfakes
    Creation and Detection: A Survey") shows a deepfake creation process where the
    feature set of face A is connected with the decoder B to reconstruct face B from
    the original face A. This approach is applied in several works such as DeepFaceLab
    [[38](#bib.bib38)], DFaker [[39](#bib.bib39)], DeepFake_tf (tensorflow-based deepfakes)
    [[40](#bib.bib40)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of notable deepfake tools'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tools | Links | Key Features |'
  prefs: []
  type: TYPE_TB
- en: '| Faceswap | https://github.com/deepfakes/faceswap | - Using two encoder-decoder
    pairs. - Parameters of the encoder are shared. |'
  prefs: []
  type: TYPE_TB
- en: '| Faceswap-GAN | https://github.com/shaoanlu/faceswap-GAN | Adversarial loss
    and perceptual loss (VGGface) are added to an auto-encoder architecture. |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Face Translation | https://github.com/shaoanlu/fewshot-face-translation-GAN
    | - Use a pre-trained face recognition model to extract latent embeddings for
    GAN processing. - Incorporate semantic priors obtained by modules from FUNIT [[41](#bib.bib41)]
    and SPADE [[42](#bib.bib42)]. |'
  prefs: []
  type: TYPE_TB
- en: '| DeepFaceLab | https://github.com/iperov/DeepFaceLab | - Expand from the Faceswap
    method with new models, e.g. H64, H128, LIAEF128, SAE [[43](#bib.bib43)]. - Support
    multiple face extraction modes, e.g. S3FD, MTCNN, dlib, or manual [[43](#bib.bib43)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| DFaker | https://github.com/dfaker/df | - DSSIM loss function [[44](#bib.bib44)]
    is used to reconstruct face. - Implemented based on Keras library. |'
  prefs: []
  type: TYPE_TB
- en: '| DeepFake_tf | https://github.com/StromWine/DeepFake_tf | Similar to DFaker
    but implemented based on tensorflow. |'
  prefs: []
  type: TYPE_TB
- en: '| AvatarMe | https://github.com/lattas/AvatarMe | - Reconstruct 3D faces from
    arbitrary “in-the-wild” images. - Can reconstruct authentic 4K by 6K-resolution
    3D faces from a single low-resolution image [[45](#bib.bib45)]. |'
  prefs: []
  type: TYPE_TB
- en: '| MarioNETte | https://hyperconnect.github.io/MarioNETte | - A few-shot face
    reenactment framework that preserves the target identity. - No additional fine-tuning
    phase is needed for identity adaptation [[46](#bib.bib46)]. |'
  prefs: []
  type: TYPE_TB
- en: '| DiscoFaceGAN | https://github.com/microsoft/DiscoFaceGAN | - Generate face
    images of virtual people with independent latent variables of identity, expression,
    pose, and illumination. - Embed 3D priors into adversarial learning [[47](#bib.bib47)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| StyleRig | https://gvv.mpi-inf.mpg.de/projects/StyleRig | - Create portrait
    images of faces with a rig-like control over a pretrained and fixed StyleGAN via
    3D morphable face models. - Self-supervised without manual annotations [[48](#bib.bib48)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| FaceShifter | https://lingzhili.com/FaceShifterPage | - Face swapping in
    high-fidelity by exploiting and integrating the target attributes. - Can be applied
    to any new face pairs without requiring subject specific training [[49](#bib.bib49)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| FSGAN | https://github.com/YuvalNirkin/fsgan | - A face swapping and reenactment
    model that can be applied to pairs of faces without requiring training on those
    faces. - Adjust to both pose and expression variations [[50](#bib.bib50)]. |'
  prefs: []
  type: TYPE_TB
- en: '| StyleGAN | https://github.com/NVlabs/stylegan | - A new generator architecture
    for GANs is proposed based on style transfer literature. - The new architecture
    leads to automatic, unsupervised separation of high-level attributes and enables
    intuitive, scale-specific control of the synthesis of images [[51](#bib.bib51)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Face2Face | https://justusthies.github.io/posts/face2face/ | - Real-time
    facial reenactment of monocular target video sequence, e.g. Youtube video. - Animate
    the facial expressions of the target video by a source actor and re-render the
    manipulated output video in a photo-realistic fashion [[52](#bib.bib52)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Neural Textures | https://github.com/SSRSGJYD/NeuralTexture | - Feature maps
    that are learned as part of the scene capture process and stored as maps on top
    of 3D mesh proxies. - Can coherently re-render or manipulate existing video content
    in both static and dynamic environments at real-time rates [[53](#bib.bib53)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Transformable Bottleneck Networks | https://github.com/kyleolsz/TB-Networks
    | - A method for fine-grained 3D manipulation of image content. - Apply spatial
    transformations in CNN models using a transformable bottleneck framework [[54](#bib.bib54)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Do as I Do” Motion Transfer | github.com/carolineec/EverybodyDanceNow |
    - Automatically transfer the motion from a source to a target person by learning
    a video-to-video translation. - Can create a motion-synchronized dancing video
    with multiple subjects [[55](#bib.bib55)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Neural Voice Puppetry | https://justusthies.github.io/posts/neural-voice-puppetry
    | - A method for audio-driven facial video synthesis. - Synthesize videos of a
    talking head from an audio sequence of another person using 3D face representation.
    [[56](#bib.bib56)]. |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/9907b7e31a2986374bd296789a04f03b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A deepfake creation model using two encoder-decoder pairs. Two networks
    use the same encoder but different decoders for training process (top). An image
    of face A is encoded with the common encoder and decoded with decoder B to create
    a deepfake (bottom). The reconstructed image (in the bottom) is the face B with
    the mouth shape of face A. Face B originally has the mouth of an upside-down heart
    while the reconstructed face B has the mouth of a conventional heart.'
  prefs: []
  type: TYPE_NORMAL
- en: By adding adversarial loss and perceptual loss implemented in VGGFace [[57](#bib.bib57)]
    to the encoder-decoder architecture, an improved version of deepfakes based on
    the generative adversarial network [[4](#bib.bib4)], i.e., faceswap-GAN, was proposed
    in [[58](#bib.bib58)]. The VGGFace perceptual loss is added to make eye movements
    to be more realistic and consistent with input faces and help to smooth out artifacts
    in segmentation mask, leading to higher quality output videos. This model facilitates
    the creation of outputs with 64x64, 128x128, and 256x256 resolutions. In addition,
    the multi-task convolutional neural network (CNN) from the FaceNet implementation
    [[59](#bib.bib59)] is used to make face detection more stable and face alignment
    more reliable. The CycleGAN [[60](#bib.bib60)] is utilized for generative network
    implementation in this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8adbcfdfc2352a144fc95b8e3d3da64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The GAN architecture consisting of a generator and a discriminator,
    and each can be implemented by a neural network. The entire system can be trained
    with backpropagation that allows both networks to improve their capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A conventional GAN model comprises two neural networks: a generator and a discriminator
    as depicted in Fig. [4](#S2.F4 "Figure 4 ‣ 2 Deepfake Creation ‣ Deep Learning
    for Deepfakes Creation and Detection: A Survey"). Given a dataset of real images
    $x$ having a distribution of $p_{data}$, the aim of the generator $G$ is to produce
    images $G(z)$ similar to real images $x$ with $z$ being noise signals having a
    distribution of $p_{z}$. The aim of the discriminator $G$ is to correctly classify
    images generated by $G$ and real images $x$. The discriminator $D$ is trained
    to improve its classification capability, i.e., to maximize $D(x)$, which represents
    the probability that $x$ is a real image rather than a fake image generated by
    $G$. On the other hand, $G$ is trained to minimize the probability that its outputs
    are classified by $D$ as synthetic images, i.e., to minimize $1-D(G(z))$. This
    is a minimax game between two players $D$ and $G$ that can be described by the
    following value function [[4](#bib.bib4)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}V(D,G)=\operatorname{\mathbb{E}}_{x\sim p_{data}(x)}[\log
    D(x)]\\ +\operatorname{\mathbb{E}}_{z\sim p_{z}(z)}[\log(1-D(G(z)))]$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: After sufficient training, both networks improve their capabilities, i.e., the
    generator $G$ is able to produce images that are really similar to real images
    while the discriminator $D$ is highly capable of distinguishing fake images from
    real ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2 Deepfake Creation ‣ Deep Learning for Deepfakes
    Creation and Detection: A Survey") presents a summary of popular deepfake tools
    and their typical features. Among them, a prominent method for face synthesis
    based on a GAN model, namely StyleGAN, was introduced in [[51](#bib.bib51)]. StyleGAN
    is motivated by style transfer [[61](#bib.bib61)] with a special generator network
    architecture that is able to create realistic face images. In a traditional GAN
    model, e.g., the progressive growing of GAN (PGGAN) [[62](#bib.bib62)], the signal
    noise (latent code) is fed to the input layer of a feedforward network that represents
    the generator. In StyleGAN, there are two networks constructed and linked together,
    a mapping network $f$ and a synthesis network $g$. The latent code $z\in Z$ is
    first converted to $w\in W$ (where $W$ is an intermediate latent space) through
    a non-linear function $f:Z\rightarrow W$, which is characterized by a neural network
    (i.e., the mapping network) consisting of several fully connected layers. Using
    an affine tranformation, the intermediate representation $w$ is specialized to
    styles $y=(y_{s},y_{b})$ that will be fed to the adaptive instance normalization
    (AdaIN) operations, specified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{AdaIN}(x_{i},y)=y_{s,i}\frac{x_{i}-\mu(x_{i})}{\sigma(x_{i})}+y_{b,i}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where each feature map $x_{i}$ is normalized separately. The StyleGAN generator
    architecture allows controlling the image synthesis by modifying the styles via
    different scales. In addition, instead of using one random latent code during
    training, this method uses two latent codes to generate a given proportion of
    images. More specifically, two latent codes $z_{1}$ and $z_{2}$ are fed to the
    mapping network to create respectively $w_{1}$ and $w_{2}$ that control the styles
    by applying $w_{1}$ before and $w_{2}$ after the crossover point. Fig. [5](#S2.F5
    "Figure 5 ‣ 2 Deepfake Creation ‣ Deep Learning for Deepfakes Creation and Detection:
    A Survey") demonstrates examples of images created by mixing two latent codes
    at three different scales where each subset of styles controls separate meaningful
    high-level attributes of the image. In other words, the generator architecture
    of StyleGAN is able to learn separation of high-level attributes (e.g., pose and
    identity when trained on human faces) and enables intuitive, scale-speciﬁc control
    of the face synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8c0b982c233bf90f257b5c47b85d1b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Examples of mixing styles using StyleGAN: the output images are generated
    by copying a specified subset of styles from source B and taking the rest from
    source A. a) Copying coarse styles from source B will generate images that have
    high-level aspects from source B and all colors and finer facial features from
    source A; b) if copying the styles of middle resolutions from B, the output images
    will have smaller scale facial features from B and preserve the pose, general
    face shape, and eyeglasses from A; c) if copying the fine styles from source B,
    the generated images will have the color scheme and microstructure of source B
    [[51](#bib.bib51)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deepfake Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deepfake detection is normally deemed a binary classification problem where
    classifiers are used to classify between authentic videos and tampered ones. This
    kind of methods requires a large database of real and fake videos to train classification
    models. The number of fake videos is increasingly available, but it is still limited
    in terms of setting a benchmark for validating various detection methods. To address
    this issue, Korshunov and Marcel [[63](#bib.bib63)] produced a notable deepfake
    dataset consisting of 620 videos based on the GAN model using the open source
    code Faceswap-GAN [[58](#bib.bib58)]. Videos from the publicly available VidTIMIT
    database [[64](#bib.bib64)] were used to generate low and high quality deepfake
    videos, which can effectively mimic the facial expressions, mouth movements, and
    eye blinking. These videos were then used to test various deepfake detection methods.
    Test results show that the popular face recognition systems based on VGG [[65](#bib.bib65)]
    and Facenet [[59](#bib.bib59), [66](#bib.bib66)] are unable to detect deepfakes
    effectively. Other methods such as lip-syncing approaches [[67](#bib.bib67), [68](#bib.bib68),
    [69](#bib.bib69)] and image quality metrics with support vector machine (SVM)
    [[70](#bib.bib70)] produce very high error rate when applied to detect deepfake
    videos from this newly produced dataset. This raises concerns about the critical
    need of future development of more robust methods that can detect deepfakes from
    genuine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section presents a survey of deepfake detection methods where we group
    them into two major categories: fake image detection methods and fake video detection
    ones (Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning for Deepfakes
    Creation and Detection: A Survey")). The latter is distinguished into two smaller
    groups: *visual artifacts* within single video frame-based methods and *temporal
    features* across frames-based ones. Whilst most of the methods based on temporal
    features use deep learning *recurrent* classification models, the methods use
    visual artifacts within video frame can be implemented by either deep or shallow
    classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Fake Image Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deepfakes are increasingly detrimental to privacy, society security and democracy
    [[71](#bib.bib71)]. Methods for detecting deepfakes have been proposed as soon
    as this threat was introduced. Early attempts were based on handcrafted features
    obtained from artifacts and inconsistencies of the fake image synthesis process.
    Recent methods, e.g., [[72](#bib.bib72), [73](#bib.bib73)], have commonly applied
    deep learning to automatically extract salient and discriminative features to
    detect deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Handcrafted Features-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most works on detection of GAN generated images do not consider the generalization
    capability of the detection models although the development of GAN is ongoing,
    and many new extensions of GAN are frequently introduced. Xuan et al. [[74](#bib.bib74)]
    used an image preprocessing step, e.g., Gaussian blur and Gaussian noise, to remove
    low level high frequency clues of GAN images. This increases the pixel level statistical
    similarity between real images and fake images and allows the forensic classifier
    to learn more intrinsic and meaningful features, which has better generalization
    capability than previous image forensics methods [[75](#bib.bib75), [76](#bib.bib76)]
    or image steganalysis networks [[77](#bib.bib77)].
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. [[78](#bib.bib78)] used the bag of words method to extract a set
    of compact features and fed it into various classifiers such as SVM [[79](#bib.bib79)],
    random forest (RF) [[80](#bib.bib80)] and multi-layer perceptrons (MLP) [[81](#bib.bib81)]
    for discriminating swapped face images from the genuine. Among deep learning-generated
    images, those synthesised by GAN models are probably most difficult to detect
    as they are realistic and high-quality based on GAN’s capability to learn distribution
    of the complex input data and generate new outputs with similar input distribution.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Agarwal and Varshney [[82](#bib.bib82)] cast the GAN-based
    deepfake detection as a hypothesis testing problem where a statistical framework
    was introduced using the information-theoretic study of authentication [[83](#bib.bib83)].
    The minimum distance between distributions of legitimate images and images generated
    by a particular GAN is defined, namely the oracle error. The analytic results
    show that this distance increases when the GAN is less accurate, and in this case,
    it is easier to detect deepfakes. In case of high-resolution image inputs, an
    extremely accurate GAN is required to generate fake images that are hard to detect
    by this method.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Deep Features-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Face swapping has a number of compelling applications in video compositing,
    transfiguration in portraits, and especially in identity protection as it can
    replace faces in photographs by ones from a collection of stock images. However,
    it is also one of the techniques that cyber attackers employ to penetrate identification
    or authentication systems to gain illegitimate access. The use of deep learning
    such as CNN and GAN has made swapped face images more challenging for forensics
    models as it can preserve pose, facial expression and lighting of the photographs
    [[84](#bib.bib84)].
  prefs: []
  type: TYPE_NORMAL
- en: Hsu et al. [[85](#bib.bib85)] introduced a two-phase deep learning method for
    detection of deepfake images. The first phase is a feature extractor based on
    the common fake feature network (CFFN) where the Siamese network architecture
    presented in [[86](#bib.bib86)] is used. The CFFN encompasses several dense units
    with each unit including different numbers of dense blocks [[61](#bib.bib61)]
    to improve the representative capability for the input images. Discriminative
    features between the fake and real images are extracted through the CFFN learning
    process based on the use of pairwise information, which is the label of each pair
    of two input images. If the two images are of the same type, i.e., fake-fake or
    real-real, the pairwise label is $1$. In contrast, if they are of different types,
    i.e., fake-real, the pairwise label is $0$. The CFFN-based discriminative features
    are then fed to a neural network classifier to distinguish deceptive images from
    genuine. The proposed method is validated for both fake face and fake general
    image detection. On the one hand, the face dataset is obtained from CelebA [[87](#bib.bib87)],
    containing 10,177 identities and 202,599 aligned face images of various poses
    and background clutter. Five GAN variants are used to generate fake images with
    size of 64x64, including deep convolutional GAN (DCGAN) [[88](#bib.bib88)], Wasserstein
    GAN (WGAN) [[89](#bib.bib89)], WGAN with gradient penalty (WGAN-GP) [[90](#bib.bib90)],
    least squares GAN [[91](#bib.bib91)], and PGGAN [[62](#bib.bib62)]. A total of
    385,198 training images and 10,000 test images of both real and fake ones are
    obtained for validating the proposed method. On the other hand, the general dataset
    is extracted from the ILSVRC12 [[92](#bib.bib92)]. The large scale GAN training
    model for high fidelity natural image synthesis (BIGGAN) [[93](#bib.bib93)], self-attention
    GAN [[94](#bib.bib94)] and spectral normalization GAN [[95](#bib.bib95)] are used
    to generate fake images with size of 128x128\. The training set consists of 600,000
    fake and real images whilst the test set includes 10,000 images of both types.
    Experimental results show the superior performance of the proposed method against
    its competing methods such as those introduced in [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, Guo et al. [[100](#bib.bib100)] proposed a CNN model, namely SCnet,
    to detect deepfake images, which are generated by the Glow-based facial forgery
    tool [[101](#bib.bib101)]. The fake images synthesized by the Glow model [[101](#bib.bib101)]
    have the facial expression maliciously tampered. These images are hyper-realistic
    with perfect visual qualities, but they still have subtle or noticeable manipulation
    traces, which are exploited by the SCnet. The SCnet is able to automatically learn
    high-level forensics features of image data thanks to a hierarchical feature extraction
    block, which is formed by stacking four convolutional layers. Each layer learns
    a new set of feature maps from the previous layer, with each convolutional operation
    is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{j}^{(n)}=\sum_{i=1}^{i}f_{i}^{(n-1)}*\omega_{ij}^{(n)}+b_{j}^{(n)}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{j}^{(n)}$ is the $j^{th}$ feature map of the $n^{th}$ layer, $\omega_{ij}^{(n)}$
    is the weight of the $i^{th}$ channel of the $j^{th}$ convolutional kernel in
    the $n^{th}$ layer, and $b_{j}^{(n)}$ is the bias term of the $j^{th}$ convolutional
    kernel in the $n^{th}$ layer. The proposed approach is evaluated using a dataset
    consisting of 321,378 face images, which are created by applying the Glow model
    [[101](#bib.bib101)] to the CelebA face image dataset [[87](#bib.bib87)]. Evaluation
    results show that the SCnet model obtains higher accuracy and better generalization
    than the Meso-4 model proposed in [[102](#bib.bib102)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1429dcf5780882393df19011f7ad3557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A two-step process for face manipulation detection where the preprocessing
    step aims to detect, crop and align faces on a sequence of frames and the second
    step distinguishes manipulated and authentic face images by combining convolutional
    neural network (CNN) and recurrent neural network (RNN) [[103](#bib.bib103)].'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Zhao et al. [[104](#bib.bib104)] proposed a method for deepfake detection
    using self-consistency of local source features, which are content-independent,
    spatially-local information of images. These features could come from either imaging
    pipelines, encoding methods or image synthesis approaches. The hypothesis is that
    a modified image would have different source features at different locations,
    while an original image will have the same source features across locations. These
    source features, represented in the form of down-sampled feature maps, are extracted
    by a CNN model using a special representation learning method called pairwise
    self-consistency learning. This learning method aims to penalize pairs of feature
    vectors that refer to locations from the same image for having a low cosine similarity
    score. At the same time, it also penalizes the pairs from different images for
    having a high similarity score. The learned feature maps are then fed to a classification
    method for deepfake detection. This proposed approach is evaluated on seven popular
    datasets, including FaceForensics++ [[105](#bib.bib105)], DeepfakeDetection [[106](#bib.bib106)],
    Celeb-DF-v1 & Celeb-DF-v2 [[107](#bib.bib107)], Deepfake Detection Challenge (DFDC)
    [[108](#bib.bib108)], DFDC Preview [[109](#bib.bib109)], and DeeperForensics-1.0
    [[110](#bib.bib110)]. Experimental results demonstrate that the proposed approach
    is superior to state-of-the-art methods. It however may have a limitation when
    dealing with fake images that are generated by methods that directly output the
    whole images whose source features are consistent across all positions within
    each image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Fake Video Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most image detection methods cannot be used for videos because of the strong
    degradation of the frame data after video compression [[102](#bib.bib102)]. Furthermore,
    videos have temporal characteristics that are varied among sets of frames and
    they are thus challenging for methods designed to detect only still fake images.
    This subsection focuses on deepfake video detection methods and categorizes them
    into two smaller groups: methods that employ temporal features and those that
    explore visual artifacts within frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Temporal Features across Video Frames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the observation that temporal coherence is not enforced effectively
    in the synthesis process of deepfakes, Sabir et al. [[103](#bib.bib103)] leveraged
    the use of spatio-temporal features of video streams to detect deepfakes. Video
    manipulation is carried out on a frame-by-frame basis so that low level artifacts
    produced by face manipulations are believed to further manifest themselves as
    temporal artifacts with inconsistencies across frames. A recurrent convolutional
    model (RCN) was proposed based on the integration of the convolutional network
    DenseNet [[61](#bib.bib61)] and the gated recurrent unit cells [[111](#bib.bib111)]
    to exploit temporal discrepancies across frames (see Fig. [6](#S3.F6 "Figure 6
    ‣ 3.1.2 Deep Features-based Methods ‣ 3.1 Fake Image Detection ‣ 3 Deepfake Detection
    ‣ Deep Learning for Deepfakes Creation and Detection: A Survey")). The proposed
    method is tested on the FaceForensics++ dataset, which includes 1,000 videos [[105](#bib.bib105)],
    and shows promising results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, Güera and Delp [[112](#bib.bib112)] highlighted that deepfake videos
    contain intra-frame inconsistencies and temporal inconsistencies between frames.
    They then proposed the temporal-aware pipeline method that uses CNN and long short
    term memory (LSTM) to detect deepfake videos. CNN is employed to extract frame-level
    features, which are then fed into the LSTM to create a temporal sequence descriptor.
    A fully-connected network is finally used for classifying doctored videos from
    real ones based on the sequence descriptor as illustrated in Fig. [7](#S3.F7 "Figure
    7 ‣ 3.2.1 Temporal Features across Video Frames ‣ 3.2 Fake Video Detection ‣ 3
    Deepfake Detection ‣ Deep Learning for Deepfakes Creation and Detection: A Survey").
    An accuracy of greater than 97% was obtained using a dataset of 600 videos, including
    300 deepfake videos collected from multiple video-hosting websites and 300 pristine
    videos randomly selected from the Hollywood human actions dataset in [[113](#bib.bib113)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e01a1e6ea1cb5d8f433adf89204f0344.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A deepfake detection method using convolutional neural network (CNN)
    and long short term memory (LSTM) to extract temporal features of a given video
    sequence, which are represented via the sequence descriptor. The detection network
    consisting of fully-connected layers is employed to take the sequence descriptor
    as input and calculate probabilities of the frame sequence belonging to either
    authentic or deepfake class [[112](#bib.bib112)].'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the use of a physiological signal, eye blinking, to detect
    deepfakes was proposed in Li et al. [[114](#bib.bib114)] based on the observation
    that a person in deepfakes has a lot less frequent blinking than that in untampered
    videos. A healthy adult human would normally blink somewhere between 2 to 10 seconds,
    and each blink would take 0.1 and 0.4 seconds. Deepfake algorithms, however, often
    use face images available online for training, which normally show people with
    open eyes, i.e., very few images published on the internet show people with closed
    eyes. Thus, without having access to images of people blinking, deepfake algorithms
    do not have the capability to generate fake faces that can blink normally. In
    other words, blinking rates in deepfakes are much lower than those in normal videos.
    To discriminate real and fake videos, Li et al. [[114](#bib.bib114)] crop eye
    areas in the videos and distribute them into long-term recurrent convolutional
    networks (LRCN) [[115](#bib.bib115)] for dynamic state prediction. The LRCN consists
    of a feature extractor based on CNN, a sequence learning based on long short term
    memory (LSTM), and a state prediction based on a fully connected layer to predict
    probability of eye open and close state. The eye blinking shows strong temporal
    dependencies and thus the implementation of LSTM helps to capture these temporal
    patterns effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Caldelli et al. [[116](#bib.bib116)] proposed the use of optical flow
    to gauge the information along the temporal axis of a frame sequence for video
    deepfake detection. The optical flow is a vector field calculated on two temporal-distinct
    frames of a video that can describe the movement of objects in a scene. The optical
    flow fields are expected to be different between synthetically created frames
    and naturally generated ones [[117](#bib.bib117)]. Unnatural movements of lips,
    eyes, or of the entire faces inserted into deepfake videos would introduce distinctive
    motion patterns when compared with pristine ones. Based on this assumption, features
    consisting of optical flow fields are fed into a CNN model for discriminating
    between deepfakes and original videos. More specifically, the ResNet50 architecture
    [[118](#bib.bib118)] is implemented as a CNN model for experiments. The results
    obtained using the FaceForensics++ dataset [[105](#bib.bib105)] show that this
    approach is comparable with state-of-the-art methods in terms of classification
    accuracy. A combination of this kind of feature with frame-based features is also
    experimented, which results in an improved deepfake detection performance. This
    demonstrates the usefulness of optical flow fields in capturing the inconsistencies
    on the temporal axis of video frames for deepfake detection.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Visual Artifacts within Video Frame
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As can be noticed in the previous subsection, the methods using temporal patterns
    across video frames are mostly based on deep recurrent network models to detect
    deepfake videos. This subsection investigates the other approach that normally
    decomposes videos into frames and explores visual artifacts within single frames
    to obtain discriminant features. These features are then distributed into either
    a deep or shallow classifier to differentiate between fake and authentic videos.
    We thus group methods in this subsection based on the types of classifiers, i.e.
    either deep or shallow.
  prefs: []
  type: TYPE_NORMAL
- en: Deep classifiers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Deepfake videos are normally created with limited resolutions, which require
    an affine face warping approach (i.e., scaling, rotation and shearing) to match
    the configuration of the original ones. Because of the resolution inconsistency
    between the warped face area and the surrounding context, this process leaves
    artifacts that can be detected by CNN models such as VGG16 [[119](#bib.bib119)],
    ResNet50, ResNet101 and ResNet152 [[118](#bib.bib118)]. A deep learning method
    to detect deepfakes based on the artifacts observed during the face warping step
    of the deepfake generation algorithms was proposed in [[120](#bib.bib120)]. The
    proposed method is evaluated on two deepfake datasets, namely the UADFV and DeepfakeTIMIT.
    The UADFV dataset [[121](#bib.bib121)] contains 49 real videos and 49 fake videos
    with 32,752 frames in total. The DeepfakeTIMIT dataset [[69](#bib.bib69)] includes
    a set of low quality videos of 64 x 64 size and another set of high quality videos
    of 128 x 128 with totally 10,537 pristine images and 34,023 fabricated images
    extracted from 320 videos for each quality set. Performance of the proposed method
    is compared with other prevalent methods such as two deepfake detection MesoNet
    methods, i.e. Meso-4 and MesoInception-4 [[102](#bib.bib102)], HeadPose [[121](#bib.bib121)],
    and the face tampering detection method two-stream NN [[122](#bib.bib122)]. Advantage
    of the proposed method is that it needs not to generate deepfake videos as negative
    examples before training the detection models. Instead, the negative examples
    are generated dynamically by extracting the face region of the original image
    and aligning it into multiple scales before applying Gaussian blur to a scaled
    image of random pick and warping back to the original image. This reduces a large
    amount of time and computational resources compared to other methods, which require
    deepfakes are generated in advance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nguyen et al. [[123](#bib.bib123)] proposed the use of capsule networks for
    detecting manipulated images and videos. The capsule network was initially introduced
    to address limitations of CNNs when applied to inverse graphics tasks, which aim
    to find physical processes used to produce images of the world [[124](#bib.bib124)].
    The recent development of capsule network based on dynamic routing algorithm [[125](#bib.bib125)]
    demonstrates its ability to describe the hierarchical pose relationships between
    object parts. This development is employed as a component in a pipeline for detecting
    fabricated images and videos as demonstrated in Fig. [8](#S3.F8 "Figure 8 ‣ Deep
    classifiers ‣ 3.2.2 Visual Artifacts within Video Frame ‣ 3.2 Fake Video Detection
    ‣ 3 Deepfake Detection ‣ Deep Learning for Deepfakes Creation and Detection: A
    Survey"). A dynamic routing algorithm is deployed to route the outputs of the
    three capsules to the output capsules through a number of iterations to separate
    between fake and real images. The method is evaluated through four datasets covering
    a wide range of forged image and video attacks. They include the well-known Idiap
    Research Institute replay-attack dataset [[126](#bib.bib126)], the deepfake face
    swapping dataset created by Afchar et al. [[102](#bib.bib102)], the facial reenactment
    FaceForensics dataset [[127](#bib.bib127)], produced by the Face2Face method [[52](#bib.bib52)],
    and the fully computer-generated image dataset generated by Rahmouni et al. [[128](#bib.bib128)].
    The proposed method yields the best performance compared to its competing methods
    in all of these datasets. This shows the potential of the capsule network in building
    a general detection system that can work effectively for various forged image
    and video attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87549fa482a980002ce51113a4973ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Capsule network takes features obtained from the VGG-19 network [[119](#bib.bib119)]
    to distinguish fake images or videos from the real ones (top). The pre-processing
    step detects face region and scales it to the size of 128x128 before VGG-19 is
    used to extract latent features for the capsule network, which comprises three
    primary capsules and two output capsules, one for real and one for fake images
    (bottom). The statistical pooling constitutes an important part of capsule network
    that deals with forgery detection [[123](#bib.bib123)].'
  prefs: []
  type: TYPE_NORMAL
- en: Shallow classifiers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Deepfake detection methods mostly rely on the artifacts or inconsistency of
    intrinsic features between fake and real images or videos. Yang et al. [[121](#bib.bib121)]
    proposed a detection method by observing the differences between 3D head poses
    comprising head orientation and position, which are estimated based on 68 facial
    landmarks of the central face region. The 3D head poses are examined because there
    is a shortcoming in the deepfake face generation pipeline. The extracted features
    are fed into an SVM classifier to obtain the detection results. Experiments on
    two datasets show the great performance of the proposed approach against its competing
    methods. The first dataset, namely UADFV, consists of 49 deep fake videos and
    their respective real videos [[121](#bib.bib121)]. The second dataset comprises
    241 real images and 252 deep fake images, which is a subset of data used in the
    DARPA MediFor GAN Image/Video Challenge [[129](#bib.bib129)]. Likewise, a method
    to exploit artifacts of deepfakes and face manipulations based on visual features
    of eyes, teeth and facial contours was studied in [[130](#bib.bib130)]. The visual
    artifacts arise from lacking global consistency, wrong or imprecise estimation
    of the incident illumination, or imprecise estimation of the underlying geometry.
    For deepfakes detection, missing reflections and missing details in the eye and
    teeth areas are exploited as well as texture features extracted from the facial
    region based on facial landmarks. Accordingly, the eye feature vector, teeth feature
    vector and features extracted from the full-face crop are used. After extracting
    the features, two classifiers including logistic regression and small neural network
    are employed to classify the deepfakes from real videos. Experiments carried out
    on a video dataset downloaded from YouTube show the best result of 0.851 in terms
    of the area under the receiver operating characteristics curve. The proposed method
    however has a disadvantage that requires images meeting certain prerequisite such
    as open eyes or visual teeth.
  prefs: []
  type: TYPE_NORMAL
- en: The use of photo response non uniformity (PRNU) analysis was proposed in [[131](#bib.bib131)]
    to detect deepfakes from authentic ones. PRNU is a component of sensor pattern
    noise, which is attributed to the manufacturing imperfection of silicon wafers
    and the inconsistent sensitivity of pixels to light because of the variation of
    the physical characteristics of the silicon wafers. The PRNU analysis is widely
    used in image forensics [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135), [136](#bib.bib136)] and advocated to use in [[131](#bib.bib131)]
    because the swapped face is supposed to alter the local PRNU pattern in the facial
    area of video frames. The videos are converted into frames, which are cropped
    to the questioned facial region. The cropped frames are then separated sequentially
    into eight groups where an average PRNU pattern is computed for each group. Normalised
    cross correlation scores are calculated for comparisons of PRNU patterns among
    these groups. A test dataset was created, consisting of 10 authentic videos and
    16 manipulated videos, where the fake videos were produced from the genuine ones
    by the DeepFaceLab tool [[38](#bib.bib38)]. The analysis shows a significant statistical
    difference in terms of mean normalised cross correlation scores between deepfakes
    and the genuine. This analysis therefore suggests that PRNU has a potential in
    deepfake detection although a larger dataset would need to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: When seeing a video or image with suspicion, users normally want to search for
    its origin. However, there is currently no feasibility for such a tool. Hasan
    and Salah [[137](#bib.bib137)] proposed the use of blockchain and smart contracts
    to help users detect deepfake videos based on the assumption that videos are only
    real when their sources are traceable. Each video is associated with a smart contract
    that links to its parent video and each parent video has a link to its child in
    a hierarchical structure. Through this chain, users can credibly trace back to
    the original smart contract associated with pristine video even if the video has
    been copied multiple times. An important attribute of the smart contract is the
    unique hashes of the interplanetary file system, which is used to store video
    and its metadata in a decentralized and content-addressable manner [[138](#bib.bib138)].
    The smart contract’s key features and functionalities are tested against several
    common security challenges such as distributed denial of services, replay and
    man in the middle attacks to ensure the solution meeting security requirements.
    This approach is generic, and it can be extended to other types of digital content,
    e.g., images, audios and manuscripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of prominent deepfake detection methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Classifiers/ Techniques | Key Features | Dealing with | Datasets
    Used |'
  prefs: []
  type: TYPE_TB
- en: '| Eye blinking [[114](#bib.bib114)] | LRCN | - Use LRCN to learn the temporal
    patterns of eye blinking. - Based on the observation that blinking frequency of
    deepfakes is much smaller than normal. | Videos | Consist of 49 interview and
    presentation videos, and their corresponding generated deepfakes. |'
  prefs: []
  type: TYPE_TB
- en: '| Intra-frame and temporal inconsistencies [[112](#bib.bib112)] | CNN and LSTM
    | CNN is employed to extract frame-level features, which are distributed to LSTM
    to construct sequence descriptor useful for classification. | Videos | A collection
    of 600 videos obtained from multiple websites. |'
  prefs: []
  type: TYPE_TB
- en: '| Using face warping artifacts [[120](#bib.bib120)] | VGG16 [[119](#bib.bib119)],
    ResNet models [[118](#bib.bib118)] | Artifacts are discovered using CNN models
    based on resolution inconsistency between the warped face area and the surrounding
    context. | Videos | - UADFV [[121](#bib.bib121)], containing 49 real videos and
    49 fake videos with 32752 frames in total. - DeepfakeTIMIT [[69](#bib.bib69)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MesoNet [[102](#bib.bib102)] | CNN | - Two deep networks, i.e. Meso-4 and
    MesoInception-4 are introduced to examine deepfake videos at the mesoscopic analysis
    level. - Accuracy obtained on deepfake and FaceForensics datasets are 98% and
    95% respectively. | Videos | Two datasets: deepfake one constituted from online
    videos and the FaceForensics one created by the Face2Face approach [[52](#bib.bib52)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Eye, teach and facial texture [[130](#bib.bib130)] | Logistic regression
    and neural network (NN) | - Exploit facial texture differences, and missing reflections
    and details in eye and teeth areas of deepfakes. - Logistic regression and NN
    are used for classifying. | Videos | A video dataset downloaded from YouTube.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spatio-temporal features with RCN [[103](#bib.bib103)] | RCN | Temporal discrepancies
    across frames are explored using RCN that integrates convolutional network DenseNet
    [[61](#bib.bib61)] and the gated recurrent unit cells [[111](#bib.bib111)] | Videos
    | FaceForensics++ dataset, including 1,000 videos [[105](#bib.bib105)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Spatio-temporal features with LSTM [[139](#bib.bib139)] | Convolutional bidirectional
    recurrent LSTM network | - An XceptionNet CNN is used for facial feature extraction
    while audio embeddings are obtained by stacking multiple convolution modules.
    - Two loss functions, i.e. cross-entropy and Kullback-Leibler divergence, are
    used. | Videos | FaceForensics++ [[105](#bib.bib105)] and Celeb-DF (5,639 deepfake
    videos) [[107](#bib.bib107)] datasets and the ASVSpoof 2019 Logical Access audio
    dataset [[140](#bib.bib140)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Analysis of PRNU [[131](#bib.bib131)] | PRNU | - Analysis of noise patterns
    of light sensitive sensors of digital cameras due to their factory defects. -
    Explore the differences of PRNU patterns between the authentic and deepfake videos
    because face swapping is believed to alter the local PRNU patterns. | Videos |
    Created by the authors, including 10 authentic and 16 deepfake videos using DeepFaceLab
    [[38](#bib.bib38)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Phoneme-viseme mismatches [[141](#bib.bib141)] | CNN | - Exploit the mismatches
    between the dynamics of the mouth shape, i.e. visemes, with a spoken phoneme.
    - Focus on sounds associated with the M, B and P phonemes as they require complete
    mouth closure while deepfakes often incorrectly synthesize it. | Videos | Four
    in-the-wild lip-sync deepfakes from Instagram and YouTube (www.instagram.com/bill_posters_uk
    and youtu.be/VWMEDacz3L4) and others are created using synthesis techniques, i.e.
    Audio-to-Video (A2V) [[68](#bib.bib68)] and Text-to-Video (T2V) [[142](#bib.bib142)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Using attribution-based confidence (ABC) metric [[143](#bib.bib143)] | ResNet50
    model [[118](#bib.bib118)], pre-trained on VGGFace2 [[144](#bib.bib144)] | - The
    ABC metric [[145](#bib.bib145)] is used to detect deepfake videos without accessing
    to training data. - ABC values obtained for original videos are greater than 0.94
    while those of deepfakes have low ABC values. | Videos | VidTIMIT and two other
    original datasets obtained from the COHFACE (https://www.idiap.ch/dataset/cohface)
    and from YouTube. datasets from COHFACE [[146](#bib.bib146)] and YouTube are used
    to generate two deepfake datasets by commercial website https://deepfakesweb.com
    and another deepfake dataset is DeepfakeTIMIT [[147](#bib.bib147)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Using appearance and behaviour [[148](#bib.bib148)] | Rules based on facial
    and behavioural features. | Temporal, behavioral biometric based on facial expressions
    and head movements are learned using ResNet-101 [[118](#bib.bib118)] while static
    facial biometric is obtained using VGG [[65](#bib.bib65)]. | Videos | The world
    leaders dataset [[1](#bib.bib1)], FaceForensics++ [[105](#bib.bib105)], Google/Jigsaw
    deepfake detection dataset [[106](#bib.bib106)], DFDC [[109](#bib.bib109)] and
    Celeb-DF [[107](#bib.bib107)]. |'
  prefs: []
  type: TYPE_TB
- en: '| FakeCatcher [[149](#bib.bib149)] | CNN | Extract biological signals in portrait
    videos and use them as an implicit descriptor of authenticity because they are
    not spatially and temporally well-preserved in deepfakes. | Videos | UADFV [[121](#bib.bib121)],
    FaceForensics [[127](#bib.bib127)], FaceForensics++ [[105](#bib.bib105)], Celeb-DF
    [[107](#bib.bib107)], and a new dataset of 142 videos, independent of the generative
    model, resolution, compression, content, and context. |'
  prefs: []
  type: TYPE_TB
- en: '| Emotion audio-visual affective cues [[150](#bib.bib150)] | Siamese network
    [[86](#bib.bib86)] | Modality and emotion embedding vectors for the face and speech
    are extracted for deepfake detection. | Videos | DeepfakeTIMIT [[147](#bib.bib147)]
    and DFDC [[109](#bib.bib109)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Head poses [[121](#bib.bib121)] | SVM | - Features are extracted using 68
    landmarks of the face region. - Use SVM to classify using the extracted features.
    | Videos/ Images | - UADFV consists of 49 deep fake videos and their respective
    real videos. - 241 real images and 252 deep fake images from DARPA MediFor GAN
    Image/Video Challenge. |'
  prefs: []
  type: TYPE_TB
- en: '| Capsule-forensics [[123](#bib.bib123)] | Capsule networks | - Latent features
    extracted by VGG-19 network [[119](#bib.bib119)] are fed into the capsule network
    for classification. - A dynamic routing algorithm [[125](#bib.bib125)] is used
    to route the outputs of three convolutional capsules to two output capsules, one
    for fake and another for real images, through a number of iterations. | Videos/
    Images | Four datasets: the Idiap Research Institute replay-attack [[126](#bib.bib126)],
    deepfake face swapping by [[102](#bib.bib102)], facial reenactment FaceForensics
    [[127](#bib.bib127)], and fully computer-generated image set using [[128](#bib.bib128)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | Classifiers/ Techniques | Key Features | Dealing with | Datasets
    Used |'
  prefs: []
  type: TYPE_TB
- en: '| Preprocessing combined with deep network [[74](#bib.bib74)] | DCGAN, WGAN-GP
    and PGGAN. | - Enhance generalization ability of deep learning models to detect
    GAN generated images. - Remove low level features of fake images.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Force deep networks to focus more on pixel level similarity between fake
    and real images to improve generalization ability. | Images | - Real dataset:
    CelebA-HQ [[62](#bib.bib62)], including high quality face images of 1024x1024
    resolution. - Fake datasets: generated by DCGAN [[88](#bib.bib88)], WGAN-GP [[90](#bib.bib90)]
    and PGGAN [[62](#bib.bib62)]. |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Analyzing convolutional traces [[151](#bib.bib151)] | KNN, SVM, and linear
    discriminant analysis (LDA) | Using expectation-maximization algorithm to extract
    local features pertaining to convolutional generative process of GAN-based image
    deepfake generators. | Images | Authentic images from CelebA and corresponding
    deepfakes are created by five different GANs (group-wise deep whitening-and-coloring
    transformation GDWCT [[152](#bib.bib152)], StarGAN [[153](#bib.bib153)], AttGAN
    [[154](#bib.bib154)], StyleGAN [[51](#bib.bib51)], StyleGAN2 [[155](#bib.bib155)]).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bag of words and shallow classifiers [[78](#bib.bib78)] | SVM, RF, MLP |
    Extract discriminant features using bag of words method and feed these features
    into SVM, RF and MLP for binary classification: innocent vs fabricated. | Images
    | The well-known LFW face database [[156](#bib.bib156)], containing 13,223 images
    with resolution of 250x250. |'
  prefs: []
  type: TYPE_TB
- en: '| Pairwise learning [[85](#bib.bib85)] | CNN concatenated to CFFN | Two-phase
    procedure: feature extraction using CFFN based on the Siamese network architecture
    [[86](#bib.bib86)] and classification using CNN. | Images | - Face images: real
    ones from CelebA [[87](#bib.bib87)], and fake ones generated by DCGAN [[88](#bib.bib88)],
    WGAN [[89](#bib.bib89)], WGAN-GP [[90](#bib.bib90)], least squares GAN [[91](#bib.bib91)],
    and PGGAN [[62](#bib.bib62)]. - General images: real ones from ILSVRC12 [[92](#bib.bib92)],
    and fake ones generated by BIGGAN [[93](#bib.bib93)], self-attention GAN [[94](#bib.bib94)]
    and spectral normalization GAN [[95](#bib.bib95)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Defenses against adversarial perturbations in deepfakes [[157](#bib.bib157)]
    | VGG [[65](#bib.bib65)] and ResNet [[118](#bib.bib118)] | - Introduce adversarial
    perturbations to enhance deepfakes and fool deepfake detectors. - Improve accuracy
    of deepfake detectors using Lipschitz regularization and deep image prior techniques.
    | Images | 5,000 real images from CelebA [[87](#bib.bib87)] and 5,000 fake images
    created by the “Few-Shot Face Translation GAN” method [[158](#bib.bib158)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Face X-ray [[159](#bib.bib159)] | CNN | - Try to locate the blending boundary
    between the target and original faces instead of capturing the synthesized artifacts
    of specific manipulations. - Can be trained without fake images. | Images | FaceForensics++
    [[105](#bib.bib105)], DeepfakeDetection (DFD) [[106](#bib.bib106)], DFDC [[109](#bib.bib109)]
    and Celeb-DF [[107](#bib.bib107)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Using common artifacts of CNN-generated images [[160](#bib.bib160)] | ResNet-50
    [[118](#bib.bib118)] pre-trained with ImageNet [[92](#bib.bib92)] | Train the
    classifier using a large number of fake images generated by a high-performing
    unconditional GAN model, i.e., PGGAN [[62](#bib.bib62)] and evaluate how well
    the classifier generalizes to other CNN-synthesized images. | Images | A new dataset
    of CNN-generated images, namely ForenSynths, consisting of synthesized images
    from 11 models such as StyleGAN [[51](#bib.bib51)], super-resolution methods [[161](#bib.bib161)]
    and FaceForensics++ [[105](#bib.bib105)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Using convolutional traces on GAN-based images [[162](#bib.bib162)] | KNN,
    SVM, and LDA | Training the expectation-maximization algorithm [[163](#bib.bib163)]
    to detect and extract discriminative features via a fingerprint that represents
    the convolutional traces left by GANs during image generation. | Images | A dataset
    of images generated by ten GAN models, including CycleGAN [[164](#bib.bib164)],
    StarGAN [[153](#bib.bib153)], AttGAN [[154](#bib.bib154)], GDWCT [[152](#bib.bib152)],
    StyleGAN [[51](#bib.bib51)], StyleGAN2 [[155](#bib.bib155)], PGGAN [[62](#bib.bib62)],
    FaceForensics++ [[105](#bib.bib105)], IMLE [[165](#bib.bib165)], and SPADE [[42](#bib.bib42)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Using deep features extracted by CNN [[100](#bib.bib100)] | A new CNN model,
    namely SCnet | The CNN-based SCnet is able to automatically learn high-level forensics
    features of image data thanks to a hierarchical feature extraction block, which
    is formed by stacking four convolutional layers. | Images | A dataset of 321,378
    face images, created by applying the Glow model [[101](#bib.bib101)] to the CelebA
    face image dataset [[87](#bib.bib87)]. |'
  prefs: []
  type: TYPE_TB
- en: 4 Discussions and Future Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the support of deep learning, deepfakes can be created easier than ever
    before. The spread of these fake contents is also quicker thanks to the development
    of social media platforms [[166](#bib.bib166)]. Sometimes deepfakes do not need
    to be spread to massive audience to cause detrimental effects. People who create
    deepfakes with malicious purpose only need to deliver them to target audiences
    as part of their sabotage strategy without using social media. For example, this
    approach can be utilized by intelligence services trying to influence decisions
    made by important people such as politicians, leading to national and international
    security threats [[167](#bib.bib167)]. Catching the deepfake alarming problem,
    research community has focused on developing deepfake detection algorithms and
    numerous results have been reported. This paper has reviewed the state-of-the-art
    methods and a summary of typical approaches is provided in Table [2](#S3.T2 "Table
    2 ‣ Shallow classifiers ‣ 3.2.2 Visual Artifacts within Video Frame ‣ 3.2 Fake
    Video Detection ‣ 3 Deepfake Detection ‣ Deep Learning for Deepfakes Creation
    and Detection: A Survey"). It is noticeable that a battle between those who use
    advanced machine learning to create deepfakes with those who make effort to detect
    deepfakes is growing.'
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes’ quality has been increasing and the performance of detection methods
    needs to be improved accordingly. The inspiration is that what AI has broken can
    be fixed by AI as well [[168](#bib.bib168)]. Detection methods are still in their
    early stage and various methods have been proposed and evaluated but using fragmented
    datasets. An approach to improve performance of detection methods is to create
    a growing updated benchmark dataset of deepfakes to validate the ongoing development
    of detection methods. This will facilitate the training process of detection models,
    especially those based on deep learning, which requires a large training set [[108](#bib.bib108)].
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance of deepfake detection methods is important, especially
    in cross-forgery and cross-dataset scenarios. Most detection models are designed
    and evaluated in the same-forgery and in-dataset experiments, which do not ensure
    their generalization capability. Some previous studies have addressed this issue,
    e.g., in [[160](#bib.bib160), [116](#bib.bib116), [104](#bib.bib104), [169](#bib.bib169),
    [170](#bib.bib170)], but more work needs to be done in this direction. A model
    trained on a specific forgery needs to be able to work against another unknown
    one because potential deepfake types are not normally known in the real-world
    scenarios. Likewise, current detection methods mostly focus on drawbacks of the
    deepfake generation pipelines, i.e., finding weakness of the competitors to attack
    them. This kind of information and knowledge is not always available in adversarial
    environments where attackers commonly attempt not to reveal such deepfake creation
    technologies. Recent works on adversarial perturbation attacks to fool DNN-based
    detectors make the deepfake detection task more difficult [[157](#bib.bib157),
    [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174)].
    These are real challenges for detection method development and a future study
    needs to focus on introducing more robust, scalable and generalizable methods.
  prefs: []
  type: TYPE_NORMAL
- en: Another research direction is to integrate detection methods into distribution
    platforms such as social media to increase its effectiveness in dealing with the
    widespread impact of deepfakes. The screening or filtering mechanism using effective
    detection methods can be implemented on these platforms to ease the deepfakes
    detection [[167](#bib.bib167)]. Legal requirements can be made for tech companies
    who own these platforms to remove deepfakes quickly to reduce its impacts. In
    addition, watermarking tools can also be integrated into devices that people use
    to make digital contents to create immutable metadata for storing originality
    details such as time and location of multimedia contents as well as their untampered
    attestment [[167](#bib.bib167)]. This integration is difficult to implement but
    a solution for this could be the use of the disruptive blockchain technology.
    The blockchain has been used effectively in many areas and there are very few
    studies so far addressing the deepfake detection problems based on this technology.
    As it can create a chain of unique unchangeable blocks of metadata, it is a great
    tool for digital provenance solution. The integration of blockchain technologies
    to this problem has demonstrated certain results [[137](#bib.bib137)] but this
    research direction is far from mature.
  prefs: []
  type: TYPE_NORMAL
- en: Using detection methods to spot deepfakes is crucial, but understanding the
    real intent of people publishing deepfakes is even more important. This requires
    the judgement of users based on social context in which deepfake is discovered,
    e.g. who distributed it and what they said about it [[175](#bib.bib175)]. This
    is critical as deepfakes are getting more and more photorealistic and it is highly
    anticipated that detection software will be lagging behind deepfake creation technology.
    A study on social context of deepfakes to assist users in such judgement is thus
    worth performing.
  prefs: []
  type: TYPE_NORMAL
- en: Videos and photographics have been widely used as evidences in police investigation
    and justice cases. They may be introduced as evidences in a court of law by digital
    media forensics experts who have background in computer or law enforcement and
    experience in collecting, examining and analysing digital information. The development
    of machine learning and AI technologies might have been used to modify these digital
    contents and thus the experts’ opinions may not be enough to authenticate these
    evidences because even experts are unable to discern manipulated contents. This
    aspect needs to take into account in courtrooms nowadays when images and videos
    are used as evidences to convict perpetrators because of the existence of a wide
    range of digital manipulation methods [[176](#bib.bib176)]. The digital media
    forensics results therefore must be proved to be valid and reliable before they
    can be used in courts. This requires careful documentation for each step of the
    forensics process and how the results are reached. Machine learning and AI algorithms
    can be used to support the determination of the authenticity of digital media
    and have obtained accurate and reliable results, e.g., [[177](#bib.bib177), [178](#bib.bib178)],
    but most of these algorithms are unexplainable. This creates a huge hurdle for
    the applications of AI in forensics problems because not only the forensics experts
    oftentimes do not have expertise in computer algorithms, but the computer professionals
    also cannot explain the results properly as most of these algorithms are black
    box models [[179](#bib.bib179)]. This is more critical as the most recent models
    with the most accurate results are based on deep learning methods consisting of
    many neural network parameters. Researchers have recently attempted to create
    white box and explainable detection methods. An example is the approach proposed
    by Giudice et al. [[180](#bib.bib180)] in which they use discrete cosine transform
    statistics to detect so-called specific GAN frequencies to differentiate between
    real images and deepfakes. Through the analysis of particular frequency statistics,
    that method can be used to mathematically explain whether a multimedia content
    is a deepfake and why it is. More research must be conducted in this area and
    explainable AI in computer vision therefore is a research direction that is needed
    to promote and utilize the advances and advantages of AI and machine learning
    in digital media forensics.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deepfakes have begun to erode trust of people in media contents as seeing them
    is no longer commensurate with believing in them. They could cause distress and
    negative effects to those targeted, heighten disinformation and hate speech, and
    even could stimulate political tension, inflame the public, violence or war. This
    is especially critical nowadays as the technologies for creating deepfakes are
    increasingly approachable and social media platforms can spread those fake contents
    quickly. This survey provides a timely overview of deepfake creation and detection
    methods and presents a broad discussion on challenges, potential trends, and future
    directions in this area. This study therefore will be valuable for the artificial
    intelligence research community to develop effective methods for tackling deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: Declaration of Competing Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authors declare no conflict of interest.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agarwal et al. [2019] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki
    Nagano, and Hao Li. Protecting world leaders against deep fakes. In *Computer
    Vision and Pattern Recognition Workshops*, volume 1, pages 38–45, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent et al. [2008] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine
    Manzagol. Extracting and composing robust features with denoising autoencoders.
    In *Proceedings of the 25th International Conference on Machine learning*, pages
    1096–1103, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational
    Bayes. *arXiv preprint arXiv:1312.6114*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in Neural Information Processing Systems*, 27:2672–2680,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makhzani et al. [2015] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian
    Goodfellow, and Brendan Frey. Adversarial autoencoders. *arXiv preprint arXiv:1511.05644*,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tewari et al. [2018] Ayush Tewari, Michael Zollhoefer, Florian Bernard, Pablo
    Garrido, Hyeongwoo Kim, Patrick Perez, and Christian Theobalt. High-fidelity monocular
    face reconstruction based on an unsupervised model-based face autoencoder. *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, 42(2):357–370, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2021] Jiacheng Lin, Yang Li, and Guanci Yang. FPGAN: Face de-identification
    method with generative adversarial networks for social robots. *Neural Networks*,
    133:132–147, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021] Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, and Arun
    Mallya. Generative adversarial networks for image and video synthesis: Algorithms
    and applications. *Proceedings of the IEEE*, 109(5):839–862, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu [2018] Siwei Lyu. Detecting ’deepfake’ videos in the blink of an eye. [http://theconversation.com/detecting-deepfake-videos-in-the-blink-of-an-eye-101072](http://theconversation.com/detecting-deepfake-videos-in-the-blink-of-an-eye-101072),
    August 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bloomberg [2018] Bloomberg. How faking videos became easy and why that’s so
    scary. [https://fortune.com/2018/09/11/deep-fakes-obama-video/](https://fortune.com/2018/09/11/deep-fakes-obama-video/),
    September 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chesney and Citron [2019] Robert Chesney and Danielle Citron. Deepfakes and
    the new disinformation war: The coming age of post-truth geopolitics. *Foreign
    Affairs*, 98:147, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hwang [2020] T. Hwang. Deepfakes: A grounded threat assessment. Technical report,
    Centre for Security and Emerging Technologies, Georgetown University, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou and Zafarani [2020] Xinyi Zhou and Reza Zafarani. A survey of fake news:
    Fundamental theories, detection methods, and opportunities. *ACM Computing Surveys
    (CSUR)*, 53(5):1–40, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliyar et al. [2021] Rohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang.
    Deepfake: improving fake news detection using tensor decomposition-based deep
    neural network. *The Journal of Supercomputing*, 77(2):1015–1037, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2020] Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, and Zhiwen Yu.
    The future of false information detection on social media: New perspectives and
    trends. *ACM Computing Surveys (CSUR)*, 53(4):1–36, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tucker [2019] Patrick Tucker. The newest AI-enabled weapon: ‘deep-faking’ photos
    of the earth. [https://www.defenseone.com/technology/2019/03/next-phase-ai-deep-faking-whole-world-and-china-ahead/155944/](https://www.defenseone.com/technology/2019/03/next-phase-ai-deep-faking-whole-world-and-china-ahead/155944/),
    March 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fish [2019] T Fish. Deep fakes: AI-manipulated media will be ‘weaponised’ to
    trick military. [https://www.express.co.uk/news/science/1109783/deep-fakes-ai-artificial-intelligence-photos-video-weaponised-china](https://www.express.co.uk/news/science/1109783/deep-fakes-ai-artificial-intelligence-photos-video-weaponised-china),
    April 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marr [2019] B Marr. The best (and scariest) examples of AI-enabled deepfakes.
    [https://www.forbes.com/sites/bernardmarr/2019/07/22/the-best-and-scariest-examples-of-ai-enabled-deepfakes/](https://www.forbes.com/sites/bernardmarr/2019/07/22/the-best-and-scariest-examples-of-ai-enabled-deepfakes/),
    July 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirsky and Lee [2021] Yisroel Mirsky and Wenke Lee. The creation and detection
    of deepfakes: A survey. *ACM Computing Surveys (CSUR)*, 54(1):1–41, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verdoliva [2020] Luisa Verdoliva. Media forensics and deepfakes: an overview.
    *IEEE Journal of Selected Topics in Signal Processing*, 14(5):910–932, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zakharov et al. [2019] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and
    Victor Lempitsky. Few-shot adversarial learning of realistic neural talking head
    models. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 9459–9468, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Damiani [2019] J Damiani. A voice deepfake was used to scam a ceo out of $243,000.
    [https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/),
    September 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samuel [2019] S Samuel. A guy made a deepfake app to turn photos of women into
    nudes. it didn’t go well. [https://www.vox.com/2019/6/27/18761639/ai-deepfake-deepnude-app-nude-women-porn](https://www.vox.com/2019/6/27/18761639/ai-deepfake-deepnude-app-nude-women-porn),
    June 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Guardian [2019] The Guardian. Chinese deepfake app Zao sparks privacy row
    after going viral. [https://www.theguardian.com/technology/2019/sep/02/chinese-face-swap-app-zao-triggers-privacy-fears-viral](https://www.theguardian.com/technology/2019/sep/02/chinese-face-swap-app-zao-triggers-privacy-fears-viral),
    September 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu [2020] Siwei Lyu. Deepfake detection: Current challenges and next steps.
    In *IEEE International Conference on Multimedia & Expo Workshops (ICMEW)*, pages
    1–6\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guarnera et al. [2020a] Luca Guarnera, Oliver Giudice, Cristina Nastasi, and
    Sebastiano Battiato. Preliminary forensics analysis of deepfake images. In *AEIT
    International Annual Conference (AEIT)*, pages 1–6. IEEE, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jafar et al. [2020] Mousa Tayseer Jafar, Mohammad Ababneh, Mohammad Al-Zoube,
    and Ammar Elhassan. Forensics and analysis of deepfake videos. In *The 11th International
    Conference on Information and Communication Systems (ICICS)*, pages 053–058\.
    IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trinh et al. [2021] Loc Trinh, Michael Tsang, Sirisha Rambhatla, and Yan Liu.
    Interpretable and trustworthy deepfake detection via dynamic prototypes. In *Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 1973–1983,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Younus and Hasan [2020] Mohammed Akram Younus and Taha Mohammed Hasan. Effective
    and fast deepfake detection method based on haar wavelet transform. In *International
    Conference on Computer Science and Software Engineering (CSASE)*, pages 186–190\.
    IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turek [2019] M Turek. Media Forensics (MediFor). [https://www.darpa.mil/program/media-forensics](https://www.darpa.mil/program/media-forensics),
    January 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schroepfer [2019] M Schroepfer. Creating a data set and a challenge for deepfakes.
    [https://ai.facebook.com/blog/deepfake-detection-challenge](https://ai.facebook.com/blog/deepfake-detection-challenge),
    September 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tolosana et al. [2020] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez,
    Aythami Morales, and Javier Ortega-Garcia. Deepfakes and beyond: A survey of face
    manipulation and fake detection. *Information Fusion*, 64:131–148, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Punnappurath and Brown [2019] Abhijith Punnappurath and Michael S Brown. Learning
    raw image reconstruction-aware deep image compressors. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 42(4):1013–1019, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. [2019] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto.
    Energy compaction-based image compression using convolutional autoencoder. *IEEE
    Transactions on Multimedia*, 22(4):860–873, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chorowski et al. [2019] Jan Chorowski, Ron J Weiss, Samy Bengio, and Aäron van den
    Oord. Unsupervised speech representation learning using WaveNet autoencoders.
    *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 27(12):2041–2053,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Faceswap: Deepfakes software for all. [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] FakeApp 2.2.0. [https://www.malavida.com/en/soft/fakeapp/](https://www.malavida.com/en/soft/fakeapp/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dee [a] DeepFaceLab. [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab),
    a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] DFaker. [https://github.com/dfaker/df](https://github.com/dfaker/df).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dee [b] DeepFake_tf: Deepfake based on tensorflow. [https://github.com/StromWine/DeepFake_tf](https://github.com/StromWine/DeepFake_tf),
    b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila,
    Jaakko Lehtinen, and Jan Kautz. Few-shot unsupervised image-to-image translation.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 10551–10560, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. [2019] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
    Semantic image synthesis with spatially-adaptive normalization. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    2337–2346, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] DeepFaceLab: Explained and usage tutorial. [https://mrdeepfakes.com/forums/thread-deepfacelab-explained-and-usage-tutorial](https://mrdeepfakes.com/forums/thread-deepfacelab-explained-and-usage-tutorial).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] DSSIM. [https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lattas et al. [2020] Alexandros Lattas, Stylianos Moschoglou, Baris Gecer,
    Stylianos Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, and Stefanos Zafeiriou.
    AvatarMe: Realistically renderable 3D facial reconstruction “in-the-wild”. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 760–769, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ha et al. [2020] Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, and Dongyoung
    Kim. Marionette: Few-shot face reenactment preserving identity of unseen targets.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pages 10893–10900, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2020] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong.
    Disentangled and controllable face image generation via 3D imitative-contrastive
    learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 5154–5163, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tewari et al. [2020] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian
    Bernard, Hans-Peter Seidel, Patrick Pérez, Michael Zollhofer, and Christian Theobalt.
    StyleRig: Rigging StyleGAN for 3D control over portrait images. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    6142–6151, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019a] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen.
    FaceShifter: Towards high fidelity and occlusion aware face swapping. *arXiv preprint
    arXiv:1912.13457*, 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nirkin et al. [2019] Yuval Nirkin, Yosi Keller, and Tal Hassner. FSGAN: Subject
    agnostic face swapping and reenactment. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 7184–7193, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. [2019] Tero Karras, Samuli Laine, and Timo Aila. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 4401–4410,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thies et al. [2016] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian
    Theobalt, and Matthias Nießner. Face2Face: Real-time face capture and reenactment
    of RGB videos. In *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 2387–2395, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thies et al. [2019] Justus Thies, Michael Zollhöfer, and Matthias Nießner.
    Deferred neural rendering: Image synthesis using neural textures. *ACM Transactions
    on Graphics (TOG)*, 38(4):1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Olszewski et al. [2019] Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao
    Li, and Linjie Luo. Transformable bottleneck networks. In *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, pages 7648–7657, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chan et al. [2019] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A
    Efros. Everybody dance now. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 5933–5942, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thies et al. [2020] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian
    Theobalt, and Matthias Nießner. Neural voice puppetry: Audio-driven facial reenactment.
    In *European Conference on Computer Vision*, pages 716–731. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Keras-VGGFace: VGGFace implementation with Keras framework. [https://github.com/rcmalli/keras-vggface](https://github.com/rcmalli/keras-vggface).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fac [a] Faceswap-GAN. [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN),
    a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fac [b] FaceNet. [https://github.com/davidsandberg/facenet](https://github.com/davidsandberg/facenet),
    b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] CycleGAN. [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. Densely connected convolutional networks. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, pages 4700–4708, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. [2017] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
    Progressive growing of GANs for improved quality, stability, and variation. *arXiv
    preprint arXiv:1710.10196*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korshunov and Marcel [2019] Pavel Korshunov and Sébastien Marcel. Vulnerability
    assessment and detection of deepfake videos. In *2019 International Conference
    on Biometrics (ICB)*, pages 1–6\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] VidTIMIT database. [http://conradsanderson.id.au/vidtimit/](http://conradsanderson.id.au/vidtimit/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parkhi et al. [2015] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep
    face recognition. In *Proceedings of the British Machine Vision Conference (BMVC)*,
    pages 41.1–41.12, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroff et al. [2015] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    FaceNet: A unified embedding for face recognition and clustering. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 815–823,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2017] Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew
    Zisserman. Lip reading sentences in the wild. In *2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pages 3444–3453\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suwajanakorn et al. [2017] Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman.
    Synthesizing Obama: learning lip sync from audio. *ACM Transactions on Graphics
    (ToG)*, 36(4):1–13, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korshunov and Marcel [2018a] Pavel Korshunov and Sébastien Marcel. Speaker inconsistency
    detection in tampered video. In *The 26th European Signal Processing Conference
    (EUSIPCO)*, pages 2375–2379\. IEEE, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Galbally and Marcel [2014] Javier Galbally and Sébastien Marcel. Face anti-spoofing
    based on general image quality assessment. In *The 22nd International Conference
    on Pattern Recognition*, pages 1173–1178\. IEEE, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chesney and Citron [2018a] Robert Chesney and Danielle Keats Citron. Deep fakes:
    A looming challenge for privacy, democracy, and national security. *Democracy,
    and National Security*, 107, 2018a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Lima et al. [2020] Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake Karwoski,
    and Annet George. Deepfake detection using spatiotemporal convolutional networks.
    *arXiv preprint arXiv:2006.14749*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amerini and Caldelli [2020] Irene Amerini and Roberto Caldelli. Exploiting prediction
    error inconsistencies through LSTM-based classifiers to detect deepfake videos.
    In *Proceedings of the 2020 ACM Workshop on Information Hiding and Multimedia
    Security*, pages 97–102, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xuan et al. [2019] Xinsheng Xuan, Bo Peng, Wei Wang, and Jing Dong. On the generalization
    of GAN image forensics. In *Chinese Conference on Biometric Recognition*, pages
    134–141\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2016] Pengpeng Yang, Rongrong Ni, and Yao Zhao. Recapture image
    forensics based on laplacian convolutional neural networks. In *International
    Workshop on Digital Watermarking*, pages 119–128\. Springer, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayar and Stamm [2016] Belhassen Bayar and Matthew C Stamm. A deep learning
    approach to universal image manipulation detection using a new convolutional layer.
    In *Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security*,
    pages 5–10, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2015] Yinlong Qian, Jing Dong, Wei Wang, and Tieniu Tan. Deep learning
    for steganalysis via convolutional neural networks. In *Media Watermarking, Security,
    and Forensics*, volume 9409, page 94090J, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2017] Ying Zhang, Lilei Zheng, and Vrizlynn LL Thing. Automated
    face swapping and its detection. In *The 2nd International Conference on Signal
    and Image Processing (ICSIP)*, pages 15–19\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2017] Xin Wang, Nicolas Thome, and Matthieu Cord. Gaze latent support
    vector machine for image classification improved by weakly supervised region selection.
    *Pattern Recognition*, 72:59–71, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai [2017] Shuang Bai. Growing random forest on deep convolutional neural networks
    for scene categorization. *Expert Systems with Applications*, 71:279–287, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2016] Lilei Zheng, Stefan Duffner, Khalid Idrissi, Christophe
    Garcia, and Atilla Baskurt. Siamese multi-layer perceptrons for dimensionality
    reduction and face identification. *Multimedia Tools and Applications*, 75(9):5055–5073,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agarwal and Varshney [2019] Sakshi Agarwal and Lav R Varshney. Limits of deepfake
    detection: A robust estimation viewpoint. *arXiv preprint arXiv:1905.03493*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maurer [2000] Ueli M Maurer. Authentication theory and hypothesis testing. *IEEE
    Transactions on Information Theory*, 46(4):1350–1356, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korshunova et al. [2017] Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas
    Theis. Fast face-swap using convolutional neural networks. In *Proceedings of
    the IEEE International Conference on Computer Vision*, pages 3677–3685, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsu et al. [2020] Chih-Chung Hsu, Yi-Xiu Zhuang, and Chia-Yen Lee. Deep fake
    image detection based on pairwise learning. *Applied Sciences*, 10(1):370, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chopra et al. [2005] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a
    similarity metric discriminatively, with application to face verification. In
    *IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)*,
    volume 1, pages 539–546\. IEEE, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep
    learning face attributes in the wild. In *Proceedings of the IEEE International
    Conference on Computer Vision*, pages 3730–3738, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2015] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised
    representation learning with deep convolutional generative adversarial networks.
    *arXiv preprint arXiv:1511.06434*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. [2017] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning*,
    pages 214–223\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulrajani et al. [2017] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
    Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. *arXiv preprint
    arXiv:1704.00028*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. [2017] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang,
    and Stephen Paul Smolley. Least squares generative adversarial networks. In *Proceedings
    of the IEEE International Conference on Computer Vision*, pages 2794–2802, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
    Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
    Bernstein, et al. ImageNet large scale visual recognition challenge. *International
    Journal of Computer Vision*, 115(3):211–252, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brock et al. [2018] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale
    GAN training for high fidelity natural image synthesis. *arXiv preprint arXiv:1809.11096*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus
    Odena. Self-attention generative adversarial networks. In *International Conference
    on Machine Learning*, pages 7354–7363\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyato et al. [2018] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi
    Yoshida. Spectral normalization for generative adversarial networks. *arXiv preprint
    arXiv:1802.05957*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farid [2009] Hany Farid. Image forgery detection. *IEEE Signal Processing Magazine*,
    26(2):16–25, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mo et al. [2018] Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces identification
    via convolutional neural network. In *Proceedings of the 6th ACM Workshop on Information
    Hiding and Multimedia Security*, pages 43–47, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marra et al. [2018] Francesco Marra, Diego Gragnaniello, Davide Cozzolino, and
    Luisa Verdoliva. Detection of GAN-generated fake images over social networks.
    In *2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)*,
    pages 384–389\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsu et al. [2018] Chih-Chung Hsu, Chia-Yen Lee, and Yi-Xiu Zhuang. Learning
    to detect fake face images in the wild. In *2018 International Symposium on Computer,
    Consumer and Control (IS3C)*, pages 388–391\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2021] Zhiqing Guo, Lipin Hu, Ming Xia, and Gaobo Yang. Blind detection
    of glow-based facial forgery. *Multimedia Tools and Applications*, 80(5):7687–7710,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Dhariwal [2018] Diederik P Kingma and Prafulla Dhariwal. Glow: generative
    flow with invertible 1$\times$ 1 convolutions. In *Proceedings of the 32nd International
    Conference on Neural Information Processing Systems*, pages 10236–10245, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Afchar et al. [2018] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and
    Isao Echizen. MesoNet: a compact facial video forgery detection network. In *2018
    IEEE International Workshop on Information Forensics and Security (WIFS)*, pages
    1–7\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabir et al. [2019] Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed,
    Iacopo Masi, and Prem Natarajan. Recurrent convolutional strategies for face manipulation
    detection in videos. *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition Workshops*, 3(1):80–87, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2021] Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun Xiong,
    and Wei Xia. Learning self-consistency for deepfake detection. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 15023–15033,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rossler et al. [2019] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian
    Riess, Justus Thies, and Matthias Nießner. FaceForensics++: Learning to detect
    manipulated facial images. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 1–11, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dufour and Gully [2019] Nick Dufour and Andrew Gully. Contributing data to deepfake
    detection research. [https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html](https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html),
    September 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2020a] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu.
    Celeb-DF: A large-scale challenging dataset for deepfake forensics. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    3207–3216, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dolhansky et al. [2020] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu,
    Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge
    dataset. *arXiv preprint arXiv:2006.07397*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dolhansky et al. [2019] Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram,
    and Cristian Canton Ferrer. The deepfake detection challenge (DFDC) preview dataset.
    *arXiv preprint arXiv:1910.08854*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. [2020] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change
    Loy. DeeperForensics-1.0: A large-scale dataset for real-world face forgery detection.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 2889–2898, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. [2014] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using RNN encoder-decoder for statistical machine translation. *arXiv preprint
    arXiv:1406.1078*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Güera and Delp [2018] David Güera and Edward J Delp. Deepfake video detection
    using recurrent neural networks. In *15th IEEE International Conference on Advanced
    Video and Signal based Surveillance (AVSS)*, pages 1–6\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laptev et al. [2008] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Benjamin
    Rozenfeld. Learning realistic human actions from movies. In *IEEE Conference on
    Computer Vision and Pattern Recognition*, pages 1–8\. IEEE, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2018] Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi:
    Exposing ai created fake videos by detecting eye blinking. In *2018 IEEE International
    Workshop on Information Forensics and Security (WIFS)*, pages 1–7\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Donahue et al. [2015] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
    Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term
    recurrent convolutional networks for visual recognition and description. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2625–2634,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caldelli et al. [2021] Roberto Caldelli, Leonardo Galteri, Irene Amerini, and
    Alberto Del Bimbo. Optical flow based CNN for detection of unlearnt deepfake manipulations.
    *Pattern Recognition Letters*, 146:31–37, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amerini et al. [2019] Irene Amerini, Leonardo Galteri, Roberto Caldelli, and
    Alberto Del Bimbo. Deepfake video detection through optical flow based CNN. In
    *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*,
    pages 1205–1207, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 770–778, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep
    convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Lyu [2018] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting
    face warping artifacts. *arXiv preprint arXiv:1811.00656*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2019] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using
    inconsistent head poses. In *IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*, pages 8261–8265\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2017] Peng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis.
    Two-stream neural networks for tampered face detection. In *IEEE Conference on
    Computer Vision and Pattern Recognition Workshops (CVPRW)*, pages 1831–1839\.
    IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. [2019] Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. Capsule-forensics:
    Using capsule networks to detect forged images and videos. In *IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pages 2307–2311\.
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. [2011] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming
    auto-encoders. In *International Conference on Artificial Neural Networks*, pages
    44–51\. Springer, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabour et al. [2017] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic
    routing between capsules. In *Proceedings of the 31st International Conference
    on Neural Information Processing Systems*, pages 3859–3869, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chingovska et al. [2012] Ivana Chingovska, André Anjos, and Sébastien Marcel.
    On the effectiveness of local binary patterns in face anti-spoofing. In *Proceedings
    of the International Conference of Biometrics Secial Interest Group (BIOSIG)*,
    pages 1–7\. IEEE, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rössler et al. [2018] Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian
    Riess, Justus Thies, and Matthias Nießner. FaceForensics: A large-scale video
    dataset for forgery detection in human faces. *arXiv preprint arXiv:1803.09179*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahmouni et al. [2017] Nicolas Rahmouni, Vincent Nozick, Junichi Yamagishi,
    and Isao Echizen. Distinguishing computer graphics from natural images using convolution
    neural networks. In *IEEE Workshop on Information Forensics and Security (WIFS)*,
    pages 1–6\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. [2019] Haiying Guan, Mark Kozak, Eric Robertson, Yooyoung Lee,
    Amy N Yates, Andrew Delgado, Daniel Zhou, Timothee Kheyrkhah, Jeff Smith, and
    Jonathan Fiscus. MFC datasets: Large-scale benchmark datasets for media forensic
    challenge evaluation. In *IEEE Winter Applications of Computer Vision Workshops
    (WACVW)*, pages 63–72\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matern et al. [2019] Falko Matern, Christian Riess, and Marc Stamminger. Exploiting
    visual artifacts to expose deepfakes and face manipulations. In *IEEE Winter Applications
    of Computer Vision Workshops (WACVW)*, pages 83–92\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koopman et al. [2018] Marissa Koopman, Andrea Macarulla Rodriguez, and Zeno
    Geradts. Detection of deepfake video manipulation. In *The 20th Irish Machine
    Vision and Image Processing Conference (IMVIP)*, pages 133–136, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenfeld and Sencar [2009] Kurt Rosenfeld and Husrev Taha Sencar. A study of
    the robustness of PRNU-based camera identification. In *Media Forensics and Security*,
    volume 7254, page 72540M, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Li [2011] Chang-Tsun Li and Yue Li. Color-decoupled photo response non-uniformity
    for digital image forensics. *IEEE Transactions on Circuits and Systems for Video
    Technology*, 22(2):260–271, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin and Li [2016] Xufeng Lin and Chang-Tsun Li. Large-scale image clustering
    based on camera fingerprints. *IEEE Transactions on Information Forensics and
    Security*, 12(4):793–808, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scherhag et al. [2019] Ulrich Scherhag, Luca Debiasi, Christian Rathgeb, Christoph
    Busch, and Andreas Uhl. Detection of face morphing attacks based on PRNU analysis.
    *IEEE Transactions on Biometrics, Behavior, and Identity Science*, 1(4):302–317,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phan et al. [2018] Quoc-Tin Phan, Giulia Boato, and Francesco GB De Natale.
    Accurate and scalable image clustering based on sparse representation of camera
    fingerprint. *IEEE Transactions on Information Forensics and Security*, 14(7):1902–1916,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hasan and Salah [2019] Haya R Hasan and Khaled Salah. Combating deepfake videos
    using blockchain and smart contracts. *IEEE Access*, 7:41596–41606, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] IPFS powers the distributed web. [https://ipfs.io/](https://ipfs.io/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chintha et al. [2020] Akash Chintha, Bao Thai, Saniat Javid Sohrawardi, Kartavya
    Bhatt, Andrea Hickerson, Matthew Wright, and Raymond Ptucha. Recurrent convolutional
    structures for audio spoof and video deepfake detection. *IEEE Journal of Selected
    Topics in Signal Processing*, 14(5):1024–1037, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Todisco et al. [2019] Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidullah,
    Héctor Delgado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen,
    and Kong Aik Lee. ASVspoof 2019: Future horizons in spoofed and fake audio detection.
    *arXiv preprint arXiv:1904.05441*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. [2020a] Shruti Agarwal, Hany Farid, Ohad Fried, and Maneesh Agrawala.
    Detecting deep-fake videos from phoneme-viseme mismatches. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*,
    pages 660–661, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fried et al. [2019] Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein,
    Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh
    Agrawala. Text-based editing of talking-head video. *ACM Transactions on Graphics
    (TOG)*, 38(4):1–14, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernandes et al. [2020] Steven Fernandes, Sunny Raj, Rickard Ewetz, Jodh Singh
    Pannu, Sumit Kumar Jha, Eddy Ortiz, Iustina Vintila, and Margaret Salter. Detecting
    deepfake videos using attribution-based confidence metric. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*,
    pages 308–309, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2018] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew
    Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In *The
    13th IEEE International Conference on Automatic Face & Gesture Recognition (FG
    2018)*, pages 67–74\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jha et al. [2019] Susmit Jha, Sunny Raj, Steven Fernandes, Sumit K Jha, Somesh
    Jha, Brian Jalaian, Gunjan Verma, and Ananthram Swami. Attribution-based confidence
    metric for deep neural networks. *Advances in Neural Information Processing Systems*,
    32:11826–11837, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernandes et al. [2019] Steven Fernandes, Sunny Raj, Eddy Ortiz, Iustina Vintila,
    Margaret Salter, Gordana Urosevic, and Sumit Jha. Predicting heart rate variations
    of deepfake videos using neural ODE. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision Workshops*, pages 1721–1729, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Korshunov and Marcel [2018b] Pavel Korshunov and Sébastien Marcel. Deepfakes:
    a new threat to face recognition? assessment and detection. *arXiv preprint arXiv:1812.08685*,
    2018b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. [2020b] Shruti Agarwal, Hany Farid, Tarek El-Gaaly, and Ser-Nam
    Lim. Detecting deep-fake videos from appearance and behavior. In *IEEE International
    Workshop on Information Forensics and Security (WIFS)*, pages 1–6\. IEEE, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ciftci et al. [2020] Umur Aybars Ciftci, Ilke Demir, and Lijun Yin. FakeCatcher:
    Detection of synthetic portrait videos using biological signals. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mittal et al. [2020] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket
    Bera, and Dinesh Manocha. Emotions don’t lie: A deepfake detection method using
    audio-visual affective cues. *arXiv preprint arXiv:2003.06711*, 3, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guarnera et al. [2020b] Luca Guarnera, Oliver Giudice, and Sebastiano Battiato.
    Deepfake detection by analyzing convolutional traces. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops*, pages 666–667,
    2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. [2019] Wonwoong Cho, Sungha Choi, David Keetae Park, Inkyu Shin,
    and Jaegul Choo. Image-to-image translation via group-wise deep whitening-and-coloring
    transformation. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 10639–10647, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2018] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun
    Kim, and Jaegul Choo. StarGAN: Unified generative adversarial networks for multi-domain
    image-to-image translation. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 8789–8797, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2019] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and
    Xilin Chen. AttGAN: Facial attribute editing by only changing what you want. *IEEE
    Transactions on Image Processing*, 28(11):5464–5478, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. [2020] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
    Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 8110–8119, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2007] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.
    Labeled faces in the wild: A database for studying face recognition in unconstrained
    environments. Technical Report 07-49, University of Massachusetts, Amherst, October
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gandhi and Jain [2020] Apurva Gandhi and Shomik Jain. Adversarial perturbations
    fool deepfake detectors. In *IEEE International Joint Conference on Neural Networks
    (IJCNN)*, pages 1–8\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Few-shot face translation GAN. [https://github.com/shaoanlu/fewshot-face-translation-GAN](https://github.com/shaoanlu/fewshot-face-translation-GAN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2020b] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen,
    Fang Wen, and Baining Guo. Face X-ray for more general face forgery detection.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 5001–5010, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens,
    and Alexei A Efros. CNN-generated images are surprisingly easy to spot… for now.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 8695–8704, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2019] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei
    Zhang. Second-order attention network for single image super-resolution. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    11065–11074, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guarnera et al. [2020c] Luca Guarnera, Oliver Giudice, and Sebastiano Battiato.
    Fighting deepfake by exposing the convolutional traces on images. *IEEE Access*,
    8:165085–165098, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moon [1996] Todd K Moon. The expectation-maximization algorithm. *IEEE Signal
    Processing Magazine*, 13(6):47–60, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*, pages
    2223–2232, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2019b] Ke Li, Tianhao Zhang, and Jitendra Malik. Diverse image synthesis
    from semantic layouts via conditional imle. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 4220–4229, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zubiaga et al. [2018] Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria
    Liakata, and Rob Procter. Detection and resolution of rumours in social media:
    A survey. *ACM Computing Surveys (CSUR)*, 51(2):1–36, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chesney and Citron [2018b] R. Chesney and D. K. Citron. Disinformation on steroids:
    The threat of deep fakes. [https://www.cfr.org/report/deep-fake-disinformation-steroids](https://www.cfr.org/report/deep-fake-disinformation-steroids),
    October 2018b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Floridi [2018] Luciano Floridi. Artificial intelligence, deepfakes and a future
    of ectypes. *Philosophy & Technology*, 31(3):317–321, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cozzolino et al. [2018] Davide Cozzolino, Justus Thies, Andreas Rössler, Christian
    Riess, Matthias Nießner, and Luisa Verdoliva. ForensicTransfer: Weakly-supervised
    domain adaptation for forgery detection. *arXiv preprint arXiv:1812.02510*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marra et al. [2019] Francesco Marra, Cristiano Saltori, Giulia Boato, and Luisa
    Verdoliva. Incremental learning for the detection and classification of gan-generated
    images. In *2019 IEEE International Workshop on Information Forensics and Security
    (WIFS)*, pages 1–6\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hussain et al. [2021] Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz
    Koushanfar, and Julian McAuley. Adversarial deepfakes: Evaluating vulnerability
    of deepfake detectors to adversarial examples. In *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pages 3348–3357, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini and Farid [2020] Nicholas Carlini and Hany Farid. Evading deepfake-image
    detectors with white-and black-box attacks. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops*, pages 658–659, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021] Chaofei Yang, Leah Ding, Yiran Chen, and Hai Li. Defending
    against gan-based deepfake attacks via transformation-aware adversarial faces.
    In *IEEE International Joint Conference on Neural Networks (IJCNN)*, pages 1–8\.
    IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeh et al. [2020] Chin-Yuan Yeh, Hsi-Wen Chen, Shang-Lun Tsai, and Sheng-De
    Wang. Disrupting image-translation-based deepfake algorithms with adversarial
    attacks. In *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision Workshops*, pages 53–62, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read [2019] M. Read. Can you spot a deepfake? does it matter? [http://nymag.com/intelligencer/2019/06/how-do-you-spot-a-deepfake-it-might-not-matter.html](http://nymag.com/intelligencer/2019/06/how-do-you-spot-a-deepfake-it-might-not-matter.html),
    June 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maras and Alexandrou [2019] Marie-Helen Maras and Alex Alexandrou. Determining
    authenticity of video evidence in the age of artificial intelligence and in the
    wake of deepfake videos. *The International Journal of Evidence & Proof*, 23(3):255–262,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. [2017] Lichao Su, Cuihua Li, Yuecong Lai, and Jianmei Yang. A fast
    forgery detection algorithm based on exponential-Fourier moments for video region
    duplication. *IEEE Transactions on Multimedia*, 20(4):825–840, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iuliani et al. [2018] Massimo Iuliani, Dasara Shullani, Marco Fontani, Saverio
    Meucci, and Alessandro Piva. A video forensic framework for the unsupervised analysis
    of MP4-like file container. *IEEE Transactions on Information Forensics and Security*,
    14(3):635–645, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malolan et al. [2020] Badhrinarayan Malolan, Ankit Parekh, and Faruk Kazi. Explainable
    deep-fake detection using visual interpretability methods. In *The 3rd International
    Conference on Information and Computer Technologies (ICICT)*, pages 289–293\.
    IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giudice et al. [2021] Oliver Giudice, Luca Guarnera, and Sebastiano Battiato.
    Fighting deepfakes by detecting GAN DCT anomalies. *arXiv preprint arXiv:2101.09781*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
