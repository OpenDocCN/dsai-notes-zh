- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:42:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2212.00017] Knowledge-augmented Deep Learning and Its Applications: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2212.00017] 知识增强深度学习及其应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2212.00017](https://ar5iv.labs.arxiv.org/html/2212.00017)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2212.00017](https://ar5iv.labs.arxiv.org/html/2212.00017)
- en: 'Knowledge-augmented Deep Learning and Its Applications: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识增强深度学习及其应用：综述
- en: 'Zijun Cui Tian Gao Kartik Talamadupula and Qiang Ji Z. Cui and Q. Ji are with
    the Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic
    Institute, Troy, NY 12180, USA (e-mail: cuiz3@rpi.edu; jiq@rpi.edu).T. Gao and
    K.Talamadupula are with IBM Research AI, USA.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 孙子俊 高天 卡尔提克·塔拉马杜普拉 和 江强。Z. Cui 和 Q. Ji 在美国纽约州特洛伊的伦斯勒理工学院电气、计算机和系统工程系工作（电子邮件：cuiz3@rpi.edu；jiq@rpi.edu）。T.
    Gao 和 K. Talamadupula 在 IBM 研究 AI 工作。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning models, though having achieved great success in many different
    fields over the past years, are usually data hungry, fail to perform well on unseen
    samples, and lack of interpretability. Various prior knowledge often exists in
    the target domain and their use can alleviate the deficiencies with deep learning.
    To better mimic the behavior of human brains, different advanced methods have
    been proposed to identify domain knowledge and integrate it into deep models for
    data-efficient, generalizable, and interpretable deep learning, which we refer
    to as knowledge-augmented deep learning (KADL). In this survey, we define the
    concept of KADL, and introduce its three major tasks, i.e., knowledge identification,
    knowledge representation, and knowledge integration. Different from existing surveys
    that are focused on a specific type of knowledge, we provide a broad and complete
    taxonomy of domain knowledge and its representations. Based on our taxonomy, we
    provide a systematic review of existing techniques, different from existing works
    that survey integration approaches agnostic to taxonomy of knowledge. This survey
    subsumes existing works and offers a bird’s-eye view of research in the general
    area of knowledge-augmented deep learning. The thorough and critical reviews of
    numerous papers help not only understand current progresses but also identify
    future directions for the research on knowledge-augmented deep learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习模型在过去几年在许多不同领域取得了巨大成功，但它们通常需要大量数据，在未见样本上表现不佳，并且缺乏可解释性。目标领域中往往存在各种先验知识，利用这些知识可以缓解深度学习的不足。为了更好地模拟人脑的行为，已经提出了各种先进的方法来识别领域知识并将其集成到深度模型中，以实现数据高效、可泛化和可解释的深度学习，我们称之为知识增强深度学习（KADL）。在这项综述中，我们定义了KADL的概念，并介绍了其三个主要任务，即知识识别、知识表示和知识集成。与现有专注于特定类型知识的综述不同，我们提供了一个广泛而完整的领域知识及其表示的分类系统。基于我们的分类系统，我们对现有技术进行了系统的回顾，与现有文献中对知识分类无关的集成方法的综述不同。此综述涵盖了现有的工作，并提供了对知识增强深度学习领域的总体视角。对大量论文的深入和批判性评审不仅有助于了解当前进展，还有助于确定未来的研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Domain Knowledge, Deep Learning, Neural-symbolic Models, Physics-informed Deep
    Learning.<svg   height="66.72" overflow="visible" version="1.1" width="604.52"><g
    transform="translate(0,66.72) matrix(1 0 0 -1 0 0) translate(-122.74,0) translate(0,-7.2)
    matrix(1.0 0.0 0.0 1.0 127.35 35.71)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="595.3" height="57.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">©2022
    IEEE. Personal use of this material is permitted. Permission from IEEE must be
    obtained for all other uses, in any current or future media, including reprinting/republishing
    this material for advertising or promotional purposes, creating new collective
    works, for resale or redistribution to servers or lists, or reuse of any copyrighted
    component of this work in other works.</foreignobject></g></svg>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 领域知识、深度学习、神经符号模型、物理信息深度学习。<svg   height="66.72" overflow="visible" version="1.1"
    width="604.52"><g transform="translate(0,66.72) matrix(1 0 0 -1 0 0) translate(-122.74,0)
    translate(0,-7.2) matrix(1.0 0.0 0.0 1.0 127.35 35.71)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="595.3" height="57.5" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">©2022 IEEE. 个人使用此材料是被允许的。所有其他用途（包括广告或宣传目的的再版/重新发布、创建新的集体作品、转售或重新分发到服务器或列表，或在其他作品中重复使用任何受版权保护的组件）必须获得IEEE的许可。</foreignobject></g></svg>
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Despite the impressive performance that existing deep models have achieved in
    various fields, they suffer from several serious deficiencies, including high
    data dependency and poor generalization [[1](#bib.bib1)]. These deficiencies originate
    primarily from the models’ data-driven nature and their inability to effectively
    exploit the domain knowledge. To address these limitations, a knowledge-augmented
    deep learning paradigm begins to attract researchers’ attention, whereby domain
    knowledge and observable data work together synergistically to produce data-efficient,
    generalizable, and interpretable deep learning algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的深度模型在各种领域取得了令人印象深刻的成绩，但它们存在若干严重缺陷，包括高数据依赖性和较差的泛化能力[[1](#bib.bib1)]。这些缺陷主要源于模型的数据驱动性质以及它们未能有效利用领域知识。为了解决这些局限性，知识增强的深度学习范式开始引起研究人员的关注，在这种范式下，领域知识和可观测数据协同工作，产生数据高效、具有良好泛化能力和可解释的深度学习算法。
- en: 'Real-world domain knowledge is rich. In the context of deep learning, the domain
    knowledge mainly originates from two sources: target knowledge and measurement
    knowledge. Target knowledge governs the behaviors and properties of the target
    variables we intend to predict, while measurement knowledge controls the underlying
    mechanism that produces the observed data of the target variables. Based on its
    representation, we propose to divide the domain knowledge that has been explored
    in deep learning into two categories: scientific knowledge and experiential knowledge.
    Scientific knowledge represents the well-established laws or theories in a domain
    that govern the properties or behaviors of target variables. In contrast, experiential
    knowledge refers to well-known facts or rules extracted from longtime observations
    and can also be inferred through humans’ reasoning. Knowledge can be represented
    and organized in various formats. Scientific knowledge is usually well represented
    rigorously with mathematical equations. Experiential knowledge, on the other hand,
    is usually represented less formally, such as through logic rules, knowledge graphs
    or probabilistic dependencies. Knowledge with different representations is integrated
    with data in a deep learning framework through different integration approaches.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的领域知识是丰富的。在深度学习的背景下，领域知识主要来源于两个方面：目标知识和测量知识。目标知识决定了我们打算预测的目标变量的行为和属性，而测量知识控制了产生目标变量观测数据的潜在机制。根据其表示方式，我们建议将深度学习中已经探索的领域知识分为两类：科学知识和经验知识。科学知识代表了在某个领域中已经确立的法律或理论，这些法律或理论决定了目标变量的属性或行为。相反，经验知识指的是从长期观察中提取出的知名事实或规则，也可以通过人类的推理进行推断。知识可以以各种格式表示和组织。科学知识通常通过数学方程式严格表示。而经验知识则通常以较不正式的方式表示，如逻辑规则、知识图谱或概率依赖。具有不同表示形式的知识通过不同的整合方法在深度学习框架中与数据集成。
- en: 'Recognizing the deficiencies with current deep learning, there is a growing
    interest recently in capturing and encoding prior knowledge into the deep learning.
    Two mainstream techniques are neural-symbolic integration and physics informed
    deep learning. Neural-symbolic integration models focus on encoding experiential
    knowledge into the traditional symbolic AI models and integrating the symbolic
    models with deep learning models. The physics informed deep learning focuses on
    encoding various theoretical physical knowledge into different stages of deep
    learning. Current survey papers in this area are limited in scope as they focus
    on reviewing either neural-symbolic models or physics-informed machine learning
    methods, while ignoring many other related works. Specifically, the existing surveys
    on neural-symbolic models mainly consist of the discussions on logic rules or
    knowledge graphs, and their integration into deep models [[2](#bib.bib2), [3](#bib.bib3)].
    Existing surveys on physics-informed machine learning, however, are limited to
    a specific scientific discipline, and the integration methodologies are usually
    task-specific, e.g., physics [[4](#bib.bib4), [5](#bib.bib5)], cyber-physical
    systems [[6](#bib.bib6)], geometry [[7](#bib.bib7)], and chemistry [[8](#bib.bib8)].
    These surveys are hence focused on the methodologies for solving scientific problems
    under a lab environment, lack of discussions on real-world applications. To address
    this limitation, we present a comprehensive yet systematic review of existing
    works on knowledge-augmented deep learning. The contributions of our survey are
    three folds:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到当前深度学习的不足，近期对将先验知识捕捉和编码到深度学习中的兴趣日益增长。两种主流技术是神经-符号整合和物理知识驱动的深度学习。神经-符号整合模型专注于将经验知识编码到传统的符号人工智能模型中，并将符号模型与深度学习模型整合。物理知识驱动的深度学习则专注于将各种理论物理知识编码到深度学习的不同阶段。当前该领域的调查论文范围有限，主要集中于回顾神经-符号模型或物理知识驱动的机器学习方法，而忽略了许多其他相关工作。具体而言，现有关于神经-符号模型的调查主要包括对逻辑规则或知识图谱的讨论，以及它们与深度模型的整合[[2](#bib.bib2),
    [3](#bib.bib3)]。然而，现有的关于物理知识驱动机器学习的调查则局限于特定科学学科，整合方法通常是任务特定的，例如，物理[[4](#bib.bib4),
    [5](#bib.bib5)]、网络物理系统[[6](#bib.bib6)]、几何[[7](#bib.bib7)]和化学[[8](#bib.bib8)]。这些调查因此专注于实验室环境下解决科学问题的方法，缺乏对实际应用的讨论。为了解决这一局限性，我们提供了对现有知识增强深度学习工作的全面而系统的回顾。我们调查的贡献有三方面：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey creates a novel taxonomy on domain knowledge, including both scientific
    knowledge and experiential knowledge. Our work subsumes existing works that focus
    on a subset of domain knowledge on specific disciplines [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)].
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查创建了一个关于领域知识的新分类法，包括科学知识和经验知识。我们的工作涵盖了现有集中于特定学科领域知识子集的研究[[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey covers a wide array of methodologies for knowledge representation
    and integration with a systematic categorization. It differs from existing surveys
    on general integration techniques, that are agnostic to the taxonomy of domain
    knowledge [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)].
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查涵盖了知识表示和整合的各种方法，并进行了系统的分类。它与现有的关于一般整合技术的调查有所不同，后者对领域知识的分类持中立态度[[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey covers the methodologies that are not only for solving scientific
    problems under a lab environment, but, more importantly, for real-world application
    tasks. Not being limited to a specific application task, this survey involves
    the tasks ranging from computer vision to natural language processing. Our survey
    is hence of interest not only to deep learning researchers but also to deep learning
    practitioners in different fields.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查涵盖了不仅用于实验室环境下科学问题的解决方法，更重要的是，涉及实际应用任务的方法。本调查不限于特定应用任务，涵盖了从计算机视觉到自然语言处理的任务。因此，本调查不仅对深度学习研究人员感兴趣，也对不同领域的深度学习从业者具有吸引力。
- en: 'We organize this survey as follows. We first introduce the concept of KADL
    in Section [II](#S2 "II Knowledge-augmented Deep Learning ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey"), whereby we define the three basic
    tasks (i.e., knowledge identification, knowledge representation, and knowledge
    integration). We then review KADL methodologies, where we categorize different
    techniques based on their domain knowledge of focus: 1) deep learning with scientific
    knowledge in Section [III](#S3 "III Deep Learning with Scientific Knowledge ‣
    Knowledge-augmented Deep Learning and Its Applications: A Survey") and 2) deep
    learning with experiential knowledge in Section [IV](#S4 "IV Deep Learning with
    Experiential Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey"). In each category, we identify the domain knowledge, its representation
    formats, and the existing methods proposed for the integration of knowledge with
    data. An overview of existing methodologies in knowledge-augmented deep learning
    is included in Table [I](#S2.T1 "TABLE I ‣ II Knowledge-augmented Deep Learning
    ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将本调查组织如下。我们首先在第[II](#S2 "II Knowledge-augmented Deep Learning ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey")节介绍KADL的概念，其中定义了三个基本任务（即知识识别、知识表示和知识整合）。然后我们回顾KADL方法学，其中我们根据其关注的领域知识对不同技术进行分类：1)
    第[III](#S3 "III Deep Learning with Scientific Knowledge ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey")节中涉及科学知识的深度学习和2) 第[IV](#S4 "IV Deep
    Learning with Experiential Knowledge ‣ Knowledge-augmented Deep Learning and Its
    Applications: A Survey")节中涉及经验知识的深度学习。在每个类别中，我们识别领域知识、其表示格式及现有的知识与数据整合方法。表[I](#S2.T1
    "TABLE I ‣ II Knowledge-augmented Deep Learning ‣ Knowledge-augmented Deep Learning
    and Its Applications: A Survey")中包含了现有的知识增强深度学习方法概述。'
- en: II Knowledge-augmented Deep Learning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 知识增强深度学习
- en: Major tasks of knowledge-augmented deep learning include knowledge identification,
    knowledge representation, and knowledge integration into deep models. In the following
    sections, we introduce each of the major tasks in detail.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 知识增强深度学习的主要任务包括知识识别、知识表示和知识整合到深度模型中。在接下来的章节中，我们将详细介绍每个主要任务。
- en: 'TABLE I: Overview of Knowledge-augmented Deep Learning'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 知识增强深度学习概述'
- en: '| Knowledge Identification | Knowledge Representation | Knowledge Integration
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 知识识别 | 知识表示 | 知识整合 |'
- en: '| Data-level | Architecture-level | Training-level | Decision-level |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 数据级 | 架构级 | 训练级 | 决策级 |'
- en: '| Scientific Knowledge | Mathematical Equations | [[14](#bib.bib14)] | [[15](#bib.bib15)][[16](#bib.bib16)][[17](#bib.bib17)][[18](#bib.bib18)]
    | [[19](#bib.bib19)] [[20](#bib.bib20)][[21](#bib.bib21)][[22](#bib.bib22)] |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 科学知识 | 数学方程式 | [[14](#bib.bib14)] | [[15](#bib.bib15)][[16](#bib.bib16)][[17](#bib.bib17)][[18](#bib.bib18)]
    | [[19](#bib.bib19)] [[20](#bib.bib20)][[21](#bib.bib21)][[22](#bib.bib22)] |  |'
- en: '|  | [[23](#bib.bib23)][[24](#bib.bib24)][[25](#bib.bib25)] | [[26](#bib.bib26)][[27](#bib.bib27)][[28](#bib.bib28)][[29](#bib.bib29)]
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | [[23](#bib.bib23)][[24](#bib.bib24)][[25](#bib.bib25)] | [[26](#bib.bib26)][[27](#bib.bib27)][[28](#bib.bib28)][[29](#bib.bib29)]
    |  |'
- en: '|  | [[30](#bib.bib30)] [[31](#bib.bib31)] [[32](#bib.bib32)][[33](#bib.bib33)]
    | [[34](#bib.bib34)][[35](#bib.bib35)] [[36](#bib.bib36)] [[37](#bib.bib37)] |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | [[30](#bib.bib30)] [[31](#bib.bib31)] [[32](#bib.bib32)][[33](#bib.bib33)]
    | [[34](#bib.bib34)][[35](#bib.bib35)] [[36](#bib.bib36)] [[37](#bib.bib37)] |  |'
- en: '|  | [[38](#bib.bib38)] [[39](#bib.bib39)][[40](#bib.bib40)][[41](#bib.bib41)]
    | [[42](#bib.bib42)][[43](#bib.bib43)][[44](#bib.bib44)][[45](#bib.bib45)] |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | [[38](#bib.bib38)] [[39](#bib.bib39)][[40](#bib.bib40)][[41](#bib.bib41)]
    | [[42](#bib.bib42)][[43](#bib.bib43)][[44](#bib.bib44)][[45](#bib.bib45)] |  |'
- en: '|  | [[46](#bib.bib46)] [[47](#bib.bib47)] [[48](#bib.bib48)][[49](#bib.bib49)]
    | [[50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)][[53](#bib.bib53)] |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | [[46](#bib.bib46)] [[47](#bib.bib47)] [[48](#bib.bib48)][[49](#bib.bib49)]
    | [[50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)][[53](#bib.bib53)] |  |'
- en: '|  |  | [[54](#bib.bib54)] [[55](#bib.bib55)] |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[54](#bib.bib54)] [[55](#bib.bib55)] |  |'
- en: '| Simulation Engines | [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)][[59](#bib.bib59)]
    |  |  |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 模拟引擎 | [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)][[59](#bib.bib59)]
    |  |  |  |'
- en: '|  | [[60](#bib.bib60)][[61](#bib.bib61)] [[62](#bib.bib62)][[63](#bib.bib63)]
    |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | [[60](#bib.bib60)][[61](#bib.bib61)] [[62](#bib.bib62)][[63](#bib.bib63)]
    |  |  |  |'
- en: '| Experiential Knowledge | Probabilistic Relationships | [[14](#bib.bib14)][[64](#bib.bib64)][[65](#bib.bib65)]
    | [[66](#bib.bib66)][[67](#bib.bib67)][[68](#bib.bib68)][[69](#bib.bib69)] | [[70](#bib.bib70)][[71](#bib.bib71)][[72](#bib.bib72)][[73](#bib.bib73)]
    | [[64](#bib.bib64)][[14](#bib.bib14)] |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 经验知识 | 概率关系 | [[14](#bib.bib14)][[64](#bib.bib64)][[65](#bib.bib65)] | [[66](#bib.bib66)][[67](#bib.bib67)][[68](#bib.bib68)][[69](#bib.bib69)]
    | [[70](#bib.bib70)][[71](#bib.bib71)][[72](#bib.bib72)][[73](#bib.bib73)] | [[64](#bib.bib64)][[14](#bib.bib14)]
    |'
- en: '|  | [[74](#bib.bib74)] |  |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | [[74](#bib.bib74)] |  |  |'
- en: '| Logic Rules | [[22](#bib.bib22)] | [[75](#bib.bib75)] | [[76](#bib.bib76)][[77](#bib.bib77)]
    |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑规则 | [[22](#bib.bib22)] | [[75](#bib.bib75)] | [[76](#bib.bib76)][[77](#bib.bib77)]
    |  |'
- en: '|  | Knowledge Graphs |  | [[78](#bib.bib78)][[79](#bib.bib79)][[80](#bib.bib80)]
    | [[81](#bib.bib81)][[82](#bib.bib82)] | [[83](#bib.bib83)][[84](#bib.bib84)]
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | 知识图谱 |  | [[78](#bib.bib78)][[79](#bib.bib79)][[80](#bib.bib80)] | [[81](#bib.bib81)][[82](#bib.bib82)]
    | [[83](#bib.bib83)][[84](#bib.bib84)] |'
- en: II-A Knowledge Identification
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 知识识别
- en: Knowledge identification involves identifying the domain knowledge for a specific
    task. For some tasks, domain knowledge is readily available and hence easy to
    identify, while in other tasks, the knowledge is less straightforward and requires
    domain experts’ efforts to identify. In this survey, we divide the domain knowledge
    into scientific knowledge and experiential knowledge. Scientific knowledge is
    prescriptive and mainly refers to well-formulated mathematical theories or physics
    laws. These laws are validated extensively through scientific experiments and
    are true in a universal setting (e.g., Newton’s laws). These laws explicitly define
    the deterministic relationships among different variables that are involved in
    the system. Recent work in physics-informed machine learning seeks to exploit
    various scientific knowledge from different disciplines for enhancing deep learning.
    Experiential knowledge is descriptive and mainly refers to well-known facts from
    daily life, indicating semantic properties of an entity or semantic relationships
    among multiple entities. Experiential knowledge usually is extracted from longtime
    observations but can also be derived from well-established studies or theories.
    The latter type of experiential knowledge is science-grounded and is focused on
    a semantic and abstract level of description. Experiential knowledge usually contains
    lots of fragmented information, and could be uncertain, imprecise, or ambiguous.
    Recent work in neural-symbolic models focuses on embedding experiential knowledge
    into deep learning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 知识识别涉及为特定任务识别领域知识。对于某些任务，领域知识容易获得，因此容易识别；而在其他任务中，知识则不那么直观，需要领域专家的努力来识别。在这项调查中，我们将领域知识分为科学知识和经验知识。科学知识是规范性的，主要指精心制定的数学理论或物理定律。这些定律通过科学实验广泛验证，在普遍的环境中是有效的（例如，牛顿定律）。这些定律明确地定义了系统中涉及的不同变量之间的确定性关系。近期在物理信息机器学习领域的工作旨在利用不同学科的各种科学知识来增强深度学习。经验知识是描述性的，主要指来自日常生活的已知事实，表明实体的语义属性或多个实体之间的语义关系。经验知识通常来源于长期观察，也可以来自于已有的研究或理论。后一种经验知识是基于科学的，着重于语义和抽象的描述层面。经验知识通常包含大量碎片化的信息，可能是不确定的、不精确的或模糊的。近期的神经符号模型研究则专注于将经验知识嵌入深度学习中。
- en: II-B Knowledge Representation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 知识表示
- en: Knowledge representation involves representing the identified domain knowledge
    in a well-organized and structured format. The appropriate representation depends
    on the type of domain knowledge. Scientific knowledge is usually expressed using
    equations. Besides, a simulation engine is also considered as an alternative representation
    of the scientific knowledge. Experiential knowledge is less formal compared to
    scientific knowledge. Experiential knowledge can be represented through probabilistic
    relationships, logic rules or knowledge graphs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 知识表示涉及将识别出的领域知识以良好组织和结构化的格式进行表示。适当的表示方式取决于领域知识的类型。科学知识通常通过方程式表达。此外，模拟引擎也被视为科学知识的一种替代表示方式。相比于科学知识，经验知识形式较少。经验知识可以通过概率关系、逻辑规则或知识图谱来表示。
- en: II-C Knowledge Integration
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 知识整合
- en: 'Knowledge integration entails integrating domain knowledge into deep models.
    Through the integration, a deep model can leverage both existing datasets and
    domain knowledge for certain tasks. Different integration methods can be employed
    depending on the types of knowledge, and can be divided into four levels: data-level,
    architecture-level, training-level, and decision-level, as shown in Figure [1](#S2.F1
    "Figure 1 ‣ II-C Knowledge Integration ‣ II Knowledge-augmented Deep Learning
    ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '知识集成涉及将领域知识整合到深度模型中。通过集成，深度模型可以利用现有数据集和领域知识来完成特定任务。根据知识的类型，可以采用不同的集成方法，主要分为四个级别：数据级、架构级、训练级和决策级，如图[1](#S2.F1
    "Figure 1 ‣ II-C Knowledge Integration ‣ II Knowledge-augmented Deep Learning
    ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey")所示。'
- en: '![Refer to caption](img/3e55fa37a6761eab2e00787d067c5aa1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3e55fa37a6761eab2e00787d067c5aa1.png)'
- en: 'Figure 1: Four levels of integration: data-level, architecture-level, training-level,
    and decision-level.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 四种集成级别：数据级、架构级、训练级和决策级。'
- en: Data-level integration focuses on integrating the knowledge by augmenting the
    original training data with pseudo data, which is generated based on the knowledge.
    Architecture-level integration embeds knowledge by modifying neural network architecture.
    Training-level integration regularizes the training of a deep model via regularization
    terms or primary loss terms that are derived from the knowledge. Finally, decision-level
    integration combines a top-down prediction from a prior model with a bottom-up
    prediction from a deep model, whereby knowledge encoded in the prior model help
    refine the predictions of a deep learning pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据级集成侧重于通过用基于知识生成的伪数据扩充原始训练数据来整合知识。架构级集成通过修改神经网络架构来嵌入知识。训练级集成通过从知识中推导出的正则化项或主要损失项来规范深度模型的训练。最后，决策级集成将来自先验模型的自上而下的预测与来自深度模型的自下而上的预测相结合，其中编码在先验模型中的知识有助于优化深度学习管道的预测。
- en: Each type of integration could be beneficial from different aspects. Data-level
    integration can help alleviate data paucity issue that faced by many deep models [[56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]. Additionally, data generation
    via automatic simulation is usually less expensive compared to human annotation.
    Architecture-level integration brings the bonus of making deep models interpretable
    and explainable, which are two crucial factors for trustworthy artificial intelligence [[85](#bib.bib85)].
    Training-level integration can be regarded as the most common approach due to
    its straightforwardness. It can be flexibly applied to different deep model frameworks.
    Flexibility of training-level integration also promotes the quantification of
    knowledge uncertainties [[53](#bib.bib53), [54](#bib.bib54), [45](#bib.bib45),
    [55](#bib.bib55)]. Decision-level integration, different from previous three approaches,
    employs knowledge independent of the training of deep models, and is relatively
    less explored by existing works.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的集成都可能从不同方面带来好处。数据级集成可以帮助缓解许多深度模型面临的数据稀缺问题[[56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59)]。此外，通过自动模拟生成数据通常比人工标注成本更低。架构级集成的优点是使深度模型具有可解释性和可解释性，这两个因素对值得信赖的人工智能至关重要[[85](#bib.bib85)]。训练级集成由于其直观性，通常被视为最常见的方法。它可以灵活地应用于不同的深度模型框架。训练级集成的灵活性也促进了知识不确定性的量化[[53](#bib.bib53),
    [54](#bib.bib54), [45](#bib.bib45), [55](#bib.bib55)]。决策级集成与前三种方法不同，它利用独立于深度模型训练的知识，并且现有工作对其研究相对较少。
- en: Selection of integration approach could be both task-dependent and knowledge-dependent.
    Architecture-level integration would be preferred if the knowledge is expected
    to be integrated in a hard way. The rest of three integration approaches (i.e.,
    data-level, training-level, or decision-level) introduce knowledge into a deep
    learning pipeline in a soft way. Secondly, if the knowledge only involves target
    variables (i.e., predictions of a neural network), training-level integration
    would be preferred. To perform the other three types of integration, the knowledge
    is expected to involve measurements (e.g., intermediate variables or observations)
    and target variables. Lastly, if the identified knowledge consists of highly nonlinear
    and complex relationships, leveraging well-established engines or simulators for
    data-level integration would be the primary choice.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 选择集成方法可能既取决于任务，也取决于知识。如果预计知识会以困难的方式进行集成，那么架构级别的集成将是首选。其余三种集成方法（即数据级、训练级或决策级）以柔和的方式将知识引入深度学习流程。其次，如果知识仅涉及目标变量（即神经网络的预测），那么训练级集成将是首选。要执行其他三种类型的集成，知识预计需要涉及测量（例如，中间变量或观察值）和目标变量。最后，如果识别出的知识包含高度非线性和复杂的关系，那么利用成熟的引擎或模拟器进行数据级集成将是首要选择。
- en: III Deep Learning with Scientific Knowledge
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度学习与科学知识
- en: Deep learning models are gaining importance in advanced science and engineering
    areas that have traditionally been dominated by mechanistic (e.g., first principle)
    models. Such models produce particularly promising performances for scientific
    problems whose undergoing mechanisms are not well understood by experts or those
    problems for which exact solutions are computationally infeasible. Existing deep
    learning, however, requires a significant amount of annotated data, and it generalizes
    poorly to novel data or settings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在传统上由机械模型（例如，第一原理模型）主导的先进科学和工程领域中变得越来越重要。这些模型对那些机制未被专家充分理解或精确解决方案计算上不可行的科学问题表现出特别有前景的性能。然而，现有的深度学习需要大量的注释数据，并且在新数据或设置中泛化性能较差。
- en: There is a growing consensus in the research community regarding combining conventional
    methodologies in science and engineering with existing data-driven deep models.
    Deep learning with scientific knowledge explores the continuum between classical
    mechanistic models and modern deep ones. There have been growing efforts in the
    machine learning community to incorporate scientific knowledge into deep learning
    (also referred to as physics-informed machine learning) to generate physically
    consistent and interpretable predictions and to reduce data dependency.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 研究界对将科学和工程中的传统方法与现有数据驱动的深度模型相结合达成了越来越广泛的共识。深度学习与科学知识探索了经典机械模型与现代深度模型之间的连续体。机器学习社区已经在努力将科学知识融入深度学习（也称为物理信息机器学习），以生成物理上一致和可解释的预测，并减少数据依赖。
- en: In the following, we first identify the types of scientific knowledge and their
    representations. We then introduce different methodologies on integrating scientific
    knowledge with deep models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们首先识别科学知识的类型及其表示形式。然后，我们介绍将科学知识与深度模型集成的不同方法。
- en: III-A Scientific Knowledge Identification
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 科学知识识别
- en: Scientific knowledge refers to well-formulated mathematical or physics equations
    that have been validated through extensive scientific experiments and are true
    in a universal setting (e.g., Newton’s laws). These laws explicitly define a deterministic
    and precise relationship among different objects that are involved in a system.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 科学知识是指经过广泛科学实验验证的、在普遍背景下成立的良好数学或物理方程（例如，牛顿定律）。这些定律明确地定义了系统中不同对象之间的确定性和精确关系。
- en: Current physics-informed deep learning seeks to explore the usage of classical
    mechanics models. For a dynamic system, the most widely considered scientific
    knowledge is Newtonian mechanics, which includes kinematics and dynamics. The
    former refers to observable motion (such as motion trajectories), which is typically
    represented with polynomial equations involving measurable properties (e.g., velocity,
    acceleration, or positions). Kinematics studies motion without regard for the
    cause. In contrast, dynamics studies the causes of a motion, whereby partial differential
    equations (PDEs) are used to capture the relationships between forces and measurable
    properties. Existing works explore the usage of dynamics in various physics systems
    (e.g., gas and fluid dynamics [[86](#bib.bib86)] and molecular dynamics of protein [[87](#bib.bib87),
    [88](#bib.bib88)]). With the understanding of dynamics, kinematics can be better
    predicted. Newtonian mechanics has hence been leveraged in real-world applications,
    such as human body behavior analysis [[29](#bib.bib29), [89](#bib.bib89), [90](#bib.bib90)].
    Newtonian mechanics, unfortunately, can result in equations of motion that are
    intractable to solve, even for a seemingly simple system (e.g., double pendulum
    system). Lagrangian mechanics or Hamiltonian mechanics can instead be considered.
    As re-formulations to Newtonian mechanics, both Lagrangian mechanics and Hamiltonian
    mechanics leverage generalized coordinates, making them flexible with respect
    to which coordinates to use to understand a system. In Lagrangian mechanics, $L$
    is defined as the difference between the kinetic energy, $T$, and potential energy,
    $U$, of a system ( i.e., $L=T-U$). The Hamiltonian $H$ resembles the Lagrangian
    $L$, and is defined as the summation of the kinetic energy, $T$, and potential
    energy, $U$, of a system (i.e., $H=T+U$). In Lagrangian mechanics, the time derivative
    of position $\dot{x}$ is considered as generalized momentum, whereas in Hamiltonian
    mechanics, momentum $p$ is considered. For simple particle systems, such differences
    are trivial, while in more complicated systems (e.g., magnetic fields), momentum
    can no longer be computed as simple a product of mass and velocity. The dynamic
    equations for both a Lagrangian system and a Hamiltonian system conserve energy
    over time with conservative forces.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的物理知识驱动的深度学习致力于探索经典力学模型的使用。对于动态系统，最广泛考虑的科学知识是牛顿力学，包括运动学和动力学。运动学指的是可观察的运动（如运动轨迹），通常用包含可测量属性（如速度、加速度或位置）的多项式方程来表示。运动学研究运动而不考虑其原因。相比之下，动力学研究运动的原因，通过偏微分方程（PDEs）来捕捉力与可测量属性之间的关系。现有研究探索了动力学在各种物理系统中的应用（如气体和流体动力学[[86](#bib.bib86)]以及蛋白质的分子动力学[[87](#bib.bib87),
    [88](#bib.bib88)]）。通过对动力学的理解，可以更好地预测运动学。牛顿力学因此被应用于实际问题，例如人体行为分析[[29](#bib.bib29),
    [89](#bib.bib89), [90](#bib.bib90)]。然而，牛顿力学有时会导致难以求解的运动方程，即使是对于看似简单的系统（如双摆系统）。因此，可以考虑拉格朗日力学或哈密顿力学。作为牛顿力学的重新表述，拉格朗日力学和哈密顿力学都利用广义坐标，使其在理解系统时对坐标的选择更加灵活。在拉格朗日力学中，$L$
    定义为系统的动能 $T$ 和势能 $U$ 的差（即 $L=T-U$）。哈密顿量 $H$ 类似于拉格朗日量 $L$，定义为系统的动能 $T$ 和势能 $U$
    的总和（即 $H=T+U$）。在拉格朗日力学中，位置的时间导数 $\dot{x}$ 被视为广义动量，而在哈密顿力学中，则考虑动量 $p$。对于简单的粒子系统，这些差异是微不足道的，而在更复杂的系统（如磁场）中，动量不再可以简单地作为质量和速度的乘积来计算。拉格朗日系统和哈密顿系统的动态方程在保守力作用下能量随时间保持不变。
- en: Symmetries have also been widely explored in physics. Philip Anderson famously
    argued, “It is only slightly overstating the case to say that physics is the study
    of symmetry” [[91](#bib.bib91)]. Discovering symmetries has been proven to be
    important in both deepening the understanding of physics and enhancing machine
    learning algorithms. Equivariant or invariant functions preserve symmetries and
    have often been exploited for incorporating these symmetries into deep learning
    algorithms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对称性在物理学中也得到了广泛的探索。菲利普·安德森著名地指出：“可以说物理学就是对称性的研究，这样的说法不过是稍微夸张了一点”[[91](#bib.bib91)]。发现对称性已被证明对加深对物理学的理解和增强机器学习算法的重要性。等变函数或不变函数保持对称性，通常被用于将这些对称性融入深度学习算法中。
- en: Optics, another type of physics knowledge, has also been considered. Optics
    studies the behavior and properties of light. Fermat’s principle [[92](#bib.bib92)]
    is the basic law in optics. In addition, the illumination models [[93](#bib.bib93)]
    and the rendering equation [[94](#bib.bib94)] capture the 3D object appearances
    with their image appearances. Existing works explore the usage of various illumination
    models for different computer vision tasks [[95](#bib.bib95), [96](#bib.bib96)]
    and computer graphics tasks [[97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 光学作为另一种物理知识也被考虑在内。光学研究光的行为和性质。费马原理 [[92](#bib.bib92)] 是光学中的基本法则。此外，照明模型 [[93](#bib.bib93)]
    和渲染方程 [[94](#bib.bib94)] 捕捉了三维物体的外观及其图像外观。现有的研究探讨了各种照明模型在不同计算机视觉任务 [[95](#bib.bib95),
    [96](#bib.bib96)] 和计算机图形任务 [[97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99)]
    中的应用。
- en: Besides physics knowledge, mathematical theories such as theorems on existing
    algorithms (e.g., sorting or ranking), as well as continuous relaxation [[100](#bib.bib100)],
    have also been considered. Projective geometry theories [[101](#bib.bib101)] are
    widely applied to various computer vision tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了物理知识，数学理论，如现有算法的定理（例如排序或排名），以及连续松弛 [[100](#bib.bib100)] 也被考虑在内。投影几何理论 [[101](#bib.bib101)]
    广泛应用于各种计算机视觉任务。
- en: III-B Representation of Scientific Knowledge
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 科学知识的表示
- en: '![Refer to caption](img/37609e9581b4e212e4fd648c93a3c9ae.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37609e9581b4e212e4fd648c93a3c9ae.png)'
- en: 'Figure 2: Taxonomy of scientific knowledge and its representations.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：科学知识及其表示的分类。
- en: 'Scientific knowledge is usually represented as equations, such as differential
    equations. Another important representation tool of scientific knowledge is simulation.
    Simulation mimics a real-world physics system and is treated as a surrogate representation
    of the physics principles that govern the real-world physics system. We visualize
    the taxonomy of scientific knowledge and its representations in Figure [2](#S3.F2
    "Figure 2 ‣ III-B Representation of Scientific Knowledge ‣ III Deep Learning with
    Scientific Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '科学知识通常通过方程表示，例如微分方程。科学知识的另一个重要表示工具是仿真。仿真模仿现实世界的物理系统，并被视为对支配现实世界物理系统的物理原则的替代表示。我们在图 [2](#S3.F2
    "Figure 2 ‣ III-B Representation of Scientific Knowledge ‣ III Deep Learning with
    Scientific Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey") 中可视化了科学知识及其表示的分类。'
- en: III-B1 Mathematical equation
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 数学方程
- en: Equations can include polynomial equations, differential equations, and integral
    equations, among which differential equations are widely explored by existing
    works. Dynamic laws are usually represented by PDEs, which indicate a deterministic
    relationship among different variables. In general, the PDEs are of the form
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 方程可以包括多项式方程、微分方程和积分方程，其中微分方程是现有研究广泛探讨的。动态规律通常由偏微分方程（PDEs）表示，这些方程指示了不同变量之间的确定性关系。一般来说，PDEs
    的形式为
- en: '|  | $\begin{split}&amp;\mathcal{N}_{\bm{x}}(u(\bm{x});\bm{\lambda}(\bm{x}))=f(\bm{x}),\quad\bm{x}\in
    D\\ &amp;\mathcal{B}_{\bm{x}}(u(\bm{x});\bm{\lambda}(\bm{x}))=b(\bm{x}),\quad\bm{x}\in\Gamma\end{split}$
    |  | (1) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\mathcal{N}_{\bm{x}}(u(\bm{x});\bm{\lambda}(\bm{x}))=f(\bm{x}),\quad\bm{x}\in
    D\\ &amp;\mathcal{B}_{\bm{x}}(u(\bm{x});\bm{\lambda}(\bm{x}))=b(\bm{x}),\quad\bm{x}\in\Gamma\end{split}$
    |  | (1) |'
- en: 'where $\bm{x}=\{x_{1},x_{2},...,x_{N}\}$ are $N$ variables involved in the
    system. $\mathcal{N}_{\bm{x}}$ is a general differential operator, and $\mathcal{B}_{\bm{x}}$
    is the boundary condition operator. $D$ represents a physical domain, and $\Gamma=\partial
    D$ represents the boundary of the domain. $\bm{\lambda}(\bm{x})$ represents the
    physics parameters involved in the PDEs. For simple systems, physical parameters
    are constants (i.e., $\bm{\lambda}(\bm{x})=\bm{\lambda}$). $f(\bm{x})$ is a forcing
    term, and $b(\bm{x})$ specifies the boundary condition, e.g., Dirichlet boundary
    conditions for the Darcy flow problem [[54](#bib.bib54)]. $u(\bm{x})$ is the solution
    of the differential equation given the specified boundary conditions. The equations
    become ordinary differential equations (ODEs) when only one variable is present.
    When terms (e.g., physics parameters $\bm{\lambda}$) exist in differential equations
    that undergo stochastic processes, the equations become stochastic differential
    equations (SDEs). The general form of SDEs resembles standard differential equations,
    except for the random event $\omega$:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x}=\{x_{1},x_{2},...,x_{N}\}$ 是系统中涉及的$N$个变量。$\mathcal{N}_{\bm{x}}$ 是一个一般的微分算子，而$\mathcal{B}_{\bm{x}}$
    是边界条件算子。$D$ 表示物理领域，$\Gamma=\partial D$ 表示领域的边界。$\bm{\lambda}(\bm{x})$ 表示涉及偏微分方程的物理参数。对于简单系统，物理参数是常数（即$\bm{\lambda}(\bm{x})=\bm{\lambda}$）。$f(\bm{x})$
    是强迫项，$b(\bm{x})$ 指定了边界条件，例如达西流问题的迪里赫特边界条件[[54](#bib.bib54)]。$u(\bm{x})$ 是在指定的边界条件下微分方程的解。当只有一个变量存在时，这些方程变成常微分方程（ODEs）。当微分方程中存在经历随机过程的项（例如，物理参数$\bm{\lambda}$）时，这些方程变成随机微分方程（SDEs）。SDEs的一般形式类似于标准微分方程，但有一个随机事件$\omega$：
- en: '|  | $\begin{split}&amp;\mathcal{N}_{\bm{x}}(u(\bm{x};\omega);\bm{\lambda}(\bm{x};\omega))=f(\bm{x};\omega),\quad\bm{x}\in\mathcal{D},\quad\omega\in\Omega\\
    &amp;\mathcal{B}_{\bm{x}}(u(\bm{x};\omega);\bm{\lambda}(\bm{x};\omega))=b(\bm{x};\omega),\quad\bm{x}\in\Gamma\end{split}$
    |  | (2) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\mathcal{N}_{\bm{x}}(u(\bm{x};\omega);\bm{\lambda}(\bm{x};\omega))=f(\bm{x};\omega),\quad\bm{x}\in\mathcal{D},\quad\omega\in\Omega\\
    &amp;\mathcal{B}_{\bm{x}}(u(\bm{x};\omega);\bm{\lambda}(\bm{x};\omega))=b(\bm{x};\omega),\quad\bm{x}\in\Gamma\end{split}$
    |  | (2) |'
- en: The physics parameters $\bm{\lambda}(\bm{x};\omega)$ and forcing terms $f(\bm{x};\omega)$
    are modeled as random processes, and thus the solution $u(\bm{x};\omega)$ follows
    stochastic processes specified by $f$ and $\bm{\lambda}$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 物理参数$\bm{\lambda}(\bm{x};\omega)$和强迫项$f(\bm{x};\omega)$被建模为随机过程，因此解$u(\bm{x};\omega)$遵循由$f$和$\bm{\lambda}$指定的随机过程。
- en: Differential equations describe a system’s evolution over time when we explicitly
    set one of the variables to correspond to time $t$, which commonly occurs in different
    dynamic systems. For example, the Euler-Lagrange equation defines the dynamics
    of a Lagrangian system,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 微分方程描述了系统随时间的演化，当我们明确将一个变量设置为时间$t$时，这种情况常见于不同的动态系统。例如，欧拉-拉格朗日方程定义了拉格朗日系统的动力学，
- en: '|  | $\frac{\partial L}{\partial x}-\frac{d}{dt}\frac{\partial L}{\partial\dot{x}}=0$
    |  | (3) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L}{\partial x}-\frac{d}{dt}\frac{\partial L}{\partial\dot{x}}=0$
    |  | (3) |'
- en: which connects the derivatives of the Lagrangian $L$ with respect to location
    in generalized coordinates $x$, time $t$, and generalized momentum $\dot{x}$.
    Differential equations have been widely explored by existing works, such as Newton’s
    second law [[16](#bib.bib16), [34](#bib.bib34)], Burgers’ equation occurring in
    gas and fluid dynamics [[86](#bib.bib86)], Hamilton’s equations in Hamiltonian
    dynamics [[25](#bib.bib25)], Euler-Lagrange equations for Lagrangian dynamics [[102](#bib.bib102)],
    and the Lorenz equations, which describe a nonlinear chaotic system used for atmospheric
    convection [[103](#bib.bib103)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程将拉格朗日量$L$的导数与广义坐标$x$、时间$t$和广义动量$\dot{x}$相关联。现有的研究广泛探讨了微分方程，例如牛顿第二定律[[16](#bib.bib16)、[34](#bib.bib34)]、在气体和流体动力学中出现的伯杰斯方程[[86](#bib.bib86)]、哈密顿动力学中的哈密顿方程[[25](#bib.bib25)]、拉格朗日动力学中的欧拉-拉格朗日方程[[102](#bib.bib102)]以及描述用于大气对流的非线性混沌系统的洛伦兹方程[[103](#bib.bib103)]。
- en: Laws of light are also expressed as equations. Fermat’s principle considers
    the integration over a light path. In [[97](#bib.bib97)], the transient $I$ in
    a transient imaging system is characterized through an integration as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 光的规律也可以用方程表示。费马原理考虑了光路径上的积分。在[[97](#bib.bib97)]中，瞬态成像系统中的瞬态$I$通过积分来表征，
- en: '|  | $I(\tau;\bm{v})=\int_{\bm{x}\in\chi}f(\bm{x};\bm{v})\delta(\tau-\tau({\bm{x};\bm{v}}))dA(p,q)$
    |  | (4) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(\tau;\bm{v})=\int_{\bm{x}\in\chi}f(\bm{x};\bm{v})\delta(\tau-\tau({\bm{x};\bm{v}}))dA(p,q)$
    |  | (4) |'
- en: where $\tau$ is the traveled pathlength and $\bm{v}$ is the visible point. $A(p,q)$
    measures the unit area of a surface, and the function $f$ absorbs reflectance
    and shading. The rendering equation represented as an integral equation has also
    been considered [[104](#bib.bib104)]. Equality algebraic equation has also been
    considered. According to the law of reflection, an image $I$ with reflections
    is a sum of the glass reflected back scene $\hat{R}$ and the glass transmitted
    front scene $\hat{T}$, i.e.,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\tau$是行进路径长度，$\bm{v}$是可见点。$A(p,q)$测量表面的单位面积，函数$f$吸收反射率和阴影。表示为积分方程的渲染方程也已被考虑[[104](#bib.bib104)]。等式代数方程也已被考虑。根据反射定律，具有反射的图像$I$是玻璃反射回场景$\hat{R}$和玻璃透射前景$\hat{T}$的总和，即，
- en: '|  | $I(x,y)=\hat{T}(x,y)+\hat{R}(x,y)$ |  | (5) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(x,y)=\hat{T}(x,y)+\hat{R}(x,y)$ |  | (5) |'
- en: Such equality algebraic equation is demonstrated to be helpful in reflection
    removal tasks [[98](#bib.bib98)]. Another example is Malus’ law, expressed as
    an algebraic equation, which defines the effect of polarization [[99](#bib.bib99)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种等式代数方程被证明在反射去除任务中是有帮助的[[98](#bib.bib98)]。另一个例子是马吕斯定律，作为代数方程表达，定义了偏振的效果[[99](#bib.bib99)]。
- en: III-B2 Simulation engines
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 模拟引擎
- en: Besides explicitly representing physics laws with equations, simulation through
    engines is another method of representation. Simulation mimics a real physical
    system governed by physics laws and is hence considered a surrogate representation
    of the knowledge. A physics engine mainly encodes the governing dynamic laws of
    a physics system, such as rigid-body, soft-body, and fluid. It computes the accelerations,
    velocity, and displacement of an object from forces, by solving the equation of
    motion. Specifically, a physics engine simulates observable kinematics given certain
    causes following governing dynamic rules. To simulate a sequence of motion of
    rigid-body objects given specific forces, a physics engine is considered [[61](#bib.bib61),
    [105](#bib.bib105), [62](#bib.bib62), [106](#bib.bib106), [36](#bib.bib36), [107](#bib.bib107)].
    Most of these simulators are non-differentiable, making them prohibited to be
    employed in an end-to-end deep learning framework. There also exist simulation
    engines that encode kinematic laws for robotic manipulation. These inverse engines,
    however, estimate control actions using kinematic equations such that a desired
    position can be reached, agnostic to the underlying dynamic laws [[35](#bib.bib35)].
    Graphic engines have also been explored. A graphic engine encodes the principled
    projection and illumination models, and renders realistic 2D observations by following
    the governing principles. For example, an engine, governed by laws of reflection,
    was proposed for the generation of faithful image rendering [[98](#bib.bib98)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过方程明确表示物理定律外，使用引擎进行模拟是另一种表示方法。模拟模仿由物理定律支配的真实物理系统，因此被视为知识的替代表示。物理引擎主要编码物理系统的主导动态定律，如刚体、软体和流体。它通过求解运动方程来计算物体的加速度、速度和位移。具体而言，物理引擎模拟给定某些原因后遵循主导动态规则的可观察运动学。为了模拟给定特定力的刚体对象的运动序列，物理引擎被认为是[[61](#bib.bib61),
    [105](#bib.bib105), [62](#bib.bib62), [106](#bib.bib106), [36](#bib.bib36), [107](#bib.bib107)]。这些模拟器大多数是不可微分的，使得它们不能在端到端深度学习框架中使用。也存在编码运动学定律用于机器人操控的模拟引擎。然而，这些逆向引擎使用运动学方程估计控制动作，以便达到期望的位置，与底层动态定律无关[[35](#bib.bib35)]。图形引擎也已经被探索。图形引擎编码了原则性的投影和照明模型，并通过遵循主导原理渲染真实的2D观察。例如，一个由反射定律支配的引擎被提出用于生成真实的图像渲染[[98](#bib.bib98)]。
- en: III-C Integration into Deep Models
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 深度模型的整合
- en: 'To integrate domain-specific scientific knowledge into deep models, existing
    methodologies can be classified into three categories: data-level, architecture-level,
    and training-level integration as shown in Figure [1](#S2.F1 "Figure 1 ‣ II-C
    Knowledge Integration ‣ II Knowledge-augmented Deep Learning ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey"). Decision-level integration is
    rarely considered for scientific knowledge. Below we review methods to integrate
    scientific knowledge into deep models using these methods.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将领域特定的科学知识整合到深度模型中，现有的方法可以分为三类：数据级、架构级和训练级整合，如图[1](#S2.F1 "Figure 1 ‣ II-C
    Knowledge Integration ‣ II Knowledge-augmented Deep Learning ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey")所示。科学知识的决策级整合很少被考虑。下面我们回顾了使用这些方法将科学知识整合到深度模型中的方法。'
- en: III-C1 Data-level Integration
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 数据级集成
- en: One way to harness domain knowledge is to train deep models with data synthetized
    from conventional mechanistic models. Physics-based mechanistic models capturing
    the domain knowledge serve as simulators, and are used for generating synthetic
    data. The simulated data can either be combined with real data to jointly train
    the model or be used independently to pre-train the model through self-supervised
    learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 利用领域知识的一种方法是用从传统力学模型合成的数据来训练深度模型。基于物理的力学模型捕捉了领域知识，作为模拟器用于生成合成数据。这些模拟数据可以与真实数据结合起来，共同训练模型，或者单独使用，通过自监督学习对模型进行预训练。
- en: 'Simulated data has been widely employed for entire training, where no additional
    real training data is required. In computer vision, Mottaghi et al. [[62](#bib.bib62)]
    proposed understanding the force acting on a query object by predicting its long-term
    motion in 3D space as the response to the force, given a static 2D image. An overview
    of its procedure appears in Figure [3](#S3.F3 "Figure 3 ‣ III-C1 Data-level Integration
    ‣ III-C Integration into Deep Models ‣ III Deep Learning with Scientific Knowledge
    ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '模拟数据已被广泛用于整个训练过程中，无需额外的真实训练数据。在计算机视觉中，Mottaghi 等人[[62](#bib.bib62)] 提出了通过预测物体在
    3D 空间中的长期运动作为对力的响应，从静态 2D 图像中理解作用在查询对象上的力。其过程概述见图 [3](#S3.F3 "Figure 3 ‣ III-C1
    Data-level Integration ‣ III-C Integration into Deep Models ‣ III Deep Learning
    with Scientific Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey")。'
- en: '![Refer to caption](img/04843349424e1e71cfbe8b310cdd4c01.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04843349424e1e71cfbe8b310cdd4c01.png)'
- en: 'Figure 3: Understanding physics from 2D images through simulation [[62](#bib.bib62)].
    The game engine takes scene configurations as input and generates in total 66
    Newtonian scenarios as an abstraction of the real physical world. The Newtonian
    neural network learns to understand the physics by classifying a static image
    into one of the 66 possible Newtonian scenarios.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：通过模拟从 2D 图像理解物理[[62](#bib.bib62)]。游戏引擎以场景配置作为输入，总共生成 66 种牛顿场景，作为真实物理世界的抽象。牛顿神经网络通过将静态图像分类为
    66 种可能的牛顿场景之一来学习理解物理。
- en: The Blender¹¹1http://www.blender.org/ game engine, consisting of a physics and
    a graphic engine, is employed for the simulation. Specifically, the physics engine
    takes a scene configuration as input (e.g., a ball sliding on a slide) and simulates
    the scene forward in time according to motion equations derived from Newton’s
    second law to produce different Newtonian scenarios. Physics parameters (e.g.,
    force magnitude) are randomly sampled. For each Newtonian scenarios, the graphic
    engine renders 2D videos from the synthetic 3D world under different viewpoints
    in perspective projection. In total, 66 synthetic videos corresponding to 12 possible
    physical generated scenarios are used for training. Similarly, to better understand
    the human interaction with objects from 2D videos, Ehsani et al. [[61](#bib.bib61)]
    proposed a physical understanding of actions by inferring contact points together
    with forces from videos. A forward physics simulation is applied to supervise
    the force estimation, given observed videos, without requiring GT labels for forces.
    In particular, the 3D contact points of a moving object are estimated over a period
    given estimated forces, through physics simulation which is governed by Newton’s
    second law. A projection operator is then applied transforming the estimated 3D
    keypoints into a 2D space. By minimizing the difference between the estimated
    and observed 2D contact points over time, physically consistent forces are obtained.
    To perform differentiable physics simulations, a finite difference method is applied
    for gradient computation using the PyBullet²²2https://pybullet.org/wordpress/
    simulator, which is focused on rigid-body simulation following Newton’s second
    law. In a similar vein, Tobin et al. [[63](#bib.bib63)] showed the effectiveness
    of synthetic samples generated by robotic simulation for training a deep model
    for object localization tasks, which is instrumental in robotic manipulation.
    Simulation is performed by employing the MuJoCo physics engine,³³3https://mujoco.org/,
    whereby simulated 2D images are generated based on its built-in graphic engine [[108](#bib.bib108)].
    In particular, the MuJoCo physics engine is built based on Newtonian mechanics.
    The built-in graphic engine renders a 2D image given a selected camera in the
    3D virtual environment through a perspective projection. To ensure sufficient
    simulation variabilities, a domain randomization strategy is proposed, where simulation
    parameters, such as the positions and orientation of objects, are all randomly
    specified during the simulation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Blender¹¹1http://www.blender.org/ 游戏引擎由物理引擎和图形引擎组成，用于模拟。具体而言，物理引擎将场景配置作为输入（例如，球在滑梯上滑动），并根据从牛顿第二定律推导出的运动方程将场景向前推进，以生成不同的牛顿场景。物理参数（例如，力的大小）是随机采样的。对于每个牛顿场景，图形引擎从合成的3D世界中在不同视角下以透视投影渲染2D视频。总共使用66个合成视频，涵盖12种可能的物理生成场景用于训练。类似地，为了更好地理解人类与物体的交互，Ehsani等人[[61](#bib.bib61)]
    提出了通过从视频中推断接触点和力来进行物理理解的方案。给定观察到的视频，应用前向物理模拟来监督力的估计，无需对力进行GT标签。特别地，给定估计的力，通过物理模拟（由牛顿第二定律支配）估计移动物体在一段时间内的3D接触点。然后应用投影算子，将估计的3D关键点转换到2D空间。通过最小化估计的和观察到的2D接触点之间的差异，获得物理一致的力。为了进行可微分的物理模拟，采用有限差分法进行梯度计算，使用PyBullet²²2https://pybullet.org/wordpress/
    模拟器，专注于遵循牛顿第二定律的刚体模拟。同样，Tobin等人[[63](#bib.bib63)]展示了通过机器人模拟生成的合成样本在训练深度模型进行物体定位任务中的有效性，这对于机器人操作至关重要。模拟是通过使用MuJoCo物理引擎³³3https://mujoco.org/
    进行的，通过其内置图形引擎生成模拟的2D图像[[108](#bib.bib108)]。特别地，MuJoCo物理引擎是基于牛顿力学构建的。内置图形引擎通过透视投影根据在3D虚拟环境中选择的相机渲染2D图像。为了确保足够的模拟变异性，提出了领域随机化策略，其中在模拟过程中，物体的位置和方向等模拟参数都是随机指定的。
- en: Deep model training often starts with a pre-training stage, followed by a fine-tuning.
    Existing studies also show that pre-training affects the final performance of
    deep models, mainly because poor pre-training can lead models to anchor in a local
    optimum. Pre-training through simulated data has shown to help improve parameters’
    initialization. Jia et al. [[56](#bib.bib56), [57](#bib.bib57)] introduced a physics-guided
    recurrent neural network (PGRNN) to model lake temperature dynamics. PGRNN is
    pre-trained on synthetic data generated from a physics-based mechanistic simulator
    and is then fine-tuned with some observation data. The simulator models the lake
    temperature dynamics as a function of physics parameters (e.g., water clarity
    and wind sheltering) through PDEs. It was shown that, even with synthetic data
    generated with an imperfect set of physics parameters, PGRNN still achieves competitive
    performance. Such an idea has also been explored in engineering disciplines. In
    robotics, Bousmalis et al. [[58](#bib.bib58)] showed that the observation data
    required for accurate object grasping is significantly reduced (by a factor of
    50) through physics-guided initialization. In autonomous driving, Shah et al. [[59](#bib.bib59)]
    pre-trained a driving algorithm with synthetic samples generated by a proposed
    simulator built on a game engine with physics laws embedded. In particular, the
    simulator includes a vehicle model and a physics engine. To describe a virtual
    3D environment, physics parameters, such as gravity, air density, air pressure,
    and magnetic field, are specified manually. With these specified parameters, the
    physics engine predicts the kinematic states, given forces and torques estimated
    from a vehicle model by following motion equations derived from Newton’s second
    law. This work showed that the data needs of driving algorithms can be drastically
    lessened through pre-training with simulated samples. In addition to augmenting
    data through simulation with physics engines, synthetic data can be generated
    from mathematical equations [[14](#bib.bib14)].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型训练通常从预训练阶段开始，然后进行微调。现有研究还表明，预训练影响深度模型的最终表现，主要是因为糟糕的预训练可能导致模型陷入局部最优。通过模拟数据进行预训练已被证明有助于改善参数初始化。贾等人[[56](#bib.bib56)，[57](#bib.bib57)]介绍了一种物理指导的递归神经网络（PGRNN）来建模湖泊温度动态。PGRNN在基于物理的机械模拟器生成的合成数据上进行预训练，然后用一些观察数据进行微调。模拟器通过
    PDEs 将湖泊温度动态建模为物理参数（例如水的清晰度和风的遮挡）的函数。研究表明，即使使用的是带有不完美物理参数的合成数据，PGRNN 仍然能够取得有竞争力的表现。这种思想在工程学科中也有探索。在机器人技术中，Bousmalis
    等人[[58](#bib.bib58)] 表示，通过物理指导的初始化，准确物体抓取所需的观察数据显著减少（减少了 50 倍）。在自动驾驶中，Shah 等人[[59](#bib.bib59)]
    使用通过嵌入物理法则的游戏引擎构建的模拟器生成的合成样本对驾驶算法进行预训练。特别是，该模拟器包括一个车辆模型和一个物理引擎。为了描述虚拟 3D 环境，物理参数，如重力、空气密度、气压和磁场，都是手动指定的。通过这些指定的参数，物理引擎预测运动状态，依据车辆模型估计的力和力矩，并遵循从牛顿第二定律推导出的运动方程。该工作表明，通过模拟样本的预训练，可以显著减少驾驶算法的数据需求。除了通过物理引擎的模拟来扩充数据外，合成数据还可以通过数学方程生成[[14](#bib.bib14)]。
- en: III-C2 Architecture-level Integration
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 架构级别整合
- en: Domain knowledge can also be integrated through a customized design of neural
    network architectures. Architecture-level integration can be accomplished by 1)
    introducing specific physically meaningful variables or neural network parameters,
    2) introducing layers derived from domain knowledge, and 3) introducing physics-inspired
    connectivity among neurons. We introduce each type of approaches at below.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 领域知识也可以通过定制化设计神经网络架构来整合。架构级别的整合可以通过以下方式实现：1）引入具有物理意义的变量或神经网络参数，2）引入源自领域知识的层，3）引入受物理启发的神经元连接。我们将在下面介绍每种方法。
- en: Integrating through variables or parameters
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过变量或参数进行整合
- en: 'One way to embed physical principles into the architecture of neural networks
    is to introduce physically meaningful variables in neural networks. The variable
    can be the output node of a neural network. Hamiltonian functionality that enforces
    energy conservation has attracted much attention [[25](#bib.bib25), [24](#bib.bib24),
    [30](#bib.bib30), [31](#bib.bib31)]. The Hamiltonian operator in physics is the
    primary tool for modeling dynamic systems with conserved quantities. In Hamiltonian
    mechanics, a classical physical system is described by a generalized coordinate,
    $q$, and a conjugate momenta, $p$. Hamiltonian $H$ then calculates the total energy
    of the system. The Hamiltonian equations defining the dynamics of a system are
    as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将物理原理嵌入神经网络结构的一种方法是引入物理上有意义的变量到神经网络中。这个变量可以是神经网络的输出节点。执行能量守恒的汉密尔顿功能受到了广泛关注[[25](#bib.bib25),
    [24](#bib.bib24), [30](#bib.bib30), [31](#bib.bib31)]。在物理学中，汉密尔顿算符是建模具有守恒量的动态系统的主要工具。在汉密尔顿力学中，一个经典物理系统由广义坐标$q$和共轭动量$p$来描述。汉密尔顿量$H$计算系统的总能量。定义系统动力学的汉密尔顿方程如下：
- en: '|  | $\dot{q}=\frac{\partial H}{\partial p};\quad\dot{p}=-\frac{\partial H}{\partial
    q}$ |  | (6) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\dot{q}=\frac{\partial H}{\partial p};\quad\dot{p}=-\frac{\partial H}{\partial
    q}$ |  | (6) |'
- en: 'Inspired by the Hamiltonian mechanics, a Hamiltonian neural network (HNN) has
    been proposed, where the output represents the Hamiltonian dynamics, through which
    energy conservation is explicitly enforced [[24](#bib.bib24)]. The differences
    between traditional NNs and HNNs can be understood easily, as shown in Figure [4](#S3.F4
    "Figure 4 ‣ Integrating through variables or parameters ‣ III-C2 Architecture-level
    Integration ‣ III-C Integration into Deep Models ‣ III Deep Learning with Scientific
    Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey"),
    which shows that traditional NNs learn to predict particle trajectories, while
    HNNs learn a particle’s Hamiltonian, upon which trajectories can be predicted.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '受到汉密尔顿力学的启发，提出了一种汉密尔顿神经网络（HNN），其中输出表示汉密尔顿动力学，从而明确地执行能量守恒[[24](#bib.bib24)]。传统神经网络（NN）和HNN之间的差异可以很容易理解，如图[4](#S3.F4
    "Figure 4 ‣ Integrating through variables or parameters ‣ III-C2 Architecture-level
    Integration ‣ III-C Integration into Deep Models ‣ III Deep Learning with Scientific
    Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey")所示，该图显示传统神经网络学习预测粒子轨迹，而HNN学习粒子的汉密尔顿量，从而可以预测轨迹。'
- en: '![Refer to caption](img/d155d23113cd7ddaa26040537c4defd3.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d155d23113cd7ddaa26040537c4defd3.png)'
- en: 'Figure 4: Comparison between a Hamiltonian neural network (HNN) and conventional
    neural network (NN). For dynamic modeling, instead of directly predicting trajectories,
    the HNN predicts the Hamiltonian $H$ of a system, through which the trajectory
    can be obtained under the energy conservation constraint.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：汉密尔顿神经网络（HNN）与传统神经网络（NN）的比较。在动态建模中，HNN不是直接预测轨迹，而是预测系统的汉密尔顿量$H$，通过这个量可以在能量守恒约束下获得轨迹。
- en: Choudhary et al. [[25](#bib.bib25)] later showed that an HNN can better handle
    highly nonlinear dynamics in a chaos system by enforcing the conservation of the
    total energy. To demonstrate the practical importance of Hamiltonian formalism,
    Hamiltonian functionality was incorporated into a generative network, where Hamiltonian
    dynamics is learned from 2D observations without domain coordinate assumptions,
    such as images. The proposed Hamiltonian generative network has been applied to
    density estimation, leading to a neural Hamiltonian flow [[24](#bib.bib24)]. By
    leveraging Hamiltonian formalism, the total probability can be easily conserved,
    while the density modeling remains expressive. Sharing a similar idea with HNNs,
    a generalized energy conservation employing Lagrangian mechanics was explored
    in the Lagrangian neural network (LNN) [[32](#bib.bib32), [33](#bib.bib33)], where
    the output of the LNN is the Lagrangian dynamics. Nevertheless, evaluations of
    all these existing proposed models remain conceptual, without real applications
    in practice [[38](#bib.bib38)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Choudhary 等人 [[25](#bib.bib25)] 后来展示了 HNN 通过强制保持总能量的守恒，能够更好地处理混沌系统中的高度非线性动态。为了展示哈密顿形式主义的实际重要性，将哈密顿功能整合到生成网络中，其中哈密顿动态从
    2D 观测中学习，而无需域坐标假设，例如图像。所提议的哈密顿生成网络已被应用于密度估计，导致了神经哈密顿流 [[24](#bib.bib24)]。通过利用哈密顿形式主义，总概率可以轻松地得到保持，同时密度建模保持表现力。与
    HNNs 分享类似的思想，Lagrangian 神经网络（LNN） [[32](#bib.bib32), [33](#bib.bib33)] 探索了一种采用拉格朗日力学的广义能量守恒，其中
    LNN 的输出是拉格朗日动态。然而，所有这些现有提出的模型的评估仍然是概念性的，没有实际应用 [[38](#bib.bib38)]。
- en: Variables can also be intermediate variables in a neural network. Jaques et
    al. [[16](#bib.bib16)] proposed a latent dynamics learning framework called the
    Newtonian Variational Autoencoder (NewtonianVAE). Inspired by Newton’s second
    law, a linear dynamic system in hidden space is defined, specifically by considering
    a rigid-body system with $D$ degrees of freedom and modeling the configuration
    of this system by a set of coordinates, $\bm{x}\in R^{D}$. Its dynamics are defined
    as
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 变量也可以是神经网络中的中间变量。Jaques 等人 [[16](#bib.bib16)] 提出了一个名为牛顿变分自编码器（NewtonianVAE）的潜在动态学习框架。受到牛顿第二定律的启发，在隐藏空间中定义了一个线性动态系统，特别是通过考虑一个具有
    $D$ 自由度的刚体系统，并通过一组坐标 $\bm{x}\in R^{D}$ 来建模该系统的配置。其动态定义为
- en: '|  | $\begin{split}&amp;\frac{d\bm{x}}{dt}=\bm{v}\\ &amp;\frac{d\bm{v}}{dt}=A(\bm{x},\bm{v})\cdot\bm{x}+B(\bm{x},\bm{v})\cdot\bm{v}+C(\bm{x},\bm{v})\cdot\bm{u}\end{split}$
    |  | (7) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\frac{d\bm{x}}{dt}=\bm{v}\\ &amp;\frac{d\bm{v}}{dt}=A(\bm{x},\bm{v})\cdot\bm{x}+B(\bm{x},\bm{v})\cdot\bm{v}+C(\bm{x},\bm{v})\cdot\bm{u}\end{split}$
    |  | (7) |'
- en: where $\bm{u}$ is the given actuation. To incorporate the above dynamic equations
    into a VAE, positions $\bm{x}$ are treated as random variables with velocities
    $\bm{v}$ being intermediate variables. Specifically, velocities are computed as
    $\bm{v}_{t}=\frac{\bm{x}-\bm{x}_{t-1}}{\Delta t}$ with time interval $\Delta t$.
    The conditional distribution of $\bm{x}_{t}$ given $\bm{u}_{t}$ now becomes
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{u}$ 是给定的激励。为了将上述动态方程融入 VAE，位置 $\bm{x}$ 被视为随机变量，而速度 $\bm{v}$ 作为中间变量。具体地，速度计算为
    $\bm{v}_{t}=\frac{\bm{x}-\bm{x}_{t-1}}{\Delta t}$，时间间隔为 $\Delta t$。给定 $\bm{u}_{t}$
    的情况下，$\bm{x}_{t}$ 的条件分布变为
- en: '|  | $\begin{split}&amp;p(\bm{x}_{t}&#124;\bm{x}_{t-1},\bm{u}_{t-1};\bm{v}_{t})\sim\mathcal{N}(\bm{x}_{t-1}+\Delta
    t\cdot\bm{v}_{t},\sigma^{2})\\ &amp;\text{with}\quad\bm{v}_{t}=\bm{v}_{t-1}+\Delta
    t\cdot(A\bm{x}_{t-1}+B\bm{v}_{t-1}+C\bm{u}_{t-1})\end{split}$ |  | (8) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;p(\bm{x}_{t}&#124;\bm{x}_{t-1},\bm{u}_{t-1};\bm{v}_{t})\sim\mathcal{N}(\bm{x}_{t-1}+\Delta
    t\cdot\bm{v}_{t},\sigma^{2})\\ &amp;\text{with}\quad\bm{v}_{t}=\bm{v}_{t-1}+\Delta
    t\cdot(A\bm{x}_{t-1}+B\bm{v}_{t-1}+C\bm{u}_{t-1})\end{split}$ |  | (8) |'
- en: 'where $A$, $B$, and $C$ are estimated through a neural network, $f$, whose
    input consists of current system states (i.e., $\{\bm{x}_{t},\bm{v}_{t},\bm{u}_{t}$}).
    Newtonian VAE then outputs 2D images given the estimated $\bm{x}$. This is in
    contrast with the existing approaches that simply assume a Gaussian distribution
    over $p(\bm{x}_{t}|\bm{x}_{t-1},\bm{u}_{t-1})$ without considering the underlying
    deterministic relationships among positions, forces, and velocities, as shown
    in Eq. ([8](#S3.E8 "In Integrating through variables or parameters ‣ III-C2 Architecture-level
    Integration ‣ III-C Integration into Deep Models ‣ III Deep Learning with Scientific
    Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey")).
    Similarly, to incorporate the physics behind the transport dynamics governed by
    advection-diffusion PDEs, Liu et al. [[15](#bib.bib15)] proposed a learning framework
    based on an autoencoder with the advection-diffusion equation explicitly incorporated.
    Two hidden variable outputs from the encoder are physically meaningful, representing
    velocity field and diffusion field, respectively, within the advection-diffusion
    equation. To model lake temperature, Daw et al. [[17](#bib.bib17)] introduced
    a physically meaningful intermediate variable for the proposed monotonicity-preserving
    long short term memory (LSTM) architecture. Specifically, the density value, as
    the intermediate variable of the LSTM, is enforced to monotonically increase as
    the depth increases, which is a crucial characteristic of lake temperature. A
    similar idea is applied in modeling drag forces acting on each particle in moving
    fluids [[18](#bib.bib18)]. Muralidhar et al. [[18](#bib.bib18)] proposed a PhyNet,
    where physics-constrained intermediate variables are introduced into a convolutional
    neural network (CNN) architecture. Specifically, two intermediate variables, characterizing
    velocity field and pressure field, respectively, are introduced into CNN for drag
    force prediction.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$、$B$ 和 $C$ 通过神经网络 $f$ 估计，$f$ 的输入包括当前系统状态（即 $\{\bm{x}_{t},\bm{v}_{t},\bm{u}_{t}$））。牛顿变分自编码器（VAE）然后根据估计的
    $\bm{x}$ 输出 2D 图像。这与现有方法形成对比，现有方法仅假设 $p(\bm{x}_{t}|\bm{x}_{t-1},\bm{u}_{t-1})$
    的高斯分布，而不考虑位置、力和速度之间的基本确定性关系，如 Eq. ([8](#S3.E8 "在通过变量或参数集成 ‣ III-C2 体系结构级集成 ‣ III-C
    集成到深度模型 ‣ III 深度学习与科学知识 ‣ 知识增强的深度学习及其应用：综述")) 所示。类似地，为了纳入由对流扩散 PDE 控制的运输动力学背后的物理学，刘等人
    [[15](#bib.bib15)] 提出了一个基于自编码器的学习框架，并显式地将对流扩散方程纳入其中。编码器的两个隐藏变量输出具有物理意义，分别表示对流扩散方程中的速度场和扩散场。为了建模湖泊温度，Daw
    等人 [[17](#bib.bib17)] 引入了一个具有物理意义的中间变量，用于提出的单调性保持长短期记忆（LSTM）架构。具体来说，作为 LSTM 中间变量的密度值被强制随着深度的增加而单调增加，这是湖泊温度的一个关键特征。类似的想法被应用于建模作用在流体中每个粒子上的阻力
    [[18](#bib.bib18)]。Muralidhar 等人 [[18](#bib.bib18)] 提出了一个 PhyNet，其中将受物理约束的中间变量引入卷积神经网络（CNN）架构。具体来说，两个中间变量，分别表征速度场和压力场，被引入
    CNN 用于阻力预测。
- en: Besides introducing physically meaningful variables, another approach is to
    directly map some of the neural network parameters to physically meaningful parameters.
    These physics parameters can either be non-modifiable during training or be fine-tuned
    through learning from observations. In geophysics, neural networks have been considered
    for modeling the dynamic process of seismic waveform inversion [[23](#bib.bib23)].
    To mimic seismic wave propagation, a theory-guided recurrent neural network (RNN)
    is proposed; an RNN is specially designed for solving the governing differential
    equations with some of the parameters assigned as the physics parameters in governing
    physics equations. In particular, given the wave equation discretized in the time
    domain, the wave field at the next time step (i.e., $u(t+\Delta t)$) is calculated
    in terms of two previous time steps (i.e., $u(t)$ and $u(t-\Delta t)$) as
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了引入具有物理意义的变量外，另一种方法是直接将神经网络的一些参数映射到物理意义的参数。这些物理参数可以在训练过程中不可修改，或者通过从观察中学习进行微调。在地球物理学中，神经网络已被考虑用于建模地震波形反演的动态过程 [[23](#bib.bib23)]。为了模拟地震波传播，提出了一种理论引导的递归神经网络（RNN）；RNN
    专门设计用于求解主导微分方程，其中一些参数被指定为主导物理方程中的物理参数。特别是，考虑到在时间域离散化的波动方程，下一时间步的波场（即 $u(t+\Delta
    t)$）是根据两个先前时间步（即 $u(t)$ 和 $u(t-\Delta t)$）计算的
- en: '|  |  $\begin{split}u(r,t+\Delta t)&amp;=v^{2}(t)\Delta t^{2}[\nabla^{2}u(r,t)\\
    &amp;-s(r,t)\delta(r-r_{s})]+2u(r,t)-u(r,t-\Delta t)\end{split}$  |  | (9) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  $\begin{split}u(r,t+\Delta t)&amp;=v^{2}(t)\Delta t^{2}[\nabla^{2}u(r,t)\\
    &amp;-s(r,t)\delta(r-r_{s})]+2u(r,t)-u(r,t-\Delta t)\end{split}$  |  | (9) |'
- en: 'where $\nabla^{2}$ is the spatial Laplacian operator, $r$ stands for position,
    and $s$ is the source function. The symbolic computation of $u(r,t+\Delta t)$
    given $u(r,t)$ and $u(r,t-\Delta t)$ is directly implemented via a neural network,
    whose trainable parameters correspond to physics parameters $v^{2}\Delta t^{2}$
    in the wave equation Eq. ([9](#S3.E9 "In Integrating through variables or parameters
    ‣ III-C2 Architecture-level Integration ‣ III-C Integration into Deep Models ‣
    III Deep Learning with Scientific Knowledge ‣ Knowledge-augmented Deep Learning
    and Its Applications: A Survey")).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\nabla^{2}$是空间拉普拉斯算子，$r$代表位置，$s$是源函数。给定$u(r,t)$和$u(r,t-\Delta t)$，$u(r,t+\Delta
    t)$的符号计算通过神经网络直接实现，神经网络的可训练参数对应于波动方程（[9](#S3.E9 "在通过变量或参数整合 ‣ III-C2 架构级整合 ‣ III-C
    深度模型中的整合 ‣ III 科学知识深度学习 ‣ 知识增强深度学习及其应用：综述")）中的物理参数$v^{2}\Delta t^{2}$。
- en: Integrating through layers
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过层整合
- en: The most representative type of knowledge that is usually integrated through
    neural network layers involves symmetries. Symmetry usually refers to a set of
    invertible transformations $g$, such as translation, rotation, or scaling.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通常通过神经网络层整合的最具代表性的知识类型涉及对称性。对称性通常指一组可逆变换$g$，如平移、旋转或缩放。
- en: Equivariance and invariance as representative types of symmetry have been widely
    considered. An invariant function is a mapping such that the output space is not
    affected by the symmetry transformations in the input space, and an equivariant
    function relaxes the invariant function. It states a mapping such that symmetries
    in the input space can be preserved in the output space. Mathematically, assume
    a symmetry transformation, $g$, and a function, $F$, mapping from $X$ to $Y$.
    $F$ is then said to be equivariant to $g$ if
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对称性的代表类型——等变性和不变性已被广泛考虑。不变函数是这样一种映射，使得输出空间不受输入空间对称变换的影响，而等变函数则放宽了不变函数的条件。它表示一种映射，使得输入空间中的对称性可以在输出空间中保持。数学上，假设有一个对称变换$g$和一个从$X$到$Y$的函数$F$。$F$如果满足对$g$的等变性，则称$F$对$g$是等变的，如果
- en: '|  | $F(g\cdot x)=g\cdot F(x)$ |  | (10) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(g\cdot x)=g\cdot F(x)$ |  | (10) |'
- en: The symmetry transformation $g$ on the input space $X$ is preserved on the output
    space $Y$. $F$ is said to be invariant if
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输入空间$X$上的对称变换$g$在输出空间$Y$上得以保持。如果$F$不变，则称为不变。
- en: '|  | $F(g\cdot x)=F(x)$ |  | (11) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(g\cdot x)=F(x)$ |  | (11) |'
- en: In other words, the output $y=F(x)$ is not affected by the symmetry transformation,
    $g$, acting upon the input space, $X$. Invariance can be a special case of equivariance,
    and vice versa. For example, transformation $g$ is an identify transformation
    with $g\cdot F(x)=F(x)$.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，输出$y=F(x)$不受作用于输入空间$X$的对称变换$g$的影响。不变性可以是等变性的特例，反之亦然。例如，变换$g$是一个恒等变换，其满足$g\cdot
    F(x)=F(x)$。
- en: 'Equivariant or invariant neural network is designed to preserve symmetries.
    Illumination invariance features were explored [[47](#bib.bib47)] in computer
    vision, where a knowledge-guided convolutional layer is incorporated into existing
    deep models. Consider a day-night domain adaptation problem, and illumination
    changes from source domain to target domain cause a distribution shift. To tackle
    the distribution shift issue, features that are invariant to illumination are
    desired, which can be derived from Kubelka-Munk theory. The Kubelka-Munk theory [[109](#bib.bib109)]
    models material reflections by defining the spectrum of light reflected from an
    object in the viewing direction. The calculation of illumination invariant features
    defined by Kubelka-Munk theory is directly implemented through the proposed Color
    Invariant Convolution (CIConv) layer, as shown in Figure [5](#S3.F5 "Figure 5
    ‣ Integrating through layers ‣ III-C2 Architecture-level Integration ‣ III-C Integration
    into Deep Models ‣ III Deep Learning with Scientific Knowledge ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey").'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 等变或不变神经网络旨在保持对称性。光照不变特征在计算机视觉中进行了探索[[47](#bib.bib47)]，在现有深度模型中结合了知识引导的卷积层。考虑到日夜领域适应问题，光照从源领域到目标领域的变化导致了分布偏移。为了解决分布偏移问题，需要具有光照不变性的特征，这可以从Kubelka-Munk理论中获得。Kubelka-Munk理论[[109](#bib.bib109)]通过定义从物体的视角反射的光谱来建模材料反射。由Kubelka-Munk理论定义的光照不变特征的计算直接通过提出的颜色不变卷积（CIConv）层实现，如图[5](#S3.F5
    "图 5 ‣ 通过层集成 ‣ III-C2 架构级集成 ‣ III-C 集成到深度模型中 ‣ III 深度学习与科学知识 ‣ 知识增强深度学习及其应用：综述")所示。
- en: '![Refer to caption](img/a959beb29098d57f4cca631878052621.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a959beb29098d57f4cca631878052621.png)'
- en: 'Figure 5: A physics layer (marked as red) for illumination invariant features [[47](#bib.bib47)].
    The new layer extracts illumination invariant features given input images and
    is customized based on Kubelka-Munk theory, termed as Color Invariant Convolution
    (CIConv) layer.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一个用于光照不变特征的物理层（标记为红色）[[47](#bib.bib47)]。这个新层从输入图像中提取光照不变特征，并基于Kubelka-Munk理论定制，称为颜色不变卷积（CIConv）层。
- en: Similarly, in turbulence modeling, rotational invariance states that the physics
    of a fluid flow does not depend on the orientation of the coordinate of an observer
    and is a fundamental physical principle. To embed the rotational invariance into
    a neural network, Ling et al. [[41](#bib.bib41)] defined a tensor basis neural
    network (TBNN), where the NN architecture is modified by adding a higher order
    multiplicative layer. Particularly, TBNN has one additional input layer accepting
    the tensor basis, and its last hidden layer performs a pair-wise multiplication
    using this tensor basis input layer to provide the output. The modified architecture
    ensures the prediction lie on a rotationally invariant tensor basis. By incorporating
    rotational invariance, TBNN achieves improved accuracy in predicting the normalized
    Reynolds stress anisotropy tensor. In the application to molecular dynamics, Anderson
    et al. [[46](#bib.bib46)] proposed a rotation invariant neural network, named
    Cormorant, whereby the behavior and properties of complex many-bodied physics
    systems are learned. Each neuron in Cormorant explicitly corresponds to a subset
    of atoms. Given specified neurons, the activation layer is ensured to be covariant
    to rotations, making the proposed Cormorant is guaranteed to be rotationally invariant.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在湍流建模中，旋转不变性表明流体流动的物理性质不依赖于观察者坐标的方向，这是一个基本的物理原理。为了将旋转不变性嵌入神经网络中，Ling等人[[41](#bib.bib41)]定义了一个张量基神经网络（TBNN），其中神经网络架构通过添加一个高阶乘法层进行了修改。特别地，TBNN有一个额外的输入层接受张量基，其最后一个隐藏层使用这个张量基输入层进行成对乘法以提供输出。修改后的架构确保预测落在旋转不变的张量基上。通过融入旋转不变性，TBNN在预测归一化雷诺应力各向异性张量时实现了更高的准确性。在分子动力学应用中，Anderson等人[[46](#bib.bib46)]提出了一种旋转不变神经网络，命名为Cormorant，旨在学习复杂多体物理系统的行为和性质。Cormorant中的每个神经元都明确对应于一部分原子。在指定神经元的情况下，激活层确保对旋转协变，使得所提出的Cormorant保证具有旋转不变性。
- en: 'Equivariance has also been explored. Wang et al. [[39](#bib.bib39)] showed
    that existing spatiotemporal deep models can achieve improved generalization ability
    by incorporating symmetries through equivariant functionality. More specifically,
    they consider four types of equivariance: time and space translation equivariance,
    rotation equivariance, uniform motion equivariance, and scale equivariance. These
    symmetries are incorporated into neural networks using customized equivariant
    layers. Through the composition of equivariant functions of layers, the network
    becomes equivariant.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 也有对等变性的探索。Wang等人[[39](#bib.bib39)]展示了通过等变功能将对称性纳入现有时空深度模型可以提高泛化能力。更具体地，他们考虑了四种类型的等变性：时间和空间平移等变性、旋转等变性、匀速运动等变性和尺度等变性。这些对称性通过定制的等变层被纳入神经网络。通过等变层的功能组合，网络变得具有等变性。
- en: In the real physical world, however, symmetries could be brittle. For example,
    a small perturbation can easily cause the discontinuous transition of a dynamic
    system or break the rotational symmetry of a pendulum system. Small perturbations
    often occur, and these can cause significant difference by accumulating over time.
    As a result, enforcing equivariance through customized layers as a hard inductive
    bias, could be problematic. To address this problem, Finzi et al. [[40](#bib.bib40)]
    recently proposed a soft way to impose equivariance constraints, whereby the proposed
    neural network architecture consists of a hybrid of restrictive layers and flexible
    layers. Restrictive layers are strictly constrained while flexible layers are
    unconstrained. Through a mixture of two types of layers, the equivariance is introduced
    as a flexible inductive bias.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在真实的物理世界中，对称性可能很脆弱。例如，稍微的扰动就能轻易导致动态系统的不连续过渡或打破摆系统的旋转对称性。小扰动经常发生，并且这些扰动会随着时间的推移累积，造成显著差异。因此，通过定制层强制执行等变性作为一种硬性归纳偏置可能会有问题。为了解决这个问题，Finzi等人[[40](#bib.bib40)]最近提出了一种柔性的方法来施加等变性约束，其中提出的神经网络架构由限制性层和灵活层的混合体组成。限制性层受到严格约束，而灵活层则不受约束。通过两种类型层的混合，等变性被引入作为一种灵活的归纳偏置。
- en: Integrating through connectivity
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过连接性进行整合
- en: Given physical dependencies among objects, connectivity among neural network
    neurons can be manually specified. To model dynamics with multiple objects involved,
    physically plausible interactions are employed to design NN connectivity. Neural
    Physics Engine (NPE), a differentiable physics simulator that combines symbolic
    structure with gradient-based learning, has been proposed [[48](#bib.bib48)].
    Different from conventional physics engines based on mechanistic models, NPE is
    realized as a learning-based neural network while remaining generalizable across
    different scenarios. The development of NPE considers the fact that physics is
    Markovian, both temporally and spatially. Temporal Markovian allows the NPE to
    predict system states by only considering the states at the current step. Spatial
    Markovian allows the NPE to factorize the interaction dynamics into pairwise interactions.
    The NPE consists of a symbolic model structure mimicking pairwise interactions
    among objects, realized as a neural network. The NPE takes 2D observations as
    input and performs forward dynamics to predict the motions of objects in the future.
    A very similar work was independently developed for n-body interaction systems [[49](#bib.bib49)].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于对象之间的物理依赖性，神经网络神经元之间的连接可以手动指定。为了模拟涉及多个对象的动态，采用物理上合理的交互来设计神经网络的连接。神经物理引擎（NPE）是一种将符号结构与基于梯度的学习相结合的可微分物理模拟器[[48](#bib.bib48)]。与基于机械模型的传统物理引擎不同，NPE作为一种基于学习的神经网络实现，同时保持跨不同场景的通用性。NPE的开发考虑了物理现象是马尔可夫性的，无论是时间上还是空间上。时间上的马尔可夫性允许NPE仅通过考虑当前步骤的状态来预测系统状态。空间上的马尔可夫性允许NPE将交互动态分解为成对的交互。NPE由模仿对象之间成对交互的符号模型结构组成，作为神经网络实现。NPE以二维观察作为输入，执行前向动态以预测未来对象的运动。一项非常类似的工作也独立开发用于n体交互系统[[49](#bib.bib49)]。
- en: III-C3 Training-level Integration
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 训练级别整合
- en: One of the most common techniques to integrate scientific knowledge into deep
    models is through the training of deep models. In particular, constraints over
    the outputs of deep models are derived from scientific knowledge and are used
    as regularization terms for training a deep model. An augmented training objective
    is generally expressed as
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将科学知识整合到深度模型中的一种常见技术是通过对深度模型进行训练。特别地，深度模型输出的约束源自科学知识，并被用作训练深度模型的正则化项。一个增强的训练目标通常表示为
- en: '|  | $\mathcal{L}=\mathcal{L}_{TRN}(y^{GT},y^{Pred})+\lambda\mathcal{L}_{PHY}(y^{Pred};x)$
    |  | (12) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{TRN}(y^{GT},y^{Pred})+\lambda\mathcal{L}_{PHY}(y^{Pred};x)$
    |  | (12) |'
- en: $\mathcal{L}_{TRN}(y^{GT},y^{Pred})$ represent standard training loss given
    predictions $y^{Pred}$ and ground truth labels $y^{GT}$. For classification task,
    $\mathcal{L}_{TRN}(y^{GT},y^{Pred})$ is usually defined based on the cross-entropy
    loss. The physics-based regularization term $\mathcal{L}_{PHY}(y^{Pred};x)$ corresponds
    to physical constraints, with an adjustable importance coefficient, $\lambda$.
    When the physics-based constraints are independent of inputs $x$, the regularization
    term is reduced to $\mathcal{L}_{PHY}(y^{Pred})$. Through $\mathcal{L}_{PHY}(y^{Pred};x)$,
    training is guided towards producing models with physically consistent outputs.
    The computation of physics-based regularization, $\mathcal{L}_{PHY}(y^{Pred};x)$,
    does not require annotations of observations and hence allows unlabeled data to
    be included in training, reducing dependencies on data. Physics-based regularization,
    $\mathcal{L}_{PHY}(y^{Pred};x)$, can also be employed directly for training deep
    models in a label-free manner.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}_{TRN}(y^{GT},y^{Pred})$ 表示给定预测 $y^{Pred}$ 和真实标签 $y^{GT}$ 的标准训练损失。对于分类任务，$\mathcal{L}_{TRN}(y^{GT},y^{Pred})$
    通常基于交叉熵损失来定义。物理学基础的正则化项 $\mathcal{L}_{PHY}(y^{Pred};x)$ 对应于物理约束，具有一个可调节的重要系数 $\lambda$。当物理约束与输入
    $x$ 无关时，正则化项简化为 $\mathcal{L}_{PHY}(y^{Pred})$。通过 $\mathcal{L}_{PHY}(y^{Pred};x)$，训练被引导到生成具有物理一致性输出的模型。物理学基础的正则化
    $\mathcal{L}_{PHY}(y^{Pred};x)$ 的计算不需要对观察结果进行标注，因此允许在训练中包含未标记的数据，从而减少对数据的依赖。物理学基础的正则化
    $\mathcal{L}_{PHY}(y^{Pred};x)$ 也可以直接用于以无标签的方式训练深度模型。
- en: '$\mathcal{L}_{PHY}(y^{Pred};x)$ can be explicit or implicit and can be flexibly
    employed in different deep learning frameworks. Explicit regularization is directly
    defined over the output of deep models based on domain knowledge, while implicit
    regularization is induced by the physics-based models that are embedded into a
    deep learning pipeline. In the following, we introduce the knowledge-guided model
    regularization under two distinguishable deep model frameworks: discriminative
    deep models and generative deep models.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}_{PHY}(y^{Pred};x)$ 可以是显式的或隐式的，并可以灵活地应用于不同的深度学习框架。显式正则化是基于领域知识直接定义在深度模型的输出上，而隐式正则化则由嵌入深度学习流程中的物理学基础模型引发。接下来，我们介绍在两种不同的深度模型框架下的知识引导模型正则化：判别深度模型和生成深度模型。
- en: Regularization with discriminative deep models
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用判别深度模型的正则化
- en: Model regularization with scientific knowledge is widely seen in discriminative
    deep models. In the context of climate modeling, constraints derived from the
    conservation laws that a physical system should satisfy are encoded as a regularization
    term. In particular, a NN, $f$ maps input $x$ to output $y$ (i.e., $y=f(x)$).
    Conservation constraints $\mathcal{C}$ are summarized as a linear system (i.e.,
    <math   alttext="\mathcal{C}=\{C\begin{bmatrix}x\\
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用科学知识进行模型正则化在判别深度模型中被广泛应用。在气候建模的背景下，源自物理系统应满足的守恒定律的约束被编码为正则化项。特别地，神经网络 (NN)
    $f$ 将输入 $x$ 映射到输出 $y$（即 $y=f(x)$）。守恒约束 $\mathcal{C}$ 被总结为一个线性系统（即，<math   alttext="\mathcal{C}=\{C\begin{bmatrix}x\\
- en: 'y\end{bmatrix}=0\}" display="inline"><semantics ><mrow ><mi >𝒞</mi><mo >=</mo><mrow
    ><mo stretchy="false"  >{</mo><mrow ><mrow ><mi >C</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo >[</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mi >x</mi></mtd></mtr><mtr
    ><mtd ><mi >y</mi></mtd></mtr></mtable><mo >]</mo></mrow></mrow><mo >=</mo><mn
    >0</mn></mrow><mo stretchy="false"  >}</mo></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply ><ci >𝒞</ci><set ><apply ><apply ><ci >𝐶</ci><apply ><csymbol cd="latexml"
    >matrix</csymbol><matrix ><matrixrow ><ci >𝑥</ci></matrixrow><matrixrow ><ci >𝑦</ci></matrixrow></matrix></apply></apply><cn
    type="integer"  >0</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex"
    >\mathcal{C}=\{C\begin{bmatrix}x\\ y\end{bmatrix}=0\}</annotation></semantics></math>),
    where $C$ is a given constraint matrix. These physical constraints are then encoded
    as a regularization term on the NN outputs:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: $ \mathcal{C} = \{ C \begin{bmatrix}x \\ y \end{bmatrix} = 0 \}$，其中 $C$ 是给定的约束矩阵。这些物理约束随后被编码为神经网络输出上的正则化项：
- en: '|  | $\mathcal{L}_{PHY}(x,y)=&#124;&#124;C\begin{bmatrix}x\\ y\end{bmatrix}&#124;&#124;$
    |  | (13) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{PHY}(x,y)=\| C \begin{bmatrix}x \\ y \end{bmatrix} \|$ |  |
    (13) |'
- en: Evaluation results show that by adding this physics-guided regularization, prediction
    performance is improved for emulating cloud processes [[19](#bib.bib19), [20](#bib.bib20)].
    Similarly, Zhang et al. [[21](#bib.bib21)] proposed parameterizing atomic energy
    for molecular dynamics through a NN, whose loss function considers the conservation
    of kinetic and potential energy. Regularization for a physics system with non-conservative
    forces has also been considered [[22](#bib.bib22)]. In a double pendulum system
    with friction, the total energy of a system is decreasing due to the existence
    of friction. Decreasing of energy is formulated as a constraint $E(\bm{x}_{t+1})<E(\bm{x}_{t})$,
    where $E(\bm{x}_{t})$ and $E(\bm{x}_{t+1})$ denote the total energy of the system
    at current time step and a future time step, respectively. Considering a neural
    network which takes the state at current time step (i.e., $\bm{x}_{t}$) as input
    and outputs the estimated state at the next time step (i.e., $\bm{y}:=\hat{\bm{x}}_{t+1}$),
    the constraint can be integrated into the neural network through the regularization
    term as
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果表明，通过添加这种物理引导的正则化，预测性能得到了提高，以模拟云过程 [[19](#bib.bib19), [20](#bib.bib20)]。类似地，张等人 [[21](#bib.bib21)]
    提出了通过神经网络对分子动力学进行原子能量参数化，其损失函数考虑了动能和势能的守恒。对于具有非保守力的物理系统，正则化也已被考虑 [[22](#bib.bib22)]。在带有摩擦的双摆系统中，由于摩擦的存在，系统的总能量在减少。能量减少被表示为一个约束
    $E(\bm{x}_{t+1})<E(\bm{x}_{t})$，其中 $E(\bm{x}_{t})$ 和 $E(\bm{x}_{t+1})$ 分别表示当前时间步和未来时间步的系统总能量。考虑一个神经网络，该网络以当前时间步的状态（即
    $\bm{x}_{t}$）作为输入，并输出下一时间步的估计状态（即 $\bm{y}:=\hat{\bm{x}}_{t+1}$），该约束可以通过正则化项集成到神经网络中，如下所示
- en: '|  | $\mathcal{L}_{PHY}(\bm{x}_{t},\bm{y})=\mathtt{ReLU}(E(\bm{y})-E(\bm{x}_{t}))$
    |  | (14) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{PHY}(\bm{x}_{t},\bm{y})=\mathtt{ReLU}(E(\bm{y})-E(\bm{x}_{t}))$
    |  | (14) |'
- en: where $E(\bm{x}_{t})$ and $E(\bm{y})$ computes the energy of the system at the
    current and next time step, respectively. According to the decreasing of energy,
    $E(\bm{y})$ is expected to be smaller than $E(\bm{x}_{t})$, leading to the constraint
    above. Similarly, a set of common physical properties of a dynamic system is considered
    in [[26](#bib.bib26)], where each of these physical properties is represented
    as an equality or inequality constraint. These physics-informed constraints are
    then incorporated into deep models as regularization via an augmented Lagrangian
    method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E(\bm{x}_{t})$ 和 $E(\bm{y})$ 分别计算当前和下一时间步的系统能量。根据能量的减少，$E(\bm{y})$ 预计会小于
    $E(\bm{x}_{t})$，从而得到上述约束。类似地，在 [[26](#bib.bib26)] 中考虑了一组动态系统的常见物理特性，其中每个物理特性都表示为等式或不等式约束。这些物理知情的约束随后通过增广拉格朗日方法被集成到深度模型中作为正则化。
- en: 'PDEs (as described in Eq. ([1](#S3.E1 "In III-B1 Mathematical equation ‣ III-B
    Representation of Scientific Knowledge ‣ III Deep Learning with Scientific Knowledge
    ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey"))) have been
    widely considered as constraints and are integrated into deep models as regularization
    terms. A physics-informed neural network (PINN) [[27](#bib.bib27)] is proposed
    for solving PDEs by leveraging NNs. A PINN learns solutions, $u$, by using both
    the observed data and PDEs whereby PDEs serve as inductive bias. Consider the
    viscous Burgers’ equation as an example [[28](#bib.bib28)]:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: PDEs（如 Eq. ([1](#S3.E1 "在 III-B1 数学方程 ‣ III-B 科学知识表示 ‣ III 深度学习与科学知识 ‣ 知识增强深度学习及其应用：综述")）所述）已被广泛视为约束，并作为正则化项集成到深度模型中。提出了一种物理信息神经网络（PINN）[[27](#bib.bib27)]，利用神经网络解决
    PDEs。PINN 通过使用观测数据和 PDEs 来学习解 $u$，其中 PDEs 作为归纳偏置。以粘性 Burgers 方程为例 [[28](#bib.bib28)]：
- en: '|  | $\frac{\partial u}{\partial t}+u\frac{\partial u}{\partial x}=v\frac{\partial^{2}u}{\partial
    x^{2}}$ |  | (15) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial u}{\partial t}+u\frac{\partial u}{\partial x}=v\frac{\partial^{2}u}{\partial
    x^{2}}$ |  | (15) |'
- en: In a PINN, a feed forward NN predicts a PDE solution, $\tilde{u}$, by taking
    positions $x$ and time $t$ as input. The objective function consists of a data
    loss term and an PDE residual,
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PINN 中，前馈神经网络通过将位置 $x$ 和时间 $t$ 作为输入来预测 PDE 解 $\tilde{u}$。目标函数由数据损失项和 PDE 残差组成，
- en: '|  | $\mathcal{L}=\mathcal{L}_{data}+\lambda\mathcal{L}_{PDE}$ |  | (16) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{data}+\lambda\mathcal{L}_{PDE}$ |  | (16) |'
- en: 'where $\mathcal{L}_{data}$ measures the difference between predicted PDE solution
    $\tilde{u}$ and given $u$ at certain positions and time step (i.e., $\mathcal{L}_{data}=\sum_{i=1}^{N_{data}}(\tilde{u}(x_{i},t_{i})-u_{i})^{2}$).
    $\mathcal{L}_{PDE}$ measures the PDE residual of predicted solution $\tilde{u}$
    at position and time step:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{data}$ 衡量了预测 PDE 解 $\tilde{u}$ 和给定的 $u$ 在特定位置和时间步长之间的差异（即 $\mathcal{L}_{data}=\sum_{i=1}^{N_{data}}(\tilde{u}(x_{i},t_{i})-u_{i})^{2}$）。$\mathcal{L}_{PDE}$
    衡量了预测解 $\tilde{u}$ 在位置和时间步长下的 PDE 残差：
- en: '|  | $\mathcal{L}_{PDE}=\sum_{j=1}^{N_{PDE}}(\frac{\partial\tilde{u}}{\partial
    t}+u\frac{\partial\tilde{u}}{\partial x}-v\frac{\partial^{2}\tilde{u}}{\partial
    x^{2}})^{2}&#124;_{(x_{j},t_{j})}$ |  | (17) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{PDE}=\sum_{j=1}^{N_{PDE}}(\frac{\partial\tilde{u}}{\partial
    t}+u\frac{\partial\tilde{u}}{\partial x}-v\frac{\partial^{2}\tilde{u}}{\partial
    x^{2}})^{2}&#124;_{(x_{j},t_{j})}$ |  | (17) |'
- en: Partial derivatives are calculated through numerical estimator given predicted
    solution $\tilde{u}$. Data points $\{(x_{i},t_{i})\}_{i=1}^{N_{data}}$ for data
    loss and $\{(x_{j},t_{j})\}_{j=1}^{N_{PDE}}$ for PDE residual are collected separately.
    $\lambda$ is the coefficient for the regularization term. In PINN, PDEs are directly
    encoded as a regularization term by measuring the solution residuals to constrain
    the model parameters. In the context of human body pose estimation, physics mechanics
    is employed to ensure physically plausible estimations, where the Euler-Lagrange
    equation represented as an ODE is derived, and is encoded as a soft constraint
    for model regularization. By integrating the Euler-Lagrange equation into data-driven
    deep models, the estimated 3D body pose is ensured to be physically plausible [[29](#bib.bib29)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过给定预测解 $\tilde{u}$ 的数值估计器计算偏导数。数据点 $\{(x_{i},t_{i})\}_{i=1}^{N_{data}}$ 用于数据损失，而
    $\{(x_{j},t_{j})\}_{j=1}^{N_{PDE}}$ 用于 PDE 残差。$\lambda$ 是正则化项的系数。在 PINN 中，PDEs
    通过测量解的残差直接编码为正则化项，以约束模型参数。在人体姿态估计的背景下，使用物理力学来确保物理上合理的估计，其中将表示为常微分方程的欧拉-拉格朗日方程推导出来，并作为模型正则化的软约束进行编码。通过将欧拉-拉格朗日方程集成到数据驱动的深度模型中，确保估计的
    3D 身体姿态在物理上是合理的 [[29](#bib.bib29)]。
- en: Physics-guided functions derived from domain knowledge have been used for training
    deep models in a label-free manner. Stewart and Ermon [[34](#bib.bib34)] proposed
    a label-free supervision of NNs with physics equations. The goal of this paper
    was to supervise NNs by specifying constraints $g$ that should hold over the output
    space $f$ instead of using labels. The loss function then becomes
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从领域知识中派生的物理指导函数已被用于以无标签的方式训练深度模型。Stewart 和 Ermon [[34](#bib.bib34)] 提出了使用物理方程进行无标签监督的方法。本文的目标是通过指定应在输出空间
    $f$ 上保持的约束 $g$ 来监督神经网络，而不是使用标签。损失函数因此变为
- en: '|  | $\hat{f}^{*}=\arg\min_{f\in F}\sum_{i=1}^{n}g(x_{i},f(x_{i}))+R(f)$ |  |
    (18) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{f}^{*}=\arg\min_{f\in F}\sum_{i=1}^{n}g(x_{i},f(x_{i}))+R(f)$ |  |
    (18) |'
- en: where $R(f)$ refers to an additional regularization term penalizing model complexity.
    One example provided in the paper is to track an object performing free fall.
    The training of a regression network is formulated as a structured prediction
    problem operating on a sequence of $N$ images (i.e., $(\mathcal{R}^{H\times W\times
    3})^{N}\rightarrow\mathcal{R}^{N}$). Newton’s second law of gravity (free fall
    motion) is represented as the algebraic equation and is directly incorporated
    into the loss function for training. In particular, for an object in free fall,
    its height at $i$-th time step with time interval $\Delta t$ is computed as $y_{i}=y_{0}+v_{0}(i\Delta
    t)+a(i\Delta t)^{2}$, with $y_{0}$ and $v_{0}$ being initial height and velocity.
    $a=-9.8m\backslash s^{2}$ is the fixed acceleration for an object performing free
    fall. Any predicted trajectory $f(\bm{x})$ hence should fit such parabola with
    fixed curvature. A loss is then defined measuring the fitting residual,
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R(f)$ 指的是一个额外的正则化项，用于惩罚模型复杂度。论文中提供的一个例子是跟踪一个自由落体的物体。回归网络的训练被表述为一个结构化预测问题，处理一个由
    $N$ 张图像组成的序列（即 $(\mathcal{R}^{H\times W\times 3})^{N}\rightarrow\mathcal{R}^{N}$）。牛顿的第二定律（自由落体运动）以代数方程的形式表示，并直接纳入训练的损失函数中。特别地，对于一个自由落体的物体，其在第
    $i$ 个时间步的高度，时间间隔为 $\Delta t$，可以计算为 $y_{i}=y_{0}+v_{0}(i\Delta t)+a(i\Delta t)^{2}$，其中
    $y_{0}$ 和 $v_{0}$ 分别是初始高度和速度。$a=-9.8m\backslash s^{2}$ 是自由落体物体的固定加速度。因此，任何预测的轨迹
    $f(\bm{x})$ 应该适应这样的固定曲率的抛物线。然后定义一个损失函数来衡量拟合残差，
- en: '|  | $\begin{split}g(\bm{x},f(\textbf{x}))&amp;=g(f(\bm{x}))=&#124;&#124;\hat{\bm{y}}-f(\bm{x})&#124;&#124;_{1}\\
    &amp;=&#124;&#124;\textbf{a}+A(A^{T}A)^{-1}A^{T}(f(\textbf{x})-\textbf{a})-f(\bm{x})&#124;&#124;_{1}\end{split}$
    |  | (19) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}g(\bm{x},f(\textbf{x}))&amp;=g(f(\bm{x}))=&#124;&#124;\hat{\bm{y}}-f(\bm{x})&#124;&#124;_{1}\\
    &amp;=&#124;&#124;\textbf{a}+A(A^{T}A)^{-1}A^{T}(f(\textbf{x})-\textbf{a})-f(\bm{x})&#124;&#124;_{1}\end{split}$
    |  | (19) |'
- en: where
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: where
- en: '|  | <math   alttext="A=\begin{bmatrix}\triangle t&amp;1\\ 2\triangle t&amp;1\\'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="A=\begin{bmatrix}\triangle t&amp;1\\ 2\triangle t&amp;1\\'
- en: 3\triangle t&amp;1\\
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 3\triangle t&amp;1\\
- en: '...\\'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '...\\'
- en: N\triangle t&amp;1\end{bmatrix}" display="block"><semantics ><mrow ><mi  >A</mi><mo
    >=</mo><mrow ><mo  >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mrow ><mi mathvariant="normal" >△</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr ><mtd ><mrow  ><mn
    >2</mn><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >△</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >t</mi></mrow></mtd><mtd ><mn >1</mn></mtd></mtr><mtr
    ><mtd  ><mrow ><mn >3</mn><mo lspace="0em" rspace="0em" >​</mo><mi mathvariant="normal"
    >△</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi></mrow></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >…</mi></mtd></mtr><mtr ><mtd  ><mrow ><mi >N</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi mathvariant="normal" >△</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >t</mi></mrow></mtd><mtd ><mn  >1</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><ci >𝐴</ci><apply
    ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><ci >△</ci><ci
    >𝑡</ci></apply><cn type="integer"  >1</cn></matrixrow><matrixrow ><apply ><cn
    type="integer" >2</cn><ci >△</ci><ci >𝑡</ci></apply><cn type="integer" >1</cn></matrixrow><matrixrow
    ><apply  ><cn type="integer"  >3</cn><ci >△</ci><ci >𝑡</ci></apply><cn type="integer"  >1</cn></matrixrow><matrixrow
    ><ci >…</ci></matrixrow><matrixrow ><apply  ><ci >𝑁</ci><ci >△</ci><ci >𝑡</ci></apply><cn
    type="integer"  >1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >A=\begin{bmatrix}\triangle t&1\\ 2\triangle t&1\\
    3\triangle t&1\\ ...\\ N\triangle t&1\end{bmatrix}</annotation></semantics></math>
    |  | (20) |
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: N\triangle t&amp;1\end{bmatrix}" display="block"><semantics ><mrow ><mi  >A</mi><mo
    >=</mo><mrow ><mo  >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mrow ><mi mathvariant="normal" >△</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi></mrow></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr ><mtd ><mrow  ><mn
    >2</mn><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >△</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >t</mi></mrow></mtd><mtd ><mn >1</mn></mtd></mtr><mtr
    ><mtd  ><mrow ><mn >3</mn><mo lspace="0em" rspace="0em" >​</mo><mi mathvariant="normal"
    >△</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi></mrow></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >…</mi></mtd></mtr><mtr ><mtd  ><mrow ><mi >N</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi mathvariant="normal" >△</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >t</mi></mrow></mtd><mtd ><mn  >1</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><ci >𝐴</ci><apply
    ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><ci >△</ci><ci
    >𝑡</ci></apply><cn type="integer"  >1</cn></matrixrow><matrixrow ><apply ><cn
    type="integer" >2</cn><ci >△</ci><ci >𝑡</ci></apply><cn type="integer" >1</cn></matrixrow><matrixrow
    ><apply  ><cn type="integer"  >3</cn><ci >△</ci><ci >𝑡</ci></apply><cn type="integer"  >1</cn></matrixrow><matrixrow
    ><ci >…</ci></matrixrow><matrixrow ><apply  ><ci >𝑁</ci><ci >△</ci><ci >𝑡</ci></apply><cn
    type="integer"  >1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >A=\begin{bmatrix}\triangle t&1\\ 2\triangle t&1\\
    3\triangle t&1\\ ...\\ N\triangle t&1\end{bmatrix}</annotation></semantics></math>
    |  | (20) |
- en: and a = [$a\triangle t^{2},a(2\triangle t)^{2},...,a(N\triangle t)^{2}$] with
    $a=-9.8m\backslash s^{2}$. Besides, the algorithmic supervisions derived based
    on well-established algorithms have been used for training a neural network such
    that direct supervisions from ground truth annotations are no longer required [[100](#bib.bib100)].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 $a = [$a\triangle t^{2},a(2\triangle t)^{2},...,a(N\triangle t)^{2}$]，其中
    $a=-9.8m\backslash s^{2}$。此外，基于成熟算法推导出的算法监督已用于训练神经网络，这样就不再需要来自真实标注的直接监督 [[100](#bib.bib100)]。
- en: 'For all the related works discussed above, the relative importance between
    generic knowledge and data information is pre-defined by the design of training
    objective, and is not adjustable after training. However, the relative importance
    could vary with different inputs. For example, given an unseen input, data-based
    prediction is less reliable and knowledge is playing a more important role in
    the final prediction. Seo et al. [[22](#bib.bib22)] proposed a framework where
    the relative importance of generic knowledge compared to data information is adjusted
    by a controlling parameter $\alpha$. The controlling parameter $\alpha$ is assumed
    to be a random variable following a pre-defined distribution $p(\alpha)$. Two
    sets of latent representations $\bm{z}_{r}$ and $\bm{z}_{d}$ extracted from input
    data $\bm{x}$ correspond to generic knowledge and data information, respectively.
    The final latent features are obtained as $\bm{z}=\alpha\bm{z}_{r}\oplus(1-\alpha)\bm{z}_{d}$
    and are used for generating final predictions $\bm{\hat{y}}$. Two sets of losses
    $\mathcal{L}_{rules}$ and $\mathcal{L}_{data}$ are defined based on generic knowledge
    and annotations from downstream tasks, respectively. $\mathcal{L}_{rules}$, being
    a function of inputs $\bm{x}$ and outputs $\bm{y}$, measures the violation of
    the rules derived based on prior knowledge specific to a target downstream task
    (e.g., Eq. [14](#S3.E14 "In Regularization with discriminative deep models ‣ III-C3
    Training-level Integration ‣ III-C Integration into Deep Models ‣ III Deep Learning
    with Scientific Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey")). The final training loss is computed as the expected loss over $p(\alpha)$,
    i.e.,'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述所有相关工作，通用知识和数据信息之间的相对重要性由训练目标的设计预先定义，并且在训练后不可调整。然而，相对重要性可能会随着不同的输入而变化。例如，对于未见过的输入，基于数据的预测不太可靠，知识在最终预测中扮演了更重要的角色。Seo
    等人 [[22](#bib.bib22)] 提出了一个框架，其中通用知识与数据信息的相对重要性由控制参数 $\alpha$ 调整。控制参数 $\alpha$
    被假设为一个遵循预定义分布 $p(\alpha)$ 的随机变量。从输入数据 $\bm{x}$ 中提取的两组潜在表示 $\bm{z}_{r}$ 和 $\bm{z}_{d}$
    分别对应于通用知识和数据信息。最终的潜在特征被表示为 $\bm{z}=\alpha\bm{z}_{r}\oplus(1-\alpha)\bm{z}_{d}$
    并用于生成最终预测 $\bm{\hat{y}}$。两个损失集 $\mathcal{L}_{rules}$ 和 $\mathcal{L}_{data}$ 分别基于通用知识和来自下游任务的注释定义。$\mathcal{L}_{rules}$
    作为输入 $\bm{x}$ 和输出 $\bm{y}$ 的一个函数，测量基于目标下游任务特定先验知识推导出的规则的违反情况（例如，参见 Eq. [14](#S3.E14
    "在具有判别深度模型的正则化 ‣ III-C3 训练级别集成 ‣ III-C 深度模型的集成 ‣ III 结合科学知识的深度学习 ‣ 知识增强深度学习及其应用：综述")）。最终训练损失被计算为
    $p(\alpha)$ 上的期望损失，即，
- en: '|  | $\mathcal{L}=\mathbb{E}_{\alpha\sim p(\alpha)}[\alpha\mathcal{L}_{rule}+\rho(1-\alpha)\mathcal{L}_{data}]$
    |  | (21) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathbb{E}_{\alpha\sim p(\alpha)}[\alpha\mathcal{L}_{rule}+\rho(1-\alpha)\mathcal{L}_{data}]$
    |  | (21) |'
- en: where $\rho$ is a scale parameter to balance the units of two loss terms. Through
    the proposed framework, $\alpha$ denoting the relative importance becomes a variable
    during testing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho$ 是一个规模参数，用于平衡两个损失项的单位。通过提出的框架，表示相对重要性的 $\alpha$ 在测试过程中成为一个变量。
- en: The physics-guided regularization terms discussed above are all explicitly defined
    over the output space of deep models. Physics-guided regularization terms can
    also be implicit (i.e., induced by a physics-based model embedded as one intermediate
    primitive of a neural network pipeline) [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)].
    For example, Wu et al. [[36](#bib.bib36)] constructed a system for understanding
    physical scenes without human annotations. The core of the system is a physical
    world representation that is first recovered by a perception module and then utilized
    by a simulation engine. The perception module is a deep neural network that is
    self-supervised without annotation. The simulation engine, consisting of a physics
    engine and graphics engine, is aimed at generating physics predictions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论的物理引导正则化项都是在深度模型的输出空间上明确定义的。物理引导正则化项也可以是隐式的（即，通过作为神经网络管道中一个中间原语嵌入的基于物理的模型诱导的）[[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37)]。例如，Wu 等人 [[36](#bib.bib36)] 构建了一个系统，用于在没有人工注释的情况下理解物理场景。该系统的核心是一个物理世界表示，该表示首先通过感知模块恢复，然后由模拟引擎使用。感知模块是一个自监督的深度神经网络，没有注释。模拟引擎由物理引擎和图形引擎组成，旨在生成物理预测。
- en: '![Refer to caption](img/216b2594ee587e9cf5b56cb15b6a03a7.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/216b2594ee587e9cf5b56cb15b6a03a7.png)'
- en: 'Figure 6: Physical scene understanding without human annotations [[36](#bib.bib36)].
    A physics engine and graphic engine are concatenated to a perception model. By
    aligning the rendered RGB images with observed ones, the perception model captures
    meaningful object features without annotations. The knowledge represented by the
    physics and graphic engines is encoded into the loss function implicitly.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：无需人工注释的物理场景理解 [[36](#bib.bib36)]。物理引擎和图形引擎被连接到感知模型中。通过将渲染的RGB图像与观察到的图像对齐，感知模型在没有注释的情况下捕捉有意义的物体特征。物理和图形引擎所表示的知识隐式地编码到损失函数中。
- en: 'The proposed pipeline is evaluated on a synthetic billiard table experiment,
    as shown in Figure [6](#S3.F6 "Figure 6 ‣ Regularization with discriminative deep
    models ‣ III-C3 Training-level Integration ‣ III-C Integration into Deep Models
    ‣ III Deep Learning with Scientific Knowledge ‣ Knowledge-augmented Deep Learning
    and Its Applications: A Survey"). Given the features extracted by the perception
    model, the physics engine predicts the future physics states of a system by following
    motion equations, and the graphic engine renders RGB images given the predicted
    physics states. The perception model is learned by maximizing the likelihood of
    estimated RGB images given observed sequences.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的流程在合成台球实验中进行了评估，如图 [6](#S3.F6 "图 6 ‣ 使用判别深度模型进行正则化 ‣ III-C3 训练级集成 ‣ III-C
    深度模型集成 ‣ III 深度学习与科学知识 ‣ 知识增强深度学习及其应用：综述")所示。根据感知模型提取的特征，物理引擎通过遵循运动方程预测系统的未来物理状态，而图形引擎则根据预测的物理状态渲染RGB图像。通过最大化给定观察序列的估计RGB图像的似然性来学习感知模型。
- en: Regularization with generative deep models
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用生成对抗深度模型进行正则化
- en: Besides discriminative models, knowledge has also been applied to deep generative
    models. To generate realistic results, different generative models have explored
    the usage of domain knowledge. For example, to efficiently employ a generative
    adversarial network (GAN) for generating solutions of PDE-governed complex systems,
    Wu et al. [[44](#bib.bib44)] proposed a statistically constrained GAN, where a
    statistical regularization term is derived measuring the distance between covariance
    structures of training samples and generated samples, respectively,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了判别模型，知识也被应用于深度生成模型。为了生成逼真的结果，不同的生成模型探讨了领域知识的使用。例如，为了高效地利用生成对抗网络（GAN）生成PDE控制的复杂系统的解，Wu等人 [[44](#bib.bib44)]
    提出了一个统计约束GAN，其中导出了一个统计正则化项，用于测量训练样本和生成样本的协方差结构之间的距离。
- en: '|  | $\mathcal{L}_{c}(D,G)=\mathcal{L}(D,G)+\lambda d(\Sigma(p_{data}),\Sigma(p_{G}))$
    |  | (22) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{c}(D,G)=\mathcal{L}(D,G)+\lambda d(\Sigma(p_{data}),\Sigma(p_{G}))$
    |  | (22) |'
- en: where $D$ and $G$ represent standard discriminator and generator, respectively,
    and $L(D,G)$ represent the standard training loss for GAN. $\Sigma(p_{data})$
    and $\Sigma(p_{G})$ represents the covariance structures of the distributions
    of training data and generated samples, respectively. $d(\cdot)$ represents a
    distance measurement, such as the Frobenius norm. $\lambda$ denotes the coefficient
    of the regularization term. Introducing the statistical constraints on the lower
    dimensional manifold (i.e., covariance structure) helps reduce the searching space
    for finding desired solutions in high dimension. As a result, not only is the
    amount of data reduced, but the training time needed to converge on solutions
    is shorten as well, as demonstrated in solving turbulence PDEs. In the application
    to computational materials science, Shah et al. [[43](#bib.bib43)] proposed a
    deep generative model named InvNet, through which synthetic structured samples
    satisfying desired physical properties are generated. InvNet is an extension of
    conventional GAN, whereby an additional invariance checker is introduced, along
    with a traditional generator and discriminator. The invariance checker is introduced
    as an intermediate primitive, based on which an implicit knowledge-guided regularization
    term is defined. An invariance loss is defined based on the proposed invariance
    checker, measuring the violation of invariance. Through the invariance loss, the
    generated samples are encouraged to satisfy certain invariances (such as motif
    invariance, planting a predefined motif in all synthetic images at a fixed location).
    Besides knowledge represented as PDEs, physical connectivity and stability have
    been considered for realistic 3D shape generation. Mezghanni et al. [[42](#bib.bib42)]
    proposed incorporating physical constraints into a deep generative model, whereby
    the physical constraints capture both the connectivity of 3D components and the
    physical stability of the 3D shape. Fully differentiable physical loss terms are
    then defined for integrating physics constraints into neural networks. Specifically,
    a neural stability predictor, implemented as a neural network classifier and pre-trained
    with simulated data, is proposed for enforcing physical stability constraints.
    For each synthetic 3D shape in the simulated data, its stability is labeled by
    the Bullet physics engine. The stability constraint is encoded via the pre-trained
    stability classifier and is integrated into the deep model via the stability loss.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 和 $G$ 分别表示标准的鉴别器和生成器，$L(D,G)$ 代表 GAN 的标准训练损失。$\Sigma(p_{data})$ 和 $\Sigma(p_{G})$
    分别表示训练数据和生成样本的分布的协方差结构。$d(\cdot)$ 表示距离度量，例如 Frobenius 范数。$\lambda$ 表示正则化项的系数。在低维流形（即协方差结构）上引入统计约束有助于减少在高维空间中寻找期望解决方案的搜索范围。因此，数据量不仅减少，而且收敛到解决方案所需的训练时间也缩短了，这在解决湍流
    PDE 时得到了验证。在计算材料科学的应用中，Shah 等人[[43](#bib.bib43)] 提出了一个名为 InvNet 的深度生成模型，通过该模型生成满足所需物理属性的合成结构样本。InvNet
    是传统 GAN 的扩展，引入了一个附加的 invariance checker，以及一个传统的生成器和鉴别器。invariance checker 作为中间原语被引入，在此基础上定义了一个隐式知识引导的正则化项。基于提出的
    invariance checker 定义了 invariance loss，用于测量不变性的违反。通过 invariance loss，生成的样本被鼓励满足某些不变性（例如，*motif
    invariance*，即在所有合成图像的固定位置植入预定义的 *motif*）。除了表示为 PDE 的知识之外，实际的 3D 形状生成还考虑了物理连通性和稳定性。Mezghanni
    等人[[42](#bib.bib42)] 提出了将物理约束纳入深度生成模型的方法，其中物理约束捕捉了 3D 组件的连通性和 3D 形状的物理稳定性。然后，定义了完全可微的物理损失项，以将物理约束集成到神经网络中。具体而言，提出了一种神经稳定性预测器，作为神经网络分类器实现，并用模拟数据进行预训练，用于强制执行物理稳定性约束。在模拟数据中的每个合成
    3D 形状，其稳定性由 Bullet 物理引擎标记。稳定性约束通过预训练的稳定性分类器进行编码，并通过稳定性损失集成到深度模型中。
- en: Variational AutoEncoder (VAE) has also been explored with physics integrated
    for robust and interpretable generative modeling [[51](#bib.bib51), [52](#bib.bib52)].
    In particular, physics knowledge represented as PDEs is integrated into VAEs.
    Latent variables of VAEs are subject to the constraints defined by PDEs. Furthermore,
    in [[50](#bib.bib50)], instead of assuming a full access to the complete expression
    of PDEs, only part of the PDE is assumed to be known with latent variables of
    VAE being partially grounded with physics meaning for the known part of PDEs.
    The rest of unknown PDEs are modelled in a data-driven manner.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）也已与物理知识相结合，用于稳健和可解释的生成建模 [[51](#bib.bib51), [52](#bib.bib52)]。特别地，作为
    PDE 表示的物理知识被整合到 VAE 中。VAE 的潜变量受到 PDE 定义的约束。此外，在 [[50](#bib.bib50)] 中，假设对 PDE 的完整表达式没有完全访问权限，仅假设
    PDE 的一部分是已知的，VAE 的潜变量部分基于已知部分的物理意义进行约束。其余未知的 PDE 以数据驱动的方式建模。
- en: 'Leveraging probabilistic framework, uncertainty quantification has been considered
    in physics-informed deep models [[53](#bib.bib53), [54](#bib.bib54), [45](#bib.bib45),
    [55](#bib.bib55)]. Zhu et al. [[54](#bib.bib54)] considered a physics-informed
    CNN solving PDEs with uncertainty quantification. The uncertainty is originated
    from the randomness of the physics parameters $\bm{\lambda}$, which is denoted
    as random vector $\bm{x}=\{\bm{\lambda}_{1},....,\bm{\lambda}_{n_{s}}\}$, where
    $n_{s}$ is the total number of possible physics parameter settings and can be
    very high. Correspondingly, the solution of PDE $u$ with respect to each possible
    physics parameter setting becomes $\bm{y}=\{u_{1},u_{2},...,u_{n_{s}}\}$. The
    task is to model $p_{\theta}(\bm{y}|\bm{x})$ given a set of observations $\mathcal{D}_{input}=\{\bm{x}^{(i)}\}_{i=1}^{N}$
    with $\bm{x}^{(i)}\sim p(\bm{x})$. $\bm{\theta}$ denote the neural network parameters
    to be learned. In the meantime, uncertainty of PDE solutions is modeled via the
    variance $Var[\bm{y}]$. To train the NN, instead of using labeled data, an energy-based
    model is defined based solely on the PDE and its boundary conditions, from which
    we obtain the reference density. In particular, the reference density $p_{\beta}(\bm{y}|\bm{x})$
    follows a Boltzmann-Gibbs distribution:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 利用概率框架，物理知识驱动的深度模型中已考虑了不确定性量化 [[53](#bib.bib53), [54](#bib.bib54), [45](#bib.bib45),
    [55](#bib.bib55)]。Zhu 等人 [[54](#bib.bib54)] 考虑了一个具有不确定性量化的物理知识驱动的 CNN 来求解 PDE。该不确定性源于物理参数
    $\bm{\lambda}$ 的随机性，表示为随机向量 $\bm{x}=\{\bm{\lambda}_{1},....,\bm{\lambda}_{n_{s}}\}$，其中
    $n_{s}$ 是可能的物理参数设置的总数，可能非常高。相应地，每个可能的物理参数设置下的 PDE 解 $u$ 变为 $\bm{y}=\{u_{1},u_{2},...,u_{n_{s}}\}$。任务是给定一组观测
    $\mathcal{D}_{input}=\{\bm{x}^{(i)}\}_{i=1}^{N}$ 进行建模 $p_{\theta}(\bm{y}|\bm{x})$，其中
    $\bm{x}^{(i)}\sim p(\bm{x})$。$\bm{\theta}$ 表示待学习的神经网络参数。同时，通过方差 $Var[\bm{y}]$
    对 PDE 解的不确定性进行建模。为了训练神经网络，而不是使用标记数据，基于 PDE 及其边界条件定义了一个基于能量的模型，从中获得参考密度。特别地，参考密度
    $p_{\beta}(\bm{y}|\bm{x})$ 遵循玻尔兹曼-吉布斯分布：
- en: '|  | $p_{\beta}(\bm{y}&#124;\bm{x})=\frac{\exp(-\beta E(\bm{y,\bm{x}}))}{Z_{\beta}(\bm{x})}$
    |  | (23) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{\beta}(\bm{y}&#124;\bm{x})=\frac{\exp(-\beta E(\bm{y,\bm{x}}))}{Z_{\beta}(\bm{x})}$
    |  | (23) |'
- en: 'with $\beta$ being a learnable parameter of the constructed energy-based model.
    Energy function $E(\bm{y,\bm{x}})=V_{PDE}(\bm{y},\bm{x})+\lambda V_{boundary}(\bm{y})$
    measures the violation of the PDE and boundary conditions. $\lambda$ is a tunable
    hyper-parameter. Physics equations are encoded into the energy-based probabilistic
    model. In the end, the NN is trained by minimizing the KL divergence between the
    estimated distribution and the reference distribution:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 是构建的基于能量的模型的可学习参数。能量函数 $E(\bm{y,\bm{x}})=V_{PDE}(\bm{y},\bm{x})+\lambda
    V_{boundary}(\bm{y})$ 衡量 PDE 和边界条件的违背程度。$\lambda$ 是一个可调的超参数。物理方程被编码到基于能量的概率模型中。最后，通过最小化估计分布和参考分布之间的
    KL 散度来训练神经网络：
- en: '|  | $\min_{\beta,\bm{\theta}}D_{KL}(p(\bm{x})p_{\bm{\theta}}(\bm{y}&#124;\bm{x})&#124;&#124;p(\bm{x})p_{\beta}(\bm{y}&#124;\bm{x}))$
    |  | (24) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\beta,\bm{\theta}}D_{KL}(p(\bm{x})p_{\bm{\theta}}(\bm{y}&#124;\bm{x})&#124;&#124;p(\bm{x})p_{\beta}(\bm{y}&#124;\bm{x}))$
    |  | (24) |'
- en: 'Through the training process, physics equations are integrated into the NN.
    Along the same line, Yang and Perdikaris [[53](#bib.bib53)] simplified the objective
    function by considering the lower bound of Eq. ([24](#S3.E24 "In Regularization
    with generative deep models ‣ III-C3 Training-level Integration ‣ III-C Integration
    into Deep Models ‣ III Deep Learning with Scientific Knowledge ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey")). Following a similar idea, Karumuri
    et al. [[55](#bib.bib55)] employed a deep residual network (ResNet) for solving
    elliptic stochastic PDEs in a label-free manner. In particular, the physics-informed
    loss function is defined as the expectation of PDE residuals over the probability
    distribution of stochastic variables. All three works discussed above focus on
    time-independent physics systems without evolution over time. Geneva and Zabaras [[45](#bib.bib45)]
    extended the idea to a dynamic system, where an auto regressive network is employed
    for predicting future physics states given a history of system states.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练过程，物理方程被整合进神经网络中。同样地，杨和Perdikaris [[53](#bib.bib53)] 通过考虑方程 ([24](#S3.E24
    "在生成深度模型中的正则化 ‣ III-C3 训练级别整合 ‣ III-C 整合到深度模型中 ‣ III 利用科学知识的深度学习 ‣ 知识增强的深度学习及其应用：综述"))的下界，简化了目标函数。类似地，Karumuri等人 [[55](#bib.bib55)]
    采用了深度残差网络（ResNet）来以无标签的方式解决椭圆型随机偏微分方程。特别地，物理信息损失函数被定义为随机变量的概率分布上的偏微分方程残差的期望。上述三项工作都集中在时间无关的物理系统上，没有时间演变。Geneva和Zabaras [[45](#bib.bib45)]
    将这一思路扩展到了动态系统，其中采用了自回归网络来预测给定系统状态历史的未来物理状态。
- en: IV Deep Learning with Experiential Knowledge
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 深度学习与经验知识
- en: Besides scientific knowledge, experiential knowledge has been widely considered,
    as the major source of knowledge for neural-symbolic models. Experiential knowledge
    refers to well-known facts from everyday life, describing semantic properties
    of an object or semantic relationships among multiple objects. It is generally
    intuitive and is derived through long time observations or well-established studies.
    Unlike the scientific knowledge, experiential knowledge, though widely available,
    is descriptive and imprecise. Experiential knowledge containing semantic information
    can serve as a strong prior knowledge for predictive tasks in deep learning (e.g.,
    regression or classification tasks), especially in the small-data regime where
    training data alone is insufficient in capturing relationships among variables [[65](#bib.bib65)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 除了科学知识，经验知识也被广泛考虑，作为神经符号模型的主要知识来源。经验知识指的是来自日常生活的常识，描述了物体的语义属性或多个物体之间的语义关系。它通常是直观的，并通过长期观察或成熟的研究得出。与科学知识不同，尽管经验知识广泛存在，但它是描述性的和不精确的。包含语义信息的经验知识可以作为深度学习中预测任务（如回归或分类任务）的强大先验知识，特别是在数据量小的数据环境中，当仅靠训练数据不足以捕捉变量之间的关系时 [[65](#bib.bib65)]。
- en: IV-A Experiential Knowledge Identification
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 经验知识识别
- en: 'Depending on the application domain, experiential knowledge may manifest in
    two types: entity properties and entity relationships. Entity relationships reveal
    semantic relationships among entities. They may be inferred from daily facts on
    relationships between named entities or be derived from well-established studies
    or theories, e.g., anatomy. For example, human anatomy has been considered widely
    in computer vision for human body and facial behavior analysis [[14](#bib.bib14),
    [110](#bib.bib110), [111](#bib.bib111)]. For facial behavior analysis, facial
    anatomy knowledge may provide information on relationships among the facial muscles
    to produce natural facial expression. Similarly, body anatomy may provide information
    on the relationships among the body joints to produce stable and physically plausible
    body pose and movement. The semantic relationships can be directly given or indirectly
    inferred from the existing ones. For example, from the facts that Helen Mirren
    acted in the Ink Heart and Helen Mirren wins the Best Actress award, one can infer
    that The Ink Heart is nominated for an award. Inferred facts, however, are susceptible
    to errors. Entity properties capture the knowledge about properties of entities.
    They can refer to ontological information describing hierarchical relations of
    concepts in the world perceived by humans [[112](#bib.bib112)]. For example, Rain
    is made of water, and Sea is a synonym for ocean. Linguistic knowledge, a major
    source of experiential knowledge, is analyzed in [[113](#bib.bib113), [114](#bib.bib114)].
    A large language model has been considered as inductive bias for an abstract textual
    reasoning task [[115](#bib.bib115)]. Linguistic knowledge such as textual explanations
    has been explored for language model refinement [[116](#bib.bib116)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用领域，经验知识可能表现为两种类型：实体属性和实体关系。实体关系揭示了实体之间的语义关系。这些关系可能从命名实体之间的日常事实中推断出来，也可能源自经过充分研究或理论建立的研究，例如解剖学。例如，人类解剖学在计算机视觉中的人体和面部行为分析中被广泛考虑[[14](#bib.bib14),
    [110](#bib.bib110), [111](#bib.bib111)]。对于面部行为分析，面部解剖学知识可能提供有关面部肌肉之间关系的信息，以产生自然的面部表情。类似地，身体解剖学可能提供有关身体关节之间关系的信息，以产生稳定且物理上合理的身体姿势和运动。语义关系可以直接给出，也可以从现有关系中间接推断出来。例如，从海伦·米伦在《墨水心》中出演和海伦·米伦赢得最佳女演员奖这两个事实中，可以推断《墨水心》获得提名。然而，推断的事实容易出现错误。实体属性捕捉有关实体属性的知识。它们可以指代描述人类感知世界中概念的层次关系的本体论信息[[112](#bib.bib112)]。例如，雨是由水组成的，海洋的同义词是海洋。语言知识作为经验知识的主要来源之一，在[[113](#bib.bib113),
    [114](#bib.bib114)]中进行了分析。大型语言模型被认为是抽象文本推理任务的归纳偏置[[115](#bib.bib115)]。例如，文本解释等语言知识已被探索用于语言模型的改进[[116](#bib.bib116)]。
- en: IV-B Representation of Experiential Knowledge
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 经验知识的表示
- en: '![Refer to caption](img/9829d3d379090277eb81cd02c9abcbd9.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9829d3d379090277eb81cd02c9abcbd9.png)'
- en: 'Figure 7: Taxonomy of experiential knowledge and its representations.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：经验知识及其表示的分类。
- en: 'Representations of experiential knowledge vary with domains. In general, the
    representations for experiential knowledge include probabilistic dependencies,
    logic rules and knowledge graphs. These representations capture both relationships
    and properties of entities revealed by the experiential knowledge. We illustrate
    the taxonomy of experiential knowledge and its representations in Figure [7](#S4.F7
    "Figure 7 ‣ IV-B Representation of Experiential Knowledge ‣ IV Deep Learning with
    Experiential Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey").'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '经验知识的表示方式因领域而异。一般来说，经验知识的表示包括概率依赖性、逻辑规则和知识图谱。这些表示方式捕捉了经验知识揭示的实体之间的关系和属性。我们在图[7](#S4.F7
    "Figure 7 ‣ IV-B Representation of Experiential Knowledge ‣ IV Deep Learning with
    Experiential Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications:
    A Survey")中展示了经验知识及其表示的分类。'
- en: IV-B1 Probabilistic Dependencies
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 概率依赖性
- en: Due to inherent uncertainty, semantic relationships among objects are widely
    represented through probabilistic dependencies. States of objects are modelled
    in a probabilistic way, whereby relationships among objects are captured via probabilistic
    dependencies. Relationships can be further divided into positive correlations
    and negative correlations. Let’s take facial action units (AUs) as an example.
    According to FACS [[117](#bib.bib117)], AUs represent facial muscles and one facial
    muscle can control one or multiple AUs. A binary AU can be on if the corresponding
    muscle is activated. AU1 (inner brow raiser) and AU2 (outer brow raiser) usually
    occur together because they are controlled by the same muscle frontalis. The pair
    AU15 (lip corner depressor) and AU24 (lip pressor) is another example of positive
    correlation, which is due to the fact that their underlying controlling muscles
    (i.e., depressor anguli oris and orbicularis oris, respectively) always move together.
    If two variables $X$ and $Y$ are positively correlated (e.g., $X$=“AU1” and $Y$=“AU2”)
    with $X=\{$0, 1$\}$ and $Y=\{$0, 1$\}$, then we have
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于固有的不确定性，对象之间的语义关系通常通过概率依赖关系表示。对象的状态以概率方式建模，对象之间的关系通过概率依赖关系捕捉。关系可以进一步分为正相关和负相关。以面部动作单位（AUs）为例。根据FACS [[117](#bib.bib117)]，AUs代表面部肌肉，一个面部肌肉可以控制一个或多个AUs。当对应的肌肉被激活时，二元AU可以开启。AU1（内眉提升肌）和AU2（外眉提升肌）通常一起出现，因为它们由同一块肌肉——额肌控制。AU15（嘴角下拉肌）和AU24（嘴唇压缩肌）这一对正相关的例子说明了这一点，因为它们的控制肌肉（即嘴角下拉肌和口轮匝肌）总是一起活动。如果两个变量
    $X$ 和 $Y$ 是正相关的（例如，$X$=“AU1” 和 $Y$=“AU2”），且 $X=\{$0, 1$\}$ 和 $Y=\{$0, 1$\}$，那么我们有
- en: '|  | $\begin{split}p(X=1,Y=1)&amp;>p(X=1,Y=0)\\ p(X=1,Y=1)&amp;>p(X=0,Y=1)\end{split}$
    |  | (25) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}p(X=1,Y=1)&amp;>p(X=1,Y=0)\\ p(X=1,Y=1)&amp;>p(X=0,Y=1)\end{split}$
    |  | (25) |'
- en: Considering negative correlation, AU12 (lip corner puller) and AU15 (lip corner
    depressor) cannot show up together as their corresponding muscles (i.e., Zgomaticus
    major and depressor anguli oris, respectively) are unlikely to be activated simultaneously.
    Negative correlation can be represented in a similar way. If two variables $X$
    and $Y$ are negatively correlated (e.g., $X$=“AU12” and $Y$=“AU15”) with $X=\{$0,
    1$\}$ and $Y=\{$0, 1$\}$, then we have
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑负相关，AU12（嘴角提拉肌）和AU15（嘴角下拉肌）不能同时出现，因为它们对应的肌肉（即颧肌大肌和嘴角下拉肌）不太可能同时被激活。负相关可以以类似的方式表示。如果两个变量
    $X$ 和 $Y$ 是负相关的（例如，$X$=“AU12” 和 $Y$=“AU15”），且 $X=\{$0, 1$\}$ 和 $Y=\{$0, 1$\}$，那么我们有
- en: '|  | $\begin{split}p(X=1,Y=1)&amp;<p(X=1,Y=0)\\ p(X=1,Y=1)&amp;<p(X=0,Y=1)\end{split}$
    |  | (26) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}p(X=1,Y=1)&amp;<p(X=1,Y=0)\\ p(X=1,Y=1)&amp;<p(X=0,Y=1)\end{split}$
    |  | (26) |'
- en: IV-B2 First-order and Propositional Logic
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 一阶逻辑和命题逻辑
- en: 'Logic can be categorized into first-order logic and propositional logic. First-order
    logic (FOL) [[118](#bib.bib118)] employs logic rules to infer new experiential
    knowledge from existing experiential knowledge; it has been employed as an inference
    method to derive different types of knowledge, e.g., textual explanations [[116](#bib.bib116)].
    The formula of FOL is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑可以分为一阶逻辑和命题逻辑。一阶逻辑（FOL） [[118](#bib.bib118)] 使用逻辑规则从现有的经验知识中推导出新的经验知识；它已被用作推理方法来推导不同类型的知识，例如文本解释 [[116](#bib.bib116)]。FOL
    的公式如下：
- en: '|  | $R_{1}(\bm{x})\land R_{2}(\bm{x})\land\cdots\land R_{n}(\bm{x})\Rightarrow
    H(\bm{x})$ |  | (27) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $R_{1}(\bm{x})\land R_{2}(\bm{x})\land\cdots\land R_{n}(\bm{x})\Rightarrow
    H(\bm{x})$ |  | (27) |'
- en: where $R_{1}(\bm{x}),R_{2}(\bm{x}),\cdots R_{n}(\bm{x})$ represent logic atoms.
    Each atom captures a known object property or relationship through a predicate.
    The atoms are combined through connective (e.g. conjunctive) operators to form
    the condition part of the logic rule. $H(\bm{x})$ is the implied result or conclusion
    of the logic rule. It represents the new experiential knowledge derived from the
    logic rule. The conditional part and the conclusion part of the rule are connected
    through the implication operator. For example, we have
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R_{1}(\bm{x}),R_{2}(\bm{x}),\cdots R_{n}(\bm{x})$ 表示逻辑原子。每个原子通过谓词捕捉一个已知的对象属性或关系。原子通过连接（例如合取）运算符组合形成逻辑规则的条件部分。$H(\bm{x})$
    是逻辑规则的隐含结果或结论。它代表从逻辑规则中得出的新经验知识。规则的条件部分和结论部分通过蕴涵运算符连接。例如，我们有
- en: '|  | $\texttt{Smokes}(x)\Rightarrow\texttt{Cough}(x),\quad\forall x$ |  | (28)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{Smokes}(x)\Rightarrow\texttt{Cough}(x),\quad\forall x$ |  | (28)
    |'
- en: $\texttt{Smokes}(x)$ is an atom, with Smokes being a predicate and $x$ as a
    logic variable. It captures the entity property knowledge that the person represented
    by variable $x$ smokes or not. $\texttt{Cough}(x)$ is the implied result or derived
    knowledge, which captures the knowledge that the person coughs or not. The rule
    states that if the conditional part, $\texttt{Smokes}(x)$ is true, then $\texttt{Cough}(x)$
    is also true.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: $\texttt{Smokes}(x)$ 是一个原子，其中 Smokes 是谓语，$x$ 是逻辑变量。它捕捉了变量 $x$ 代表的人的吸烟或不吸烟的实体属性知识。$\texttt{Cough}(x)$
    是隐含的结果或推导出的知识，捕捉了该人是否咳嗽的知识。规则表明，如果条件部分 $\texttt{Smokes}(x)$ 为真，则 $\texttt{Cough}(x)$
    也为真。
- en: IV-B3 Knowledge Graph
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 知识图谱
- en: 'The knowledge graph is another symbolic representation of experiential knowledge,
    that is used primarily to capture the semantic relationships among objects, whereby
    semantic knowledge is expressed in triple format: (subject, predicate, object).
    The number of such triples is usually huge. In a knowledge graph, these triples
    are organized as a graph containing nodes and edges. Nodes represent subjects
    or objects, such as animals or places, as well as named entities, such as a person
    named Mary Kelley. Edges represent the predicate and connect pairs of nodes and
    describe the relationship between them. Besides, edges can also represent properties
    of an entity with nodes representing the attributes. Taking the triplet (cat,
    attribute, paw) as an example, nodes are cat and claw, and the relationship is
    attribute. This triple states a fact: “The attribute of a cat is paw”. Edges can
    be directed or undirected, for example, the food chain relationship between animals
    or the social relationship between people. Knowledge graphs can encode a large
    amount of commonsense, rules, and domain knowledge that capture semantic relationships
    and properties about entities. A knowledge graph hence is an important basic resource
    for obtaining experiential knowledge. For example, experiential knowledge of the
    semantic meanings of objects can be organized in a knowledge graph for image classification [[119](#bib.bib119)].
    The Miscrosoft Concept Graph [[120](#bib.bib120)] is another example of a knowledge
    graph, where vertices in the Miscrosoft Concept Graph [[120](#bib.bib120)] could
    represent food such as fruit, mammals such as dog and cat, or facilities such
    as bus and gas station. The edges indicate the relationships between concepts
    based on daily facts, such as a cat is a mammal, where is reflects relationship
    between cat and mammal.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是经验知识的另一种符号化表示，主要用于捕捉对象之间的语义关系，其中语义知识以三元组的格式表达：（主语，谓语，宾语）。这些三元组的数量通常非常庞大。在知识图谱中，这些三元组被组织成一个包含节点和边的图。节点表示主语或宾语，例如动物或地点，以及命名实体，如名为玛丽·凯利的人。边表示谓语，并连接节点对，描述它们之间的关系。此外，边还可以表示实体的属性，其中节点代表属性。以三元组（猫，属性，爪子）为例，节点是猫和爪子，关系是属性。这个三元组陈述了一个事实：“猫的属性是爪子”。边可以是有向的或无向的，例如动物之间的食物链关系或人之间的社会关系。知识图谱可以编码大量的常识、规则和领域知识，捕捉关于实体的语义关系和属性。因此，知识图谱是获取经验知识的重要基础资源。例如，物体的语义意义的经验知识可以组织在知识图谱中用于图像分类[[119](#bib.bib119)]。微软概念图谱[[120](#bib.bib120)]是另一个知识图谱的例子，其中微软概念图谱[[120](#bib.bib120)]中的顶点可以代表水果等食物、狗和猫等哺乳动物，或公交车和加油站等设施。边表示基于日常事实的概念之间的关系，例如猫是一种哺乳动物，这反映了猫和哺乳动物之间的关系。
- en: IV-C Integration into Deep Models
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 深度模型中的集成
- en: 'To integrate experiential knowledge into deep models, existing methodologies
    cover four types of integration: data-level, architecture-level, training-level,
    and decision-level as shown in Figure [1](#S2.F1 "Figure 1 ‣ II-C Knowledge Integration
    ‣ II Knowledge-augmented Deep Learning ‣ Knowledge-augmented Deep Learning and
    Its Applications: A Survey"). We introduce each group of the approaches in the
    following subsections.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将经验知识集成到深度模型中，现有的方法覆盖了四种集成类型：数据层、架构层、训练层和决策层，如图[1](#S2.F1 "图 1 ‣ II-C 知识集成
    ‣ II 知识增强深度学习 ‣ 知识增强深度学习及其应用：综述")所示。我们在以下小节中介绍每组方法。
- en: IV-C1 Data-level Integration
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 数据层集成
- en: Pseudo training data is usually considered to incorporate experiential knowledge
    represented as probabilistic dependencies, and is employed to augment existing
    training data. For example, Teshima and Sugiyama [[65](#bib.bib65)] proposed to
    incorporate the conditional independence relationships among variables into predictive
    modeling. A set of conditional independence relationships among variables are
    firstly extracted from prior knowledge. Training data is then augmented by generating
    synthetic data that satisfies the extracted conditional independence relationships.
    Besides augmenting training data, pseudo data can also be employed for constructing
    a prior knowledge model. For facial AU recognition, Li et al. [[64](#bib.bib64)]
    proposed to leverage pseudo data generated based on the knowledge for constructing
    a data-free prior model that captures the prior distribution of the target variables
    for downstream tasks. Constraints on parameters and variables are firstly derived
    from generic AU knowledge. Effective sampling methods are then proposed for generating
    the pseudo data satisfying the variable and parameter constraints. A Bayesian
    network is then learned from pseudo data, serving as the data-free prior model.
    Similar idea has been exploited in upper body pose estimation task [[14](#bib.bib14)].
    Four types of constraints (i.e., connectivity constraint, body length constraint,
    kinesiology constraint and symmetry constraint) are firstly derived from human
    anatomic knowledge. Synthetic data is then generated given these constraints,
    based on which a prior probabilistic model is learned.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Besides, synthetic data can be generated from generic knowledge represented
    as Boolean rules [[22](#bib.bib22)]. For instance, the probability of $j$-th class
    (i.e., $\hat{\bm{y}}_{j}$) is higher when $k$-th input feature is bigger than
    a constant $a$ (i.e., $\bm{x}_{k}>a$). To incorporate this Boolean rule into a
    deep model, Seo et al. [[22](#bib.bib22)] proposed to augment each of the training
    data point $\bm{x}$ with a paired perturbed one $\bm{x}_{p}=\bm{x}+\delta\bm{x}$.
    $\delta\bm{x}$ is a small positive value for perturbation. The regularization
    is then be defined as
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  $\mathcal{L}(\bm{x},\bm{x}_{p},\bm{\hat{y}}_{j},\bm{\hat{y}}_{p,j})=\texttt{ReLU}(\bm{\hat{y}}_{j}-\bm{\hat{y}}_{p,j})\cdot\mathbb{I}(\bm{x}_{k}<a)\cdot\mathbb{I}(\bm{x}_{p,k}>a)$  |  |
    (29) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: If $\bm{x}_{k}<a$ and $\bm{x}_{p,k}>a$, according to the prior knowledge, we
    should have $\bm{\hat{y}}_{j}-\bm{\hat{y}}_{p,j}<0$. If the constraint is satisfied,
    we have $\mathcal{L}(\bm{x},\bm{x}_{p},\bm{\hat{y}}_{j},\bm{\hat{y}}_{p,j})=0$.
    Otherwise, $\mathcal{L}(\bm{x},\bm{x}_{p},\bm{\hat{y}}_{j},\bm{\hat{y}}_{p,j})>0$.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Architecture-level Integraion
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Domain knowledge describing relationships between variables can be integrated
    into deep models through architecture design. We discuss the architecture-level
    integration methods for experiential knowledge represented as probabilistic dependencies,
    logic rules, and knowledge graphs, respectively.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 描述变量之间关系的领域知识可以通过架构设计集成到深度模型中。我们分别讨论了将经验知识（表示为概率依赖、逻辑规则和知识图谱）集成到架构级别的方法。
- en: Architecture design to incorporate probabilistic dependencies
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 集成概率依赖的架构设计
- en: One representative line of architecture-level integration is focused on the
    experiential knowledge represented as probabilistic dependencies, whereby a probabilistic
    model, constructed from the knowledge, serves as a prior model and is embedded
    as one layer of a neural network. Semantic relationships among variables can hence
    be incorporated into neural network in a probabilistic way. Usually, a probabilistic
    model is concatenated to the last layer of a neural network. A conditional random
    field (CRF) is usually employed, which takes hidden features from neural networks
    as input and outputs final predictions that satisfy the knowledge encoded in the
    CRF (e.g., among AUs [[66](#bib.bib66)]). In [[67](#bib.bib67)], a fully-connected
    CRF is concatenated to the last layer of a CNN to jointly perform facial landmark
    detection. By leveraging the fully-connected CNN-CRF, probabilistic predictions
    of facial landmark locations are obtained capturing the structural dependencies
    among landmark points. For scene graph generation [[68](#bib.bib68)], structured
    relationships among entities and relations are firstly captured via an energy-based
    probabilistic model. The energy-based probabilistic model takes the output of
    a typical scene graph generation model as input and refines it by minimizing the
    energy. Prior knowledge captured via a probabilistic model can also be leveraged
    to define the adjacency matrix of a graph convolutional network [[69](#bib.bib69)].
    For facial action unit (AU) density estimation task, a Bayesian network is employed
    to capture the inherent dependencies among AUs. A probabilistic graph convolution
    is then proposed whereby its adjacency matrix is defined by the structure of the
    Bayesian network. In addition, a probabilistic model can also be introduced as
    a learnable intermediate layer of a neural network. A CausalVAE [[74](#bib.bib74)]
    was proposed whereby a causal layer is introduced to the latent space of a variational
    auto-encoder (VAE). The causal layer essentially describes a structural causal
    model (SCM). Through the causal layer, independent exogenous factors are transformed
    to causal endogenous factors for causal representation learning.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的架构级别集成方法集中于将经验知识表示为概率依赖，其中，从知识构建的概率模型作为先验模型，并被嵌入为神经网络的一个层。因此，变量之间的语义关系可以以概率方式集成到神经网络中。通常，概率模型被连接到神经网络的最后一层。通常采用条件随机场（CRF），它将神经网络中的隐藏特征作为输入，并输出满足CRF中编码的知识的最终预测（例如，在AUs
    [[66](#bib.bib66)]之间）。在[[67](#bib.bib67)]中，一个全连接的CRF被连接到CNN的最后一层，以共同进行面部关键点检测。通过利用全连接的CNN-CRF，获得了面部关键点位置的概率预测，捕捉了关键点之间的结构依赖关系。在场景图生成[[68](#bib.bib68)]中，首先通过基于能量的概率模型捕捉实体和关系之间的结构化关系。基于能量的概率模型将典型场景图生成模型的输出作为输入，并通过最小化能量进行精炼。通过概率模型捕捉的先验知识也可以用于定义图卷积网络的邻接矩阵[[69](#bib.bib69)]。对于面部动作单元（AU）密度估计任务，采用贝叶斯网络来捕捉AU之间的内在依赖关系。然后提出了一种概率图卷积，其中其邻接矩阵由贝叶斯网络的结构定义。此外，概率模型也可以作为神经网络的可学习中间层引入。提出了一种CausalVAE
    [[74](#bib.bib74)]，其中在变分自编码器（VAE）的潜在空间中引入了因果层。因果层本质上描述了一个结构性因果模型（SCM）。通过因果层，独立的外生因素被转化为因果内生因素，以进行因果表示学习。
- en: Architecture design to incorporate logic rules
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 集成逻辑规则的架构设计
- en: Integration through the architecture of neural network is a conventional neural-symbolic
    approach to integrate symbolic logic rules into deep models. Logic rules are integrated
    into neural network architectures by introducing logic variables or parameters.
    Such an approach can be traced back to the 1990s, when knowledge-based artificial
    neural network (KBANN) [[121](#bib.bib121)] and connectionist inductive learning
    and logic programming (CILP) [[122](#bib.bib122)] methods were introduced. More
    recently, logical neural network (LNN) [[123](#bib.bib123)] is proposed where
    each neuron represents an element in a logic formula, which can either be a concept
    (e.g., cat) or a logical connective (e.g., AND, OR). These works, however, are
    focused on leveraging neural networks for differentiable and scalable logic reasoning.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Few works are proposed to improve deep models via customizing their architectures
    via logic rules. To leverage logic rules for improved deep model performance,
    logic rules are encoded into a Markov Logic Network (MLN), and the constructed
    MLN, serving as a prior model, is embedded into a neural network as an output
    layer for improved knowledge graph completion task [[75](#bib.bib75)]. Particularly,
    four types of logic rules are firstly identified for capturing the knowledge in
    a Knowledge graph: (1) composition rules: a predicate $R_{k}$ is composed of two
    predicates $R_{i}$ and $R_{j}$ if for any three variables $Y_{1},Y_{2},Y_{3}$
    we have $R_{i}(Y_{1},Y_{2})\land R_{j}(Y_{2},Y_{2})\Rightarrow R_{k}(Y_{1},Y_{3})$;
    (2) Inverse rules: a predicate $R_{i}$ is an inverse of $R_{j}$ if for any two
    variables $Y_{1}$ and $Y_{2}$ we have $R_{i}(Y_{1},Y_{2})\Rightarrow R_{j}(Y_{2},Y_{1})$;
    (3) Symmetric rules: a predicate $R$ is symmetric if for any two variables $Y_{1}$
    and $Y_{2}$ we have $R(Y_{1},Y_{2})\Rightarrow R(Y_{2},Y_{1})$; (4) Subset rules:
    a predicate $R_{j}$ is a subset of $R_{i}$ if for any two variables $Y_{1}$ and
    $Y_{2}$ we have $R_{j}(Y_{1},Y_{2})\Rightarrow R_{i}(Y_{2},Y_{1})$. Given a set
    of identified logic rules $l\in\mathcal{L}$, a Markov logic network (MLN) defines
    a joint distribution of target variables $y$ of a triple as'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(y)\propto\exp(\sum_{l\in\mathcal{L}}w_{l}\sum_{y}\phi_{l}(y_{l}))$
    |  | (30) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: where $\phi_{l}$ is the potential function and is computed given the observed
    triples. Such an MLN is then concatenated to the last layer of a deep model whereby
    the confidence scores $w_{l}$ are learned via the deep model given the observed
    triples. By introducing the MLN, the task of predicting the missing triplets is
    re-formulated as inferring the posterior distribution of unseen configurations
    following the encoded logic rules.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Architecture design to incorporate knowledge graphs
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The knowledge graph can also be integrated into the architecture of neural
    networks as one layer. Liang et al. [[78](#bib.bib78)] proposed a graph convolution
    with symbolic reasoning. Prior knowledge from the knowledge graph is specifically
    integrated into the neural network through a proposed symbolic graph reasoning
    (SGR) layer, as shown in Figure [8](#S4.F8 "Figure 8 ‣ Architecture design to
    incorporate knowledge graphs ‣ IV-C2 Architecture-level Integraion ‣ IV-C Integration
    into Deep Models ‣ IV Deep Learning with Experiential Knowledge ‣ Knowledge-augmented
    Deep Learning and Its Applications: A Survey").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '知识图谱还可以作为一个层集成到神经网络的架构中。Liang 等人[[78](#bib.bib78)] 提出了带有符号推理的图卷积。通过所提出的符号图推理（SGR）层，知识图谱中的先验知识被特别集成到神经网络中，如图[8](#S4.F8
    "Figure 8 ‣ Architecture design to incorporate knowledge graphs ‣ IV-C2 Architecture-level
    Integraion ‣ IV-C Integration into Deep Models ‣ IV Deep Learning with Experiential
    Knowledge ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey")所示。'
- en: '![Refer to caption](img/7144eb0e25b30ecd371c6b4d2c590c0c.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7144eb0e25b30ecd371c6b4d2c590c0c.png)'
- en: 'Figure 8: Image is adopted from [[78](#bib.bib78)]. Nodes in the current convolutional
    layer are first mapped to symbolic nodes. Connectivities among the symbolic nodes
    are defined based on a knowledge graph. Through a graph reasoning based on the
    knowledge graph, evolved symbolic nodes are finally mapped to the nodes in the
    next convolutional layer.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：图像摘自[[78](#bib.bib78)]。当前卷积层中的节点首先映射到符号节点。符号节点之间的连接基于知识图谱定义。通过基于知识图谱的图推理，进化的符号节点最终映射到下一个卷积层的节点。
- en: To make the proposed SGR cooperate with a convolutional layer, local hidden
    features from the current convolutional layer are firstly transferred to features
    of corresponding symbolic nodes in SGR. Connectivities among the symbolic nodes
    in SGR are defined based on prior knowledge from the knowledge graph. Guided by
    prior knowledge, SGR then performs graph reasoning and updates features. Finally,
    the updated features are mapped onto local features in the next convolutional
    layer. To generate a medical report in an unsupervised manner in a cross visual
    domain and textual domain, a knowledge-driven encoder-decoder model is proposed
    to leverage knowledge graphs [[79](#bib.bib79)]. The knowledge graph is encoded
    into the knowledge-driven attention module within the encoder. Taking an image
    and a medical report as inputs, the encoder first obtains the image embedding
    and the report embedding through standard deep models, respectively. An attention
    mechanism is then introduced whereby the embeddings are the queries and a knowledge
    graph is used to define the lookup matrix. The learned representations from the
    attention module bridge the vision and the textual domain by leveraging the knowledge
    graph. During training, the proposed model is learned by minimizing reconstruction
    error between generated and observed medical reports in the textual domain. During
    testing, the knowledge-driven encoder-decoder model can generate medical reports
    from medical images by leveraging experiential knowledge in knowledge graphs that
    is applicable to both visual and textual domains.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为使提出的SGR与卷积层协作，当前卷积层中的局部隐藏特征首先被转移到SGR中相应符号节点的特征。SGR中符号节点之间的连接基于知识图谱中的先验知识定义。在先验知识的指导下，SGR进行图推理并更新特征。最终，更新后的特征被映射到下一个卷积层的局部特征中。为了在跨视觉领域和文本领域中以无监督方式生成医学报告，提出了一种知识驱动的编码器-解码器模型，以利用知识图谱[[79](#bib.bib79)]。知识图谱被编码到编码器中的知识驱动注意模块中。以图像和医学报告作为输入，编码器首先通过标准深度模型分别获取图像嵌入和报告嵌入。然后引入一种注意机制，其中嵌入是查询，知识图谱用于定义查找矩阵。来自注意模块的学习表示通过利用知识图谱桥接视觉和文本领域。在训练过程中，通过最小化生成的和观察到的医学报告之间的重建误差来学习提出的模型。在测试过程中，知识驱动的编码器-解码器模型可以通过利用适用于视觉和文本领域的知识图谱中的经验知识，从医学图像中生成医学报告。
- en: CRF has been leveraged to capture the experiential knowledge in a knowledge
    graph and is integrated as one layer of a neural network. Luo et al. [[80](#bib.bib80)]
    proposed a context-aware zero-shot recognition (CA-ZSL) method. Prior inter-object
    relation is extracted from knowledge graphs and encoded using a conditional random
    field (CRF). For an image containing $N$ objects, an image region and a class
    assignment of each object are denoted as $B_{i}$ and $c_{i}$, respectively, with
    $i=1,2,...,N$. The CRF model is then defined as
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;P(c_{1},...,c_{N}&#124;B_{1},...,B_{N})\\ &amp;\propto\exp(\sum_{i}\theta(c_{i}&#124;B_{i})+\gamma\sum_{i&#124;\neq
    j}\phi(c_{i},c_{j}&#124;B_{i},B_{j}))\end{split}$ |  | (31) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: where the unary potential $\theta(c_{i}|B_{i})$ is estimated given extracted
    features of each object correspondingly. Pairwise potential $\phi(c_{i},c_{j}|B_{i},B_{j})$
    is estimated using both the extracted features and the knowledge graph. Semantic
    relations extracted from the knowledge graph are encoded in the pairwise potential
    functions, where $\gamma$ is a tunable hyper-parameter. Neural networks are trained
    by maximizing the log-likelihood. During testing, labels of unseen objects are
    inferred in a context-aware manner through a maximum-a-posteriori (MAP) inference
    in the learned CRF model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Training-level Integration
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Experiential knowledge is treated as prior bias that guides the training of
    a deep model. Constraints are obtained based on knowledge and are integrated as
    regularization terms into a deep model [[77](#bib.bib77), [124](#bib.bib124),
    [125](#bib.bib125), [112](#bib.bib112)]. Regularization can be derived from probabilistic
    dependencies, logic rules, or knowledge graphs, and we discuss further in the
    paragraphs below.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Regularization with probabilistic dependencies
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Probabilistic dependencies derived from knowledge about semantic relationships
    are commonly integrated into deep models through regularization. Srinivas Kancheti
    et al. [[70](#bib.bib70)] considered causal domain priors for regularizing neural
    networks during training, whereby the learned causal effects in NNs are enforced
    to match the prior knowledge on causal relationships through regularization. Consider
    a neural network $f$ with $d$ inputs and $C$ outputs, and for $j$th input, $\delta
    G^{j}$ is a $C\times d$ matrix containing prior causal knowledge (in terms of
    gradients). To enforce $f$ to be consistent with the prior knowledge, a regularization
    is defined
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R=\frac{1}{N}\sum_{j=1}^{N}\max\{0,&#124;&#124;\nabla_{j}f\odot M-\delta
    G^{j}&#124;&#124;_{1}-\epsilon\}$ |  | (32) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: where $M$ is a $C\times d$ binary matrix indicating the availability of prior
    knowledge and $N$ is the total number of training samples. $\nabla_{j}f$ is the
    $C\times d$ Jacobian of $f$ w.r.t. $j$th input. $\epsilon$ indicates an acceptable
    error margin, and $\odot$ is the element-wise product. Similarly, Rieger et al. [[71](#bib.bib71)]
    proposed to penalize model explanations that did not align with prior knowledge
    through an explanation loss. For AU detection task, probabilistic relationships
    among AUs are derived from facial anatomic knowledge. Each of these probabilistic
    relationships is formulated as a constraint. In [[72](#bib.bib72)], a loss function
    measuring the satisfaction of each of these constraints is correspondingly defined
    and is employed for learning an AU detector. Differently, Cui et al. [[73](#bib.bib73)]
    proposed to learn a Bayesian network (BN) to compactly capture a large set of
    constraints on AU relationships. The BN is then used to construct the expected
    cross-entropy loss to train a deep neural network for AU detection.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Regularization with logic rules
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Logical knowledge is encoded as constraints for model regularization. Through
    regularization, a deep model is penalized if its output violates the constraints
    derived from logical rules. Xu et al. [[76](#bib.bib76)] proposed to combine the
    automatic reasoning technology of propositional logic with existing deep learning
    models. Propositional logic was encoded in the loss function through the proposed
    semantic loss. A sentence $\alpha$ in propositional logic is defined over variables
    $\bm{X}=\{X_{1},...,X_{n}\}$. The sentence is the semantic constraint to be imposed
    on the outputs of a neural network. Suppose that $p$ is a vector of probabilities,
    where each element $p_{i}$ denotes the predicted probability of variable $X_{i}$
    and corresponds to a single output of the neural net. The semantic loss $L^{s}(\alpha,p)$
    measures the violation of $\alpha$ given $p$ as
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L^{s}(\alpha,p)\propto-\log\sum_{\bm{x}\models\alpha}\prod_{i:\bm{x}\models
    X_{i}}p_{i}\prod_{i:\bm{x}\models\neg X_{i}}(1-p_{i})$ |  | (33) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: $\bm{x}\models\alpha$ represents that the state $x$ satisfies the sentence $\alpha$.
    The larger the probability of the state satisfying the sentence, the smaller the
    semantic loss. The proposed semantic loss bridges neural network regularization
    with logic reasoning. It is effective for different applications, such as classification
    and preference ranking.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: For relation prediction task, a logic embedding network with semantic regularization
    (LENSR) was proposed [[77](#bib.bib77)], where a propositional logic is integrated
    into a relation detection model. For a given image, the probability distribution
    of the relation predicate is first estimated using a standard visual relation
    detection model. Another probability distribution of the relation predicate is
    then proposed based on the propositional logic formula pre-defined given the input
    image. Finally, a semantic regularization is defined to align these two probability
    distributions by minimizing their difference.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Regularization with knowledge graph
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The knowledge graph, a graphical representation of experiential knowledge, has
    also been employed for model regularization. Fang et al. [[81](#bib.bib81)] proposed
    to extract semantic consistency constraints from knowledge graph which are then
    used as regularization terms. Particularly, a consistency score between a pair
    of object $O$ and subject $S$ is calculated through random walk with restart
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{C}_{O,S}=\lim_{T\rightarrow\infty}p(Y_{T}=Y_{S}&#124;Y_{0}=Y_{O};\alpha)$
    |  | (34) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{C}\in R^{|O|\times|S|}$. $|O|$ and $|S|$ represent the total
    number of target objects and target subjects. $T$ is the total moving steps and
    $\alpha$ is the restart probability, meaning at each moving step, there is a probability
    of $\alpha$ to restart from the starting node instead of moving to one of the
    neighbors of node. The calculated matrix $\mathcal{C}$ is employed as constraints
    on semantic consistency and is used for regularizing the neural network for an
    object detection task. Similarly, Gu et al. [[82](#bib.bib82)] proposed extracting
    external knowledge from KGs and applying image reconstruction to improve scene
    graph generation, especially when the dataset is biased, or annotations are noisy
    or missing. Object relationships are retrieved from ConceptNet as the external
    domain knowledge and are applied to refine object features through an object-to-image
    generation branch. The object-to-image generation branch reconstructs images based
    on object features and prior relational knowledge about the objects. Semantically
    meaningful object features can be learned by minimizing reconstruction error.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 Decision-level Integration
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Predictions from deep models and from the prior knowledge can be directly combined
    through the joint top-down and bottom-up prediction strategy. Through integrating
    two sets of predictions, the final prediction can be more accurate and robust.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'For open-domain knowledge-based visual question answering, Marino et al. [[83](#bib.bib83)]
    combined implicit and symbolic knowledge. Implicit knowledge refers to knowledge
    learned from data (e.g., raw texts). Symbolic knowledge refers to graph-based
    knowledge encoded in existing knowledge graphs (e.g., DBpedia [[126](#bib.bib126)]
    and ConceptNet [[127](#bib.bib127)]). The proposed KRISP model contains two sub-modules:
    implicit knowledge reasoning and explicit knowledge reasoning. Two sources of
    knowledge are then combined to generate the final output by using a late-fusion
    strategy. Through the late-fusion strategy, predictions from data and symbolic
    knowledge are directly combined, independent of the training of deep models.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Two sets of predictions can be combined in a probabilistic way by following
    Bayes’ rule. To obtain probabilistic knowledge-based predictions, a PGM model
    capturing prior knowledge is employed as a prior model and its prediction is obtained
    via a probabilistic inference. For AU recognition task, Li et al. [[64](#bib.bib64)]
    considered a top-down and bottom-up integration, where a Bayesian network learned
    from generic knowledge serves as the top-down model and a data-driven model serves
    as the bottom-up model. Predictions from two models are then combined using Bayes’
    rule to produce the final predictions. Probabilistic knowledge-based predictions
    can also be directly defined based on the knowledge. For a knowledge graph completion
    task, Cui et al. [[84](#bib.bib84)] derived a prior distribution on relations
    based on type information. The prior distribution is then combined with bottom-up
    predictions from existing embedding-based models through Bayes’ rule for final
    prediction.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: V Discussion and Future Direction
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we reviewed traditional and prevailing technologies for knowledge-augmented
    deep learning, including knowledge identification, knowledge representation, and
    integration. We divided knowledge into two categories: scientific knowledge and
    experiential knowledge. Within each category, we introduced knowledge identification,
    representations, and integration into deep learning. As we have discussed, much
    work has been done to improve deep learning with related prior knowledge to yield
    data-efficient, generalizable, and interpretable deep learning models. To help
    readers better understand and apply the KADL to their work, we provide a prescriptive
    tree based on the summary of existing works (Table [I](#S2.T1 "TABLE I ‣ II Knowledge-augmented
    Deep Learning ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey")).
    The prescriptive tree as shown in Figure [9](#S5.F9 "Figure 9 ‣ V Discussion and
    Future Direction ‣ Knowledge-augmented Deep Learning and Its Applications: A Survey")
    serves as a recipe that includes different pathways to incorporate a particular
    type of prior knowledge into deep models. Each pathway consists of a specific
    knowledge type, a knowledge representation format, and a knowledge integration
    method, accompanied with the references to related works.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/333874fa43fb6996409115c3ff55e23f.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Prescriptive tree on knowledge-augmented deep learning'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Despite these efforts, existing methods suffer from several shortcomings. In
    the following paragraphs, we discuss existing techniques and highlight promising
    directions to pursue in the future.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Diverse types of knowledge
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Existing knowledge-augmented deep models explore different types of domain knowledge,
    including scientific knowledge and experiential knowledge. However, most of the
    knowledge explored are scientific knowledge in physics and symbolically represented
    experiential knowledge. Injecting well-established algorithmic knowledge into
    deep models has already began to attract researchers’ attention whereby deep models
    are trained through algorithmic supervisions instead of ground truth annotations [[100](#bib.bib100)].
    Furthermore, existing approaches are usually limited to one specific type of knowledge.
    For a certain application task, both scientific knowledge and experiential knowledge
    could exists from multiple sources. Hence, diverse types of knowledge can be combined
    jointly for improved deep model performance.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Effective knowledge integration
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Existing integration methods utilize synthetic data, model architecture design,
    regularization function, or prediction refinement. Among them, the majority of
    the integration methods are performed during training. Existing integration techniques
    hence heavily depend on the specific training procedure, through which deep models
    are trained by considering two sources of information jointly, without explicitly
    differentiating data from knowledge. The problem can be addressed through decision-level
    fusion. Decision-level knowledge integration scheme using a prior model to capture
    the domain knowledge attracts relatively less attention. Knowledge integration
    with a prior model to combine top-down prediction from the knowledge and bottom-up
    estimation from the data could be beneficial from several aspects. Firstly, the
    construction of a prior model is independent of deep models whereby deep models
    are initialized through observable data. Since the prior model and the deep model
    are built independent of the integration process, top-down and bottom-up integration
    process can be flexibly applied to any deep model and prior model. Secondly, knowledge
    integration is performed by following Bayes’ rule in a principled manner. Refinement
    of data-based prediction based on knowledge becomes tractable and interpretable.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid integration methods
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Existing methods tend to integrate scientific knowledge and experiential knowledge
    separately. In addition, they tend to employ one specific approach to perform
    the knowledge integration. For certain application domains, both types of knowledge
    may exist at the same time. Hence, they should be integrated jointly to further
    improve the performance of the deep models. In addition, users always need to
    choose an integration approach. There is no universal integration scheme that
    applies to all types of knowledge, and it remains an open question on how to automatically
    integrate knowledge with data in an optimal way. Hence, given the complementary
    nature of different integration methods, it may be beneficial to simultaneously
    employ different integration methods to exploit their respective strengths.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge integration with uncertainty
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Existing works have explored the encoding of experiential knowledge in a probabilistic
    way such as capturing the uncertain relationships using a probabilistic graphical
    model. But in general, existing knowledge integration methods are deterministic
    methods, ignoring the underlying knowledge uncertainty and their impacts on deep
    model learning and inference. Uncertainty not only exists with experiential knowledge
    but also with scientific knowledge. For example, in physics, uncertainty arises
    from random physics parameters or unknown physics parameters or incomplete observations.
    Existing works along this line aim to measure the quality of PDE solutions governing
    a physics system and thus are subject to specific domain assumptions. Probabilistic
    tools, such as probabilistic graphical models (PGMs), are powerful in capturing
    uncertainties for experiential knowledge. However, few works explored the usage
    of PGMs in modeling uncertainties in scientific laws [[128](#bib.bib128), [129](#bib.bib129)].
    It remains an open question to deep learning community on how to effectively and
    systematically model uncertainties in scientific knowledge for real-world application
    tasks.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Marcus, “Deep learning: A critical appraisal,” *arXiv preprint arXiv:1801.00631*,
    2018.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] T. R. Besold, A. d. Garcez, S. Bader, H. Bowman, P. Domingos, P. Hitzler,
    K.-U. Kühnberger, L. C. Lamb, D. Lowd, P. M. V. Lima *et al.*, “Neural-symbolic
    learning and reasoning: A survey and interpretation,” *arXiv preprint arXiv:1711.03902*,
    2017.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Yu, B. Yang, D. Liu, and H. Wang, “A survey on neural-symbolic systems,”
    *arXiv preprint arXiv:2111.08164*, 2021.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Willard, X. Jia, S. Xu, M. Steinbach, and V. Kumar, “Integrating physics-based
    modeling with machine learning: A survey,” *arXiv preprint arXiv:2003.04919*,
    vol. 1, no. 1, pp. 1–34, 2020.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Han, L. Zhang *et al.*, “Integrating machine learning with physics-based
    modeling,” *arXiv preprint arXiv:2006.02619*, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. Rai and C. K. Sahu, “Driven by data or derived through physics? a review
    of hybrid physics guided machine learning techniques with cyber-physical system
    (cps) focus,” *IEEE Access*, vol. 8, pp. 71 050–71 073, 2020.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Rath and A. P. Condurache, “Boosting deep neural networks with geometrical
    prior knowledge: A survey,” *arXiv preprint arXiv:2006.16867*, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] P. Nowack, P. Braesicke, J. Haigh, N. L. Abraham, J. Pyle, and A. Voulgarakis,
    “Using machine learning to build temperature-based ozone parameterizations for
    climate sensitivity simulations,” *Environmental Research Letters*, vol. 13, no. 10,
    p. 104016, 2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C. Deng, X. Ji, C. Rainey, J. Zhang, and W. Lu, “Integrating machine learning
    with human knowledge,” *Iscience*, vol. 23, no. 11, p. 101656, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] L. von Rueden, S. Mayer, K. Beckh, B. Georgiev, S. Giesselbach, R. Heese,
    B. Kirsch, J. Pfrommer, A. Pick, R. Ramamurthy *et al.*, “Informed machine learning–a
    taxonomy and survey of integrating knowledge into learning systems,” *arXiv preprint
    arXiv:1903.12394*, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. W. Kim, I. Kim, J. Lee, and S. Lee, “Knowledge integration into deep
    learning in dynamical systems: An overview and taxonomy,” *Journal of Mechanical
    Science and Technology*, vol. 35, no. 4, pp. 1331–1342, 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] L. von Rueden, S. Mayer, R. Sifa, C. Bauckhage, and J. Garcke, “Combining
    machine learning and simulation to a hybrid modelling approach: Current and future
    directions,” in *International Symposium on Intelligent Data Analysis*.   Springer,
    2020, pp. 548–560.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Sagel, A. Sahu, S. Matthes, H. Pfeifer, T. Qiu, H. Rueß, H. Shen, and
    J. Wörmann, “Knowledge as invariance–history and perspectives of knowledge-augmented
    machine learning,” *arXiv preprint arXiv:2012.11406*, 2020.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Chen, S. Nie, and Q. Ji, “Data-free prior model for upper body pose
    estimation and tracking,” *IEEE Transactions on Image Processing*, vol. 22, no. 12,
    pp. 4627–4639, 2013.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] P. Liu, L. Tian, Y. Zhang, S. Aylward, Y. Lee, and M. Niethammer, “Discovering
    hidden physics behind transport dynamics,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 10 082–10 092.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Jaques, M. Burke, and T. M. Hospedales, “Newtonianvae: Proportional
    control and goal identification from pixels via physical latent spaces,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 4454–4463.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Daw, R. Q. Thomas, C. C. Carey, J. S. Read, A. P. Appling, and A. Karpatne,
    “Physics-guided architecture (pga) of neural networks for quantifying uncertainty
    in lake temperature modeling,” in *Proceedings of the 2020 siam international
    conference on data mining*.   SIAM, 2020, pp. 532–540.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Muralidhar, J. Bu, Z. Cao, L. He, N. Ramakrishnan, D. Tafti, and A. Karpatne,
    “Phynet: Physics guided neural networks for particle drag force prediction in
    assembly,” in *Proceedings of the 2020 SIAM International Conference on Data Mining*.   SIAM,
    2020, pp. 559–567.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] T. Beucler, S. Rasp, M. Pritchard, and P. Gentine, “Achieving conservation
    of energy in neural network emulators for climate modeling,” *arXiv preprint arXiv:1906.06622*,
    2019.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] T. Beucler, M. Pritchard, S. Rasp, J. Ott, P. Baldi, and P. Gentine, “Enforcing
    analytic constraints in neural networks emulating physical systems,” *Physical
    Review Letters*, vol. 126, no. 9, p. 098302, 2021.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] L. Zhang, J. Han, H. Wang, R. Car, and E. Weinan, “Deep potential molecular
    dynamics: a scalable model with the accuracy of quantum mechanics,” *Physical
    review letters*, vol. 120, no. 14, p. 143001, 2018.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S. Seo, S. O. Arik, J. Yoon, X. Zhang, K. Sohn, and T. Pfister, “Controlling
    neural networks with rule representations,” *arXiv preprint arXiv:2106.07804*,
    2021.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Sun, Z. Niu, K. A. Innanen, J. Li, and D. O. Trad, “A theory-guided
    deep-learning formulation and optimization of seismic waveform inversion,” *Geophysics*,
    vol. 85, no. 2, pp. R87–R99, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] P. Toth, D. J. Rezende, A. Jaegle, S. Racanière, A. Botev, and I. Higgins,
    “Hamiltonian generative networks,” *arXiv preprint arXiv:1909.13789*, 2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Choudhary, J. F. Lindner, E. G. Holliday, S. T. Miller, S. Sinha, and
    W. L. Ditto, “Physics-enhanced neural networks learn order and chaos,” *Physical
    Review E*, vol. 101, no. 6, p. 062207, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] F. Djeumou, C. Neary, E. Goubault, S. Putot, and U. Topcu, “Neural networks
    with physics-informed architectures and constraints for dynamical systems modeling,”
    *arXiv preprint arXiv:2109.06407*, 2021.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed neural
    networks: A deep learning framework for solving forward and inverse problems involving
    nonlinear partial differential equations,” *Journal of Computational Physics*,
    vol. 378, pp. 686–707, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and
    L. Yang, “Physics-informed machine learning,” *Nature Reviews Physics*, vol. 3,
    no. 6, pp. 422–440, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Z. Li, J. Sedlar, J. Carpentier, I. Laptev, N. Mansard, and J. Sivic,
    “Estimating 3d motion and forces of person-object interactions from monocular
    video,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 8640–8649.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. J. Greydanus, M. Dzumba, and J. Yosinski, “Hamiltonian neural networks,”
    2019.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. D. Zhong, B. Dey, and A. Chakraborty, “Symplectic ode-net: Learning
    hamiltonian dynamics with control,” *arXiv preprint arXiv:1909.12077*, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho,
    “Lagrangian neural networks,” *arXiv preprint arXiv:2003.04630*, 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. Allen-Blanchette, S. Veer, A. Majumdar, and N. E. Leonard, “Lagnetvip:
    A lagrangian neural network for video prediction,” *arXiv preprint arXiv:2010.12932*,
    2020.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] R. Stewart and S. Ermon, “Label-free supervision of neural networks with
    physics and domain knowledge,” in *Thirty-First AAAI Conference on Artificial
    Intelligence*, 2017.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] F. Xie, A. Chowdhury, M. Kaluza, L. Zhao, L. L. Wong, and R. Yu, “Deep
    imitation learning for bimanual robotic manipulation,” *arXiv preprint arXiv:2010.05134*,
    2020.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Wu, E. Lu, P. Kohli, B. Freeman, and J. Tenenbaum, “Learning to see
    physics via visual de-animation,” *Advances in Neural Information Processing Systems*,
    vol. 30, pp. 153–164, 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y.-L. Qiao, J. Liang, V. Koltun, and M. Lin, “Differentiable simulation
    of soft multi-body systems,” in *Thirty-Fifth Conference on Neural Information
    Processing Systems*, 2021.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Botev, A. Jaegle, P. Wirnsberger, D. Hennes, and I. Higgins, “Which
    priors matter? benchmarking models for learning latent dynamics,” 2021.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Wang, R. Walters, and R. Yu, “Incorporating symmetry into deep dynamics
    models for improved generalization,” *arXiv preprint arXiv:2002.03061*, 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Finzi, G. Benton, and A. G. Wilson, “Residual pathway priors for soft
    equivariance constraints,” *Advances in Neural Information Processing Systems*,
    vol. 34, 2021.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Ling, A. Kurzawski, and J. Templeton, “Reynolds averaged turbulence
    modelling using deep neural networks with embedded invariance,” *Journal of Fluid
    Mechanics*, vol. 807, pp. 155–166, 2016.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Mezghanni, M. Boulkenafed, A. Lieutier, and M. Ovsjanikov, “Physically-aware
    generative network for 3d shape modeling,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 9330–9341.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] V. Shah, A. Joshi, S. Ghosal, B. Pokuri, S. Sarkar, B. Ganapathysubramanian,
    and C. Hegde, “Encoding invariances in deep generative models,” *arXiv preprint
    arXiv:1906.01626*, 2019.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] J.-L. Wu, K. Kashinath, A. Albert, D. Chirila, H. Xiao *et al.*, “Enforcing
    statistical constraints in generative adversarial networks for modeling chaotic
    dynamical systems,” *Journal of Computational Physics*, vol. 406, p. 109209, 2020.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] N. Geneva and N. Zabaras, “Modeling the dynamics of pde systems with physics-constrained
    deep auto-regressive networks,” *Journal of Computational Physics*, vol. 403,
    p. 109056, 2020.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] B. Anderson, T.-S. Hy, and R. Kondor, “Cormorant: Covariant molecular
    neural networks,” *arXiv preprint arXiv:1906.04015*, 2019.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Lengyel, S. Garg, M. Milford, and J. C. van Gemert, “Zero-shot day-night
    domain adaptation with a physics prior,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 4399–4409.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, “A compositional
    object-based approach to learning physical dynamics,” *arXiv preprint arXiv:1612.00341*,
    2016.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] P. W. Battaglia, R. Pascanu, M. Lai, D. Rezende, and K. Kavukcuoglu, “Interaction
    networks for learning about objects, relations and physics,” *arXiv preprint arXiv:1612.00222*,
    2016.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Takeishi and A. Kalousis, “Physics-integrated variational autoencoders
    for robust and interpretable generative modeling,” *arXiv preprint arXiv:2102.13156*,
    2021.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Yin, V. Le Guen, J. Dona, E. de Bézenac, I. Ayed, N. Thome, and P. Gallinari,
    “Augmenting physical models with deep networks for complex dynamics forecasting,”
    *Journal of Statistical Mechanics: Theory and Experiment*, 2021.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] O. Linial, N. Ravid, D. Eytan, and U. Shalit, “Generative ode modeling
    with known unknowns,” in *Proceedings of the Conference on Health, Inference,
    and Learning*, 2021, pp. 79–94.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Yang and P. Perdikaris, “Adversarial uncertainty quantification in
    physics-informed neural networks,” *Journal of Computational Physics*, vol. 394,
    pp. 136–152, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Zhu, N. Zabaras, P.-S. Koutsourelakis, and P. Perdikaris, “Physics-constrained
    deep learning for high-dimensional surrogate modeling and uncertainty quantification
    without labeled data,” *Journal of Computational Physics*, vol. 394, pp. 56–81,
    2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Karumuri, R. Tripathy, I. Bilionis, and J. Panchal, “Simulator-free
    solution of high-dimensional stochastic elliptic partial differential equations
    using deep neural networks,” *Journal of Computational Physics*, vol. 404, p.
    109120, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Jia, J. Willard, A. Karpatne, J. Read, J. Zwart, M. Steinbach, and
    V. Kumar, “Physics guided rnns for modeling dynamical systems: A case study in
    simulating lake temperature profiles,” in *Proceedings of the 2019 SIAM International
    Conference on Data Mining*.   SIAM, 2019, pp. 558–566.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Jia, J. Willard, A. Karpatne, J. S. Read, J. A. Zwart, M. Steinbach,
    and V. Kumar, “Physics-guided machine learning for scientific discovery: An application
    in simulating lake temperature profiles,” *ACM/IMS Transactions on Data Science*,
    vol. 2, no. 3, pp. 1–26, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
    L. Downs, J. Ibarz, P. Pastor, K. Konolige *et al.*, “Using simulation and domain
    adaptation to improve efficiency of deep robotic grasping,” in *2018 IEEE international
    conference on robotics and automation (ICRA)*.   IEEE, 2018, pp. 4243–4250.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual
    and physical simulation for autonomous vehicles,” in *Field and service robotics*.   Springer,
    2018, pp. 621–635.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation
    for games, robotics and machine learning,” 2016.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] K. Ehsani, S. Tulsiani, S. Gupta, A. Farhadi, and A. Gupta, “Use the force,
    luke! learning to predict physical forces by simulating effects,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 224–233.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] R. Mottaghi, H. Bagherinezhad, M. Rastegari, and A. Farhadi, “Newtonian
    scene understanding: Unfolding the dynamics of objects in static images,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp.
    3521–3529.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
    randomization for transferring deep neural networks from simulation to the real
    world,” in *2017 IEEE/RSJ international conference on intelligent robots and systems
    (IROS)*.   IEEE, 2017, pp. 23–30.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Li, J. Chen, Y. Zhao, and Q. Ji, “Data-free prior model for facial
    action unit recognition,” *IEEE Transactions on affective computing*, vol. 4,
    no. 2, pp. 127–141, 2013.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] T. Teshima and M. Sugiyama, “Incorporating causal graphical prior knowledge
    into predictive modeling via simple data augmentation,” in *Uncertainty in Artificial
    Intelligence*.   PMLR, 2021, pp. 86–96.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] C. Corneanu, M. Madadi, and S. Escalera, “Deep structure inference network
    for facial action unit recognition,” in *Proceedings of European Conference on
    Computer Vision*, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] L. Chen, H. Su, and Q. Ji, “Deep structured prediction for facial landmark
    detection,” *Advances in neural information processing systems*, vol. 32, 2019.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] M. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath, G. Medioni,
    and L. Sigal, “Energy-based learning for scene graph generation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 13 936–13 945.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] T. Song, Z. Cui, W. Zheng, and Q. Ji, “Hybrid message passing with performance-driven
    structures for facial action unit detection,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 6267–6276.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] S. Srinivas Kancheti, A. Gowtham Reddy, V. N. Balasubramanian, and A. Sharma,
    “Matching learned causal effects of neural networks with domain priors,” *arXiv
    e-prints*, pp. arXiv–2111, 2021.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] L. Rieger, C. Singh, W. Murdoch, and B. Yu, “Interpretations are useful:
    penalizing explanations to align neural networks with prior knowledge,” in *International
    conference on machine learning*.   PMLR, 2020, pp. 8116–8126.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Y. Zhang, W. Dong, B.-G. Hu, and Q. Ji, “Classifier learning with prior
    probabilities for facial action unit recognition,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018, pp. 5108–5116.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Z. Cui, T. Song, Y. Wang, and Q. Ji, “Knowledge augmented deep neural
    networks for joint facial expression and action unit recognition,” *Advances in
    Neural Information Processing Systems*, vol. 33, 2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Yang, F. Liu, Z. Chen, X. Shen, J. Hao, and J. Wang, “Causalvae: Disentangled
    representation learning via neural structural causal models,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 9593–9602.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Qu and J. Tang, “Probabilistic logic neural networks for reasoning,”
    *arXiv preprint arXiv:1906.08495*, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J. Xu, Z. Zhang, T. Friedman, Y. Liang, and G. Broeck, “A semantic loss
    function for deep learning with symbolic knowledge,” in *International conference
    on machine learning*.   PMLR, 2018, pp. 5502–5511.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Y. Xie, Z. Xu, M. S. Kankanhalli, K. S. Meel, and H. Soh, “Embedding symbolic
    knowledge into deep networks,” *arXiv preprint arXiv:1909.01161*, 2019.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Liang, Z. Hu, H. Zhang, L. Lin, and E. P. Xing, “Symbolic graph reasoning
    meets convolutions,” *Advances in Neural Information Processing Systems*, vol. 31,
    pp. 1853–1863, 2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] F. Liu, C. You, X. Wu, S. Ge, X. Sun *et al.*, “Auto-encoding knowledge
    graph for unsupervised medical report generation,” *Advances in Neural Information
    Processing Systems*, vol. 34, 2021.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] R. Luo, N. Zhang, B. Han, and L. Yang, “Context-aware zero-shot recognition,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 07,
    2020, pp. 11 709–11 716.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Fang, K. Kuan, J. Lin, C. Tan, and V. Chandrasekhar, “Object detection
    meets knowledge graphs.”   International Joint Conferences on Artificial Intelligence,
    2017.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Gu, H. Zhao, Z. Lin, S. Li, J. Cai, and M. Ling, “Scene graph generation
    with external knowledge and image reconstruction,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 1969–1978.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] K. Marino, X. Chen, D. Parikh, A. Gupta, and M. Rohrbach, “Krisp: Integrating
    implicit and symbolic knowledge for open-domain knowledge-based vqa,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 14 111–14 121.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Z. Cui, P. Kapanipathi, K. Talamadupula, T. Gao, and Q. Ji, “Type-augmented
    relation prediction in knowledge graphs,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 35, no. 8, 2021, pp. 7151–7159.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] D. Kaur, S. Uslu, K. J. Rittichier, and A. Durresi, “Trustworthy artificial
    intelligence: a review,” *ACM Computing Surveys (CSUR)*, vol. 55, no. 2, pp. 1–38,
    2022.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Z. Wang, W. Xing, R. Kirby, and S. Zhe, “Physics informed deep kernel
    learning,” 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. M. Sultan, H. K. Wayment-Steele, and V. S. Pande, “Transferable neural
    networks for enhanced sampling of protein dynamics,” *Journal of chemical theory
    and computation*, vol. 14, no. 4, pp. 1887–1894, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] O. T. Unke, M. Bogojeski, M. Gastegger, M. Geiger, T. Smidt, and K.-R.
    Müller, “Se (3)-equivariant prediction of molecular wavefunctions and electronic
    densities,” *arXiv preprint arXiv:2106.02347*, 2021.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Yuan, S.-E. Wei, T. Simon, K. Kitani, and J. Saragih, “Simpoe: Simulated
    character control for 3d human pose estimation,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 7159–7169.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Shimada, V. Golyanik, W. Xu, P. Pérez, and C. Theobalt, “Neural monocular
    3d human motion capture with physical awareness,” *ACM Transactions on Graphics
    (TOG)*, vol. 40, no. 4, pp. 1–15, 2021.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] P. W. Anderson, “More is different,” *Science*, vol. 177, no. 4047, pp.
    393–396, 1972.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Born and E. Wolf, *Principles of optics: electromagnetic theory of
    propagation, interference and diffraction of light*.   Elsevier, 2013.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] M. Oren and S. K. Nayar, “Generalization of the lambertian model and implications
    for machine vision,” *International Journal of Computer Vision*, vol. 14, no. 3,
    pp. 227–251, 1995.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. T. Kajiya, “The rendering equation,” in *Proceedings of the 13th annual
    conference on Computer graphics and interactive techniques*, 1986, pp. 143–150.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Cao, Z. Chen, A. Chen, X. Chen, S. Li, and J. Yu, “Sparse photometric
    3d face reconstruction guided by morphable models,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018, pp. 4635–4644.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. Wang, S. Z. Li, Y. Wang, and W. Zhang, “Illumination modeling and normalization
    for face recognition,” in *2003 IEEE International SOI Conference. Proceedings
    (Cat. No. 03CH37443)*.   IEEE, 2003, pp. 104–111.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Xin, S. Nousias, K. N. Kutulakos, A. C. Sankaranarayanan, S. G. Narasimhan,
    and I. Gkioulekas, “A theory of fermat paths for non-line-of-sight shape reconstruction,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 6800–6809.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Kim, Y. Huo, and S.-E. Yoon, “Single image reflection removal with
    physically-based training images,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 5164–5173.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] C. Zhou, M. Teng, Y. Han, C. Xu, and B. Shi, “Learning to dehaze with
    polarization,” in *Thirty-Fifth Conference on Neural Information Processing Systems*,
    2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, “Learning with algorithmic
    supervision via continuous relaxations,” in *Thirty-Fifth Conference on Neural
    Information Processing Systems*, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] R. Hartley and A. Zisserman, *Multiple view geometry in computer vision*.   Cambridge
    university press, 2003.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Lutter, C. Ritter, and J. Peters, “Deep lagrangian networks: Using
    physics as model prior for deep learning,” *arXiv preprint arXiv:1907.04490*,
    2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] V. G. Satorras, Z. Akata, and M. Welling, “Combining generative and discriminative
    models for hybrid inference,” 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Zhang, J. Sun, X. He, H. Fu, R. Jia, and X. Zhou, “Modeling indirect
    illumination for inverse rendering,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 18 643–18 652.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. Zeng, S. Song, J. Lee, A. Rodriguez, and T. Funkhouser, “Tossingbot:
    Learning to throw arbitrary objects with residual physics,” *IEEE Transactions
    on Robotics*, 2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] R. Mottaghi, M. Rastegari, A. Gupta, and A. Farhadi, ““what happens if…”
    learning to predict the effect of forces in images,” in *European conference on
    computer vision*.   Springer, 2016, pp. 269–285.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum, “Galileo:
    Perceiving physical object properties by integrating a physics engine with deep
    learning,” *Advances in neural information processing systems*, vol. 28, pp. 127–135,
    2015.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-based
    control,” in *2012 IEEE/RSJ International Conference on Intelligent Robots and
    Systems*.   IEEE, 2012, pp. 5026–5033.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] P. Kubelka and F. Munk, “An article on optics of paint layers,” *Z. Tech.
    Phys*, vol. 12, no. 593-601, pp. 259–274, 1931.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. Haker, M. Böhme, T. Martinetz, and E. Barth, “Self-organizing maps
    for pose estimation with a time-of-flight camera,” in *Workshop on Dynamic 3D
    Imaging*.   Springer, 2009, pp. 142–153.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo, “Anatomy-aware
    3d human pose estimation with bone-based pose decomposition,” *IEEE Transactions
    on Circuits and Systems for Video Technology*, 2021.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. Li, T. Luo, Z. Lu, T. Xiang, and L. Wang, “Large-scale few-shot learning:
    Knowledge transfer with class hierarchy,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 7212–7220.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] R. Yu, A. Li, V. I. Morariu, and L. S. Davis, “Visual relationship detection
    with internal and external linguistic knowledge distillation,” in *Proceedings
    of the IEEE international conference on computer vision*, 2017, pp. 1974–1982.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] A. Akula, V. Jampani, S. Changpinyo, and S.-C. Zhu, “Robust visual reasoning
    via language guided neural module networks,” *Advances in Neural Information Processing
    Systems*, vol. 34, 2021.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] C. Rytting and D. Wingate, “Leveraging the inductive bias of large language
    models for abstract textual reasoning,” *Advances in Neural Information Processing
    Systems*, vol. 34, 2021.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] H. Yao, Y. Chen, Q. Ye, X. Jin, and X. Ren, “Refining language models
    with compositional explanations,” *Advances in Neural Information Processing Systems*,
    vol. 34, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] P. Ekman and W. V. Friesen, “Facial action coding system,” *Environmental
    Psychology & Nonverbal Behavior*, 1978.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] H. B. Enderton, *A mathematical introduction to logic*.   Elsevier, 2001.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] C. Menglong, J. Detao, Z. Ting, Z. Dehai, X. Cheng, C. Zhibo, and X. Xiaoqiang,
    “Image classification based on image knowledge graph and semantics,” in *2019
    IEEE 23rd International Conference on Computer Supported Cooperative Work in Design
    (CSCWD)*, 2019, pp. 81–86.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] L. Ji, Y. Wang, B. Shi, D. Zhang, Z. Wang, and J. Yan, “Microsoft concept
    graph: Mining semantic concepts for short text understanding,” *Data Intelligence*,
    vol. 1, no. 3, pp. 238–270, 2019.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] G. G. Towell and J. W. Shavlik, “Knowledge-based artificial neural networks,”
    *Artificial intelligence*, vol. 70, no. 1-2, pp. 119–165, 1994.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] A. S. A. Garcez and G. Zaverucha, “The connectionist inductive learning
    and logic programming system,” *Applied Intelligence*, vol. 11, no. 1, pp. 59–77,
    1999.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] R. Riegel, A. Gray, F. Luus, N. Khan, N. Makondo, I. Y. Akhalwaya, H. Qian,
    R. Fagin, F. Barahona, U. Sharma *et al.*, “Logical neural networks,” *arXiv preprint
    arXiv:2006.13155*, 2020.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] I. Donadello, L. Serafini, and A. D. Garcez, “Logic tensor networks for
    semantic image interpretation,” *arXiv preprint arXiv:1705.08968*, 2017.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Diligenti, M. Gori, and C. Sacca, “Semantic-based regularization for
    learning and inference,” *Artificial Intelligence*, vol. 244, pp. 143–165, 2017.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
    S. Hellmann, M. Morsey, P. Van Kleef, S. Auer *et al.*, “Dbpedia–a large-scale,
    multilingual knowledge base extracted from wikipedia,” *Semantic web*, vol. 6,
    no. 2, pp. 167–195, 2015.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open multilingual
    graph of general knowledge,” in *Thirty-First AAAI Conference on Artificial Intelligence*,
    2017.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] C. G. Enright, M. G. Madden, and N. Madden, “Bayesian networks for mathematical
    models: techniques for automatic construction and efficient inference,” *International
    Journal of Approximate Reasoning*, vol. 54, no. 2, pp. 323–342, 2013.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] S. Evers and P. J. Lucas, “Constructing bayesian networks for linear
    dynamic systems,” *BMAW-11 Preface*, 2011.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Zijun Cui received the B.S. degree from the Department of Physics, University
    of Science and Technology of China, Hefei, China, in 2015, and received the M.S.
    degree from the School of Engineering, Brown University, Rhode Island, USA, in
    2017. She is currently pursuing the Ph.D. degree with the Rensselaer Polytechnic
    Institute, NY, USA. She has broad experiences with deep learning and probabilistic
    graphical models. Her current research interests include knowledge-augmented deep
    learning, learning and inference on probabilistic graphical models, and their
    applications to computer vision and natural language processing. |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| Tian Gao received the Ph.D and B.S degrees both from the Department of Electrical,
    Computer, and System Engineering, Rensselaer Polytechnic Institute, Troy, NY,
    USA. He is currently a research staff member of IBM Research AI at T. J. Watson
    Research Center. His research focuses on machine learning and its application
    in computer vision and natural language processing. He has worked on probabilistic
    graphical models, knowledge extraction, causal discovery, and other aspects of
    machine learning. From 2010 to 2012, he was a National Science Foundation Triple
    Helix Program Fellow. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| Kartik Talamadupula is a Senior Research Scientist and Research Manager at
    IBM Research AI. His background is in automated planning and sequential decision
    making. He has applied decision-making techniques to human-in-the-loop AI, data-driven
    dialog, and human-agent collaboration. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| Qiang Ji received his Ph.D degree in Electrical Engineering from the University
    of Washington. He is currently a Professor with the Department of Electrical,
    Computer, and Systems Engineering at Rensselaer Polytechnic Institute (RPI). From
    2009 to 2010, he served as a program director at the National Science Foundation
    (NSF), Arlington, VA, USA, where he managed NSF’s computer vision and machine
    learning programs. He also held teaching and research positions with the Beckman
    Institute at University of Illinois at Urbana-Champaign, Urbana, IL, USA; the
    Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; the Dept.
    of Computer Science at University of Nevada, Reno, Nevada, USA; and the Air Force
    Research Laboratory, Rome, NY, USA. Prof. Ji currently serves as the director
    of the Intelligent Systems Laboratory (ISL) at RPI. Prof. Ji’s research interests
    are in human-centered computer vision, probabilistic graphical models, probabilistic
    deep learning, and their applications in various fields. He has published over
    300 papers in peer-reviewed journals and conferences, and has received multiple
    awards for his work. Prof. Ji is has served as an editor on several related IEEE
    and international journals and as a general chair, program chair, technical area
    chair, and program committee member for numerous international conferences/workshops.
    Prof. Ji is a fellow of the IEEE and the IAPR. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
