- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:08:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[1801.06889] Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1801.06889](https://ar5iv.labs.arxiv.org/html/1801.06889)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Visual Analytics in Deep Learning:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Interrogative Survey for the Next Frontiers
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Fred Hohman,  Minsuk Kahng,  Robert Pienta,  and Duen Horng Chau F. Hohman,
    M. Kahng, R. Pienta, and D. H. Chau are with the College of Computing, Georgia
    Tech, Atlanta, Georgia 30332, U.S.A.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: {fredhohman, kahng, pientars, polo}@gatech.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning has recently seen rapid development and received significant attention
    due to its state-of-the-art performance on previously-thought hard problems. However,
    because of the internal complexity and nonlinear structure of deep neural networks,
    the underlying decision making processes for why these models are achieving such
    performance are challenging and sometimes mystifying to interpret. As deep learning
    spreads across domains, it is of paramount importance that we equip users of deep
    learning with tools for understanding when a model works correctly, when it fails,
    and ultimately how to improve its performance. Standardized toolkits for building
    neural networks have helped democratize deep learning; visual analytics systems
    have now been developed to support model explanation, interpretation, debugging,
    and improvement. We present a survey of the role of visual analytics in deep learning
    research, which highlights its short yet impactful history and thoroughly summarizes
    the state-of-the-art using a human-centered interrogative framework, focusing
    on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by
    highlighting research directions and open research problems. This survey helps
    researchers and practitioners in both visual analytics and deep learning to quickly
    learn key aspects of this young and rapidly growing body of research, whose impact
    spans a diverse range of domains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, visual analytics, information visualization, neural networks![Refer
    to caption](img/c2bc6fa9c519fbe8d995f549ec635aca.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: A visual overview of our interrogative survey, and how each of the
    six questions, ”Why, Who, What, How, When, and Where,” relate to one another.
    Each question corresponds to one section of this survey, indicated by the numbered
    tag, near each question title. Each section lists its major subsections discussed
    in the survey.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is a specific set of techniques from the broader field of machine
    learning (ML) that focus on the study and usage of deep artificial neural networks
    to learn structured representations of data. First mentioned as early as the 1940s [[1](#bib.bib1)],
    artificial neural networks have a rich history [[2](#bib.bib2)], and have recently
    seen a dominate and pervasive resurgence [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]
    in many research domains by producing state-of-the-art results [[6](#bib.bib6),
    [7](#bib.bib7)] on a number of diverse big data tasks [[8](#bib.bib8), [9](#bib.bib9)].
    For example, the premiere machine learning, deep learning, and artificial intelligence
    (AI) conferences have seen enormous growth in attendance and paper submissions
    since early 2010s. Furthermore, open-source toolkits and programming libraries
    for building, training, and evaluating deep neural networks have become more robust
    and easy to use, democratizing deep learning. As a result, the barrier to developing
    deep learning models is lower than ever before and deep learning applications
    are becoming pervasive.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: While this technological progress is impressive, it comes with unique and novel
    challenges. For example, the lack of interpretability and transparency of neural
    networks, from the learned representations to the underlying decision process,
    is an important problem to address. Making sense of why a particular model misclassifies
    test data instances or behaves poorly at times is a challenging task for model
    developers. Similarly, end-users interacting with an application that relies on
    deep learning to make critical decisions may question its reliability if no explanation
    is given by the model, or become baffled if the explanation is convoluted. While
    explaining neural network decisions is important, there are numerous other problems
    that arise from deep learning, such as AI safety and security (e.g., when using
    models in applications such as self-driving vehicles), and compromised trust due
    to bias in models and datasets, just to name a few. These challenges are often
    compounded, due to the large datasets required to train most deep learning models.
    As worrisome as these problems are, they will likely become even more widespread
    as more AI-powered systems are deployed in the world. Therefore, a general sense
    of model understanding is not only beneficial, but often required to address the
    aforementioned issues.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization and visual analytics excel at knowledge communication and
    insight discovery by using encodings to transform abstract data into meaningful
    representations. In the seminal work by Zeiler and Fergus [[10](#bib.bib10)],
    a technique called deconvolutional networks enabled projection from a model’s
    learned feature space back to the pixel space. Their technique and results give
    insight into what types of features deep neural networks are learning at specific
    layers, and also serve as a debugging tool for improving a model. This work is
    often credited for popularizing visualization in the machine learning and computer
    vision communities in recent years, putting a spotlight on it as a powerful tool
    that helps people understand and improve deep learning models. However, visualization
    research for neural networks started well before [[11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]. Over just a handful of years, many different techniques have
    been introduced to help interpret what neural networks are learning. Many such
    techniques generate static images, such as attention maps and heatmaps for image
    classification, indicating which parts of an image are most important to the classification.
    However, interaction has also been incorporated into the model understanding process
    in visual analytics tools to help people gain insight [[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)]. This hybrid research area has grown in both academia and industry,
    forming the basis for many new research papers, academic workshops, and deployed
    industry tools.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化和视觉分析通过使用编码将抽象数据转化为有意义的表示，擅长于知识传递和洞察发现。在Zeiler和Fergus的开创性工作中[[10](#bib.bib10)]，一种叫做去卷积网络的技术使得从模型学习的特征空间投影回像素空间成为可能。他们的技术和结果提供了对深度神经网络在特定层次学习到的特征类型的洞察，同时也作为改进模型的调试工具。这项工作通常被认为在近年来使得可视化在机器学习和计算机视觉领域广受欢迎，突显了它作为一个帮助人们理解和改进深度学习模型的强大工具。然而，神经网络的可视化研究早在[[11](#bib.bib11)、[12](#bib.bib12)、[13](#bib.bib13)]之前就已开始。在短短几年间，许多不同的技术被提出以帮助解释神经网络所学习的内容。许多这样的技术生成静态图像，如用于图像分类的注意力图和热图，指出图像的哪些部分对分类最为重要。然而，互动也已被纳入视觉分析工具中的模型理解过程中，以帮助人们获得洞察[[14](#bib.bib14)、[15](#bib.bib15)、[16](#bib.bib16)]。这个混合研究领域在学术界和工业界都得到了发展，形成了许多新的研究论文、学术研讨会和部署的行业工具的基础。
- en: 'In this survey, we summarize a large number of deep learning visualization
    works using the Five W’s and How (Why, Who, What, How, When, and Where). Figure
    [1](#S0.F1 "Figure 1 ‣ Visual Analytics in Deep Learning: An Interrogative Survey
    for the Next Frontiers") presents a visual overview of how these interrogative
    questions reveal and organize the various facets of deep learning visualization
    research and their related topics. By framing the survey in this way, many existing
    works fit a description as the following fictional example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项调查中，我们使用“五个W和一个H”（为什么、谁、什么、如何、何时和哪里）总结了大量的深度学习可视化工作。图[1](#S0.F1 "Figure
    1 ‣ Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers")展示了这些提问如何揭示和组织深度学习可视化研究的各个方面及其相关主题的可视化概述。通过这种方式框架调查，许多现有工作符合以下虚构示例的描述：'
- en: To interpret representations learned by deep models (why), model developers
    (who) visualize neuron activations in convolutional neural networks (what) using
    t-SNE embeddings (how) after the training phase (when) to solve an urban planning
    problem (where).
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了解释深度模型学到的表示（为什么），模型开发者（谁）在训练阶段后（何时）使用t-SNE嵌入（如何）可视化卷积神经网络中的神经元激活（什么），以解决城市规划问题（哪里）。
- en: This framing captures the needs, audience, and techniques of deep learning visualization,
    and positions new work’s contributions in the context of existing literature.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种框架捕捉了深度学习可视化的需求、受众和技术，并将新工作的贡献置于现有文献的背景中。
- en: We conclude by highlighting prominent research directions and open problems.
    We hope that this survey acts as a companion text for researchers and practitioners
    wishing to understand how visualization supports deep learning research and applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过突出重要的研究方向和未解的问题来结束。我们希望这项调查能够作为研究人员和实践者理解可视化如何支持深度学习研究和应用的参考资料。
- en: 2 Our Contributions & Method of Survey
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 我们的贡献与调查方法
- en: 2.1 Our Contributions
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 我们的贡献
- en: C1.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C1.
- en: We present a comprehensive, timely survey on visualization and visual analytics
    in deep learning research, using a human-centered, interrogative framework. This
    method enables us to position each work with respect to its Five Ws and How (Why,
    Who, What, How, When, and Where), and flexibly discuss and highlight existing
    works’ multifaceted contributions.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our human-centered approach using the Five W’s and How — based on how we familiarize
    ourselves with new topics in everyday settings — enables readers to quickly grasp
    important facets of this young and rapidly growing body of research.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our interrogative process provides a framework to describe existing works, as
    well as a model to base new work off of.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: C2.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To highlight and align the cross-cutting impact that visual analytics has had
    on deep learning across a broad range of domains, our survey goes beyond visualization-focused
    venues, extending a wide scope that encompasses most relevant works from many
    top venues in artificial intelligence, machine learning, deep learning, and computer
    vision. We highlight how visual analytics has been an integral component in solving
    some of AI’s biggest modern problems, such as neural network interpretability,
    trust, and security.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C3.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As deep learning, and more generally AI, touches more aspects of our daily lives,
    we highlight important research directions and open problems that we distilled
    from the survey. These include improving the capabilities of visual analytics
    systems for furthering interpretability, conducting more effective design studies
    for evaluating system usability and utility, advocating humans’ important roles
    in AI-powered systems, and promoting proper and ethical use of AI applications
    to benefit society.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 Survey Methodology & Summarization Process
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We selected existing works from top computer science journals and conferences
    in visualization (e.g., IEEE Transactions on Visualization and Computer Graphics
    (TVCG)), visual analytics (e.g., IEEE Conference on Visual Analytics Science and
    Technology (VAST)) and deep learning (e.g., Conference on Neural Information Processing
    Systems (NIPS) and the International Conference on Machine Learning (ICML)). Since
    deep learning visualization is relatively new, much of the relevant work has appeared
    in workshops at the previously mentioned venues; therefore, we also include those
    works in our survey. Table [I](#S2.T1 "TABLE I ‣ 2.2 Survey Methodology & Summarization
    Process ‣ 2 Our Contributions & Method of Survey ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers") lists some of the most prominent
    publication venues and their acronyms. We also inspected preprints posted on arXiv
    ([https://arxiv.org/](https://arxiv.org/)), an open-access, electronic repository
    of manuscript preprints, whose computer science subject has become a hub for new
    deep learning research. Finally, aside from the traditional aforementioned venues,
    we include non-academic venues with significant attention such as Distill, industry
    lab research blogs, and research blogs of influential figures. Because of the
    rapid growth of deep learning research and the lack of a perfect fit for publishing
    and disseminating work in this hybrid area, therefore, the inclusion of these
    non-traditional sources are important to review, as they are highly influential
    and impactful to the field.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Relevant visualization and AI venues, in the order of: journals, conferences,
    workshops, open access journals, and preprint repositories. In each category,
    visualization venues precede AI venues.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '| TVCG | IEEE Transactions on Visualization and Computer Graphics |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| VAST | IEEE Conference on Visual Analytics Science and Technology |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| InfoVis | IEEE Information Visualization |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| VIS | IEEE Visualization Conference (VAST+InfoVis+SciVis) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| CHI | ACM Conference on Human Factors in Computing Systems |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| NIPS | Conference on Neural Information Processing Systems |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| ICML | International Conference on Machine Learning |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| CVPR | Conference on Computer Vision and Pattern Recognition |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| ICLR | International Conference on Learning Representations |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| VADL | IEEE VIS Workshop on Visual Analytics for Deep Learning |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| HCML | CHI Workshop on Human Centered Machine Learning |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| IDEA | KDD Workshop on Interactive Data Exploration & Analytics |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '|  | ICML Workshop on Visualization for Deep Learning |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| WHI | ICML Workshop on Human Interpretability in ML |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '|  | NIPS Workshop on Interpreting, Explaining and Visualizing Deep Learning
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '|  | NIPS Interpretable ML Symposium |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| FILM | NIPS Workshop on Future of Interactive Learning Machines |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '|  | ACCV Workshop on Interpretation and Visualization of Deep Neural Nets
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '|  | ICANN Workshop on Machine Learning and Interpretability |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Distill | Distill: Journal for Supporting Clarity in Machine Learning |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Distill | Distill: Journal for Supporting Clarity in Machine Learning |'
- en: '| arXiv | arXiv.org e-Print Archive |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| arXiv | arXiv.org e-Print Archive |'
- en: 'Visualization takes many forms throughout the deep learning literature. This
    survey focuses on visual analytics for deep learning. We also include related
    works from the AI and computer vision communities that contribute novel static
    visualizations. So far, the majority of work surrounds convolutional neural networks
    (CNNs) and image data; more recent work has begun to visualize other models, e.g.,
    recurrent neural networks (RNNs), long short-term memory units (LSTMs), and generative
    adversarial networks (GANs). For each work, we recorded the following information
    if present:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉化在深度学习文献中有多种形式。本调查专注于深度学习的视觉分析。我们还包括了来自人工智能和计算机视觉社区的相关工作，这些工作提供了新的静态视觉化。目前，大多数工作围绕卷积神经网络（CNNs）和图像数据展开；最近的工作开始可视化其他模型，例如递归神经网络（RNNs）、长短期记忆单元（LSTMs）和生成对抗网络（GANs）。对于每项工作，如果有，我们记录了以下信息：
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Metadata (title, authors, venue, and year published)
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元数据（标题、作者、刊物和出版年份）
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: General approach and short summary
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一般方法和简要总结
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Explicit contributions
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 明确的贡献
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future work
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Design component (e.g. user-centered design methodologies, interviews, evaluation)
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计组件（例如用户中心设计方法、访谈、评估）
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Industry involvement and open-source code
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行业参与和开源代码
- en: With this information, we used the Five W’s and How (Why, Who, What, How, When,
    and Where) to organize these existing works and the current state-of-the-art of
    visualization and visual analytics in deep learning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们使用了“五个W和一个H”（为什么、谁、什么、如何、何时和哪里）来组织这些现有工作和深度学习中的视觉化与视觉分析的最新状态。
- en: 2.3 Related Surveys
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 相关调查
- en: While there is a larger literature for visualization for machine learning, including
    predictive visual analytics [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]
    and human-in-the-loop interactive machine learning [[20](#bib.bib20), [21](#bib.bib21)],
    to our knowledge there is no comprehensive survey of visualization and visual
    analytics for deep learning. Regarding deep neural networks, related surveys include
    a recent book chapter that discusses visualization of deep neural networks related
    to the field of computer vision [[22](#bib.bib22)], an unpublished essay that
    proposes a preliminary taxonomy for visualization techniques [[23](#bib.bib23)],
    and an article that focuses on describing interactive model analysis, which mentions
    deep learning in a few contexts while describing a high-level framework for general
    machine learning models [[24](#bib.bib24)]. A recent overview article by Choo
    and Liu [[25](#bib.bib25)] is the closest in spirit to our survey. Our survey
    provides wider coverage and more detailed analysis of the literature.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于机器学习的视觉化文献较多，包括预测性视觉分析[[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]和人机交互式机器学习[[20](#bib.bib20),
    [21](#bib.bib21)]，但据我们了解，没有针对深度学习的视觉化和视觉分析的全面调查。关于深度神经网络，相关调查包括最近一本书的章节，讨论了与计算机视觉领域相关的深度神经网络的可视化[[22](#bib.bib22)]，一篇未发表的论文提出了视觉化技术的初步分类法[[23](#bib.bib23)]，以及一篇专注于描述交互式模型分析的文章，在描述通用机器学习模型的高级框架时提到了一些深度学习的背景[[24](#bib.bib24)]。Choo和Liu最近的一篇概述文章[[25](#bib.bib25)]在精神上最接近我们的调查。我们的调查提供了更广泛的覆盖和更详细的文献分析。
- en: Different from all the related articles mentioned above, our survey provides
    a comprehensive, human-centered, and interrogative framework to describe deep
    learning visual analytics tools, discusses the new, rapidly growing community
    at large, and presents the major research trajectories synthesized from existing
    literature.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述所有相关文献不同，我们的调查提供了一个全面的、人本中心的、探究性框架来描述深度学习视觉分析工具，讨论了快速增长的新兴社区，并呈现了从现有文献中综合出的主要研究轨迹。
- en: 2.4 Survey Overview & Organization
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 调查概述与组织
- en: 'Section [3](#S3 "3 Common Terminology ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers") introduces common deep learning
    terminology. Figure [1](#S0.F1 "Figure 1 ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers") shows a visual overview of this
    survey’s structure and Table [II](#S3.T2 "TABLE II ‣ 3 Common Terminology ‣ Visual
    Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers") summarizes
    representative works. Each interrogative question (Why, Who, What, How, When,
    and Where) is given its own section for discussion, ordered to best motivate why
    visualization and visual analytics in deep learning is such a rich and exciting
    area of research.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S2.I3.ix1.1.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="33.06"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">$\boldsymbol{\S}$
    [4](#S4 "4 Why Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An
    Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we want to visualize deep learning? Why and for what purpose would one
    want to use visualization in deep learning?
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<svg id="S2.I3.ix2.1.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="33.06"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">$\boldsymbol{\S}$
    [5](#S5 "5 Who Uses Deep Learning Visualization ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who wants to visualize deep learning? Who are the types of people and users
    that would use and stand to benefit from visualizing deep learning?
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<svg id="S2.I3.ix3.1.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="33.06"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">$\boldsymbol{\S}$
    [6](#S6 "6 What to Visualize in Deep Learning ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can we visualize in deep learning? What data, features, and relationships
    are inherent to deep learning that can be visualized?
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<svg id="S2.I3.ix4.1.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="33.06"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">$\boldsymbol{\S}$
    [7](#S7 "7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we visualize deep learning? How can we visualize the aforementioned
    data, features, and relationships?
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<svg id="S2.I3.ix5.1.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="33.06"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">$\boldsymbol{\S}$
    [8](#S8 "8 When to Visualize in the Deep Learning Process ‣ Visual Analytics in
    Deep Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When can we visualize deep learning? When in the deep learning process is visualization
    used and best suited?
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<svg id="S2.I3.ix6.1.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="33.06"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">$\boldsymbol{\S}$
    [9](#S9 "9 Where is Deep Learning Visualization ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is deep learning visualization being used? Where has deep learning visualization
    been used?
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Section [10](#S10 "10 Research Directions & Open Problems ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers") presents research
    directions and open problems that we gathered and distilled from the literature
    survey. Section [11](#S11 "11 Conclusion ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers") concludes the survey.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 3 Common Terminology
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To enhance readability of this survey, and to provide quick references for
    readers new to deep learning, we have tabulated a sample of relevant and common
    deep learning terminology used in this work, shown in Table [III](#S3.T3 "TABLE
    III ‣ 3 Common Terminology ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers"). The reader may want to refer to Table [III](#S3.T3
    "TABLE III ‣ 3 Common Terminology ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers") throughout this survey for technical terms, meanings,
    and synonyms used in various contexts of discussion. The table serves as an introduction
    and summarization of the state-of-the-art. For definitive technical and mathematical
    descriptions, we encourage the reader to refer to excellent texts on deep learning
    and neural network design, such as the Deep Learning textbook [[26](#bib.bib26)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Overview of representative works in visual analytics for deep learning.
    Each row is one work; works are sorted alphabetically by first author’s last name.
    Each column corresponds to a subsection from the six interrogative questions.
    A work’s relevant subsection is indicated by a colored cell.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '|  | WHY | WHO | WHAT | HOW | WHEN | WHERE |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: '| Work | <svg id="S3.T2.2.2.2.1.pic1" class="ltx_picture" height="20.06" overflow="visible"
    version="1.1" width="22.29"><g transform="translate(0,20.06) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject width="15.37" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[4.1](#S4.SS1
    "4.1 Interpretability & Explainability ‣ 4 Why Visualize Deep Learning ‣ Visual
    Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretability & Explainability  | <svg id="S3.T2.3.3.3.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[4.2](#S4.SS2 "4.2 Debugging & Improving Models ‣ 4 Why Visualize
    Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Debugging & Improving Models  | <svg id="S3.T2.4.4.4.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[4.3](#S4.SS3 "4.3 Comparing & Selecting Models ‣ 4 Why Visualize
    Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing & Selecting Models  | <svg id="S3.T2.5.5.5.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[4.4](#S4.SS4 "4.4 Teaching Deep Learning Concepts ‣ 4 Why Visualize
    Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Teaching Deep Learning Concepts  | <svg id="S3.T2.6.6.6.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[5.1](#S5.SS1 "5.1 Model Developers & Builders ‣ 5 Who Uses Deep
    Learning Visualization ‣ Visual Analytics in Deep Learning: An Interrogative Survey
    for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Developers & Builders  | <svg id="S3.T2.7.7.7.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[5.2](#S5.SS2 "5.2 Model Users ‣ 5 Who Uses Deep Learning Visualization
    ‣ Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Users  | <svg id="S3.T2.8.8.8.1.pic1" class="ltx_picture" height="20.06"
    overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[5.3](#S5.SS3 "5.3 Non-experts ‣ 5 Who Uses Deep Learning Visualization
    ‣ Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-experts  | <svg id="S3.T2.9.9.9.1.pic1" class="ltx_picture" height="20.06"
    overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[6.1](#S6.SS1 "6.1 Computational Graph & Network Architecture
    ‣ 6 What to Visualize in Deep Learning ‣ Visual Analytics in Deep Learning: An
    Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Computational Graph & Network Architecture  | <svg id="S3.T2.10.10.10.1.pic1"
    class="ltx_picture" height="20.06" overflow="visible" version="1.1" width="22.29"><g
    transform="translate(0,20.06) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46
    6.92)"><foreignobject width="15.37" height="13.84" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[6.2](#S6.SS2 "6.2 Learned Model Parameters
    ‣ 6 What to Visualize in Deep Learning ‣ Visual Analytics in Deep Learning: An
    Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Learned Model Parameters  | <svg id="S3.T2.11.11.11.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[6.3](#S6.SS3 "6.3 Individual Computational Units ‣ 6 What to
    Visualize in Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Individual Computational Units  | <svg id="S3.T2.12.12.12.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[6.4](#S6.SS4 "6.4 Neurons in High-dimensional Space ‣ 6 What
    to Visualize in Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Neurons in High-dimensional Space  | <svg id="S3.T2.13.13.13.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[6.5](#S6.SS5 "6.5 Aggregated Information ‣ 6 What to Visualize
    in Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey
    for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Aggregated Information  | <svg id="S3.T2.14.14.14.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[7.1](#S7.SS1 "7.1 Node-link Diagrams for Network Architectures
    ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Node-link Diagrams for Network Architecture  | <svg id="S3.T2.15.15.15.1.pic1"
    class="ltx_picture" height="20.06" overflow="visible" version="1.1" width="22.29"><g
    transform="translate(0,20.06) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46
    6.92)"><foreignobject width="15.37" height="13.84" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[7.2](#S7.SS2 "7.2 Dimensionality
    Reduction & Scatter Plots ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality Reduction & Scatter Plots  | <svg id="S3.T2.16.16.16.1.pic1"
    class="ltx_picture" height="20.06" overflow="visible" version="1.1" width="22.29"><g
    transform="translate(0,20.06) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46
    6.92)"><foreignobject width="15.37" height="13.84" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[7.3](#S7.SS3 "7.3 Line Charts for
    Temporal Metrics ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep
    Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Line Charts for Temporal Metrics  | <svg id="S3.T2.17.17.17.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[7.4](#S7.SS4 "7.4 Instance-based Analysis & Exploration ‣ 7 How
    to Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance-based Analysis & Exploration  | <svg id="S3.T2.18.18.18.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[7.5](#S7.SS5 "7.5 Interactive Experimentation ‣ 7 How to Visualize
    Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Interactive Experimentation  | <svg id="S3.T2.19.19.19.1.pic1" class="ltx_picture"
    height="20.06" overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[7.6](#S7.SS6 "7.6 Algorithms for Attribution & Feature Visualization
    ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithms for Attribution & Feature Visualization  | <svg id="S3.T2.20.20.20.1.pic1"
    class="ltx_picture" height="20.06" overflow="visible" version="1.1" width="22.29"><g
    transform="translate(0,20.06) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46
    6.92)"><foreignobject width="15.37" height="13.84" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[8.1](#S8.SS1 "8.1 During Training
    ‣ 8 When to Visualize in the Deep Learning Process ‣ Visual Analytics in Deep
    Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'During Training  | <svg id="S3.T2.21.21.21.1.pic1" class="ltx_picture" height="20.06"
    overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[8.2](#S8.SS2 "8.2 After Training ‣ 8 When to Visualize in the
    Deep Learning Process ‣ Visual Analytics in Deep Learning: An Interrogative Survey
    for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'After Training  |  <svg id="S3.T2.23.23.23.1.pic1" class="ltx_picture" height="20.06"
    overflow="visible" version="1.1" width="22.29"><g transform="translate(0,20.06)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 3.46 6.92)"><foreignobject
    width="15.37" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[9.2](#S9.SS2 "9.2 A Vibrant Research Community: Hybrid, Apace,
    & Open-sourced ‣ 9 Where is Deep Learning Visualization ‣ Visual Analytics in
    Deep Learning: An Interrogative Survey for the Next Frontiers")</foreignobject></g></g></svg>'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Publication Venue  |
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '| Abadi, et al., 2016 [[27](#bib.bib27)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     arXiv |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Bau, et al., 2017 [[28](#bib.bib28)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     CVPR |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Bilal, et al., 2017 [[29](#bib.bib29)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Bojarski, et al., 2016 [[30](#bib.bib30)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     arXiv |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| Bruckner, 2014 [[31](#bib.bib31)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     MS Thesis |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Carter, et al., 2016 [[32](#bib.bib32)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     Distill |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Cashman, et al., 2017 [[33](#bib.bib33)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     VADL |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Chae, et al., 2017 [[34](#bib.bib34)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     VADL |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Chung, et al., 2016 [[35](#bib.bib35)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     FILM |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| Goyal, et al., 2016 [[36](#bib.bib36)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     arXiv |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Harley, 2015 [[37](#bib.bib37)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ISVC |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Hohman, et al., 2017 [[38](#bib.bib38)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     CHI |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| Kahng, et al., 2018 [[39](#bib.bib39)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| Karpathy, et al., 2015 [[40](#bib.bib40)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     arXiv |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| Li, et al., 2015 [[41](#bib.bib41)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     arXiv |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| Liu, et al., 2017 [[14](#bib.bib14)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| Liu, et al., 2018 [[42](#bib.bib42)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| Ming, et al., 2017 [[43](#bib.bib43)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     VAST |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| Norton & Qi, 2017 [[44](#bib.bib44)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     VizSec |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| Olah, 2014 [[45](#bib.bib45)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     Web |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| Olah, et al., 2018 [[46](#bib.bib46)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     Distill |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| Pezzotti, et al., 2017 [[47](#bib.bib47)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| Rauber, et al., 2017 [[48](#bib.bib48)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| Robinson, et al., 2017 [[49](#bib.bib49)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     GeoHum. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| Rong & Adar, 2016 [[50](#bib.bib50)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ICML VIS |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| Smilkov, et al., 2016 [[51](#bib.bib51)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     NIPS WS. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Smilkov, et al., 2017 [[16](#bib.bib16)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ICML VIS |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Strobelt, et al., 2018 [[52](#bib.bib52)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Tzeng & Ma, 2005 [[13](#bib.bib13)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     VIS |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Wang, et al., 2018 [[53](#bib.bib53)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Webster, et al., 2017 [[54](#bib.bib54)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     Web |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Wongsuphasawat, et al., 2018 [[15](#bib.bib15)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     TVCG |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Yosinski, et al., 2015 [[55](#bib.bib55)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ICML DL |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| Zahavy, et al., 2016 [[56](#bib.bib56)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ICML |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| Zeiler, et al., 2014 [[10](#bib.bib10)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ECCV |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| Zeng, et al., 2017 [[57](#bib.bib57)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     VADL |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Zhong, et al., 2017 [[58](#bib.bib58)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ICML VIS |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Zhu, et al., 2016 [[59](#bib.bib59)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
     ECCV |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Foundational deep learning terminology used in this paper, sorted
    by importance. In a term’s “meaning” (last column), defined terms are italicized.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '| Technical Term | Synonyms | Meaning |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Neural Network | Artificial neural net, net | Biologically-inspired models
    that form the basis of deep learning; approximate functions dependent upon a large
    and unknown amount of inputs consisting of layers of neurons |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Neuron | Computational unit, node | Building blocks of neural networks, entities
    that can apply activation functions |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| Weights | Edges | The trained and updated parameters in the neural network
    model that connect neurons to one another |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Layer | Hidden layer | Stacked collection of neurons that attempt to extract
    features from data; a layer’s input is connected to a previous layer’s output
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Computational Graph | Dataflow graph | Directed graph where nodes represent
    operations and edges represent data paths; when implementing neural network models,
    often times they are represented as these |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Activation Functions | Transform function | Functions embedded into each
    layer of a neural network that enable the network represent complex non-linear
    decisions boundaries |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Activations | Internal representation | Given a trained network one can pass
    in data and recover the activations at any layer of the network to obtain its
    current representation inside the network |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Convolutional Neural Network | CNN, convnet | Type of neural network composed
    of convolutional layers that typically assume image data as input; these layers
    have depth unlike typical layers that only have width (number of neurons in a
    layer); they make use of filters (feature & pattern detectors) to extract spatially
    invariant representations |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Long Short-Term Memory | LSTM | Type of neural network, often used in text
    analysis, that addresses the vanishing gradient problem by using memory gates
    to propagate gradients through the network to learn long-range dependencies |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| Loss Function | Objective function, cost function, error | Also seen in general
    ML contexts, defines what success looks like when learning a representation, i.e.,
    a measure of difference between a neural network’s prediction and ground truth
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| Embedding | Encoding | Representation of input data (e.g., images, text,
    audio, time series) as vectors of numbers in a high-dimensional space; oftentimes
    reduced so data points (i.e., their vectors) can be more easily analyzed (e.g.,
    compute similarity) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| Recurrent Neural Network | RNN | Type of neural network where recurrent connections
    allow the persistence (or “memory“) of previous inputs in the network’s internal
    state which are used to influence the network output |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| Generative Adversarial Networks | GAN | Method to conduct unsupervised learning
    by pitting a generative network against a discriminative network; the first network
    mimics the probability distribution of a training dataset in order to fool the
    discriminative network into judging that the generated data instance belongs to
    the training set |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| Epoch | Data pass | A complete pass through a given dataset; by the end of
    one epoch, a neural network will have seen every datum within the dataset once
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: 4 Why Visualize Deep Learning
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Interpretability & Explainability
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most abundant, and to some, the most important reason why people want to
    visualize deep learning is to understand how deep learning models make decisions
    and what representations they have learned, so we can place trust in a model [[60](#bib.bib60)].
    This notion of general model understanding has been called the interpretability
    or explainability when referring to machine learning models [[60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]. However, neural networks particularly suffer
    from this problem since oftentimes real world and high-performance models contain
    a large number of parameters (in the millions) and exhibit extreme internal complexity
    by using many non-linear transformations at different stages during training.
    Many works motivate this problem by using phrases such as “opening and peering
    through the black-box,” “transparency,” and “interpretable neural networks,” [[13](#bib.bib13),
    [63](#bib.bib63), [56](#bib.bib56)], referring the internal complexity of neural
    networks.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Discordant Definitions for Interpretability
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unfortunately, there is no universally formalized and agreed upon definition
    for explainability and interpretability in deep learning, which makes classifying
    and qualifying interpretations and explanations troublesome. In Lipton’s work
    “The Mythos of Model Interpretability [[60](#bib.bib60)],” he surveys interpretability-related
    literature, and discovers diverse motivations for why interpretability is important
    and is occasionally discordant. Despite this ambiguity, he attempts to refine
    the notion of interpretability by making a first step towards providing a comprehensive
    taxonomy of both the desiderata and methods in interpretability research. One
    important point that Lipton makes is the difference between interpretability and
    an explanation; an explanation can show predictions without elucidating the mechanisms
    by which models work [[60](#bib.bib60)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In another work originally presented as a tutorial at the International Conference
    on Acoustics, Speech, and Signal Processing by Montavona et al. [[61](#bib.bib61)],
    the authors propose exact definitions of both an interpretation and an explanation.
    First, an interpretation is “the mapping of an abstract concept (e.g., a predicted
    class) into a domain that the human can make sense of.” They then provide some
    examples of interpretable domains, such as images (arrays of pixels) and text
    (sequences of words), and noninterpretable domains, such as abstract vector spaces
    (word embeddings). Second, an explanation is “the collection of features of the
    interpretable domain, that have contributed for a given example to produce a decision
    (e.g., classification or regression).” For example, an explanation can be a heatmap
    highlighting which pixels of the input image most strongly support an image classification
    decision, or in natural language processing, explanations can highlight certain
    phrases of text.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'However, both of the previous works are written by members of the AI community,
    whereas work by Miller titled “Explanation in Artificial Intelligence: Insights
    from the Social Sciences” [[62](#bib.bib62)] postulates that much of the current
    research uses only AI researchers’ intuition of what constitutes a “good” explanation.
    He suggests that if the focus on explaining decisions or actions to a human observer
    is the goal, then if these techniques are to succeed, the explanations they generate
    should have a structure that humans accept. Much of Miller’s work highlights vast
    and valuable bodies of research in philosophy, psychology, and cognitive science
    for how people define, generate, select, evaluate, and present explanations, and
    he argues that interpretability and explainability research should leverage and
    build upon this history [[62](#bib.bib62)]. In another essay, Offert [[64](#bib.bib64)]
    argues that to make interpretability more rigorous, we must first identify where
    it is impaired by intuitive considerations. That is, we have to “consider it precisely
    in terms of what it is not.” While multiple works bring different perspectives,
    Lipton makes the keen observation that for the field to progress, the community
    must critically engage with this problem formulation issue [[60](#bib.bib60)].
    Further research will help solidify the notions of interpretation and explanation.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Interpretation as Qualitative Support for Model Evaluation in Various
    Application Domains
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While research into interpretation itself is relatively new, its impact has
    already been seen in applied deep learning contexts. A number of applied data
    science and AI projects that use deep learning models include a section on interpretation
    to qualitatively evaluate and support the model’s predictions and the work’s claims
    overall. An example of this is an approach for end-to-end neural machine translation.
    In the work by Johnson et al. [[65](#bib.bib65)], the authors present a simple
    and efficient way to translate between multiple languages using a single model,
    taking advantage of multilingual data to improve neural machine translation for
    all languages involved. The authors visualize an embedding of text sequences,
    for example, sentences from multiple languages, to support and hint at a universal
    interlingua representation. Another work that visualizes large machine learning
    embeddings is by Zahavy et al. [[56](#bib.bib56)], where the authors analyze deep
    Q-networks (DQN), a popular reinforcement learning model, to understand and describe
    the policies learned by DQNs for three different Atari 2600 video games. An application
    for social good by Robinson et al. [[49](#bib.bib49)] demonstrates how to apply
    deep neural networks on satellite imagery to perform population prediction and
    disaggregation, jointly answering the questions “where do people live?” and “how
    many people live there?”. In general, they show how their methodology can be an
    effective tool for extracting information from inherently unstructured, remotely-sensed
    data to provide effective solutions to social problems.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: These are only a few domains where visualization and deep learning interpretation
    have been successfully used. Others include building trust in autonomous driving
    vehicles [[30](#bib.bib30)], explaining decisions made by medical imaging models,
    such as MRIs on brain scans, to provide medical experts more information for making
    diagnoses [[66](#bib.bib66)], and using visual analytics to explore automatically-learned
    features from street imagery to gain perspective into identity, function, demographics,
    and affluence in urban spaces, which is useful for urban design and planning [[67](#bib.bib67)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In this survey we will mention interpretation and explanation often, as they
    are the most common motivations for deep learning visualization. Later, we will
    discuss the different visualization techniques and visual analytics systems that
    focus on neural network interpretability for embeddings [[51](#bib.bib51)], text [[41](#bib.bib41),
    [40](#bib.bib40), [32](#bib.bib32)], quantifying interpretability [[28](#bib.bib28)],
    and many different image-based techniques stemming from the AI communities [[68](#bib.bib68),
    [10](#bib.bib10), [69](#bib.bib69), [4](#bib.bib4), [70](#bib.bib70)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Debugging & Improving Models
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building machine learning models is an iterative design process [[71](#bib.bib71),
    [72](#bib.bib72), [73](#bib.bib73)], and developing deep neural networks is no
    different. While mathematical foundations have been laid, deep learning still
    has many open research questions. For example, finding the exact combinations
    of model depth, layer width, and finely tuned hyperparameters is nontrivial. In
    response to this, many visual analytics systems have been proposed to help model
    developers build and debug their models, with the hope of expediting the iterative
    experimentation process to ultimately improve performance [[15](#bib.bib15), [52](#bib.bib52),
    [47](#bib.bib47)]. Oftentimes this requires monitoring models during the training
    phase [[58](#bib.bib58), [42](#bib.bib42)], identifying misclassified instances
    and testing a handful of well-known data instances to observe performance [[39](#bib.bib39),
    [29](#bib.bib29), [50](#bib.bib50)], and allowing a system to suggest potential
    directions for the model developer to explore [[34](#bib.bib34)]. This reason
    for why we wish to visualize deep learning ultimately provides better tools to
    speed up model development for engineers and researchers so that they can quickly
    identify and fix problems within a model to improve overall performance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Comparing & Selecting Models
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While certainly related to model debugging and improvement, model comparison
    and selection are slightly different tasks in which visualization can be useful [[74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76)]. Oftentimes model comparison describes the
    notion of choosing a single model among an ensemble of well-performing models.
    That is, no debugging needs to be done; all models have “learned” or have been
    trained semi-successfully. Therefore, the act of selecting a single, best-performing
    model requires inspecting model metrics and visualizing parts of the model to
    pick the one that has the highest accuracy, the lowest loss, or is the most generalizable,
    while avoiding pitfalls such as memorizing training data or overfitting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Some systems take a high-level approach and compare user-defined model metrics,
    like accuracy and loss, and aggregate them on interactive charts for performance
    comparison [[27](#bib.bib27)]. Other frameworks compare neural networks trained
    on different random initializations (an important step in model design) to discover
    how they would affect performance, while also quantifying performance and interpretation [[28](#bib.bib28)].
    Some approaches compare models on image generation techniques, such as performing
    image reconstruction from the internal representations of each layer of different
    networks to compare different network architectures [[77](#bib.bib77)]. Similar
    to comparing model architectures, some systems solely rely on data visualization
    representations and encodings to compare models [[43](#bib.bib43)], while others
    compare different snapshots of a single model as it trains over time, i.e., comparing
    a model after $n_{1}$ epochs and the same model after $n_{2}$ epochs of training
    time [[57](#bib.bib57)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Teaching Deep Learning Concepts
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apart from AI experts, another important reason why we may wish to visualize
    deep learning is to educate non-expert users about AI. The exact definition of
    non-experts varies by source and is discussed further in Section [5.3](#S5.SS3
    "5.3 Non-experts ‣ 5 Who Uses Deep Learning Visualization ‣ Visual Analytics in
    Deep Learning: An Interrogative Survey for the Next Frontiers"). An example that
    targets the general public is Teachable Machines [[54](#bib.bib54)], a web-based
    AI experiment that explores and teaches the foundations of an image classifier.
    Users train a three-way image classifier by using their computer’s webcam to generate
    the training data. After providing three different examples of physical objects
    around the user (e.g., holding up a pencil, a coffee mug, and a phone), the system
    then performs real-time inference on whichever object is in view of the webcam,
    and shows a bar chart with the corresponding classification scores. Since inference
    is computed in real-time, the bar charts wiggles and jumps back and forth as the
    user removes an object, say the pencil, from the view and instead holds up the
    coffee mug. The visualization used is a simple bar chart, which provides an approachable
    introduction into image classification, a modern-day computer vision and AI problem.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Another example for teaching deep learning concepts, the Deep Visualization
    Toolbox [[55](#bib.bib55)] discussed later in this survey, also uses a webcam
    for instant feedback when interacting with a neural network. Taking instantaneous
    feedback a step further, some works have used direct manipulation to engage non-experts
    in the learning process. TensorFlow Playground [[16](#bib.bib16)], a robust, web-based
    visual analytics tool for exploring simple neural networks, uses direct manipulation
    to reinforce deep learning concepts, and importantly, evokes the user’s intuition
    about how neural networks work. Other non-traditional mediums have been used to
    teach deep learning concepts and build an intuition for how neural networks behave
    too. Longform, interactive scrollytelling works focusing on particular AI topics
    that use interactive visualizations as supporting evidence are gaining popularity.
    Examples include “How to Use t-SNE Effectively,” where users can play with hundreds
    of small datasets and vary single parameters to observe their effect on an embedding
     [[78](#bib.bib78)], and a similar interactive article titled “Visualizing MNIST”
    that visualizes different types of embeddings produced by different algorithms [[45](#bib.bib45)].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 5 Who Uses Deep Learning Visualization
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section describes the groups of people who may stand to benefit from deep
    learning visualization and visual analytics. We loosely organize them into three
    non-mutually exclusive groups by their level of deep learning knowledge (most
    to least): model developers, model users, and non-experts. Note that many of the
    works discussed can benefit multiple groups, e.g., a model developer may use a
    tool aimed at non-experts to reinforce their own intuition for how neural networks
    learn.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Model Developers & Builders
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first group of people who use deep learning visualization are individuals
    whose job is primarily focused on developing, experimenting with, and deploying
    deep neural networks. These model developers and builders, whether they are researchers
    or engineers, have a strong understanding of deep learning techniques and a well-developed
    intuition surrounding model building. Their knowledge expedites key decisions
    in deep learning workflows, such as identifying the which types of models perform
    best on which types of data. These individuals wield mastery over models, e.g.,
    knowing how to vary hyperparameters in the right fashion to achieve better performance.
    These individuals are typically seasoned in building large-scale models and training
    them on high-performance machines to solve real-world problems [[24](#bib.bib24)].
    Therefore, tooling and research for these users is much more technically focused,
    e.g., exposing many hyperparameters for detailed model control.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the existing deep learning visual analytics tools published, a handful tackle
    the problem of developing tools for model developers, but few have seen widespread
    adoption. Arguably the most well-known system is TensorBoard [[27](#bib.bib27)]:
    Google’s included open-source visualization platform for its dataflow graph library
    TensorFlow. TensorBoard includes a number of built-in components to help model
    developers understand, debug, and optimize TensorFlow programs. It includes real-time
    plotting of quantitative model metrics during training, instance-level predictions,
    and a visualization of the computational graph. The computational graph component
    was published separately by Wongsuphasawat et al. [[15](#bib.bib15)] and works
    by applying a series of graph transformations that enable standard layout techniques
    to produce interactive diagrams of TensorFlow models.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Other tools, such as DeepEyes [[47](#bib.bib47)], assist in a number of model
    building tasks, e.g., identifying stable layers during the training process, identifying
    unnecessary layers and degenerated filters that do not contribute to a model’s
    decisions, pruning such entities, and identifying patterns undetected by the network,
    indicating that more filters or layers may be needed. Another tool, Blocks [[29](#bib.bib29)],
    allows a model builder to accelerate model convergence and alleviate overfitting,
    through visualizing class-level confusion patterns. Other research has developed
    new metrics beyond measures like loss and accuracy, to help developers inspect
    and evaluate networks while training them [[58](#bib.bib58)].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Some tools also address the inherent iterative nature of training neural networks.
    For example, ML-o-scope [[31](#bib.bib31)] utilizes a time-lapse engine to inspect
    a model’s training dynamics to better tune hyperparameters, while work by Chae
    et al. [[34](#bib.bib34)] visualizes classification results during training and
    suggests potential directions to improve performance in the model building pipeline.
    Lastly, visual analytics tools are beginning to be built for expert users who
    wish to use models that are more challenging to work with. For example, DGMTracker [[42](#bib.bib42)]
    is a visual analytics tool built to help users understand and diagnose the training
    process of deep generative models: powerful networks that perform unsupervised
    and semi-supervised learning where the primary focus is to discover the hidden
    structure of data without resorting to external labels.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49c0a4f817d1624fb9ccb7a75a4aa2bc.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: ActiVis [[39](#bib.bib39)]: a visual analytics system for interpreting
    neural network results using a novel visualization that unifies instance- and
    subset-level inspections of neuron activations deployed at Facebook.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Model Users
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second group of people who may benefit from deep learning visualization
    are model users. These are users who may have some technical background but are
    neural network novices. Common tasks include using well-known neural network architectures
    for developing domain specific applications, training smaller-scale models, and
    downloading pre-trained model weights online to use as a starting point. This
    group of users also include machine learning artists who use models to enable
    and showcase new forms of artistic expression.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'An example visual analytics system for these model users is ActiVis [[39](#bib.bib39)]:
    a visual analytics system for interpreting the results of neural networks by using
    a novel visual representation that unifies instance- and subset-level inspections
    of neuron activations. Model users can flexibly specify subsets using input features,
    labels, or any intermediate outcomes in a machine learning pipeline. ActiVis was
    built for engineers and data scientists at Facebook to explore and interpret deep
    learning models results and is deployed on Facebook’s internal system. LSTMVis [[52](#bib.bib52)]
    is a visual analysis tool for recurrent neural networks with a focus on understanding
    hidden state dynamics in sequence modeling. The tool allows model users to perform
    hypothesis testing by selecting an input range to focus on local state changes,
    then to match these states changes to similar patterns in a large dataset, and
    finally align the results with structural annotations. The LSTMVis work describes
    three types of users: architects, those who wish to develop new deep learning
    methodologies; trainers, those who wish to apply LSTMs to a task in which they
    are domain experts in; and end users, those who use pretrained models for various
    tasks. Lastly, Embedding Projector [[51](#bib.bib51)], while not specifically
    deep learning exclusive, is a visual analytics tool to support interactive visualization
    and interpretation of large-scale embeddings, which are common outputs from neural
    network models. The work presents three important tasks that model users often
    perform while using embeddings; these include exploring local neighborhoods, viewing
    the global geometry to find clusters, and finding meaningful directions within
    an embedding.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Non-experts
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third group of people whom visualization could aid are non-experts in deep
    learning. These are individuals who typically have no prior knowledge about deep
    learning, and may or may not have a technical background. Much of the research
    targeted at this group is for educational purposes, trying to explain what a neural
    network is and how it works at a high-level, sometimes without revealing deep
    learning is present. These group also includes people who simply use AI-powered
    devices and consumer applications.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from Teachable Machines [[54](#bib.bib54)] and the Deep Visualization
    Toolbox [[55](#bib.bib55)] mentioned in Section [4.4](#S4.SS4 "4.4 Teaching Deep
    Learning Concepts ‣ 4 Why Visualize Deep Learning ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers"), TensorFlow Playground [[16](#bib.bib16)],
    a web-based interactive visualization of a simple dense network, has become a
    go-to tool for gaining intuition about how neural networks learn. TensorFlow Playground
    uses direct manipulation experimentation rather than coding, enabling users to
    quickly build an intuition about neural networks. The system has been used to
    teach students about foundational neural network properties by using “living lessons,”
    and also makes it straightforward to create a dynamic, interactive educational
    experience. Another web-browser based system, ShapeShop [[38](#bib.bib38)], allows
    users to explore and understand the relationship between input data and a network’s
    learned representations. ShapeShop uses a feature visualization technique called
    class activation maximization to visualize specific classes of an image classifier.
    The system allows users to interactively select classes from a collection of simple
    shapes, select a few hyperparameters, train a model, and view the generated visualizations
    all in real-time.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Tools built for non-experts, particularly with an educational focus, are becoming
    more popular on the web. A number of web-based JavaScript frameworks for training
    neural networks and inference have been developed; however, ConvNetJS ([http://cs.stanford.edu/people/karpathy/convnetjs/](http://cs.stanford.edu/people/karpathy/convnetjs/))
    and TensorFlow.js ([https://js.tensorflow.org/](https://js.tensorflow.org/)) are
    the most used and have enabled developers to create highly interactive explorable
    explanations for deep learning models.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 6 What to Visualize in Deep Learning
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section discusses the technical components of neural networks that could
    be visualized. This section is strongly related to the next section, Section [7](#S7
    "7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers") “How,” which describes how the components of these
    networks are visualized in existing work. By first describing what may be visualized
    (this section), we can more easily ground our discussion on how to visualize them
    (next section).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Computational Graph & Network Architecture
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first thing that can be visualized in a deep learning model is the model
    architecture. This includes the computational graph that defines how a neural
    network model would train, test, save data to disk, and checkpoint after epoch
    iterations [[27](#bib.bib27)]. Also called the dataflow graph [[27](#bib.bib27)],
    this defines how data flows from operation to operation to successfully train
    and use a model. This is different than the neural network’s edges and weights,
    discussed next, which are the parameters to be tweaked during training. The dataflow
    graph can be visualized to potentially inform model developers of the types of
    computations occurring within their model, as discussed in Section [7.1](#S7.SS1
    "7.1 Node-link Diagrams for Network Architectures ‣ 7 How to Visualize Deep Learning
    ‣ Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers").'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Learned Model Parameters
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Other components that can be visualized are the learned parameters in the network
    during and after training.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Neural Network Edge Weights
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Neural network models are built of many, and sometimes diverse, constructions
    of layers of computational units [[26](#bib.bib26)]. These layers send information
    throughout the network by using edges that connect layers to one another, oftentimes
    in a linear manner, yet some more recent architectures have shown that skipping
    certain layers and combining information in unique ways can lead to better performance.
    Regardless, each node has an outgoing edge with an accompanying weight that sends
    signal from one neuron in a layer to potentially thousands of neurons in an adjacent
    layer [[16](#bib.bib16)]. These are the parameters that are tweaked during the
    backpropagation phase of training a deep model, and could be worthwhile to visualize
    for understanding what the model has learned, as seen in Section [7.1](#S7.SS1
    "7.1 Node-link Diagrams for Network Architectures ‣ 7 How to Visualize Deep Learning
    ‣ Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers").'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Convolutional Filters
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Convolutional neural networks are built using a particular type of layer, aptly
    called the convolutional layer. These convolutional layers apply filters over
    the input data, oftentimes images represented as a two-dimensional matrix of values,
    to generate smaller representations of the data to pass to later layers in the
    network. These filters, like the previously mentioned traditional weights, are
    then updated throughout the training process, i.e., learned by the network, to
    support a given task. Therefore, visualizing the learned filters could be useful
    as an alternate explanation for what a model has learned [[10](#bib.bib10), [55](#bib.bib55)],
    as seen in Section [7.6](#S7.SS6 "7.6 Algorithms for Attribution & Feature Visualization
    ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers").'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Individual Computational Units
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Albeit reductionist, neural networks can be thought as a collection of layers
    of neurons connected by edge weights. Above, we discussed that the edges can be
    visualized, but the neurons too can be a source of data to investigate.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Activations
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When given a trained model, one can perform inference on the model using a
    new data instance to obtain the neural network’s output, e.g., a classification
    or a specific predicted value. Throughout the network, the neurons compute activations
    using activation functions (e.g., weighted sum) to combine the signal from the
    previous layer into a new node [[26](#bib.bib26), [55](#bib.bib55)]. This mapping
    is one of the characteristics that allows a neural network to learn. During inference,
    we can recover the activations produced at each layer. We can use activations
    in multiple ways, e.g., as a collection of individual neurons, spatial positions,
    or channels [[46](#bib.bib46)]. Although these feature representations are typically
    high-dimensional vectors of the input data at a certain stage within the network [[46](#bib.bib46)],
    it could be valuable in helping people visualize how input data is transformed
    into higher-level features, as seen in Section [7.2](#S7.SS2 "7.2 Dimensionality
    Reduction & Scatter Plots ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers"). Feature representations
    may also shed light upon how the network and its components respond to particular
    data instances [[55](#bib.bib55)], commonly called instance-level observation;
    we will discuss this in detail in Section [7.4](#S7.SS4 "7.4 Instance-based Analysis
    & Exploration ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers") and [7.5](#S7.SS5 "7.5 Interactive
    Experimentation ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep
    Learning: An Interrogative Survey for the Next Frontiers").'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Gradients for Error Measurement
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To train a neural network, we commonly use a process known as backpropagation [[26](#bib.bib26)].
    Backpropagation, or sometimes called the backpropagation of errors, is a method
    to calculate the gradient of a specified loss function. When used in combination
    with an optimization algorithm, e.g., gradient descent, we can compute the error
    at the output layer of a neural network and redistribute the error by updating
    the model weights using the computed gradient. These gradients flow over the same
    edges defined in the network that contain the weights, but flow in the opposite
    direction., e.g., from the output layer to the input layer. Therefore, it could
    be useful to visualize the gradients of a network to see how much error is produced
    at certain outputs and where it is distributed [[35](#bib.bib35), [33](#bib.bib33)],
    as mentioned in Section [7.6](#S7.SS6 "7.6 Algorithms for Attribution & Feature
    Visualization ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers").'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Neurons in High-dimensional Space
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Continuing the discussion of visualizing activations of a data instance, we
    can think of the feature vectors recovered as vectors in a high-dimensional space.
    Each neuron in a layer then becomes a “dimension.” This shift in perspective is
    powerful, since we can now take advantage of high-dimensional visualization techniques
    to visualize extracted activations [[48](#bib.bib48), [79](#bib.bib79)]. Sometimes,
    people use neural networks simply as feature vector generators, and defer the
    actual task to other computational techniques, e.g., traditional machine learning
    models [[49](#bib.bib49), [4](#bib.bib4)]. In this perspective, we now can think
    of deep neural networks as feature generators, whose output embeddings could be
    worth exploring. A common technique is to use dimensionality reduction to take
    the space spanned by the activations and embed it into 2D or 3D for visualization
    purposes [[79](#bib.bib79), [51](#bib.bib51), [48](#bib.bib48)], as discussed
    in Section [7.2](#S7.SS2 "7.2 Dimensionality Reduction & Scatter Plots ‣ 7 How
    to Visualize Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative
    Survey for the Next Frontiers").'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Aggregated Information
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.5.1 Groups of Instances
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned earlier, instance-level activations allow one to recover the mapping
    from data input to a feature vector output. While this can be done for a single
    data instance, it can also be done on collections of instances. While at first
    this does not seem like a major differentiation from before, instance groups provide
    some unique advantages [[39](#bib.bib39), [43](#bib.bib43)]. For example, since
    instance groups by definition are composed of many instances, one can compute
    all the activations simultaneously. Using visualization, we can now compare these
    individual activations to see how similar or different they are from one another.
    Taking this further, with instance groups, we can now take multiple groups, potentially
    from differing classes, and compare how the distribution of activations from one
    group compares or differs from another. This aggregation of known instances into
    higher-level groups could be useful for uncovering the learned decision boundary
    in classification tasks, as seen in Section [7.2](#S7.SS2 "7.2 Dimensionality
    Reduction & Scatter Plots ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers") and Section
    [7.4](#S7.SS4 "7.4 Instance-based Analysis & Exploration ‣ 7 How to Visualize
    Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers").'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Model Metrics
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While instance- and group-level activations could be useful for investigating
    how neural networks respond to particular results a-priori, they suffer from scalability
    issues, since deep learning models typically wrangle large datasets. An alternative
    object to visualize are model metrics, including loss, accuracy, and other measures
    of error [[27](#bib.bib27)]. These summary statistics are typically computed every
    epoch and represented as a time series over the course of a model’s training phase.
    Representing the state of a model through a single number, or handful of numbers,
    abstracts away much of the subtle and interesting features of deep neural networks;
    however, these metrics are key indicators for communicating how the network is
    progressing during the training phase [[47](#bib.bib47)]. For example, is the
    network “learning” anything at all or is it learning “too much” and is simply
    memorizing data causing it to overfit? Not only do these metrics describe notions
    of a single model’s performance over time, but in the case of model comparison,
    these metrics become more important, as they can provide a quick and easy way
    to compare multiple models at once. For this reason, visualizing model metrics
    can be an important and powerful tool to consider for visual analytics, as discussed
    in Section [7.3](#S7.SS3 "7.3 Line Charts for Temporal Metrics ‣ 7 How to Visualize
    Deep Learning ‣ Visual Analytics in Deep Learning: An Interrogative Survey for
    the Next Frontiers").'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 7 How to Visualize Deep Learning
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we described what technical components of neural networks
    could be visualized. In this section, we summarize how the components are visualized
    and interacted with in existing literature. For most neural network components,
    they are often visualized using a few common approaches. For example, network
    architectures are often represented as node-link diagrams; embeddings of many
    activations are typically represented as scatter plots; and model metrics over
    epoch time are almost always represented as line charts. In this section, we will
    also discuss other representations, going beyond the typical approaches.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Node-link Diagrams for Network Architectures
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a neural network’s dataflow graph or model architecture, the most common
    way to visualize where data flows and the magnitude of edge weights is a node-link
    diagram. Neurons are shown as nodes, and edge weights as links. For computational
    and dataflow graphs, Kahng et al. [[39](#bib.bib39)] describe two methods for
    creating node-link diagrams. The first represents only operations as nodes, while
    the second represents both operations and data as nodes. The first way is becoming
    the standard due to the popularity of TensorBoard [[27](#bib.bib27)] and the inclusion
    of its interactive dataflow graph visualization [[15](#bib.bib15)]. However, displaying
    large numbers of links from complex models can generate “hairball” visualizations
    where many edge crossings impede pattern discovery. To address this problem, Wongsuphasawat
    et al. [[15](#bib.bib15)] extracts high-degree nodes (responsible for many of
    the edge crossings), visualizes them separately from the main graph, and allow
    users to define super-groups within the code. Another approach to reduce clutter
    is to place more information on each node; DGMTracker [[42](#bib.bib42)] provides
    a quick snapshot of the dataflow in and out of a node by visualizing its activations
    within each node.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Regarding neural network architecture, many visual analytics systems use node-link
    diagrams (neurons as nodes, weights as links) [[13](#bib.bib13), [16](#bib.bib16),
    [37](#bib.bib37), [14](#bib.bib14), [35](#bib.bib35)]. The weight magnitude and
    sign can then be encoded using color or link thickness. This technique was one
    of the the first to be proposed [[13](#bib.bib13)], and the trend has continued
    on in literature. Building on this technique, Harley [[37](#bib.bib37)] visualizes
    the convolution windows on each layer and how the activations propagate through
    the network to make the final classification. Similar to the dataflow graph examples
    above, some works include richer information inside each node besides an activation
    value, such as showing a list of images that highly activate that neuron or the
    activations at a neuron as a matrix [[14](#bib.bib14)]. As mentioned in the dataflow
    graph visualizations, node-link diagrams for network architecture work well for
    smaller networks [[16](#bib.bib16)], but they also suffer from scalabilty issues.
    CNNVis [[14](#bib.bib14)], a visual analytics system that visualizes convolutional
    neural networks, proposes to use a bi-clustering-based edge bundling technique
    to reduce visual clutter caused by too many links.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b743ac88f81155230e699f5c55beae77.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Each point is a data instance’s high-dimensional activations at a
    particular layer inside of a neural network, dimensionally reduced, and plotted
    in 2D. Notice as the data flows through the network the activation patterns become
    more discernible (left to right) [[39](#bib.bib39)].'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Dimensionality Reduction & Scatter Plots
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [6](#S6 "6 What to Visualize in Deep Learning ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers"), “What,” we
    discussed different types of high-dimensional embeddings: text can be represented
    as vectors in word embeddings for natural language processing and images can be
    represented as feature vectors inside of a neural network. Both of these types
    of embeddings are mathematically represented as large tensors, or sometimes as
    2D matrices, where each row may correspond to an instance and each column a feature.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The most common technique to visualize these embeddings is performing dimensionality
    reduction to reduce the number of columns (e.g., features) to two or three. Projecting
    onto two dimensions would mean computing $(x,y)$ coordinates for every data instance;
    for three dimensions, we compute an additional $z$ component, resulting in $(x,y,z)$.
    In the 2D case, we can plot all data instances as points in a scatter plot where
    the axes may or may not have interpretable meaning, depending on the reduction
    technique used, e.g., principal component analysis (PCA) or t-distributed stochastic
    neighbor embedding (t-SNE) [[79](#bib.bib79)]. In the 3D case, we can still plot
    each data instance as a point in 3D space and use interactions to pan, rotate,
    and navigate this space [[51](#bib.bib51)]. These types of embeddings are often
    included in visual analytics systems as one of the main views [[47](#bib.bib47),
    [35](#bib.bib35)], and are also used in application papers as static figures [[65](#bib.bib65),
    [56](#bib.bib56)]. However, viewing a 3D space on a 2D medium (e.g., computer
    screen) may not be ideal for tasks like comparing exact distances.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Since each reduced point corresponds to an original data instance, another common
    approach is to retrieve the original image and place it at the reduced coordinate
    location. Although the image size must be greatly reduced to prevent excessive
    overlap, viewing all the images at once can provide insight into what a deep learning
    model has learned, as seen in the example in [[77](#bib.bib77)] where the authors
    visualize ImageNet test data, or in [[80](#bib.bib80)] where the authors create
    many synthetic images from a single class and compare the variance across many
    random initial starting seeds for the generation algorithm. We have discussed
    the typical case where each dot in the scatter plot is a data instance, but some
    work has also visualized neurons in a layer as separate data instances [[58](#bib.bib58)].
    Another work studies closely how data instances are transformed as their information
    is passed through the deep network, which in effect visualizes how the neural
    network separates various classes along approximated decision boundaries [[48](#bib.bib48)].
    It is also possible to use time-dependent data and visualize how an embedding
    changes over time, or in the case of deep learning, over epochs [[81](#bib.bib81)].
    This can be useful for evaluating the quality of the embedding during the training
    phase.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: However, these scatter plots raise problems too. The quality of the embeddings
    greatly depends on the algorithm used to perform the reduction. Some works have
    studied how PCA and t-SNE differ, mathematical and visually, and suggest new reduction
    techniques to capture the semantic and syntactic qualities within word embeddings [[82](#bib.bib82)].
    It has also been shown that popular reduction techniques like t-SNE are sensitive
    to changes in the hyperparameter space. Wattenberg meticulously explores the hyperparameter
    space for t-SNE, and offers lessons learned and practical advice for those who
    wish to use dimensionality reduction methods [[78](#bib.bib78)]. While these techniques
    are commonplace, there are still iterative improvements that can be done using
    clever interaction design, such as finding instances similar to a target instance,
    i.e., those “near” the target in the projected space, helping people build intuition
    for how data is spatially arranged [[51](#bib.bib51)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Line Charts for Temporal Metrics
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model developers track the progression of their deep learning models by monitoring
    and observing a number of different metrics computed after each epoch, including
    the loss, accuracy, and different measure of errors. This can be useful for diagnosing
    the long training process of deep learning models., The most common visualization
    technique for visualizing this data is by considering the metrics as time series
    and plotting them in line charts [[27](#bib.bib27)]. This approach is widely used
    in deep learning visual analytics tools [[47](#bib.bib47), [35](#bib.bib35)].
    After each epoch, a new entry in the time series is computed, therefore some tools,
    like TensorBoard, run alongside models as they train and update with the latest
    status [[27](#bib.bib27)]. TensorBoard focuses much of its screen real-estate
    to these types of charts and supports interactions for plotting multiple metrics
    in small multiples, plotting multiple models on top of one another, filtering
    different models, providing tooltips for the exact metric values, and resizing
    charts for closer inspection. This technique appears in many visual analytics
    systems and has become a staple for model training, comparison, and selection.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Instance-based Analysis & Exploration
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another technique to help interpret and debug deep learning models is testing
    specific data instances to understand how they progress throughout a model. Many
    experts have built up their own collection of data instances over time, having
    developed deep knowledge about their expected behaviors in models while also knowing
    their ground truth labels [[39](#bib.bib39), [19](#bib.bib19)]. For example, an
    instance consisting of a single image or a single text phrase is much easier to
    understand than an entire image dataset or word embedding consisting of thousands
    of numerical features extracted from an end user’s data. This is called instance-level
    observation, where intensive analysis and scrutiny is placed on a single data
    instance’s transformation process throughout the network, and ultimately its final
    output.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Identifying & Analyzing Misclassified Instances
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One application of instance-level analysis is using instances as unit tests
    for deep learning models. In the best case scenario, all the familiar instances
    are classified or predicted correctly; however, it is important to understand
    when a specific instance can fail and how it fails. For example, in the task of
    predicting population from satellite imagery, the authors showcase three maps
    of areas with high errors by using a translucent heatmap overlaid on the satellite
    imagery [[49](#bib.bib49)]. Inspecting these instances reveals three geographic
    areas that contain high amounts of man-made features and signs of activity but
    have no registered people living in them: an army base, a national lab, and Walt
    Disney World. The visualization helps demonstrate that the proposed model is indeed
    learning high-level features about the input data. Another technique, HOGgles [[83](#bib.bib83)],
    uses an algorithm to visualize feature spaces by using object detectors while
    inverting visual features back to natural images. The authors find that when visualizing
    the features of misclassified images, although the classification is wrong in
    the image space, they look deceptively similar to the true positives in the feature
    space. Therefore, by visualizing feature spaces of misclassified instances, we
    can gain a more intuitive understanding of recognition systems.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: For textual data, a popular technique for analyzing particular data instances
    is to use color as the primary encoding. For example, the background of particular
    characters in a phrase of words in a sentence would be colored using a divergent
    color scheme according to some criteria, often their activation magnitudes [[40](#bib.bib40),
    [32](#bib.bib32), [36](#bib.bib36)]. This helps identify particular data instances
    that may warrant deeper inspection (e.g., those misclassified) [[19](#bib.bib19)].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: When pre-defined data instances are not unavailable (e.g., when analyzing a
    new dataset), how can we guide users towards important and interesting instances?
    To address this problem, a visual analytics system called Blocks [[29](#bib.bib29)]
    uses confusion matrices, a technique for summarizing the performance of a classification
    algorithm, and matrix-level sorting interactions to reveal that class error often
    occurs in hierarchies. Blocks incorporates these techniques with a sample viewer
    in the user interface to show selected samples potentially worth exploring.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Analyzing Groups of Instances
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of using individual data instances for testing and debugging a model,
    it is also common for experts to perform similar similar tasks using groups of
    instances [[19](#bib.bib19)]. While some detail may be lost when performing group-level
    analysis it allows experts to further test the model by evaluating its average
    and aggregate performance across different groups.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Much of the work using this technique is done on text data using LSTM models [[52](#bib.bib52)].
    Some approaches compute the saliency for groups of words across the model and
    visualize the values as a matrix [[41](#bib.bib41)], while others use matrix visualizations
    to show the activations of word groups when represented as feature vectors in
    word embeddings [[50](#bib.bib50), [84](#bib.bib84)]. One system, ActiVis [[39](#bib.bib39)],
    places instance group analysis at the focus of its interactive interface, allowing
    users to compare preset and user-defined groups of activations. Similar to the
    matrix visualization that summarizes activations for each class in CNNVis [[14](#bib.bib14)],
    ActiVis also uses a scrolling matrix visualization to unify both instance-level
    and group-level analysis into a single view where users can compare the activations
    of the user-defined instances.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: However, sometimes it can be challenging to define groups for images or text.
    For textual data, people often use words to group documents and provide aggregated
    data. ConceptVector [[85](#bib.bib85)] addresses the instance group generation
    problem by providing an interactive interface to create interesting groups of
    concepts for model testing. Furthermore, this system also suggests additional
    words to include in the user-defined groups, helping guide the user to create
    semantically sound concepts.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Interactive Experimentation
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactive experimentation, another interesting area that integrates deep learning
    visualization, makes heavy use of interactions for users to experiment with models [[86](#bib.bib86)].
    By using direct manipulation for testing models, a user can pose “what if?” questions
    and observe how the input data affects the results. Called explorable explanations [[87](#bib.bib87)],
    this type of visual experimentation is popular for making sense of complex concepts
    and systems.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 Models Responding to User-provided Input Data
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To engage the user with the desired concepts to be taught, many systems require
    the user to provide some kind of input data into the system to obtain results.
    Some visual analytics systems use a webcam to capture live videos, and visualize
    how the internals of neural network models respond to these dynamic inputs [[55](#bib.bib55)].
    Another example is a 3D visualization of a CNN trained on the classic MNIST dataset
    ¹¹1MNIST is a small, popular dataset consisting of thousands of 28$\times$28px
    images of handwritten digits (0 to 9). MNIST is commonly used as a benchmark for
    image classification models. that shows the convolution windows and activations
    on images that the user draws by hand [[37](#bib.bib37)]. For example, drawing
    a “5” in the designated area passes that example throughout the network and populates
    the visualization with the corresponding activations using a node-link diagram.
    A final example using image data is ShapeShop [[38](#bib.bib38)], a system that
    allows a user to select data from a bank of simple shapes to be classified. The
    system then trains a neural network and using the class activation maximization
    technique to generate visualizations of the learned features of the model. This
    can be done in real-time, therefore a user can quickly train multiple models with
    different shapes to observe the effect of adding more diverse data to improve
    the internal model representation.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79a0734619162fdb0c2d5643df2127e8.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: TensorFlow Playground [[16](#bib.bib16)]: a web-based visual analytics
    tool for exploring simple neural networks that uses direct manipulation rather
    than programming to teach deep learning concepts and develop an intuition about
    how neural networks behave.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: An example using textual data is the online, interactive Distill article for
    handwriting prediction [[32](#bib.bib32)], which allows a user to write words
    on screen, and in real-time, the system draws multiple to-be-drawn curves predicting
    what the user’s next stroke would be, while also visualizing the model’s activations.
    Another system uses GANs to interactively generate images based off of user’s
    sketches [[59](#bib.bib59)]. By sketching a few colored lines, the system presents
    the user with multiple synthetic images using the sketch as a guideline for what
    to generate. A final example is the Adversarial Playground [[44](#bib.bib44)],
    a visual analytics system that enables users to compare adversarially-perturbed
    images, to help users understand why an adversarial example can fool a CNN image
    classifier. The user can select from one of the MNIST digits and adjust the strength
    of adversarial attack. The system then compares the classifications scores in
    a bar chart to observe how simple perturbations can greatly impact classification
    accuracy.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2 How Hyperparameters Affect Results
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While deep learning models automatically adjust their internal parameters, their
    hyperparameters still require fine-tuning. These hyperparameters can have major
    impact on model performance and robustness. Some visual analytics systems expose
    model hyperparameters to the user for interactive experimentation. One example
    previously mentioned is TensorFlow Playground [[16](#bib.bib16)], where users
    can use direct manipulation to adjust the architecture of a simple, fully-connected
    neural network, as well as the hyperparameters associated with its training, such
    as the learning rate, activation function, and regularization. Another example
    is a Distill article that meticulously explores the hyperparaemters of the t-SNE
    dimensionality reduction method [[78](#bib.bib78)]. This article tests dozens
    of synthetic datasets in different arrangements, while varying hyperparameters
    such as the t-SNE perplexity and the number of iterations to run the algorithm
    for.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Algorithms for Attribution & Feature Visualization
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final method for how to visualize deep learning hails from the AI and computer
    vision communities. These are algorithmic techniques that entail image generation.
    Given a trained a model, one can select a single image instance and use one of
    the algorithmic techniques to generate a new image of the same size that either
    highlights important regions of the image (often called attribution) or is an
    entirely new image that supposedly is representative of the same class (often
    called feature visualization) [[88](#bib.bib88), [46](#bib.bib46)]. In these works,
    it is common to see large, full-page figures consisting of hundreds of such images
    corresponding to multiple images classes [[89](#bib.bib89)]. However, it is uncommon
    to see interactivity in these works, as the primary contribution is often about
    algorithms, not interactive techniques or systems. Since the focus of this interrogative
    survey is on visual analytics in deep learning, we do not discuss in detail the
    various types of algorithmic techniques. Rather, we mention the most prominent
    techniques developed, since they are impactful to the growing field of deep learning
    visualization and could be incorporated into visual analytics systems in the future.
    For more details about these techniques, such as input modification, deconvolutional
    methods [[10](#bib.bib10)], and input reconstruction methods, we refer our readers
    to the taxonomies [[90](#bib.bib90)] and literature surveys for visualizing learned
    features in CNNs [[22](#bib.bib22), [91](#bib.bib91)], and a tutorial that presents
    the theory behind many of these interpretation techniques and discusses tricks
    and recommendations to efficiently use them on real data [[61](#bib.bib61)].
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.1 Heatmaps for Attribution, Attention, & Saliency
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One research area generates translucent heatmaps that overlay images to highlight
    important regions that contribute towards classification and their sensitivity [[4](#bib.bib4),
    [92](#bib.bib92), [93](#bib.bib93), [69](#bib.bib69), [94](#bib.bib94)]. One technique
    called visual backpropagation attempts to visualize which parts of an image have
    contributed to the classification, and can do so in real-time in a model debugging
    tool for self-driving vehicles [[30](#bib.bib30)]. Another technique is to invert
    representations, i.e., attempt to reconstruct an image using a feature vector
    to understand the what a CNN has learned [[95](#bib.bib95), [96](#bib.bib96),
    [77](#bib.bib77)]. Prediction difference analysis is a method that highlights
    features in an image to provide evidence for or against a certain class[[66](#bib.bib66)].
    Other work hearkens back to more traditional computer vision techniques by exploring
    how object detectors emerge in CNNs and attempts to give humans object detector
    vision capabilities to better align humans and deep learning vision for images [[97](#bib.bib97),
    [83](#bib.bib83)]. Visualizing CNN filters is also popular, and has famously shown
    to generate dream-like images, becoming popular in artistic tasks [[98](#bib.bib98),
    [99](#bib.bib99)] . Some work for interpreting visual question answering (VQA)
    models and tasks use these heatmaps to explain which parts of an image a VQA model
    is looking at in unison with text activation maps when answering the given textual
    questions [[36](#bib.bib36)]. However, recent work has shown that some of these
    methods fail to provide correct results and argue that we should develop explanation
    methods that work on simpler models before extending them to the more complex
    ones [[91](#bib.bib91)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.2 Feature Visualization
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For feature visualization, while some techniques have proven interesting [[100](#bib.bib100)],
    one of the most studied techniques, class activation maximization, maximizes the
    activation of a chosen, specific neuron using an optimization scheme, such as
    gradient ascent, and generates synthetic images that are representative of what
    the model has learned about the chosen class [[68](#bib.bib68)]. This led to a
    number of works improving the quality of the generated images. Some studies generated
    hundreds of these non-deterministic synthetic images and clustered them to see
    how variations in the class activation maximization algorithm affects the output
    image [[80](#bib.bib80)]. In some of their most recent work on this topic, Ngyuen
    et al. [[70](#bib.bib70)] present hundreds of high-quality images using a deep
    generator network to improve upon the state-of-the-art, and include figures comparing
    their technique to many of the existing and previous attempts to improve the quality
    of generated images. The techniques developed in this research area have improved
    dramatically over the past few years, where now it is possibly to synthetically
    generate photorealistic images [[101](#bib.bib101)]. A recent comparison of feature
    visualization techniques highlights their usefulness [[88](#bib.bib88)]; however,
    the authors note that they remain skeptical of their trustworthiness, e.g., do
    neurons have a consistent meaning across different inputs, and if so, is that
    meaning accurately reified by feature visualization [[46](#bib.bib46)]?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 8 When to Visualize in the Deep Learning Process
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section describes when visualizing deep learning may be most relevant
    and useful. Our discussion primarily centers around the training process: an iterative,
    foundational procedure for using deep learning models. We identify two distinct,
    non-mutually exclusive times for when to visualize: during training and after
    training. Some works propose that visualization be used both during and after
    training.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 During Training
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artificial neural networks learn higher-level features that are useful for class
    discrimination as training progress [[102](#bib.bib102)]. By using visualization
    during the training process, there is potential to monitor one’s model as it learns
    to closely observe and track the model’s performance [[48](#bib.bib48)].
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the systems in this category run in a separate web-browser alongside
    the training process, and interface with the underlying model to query the latest
    model status. This way, users can visually explore and rigorously monitor their
    models in real time, while they are trained elsewhere. The visualization systems
    dynamically update the charts with metrics recomputed after every epoch, e.g.,
    the loss, accuracy, and training time. Such metrics are important to model developers
    because they rely on them to determine if a model (1) has begun to learn anything
    at all; (2) is converging and reaching the peak of its performance; or (3) has
    potentially overfitted and memorized the training data. Therefore, many of the
    visual analytics systems used during training support and show these updating
    visualizations as a primary view in the interface [[27](#bib.bib27), [16](#bib.bib16),
    [35](#bib.bib35), [47](#bib.bib47), [42](#bib.bib42), [34](#bib.bib34)]. One system,
    Deep View [[58](#bib.bib58)], visualizes model metrics during the training process
    and uses its own defined metrics for monitoring (rather than the loss): a discriminability
    metric, which evaluates neuron evolution, and a density metric which evaluates
    the output feature maps. This way, for detecting overfitting, the user does not
    need to wait long to view to infer overfitting; they simply observe the neuron
    density early in training phase.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, some systems help reduce development time and save computational
    resources by visualizing metrics that indicate whether a model is successfully
    learning or not, allowing a user to stop the training process early [[16](#bib.bib16)].
    By using visualization during model training, users can save development time
    through model steering [[35](#bib.bib35)] and utilizing suggestions for model
    improvement [[34](#bib.bib34)]. Lastly, another model development time minimization
    focuses on diagnosing neurons and layers that are not training correctly or are
    misclassifying data instances. Examples include DeepEyes [[47](#bib.bib47)], a
    system that identifies stable and unstable layers and neurons so users may prune
    their models to speed up training; Blocks [[29](#bib.bib29)], a system that visualizes
    class confusion and reveals that confusion patterns follow a hierarchical structure
    over the classes which can then be exploited to design hierarchy-aware architectures;
    and DGMTracker [[42](#bib.bib42)], a system that proposes a credit assignment
    algorithm that indicates how other neurons contribute to the output of particular
    failing neurons.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 After Training
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While some works support neural network design during the iterative model building
    process, there are other works that focus their visualization efforts after a
    model has been trained. In other words, these works assume a trained model as
    input to the system or visualization technique. Note that many, if not most, of
    the previously mentioned algorithmic techniques developed in the AI fields, such
    as attribution and feature visualization, are performed after training. These
    techniques are discussed more in Section [7.6](#S7.SS6 "7.6 Algorithms for Attribution
    & Feature Visualization ‣ 7 How to Visualize Deep Learning ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers").'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The Embedding Projector [[51](#bib.bib51)] specializes in visualizing 2D and
    3D embeddings produced by trained neural networks. While users can visualize typical
    high-dimensional datasets in this tool, the Embedding Projector tailors the experience
    towards embeddings commonly used deep learning. Once a neural network model has
    been trained, one can compute the activations for a given test dataset and visualize
    the activations in the Embedding Projector to visualize and explore the space
    that the network has learned. Instead of generating an overview embedding, another
    previously discussed system, the Deep Visualization Toolbox [[55](#bib.bib55)],
    uses a trained model to visualize live activations in a large small-multiples
    view to understand of what types of filters a convolutional network has learned.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: More traditional visual analytics systems have also been developed to inspect
    a model after it has finished training. ActiVis [[39](#bib.bib39)], a visual analytics
    system for neural network interpretation deployed at Facebook reports that Facebook
    engineers and data scientists use visual analytics systems often in their normal
    workflow. Another system, RNNVis [[43](#bib.bib43)], visualizes and compares different
    RNN models for various natural language processing tasks. This system positions
    itself as a natural extension of TensorFlow; using multiple TensorFlow models
    as input, the system then analyzes the trained models to extract learned representations
    in hidden states, and further processes the evaluation results for visualization.
    Lastly, the LSTMVis [[52](#bib.bib52)] system, a visual analysis tool for RNN
    interpretability, separates model training from the visualization. This system
    takes a model as input that must be trained separately, and from the model, gathers
    the required information to produce the interactive visualizations to be rendered
    in a web-based front-end.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 9 Where is Deep Learning Visualization
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the last question of the interrogative survey, we divide up “Where” into
    two subsections: where deep learning visualization research has been applied,
    and where deep learning visualization research has been conducted, describing
    the new and hybrid community. This division provides a concise summary for practitioners
    who wish to investigate the usage of the described techniques for their own work,
    and provides new researchers with the main venues for this research area to investigate
    existing literature.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Application Domains & Models
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While many non-neural approaches are used for real-world applications, deep
    learning has successfully achieved state-of-the-art performance in several domains.
    Previously in Section [4.1.2](#S4.SS1.SSS2 "4.1.2 Interpretation as Qualitative
    Support for Model Evaluation in Various Application Domains ‣ 4.1 Interpretability
    & Explainability ‣ 4 Why Visualize Deep Learning ‣ Visual Analytics in Deep Learning:
    An Interrogative Survey for the Next Frontiers"), we presented works that apply
    neural networks to particular domains and use visualizations to lend qualitative
    support to their usual quantitative results to strengthen users’ trust in their
    models. These domains included neural machine translation [[65](#bib.bib65)],
    reinforcement learning [[56](#bib.bib56)], social good [[49](#bib.bib49)], autonomous
    vehicles [[30](#bib.bib30)], medical imaging diagnostics [[66](#bib.bib66)], and
    urban planning [[67](#bib.bib67)].'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Next we summarize the types of models that have been used in deep learning visualization.
    Much of the existing work has used image-based data and models, namely CNNs, to
    generate attribution and feature visualization explanations for what a model has
    learned from an image dataset. CNNs, while not exclusively used for images, have
    become popular in the computer vision community and are often used for image classification
    and interactive, image-based creative tasks [[59](#bib.bib59), [103](#bib.bib103)].
    Besides images, sequential data (e.g., text, time series data, and music) has
    also been studied. This research stems from the natural language processing community,
    where researchers typically favor RNNs for learning representations of large text
    corpora. These researchers make sense of large word embeddings by using interactive
    tools that support dimensionality reduction techniques to solve problems such
    as sequence-to-sequence conversion, translation, and audio recognition. Research
    combining both image and text data has also been done, such as image captioning
    and visual question answering [[104](#bib.bib104), [105](#bib.bib105)]. Harder
    still are new types of networks called generative adversarial networks, or GANs
    for short, that have produced remarkable results for data generation [[106](#bib.bib106)],
    e.g., producing real-looking yet fake images [[107](#bib.bib107)]. While GANs
    have only existed for a couple of years, they are now receiving significant research
    attention. To make sense of the learned features and distributions from GANs,
    two visual analytics systems, DGMTracker [[42](#bib.bib42)] and GANViz [[53](#bib.bib53)],
    focus on understanding the training dynamics of GANs to help model developers
    better train these complex models, often consisting of multiple dueling neural
    networks.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '9.2 A Vibrant Research Community: Hybrid, Apace, & Open-sourced'
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As seen from this survey, bringing together the visualization communities with
    the AI communities has led to the design and development of numerous tools and
    techniques for improving deep learning interpretability and democratization. This
    hybrid research area has seen accelerated attention and interest due to its widespread
    impact, as evidenced by the large number of works published in just a few years,
    as seen in Table [II](#S3.T2 "TABLE II ‣ 3 Common Terminology ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers"). A consequence
    of this rapid progress is that deep learning visualization research are being
    disseminated across multiple related venues. In academia, the premiere venues
    for deep learning visualization research consists of two main groups: the information
    visualization and visual analytics communities; and the artificial intelligence
    and deep learning communities. Furthermore, since this area is relatively new,
    it has seen more attention at multiple workshops at the previously mentioned academic
    conferences, as tabulated in Table [I](#S2.T1 "TABLE I ‣ 2.2 Survey Methodology
    & Summarization Process ‣ 2 Our Contributions & Method of Survey ‣ Visual Analytics
    in Deep Learning: An Interrogative Survey for the Next Frontiers").'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Another consequence of this rapidly developing area is that new work is immediately
    publicized and open-sourced, without waiting for it to be “officially” published
    at conferences, journals, etc. Many of these releases take the form of a preprint
    publication posted on arXiv, where a deep learning presence has thrived. Not only
    is it common for academic research labs and individuals to publish work on arXiv,
    but companies from industry are also publishing results, code, and tools. For
    example, the most popular libraries²²2Popular libraries include TensorFlow [[27](#bib.bib27)],
    Keras, Caffe, PyTorch, and Theano. for implementing neural networks are open-source
    and have consistent contributions for improving all areas of the codebase, e.g.,
    installation, computation, and deployment into specific programming languages’
    open-source environments.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Some works have a corresponding blog post on an industry research blog³³3High
    impact industry blogs include: Google Research Blog, OpenAI, Facebook Research
    Blog, the Apple Machine Learning Journal, NVIDIA Deep Learning AI, and Uber AI,
    which, while non-traditional, has large impact due to their prominent visibility
    and large readership. While posting preprints may have its downsides (e.g., little
    quality control) the communities have been promoting the good practices of open-sourcing
    developed code and including direct links within the preprints; both practices
    are now the norm. Although it may be overwhelming to digest the amount of new
    research published daily, having access to the work with its code could encourage
    reproducibility and allow the communities to progress faster. In summary, given
    the increasing interest in deep learning visualization research and its importance,
    we believe our communities will continue to thrive, and will positively impact
    many domains for years to come.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 10 Research Directions & Open Problems
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we present research directions and open problems for future research distilled
    from the surveyed works.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Furthering Interpretability
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsurprisingly, with the amount of attention and importance on interpretability
    and explainability in the deep learning visualization literature, the first area
    for future work is continuing to create new interpretable methods for deep learning
    models. For the information visualization and visual analytics communities, this
    could constitute creating new visual representations for the components in deep
    learning models, or developing new interaction techniques to reveal deeper insights
    about one’s model. For the AI communities, more insightful attribution and feature
    visualization techniques for trained models that are fast (computationally cheap)
    could be incorporated into visualization systems. Combining visual representations,
    helpful interactions, and state-of-the-art attribution and feature visualization
    techniques together into rich user interfaces could lead to major breakthroughs
    for understanding neural networks [[46](#bib.bib46)].
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2819753867197638784a43583ff8722c.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Distill: The Building Blocks of Interpretability [[46](#bib.bib46)]:
    an interactive user interface that combines feature visualization and attribution
    techniques to interpret neural networks.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 System & Visual Scalability
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout this survey, we have covered many visual analytics systems that facilitate
    interpretation and model understanding. However, some systems suffer from scalability
    problems. Visual scalability challenges arise when dealing with large data, e.g.,
    large number of hyperparameters and millions of parameters in deep neural networks.
    Some research has started to address this, by simplifying complex dataflow graphs
    and network weights for better model explanations [[15](#bib.bib15), [14](#bib.bib14),
    [52](#bib.bib52)]. But, regarding activations and embeddings, dimensionality reduction
    techniques have a limit to their usability when it comes to the number of points
    to visualize [[48](#bib.bib48)]. We think this is an important research direction,
    especially given that the information visualization communities have developed
    techniques to visualize large, high-dimensional data that could potentially be
    applicable to deep learning [[108](#bib.bib108)].
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Aside from visual scalability, some tools also suffer from system scalability.
    While some of these problems may be more engineering-centric, we think that for
    visual analytics systems to adopted, they need to handle state-of-the-art deep
    models without penalizing performance or increasing model development time. Furthermore,
    these systems (often web-based) will greatly benefit from fast computations, supporting
    real-time, rich user interactions [[50](#bib.bib50)]. This is especially important
    for visual systems that need to perform pre-computation before rendering visualizations
    to the screen.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '10.3 Design Studies for Evaluation: Utility & Usability'
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An important facet of visualization research is the evaluation of the utility
    and usefulness of the visual representation. Equally important is to evaluate
    the usability of deployed systems and their interactive visual analytics techniques.
    It is encouraging to see many of the visual analytics systems recognize this importance
    and report on design studies conducted with AI experts before building a tool
    to understand the users and their needs [[15](#bib.bib15), [39](#bib.bib39), [52](#bib.bib52),
    [14](#bib.bib14), [43](#bib.bib43), [19](#bib.bib19)]. It is common to see example
    use cases or illustrative usage scenarios that demonstrate the capabilities of
    the interactive systems. Some works go beyond these and conduct user studies to
    evaluate utility and usability [[31](#bib.bib31)]. In the AI communities, most
    works do not include user studies. For those that do, they greatly benefit from
    showing why their proposed methods are superior to the ones being tested against [[69](#bib.bib69),
    [109](#bib.bib109), [89](#bib.bib89), [110](#bib.bib110)]. Taking this idea to
    the quantifiable extreme, a related avenue of evaluating these techniques is the
    notion of quantifying interpretability, which has been recently studied [[28](#bib.bib28),
    [111](#bib.bib111)]. Other domains have recognized that interpretable deep learning
    research may require evaluation techniques for their interpretations, and argue
    that there is a large body of work from fields such as philosophy, cognitive science,
    and social psychology that could be utilized [[62](#bib.bib62), [112](#bib.bib112)].
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: When surveying the interfaces of deep learning visual analytics tools, many
    of them contain multiple-coordinated views with many visual representations. Displaying
    this much information at once can be overwhelming, and when interpretability is
    the primary focus, it is critical for these systems to have superior usability.
    Therefore, we think future works could further benefit from including more members
    of the human-computer interaction communities, including interface and user experience
    designers, that could help organize and prioritize interfaces using well-studied
    guidelines [[86](#bib.bib86)].
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 The Human Role in Interpretability
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 10.4.1 Human v. Machine Understanding of the World
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In deep learning interpretability work, researchers are developing methods to
    produce better explanations to “see through the black-box,” but unfortunately
    some of these methods produce visualizations that, while visually interesting
    and thought-provoking [[98](#bib.bib98)], are not fully understandable by their
    human viewers. That is an important facet of deep learning interpretability, namely,
    producing visualizations and interpretations that are human understandable [[88](#bib.bib88)].
    Some methods compare algorithmic results with an empirically derived human baseline;
    this enables comparison between machine and human generated responses to objects
    in the world, particularly in images [[113](#bib.bib113)]. Ultimately, researchers
    seek to understand the commonalities and differences between how humans and machines
    see and decompose the world [[114](#bib.bib114)]. Some tools that we have surveyed
    achieve this by using live-video to compare the input images and the neural network’s
    activations and filters in real time [[55](#bib.bib55)]. Other tools give users
    explicit control of an experiment by training multiple small models with only
    a few exposed hyperparameters, automatically generating visualizations to then
    see the effect that the input data has on the learned representation [[38](#bib.bib38)].
    These “what-if” tools and scenarios could potentially be extended to incorporate
    human feedback into the training or model steering process of neural network to
    better improve performance.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Human-AI Pairing
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Much of this survey is dedicated towards reviewing the state-of-the-art in
    visual analytics for deep learning, with a focus on interpretability. These works
    use visualization to explain, explore, and debug models in order to choose the
    best preforming model for a given task, often by placing a human in the loop.
    However, a slight twist on this idea hearkening back to the original envisioning
    of the computer has lead to the emergence of a new research area, one where tasks
    are not exclusively performed by humans or machines, but one where the two complement
    each other. This area, recently dubbed artificial intelligence augmentation describes
    the use of AI systems to help develop new methods for intelligence augmentation [[103](#bib.bib103)].
    Some related works we have covered already propose artificial intelligence augmentation
    ideas, such as a system that suggests potentially interesting directions to explore
    in a high-dimensional 3D embedding [[51](#bib.bib51)], predicting and showing
    where the next stroke of a word could be when handwriting text [[32](#bib.bib32)],
    automatically generating images based off of user-provided sketches [[59](#bib.bib59)],
    and dynamically changing and steering a neural network model while it trains [[35](#bib.bib35)].
    We believe this is a rich, under-explored area for future research: using well-designed
    interfaces for humans to interact with machine learning models, and for these
    machine learning models to augment creative human tasks.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Social Good & Bias Detection
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aspirational pairing of humans and machines is a long-term research endeavor.
    To quicken our pace, we must continue to democratize artificial intelligence via
    educational tools, perhaps by using direct manipulation as an invitation for people
    to engage with AI [[16](#bib.bib16), [46](#bib.bib46)], clear explanations for
    model decision making, and robust tooling and libraries for programming languages
    for people to develop such models [[27](#bib.bib27), [15](#bib.bib15)]. While
    doing this, we must also ensure that AI applications remain ethical, fair, safe,
    transparent, and are benefiting society [[63](#bib.bib63)].
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Another important consideration for future research is detecting bias. This
    has been identified as a major problem in deep learning [[115](#bib.bib115), [116](#bib.bib116)],
    and a number of researchers are using visualization to understand why a model
    may be biased [[117](#bib.bib117)]. One example that aims to detect data bias
    is Google’s Facets tool [[118](#bib.bib118)], a visual analytics system designed
    specifically to preview and visualize machine learning datasets before training.
    This allows one to inspect large datasets by exploring the different classes or
    data instances, to see if there are any high-level imbalances in the class or
    data distribution.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Other works have begun to explore if the mathematical algorithms themselves
    can be biased towards particular decisions. An example of this is an interactive
    article titled “Attacking discrimination with smarter machine learning” [[117](#bib.bib117)],
    which explores how one can can create both fair and unfair threshold classifiers
    in an example task such as loan granting scenarios where a bank may grant or deny
    a loan based on a single, automatically computed number such as a credit score.
    The article aims to highlight that equal opportunity [[119](#bib.bib119)] is not
    preserved by machine learning algorithms, and that as AI-powered systems continue
    to make important decisions across core social domains, it is critical to ensure
    decisions are not discriminatory.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Finally, aside from data and model bias, humans are often inherently biased
    decision makers. In response, there is a growing area of research into detecting
    and understanding bias in visual analytics ⁴⁴4The DECISIVe Workshop ([http://decisive-workshop.dbvis.de/](http://decisive-workshop.dbvis.de/))
    at IEEE VIS is dedicated to understanding cognitive bias in visualization. and
    its affect on the decision making process [[120](#bib.bib120)]. Some work has
    developed metrics to detect types of bias to present to a user during data analysis [[120](#bib.bib120)]
    which could also be applied to visual tools for deep learning in the future. Some
    work has employed developmental and cognitive psychology analysis techniques to
    understand how humans learn, focusing on uncovering how human bias is developed
    and influences learning, to ultimately influence artificial neural network design [[112](#bib.bib112)].
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Protecting Against Adversarial Attacks
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regardless of the benefits AI systems are bringing to society, we would be remiss
    to immediately trust them; like most technologies, AI too has security faults.
    Identified and studied in seminal works, it has been shown that deep learning
    models such as image classifiers can be easily fooled by perturbing an input image [[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123)]. Most alarming, some perturbations are
    so subtle that they are untraceable by the human eye, yet would completely fool
    a model into misclassification [[122](#bib.bib122)]. This sparked great interest
    in the AI communities, and much work has been done to understand how fragile deep
    neural network image classifiers are, identify in what ways can they break, and
    explore methods for protecting them. Norton et al. [[44](#bib.bib44)] demonstrate
    adding adversarial perturbations to images in an interactive tool, where users
    can tweak the type and intensity of the attack, and observe the resulting (mis)classification.
    This is a great first start for using visualization to identify potential attacks,
    but we think visualization can be majorly impactful in this research space, by
    not only showcasing how the attacks work and detecting them, but also by taking
    action and protecting AI systems from the attacks themselves. While some work,
    primarily originating from the AI communities, has proposed computational techniques
    to protect AI from attacks, such as identifying adversarial examples before classification [[124](#bib.bib124)],
    modifying the network architecture [[125](#bib.bib125)], modifying the training
    process [[126](#bib.bib126), [122](#bib.bib122)], and performing pre-processing
    steps before classification [[127](#bib.bib127), [128](#bib.bib128)], we think
    visualization can have great impact for combating adversarial machine learning.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 11 Conclusion
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We presented a comprehensive, timely survey on visualization and visual analytics
    in deep learning research, using a human-centered, interrogative framework. Our
    method helps researchers and practitioners in visual analytics and deep learning
    to quickly learn key aspects of this young and rapidly growing body of research,
    whose impact spans a broad range of domains. Our survey goes beyond visualization-focused
    venues to extend a wide scope that also encompasses relevant works from top venues
    in AI, ML, and computer vision. We highlighted visual analytics as an integral
    component in addressing pressing issues in modern AI, helping to discover and
    communicate insight, from discerning model bias, understanding models, to promoting
    AI safety. We concluded by highlighting impactful research directions and open
    problems.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by NSF grants IIS-1563816, CNS-1704701, and TWC-1526254;
    NIBIB grant U54EB020404; NSF GRFP DGE-1650044; NASA Space Technology Research
    Fellowship; and gifts from Intel, Google, Symantec.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent
    in nervous activity,” *The bulletin of mathematical biophysics*, vol. 5, no. 4,
    1943.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] W. Rawat and Z. Wang, “Deep convolutional neural networks for image classification:
    A comprehensive review,” *Neural computation*, vol. 29, no. 9, 2017.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification
    with deep convolutional neural networks,” in *NIPS*, 2012.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional networks:
    Visualising image classification models and saliency maps,” *arXiv:1312.6034*,
    2013.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Karpathy, “What I learned from competing against a convnet on ImageNet,”
    2014\. [Online]. Available: [http://karpathy.github.io/2014/09/02/what-i-learned-from-eompeting-against-a-convnet-on-imagenet](http://karpathy.github.io/2014/09/02/what-i-learned-from-eompeting-against-a-convnet-on-imagenet)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A large-scale hierarchical image database,” in *CVPR*, 2009.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *et al.*, “Imagenet large scale visual recognition
    challenge,” *IJCV*, vol. 115, no. 3, 2015.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in *ECCV*.   Springer, 2014.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. W. Craven and J. W. Shavlik, “Visualizing learning and computation
    in artificial neural networks,” *International Journal on Artificial Intelligence
    Tools*, vol. 1, no. 03, 1992.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. J. Streeter, M. O. Ward, and S. A. Alvarez, “NVIS: An interactive visualization
    tool for neural networks,” in *Visual Data Exploration and Analysis VIII*, vol.
    4302.   International Society for Optics and Photonics, 2001.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] F.-Y. Tzeng and K.-L. Ma, “Opening the black box: Data driven visualization
    of neural networks,” in *IEEE Visualization*, 2005.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu, “Towards better analysis
    of deep convolutional neural networks,” *IEEE TVCG*, vol. 23, no. 1, 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. Mané, D. Fritz,
    D. Krishnan, F. B. Viégas, and M. Wattenberg, “Visualizing dataflow graphs of
    deep learning models in TensorFlow,” *IEEE TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Smilkov, S. Carter, D. Sculley, F. B. Viegas, and M. Wattenberg, “Direct-manipulation
    visualization of deep networks,” in *ICML Workshop on Vis for Deep Learning*,
    2016.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Lu, W. Chen, Y. Ma, J. Ke, Z. Li, F. Zhang, and R. Maciejewski, “Recent
    progress and trends in predictive visual analytics,” *Frontiers of Computer Science*,
    2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Lu, R. Garcia, B. Hansen, M. Gleicher, and R. Maciejewski, “The state-of-the-art
    in predictive visual analytics,” in *Computer Graphics Forum*, vol. 36, no. 3.   Wiley
    Online Library, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. Ren, S. Amershi, B. Lee, J. Suh, and J. D. Williams, “Squares: Supporting
    interactive performance analysis for multiclass classifiers,” *IEEE TVCG*, vol. 23,
    no. 1, 2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Amershi, M. Cakmak, W. B. Knox, and T. Kulesza, “Power to the people:
    The role of humans in interactive machine learning,” *AI Magazine*, vol. 35, no. 4,
    2014.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Sacha, M. Sedlmair, L. Zhang, J. A. Lee, D. Weiskopf, S. North, and
    D. Keim, “Human-centered machine learning through interactive visualization,”
    in *ESANN*, 2016.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C. Seifert, A. Aamir, A. Balagopalan, D. Jain, A. Sharma, S. Grottel,
    and S. Gumhold, “Visualizations of deep neural networks in computer vision: A
    survey,” in *Transparent Data Mining for Big and Small Data*.   Springer, 2017.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. Zeng, “Towards better understanding of deep learning with visualization,”
    *The Hong Kong University of Science and Technology*, 2016.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Liu, X. Wang, M. Liu, and J. Zhu, “Towards better analysis of machine
    learning models: A visual analytics perspective,” *Visual Informatics*, vol. 1,
    no. 1, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J. Choo and S. Liu, “Visual analytics for explainable deep learning,”
    *IEEE Computer Graphics and Applications*, 2018.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*.   MIT Press,
    2016, [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado,
    A. Davis, J. Dean, M. Devin *et al.*, “TensorFlow: Large-scale machine learning
    on heterogeneous distributed systems,” *arXiv:1603.04467*, 2016.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Network Dissection:
    Quantifying interpretability of deep visual representations,” in *CVPR*, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Bilal, A. Jourabloo, M. Ye, X. Liu, and L. Ren, “Do convolutional neural
    networks learn class hierarchy?” *IEEE TVCG*, vol. 24, no. 1, pp. 152–162, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Bojarski, A. Choromanska, K. Choromanski, B. Firner, L. Jackel, U. Muller,
    and K. Zieba, “Visualbackprop: visualizing cnns for autonomous driving,” *arXiv:1611.05418*,
    2016.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Bruckner, “Ml-o-scope: a diagnostic visualization system for deep machine
    learning pipelines,” Master’s thesis, EECS Department, University of California,
    Berkeley, May 2014\. [Online]. Available: [http://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-99.html](http://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-99.html)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Carter, D. Ha, I. Johnson, and C. Olah, “Experiments in handwriting
    with a neural network,” *Distill*, 2016.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] D. Cashman, G. Patterson, A. Mosca, and R. Chang, “RNNbow: Visualizing
    learning via backpropagation gradients in recurrent neural networks,” in *Workshop
    on Visual Analytics for Deep Learning*, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Chae, S. Gao, A. Ramanthan, C. Steed, and G. D. Tourassi, “Visualization
    for classification in deep neural networks,” in *Workshop on Visual Analytics
    for Deep Learning*, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. Chung, C. Park, S. Suh, K. Kang, J. Choo, and B. C. Kwon, “ReVACNN:
    Steering convolutional neural network via real-time visual analytics,” in *NIPS
    Workshop on Future of Interactive Learning Machines*, 2016.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Goyal, A. Mohapatra, D. Parikh, and D. Batra, “Towards transparent
    ai systems: Interpreting visual question answering models,” *arXiv:1608.08974*,
    2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] A. W. Harley, “An interactive node-link visualization of convolutional
    neural networks,” in *ISVC*, 2015, pp. 867–877.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] F. Hohman, N. Hodas, and D. H. Chau, “ShapeShop: Towards understanding
    deep learning representations via interactive experimentation,” in *CHI, Extended
    Abstracts*, 2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M. Kahng, P. Andrews, A. Kalro, and D. H. Chau, “ActiVis: Visual exploration
    of industry-scale deep neural network models,” *IEEE TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Karpathy, J. Johnson, and L. Fei-Fei, “Visualizing and understanding
    recurrent networks,” *arXiv:1506.02078*, 2015.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Li, X. Chen, E. Hovy, and D. Jurafsky, “Visualizing and understanding
    neural models in nlp,” *arXiv:1506.01066*, 2015.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Liu, J. Shi, K. Cao, J. Zhu, and S. Liu, “Analyzing the training processes
    of deep generative models,” *IEEE TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Ming, S. Cao, R. Zhang, Z. Li, and Y. Chen, “Understanding hidden memories
    of recurrent neural networks,” *VAST*, 2017.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. P. Norton and Y. Qi, “Adversarial-Playground: A visualization suite
    showing how adversarial examples fool deep learning,” in *VizSec*.   IEEE, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Olah, “Visualizing MNIST,” *Olah’s Blog*, 2014\. [Online]. Available:
    [http://colah.github.io/posts/2014-10-Visualizing-MNIST/](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, and
    A. Mordvintsev, “The building blocks of interpretability,” *Distill*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] N. Pezzotti, T. Höllt, J. Van Gemert, B. P. Lelieveldt, E. Eisemann, and
    A. Vilanova, “DeepEyes: Progressive visual analytics for designing deep neural
    networks,” *IEEE TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] P. E. Rauber, S. G. Fadel, A. X. Falcao, and A. C. Telea, “Visualizing
    the hidden activity of artificial neural networks,” *IEEE TVCG*, vol. 23, no. 1,
    2017.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C. Robinson, F. Hohman, and B. Dilkina, “A deep learning approach for
    population estimation from satellite imagery,” in *SIGSPATIAL Workshop on Geospatial
    Humanities*, 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Rong and E. Adar, “Visual tools for debugging neural language models,”
    in *ICML Workshop on Vis for Deep Learning*, 2016.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] D. Smilkov, N. Thorat, C. Nicholson, E. Reif, F. B. Viégas, and M. Wattenberg,
    “Embedding Projector: Interactive visualization and interpretation of embeddings,”
    in *NIPS Workshop on Interpretable ML in Complex Systems*, 2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush, “LSTMVis: A tool
    for visual analysis of hidden state dynamics in recurrent neural networks,” *IEEE
    TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] J. Wang, L. Gou, H. Yang, and H.-W. Shen, “GANViz: A visual analytics
    approach to understand the adversarial game,” *IEEE TVCG*, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] B. Webster, “Now anyone can explore machine learning, no coding required,”
    *Google Official Blog*, 2017\. [Online]. Available: [https://www.blog.google/topics/machine-learning/now-anyone-can-explore-machine-learning-no-coding-required/](https://www.blog.google/topics/machine-learning/now-anyone-can-explore-machine-learning-no-coding-required/)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, “Understanding
    neural networks through deep visualization,” in *ICML Deep Learning Workshop*,
    2015.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] T. Zahavy, N. Ben-Zrihem, and S. Mannor, “Graying the black box: Understanding
    DQNs,” in *ICML*, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Zeng, H. Haleem, X. Plantaz, N. Cao, and H. Qu, “CNNComparator: Comparative
    analytics of convolutional neural networks,” in *Workshop on Visual Analytics
    for Deep Learning*, 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] W. Zhong, C. Xie, Y. Zhong, Y. Wang, W. Xu, S. Cheng, and K. Mueller,
    “Evolutionary visual analysis of deep neural networks,” in *ICML Workshop on Vis
    for Deep Learning*, 2017.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative visual
    manipulation on the natural image manifold,” in *ECCV*.   Springer, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Z. C. Lipton, “The mythos of model interpretability,” *arXiv:1606.03490*,
    2016.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Montavon, W. Samek, and K.-R. Müller, “Methods for interpreting and
    understanding deep neural networks,” *Digital Signal Processing*, 2017.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] T. Miller, “Explanation in artificial intelligence: Insights from the
    social sciences,” *arXiv:1706.07269*, 2017.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Weller, “Challenges for transparency,” *ICML Workshop on Human Interpretability
    in ML*, 2017.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] F. Offert, “”i know it when I see it”. visualization and intuitive interpretability,”
    *NIPS Symposium on Interpretable ML*, 2017.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat,
    F. Viégas, M. Wattenberg, G. Corrado *et al.*, “Google’s multilingual neural machine
    translation system: enabling zero-shot translation,” *arXiv:1611.04558*, 2016.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling, “Visualizing deep
    neural network decisions: Prediction difference analysis,” *arXiv:1702.04595*,
    2017.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] L. Li, J. Tompkin, P. Michalatos, and H. Pfister, “Hierarchical visual
    feature analysis for city street view datasets,” in *Workshop on Visual Analytics
    for Deep Learning*, 2017.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] D. Erhan, Y. Bengio, A. Courville, and P. Vincent, “Visualizing higher-layer
    features of a deep network,” *University of Montreal*, vol. 1341, 2009.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra,
    “Grad-CAM: Why did you say that? visual explanations from deep networks via gradient-based
    localization,” *arXiv:1610.02391*, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune, “Synthesizing
    the preferred inputs for neurons in neural networks via deep generator networks,”
    in *NIPS*, 2016.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] K. Patel, J. Fogarty, J. A. Landay, and B. Harrison, “Investigating statistical
    machine learning as a tool for software development,” in *CHI*, 2008.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] T. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf, “Principles of explanatory
    debugging to personalize interactive machine learning,” in *IUI*, 2015.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] B. Nushi, E. Kamar, E. Horvitz, and D. Kossmann, “On human intellect and
    machine failures: Troubleshooting integrative machine learning systems,” in *AAAI*,
    2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] E. Alexander and M. Gleicher, “Task-driven comparison of topic models,”
    *IEEE TVCG*, vol. 22, no. 1, 2016.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie,
    T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu, M. Wattenberg, A. M.
    Hrafnkelsson, T. Boulos, and J. Kubica, “Ad click prediction: A view from the
    trenches,” in *KDD*, 2013.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Kahng, D. Fang, and D. H. P. Chau, “Visual exploration of machine learning
    results using data cube analysis,” in *SIGMOD Workshop on Human-In-the-Loop Data
    Analytics*, 2016.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] W. Yu, K. Yang, Y. Bai, H. Yao, and Y. Rui, “Visualizing and comparing
    convolutional neural networks,” *arXiv:1412.6631*, 2014.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Wattenberg, F. Viégas, and I. Johnson, “How to use t-SNE effectively,”
    *Distill*, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L. v. d. Maaten and G. Hinton, “Visualizing data using t-SNE,” *JMLR*,
    vol. 9, no. Nov, 2008.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Nguyen, J. Yosinski, and J. Clune, “Multifaceted feature visualization:
    Uncovering the different types of features learned by each neuron in deep neural
    networks,” in *ICML Workshop on Vis for Deep Learning*, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] P. E. Rauber, A. X. Falcão, and A. C. Telea, “Visualizing time-dependent
    data using dynamic t-sne,” *EuroVis*, vol. 2, no. 5, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Liu, P.-T. Bremer, J. J. Thiagarajan, V. Srikumar, B. Wang, Y. Livnat,
    and V. Pascucci, “Visual exploration of semantic relationships in neural word
    embeddings,” *IEEE TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba, “Hoggles: Visualizing
    object detection features,” in *ICCV*, 2013.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] X. Rong, “word2vec parameter learning explained,” *arXiv:1411.2738*, 2014.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] D. Park, S. Kim, J. Lee, J. Choo, N. Diakopoulos, and N. Elmqvist, “ConceptVector:
    text visual analytics via interactive lexicon building using word embedding,”
    *IEEE TVCG*, vol. 24, no. 1, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] D. S. Weld and G. Bansal, “Intelligible artificial intelligence,” *arXiv:1803.04263*,
    2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] C. Olah and S. Carter, “Research debt,” *Distill*, 2017.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualization,” *Distill*,
    2017.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Mahendran and A. Vedaldi, “Visualizing deep convolutional neural networks
    using natural pre-images,” *IJCV*, vol. 120, no. 3, 2016.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] F. Grün, C. Rupprecht, N. Navab, and F. Tombari, “A taxonomy and library
    for visualizing learned features in convolutional neural networks,” *ICML Workshop
    on Vis for Deep Learning*, 2016.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] P.-J. Kindermans, K. T. Schütt, M. Alber, K.-R. Müller, and S. Dähne,
    “Learning how to explain neural networks: Patternnet and patternattribution,”
    *arXiv:1705.05598*, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] H. Li, K. Mueller, and X. Chen, “Beyond saliency: understanding convolutional
    neural networks from saliency prediction on layer-wise relevance propagation,”
    *arXiv:1712.08268*, 2017.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] D. Smilkov, N. Thorat, B. Kim, F. Viégas, and M. Wattenberg, “SmoothGrad:
    removing noise by adding noise,” in *ICML Workshop on Vis for Deep Learning*,
    2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *CVPR*, 2016.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A. Dosovitskiy and T. Brox, “Inverting visual representations with convolutional
    networks,” in *CVPR*, 2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Mahendran and A. Vedaldi, “Understanding deep image representations
    by inverting them,” in *CVPR*, 2015.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Object detectors
    emerge in deep scene CNNs,” *arXiv:1412.6856*, 2014.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] A. Mordvintsev, C. Olah, and M. Tyka, “Inceptionism: Going deeper into
    neural networks,” *Google Research Blog*, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving
    for simplicity: The all convolutional net,” *arXiv:1412.6806*, 2014.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] D. Wei, B. Zhou, A. Torrabla, and W. Freeman, “Understanding intra-class
    knowledge inside CNN,” *arXiv:1507.02379*, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Nguyen, J. Yosinski, Y. Bengio, A. Dosovitskiy, and J. Clune, “Plug
    & play generative networks: Conditional iterative generation of images in latent
    space,” *arXiv:1612.00005*, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Bengio *et al.*, “Learning deep architectures for ai,” *Foundations
    and trends in Machine Learning*, vol. 2, no. 1, 2009.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Carter and M. Nielsen, “Using artificial intelligence to augment human
    intelligence,” *Distill*, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural
    image caption generator,” in *CVPR*, 2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,
    and D. Parikh, “Vqa: Visual question answering,” in *ICCV*, 2015.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] I. Goodfellow, “NIPS 2016 tutorial: Generative adversarial networks,”
    *arXiv:1701.00160*, 2016.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] S. Liu, D. Maljovec, B. Wang, P.-T. Bremer, and V. Pascucci, “Visualizing
    high-dimensional data: Advances in the past decade,” *IEEE TVCG*, vol. 23, no. 3,
    2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K.-R. Müller, “Evaluating
    the visualization of what a deep neural network has learned,” *IEEE transactions
    on neural networks and learning systems*, 2017.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust you?: Explaining
    the predictions of any classifier,” in *KDD*, 2016.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C.-Y. Tsai and D. D. Cox, “Characterizing visual representations within
    convolutional neural networks: Toward a quantitative approach,” *ICML Workshop
    on Vis for Deep Learning*, 2016.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Ritter, D. G. Barrett, A. Santoro, and M. M. Botvinick, “Cognitive
    psychology for deep neural networks: A shape bias case study,” *arXiv:1706.08606*,
    2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human attention
    in visual question answering: Do humans and deep networks look at the same regions?”
    *Computer Vision and Image Understanding*, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] G. K. Tam, V. Kothari, and M. Chen, “An analysis of machine-and human-analytics
    in classification,” *IEEE TVCG*, vol. 23, no. 1, 2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Barocas and A. D. Selbst, “Big data’s disparate impact,” *Calif. L.
    Rev.*, vol. 104, pp. 671–769, 2016.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics derived automatically
    from language corpora contain human-like biases,” *Science*, vol. 356, no. 6334,
    2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M. Wattenberg, F. Viegas, and M. Hardt, “Attacking discrimination with
    smarter machine learning,” *Google Research Website*, 2016\. [Online]. Available:
    [https://research.google.com/bigpicture/attacking-discrimination-in-ml/](https://research.google.com/bigpicture/attacking-discrimination-in-ml/)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] “Facets,” *Google PAIR*, 2017\. [Online]. Available: [https://pair-code.github.io/facets/](https://pair-code.github.io/facets/)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Hardt, E. Price, N. Srebro *et al.*, “Equality of opportunity in supervised
    learning,” in *NIPS*, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] E. Wall, L. Blaha, L. Franklin, and A. Endert, “Warning, bias may occur:
    A proposed approach to detecting cognitive bias in interactive visual analytics,”
    *VAST*, 2017.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” *arXiv:1312.6199*,
    2013.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” *ICLR*, 2014.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images,” in *CVPR*, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, “On detecting
    adversarial perturbations,” in *ICLR*, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] S. Gu and L. Rigazio, “Towards deep neural network architectures robust
    to adversarial examples,” *arXiv:1412.5068*, 2014.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
    as a defense to adversarial perturbations against deep neural networks,” in *Security
    and Privacy*, 2016.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li, L. Chen, M. E. Kounavis,
    and D. H. Chau, “Shield: Fast, practical defense and vaccination for deep learning
    using jpeg compression,” *arXiv:1802.06816*, 2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
    physical world,” *arXiv:1607.02533*, 2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/4c15581baa77108eb45c66351abd7738.png) | Fred Hohman
    is a PhD student at Georgia Tech’s College of Computing. His research combines
    HCI principles and ML techniques to improve deep learning interpretability. He
    won the NASA Space Technology Research Fellowship. He received his B.S. in mathematics
    and physics. He won SIGMOD’17 Best Demo, Honorable Mention; Microsoft AI for Earth
    Award for using AI to improve sustainability; and the President’s Fellowship for
    top incoming PhD students. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/96bbfa2e1fbaf0d882784bd8f86a7df3.png) | Minsuk
    Kahng is a computer science PhD student at Georgia Tech. His thesis research focuses
    on building visual analytics tools for exploring, interpreting, and interacting
    with complex machine learning models and results, by combining methods from information
    visualization, machine learning, and databases. He received the Google PhD Fellowship
    and NSF Graduate Research Fellowship. His ActiVis deep learning visualization
    system has been deployed on Facebook’s machine learning platform. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/5032f16dab9b44b982a296a0d1fca350.png) | Robert
    Pienta is an industry researcher in applied machine learning and visual analytics.
    He received his PhD degree in computational science and engineering from Georgia
    Tech in 2017\. He was an NSF FLAMEL fellow and presidential scholar at Georgia
    Tech. His research interests include visual analytics, graph analytics, and machine
    learning. In particular, the algorithms and design techniques for interactive
    graph querying and exploration. |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a6519b46820b2b5cb798eb1eeb205cb6.png) | Duen Horng
    (Polo) Chau is an Associate Professor at Georgia Tech. His research bridges data
    mining and HCI to make sense of massive datasets. His thesis won Carnegie Mellon’s
    CS Dissertation Award, Honorable Mention. He received awards from Intel, Google,
    Yahoo, LexisNexis, and Symantec; He won paper awards at SIGMOD, KDD and SDM. He
    is an ACM IUI steering committee member, IUI’15 co-chair, and IUI’19 program co-chair.
    His research is deployed by Facebook, Symantec, and Yahoo. |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
