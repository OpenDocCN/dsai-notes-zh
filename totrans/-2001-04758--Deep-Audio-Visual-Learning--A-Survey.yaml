- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:02:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2001.04758] Deep Audio-Visual Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2001.04758] 深度音视学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2001.04758](https://ar5iv.labs.arxiv.org/html/2001.04758)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2001.04758](https://ar5iv.labs.arxiv.org/html/2001.04758)
- en: \firstheadname\firstfootname\headevenname\headoddname
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \firstheadname\firstfootname\headevenname\headoddname
- en: 'Deep Audio-Visual Learning: A Survey'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度音视学习：综述
- en: Hao Zhu^(1,2)   Mandi Luo^(2,3)   Rui Wang^(1,2)   Aihua Zheng¹   Ran He^(2,3,4)
    ¹School of Computer Science and Technology, Anhui University, Hefei, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 赵浩^(1,2)   罗曼蒂^(2,3)   王睿^(1,2)   郑爱华¹   何然^(2,3,4) ¹中国合肥，安徽大学计算机科学与技术学院
- en: ²Center for Research on Intelligent Perception and Computing (CRIPAC) and National
    Laboratory of Pattern Recognition (NLPR), CASIA, Beijing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²中国北京，中科院智能感知与计算中心（CRIPAC）及模式识别国家实验室（NLPR）
- en: ³School of Artificial Intelligence, University of the Chinese Academy of Sciences,
    Beijing, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³中国北京，中国科学院人工智能学院
- en: ⁴Center for Excellence in Brain Science and Intelligence Technology, CAS, Beijing,
    China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴中国北京，中科院脑科学与智能技术卓越中心
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Audio-visual learning, aimed at exploiting the relationship between audio and
    visual modalities, has drawn considerable attention since deep learning started
    to be used successfully. Researchers tend to leverage these two modalities either
    to improve the performance of previously considered single-modality tasks or to
    address new challenging problems. In this paper, we provide a comprehensive survey
    of recent audio-visual learning development. We divide the current audio-visual
    learning tasks into four different subfields: audio-visual separation and localization,
    audio-visual correspondence learning, audio-visual generation, and audio-visual
    representation learning. State-of-the-art methods as well as the remaining challenges
    of each subfield are further discussed. Finally, we summarize the commonly used
    datasets and performance metrics.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 音视学习，旨在利用音频和视觉模态之间的关系，自深度学习成功应用以来引起了相当大的关注。研究人员倾向于利用这两种模态来提高之前单一模态任务的性能或解决新的挑战性问题。本文对近年来音视学习的发展进行了全面的综述。我们将当前的音视学习任务分为四个不同的子领域：音视分离与定位、音视对应学习、音视生成和音视表征学习。我们进一步讨论了每个子领域的最先进方法以及剩余挑战。最后，我们总结了常用的数据集和性能指标。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: audio-visual learning, deep learning, survey.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 音视学习，深度学习，综述。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Human perception is multidimensional and includes vision, hearing, touch, taste,
    and smell. In recent years, along with the vigorous development of artificial
    intelligence technology, the trend from single-modality learning to multimodality
    learning has become crucial to better machine perception. Analyses of audio and
    visual information, representing the two most important perceptual modalities
    in our daily life, have been widely developed in both academia and industry in
    the past decades. Prominent achievements include speech recognition [[1](#bib.bib1),
    [2](#bib.bib2)], facial recognition [[3](#bib.bib3), [4](#bib.bib4)], etc. Audio-visual
    learning (AVL) using both modalities has been introduced to overcome the limitation
    of perception tasks in each modality. In addition, exploring the relationship
    between audio and visual information leads to more interesting and important research
    topics and ultimately better perspectives on machine learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人类感知是多维的，包括视觉、听觉、触觉、味觉和嗅觉。近年来，随着人工智能技术的蓬勃发展，从单一模态学习到多模态学习的趋势已成为提高机器感知能力的关键。音频和视觉信息的分析，代表了我们日常生活中最重要的感知模态，已在过去几十年中广泛发展于学术界和工业界。显著的成就包括语音识别 [[1](#bib.bib1),
    [2](#bib.bib2)]、面部识别 [[3](#bib.bib3), [4](#bib.bib4)]等。使用两种模态的音视学习（AVL）已被引入，以克服每种模态感知任务的局限性。此外，探索音频与视觉信息之间的关系带来了更有趣且重要的研究主题，并最终为机器学习提供了更好的视角。
- en: 'The purpose of this article is to provide an overview of the key methodologies
    in audio-visual learning, which aims to discover the relationship between audio
    and visual data for many challenging tasks. In this paper, we mainly divide these
    efforts into four categories: (1) audio-visual separation and localization, (2)
    audio-visual corresponding learning, (3) audio and visual generation, and (4)
    audio-visual representation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是概述音视频学习中的关键方法论，这些方法旨在发现音频和视觉数据之间的关系，以应对许多具有挑战性的任务。在本文中，我们主要将这些努力分为四类：(1)
    音视频分离与定位，(2) 音视频对应学习，(3) 音频和视觉生成，以及 (4) 音视频表示。
- en: Audio-visual separation and localization aim to separate specific sounds emanating
    from the corresponding objects and localize each sound in the visual context,
    as illustrated in Fig. LABEL:fig:overall (a). Audio separation has been investigated
    extensively in the signal processing community during the past two decades. With
    the addition of the visual modality, audio separation can be transformed into
    audio-visual separation, which has proven to be more effective in noisy scenes
    [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]. Furthermore, introducing the
    visual modality allows for audio localization, i.e., the localization of a sound
    in the visual modality according to the audio input. The tasks of audio-visual
    separation and localization themselves not only lead to valuable applications
    but also provide the foundation for other audio-visual tasks, e.g., generating
    spatial audio for 360^∘ video [[8](#bib.bib8)]. Most studies in this area focus
    on unsupervised learning due to the lack of training labels.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 音视频分离和定位的目标是分离来自相应物体的特定声音，并在视觉背景中定位每个声音，如图 LABEL:fig:overall (a) 所示。在过去二十年中，音频分离在信号处理领域得到了广泛研究。随着视觉模态的加入，音频分离可以转变为音视频分离，这在嘈杂场景中证明了更高的效果[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]。此外，引入视觉模态使得音频定位成为可能，即根据音频输入在视觉模态中定位声音。音视频分离和定位任务本身不仅带来了有价值的应用，还为其他音视频任务提供了基础，例如生成
    360^∘ 视频的空间音频 [[8](#bib.bib8)]。由于缺乏训练标签，这一领域的大多数研究都集中在无监督学习上。
- en: Audio-visual correspondence learning focuses on discovering the global semantic
    relation between audio and visual modalities, as shown in Fig. LABEL:fig:overall
    (b). It consists of audio-visual retrieval and audio-visual speech recognition
    tasks. The former uses audio or an image to search for its counterpart in another
    modality, while the latter derives from the conventional speech recognition task
    that leverages visual information to provide a more semantic prior to improve
    recognition performance. Although both of these two tasks have been extensively
    studied, they still entail major challenges, especially for fine-grained cross-modality
    retrieval and homonyms in speech recognition.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 音视频对应学习侧重于发现音频和视觉模态之间的全局语义关系，如图 LABEL:fig:overall (b) 所示。它包括音视频检索和音视频语音识别任务。前者使用音频或图像在另一模态中搜索其对应项，而后者源自传统的语音识别任务，通过利用视觉信息提供更多的语义先验以提高识别性能。尽管这两个任务都已经得到了广泛的研究，但它们仍然面临重大挑战，特别是在细粒度跨模态检索和语音识别中的同音词问题上。
- en: 'Audio-visual generation tries to synthesize the other modality based on one
    of them, which is different from the above two tasks leveraging both audio and
    visual modalities as inputs. Trying to make a machine that is creative is always
    challenging, and many generative models have been proposed [[9](#bib.bib9), [10](#bib.bib10)].
    Audio-visual cross-modality generation has recently drawn considerable attention.
    It aims to generate audio from visual signals, or vice versa. Although it is easy
    for a human to perceive the natural correlation between sounds and appearance,
    this task is challenging for machines due to heterogeneity across modalities.
    As shown in Fig. LABEL:fig:overall (c), vision to audio generation mainly focuses
    on recovering speech from lip sequences or predicting the sounds that may occur
    in the given scenes. In contrast, audio to vision generation can be classified
    into three categories: audio-driven image generation, body motion generation,
    and talking face generation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 音频-视觉生成试图基于其中一种模态合成另一种模态，这与上述两项任务不同，上述任务利用音频和视觉模态作为输入。试图让机器具有创造力总是具有挑战性的，许多生成模型已经被提出
    [[9](#bib.bib9), [10](#bib.bib10)]。音频-视觉跨模态生成最近引起了相当大的关注。其目标是从视觉信号生成音频，或反之。尽管人类可以轻松感知声音和外观之间的自然关联，但由于模态之间的异质性，这一任务对机器来说是具有挑战性的。如图Fig.
    LABEL:fig:overall (c)所示，视觉到音频生成主要集中在从唇部序列恢复语音或预测在给定场景中可能发生的声音。相反，音频到视觉生成可以分为三类：音频驱动的图像生成、身体运动生成和说话的面部生成。
- en: The last task—audio-visual representation learning—aims to automatically discover
    the representation from raw data. A human can easily recognize audio or video
    based on long-term brain cognition. However, machine learning algorithms such
    as deep learning models are heavily dependent on data representation. Therefore,
    learning suitable data representations for machine learning algorithms may improve
    performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项任务——音频-视觉表示学习——旨在从原始数据中自动发现表示。人类可以基于长期的脑部认知轻松识别音频或视频。然而，像深度学习模型这样的机器学习算法严重依赖数据表示。因此，为机器学习算法学习合适的数据表示可能会提高性能。
- en: Unfortunately, real-world data such as images, videos and audio do not possess
    specific algorithmically defined features [[11](#bib.bib11)]. Therefore, an effective
    representation of data determines the success of machine learning algorithms.
    Recent studies seeking better representation have designed various tasks, such
    as audio-visual correspondence (AVC) [[12](#bib.bib12)] and audio-visual temporal
    synchronization (AVTS) [[13](#bib.bib13)]. By leveraging such a learned representation,
    one can more easily solve audio-visual tasks mentioned in the very beginning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现实世界的数据如图像、视频和音频并不具备特定的算法定义特征 [[11](#bib.bib11)]。因此，数据的有效表示决定了机器学习算法的成功。近期的研究在寻求更好表示的过程中设计了各种任务，如音频-视觉对应（AVC）
    [[12](#bib.bib12)] 和音频-视觉时间同步（AVTS） [[13](#bib.bib13)]。通过利用这样的学习表示，可以更容易地解决前面提到的音频-视觉任务。
- en: 'In this paper, we present a comprehensive survey of the above four directions
    of audio-visual learning. The rest of this paper is organized as follows. We introduce
    the four directions in Secs. [2](#S2 "2 Audio-visual Separation and Localization
    ‣ Deep Audio-Visual Learning: A Survey"), [3](#S3 "3 Audio-visual Correspondence
    Learning ‣ Deep Audio-Visual Learning: A Survey"), [4](#S4 "4 Audio and Visual
    Generation ‣ Deep Audio-Visual Learning: A Survey") and [5](#S5 "5 Audio-visual
    Representation Learning ‣ Deep Audio-Visual Learning: A Survey"). Sec. [6](#S6
    "6 Recent Public Audio-visual Datasets ‣ Deep Audio-Visual Learning: A Survey")
    summarizes the commonly used public audio-visual datasets. Finally, Sec. [8](#S8
    "8 Conclusions ‣ Deep Audio-Visual Learning: A Survey") concludes the paper.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们对上述四个方向的音频-视觉学习进行了全面的综述。本文其余部分组织如下。我们在第[2](#S2 "2 Audio-visual Separation
    and Localization ‣ Deep Audio-Visual Learning: A Survey")、[3](#S3 "3 Audio-visual
    Correspondence Learning ‣ Deep Audio-Visual Learning: A Survey")、[4](#S4 "4 Audio
    and Visual Generation ‣ Deep Audio-Visual Learning: A Survey")和[5](#S5 "5 Audio-visual
    Representation Learning ‣ Deep Audio-Visual Learning: A Survey")节中介绍这四个方向。第[6](#S6
    "6 Recent Public Audio-visual Datasets ‣ Deep Audio-Visual Learning: A Survey")节总结了常用的公共音频-视觉数据集。最后，第[8](#S8
    "8 Conclusions ‣ Deep Audio-Visual Learning: A Survey")节对本文进行了总结。'
- en: 2 Audio-visual Separation and Localization
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 音频-视觉分离与定位
- en: 'The objective of audio-visual separation is to separate different sounds from
    the corresponding objects, while audio-visual localization mainly focuses on localizing
    a sound in a visual context. As shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1 Speaker
    Separation ‣ 2 Audio-visual Separation and Localization ‣ Deep Audio-Visual Learning:
    A Survey"), we classify types of this task by different identities: speakers (Fig.
    [1](#S2.F1 "Figure 1 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and
    Localization ‣ Deep Audio-Visual Learning: A Survey") (a)) and objects (Fig. [1](#S2.F1
    "Figure 1 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and Localization
    ‣ Deep Audio-Visual Learning: A Survey") (b)).The former concentrates on a person’s
    speech that can be used for television programs to enhance the target speakers’
    voice, while the latter is a more general and challenging task that separates
    arbitrary objects rather than speakers only. In this section, we provide an overview
    of these two tasks, examining the motivations, network architectures, advantages,
    and disadvantages.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '音视频分离的目标是将不同的声音从相应的对象中分离出来，而音视频定位主要集中于在视觉背景中定位声音。如图 [1](#S2.F1 "Figure 1 ‣
    2.1 Speaker Separation ‣ 2 Audio-visual Separation and Localization ‣ Deep Audio-Visual
    Learning: A Survey") 所示，我们通过不同的身份对这一任务进行分类：说话者（图 [1](#S2.F1 "Figure 1 ‣ 2.1 Speaker
    Separation ‣ 2 Audio-visual Separation and Localization ‣ Deep Audio-Visual Learning:
    A Survey") (a)）和物体（图 [1](#S2.F1 "Figure 1 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual
    Separation and Localization ‣ Deep Audio-Visual Learning: A Survey") (b)）。前者集中于一个人的语音，可用于电视节目中增强目标说话者的声音，而后者是一个更一般且更具挑战性的任务，涉及到分离任意物体，而不仅仅是说话者。在本节中，我们提供了这两项任务的概述，审视了它们的动机、网络架构、优点和缺点。'
- en: 2.1 Speaker Separation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 说话者分离
- en: The speaker separation task is a challenging task and is also known as the ‘cocktail
    party problem’. It aims to isolate a single speech signal in a noisy scene. Some
    studies tried to solve the problem of audio separation with only the audio modality
    and achieved exciting results [[14](#bib.bib14), [15](#bib.bib15)]. Advanced approaches [[5](#bib.bib5),
    [7](#bib.bib7)] tried to utilize visual information to aid the speaker separation
    task and significantly surpassed single modality-based methods. The early attempts
    leveraged mutual information to learn the joint distribution between the audio
    and the video [[16](#bib.bib16), [17](#bib.bib17)]. Subsequently, several methods
    focused on analyzing videos containing salient motion signals and the corresponding
    audio events (e.g., a mouth starting to move or a hand on piano suddenly accelerating)
    [[18](#bib.bib18), [19](#bib.bib19)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 说话者分离任务是一项具有挑战性的任务，也被称为“鸡尾酒会问题”。其目标是在嘈杂的场景中隔离单一的语音信号。一些研究尝试仅利用音频模态解决音频分离问题，并取得了令人兴奋的结果
    [[14](#bib.bib14), [15](#bib.bib15)]。先进的方法[[5](#bib.bib5), [7](#bib.bib7)] 试图利用视觉信息来辅助说话者分离任务，并显著超越了基于单一模态的方法。早期尝试利用互信息学习音频和视频之间的联合分布
    [[16](#bib.bib16), [17](#bib.bib17)]。随后，一些方法专注于分析包含显著运动信号和相应音频事件的视频（例如，嘴巴开始移动或钢琴上的手突然加速）
    [[18](#bib.bib18), [19](#bib.bib19)]。
- en: '![Refer to caption](img/c55ef362af1c7db9a2049b2d00958289.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c55ef362af1c7db9a2049b2d00958289.png)'
- en: 'Figure 1: Illustration of audio-visual separation and localization task. Paths
    1 and 2 denote separation and localization tasks, respectively.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：音视频分离和定位任务的示意图。路径 1 和 2 分别表示分离和定位任务。
- en: '![Refer to caption](img/e164400132110dd731c0f1589692f253.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e164400132110dd731c0f1589692f253.png)'
- en: 'Figure 2: Basic pipeline of a noisy audio filter.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：噪声音频过滤的基本流程。
- en: 'Gabbay et al.[[5](#bib.bib5)] proposed isolating the voice of a specific speaker
    and eliminating other sounds in an audio-visual manner. Instead of directly extracting
    the target speaker’s voice from the noisy sound, which may bias the training model,
    the researchers first fed the video frames into a video-to-speech model and then
    predicted the speaker’s voice by the facial movements captured in the video. Afterwards,
    the predicted voice was used to filter the mixtures of sounds, as shown in Fig.
    [2](#S2.F2 "Figure 2 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and
    Localization ‣ Deep Audio-Visual Learning: A Survey").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gabbay 等人[[5](#bib.bib5)] 提出了通过音视频方式隔离特定说话者的声音并消除其他声音的方法。研究人员并没有直接从噪声中提取目标说话者的声音，因为这样可能会使训练模型产生偏差，而是首先将视频帧输入到视频转语音模型中，然后通过视频中捕捉到的面部动作预测说话者的声音。之后，预测的声音被用来过滤混合的声音，如图
    [2](#S2.F2 "Figure 2 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and
    Localization ‣ Deep Audio-Visual Learning: A Survey") 所示。'
- en: Although Gabbay et al. [[5](#bib.bib5)] improved the quality of separated voice
    by adding the visual modality, their approach was only applicable in controlled
    environments. To obtain intelligible speech in an unconstrained environment, Afouras
    et al. [[6](#bib.bib6)] proposed a deep audio-visual speech enhancement network
    to separate the speaker’s voice of the given lip region by predicting both the
    magnitude and phase of the target signal. The authors treated the spectrograms
    as temporal signals rather than images for a network. Additionally, instead of
    directly predicting clean signal magnitudes, they also tried to generate a more
    effective soft mask for filtering.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Gabbay等人[[5](#bib.bib5)]通过添加视觉模态来提高了分离语音的质量，但他们的方法仅适用于受控环境。为了在无约束环境中获得可理解的语音，Afouras等人[[6](#bib.bib6)]提出了一种深度音频-视觉语音增强网络，通过预测目标信号的幅度和相位来分离给定唇部区域的说话者声音。作者将声谱图视作时间信号而非网络的图像。此外，他们不仅直接预测干净信号的幅度，还尝试生成更有效的软掩模进行过滤。
- en: In contrast to previous approaches that require training a separate model for
    each speaker of interest (speaker-dependent models), Ephrat et al. [[7](#bib.bib7)]
    proposed a speaker-independent model that was only trained once and was then applicable
    to any speaker. This approach even outperformed the state-of-the-art speaker-dependent
    audio-visual speech separation methods. The relevant model consists of multiple
    visual streams and one audio stream, concatenating the features from different
    streams into a joint audio-visual representation. This feature is further processed
    by a bidirectional LSTM and three fully connected layers. Finally, an elaborate
    spectrogram mask is learned for each speaker to be multiplied by the noisy input.
    Finally, the researchers converted it back to waveforms to obtain an isolated
    speech signal for each speaker. Lu et al. [[20](#bib.bib20)] designed a network
    similar to that of [[7](#bib.bib7)]. The difference is that the authors enforced
    an audio-visual matching network to distinguish the correspondence between speech
    and human lip movements. Therefore, they could obtain clear speech.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前需要为每个感兴趣的说话者训练单独模型（说话者依赖模型）的方法不同，Ephrat等人[[7](#bib.bib7)]提出了一种说话者独立模型，该模型只训练一次，然后适用于任何说话者。这种方法甚至超越了最先进的说话者依赖音频-视觉语音分离方法。相关模型由多个视觉流和一个音频流组成，将不同流的特征连接成一个联合音频-视觉表示。这个特征通过双向LSTM和三个全连接层进一步处理。最后，为每个说话者学习一个精细的声谱图掩模，并将其乘以噪声输入。最终，研究人员将其转换回波形，以获得每个说话者的孤立语音信号。Lu等人[[20](#bib.bib20)]设计了一个类似于[[7](#bib.bib7)]的网络。不同之处在于，作者强制使用音频-视觉匹配网络来区分语音与人类唇部动作之间的对应关系。因此，他们能够获得清晰的语音。
- en: Instead of directly utilizing video as a condition, Morrone et al. [[21](#bib.bib21)]
    further introduced landmarks as a fine-grained feature to generate time-frequency
    masks to filter mixed-speech spectrogram.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接利用视频作为条件的方法不同，Morrone等人[[21](#bib.bib21)]进一步引入了地标作为细粒度特征，以生成时频掩模来过滤混合语音的声谱图。
- en: 2.2 Separating and Localizing Objects’ Sounds
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 分离与定位物体的声音
- en: Instead of matching a specific lip movement from a noisy environment as in the
    speaker separation task, humans focus more on objects while dealing with sound
    separation and localization. It is difficult to find a clear correspondence between
    audio and visual modalities due to the challenge of exploring the prior sounds
    from different objects.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与在嘈杂环境中匹配特定唇部动作的说话者分离任务不同，人类在处理声音分离和定位时更多关注物体。由于探索不同物体的先验声音的挑战，音频和视觉模态之间很难找到清晰的对应关系。
- en: 2.2.1 Separation
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 分离
- en: The early attempt to solve this localization problem can be traced back to 2000
    [[22](#bib.bib22)] and a study that synchronized low-level features of sounds
    and videos. Fisher et al. [[17](#bib.bib17)] later proposed using a nonparametric
    approach to learn a joint distribution of visual and audio signals and then project
    both of them to a learned subspace. Furthermore, several acoustics-based methods
    [[23](#bib.bib23), [24](#bib.bib24)] were described that required specific devices
    for surveillance and instrument engineering, such as microphone arrays used to
    capture the differences in the arrival of sounds.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 早期尝试解决这一定位问题可以追溯到 2000 年 [[22](#bib.bib22)]，当时的一项研究同步了声音和视频的低级特征。Fisher et al.
    [[17](#bib.bib17)] 随后提出使用非参数方法学习视觉和音频信号的联合分布，然后将这两者投影到学习的子空间中。此外，描述了几种基于声学的方法
    [[23](#bib.bib23), [24](#bib.bib24)]，这些方法需要特定的监控和仪器工程设备，例如用于捕捉声音到达差异的麦克风阵列。
- en: To learn audio source separation from large-scale in-the-wild videos containing
    multiple audio sources per video, Gao et al. [[25](#bib.bib25)] suggested learning
    an audio-visual localization model from unlabeled videos and then exploiting the
    visual context for audio source separation. Researchers’ approach relied on a
    multi-instance multilabel learning framework to disentangle the audio frequencies
    related to individual visual objects even without observing or hearing them in
    isolation. The multilabel learning framework was fed by a bag of audio basis vectors
    for each video, and then, the bag-level prediction of the objects presented in
    the audio was obtained.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从包含每个视频多个音频源的大规模野外视频中学习音频源分离，Gao et al. [[25](#bib.bib25)] 提出了从未标记的视频中学习音频-视觉定位模型，然后利用视觉上下文进行音频源分离。研究人员的方法依赖于多实例多标签学习框架，即使在未观察或听到这些视觉物体的情况下，也能解开与各个视觉物体相关的音频频率。多标签学习框架通过每个视频的音频基向量包进行训练，然后获得音频中呈现物体的包级预测。
- en: 'Table 1: Summary of recent audio-visual separation and localization approaches.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：最近音频-视觉分离和定位方法的总结。
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 思路与优点 | 缺点 |'
- en: '|  | Gabbay et al. [[5](#bib.bib5)] |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | Gabbay et al. [[5](#bib.bib5)] |'
- en: '&#124; Predict speaker’s voice based on &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据 &#124; 预测说话者的声音'
- en: '&#124; faces in video used as a filter &#124;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频中的面孔用作过滤器 &#124;'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Can only be used &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅能用于 &#124;'
- en: '&#124; in controlled environments &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在受控环境中 &#124;'
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Afouras et al. [[6](#bib.bib6)] |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | Afouras et al. [[6](#bib.bib6)] |'
- en: '&#124; Generate a soft mask for &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 为 &#124; 生成软掩模'
- en: '&#124; filtering in the wild &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 野外过滤 &#124;'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Requires training a &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要训练一个 &#124;'
- en: '&#124; separate model for &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单独的模型用于 &#124;'
- en: '&#124; each speaker of interest &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关注的每个说话者 &#124;'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Speaker Separation | Lu et al. [[20](#bib.bib20)] |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 说话者分离 | Lu et al. [[20](#bib.bib20)] |'
- en: '&#124; Distinguish the correspondence &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区分对应关系 &#124;'
- en: '&#124; between speech and human &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音与人类之间 &#124;'
- en: '&#124; speech lip movements &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音唇部动作 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Two speakers only; &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅限两个说话者； &#124;'
- en: '&#124; hardly applied for &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 几乎无法应用于 &#124;'
- en: '&#124; background noise &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 背景噪音 &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Ephrat et al. [[7](#bib.bib7)] |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | Ephrat et al. [[7](#bib.bib7)] |'
- en: '&#124; Predict a complex spectrogram &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预测复杂的谱图 &#124;'
- en: '&#124; mask for each speaker; &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每个说话者的掩模； &#124;'
- en: '&#124; trained once, applicable to &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练一次，适用于 &#124;'
- en: '&#124; any speaker &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任何说话者 &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The model is too complicated &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型过于复杂 &#124;'
- en: '&#124; and lacks explanation &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并且缺乏解释 &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Morrone et al. [[21](#bib.bib21)] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | Morrone et al. [[21](#bib.bib21)] |'
- en: '&#124; Use landmarks to generate &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用地标生成 &#124;'
- en: '&#124; time-frequency masks &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间-频率掩模 &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Additional landmark &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 额外的地标 &#124;'
- en: '&#124; detection required &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要检测 &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Gao et al. [[25](#bib.bib25)] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | Gao et al. [[25](#bib.bib25)] |'
- en: '&#124; Disentangle audio frequencies &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解开音频频率 &#124;'
- en: '&#124; related to visual objects &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与视觉物体相关 &#124;'
- en: '| Separated audio only |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 仅分离音频 |'
- en: '|  | Senocak et al [[26](#bib.bib26)] |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | Senocak et al [[26](#bib.bib26)] |'
- en: '&#124; Focus on the primary &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关注主要 &#124;'
- en: '&#124; area by using attention &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过使用注意力机制的区域 &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Localized sound &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 局部化声音 &#124;'
- en: '&#124; source only &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅源 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Tian et al. [[27](#bib.bib27)] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | Tian et al. [[27](#bib.bib27)] |'
- en: '&#124; Joint modeling of auditory &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 听觉的联合建模 &#124;'
- en: '&#124; and visual modalities &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 及视觉模态 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Localized sound &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 定位的声音 &#124;'
- en: '&#124; source only &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅源 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Separate and Localize &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分离和定位 &#124;'
- en: '&#124; Objects’ Sounds &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 物体的声音 &#124;'
- en: '| Pu et al. [[19](#bib.bib19)] |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Pu 等人 [[19](#bib.bib19)] |'
- en: '&#124; Use low rank to extract the &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用低秩提取 &#124;'
- en: '&#124; sparsely correlated components &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稀疏相关的组件 &#124;'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Not for the in-the-wild &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不适用于实际环境 &#124;'
- en: '&#124; environment &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 环境 &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Zhao et al. [[28](#bib.bib28)] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhao 等人 [[28](#bib.bib28)] |'
- en: '&#124; Mix and separate a given audio; &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 混合和分离给定音频; &#124;'
- en: '&#124; without traditional supervision &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无传统监督 &#124;'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Motion information &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运动信息 &#124;'
- en: '&#124; is not considered &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未考虑 &#124;'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Zhao et al. [[29](#bib.bib29)] |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhao 等人 [[29](#bib.bib29)] |'
- en: '&#124; Introduce motion trajectory &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 介绍运动轨迹 &#124;'
- en: '&#124; and curriculum learning &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和课程学习 &#124;'
- en: '|'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Only suitable for synchronized &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅适用于同步 &#124;'
- en: '&#124; video and audio input &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频和音频输入 &#124;'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Rouditchenko et al. [[30](#bib.bib30)] |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | Rouditchenko 等人 [[30](#bib.bib30)] |'
- en: '&#124; Separation and localization use &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分离和定位使用 &#124;'
- en: '&#124; only one modality input &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅一种模态输入 &#124;'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Does not fully utilize &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未充分利用 &#124;'
- en: '&#124; temporal information &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间信息 &#124;'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Parekh et al. [[31](#bib.bib31)] |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | Parekh 等人 [[31](#bib.bib31)] |'
- en: '&#124; Weakly supervised learning &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 弱监督学习 &#124;'
- en: '&#124; via multiple-instance learning &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过多实例学习 &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Only a bounding box &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅限于边界框 &#124;'
- en: '&#124; proposed on the image &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在图像上提出 &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 2.2.2 Localization
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 定位
- en: Instead of only separating audio, can machines localize the sound source merely
    by observing sound and visual scene pairs as a human can? There is evidence both
    in physiology and psychology that sound localization of acoustic signals is strongly
    influenced by synchronicity of their visual signals [[22](#bib.bib22)]. The past
    efforts in this domain were limited to requiring specific devices or additional
    features. Izadinia et al. [[32](#bib.bib32)] proposed utilizing the velocity and
    acceleration of moving objects as visual features to assign sounds to them. Zunino
    et al. [[24](#bib.bib24)] presented a new hybrid device for sound and optical
    imaging that was primarily suitable for automatic monitoring.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 机器能否仅通过观察声音和视觉场景对来定位声音源，就像人类一样？生理学和心理学上都有证据表明，声学信号的声音定位受到其视觉信号同步性的强烈影响 [[22](#bib.bib22)]。过去在这一领域的努力局限于需要特定设备或附加特征。Izadinia
    等人 [[32](#bib.bib32)] 提出了利用移动物体的速度和加速度作为视觉特征来为声音分配对象。Zunino 等人 [[24](#bib.bib24)]
    提出了适合自动监测的声音和光学成像的新型混合设备。
- en: As the number of unlabeled videos on the Internet has been increasing dramatically,
    recent methods mainly focus on unsupervised learning. Additionally, modeling audio
    and visual modalities simultaneously tends to outperform independent modeling.
    Senocak et al. [[26](#bib.bib26)] learned to localize sound sources by merely
    watching and listening to videos. The relevant model mainly consisted of three
    networks, namely, sound and visual networks and an attention network trained via
    the distance ratio [[33](#bib.bib33)] unsupervised loss.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网中未标记视频数量的剧增，最近的方法主要集中在无监督学习上。此外，同时建模音频和视觉模态通常优于独立建模。Senocak 等人 [[26](#bib.bib26)]
    通过观看和听取视频学习了声音源定位。相关模型主要由三部分组成，即声音和视觉网络以及通过距离比 [[33](#bib.bib33)] 无监督损失训练的注意力网络。
- en: Attention mechanisms cause the model to focus on the primary area. They provide
    prior knowledge in a semisupervised setting. As a result, the network can be converted
    into a unified one that can learn better from data without additional annotations.
    To enable cross-modality localization, Tian et al. [[27](#bib.bib27)] proposed
    capturing the semantics of sound-emitting objects via the learned attention and
    leveraging temporal alignment to discover the correlations between the two modalities.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制使模型集中于主要区域。它们在半监督设置中提供先验知识。因此，网络可以转换为一个统一的网络，可以更好地从数据中学习，而无需额外的注释。为了实现跨模态定位，Tian
    等人 [[27](#bib.bib27)] 提出了通过学习的注意力捕捉声音发射对象的语义，并利用时间对齐来发现两种模态之间的相关性。
- en: 2.2.3 Simultaneous Separation and Localization
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 同时分离和定位
- en: 'Sound source separation and localization can be strongly associated with each
    other by assigning one modality’s information to another. Therefore, several researchers
    attempted to perform localization and separation simultaneously. Pu et al. [[19](#bib.bib19)]
    used a low-rank and sparse framework to model the background. The researchers
    extracted components with sparse correlations between the audio and visual modalities.
    However, the scenario of this method had a major limitation: it could only be
    applied to videos with a few sound-generating objects. Therefore, Zhao et al.
    [[28](#bib.bib28)] introduced a system called PixelPlayer that used a two-stream
    network and presented a mix-and-separate framework to train the entire network.
    In this framework, audio signals from two different videos were added to produce
    a mixed signal as input. The input was then fed into the network that was trained
    to separate the audio source signals based on the corresponding video frames.
    The two separated sound signals were treated as outputs. The system thus learned
    to separate individual sources without traditional supervision.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 声音源分离和定位可以通过将一种模态的信息分配给另一种模态来强烈关联。因此，一些研究者尝试同时进行定位和分离。Pu 等人 [[19](#bib.bib19)]
    使用了一个低秩和稀疏框架来建模背景。研究人员提取了音频和视觉模态之间具有稀疏相关性的组件。然而，这种方法的场景有一个主要限制：它只能应用于少数声音产生对象的视频。因此，赵等人
    [[28](#bib.bib28)] 引入了一个叫做 PixelPlayer 的系统，该系统使用了双流网络，并提出了一个混合和分离框架来训练整个网络。在这个框架中，将来自两个不同视频的音频信号加在一起以生成混合信号作为输入。然后将输入送入网络，该网络被训练以根据相应的视频帧分离音频源信号。两个分离的声音信号被视为输出。因此，该系统学会了在没有传统监督的情况下分离个体源。
- en: Instead of merely relying on image semantics while ignoring the temporal motion
    information in the video, Zhao et al. [[29](#bib.bib29)] subsequently proposed
    an end-to-end network called deep dense trajectory to learn the motion information
    for audio-visual sound separation. Furthermore, due to the lack of training samples,
    directly separating sound for a single class of instruments tend to lead to overfitting.
    Therefore, the authors proposed a curriculum strategy, starting by separating
    sounds from different instruments and proceeding to sounds from the same instrument.
    This gradual approach provided a good start for the network to converge better
    on the separation and localization tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 赵等人 [[29](#bib.bib29)] 随后提出了一种名为深度密集轨迹的端到端网络，以学习音视频声音分离的运动信息，而不仅仅依赖图像语义而忽视视频中的时间运动信息。此外，由于训练样本的缺乏，直接分离单一类别的乐器声音往往会导致过拟合。因此，作者提出了一种课程策略，从分离不同乐器的声音开始，然后过渡到分离同一乐器的声音。这种渐进的方法为网络在分离和定位任务上更好地收敛提供了良好的起点。
- en: The methods of previous studies [[19](#bib.bib19), [28](#bib.bib28), [29](#bib.bib29)]
    could only be applied to videos with synchronized audio. Hence, Rouditchenko et
    al. [[30](#bib.bib30)] tried to perform localization and separation tasks using
    only video frames or sound by disentangling concepts learned by neural networks.
    The researchers proposed an approach to produce sparse activations that could
    correspond to semantic categories in the input using the sigmoid activation function
    during the training stage and softmax activation during the fine-tuning stage.
    Afterwards, the researchers assigned these semantic categories to intermediate
    network feature channels using labels available in the training dataset. In other
    words, given a video frame or a sound, the approach used the category-to-feature-channel
    correspondence to select a specific type of source or object for separation or
    localization. Aiming to introduce weak labels to improve performance, Parekh et
    al. [[31](#bib.bib31)] designed an approach based on multiple-instance learning,
    a well-known strategy for weakly supervised learning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究方法 [[19](#bib.bib19), [28](#bib.bib28), [29](#bib.bib29)] 仅能应用于具有同步音频的视频。因此，Rouditchenko
    等人 [[30](#bib.bib30)] 尝试通过解缠神经网络学习的概念，使用仅视频帧或声音来执行定位和分离任务。研究人员提出了一种方法，通过在训练阶段使用
    sigmoid 激活函数和在微调阶段使用 softmax 激活函数来产生稀疏激活，这些激活可以对应于输入中的语义类别。随后，研究人员使用训练数据集中可用的标签将这些语义类别分配给中间网络特征通道。换句话说，给定一个视频帧或一个声音，该方法使用类别到特征通道的对应关系来选择特定类型的源或对象进行分离或定位。为提高性能，Parekh
    等人 [[31](#bib.bib31)] 设计了一种基于多实例学习的方案，这是一种著名的弱监督学习策略。
- en: 3 Audio-visual Correspondence Learning
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 音视频对应学习
- en: In this section, we introduce several studies that explored the global semantic
    relation between audio and visual modalities. We name this branch of research
    “audio-visual correspondence learning”; it consists of 1) the audio-visual matching
    task and 2) the audio-visual speech recognition task.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些探讨音频和视觉模态之间全球语义关系的研究。我们将这类研究称为“音频-视觉对应学习”；它包括 1) 音频-视觉匹配任务和 2) 音频-视觉语音识别任务。
- en: 3.1 Audio-visual Matching
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 音频-视觉匹配
- en: Biometric authentication, ranging from facial recognition to fingerprint and
    iris authentication, is a popular topic that has been researched over many years,
    while evidence shows that this system can be attacked maliciously. To detect such
    attacks, recent studies particularly focus on speech antispoofing measures.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 生物识别认证，从面部识别到指纹和虹膜认证，是一个热门话题，经过多年研究，已有证据表明该系统可能受到恶意攻击。为了检测这种攻击，近期研究特别关注语音反欺骗措施。
- en: Sriskandaraja et al. [[34](#bib.bib34)] proposed a network based on a Siamese
    architecture to evaluate the similarities between pairs of speech samples. [[35](#bib.bib35)]
    presented a two-stream network, where the first network was a Bayesian neural
    network assumed to be overfitting, and the second network was a CNN used to improve
    generalization. Alanis et al. [[36](#bib.bib36)] further incorporated LightCNN
    [[37](#bib.bib37)] and a gated recurrent unit (GRU) [[38](#bib.bib38)] as a robust
    feature extractor to represent speech signals in utterance-level analysis to improve
    performance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Sriskandaraja 等人 [[34](#bib.bib34)] 提出了基于 Siamese 结构的网络来评估语音样本对之间的相似性。[[35](#bib.bib35)]
    提出了一个双流网络，其中第一个网络是一个被假设为过拟合的贝叶斯神经网络，第二个网络是一个 CNN，用于提高泛化能力。Alanis 等人 [[36](#bib.bib36)]
    进一步结合了 LightCNN [[37](#bib.bib37)] 和一个门控递归单元 (GRU) [[38](#bib.bib38)] 作为一个强大的特征提取器，以在发音级别分析中表示语音信号，从而提高性能。
- en: We note that cross-modality matching is a special form of such authentication
    that has recently been extensively studied. It attempts to learn the similarity
    between pairs. We divide this matching task into fine-grained voice-face matching
    and coarse-grained audio-image retrieval.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，跨模态匹配是这种认证的一种特殊形式，最近得到了广泛研究。它试图学习对之间的相似性。我们将这个匹配任务分为细粒度的语音-面部匹配和粗粒度的音频-图像检索。
- en: 3.1.1 Voice-Facial Matching
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 语音-面部匹配
- en: 'Given facial images of different identities and the corresponding audio sequences,
    voice-facial matching aims to identify the face that the audio belongs to (the
    V2F task) or vice versa (the F2V task), as shown in Fig. [3](#S3.F3 "Figure 3
    ‣ 3.1.1 Voice-Facial Matching ‣ 3.1 Audio-visual Matching ‣ 3 Audio-visual Correspondence
    Learning ‣ Deep Audio-Visual Learning: A Survey"). The key point is finding the
    embedding between audio and visual modalities. Nagrani et al. [[39](#bib.bib39)]
    proposed using three networks to address the audio-visual matching problem: a
    static network, a dynamic network, and an N-way network. The static network and
    the dynamic network could only handle the problem with a specific number of images
    and audio tracks. The difference was that the dynamic network added to each image
    temporal information such as the optical flow or a 3D convolution [[40](#bib.bib40),
    [41](#bib.bib41)]. Based on the static network, the authors increased the number
    of samples to form an N-way network that was able to solve the $N:1$ identification
    problem.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 给定不同身份的面部图像和对应的音频序列，语音-面部匹配旨在识别音频所属的面孔（V2F 任务）或反之（F2V 任务），如图 [3](#S3.F3 "图 3
    ‣ 3.1.1 语音-面部匹配 ‣ 3.1 音频-视觉匹配 ‣ 3 音频-视觉对应学习 ‣ 深度音频-视觉学习：综述") 所示。关键点在于找到音频和视觉模态之间的嵌入。Nagrani
    等人 [[39](#bib.bib39)] 提出了使用三个网络来解决音频-视觉匹配问题：一个静态网络、一个动态网络和一个 N-路网络。静态网络和动态网络只能处理特定数量的图像和音频轨道。不同之处在于动态网络为每张图像添加了时间信息，例如光流或
    3D 卷积 [[40](#bib.bib40), [41](#bib.bib41)]。基于静态网络，作者增加了样本数量形成了一个 N-路网络，该网络能够解决
    $N:1$ 识别问题。
- en: However, the correlation between the two modalities was not fully utilized in
    the above method. Therefore, Wen et al. [[42](#bib.bib42)] proposed a disjoint
    mapping network (DIMNets) to fully use the covariates (e.g., gender and nationality)
     [[43](#bib.bib43), [44](#bib.bib44)] to bridge the relation between voice and
    face information. The intuitive assumption was that for a given voice and face
    pair, the more covariates were shared between the two modalities, the higher the
    probability of being a match. The main drawback of this framework was that a large
    number of covariates led to high data costs. Therefore, Hoover et al. [[45](#bib.bib45)]
    suggested a low-cost but robust approach of detection and clustering on audio
    clips and facial images. For the audio stream, the researchers applied a neural
    network model to detect speech for clustering and subsequently assigned a frame
    cluster to the given audio cluster according to the majority principle. Doing
    so required a small amount of data for pretraining.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述方法中两种模态之间的相关性并没有得到充分利用。因此，Wen等人[[42](#bib.bib42)]提出了一种离散映射网络（DIMNets），以充分利用协变量（例如性别和国籍）[[43](#bib.bib43)，[44](#bib.bib44)]来桥接语音和面部信息之间的关系。直观的假设是，对于给定的语音和面部配对，两个模态之间共享的协变量越多，匹配的概率就越高。这个框架的主要缺点是大量的协变量导致了高数据成本。因此，Hoover等人[[45](#bib.bib45)]提出了一种低成本但稳健的检测和聚类方法，用于音频片段和面部图像。对于音频流，研究人员应用了神经网络模型来检测语音进行聚类，然后根据多数原则将一个帧簇分配给给定的音频簇。这样做只需要少量的数据进行预训练。
- en: To further enhance the robustness of the network, Chung et al. [[46](#bib.bib46)]
    proposed an improved two-stream training method that increased the number of negative
    samples to improve the error-tolerance rate of the network. The cross-modality
    matching task, which is essentially a classification task, allows for wide-ranging
    applications of the triplet loss. However, it is fragile in the case of multiple
    samples. To overcome this defect, Wang et al. [[47](#bib.bib47)] proposed a novel
    loss function to expand the triplet loss for multiple samples and a new elastic
    network (called Emnet) based on a two-stream architecture that can tolerate a
    variable number of inputs to increase the flexibility of the network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步增强网络的鲁棒性，Chung等人[[46](#bib.bib46)]提出了一种改进的双流训练方法，通过增加负样本的数量来提高网络的容错率。跨模态匹配任务，本质上是一个分类任务，允许三元组损失的广泛应用。然而，在处理多个样本时，它的表现较为脆弱。为克服这一缺陷，Wang等人[[47](#bib.bib47)]提出了一种新颖的损失函数，用于扩展三元组损失以处理多个样本，并基于双流结构提出了一种新的弹性网络（称为Emnet），该网络可以容忍可变数量的输入，以提高网络的灵活性。
- en: '![Refer to caption](img/3260d132e0e28b3c2bb182209a02b8a2.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3260d132e0e28b3c2bb182209a02b8a2.png)'
- en: 'Figure 3: Demonstration of Audio-to-Image retrieval (a) and Image-to-Audio
    retrieval (b).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：音频到图像检索（a）和图像到音频检索（b）的演示。
- en: 3.1.2 Audio-image Retrieval
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 音频-图像检索
- en: The cross-modality retrieval task aims to discover the relationship between
    different modalities. Given one sample in the source modality, the proposed model
    can retrieve the corresponding sample with the same identity in the target modality.
    For audio-image retrieval as an example, the aim is to return a relevant piano
    sound, given a picture of a girl playing a piano. Compared with the previously
    considered voice and face matching, this task is more coarse-grained.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态检索任务旨在发现不同模态之间的关系。给定一个源模态中的样本，提出的模型可以检索目标模态中具有相同身份的对应样本。例如，对于音频-图像检索，目标是返回一个相关的钢琴声音，给定一张女孩弹钢琴的图片。与之前考虑的语音和面部匹配相比，这项任务更为粗略。
- en: Unlike other retrieval tasks such as the text-image task [[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50)] or the sound-text task [[51](#bib.bib51)],
    the audio-visual retrieval task mainly focuses on subspace learning. Didac et
    al. [[52](#bib.bib52)] proposed a new joint embedding model that mapped two modalities
    into a joint embedding space, and then directly calculated the Euclidean distance
    between them. The authors leveraged cosine similarity to ensure that the two modalities
    in the same space were as close as possible while not overlapping. Note that the
    designed architecture would have a large number of parameters due to the existence
    of a large number of fully connected layers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他检索任务如文本-图像任务 [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)] 或声音-文本任务
    [[51](#bib.bib51)] 不同，视听检索任务主要关注子空间学习。Didac 等人 [[52](#bib.bib52)] 提出了一个新的联合嵌入模型，将两个模态映射到一个联合嵌入空间中，然后直接计算它们之间的欧几里得距离。作者利用余弦相似度确保同一空间中的两个模态尽可能接近而不重叠。请注意，由于存在大量的全连接层，设计的架构会有大量的参数。
- en: Hong et al. [[53](#bib.bib53)] proposed a joint embedding model that relied
    on pretrained networks and used CNNs to replace fully connected layers to reduce
    the number of parameters to some extent. The video and music were fed to the pretrained
    network and then aggregated, followed by a two-stream network trained via the
    intermodal ranking loss. In addition, to preserve modality-specific characteristics,
    the researchers proposed a novel soft intramodal structure loss. However, the
    resulting network was very complex and difficult to apply in practice. To solve
    this problem, Arsha et al. [[54](#bib.bib54)] proposed a cross-modality self-supervised
    method to learn the embedding of audio and visual information from a video and
    significantly reduced the complexity of the network. For sample selection, the
    authors designed a novel curriculum learning schedule to further improve performance.
    In addition, the resulting joint embedding could be efficiently and effectively
    applied in practical applications.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Hong 等人 [[53](#bib.bib53)] 提出了一个依赖预训练网络的联合嵌入模型，该模型使用 CNN 替代全连接层，从而在一定程度上减少了参数数量。视频和音乐被输入到预训练网络中，然后进行聚合，接着通过跨模态排序损失训练了一个双流网络。此外，为了保留模态特定的特征，研究人员提出了一种新颖的软内模态结构损失。然而，结果网络非常复杂，实践中难以应用。为了解决这个问题，Arsha
    等人 [[54](#bib.bib54)] 提出了一个跨模态自监督方法，以学习视频中音频和视觉信息的嵌入，并显著降低了网络的复杂性。为了样本选择，作者设计了一种新颖的课程学习计划，以进一步提高性能。此外，得到的联合嵌入可以在实际应用中高效、有效地应用。
- en: 'Table 2: Summary of audio-visual correspondence learning.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：视听对应学习总结。
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 思路与优势 | 劣势 |'
- en: '|  | Nagrani et al.  [[39](#bib.bib39)] |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | Nagrani 等人  [[39](#bib.bib39)] |'
- en: '&#124; The method is novel and &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法新颖且 &#124;'
- en: '&#124; incorporates dynamic information &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包含动态信息 &#124;'
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; As the sample size increases, &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 随着样本大小增加，&#124;'
- en: '&#124; the accuracy decreases excessively &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确度过度下降 &#124;'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Wen et al.  [[42](#bib.bib42)]. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | Wen 等人  [[42](#bib.bib42)]。 |'
- en: '&#124; The correlation between &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关性 &#124;'
- en: '&#124; modes is utilized &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式被利用 &#124;'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataset acquisition is difficult &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集获取困难 &#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Voice-Face Matching | Wang et al. [[55](#bib.bib55)] |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 声音-面部匹配 | Wang 等人 [[55](#bib.bib55)] |'
- en: '&#124; Can deal with multiple samples &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 能处理多个样本 &#124;'
- en: '&#124; Can change the size of input &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可以改变输入的大小 &#124;'
- en: '|'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Static image only; &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅静态图像；&#124;'
- en: '&#124; model complexity &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型复杂性 &#124;'
- en: '|'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Hoover et al. [[45](#bib.bib45)] |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | Hoover 等人 [[45](#bib.bib45)] |'
- en: '&#124; Easy to implement &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 易于实现 &#124;'
- en: '&#124; Robust &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲁棒性 &#124;'
- en: '&#124; Efficient &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高效 &#124;'
- en: '| Cannot handle large-scale data |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 不能处理大规模数据 |'
- en: '|  | Hong et al. [[53](#bib.bib53)] |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | Hong 等人 [[53](#bib.bib53)] |'
- en: '&#124; Preserve modality- &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 保留模态- &#124;'
- en: '&#124; specific characteristics &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具体特征 &#124;'
- en: '&#124; Soft intra-modality structure loss &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软内模态结构损失 &#124;'
- en: '| Complex network |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 复杂网络 |'
- en: '| Audio-visual retrieval | Didac et al. [[52](#bib.bib52)] |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 视听检索 | Didac 等人 [[52](#bib.bib52)] |'
- en: '&#124; Metric Learning &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 度量学习 &#124;'
- en: '&#124; Using fewer parameters &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用更少的参数 &#124;'
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Only two faces &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅两个面孔 &#124;'
- en: '&#124; Static images &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 静态图像 &#124;'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Arsha et al. [[54](#bib.bib54)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | Arsha 等人 [[54](#bib.bib54)] |'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '&#124; Applied value &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用价值 &#124;'
- en: '&#124; Low data cost &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低数据成本 &#124;'
- en: '| Low accuracy for multiple samples |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 多样本准确率低 |'
- en: '|  | Petridis et al.  [[56](#bib.bib56)] |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | Petridis 等 [[56](#bib.bib56)] |'
- en: '&#124; Simultaneously obtain &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 同时获取 &#124;'
- en: '&#124; feature and classification &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征和分类 &#124;'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lack of audio information &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缺乏音频信息 &#124;'
- en: '|'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Wand et al.  [[57](#bib.bib57)]. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | Wand 等 [[57](#bib.bib57)]。 |'
- en: '&#124; LSTM &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSTM &#124;'
- en: '&#124; Simple method &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 简单方法 &#124;'
- en: '|'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Word-level &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单词级别 &#124;'
- en: '|'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Audio-visual Speech Recognition | Shillingford et al. [[58](#bib.bib58)]
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 视听语音识别 | Shillingford 等 [[58](#bib.bib58)] |'
- en: '&#124; Sentence-level &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 句子级别 &#124;'
- en: '&#124; LipNet &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LipNet &#124;'
- en: '&#124; CTC loss &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CTC 损失 &#124;'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; No audio information &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无音频信息 &#124;'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Chung et al. [[59](#bib.bib59)] |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | Chung 等 [[59](#bib.bib59)] |'
- en: '&#124; Audio and visual information &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 音频和视觉信息 &#124;'
- en: '&#124; LRS dataset &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRS 数据集 &#124;'
- en: '| Noise is not considered |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 噪声未被考虑 |'
- en: '|  | Trigeorgis et al. [[60](#bib.bib60)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | Trigeorgis 等 [[60](#bib.bib60)] |'
- en: '&#124; Audio information &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 音频信息 &#124;'
- en: '&#124; The algorithm is robust &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 算法稳健 &#124;'
- en: '| Noise is not considered |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 噪声未被考虑 |'
- en: '|  | Afouras et al. [[61](#bib.bib61)] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | Afouras 等 [[61](#bib.bib61)] |'
- en: '&#124; Study noise in audio &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究音频中的噪声 &#124;'
- en: '&#124; LRS2-BBC Dataset &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRS2-BBC 数据集 &#124;'
- en: '| Complex network |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 复杂网络 |'
- en: '![Refer to caption](img/ab4ed84668338c061d7d8246fbd7913a.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/ab4ed84668338c061d7d8246fbd7913a.png)'
- en: 'Figure 4: Demonstration of audio-visual speech recognition.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 视听语音识别演示。'
- en: 3.2 Audio-visual Speech Recognition
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 视听语音识别
- en: 'The recognition of content of a given speech clip has been studied for many
    years, yet despite great achievements, researchers are still aiming for satisfactory
    performance in challenging scenarios. Due to the correlation between audio and
    vision, combining these two modalities tends to offer more prior information.
    For example, one can predict the scene where the conversation took place, which
    provides a strong prior for speech recognition, as shown in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1.2 Audio-image Retrieval ‣ 3.1 Audio-visual Matching ‣ 3 Audio-visual Correspondence
    Learning ‣ Deep Audio-Visual Learning: A Survey").'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定语音片段内容的识别已经研究多年，尽管取得了重大成就，研究人员仍在追求在挑战性场景下的满意表现。由于音频与视觉之间的相关性，结合这两种模态通常能提供更多的先验信息。例如，可以预测对话发生的场景，这为语音识别提供了强有力的先验，如图[4](#S3.F4
    "图 4 ‣ 3.1.2 音频-图像检索 ‣ 3.1 视听匹配 ‣ 3 视听对应学习 ‣ 深度视听学习：综述")所示。
- en: 'Earlier efforts on audio-visual fusion models usually consisted of two steps:
    1) extracting features from the image and audio signals and 2) combining the features
    for joint classification  [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64)].
    Later, taking advantage of deep learning, feature extraction was replaced with
    a neural network encoder [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)].
    Several recently studies have shown a tendency to use an end-to-end approach to
    visual speech recognition. These studies can be mainly divided into two groups.
    They either leverage the fully connected layers and LSTM to extract features and
    model the temporal information [[56](#bib.bib56), [57](#bib.bib57)] or use a 3D
    convolutional layer followed by a combination of CNNs and LSTMs [[58](#bib.bib58),
    [68](#bib.bib68)]. Instead of adopting a two-step strategy, Petridis et al. [[56](#bib.bib56)]
    introduced an audio-visual fusion model that simultaneously extracted features
    directly from pixels and spectrograms and performed classification of speech and
    nonlinguistic vocalizations. Furthermore, temporal information was extracted by
    a bidirectional LSTM. Although this method could perform feature extraction and
    classification at the same time, it still followed the two-step strategy.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 早期对视听融合模型的研究通常包括两个步骤：1) 从图像和音频信号中提取特征，2) 将特征组合用于联合分类 [[62](#bib.bib62), [63](#bib.bib63),
    [64](#bib.bib64)]。后来，利用深度学习，特征提取被神经网络编码器所取代 [[65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67)]。最近的几项研究显示出使用端到端方法进行视觉语音识别的趋势。这些研究主要可以分为两组。它们要么利用全连接层和 LSTM 提取特征并建模时间信息
    [[56](#bib.bib56), [57](#bib.bib57)]，要么使用 3D 卷积层，随后结合 CNN 和 LSTM [[58](#bib.bib58),
    [68](#bib.bib68)]。Petridis 等人 [[56](#bib.bib56)] 引入了一种视听融合模型，该模型同时从像素和光谱图中提取特征，并对语音和非语言发声进行分类。此外，时间信息通过双向
    LSTM 提取。尽管这种方法能够同时进行特征提取和分类，但它仍然遵循两步策略。
- en: To this end, Wand et al. [[57](#bib.bib57)] presented a word-level lip-reading
    system using LSTM. In contrast to previous methods, Assael et.al [[58](#bib.bib58)]
    proposed a novel end-to-end LipNet model based on sentence-level sequence prediction,
    which consisted of spatial-temporal convolutions, a recurrent network and a model
    trained via the connectionist temporal classification (CTC) loss. Experiments
    showed that lip-reading outperformed the two-step strategy.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，Wand 等人 [[57](#bib.bib57)] 提出了一个使用 LSTM 的词级读唇系统。与之前的方法相比，Assael 等人 [[58](#bib.bib58)]
    提出了一个基于句子级序列预测的全新的端到端 LipNet 模型，该模型包含空间-时间卷积、递归网络以及通过连接主义时间分类（CTC）损失训练的模型。实验表明，读唇技术优于两步策略。
- en: However, the limited information in the visual modality may lead to a performance
    bottleneck. To combine both audio and visual information for various scenes, especially
    in noisy conditions, Trigeorgis et al.  [[60](#bib.bib60)] introduced an end-to-end
    model to obtain a ‘context-aware’ feature from the raw temporal representation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，视觉模态中的信息有限可能导致性能瓶颈。为了结合音频和视觉信息以适应各种场景，特别是在嘈杂条件下，Trigeorgis 等人 [[60](#bib.bib60)]
    引入了一个端到端模型，从原始时间表示中获取“上下文感知”特征。
- en: 'Chung et al. [[59](#bib.bib59)] presented a “Watch, Listen, Attend, and Spell”
    (WLAS) network to explain the influence of audio on the recognition task. The
    model took advantage of the dual attention mechanism and could operate on a single
    or combined modality. To speed up the training and avoid overfitting, the researchers
    also used a curriculum learning strategy. To analyze an “in-the-wild” dataset,
    Cui et al. [[69](#bib.bib69)] proposed another model based on residual networks
    and a bidirectional GRU [[38](#bib.bib38)]. However, the authors did not take
    the ubiquitous noise in the audio into account. To solve this problem, Afouras
    et al. [[61](#bib.bib61)] proposed a model for performing speech recognition tasks.
    The researchers compared two common sequence prediction types: connectionist temporal
    classification and sequence-to-sequence (seq2seq) methods in their models. In
    the experiment, they observed that the model using seq2seq could perform better
    according to word error rate (WER) when it was only provided with silent videos.
    For pure-audio or audio-visual tasks, the two methods behaved similarly. In a
    noisy environment, the performance of the seq2seq model was worse than that of
    the corresponding CTC model, suggesting that the CTC model could better handle
    background noises.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Chung 等人[[59](#bib.bib59)] 提出了一个“观察、听取、关注和拼写”（WLAS）网络，以解释音频对识别任务的影响。该模型利用了双重注意机制，能够在单一或组合模态下运行。为了加快训练速度并避免过拟合，研究人员还采用了课程学习策略。为了分析“实际环境”数据集，Cui
    等人[[69](#bib.bib69)] 提出了另一种基于残差网络和双向 GRU[[38](#bib.bib38)] 的模型。然而，作者没有考虑到音频中的普遍噪声。为了解决这个问题，Afouras
    等人[[61](#bib.bib61)] 提出了一个用于执行语音识别任务的模型。研究人员在他们的模型中比较了两种常见的序列预测类型：连接时序分类和序列到序列（seq2seq）方法。在实验中，他们观察到，当模型仅提供静音视频时，使用
    seq2seq 的模型在词错误率（WER）上表现更好。对于纯音频或音频视觉任务，这两种方法的表现相似。在嘈杂环境中，seq2seq 模型的表现不如相应的 CTC
    模型，这表明 CTC 模型能够更好地处理背景噪声。
- en: 4 Audio and Visual Generation
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 音频和视觉生成
- en: The previously introduced retrieval task shows that the trained model is able
    to find the most similar audio or visual counterpart. While humans can imagine
    the scenes corresponding to sounds, and vice versa, researchers have tried to
    endow machines with this kind of imagination for many years. Following the invention
    and advances of generative adversarial networks (GANs) [[70](#bib.bib70)], image
    or video generation has emerged as a topic. It involves several subtasks, including
    generating images or video from a potential space [[71](#bib.bib71)], cross-modality
    generation [[72](#bib.bib72), [73](#bib.bib73)], etc. These applications are also
    relevant to other tasks, e.g., domain adaptation [[74](#bib.bib74), [75](#bib.bib75)].
    Due to the difference between audio and visual modalities, the potential correlation
    between them is nonetheless difficult for machines to discover. Generating sound
    from a visual signal or vice versa, therefore, becomes a challenging task.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 之前介绍的检索任务表明，训练好的模型能够找到最相似的音频或视觉对应物。虽然人类可以想象与声音对应的场景，反之亦然，研究人员多年来一直尝试赋予机器这种想象力。随着生成对抗网络（GANs）[[70](#bib.bib70)]
    的发明和进展，图像或视频生成成为一个话题。它涉及几个子任务，包括从潜在空间[[71](#bib.bib71)] 生成图像或视频、跨模态生成[[72](#bib.bib72),
    [73](#bib.bib73)] 等。这些应用也与其他任务相关，如领域适应[[74](#bib.bib74), [75](#bib.bib75)]。由于音频和视觉模态之间的差异，它们之间的潜在关联仍然难以被机器发现。因此，从视觉信号生成声音或反之成为一个具有挑战性的任务。
- en: 'In this section, we will mainly review the recent development of audio and
    visual generation, i.e., generating audio from visual signals or vice versa. Visual
    signals here mainly refer to images, motion dynamics, and videos. The subsection
    ‘Visual to Audio’ mainly focuses on recovering the speech from the video of the
    lip area (Fig. [5](#S4.F5 "Figure 5 ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual
    Learning: A Survey") (a)) or generating sounds that may occur in the given scenes
    (Fig. [5](#S4.F5 "Figure 5 ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual
    Learning: A Survey") (a)). In contrast, the discussion of ‘Audio to Visual’ generation
    (Fig. [5](#S4.F5 "Figure 5 ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual
    Learning: A Survey") (b)) will examine generating images from a given audio (Fig.
    [6](#S4.F6 "Figure 6 ‣ 4.2.1 Audio to Image ‣ 4.2 Audio to Vision ‣ 4 Audio and
    Visual Generation ‣ Deep Audio-Visual Learning: A Survey") (a)), body motion generation
    (Fig. [6](#S4.F6 "Figure 6 ‣ 4.2.1 Audio to Image ‣ 4.2 Audio to Vision ‣ 4 Audio
    and Visual Generation ‣ Deep Audio-Visual Learning: A Survey") (b)), and talking
    face generation (Fig. [6](#S4.F6 "Figure 6 ‣ 4.2.1 Audio to Image ‣ 4.2 Audio
    to Vision ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual Learning: A Survey")
    (c)).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将主要回顾音频和视觉生成的最新进展，即从视觉信号生成音频或反之亦然。这里的视觉信号主要指图像、运动动态和视频。子节“视觉转音频”主要关注从唇部区域的视频中恢复语音（图
    [5](#S4.F5 "图 5 ‣ 4 音频与视觉生成 ‣ 深度音频-视觉学习：综述") (a)）或生成可能出现在给定场景中的声音（图 [5](#S4.F5
    "图 5 ‣ 4 音频与视觉生成 ‣ 深度音频-视觉学习：综述") (a)）。相比之下，“音频转视觉”生成的讨论（图 [5](#S4.F5 "图 5 ‣ 4
    音频与视觉生成 ‣ 深度音频-视觉学习：综述") (b)）将审视从给定音频生成图像（图 [6](#S4.F6 "图 6 ‣ 4.2.1 音频到图像 ‣ 4.2
    音频到视觉 ‣ 4 音频与视觉生成 ‣ 深度音频-视觉学习：综述") (a)），身体运动生成（图 [6](#S4.F6 "图 6 ‣ 4.2.1 音频到图像
    ‣ 4.2 音频到视觉 ‣ 4 音频与视觉生成 ‣ 深度音频-视觉学习：综述") (b)），以及说话脸部生成（图 [6](#S4.F6 "图 6 ‣ 4.2.1
    音频到图像 ‣ 4.2 音频到视觉 ‣ 4 音频与视觉生成 ‣ 深度音频-视觉学习：综述") (c)）。
- en: '![Refer to caption](img/938f50f464ef70c47cafaddbd9011a7b.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/938f50f464ef70c47cafaddbd9011a7b.png)'
- en: (a) Demonstration of generating speech from lip sequences
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 从唇部序列生成语音演示
- en: '![Refer to caption](img/12fa0f4eba6b9b960efb3d843aec89f7.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/12fa0f4eba6b9b960efb3d843aec89f7.png)'
- en: (b) Demonstration of video-to-audio generation
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 视频到音频生成演示
- en: 'Figure 5: Demonstration of visual-to-audio generation.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 视觉到音频生成演示。'
- en: 'Table 3: Summary of recent approaches to video-to-audio generation.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 最近的视频转音频生成方法汇总。'
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 方法 | 想法与优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Cornu et al. [[76](#bib.bib76)] |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | Cornu 等人 [[76](#bib.bib76)] |'
- en: '&#124; Reconstruct intelligible &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建可理解的 &#124;'
- en: '&#124; speech only from &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅从语音中 &#124;'
- en: '&#124; visual speech features &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉语音特征 &#124;'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Applied to limited scenarios &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用于有限场景 &#124;'
- en: '|'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lip sequence &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 唇部序列 &#124;'
- en: '&#124; to Speech &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转为语音 &#124;'
- en: '| Ephrat et al. [[77](#bib.bib77)] |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Ephrat 等人 [[77](#bib.bib77)] |'
- en: '&#124; Compute optical flow &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算光流 &#124;'
- en: '&#124; between frames &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 帧间 &#124;'
- en: '| Applied to limited scenarios |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 应用于有限场景 |'
- en: '|  | Cornu et al. [[78](#bib.bib78)] |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | Cornu 等人 [[78](#bib.bib78)] |'
- en: '&#124; Reconstruct speech using &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用 &#124; 重建语音'
- en: '&#124; a classification approach &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类方法 &#124;'
- en: '&#124; combined with feature-level &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合特征层面 &#124;'
- en: '&#124; temporal information &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间信息 &#124;'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cannot apply to real-time &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不能应用于实时 &#124;'
- en: '&#124; conversational speech &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对话语音 &#124;'
- en: '|'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Davis et al. [[79](#bib.bib79)] |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | Davis 等人 [[79](#bib.bib79)] |'
- en: '&#124; Recover real-world audio by &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124; 恢复真实世界的音频'
- en: '&#124; capturing vibrations of objects &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉物体的振动 &#124;'
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Requires a specific device; &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要特定设备; &#124;'
- en: '&#124; can only be applied to &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅适用于 &#124;'
- en: '&#124; soft objects &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软物体 &#124;'
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Owens et al. [[80](#bib.bib80)] |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | Owens 等人 [[80](#bib.bib80)] |'
- en: '&#124; Use LSTM to capture &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用 LSTM 捕捉 &#124;'
- en: '&#124; the relation between material &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 材料之间的关系 &#124;'
- en: '&#124; and motion &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和运动 &#124;'
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; For a lab-controlled &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适用于实验室控制的 &#124;'
- en: '&#124; environment only &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅环境 &#124;'
- en: '|'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; General Video &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一般视频 &#124;'
- en: '&#124; to Audio &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转为音频 &#124;'
- en: '| Zhou et al. [[81](#bib.bib81)] |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Zhou 等人 [[81](#bib.bib81)] |'
- en: '&#124; Leverage a hierarchical &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用层次化 &#124;'
- en: '&#124; RNN to generate &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN生成&#124;'
- en: '&#124; in-the-wild sounds &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然声音&#124;'
- en: '| Monophonic audio only |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 单声道音频仅 |'
- en: '|  | Morgado et al. [[8](#bib.bib8)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | Morgado等人[[8](#bib.bib8)] |'
- en: '&#124; Localize and &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本地化和&#124;'
- en: '&#124; separate sounds to &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分离声音以&#124;'
- en: '&#124; generate spatial audio &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成空间音频&#124;'
- en: '&#124; from 360^∘ video &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从360^∘视频&#124;'
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fails sometimes; &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有时会失败; &#124;'
- en: '&#124; 360^∘ video required &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 360^∘视频要求&#124;'
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 4.1 Vision-to-Audio Generation
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 视觉到音频生成
- en: 'Many methods have been explored to extract audio information from visual information,
    including predicting sounds from visually observed vibrations and generating audio
    via a video signal. We divide the visual-to-audio generation tasks into two categories:
    generating speech from lip video and synthesizing sounds from general videos without
    scene limitations.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 已有许多方法被探讨以从视觉信息中提取音频信息，包括从视觉观察到的振动预测声音和通过视频信号生成音频。我们将视觉到音频生成任务分为两类：从唇部视频生成语音和从没有场景限制的一般视频中合成声音。
- en: 4.1.1 Lip Sequence to Speech
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 唇部序列到语音
- en: There is a natural relationship between speech and lips. Separately from understanding
    the content of speech by observing lips (lip-reading), several studies have tried
    to reconstruct speech by observing lips. Cornu et al. [[76](#bib.bib76)] attempted
    to predict the spectral envelope from visual features, combining it with artificial
    excitation signals, and to synthesize audio signals in a speech production model.
    Ephrat et al. [[82](#bib.bib82)] proposed an end-to-end model based on a CNN to
    generate audio features for each silent video frame based on its adjacent frames.
    The waveform was therefore reconstructed based on the learned features to produce
    understandable speech.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 语音与嘴唇之间存在自然的关系。除了通过观察嘴唇来理解语音内容（唇读），还有几项研究尝试通过观察嘴唇重建语音。Cornu等人[[76](#bib.bib76)]尝试从视觉特征预测谱包络，并将其与人工激励信号结合，以在语音生成模型中合成音频信号。Ephrat等人[[82](#bib.bib82)]提出了一种基于CNN的端到端模型，通过相邻帧生成每个静默视频帧的音频特征。因此，基于学习到的特征重建波形以生成可理解的语音。
- en: Using temporal information to improve speech reconstruction has been extensively
    explored. Ephrat et al. [[77](#bib.bib77)] proposed leveraging the optical flow
    to capture the temporal motion at the same time. Cornu et al. [[78](#bib.bib78)]
    leveraged recurrent neural networks to incorporate temporal information into the
    prediction.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 利用时间信息改善语音重建已被广泛探讨。Ephrat等人[[77](#bib.bib77)]提出利用光流同时捕捉时间运动。Cornu等人[[78](#bib.bib78)]利用递归神经网络将时间信息纳入预测中。
- en: 4.1.2 General Video to Audio
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 一般视频到音频
- en: When a sound hits the surfaces of some small objects, the latter will vibrate
    slightly. Therefore, Davis et al. [[79](#bib.bib79)] utilized this specific feature
    to recover the sound from vibrations observed passively by a high-speed camera.
    Note that it should be easily for suitable objects to vibrate, which is the case
    for a glass of water, a pot of plants, or a box of napkins. We argue that this
    work is similar to the previously introduced speech reconstruction studies [[76](#bib.bib76),
    [82](#bib.bib82), [77](#bib.bib77), [78](#bib.bib78)] since all of them use the
    relation between visual and sound context. In speech reconstruction, the visual
    part concentrates more on lip movement, while in this work, it focuses on small
    vibrations.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当声音撞击到一些小物体的表面时，后者会轻微振动。因此，Davis等人[[79](#bib.bib79)]利用这一特性从高速摄像机被动观察到的振动中恢复声音。注意，对于适合的物体来说，它们应该容易振动，这对于一杯水、一盆植物或一盒纸巾来说都是如此。我们认为这项工作类似于之前介绍的语音重建研究[[76](#bib.bib76),
    [82](#bib.bib82), [77](#bib.bib77), [78](#bib.bib78)]，因为它们都利用了视觉和声音上下文之间的关系。在语音重建中，视觉部分更多关注于唇部运动，而在这项工作中，重点是小的振动。
- en: Owens et al. [[80](#bib.bib80)] observed that when different materials were
    hit or scratched, they emitted a variety of sounds. Thus, the researchers introduced
    a model that learned to synthesize sound from a video in which objects made of
    different materials were hit with a drumstick at different angles and velocities.
    The researchers demonstrated that their model could not only identify different
    sounds originating from different materials but also learn the pattern of interaction
    with objects (different actions applied to objects result in different sounds).
    The model leveraged an RNN to extract sound features from video frames and subsequently
    generated waveforms through an instance-based synthesis process.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Owens 等人 [[80](#bib.bib80)] 观察到，当不同材料被撞击或划伤时，它们会发出各种声音。因此，研究人员引入了一个模型，该模型学习从视频中合成声音，这些视频中的物体由不同材料制成，并且以不同的角度和速度用鼓槌击打。研究人员展示了他们的模型不仅能够识别源自不同材料的不同声音，还能学习与物体的交互模式（对物体施加的不同动作会产生不同的声音）。该模型利用
    RNN 从视频帧中提取声音特征，并通过基于实例的合成过程生成波形。
- en: Although Owens et al. [[80](#bib.bib80)] could generate sound from various materials,
    the authors’ approach still could not be applied to real-life applications since
    the network was trained by videos shot in a lab environment under strict constraints.
    To improve the result and generate sounds from in-the-wild videos, Zhou et al.
    [[81](#bib.bib81)] designed an end-to-end model. It was structured as a video
    encoder and a sound generator to learn the mapping from video frames to sounds.
    Afterwards, the network leveraged a hierarchical RNN [[83](#bib.bib83)] for sound
    generation. Specifically, the authors trained a model to directly predict raw
    audio signals (waveform samples) from input videos. They demonstrated that this
    model could learn the correlation between sound and visual input for various scenes
    and object interactions.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Owens 等人 [[80](#bib.bib80)] 能够生成来自各种材料的声音，但作者的方法仍然无法应用于现实生活中的应用，因为网络是在实验室环境下拍摄的视频中训练的，受到了严格的限制。为了改善结果并从实际环境中的视频中生成声音，Zhou
    等人 [[81](#bib.bib81)] 设计了一个端到端的模型。该模型结构为视频编码器和声音生成器，以学习从视频帧到声音的映射。之后，网络利用层次化 RNN
    [[83](#bib.bib83)] 进行声音生成。具体而言，作者训练了一个模型，以直接从输入视频中预测原始音频信号（波形样本）。他们展示了该模型能够学习声音和视觉输入之间的相关性，适用于各种场景和物体交互。
- en: 'The previous efforts we have mentioned focused on monophonic audio generation,
    while Morgado et al. [[8](#bib.bib8)] attempted to convert monophonic audio recorded
    by a 360^∘ video camera into spatial audio. Performing such a task of audio specialization
    requires addressing two primary issues: source separation and localization. Therefore,
    the researchers designed a model to separate the sound sources from mixed-input
    audio and then localize them in the video. Another multimodality model was used
    to guide the separation and localization since the audio and video were complementary.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的前期工作主要集中在单声道音频生成上，而 Morgado 等人 [[8](#bib.bib8)] 尝试将由 360^∘ 视频摄像机录制的单声道音频转换为空间音频。执行这样的音频专门化任务需要解决两个主要问题：源分离和定位。因此，研究人员设计了一个模型来从混合输入音频中分离声音源，然后在视频中定位它们。另一个多模态模型用于指导分离和定位，因为音频和视频是互补的。
- en: 4.2 Audio to Vision
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 音频到视觉
- en: In this section, we provide a detailed review of audio-to-visual generation.
    We first introduce audio-to-images generation, which is easier than video generation
    since it does not require temporal consistency between the generated images.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们详细回顾了音频到视觉的生成。我们首先介绍音频到图像的生成，这比视频生成要简单，因为它不需要生成图像之间的时间一致性。
- en: 4.2.1 Audio to Image
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 音频到图像
- en: To generate images of better quality, Wan et al. [[84](#bib.bib84)] put forward
    a model that combined the spectral norm, an auxiliary classifier, and a projection
    discriminator to form the researchers’ conditional GAN model. The model could
    output images of different scales according to the volume of the sound, even for
    the same sound. Instead of generating real-world scenes of the sound that had
    occurred, Qiu et al. [[85](#bib.bib85)] suggested imagining the content from music.
    The authors extracted features by feeding the music and images into two networks
    and learning the correlation between those features and finally generated images
    from the learned correlation.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成更高质量的图像，Wan 等人 [[84](#bib.bib84)] 提出了一个结合了谱范数、辅助分类器和投影判别器的模型，形成了研究人员的条件
    GAN 模型。该模型可以根据声音的音量输出不同尺度的图像，即使是相同的声音。Qiu 等人 [[85](#bib.bib85)] 建议从音乐中想象内容，而不是生成发生过的真实场景。作者通过将音乐和图像输入到两个网络中提取特征，并学习这些特征之间的关联，最终从学习到的关联生成图像。
- en: 'Several studies have focused on audio-visual mutual generation. Chen et al.
    [[72](#bib.bib72)] were the first to attempt to solve this cross-modality generation
    problem using conditional GANs. The researchers defined a sound-to-image (S2I)
    network and an image-to-sound (I2S) network that generated images and sounds,
    respectively. Instead of separating S2I and I2S generation, Hao et al. [[86](#bib.bib86)]
    combined the respective networks into one network by considering a cross-modality
    cyclic generative adversarial network (CMCGAN) for the cross-modality visual-audio
    mutual generation task. Following the principle of cyclic consistency, CMCGAN
    consisted of four subnetworks: audio-to-visual, visual-to-audio, audio-to-audio,
    and visual-to-visual.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究集中在音频-视觉互生上。Chen 等人 [[72](#bib.bib72)] 是首个尝试使用条件 GAN 解决这一跨模态生成问题的研究者。研究人员定义了一个声音到图像（S2I）网络和一个图像到声音（I2S）网络，分别生成图像和声音。Hao
    等人 [[86](#bib.bib86)] 通过考虑一个跨模态循环生成对抗网络（CMCGAN）将这两个网络结合成一个网络，以处理跨模态视觉-音频互生任务。按照循环一致性的原则，CMCGAN
    包含了四个子网络：音频到视觉、视觉到音频、音频到音频和视觉到视觉。
- en: Most recently, some studies have tried to reconstruct facial images from speech
    clips. Duarte et al. [[87](#bib.bib87)] synthesized facial images containing expressions
    and poses through the GAN model. Moreover, the authors enhanced their model’s
    generation quality by searching for the optimal input audio length. To better
    learn normalized faces from speech, Oh et al. [[88](#bib.bib88)] explored a reconstructive
    model. The researchers trained an audio encoder by learning to align the feature
    space of speech with a pretrained face encoder and decoder.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究尝试从语音片段中重建面部图像。Duarte 等人 [[87](#bib.bib87)] 通过 GAN 模型合成了包含表情和姿势的面部图像。此外，作者通过寻找最佳输入音频长度来提升模型的生成质量。为了更好地从语音中学习规范化的面孔，Oh
    等人 [[88](#bib.bib88)] 探索了一个重建模型。研究人员通过学习将语音的特征空间与预训练的面部编码器和解码器对齐来训练音频编码器。
- en: '![Refer to caption](img/4c5975e52185686db01d0bcce964c09c.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4c5975e52185686db01d0bcce964c09c.png)'
- en: (a) Demonstration of audio-to-images generation.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 音频到图像生成的演示。
- en: '![Refer to caption](img/0d5d3be531cf2422943bc87b81a33916.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d5d3be531cf2422943bc87b81a33916.png)'
- en: (b) Demonstration of a moving body.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 移动体的演示。
- en: '![Refer to caption](img/858cb41549c15fb9e41c8aba94839e19.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/858cb41549c15fb9e41c8aba94839e19.png)'
- en: (c) Demonstration of a talking face.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 说话面孔的演示。
- en: 'Figure 6: Demonstration of talking face generation and moving body generation.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：说话面孔生成和移动体生成的演示。
- en: 'Table 4: Summary of recent studies of audio-to-visual generation.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：音频到视觉生成的最新研究总结。
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 思路与优势 | 弱点 |'
- en: '|  | Wan et al. [[84](#bib.bib84)] |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | Wan 等人 [[84](#bib.bib84)] |'
- en: '&#124; Combined many existing techniques &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合了许多现有技术 &#124;'
- en: '&#124; to form a GAN &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 形成一个 GAN &#124;'
- en: '| Low quality |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 低质量 |'
- en: '|  | Qiu et al. [[85](#bib.bib85)] |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | Qiu 等人 [[85](#bib.bib85)] |'
- en: '&#124; Generated images &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成了图像 &#124;'
- en: '&#124; related to music &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与音乐相关 &#124;'
- en: '| Low quality |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 低质量 |'
- en: '| Audio to Image | Chen et al. [[72](#bib.bib72)] |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 音频到图像 | Chen 等人 [[72](#bib.bib72)] |'
- en: '&#124; Generated both audio-to-visual and &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成了音频到视觉和 &#124;'
- en: '&#124; visual-to-audio models &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉到音频模型 &#124;'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The models were &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这些模型是 &#124;'
- en: '&#124; independent &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 独立的 &#124;'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Hao et al. [[86](#bib.bib86)] |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | Hao 等人 [[86](#bib.bib86)] |'
- en: '&#124; Proposed a cross-modality cyclic &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了跨模态循环 &#124;'
- en: '&#124; generative adversarial network &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成对抗网络 &#124;'
- en: '| Generated images only |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 仅生成图像 |'
- en: '|  | Alemi et al. [[89](#bib.bib89)] |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | Alemi 等人 [[89](#bib.bib89)] |'
- en: '&#124; Generated dance movements from &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从 &#124;'
- en: '&#124; music via real-time GrooveNet &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过实时 GrooveNet 进行音乐 &#124;'
- en: '|  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | Lee et al. [[90](#bib.bib90)] |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | 李等人 [[90](#bib.bib90)] |'
- en: '&#124; Generated a choreography system &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成了一个编舞系统 &#124;'
- en: '&#124; via an autoregressive &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过自回归 &#124;'
- en: '&#124; encoder-decoder network &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器-解码器网络 &#124;'
- en: '|  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Audio to Motions | Shlizerman et al. [[91](#bib.bib91)] |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 音频到动作 | Shlizerman 等人 [[91](#bib.bib91)] |'
- en: '&#124; Applied a “target delay” LSTM &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用了“目标延迟” LSTM &#124;'
- en: '&#124; to predict body keypoints &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以预测身体关键点 &#124;'
- en: '|'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Constrained to &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 受限于 &#124;'
- en: '&#124; the given dataset &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给定的数据集 &#124;'
- en: '|'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Tang et al. [[92](#bib.bib92)] |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | 汤等人 [[92](#bib.bib92)] |'
- en: '&#124; Developed a music-oriented dance &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发了一个以音乐为导向的舞蹈 &#124;'
- en: '&#124; choreography synthesis method &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编舞合成方法 &#124;'
- en: '|  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | Yalta et al. [[93](#bib.bib93)] |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | Yalta 等人 [[93](#bib.bib93)] |'
- en: '&#124; Produced weak labels from &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成了弱标签 &#124;'
- en: '&#124; motion directions for &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动作方向 &#124;'
- en: '&#124; motion-music alignment &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动作-音乐对齐 &#124;'
- en: '|  |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Kumar et al. [[94](#bib.bib94)] and &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Kumar 等人 [[94](#bib.bib94)] 和 &#124;'
- en: '&#124; Supasorn et al. [[95](#bib.bib95)] &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Supasorn 等人 [[95](#bib.bib95)] &#124;'
- en: '|'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generated keypoints &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成关键点 &#124;'
- en: '&#124; by a time-delayed &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由时间延迟的 &#124;'
- en: '&#124; LSTM &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSTM &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Needed retraining for &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要重新训练 &#124;'
- en: '&#124; another identity &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 另一个身份 &#124;'
- en: '|'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Chung et al. [[96](#bib.bib96)] |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | Chung 等人 [[96](#bib.bib96)] |'
- en: '&#124; Developed an encoder-decoder &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发了一个编码器-解码器 &#124;'
- en: '&#124; CNN model suitable &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适用的 CNN 模型 &#124;'
- en: '&#124; for more identities &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更多身份 &#124;'
- en: '|  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | Jalalifar et al. [[97](#bib.bib97)] |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  | Jalalifar 等人 [[97](#bib.bib97)] |'
- en: '&#124; Combined RNN and GAN &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合了 RNN 和 GAN &#124;'
- en: '&#124; and applied keypoints &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并应用了关键点 &#124;'
- en: '|'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; For a lab-controlled &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于实验室控制的 &#124;'
- en: '&#124; environment only &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅限于环境 &#124;'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Talking Face | Vougioukas et al. [[98](#bib.bib98)] |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| Talking Face | Vougioukas 等人 [[98](#bib.bib98)] |'
- en: '&#124; Applied a temporal GAN for &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用了一种时间 GAN 用于 &#124;'
- en: '&#124; more temporal consistency &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更多时间一致性 &#124;'
- en: '|  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | Chen et al. [[99](#bib.bib99)] | Applied optical flow | Generated lips
    only |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | 陈等人 [[99](#bib.bib99)] | 应用了光流 | 仅生成嘴唇 |'
- en: '|  | Zhou et al. [[100](#bib.bib100)] | Disentangled information | Lacked realism
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | 周等人 [[100](#bib.bib100)] | 解耦信息 | 缺乏真实感 |'
- en: '|  | Zhu et al. [[73](#bib.bib73)] |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  | 朱等人 [[73](#bib.bib73)] |'
- en: '&#124; Asymmetric mutual information estimation &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不对称互信息估计 &#124;'
- en: '&#124; to capture modality coherence &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉模态一致性 &#124;'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Suffered from the “zoom-in &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遭遇“放大 &#124;'
- en: '&#124; -and-out” condition &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -and-out” 条件 &#124;'
- en: '|'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Chen et al. [[101](#bib.bib101)] | Dynamic pixelwise loss |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | 陈等人 [[101](#bib.bib101)] | 动态像素损失 |'
- en: '&#124; Required multistage &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要多阶段 &#124;'
- en: '&#124; training &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '|'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Wiles et al. [[102](#bib.bib102)] |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | Wiles 等人 [[102](#bib.bib102)] |'
- en: '&#124; Self-supervised model for &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自监督模型用于 &#124;'
- en: '&#124; multimodality driving &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态驱动 &#124;'
- en: '| Relatively low quality |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 相对较低的质量 |'
- en: 4.2.2 Body Motion Generation
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 身体动作生成
- en: Instead of directly generating videos, numerous studies have tried to animate
    avatars using motions. The motion synthesis methods leveraged multiple techniques,
    such as dimensionality reduction [[103](#bib.bib103), [104](#bib.bib104)], hidden
    Markov models [[105](#bib.bib105)], Gaussian processes [[106](#bib.bib106)], and
    neural networks [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)].
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 不是直接生成视频，许多研究尝试使用动作来动画化虚拟角色。动作合成方法利用了多种技术，如降维 [[103](#bib.bib103), [104](#bib.bib104)]、隐马尔可夫模型
    [[105](#bib.bib105)]、高斯过程 [[106](#bib.bib106)] 和神经网络 [[107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109)]。
- en: Alemi et al. [[89](#bib.bib89)] proposed a real-time GrooveNet based on conditional
    restricted Boltzmann machines and recurrent neural networks to generate dance
    movements from music. Lee et al. [[90](#bib.bib90)] utilized an autoregressive
    encoder-decoder network to generate a choreography system from music. Shlizerman
    et al. [[91](#bib.bib91)] further introduced a model that used a “target delay”
    LSTM to predict body landmarks. The latter was further used as agents to generate
    body dynamics. The key idea was to create an animation from the audio that was
    similar to the action of a pianist or a violinist. In summary, the entire process
    generated a video of artists’ performance corresponding to input audio.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Alemi 等人 [[89](#bib.bib89)] 提出了基于条件限制玻尔兹曼机和递归神经网络的实时 GrooveNet，用于从音乐中生成舞蹈动作。Lee
    等人 [[90](#bib.bib90)] 利用自回归编码器-解码器网络从音乐中生成编舞系统。Shlizerman 等人 [[91](#bib.bib91)]
    进一步引入了一种使用“目标延迟”LSTM 来预测身体标记的模型。后者进一步用作生成身体动态的代理。关键思想是从音频创建类似于钢琴家或小提琴家的动作的动画。总之，整个过程生成了一个与输入音频相对应的艺术家表演视频。
- en: Although previous methods could generate body motion dynamics, the intrinsic
    beat information of the music has not been used. Tang et al. [[92](#bib.bib92)]
    proposed a music-oriented dance choreography synthesis method that extracted a
    relation between acoustic and motion features via an LSTM-autoencoder model. Moreover,
    to achieve better performance, the researchers improved their model with a masking
    method and temporal indexes. Providing weak supervision, Yalta et al. [[93](#bib.bib93)]
    explored producing weak labels from motion direction for motion-music alignment.
    The authors generated long dance sequences via a conditional autoconfigured deep
    RNN that was fed by audio spectrum.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的方法能够生成身体运动动态，但音乐的内在节拍信息尚未被利用。Tang 等人 [[92](#bib.bib92)] 提出了基于音乐的舞蹈编排合成方法，该方法通过
    LSTM 自编码器模型提取了声学特征和运动特征之间的关系。此外，为了实现更好的性能，研究人员通过掩蔽方法和时间索引改进了他们的模型。提供弱监督，Yalta
    等人 [[93](#bib.bib93)] 探索了从运动方向产生弱标签以进行运动-音乐对齐。作者通过条件自配置的深度 RNN 生成了长舞蹈序列，该 RNN
    由音频频谱驱动。
- en: 4.2.3 Talking Face Generation
  id: totrans-431
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 对话面孔生成
- en: Exploring audio-to-video generation, many researchers showed great interest
    in synthesizing people’s faces from speech or music. This has many applications,
    such as animating movies, teleconferencing, talking agents and enhancing speech
    comprehension while preserving privacy. Earlier studies of talking face generation
    mainly synthesized a specific identity from the dataset based on an audio of arbitrary
    speech. Kumar et al. [[94](#bib.bib94)] attempted to generate key points synced
    to audio by utilizing a time-delayed LSTM [[110](#bib.bib110)] and then generated
    the video frames conditioned on the key points by another network. Furthermore,
    Supasorn et al. [[95](#bib.bib95)] proposed a “teeth proxy” to improve the visual
    quality of teeth during generation.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索音频到视频生成的过程中，许多研究人员对从语音或音乐合成人的面孔表现出了极大的兴趣。这有许多应用，例如动画电影、远程会议、对话代理和在保护隐私的同时提升语音理解能力。早期的对话面孔生成研究主要基于任意语音的音频，从数据集中合成特定的身份。Kumar
    等人 [[94](#bib.bib94)] 通过利用时间延迟的 LSTM [[110](#bib.bib110)] 尝试生成与音频同步的关键点，然后通过另一个网络生成以这些关键点为条件的视频帧。此外，Supasorn
    等人 [[95](#bib.bib95)] 提出了“牙齿代理”以改善生成过程中牙齿的视觉质量。
- en: Subsequently, Chung et al. [[96](#bib.bib96)] attempted to use an encoder-decoder
    CNN model to learn the correspondences between raw audio and videos. Combining
    RNN and GAN [[70](#bib.bib70)], Jalalifar et al. [[97](#bib.bib97)] produced a
    sequence of realistic faces that were synchronized with the input audio by two
    networks. One was an LSTM network used to create lip landmarks out of audio input.
    The other was a conditional GAN (cGAN) used to generate the resulting faces conditioned
    on a given set of lip landmarks. Instead of applying cGAN, [[98](#bib.bib98)]
    proposed using a temporal GAN [[111](#bib.bib111)] to improve the quality of synthesis.
    However, the above methods were only applicable to synthesizing talking faces
    with identities limited to those in a dataset.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，Chung 等人 [[96](#bib.bib96)] 尝试使用编码器-解码器 CNN 模型学习原始音频和视频之间的对应关系。Jalalifar
    等人 [[97](#bib.bib97)] 结合 RNN 和 GAN [[70](#bib.bib70)]，通过两个网络生成与输入音频同步的现实面孔序列。其中一个是用于从音频输入创建唇部标记的
    LSTM 网络。另一个是条件 GAN (cGAN)，用于根据给定的一组唇部标记生成最终的面孔。[[98](#bib.bib98)] 提出了使用时间 GAN
    [[111](#bib.bib111)] 来提高合成质量，而不是应用 cGAN。然而，上述方法仅适用于合成数据集中已存在的身份的对话面孔。
- en: Synthesis of talking faces of arbitrary identities has recently drawn significant
    attention. Chen et al. [[99](#bib.bib99)] considered correlations among speech
    and lip movements while generating multiple lip images. The researchers used the
    optical flow to better express the information between the frames. The fed optical
    flow represented not only the information of the current shape but also the previous
    temporal information.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 任意身份的说话面孔合成最近引起了显著关注。Chen 等人 [[99](#bib.bib99)] 在生成多个唇部图像时考虑了语音和唇部运动之间的相关性。研究人员使用光流更好地表达帧之间的信息。输入的光流不仅表示当前形状的信息，还表示先前的时间信息。
- en: A frontal face photo usually has both identity and speech information. Assuming
    this, Zhou et al. [[100](#bib.bib100)] used an adversarial learning method to
    disentangle different types of information of one image during generation. The
    disentangled representation had a convenient property that both audio and video
    could serve as the source of speech information for the generation process. As
    a result, it was possible to not only output the features but also express them
    more explicitly while applying the resulting network.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 一张正面的脸部照片通常包含身份和语音信息。基于此，Zhou 等人 [[100](#bib.bib100)] 使用对抗学习方法在生成过程中解缠绕图像的不同信息类型。解缠绕的表示具有一个便利的特性，即音频和视频都可以作为生成过程中的语音信息来源。因此，不仅可以输出特征，还可以在应用生成网络时更明确地表达这些特征。
- en: Most recently, to discover the high-level correlation between audio and video,
    Zhu et al. [[73](#bib.bib73)] proposed a mutual information approximation to approximate
    mutual information between modalities. Chen et al. [[101](#bib.bib101)] applied
    landmark and motion attention to generating talking faces. The authors further
    proposed a dynamic pixelwise loss for temporal consistency. Facial generation
    is not limited to specific modalities such as audio or visual since the crucial
    point is whether there is a mutual pattern between these different modalities.
    Wiles et al. [[102](#bib.bib102)] put forward a self-supervising framework called
    X2Face to learn the embedded features and generate target facial motions. It could
    produce videos from any input as long as embedded features were learned.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，为了发现音频和视频之间的高级相关性，Zhu 等人 [[73](#bib.bib73)] 提出了一个互信息近似方法，用于近似模态之间的互信息。Chen
    等人 [[101](#bib.bib101)] 应用了地标和运动注意力来生成说话的面孔。作者进一步提出了一种用于时间一致性的动态像素级损失。面部生成不局限于音频或视觉等特定模态，因为关键点在于这些不同模态之间是否存在共同的模式。Wiles
    等人 [[102](#bib.bib102)] 提出了一个自监督框架 X2Face，以学习嵌入特征并生成目标面部运动。只要学习了嵌入特征，它就可以从任何输入生成视频。
- en: 5 Audio-visual Representation Learning
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 音视频表示学习
- en: 'Table 5: Summary of recent audio-visual representation learning studies.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：近期音视频表示学习研究的总结。
- en: '| Type | Method | Ideas & Strengths | Weaknesses |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法 | 思路与优点 | 缺点 |'
- en: '|'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Single &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单模态 &#124;'
- en: '&#124; modality &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态 &#124;'
- en: '|'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Aytar et al. [[112](#bib.bib112)] &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Aytar 等人 [[112](#bib.bib112)] &#124;'
- en: '|'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Student-teacher training &#124;'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学生-教师训练 &#124;'
- en: '&#124; procedure with natural &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然的 &#124;'
- en: '&#124; video synchronization &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频同步 &#124;'
- en: '|'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Only learned the &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅学习了 &#124;'
- en: '&#124; audio representation &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 音频表示 &#124;'
- en: '|'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Leidal et al. [[113](#bib.bib113)] &#124;'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Leidal 等人 [[113](#bib.bib113)] &#124;'
- en: '|'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Regularized the amount of &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对模态的数量进行正则化 &#124;'
- en: '&#124; information encoded in the &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码在 &#124;'
- en: '&#124; semantic embedding &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语义嵌入 &#124;'
- en: '|'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Focused on spoken utterances &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关注口语表达 &#124;'
- en: '&#124; and handwritten digits &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和手写数字 &#124;'
- en: '|'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Arandjelovic et al. &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Arandjelovic 等人 &#124;'
- en: '&#124; [[12](#bib.bib12), [114](#bib.bib114)] &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[12](#bib.bib12), [114](#bib.bib114)] &#124;'
- en: '| Proposed the AVC task |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 提出了 AVC 任务 |'
- en: '&#124; Considered only audio and &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅考虑音频和 &#124;'
- en: '&#124; video correspondence &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频对应关系 &#124;'
- en: '|'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dual &#124;'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 双模态 &#124;'
- en: '&#124; modalities &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态 &#124;'
- en: '| Owens et al. [[13](#bib.bib13)] |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| Owens 等人 [[13](#bib.bib13)] |'
- en: '&#124; Proposed the AVTS task &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了 AVTS 任务 &#124;'
- en: '&#124; with curriculum learning &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带有课程学习 &#124;'
- en: '|'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The sound source has to &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 声音源必须 &#124;'
- en: '&#124; feature in the video; only &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频中的特征；仅 &#124;'
- en: '&#124; one sound source &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个声音源 &#124;'
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Parekh et al. [[115](#bib.bib115)] |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | Parekh 等人 [[115](#bib.bib115)] |'
- en: '&#124; Use video labels for weakly &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用视频标签进行弱监督 &#124;'
- en: '&#124; supervised learning &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 监督学习 &#124;'
- en: '|'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Leverage the prior knowledge &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用先验知识 &#124;'
- en: '&#124; of event classification &#124;'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 事件分类 &#124;'
- en: '|'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Hu et al. [[116](#bib.bib116)] |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  | Hu 等人 [[116](#bib.bib116)] |'
- en: '&#124; Disentangle each &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解开每个 &#124;'
- en: '&#124; modality into a set &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态转化为集合 &#124;'
- en: '&#124; of distinct components &#124;'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不同组件的 &#124;'
- en: '|'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Require a predefined &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要预定义的 &#124;'
- en: '&#124; number of clusters &#124;'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 聚类数量 &#124;'
- en: '|'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Representation learning aims to discover the pattern representation from data
    automatically. It is motivated by the fact that the choice of data representation
    usually greatly impacts performance of machine learning [[11](#bib.bib11)]. However,
    real-world data such as images, videos and audio are not amenable to defining
    specific features algorithmically.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习旨在自动发现数据的模式表示。其动机来自于数据表示的选择通常会显著影响机器学习的性能[[11](#bib.bib11)]。然而，现实世界的数据，如图像、视频和音频，并不容易通过算法定义特定的特征。
- en: Additionally, the quality of data representation usually determines the success
    of machine learning algorithms. Bengio et al.[[11](#bib.bib11)] assumed the reason
    for this to be that different representations could better explain the laws underlying
    data, and the recent enthusiasm for AI has motivated the design of more powerful
    representation learning algorithms to achieve these priors.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据表示的质量通常决定了机器学习算法的成功。Bengio 等人[[11](#bib.bib11)] 认为其原因在于不同的表示可以更好地解释数据的潜在规律，而最近对人工智能的热情促使了更强大的表示学习算法的设计以实现这些先验知识。
- en: 'In this section, we will review a series of audio-visual learning methods ranging
    from single-modality [[112](#bib.bib112)] to dual-modality representation learning
    [[114](#bib.bib114), [12](#bib.bib12), [13](#bib.bib13), [113](#bib.bib113), [116](#bib.bib116)].
    The basic pipeline of such studies is shown in Fig. [7](#S5.F7 "Figure 7 ‣ 5 Audio-visual
    Representation Learning ‣ Deep Audio-Visual Learning: A Survey").'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一系列音视频学习方法，从单模态 [[112](#bib.bib112)] 到双模态表示学习 [[114](#bib.bib114),
    [12](#bib.bib12), [13](#bib.bib13), [113](#bib.bib113), [116](#bib.bib116)]。这些研究的基本流程如图
    [7](#S5.F7 "图 7 ‣ 5 音视频表示学习 ‣ 深度音视频学习：综述") 所示。
- en: '![Refer to caption](img/8ae983468b9c3f4b271ce98a497fcdac.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ae983468b9c3f4b271ce98a497fcdac.png)'
- en: 'Figure 7: Basic pipeline of representation learning.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 表示学习的基本流程。'
- en: 5.1 Single-Modality Representation Learning
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 单模态表示学习
- en: Naturally, to determine whether audio and video are related to each other, researchers
    focus on determining whether audio and video are from the same video or whether
    they are synchronized in the same video. Aytar et al. [[112](#bib.bib112)] exploited
    the natural synchronization between video and sound to learn an acoustic representation
    of a video. The researchers proposed a student-teacher training process that used
    an unlabeled video as a bridge to transfer discernment knowledge from a sophisticated
    visual identity model to the sound modality. Although the proposed approach managed
    to learn audio-modality representation in an unsupervised manner, discovering
    audio and video representations simultaneously remained to be solved.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，为了确定音频和视频是否相关，研究人员专注于确定音频和视频是否来自同一个视频或它们在同一个视频中是否同步。Aytar 等人 [[112](#bib.bib112)]
    利用视频和声音之间的自然同步来学习视频的声学表示。研究人员提出了一种学生-教师训练过程，该过程使用未标记的视频作为桥梁，将来自复杂视觉识别模型的知识传递到声音模态。尽管所提出的方法成功地以无监督的方式学习了音频模态表示，但同时发现音频和视频表示仍待解决。
- en: 5.2 Learning an Audio-visual Representation
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 学习音视频表示
- en: In the corresponding audio and images, the information concerning modality tends
    to be noisy, while we only require semantic content rather than the exact visual
    content. Leidal et al. [[113](#bib.bib113)] explored unsupervised learning of
    the semantic embedded space, which required a close distribution of the related
    audio and image. The researchers proposed a model to map an input to vectors of
    the mean and the logarithm of variance of a diagonal Gaussian distribution, and
    the sample semantic embeddings were drawn from these vectors.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在对应的音频和图像中，与模态相关的信息往往是嘈杂的，而我们只需要语义内容，而非准确的视觉内容。Leidal 等人 [[113](#bib.bib113)]
    探索了语义嵌入空间的无监督学习，这需要相关音频和图像之间有较紧密的分布。研究人员提出了一个模型，将输入映射到对角高斯分布的均值和方差的对数的向量，样本的语义嵌入从这些向量中提取。
- en: 'To learn audio and video’s semantic information by simply watching and listening
    to a large number of unlabeled videos, Arandjelovic et al. [[12](#bib.bib12)]
    introduced an audio-visual correspondence learning task (AVC) for training two
    (visual and audio) networks from scratch, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.2 Learning an Audio-visual Representation ‣ 5 Audio-visual Representation
    Learning ‣ Deep Audio-Visual Learning: A Survey") (a). In this task, the corresponding
    audio and visual pairs (positive samples) were obtained from the same video, while
    mismatched (negative) pairs were extracted from different videos. To solve this
    task, the authors proposed an $L^{3}$-Net that detected whether the semantics
    in visual and audio fields were consistent. Although this model was trained without
    additional supervision, it could learn representations of dual modalities effectively.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '为了通过简单地观看和听取大量未标记的视频来学习音频和视频的语义信息，Arandjelovic 等人 [[12](#bib.bib12)] 引入了一个音视频对应学习任务（AVC），用于从头训练两个（视觉和音频）网络，如图
    [8](#S5.F8 "Figure 8 ‣ 5.2 Learning an Audio-visual Representation ‣ 5 Audio-visual
    Representation Learning ‣ Deep Audio-Visual Learning: A Survey") (a) 所示。在这个任务中，相应的音频和视觉对（正样本）来自同一个视频，而不匹配的（负样本）对则从不同的视频中提取。为了解决这个任务，作者提出了一个
    $L^{3}$-Net 来检测视觉和音频领域中的语义是否一致。虽然这个模型在没有额外监督的情况下进行了训练，但它能够有效地学习双模态的表示。'
- en: Exploring the proposed audio-visual coherence (AVC) task, Arandjelovic et al.
    [[114](#bib.bib114)] continued to investigate AVE-Net that aimed at finding the
    most similar visual area to the current audio clip. Owens et al. [[117](#bib.bib117)]
    proposed adopting a model similar to that of [[12](#bib.bib12)] but used a 3D
    convolution network for the videos instead, which could capture the motion information
    for sound localization.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了所提出的音视频一致性（AVC）任务后，Arandjelovic 等人 [[114](#bib.bib114)] 继续研究了 AVE-Net，其目标是寻找与当前音频片段最相似的视觉区域。Owens
    等人 [[117](#bib.bib117)] 提出了采用类似于 [[12](#bib.bib12)] 的模型，但使用了 3D 卷积网络来处理视频，从而能够捕捉声音定位的运动信息。
- en: 'In contrast to previous AVC task-based solutions, Korbar et al. [[13](#bib.bib13)]
    introduced another proxy task called audio-visual time synchronization (AVTS)
    that further considered whether a given audio sample and video clip were “synchronized”
    or “not synchronized.” In previous AVC tasks, negative samples were obtained as
    audio and visual samples from different videos. However, exploring AVTS, the researchers
    trained the model using “harder” negative samples representing unsynchronized
    audio and visual segments sampled from the same video, forcing the model to learn
    the relevant temporal features. At this time, not only the semantic correspondence
    was enforced between the video and the audio, but more importantly, the synchronization
    between them was also achieved. The researchers applied the curriculum learning
    strategy [[118](#bib.bib118)] to this task and divided the samples into four categories:
    positives (the corresponding audio-video pairs), easy negatives (audio and video
    clips originating from different videos), difficult negatives (audio and video
    clips originating from the same video without overlap), and super-difficult negatives
    (audio and video clips that partly overlap), as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.2 Learning an Audio-visual Representation ‣ 5 Audio-visual Representation
    Learning ‣ Deep Audio-Visual Learning: A Survey") (b).'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往基于 AVC 任务的解决方案相比，Korbar 等人 [[13](#bib.bib13)] 引入了另一种代理任务，称为音视频时间同步（AVTS），进一步考虑了给定的音频样本和视频片段是否“同步”或“不同步”。在之前的
    AVC 任务中，负样本是从不同视频中获取的音频和视觉样本。然而，在探索 AVTS 时，研究人员使用了“更难”的负样本，即从同一视频中采样的不同步的音频和视觉片段，训练模型，迫使模型学习相关的时间特征。这时，不仅加强了视频和音频之间的语义对应关系，更重要的是也实现了它们之间的同步。研究人员将课程学习策略
    [[118](#bib.bib118)] 应用于这一任务，将样本分为四类：正样本（对应的音视频对）、简单负样本（来自不同视频的音频和视频片段）、困难负样本（来自同一视频但不重叠的音频和视频片段）以及超困难负样本（部分重叠的音频和视频片段），如图
    [8](#S5.F8 "图 8 ‣ 5.2 学习音视频表示 ‣ 5 音视频表示学习 ‣ 深度音视频学习：综述") (b) 所示。
- en: '![Refer to caption](img/c39ba0599090dd4c2d625daa1492a7a1.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c39ba0599090dd4c2d625daa1492a7a1.png)'
- en: (a) Introduction to the AVC task
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: (a) AVC 任务简介
- en: '![Refer to caption](img/a2b996fcc7d3bce9bdab7b177ff2ceea.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a2b996fcc7d3bce9bdab7b177ff2ceea.png)'
- en: (b) Introduction to the AVTS task
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AVTS 任务简介
- en: 'Figure 8: Introduction to the representation task'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：表示任务简介
- en: '![Refer to caption](img/62c261e3c937c1ed1ebbeb0c9c415e2a.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/62c261e3c937c1ed1ebbeb0c9c415e2a.png)'
- en: 'Figure 9: Demonstration of audio-visual datasets.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：音视频数据集演示。
- en: 'The above studies rely on two latent assumptions: 1) the sound source should
    be present in the video, and 2) only one sound source is expected. However, these
    assumptions limit the applications of the respective approaches to real-life videos.
    Therefore, Parekh et al. [[115](#bib.bib115)] leveraged class-agnostic proposals
    from both video frames to model the problem as a multiple-instance learning task
    for audio. As a result, the classification and localization problems could be
    solved simultaneously. The researchers focused on localizing salient audio and
    visual components using event classes in a weakly supervised manner. This framework
    was able to deal with the difficult case of asynchronous audio-visual events.
    To leverage more detailed relations between modalities, Hu et al. [[116](#bib.bib116)]
    recommended a deep coclustering model that extracted a set of distinct components
    from each modality. The model continually learned the correspondence between such
    representations of different modalities. The authors further introduced K-means
    clustering to distinguish concrete objects or sounds.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 上述研究依赖于两个潜在假设：1）声音源应存在于视频中，2）期望只有一个声音源。然而，这些假设限制了这些方法在实际视频中的应用。因此，Parekh 等人
    [[115](#bib.bib115)] 利用来自视频帧的类无关提案，将问题建模为音频的多实例学习任务。结果，分类和定位问题可以同时解决。研究人员专注于使用事件类别以弱监督的方式定位显著的音频和视觉组件。该框架能够处理音视频事件异步的困难情况。为了利用模态间更详细的关系，Hu
    等人 [[116](#bib.bib116)] 推荐了一种深度共同聚类模型，从每个模态中提取一组不同的组件。该模型不断学习这些不同模态表示之间的对应关系。作者进一步引入了
    K-means 聚类来区分具体的对象或声音。
- en: 6 Recent Public Audio-visual Datasets
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 最近的公开音视频数据集
- en: 'Many audio-visual datasets ranging from speech- to event-related data have
    been collected and released. We divide datasets into two categories: audio-visual
    speech datasets that record human face with the corresponding speech, and audio-visual
    event datasets that consist of musical instrument videos and real events’ videos.
    In this section, we summarize the information of recent audio-visual datasets
    (Table [6](#S6.T6 "Table 6 ‣ 6.1.1 Lab-controlled Environment ‣ 6.1 Audio-visual
    Speech Datasets ‣ 6 Recent Public Audio-visual Datasets ‣ Deep Audio-Visual Learning:
    A Survey")).'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从语音到事件相关的数据的音视频数据集已被收集和发布。我们将数据集分为两类：音视频语音数据集记录了人脸及其对应的语音，音视频事件数据集包括音乐器材视频和真实事件的视频。在本节中，我们总结了最近的音视频数据集的信息（表
    [6](#S6.T6 "表 6 ‣ 6.1.1 实验室控制环境 ‣ 6.1 音视频语音数据集 ‣ 6 最近公共音视频数据集 ‣ 深度音视频学习：综述")）。
- en: 6.1 Audio-visual Speech Datasets
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 音视频语音数据集
- en: Constructing datasets containing audio-visual corpora is crucial to understanding
    audio-visual speech. The datasets are collected in lab-controlled environments
    where volunteers read the prepared phrases or sentences, or in-the-wild environments
    of TV interviews or talks.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 构建包含音视频语料的数据集对理解音视频语音至关重要。这些数据集在实验室控制环境中收集，志愿者阅读预设的短语或句子，或在自然环境中如电视采访或演讲中收集。
- en: 6.1.1 Lab-controlled Environment
  id: totrans-520
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 实验室控制环境
- en: 'Table 6: Summary of speech-related audio-visual datasets. These datasets can
    be used for all tasks related to speech we have mentioned above. Note that the
    length of a ‘speech’ dataset denotes the number of video clips, while for ‘music’
    or ’real event’ datasets, the length represents the total number of hours of the
    dataset.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：与语音相关的音视频数据集总结。这些数据集可以用于我们上述提到的所有语音相关任务。请注意，“语音”数据集的长度表示视频剪辑的数量，而“音乐”或“真实事件”数据集的长度表示数据集的总小时数。
- en: '| Category | Dataset | Env. | Classes | Length* | Year |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 数据集 | 环境 | 类别数 | 长度* | 年份 |'
- en: '|  | GRID [[119](#bib.bib119)] | Lab | 34 | 33,000 | 2006 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|  | GRID [[119](#bib.bib119)] | 实验室 | 34 | 33,000 | 2006 |'
- en: '|  | Lombard Grid [[120](#bib.bib120)] | Lab | 54 | 54,000 | 2018 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | Lombard Grid [[120](#bib.bib120)] | 实验室 | 54 | 54,000 | 2018 |'
- en: '|  | TCD TIMIT [[121](#bib.bib121)] | Lab | 62 | - | 2015 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | TCD TIMIT [[121](#bib.bib121)] | 实验室 | 62 | - | 2015 |'
- en: '|  | Vid TIMIT [[122](#bib.bib122)] | Lab | 43 | - | 2009 |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  | Vid TIMIT [[122](#bib.bib122)] | 实验室 | 43 | - | 2009 |'
- en: '|  | RAVDESS [[123](#bib.bib123)] | Lab | 24 | - | 2018 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '|  | RAVDESS [[123](#bib.bib123)] | 实验室 | 24 | - | 2018 |'
- en: '|  | SEWA [[124](#bib.bib124)] | Lab | 180 | - | 2017 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | SEWA [[124](#bib.bib124)] | 实验室 | 180 | - | 2017 |'
- en: '| Speech | OuluVS [[125](#bib.bib125)] | Lab | 20 | 1000 | 2009 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 语音 | OuluVS [[125](#bib.bib125)] | 实验室 | 20 | 1000 | 2009 |'
- en: '|  | OuluVS2 [[126](#bib.bib126)] | Lab | 52 | 3640 | 2016 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | OuluVS2 [[126](#bib.bib126)] | 实验室 | 52 | 3640 | 2016 |'
- en: '|  | Voxceleb [[127](#bib.bib127)] | Wild | 1,251 | 154,516 | 2017 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '|  | Voxceleb [[127](#bib.bib127)] | 自然 | 1,251 | 154,516 | 2017 |'
- en: '|  | Voxceleb2 [[128](#bib.bib128)] | Wild | 6,112 | 1,128,246 | 2018 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '|  | Voxceleb2 [[128](#bib.bib128)] | 自然 | 6,112 | 1,128,246 | 2018 |'
- en: '|  | LRW [[129](#bib.bib129)] | Wild | $\sim$1000 | 500,000 | 2016 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  | LRW [[129](#bib.bib129)] | 自然 | $\sim$1000 | 500,000 | 2016 |'
- en: '|  | LRS [[59](#bib.bib59)] | Wild | $\sim$1000 | 118,116 | 2017 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | LRS [[59](#bib.bib59)] | 自然 | $\sim$1000 | 118,116 | 2017 |'
- en: '|  | LRS3 [[130](#bib.bib130)] | Wild | $\sim$1000 | 74,564 | 2017 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '|  | LRS3 [[130](#bib.bib130)] | 自然 | $\sim$1000 | 74,564 | 2017 |'
- en: '|  | AVA-ActiveSpeaker [[131](#bib.bib131)] | Wild | - | 90,341 | 2019 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | AVA-ActiveSpeaker [[131](#bib.bib131)] | 自然 | - | 90,341 | 2019 |'
- en: '|  | C4S [[132](#bib.bib132)] | Lab | - | 4.5 | 2017 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '|  | C4S [[132](#bib.bib132)] | 实验室 | - | 4.5 | 2017 |'
- en: '| Music | ENST-Drums [[133](#bib.bib133)] | Lab | - | 3.75 | 2006 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 音乐 | ENST-Drums [[133](#bib.bib133)] | 实验室 | - | 3.75 | 2006 |'
- en: '|  | URMP [[134](#bib.bib134)] | Lab | - | 1.3 | 2019 |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '|  | URMP [[134](#bib.bib134)] | 实验室 | - | 1.3 | 2019 |'
- en: '|  | YouTube-8M [[135](#bib.bib135)] | Wild | 3862 | 350,000 | 2016 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '|  | YouTube-8M [[135](#bib.bib135)] | 自然 | 3862 | 350,000 | 2016 |'
- en: '|  | AudioSet [[136](#bib.bib136)] | Wild | 632 | 4971 | 2016 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '|  | AudioSet [[136](#bib.bib136)] | 自然 | 632 | 4971 | 2016 |'
- en: '| Real Event | Kinetics-400 [[137](#bib.bib137)] | Wild | 400 | 850* | 2018
    |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 真实事件 | Kinetics-400 [[137](#bib.bib137)] | 自然 | 400 | 850* | 2018 |'
- en: '|  | Kinetics-600 [[138](#bib.bib138)] | Wild | 600 | 1400* | 2018 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  | Kinetics-600 [[138](#bib.bib138)] | 自然 | 600 | 1400* | 2018 |'
- en: '|  | Kinetics-700 [[139](#bib.bib139)] | Wild | 700 | 1806* | 2018 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '|  | Kinetics-700 [[139](#bib.bib139)] | 自然 | 700 | 1806* | 2018 |'
- en: 'Lab-controlled speech datasets are captured in specific environments, where
    volunteers are required to read the given phases or sentences. Some of the datasets
    only contain videos of speakers that utter the given sentences; these datasets
    include GRID [[119](#bib.bib119)], TCD TIMIT [[121](#bib.bib121)], and VidTIMIT
    [[122](#bib.bib122)]. Such datasets can be used for lip reading, talking face
    generation, and speech reconstruction. Development of more advanced datasets has
    continued: e.g., Livingstone et al. offered the RAVDESS dataset [[123](#bib.bib123)]
    that contained emotional speeches and songs. The items in it are also rated according
    to emotional validity, intensity and authenticity.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 实验室控制的语音数据集是在特定环境中捕获的，志愿者需要朗读给定的短语或句子。有些数据集仅包含说话者朗读给定句子的录像；这些数据集包括 GRID [[119](#bib.bib119)]、TCD
    TIMIT [[121](#bib.bib121)] 和 VidTIMIT [[122](#bib.bib122)]。这些数据集可以用于唇读、说话人面孔生成和语音重建。更高级的数据集的开发也在继续：例如，Livingstone
    等人提供了 RAVDESS 数据集 [[123](#bib.bib123)]，其中包含情感演讲和歌曲。数据集中的条目还根据情感有效性、强度和真实性进行评分。
- en: Some datasets such as Lombard Grid [[120](#bib.bib120)] and OuluVS [[125](#bib.bib125),
    [126](#bib.bib126)] focus on multiview videos. In addition, a dataset named SEWA
    offers rich annotations, including answers to a questionnaire, facial landmarks,
    (low-level descriptors of) LLD features, hand gestures, head gestures, transcript,
    valence, arousal, liking or disliking, template behaviors, episodes of agreement
    or disagreement, and episodes of mimicry.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集，如 Lombard Grid [[120](#bib.bib120)] 和 OuluVS [[125](#bib.bib125)、[126](#bib.bib126)]，专注于多视角视频。此外，一个名为
    SEWA 的数据集提供了丰富的注释，包括问卷回答、面部标志点、（低级描述符的）LLD 特征、手势、头部动作、转录、情感、唤醒、喜欢或不喜欢、模板行为、同意或不同意的情节以及模仿的情节。
- en: 6.1.2 In-the-wild Environment
  id: totrans-547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 实际环境
- en: The above datasets were collected in lab environments; as a result, models trained
    on those datasets are difficult to apply in real-world scenarios. Thus, researchers
    have tried to collect real-world videos from TV interviews, talks and movies and
    released several real-world datasets, including LRW, LRW variants [[129](#bib.bib129),
    [59](#bib.bib59), [130](#bib.bib130)], Voxceleb and its variants [[127](#bib.bib127),
    [128](#bib.bib128)], AVA-ActiveSpeaker [[131](#bib.bib131)] and AVSpeech [[7](#bib.bib7)].
    The LRW dataset consists of 500 sentences [[129](#bib.bib129)], while its variant
    contains 1000 sentences[[59](#bib.bib59), [130](#bib.bib130)], all of which were
    spoken by hundreds of different speakers. VoxCeleb and its variants contain over
    100,000 utterances of 1,251 celebrities [[127](#bib.bib127)] and over a million
    utterances of 6,112 identities [[128](#bib.bib128)], respectively.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据集是在实验室环境中收集的；因此，在这些数据集上训练的模型在现实世界场景中难以应用。因此，研究人员尝试从电视采访、演讲和电影中收集真实世界的视频，并发布了多个真实世界的数据集，包括
    LRW、LRW 变体 [[129](#bib.bib129)、[59](#bib.bib59)、[130](#bib.bib130)]、Voxceleb 及其变体
    [[127](#bib.bib127)、[128](#bib.bib128)]、AVA-ActiveSpeaker [[131](#bib.bib131)]
    和 AVSpeech [[7](#bib.bib7)]。LRW 数据集包含 500 个句子 [[129](#bib.bib129)]，而其变体包含 1000
    个句子 [[59](#bib.bib59)、[130](#bib.bib130)]，所有句子均由数百位不同的说话者朗读。VoxCeleb 及其变体分别包含超过
    100,000 次 1,251 位名人的发言 [[127](#bib.bib127)] 和超过一百万次 6,112 个身份的发言 [[128](#bib.bib128)]。
- en: 'AVA-ActiveSpeaker [[131](#bib.bib131)] and AVSpeech [[7](#bib.bib7)] datasets
    contain even more videos. The AVA-ActiveSpeaker [[131](#bib.bib131)] dataset consists
    of 3.65 million human-labeled video frames (approximately 38.5 hrs) The AVSpeech
    [[7](#bib.bib7)] dataset contains approximately 4700 hours of video segments from
    a total of 290k YouTube videos spanning a wide variety of people, languages, and
    face poses. The details are reported in Table [6](#S6.T6 "Table 6 ‣ 6.1.1 Lab-controlled
    Environment ‣ 6.1 Audio-visual Speech Datasets ‣ 6 Recent Public Audio-visual
    Datasets ‣ Deep Audio-Visual Learning: A Survey").'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 'AVA-ActiveSpeaker [[131](#bib.bib131)] 和 AVSpeech [[7](#bib.bib7)] 数据集包含更多的视频。AVA-ActiveSpeaker
    [[131](#bib.bib131)] 数据集包含 365 万个人标注的视频帧（大约 38.5 小时）。AVA-ActiveSpeaker [[7](#bib.bib7)]
    数据集包含大约 4700 小时的视频片段，来自 290,000 个 YouTube 视频，涵盖了各种人群、语言和面部姿态。详细信息见表 [6](#S6.T6
    "Table 6 ‣ 6.1.1 Lab-controlled Environment ‣ 6.1 Audio-visual Speech Datasets
    ‣ 6 Recent Public Audio-visual Datasets ‣ Deep Audio-Visual Learning: A Survey")。'
- en: 6.2 Audio-visual Event Datasets
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 视听事件数据集
- en: Another audio-visual dataset category consists of music or real-world event
    videos. These datasets are different from the aforementioned audio-visual speech
    datasets in not being limited to facial videos.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类视听数据集包括音乐或现实世界事件视频。这些数据集与前述的视听语音数据集不同，不仅限于面部视频。
- en: 6.2.1 Music-related Datasets
  id: totrans-552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 与音乐相关的数据集
- en: Most music-related datasets were constructed in the lab environment. For example,
    ENST-Drums [[133](#bib.bib133)] merely contains drum videos of three professional
    drummers specializing in different music genres. The C4S dataset [[132](#bib.bib132)]
    consists of 54 videos of 9 distinct clarinetists, each performing 3 different
    classical music pieces twice (4.5h in total).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: The URMP [[134](#bib.bib134)] dataset contains a number of multi-instrument
    musical pieces. However, these videos were recorded separately and then combined.
    To simplify the use of the URMP dataset, Chen et al. further proposed the Sub-URMP
    [[72](#bib.bib72)] dataset that contains multiple video frames and audio files
    extracted from the URMP dataset.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Real Events-related Datasets
  id: totrans-555
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: More and more real-world audio-visual event datasets have recently been released
    that consist of numerous videos uploaded to the Internet. The datasets often comprise
    hundreds or thousands of event classes and the corresponding videos. Representative
    datasets include the following.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: Kinetics-400 [[137](#bib.bib137)], Kinetics-600 [[138](#bib.bib138)] and Kinetics-700
    [[139](#bib.bib139)] contain 400, 600 and 700 human action classes with at least
    400, 600, and 600 video clips for each action, respectively. Each clip lasts approximately
    10 s and is taken from a distinct YouTube video. The actions cover a broad range
    of classes, including human-object interactions such as playing instruments, as
    well as human-human interactions such as shaking hands. The AVA-Actions dataset
    [[140](#bib.bib140)] densely annotated 80 atomic visual actions in 43015 minutes
    of movie clips, where actions were localized in space and time, resulting in 1.58M
    action labels with multiple labels corresponding to a certain person.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: AudioSet [[136](#bib.bib136)], a more general dataset, consists of an expanding
    ontology of 632 audio event classes and a collection of 2,084,320 human-labeled
    10-second sound clips. The clips were extracted from YouTube videos and cover
    a wide range of human and animal sounds, musical instruments and genres, and common
    everyday environmental sounds. YouTube-8M [[135](#bib.bib135)] is a large-scale
    labeled video dataset that consists of millions of YouTube video IDs with high-quality
    machine-generated annotations from a diverse vocabulary of 3,800+ visual entities.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Audio-visual learning (AVL) is a foundation of the multimodality problem that
    integrates the two most important perceptions of our daily life. Despite great
    efforts focused on AVL, there is still a long way to go for real-life applications.
    In this section, we briefly discuss the key challenges and the potential research
    directions in each category.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Challenges
  id: totrans-561
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The heterogeneous nature of the discrepancy in AVL determines its inherent challenges.
    Audio tracks use a level of electrical voltage to represent analog signals, while
    the visual modality is usually represented in the RGB color space; the large gap
    between the two poses a major challenge to AVL. The essence of this problem is
    to understand the relation between audio and vision, which also is the basic challenge
    of AVL.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: AVL中差异的异质性决定了其固有的挑战。音频轨道使用一定电压水平来表示模拟信号，而视觉模态通常在RGB颜色空间中表示；两者之间的巨大差距对AVL构成了重大挑战。这个问题的本质在于理解音频和视觉之间的关系，这也是AVL的基本挑战。
- en: Audio-visual Separation and Localization is a longstanding problem in many real-life
    applications. Regardless of the previous advances in speaker-related or recent
    object-related separation and localization, the main challenges are failing to
    distinguish the timbre of various objects and exploring ways of generating the
    sounds of different objects. Addressing these challenges requires us to carefully
    design the models or ideas (e.g., the attention mechanism) for dealing with different
    objects. Audio-visual correspondence learning has vast potential applications,
    such as those in criminal investigations, medical care, transportation, and other
    industries. Many studies have tried to map different modalities into the shared
    feature space. However, it is challenging to obtain satisfactory results since
    extracting clear and effective information from ambiguous input and target modalities
    remains difficult. Therefore, sufficient prior information (the specific patterns
    people usually focus on) has a significant impact on obtaining more accurate results.
    Audio and vision generation focuses on empowered machine imagination. In contrast
    to the conventional discriminative problem, the task of cross-modality generation
    is to fit a mapping between probability distributions. Therefore, it is usually
    a many-to-many mapping problem that is difficult to learn. Moreover, despite the
    large difference between audio and visual modalities, humans are sensitive to
    the difference between real-world and generated results, and subtle artifacts
    can be easily noticed, which makes this task more challenging. Finally, audio-visual
    representation learning can be regarded as a generalization of other tasks. As
    we discussed before, both audio represented by electrical voltage and vision represented
    by the RGB color space are designed to be perceived by humans while not making
    it easy for a machine to discover the common features. The difficulty stems from
    having only two modalities and lacking explicit constraints. Therefore, the main
    challenge of this task is to find a suitable constraint. Unsupervised learning
    as a prevalent approach to this task provides a well-designed solution, while
    not having external supervision makes it difficult to achieve our goal. The challenging
    of the weakly supervised approach is to find correct implicit supervision.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 音视频分离和定位在许多现实应用中是一个长期存在的问题。尽管在扬声器相关或最近的物体相关分离和定位方面已有进展，但主要挑战在于无法区分不同物体的音色以及探索生成不同物体声音的方法。解决这些挑战需要我们精心设计处理不同物体的模型或思想（例如，注意力机制）。音视频对应学习具有广泛的潜在应用，如刑事调查、医疗护理、交通运输等行业。许多研究尝试将不同的模态映射到共享特征空间。然而，由于从模糊的输入和目标模态中提取清晰有效的信息仍然困难，因此获得令人满意的结果具有挑战性。因此，充分的先验信息（人们通常关注的特定模式）对获得更准确的结果具有重要影响。音频和视觉生成关注于增强的机器想象力。与传统的判别性问题相比，跨模态生成的任务是适应概率分布之间的映射。因此，这通常是一个难以学习的多对多映射问题。此外，尽管音频和视觉模态之间的差异很大，人类对真实世界和生成结果之间的差异非常敏感，细微的伪影容易被察觉，这使得这一任务更具挑战性。最后，音视频表示学习可以视为其他任务的泛化。如前所述，电压表示的音频和RGB颜色空间表示的视觉是为了被人类感知而设计的，而不是让机器容易发现共同特征。困难在于只有两种模态且缺乏明确约束。因此，这项任务的主要挑战是找到合适的约束。无监督学习作为这项任务的普遍方法提供了精心设计的解决方案，但缺乏外部监督使得实现目标变得困难。弱监督方法的挑战在于找到正确的隐式监督。
- en: 7.2 Directions for Future Research
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 未来研究方向
- en: AVL has been an active research field for many years [[16](#bib.bib16), [17](#bib.bib17)]
    and is crucial to modern life. However, there are still many open questions in
    AVL due to the challenging nature of the domain itself and people’s increasing
    demands.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: AVL已经成为一个活跃的研究领域多年[[16](#bib.bib16), [17](#bib.bib17)]，并且对现代生活至关重要。然而，由于领域本身的挑战性和人们日益增长的需求，AVL仍然存在许多未解的问题。
- en: First, from a macro perspective, as AVL is a classic multimodality problem,
    its primary issue is to learn the mapping between modalities, specifically to
    map the attributes in audio and the objects in an image or a video. We think that
    mimicking the human learning process, e.g., by following the ideas of the attention
    mechanism and a memory bank may improve performance of learning this mapping.
    Furthermore, the second most difficult goal is to learn logical reasoning. Endowing
    a machine with the ability to reason is not only important for AVL but also an
    open question for the entire AI community. Instead of directly empowering a machine
    with the full logic capability, which is a long way to go from the current state
    of development, we can simplify this problem and consider fully utilizing the
    prior information and constructing the knowledge graph. Building a comprehensive
    knowledge graph and leveraging it in specific areas properly may help machine
    thinking.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从宏观角度来看，由于AVL是一个经典的多模态问题，其主要问题是学习模态之间的映射，特别是将音频中的属性与图像或视频中的对象进行映射。我们认为，模仿人类学习过程，例如，通过关注机制和记忆库的思想，可能会提高学习这种映射的性能。此外，第二个最困难的目标是学习逻辑推理。赋予机器推理能力不仅对AVL很重要，也是整个AI社区面临的一个开放问题。与其直接赋予机器完整的逻辑能力（这离当前的发展状态还有很长的路要走），不如简化这个问题，考虑充分利用先验信息并构建知识图谱。建立一个全面的知识图谱并在特定领域内合理利用它，可能有助于机器思考。
- en: 'As to each task we have summarized before, Sec. [2](#S2 "2 Audio-visual Separation
    and Localization ‣ Deep Audio-Visual Learning: A Survey") and Sec. [3](#S3 "3
    Audio-visual Correspondence Learning ‣ Deep Audio-Visual Learning: A Survey")
    can be referred to as the problem of ‘understanding’, while Sec. [4](#S4 "4 Audio
    and Visual Generation ‣ Deep Audio-Visual Learning: A Survey") and Sec. [5](#S5
    "5 Audio-visual Representation Learning ‣ Deep Audio-Visual Learning: A Survey")
    can be referred to as ‘generation’ and ‘representation learning’ respectively.
    Significant advances in understanding and generation tasks such as lip-reading,
    speaker separation, and talking face generation have recently been achieved for
    human faces. The domain of faces is comparatively simple yet important since the
    scenes are normally constrained, and it has a sizable amount of available useful
    prior information. For example, consider a 3d face model. These faces usually
    have neutral expressions, while the emotions that are the basis of the face have
    not been studied well. Furthermore, apart from faces, the more complicated in-the-wild
    scenes with more conditions are worth considering. Adapting models to the new
    varieties of audio (stereoscopic audio) or vision (3D video and AR) also leads
    in a new direction. The datasets, especially large and high-quality ones that
    can significantly improve the performance of machine learning, are fundamental
    to the research community [[141](#bib.bib141)]. However, collecting a dataset
    is labor- and time-intensive. Small-sample learning also benefits the application
    of AVL. Learning representations, which is a more general and basic form of other
    tasks, can also mitigate the dataset problem. While recent studies lacked sufficient
    prior information or supervision to guide the training procedure, exploring suitable
    prior information may allow models to learn better representations.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们之前总结的每个任务，Sec. [2](#S2 "2 视听分离与定位 ‣ 深度视听学习：综述") 和 Sec. [3](#S3 "3 视听对应学习
    ‣ 深度视听学习：综述") 可以视为‘理解’的问题，而 Sec. [4](#S4 "4 音频与视觉生成 ‣ 深度视听学习：综述") 和 Sec. [5](#S5
    "5 视听表征学习 ‣ 深度视听学习：综述") 则可以视为‘生成’和‘表征学习’。最近在理解和生成任务如唇读、说话人分离和说话脸生成方面取得了显著进展。人脸领域相对简单但重要，因为场景通常受到限制，并且有大量可用的有用先验信息。例如，考虑一个3D人脸模型。这些人脸通常有中性表情，而作为人脸基础的情感尚未得到充分研究。此外，除了人脸，更复杂的实际场景中包含更多条件也是值得考虑的。将模型适应新的音频（立体声）或视觉（3D视频和增强现实）的变化也指向了一个新的方向。数据集，特别是那些能够显著提高机器学习性能的大型和高质量数据集，对于研究社区至关重要[[141](#bib.bib141)]。然而，收集数据集既费力又耗时。小样本学习也有利于视听学习的应用。学习表征，作为其他任务的更一般和基本形式，也可以缓解数据集问题。尽管最近的研究缺乏足够的先验信息或监督来指导训练过程，但探索适当的先验信息可能使模型能够学习更好的表征。
- en: Finally, many studies focus on building more complex networks to improve performance,
    and the resulting networks generally entail unexplainable mechanisms. To make
    a model or an algorithm more robust and explainable, it is necessary to learn
    the essence of the earlier explainable algorithms to advance AVL.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多研究专注于构建更复杂的网络以提高性能，而结果网络通常涉及不可解释的机制。为了使模型或算法更具鲁棒性和可解释性，有必要学习早期可解释算法的本质，以推动视听学习的进步。
- en: 8 Conclusions
  id: totrans-569
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'The desire to better understand the world from the human perspective has drawn
    considerable attention to audio-visual learning in the deep learning community.
    This paper provides a comprehensive review of recent advances in audio-visual
    learning categorized into four research areas: audio-visual separation and localization,
    audio-visual correspondence learning, audio and visual generation, and audio-visual
    representation learning. Furthermore, we present a summary of datasets commonly
    used in audio-visual learning. The discussion section identifies the key challenges
    of each category followed by potential research directions.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类视角更好地理解世界的愿望引起了深度学习社区对视听学习的广泛关注。本文提供了对视听学习的最新进展的全面综述，分类为四个研究领域：视听分离与定位、视听对应学习、音频与视觉生成，以及视听表征学习。此外，我们总结了在视听学习中常用的数据集。讨论部分指出了每个类别的主要挑战，并提出了潜在的研究方向。
- en: References
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. V. Shannon, F.-G. Zeng, V. Kamath, J. Wygonski, and M. Ekelid, “Speech
    recognition with primarily temporal cues,” *Science*, pp. 303–304, 1995.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. V. Shannon, F.-G. Zeng, V. Kamath, J. Wygonski 和 M. Ekelid， “主要基于时间线索的语音识别，”
    *科学*，第303–304页，1995年。'
- en: '[2] G. Krishna, C. Tran, J. Yu, and A. H. Tewfik, “Speech recognition with
    no speech or with noisy speech,” in *ICASSP*, 2019, pp. 1090–1094.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Krishna, C. Tran, J. Yu 和 A. H. Tewfik，“在没有语音或有噪声语音的情况下进行语音识别，” 见 *ICASSP*，2019年，第1090–1094页。'
- en: '[3] R. He, W.-S. Zheng, and B.-G. Hu, “Maximum correntropy criterion for robust
    face recognition,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    pp. 1561–1576, 2010.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. He, W.-S. Zheng 和 B.-G. Hu，“用于鲁棒人脸识别的最大相关熵准则，” *IEEE 模式分析与机器智能汇刊*，第1561–1576页，2010年。'
- en: '[4] C. Fu, X. Wu, Y. Hu, H. Huang, and R. He, “Dual variational generation
    for low-shot heterogeneous face recognition,” *NeurIPS*, 2019.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Fu, X. Wu, Y. Hu, H. Huang 和 R. He，“低样本异构人脸识别的双变分生成，” *NeurIPS*，2019年。'
- en: '[5] A. Gabbay, A. Ephrat, T. Halperin, and S. Peleg, “Seeing through noise:
    Visually driven speaker separation and enhancement,” in *ICASSP*, 2018, pp. 3051–3055.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Gabbay, A. Ephrat, T. Halperin 和 S. Peleg，“穿透噪声：视觉驱动的说话人分离与增强，” 见 *ICASSP*，2018年，第3051–3055页。'
- en: '[6] T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep audio-visual
    speech enhancement,” *arXiv preprint arXiv:1804.04121*, 2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Afouras, J. S. Chung 和 A. Zisserman，“对话：深度音频-视觉语音增强，” *arXiv 预印本 arXiv:1804.04121*，2018年。'
- en: '[7] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T.
    Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: A speaker-independent
    audio-visual model for speech separation,” *ACM Trans. Graph.*, pp. 112:1–112:11,
    2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T.
    Freeman 和 M. Rubinstein，“在鸡尾酒会中注视以听：一种说话人独立的音频-视觉语音分离模型，” *ACM 图形学汇刊*，第112:1–112:11页，2018年。'
- en: '[8] P. Morgado, N. Vasconcelos, T. Langlois, and O. Wang, “Self-supervised
    generation of spatial audio for 360 video,” *arXiv preprint arXiv:1809.02587*,
    2018.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] P. Morgado, N. Vasconcelos, T. Langlois 和 O. Wang，“360度视频的自监督空间音频生成，” *arXiv
    预印本 arXiv:1809.02587*，2018年。'
- en: '[9] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,
    “Improved training of wasserstein gans,” in *NIPS*, 2017, pp. 5767–5777.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin 和 A. C. Courville，“改进的
    Wasserstein GANs 训练，” 见 *NIPS*，2017年，第5767–5777页。'
- en: '[10] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
    for generative adversarial networks,” in *CVPR*, 2019, pp. 4401–4410.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] T. Karras, S. Laine 和 T. Aila，“基于风格的生成对抗网络生成器架构，” 见 *CVPR*，2019年，第4401–4410页。'
- en: '[11] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review
    and new perspectives,” *IEEE transactions on pattern analysis and machine intelligence*,
    pp. 1798–1828, 2013.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y. Bengio, A. Courville 和 P. Vincent，“表示学习：综述与新视角，” *IEEE 模式分析与机器智能汇刊*，第1798–1828页，2013年。'
- en: '[12] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in *2017 IEEE
    International Conference on Computer Vision (ICCV)*, 2017, pp. 609–617.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] R. Arandjelovic 和 A. Zisserman，“看、听和学习，” 见 *2017 IEEE 国际计算机视觉大会（ICCV）*，2017年，第609–617页。'
- en: '[13] B. Korbar, D. Tran, and L. Torresani, “Co-training of audio and video
    representations from self-supervised temporal synchronization,” *arXiv preprint
    arXiv:1807.00230*, 2018.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] B. Korbar, D. Tran 和 L. Torresani，“从自监督时间同步中共同训练音频和视频表示，” *arXiv 预印本 arXiv:1807.00230*，2018年。'
- en: '[14] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-channel
    multi-speaker separation using deep clustering,” *arXiv preprint arXiv:1607.02173*,
    2016.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe 和 J. R. Hershey，“使用深度聚类的单通道多说话人分离，”
    *arXiv 预印本 arXiv:1607.02173*，2016年。'
- en: '[15] Y. Luo, Z. Chen, and N. Mesgarani, “Speaker-independent speech separation
    with deep attractor network,” *IEEE/ACM Transactions on Audio, Speech, and Language
    Processing*, pp. 787–796, 2018.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Luo, Z. Chen 和 N. Mesgarani，“使用深度吸引子网络的说话人独立语音分离，” *IEEE/ACM 音频、语音和语言处理汇刊*，第787–796页，2018年。'
- en: '[16] T. Darrell, J. W. Fisher, and P. Viola, “Audio-visual segmentation and
    “the cocktail party effect”,” in *Advances in Multimodal Interfaces—ICMI 2000*,
    2000, pp. 32–40.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] T. Darrell, J. W. Fisher 和 P. Viola，“音频-视觉分割与“鸡尾酒会效应”，” 见 *多模态接口进展——ICMI
    2000*，2000年，第32–40页。'
- en: '[17] J. W. Fisher III, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning
    joint statistical models for audio-visual fusion and segregation,” in *Advances
    in Neural Information Processing Systems 13*, 2001, pp. 772–778.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. W. Fisher III, T. Darrell, W. T. Freeman 和 P. A. Viola，“学习音频-视觉融合与分离的联合统计模型，”
    见 *神经信息处理系统进展 13*，2001年，第772–778页。'
- en: '[18] K. D. Bochen Li, Z. Duan, and G. Sharma, “See and listen: Score-informed
    association of sound tracks to players in chamber music performance videos,” in
    *IEEE International Conference on Acoustics, Speech and Signal Processing*, 2017.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] K. D. Bochen Li, Z. Duan, 和 G. Sharma，“观看和聆听：根据乐谱信息关联室内乐表演视频中的音轨与演奏者”，发表于
    *IEEE国际声学、语音和信号处理会议*，2017年。'
- en: '[19] J. Pu, Y. Panagakis, S. Petridis, and M. Pantic, “Audio-visual object
    localization and separation using low-rank and sparsity,” 2017.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Pu, Y. Panagakis, S. Petridis, 和 M. Pantic，“使用低秩和稀疏性进行音频-视觉物体定位和分离”，2017年。'
- en: '[20] R. Lu, Z. Duan, and C. Zhang, “Listen and look: Audio–visual matching
    assisted speech source separation,” *IEEE Signal Processing Letters*, pp. 1315–1319,
    2018.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. Lu, Z. Duan, 和 C. Zhang，“听与看：音频-视觉匹配辅助的语音源分离”，*IEEE信号处理快报*，第1315–1319页，2018年。'
- en: '[21] G. Morrone, S. Bergamaschi, L. Pasa, L. Fadiga, V. Tikhanoff, and L. Badino,
    “Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker
    environments,” in *ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2019, pp. 6900–6904.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] G. Morrone, S. Bergamaschi, L. Pasa, L. Fadiga, V. Tikhanoff, 和 L. Badino，“基于面部标志的讲者独立音频-视觉语音增强在多说话者环境中的应用”，发表于
    *ICASSP 2019-2019 IEEE国际声学、语音和信号处理会议（ICASSP）*，2019年，第6900–6904页。'
- en: '[22] J. Hershey and J. Movellan, “Audio-vision: Using audio-visual synchrony
    to locate sounds,” in *Advances in Neural Information Processing Systems 12*,
    2000, pp. 813–819.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Hershey 和 J. Movellan，“音频-视觉：利用音频-视觉同步定位声音”，发表于 *神经信息处理系统进展第12卷*，2000年，第813–819页。'
- en: '[23] H. L. Van Trees, *Optimum array processing: Part IV of detection, estimation,
    and modulation theory*.   John Wiley & Sons, 2004.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. L. Van Trees, *最优阵列处理：检测、估计和调制理论第IV部分*。约翰·威利父子公司，2004年。'
- en: '[24] A. Zunino, M. Crocco, S. Martelli, A. Trucco, A. Del Bue, and V. Murino,
    “Seeing the sound: A new multimodal imaging device for computer vision,” in *Proceedings
    of the IEEE International Conference on Computer Vision Workshops*, 2015, pp.
    6–14.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Zunino, M. Crocco, S. Martelli, A. Trucco, A. Del Bue, 和 V. Murino，“看声音：一种新的多模态成像设备用于计算机视觉”，发表于
    *IEEE国际计算机视觉会议论文集*，2015年，第6–14页。'
- en: '[25] R. Gao, R. Feris, and K. Grauman, “Learning to separate object sounds
    by watching unlabeled video,” in *ECCV*, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. Gao, R. Feris, 和 K. Grauman，“通过观看未标记的视频学习分离物体声音”，发表于 *ECCV*，2018年。'
- en: '[26] A. Senocak, T.-H. Oh, J. Kim, M.-H. Yang, and I. S. Kweon, “Learning to
    localize sound source in visual scenes,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 4358–4366.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Senocak, T.-H. Oh, J. Kim, M.-H. Yang, 和 I. S. Kweon，“学习在视觉场景中定位声音源”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2018年，第4358–4366页。'
- en: '[27] Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu, “Audio-visual event localization
    in unconstrained videos,” in *Computer Vision - ECCV 2018 - 15th European Conference,
    Munich, Germany, September 8-14, 2018, Proceedings, Part II*, 2018, pp. 252–268.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Tian, J. Shi, B. Li, Z. Duan, 和 C. Xu，“在非约束视频中的音频-视觉事件定位”，发表于 *计算机视觉
    - ECCV 2018 - 第15届欧洲会议，德国慕尼黑，2018年9月8-14日，论文集，第二部分*，2018年，第252–268页。'
- en: '[28] H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott, and A. Torralba,
    “The sound of pixels,” *arXiv preprint arXiv:1804.03160*, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott, 和 A. Torralba，“像素的声音”，*arXiv预印本
    arXiv:1804.03160*，2018年。'
- en: '[29] H. Zhao, C. Gan, W. Ma, and A. Torralba, “The sound of motions,” *CoRR*,
    2019.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] H. Zhao, C. Gan, W. Ma, 和 A. Torralba，“运动的声音”，*CoRR*，2019年。'
- en: '[30] A. Rouditchenko, H. Zhao, C. Gan, J. H. McDermott, and A. Torralba, “Self-supervised
    audio-visual co-segmentation,” *CoRR*, 2019.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Rouditchenko, H. Zhao, C. Gan, J. H. McDermott, 和 A. Torralba，“自监督音频-视觉共同分割”，*CoRR*，2019年。'
- en: '[31] S. Parekh, A. Ozerov, S. Essid, N. Q. K. Duong, P. Pérez, and G. Richard,
    “Identify, locate and separate: Audio-visual object extraction in large video
    collections using weak supervision,” *CoRR*, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Parekh, A. Ozerov, S. Essid, N. Q. K. Duong, P. Pérez, 和 G. Richard，“识别、定位和分离：使用弱监督在大型视频集合中进行音频-视觉物体提取”，*CoRR*，2018年。'
- en: '[32] H. Izadinia, I. Saleemi, and M. Shah, “Multimodal analysis for identification
    and segmentation of moving-sounding objects,” *IEEE Transactions on Multimedia*,
    pp. 378–390, 2013.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] H. Izadinia, I. Saleemi, 和 M. Shah，“多模态分析用于识别和分割移动发声物体”，*IEEE多媒体学报*，第378–390页，2013年。'
- en: '[33] E. Hoffer and N. Ailon, “Deep metric learning using triplet network,”
    in *International Workshop on Similarity-Based Pattern Recognition*, 2015, pp.
    84–92.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] E. Hoffer 和 N. Ailon，“使用三重网络进行深度度量学习”，发表于 *基于相似性的模式识别国际研讨会*，2015年，第84–92页。'
- en: '[34] K. Sriskandaraja, V. Sethu, and E. Ambikairajah, “Deep siamese architecture
    based replay detection for secure voice biometric.” in *Interspeech*, 2018, pp.
    671–675.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] R. Białobrzeski, M. Kośmider, M. Matuszewski, M. Plata, and A. Rakowski,
    “Robust bayesian and light neural networks for voice spoofing detection,” *Proc.
    Interspeech 2019*, pp. 1028–1032, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Gomez-Alanis, A. M. Peinado, J. A. Gonzalez, and A. M. Gomez, “A light
    convolutional gru-rnn deep feature extractor for asv spoofing detection,” *Proc.
    Interspeech 2019*, pp. 1068–1072, 2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] X. Wu, R. He, Z. Sun, and T. Tan, “A light cnn for deep face representation
    with noisy labels,” *IEEE Transactions on Information Forensics and Security*,
    pp. 2884–2896, 2018.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Nagrani, S. Albanie, and A. Zisserman, “Seeing voices and hearing faces:
    Cross-modal biometric matching,” *CoRR*, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Torfi, S. M. Iranmanesh, N. M. Nasrabadi, and J. M. Dawson, “Coupled
    3d convolutional neural networks for audio-visual recognition,” *CoRR*, 2017.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] K. Simonyan and A. Zisserman, “Two-Stream Convolutional Networks for Action
    Recognition in Videos,” in *Advances in Neural Information Processing Systems
    27*, 2014, pp. 568–576.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Wen, M. A. Ismail, W. Liu, B. Raj, and R. Singh, “Disjoint Mapping
    Network for Cross-modal Matching of Voices and Faces,” *ArXiv e-prints*, p. arXiv:1807.04836,
    2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” *CoRR*, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] C. Lippert, R. Sabatini, M. C. Maher, E. Y. Kang, S. Lee, O. Arikan, A. Harley,
    A. Bernal, P. Garst, V. Lavrenko, K. Yocum, T. Wong, M. Zhu, W.-Y. Yang, C. Chang,
    T. Lu, C. W. H. Lee, B. Hicks, S. Ramakrishnan, H. Tang, C. Xie, J. Piper, S. Brewerton,
    Y. Turpaz, A. Telenti, R. K. Roby, F. J. Och, and J. C. Venter, “Identification
    of individuals by trait prediction using whole-genome sequencing data,” *Proceedings
    of the National Academy of Sciences*, pp. 10 166–10 171, 2017.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] K. Hoover, S. Chaudhuri, C. Pantofaru, M. Slaney, and I. Sturdy, “Putting
    a face to the voice: Fusing audio and visual signals across a video to determine
    speakers,” *CoRR*, 2017.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S.-W. Chung, J. Son Chung, and H.-G. Kang, “Perfect match: Improved cross-modal
    embeddings for audio-visual synchronisation,” *ArXiv e-prints*, p. arXiv:1809.08001,
    2018.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Wang, H. Huang, X. Zhang, J. Ma, and A. Zheng, “A novel distance learning
    for elastic cross-modal audio-visual matching,” in *2019 IEEE International Conference
    on Multimedia & Expo Workshops (ICMEW)*, 2019, pp. 300–305.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] R. K. Srihari, “Combining text and image information in content-based
    retrieval,” in *Proceedings., International Conference on Image Processing*, 1995,
    pp. 326–329 vol.1.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] L. R. Long, L. E. Berman, and G. R. Thoma, “Prototype client/server application
    for biomedical text/image retrieval on the Internet,” in *Storage and Retrieval
    for Still Image and Video Databases IV*, 1996, pp. 362 – 372.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,
    R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,”
    in *Proceedings of the 18th ACM International Conference on Multimedia*, 2010,
    pp. 251–260.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Aytar, C. Vondrick, and A. Torralba, “See, hear, and read: Deep aligned
    representations,” *CoRR*, 2017.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] D. Surís, A. Duarte, A. Salvador, J. Torres, and X. Giró i Nieto, “Cross-modal
    embeddings for video and audio retrieval,” *CoRR*, 2018.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Hong, W. Im, and H. S. Yang, “Deep learning for content-based, cross-modal
    retrieval of videos and music,” *CoRR*, 2017.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Nagrani, S. Albanie, and A. Zisserman, “Learnable pins: Cross-modal
    embeddings for person identity,” *CoRR*, 2018.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] R. Wang, H. Huang, X. Zhang, J. Ma, and A. Zheng, “A novel distance learning
    for elastic cross-modal audio-visual matching,” in *2019 IEEE International Conference
    on Multimedia Expo Workshops (ICMEW)*, 2019, pp. 300–305.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Petridis, Z. Li, and M. Pantic, “End-to-end visual speech recognition
    with lstms,” in *Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International
    Conference on*, 2017, pp. 2592–2596.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Wand, J. Koutník, and J. Schmidhuber, “Lipreading with long short-term
    memory,” in *Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International
    Conference on*, 2016, pp. 6115–6119.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. M. Assael, B. Shillingford, S. Whiteson, and N. de Freitas, “Lipnet:
    Sentence-level lipreading,” *arXiv preprint*, 2016.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. S. Chung, A. W. Senior, O. Vinyals, and A. Zisserman, “Lip reading
    sentences in the wild,” in *CVPR*, 2017, pp. 3444–3453.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller,
    and S. Zafeiriou, “Adieu features? end-to-end speech emotion recognition using
    a deep convolutional recurrent network,” in *Acoustics, Speech and Signal Processing
    (ICASSP), 2016 IEEE International Conference on*, 2016, pp. 5200–5204.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Deep
    audio-visual speech recognition,” *arXiv preprint arXiv:1809.02108*, 2018.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Dupont and J. Luettin, “Audio-visual speech modeling for continuous
    speech recognition,” *IEEE transactions on multimedia*, pp. 141–151, 2000.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Petridis and M. Pantic, “Prediction-based audiovisual fusion for classification
    of non-linguistic vocalisations,” *IEEE Transactions on Affective Computing*,
    pp. 45–58, 2016.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] G. Potamianos, C. Neti, G. Gravier, A. Garg, and A. W. Senior, “Recent
    advances in the automatic recognition of audiovisual speech,” *Proceedings of
    the IEEE*, pp. 1306–1326, 2003.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] D. Hu, X. Li *et al.*, “Temporal multimodal learning in audiovisual speech
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 3574–3582.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, “Multimodal
    deep learning,” in *Proceedings of the 28th international conference on machine
    learning (ICML-11)*, 2011, pp. 689–696.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Ninomiya, N. Kitaoka, S. Tamura, Y. Iribe, and K. Takeda, “Integration
    of deep bottleneck features for audio-visual speech recognition,” in *Sixteenth
    Annual Conference of the International Speech Communication Association*, 2015.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. Stafylakis and G. Tzimiropoulos, “Combining residual networks with
    lstms for lipreading,” *arXiv preprint arXiv:1703.04105*, 2017.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Nussbaum-Thom, J. Cui, B. Ramabhadran, and V. Goel, “Acoustic modeling
    using bidirectional gated recurrent convolutional units,” 2016, pp. 390–394.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” *arXiv preprint
    arXiv:1701.07875*, 2017.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] L. Chen, S. Srivastava, Z. Duan, and C. Xu, “Deep cross-modal audio-visual
    generation,” in *Proceedings of the on Thematic Workshops of ACM Multimedia 2017*,
    2017, pp. 349–357.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] H. Zhu, A. Zheng, H. Huang, and R. He, “High-resolution talking face generation
    via mutual information approximation,” *arXiv preprint arXiv:1812.06589*, 2018.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to bridge
    domain gap for person re-identification,” in *CVPR*, 2018, pp. 79–88.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, and S.-H. Lai,
    “Auggan: Cross domain adaptation with gan-based data augmentation,” in *ECCV*,
    2018, pp. 718–731.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] T. L. Cornu and B. Milner, “Reconstructing intelligible audio speech from
    visual speech features,” in *Sixteenth Annual Conference of the International
    Speech Communication Association*, 2015.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] A. Ephrat, T. Halperin, and S. Peleg, “Improved speech reconstruction
    from silent video,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2017, pp. 455–462.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] T. Thomas Le Cornu and B. Milner, “Generating intelligible audio speech
    from visual speech,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    pp. 1751–1761, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Davis, M. Rubinstein, N. Wadhwa, G. J. Mysore, F. Durand, and W. T.
    Freeman, “The visual microphone: passive recovery of sound from video,” 2014.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adelson, and W. T.
    Freeman, “Visually indicated sounds,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2016, pp. 2405–2413.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Zhou, Z. Wang, C. Fang, T. Bui, and T. L. Berg, “Visual to sound: Generating
    natural sound for videos in the wild,” *arXiv preprint arXiv:1712.01393*, 2017.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] A. Ephrat and S. Peleg, “Vid2speech: Speech reconstruction from silent
    video,” in *2017 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2017, pp. 5095–5099.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo, A. Courville,
    and Y. Bengio, “Samplernn: An unconditional end-to-end neural audio generation
    model,” *arXiv preprint arXiv:1612.07837*, 2016.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C.-H. Wan, S.-P. Chuang, and H.-Y. Lee, “Towards audio to scene image
    synthesis using generative adversarial network,” *arXiv preprint arXiv:1808.04108*,
    2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Qiu and H. Kataoka, “Image generation associated with music data,”
    in *2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops,
    CVPR Workshops 2018, Salt Lake City, UT, USA, June 18-22, 2018*, 2018, pp. 2510–2513.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] W. Hao, Z. Zhang, and H. Guan, “Cmcgan: A uniform framework for cross-modal
    visual-audio mutual generation,” *arXiv preprint arXiv:1711.08102*, 2017.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador, E. Mohedano,
    K. McGuinness, J. Torres, and X. Giro-i Nieto, “Speech-conditioned face generation
    using generative adversarial networks,” 2019.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    and W. Matusik, “Speech2face: Learning the face behind a voice,” *arXiv preprint
    arXiv:1905.09773*, 2019.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] O. Alemi, J. Françoise, and P. Pasquier, “Groovenet: Real-time music-driven
    dance movement generation using artificial neural networks,” *networks*, vol. 8,
    no. 17, p. 26, 2017.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] J. Lee, S. Kim, and K. Lee, “Listen to dance: Music-driven choreography
    generation using autoregressive encoder-decoder network,” *CoRR*, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] E. Shlizerman, L. Dery, H. Schoen, and I. Kemelmacher-Shlizerman, “Audio
    to body dynamics,” in *Proc. CVPR*, 2018.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Tang, J. Jia, and H. Mao, “Dance with melody: An lstm-autoencoder approach
    to music-oriented dance synthesis,” in *2018 ACM Multimedia Conference on Multimedia
    Conference, MM 2018, Seoul, Republic of Korea, October 22-26, 2018*, 2018, pp.
    1598–1606.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] N. Yalta, S. Watanabe, K. Nakadai, and T. Ogata, “Weakly supervised deep
    recurrent neural networks for basic dance step generation,” *CoRR*, 2018.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] R. Kumar, J. Sotelo, K. Kumar, A. de Brébisson, and Y. Bengio, “Obamanet:
    Photo-realistic lip-sync from text,” *arXiv preprint arXiv:1801.01442*, 2017.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-Shlizerman, “Synthesizing
    obama: learning lip sync from audio,” *ACM Transactions on Graphics (TOG)*, p. 95,
    2017.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. S. Chung, A. Jamaludin, and A. Zisserman, “You said that?” *CoRR*,
    2017.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. A. Jalalifar, H. Hasani, and H. Aghajan, “Speech-driven facial reenactment
    using conditional generative adversarial networks,” *CoRR*, 2018.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven facial
    animation with temporal gans,” in *BMVC*, 2018.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] L. Chen, Z. Li, R. K. Maddox, Z. Duan, and C. Xu, “Lip movements generation
    at a glance,” *CoRR*, 2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation
    by adversarially disentangled audio-visual representation,” *CoRR*, 2018.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Z. D. C. X. Lele Chen, Ross K Maddox, “Hierarchical cross-modal talking
    face generation with dynamic pixel-wise loss,” in *The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2019.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] O. Wiles, A. Koepke, and A. Zisserman, “X2face: A network for controlling
    face generation by using images, audio, and pose codes,” in *European Conference
    on Computer Vision*, 2018.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] A. Samadani, E. Kubica, R. Gorbet, and D. Kulic, “Perception and generation
    of affective hand movements,” *I. J. Social Robotics*, pp. 35–51, 2013.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Tilmanne and T. Dutoit, “Expressive gait synthesis using PCA and gaussian
    modeling,” in *Motion in Games - Third International Conference, MIG 2010, Utrecht,
    The Netherlands, November 14-16, 2010\. Proceedings*, 2010, pp. 363–374.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] M. Brand and A. Hertzmann, “Style machines,” in *Proceedings of the 27th
    Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2000,
    New Orleans, LA, USA, July 23-28, 2000*, 2000, pp. 183–192.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. M. Wang, D. J. Fleet, and A. Hertzmann, “Multifactor gaussian process
    models for style-content separation,” in *Machine Learning, Proceedings of the
    Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June
    20-24, 2007*, 2007, pp. 975–982.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] G. W. Taylor and G. E. Hinton, “Factored conditional restricted boltzmann
    machines for modeling motion style,” in *Proceedings of the 26th Annual International
    Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18,
    2009*, 2009, pp. 1025–1032.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] L. Crnkovic-Friis and L. Crnkovic-Friis, “Generative choreography using
    deep learning,” *CoRR*, 2016.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] D. Holden, J. Saito, and T. Komura, “A deep learning framework for character
    motion synthesis and editing,” *ACM Transactions on Graphics (TOG)*, p. 138, 2016.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Graves and J. Schmidhuber, “Framewise phoneme classification with
    bidirectional lstm and other neural network architectures,” *Neural Networks*,
    pp. 602–610, 2005.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Saito, E. Matsumoto, and S. Saito, “Temporal generative adversarial
    nets with singular value clipping,” in *IEEE International Conference on Computer
    Vision (ICCV)*, 2017, p. 5.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Aytar, C. Vondrick, and A. Torralba, “Soundnet: Learning sound representations
    from unlabeled video,” in *Advances in Neural Information Processing Systems*,
    2016, pp. 892–900.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] K. Leidal, D. Harwath, and J. R. Glass, “Learning modality-invariant
    representations for speech and images,” *2017 IEEE Automatic Speech Recognition
    and Understanding Workshop (ASRU)*, pp. 424–429, 2017.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] R. Arandjelović and A. Zisserman, “Objects that sound,” *arXiv preprint
    arXiv:1712.06651*, 2017.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Parekh, S. Essid, A. Ozerov, N. Q. Duong, P. Pérez, and G. Richard,
    “Weakly supervised representation learning for unsynchronized audio-visual events,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops*, 2018, pp. 2518–2519.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] D. Hu, F. Nie, and X. Li, “Deep co-clustering for unsupervised audiovisual
    learning,” *arXiv preprint arXiv:1807.03094*, 2018.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised
    multisensory features,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 631–648.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proceedings of the 26th annual international conference on machine learning*,
    2009, pp. 41–48.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An audio-visual corpus
    for speech perception and automatic speech recognition,” *The Journal of the Acoustical
    Society of America*, pp. 2421–2424, 2006.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] N. Alghamdi, S. Maddock, R. Marxer, J. Barker, and G. J. Brown, “A corpus
    of audio-visual lombard speech with frontal and profile views,” *The Journal of
    the Acoustical Society of America*, pp. EL523–EL529, 2018.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] N. Harte and E. Gillen, “Tcd-timit: An audio-visual corpus of continuous
    speech,” *IEEE Transactions on Multimedia*, pp. 603–615, 2015.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Sanderson and B. C. Lovell, “Multi-region probabilistic histograms
    for robust and scalable identity inference,” in *International Conference on Biometrics*,
    2009, pp. 199–208.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. R. Livingstone and F. A. Russo, “The ryerson audio-visual database
    of emotional speech and song (ravdess): A dynamic, multimodal set of facial and
    vocal expressions in north american english,” *PloS one*, p. e0196391, 2018.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Kossaifi, R. Walecki, Y. Panagakis, J. Shen, M. Schmitt, F. Ringeval,
    J. Han, V. Pandit, B. Schuller, K. Star *et al.*, “Sewa db: A rich database for
    audio-visual emotion and sentiment research in the wild,” *arXiv preprint arXiv:1901.02839*,
    2019.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] G. Zhao, M. Barnard, and M. Pietikainen, “Lipreading with local spatiotemporal
    descriptors,” *IEEE Transactions on Multimedia*, pp. 1254–1265, 2009.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] I. Anina, Z. Zhou, G. Zhao, and M. Pietikäinen, “Ouluvs2: A multi-view
    audiovisual database for non-rigid mouth motion analysis,” in *Automatic Face
    and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops
    on*, 2015, pp. 1–5.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker
    identification dataset,” *arXiv preprint arXiv:1706.08612*, 2017.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,”
    *arXiv preprint arXiv:1806.05622*, 2018.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. S. Chung and A. Zisserman, “Lip reading in the wild,” in *Asian Conference
    on Computer Vision*, 2016, pp. 87–103.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] ——, “Lip reading in profile,” in *British Machine Vision Conference*,
    2017.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Roth, S. Chaudhuri, O. Klejch, R. Marvin, A. Gallagher, L. Kaver,
    S. Ramaswamy, A. Stopczynski, C. Schmid, Z. Xi *et al.*, “Ava-activespeaker: An
    audio-visual dataset for active speaker detection,” *arXiv preprint arXiv:1901.01342*,
    2019.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] A. Bazzica, J. van Gemert, C. C. Liem, and A. Hanjalic, “Vision-based
    detection of acoustic timed events: a case study on clarinet note onsets,” *arXiv
    preprint arXiv:1706.09556*, 2017.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] O. Gillet and G. Richard, “Enst-drums: an extensive audio-visual database
    for drum signals processing.” in *ISMIR*, 2006, pp. 156–159.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] B. Li, X. Liu, K. Dinesh, Z. Duan, and G. Sharma, “Creating a multitrack
    classical music performance dataset for multimodal music analysis: Challenges,
    insights, and applications,” *IEEE Transactions on Multimedia*, pp. 522–535, 2019.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,
    and S. Vijayanarasimhan, “Youtube-8m: A large-scale video classification benchmark,”
    *arXiv preprint arXiv:1609.08675*, 2016.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
    for audio events,” in *Acoustics, Speech and Signal Processing (ICASSP), 2017
    IEEE International Conference on*, 2017, pp. 776–780.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *et al.*, “The kinetics human action video
    dataset,” *arXiv preprint arXiv:1705.06950*, 2017.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman,
    “A short note about kinetics-600,” *arXiv preprint arXiv:1808.01340*, 2018.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short note on
    the kinetics-700 human action dataset,” *arXiv preprint arXiv:1907.06987*, 2019.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 6047–6056.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting unreasonable
    effectiveness of data in deep learning era,” in *ICCV*, 2017, pp. 843–852.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
