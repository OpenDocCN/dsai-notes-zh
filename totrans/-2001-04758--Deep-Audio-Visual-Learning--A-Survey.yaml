- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2001.04758] Deep Audio-Visual Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2001.04758](https://ar5iv.labs.arxiv.org/html/2001.04758)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \firstheadname\firstfootname\headevenname\headoddname
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Audio-Visual Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hao Zhu^(1,2)   Mandi Luo^(2,3)   Rui Wang^(1,2)   Aihua Zheng¹   Ran He^(2,3,4)
    ¹School of Computer Science and Technology, Anhui University, Hefei, China
  prefs: []
  type: TYPE_NORMAL
- en: ²Center for Research on Intelligent Perception and Computing (CRIPAC) and National
    Laboratory of Pattern Recognition (NLPR), CASIA, Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: ³School of Artificial Intelligence, University of the Chinese Academy of Sciences,
    Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Center for Excellence in Brain Science and Intelligence Technology, CAS, Beijing,
    China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Audio-visual learning, aimed at exploiting the relationship between audio and
    visual modalities, has drawn considerable attention since deep learning started
    to be used successfully. Researchers tend to leverage these two modalities either
    to improve the performance of previously considered single-modality tasks or to
    address new challenging problems. In this paper, we provide a comprehensive survey
    of recent audio-visual learning development. We divide the current audio-visual
    learning tasks into four different subfields: audio-visual separation and localization,
    audio-visual correspondence learning, audio-visual generation, and audio-visual
    representation learning. State-of-the-art methods as well as the remaining challenges
    of each subfield are further discussed. Finally, we summarize the commonly used
    datasets and performance metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: audio-visual learning, deep learning, survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human perception is multidimensional and includes vision, hearing, touch, taste,
    and smell. In recent years, along with the vigorous development of artificial
    intelligence technology, the trend from single-modality learning to multimodality
    learning has become crucial to better machine perception. Analyses of audio and
    visual information, representing the two most important perceptual modalities
    in our daily life, have been widely developed in both academia and industry in
    the past decades. Prominent achievements include speech recognition [[1](#bib.bib1),
    [2](#bib.bib2)], facial recognition [[3](#bib.bib3), [4](#bib.bib4)], etc. Audio-visual
    learning (AVL) using both modalities has been introduced to overcome the limitation
    of perception tasks in each modality. In addition, exploring the relationship
    between audio and visual information leads to more interesting and important research
    topics and ultimately better perspectives on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this article is to provide an overview of the key methodologies
    in audio-visual learning, which aims to discover the relationship between audio
    and visual data for many challenging tasks. In this paper, we mainly divide these
    efforts into four categories: (1) audio-visual separation and localization, (2)
    audio-visual corresponding learning, (3) audio and visual generation, and (4)
    audio-visual representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Audio-visual separation and localization aim to separate specific sounds emanating
    from the corresponding objects and localize each sound in the visual context,
    as illustrated in Fig. LABEL:fig:overall (a). Audio separation has been investigated
    extensively in the signal processing community during the past two decades. With
    the addition of the visual modality, audio separation can be transformed into
    audio-visual separation, which has proven to be more effective in noisy scenes
    [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]. Furthermore, introducing the
    visual modality allows for audio localization, i.e., the localization of a sound
    in the visual modality according to the audio input. The tasks of audio-visual
    separation and localization themselves not only lead to valuable applications
    but also provide the foundation for other audio-visual tasks, e.g., generating
    spatial audio for 360^∘ video [[8](#bib.bib8)]. Most studies in this area focus
    on unsupervised learning due to the lack of training labels.
  prefs: []
  type: TYPE_NORMAL
- en: Audio-visual correspondence learning focuses on discovering the global semantic
    relation between audio and visual modalities, as shown in Fig. LABEL:fig:overall
    (b). It consists of audio-visual retrieval and audio-visual speech recognition
    tasks. The former uses audio or an image to search for its counterpart in another
    modality, while the latter derives from the conventional speech recognition task
    that leverages visual information to provide a more semantic prior to improve
    recognition performance. Although both of these two tasks have been extensively
    studied, they still entail major challenges, especially for fine-grained cross-modality
    retrieval and homonyms in speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Audio-visual generation tries to synthesize the other modality based on one
    of them, which is different from the above two tasks leveraging both audio and
    visual modalities as inputs. Trying to make a machine that is creative is always
    challenging, and many generative models have been proposed [[9](#bib.bib9), [10](#bib.bib10)].
    Audio-visual cross-modality generation has recently drawn considerable attention.
    It aims to generate audio from visual signals, or vice versa. Although it is easy
    for a human to perceive the natural correlation between sounds and appearance,
    this task is challenging for machines due to heterogeneity across modalities.
    As shown in Fig. LABEL:fig:overall (c), vision to audio generation mainly focuses
    on recovering speech from lip sequences or predicting the sounds that may occur
    in the given scenes. In contrast, audio to vision generation can be classified
    into three categories: audio-driven image generation, body motion generation,
    and talking face generation.'
  prefs: []
  type: TYPE_NORMAL
- en: The last task—audio-visual representation learning—aims to automatically discover
    the representation from raw data. A human can easily recognize audio or video
    based on long-term brain cognition. However, machine learning algorithms such
    as deep learning models are heavily dependent on data representation. Therefore,
    learning suitable data representations for machine learning algorithms may improve
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, real-world data such as images, videos and audio do not possess
    specific algorithmically defined features [[11](#bib.bib11)]. Therefore, an effective
    representation of data determines the success of machine learning algorithms.
    Recent studies seeking better representation have designed various tasks, such
    as audio-visual correspondence (AVC) [[12](#bib.bib12)] and audio-visual temporal
    synchronization (AVTS) [[13](#bib.bib13)]. By leveraging such a learned representation,
    one can more easily solve audio-visual tasks mentioned in the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we present a comprehensive survey of the above four directions
    of audio-visual learning. The rest of this paper is organized as follows. We introduce
    the four directions in Secs. [2](#S2 "2 Audio-visual Separation and Localization
    ‣ Deep Audio-Visual Learning: A Survey"), [3](#S3 "3 Audio-visual Correspondence
    Learning ‣ Deep Audio-Visual Learning: A Survey"), [4](#S4 "4 Audio and Visual
    Generation ‣ Deep Audio-Visual Learning: A Survey") and [5](#S5 "5 Audio-visual
    Representation Learning ‣ Deep Audio-Visual Learning: A Survey"). Sec. [6](#S6
    "6 Recent Public Audio-visual Datasets ‣ Deep Audio-Visual Learning: A Survey")
    summarizes the commonly used public audio-visual datasets. Finally, Sec. [8](#S8
    "8 Conclusions ‣ Deep Audio-Visual Learning: A Survey") concludes the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Audio-visual Separation and Localization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The objective of audio-visual separation is to separate different sounds from
    the corresponding objects, while audio-visual localization mainly focuses on localizing
    a sound in a visual context. As shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1 Speaker
    Separation ‣ 2 Audio-visual Separation and Localization ‣ Deep Audio-Visual Learning:
    A Survey"), we classify types of this task by different identities: speakers (Fig.
    [1](#S2.F1 "Figure 1 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and
    Localization ‣ Deep Audio-Visual Learning: A Survey") (a)) and objects (Fig. [1](#S2.F1
    "Figure 1 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and Localization
    ‣ Deep Audio-Visual Learning: A Survey") (b)).The former concentrates on a person’s
    speech that can be used for television programs to enhance the target speakers’
    voice, while the latter is a more general and challenging task that separates
    arbitrary objects rather than speakers only. In this section, we provide an overview
    of these two tasks, examining the motivations, network architectures, advantages,
    and disadvantages.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Speaker Separation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The speaker separation task is a challenging task and is also known as the ‘cocktail
    party problem’. It aims to isolate a single speech signal in a noisy scene. Some
    studies tried to solve the problem of audio separation with only the audio modality
    and achieved exciting results [[14](#bib.bib14), [15](#bib.bib15)]. Advanced approaches [[5](#bib.bib5),
    [7](#bib.bib7)] tried to utilize visual information to aid the speaker separation
    task and significantly surpassed single modality-based methods. The early attempts
    leveraged mutual information to learn the joint distribution between the audio
    and the video [[16](#bib.bib16), [17](#bib.bib17)]. Subsequently, several methods
    focused on analyzing videos containing salient motion signals and the corresponding
    audio events (e.g., a mouth starting to move or a hand on piano suddenly accelerating)
    [[18](#bib.bib18), [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c55ef362af1c7db9a2049b2d00958289.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of audio-visual separation and localization task. Paths
    1 and 2 denote separation and localization tasks, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e164400132110dd731c0f1589692f253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Basic pipeline of a noisy audio filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gabbay et al.[[5](#bib.bib5)] proposed isolating the voice of a specific speaker
    and eliminating other sounds in an audio-visual manner. Instead of directly extracting
    the target speaker’s voice from the noisy sound, which may bias the training model,
    the researchers first fed the video frames into a video-to-speech model and then
    predicted the speaker’s voice by the facial movements captured in the video. Afterwards,
    the predicted voice was used to filter the mixtures of sounds, as shown in Fig.
    [2](#S2.F2 "Figure 2 ‣ 2.1 Speaker Separation ‣ 2 Audio-visual Separation and
    Localization ‣ Deep Audio-Visual Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Although Gabbay et al. [[5](#bib.bib5)] improved the quality of separated voice
    by adding the visual modality, their approach was only applicable in controlled
    environments. To obtain intelligible speech in an unconstrained environment, Afouras
    et al. [[6](#bib.bib6)] proposed a deep audio-visual speech enhancement network
    to separate the speaker’s voice of the given lip region by predicting both the
    magnitude and phase of the target signal. The authors treated the spectrograms
    as temporal signals rather than images for a network. Additionally, instead of
    directly predicting clean signal magnitudes, they also tried to generate a more
    effective soft mask for filtering.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to previous approaches that require training a separate model for
    each speaker of interest (speaker-dependent models), Ephrat et al. [[7](#bib.bib7)]
    proposed a speaker-independent model that was only trained once and was then applicable
    to any speaker. This approach even outperformed the state-of-the-art speaker-dependent
    audio-visual speech separation methods. The relevant model consists of multiple
    visual streams and one audio stream, concatenating the features from different
    streams into a joint audio-visual representation. This feature is further processed
    by a bidirectional LSTM and three fully connected layers. Finally, an elaborate
    spectrogram mask is learned for each speaker to be multiplied by the noisy input.
    Finally, the researchers converted it back to waveforms to obtain an isolated
    speech signal for each speaker. Lu et al. [[20](#bib.bib20)] designed a network
    similar to that of [[7](#bib.bib7)]. The difference is that the authors enforced
    an audio-visual matching network to distinguish the correspondence between speech
    and human lip movements. Therefore, they could obtain clear speech.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of directly utilizing video as a condition, Morrone et al. [[21](#bib.bib21)]
    further introduced landmarks as a fine-grained feature to generate time-frequency
    masks to filter mixed-speech spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Separating and Localizing Objects’ Sounds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of matching a specific lip movement from a noisy environment as in the
    speaker separation task, humans focus more on objects while dealing with sound
    separation and localization. It is difficult to find a clear correspondence between
    audio and visual modalities due to the challenge of exploring the prior sounds
    from different objects.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Separation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The early attempt to solve this localization problem can be traced back to 2000
    [[22](#bib.bib22)] and a study that synchronized low-level features of sounds
    and videos. Fisher et al. [[17](#bib.bib17)] later proposed using a nonparametric
    approach to learn a joint distribution of visual and audio signals and then project
    both of them to a learned subspace. Furthermore, several acoustics-based methods
    [[23](#bib.bib23), [24](#bib.bib24)] were described that required specific devices
    for surveillance and instrument engineering, such as microphone arrays used to
    capture the differences in the arrival of sounds.
  prefs: []
  type: TYPE_NORMAL
- en: To learn audio source separation from large-scale in-the-wild videos containing
    multiple audio sources per video, Gao et al. [[25](#bib.bib25)] suggested learning
    an audio-visual localization model from unlabeled videos and then exploiting the
    visual context for audio source separation. Researchers’ approach relied on a
    multi-instance multilabel learning framework to disentangle the audio frequencies
    related to individual visual objects even without observing or hearing them in
    isolation. The multilabel learning framework was fed by a bag of audio basis vectors
    for each video, and then, the bag-level prediction of the objects presented in
    the audio was obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of recent audio-visual separation and localization approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '|  | Gabbay et al. [[5](#bib.bib5)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Predict speaker’s voice based on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; faces in video used as a filter &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Can only be used &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in controlled environments &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Afouras et al. [[6](#bib.bib6)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generate a soft mask for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; filtering in the wild &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Requires training a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; separate model for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; each speaker of interest &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Speaker Separation | Lu et al. [[20](#bib.bib20)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Distinguish the correspondence &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; between speech and human &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; speech lip movements &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Two speakers only; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; hardly applied for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; background noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Ephrat et al. [[7](#bib.bib7)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Predict a complex spectrogram &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mask for each speaker; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; trained once, applicable to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; any speaker &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The model is too complicated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and lacks explanation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Morrone et al. [[21](#bib.bib21)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Use landmarks to generate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; time-frequency masks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Additional landmark &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detection required &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Gao et al. [[25](#bib.bib25)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Disentangle audio frequencies &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; related to visual objects &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Separated audio only |'
  prefs: []
  type: TYPE_TB
- en: '|  | Senocak et al [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Focus on the primary &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; area by using attention &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Localized sound &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; source only &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Tian et al. [[27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Joint modeling of auditory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and visual modalities &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Localized sound &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; source only &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Separate and Localize &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Objects’ Sounds &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pu et al. [[19](#bib.bib19)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Use low rank to extract the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sparsely correlated components &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Not for the in-the-wild &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; environment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Zhao et al. [[28](#bib.bib28)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Mix and separate a given audio; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; without traditional supervision &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Motion information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; is not considered &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Zhao et al. [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Introduce motion trajectory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and curriculum learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Only suitable for synchronized &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; video and audio input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Rouditchenko et al. [[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Separation and localization use &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; only one modality input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Does not fully utilize &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; temporal information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Parekh et al. [[31](#bib.bib31)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Weakly supervised learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; via multiple-instance learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Only a bounding box &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; proposed on the image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Localization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of only separating audio, can machines localize the sound source merely
    by observing sound and visual scene pairs as a human can? There is evidence both
    in physiology and psychology that sound localization of acoustic signals is strongly
    influenced by synchronicity of their visual signals [[22](#bib.bib22)]. The past
    efforts in this domain were limited to requiring specific devices or additional
    features. Izadinia et al. [[32](#bib.bib32)] proposed utilizing the velocity and
    acceleration of moving objects as visual features to assign sounds to them. Zunino
    et al. [[24](#bib.bib24)] presented a new hybrid device for sound and optical
    imaging that was primarily suitable for automatic monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of unlabeled videos on the Internet has been increasing dramatically,
    recent methods mainly focus on unsupervised learning. Additionally, modeling audio
    and visual modalities simultaneously tends to outperform independent modeling.
    Senocak et al. [[26](#bib.bib26)] learned to localize sound sources by merely
    watching and listening to videos. The relevant model mainly consisted of three
    networks, namely, sound and visual networks and an attention network trained via
    the distance ratio [[33](#bib.bib33)] unsupervised loss.
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms cause the model to focus on the primary area. They provide
    prior knowledge in a semisupervised setting. As a result, the network can be converted
    into a unified one that can learn better from data without additional annotations.
    To enable cross-modality localization, Tian et al. [[27](#bib.bib27)] proposed
    capturing the semantics of sound-emitting objects via the learned attention and
    leveraging temporal alignment to discover the correlations between the two modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Simultaneous Separation and Localization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sound source separation and localization can be strongly associated with each
    other by assigning one modality’s information to another. Therefore, several researchers
    attempted to perform localization and separation simultaneously. Pu et al. [[19](#bib.bib19)]
    used a low-rank and sparse framework to model the background. The researchers
    extracted components with sparse correlations between the audio and visual modalities.
    However, the scenario of this method had a major limitation: it could only be
    applied to videos with a few sound-generating objects. Therefore, Zhao et al.
    [[28](#bib.bib28)] introduced a system called PixelPlayer that used a two-stream
    network and presented a mix-and-separate framework to train the entire network.
    In this framework, audio signals from two different videos were added to produce
    a mixed signal as input. The input was then fed into the network that was trained
    to separate the audio source signals based on the corresponding video frames.
    The two separated sound signals were treated as outputs. The system thus learned
    to separate individual sources without traditional supervision.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of merely relying on image semantics while ignoring the temporal motion
    information in the video, Zhao et al. [[29](#bib.bib29)] subsequently proposed
    an end-to-end network called deep dense trajectory to learn the motion information
    for audio-visual sound separation. Furthermore, due to the lack of training samples,
    directly separating sound for a single class of instruments tend to lead to overfitting.
    Therefore, the authors proposed a curriculum strategy, starting by separating
    sounds from different instruments and proceeding to sounds from the same instrument.
    This gradual approach provided a good start for the network to converge better
    on the separation and localization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The methods of previous studies [[19](#bib.bib19), [28](#bib.bib28), [29](#bib.bib29)]
    could only be applied to videos with synchronized audio. Hence, Rouditchenko et
    al. [[30](#bib.bib30)] tried to perform localization and separation tasks using
    only video frames or sound by disentangling concepts learned by neural networks.
    The researchers proposed an approach to produce sparse activations that could
    correspond to semantic categories in the input using the sigmoid activation function
    during the training stage and softmax activation during the fine-tuning stage.
    Afterwards, the researchers assigned these semantic categories to intermediate
    network feature channels using labels available in the training dataset. In other
    words, given a video frame or a sound, the approach used the category-to-feature-channel
    correspondence to select a specific type of source or object for separation or
    localization. Aiming to introduce weak labels to improve performance, Parekh et
    al. [[31](#bib.bib31)] designed an approach based on multiple-instance learning,
    a well-known strategy for weakly supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Audio-visual Correspondence Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce several studies that explored the global semantic
    relation between audio and visual modalities. We name this branch of research
    “audio-visual correspondence learning”; it consists of 1) the audio-visual matching
    task and 2) the audio-visual speech recognition task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Audio-visual Matching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Biometric authentication, ranging from facial recognition to fingerprint and
    iris authentication, is a popular topic that has been researched over many years,
    while evidence shows that this system can be attacked maliciously. To detect such
    attacks, recent studies particularly focus on speech antispoofing measures.
  prefs: []
  type: TYPE_NORMAL
- en: Sriskandaraja et al. [[34](#bib.bib34)] proposed a network based on a Siamese
    architecture to evaluate the similarities between pairs of speech samples. [[35](#bib.bib35)]
    presented a two-stream network, where the first network was a Bayesian neural
    network assumed to be overfitting, and the second network was a CNN used to improve
    generalization. Alanis et al. [[36](#bib.bib36)] further incorporated LightCNN
    [[37](#bib.bib37)] and a gated recurrent unit (GRU) [[38](#bib.bib38)] as a robust
    feature extractor to represent speech signals in utterance-level analysis to improve
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: We note that cross-modality matching is a special form of such authentication
    that has recently been extensively studied. It attempts to learn the similarity
    between pairs. We divide this matching task into fine-grained voice-face matching
    and coarse-grained audio-image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Voice-Facial Matching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given facial images of different identities and the corresponding audio sequences,
    voice-facial matching aims to identify the face that the audio belongs to (the
    V2F task) or vice versa (the F2V task), as shown in Fig. [3](#S3.F3 "Figure 3
    ‣ 3.1.1 Voice-Facial Matching ‣ 3.1 Audio-visual Matching ‣ 3 Audio-visual Correspondence
    Learning ‣ Deep Audio-Visual Learning: A Survey"). The key point is finding the
    embedding between audio and visual modalities. Nagrani et al. [[39](#bib.bib39)]
    proposed using three networks to address the audio-visual matching problem: a
    static network, a dynamic network, and an N-way network. The static network and
    the dynamic network could only handle the problem with a specific number of images
    and audio tracks. The difference was that the dynamic network added to each image
    temporal information such as the optical flow or a 3D convolution [[40](#bib.bib40),
    [41](#bib.bib41)]. Based on the static network, the authors increased the number
    of samples to form an N-way network that was able to solve the $N:1$ identification
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the correlation between the two modalities was not fully utilized in
    the above method. Therefore, Wen et al. [[42](#bib.bib42)] proposed a disjoint
    mapping network (DIMNets) to fully use the covariates (e.g., gender and nationality)
     [[43](#bib.bib43), [44](#bib.bib44)] to bridge the relation between voice and
    face information. The intuitive assumption was that for a given voice and face
    pair, the more covariates were shared between the two modalities, the higher the
    probability of being a match. The main drawback of this framework was that a large
    number of covariates led to high data costs. Therefore, Hoover et al. [[45](#bib.bib45)]
    suggested a low-cost but robust approach of detection and clustering on audio
    clips and facial images. For the audio stream, the researchers applied a neural
    network model to detect speech for clustering and subsequently assigned a frame
    cluster to the given audio cluster according to the majority principle. Doing
    so required a small amount of data for pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: To further enhance the robustness of the network, Chung et al. [[46](#bib.bib46)]
    proposed an improved two-stream training method that increased the number of negative
    samples to improve the error-tolerance rate of the network. The cross-modality
    matching task, which is essentially a classification task, allows for wide-ranging
    applications of the triplet loss. However, it is fragile in the case of multiple
    samples. To overcome this defect, Wang et al. [[47](#bib.bib47)] proposed a novel
    loss function to expand the triplet loss for multiple samples and a new elastic
    network (called Emnet) based on a two-stream architecture that can tolerate a
    variable number of inputs to increase the flexibility of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3260d132e0e28b3c2bb182209a02b8a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Demonstration of Audio-to-Image retrieval (a) and Image-to-Audio
    retrieval (b).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Audio-image Retrieval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The cross-modality retrieval task aims to discover the relationship between
    different modalities. Given one sample in the source modality, the proposed model
    can retrieve the corresponding sample with the same identity in the target modality.
    For audio-image retrieval as an example, the aim is to return a relevant piano
    sound, given a picture of a girl playing a piano. Compared with the previously
    considered voice and face matching, this task is more coarse-grained.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other retrieval tasks such as the text-image task [[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50)] or the sound-text task [[51](#bib.bib51)],
    the audio-visual retrieval task mainly focuses on subspace learning. Didac et
    al. [[52](#bib.bib52)] proposed a new joint embedding model that mapped two modalities
    into a joint embedding space, and then directly calculated the Euclidean distance
    between them. The authors leveraged cosine similarity to ensure that the two modalities
    in the same space were as close as possible while not overlapping. Note that the
    designed architecture would have a large number of parameters due to the existence
    of a large number of fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Hong et al. [[53](#bib.bib53)] proposed a joint embedding model that relied
    on pretrained networks and used CNNs to replace fully connected layers to reduce
    the number of parameters to some extent. The video and music were fed to the pretrained
    network and then aggregated, followed by a two-stream network trained via the
    intermodal ranking loss. In addition, to preserve modality-specific characteristics,
    the researchers proposed a novel soft intramodal structure loss. However, the
    resulting network was very complex and difficult to apply in practice. To solve
    this problem, Arsha et al. [[54](#bib.bib54)] proposed a cross-modality self-supervised
    method to learn the embedding of audio and visual information from a video and
    significantly reduced the complexity of the network. For sample selection, the
    authors designed a novel curriculum learning schedule to further improve performance.
    In addition, the resulting joint embedding could be efficiently and effectively
    applied in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of audio-visual correspondence learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '|  | Nagrani et al.  [[39](#bib.bib39)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The method is novel and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; incorporates dynamic information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; As the sample size increases, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the accuracy decreases excessively &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Wen et al.  [[42](#bib.bib42)]. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The correlation between &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modes is utilized &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dataset acquisition is difficult &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Voice-Face Matching | Wang et al. [[55](#bib.bib55)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Can deal with multiple samples &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Can change the size of input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Static image only; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; model complexity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Hoover et al. [[45](#bib.bib45)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Easy to implement &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Robust &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cannot handle large-scale data |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hong et al. [[53](#bib.bib53)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Preserve modality- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; specific characteristics &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Soft intra-modality structure loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex network |'
  prefs: []
  type: TYPE_TB
- en: '| Audio-visual retrieval | Didac et al. [[52](#bib.bib52)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Metric Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Using fewer parameters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Only two faces &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Static images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Arsha et al. [[54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Curriculum learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Applied value &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Low data cost &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Low accuracy for multiple samples |'
  prefs: []
  type: TYPE_TB
- en: '|  | Petridis et al.  [[56](#bib.bib56)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Simultaneously obtain &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; feature and classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lack of audio information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Wand et al.  [[57](#bib.bib57)]. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Simple method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Word-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Audio-visual Speech Recognition | Shillingford et al. [[58](#bib.bib58)]
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sentence-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LipNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CTC loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; No audio information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Chung et al. [[59](#bib.bib59)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Audio and visual information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LRS dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Noise is not considered |'
  prefs: []
  type: TYPE_TB
- en: '|  | Trigeorgis et al. [[60](#bib.bib60)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Audio information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The algorithm is robust &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Noise is not considered |'
  prefs: []
  type: TYPE_TB
- en: '|  | Afouras et al. [[61](#bib.bib61)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Study noise in audio &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LRS2-BBC Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex network |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/ab4ed84668338c061d7d8246fbd7913a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Demonstration of audio-visual speech recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Audio-visual Speech Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The recognition of content of a given speech clip has been studied for many
    years, yet despite great achievements, researchers are still aiming for satisfactory
    performance in challenging scenarios. Due to the correlation between audio and
    vision, combining these two modalities tends to offer more prior information.
    For example, one can predict the scene where the conversation took place, which
    provides a strong prior for speech recognition, as shown in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1.2 Audio-image Retrieval ‣ 3.1 Audio-visual Matching ‣ 3 Audio-visual Correspondence
    Learning ‣ Deep Audio-Visual Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier efforts on audio-visual fusion models usually consisted of two steps:
    1) extracting features from the image and audio signals and 2) combining the features
    for joint classification  [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64)].
    Later, taking advantage of deep learning, feature extraction was replaced with
    a neural network encoder [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)].
    Several recently studies have shown a tendency to use an end-to-end approach to
    visual speech recognition. These studies can be mainly divided into two groups.
    They either leverage the fully connected layers and LSTM to extract features and
    model the temporal information [[56](#bib.bib56), [57](#bib.bib57)] or use a 3D
    convolutional layer followed by a combination of CNNs and LSTMs [[58](#bib.bib58),
    [68](#bib.bib68)]. Instead of adopting a two-step strategy, Petridis et al. [[56](#bib.bib56)]
    introduced an audio-visual fusion model that simultaneously extracted features
    directly from pixels and spectrograms and performed classification of speech and
    nonlinguistic vocalizations. Furthermore, temporal information was extracted by
    a bidirectional LSTM. Although this method could perform feature extraction and
    classification at the same time, it still followed the two-step strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: To this end, Wand et al. [[57](#bib.bib57)] presented a word-level lip-reading
    system using LSTM. In contrast to previous methods, Assael et.al [[58](#bib.bib58)]
    proposed a novel end-to-end LipNet model based on sentence-level sequence prediction,
    which consisted of spatial-temporal convolutions, a recurrent network and a model
    trained via the connectionist temporal classification (CTC) loss. Experiments
    showed that lip-reading outperformed the two-step strategy.
  prefs: []
  type: TYPE_NORMAL
- en: However, the limited information in the visual modality may lead to a performance
    bottleneck. To combine both audio and visual information for various scenes, especially
    in noisy conditions, Trigeorgis et al.  [[60](#bib.bib60)] introduced an end-to-end
    model to obtain a ‘context-aware’ feature from the raw temporal representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chung et al. [[59](#bib.bib59)] presented a “Watch, Listen, Attend, and Spell”
    (WLAS) network to explain the influence of audio on the recognition task. The
    model took advantage of the dual attention mechanism and could operate on a single
    or combined modality. To speed up the training and avoid overfitting, the researchers
    also used a curriculum learning strategy. To analyze an “in-the-wild” dataset,
    Cui et al. [[69](#bib.bib69)] proposed another model based on residual networks
    and a bidirectional GRU [[38](#bib.bib38)]. However, the authors did not take
    the ubiquitous noise in the audio into account. To solve this problem, Afouras
    et al. [[61](#bib.bib61)] proposed a model for performing speech recognition tasks.
    The researchers compared two common sequence prediction types: connectionist temporal
    classification and sequence-to-sequence (seq2seq) methods in their models. In
    the experiment, they observed that the model using seq2seq could perform better
    according to word error rate (WER) when it was only provided with silent videos.
    For pure-audio or audio-visual tasks, the two methods behaved similarly. In a
    noisy environment, the performance of the seq2seq model was worse than that of
    the corresponding CTC model, suggesting that the CTC model could better handle
    background noises.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Audio and Visual Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previously introduced retrieval task shows that the trained model is able
    to find the most similar audio or visual counterpart. While humans can imagine
    the scenes corresponding to sounds, and vice versa, researchers have tried to
    endow machines with this kind of imagination for many years. Following the invention
    and advances of generative adversarial networks (GANs) [[70](#bib.bib70)], image
    or video generation has emerged as a topic. It involves several subtasks, including
    generating images or video from a potential space [[71](#bib.bib71)], cross-modality
    generation [[72](#bib.bib72), [73](#bib.bib73)], etc. These applications are also
    relevant to other tasks, e.g., domain adaptation [[74](#bib.bib74), [75](#bib.bib75)].
    Due to the difference between audio and visual modalities, the potential correlation
    between them is nonetheless difficult for machines to discover. Generating sound
    from a visual signal or vice versa, therefore, becomes a challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will mainly review the recent development of audio and
    visual generation, i.e., generating audio from visual signals or vice versa. Visual
    signals here mainly refer to images, motion dynamics, and videos. The subsection
    ‘Visual to Audio’ mainly focuses on recovering the speech from the video of the
    lip area (Fig. [5](#S4.F5 "Figure 5 ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual
    Learning: A Survey") (a)) or generating sounds that may occur in the given scenes
    (Fig. [5](#S4.F5 "Figure 5 ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual
    Learning: A Survey") (a)). In contrast, the discussion of ‘Audio to Visual’ generation
    (Fig. [5](#S4.F5 "Figure 5 ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual
    Learning: A Survey") (b)) will examine generating images from a given audio (Fig.
    [6](#S4.F6 "Figure 6 ‣ 4.2.1 Audio to Image ‣ 4.2 Audio to Vision ‣ 4 Audio and
    Visual Generation ‣ Deep Audio-Visual Learning: A Survey") (a)), body motion generation
    (Fig. [6](#S4.F6 "Figure 6 ‣ 4.2.1 Audio to Image ‣ 4.2 Audio to Vision ‣ 4 Audio
    and Visual Generation ‣ Deep Audio-Visual Learning: A Survey") (b)), and talking
    face generation (Fig. [6](#S4.F6 "Figure 6 ‣ 4.2.1 Audio to Image ‣ 4.2 Audio
    to Vision ‣ 4 Audio and Visual Generation ‣ Deep Audio-Visual Learning: A Survey")
    (c)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/938f50f464ef70c47cafaddbd9011a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Demonstration of generating speech from lip sequences
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12fa0f4eba6b9b960efb3d843aec89f7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Demonstration of video-to-audio generation
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Demonstration of visual-to-audio generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of recent approaches to video-to-audio generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cornu et al. [[76](#bib.bib76)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reconstruct intelligible &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; speech only from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; visual speech features &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Applied to limited scenarios &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lip sequence &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to Speech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ephrat et al. [[77](#bib.bib77)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Compute optical flow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; between frames &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Applied to limited scenarios |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cornu et al. [[78](#bib.bib78)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reconstruct speech using &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; a classification approach &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; combined with feature-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; temporal information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cannot apply to real-time &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; conversational speech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Davis et al. [[79](#bib.bib79)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Recover real-world audio by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; capturing vibrations of objects &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Requires a specific device; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; can only be applied to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; soft objects &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Owens et al. [[80](#bib.bib80)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Use LSTM to capture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the relation between material &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and motion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; For a lab-controlled &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; environment only &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; General Video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to Audio &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhou et al. [[81](#bib.bib81)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Leverage a hierarchical &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RNN to generate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in-the-wild sounds &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Monophonic audio only |'
  prefs: []
  type: TYPE_TB
- en: '|  | Morgado et al. [[8](#bib.bib8)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Localize and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; separate sounds to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; generate spatial audio &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from 360^∘ video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fails sometimes; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 360^∘ video required &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Vision-to-Audio Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many methods have been explored to extract audio information from visual information,
    including predicting sounds from visually observed vibrations and generating audio
    via a video signal. We divide the visual-to-audio generation tasks into two categories:
    generating speech from lip video and synthesizing sounds from general videos without
    scene limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Lip Sequence to Speech
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a natural relationship between speech and lips. Separately from understanding
    the content of speech by observing lips (lip-reading), several studies have tried
    to reconstruct speech by observing lips. Cornu et al. [[76](#bib.bib76)] attempted
    to predict the spectral envelope from visual features, combining it with artificial
    excitation signals, and to synthesize audio signals in a speech production model.
    Ephrat et al. [[82](#bib.bib82)] proposed an end-to-end model based on a CNN to
    generate audio features for each silent video frame based on its adjacent frames.
    The waveform was therefore reconstructed based on the learned features to produce
    understandable speech.
  prefs: []
  type: TYPE_NORMAL
- en: Using temporal information to improve speech reconstruction has been extensively
    explored. Ephrat et al. [[77](#bib.bib77)] proposed leveraging the optical flow
    to capture the temporal motion at the same time. Cornu et al. [[78](#bib.bib78)]
    leveraged recurrent neural networks to incorporate temporal information into the
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 General Video to Audio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When a sound hits the surfaces of some small objects, the latter will vibrate
    slightly. Therefore, Davis et al. [[79](#bib.bib79)] utilized this specific feature
    to recover the sound from vibrations observed passively by a high-speed camera.
    Note that it should be easily for suitable objects to vibrate, which is the case
    for a glass of water, a pot of plants, or a box of napkins. We argue that this
    work is similar to the previously introduced speech reconstruction studies [[76](#bib.bib76),
    [82](#bib.bib82), [77](#bib.bib77), [78](#bib.bib78)] since all of them use the
    relation between visual and sound context. In speech reconstruction, the visual
    part concentrates more on lip movement, while in this work, it focuses on small
    vibrations.
  prefs: []
  type: TYPE_NORMAL
- en: Owens et al. [[80](#bib.bib80)] observed that when different materials were
    hit or scratched, they emitted a variety of sounds. Thus, the researchers introduced
    a model that learned to synthesize sound from a video in which objects made of
    different materials were hit with a drumstick at different angles and velocities.
    The researchers demonstrated that their model could not only identify different
    sounds originating from different materials but also learn the pattern of interaction
    with objects (different actions applied to objects result in different sounds).
    The model leveraged an RNN to extract sound features from video frames and subsequently
    generated waveforms through an instance-based synthesis process.
  prefs: []
  type: TYPE_NORMAL
- en: Although Owens et al. [[80](#bib.bib80)] could generate sound from various materials,
    the authors’ approach still could not be applied to real-life applications since
    the network was trained by videos shot in a lab environment under strict constraints.
    To improve the result and generate sounds from in-the-wild videos, Zhou et al.
    [[81](#bib.bib81)] designed an end-to-end model. It was structured as a video
    encoder and a sound generator to learn the mapping from video frames to sounds.
    Afterwards, the network leveraged a hierarchical RNN [[83](#bib.bib83)] for sound
    generation. Specifically, the authors trained a model to directly predict raw
    audio signals (waveform samples) from input videos. They demonstrated that this
    model could learn the correlation between sound and visual input for various scenes
    and object interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous efforts we have mentioned focused on monophonic audio generation,
    while Morgado et al. [[8](#bib.bib8)] attempted to convert monophonic audio recorded
    by a 360^∘ video camera into spatial audio. Performing such a task of audio specialization
    requires addressing two primary issues: source separation and localization. Therefore,
    the researchers designed a model to separate the sound sources from mixed-input
    audio and then localize them in the video. Another multimodality model was used
    to guide the separation and localization since the audio and video were complementary.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Audio to Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide a detailed review of audio-to-visual generation.
    We first introduce audio-to-images generation, which is easier than video generation
    since it does not require temporal consistency between the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Audio to Image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To generate images of better quality, Wan et al. [[84](#bib.bib84)] put forward
    a model that combined the spectral norm, an auxiliary classifier, and a projection
    discriminator to form the researchers’ conditional GAN model. The model could
    output images of different scales according to the volume of the sound, even for
    the same sound. Instead of generating real-world scenes of the sound that had
    occurred, Qiu et al. [[85](#bib.bib85)] suggested imagining the content from music.
    The authors extracted features by feeding the music and images into two networks
    and learning the correlation between those features and finally generated images
    from the learned correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several studies have focused on audio-visual mutual generation. Chen et al.
    [[72](#bib.bib72)] were the first to attempt to solve this cross-modality generation
    problem using conditional GANs. The researchers defined a sound-to-image (S2I)
    network and an image-to-sound (I2S) network that generated images and sounds,
    respectively. Instead of separating S2I and I2S generation, Hao et al. [[86](#bib.bib86)]
    combined the respective networks into one network by considering a cross-modality
    cyclic generative adversarial network (CMCGAN) for the cross-modality visual-audio
    mutual generation task. Following the principle of cyclic consistency, CMCGAN
    consisted of four subnetworks: audio-to-visual, visual-to-audio, audio-to-audio,
    and visual-to-visual.'
  prefs: []
  type: TYPE_NORMAL
- en: Most recently, some studies have tried to reconstruct facial images from speech
    clips. Duarte et al. [[87](#bib.bib87)] synthesized facial images containing expressions
    and poses through the GAN model. Moreover, the authors enhanced their model’s
    generation quality by searching for the optimal input audio length. To better
    learn normalized faces from speech, Oh et al. [[88](#bib.bib88)] explored a reconstructive
    model. The researchers trained an audio encoder by learning to align the feature
    space of speech with a pretrained face encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4c5975e52185686db01d0bcce964c09c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Demonstration of audio-to-images generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d5d3be531cf2422943bc87b81a33916.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Demonstration of a moving body.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/858cb41549c15fb9e41c8aba94839e19.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Demonstration of a talking face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Demonstration of talking face generation and moving body generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of recent studies of audio-to-visual generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Ideas & Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wan et al. [[84](#bib.bib84)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combined many existing techniques &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to form a GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Low quality |'
  prefs: []
  type: TYPE_TB
- en: '|  | Qiu et al. [[85](#bib.bib85)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generated images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; related to music &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Low quality |'
  prefs: []
  type: TYPE_TB
- en: '| Audio to Image | Chen et al. [[72](#bib.bib72)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generated both audio-to-visual and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; visual-to-audio models &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The models were &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; independent &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Hao et al. [[86](#bib.bib86)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Proposed a cross-modality cyclic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; generative adversarial network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Generated images only |'
  prefs: []
  type: TYPE_TB
- en: '|  | Alemi et al. [[89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generated dance movements from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; music via real-time GrooveNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lee et al. [[90](#bib.bib90)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generated a choreography system &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; via an autoregressive &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; encoder-decoder network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Audio to Motions | Shlizerman et al. [[91](#bib.bib91)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Applied a “target delay” LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to predict body keypoints &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Constrained to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the given dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Tang et al. [[92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Developed a music-oriented dance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; choreography synthesis method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Yalta et al. [[93](#bib.bib93)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Produced weak labels from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; motion directions for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; motion-music alignment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Kumar et al. [[94](#bib.bib94)] and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Supasorn et al. [[95](#bib.bib95)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generated keypoints &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; by a time-delayed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Needed retraining for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; another identity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Chung et al. [[96](#bib.bib96)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Developed an encoder-decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN model suitable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for more identities &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Jalalifar et al. [[97](#bib.bib97)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combined RNN and GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and applied keypoints &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; For a lab-controlled &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; environment only &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Talking Face | Vougioukas et al. [[98](#bib.bib98)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Applied a temporal GAN for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; more temporal consistency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Chen et al. [[99](#bib.bib99)] | Applied optical flow | Generated lips
    only |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zhou et al. [[100](#bib.bib100)] | Disentangled information | Lacked realism
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zhu et al. [[73](#bib.bib73)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Asymmetric mutual information estimation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to capture modality coherence &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Suffered from the “zoom-in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -and-out” condition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Chen et al. [[101](#bib.bib101)] | Dynamic pixelwise loss |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Required multistage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Wiles et al. [[102](#bib.bib102)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Self-supervised model for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multimodality driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Relatively low quality |'
  prefs: []
  type: TYPE_TB
- en: 4.2.2 Body Motion Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of directly generating videos, numerous studies have tried to animate
    avatars using motions. The motion synthesis methods leveraged multiple techniques,
    such as dimensionality reduction [[103](#bib.bib103), [104](#bib.bib104)], hidden
    Markov models [[105](#bib.bib105)], Gaussian processes [[106](#bib.bib106)], and
    neural networks [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)].
  prefs: []
  type: TYPE_NORMAL
- en: Alemi et al. [[89](#bib.bib89)] proposed a real-time GrooveNet based on conditional
    restricted Boltzmann machines and recurrent neural networks to generate dance
    movements from music. Lee et al. [[90](#bib.bib90)] utilized an autoregressive
    encoder-decoder network to generate a choreography system from music. Shlizerman
    et al. [[91](#bib.bib91)] further introduced a model that used a “target delay”
    LSTM to predict body landmarks. The latter was further used as agents to generate
    body dynamics. The key idea was to create an animation from the audio that was
    similar to the action of a pianist or a violinist. In summary, the entire process
    generated a video of artists’ performance corresponding to input audio.
  prefs: []
  type: TYPE_NORMAL
- en: Although previous methods could generate body motion dynamics, the intrinsic
    beat information of the music has not been used. Tang et al. [[92](#bib.bib92)]
    proposed a music-oriented dance choreography synthesis method that extracted a
    relation between acoustic and motion features via an LSTM-autoencoder model. Moreover,
    to achieve better performance, the researchers improved their model with a masking
    method and temporal indexes. Providing weak supervision, Yalta et al. [[93](#bib.bib93)]
    explored producing weak labels from motion direction for motion-music alignment.
    The authors generated long dance sequences via a conditional autoconfigured deep
    RNN that was fed by audio spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Talking Face Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exploring audio-to-video generation, many researchers showed great interest
    in synthesizing people’s faces from speech or music. This has many applications,
    such as animating movies, teleconferencing, talking agents and enhancing speech
    comprehension while preserving privacy. Earlier studies of talking face generation
    mainly synthesized a specific identity from the dataset based on an audio of arbitrary
    speech. Kumar et al. [[94](#bib.bib94)] attempted to generate key points synced
    to audio by utilizing a time-delayed LSTM [[110](#bib.bib110)] and then generated
    the video frames conditioned on the key points by another network. Furthermore,
    Supasorn et al. [[95](#bib.bib95)] proposed a “teeth proxy” to improve the visual
    quality of teeth during generation.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, Chung et al. [[96](#bib.bib96)] attempted to use an encoder-decoder
    CNN model to learn the correspondences between raw audio and videos. Combining
    RNN and GAN [[70](#bib.bib70)], Jalalifar et al. [[97](#bib.bib97)] produced a
    sequence of realistic faces that were synchronized with the input audio by two
    networks. One was an LSTM network used to create lip landmarks out of audio input.
    The other was a conditional GAN (cGAN) used to generate the resulting faces conditioned
    on a given set of lip landmarks. Instead of applying cGAN, [[98](#bib.bib98)]
    proposed using a temporal GAN [[111](#bib.bib111)] to improve the quality of synthesis.
    However, the above methods were only applicable to synthesizing talking faces
    with identities limited to those in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis of talking faces of arbitrary identities has recently drawn significant
    attention. Chen et al. [[99](#bib.bib99)] considered correlations among speech
    and lip movements while generating multiple lip images. The researchers used the
    optical flow to better express the information between the frames. The fed optical
    flow represented not only the information of the current shape but also the previous
    temporal information.
  prefs: []
  type: TYPE_NORMAL
- en: A frontal face photo usually has both identity and speech information. Assuming
    this, Zhou et al. [[100](#bib.bib100)] used an adversarial learning method to
    disentangle different types of information of one image during generation. The
    disentangled representation had a convenient property that both audio and video
    could serve as the source of speech information for the generation process. As
    a result, it was possible to not only output the features but also express them
    more explicitly while applying the resulting network.
  prefs: []
  type: TYPE_NORMAL
- en: Most recently, to discover the high-level correlation between audio and video,
    Zhu et al. [[73](#bib.bib73)] proposed a mutual information approximation to approximate
    mutual information between modalities. Chen et al. [[101](#bib.bib101)] applied
    landmark and motion attention to generating talking faces. The authors further
    proposed a dynamic pixelwise loss for temporal consistency. Facial generation
    is not limited to specific modalities such as audio or visual since the crucial
    point is whether there is a mutual pattern between these different modalities.
    Wiles et al. [[102](#bib.bib102)] put forward a self-supervising framework called
    X2Face to learn the embedded features and generate target facial motions. It could
    produce videos from any input as long as embedded features were learned.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Audio-visual Representation Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 5: Summary of recent audio-visual representation learning studies.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Method | Ideas & Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Single &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modality &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Aytar et al. [[112](#bib.bib112)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Student-teacher training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; procedure with natural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; video synchronization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Only learned the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; audio representation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Leidal et al. [[113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Regularized the amount of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; information encoded in the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; semantic embedding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Focused on spoken utterances &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and handwritten digits &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Arandjelovic et al. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[12](#bib.bib12), [114](#bib.bib114)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Proposed the AVC task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Considered only audio and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; video correspondence &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modalities &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Owens et al. [[13](#bib.bib13)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Proposed the AVTS task &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with curriculum learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The sound source has to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; feature in the video; only &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; one sound source &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Parekh et al. [[115](#bib.bib115)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Use video labels for weakly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; supervised learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Leverage the prior knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of event classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Hu et al. [[116](#bib.bib116)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Disentangle each &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modality into a set &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of distinct components &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Require a predefined &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; number of clusters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Representation learning aims to discover the pattern representation from data
    automatically. It is motivated by the fact that the choice of data representation
    usually greatly impacts performance of machine learning [[11](#bib.bib11)]. However,
    real-world data such as images, videos and audio are not amenable to defining
    specific features algorithmically.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the quality of data representation usually determines the success
    of machine learning algorithms. Bengio et al.[[11](#bib.bib11)] assumed the reason
    for this to be that different representations could better explain the laws underlying
    data, and the recent enthusiasm for AI has motivated the design of more powerful
    representation learning algorithms to achieve these priors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will review a series of audio-visual learning methods ranging
    from single-modality [[112](#bib.bib112)] to dual-modality representation learning
    [[114](#bib.bib114), [12](#bib.bib12), [13](#bib.bib13), [113](#bib.bib113), [116](#bib.bib116)].
    The basic pipeline of such studies is shown in Fig. [7](#S5.F7 "Figure 7 ‣ 5 Audio-visual
    Representation Learning ‣ Deep Audio-Visual Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ae983468b9c3f4b271ce98a497fcdac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Basic pipeline of representation learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Single-Modality Representation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Naturally, to determine whether audio and video are related to each other, researchers
    focus on determining whether audio and video are from the same video or whether
    they are synchronized in the same video. Aytar et al. [[112](#bib.bib112)] exploited
    the natural synchronization between video and sound to learn an acoustic representation
    of a video. The researchers proposed a student-teacher training process that used
    an unlabeled video as a bridge to transfer discernment knowledge from a sophisticated
    visual identity model to the sound modality. Although the proposed approach managed
    to learn audio-modality representation in an unsupervised manner, discovering
    audio and video representations simultaneously remained to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Learning an Audio-visual Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the corresponding audio and images, the information concerning modality tends
    to be noisy, while we only require semantic content rather than the exact visual
    content. Leidal et al. [[113](#bib.bib113)] explored unsupervised learning of
    the semantic embedded space, which required a close distribution of the related
    audio and image. The researchers proposed a model to map an input to vectors of
    the mean and the logarithm of variance of a diagonal Gaussian distribution, and
    the sample semantic embeddings were drawn from these vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn audio and video’s semantic information by simply watching and listening
    to a large number of unlabeled videos, Arandjelovic et al. [[12](#bib.bib12)]
    introduced an audio-visual correspondence learning task (AVC) for training two
    (visual and audio) networks from scratch, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.2 Learning an Audio-visual Representation ‣ 5 Audio-visual Representation
    Learning ‣ Deep Audio-Visual Learning: A Survey") (a). In this task, the corresponding
    audio and visual pairs (positive samples) were obtained from the same video, while
    mismatched (negative) pairs were extracted from different videos. To solve this
    task, the authors proposed an $L^{3}$-Net that detected whether the semantics
    in visual and audio fields were consistent. Although this model was trained without
    additional supervision, it could learn representations of dual modalities effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the proposed audio-visual coherence (AVC) task, Arandjelovic et al.
    [[114](#bib.bib114)] continued to investigate AVE-Net that aimed at finding the
    most similar visual area to the current audio clip. Owens et al. [[117](#bib.bib117)]
    proposed adopting a model similar to that of [[12](#bib.bib12)] but used a 3D
    convolution network for the videos instead, which could capture the motion information
    for sound localization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to previous AVC task-based solutions, Korbar et al. [[13](#bib.bib13)]
    introduced another proxy task called audio-visual time synchronization (AVTS)
    that further considered whether a given audio sample and video clip were “synchronized”
    or “not synchronized.” In previous AVC tasks, negative samples were obtained as
    audio and visual samples from different videos. However, exploring AVTS, the researchers
    trained the model using “harder” negative samples representing unsynchronized
    audio and visual segments sampled from the same video, forcing the model to learn
    the relevant temporal features. At this time, not only the semantic correspondence
    was enforced between the video and the audio, but more importantly, the synchronization
    between them was also achieved. The researchers applied the curriculum learning
    strategy [[118](#bib.bib118)] to this task and divided the samples into four categories:
    positives (the corresponding audio-video pairs), easy negatives (audio and video
    clips originating from different videos), difficult negatives (audio and video
    clips originating from the same video without overlap), and super-difficult negatives
    (audio and video clips that partly overlap), as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.2 Learning an Audio-visual Representation ‣ 5 Audio-visual Representation
    Learning ‣ Deep Audio-Visual Learning: A Survey") (b).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c39ba0599090dd4c2d625daa1492a7a1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Introduction to the AVC task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2b996fcc7d3bce9bdab7b177ff2ceea.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Introduction to the AVTS task
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Introduction to the representation task'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62c261e3c937c1ed1ebbeb0c9c415e2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Demonstration of audio-visual datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above studies rely on two latent assumptions: 1) the sound source should
    be present in the video, and 2) only one sound source is expected. However, these
    assumptions limit the applications of the respective approaches to real-life videos.
    Therefore, Parekh et al. [[115](#bib.bib115)] leveraged class-agnostic proposals
    from both video frames to model the problem as a multiple-instance learning task
    for audio. As a result, the classification and localization problems could be
    solved simultaneously. The researchers focused on localizing salient audio and
    visual components using event classes in a weakly supervised manner. This framework
    was able to deal with the difficult case of asynchronous audio-visual events.
    To leverage more detailed relations between modalities, Hu et al. [[116](#bib.bib116)]
    recommended a deep coclustering model that extracted a set of distinct components
    from each modality. The model continually learned the correspondence between such
    representations of different modalities. The authors further introduced K-means
    clustering to distinguish concrete objects or sounds.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Recent Public Audio-visual Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many audio-visual datasets ranging from speech- to event-related data have
    been collected and released. We divide datasets into two categories: audio-visual
    speech datasets that record human face with the corresponding speech, and audio-visual
    event datasets that consist of musical instrument videos and real events’ videos.
    In this section, we summarize the information of recent audio-visual datasets
    (Table [6](#S6.T6 "Table 6 ‣ 6.1.1 Lab-controlled Environment ‣ 6.1 Audio-visual
    Speech Datasets ‣ 6 Recent Public Audio-visual Datasets ‣ Deep Audio-Visual Learning:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Audio-visual Speech Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constructing datasets containing audio-visual corpora is crucial to understanding
    audio-visual speech. The datasets are collected in lab-controlled environments
    where volunteers read the prepared phrases or sentences, or in-the-wild environments
    of TV interviews or talks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Lab-controlled Environment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 6: Summary of speech-related audio-visual datasets. These datasets can
    be used for all tasks related to speech we have mentioned above. Note that the
    length of a ‘speech’ dataset denotes the number of video clips, while for ‘music’
    or ’real event’ datasets, the length represents the total number of hours of the
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Dataset | Env. | Classes | Length* | Year |'
  prefs: []
  type: TYPE_TB
- en: '|  | GRID [[119](#bib.bib119)] | Lab | 34 | 33,000 | 2006 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lombard Grid [[120](#bib.bib120)] | Lab | 54 | 54,000 | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TCD TIMIT [[121](#bib.bib121)] | Lab | 62 | - | 2015 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Vid TIMIT [[122](#bib.bib122)] | Lab | 43 | - | 2009 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RAVDESS [[123](#bib.bib123)] | Lab | 24 | - | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SEWA [[124](#bib.bib124)] | Lab | 180 | - | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech | OuluVS [[125](#bib.bib125)] | Lab | 20 | 1000 | 2009 |'
  prefs: []
  type: TYPE_TB
- en: '|  | OuluVS2 [[126](#bib.bib126)] | Lab | 52 | 3640 | 2016 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Voxceleb [[127](#bib.bib127)] | Wild | 1,251 | 154,516 | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Voxceleb2 [[128](#bib.bib128)] | Wild | 6,112 | 1,128,246 | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LRW [[129](#bib.bib129)] | Wild | $\sim$1000 | 500,000 | 2016 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LRS [[59](#bib.bib59)] | Wild | $\sim$1000 | 118,116 | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LRS3 [[130](#bib.bib130)] | Wild | $\sim$1000 | 74,564 | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AVA-ActiveSpeaker [[131](#bib.bib131)] | Wild | - | 90,341 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|  | C4S [[132](#bib.bib132)] | Lab | - | 4.5 | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Music | ENST-Drums [[133](#bib.bib133)] | Lab | - | 3.75 | 2006 |'
  prefs: []
  type: TYPE_TB
- en: '|  | URMP [[134](#bib.bib134)] | Lab | - | 1.3 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|  | YouTube-8M [[135](#bib.bib135)] | Wild | 3862 | 350,000 | 2016 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AudioSet [[136](#bib.bib136)] | Wild | 632 | 4971 | 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| Real Event | Kinetics-400 [[137](#bib.bib137)] | Wild | 400 | 850* | 2018
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Kinetics-600 [[138](#bib.bib138)] | Wild | 600 | 1400* | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Kinetics-700 [[139](#bib.bib139)] | Wild | 700 | 1806* | 2018 |'
  prefs: []
  type: TYPE_TB
- en: 'Lab-controlled speech datasets are captured in specific environments, where
    volunteers are required to read the given phases or sentences. Some of the datasets
    only contain videos of speakers that utter the given sentences; these datasets
    include GRID [[119](#bib.bib119)], TCD TIMIT [[121](#bib.bib121)], and VidTIMIT
    [[122](#bib.bib122)]. Such datasets can be used for lip reading, talking face
    generation, and speech reconstruction. Development of more advanced datasets has
    continued: e.g., Livingstone et al. offered the RAVDESS dataset [[123](#bib.bib123)]
    that contained emotional speeches and songs. The items in it are also rated according
    to emotional validity, intensity and authenticity.'
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets such as Lombard Grid [[120](#bib.bib120)] and OuluVS [[125](#bib.bib125),
    [126](#bib.bib126)] focus on multiview videos. In addition, a dataset named SEWA
    offers rich annotations, including answers to a questionnaire, facial landmarks,
    (low-level descriptors of) LLD features, hand gestures, head gestures, transcript,
    valence, arousal, liking or disliking, template behaviors, episodes of agreement
    or disagreement, and episodes of mimicry.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 In-the-wild Environment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The above datasets were collected in lab environments; as a result, models trained
    on those datasets are difficult to apply in real-world scenarios. Thus, researchers
    have tried to collect real-world videos from TV interviews, talks and movies and
    released several real-world datasets, including LRW, LRW variants [[129](#bib.bib129),
    [59](#bib.bib59), [130](#bib.bib130)], Voxceleb and its variants [[127](#bib.bib127),
    [128](#bib.bib128)], AVA-ActiveSpeaker [[131](#bib.bib131)] and AVSpeech [[7](#bib.bib7)].
    The LRW dataset consists of 500 sentences [[129](#bib.bib129)], while its variant
    contains 1000 sentences[[59](#bib.bib59), [130](#bib.bib130)], all of which were
    spoken by hundreds of different speakers. VoxCeleb and its variants contain over
    100,000 utterances of 1,251 celebrities [[127](#bib.bib127)] and over a million
    utterances of 6,112 identities [[128](#bib.bib128)], respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'AVA-ActiveSpeaker [[131](#bib.bib131)] and AVSpeech [[7](#bib.bib7)] datasets
    contain even more videos. The AVA-ActiveSpeaker [[131](#bib.bib131)] dataset consists
    of 3.65 million human-labeled video frames (approximately 38.5 hrs) The AVSpeech
    [[7](#bib.bib7)] dataset contains approximately 4700 hours of video segments from
    a total of 290k YouTube videos spanning a wide variety of people, languages, and
    face poses. The details are reported in Table [6](#S6.T6 "Table 6 ‣ 6.1.1 Lab-controlled
    Environment ‣ 6.1 Audio-visual Speech Datasets ‣ 6 Recent Public Audio-visual
    Datasets ‣ Deep Audio-Visual Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Audio-visual Event Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another audio-visual dataset category consists of music or real-world event
    videos. These datasets are different from the aforementioned audio-visual speech
    datasets in not being limited to facial videos.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Music-related Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most music-related datasets were constructed in the lab environment. For example,
    ENST-Drums [[133](#bib.bib133)] merely contains drum videos of three professional
    drummers specializing in different music genres. The C4S dataset [[132](#bib.bib132)]
    consists of 54 videos of 9 distinct clarinetists, each performing 3 different
    classical music pieces twice (4.5h in total).
  prefs: []
  type: TYPE_NORMAL
- en: The URMP [[134](#bib.bib134)] dataset contains a number of multi-instrument
    musical pieces. However, these videos were recorded separately and then combined.
    To simplify the use of the URMP dataset, Chen et al. further proposed the Sub-URMP
    [[72](#bib.bib72)] dataset that contains multiple video frames and audio files
    extracted from the URMP dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Real Events-related Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: More and more real-world audio-visual event datasets have recently been released
    that consist of numerous videos uploaded to the Internet. The datasets often comprise
    hundreds or thousands of event classes and the corresponding videos. Representative
    datasets include the following.
  prefs: []
  type: TYPE_NORMAL
- en: Kinetics-400 [[137](#bib.bib137)], Kinetics-600 [[138](#bib.bib138)] and Kinetics-700
    [[139](#bib.bib139)] contain 400, 600 and 700 human action classes with at least
    400, 600, and 600 video clips for each action, respectively. Each clip lasts approximately
    10 s and is taken from a distinct YouTube video. The actions cover a broad range
    of classes, including human-object interactions such as playing instruments, as
    well as human-human interactions such as shaking hands. The AVA-Actions dataset
    [[140](#bib.bib140)] densely annotated 80 atomic visual actions in 43015 minutes
    of movie clips, where actions were localized in space and time, resulting in 1.58M
    action labels with multiple labels corresponding to a certain person.
  prefs: []
  type: TYPE_NORMAL
- en: AudioSet [[136](#bib.bib136)], a more general dataset, consists of an expanding
    ontology of 632 audio event classes and a collection of 2,084,320 human-labeled
    10-second sound clips. The clips were extracted from YouTube videos and cover
    a wide range of human and animal sounds, musical instruments and genres, and common
    everyday environmental sounds. YouTube-8M [[135](#bib.bib135)] is a large-scale
    labeled video dataset that consists of millions of YouTube video IDs with high-quality
    machine-generated annotations from a diverse vocabulary of 3,800+ visual entities.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Audio-visual learning (AVL) is a foundation of the multimodality problem that
    integrates the two most important perceptions of our daily life. Despite great
    efforts focused on AVL, there is still a long way to go for real-life applications.
    In this section, we briefly discuss the key challenges and the potential research
    directions in each category.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The heterogeneous nature of the discrepancy in AVL determines its inherent challenges.
    Audio tracks use a level of electrical voltage to represent analog signals, while
    the visual modality is usually represented in the RGB color space; the large gap
    between the two poses a major challenge to AVL. The essence of this problem is
    to understand the relation between audio and vision, which also is the basic challenge
    of AVL.
  prefs: []
  type: TYPE_NORMAL
- en: Audio-visual Separation and Localization is a longstanding problem in many real-life
    applications. Regardless of the previous advances in speaker-related or recent
    object-related separation and localization, the main challenges are failing to
    distinguish the timbre of various objects and exploring ways of generating the
    sounds of different objects. Addressing these challenges requires us to carefully
    design the models or ideas (e.g., the attention mechanism) for dealing with different
    objects. Audio-visual correspondence learning has vast potential applications,
    such as those in criminal investigations, medical care, transportation, and other
    industries. Many studies have tried to map different modalities into the shared
    feature space. However, it is challenging to obtain satisfactory results since
    extracting clear and effective information from ambiguous input and target modalities
    remains difficult. Therefore, sufficient prior information (the specific patterns
    people usually focus on) has a significant impact on obtaining more accurate results.
    Audio and vision generation focuses on empowered machine imagination. In contrast
    to the conventional discriminative problem, the task of cross-modality generation
    is to fit a mapping between probability distributions. Therefore, it is usually
    a many-to-many mapping problem that is difficult to learn. Moreover, despite the
    large difference between audio and visual modalities, humans are sensitive to
    the difference between real-world and generated results, and subtle artifacts
    can be easily noticed, which makes this task more challenging. Finally, audio-visual
    representation learning can be regarded as a generalization of other tasks. As
    we discussed before, both audio represented by electrical voltage and vision represented
    by the RGB color space are designed to be perceived by humans while not making
    it easy for a machine to discover the common features. The difficulty stems from
    having only two modalities and lacking explicit constraints. Therefore, the main
    challenge of this task is to find a suitable constraint. Unsupervised learning
    as a prevalent approach to this task provides a well-designed solution, while
    not having external supervision makes it difficult to achieve our goal. The challenging
    of the weakly supervised approach is to find correct implicit supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Directions for Future Research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AVL has been an active research field for many years [[16](#bib.bib16), [17](#bib.bib17)]
    and is crucial to modern life. However, there are still many open questions in
    AVL due to the challenging nature of the domain itself and people’s increasing
    demands.
  prefs: []
  type: TYPE_NORMAL
- en: First, from a macro perspective, as AVL is a classic multimodality problem,
    its primary issue is to learn the mapping between modalities, specifically to
    map the attributes in audio and the objects in an image or a video. We think that
    mimicking the human learning process, e.g., by following the ideas of the attention
    mechanism and a memory bank may improve performance of learning this mapping.
    Furthermore, the second most difficult goal is to learn logical reasoning. Endowing
    a machine with the ability to reason is not only important for AVL but also an
    open question for the entire AI community. Instead of directly empowering a machine
    with the full logic capability, which is a long way to go from the current state
    of development, we can simplify this problem and consider fully utilizing the
    prior information and constructing the knowledge graph. Building a comprehensive
    knowledge graph and leveraging it in specific areas properly may help machine
    thinking.
  prefs: []
  type: TYPE_NORMAL
- en: 'As to each task we have summarized before, Sec. [2](#S2 "2 Audio-visual Separation
    and Localization ‣ Deep Audio-Visual Learning: A Survey") and Sec. [3](#S3 "3
    Audio-visual Correspondence Learning ‣ Deep Audio-Visual Learning: A Survey")
    can be referred to as the problem of ‘understanding’, while Sec. [4](#S4 "4 Audio
    and Visual Generation ‣ Deep Audio-Visual Learning: A Survey") and Sec. [5](#S5
    "5 Audio-visual Representation Learning ‣ Deep Audio-Visual Learning: A Survey")
    can be referred to as ‘generation’ and ‘representation learning’ respectively.
    Significant advances in understanding and generation tasks such as lip-reading,
    speaker separation, and talking face generation have recently been achieved for
    human faces. The domain of faces is comparatively simple yet important since the
    scenes are normally constrained, and it has a sizable amount of available useful
    prior information. For example, consider a 3d face model. These faces usually
    have neutral expressions, while the emotions that are the basis of the face have
    not been studied well. Furthermore, apart from faces, the more complicated in-the-wild
    scenes with more conditions are worth considering. Adapting models to the new
    varieties of audio (stereoscopic audio) or vision (3D video and AR) also leads
    in a new direction. The datasets, especially large and high-quality ones that
    can significantly improve the performance of machine learning, are fundamental
    to the research community [[141](#bib.bib141)]. However, collecting a dataset
    is labor- and time-intensive. Small-sample learning also benefits the application
    of AVL. Learning representations, which is a more general and basic form of other
    tasks, can also mitigate the dataset problem. While recent studies lacked sufficient
    prior information or supervision to guide the training procedure, exploring suitable
    prior information may allow models to learn better representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, many studies focus on building more complex networks to improve performance,
    and the resulting networks generally entail unexplainable mechanisms. To make
    a model or an algorithm more robust and explainable, it is necessary to learn
    the essence of the earlier explainable algorithms to advance AVL.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The desire to better understand the world from the human perspective has drawn
    considerable attention to audio-visual learning in the deep learning community.
    This paper provides a comprehensive review of recent advances in audio-visual
    learning categorized into four research areas: audio-visual separation and localization,
    audio-visual correspondence learning, audio and visual generation, and audio-visual
    representation learning. Furthermore, we present a summary of datasets commonly
    used in audio-visual learning. The discussion section identifies the key challenges
    of each category followed by potential research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. V. Shannon, F.-G. Zeng, V. Kamath, J. Wygonski, and M. Ekelid, “Speech
    recognition with primarily temporal cues,” *Science*, pp. 303–304, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Krishna, C. Tran, J. Yu, and A. H. Tewfik, “Speech recognition with
    no speech or with noisy speech,” in *ICASSP*, 2019, pp. 1090–1094.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] R. He, W.-S. Zheng, and B.-G. Hu, “Maximum correntropy criterion for robust
    face recognition,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    pp. 1561–1576, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Fu, X. Wu, Y. Hu, H. Huang, and R. He, “Dual variational generation
    for low-shot heterogeneous face recognition,” *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Gabbay, A. Ephrat, T. Halperin, and S. Peleg, “Seeing through noise:
    Visually driven speaker separation and enhancement,” in *ICASSP*, 2018, pp. 3051–3055.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep audio-visual
    speech enhancement,” *arXiv preprint arXiv:1804.04121*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T.
    Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: A speaker-independent
    audio-visual model for speech separation,” *ACM Trans. Graph.*, pp. 112:1–112:11,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] P. Morgado, N. Vasconcelos, T. Langlois, and O. Wang, “Self-supervised
    generation of spatial audio for 360 video,” *arXiv preprint arXiv:1809.02587*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,
    “Improved training of wasserstein gans,” in *NIPS*, 2017, pp. 5767–5777.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
    for generative adversarial networks,” in *CVPR*, 2019, pp. 4401–4410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review
    and new perspectives,” *IEEE transactions on pattern analysis and machine intelligence*,
    pp. 1798–1828, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in *2017 IEEE
    International Conference on Computer Vision (ICCV)*, 2017, pp. 609–617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] B. Korbar, D. Tran, and L. Torresani, “Co-training of audio and video
    representations from self-supervised temporal synchronization,” *arXiv preprint
    arXiv:1807.00230*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-channel
    multi-speaker separation using deep clustering,” *arXiv preprint arXiv:1607.02173*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Luo, Z. Chen, and N. Mesgarani, “Speaker-independent speech separation
    with deep attractor network,” *IEEE/ACM Transactions on Audio, Speech, and Language
    Processing*, pp. 787–796, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] T. Darrell, J. W. Fisher, and P. Viola, “Audio-visual segmentation and
    “the cocktail party effect”,” in *Advances in Multimodal Interfaces—ICMI 2000*,
    2000, pp. 32–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. W. Fisher III, T. Darrell, W. T. Freeman, and P. A. Viola, “Learning
    joint statistical models for audio-visual fusion and segregation,” in *Advances
    in Neural Information Processing Systems 13*, 2001, pp. 772–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] K. D. Bochen Li, Z. Duan, and G. Sharma, “See and listen: Score-informed
    association of sound tracks to players in chamber music performance videos,” in
    *IEEE International Conference on Acoustics, Speech and Signal Processing*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Pu, Y. Panagakis, S. Petridis, and M. Pantic, “Audio-visual object
    localization and separation using low-rank and sparsity,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Lu, Z. Duan, and C. Zhang, “Listen and look: Audio–visual matching
    assisted speech source separation,” *IEEE Signal Processing Letters*, pp. 1315–1319,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] G. Morrone, S. Bergamaschi, L. Pasa, L. Fadiga, V. Tikhanoff, and L. Badino,
    “Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker
    environments,” in *ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2019, pp. 6900–6904.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Hershey and J. Movellan, “Audio-vision: Using audio-visual synchrony
    to locate sounds,” in *Advances in Neural Information Processing Systems 12*,
    2000, pp. 813–819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. L. Van Trees, *Optimum array processing: Part IV of detection, estimation,
    and modulation theory*.   John Wiley & Sons, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Zunino, M. Crocco, S. Martelli, A. Trucco, A. Del Bue, and V. Murino,
    “Seeing the sound: A new multimodal imaging device for computer vision,” in *Proceedings
    of the IEEE International Conference on Computer Vision Workshops*, 2015, pp.
    6–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] R. Gao, R. Feris, and K. Grauman, “Learning to separate object sounds
    by watching unlabeled video,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Senocak, T.-H. Oh, J. Kim, M.-H. Yang, and I. S. Kweon, “Learning to
    localize sound source in visual scenes,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 4358–4366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu, “Audio-visual event localization
    in unconstrained videos,” in *Computer Vision - ECCV 2018 - 15th European Conference,
    Munich, Germany, September 8-14, 2018, Proceedings, Part II*, 2018, pp. 252–268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott, and A. Torralba,
    “The sound of pixels,” *arXiv preprint arXiv:1804.03160*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] H. Zhao, C. Gan, W. Ma, and A. Torralba, “The sound of motions,” *CoRR*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] A. Rouditchenko, H. Zhao, C. Gan, J. H. McDermott, and A. Torralba, “Self-supervised
    audio-visual co-segmentation,” *CoRR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Parekh, A. Ozerov, S. Essid, N. Q. K. Duong, P. Pérez, and G. Richard,
    “Identify, locate and separate: Audio-visual object extraction in large video
    collections using weak supervision,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] H. Izadinia, I. Saleemi, and M. Shah, “Multimodal analysis for identification
    and segmentation of moving-sounding objects,” *IEEE Transactions on Multimedia*,
    pp. 378–390, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] E. Hoffer and N. Ailon, “Deep metric learning using triplet network,”
    in *International Workshop on Similarity-Based Pattern Recognition*, 2015, pp.
    84–92.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] K. Sriskandaraja, V. Sethu, and E. Ambikairajah, “Deep siamese architecture
    based replay detection for secure voice biometric.” in *Interspeech*, 2018, pp.
    671–675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] R. Białobrzeski, M. Kośmider, M. Matuszewski, M. Plata, and A. Rakowski,
    “Robust bayesian and light neural networks for voice spoofing detection,” *Proc.
    Interspeech 2019*, pp. 1028–1032, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Gomez-Alanis, A. M. Peinado, J. A. Gonzalez, and A. M. Gomez, “A light
    convolutional gru-rnn deep feature extractor for asv spoofing detection,” *Proc.
    Interspeech 2019*, pp. 1068–1072, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] X. Wu, R. He, Z. Sun, and T. Tan, “A light cnn for deep face representation
    with noisy labels,” *IEEE Transactions on Information Forensics and Security*,
    pp. 2884–2896, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Nagrani, S. Albanie, and A. Zisserman, “Seeing voices and hearing faces:
    Cross-modal biometric matching,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Torfi, S. M. Iranmanesh, N. M. Nasrabadi, and J. M. Dawson, “Coupled
    3d convolutional neural networks for audio-visual recognition,” *CoRR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] K. Simonyan and A. Zisserman, “Two-Stream Convolutional Networks for Action
    Recognition in Videos,” in *Advances in Neural Information Processing Systems
    27*, 2014, pp. 568–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Wen, M. A. Ismail, W. Liu, B. Raj, and R. Singh, “Disjoint Mapping
    Network for Cross-modal Matching of Voices and Faces,” *ArXiv e-prints*, p. arXiv:1807.04836,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” *CoRR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] C. Lippert, R. Sabatini, M. C. Maher, E. Y. Kang, S. Lee, O. Arikan, A. Harley,
    A. Bernal, P. Garst, V. Lavrenko, K. Yocum, T. Wong, M. Zhu, W.-Y. Yang, C. Chang,
    T. Lu, C. W. H. Lee, B. Hicks, S. Ramakrishnan, H. Tang, C. Xie, J. Piper, S. Brewerton,
    Y. Turpaz, A. Telenti, R. K. Roby, F. J. Och, and J. C. Venter, “Identification
    of individuals by trait prediction using whole-genome sequencing data,” *Proceedings
    of the National Academy of Sciences*, pp. 10 166–10 171, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] K. Hoover, S. Chaudhuri, C. Pantofaru, M. Slaney, and I. Sturdy, “Putting
    a face to the voice: Fusing audio and visual signals across a video to determine
    speakers,” *CoRR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S.-W. Chung, J. Son Chung, and H.-G. Kang, “Perfect match: Improved cross-modal
    embeddings for audio-visual synchronisation,” *ArXiv e-prints*, p. arXiv:1809.08001,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Wang, H. Huang, X. Zhang, J. Ma, and A. Zheng, “A novel distance learning
    for elastic cross-modal audio-visual matching,” in *2019 IEEE International Conference
    on Multimedia & Expo Workshops (ICMEW)*, 2019, pp. 300–305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] R. K. Srihari, “Combining text and image information in content-based
    retrieval,” in *Proceedings., International Conference on Image Processing*, 1995,
    pp. 326–329 vol.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] L. R. Long, L. E. Berman, and G. R. Thoma, “Prototype client/server application
    for biomedical text/image retrieval on the Internet,” in *Storage and Retrieval
    for Still Image and Video Databases IV*, 1996, pp. 362 – 372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,
    R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia retrieval,”
    in *Proceedings of the 18th ACM International Conference on Multimedia*, 2010,
    pp. 251–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Aytar, C. Vondrick, and A. Torralba, “See, hear, and read: Deep aligned
    representations,” *CoRR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] D. Surís, A. Duarte, A. Salvador, J. Torres, and X. Giró i Nieto, “Cross-modal
    embeddings for video and audio retrieval,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Hong, W. Im, and H. S. Yang, “Deep learning for content-based, cross-modal
    retrieval of videos and music,” *CoRR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Nagrani, S. Albanie, and A. Zisserman, “Learnable pins: Cross-modal
    embeddings for person identity,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] R. Wang, H. Huang, X. Zhang, J. Ma, and A. Zheng, “A novel distance learning
    for elastic cross-modal audio-visual matching,” in *2019 IEEE International Conference
    on Multimedia Expo Workshops (ICMEW)*, 2019, pp. 300–305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Petridis, Z. Li, and M. Pantic, “End-to-end visual speech recognition
    with lstms,” in *Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International
    Conference on*, 2017, pp. 2592–2596.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Wand, J. Koutník, and J. Schmidhuber, “Lipreading with long short-term
    memory,” in *Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International
    Conference on*, 2016, pp. 6115–6119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. M. Assael, B. Shillingford, S. Whiteson, and N. de Freitas, “Lipnet:
    Sentence-level lipreading,” *arXiv preprint*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. S. Chung, A. W. Senior, O. Vinyals, and A. Zisserman, “Lip reading
    sentences in the wild,” in *CVPR*, 2017, pp. 3444–3453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller,
    and S. Zafeiriou, “Adieu features? end-to-end speech emotion recognition using
    a deep convolutional recurrent network,” in *Acoustics, Speech and Signal Processing
    (ICASSP), 2016 IEEE International Conference on*, 2016, pp. 5200–5204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Deep
    audio-visual speech recognition,” *arXiv preprint arXiv:1809.02108*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Dupont and J. Luettin, “Audio-visual speech modeling for continuous
    speech recognition,” *IEEE transactions on multimedia*, pp. 141–151, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Petridis and M. Pantic, “Prediction-based audiovisual fusion for classification
    of non-linguistic vocalisations,” *IEEE Transactions on Affective Computing*,
    pp. 45–58, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] G. Potamianos, C. Neti, G. Gravier, A. Garg, and A. W. Senior, “Recent
    advances in the automatic recognition of audiovisual speech,” *Proceedings of
    the IEEE*, pp. 1306–1326, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] D. Hu, X. Li *et al.*, “Temporal multimodal learning in audiovisual speech
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 3574–3582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, “Multimodal
    deep learning,” in *Proceedings of the 28th international conference on machine
    learning (ICML-11)*, 2011, pp. 689–696.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Ninomiya, N. Kitaoka, S. Tamura, Y. Iribe, and K. Takeda, “Integration
    of deep bottleneck features for audio-visual speech recognition,” in *Sixteenth
    Annual Conference of the International Speech Communication Association*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. Stafylakis and G. Tzimiropoulos, “Combining residual networks with
    lstms for lipreading,” *arXiv preprint arXiv:1703.04105*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Nussbaum-Thom, J. Cui, B. Ramabhadran, and V. Goel, “Acoustic modeling
    using bidirectional gated recurrent convolutional units,” 2016, pp. 390–394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” *arXiv preprint
    arXiv:1701.07875*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] L. Chen, S. Srivastava, Z. Duan, and C. Xu, “Deep cross-modal audio-visual
    generation,” in *Proceedings of the on Thematic Workshops of ACM Multimedia 2017*,
    2017, pp. 349–357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] H. Zhu, A. Zheng, H. Huang, and R. He, “High-resolution talking face generation
    via mutual information approximation,” *arXiv preprint arXiv:1812.06589*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to bridge
    domain gap for person re-identification,” in *CVPR*, 2018, pp. 79–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, and S.-H. Lai,
    “Auggan: Cross domain adaptation with gan-based data augmentation,” in *ECCV*,
    2018, pp. 718–731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] T. L. Cornu and B. Milner, “Reconstructing intelligible audio speech from
    visual speech features,” in *Sixteenth Annual Conference of the International
    Speech Communication Association*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] A. Ephrat, T. Halperin, and S. Peleg, “Improved speech reconstruction
    from silent video,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2017, pp. 455–462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] T. Thomas Le Cornu and B. Milner, “Generating intelligible audio speech
    from visual speech,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    pp. 1751–1761, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Davis, M. Rubinstein, N. Wadhwa, G. J. Mysore, F. Durand, and W. T.
    Freeman, “The visual microphone: passive recovery of sound from video,” 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adelson, and W. T.
    Freeman, “Visually indicated sounds,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2016, pp. 2405–2413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Zhou, Z. Wang, C. Fang, T. Bui, and T. L. Berg, “Visual to sound: Generating
    natural sound for videos in the wild,” *arXiv preprint arXiv:1712.01393*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] A. Ephrat and S. Peleg, “Vid2speech: Speech reconstruction from silent
    video,” in *2017 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2017, pp. 5095–5099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo, A. Courville,
    and Y. Bengio, “Samplernn: An unconditional end-to-end neural audio generation
    model,” *arXiv preprint arXiv:1612.07837*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C.-H. Wan, S.-P. Chuang, and H.-Y. Lee, “Towards audio to scene image
    synthesis using generative adversarial network,” *arXiv preprint arXiv:1808.04108*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Qiu and H. Kataoka, “Image generation associated with music data,”
    in *2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops,
    CVPR Workshops 2018, Salt Lake City, UT, USA, June 18-22, 2018*, 2018, pp. 2510–2513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] W. Hao, Z. Zhang, and H. Guan, “Cmcgan: A uniform framework for cross-modal
    visual-audio mutual generation,” *arXiv preprint arXiv:1711.08102*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador, E. Mohedano,
    K. McGuinness, J. Torres, and X. Giro-i Nieto, “Speech-conditioned face generation
    using generative adversarial networks,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    and W. Matusik, “Speech2face: Learning the face behind a voice,” *arXiv preprint
    arXiv:1905.09773*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] O. Alemi, J. Françoise, and P. Pasquier, “Groovenet: Real-time music-driven
    dance movement generation using artificial neural networks,” *networks*, vol. 8,
    no. 17, p. 26, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] J. Lee, S. Kim, and K. Lee, “Listen to dance: Music-driven choreography
    generation using autoregressive encoder-decoder network,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] E. Shlizerman, L. Dery, H. Schoen, and I. Kemelmacher-Shlizerman, “Audio
    to body dynamics,” in *Proc. CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Tang, J. Jia, and H. Mao, “Dance with melody: An lstm-autoencoder approach
    to music-oriented dance synthesis,” in *2018 ACM Multimedia Conference on Multimedia
    Conference, MM 2018, Seoul, Republic of Korea, October 22-26, 2018*, 2018, pp.
    1598–1606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] N. Yalta, S. Watanabe, K. Nakadai, and T. Ogata, “Weakly supervised deep
    recurrent neural networks for basic dance step generation,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] R. Kumar, J. Sotelo, K. Kumar, A. de Brébisson, and Y. Bengio, “Obamanet:
    Photo-realistic lip-sync from text,” *arXiv preprint arXiv:1801.01442*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-Shlizerman, “Synthesizing
    obama: learning lip sync from audio,” *ACM Transactions on Graphics (TOG)*, p. 95,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. S. Chung, A. Jamaludin, and A. Zisserman, “You said that?” *CoRR*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. A. Jalalifar, H. Hasani, and H. Aghajan, “Speech-driven facial reenactment
    using conditional generative adversarial networks,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven facial
    animation with temporal gans,” in *BMVC*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] L. Chen, Z. Li, R. K. Maddox, Z. Duan, and C. Xu, “Lip movements generation
    at a glance,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation
    by adversarially disentangled audio-visual representation,” *CoRR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Z. D. C. X. Lele Chen, Ross K Maddox, “Hierarchical cross-modal talking
    face generation with dynamic pixel-wise loss,” in *The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] O. Wiles, A. Koepke, and A. Zisserman, “X2face: A network for controlling
    face generation by using images, audio, and pose codes,” in *European Conference
    on Computer Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] A. Samadani, E. Kubica, R. Gorbet, and D. Kulic, “Perception and generation
    of affective hand movements,” *I. J. Social Robotics*, pp. 35–51, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Tilmanne and T. Dutoit, “Expressive gait synthesis using PCA and gaussian
    modeling,” in *Motion in Games - Third International Conference, MIG 2010, Utrecht,
    The Netherlands, November 14-16, 2010\. Proceedings*, 2010, pp. 363–374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] M. Brand and A. Hertzmann, “Style machines,” in *Proceedings of the 27th
    Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2000,
    New Orleans, LA, USA, July 23-28, 2000*, 2000, pp. 183–192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. M. Wang, D. J. Fleet, and A. Hertzmann, “Multifactor gaussian process
    models for style-content separation,” in *Machine Learning, Proceedings of the
    Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June
    20-24, 2007*, 2007, pp. 975–982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] G. W. Taylor and G. E. Hinton, “Factored conditional restricted boltzmann
    machines for modeling motion style,” in *Proceedings of the 26th Annual International
    Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18,
    2009*, 2009, pp. 1025–1032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] L. Crnkovic-Friis and L. Crnkovic-Friis, “Generative choreography using
    deep learning,” *CoRR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] D. Holden, J. Saito, and T. Komura, “A deep learning framework for character
    motion synthesis and editing,” *ACM Transactions on Graphics (TOG)*, p. 138, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Graves and J. Schmidhuber, “Framewise phoneme classification with
    bidirectional lstm and other neural network architectures,” *Neural Networks*,
    pp. 602–610, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Saito, E. Matsumoto, and S. Saito, “Temporal generative adversarial
    nets with singular value clipping,” in *IEEE International Conference on Computer
    Vision (ICCV)*, 2017, p. 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Aytar, C. Vondrick, and A. Torralba, “Soundnet: Learning sound representations
    from unlabeled video,” in *Advances in Neural Information Processing Systems*,
    2016, pp. 892–900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] K. Leidal, D. Harwath, and J. R. Glass, “Learning modality-invariant
    representations for speech and images,” *2017 IEEE Automatic Speech Recognition
    and Understanding Workshop (ASRU)*, pp. 424–429, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] R. Arandjelović and A. Zisserman, “Objects that sound,” *arXiv preprint
    arXiv:1712.06651*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Parekh, S. Essid, A. Ozerov, N. Q. Duong, P. Pérez, and G. Richard,
    “Weakly supervised representation learning for unsynchronized audio-visual events,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops*, 2018, pp. 2518–2519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] D. Hu, F. Nie, and X. Li, “Deep co-clustering for unsupervised audiovisual
    learning,” *arXiv preprint arXiv:1807.03094*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised
    multisensory features,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 631–648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proceedings of the 26th annual international conference on machine learning*,
    2009, pp. 41–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An audio-visual corpus
    for speech perception and automatic speech recognition,” *The Journal of the Acoustical
    Society of America*, pp. 2421–2424, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] N. Alghamdi, S. Maddock, R. Marxer, J. Barker, and G. J. Brown, “A corpus
    of audio-visual lombard speech with frontal and profile views,” *The Journal of
    the Acoustical Society of America*, pp. EL523–EL529, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] N. Harte and E. Gillen, “Tcd-timit: An audio-visual corpus of continuous
    speech,” *IEEE Transactions on Multimedia*, pp. 603–615, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Sanderson and B. C. Lovell, “Multi-region probabilistic histograms
    for robust and scalable identity inference,” in *International Conference on Biometrics*,
    2009, pp. 199–208.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. R. Livingstone and F. A. Russo, “The ryerson audio-visual database
    of emotional speech and song (ravdess): A dynamic, multimodal set of facial and
    vocal expressions in north american english,” *PloS one*, p. e0196391, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Kossaifi, R. Walecki, Y. Panagakis, J. Shen, M. Schmitt, F. Ringeval,
    J. Han, V. Pandit, B. Schuller, K. Star *et al.*, “Sewa db: A rich database for
    audio-visual emotion and sentiment research in the wild,” *arXiv preprint arXiv:1901.02839*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] G. Zhao, M. Barnard, and M. Pietikainen, “Lipreading with local spatiotemporal
    descriptors,” *IEEE Transactions on Multimedia*, pp. 1254–1265, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] I. Anina, Z. Zhou, G. Zhao, and M. Pietikäinen, “Ouluvs2: A multi-view
    audiovisual database for non-rigid mouth motion analysis,” in *Automatic Face
    and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops
    on*, 2015, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker
    identification dataset,” *arXiv preprint arXiv:1706.08612*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,”
    *arXiv preprint arXiv:1806.05622*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. S. Chung and A. Zisserman, “Lip reading in the wild,” in *Asian Conference
    on Computer Vision*, 2016, pp. 87–103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] ——, “Lip reading in profile,” in *British Machine Vision Conference*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Roth, S. Chaudhuri, O. Klejch, R. Marvin, A. Gallagher, L. Kaver,
    S. Ramaswamy, A. Stopczynski, C. Schmid, Z. Xi *et al.*, “Ava-activespeaker: An
    audio-visual dataset for active speaker detection,” *arXiv preprint arXiv:1901.01342*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] A. Bazzica, J. van Gemert, C. C. Liem, and A. Hanjalic, “Vision-based
    detection of acoustic timed events: a case study on clarinet note onsets,” *arXiv
    preprint arXiv:1706.09556*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] O. Gillet and G. Richard, “Enst-drums: an extensive audio-visual database
    for drum signals processing.” in *ISMIR*, 2006, pp. 156–159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] B. Li, X. Liu, K. Dinesh, Z. Duan, and G. Sharma, “Creating a multitrack
    classical music performance dataset for multimodal music analysis: Challenges,
    insights, and applications,” *IEEE Transactions on Multimedia*, pp. 522–535, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,
    and S. Vijayanarasimhan, “Youtube-8m: A large-scale video classification benchmark,”
    *arXiv preprint arXiv:1609.08675*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
    for audio events,” in *Acoustics, Speech and Signal Processing (ICASSP), 2017
    IEEE International Conference on*, 2017, pp. 776–780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *et al.*, “The kinetics human action video
    dataset,” *arXiv preprint arXiv:1705.06950*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman,
    “A short note about kinetics-600,” *arXiv preprint arXiv:1808.01340*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short note on
    the kinetics-700 human action dataset,” *arXiv preprint arXiv:1907.06987*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 6047–6056.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting unreasonable
    effectiveness of data in deep learning era,” in *ICCV*, 2017, pp. 843–852.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
