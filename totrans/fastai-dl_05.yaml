- en: 'Deep Learning 2: Part 1 Lesson 5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第1部分第5课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自* [*fast.ai 课程*](http://www.fast.ai/)* 的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: '[Lesson 5](http://forums.fast.ai/t/wiki-lesson-5/9403)'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[Lesson 5](http://forums.fast.ai/t/wiki-lesson-5/9403)'
- en: I. Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I. 介绍
- en: 'There is not enough publications on structured deep learning, but it is definitely
    happening in industries:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化深度学习方面的出版物还不够，但在行业中肯定正在发生：
- en: '[](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848?source=post_page-----dd904506bee8--------------------------------)
    [## Structured Deep Learning'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848?source=post_page-----dd904506bee8--------------------------------)
    [## 结构化深度学习'
- en: by Kerem Turgutlu
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: by Kerem Turgutlu
- en: towardsdatascience.com](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848?source=post_page-----dd904506bee8--------------------------------)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848?source=post_page-----dd904506bee8--------------------------------)
- en: 'You can download images from Google by using [this tool](https://github.com/hardikvasa/google-images-download)
    and solve your own problems:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用[这个工具](https://github.com/hardikvasa/google-images-download)从Google下载图片并解决自己的问题：
- en: '[](https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96?source=post_page-----dd904506bee8--------------------------------)
    [## Fun with small image data-sets (Part 2)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96?source=post_page-----dd904506bee8--------------------------------)
    [## 小图像数据集的乐趣（第2部分）'
- en: by Nikhil B
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: by Nikhil B
- en: towardsdatascience.com](https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96?source=post_page-----dd904506bee8--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96?source=post_page-----dd904506bee8--------------------------------)
- en: 'Introduction on how to train Neural Net (a great technical writing):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何训练神经网络的介绍（一篇很棒的技术文章）：
- en: '[](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73?source=post_page-----dd904506bee8--------------------------------)
    [## How do we ‘train’ neural networks ?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73?source=post_page-----dd904506bee8--------------------------------)
    [## 我们如何‘训练’神经网络？'
- en: by Vitaly Bushaev
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: by Vitaly Bushaev
- en: towardsdatascience.com](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73?source=post_page-----dd904506bee8--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73?source=post_page-----dd904506bee8--------------------------------)
- en: Students are competing with Jeremy in [Kaggle seedling classification competition](https://www.kaggle.com/c/plant-seedlings-classification/leaderboard).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 学生们正在与Jeremy一起参加[Kaggle幼苗分类竞赛](https://www.kaggle.com/c/plant-seedlings-classification/leaderboard)。
- en: II. Collaborative Filtering — using MovieLens dataset
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II. 协同过滤 — 使用MovieLens数据集
- en: The notebook discussed can be found [here(lesson5-movielens.ipynb)](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的笔记本可以在[这里（lesson5-movielens.ipynb）](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)找到。
- en: Let’s take a look at the data. We will use `userId` (categorical), `movieId`(categorical)
    and `rating` (dependent) for modeling.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据。我们将使用`userId`（分类）、`movieId`（分类）和`rating`（依赖）进行建模。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Create subset for Excel**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为Excel创建子集**'
- en: We create a crosstab of the most popular movies and most movie-addicted users
    which we will copy into Excel for visualization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了最受欢迎的电影和最痴迷于电影的用户的交叉表，我们将把它复制到Excel中进行可视化。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[This](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/collab_filter.xlsx)
    is the excel file with above information. To begin with, we will use **matrix
    factorization/decomposition** instead of building a neural net.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[这](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/collab_filter.xlsx)是包含上述信息的Excel文件。首先，我们将使用**矩阵分解**而不是构建神经网络。'
- en: Blue cells — the actual rating
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色单元格 — 实际评分
- en: Purple cells — our predictions
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紫色单元格 — 我们的预测
- en: Red cell — our loss function i.e. Root Mean Squared Error (RMSE)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红色单元格 — 我们的损失函数即均方根误差（RMSE）
- en: Green cells — movie embeddings (randomly initialized)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绿色单元格 — 电影嵌入（随机初始化）
- en: Orange cells — user embeddings (randomly initialized)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 橙色单元格 — 用户嵌入（随机初始化）
- en: Each prediction is a dot product of movie embedding vector and user embedding
    vector. In linear algebra term, it is equivalent of matrix product as one is a
    row and one is a column. If there is no actual rating, we set the prediction to
    zero (think of this as test data — not training data).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测是电影嵌入向量和用户嵌入向量的点积。在线性代数术语中，这相当于矩阵乘积，因为一个是行，一个是列。如果没有实际评分，我们将预测设为零（将其视为测试数据
    — 而不是训练数据）。
- en: We then use Gradient Descent to minimize our loss. Microsoft excel has a “solver”
    in the add-ins that would minimize a variable by changing selected cells (`GRG
    Nonlinear` is the method you want to use).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用梯度下降来最小化我们的损失。Microsoft Excel中有一个“求解器”插件，可以通过更改选定的单元格来最小化一个变量（`GRG Nonlinear`是您要使用的方法）。
- en: This can be called “shallow learning” (as opposed to deep learning) as there
    is no nonlinear layer or a second linear layer. So what did we just do intuitively?
    The five numbers for each movie is called “embeddings” (latent factors) — the
    first number might represent how much it is sci-fi and fantasy, the second might
    be how much special effect is used for a movie, the third might be how dialog
    driven it is, etc. Similarly, each user also has 5 numbers representing, for example,
    how much does the user like sci-fi fantasy, special effects, and dialog-driven
    in movies. Our prediction is a cross product of these vectors. Since we do not
    have every movie review for every user, we are trying to figure out which movies
    are similar this movie and how other users who rated other movies similarly to
    this user rate this movie (hence the name “collaborative”).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以称为“浅层学习”（与深度学习相对），因为没有非线性层或第二个线性层。那么我们刚刚直观地做了什么？每部电影的五个数字称为“嵌入”（潜在因素） - 第一个数字可能表示它有多少科幻和奇幻，第二个可能表示为电影使用了多少特效，第三个可能表示它有多少对话驱动，等等。同样，每个用户也有5个数字，例如，表示用户有多喜欢科幻奇幻、特效和对话驱动的电影。我们的预测是这些向量的叉积。由于我们没有每个用户对每部电影的评论，我们试图找出哪些电影与这部电影相似，以及其他用户如何评价这部电影（因此称为“协同”）。
- en: What do we do with a new user or a new movie — do we have to retrain a model?
    We do not have a time to cover this now, but basically you need to have a new
    user model or a new movie model that you would use initially and over time you
    will need to re-train the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新用户或新电影，我们该怎么办 - 我们需要重新训练模型吗？我们现在没有时间来讨论这个问题，但基本上您需要有一个新的用户模型或新的电影模型，最初会使用它，随着时间的推移，您将需要重新训练模型。
- en: '**Simple Python version [**[**26:03**](https://youtu.be/J99NV9Cr75I?t=26m3s)**]**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简单的Python版本[26:03]**'
- en: This should look familiar by now. We create a validation set by picking random
    set of ID’s. `wd` is a weight decay for L2 regularization, and `n_factors` is
    how big an embedding matrix we want.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该现在看起来很熟悉。我们通过选择随机ID集创建一个验证集。`wd`是L2正则化的权重衰减，`n_factors`是我们想要的嵌入矩阵有多大。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We create a model data object from CSV file:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从CSV文件创建一个模型数据对象：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then get a learner that is suitable for the model data, and fit the model:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到一个适合模型数据的学习器，并拟合模型：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Output MSE
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输出MSE
- en: 'Since the output is Mean Squared Error, you can take RMSE by:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输出是均方误差，您可以通过以下方式获得RMSE：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output is about 0.88 which outperforms the bench mark of 0.91.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出约为0.88，优于0.91的基准。
- en: 'You can get a prediction in a usual way:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以通常的方式获得预测：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And you can also plot using seaborn `sns` (built on top of matplotlib):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用seaborn `sns`进行绘图（建立在matplotlib之上）：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Dot product with Python**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用Python进行点积**'
- en: '`T` is a tensor in Torch'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`T`是Torch中的张量'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When we have a mathematical operator between tensors in numpy or PyTorch, it
    will do element-wise assuming that they both have the same dimensionality. The
    below is how you would calculate the dot product of two vectors (e.g. (1, 2)⋅(2,
    2) = 6 — the first rows of matrix a and b):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在numpy或PyTorch中的张量之间有数学运算符时，它将假定它们具有相同的维度进行逐元素操作。下面是如何计算两个向量的点积的方法（例如(1,
    2)⋅(2, 2) = 6 - 矩阵a和b的第一行）：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Building our first custom layer (i.e. PyTorch module) [**[**33:55**](https://youtu.be/J99NV9Cr75I?t=33m55s)**]**'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**构建我们的第一个自定义层（即PyTorch模块）[33:55]**'
- en: We do this by creating a Python class that extends `nn.Module` and override`forward`
    function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建一个扩展`nn.Module`并覆盖`forward`函数的Python类来实现这一点。
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can call it and get the expected result (notice that we do not need
    to say `model.forward(a, b)` to call the `forward` function — it is a PyTorch
    magic.) [[40:14](https://youtu.be/J99NV9Cr75I?t=40m14s)]:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用它并获得预期结果（请注意，我们不需要说`model.forward(a, b)`来调用`forward`函数 - 这是PyTorch的魔法。）[40:14]：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Building more complex module [**[**41:31**](https://youtu.be/J99NV9Cr75I?t=41m31s)**]**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**构建更复杂的模块[41:31]**'
- en: 'This implementation has two additions to the `DotProduct` class:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现对`DotProduct`类有两个添加：
- en: Two `nn.Embedding` matrices
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个`nn.Embedding`矩阵
- en: Look up our users and movies in above embedding matrices
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上面的嵌入矩阵中查找我们的用户和电影
- en: It is quite possible that user ID’s are not contiguous which makes it hard to
    use as an index of embedding matrix. So we will start by creating indexes that
    starts from zero and contiguous and replace `ratings.userId` column with the index
    by using Panda’s `apply` function with an anonymous function `lambda` and do the
    same for `ratings.movieId` .
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 用户ID可能不是连续的，这使得难以用作嵌入矩阵的索引。因此，我们将从零开始创建连续的索引，并用Panda的`apply`函数和匿名函数`lambda`替换`ratings.userId`列，并对`ratings.movieId`执行相同操作。
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Tip:* `{o:i for i,o in enumerate(u_uniq)}` is a handy line of code to keep
    in your tool belt!'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示：*`{o:i for i,o in enumerate(u_uniq)}`是一行方便的代码，可以保存在您的工具包中！'
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that `__init__` is a constructor which is now needed because our class
    needs to keep track of “states” (how many movies, mow many users, how many factors,
    etc). We initialized the weights to random numbers between 0 and 0.05 and you
    can find more information about a standard algorithm for weight initialization,
    “Kaiming Initialization” [here](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)
    (PyTorch has He initialization utility function but we are trying to do things
    from scratch here) [[46:58](https://youtu.be/J99NV9Cr75I?t=46m58s)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`__init__`是一个构造函数，现在需要它，因为我们的类需要跟踪“状态”（有多少电影，有多少用户，有多少因素等）。我们将权重初始化为0到0.05之间的随机数，您可以在这里找到有关权重初始化的标准算法“Kaiming
    Initialization”的更多信息（PyTorch具有He初始化实用程序函数，但我们正在尝试从头开始做事）[46:58]。
- en: '`Embedding` is not a tensor but a **variable**. A variable does the exact same
    operations as a tensor but it also does automatic differentiation. To pull a tensor
    out of a variable, call `data` attribute. All the tensor functions have a variation
    with trailing underscore (e.g. `uniform_`) will do things in-place.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`Embedding`不是张量，而是**变量**。变量执行与张量完全相同的操作，但它还执行自动微分。要从变量中提取张量，请调用`data`属性。所有张量函数都有一个带有下划线的变体（例如`uniform_`），将在原地执行操作。'
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are reusing `ColumnarModelData` (from fast.ai library) from Rossmann notebook,
    and that is the reason behind why there are both categorical and continuous variables
    in `def forward(self, cats, conts)` function in `EmbeddingDot` class [[50:20](https://youtu.be/J99NV9Cr75I?t=50m20s)].
    Since we do not have continuous variable in this case, we will ignore `conts`
    and use the first and second columns of `cats` as `users` and `movies` . Note
    that they are mini-batches of users and movies. It is important not to manually
    loop through mini-batches because you will not get GPU acceleration, instead,
    process a whole mini-batch at a time as you see in line 3 and 4 of `forward` function
    above [[51:00](https://youtu.be/J99NV9Cr75I?t=51m)–52:05].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在重用来自Rossmann笔记本的`ColumnarModelData`（来自fast.ai库），这就是为什么在`EmbeddingDot`类的`def
    forward(self, cats, conts)`函数中有分类和连续变量的原因[[50:20](https://youtu.be/J99NV9Cr75I?t=50m20s)]。由于在这种情况下我们没有连续变量，我们将忽略`conts`，并使用`cats`的第一列和第二列作为`users`和`movies`。请注意，它们是用户和电影的小批量。重要的是不要手动循环遍历小批量，因为这样不会获得GPU加速，而是一次处理整个小批量，就像您在上面`forward`函数的第3和第4行中看到的那样[[51:00](https://youtu.be/J99NV9Cr75I?t=51m)–52:05]。
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`optim` is what gives us the optimizers in PyTorch. `model.parameters()` is
    one of the function inherited from `nn.Modules` that gives us all the weight to
    be updated/learned.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`optim`是PyTorch中提供优化器的东西。`model.parameters()`是从`nn.Modules`继承的一个函数，它给出所有需要更新/学习的权重。'
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function is from fast.ai library [[54:40](https://youtu.be/J99NV9Cr75I?t=54m40s)]
    and is closer to regular PyTorch approach compared to `learner.fit()` we have
    been using. It will not give you features like “stochastic gradient descent with
    restarts” or “differential learning rate” out of box.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数来自fast.ai库[[54:40](https://youtu.be/J99NV9Cr75I?t=54m40s)]，与我们一直在使用的`learner.fit()`相比，更接近常规的PyTorch方法。它不会为您提供“随机梯度下降重启”或“不同学习率”等功能。
- en: '**Let’s improve our model**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**让我们改进我们的模型**'
- en: '**Bias** — to adjust to generally popular movies or generally enthusiastic
    users.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差** - 调整到普遍受欢迎的电影或普遍热情的用户。'
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`squeeze` is PyTorch version of *broadcasting* [[1:04:11](https://youtu.be/J99NV9Cr75I?t=1h4m11s)]
    for more information, see Machine Learning class or [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`squeeze`是PyTorch版本的*广播*[[1:04:11](https://youtu.be/J99NV9Cr75I?t=1h4m11s)]，有关更多信息，请参阅机器学习课程或[numpy文档](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html)。'
- en: Can we squish the ratings so that it is between 1 and 5? Yes! By putting the
    prediction through sigmoid function will result in number between 1 and 0\. So
    in our case, we can multiply that by 4 and add 1 — which will result in number
    between 1 and 5.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以压缩评分，使其在1和5之间吗？可以！通过将预测通过sigmoid函数，将得到1和0之间的数字。因此，在我们的情况下，我们可以将其乘以4并加1 -
    这将得到1和5之间的数字。
- en: '`F` is a PyTorch functional (`torch.nn.functional`) that contains all functions
    for tensors, and is imported as `F` in most cases.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`F`是PyTorch功能（`torch.nn.functional`），包含所有张量的函数，并在大多数情况下导入为`F`。'
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s take a look at fast.ai code [[1:13:44](https://youtu.be/J99NV9Cr75I?t=1h13m44s)]
    we used in our **Simple Python version.** In `column_data.py` file, `CollabFilterDataSet.get_leaner`
    calls `get_model` function that creates `EmbeddingDotBias` class that is identical
    to what we created.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们在**简单的Python版本**中使用的fast.ai代码[[1:13:44](https://youtu.be/J99NV9Cr75I?t=1h13m44s)]。在`column_data.py`文件中，`CollabFilterDataSet.get_leaner`调用`get_model`函数，该函数创建了`EmbeddingDotBias`类，与我们创建的内容相同。
- en: Neural Net Version [[1:17:21](https://youtu.be/J99NV9Cr75I?t=1h17m21s)]
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络版本[[1:17:21](https://youtu.be/J99NV9Cr75I?t=1h17m21s)]
- en: We go back to excel sheet to understand the intuition. Notice that we create
    user_idx to look up Embeddings just like we did in the python code earlier. If
    we were to one-hot-encode the user_idx and multiply it by user embeddings, we
    will get the applicable row for the user. If it is just matrix multiplication,
    why do we need Embeddings? It is for computational performance optimization purposes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到Excel表格来理解直觉。请注意，我们创建了`user_idx`来查找嵌入，就像我们之前在Python代码中所做的那样。如果我们要对`user_idx`进行独热编码并将其乘以用户嵌入，我们将得到用户的适用行。如果只是矩阵乘法，为什么我们需要嵌入？这是为了计算性能优化的目的。
- en: Rather than calculating the dot product of user embedding vector and movie embedding
    vector to get a prediction, we will concatenate the two and feed it through neural
    net.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算用户嵌入向量和电影嵌入向量的点积以获得预测不同，我们将连接这两者并将其馈送到神经网络中。
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that we no longer has bias terms since `Linear` layer in PyTorch already
    has a build in bias. `nh` is a number of activations a linear layer creates (Jeremy
    calls it “num hidden”).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们不再有偏差项，因为PyTorch中的`Linear`层已经内置了偏差。`nh`是线性层创建的激活数量（Jeremy称之为“num hidden”）。
- en: It only has one hidden layer, so maybe not “deep”, but this is definitely a
    neural network.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 它只有一个隐藏层，所以可能不是“深度”，但这绝对是一个神经网络。
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice that the loss functions are also in `F` (here, it s mean squared loss).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，损失函数也在`F`中（这里是均方损失）。
- en: 'Now that we have neural net, there are many things we can try:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了神经网络，我们可以尝试很多事情：
- en: Add dropouts
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加丢弃
- en: Use different embedding sizes for user embedding and movie embedding
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为用户嵌入和电影嵌入使用不同的嵌入大小
- en: Not only user and movie embeddings, but append movie genre embedding and/or
    timestamp from the original data.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不仅用户和电影嵌入，还可以附加电影类型嵌入和/或原始数据中的时间戳。
- en: Increase/decrease number of hidden layers and activations
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加/减少隐藏层和激活数量
- en: Increase/decrease regularization
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加/减少正则化
- en: '**What is happening in the training loop?** [[1:33:21](https://youtu.be/J99NV9Cr75I?t=1h33m21s)]'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练循环中发生了什么？**[[1:33:21](https://youtu.be/J99NV9Cr75I?t=1h33m21s)]'
- en: Currently, we are passing off the updating of weights to PyTorch’s optimizer.
    What does an optimizer do? and what is a `momentum`?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将权重的更新交给PyTorch的优化器。优化器做什么？`动量`是什么？
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We are going to implement gradient descent in an excel sheet ([graddesc.xlsm](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/graddesc.xlsm))
    — see worksheets right to left. First we create a random *x*’s, and *y*’s that
    are linearly correlated with the *x*’s (e.g. *y*= *a*x* + *b*). By using sets
    of *x*’s and *y*’s, we will try to learn *a* and *b.*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Excel表格中实现梯度下降（[graddesc.xlsm](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/graddesc.xlsm)）-
    从右到左查看工作表。首先我们创建一组随机的*x*和*y*，它们与*x*线性相关（例如*y*= *a*x* + *b*）。通过使用一组*x*和*y*，我们将尝试学习*a*和*b*。
- en: 'To calculate the error, we first need a prediction, and square the difference:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算误差，我们首先需要一个预测，并计算差的平方：
- en: To reduce the error, we increase/decrease *a* and *b* a little bit and figure
    out what would make the error decrease. This is called finding the derivative
    through finite differencing.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少误差，我们稍微增加/减少*a*和*b*，并找出什么会使误差减少。这被称为通过有限差分找到导数。
- en: 'Finite differencing gets complicated in high dimensional spaces [[1:41:46](https://youtu.be/J99NV9Cr75I?t=1h41m46s)],
    and it becomes very memory intensive and takes a long time. So we want to find
    some way to do this more quickly. It is worthwhile to look up things like Jacobian
    and Hessian (Deep Learning book: [section 4.3.1 page 84](http://www.deeplearningbook.org/contents/numerical.html)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中，有限差分变得复杂[[1:41:46](https://youtu.be/J99NV9Cr75I?t=1h41m46s)]，并且变得非常占用内存且需要很长时间。因此，我们希望找到一种更快地完成这项工作的方法。值得查阅雅可比和黑塞（深度学习书籍：[第4.3.1节第84页](http://www.deeplearningbook.org/contents/numerical.html)）。
- en: Chain Rule and Backpropagation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链式规则和反向传播
- en: 'The faster approach is to do this analytically [[1:45:27](https://youtu.be/J99NV9Cr75I?t=1h45m27s)].
    For this, we need a chain rule:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 更快的方法是通过分析进行[[1:45:27](https://youtu.be/J99NV9Cr75I?t=1h45m27s)]。为此，我们需要一个链式规则：
- en: Overview of [chain rule](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[链式规则概述](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version)'
- en: Here is a great article by Chris Olah on [Backpropagation as a chain rule](http://colah.github.io/posts/2015-08-Backprop/).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Chris Olah关于[反向传播作为链式规则](http://colah.github.io/posts/2015-08-Backprop/)的一篇很棒的文章。
- en: 'Now we replace the finite-difference with an actual derivative [WolframAlpha](https://www.wolframalpha.com/)
    gave us (notice that finite-difference output is fairly close to the actual derivative
    and good way to do quick sanity check if you need to calculate your own derivative):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用实际导数替换有限差分[WolframAlpha](https://www.wolframalpha.com/)给我们提供了（请注意，有限差分输出与实际导数非常接近，是计算自己的导数的快速检查的好方法）：
- en: “Online” training — mini-batch with size 1
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “在线”训练 - 小批量大小为1
- en: And this is how you do SGD with excel sheet. If you were to change the prediction
    value with the output from CNN spreadsheet, we can train CNN with SGD.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你在Excel表格中使用SGD的方法。如果你将预测值更改为CNN电子表格的输出，我们可以使用SGD训练CNN。
- en: Momentum [[1:53:47](https://youtu.be/J99NV9Cr75I?t=1h53m47s)]
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动量[[1:53:47](https://youtu.be/J99NV9Cr75I?t=1h53m47s)]
- en: Come on, take a hint — that’s a good direction. Please keep doing that but more.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来吧，给个提示 - 那是一个好方向。请继续这样做，但更多。
- en: 'With this approach, we will use a linear interpolation between the current
    mini-batch’s derivative and the step (and direction) we took after the last mini-batch
    (cell K9):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，我们将使用当前小批量导数和上一个小批量之后我们采取的步骤（以及方向）之间的线性插值（单元格K9）：
- en: Compared to *de*/*db* whose sign (+/-) is random, the one with momentum will
    keep going the same direction a little bit faster up till certain point. This
    will reduce a number of epochs required for training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机符号（+/-）的*de*/*db*相比，具有动量的方向会保持相同的方向，直到某个点为止。这将减少训练所需的周期数。
- en: Adam [[1:59:04](https://youtu.be/J99NV9Cr75I?t=1h59m4s)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adam[[1:59:04](https://youtu.be/J99NV9Cr75I?t=1h59m4s)]
- en: Adam is much faster but the issue has been that final predictions are not as
    good as as they are with SGD with momentum. It seems as though that it was due
    to the combined usage of Adam and weight decay. The new version that fixes this
    issue is called **AdamW**.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Adam速度更快，但问题在于最终预测不如使用动量的SGD那么好。似乎是由于Adam和权重衰减的结合使用。修复此问题的新版本称为**AdamW**。
- en: '`cell J8` : a linear interpolation of derivative and previous direction (identical
    to what we had in momentum)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`单元格J8`：导数和上一个方向的线性插值（与动量中的相同）'
- en: '`cell L8` : a linear interpolation of derivative squared + derivative squared
    from last step ( `cell L7`)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`单元格L8`：导数平方和上一步的导数平方的线性插值（`单元格L7`）'
- en: The idea is called “exponentially weighted moving average” (in another words,
    average with previous values multiplicatively decreased)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个想法被称为“指数加权移动平均”（换句话说，平均值随着先前值的乘法递减）
- en: Learning rate is much higher than before because we are dividing it by square
    root of `L8` .
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率比以前高得多，因为我们将其除以`L8`的平方根。
- en: If you take a look at fast.ai library (model.py), you will notice that in `fit`
    function, it does not just calculate average loss, but it is calculating the **exponentially
    weighted moving average of loss**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下fast.ai库（model.py），你会注意到在`fit`函数中，它不仅计算平均损失，而且计算**损失的指数加权移动平均**。
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Another helpful concept is whenever you see `α(…) + (1-α)(…)`, immediately think
    **linear interpolation.**
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的概念是每当你看到`α(…) + (1-α)(…)`时，立即想到**线性插值**。
- en: '**Some intuitions**'
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**一些直觉**'
- en: We calculated exponentially weighted moving average of gradient squared, take
    a square root of that, and divided the learning rate by it.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计算了梯度平方的指数加权移动平均值，对其取平方根，并将学习率除以它。
- en: Gradient squared is always positive.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度的平方始终为正。
- en: When there is high variance in gradients, gradient squared will be large.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当梯度变化很大时，梯度的平方会很大。
- en: When the gradients are constant, gradient squared will be small.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当梯度恒定时，梯度的平方会很小。
- en: If gradients are changing a lot, we want to be careful and divide the learning
    rate by a big number (slow down)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果梯度变化很大，我们希望小心谨慎，并通过一个大数来除以学习率（减慢速度）
- en: If gradients are not changing much, we will take a bigger step by dividing the
    learning rate with a small number
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果梯度变化不大，我们将通过一个小数来除以学习率，从而迈出更大的一步
- en: '**Adaptive learning rate** — keep track of the average of the squares of the
    gradients and use that to adjust the learning rate. So there is just one learning
    rage, but effectively every parameter at every epoch is getting a bigger jump
    if the gradient is constant; smaller jump otherwise.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应学习率** ——跟踪梯度平方的平均值，并使用它来调整学习率。因此只有一个学习率，但如果梯度恒定，则每个参数在每个时期都会跳得更远；否则跳得更小。'
- en: There are two momentums — one for gradient, and the other for gradient squared
    (in PyTorch, it is called a beta which is a tuple of two numbers)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有两种动量 —— 一个用于梯度，另一个用于梯度的平方（在PyTorch中，它被称为beta，是两个数字的元组）
- en: AdamW[[2:11:18](https://youtu.be/J99NV9Cr75I?t=2h11m18s)]
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdamW[[2:11:18](https://youtu.be/J99NV9Cr75I?t=2h11m18s)]
- en: When there are much more parameters than data points, regularizations become
    important. We had seen dropout previously, and weight decay is another type of
    regularization. Weight decay (L2 regularization) penalizes large weights by adding
    squared weights (times weight decay multiplier) to the loss. Now the loss function
    wants to keep the weights small because increasing the weights will increase the
    loss; hence only doing so when the loss improves by more than the penalty.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当参数比数据点多得多时，正则化变得重要。我们之前见过dropout，权重衰减是另一种正则化方法。权重衰减（L2正则化）通过将平方权重（乘以权重衰减系数）添加到损失中来惩罚大权重。现在损失函数希望保持权重较小，因为增加权重会增加损失；因此只有在损失提高超过惩罚时才会这样做。
- en: The problem is that since we added the squared weights to the loss function,
    this affects the moving average of gradients and the moving average of the squared
    gradients for Adam. This result in decreasing the amount of weight decay when
    there is high variance in gradients, and increasing the amount of weight decay
    when there is little variation. In other words, “penalize large weights unless
    gradients varies a lot” which is not what we intended. AdamW removed the weight
    decay out of the loss function, and added it directly when updating the weights.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，由于我们将平方权重添加到损失函数中，这会影响Adam的梯度移动平均和梯度平方移动平均。这会导致在梯度变化很大时减少权重衰减的量，在变化很小时增加权重衰减的量。换句话说，“惩罚大权重，除非梯度变化很大”，这不是我们的初衷。AdamW将权重衰减从损失函数中移除，并在更新权重时直接添加。
