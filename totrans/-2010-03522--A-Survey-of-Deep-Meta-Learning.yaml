- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:58:56'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2010.03522] A Survey of Deep Meta-Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2010.03522](https://ar5iv.labs.arxiv.org/html/2010.03522)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹institutetext: M. Huisman ²²institutetext: J.N. van Rijn ³³institutetext:
    A. Plaat ⁴⁴institutetext: Leiden Institute of Advanced Computer Science'
  prefs: []
  type: TYPE_NORMAL
- en: Niels Bohrweg 1, 2333CA Leiden, The Netherlands
  prefs: []
  type: TYPE_NORMAL
- en: '⁴⁴email: m.huisman@liacs.leidenuniv.nl'
  prefs: []
  type: TYPE_NORMAL
- en: A Survey of Deep Meta-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mike Huisman    Jan N. van Rijn    Aske Plaat
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep neural networks can achieve great successes when presented with large data
    sets and sufficient computational resources. However, their ability to learn new
    concepts quickly is limited. Meta-learning is one approach to address this issue,
    by enabling the network to learn how to learn. The field of Deep Meta-Learning
    advances at great speed, but lacks a unified, in-depth overview of current techniques.
    With this work, we aim to bridge this gap. After providing the reader with a theoretical
    foundation, we investigate and summarize key methods, which are categorized into
    i) metric-, ii) model-, and iii) optimization-based techniques. In addition, we
    identify the main open challenges, such as performance evaluations on heterogeneous
    benchmarks, and reduction of the computational costs of meta-learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Meta-learning Learning to learn Few-shot learning Transfer learning Deep learning
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, deep learning techniques have achieved remarkable successes
    on various tasks, including game-playing (Mnih et al., [2013](#bib.bib54); Silver
    et al., [2016](#bib.bib73)), image recognition (Krizhevsky et al., [2012](#bib.bib41);
    He et al., [2015](#bib.bib29)), machine translation (Wu et al., [2016](#bib.bib90)),
    and automatic classification in biomedical domains (Goceri, [2019a](#bib.bib19);
    Goceri and Karakas, [2020](#bib.bib22); Iqbal et al., [2020](#bib.bib36), [2019b](#bib.bib35),
    [2019a](#bib.bib34)). Despite these advances and recent solutions (Goceri, [2019b](#bib.bib20),
    [2020](#bib.bib21)), ample challenges remain to be solved, such as the large amounts
    of data and training that are needed to achieve good performance. These requirements
    severely constrain the ability of deep neural networks to learn new concepts quickly,
    one of the defining aspects of human intelligence (Jankowski et al., [2011](#bib.bib37);
    Lake et al., [2017](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Meta-learning has been suggested as one strategy to overcome this challenge
    (Naik and Mammone, [1992](#bib.bib57); Schmidhuber, [1987](#bib.bib69); Thrun,
    [1998](#bib.bib79)). The key idea is that meta-learning agents improve their learning
    ability over time, or equivalently, learn to learn. The learning process is primarily
    concerned with tasks (set of observations) and takes place at two different levels:
    an inner- and an outer-level. At the inner-level, a new task is presented, and
    the agent tries to quickly learn the associated concepts from the training observations.
    This quick adaptation is facilitated by knowledge that it has accumulated across
    earlier tasks at the outer-level. Thus, whereas the inner-level concerns a single
    task, the outer-level concerns a multitude of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca316fbc5c2bcb1c70128f139fdef950.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The accuracy scores of the covered techniques on 1-shot miniImageNet
    classification. The used feature extraction backbone is displayed on the x-axis.
    As one can see, there is a strong relationship between the network complexity
    and the classification performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the term meta-learning has been used with various scopes. In its
    broadest sense, it encapsulates all systems that leverage prior learning experience
    in order to learn new tasks more quickly (Vanschoren, [2018](#bib.bib82)). This
    broad notion includes more traditional algorithm selection and hyperparameter
    optimization techniques for Machine Learning (Brazdil et al., [2008](#bib.bib8)).
    In this work, however, we focus on a subset of the meta-learning field which develops
    meta-learning procedures to learn a good inductive bias for (deep) neural networks.¹¹1Here,
    inductive bias refers to the assumptions of a model which guide predictions on
    unseen data (Mitchell, [1980](#bib.bib53)). Henceforth, we use the term Deep Meta-Learning
    to refer to this subfield of meta-learning.
  prefs: []
  type: TYPE_NORMAL
- en: The field of Deep Meta-Learning is advancing at a quick pace, while it lacks
    a coherent, unifying overview, providing detailed insights into the key techniques.
    Vanschoren ([2018](#bib.bib82)) has surveyed meta-learning techniques, where meta-learning
    was used in the broad sense, limiting its account of Deep Meta-Learning techniques.
    Also, many exciting developments in deep meta-learning have happened after the
    survey was published. A more recent survey by Hospedales et al. ([2020](#bib.bib32))
    adopts the same notion of deep meta-learning as we do, but aims to give a broad
    overview, omitting technical details of the various techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We attempt to fill this gap by providing detailed explications of contemporary
    Deep Meta-Learning techniques, using a unified notation. More specifically, we
    cover modern techniques in the field for supervised and reinforcement learning,
    that have achieved state-of-the-art performance, obtained popularity in the field,
    and presented novel ideas. Extra attention is paid to MAML (Finn et al., [2017](#bib.bib14)),
    and related techniques, because of their impact on the field. We show how the
    techniques relate to each other, detail their strengths and weaknesses, identify
    current challenges, and provide an overview of promising future research directions.
    One of the observations that we make is that the network complexity is highly
    related to the few-shot classification performance (see [Figure 1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey of Deep Meta-Learning")). One might expect that
    in a few-shot setting, where only a few examples are available to learn from,
    the number of network parameters should be kept small to prevent overfitting.
    Clearly, the figure shows that this does not hold, as techniques that use larger
    backbones tend to achieve better performance. One important factor might be that
    due to the large number of tasks that have been seen by the network, we are in
    a setting where similarly large amounts of observations have been evaluated. This
    result suggests that the size of the network should be taken into account when
    comparing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This work can serve as an educational introduction to the field of Deep Meta-Learning,
    and as reference material for experienced researchers in the field. Throughout,
    we will adopt the taxonomy used by Vinyals ([2017](#bib.bib85)), which identifies
    three categories of Deep Meta-Learning approaches: i) metric-based, ii) model-based,
    and iii) optimization-based meta-learning techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this work is structured as follows. Section 2 builds a common
    foundation on which we will base our overview of Deep Meta-Learning techniques.
    Sections 3, 4, and 5 cover the main metric-, model-, and optimization-based meta-learning
    techniques, respectively. Section 6 provides a helicopter view of the field and
    summarizes the key challenges and open questions. [Table 1](#S1.T1 "Table 1 ‣
    1 Introduction ‣ A Survey of Deep Meta-Learning") gives an overview of notation
    that we will use throughout this paper.
  prefs: []
  type: TYPE_NORMAL
- en: '| Expression | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| Meta-learning | Learning to learn |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$ |
    A task consisting of a labeled support and query set |'
  prefs: []
  type: TYPE_TB
- en: '| Support set | The train set $D^{tr}_{\mathcal{T}_{j}}$ associated with a
    task $\mathcal{T}_{j}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Query set | The test set $D^{test}_{\mathcal{T}_{j}}$ associated with a task
    $\mathcal{T}_{j}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{x}_{i}$ | Example input vector $i$ in the support set |'
  prefs: []
  type: TYPE_TB
- en: '| $y_{i}$ | (One-hot encoded) label of example input $\boldsymbol{x}_{i}$ from
    the support set |'
  prefs: []
  type: TYPE_TB
- en: '| $k$ | Number of examples per class in the support set |'
  prefs: []
  type: TYPE_TB
- en: '| $N$ | Number of classes in the support and query sets of a task |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{x}$ | Input in the query set |'
  prefs: []
  type: TYPE_TB
- en: '| $y$ | A (one-hot encoded) label for input $\boldsymbol{x}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $(f/g/h)_{\circ}$ | Neural network function with parameters $\circ$ |'
  prefs: []
  type: TYPE_TB
- en: '| Inner-level | At the level of a single task |'
  prefs: []
  type: TYPE_TB
- en: '| Outer-level | At the meta-level: across tasks |'
  prefs: []
  type: TYPE_TB
- en: '| Fast weights | A term used in the literature to denote task-specific parameters
    |'
  prefs: []
  type: TYPE_TB
- en: '| Base-learner | Learner that works at the inner-level |'
  prefs: []
  type: TYPE_TB
- en: '| Meta-learner | Learner that operates at the outer-level |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{\theta}$ | The parameters of the base-learner network |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{L}_{D}$ | Loss function with respect to task/dataset $D$ |'
  prefs: []
  type: TYPE_TB
- en: '| Input embedding | Penultimate layer representation of the input |'
  prefs: []
  type: TYPE_TB
- en: '| Task embedding | An internal representation of a task in a network/system
    |'
  prefs: []
  type: TYPE_TB
- en: '| SL | Supervised Learning |'
  prefs: []
  type: TYPE_TB
- en: '| RL | Reinforcement Learning |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Some notation and meaning, which we use throughout this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we build the necessary foundation for investigating Deep Meta-Learning
    techniques in a consistent manner. To begin with, we contrast regular learning
    and meta-learning. Afterwards, we briefly discuss how Deep Meta-Learning relates
    to different fields, what the usual training and evaluation procedure looks like,
    and which benchmarks are often used for this purpose. We finish this section by
    describing the context and some applications of the meta-learning field.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 The Meta Abstraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we contrast base-level (regular) learning and meta-learning
    for two different paradigms, i.e., supervised and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Regular Supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In supervised learning, we wish to learn a function $f_{\boldsymbol{\theta}}:X\rightarrow
    Y$ that learns to map inputs $\boldsymbol{x}_{i}\in X$ to their corresponding
    outputs $y_{i}\in Y$. Here, $\boldsymbol{\theta}$ are model parameters (e.g. weights
    in a neural network) that determine the function’s behavior. To learn these parameters,
    we are given a data set of $m$ observations: $D=\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{m}$.
    Thus, given a data set $\mathcal{D}$, learning boils down to finding the correct
    setting for $\boldsymbol{\theta}$ that minimizes an empirical loss function $\mathcal{L}_{D}$,
    which must capture how the model is performing, such that appropriate adjustments
    to its parameters can be made. In short, we wish to find'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{SL}:=\operatorname*{arg\,min}_{\boldsymbol{\theta}}\,\mathcal{L}_{D}(\boldsymbol{\theta}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $SL$ stands for “supervised learning". Note that this objective is specific
    to data set $\mathcal{D}$, meaning that our model $f_{\boldsymbol{\theta}}$ may
    not generalize to examples outside of $\mathcal{D}$. To measure generalization,
    one could evaluate the performance on a separate test data set, which contains
    unseen examples. A popular way to do this is through cross-validation, where one
    repeatedly creates train and test splits $D^{tr},D^{test}\subset D$ and uses these
    to train and evaluate a model respectively (Hastie et al., [2009](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: Finding globally optimal parameters $\boldsymbol{\theta}_{SL}$ is often computationally
    infeasible. We can, however, approximate them, guided by pre-defined meta-knowledge
    $\omega$ (Hospedales et al., [2020](#bib.bib32)), which includes, e.g., the initial
    model parameters $\boldsymbol{\theta}$, choice of optimizer, and learning rate
    schedule. As such, we approximate
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{SL}\approx g_{\omega}(D,\mathcal{L}_{D}),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $g_{\omega}$ is an optimization procedure that uses pre-defined meta-knowledge
    $\omega$, data set $\mathcal{D}$, and loss function $\mathcal{L}_{D}$, to produce
    updated weights $g_{\omega}(D,\mathcal{L}_{D})$ that (presumably) perform well
    on $\mathcal{D}$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Supervised Meta-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast, supervised meta-learning does not assume that any meta-knowledge
    $\omega$ is given, or pre-defined. Instead, the goal of meta-learning is to find
    the best $\omega$, such that our (regular) base-learner can learn new tasks (data
    sets) as quickly as possible. Thus, whereas supervised regular learning involves
    one data set, supervised meta-learning involves a group of data sets. The goal
    is to learn meta-knowledge $\omega$ such that our model can learn many different
    tasks well. Thus, our model is learning to learn.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, we have a probability distribution of tasks $p(\mathcal{T})$
    and wish to find optimal meta-knowledge
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\omega^{*}:=\operatorname*{arg\,min}_{\omega}\,\underbrace{\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}}_{\textrm{Outer-level}}[\underbrace{\mathcal{L}_{\mathcal{T}_{j}}(g_{\omega}(\mathcal{T}_{j},\mathcal{L}_{\mathcal{T}_{j}}))}_{\textrm{Inner-level}}].$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, the inner-level concerns task-specific learning, while the outer-level
    concerns multiple tasks. One can now easily see why this is meta-learning: we
    learn $\omega$, which allows for quick learning of tasks $\mathcal{T}_{j}$ at
    the inner-level. Hence, we are learning to learn.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Regular Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In reinforcement learning, we have an agent that learns from experience. That
    is, it interacts with an environment, modeled by a Markov Decision Process (MDP)
    $M=(S,A,P,r,p_{0},\gamma,T)$. Here, $S$ is the set of states, $A$ the set of actions,
    $P$ the transition probability distribution defining $P(s_{t+1}|s_{t},a_{t})$,
    $r:S\times A\rightarrow\mathbb{R}$ the reward function, $p_{0}$ the probability
    distribution over initial states, $\gamma\in[0,1]$ the discount factor, and $T$
    the time horizon (maximum number of time steps) (Sutton and Barto, [2018](#bib.bib77);
    Duan et al., [2016](#bib.bib11)).
  prefs: []
  type: TYPE_NORMAL
- en: At every time step $t$, the agent finds itself in state $s_{t}$, in which the
    agent performs an action $a_{t}$, computed by a policy function $\pi_{\boldsymbol{\theta}}$
    (i.e., $a_{t}=\pi_{\boldsymbol{\theta}}(s_{t})$), which is parameterized by weights
    $\boldsymbol{\theta}$. In turn, it receives a reward $r_{t}=r(s_{t},\pi_{\boldsymbol{\theta}}(s_{t}))\in\mathbb{R}$
    and a new state $s_{t+1}$. This process of interactions continues until a termination
    criterion is met (e.g. fixed time horizon $T$ reached). The goal of the agent
    is to learn how to act in order to maximize its expected reward. The reinforcement
    learning (RL) goal is to find
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{RL}:=\operatorname*{arg\,min}_{\boldsymbol{\theta}}\,\mathbb{E}_{\mbox{traj}}\sum_{t=0}^{T}\gamma^{t}r(s_{t},\pi_{\boldsymbol{\theta}}(s_{t})),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where we take the expectation over the possible trajectories $\mbox{traj}=(s_{0},\pi_{\boldsymbol{\theta}}(s_{0}),\allowbreak\ldots
    s_{T},\pi_{\boldsymbol{\theta}}(s_{T}))$ due to the random nature of MDPs (Duan
    et al., [2016](#bib.bib11)). Note that $\gamma$ is a hyperparameter that can prioritize
    short- or long-term rewards by decreasing or increasing it, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Also in the case of reinforcement learning it is often infeasible to find the
    global optimum $\boldsymbol{\theta}_{RL}$, and thus we settle for approximations.
    In short, given a learning method $\omega$, we approximate
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{RL}\approx g_{\omega}(\mathcal{T}_{j},\mathcal{L}_{\mathcal{T}_{j}}),$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where again $\mathcal{T}_{j}$ is the given MDP, and $g_{\omega}$ is the optimization
    algorithm, guided by pre-defined meta-knowledge $\omega$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in a Markov Decision Process (MDP), the agent knows the state at any
    given time step $t$. When this is not the case, it becomes a Partially Observable
    Markov Decision Process (POMDP), where the agent receives only observations $O$,
    and uses these to update its belief with regard to the state it is in (Sutton
    and Barto, [2018](#bib.bib77)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Meta Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The meta abstraction has as its object a group of tasks, or Markov Decision
    Processes (MDPs) in the case of reinforcement learning. Thus, instead of maximizing
    the expected reward on a single MDP, the meta reinforcement learning objective
    is to maximize the expected reward over various MDPs, by learning meta-knowledge
    $\omega$. Here, the MDPs are sampled from some distribution $p(\mathcal{T})$.
    So, we wish to find a set of parameters
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\omega}^{*}:=\operatorname*{arg\,min}_{\boldsymbol{\omega}}\,\underbrace{\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}}_{\textrm{Outer-level}}\left[\underbrace{\mathbb{E}_{traj}\sum_{t=0}^{T}\gamma^{t}r(s_{t},\pi_{g_{\omega}(\mathcal{T}_{j},\mathcal{L}_{\mathcal{T}_{j}})}(s_{t}))}_{\textrm{Inner-level}}\right].$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 2.1.5 Contrast with other Fields
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have provided a formal basis for our discussion for both supervised
    and reinforcement meta-learning, it is time to contrast meta-learning briefly
    with two related areas of machine learning that also have the goal to improve
    the speed of learning. We will start with transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning In Transfer Learning, one tries to transfer knowledge of previous
    tasks to new, unseen tasks (Pan and Yang, [2009](#bib.bib61); Taylor and Stone,
    [2009](#bib.bib78)), which can be challenging when the new task comes from a different
    distribution than the one used for training Iqbal et al. ([2018](#bib.bib33)).
    The distinction between Transfer Learning and Meta-Learning has become more opaque
    over time. A key property of meta-learning techniques, however, is their meta-objective,
    which explicitly aims to optimize performance across a distribution over tasks
    (as seen in previous sections by taking the expected loss over a distribution
    of tasks). This objective need not always be present in Transfer Learning techniques,
    e.g., when one pre-trains a model on a large data set, and fine-tunes the learned
    weights on a smaller data set.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task learning Another, closely related field, is that of multi-task learning.
    In multi-task learning, a model is jointly trained to perform well on multiple
    fixed tasks (Hospedales et al., [2020](#bib.bib32)). Meta-learning, in contrast,
    aims to find a model that can learn new (previously unseen) tasks quickly. This
    difference is illustrated in [footnote 2](#footnote2 "footnote 2 ‣ Figure 2 ‣
    2.1.5 Contrast with other Fields ‣ 2.1 The Meta Abstraction ‣ 2 Foundation ‣ A
    Survey of Deep Meta-Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4568d4795fab33dad1833f70976aac19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The difference between multi-task learning and meta-learning²²2Adapted
    from [https://meta-world.github.io/](https://meta-world.github.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 The Meta-Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we have described the learning objectives for (meta)
    supervised and reinforcement learning. We will now describe the general setting
    that can be used to achieve these objectives. In general, one optimizes a meta-objective
    by using various tasks, which are data sets in the context of supervised learning,
    and (Partially Observable) Markov Decision Processes in the case of reinforcement
    learning. This is done in three stages: the i) meta-train stage, ii) meta-validation
    stage, and iii) meta-test stage, each of which is associated with a set of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: First, in the meta-train stage, the meta-learning algorithm is applied to the
    meta-train tasks. Second, the meta-validation tasks can then be used to evaluate
    the performance on unseen tasks, which were not used for training. Effectively,
    this measures the meta-generalization ability of the trained network, which serves
    as feedback to tune, e.g., hyper-parameters of the meta-learning algorithm. Third,
    the meta-test tasks are used to give a final performance estimate of the meta-learning
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae59762e05066ec6a749fbe7d1395145.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of $N$-way, $k$-shot classification, where $N=5$, and
    $k=1$. Meta-validation tasks are not displayed. Adapted from Ravi and Larochelle
    ([2017](#bib.bib65)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 $N$-way, $k$-shot Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A frequently used instantiation of this general meta-setup is called $N$-way,
    $k$-shot classification (see [Figure 3](#S2.F3 "Figure 3 ‣ 2.2 The Meta-Setup
    ‣ 2 Foundation ‣ A Survey of Deep Meta-Learning")). This setup is also divided
    into the three stages—meta-train, meta-validation, and meta-test—which are used
    for meta-learning, meta-learner hyperparameter optimization, and evaluation, respectively.
    Each stage has a corresponding set of disjoint labels, i.e., $L^{tr},L^{val},L^{test}\subset
    Y$, such that $L^{tr}\cap L^{val}=\emptyset,L^{tr}\cap L^{test}=\emptyset$, and
    $L^{val}\cap L^{test}=\emptyset$. In a given stage $s$, tasks/episodes $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$
    are obtained by sampling examples $(\boldsymbol{x}_{i},y_{i})$ from the full data
    set $\mathcal{D}$, such that every $y_{i}\in L^{s}$. Note that this requires access
    to a data set $\mathcal{D}$. The sampling process is guided by the $N$-way, $k$-shot
    principle, which states that every training data set $D^{tr}_{\mathcal{T}_{j}}$
    should contain exactly $N$ classes and $k$ examples per class, implying that $|D^{tr}_{\mathcal{T}_{j}}|=N\cdot
    k$. Furthermore, the true labels of examples in the test set $D_{\mathcal{T}_{j}}^{test}$
    must be present in the train set $D^{tr}_{\mathcal{T}_{j}}$ of a given task $\mathcal{T}_{j}$.
    $D^{tr}_{\mathcal{T}{j}}$ acts as a support set, literally supporting classification
    decisions on the query set $D^{test}_{\mathcal{T}_{j}}$. Importantly, note that
    with this terminology, the query set (or test set) of a task is actually used
    during the meta-training phase. Furthermore, the fact that the labels across stages
    are disjoint ensures that we test the ability of a model to learn new concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The meta-learning objective in the training phase is to minimize the loss function
    of the model predictions on the query sets, conditioned on the support sets. As
    such, for a given task $\mathcal{T}_{j}$, the model ‘sees’ the support set, and
    extracts information from the support set to guide its predictions on the query
    set. By applying this procedure to different episodes/tasks $\mathcal{T}_{j}$,
    the model will slowly accumulate meta-knowledge $\omega$, which can ultimately
    speed up learning on new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to achieve this is by doing this with regular neural networks,
    but as was pointed out by various authors (see, e.g., Finn et al. ([2017](#bib.bib14)))
    more sophisticated architectures will vastly outperform such networks. In the
    remainder of this work, we will review such architectures.
  prefs: []
  type: TYPE_NORMAL
- en: At the meta-validation and meta-test stages, or evaluation phases, the learned
    meta-information in $\omega$ is fixed. The model is, however, still allowed to
    make task-specific updates to its parameters $\boldsymbol{\theta}$ (which implies
    that it is learning). After task-specific updates, we can evaluate the performance
    on the test sets. In this way, we test how well a technique performs at meta-learning.
  prefs: []
  type: TYPE_NORMAL
- en: $N$-way, $k$-shot classification is often performed for small values of $k$
    (since we want our models to learn new concepts quickly, i.e., from few examples).
    In that case, one can refer to it as few-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Common Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we briefly describe some benchmarks that can be used to evaluate meta-learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Omniglot (Lake et al., [2011](#bib.bib42)): This data set presents an image
    recognition task. Each image corresponds to one out of 1 623 characters from 50
    different alphabets. Every character was drawn by 20 people. Note that in this
    case, the characters are the classes/labels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ImageNet (Deng et al., [2009](#bib.bib10)): This is the largest image classification
    data set, containing more than 20K classes and over 14 million colored images.
    miniImageNet is a mini variant of the large ImageNet data set (Deng et al., [2009](#bib.bib10))
    for image classification, proposed by Vinyals et al. ([2016](#bib.bib86)) to reduce
    the engineering efforts to run experiments. The mini data set contains 60 000
    colored images of size $84\times 84$. There are a total of 100 classes present,
    each accorded by 600 examples. tieredImageNet (Ren et al., [2018](#bib.bib66))
    is another variation of the large ImageNet data set. It is similar to miniImageNet,
    but contains a hierarchical structure. That is, there are 34 classes, each with
    its own sub-classes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CIFAR-10 and CIFAR-100 (Krizhevsky, [2009](#bib.bib40)): Two other image recognition
    data sets. Each one contains 60K RGB images of size $32\times 32$. CIFAR-10 and
    CIFAR-100 contain 10 and 100 classes respectively, with a uniform number of examples
    per class (6 000 and 600 respectively). Every class in CIFAR-100 also has a super-class,
    of which there are 20 in the full data set. Many variants of the CIFAR data sets
    can be sampled, giving rise to e.g. CIFAR-FS (Bertinetto et al., [2019](#bib.bib7))
    and FC-100 (Oreshkin et al., [2018](#bib.bib60)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CUB-200-2011 (Wah et al., [2011](#bib.bib88)): The CUB-200-2011 data set contains
    roughly 12K RGB images of birds from 200 species. Every image has some labeled
    attributes (e.g. crown color, tail shape).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MNIST (LeCun et al., [2010](#bib.bib44)): MNIST presents a hand-written digit
    recognition task, containing ten classes (for digits 0 through 9). In total, the
    data set is split into a 60K train and 10K test gray scale images of hand-written
    digits.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta-Dataset (Triantafillou et al., [2020](#bib.bib81)): This data set comprises
    several other data sets such as Omniglot (Lake et al., [2011](#bib.bib42)), CUB-200
    (Wah et al., [2011](#bib.bib88)), ImageNet (Deng et al., [2009](#bib.bib10)),
    and more (Triantafillou et al., [2020](#bib.bib81)). An episode is then constructed
    by sampling a data set (e.g. Omniglot) and selecting a subset of labels to create
    train and test splits as before. In this way, broader generalization is enforced
    since the tasks are more distant from each other.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta-world (Yu et al., [2019](#bib.bib94)): A meta reinforcement learning data
    set, containing 50 robotic manipulation tasks (control a robot arm to achieve
    some pre-defined goal, e.g. unlocking a door, or playing soccer). It was specifically
    designed to cover a broad range of tasks, such that meaningful generalization
    can be measured (Yu et al., [2019](#bib.bib94)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eaaece9b5514435edc56d41a28809dc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Learning continuous robotic control tasks is an important application
    of Deep Meta-Learning techniques. Image taken from (Yu et al., [2019](#bib.bib94)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Some Applications of Meta-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep neural networks have achieved remarkable results on various tasks including
    image recognition, text processing, game playing, and robotics (Silver et al.,
    [2016](#bib.bib73); Mnih et al., [2013](#bib.bib54); Wu et al., [2016](#bib.bib90)),
    but their success depends on the amount of available data (Sun et al., [2017](#bib.bib75))
    and computing resources. Deep meta-learning reduces this dependency by allowing
    deep neural networks to learn new concepts quickly. As a result, meta-learning
    widens the applicability of deep learning techniques to many application domains.
    Such areas include few-shot image classification (Finn et al., [2017](#bib.bib14);
    Snell et al., [2017](#bib.bib74); Ravi and Larochelle, [2017](#bib.bib65)), robotic
    control policy learning (Gupta et al., [2018](#bib.bib25); Nagabandi et al., [2019](#bib.bib56))
    (see [Figure 4](#S2.F4 "Figure 4 ‣ 2.2.2 Common Benchmarks ‣ 2.2 The Meta-Setup
    ‣ 2 Foundation ‣ A Survey of Deep Meta-Learning")), hyperparameter optimization
    (Antoniou et al., [2019](#bib.bib3); Schmidhuber et al., [1997](#bib.bib71)),
    meta-learning learning rules (Bengio et al., [1991](#bib.bib6), [1997](#bib.bib5);
    Miconi et al., [2018](#bib.bib50), [2019](#bib.bib51)), abstract reasoning (Barrett
    et al., [2018](#bib.bib4)), and many more. For a larger overview of applications,
    we refer interested readers to Hospedales et al. ([2020](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 The Meta-Learning Field
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in the introduction, meta-learning is a broad area of research,
    as it encapsulates all techniques that leverage prior learning experience to learn
    new tasks more quickly (Vanschoren, [2018](#bib.bib82)). We can classify two distinct
    communities in the field with a different focus: i) algorithm selection and hyperparameter
    optimization for machine learning techniques, and ii) search for inductive bias
    in deep neural networks. We will refer to these communities as group i) and group
    ii) respectively. Now, we will give a brief description of the first field, and
    a historical overview of the second.'
  prefs: []
  type: TYPE_NORMAL
- en: Group i) uses a more traditional approach, to select a suitable machine learning
    algorithm and hyperparameters for a new data set $\mathcal{D}$ (Peng et al., [2002](#bib.bib62)).
    This selection can for example be made by leveraging prior model evaluations on
    various data sets $D^{\prime}$, and by using the model which achieved the best
    performance on the most similar data set (Vanschoren, [2018](#bib.bib82)). Such
    traditional approaches require (large) databases of prior model evaluations, for
    many different algorithms. This has led to initiatives such as OpenML (Vanschoren
    et al., [2014](#bib.bib83)), where researchers can share such information. The
    usage of these systems would limit the freedom in picking the neural network architecture
    as they would be constrained to using architectures that have been evaluated beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, group ii) adopts the view of a self-improving (neural) agent, which
    improves its learning ability over time by finding a good inductive bias (a set
    of assumptions that guide predictions). We now present a brief historical overview
    of developments in this field of Deep Meta-Learning, based on Hospedales et al.
    ([2020](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: Pioneering work was done by Schmidhuber ([1987](#bib.bib69)) and Hinton and
    Plaut ([1987](#bib.bib30)). Schmidhuber developed a theory of self-referential
    learning, where the weights of a neural network can serve as input to the model
    itself, which then predicts updates (Schmidhuber, [1987](#bib.bib69), [1993](#bib.bib70)).
    In that same year, Hinton and Plaut ([1987](#bib.bib30)) proposed to use two weights
    per neural network connection, i.e., slow and fast weights, which serve as long-
    and short-term memory respectively. Later came the idea of meta-learning learning
    rules (Bengio et al., [1991](#bib.bib6), [1997](#bib.bib5)). Meta-learning techniques
    that use gradient-descent and backpropagation were proposed by Hochreiter et al.
    ([2001](#bib.bib31)) and Younger et al. ([2001](#bib.bib93)). These two works
    have been pivotal to the current field of Deep Meta-Learning, as the majority
    of techniques rely on backpropagation, as we will see on our journey of contemporary
    Deep Meta-Learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Overview of the rest of this Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the remainder of this work, we will look in more detail at individual meta-learning
    methods. As indicated before, the techniques can be grouped into three main categories
    (Vinyals, [2017](#bib.bib85)), namely i) metric-, ii) model-, and iii) optimization-based
    methods. We will discuss them in that order.
  prefs: []
  type: TYPE_NORMAL
- en: To help give an overview of the methods, we draw your attention to the following
    tables. [Table 2](#S2.T2 "Table 2 ‣ 2.4 Overview of the rest of this Work ‣ 2
    Foundation ‣ A Survey of Deep Meta-Learning") summarizes the three categories
    and provides key ideas, and strengths of the approaches. The terms and technical
    details are explained more fully in the remainder of this paper. [Table 3](#S2.T3
    "Table 3 ‣ 2.4 Overview of the rest of this Work ‣ 2 Foundation ‣ A Survey of
    Deep Meta-Learning") contains an overview of all techniques that are discussed
    further on.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Metric | Model | Optimization |'
  prefs: []
  type: TYPE_TB
- en: '| Key idea | Input similarity | Internal task representation | Optimize for
    fast adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| Strength | Simple and effective | Flexible | More robust generalizability
    |'
  prefs: []
  type: TYPE_TB
- en: '| $p_{\boldsymbol{\theta}}(Y&#124;\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})$
    | $\sum\limits_{(\boldsymbol{x}_{i},y_{i})\in D^{tr}_{\mathcal{T}_{j}}}k_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{x}_{i})y_{i}$
    | $f_{\boldsymbol{\theta}}(\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})$ | $f_{g_{\boldsymbol{\varphi}(\boldsymbol{\theta},D_{\mathcal{T}_{j}}^{tr},\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}})}}(\boldsymbol{x})$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: High-level overview of the three Deep Meta-Learning categories, i.e.,
    i) metric-, ii) model-, and iii) optimization-based techniques, and their main
    strengths and weaknesses. Recall that $\mathcal{T}_{j}$ is a task, $D^{tr}_{\mathcal{T}_{j}}$
    the corresponding support set, $k_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{x}_{i})$
    a kernel function returning the similarity between the two inputs $\boldsymbol{x}$
    and $\boldsymbol{x}_{i}$, $y_{i}$ are true labels for known inputs $\boldsymbol{x}_{i}$,
    $\theta$ are base-learner parameters, and $g_{\boldsymbol{\varphi}}$ is a (learned)
    optimizer with parameters $\boldsymbol{\varphi}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | RL | Key idea | Bench. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Metric-based |  | Input similarity | - |'
  prefs: []
  type: TYPE_TB
- en: '|   Siamese networks | ✗ | Two-input, shared-weight, class identity network
    | 1, 8 |'
  prefs: []
  type: TYPE_TB
- en: '|   Matching networks | ✗ | Learn input embeddings for cosine-similarity weighted
    predictions | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   Prototypical networks | ✗ | Input embeddings for class prototype clustering
    | 1, 2, 7 |'
  prefs: []
  type: TYPE_TB
- en: '|   Relation networks | ✗ | Learn input embeddings and similarity metric |
    1, 2, 7 |'
  prefs: []
  type: TYPE_TB
- en: '|   ARC | ✗ | LSTM-based input fusion through interleaved glimpses | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   GNN | ✗ | Propagate label information to unlabeled inputs in a graph |
    1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Model-based |  | Internal and stateful latent task representations | - |'
  prefs: []
  type: TYPE_TB
- en: '|   Reccurrent ml. | ✓ | Deploy Recurrent networks on RL problems | - |'
  prefs: []
  type: TYPE_TB
- en: '|   MANNs | ✗ | External short-term memory module for fast learning | 1 |'
  prefs: []
  type: TYPE_TB
- en: '|   Meta networks | ✓ | Fast reparameterization of base-learner by distinct
    meta-learner | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   SNAIL | ✓ | Attention mechanism coupled with temporal convolutions | 1,
    2 |'
  prefs: []
  type: TYPE_TB
- en: '|   CNP | ✗ | Condition predictive model on embedded contextual task data |
    1, 8 |'
  prefs: []
  type: TYPE_TB
- en: '|   Neural stat. | ✗ | Similarity between latent task embeddings | 1, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Opt.-based |  | Optimize for fast task-specific adaptation | - |'
  prefs: []
  type: TYPE_TB
- en: '|   LSTM optimizer | ✗ | RNN proposing weight updates for base-leaner | 6,
    8 |'
  prefs: []
  type: TYPE_TB
- en: '|   LSTM ml. | ✓ | Embed base-learner parameters in cell state of LSTM | 2
    |'
  prefs: []
  type: TYPE_TB
- en: '|   RL optimizer | ✗ | View optimization as RL problem | 4, 6 |'
  prefs: []
  type: TYPE_TB
- en: '|   MAML | ✓ | Learn initialization weights $\boldsymbol{\theta}$ for fast
    adaptation | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   iMAML | ✓ | Approx. higher-order gradients, independent of optimization
    path | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   Meta-SGD | ✓ | Learn both the initialization and updates | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   Reptile | ✓ | Move initialization towards task-specific updated weights
    | 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   LEO | ✗ | Optimize in lower-dimensional latent parameter space | 2, 3 |'
  prefs: []
  type: TYPE_TB
- en: '|   Online MAML | ✗ | Accumulate task data for MAML-like training | 4, 8 |'
  prefs: []
  type: TYPE_TB
- en: '|   LLAMA | ✗ | Maintain probability distribution over post-update parameters
    $\boldsymbol{\theta}^{\prime}_{j}$ | 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   PLATIPUS | ✗ | Learn a probability distribution over weight initializations
    $\boldsymbol{\theta}$ | - |'
  prefs: []
  type: TYPE_TB
- en: '|   BMAML | ✓ | Learn multiple initializations $\boldsymbol{\Theta}$, jointly
    optimized by SVGD | 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   Diff. solvers | ✗ | Learn input embeddings for simple base-learners | 1,
    2, 3, 4, 5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Overview of the discussed Deep Meta-Learning techniques. The table
    is partitioned into three sections, i.e., metric-, model-, and optimization-based
    techniques. All methods in one section adhere to the key idea of its corresponding
    category, which is mentioned in bold font. The columns RL and Bench show whether
    the techniques are applicable to reinforcement learning settings and the used
    benchmarks for testing the performance of the techniques. Note that all techniques
    are applicable to supervised learning, with the exception of RMLs. The benchmark
    column displays which benchmarks from [Section 2.2.2](#S2.SS2.SSS2 "2.2.2 Common
    Benchmarks ‣ 2.2 The Meta-Setup ‣ 2 Foundation ‣ A Survey of Deep Meta-Learning")
    were used in the paper proposing the technique. The used coding scheme for this
    column is the following. 1: Omniglot, 2: miniImageNet, 3: tieredImageNet, 4: CIFAR-100,
    5: CIFAR-FS, 6: CIFAR-10, 7: CUB, 8: MNIST, “-": used other evaluation method
    that are non-standard in Deep Meta-Learning and thus not covered in [Section 2.2.2](#S2.SS2.SSS2
    "2.2.2 Common Benchmarks ‣ 2.2 The Meta-Setup ‣ 2 Foundation ‣ A Survey of Deep
    Meta-Learning"). Used abbreviations: “opt.": optimization, “diff.": differentiable,
    “bench.": benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Metric-based Meta-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, the goal of metric-based techniques is to acquire—among others—meta-knowledge
    $\omega$ in the form of a good feature space that can be used for various new
    tasks. In the context of neural networks, this feature space coincides with the
    weights $\boldsymbol{\theta}$ of the networks. Then, new tasks can be learned
    by comparing new inputs to example inputs (of which we know the labels) in the
    meta-learned feature space. The higher the similarity between a new input and
    an example, the more likely it is that the new input will have the same label
    as the example input.
  prefs: []
  type: TYPE_NORMAL
- en: Metric-based techniques are a form of meta-learning as they leverage their prior
    learning experience (meta-learned feature space) to ‘learn’ new tasks more quickly.
    Here, ‘learn’ is used in a non-standard way since metric-based techniques do not
    make any network changes when presented with new tasks, as they rely solely on
    input comparisons in the already meta-learned feature space. These input comparisons
    are a form of non-parametric learning, i.e., new task information is not absorbed
    into the network parameters.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, metric-based learning techniques aim to learn a similarity kernel,
    or equivalently, attention mechanism $k_{\boldsymbol{\theta}}$ (parameterized
    by $\boldsymbol{\theta}$), that takes two inputs $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$,
    and outputs their similarity score. Larger scores indicate larger similarity.
    Class predictions for new inputs $\boldsymbol{x}$ can then be made by comparing
    $\boldsymbol{x}$ to example inputs $\boldsymbol{x}_{i}$, of which we know the
    true labels $y_{i}$. The underlying idea being that the larger the similarity
    between $\boldsymbol{x}$ and $\boldsymbol{x}_{i}$, the more likely it becomes
    that $\boldsymbol{x}$ also has label $y_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Given a task $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$
    and an unseen input vector $\boldsymbol{x}\in D^{test}_{\mathcal{T}_{j}}$, a probability
    distribution over classes $Y$ is computed/predicted as a weighted combination
    of labels from the support set $D^{tr}_{\mathcal{T}_{j}}$, using similarity kernel
    $k_{\boldsymbol{\theta}}$, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{\boldsymbol{\theta}}(Y&#124;\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})=\sum_{(\boldsymbol{x}_{i},y_{i})\in
    D^{tr}_{\mathcal{T}_{j}}}k_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{x}_{i})y_{i}.$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Importantly, the labels $y_{i}$ are assumed to be one-hot encoded, meaning that
    they are represented by zero vectors with a ‘1’ on the position of the true class.
    For example, suppose there are five classes in total, and our example $\boldsymbol{x}_{1}$
    has true class 4\. Then, the one-hot encoded label is $y_{1}=[0,0,0,1,0]$. Note
    that the probability distribution $p_{\boldsymbol{\theta}}(Y|\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})$
    over classes is a vector of size $|Y|$, in which the $i$-th entry corresponds
    to the probability that input $\boldsymbol{x}$ has class $Y_{i}$ (given the support
    set). The predicted class is thus $\hat{y}=\operatorname*{arg\,max}_{i=1,2,\ldots,|Y|}p_{\boldsymbol{\theta}}(Y|\boldsymbol{x},S)_{i}$,
    where $p_{\boldsymbol{\theta}}(Y|\boldsymbol{x},S)_{i}$ is the computed probability
    that input $\boldsymbol{x}$ has class $Y_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22659e97f0aef48dcab2243557a019e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Illustration of our metric-based example. The blue vector represents
    the new input from the query set, whereas the red vectors are inputs from the
    support set which can be used to guide our prediction for the new input.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that we are given a task $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$.
    Furthermore, suppose that $D^{tr}_{\mathcal{T}_{j}}=\{([0,-4],1),([-2,-4],2),([-2,4],3),([6,0],4)\}$,
    where a tuple denotes a pair $(\boldsymbol{x}_{i},y_{i})$. For simplicity, the
    example will not use an embedding function, which maps example inputs onto an
    (more informative) embedding space. Our query set only contains one example $D^{test}_{\mathcal{T}_{j}}=\{([4,0.5],y)\}$.
    Then, the goal is to predict the correct label for new input $[4,0.5]$ using only
    examples in $D^{tr}_{\mathcal{T}_{j}}$. The problem is visualized in [Figure 5](#S3.F5
    "Figure 5 ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning"), where
    red vectors correspond to example inputs from our support set. The blue vector
    is the new input that needs to be classified. Intuitively, this new input is most
    similar to the vector $[6,0]$, which means that we expect the label for the new
    input to be the same as that for $[6,0]$, i.e., $4$.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we use a fixed similarity kernel, namely the cosine similarity, i.e.,
    $k(\boldsymbol{x},\boldsymbol{x}_{i})=\frac{\boldsymbol{x}\cdot\boldsymbol{x}_{i}^{T}}{||\boldsymbol{x}||\cdot||\boldsymbol{x}_{i}||}$,
    where $||\boldsymbol{v}||$ denotes the length of vector $\boldsymbol{v}$, i.e.,
    $||\boldsymbol{v}||=\sqrt{(\sum_{n}v_{n}^{2})}$. Here, $v_{n}$ denotes the $n$-th
    element of placeholder vector $\boldsymbol{v}$ (substitute $\boldsymbol{v}$ by
    $\boldsymbol{x}$ or $\boldsymbol{x}_{i}$). We can now compute the cosine similarity
    between the new input $[4,0.5]$ and every example input $\boldsymbol{x}_{i}$,
    as done in [Table 4](#S3.T4 "Table 4 ‣ 3.1 Example ‣ 3 Metric-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"), where we used the facts that $||\boldsymbol{x}||=||\,[4,0.5]\,||=\sqrt{4^{2}+0.5^{2}}\approx
    4.03$, and $\frac{\boldsymbol{x}}{||\boldsymbol{x}||}\approx\frac{[4,0.5]}{4.03}=[0.99,0.12]$.
  prefs: []
  type: TYPE_NORMAL
- en: From this table and [Equation 7](#S3.E7 "7 ‣ 3 Metric-based Meta-Learning ‣
    A Survey of Deep Meta-Learning"), it follows that the predicted probability distribution
    $p_{\boldsymbol{\theta}}(Y|\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})=-0.12y_{1}-0.58y_{2}-0.37y_{3}+0.99y_{4}=-0.12[1,0,0,0]-0.58[0,1,0,0]-0.37[0,0,1,0]+0.99[0,0,0,1]=\allowbreak[-0.12,\allowbreak-0.58,\allowbreak-0.37,\allowbreak
    0.99]$. Note that this is not really a probability distribution. That would require
    normalization such that every element is at least $0$ and the sum of all elements
    is $1$. For the sake of this example, we do not perform this normalization, as
    it is clear that class 4 (the class of the most similar example input $[6,0]$)
    will be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '| $\boldsymbol{x}_{i}$ | $y_{i}$ | $&#124;&#124;\boldsymbol{x}_{i}&#124;&#124;$
    | $\frac{\boldsymbol{x}_{i}}{&#124;&#124;\boldsymbol{x}_{i}&#124;&#124;}$ | $\frac{\boldsymbol{x}_{i}}{&#124;&#124;\boldsymbol{x}_{i}&#124;&#124;}\cdot\frac{\boldsymbol{x}}{&#124;&#124;\boldsymbol{x}&#124;&#124;}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $[0,-4]$ | $[1,0,0,0]$ | $4$ | $[0,-1]$ | $-0.12$ |'
  prefs: []
  type: TYPE_TB
- en: '| $[-2,-4]$ | $[0,1,0,0]$ | $4.47$ | $[-0.48,-0.89]$ | $-0.58$ |'
  prefs: []
  type: TYPE_TB
- en: '| $[-2,4]$ | $[0,0,1,0]$ | $4.47$ | $[-0.48,0.89]$ | $-0.37$ |'
  prefs: []
  type: TYPE_TB
- en: '| $[6,0]$ | $[0,0,0,1]$ | $6$ | $[1,0]$ | $0.99$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Example showing pair-wise input comparisons. Numbers were rounded
    to two decimals.'
  prefs: []
  type: TYPE_NORMAL
- en: One may wonder why such techniques are meta-learners, for we could take any
    single data set $\mathcal{D}$ and use pair-wise comparisons to compute predictions.
    At the outer-level, metric-based meta-learners are trained on a distribution of
    different tasks, in order to learn (among others) a good input embedding function.
    This embedding function facilitates inner-level learning, which is achieved through
    pair-wise comparisons. As such, one learns an embedding function across tasks
    to facilitate task-specific learning, which is equivalent to “learning to learn",
    or meta-learning.
  prefs: []
  type: TYPE_NORMAL
- en: After this introduction to metric-based methods, we will now cover some key
    metric-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Siamese Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Siamese neural network (Koch et al., [2015](#bib.bib39)) consists of two neural
    networks $f_{\boldsymbol{\theta}}$ that share the same weights $\boldsymbol{\theta}$.
    Siamese neural networks take two inputs $\boldsymbol{x}_{1},\boldsymbol{x}_{2}$,
    and compute two hidden states $f_{\boldsymbol{\theta}}(\boldsymbol{x}_{1}),f_{\boldsymbol{\theta}}(\boldsymbol{x}_{2})$,
    corresponding to the activation patterns in the final hidden layers. These hidden
    states are fed into a distance layer, which computes a distance vector $\boldsymbol{d}=|f_{\boldsymbol{\theta}}(\boldsymbol{x}_{1})-f_{\boldsymbol{\theta}}(\boldsymbol{x}_{2})|$,
    where $d_{i}$ is the absolute distance between the $i$-th elements of $f_{\boldsymbol{\theta}}(\boldsymbol{x}_{1})$
    and $f_{\boldsymbol{\theta}}(\boldsymbol{x}_{2})$. From this distance vector,
    the similarity between $\boldsymbol{x}_{1},\boldsymbol{x}_{2}$ is computed as
    $\sigma(\boldsymbol{\alpha}^{T}\boldsymbol{d})$, where $\sigma$ is the sigmoid
    function (with output range [0,1]), and $\boldsymbol{\alpha}$ is a vector of free
    weighting parameters, determining the importance of each $d_{i}$. This network
    structure can be seen in [Figure 6](#S3.F6 "Figure 6 ‣ 3.2 Siamese Neural Networks
    ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2c31e6145b713e7cef5dfea9941367d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Example of a Siamese neural network. Source: Koch et al. ([2015](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: Koch et al. ([2015](#bib.bib39)) applied this technique to few-shot image recognition
    in two stages. In the first stage, they train the twin network on an image verification
    task, where the goal is to output whether two input images $\boldsymbol{x}_{1}$
    and $\boldsymbol{x}_{2}$ have the same class. The network is thus stimulated to
    learn discriminative features. In the second stage, where the model is confronted
    with a new task, the network leverages its prior learning experience. That is,
    given a task $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$,
    and previously unseen input $\boldsymbol{x}\in D^{test}_{\mathcal{T}_{j}}$, the
    predicted class $\hat{y}$ is equal to the label $y_{i}$ of the example $(\boldsymbol{x}_{i},y_{i})\in
    D^{tr}_{\mathcal{T}_{j}}$ which yields the highest similarity score to $\boldsymbol{x}$.
    In contrast to other techniques mentioned further in this section, Siamese neural
    networks do not directly optimize for good performance across tasks (consisting
    of support and query sets). However, they do leverage learned knowledge from the
    verification task to learn new tasks quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Siamese neural networks are a simple and elegant approach to perform
    few-shot learning. However, they are not readily applicable outside the supervised
    learning setting.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Matching Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Matching networks (Vinyals et al., [2016](#bib.bib86)) build upon the idea
    that underlies Siamese neural networks (Koch et al., [2015](#bib.bib39)). That
    is, they leverage pair-wise comparisons between the given support set $D^{tr}_{\mathcal{T}_{j}}=\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{m}$
    (for a task $\mathcal{T}_{j}$), and new inputs $\boldsymbol{x}\in D^{test}_{\mathcal{T}_{j}}$
    from the query set which we want to classify. However, instead of assigning the
    class $y_{i}$ of the most similar example input $\boldsymbol{x}_{i}$, matching
    networks use a weighted combination of all example labels $y_{i}$ in the support
    set, based on the similarity of inputs $\boldsymbol{x}_{i}$ to new input $\boldsymbol{x}$.
    More specifically, predictions are computed as follows: $\hat{y}=\sum_{i=1}^{m}a(\boldsymbol{x},\boldsymbol{x}_{i})y_{i}$,
    where $a$ is a non-parametric (non-trainable) attention mechanism, or similarity
    kernel. This classification process is shown in [Figure 7](#S3.F7 "Figure 7 ‣
    3.3 Matching Networks ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    In this figure, the input to $f_{\boldsymbol{\theta}}$ has to be classified, using
    the support set $D^{tr}_{\mathcal{T}_{j}}$ (input to $g_{\boldsymbol{\theta}}$).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df9584b383ad21a47c607276883631e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Architecture of matching networks. Source: Vinyals et al. ([2016](#bib.bib86)).'
  prefs: []
  type: TYPE_NORMAL
- en: The attention that is used consists of a softmax over the cosine similarity
    $c$ between the input representations, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a(\boldsymbol{x},\boldsymbol{x}_{i})=\frac{e^{c(f_{\boldsymbol{\phi}}(\boldsymbol{x}),g_{\boldsymbol{\varphi}}(\boldsymbol{x}_{i}))}}{\sum_{j=1}^{m}e^{c(f_{\boldsymbol{\phi}}(\boldsymbol{x}),g_{\boldsymbol{\varphi}}(\boldsymbol{x}_{j}))}},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{\boldsymbol{\phi}}$ and $g_{\boldsymbol{\varphi}}$ are neural networks,
    parameterized by $\boldsymbol{\phi}$ and $\boldsymbol{\varphi}$, that map raw
    inputs to a (lower-dimensional) latent vector, which corresponds to the output
    of the final hidden layer of a neural network. As such, the neural networks act
    as embedding functions. The larger the cosine similarity between the embeddings
    of $\boldsymbol{x}$ and $\boldsymbol{x}_{i}$, the larger $a(\boldsymbol{x},\boldsymbol{x}_{i})$,
    and thus the influence of label $y_{i}$ on the predicted label $\hat{y}$ for input
    $\boldsymbol{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: Vinyals et al. ([2016](#bib.bib86)) propose two main choices for the embedding
    functions. The first is to use a single neural network, granting us $\boldsymbol{\theta}=\boldsymbol{\phi}=\boldsymbol{\varphi}$
    and thus $f_{\boldsymbol{\phi}}=g_{\boldsymbol{\varphi}}$. This setup is the default
    form of matching networks, as shown in [Figure 7](#S3.F7 "Figure 7 ‣ 3.3 Matching
    Networks ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning"). The
    second choice is to make $f_{\boldsymbol{\phi}}$ and $g_{\boldsymbol{\varphi}}$
    dependent on the support set $D^{tr}_{\mathcal{T}_{j}}$ using Long Short-Term
    Memory networks (LSTMs). In that case, $f_{\boldsymbol{\phi}}$ is represented
    by an attention LSTM, and $g_{\boldsymbol{\varphi}}$ by a bidirectional one. This
    choice for embedding functions is called Full Context Embeddings (FCE), and yielded
    an accuracy improvement of roughly 2% on miniImageNet compared to the regular
    matching networks, indicating that task-specific embeddings can aid the classification
    of new data points from the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Matching networks learn a good feature space across tasks for making pair-wise
    comparisons between inputs. In contrast to Siamese neural networks (Koch et al.,
    [2015](#bib.bib39)), this feature space (given by weights $\boldsymbol{\theta}$)
    is learned across tasks, instead of on a distinct verification task.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, matching networks are an elegant and simple approach to metric-based
    meta-learning. However, these networks are not readily applicable outside of supervised
    learning settings and suffer from performance degradation when label distributions
    are biased (Vinyals et al., [2016](#bib.bib86)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Prototypical Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like matching networks (Vinyals et al., [2016](#bib.bib86)), prototypical
    networks (Snell et al., [2017](#bib.bib74)) base their class predictions on the
    entire support set $D^{tr}_{\mathcal{T}_{j}}$. However, instead of computing the
    similarity between new inputs and examples in the support set, prototypical networks
    only compare new inputs to class prototypes (centroids), which are single vector
    representations of classes in some embedding space. Since there are fewer (or
    equal) class prototypes than the number of examples in the support set, the amount
    of required pair-wise comparisons decreases, saving computational costs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6881ca7cac3123681382e7f415cc8c45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Prototypical networks for the case of few-shot learning. The $\boldsymbol{c}_{k}$
    are class prototypes for class $k$ which are computed by averaging the representations
    of inputs (colored circles) in the support set. Note that the representation space
    is partitioned into three disjoint areas, where each area corresponds to one class.
    The class with the closest prototype to the new input $\boldsymbol{x}$ in the
    query set is then given as prediction. Source: Snell et al. ([2017](#bib.bib74)).'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying idea of class prototypes is that for a task $\mathcal{T}_{j}$,
    there exists an embedding function that maps the support set onto a space where
    class instances cluster nicely around the corresponding class prototypes (Snell
    et al., [2017](#bib.bib74)). Then, for a new input $\boldsymbol{x}$, the class
    of the prototype nearest to that input will be predicted. As such, prototypical
    networks perform nearest centroid/prototype classification in a meta-learned embedding
    space. This is visualized in [Figure 8](#S3.F8 "Figure 8 ‣ 3.4 Prototypical Networks
    ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
  prefs: []
  type: TYPE_NORMAL
- en: More formally, given a distance function $d:X\times X\rightarrow[0,+\infty)$
    (e.g. Euclidean distance) and embedding function $f_{\boldsymbol{\theta}}$, parameterized
    by $\boldsymbol{\theta}$, prototypical networks compute class probabilities $p_{\boldsymbol{\theta}}(Y|\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})$
    as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{\boldsymbol{\theta}}(y=k&#124;\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})=\frac{exp[-d(f_{\theta}(\boldsymbol{x}),\boldsymbol{c}_{k})]}{\sum_{y_{i}}exp[-d(f_{\theta}(\boldsymbol{x}),\boldsymbol{c}_{y_{i}})]},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{c}_{k}$ is the prototype/centroid for class $k$ and $y_{i}$
    are the classes in the support set $D^{tr}_{\mathcal{T}_{j}}$. Here, a class prototype
    for class $k$ is defined as the average of all vectors $\boldsymbol{x}_{i}$ in
    the support set such that $y_{i}=k$. Thus, classes with prototypes that are nearer
    to the new input $\boldsymbol{x}$ obtain larger probability scores.
  prefs: []
  type: TYPE_NORMAL
- en: Snell et al. ([2017](#bib.bib74)) found that the squared Euclidean distance
    function as $d$ gave rise to the best performance. With that distance function,
    prototypical networks can be seen as linear models. To see this, note that $-d(f_{\theta}(\boldsymbol{x}),\boldsymbol{c}_{k})=-||f_{\theta}(\boldsymbol{x})-\boldsymbol{c}_{k}||^{2}=-f_{\theta}(\boldsymbol{x})^{T}f_{\theta}(\boldsymbol{x})+2\boldsymbol{c}_{k}^{T}f_{\theta}(\boldsymbol{x})-\boldsymbol{c}_{k}^{T}\boldsymbol{c}_{k}$.
    The first term does not depend on the class $k$, and does thus not affect the
    classification decision. The remainder can be written as $\boldsymbol{w}_{k}^{T}f_{\theta}(\boldsymbol{x})+\boldsymbol{b}_{k}$,
    where $\boldsymbol{w}_{k}=2\boldsymbol{c}_{k}$ and $\boldsymbol{b}_{k}=-\boldsymbol{c}_{k}^{T}\boldsymbol{c}_{k}$.
    Note that this is linear in the output of network $f_{\theta}$, not linear in
    the input of the network $\boldsymbol{x}$. Also, Snell et al. ([2017](#bib.bib74))
    show that prototypical networks (coupled with Euclidean distance) are equivalent
    to matching networks in one-shot learning settings, as every example in the support
    set will be its prototype.
  prefs: []
  type: TYPE_NORMAL
- en: In short, prototypical networks save computational costs by reducing the required
    number of pair-wise comparisons between new inputs and the support set, by adopting
    the concept of class prototypes. Additionally, prototypical networks were found
    to outperform matching networks (Vinyals et al., [2016](#bib.bib86)) in 5-way,
    $k$-shot learning for $k=1,5$ on Omniglot (Lake et al., [2011](#bib.bib42)) and
    miniImageNet (Vinyals et al., [2016](#bib.bib86)), even though they do not use
    complex task-specific embedding functions. Despite these advantages, prototypical
    networks are not readily applicable outside of supervised learning settings.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Relation Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f3ce9ffbfb29735d26ed8d594647627.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Relation network architecture. First, the embedding network $f_{\boldsymbol{\varphi}}$
    embeds all inputs from the support set $D^{tr}_{\mathcal{T}_{j}}$ (the five example
    inputs on the left), and the query input (below the $f_{\boldsymbol{\varphi}}$
    block). All support set embeddings $f_{\boldsymbol{\varphi}}(\boldsymbol{x}_{i})$
    are then concatenated to the query embedding $f_{\boldsymbol{\varphi}}(\boldsymbol{x})$.
    These concatenated embeddings are passed into a relation network $g_{\boldsymbol{\phi}}$,
    which computes a relation score for every pair $(\boldsymbol{x}_{i},\boldsymbol{x})$.
    The class of the input $\boldsymbol{x}_{i}$ that yields the largest relation score
    $g_{\boldsymbol{\phi}}([f_{\boldsymbol{\varphi}}(\boldsymbol{x}),f_{\boldsymbol{\varphi}}(\boldsymbol{x}_{i})])$
    is then predicted. Source: Sung et al. ([2018](#bib.bib76)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to previously discussed metric-based techniques, Relation networks
    (Sung et al., [2018](#bib.bib76)) employ a trainable similarity metric, instead
    of a pre-defined one (e.g. cosine similarity as used in matching networks (Vinyals
    et al., [2016](#bib.bib86))). More specifically, matching networks consist of
    two chained, neural network modules: the embedding network/module $f_{\boldsymbol{\varphi}}$
    which is responsible for embedding inputs, and the relation network $g_{\boldsymbol{\phi}}$
    which computes similarity scores between new inputs $\boldsymbol{x}$ and example
    inputs $\boldsymbol{x}_{i}$ of which we know the labels. A classification decision
    is then made by picking the class of the example input which yields the largest
    relation score (or similarity). Note that Relation networks thus do not use the
    idea of class prototypes, and simply compare new inputs $\boldsymbol{x}$ to all
    example inputs $\boldsymbol{x}_{i}$ in the support set, as done by, e.g., matching
    networks (Vinyals et al., [2016](#bib.bib86)).'
  prefs: []
  type: TYPE_NORMAL
- en: More formally, we are given a support set $D^{tr}_{\mathcal{T}_{j}}$ with some
    examples $(\boldsymbol{x}_{i},y_{i})$, and a new (previously unseen) input $\boldsymbol{x}$.
    Then, for every combination $(\boldsymbol{x},\boldsymbol{x}_{i})$, the Relation
    network produces a concatenated embedding $[f_{\boldsymbol{\varphi}}(\boldsymbol{x}),f_{\boldsymbol{\varphi}}(\boldsymbol{x}_{i})]$,
    which is vector obtained by concatenating the respective embeddings of $\boldsymbol{x}$
    and $\boldsymbol{x}_{i}$. This concatenated embedding is then fed into the relation
    module $g_{\boldsymbol{\phi}}$. Finally, $g_{\boldsymbol{\phi}}$ computes the
    relation score between $\boldsymbol{x}$ and $\boldsymbol{x}_{i}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r_{i}=g_{\boldsymbol{\phi}}([f_{\boldsymbol{\varphi}}(\boldsymbol{x}),f_{\boldsymbol{\varphi}}(\boldsymbol{x}_{i})]).$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: The predicted class is then $\hat{y}=y_{\operatorname*{arg\,max}_{i}r_{i}}$.
    This entire process is shown in [Figure 9](#S3.F9 "Figure 9 ‣ 3.5 Relation Networks
    ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning"). Remarkably
    enough, Relation networks use the Mean-Squared Error (MSE) of the relation scores,
    rather than the more standard cross-entropy loss. The MSE is then propagated backwards
    through the entire architecture ([Figure 9](#S3.F9 "Figure 9 ‣ 3.5 Relation Networks
    ‣ 3 Metric-based Meta-Learning ‣ A Survey of Deep Meta-Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of Relation networks is their expressive power, induced by
    the usage of a trainable similarity function. This expressivity makes this technique
    very powerful. As a result, it yields better performance than previously discussed
    techniques that use a fixed similarity metric.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Graph Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graph neural networks (Garcia and Bruna, [2017](#bib.bib17)) use a more general
    and flexible approach than previously discussed techniques for $N$-way, $k$-shot
    classification. As such, graph neural networks subsume Siamese (Koch et al., [2015](#bib.bib39))
    and prototypical networks (Snell et al., [2017](#bib.bib74)). The graph neural
    network approach represents each task $\mathcal{T}_{j}$ as a fully-connected graph
    $G=(V,E)$, where $V$ is a set of nodes/vertices and $E$ a set of edges connecting
    nodes. In this graph, nodes $\boldsymbol{v}_{i}$ correspond to input embeddings
    $f_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})$, concatenated with their one-hot
    encoded labels $y_{i}$, i.e., $\boldsymbol{v}_{i}=[f_{\boldsymbol{\theta}}(\boldsymbol{x}_{i}),y_{i}]$.
    For inputs $\boldsymbol{x}$ from the query set (for which we do not have the labels),
    a uniform prior over all $N$ possible labels is used: $y=[\frac{1}{N},\ldots,\frac{1}{N}]$.
    Thus, each node contains an input and label section. Edges are weighted links
    that connect these nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: The graph neural network then propagates information in the graph using a number
    of local operators. The underlying idea is that label information can be transmitted
    from nodes of which we do have the labels, to nodes for which we have to predict
    labels. Which local operators are used, is out of scope for this paper, and the
    reader is referred to Garcia and Bruna ([2017](#bib.bib17)) for details.
  prefs: []
  type: TYPE_NORMAL
- en: By exposing the graph neural network to various tasks $\mathcal{T}_{j}$, the
    propagation mechanism can be altered to improve the flow of label information
    in such a way that predictions become more accurate. As such, in addition to learning
    a good input representation function $f_{\boldsymbol{\theta}}$, graph neural networks
    also learn to propagate label information from labeled examples to unlabeled inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph neural networks achieve good performance in few-shot settings (Garcia
    and Bruna, [2017](#bib.bib17)) and are also applicable in semi-supervised and
    active learning settings.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Attentive Recurrent Comparators (ARCs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7d7db0a5f26fccb6bdd349ffc0eb745.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Processing in an attentive recurrent comparator. At every time step,
    the model takes a glimpse of a part of an image and incorporates this information
    into the hidden state $h_{t}$. The final hidden state after taking various glimpses
    of a pair of images is then used to compute a class similarity score. Source:
    Shyam et al. ([2017](#bib.bib72)).'
  prefs: []
  type: TYPE_NORMAL
- en: Attentive recurrent comparators (ARCs) (Shyam et al., [2017](#bib.bib72)) differ
    from previously discussed techniques as they do not compare inputs as a whole,
    but by parts. This approach is inspired by how humans would make a decision concerning
    the similarity of objects. That is, we shift our attention from one object to
    the other, and move back and forth to take glimpses of different parts of both
    objects. In this way, information of two objects is fused from the beginning,
    whereas other techniques (e.g., matching networks (Vinyals et al., [2016](#bib.bib86))
    and graph neural networks (Garcia and Bruna, [2017](#bib.bib17))) only combine
    information at the end (after embedding both images) (Shyam et al., [2017](#bib.bib72)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two inputs $\boldsymbol{x}_{i}$ and $\boldsymbol{x}$, we feed them in
    interleaved fashion repeatedly into a recurrent neural network (controller): $\boldsymbol{x}_{i},\boldsymbol{x},\ldots,\boldsymbol{x}_{i},\boldsymbol{x}$.
    Thus, the image at time step $t$ is given by $I_{t}=\boldsymbol{x}_{i}$ if $t$
    is even else $\boldsymbol{x}$. Then, at each time step $t$, the attention mechanism
    focuses on a square region of the current image: $G_{t}=attend(I_{t},\Omega_{t})$,
    where $\Omega_{t}=W_{g}h_{t-1}$ are attention parameters, which are computed from
    the previous hidden state $h_{t-1}$. The next hidden state $h_{t+1}=\mbox{RNN}(G_{t},h_{t-1})$
    is given by the glimpse at time t, i.e., $G_{t}$, and the previous hidden state
    $h_{t-1}$. The entire sequence consists of $g$ glimpses per image. After this
    sequence is fed into the recurrent neural network (indicated by RNN($\circ$)),
    the final hidden state $h_{2g}$ is used as combined representation of $\boldsymbol{x}_{i}$
    relative to $\boldsymbol{x}$. This process is summarized in [Figure 10](#S3.F10
    "Figure 10 ‣ 3.7 Attentive Recurrent Comparators (ARCs) ‣ 3 Metric-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"). Classification decisions can then be made
    by feeding the combined representations into a classifier. Optionally, the combined
    representations can be processed by bi-directional LSTMs before passing them to
    the classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: The attention approach is biologically inspired, and biologically plausible.
    A downside of attentive recurrent comparators is the higher computational cost,
    while the performance is often not better than less biologically plausible techniques,
    such as graph neural networks (Garcia and Bruna, [2017](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Metric-based Techniques, in conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we have seen various metric-based techniques. The metric-based
    techniques meta-learn an informative feature space that can be used to compute
    class predictions based on input similarity scores. [Figure 11](#S3.F11 "Figure
    11 ‣ 3.8 Metric-based Techniques, in conclusion ‣ 3 Metric-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning") shows the relationships between the various
    metric-based techniques that we have covered.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, Siamese networks (Koch et al., [2015](#bib.bib39)) mark the beginning
    of metric-based, deep meta-learning techniques in few-shot learning settings.
    They are the first to use the idea of predicting classes by comparing inputs from
    the support and query sets. This idea was generalized in graph neural networks
    (GNNs) (Hamilton et al., [2017](#bib.bib26); Garcia and Bruna, [2017](#bib.bib17))
    where the information flow between support and query inputs is parametric and
    thus more flexible. Matching networks (Vinyals et al., [2016](#bib.bib86)) are
    directly inspired by Siamese networks as they use the same core idea (comparing
    inputs for making predictions), but directly train in the few-shot setting and
    use cosine similarity as a similarity function. Thus, the auxiliary, binary classification
    task used by Siamese networks is left out, and matching networks directly train
    on tasks. Prototypical networks (Snell et al., [2017](#bib.bib74)) increase the
    robustness of input comparisons by comparing every query set input with a class
    prototype instead of individual support set examples. This reduces the number
    of required input comparisons for a single query input to $N$ instead of $k\cdot
    N$. Relation networks (Sung et al., [2018](#bib.bib76)) replace the fixed, pre-defined
    similarity metrics used in matching and prototypical networks by a neural network,
    which allows for learning a domain-specific similarity function. Lastly, attentive
    recurrent comparators (Shyam et al., [2017](#bib.bib72)) take a more biologically
    plausible approach by not comparing entire inputs but by taking multiple interleaved
    glimpses at various parts of the inputs that are being compared.
  prefs: []
  type: TYPE_NORMAL
- en: Key advantages of these metric-based techniques are that i) the underlying idea
    of similarity-based predictions is conceptually simple, and ii) they can be fast
    at test-time when tasks are small, as the networks do not need to make task-specific
    adjustments. However, when tasks at meta-test time become more distant from the
    tasks that were used at meta-train time, metric-learning techniques are unable
    to absorb new task information into the network weights. Consequently, performance
    may degrade.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, when tasks become larger, pair-wise comparisons may become prohibitively
    expensive. Lastly, most metric-based techniques rely on the presence of labeled
    examples, which make them inapplicable outside of supervised learning settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80ef5c438ab941b1cbbf2e91d063fe01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The relationships between the covered metric-based meta-learning
    techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Model-based Meta-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A different approach to Deep Meta-Learning is the model-based approach. On a
    high level, model-based techniques rely upon an adaptive, internal state, in contrast
    to metric-based techniques, which generally use a fixed neural network at test-time.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, model-based techniques maintain a stateful, internal representation
    of a task. When presented with a task, a model-based neural network processes
    the support set in a sequential fashion. At every time step, an input enters and
    alters the internal state of the model. Thus, the internal state can capture relevant
    task-specific information, which can be used to make predictions for new inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Because the predictions are based on internal dynamics that are hidden from
    the outside, model-based techniques are also called black-boxes. Information from
    previous inputs must be remembered, which is why model-based techniques have a
    memory component, either in- or externally.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the mechanics of metric-based techniques were limited to pair-wise
    input comparisons. This is not the case for model-based techniques, where the
    human designer has the freedom to choose the internal dynamics of the algorithm.
    As a result, model-based techniques are not restricted to meta-learning good feature
    spaces, as they can also learn internal dynamics, used to process and predict
    input data of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, given a support set $D^{tr}_{\mathcal{T}_{j}}$ corresponding
    to task $\mathcal{T}_{j}$, model-based techniques compute a class probability
    distribution for a new input $\boldsymbol{x}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{\boldsymbol{\theta}}(Y&#124;\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})=f_{\boldsymbol{\theta}}(\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}}),$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $f$ represents the black-box neural network model, and $\boldsymbol{\theta}$
    its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the same example as in Section [3](#S3 "3 Metric-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"), suppose we are given a task support set $D^{tr}_{\mathcal{T}_{j}}=\{([0,-4],1),([-2,-4],2),([-2,4],3),([6,0],4)\}$,
    where a tuple denotes a pair $(\boldsymbol{x}_{i},y_{i})$. Furthermore, suppose
    our query set only contains one example $D^{test}_{\mathcal{T}_{j}}=\{([4,0.5],4)\}$.
    This problem has been visualized in [Figure 5](#S3.F5 "Figure 5 ‣ 3 Metric-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning") (in Section [3](#S3 "3 Metric-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning")). For the sake of the example,
    we do not use an input embedding function: our model will operate on the raw inputs
    of $D^{tr}_{\mathcal{T}_{j}}$ and $D^{test}_{\mathcal{T}_{j}}$. As an internal
    state, our model uses an external memory matrix $M\in\mathbb{R}^{4\times(2+1)}$,
    with four rows (one for each example in our support set), and three columns (the
    dimensionality of input vectors, plus one dimension for the correct label). Our
    model proceeds to process the support set sequentially, reading the examples from
    $D^{tr}_{\mathcal{T}_{j}}$ one by one, and by storing the $i$-th example in the
    $i$-th row of the memory module. After processing the support set, the memory
    matrix contains all examples, and as such, serves as internal task representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the new input $[4,0.5]$, our model could use many different techniques
    to make a prediction based on this representation. For simplicity, assume that
    it computes the dot product between $\boldsymbol{x}$, and every memory $M(i)$
    (the 2-D vector in the $i$-th row of $M$, ignoring the correct label), and predicts
    the class of the input which yields the largest dot product. This would produce
    scores $-2,-10,-6,$ and $24$ for the examples in $D^{tr}_{\mathcal{T}_{j}}$ respectively.
    Since the last example $[6,0]$ yields the largest dot product, we predict that
    class, i.e., $4$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this example could be seen as a metric-based technique where the dot
    product is used as a similarity function. However, the reason that this technique
    is model-based is that it stores the entire task inside a memory module. This
    example was deliberately easy for illustrative purposes. More advanced and successful
    techniques have been proposed, which we will now cover.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Recurrent Meta-Learners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent meta-learners (Duan et al., [2016](#bib.bib11); Wang et al., [2016](#bib.bib89))
    are, as the name suggests, meta-learners based on recurrent neural networks. The
    recurrent network serves as dynamic task embedding storage. These recurrent meta-learners
    were specifically proposed for reinforcement learning problems, hence we will
    explain them in that setting.
  prefs: []
  type: TYPE_NORMAL
- en: The recurrence is implemented by e.g. an LSTM (Wang et al., [2016](#bib.bib89))
    or a GRU (Duan et al., [2016](#bib.bib11)). The internal dynamics of the chosen
    Recurrent Neural Network (RNN) allows for fast adaptation to new tasks, while
    the algorithm used to train the recurrent net gradually accumulates knowledge
    about the task structure, where each task is modelled as an episode (or set of
    episodes).
  prefs: []
  type: TYPE_NORMAL
- en: The idea of recurrent meta-learners is quite simple. That is, given a task $\mathcal{T}_{j}$,
    we simply feed the (potentially processed) environment variables $[s_{t+1},a_{t},r_{t},d_{t}]$
    (see [Section 2.1.3](#S2.SS1.SSS3 "2.1.3 Regular Reinforcement Learning ‣ 2.1
    The Meta Abstraction ‣ 2 Foundation ‣ A Survey of Deep Meta-Learning")) into an
    RNN at every time step $t$. Recall that $s,a,r,d$ denote the state, action, reward,
    and termination flag respectively. At every time step $t$, the RNN outputs an
    action and a hidden state. Conditioned on its hidden state $h_{t}$, the network
    outputs an action $a_{t}$. The goal is to maximize the expected reward in each
    trial. See [Figure 12](#S4.F12 "Figure 12 ‣ 4.2 Recurrent Meta-Learners ‣ 4 Model-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning") for a visual depiction. From
    this figure, it also becomes clear why these techniques are model-based. That
    is, they embed information from previously seen inputs in the hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4a3b50d9e177eb99c26cdc65270bd7db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Workflow of recurrent meta-learners in reinforcement learning contexts.
    As mentioned in [Section 2.1.3](#S2.SS1.SSS3 "2.1.3 Regular Reinforcement Learning
    ‣ 2.1 The Meta Abstraction ‣ 2 Foundation ‣ A Survey of Deep Meta-Learning"),
    $s_{t},r_{t},$ and $d_{t}$ denote the state, reward, and termination flag at time
    step $t$. $h_{t}$ refers to the hidden state at time $t$. Source: Duan et al.
    ([2016](#bib.bib11)).'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent meta-learners have shown to perform almost as well as asymptotically
    optimal algorithms on simple reinforcement learning tasks (Wang et al., [2016](#bib.bib89);
    Duan et al., [2016](#bib.bib11)). However, their performance degrades in more
    complex settings, where temporal dependencies can span a longer horizon. Making
    recurrent meta-learners better at such complex tasks is a direction for future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Memory-Augmented Neural Networks (MANNs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key idea of memory-augmented neural networks (MANNs) (Santoro et al., [2016](#bib.bib68))
    is to enable neural networks to learn quickly with the help of an external memory.
    The main controller (the recurrent neural network interacting with the memory)
    then gradually accumulates knowledge across tasks, while the external memory allows
    for quick task-specific adaptation. For this, Santoro et al. ([2016](#bib.bib68))
    used Neural Turing Machines (Graves et al., [2014](#bib.bib24)). Here, the controller
    is parameterized by $\boldsymbol{\theta}$ and acts as the long-term memory of
    the memory-augmented neural network, while the external memory module is the short-term
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: The workflow of memory-augmented neural networks is displayed in [Figure 13](#S4.F13
    "Figure 13 ‣ 4.3 Memory-Augmented Neural Networks (MANNs) ‣ 4 Model-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"). Note that the data from a task is processed
    as a sequence, i.e., data are fed into the network one by one. The support set
    is fed into the memory-augmented neural network first. Afterwards, the query set
    is processed. During the meta-train phase, training tasks can be fed into the
    network in arbitrary order. At time step $t$, the model receives input $\boldsymbol{x}_{t}$
    with the label of the previous input, i.e., $y_{t-1}$. This was done to prevent
    the network from mapping class labels directly to the output (Santoro et al.,
    [2016](#bib.bib68)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b4af1929e04789ef50728d500885d16.png)'
  prefs: []
  type: TYPE_IMG
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: Workflow of memory-augmented neural networks. Here, an episode corresponds
    to a given task $\mathcal{T}_{j}$. After every episode, the order of labels, classes,
    and samples should be shuffled to minimize dependence on arbitrarily assigned
    orders. Source: Santoro et al. ([2016](#bib.bib68)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5a1bd2a29edfedc346bfc5a2976895a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Controller-memory interaction in memory-augmented neural networks.
    Source: Santoro et al. ([2016](#bib.bib68)).'
  prefs: []
  type: TYPE_NORMAL
- en: The interaction between the controller and memory is visualized in [Figure 14](#S4.F14
    "Figure 14 ‣ 4.3 Memory-Augmented Neural Networks (MANNs) ‣ 4 Model-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"). The idea is that the external memory module,
    containing representations of previously seen inputs, can be used to make predictions
    for new inputs. In short, previously obtained knowledge is leveraged to aid the
    classification of new inputs. Note that neural networks also attempt to do this,
    however, their prior knowledge is slowly accumulated into the network weights,
    while an external memory module can directly store such information.
  prefs: []
  type: TYPE_NORMAL
- en: Given an input $\boldsymbol{x}_{t}$ at time $t$, the controller generates a
    key $\boldsymbol{k}_{t}$, which can be stored in memory matrix $M$ and can be
    used to retrieve previous representations from memory matrix $M$. When reading
    from memory, the aim is to produce a linear combination of stored keys in memory
    matrix $M$, giving greater weight to those which have a larger cosine similarity
    with the current key $\boldsymbol{k}_{t}$. More specifically, a read vector $\boldsymbol{w}^{r}_{t}$
    is created, in which each entry $i$ denotes the cosine similarity between key
    $\boldsymbol{k}_{t}$ and the memory (from a previous input) stored in row $i$,
    i.e., $M_{t}(i)$. Then, the representation $\boldsymbol{r}_{t}=\sum_{i}w_{t}^{r}(i)M(i)$
    is retrieved, which is simply a linear combination of all keys (i.e., rows) in
    memory matrix $M$.
  prefs: []
  type: TYPE_NORMAL
- en: Predictions are made as follows. Given an input $\boldsymbol{x}_{t}$, memory-augmented
    neural networks use the external memory to compute the corresponding representation
    $\boldsymbol{r}_{t}$, which could be fed into a softmax layer, resulting in class
    probabilities. Across tasks, memory-augmented neural networks learn a good input
    embedding function $f_{\boldsymbol{\theta}}$ and classifier weights, which can
    be exploited when presented with new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To write input representations to memory, Santoro et al. ([2016](#bib.bib68))
    propose a new mechanism called Least Recently Used Access (LRUA). LRUA either
    writes to the least, or most recently used memory location. In the former case,
    it preserves recent memories, and in the latter it updates recently obtained information.
    The writing mechanism works by keeping track of how often every memory location
    is accessed in a usage vector $\boldsymbol{w}_{t}^{u}$, which is updated at every
    time step according to the following update rule: $\boldsymbol{w}_{t}^{u}:=\gamma\boldsymbol{w}^{u}_{t-1}+\boldsymbol{w}_{t}^{r}+\boldsymbol{w}_{t}^{w}$,
    where superscripts $u,w$ and $r$ refer to usage, write and read vectors, respectively.
    In words, the previous usage vector is decayed (using parameter $\gamma$), while
    current reads ($\boldsymbol{w}_{t}^{r}$) and writes ($\boldsymbol{w}_{t}^{w}$)
    are added to the usage. Let $n$ be the total number of reads to memory, and $\ell
    u(n)$ ($\ell u$ for ‘least used’) be the $n$-th smallest value in the usage vector
    $\boldsymbol{w}^{u}_{t}$. Then, the least-used weights are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{w}^{\ell u}_{t}(i)=\begin{cases}0&amp;\text{if $w^{u}_{t}(i)>\ell
    u(n)$}\\ 1&amp;else\end{cases}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Then, the write vector $\boldsymbol{w}_{t}^{w}$ is computed as $\boldsymbol{w}^{w}_{t}=\sigma(\alpha)\boldsymbol{w}^{r}_{t-1}+(1-\sigma(\alpha))\boldsymbol{w}^{\ell
    u}_{t-1}$, where $\alpha$ is a parameter that interpolates between the two weight
    vectors. As such, if $\sigma(\alpha)=1$, we write to the most recently used memory,
    whereas when $\sigma(\alpha)=0$, we write to the least recently used memory locations.
    Finally, writing is performed as follows: $M_{t}(i):=M_{t-1}(i)+w_{t}^{w}(i)\boldsymbol{k}_{t}$,
    for all $i$.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, memory-augmented neural networks (Santoro et al., [2016](#bib.bib68))
    combine external memory and a neural network to achieve meta-learning. The interaction
    between a controller, with long-term memory parameters $\boldsymbol{\theta}$,
    and memory $M$, may also be interesting for studying human meta-learning (Santoro
    et al., [2016](#bib.bib68)). In contrast to many metric-based techniques, this
    model-based technique is applicable to both classification and regression problems.
    A downside of this approach is the architectural complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Meta Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84a56d9bf1098190499723add9a8bb61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Architecture of a Meta Network. Source: Munkhdalai and Yu ([2017](#bib.bib55)).'
  prefs: []
  type: TYPE_NORMAL
- en: Meta networks are divided into two distinct subsystems (consisting of neural
    networks), i.e., the base- and meta-learner (whereas in memory-augmented neural
    networks the base- and meta-components are intertwined). The base-learner is responsible
    for performing tasks, and for providing the meta-learner with meta-information,
    such as loss gradients. The meta-learner can then compute fast task-specific weights
    for itself and the base-learner, such that it can perform better on the given
    task $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$.
    This workflow is depicted in [Figure 15](#S4.F15 "Figure 15 ‣ 4.4 Meta Networks
    ‣ 4 Model-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
  prefs: []
  type: TYPE_NORMAL
- en: The meta-learner consists of neural networks $u_{\boldsymbol{\phi}},m_{\boldsymbol{\varphi}}$,
    and $d_{\boldsymbol{\psi}}$. Network $u_{\boldsymbol{\phi}}$ is used as input
    representation function. Networks $d_{\boldsymbol{\psi}}$ and $m_{\boldsymbol{\varphi}}$
    are used to compute task-specific weights $\boldsymbol{\phi}^{*}$ and example-level
    fast weights $\boldsymbol{\theta}^{*}$. Lastly, $b_{\boldsymbol{\theta}}$ is the
    base-learner which performs input predictions. Note that we used the term fast-weights
    throughout, which refers to task- or input-specific versions of slow (initial)
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: In similar fashion to memory-augmented neural networks (Santoro et al., [2016](#bib.bib68)),
    meta networks (Munkhdalai and Yu, [2017](#bib.bib55)) also leverage the idea of
    an external memory module. However, meta networks use the memory for a different
    purpose. The memory stores for each observation $\boldsymbol{x}_{i}$ in the support
    set two components, i.e., its representation $\boldsymbol{r}_{i}$ and the fast
    weights $\boldsymbol{\theta}_{i}^{*}$. These are then used to compute a attention-based
    representation and fast weights for new inputs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Meta networks, by Munkhdalai and Yu ([2017](#bib.bib55))
  prefs: []
  type: TYPE_NORMAL
- en: 1:Sample $S=\{(\boldsymbol{x}_{i},y_{i})\backsim D^{tr}_{\mathcal{T}_{j}}\}_{i=1}^{T}$
    from the support set2:for $(\boldsymbol{x}_{i},y_{i})\in S$ do3:     $\mathcal{L}_{i}=\mbox{error}(u_{\boldsymbol{\phi}}(\boldsymbol{x}_{i}),y_{i})$4:end for5:$\boldsymbol{\phi}^{*}=d_{\boldsymbol{\psi}}(\{\nabla_{\boldsymbol{\phi}}\mathcal{L}_{i}\}_{i=1}^{T})$6:for $(\boldsymbol{x}_{i},y_{i})\in
    D^{tr}_{\mathcal{T}_{j}}$ do7:     $\mathcal{L}_{i}=\mbox{error}(b_{\boldsymbol{\theta}}(\boldsymbol{x}_{i}),y_{i})$8:     $\boldsymbol{\theta}_{i}^{*}=m_{\boldsymbol{\varphi}}(\nabla_{\boldsymbol{\theta}}\mathcal{L}_{i})$9:     Store
    $\boldsymbol{\theta}_{i}^{*}$ in $i$-th position of example-level weight memory
    $M$10:     $\boldsymbol{r}_{i}=u_{\boldsymbol{\phi},\boldsymbol{\phi}^{*}}(\boldsymbol{x}_{i})$11:     Store
    $\boldsymbol{r}_{i}$ in $i$-th position of representation memory $R$12:end for13:$\mathcal{L}_{task}=0$14:for $(\boldsymbol{x},y)\in
    D^{test}_{\mathcal{T}_{j}}$ do15:     $\boldsymbol{r}=u_{\boldsymbol{\phi},\boldsymbol{\phi}^{*}}(\boldsymbol{x})$16:     $\boldsymbol{a}=\mbox{attention}(R,\boldsymbol{r})$
    $\triangleright$ $a_{k}$ is the cosine similarity between $\boldsymbol{r}$ and
    $R(k)$17:     $\boldsymbol{\theta}^{*}=\mbox{softmax}(\boldsymbol{a})^{T}M$18:     $\mathcal{L}_{task}=\mathcal{L}_{task}+\mbox{error}(b_{\boldsymbol{\theta},\boldsymbol{\theta}^{*}}(\boldsymbol{x}),y)$19:end for20:Update
    $\Theta=\{\boldsymbol{\theta},\boldsymbol{\phi},\boldsymbol{\psi},\boldsymbol{\varphi}\}$
    using $\nabla_{\Theta}\mathcal{L}_{task}$
  prefs: []
  type: TYPE_NORMAL
- en: The pseudocode for meta networks is displayed in [Algorithm 1](#alg1 "Algorithm
    1 ‣ 4.4 Meta Networks ‣ 4 Model-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    First, a sample of the support set is created (line 1), which is used to compute
    task-specific weights $\boldsymbol{\phi}^{*}$ for the representation network $u_{\boldsymbol{\phi}}$
    (lines 2-5). Note that $u_{\boldsymbol{\phi}}$ has two tasks, i) it should compute
    a representation for inputs $(\boldsymbol{x}_{i}$ (line 10 and 15), and ii) it
    needs to make predictions for inputs $(\boldsymbol{x}_{i}$, in order to compute
    a loss (line 3). To achieve both goals, a conventional neural network can be used
    that makes class predictions. The states of the final hidden layer are then used
    as representations. Typically, the cross entropy is calculated over the predictions
    of representation network $u_{\boldsymbol{\phi}}$. When there are multiple examples
    per class in the support set, an alternative is to use a contrastive loss function
    (Munkhdalai and Yu, [2017](#bib.bib55)).
  prefs: []
  type: TYPE_NORMAL
- en: Then, meta networks iterate over every example $(\boldsymbol{x}_{i},y_{i})$
    in the support set $D^{tr}_{\mathcal{T}_{j}}$. The base-learner $b_{\boldsymbol{\theta}}$
    attempts to make class predictions for these examples, resulting in loss values
    $\mathcal{L}_{i}$ (line 7-8). The gradients of these losses are used to compute
    fast weights $\boldsymbol{\theta}^{*}$ for example $i$ (line 8), which are then
    stored in the $i$-th row of memory matrix $M$ (line 9). Additionally, input representations
    $\boldsymbol{r}_{i}$ are computed and stored in memory matrix $R$ (lines 10-11).
  prefs: []
  type: TYPE_NORMAL
- en: Now, meta networks are ready to address the query set $D^{test}_{\mathcal{T}_{j}}$.
    They iterate over every example $(\boldsymbol{x},y)$, and compute a representation
    $\boldsymbol{r}$ of it (line 15). This representation is matched against the representations
    of the support set, which are stored in memory matrix $R$. This matching gives
    us a similarity vector $\boldsymbol{a}$, where every entry $k$ denotes the similarity
    between input representation $\boldsymbol{r}$ and the $k$-th row in memory matrix
    R, i.e., $R(k)$ (line 16). A softmax over this similarity vector is performed
    to normalize the entries. The resulting vector is used to compute a linear combination
    of weights that were generated for inputs in the support set (line 17). These
    weights $\boldsymbol{\theta}^{*}$ are specific for input $\boldsymbol{x}$ in the
    query set and can be used by the base-learner $b$ to make predictions for that
    input (line 18). The observed error is added to the task loss. After the entire
    query set is processed, all involved parameters can be updated using backpropagation
    (line 20).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/864ab7d009d7fb5f840ea950043e2260.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Layer augmentation setup used to combine slow and fast weights.
    Source: Munkhdalai and Yu ([2017](#bib.bib55)).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that some neural networks use both slow- and fast-weights at the same time.
    Munkhdalai and Yu ([2017](#bib.bib55)) use a so-called augmentation setup for
    this, as depicted in [Figure 16](#S4.F16 "Figure 16 ‣ 4.4 Meta Networks ‣ 4 Model-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning").
  prefs: []
  type: TYPE_NORMAL
- en: In short, meta networks rely on a reparameterization of the meta- and base-learner
    for every task. Despite the flexibility and applicability to both supervised and
    reinforcement learning settings, the approach is quite complex. It consists of
    many components, each with its own set of parameters, which can be a burden on
    memory usage and computation time. Additionally, finding the correct architecture
    for all the involved components can be time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Simple Neural Attentive Meta-Learner (SNAIL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of an external memory matrix, SNAIL (Mishra et al., [2018](#bib.bib52))
    relies on a special model architecture to serve as memory. Mishra et al. ([2018](#bib.bib52))
    argue that it is not possible to use Recurrent Neural Networks for this, as they
    have limited memory capacity, and cannot pinpoint specific prior experiences (Mishra
    et al., [2018](#bib.bib52)). Hence, SNAIL uses a different architecture, consisting
    of 1D temporal convolutions (Oord et al., [2016](#bib.bib59)) and a soft attention
    mechanism (Vaswani et al., [2017](#bib.bib84)). The temporal convolutions allow
    for ‘high bandwidth’ memory access, and the attention mechanism allows one to
    pinpoint specific experiences. [Figure 17](#S4.F17 "Figure 17 ‣ 4.5 Simple Neural
    Attentive Meta-Learner (SNAIL) ‣ 4 Model-based Meta-Learning ‣ A Survey of Deep
    Meta-Learning") visualizes the architecture and workflow of SNAIL for supervised
    learning problems. From this figure, it becomes clear why this technique is model-based.
    That is, model outputs are based upon the internal state, computed from earlier
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e70d5ca0687b28bd747f46f182d1d17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Architecture and workflow of SNAIL for supervised and reinforcement
    learning settings. The input layer is red. Temporal Convolution blocks are orange;
    attention blocks are green. Source: Mishra et al. ([2018](#bib.bib52)).'
  prefs: []
  type: TYPE_NORMAL
- en: SNAIL consists of three building blocks. The first is the DenseBlock, which
    applies a single 1D convolution to the input, and concatenates (in the feature/horizontal
    direction) the result. The second is a TCBlock, which is simply a series of DenseBlocks
    with exponentially increasing dilation rate of the temporal convolutions (Mishra
    et al., [2018](#bib.bib52)). Note that the dilation is nothing but the temporal
    distance between two nodes in a network. For example, if we use a dilation of
    2, a node at position $p$ in layer $L$ will receive the activation from node $p-2$
    from layer $L-1$. The third block is the AttentionBlock, which learns to focus
    on the important parts of prior experience.
  prefs: []
  type: TYPE_NORMAL
- en: In similar fashion to memory-augmented neural networks (Santoro et al., [2016](#bib.bib68))
    ([Section 4.3](#S4.SS3 "4.3 Memory-Augmented Neural Networks (MANNs) ‣ 4 Model-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning")), SNAIL also processes task data
    in sequence, as shown in [Figure 17](#S4.F17 "Figure 17 ‣ 4.5 Simple Neural Attentive
    Meta-Learner (SNAIL) ‣ 4 Model-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    However, the input at time $t$ is accompanied by the label at time $t$, instead
    of $t-1$ (as was the case for memory-augmented neural networks). SNAIL learns
    internal dynamics from seeing various tasks so that it can make good predictions
    on the query set, conditioned upon the support set.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of SNAIL is that it can be applied to both supervised and reinforcement
    learning tasks. In addition, it achieves good performance compared to previously
    discussed techniques. A downside of SNAIL is that finding the correct architecture
    of TCBlocks and DenseBlocks can be time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Conditional Neural Processes (CNPs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30b0da4cb14b733fa0c813e306fc4b37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Schematic view of how conditional neural processes work. Here, $h$
    denotes a network outputting a representation for a observation, $a$ denotes an
    aggregation function for these representations, and $g$ denotes a neural network
    that makes predictions for unlabelled observations, based on the aggregated representation.
    Source: Garnelo et al. ([2018](#bib.bib18)).'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to previous techniques, a conditional neural process (CNP) (Garnelo
    et al., [2018](#bib.bib18)) does not rely on an external memory module. Instead,
    it aggregates the support set into a single aggregated latent representation.
    The general architecture is shown in [Figure 18](#S4.F18 "Figure 18 ‣ 4.6 Conditional
    Neural Processes (CNPs) ‣ 4 Model-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    As we can see, the conditional neural process operates in three phases on task
    $\mathcal{T}_{j}$. First, it observes the support set $D^{tr}_{\mathcal{T}_{j}}$,
    including the ground-truth outputs $y_{i}$. Examples $(\boldsymbol{x}_{i},y_{i})\in
    D^{tr}_{\mathcal{T}_{j}}$ are embedded using a neural network $h_{\boldsymbol{\theta}}$
    into representations $\boldsymbol{r}_{i}$. Second, these representations are aggregated
    using operator $a$ to produce a single representation $\boldsymbol{r}$ of $D^{tr}_{\mathcal{T}_{j}}$
    (hence it is model-based). Third, a neural network $g_{\boldsymbol{\phi}}$ processes
    this single representation $\boldsymbol{r}$, new inputs $\boldsymbol{x}$, and
    produces predictions $\hat{y}$.
  prefs: []
  type: TYPE_NORMAL
- en: Let the entire conditional neural process model be denoted by $Q_{\boldsymbol{\Theta}}$,
    where $\Theta$ is a set of all involved parameters $\{\boldsymbol{\theta},\boldsymbol{\phi}\}$.
    The training process is different compared to other techniques. Let $\boldsymbol{x}_{\mathcal{T}_{j}}$
    and $\boldsymbol{y}_{\mathcal{T}_{j}}$ denote all inputs and corresponding outputs
    in $D_{\mathcal{T}_{j}}^{tr}$. Then, the first $\ell\backsim U(0,\ldots,k\cdot
    N-1)$ examples in $D^{tr}_{\mathcal{T}_{j}}$ are used as a conditioning set $D^{c}_{\mathcal{T}_{j}}$
    (effectively splitting the support set in a true training set and a validation
    set). Given a value of $\ell$, the goal is to maximize the log likelihood (or
    minimize the negative log likelihood) of the labels $\boldsymbol{y}_{\mathcal{T}_{j}}$
    in the entire support set $D^{tr}_{\mathcal{T}_{j}}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}(\boldsymbol{\Theta})=-\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}\left[\mathbb{E}_{\ell\backsim U(0,\ldots,k\cdot N-1)}\left(Q_{\boldsymbol{\Theta}}(\boldsymbol{y}_{\mathcal{T}_{j}}&#124;D^{c}_{\mathcal{T}_{j}},\boldsymbol{x}_{\mathcal{T}_{j}})\right)\right].$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: Conditional neural processes are trained by repeatedly sampling various tasks
    and values of $\ell$, and propagating the observed loss backwards.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, conditional neural processes use compact representations of previously
    seen inputs to aid the classification of new observations. Despite its simplicity
    and elegance, a disadvantage of this technique is that it is often outperformed
    in few-shot settings by other techniques such as matching networks (Vinyals et al.,
    [2016](#bib.bib86)) (see [Section 3.3](#S3.SS3 "3.3 Matching Networks ‣ 3 Metric-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Neural Statistician
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be20c3ce838655cef75e5af094b0d6bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Neural statistician architecture. Edges are neural networks. All
    incoming inputs to a node are concatenated.'
  prefs: []
  type: TYPE_NORMAL
- en: A neural statistician (Edwards and Storkey, [2017](#bib.bib12)) differs from
    earlier approaches as it learns to compute summary statistics, or meta-features,
    of data sets in an unsupervised manner. These latent embeddings (making the approach
    model-based) can then later be used for making predictions. Despite the broad
    applicability of the model, we discuss it in the context of Deep Meta-Learning.
  prefs: []
  type: TYPE_NORMAL
- en: A neural statistician performs both learning and inference. In the learning
    phase, the model attempts to produce generative models $\hat{P}_{i}$ for every
    data set $D_{i}$. The key assumption that is made by Edwards and Storkey ([2017](#bib.bib12))
    is that there exists a generative process $P_{i}$, which conditioned on a latent
    context vector $\boldsymbol{c}_{i}$, can produce data set $D_{i}$. At inference
    time, the goal is to infer a (posterior) probability distribution over the context
    $q(\boldsymbol{c}|D)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model uses a variational autoencoder, which consists of an encoder and
    decoder. The encoder is responsible for producing a distribution over latent vectors
    $\boldsymbol{z}$: $q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})$, where
    $\boldsymbol{x}$ is an input vector, and $\boldsymbol{\phi}$ are the encoder parameters.
    The encoded input $\boldsymbol{z}$, which is often of lower dimensionality than
    the original input $\boldsymbol{x}$, can then be decoded by the decoder $p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})$.
    Here, $\boldsymbol{\theta}$ are the parameters of the decoder. To capture more
    complex patterns in data sets, the model uses multiple latent layers $\boldsymbol{z}_{1},\ldots,\boldsymbol{z}_{L}$,
    as shown in [Figure 19](#S4.F19 "Figure 19 ‣ 4.7 Neural Statistician ‣ 4 Model-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning"). Given this architecture, the
    posterior over $c$ and $\boldsymbol{z}_{1},..,\boldsymbol{z}_{L}$ (shorthand $\boldsymbol{z}_{1:L}$)
    is given by'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(\boldsymbol{c},\boldsymbol{z}_{1:L}&#124;D;\boldsymbol{\phi})=q(\boldsymbol{c}&#124;D;\boldsymbol{\phi})\prod_{\boldsymbol{x}\in
    D}q(z_{L}&#124;\boldsymbol{x},\boldsymbol{c};\boldsymbol{\phi})\prod_{i=1}^{L-1}q(\boldsymbol{z}_{i}&#124;\boldsymbol{z}_{i+1},\boldsymbol{x},\boldsymbol{c};\boldsymbol{\phi}).$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: The neural statistician is trained to minimize a three-component loss function,
    consisting of the reconstruction loss (how well it models the data), context loss
    (how well the inferred context $q(\boldsymbol{c}|D;\boldsymbol{\phi})$ corresponds
    to the prior $P(\boldsymbol{c})$, and latent loss (how well the inferred latent
    variables $\boldsymbol{z}_{i}$ are modelled).
  prefs: []
  type: TYPE_NORMAL
- en: This model can be applied to $N$-way, few-shot learning as follows. Construct
    $N$ data sets for every of the $N$ classes, such that one data set contains only
    examples of the same class. Then, the neural statistician is provided with a new
    input $\boldsymbol{x}$, and has to predict its class. It computes a context posterior
    $N_{\boldsymbol{x}}=q(\boldsymbol{c}|\boldsymbol{x};\boldsymbol{\phi})$ depending
    on new input $\boldsymbol{x}$. In similar fashion, context posteriors are computed
    for all of the data sets $N_{i}=q(\boldsymbol{c}|D_{i};\boldsymbol{\phi})$. Lastly,
    it assigns the label $i$ such that the difference between $N_{i}$ and $N_{\boldsymbol{x}}$
    is minimal.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the neural statistician (Edwards and Storkey, [2017](#bib.bib12))
    allows for quick learning on new tasks through data set modeling. Additionally,
    it is applicable to both supervised and unsupervised settings. A downside is that
    the approach requires many data sets to achieve good performance (Edwards and
    Storkey, [2017](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Model-based Techniques, in conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we have discussed various model-based techniques. Despite apparent
    differences, they all build on the notion of task internalization. That is, tasks
    are processed and represented in the state of the model-based system. This state
    can then be used to make predictions. [Figure 20](#S4.F20 "Figure 20 ‣ 4.8 Model-based
    Techniques, in conclusion ‣ 4 Model-based Meta-Learning ‣ A Survey of Deep Meta-Learning")
    displays the relationships between the covered model-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-augmented neural networks (MANNs) (Santoro et al., [2016](#bib.bib68))
    mark the beginning of the deep model-based meta-learning techniques. They use
    the idea of feeding the entire support set in sequential fashion into the model
    and then making predictions for the query set inputs using the internal state
    of the model. Such a model-based approach, where inputs sequentially enter the
    model was also taken by recurrent meta-learners (Duan et al., [2016](#bib.bib11);
    Wang et al., [2016](#bib.bib89)) in the reinforcement learning setting. Meta networks
    (Munkhdalai and Yu, [2017](#bib.bib55)) also use a large black-box solution but
    generate task-specific weights for every task that is encountered. SNAIL (Mishra
    et al., [2018](#bib.bib52)) tries to improve the memory capacity and ability to
    pinpoint memories, which is limited in recurrent neural networks, by using attention
    mechanisms coupled with special temporal layers. Lastly, the neural statistician
    and conditional neural process (CPN) are two techniques that try to learn the
    meta-features of data sets in an end-to-end fashion. The neural statistician uses
    the distance between meta-features to make class predictions, while the conditional
    neural process conditions classifiers on these features.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of model-based approaches include the flexibility of the internal
    dynamics of the systems, and their broader applicability compared to most metric-based
    techniques. However, model-based techniques are often outperformed by metric-based
    techniques in supervised settings (e.g. graph neural networks (Garcia and Bruna,
    [2017](#bib.bib17)); [Section 3.6](#S3.SS6 "3.6 Graph Neural Networks ‣ 3 Metric-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning")), may not perform well when presented
    with larger data sets (Hospedales et al., [2020](#bib.bib32)), and generalize
    less well to more distant tasks than optimization-based techniques (Finn and Levine,
    [2018](#bib.bib13)). We discuss this optimization-based approach next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ea06c59d07346ffc62bc41d061b1502.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The relationships between the covered model-based meta-learning
    techniques. The neural statistician and conditional neural process (CNP) form
    an island in the model-based approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Optimization-based Meta-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimization-based techniques adopt a different perspective on meta-learning
    than the previous two approaches. They explicitly optimize for fast learning.
    Most optimization-based techniques do so by approaching meta-learning as a bi-level
    optimization problem. At the inner-level, a base-learner makes task-specific updates
    using some optimization strategy (such as gradient descent). At the outer-level,
    the performance across tasks is optimized.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, given a task $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$
    with new input $\boldsymbol{x}\in D^{test}_{\mathcal{T}_{j}}$ and base-learner
    parameters $\boldsymbol{\theta}$, optimization-based meta-learners return
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(Y&#124;\boldsymbol{x},D^{tr}_{\mathcal{T}_{j}})=f_{g_{\boldsymbol{\varphi}(\boldsymbol{\theta},D_{\mathcal{T}_{j}}^{tr},\mathcal{L}_{\mathcal{T}_{j}})}}(\boldsymbol{x}),$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $f$ is the base-learner, $g_{\boldsymbol{\varphi}}$ is a (learned) optimizer
    that makes task-specific updates to the base-learner parameters $\boldsymbol{\theta}$
    using the support data $D_{\mathcal{T}_{i}}^{tr}$, and loss function $\mathcal{L}_{\mathcal{T}_{j}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we are faced with a linear regression problem, where every task is
    associated with a different function $f(x)$. For this example, suppose our model
    only has two parameters: $a$ and $b$, which together form the function $\hat{f}(x)=ax+b$.
    Suppose further that our meta-training set consists of four different tasks, i.e.,
    A, B, C, and D. Then, according to the optimization-based view, we wish to find
    a single set of parameters $\{a,b\}$ from which we can quickly learn the optimal
    parameters for each of the four tasks, as displayed in [Figure 21](#S5.F21 "Figure
    21 ‣ 5.1 Example ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    In fact, this is the intuition behind the popular optimization-based technique
    MAML (Finn et al., [2017](#bib.bib14)). By exposing our model to various meta-training
    tasks, we can update the parameters $a$ and $b$ to facilitate quick adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29b19f81ec1a2162509dc4b66daaa796.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Example of an optimization-based technique, inspired by Finn et al.
    ([2017](#bib.bib14)).'
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss the core optimization-based techniques in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 LSTM Optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standard gradient update rules have the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{t+1}:=\boldsymbol{\theta}_{t}-\alpha\nabla_{\boldsymbol{\theta}_{t}}\mathcal{L}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{t}),$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is the learning rate, and $\mathcal{L}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{t})$
    is the loss function with respect to task $\mathcal{T}_{j}$ and network parameters
    at time $t$, i.e., $\boldsymbol{\theta}_{t}$. The key idea underlying LSTM optimizers
    (Andrychowicz et al., [2016](#bib.bib2)) is to replace the update term ($-\alpha\nabla\mathcal{L}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{t})$)
    by an update proposed by an LSTM $g$ with parameters $\boldsymbol{\varphi}$. Then,
    the new update becomes
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{t+1}:=\boldsymbol{\theta}_{t}+g_{\boldsymbol{\varphi}}(\nabla_{\boldsymbol{\theta}_{t}}\mathcal{L}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{t})).$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/bd95e843adddd89f75f4ff664e883e98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Workflow of the LSTM optimizer. Gradients can only propagate backwards
    through solid edges. $f_{t}$ denotes the observed loss at time step $t$. Source:
    Andrychowicz et al. ([2016](#bib.bib2)).'
  prefs: []
  type: TYPE_NORMAL
- en: This new update allows the optimization strategy to be tailored to a specific
    family of tasks. Note that this is meta-learning, i.e., the LSTM learns to learn.
    As such, this technique basically learns an update policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function used to train an LSTM optimizer is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}(\boldsymbol{\varphi})=\mathbb{E}_{\mathcal{L}_{\mathcal{T}_{j}}}\left[\sum_{t=1}^{T}w_{t}\mathcal{L}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{t})\right],$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: where $T$ is the number of parameter updates that are made, and $w_{t}$ are
    weights indicating the importance of performance after $t$ steps. Note that generally,
    we are only interested in the final performance after $T$ steps. However, the
    authors found that the optimization procedure was better guided by equally weighting
    the performance after each gradient descent step. As is often done, second-order
    derivatives (arising from the dependency between the updated weights and the LSTM
    optimizer) were ignored due to the computational expenses associated with the
    computation thereof. This loss function is fully differentiable and thus allows
    for training an LSTM optimizer (see [Figure 22](#S5.F22 "Figure 22 ‣ 5.2 LSTM
    Optimizer ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning")).
    To prevent a parameter explosion, the same network is used for every coordinate/weight
    in the base-learner’s network, causing the update rule to be the same for every
    parameter. Of course, the updates depend on their prior values and gradients.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of LSTM optimizers is that they can enable faster learning
    compared to hand-crafted optimizers, also on different data sets than those used
    to train the optimizer. However, Andrychowicz et al. ([2016](#bib.bib2)) did not
    apply this technique to few-shot learning. In fact, they did not apply it across
    tasks at all. Thus, it is unclear whether this technique can perform well in few-shot
    settings, where few data per class are available for training. Furthermore, the
    question remains whether it can scale to larger base-learner architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 LSTM Meta-Learner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of having an LSTM predict gradient updates, Ravi and Larochelle ([2017](#bib.bib65))
    embed the weights of the base-learner parameters into the cell state (long-term
    memory component) of the LSTM, giving rise to LSTM meta-learners. As such, the
    base-learner parameters $\boldsymbol{\theta}$ are literally inside the LSTM memory
    component (cell state). In this way, cell state updates correspond to base-learner
    parameter updates. This idea was inspired by the resemblance between the gradient
    and cell state update rules. Gradient updates often have the form as shown in
    [Equation 15](#S5.E15 "15 ‣ 5.2 LSTM Optimizer ‣ 5 Optimization-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"). The LSTM cell state update rule, in contrast,
    looks as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{c}_{t}:=f_{t}\odot\boldsymbol{c}_{t-1}+\alpha_{t}\odot\bar{\boldsymbol{c}}_{t},$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{t}$ is the forget gate (which determines which information should
    be forgotten) at time $t$, $\odot$ represents the element-wise product, $\boldsymbol{c}_{t}$
    is the cell state at time $t$, and $\bar{\boldsymbol{c}}_{t}$ the candidate cell
    state for time step $t$, and $\alpha_{t}$ the learning rate at time step $t$.
    Note that if $f_{t}=\boldsymbol{1}$ (vector of ones), $\alpha_{t}=\alpha$, $\boldsymbol{c}_{t-1}=\boldsymbol{\theta}_{t-1}$,
    and $\bar{\boldsymbol{c}}_{t}=-\nabla_{\boldsymbol{\theta}_{t-1}}\mathcal{L}_{\mathcal{T}_{t}}(\boldsymbol{\theta}_{t-1})$,
    this update is equivalent to the one used by gradient-descent. This similarity
    inspired Ravi and Larochelle ([2017](#bib.bib65)) to use an LSTM as meta-learner
    that learns to make updates for a base-learner, as shown in [Figure 23](#S5.F23
    "Figure 23 ‣ 5.3 LSTM Meta-Learner ‣ 5 Optimization-based Meta-Learning ‣ A Survey
    of Deep Meta-Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/726c41203ebfa242c77b5276bd15e98a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: LSTM meta-learner computation graph. Gradients can only propagate
    backwards through solid edges. The base-learner is denoted as $M$. $(X_{t},Y_{t})$
    are training sets, whereas $(X,Y)$ is the test set. Source: Ravi and Larochelle
    ([2017](#bib.bib65)).'
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, the cell state of the LSTM is initialized with $c_{0}=\boldsymbol{\theta}_{0}$,
    which will be adjusted by the LSTM to a good common initialization point across
    different tasks. Then, to update the weights of the base-learner for the next
    time step $t+1$, the LSTM computes $\boldsymbol{c}_{t+1}$ and sets the weights
    of the base-learner equal to that. There is thus a one-to-one correspondence between
    $\boldsymbol{c}_{t}$ and $\boldsymbol{\theta}_{t}$. The meta-learner’s learning
    rate $\alpha_{t}$ (see [Equation 18](#S5.E18 "18 ‣ 5.3 LSTM Meta-Learner ‣ 5 Optimization-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning")), is set equal to $\sigma(\boldsymbol{w}_{\alpha}\cdot[\nabla_{\theta_{t-1}}\mathcal{L}_{\mathcal{T}_{t}}(\boldsymbol{\theta}_{t-1}),\mathcal{L}_{\mathcal{T}_{t}}(\boldsymbol{\theta}_{t}),\theta_{t-1},\alpha_{t-1}]+\boldsymbol{b}_{\alpha})$,
    where $\sigma$ is the sigmoid function. Note that the output is a vector, with
    values between 0 and 1, which denote the the learning rates for the corresponding
    parameters. Furthermore, $\boldsymbol{w}_{\alpha}$ and $\boldsymbol{b}_{\alpha}$
    are trainable parameters that part of the LSTM meta-learner. In words, the learning
    rate at any time depends on the loss gradients, the loss value, the previous parameters,
    and the previous learning rate. The forget gate, $f_{t}$, determines what part
    of the cell state should be forgotten, and is computed in a similar fashion, but
    with different weights.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent an explosion of meta-learner parameters, weight-sharing is used,
    in similar fashion to LSTM optimizers proposed by Andrychowicz et al. ([2016](#bib.bib2))
    ([Section 5.2](#S5.SS2 "5.2 LSTM Optimizer ‣ 5 Optimization-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning")). This implies that the same update rule is
    applied to every weight at a given time step. The exact update, however, depends
    on the history of that specific parameter in terms of the previous learning rate,
    loss, etc. For simplicity, second-order derivatives were ignored, by assuming
    the base-learner’s loss does not depend on the cell state of the LSTM optimizer.
    Batch normalization was applied to stabilize and speed up the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: In short, LSTM optimizers can learn to optimize a base-learner by maintaining
    a one-to-one correspondence over time between the base-learner’s weights and the
    LSTM cell state. This allows the LSTM to exploit commonalities in the tasks, allowing
    for quicker optimization. However, there are simpler approaches (e.g. MAML (Finn
    et al., [2017](#bib.bib14))) that outperform this technique.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Reinforcement Learning Optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Li and Malik ([2018](#bib.bib46)) proposed a framework that casts optimization
    as a reinforcement learning problem. Optimization can then be performed by existing
    reinforcement learning techniques. At a high-level, an optimization algorithm
    $g$ takes as input an initial set of weights $\boldsymbol{\theta}_{0}$ and a task
    $\mathcal{T}_{j}$ with corresponding loss function $\mathcal{L}_{\mathcal{T}_{j}}$,
    and produces a sequence of new weights $\boldsymbol{\theta}_{1},\ldots,\boldsymbol{\theta}_{T}$,
    where $\boldsymbol{\theta}_{T}$ is the final solution found. On this sequence
    of proposed new weights, we can define a loss function $\mathcal{L}$ that captures
    unwanted properties (e.g. slow convergence, oscillations, etc.). The goal of learning
    an optimizer can then be formulated more precisely as follows. We wish to learn
    an optimal optimizer
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g^{*}=argmin_{g}\,\mathbb{E}_{\mathcal{T}_{j}\backsim p(\mathcal{T}),\boldsymbol{\theta}_{0}\backsim
    p(\boldsymbol{\theta}_{0})}[\mathcal{L}(g(\mathcal{L}_{\mathcal{T}_{j}},\boldsymbol{\theta}_{0}))]$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: The key insight is that the optimization can be formulated as a Partially Observable
    Markov Decision Process (POMDP). Then, the state corresponds to the current set
    of weights $\boldsymbol{\theta}_{t}$, the action to the proposed update at time
    step t, i.e., $\Delta\boldsymbol{\theta}_{t}$, and the policy to the function
    that computes the update. With this formulation, the optimizer $g$ can be learned
    by existing reinforcement learning techniques. In their paper, they used a recurrent
    neural network as an optimizer. At each time step, they feed it observation features,
    which depend on the previous set of weights, loss gradients, and objective functions,
    and use guided policy search to train it.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Li and Malik ([2018](#bib.bib46)) made the first step towards general
    optimization through reinforcement learning optimizers, which were shown able
    to generalize across network architectures and data sets. However, the base-learner
    architecture that was used was quite small. The question remains whether this
    approach can scale to larger architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 MAML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/abc051366a2bf6f47176086a7938a879.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: MAML learns an initialization point from which it can perform well
    on various tasks. Source: Finn et al. ([2017](#bib.bib14)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model-agnostic meta-learning (MAML) (Finn et al., [2017](#bib.bib14)) uses
    a simple gradient-based inner optimization procedure (e.g. stochastic gradient
    descent), instead of more complex LSTM procedures or procedures based on reinforcement
    learning. The key idea of MAML is to explicitly optimize for fast adaptation to
    new tasks by learning a good set of initialization parameters $\boldsymbol{\theta}$.
    This is shown in [Figure 24](#S5.F24 "Figure 24 ‣ 5.5 MAML ‣ 5 Optimization-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning"): from the learned initialization
    $\boldsymbol{\theta}$, we can quickly move to the best set of parameters for task
    $\mathcal{T}_{j}$, i.e., $\boldsymbol{\theta}^{*}_{j}$ for $j=1,2,3$. The learned
    initialization can be seen as the inductive bias of the model, or simply the set
    of assumptions (encapsulated in $\boldsymbol{\theta}$) that the model makes concerning
    the overall task structure.'
  prefs: []
  type: TYPE_NORMAL
- en: More formally, let $\boldsymbol{\theta}$ denote the initial model parameters
    of a model. The goal is to quickly learn new concepts, which is equivalent to
    achieving a minimal loss in few gradient update steps. The amount of gradient
    steps $s$ has to be specified upfront, such that MAML can explicitly optimize
    for achieving good performance within that number of steps. Suppose we pick only
    one gradient update step, i.e., $s=1$. Then, given a task $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$,
    gradient descent would produce updated parameters (fast weights)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}^{\prime}_{j}=\boldsymbol{\theta}-\alpha\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}),$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: specific to task $j$. The meta-loss of quick adaptation (using $s=1$ gradient
    steps) across tasks can then be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathit{ML}:=\sum_{\mathcal{T}_{j}\backsim p(\mathcal{T})}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})=\sum_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}-\alpha\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})),$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $p(\mathcal{T})$ is a probability distribution over tasks. This expression
    contains an inner gradient ($\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{j})$).
    As such, by optimizing this meta-loss using gradient-based techniques, we have
    to compute second-order gradients. One can easily see this in the computation
    below
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\boldsymbol{\theta}}\mathit{ML}$ | $\displaystyle=\nabla_{\boldsymbol{\theta}}\sum_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{\mathcal{T}_{j}\backsim p(\mathcal{T})}\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{\mathcal{T}_{j}\backsim p(\mathcal{T})}\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\nabla_{\boldsymbol{\theta}}(\boldsymbol{\theta}^{\prime}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{\mathcal{T}_{j}\backsim p(\mathcal{T})}\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})\nabla_{\boldsymbol{\theta}}(\boldsymbol{\theta}-\alpha\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}(\boldsymbol{\theta})})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\underbrace{\sum_{\mathcal{T}_{j}\backsim p(\mathcal{T})}\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})}_{\textrm{FOMAML}}(\nabla_{\boldsymbol{\theta}}\boldsymbol{\theta}-\alpha\nabla_{\boldsymbol{\theta}}^{2}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})),$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: where we used $\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})$
    to denote the derivative of the loss function with respect to the query set, evaluated
    at the post-update parameters $\boldsymbol{\theta}_{j}^{\prime}$. The term $\alpha\nabla_{\boldsymbol{\theta}}^{2}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})$
    contains the second-order gradients. The computation thereof is expensive in terms
    of time and memory costs, especially when the optimization trajectory is large
    (when using a larger number of gradient updates $s$ per task). Finn et al. ([2017](#bib.bib14))
    experimented with leaving out second-order gradients, by assuming $\nabla_{\boldsymbol{\theta}}\boldsymbol{\theta}^{\prime}_{j}=I$,
    giving us First Order MAML (FOMAML, see [Equation 22](#S5.E22 "22 ‣ 5.5 MAML ‣
    5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning")). They found
    that FOMAML performed reasonably similar to MAML. This means that updating the
    initialization using only first order gradients $\sum_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})$
    is roughly equal to using the full gradient expression of the meta-loss in [Equation 22](#S5.E22
    "22 ‣ 5.5 MAML ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    One can extend the meta-loss to incorporate multiple gradient steps by substituting
    $\boldsymbol{\theta}_{j}^{\prime}$ by a multi-step variant.
  prefs: []
  type: TYPE_NORMAL
- en: MAML is trained as follows. The initialization weights $\boldsymbol{\theta}$
    are updated by continuously sampling a batch of $m$ tasks $B=\{\mathcal{T}_{j}\backsim
    p(\mathcal{T})\}_{i=1}^{m}$. Then, for every task $\mathcal{T}_{j}\in B$, an inner
    update is performed to obtain $\boldsymbol{\theta}_{j}^{\prime}$, in turn granting
    an observed loss $\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})$.
    These losses across a batch of tasks are used in the outer update
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}:=\boldsymbol{\theta}-\beta\nabla_{\boldsymbol{\theta}}\sum_{\mathcal{T}_{j}\in
    B}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime}).$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: The complete training procedure of MAML is displayed in [Algorithm 2](#alg2
    "Algorithm 2 ‣ 5.5 MAML ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep
    Meta-Learning"). At test-time, when presented with a new task $\mathcal{T}_{j}$,
    the model is initialized with $\boldsymbol{\theta}$, and performs a number of
    gradient updates on the task data. Note that the algorithm for FOMAML is equivalent
    to [Algorithm 2](#alg2 "Algorithm 2 ‣ 5.5 MAML ‣ 5 Optimization-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"), except for the fact that the update on line
    8 is done differently. That is, FOMAML updates the initialization with the rule
    $\boldsymbol{\theta}=\boldsymbol{\theta}-\beta\sum_{\mathcal{T}_{j}\backsim p(\mathcal{T})}\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 One-step MAML for supervised learning, by Finn et al. ([2017](#bib.bib14))
  prefs: []
  type: TYPE_NORMAL
- en: 1:Randomly initialize $\boldsymbol{\theta}$2:while not done do3:     Sample
    batch of $J$ tasks $B=\mathcal{T}_{1},\ldots,\mathcal{T}_{J}\backsim p(\mathcal{T})$4:     for $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})\in
    B$ do5:         Compute $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})$6:         Compute
    $\boldsymbol{\theta}_{j}^{\prime}=\boldsymbol{\theta}-\alpha\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})$7:     end for8:     Update
    $\boldsymbol{\theta}=\boldsymbol{\theta}-\beta\nabla_{\boldsymbol{\theta}}\sum_{\mathcal{T}_{j}\in
    B}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})$9:end while
  prefs: []
  type: TYPE_NORMAL
- en: Antoniou et al. ([2019](#bib.bib3)), in response to MAML, proposed many technical
    improvements that can improve training stability, performance, and generalization
    ability. Improvements include i) updating the initialization $\boldsymbol{\theta}$
    after every inner update step (instead of after all steps are done) to increase
    gradient propagation, ii) using second-order gradients only after 50 epochs to
    increase the training speed, iii) learning layer-wise learning rates to improve
    flexibility, iv) annealing the meta-learning rate $\beta$ over time, and v) some
    Batch Normalization tweaks (keep running statistics instead of batch-specific
    ones, and using per-step biases).
  prefs: []
  type: TYPE_NORMAL
- en: MAML has obtained great attention within the field of Deep Meta-Learning, perhaps
    due to its i) simplicity (only requires two hyperparameters), ii) general applicability,
    and iii) strong performance. A downside of MAML, as mentioned above, is that it
    can be quite expensive in terms of running time and memory to optimize a base-learner
    for every task and compute higher-order derivatives from the optimization trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 iMAML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of ignoring higher-order derivatives (as done by FOMAML), which potentially
    decreases the performance compared to regular MAML, iMAML (Rajeswaran et al.,
    [2019](#bib.bib64)) approximates these derivatives in a way that is less memory-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Let $\mathcal{A}$ denote an inner optimization algorithm (e.g., stochastic gradient
    descent), which takes a support set $D^{tr}_{\mathcal{T}_{j}}$ corresponding to
    task $\mathcal{T}_{j}$ and initial model weights $\boldsymbol{\theta}$, and produces
    new weights $\boldsymbol{\theta}^{\prime}_{j}=\mathcal{A}(\boldsymbol{\theta},D^{tr}_{\mathcal{T}_{j}})$.
    MAML has to compute the derivative
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})=\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}^{\prime}(\boldsymbol{\theta}^{\prime}_{j})\nabla_{\boldsymbol{\theta}}(\boldsymbol{\theta}^{\prime}_{j}),$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: where $D^{test}_{\mathcal{T}_{j}}$ is the query set corresponding to task $\mathcal{T}_{j}$.
    This equation is a simple result of applying the chain rule. Importantly, note
    that $\nabla_{\boldsymbol{\theta}}(\boldsymbol{\theta}_{j}^{\prime})$ differentiates
    through $\mathcal{A}(\boldsymbol{\theta},D^{tr}_{\mathcal{T}_{j}})$, while $\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}^{\prime}(\boldsymbol{\theta}^{\prime}_{j})$
    does not, as it represents the gradient of the loss function evaluated at $\boldsymbol{\theta}^{\prime}_{j}$.
    Rajeswaran et al. ([2019](#bib.bib64)) make use of the following lemma.
  prefs: []
  type: TYPE_NORMAL
- en: If $(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j}))$
    is invertible (i.e., $(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j}))^{-1}$
    exists), then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\boldsymbol{\theta}}(\boldsymbol{\theta}_{j}^{\prime})=\left(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\right)^{-1}.$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\lambda$ is a regularization parameter. The reason for this is discussed
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Combining [Equation 24](#S5.E24 "24 ‣ 5.6 iMAML ‣ 5 Optimization-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning") and [Equation 25](#S5.E25 "25 ‣ 5.6 iMAML ‣
    5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning"), we have
    that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})=\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\left(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\right)^{-1}.$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: The idea is to obtain an approximate gradient vector $\boldsymbol{g}_{j}$ that
    is close to this expression, i.e., we want the difference to be small
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{g}_{j}-\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\left(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\right)^{-1}=\boldsymbol{\epsilon},$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: for some small tolerance vector $\boldsymbol{\epsilon}$. If we multiply both
    sides by the inverse of the inverse factor, i.e., $\left(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\right)$,
    we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{g}_{j}^{T}\left(\boldsymbol{I}+\frac{1}{\lambda}\nabla^{2}_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})\right)\boldsymbol{g}_{j}-\boldsymbol{g}_{j}^{T}\mathcal{L}^{\prime}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})=\boldsymbol{\epsilon}^{\prime},$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\epsilon}^{\prime}$ absorbed the multiplication factor. We
    wish to minimize this expression for $\boldsymbol{g}_{j}$, and that can be performed
    using optimization techniques such as the conjugate gradient algorithm (Rajeswaran
    et al., [2019](#bib.bib64)). This algorithm does not need to store Hessian matrices,
    which decreases the memory cost significantly. In turn, this allows iMAML to work
    with more inner gradient update steps. Note, however, that one needs to perform
    explicit regularization in that case to avoid overfitting. The conventional MAML
    did not require this, as it uses only a few number of gradient steps (equivalent
    to an early stopping mechanism).
  prefs: []
  type: TYPE_NORMAL
- en: At each inner loop step, iMAML computes the meta-gradient $\boldsymbol{g}_{j}$.
    After processing a batch of tasks, these gradients are averaged and used to update
    the initialization $\boldsymbol{\theta}$. Since it does not differentiate through
    the optimization process, we are free to use any other (non-differentiable) inner-optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, iMAML reduces memory costs significantly as it need not differentiate
    through the optimization trajectory, also allowing for greater flexibility in
    the choice of inner optimizer. Additionally, it can account for larger optimization
    paths. The computational costs stay roughly the same compared to MAML (Finn et al.,
    [2017](#bib.bib14)). Future work could investigate more inner optimization procedures
    (Rajeswaran et al., [2019](#bib.bib64)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Meta-SGD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8098ca87366f8aa3676cb1787e5800f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: Meta-SGD learning process. Source: Li et al. ([2017](#bib.bib47)).'
  prefs: []
  type: TYPE_NORMAL
- en: Meta-SGD (Li et al., [2017](#bib.bib47)), or meta-stochastic gradient descent,
    is similar to MAML (Finn et al., [2017](#bib.bib14)) ([Section 5.5](#S5.SS5 "5.5
    MAML ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning")).
    However, on top of learning an initialization, Meta-SGD also learns learning rates
    for every model parameter in $\boldsymbol{\theta}$, building on the insight that
    the optimizer can be seen as a trainable entity.
  prefs: []
  type: TYPE_NORMAL
- en: The standard SGD update rule is given in [Equation 15](#S5.E15 "15 ‣ 5.2 LSTM
    Optimizer ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    The meta-SGD optimizer uses a more general update, namely
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{j}^{\prime}\leftarrow\boldsymbol{\theta}-\boldsymbol{\alpha}\odot\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}),$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: where $\odot$ is the element-wise product. Note that this means that alpha (learning
    rate) is now a vector—hence the bold font— instead of scalar, which allows for
    greater flexibility in the sense that each parameter has its own learning rate.
    The goal is to learn the initialization $\boldsymbol{\theta}$, and learning rate
    vector $\boldsymbol{\alpha}$, such that the generalization ability is as large
    as possible. More mathematically precise, the learning objective is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle min_{\boldsymbol{\alpha},\boldsymbol{\theta}}\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}[\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}_{j}^{\prime})]=\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}[\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}-\boldsymbol{\alpha}\odot\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D_{\mathcal{T}_{j}}^{tr}}(\boldsymbol{\theta}))],$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: where we used a simple substitution for $\boldsymbol{\theta}_{j}^{\prime}$.
    $\mathcal{L}_{D_{\mathcal{T}_{j}}^{tr}}$ and $\mathcal{L}_{D_{\mathcal{T}_{j}}^{test}}$
    are the losses computed on the support and query set respectively. Note that this
    formulation stimulates generalization ability (as it includes the query set loss
    $\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}$, which can be observed during the meta-training
    phase). The learning process is visualized in [Figure 25](#S5.F25 "Figure 25 ‣
    5.7 Meta-SGD ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    Note that the meta-SGD optimizer is trained to maximize generalization ability
    after only one update step. Since this learning objective has a fully differentiable
    loss function, the meta-SGD optimizer itself can be trained using standard SGD.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Meta-SGD is more expressive than MAML as it does not only learn
    an initialization but also learning rates per parameter. This, however, does come
    at the cost of an increased number of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Reptile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reptile (Nichol et al., [2018](#bib.bib58)) is another optimization-based technique
    that, like MAML (Finn et al., [2017](#bib.bib14)), solely attempts to find a good
    set of initialization parameters $\boldsymbol{\theta}$. The way in which Reptile
    attempts to find this initialization is quite different from MAML. It repeatedly
    samples a task, trains on the task, and moves the model weights towards the trained
    weights (Nichol et al., [2018](#bib.bib58)). [Algorithm 3](#alg3 "Algorithm 3
    ‣ 5.8 Reptile ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning")
    displays the pseudocode describing this simple process.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Reptile, by Nichol et al. ([2018](#bib.bib58))
  prefs: []
  type: TYPE_NORMAL
- en: 1:Initialize $\boldsymbol{\theta}$2:for $i=1,2,\ldots$ do3:     Sample task
    $\mathcal{T}_{j}=(D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}})$ and corresponding
    loss function $\mathcal{L}_{\mathcal{T}_{j}}$4:     $\boldsymbol{\theta}^{\prime}_{j}=SGD(\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}},\boldsymbol{\theta},k)$
    $\triangleright$ Perform $k$ gradient update steps to get $\boldsymbol{\theta}_{j}^{\prime}$5:     $\boldsymbol{\theta}:=\boldsymbol{\theta}+\epsilon(\boldsymbol{\theta}^{\prime}_{j}-\boldsymbol{\theta})$
    $\triangleright$ Move initialization point $\boldsymbol{\theta}$ towards $\boldsymbol{\theta}_{j}^{\prime}$6:end for
  prefs: []
  type: TYPE_NORMAL
- en: Nichol et al. ([2018](#bib.bib58)) note that it is possible to treat $(\boldsymbol{\theta}-\boldsymbol{\theta}_{j}^{\prime})/\alpha$
    as gradients, where $\alpha$ is the learning rate of the inner stochastic gradient
    descent optimizer (line 4 in the pseudocode), and to feed that into a meta-optimizer
    (e.g. Adam). Moreover, instead of sampling one task at a time, one could sample
    a batch of $n$ tasks, and move the initialization $\boldsymbol{\theta}$ towards
    the average update direction $\bar{\boldsymbol{\theta}}=\frac{1}{n}\sum_{j=1}^{n}(\boldsymbol{\theta}^{\prime}_{j}-\boldsymbol{\theta})$,
    granting the update rule $\boldsymbol{\theta}:=\boldsymbol{\theta}+\epsilon\bar{\boldsymbol{\theta}}$.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind Reptile is that updating the initialization weights towards
    updated parameters will grant a good inductive bias for tasks from the same family.
    By performing Taylor expansions of the gradients of Reptile and MAML (both first-order
    and second-order), Nichol et al. ([2018](#bib.bib58)) show that the expected gradients
    differ in their direction. They argue, however, that in practice, the gradients
    of Reptile will also bring the model towards a point minimizing the expected loss
    over tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A mathematical argument as to why Reptile works goes as follows. Let $\boldsymbol{\theta}$
    denote the initial parameters, and $\boldsymbol{\theta}^{*}_{j}$ the optimal set
    of weights for task $\mathcal{T}_{j}$. Lastly, let $d$ be the Euclidean distance
    function. Then, the goal is to minimize the distance between the initialization
    point $\boldsymbol{\theta}$ and the optimal point $\boldsymbol{\theta}^{*}_{j}$,
    i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle min_{\boldsymbol{\theta}}\,\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}[\frac{1}{2}d(\boldsymbol{\theta},\boldsymbol{\theta}^{*}_{j})^{2}].$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: The gradient of this expected distance with respect to the initialization $\boldsymbol{\theta}$
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\boldsymbol{\theta}}\mathbb{E}_{\mathcal{T}_{j}\backsim
    p(\mathcal{T})}[\frac{1}{2}d(\boldsymbol{\theta},\boldsymbol{\theta}^{*}_{j})^{2}]$
    | $\displaystyle=\mathbb{E}_{\mathcal{T}_{j}\backsim p(\mathcal{T})}[\frac{1}{2}\nabla_{\boldsymbol{\theta}}d(\boldsymbol{\theta},\boldsymbol{\theta}^{*}_{j})^{2}]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\mathcal{T}_{j}\backsim p(\mathcal{T})}[\boldsymbol{\theta}-\boldsymbol{\theta}^{*}_{j}],$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: where we used the fact that the gradient of the squared Euclidean distance between
    two points $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$ is the vector $2(\boldsymbol{x}_{1}-\boldsymbol{x}_{2})$.
    Nichol et al. ([2018](#bib.bib58)) go on to argue that performing gradient descent
    on this objective would result in the following update rule
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}$ | $\displaystyle=\boldsymbol{\theta}-\epsilon\nabla_{\boldsymbol{\theta}}\frac{1}{2}d(\boldsymbol{\theta},\boldsymbol{\theta}^{*}_{j})^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\boldsymbol{\theta}-\epsilon(\boldsymbol{\theta}^{*}_{j}-\boldsymbol{\theta}).$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: Since we do not know $\boldsymbol{\theta}^{*}_{\mathcal{T}_{j}}$, one can approximate
    this by term by $k$ steps of gradient descent $SGD(\mathcal{L}_{\mathcal{T}_{j}},\boldsymbol{\theta},k)$.
    In short, Reptile can be seen as gradient descent on the distance minimization
    objective given in [Equation 31](#S5.E31 "31 ‣ 5.8 Reptile ‣ 5 Optimization-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning"). A visualization is shown in
    [Figure 26](#S5.F26 "Figure 26 ‣ 5.8 Reptile ‣ 5 Optimization-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning"). The initialization $\boldsymbol{\theta}$ is
    moving towards the optimal weights for tasks 1 and 2 in interleaved fashion (hence
    the oscillations).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8be7d9c208fc2a069218cd1e5fdf7cc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: Schematic visualization of Reptile’s learning trajectory. Here,
    $\boldsymbol{\theta}_{1}^{*}$ and $\boldsymbol{\theta}_{2}^{*}$ are the optimal
    weights for tasks $\mathcal{T}_{1}$ and $\mathcal{T}_{2}$ respectively. The initialization
    parameters $\boldsymbol{\theta}$ oscillate between these. Adapted from Nichol
    et al. ([2018](#bib.bib58)).'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Reptile is an extremely simple meta-learning technique, which
    does not need to differentiate through the optimization trajectory like, e.g.,
    MAML (Finn et al., [2017](#bib.bib14)), saving time and memory costs. However,
    the theoretical foundation is a bit weaker due to the fact that it does not directly
    optimize for fast learning as done by MAML, and performance may be a bit worse
    than that of MAML in some settings.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9 Latent embedding optimization (LEO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Latent Embedding Optimization, or LEO, was proposed by Rusu et al. ([2018](#bib.bib67))
    to combat an issue of gradient-based meta-learners, such as MAML (see [Section 5.5](#S5.SS5
    "5.5 MAML ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning")),
    in few-shot settings ($N$-way, $k$-shot). These techniques operate in a high-dimensional
    parameter space using gradient information from only a few examples, which could
    lead to poor generalization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/809a69e8965687f125467554cf2f691d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Workflow of LEO. adapted from Rusu et al. ([2018](#bib.bib67)).'
  prefs: []
  type: TYPE_NORMAL
- en: LEO alleviates this issue by learning a lower-dimensional latent embedding space,
    which indirectly allows us to learn a good set of initial parameters $\boldsymbol{\theta}$.
    Additionally, the embedding space is conditioned upon tasks, allowing for more
    expressivity. In theory, LEO could find initial parameters for the entire base-learner
    network, but the authors only experimented with setting the parameters for the
    final layers.
  prefs: []
  type: TYPE_NORMAL
- en: The complete workflow of LEO is shown in [Figure 27](#S5.F27 "Figure 27 ‣ 5.9
    Latent embedding optimization (LEO) ‣ 5 Optimization-based Meta-Learning ‣ A Survey
    of Deep Meta-Learning"). As we can see, given a task $\mathcal{T}_{j}$, the corresponding
    support set $D^{tr}_{\mathcal{T}_{j}}$ is fed into an encoder, which produces
    hidden codes for each example in that set. These hidden codes are paired and concatenated
    in every possible manner, granting us $(Nk)^{2}$ pairs, where $N$ is the number
    of classes in the training set, and $k$ the number of examples per class. These
    paired codes are then fed into a relation net (Sung et al., [2018](#bib.bib76))
    (see [Section 3.5](#S3.SS5 "3.5 Relation Networks ‣ 3 Metric-based Meta-Learning
    ‣ A Survey of Deep Meta-Learning")). The resulting embeddings are grouped by class,
    and parameterize a probability distribution over latent codes $\boldsymbol{z}_{n}$
    (for class $n$) in a low dimensional space $\mathcal{Z}$. More formally, let $\boldsymbol{x}^{\ell}_{n}$
    denote the $\ell$-th example of class $n$ in $D^{tr}_{\mathcal{T}_{j}}$. Then,
    the mean $\boldsymbol{\mu}^{e}_{n}$ and variance $\boldsymbol{\sigma}^{e}_{n}$
    of a Gaussian distribution over latent codes for class $n$ are computed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\mu}_{n}^{e},\boldsymbol{\sigma}^{e}_{n}=\frac{1}{Nk^{2}}\sum_{\ell_{p}=1}^{k}\sum^{N}_{m=1}\sum_{\ell_{q}=1}^{k}g_{\boldsymbol{\phi}_{r}}\left(g_{\boldsymbol{\phi}_{e}}(\boldsymbol{x}^{\ell_{p}}_{n}),g_{\boldsymbol{\phi}_{e}}(\boldsymbol{x}^{\ell_{q}}_{m})\right),$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\phi}_{r},\boldsymbol{\phi}_{e}$ are parameters for the relation
    net and encoder respectively. Intuitively, the three summations ensure that every
    example with class $n$ in $D^{tr}_{\mathcal{T}_{j}}$ is paired with every example
    from all classes $n$. Given $\boldsymbol{\mu}_{n}^{e}$, and $\boldsymbol{\sigma}_{n}^{e}$,
    one can sample a latent code $\boldsymbol{z}_{n}\backsim N(\boldsymbol{\mu}_{n}^{e},diag(\boldsymbol{\sigma}_{n}^{e2}))$
    for class $n$, which serves as latent embedding of the task training data.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder can then generate a task-specific initialization $\boldsymbol{\theta}_{n}$
    for class $n$ as follows. First, one computes a mean and variance for a Gaussian
    distribution using the latent code
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\mu}_{n}^{d},\boldsymbol{\sigma}_{n}^{d}=g_{\boldsymbol{\phi}_{d}}(\boldsymbol{z}_{n}).$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: These are then used to sample initialization weights $\boldsymbol{\theta}_{n}\backsim
    N(\boldsymbol{\mu}^{d}_{n},diag(\boldsymbol{\sigma}^{d2}_{n}))$. The loss from
    the generated weights can then be propagated backwards to adjust the embedding
    space. In practice, generating such a high-dimensional set of parameters from
    a low-dimensional embedding can be quite problematic. Therefore, LEO uses pre-trained
    models, and only generates weights for the final layer, which limits the expressivity
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of LEO is that it optimizes in a lower-dimensional latent embedding
    space, which aids generalization performance. However, the approach is more complex
    than e.g. MAML (Finn et al., [2017](#bib.bib14)), and its applicability is limited
    to few-shot learning settings.
  prefs: []
  type: TYPE_NORMAL
- en: 5.10 Online MAML (FTML)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Online MAML (Finn et al., [2019](#bib.bib16)) is an extension of MAML (Finn
    et al., [2017](#bib.bib14)) to make it applicable to online learning settings
    (Anderson, [2008](#bib.bib1)). In the online setting, we are presented with a
    sequence of tasks $\mathcal{T}_{t}$ with corresponding loss functions $\{\mathcal{L}_{\mathcal{T}_{t}}\}_{t=1}^{T}$,
    for some potentially infinite time horizon $T$. The goal is to pick a sequence
    of parameters $\{\boldsymbol{\theta}_{t}\}_{t=1}^{T}$ that performs well on the
    presented loss functions. This objective is captured by the $Regret_{T}$ over
    the entire sequence, which is defined by Finn et al. ([2019](#bib.bib16)) as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Regret_{T}=\sum_{t=1}^{T}\mathcal{L}_{\mathcal{T}_{t}}(\boldsymbol{\theta}_{t}^{\prime})-min_{\boldsymbol{\theta}}\sum_{t=1}^{T}\mathcal{L}_{\mathcal{T}_{t}}(\boldsymbol{\theta}^{\prime}_{t}),$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\theta}$ are the initial model parameters (just as MAML),
    and $\boldsymbol{\theta}_{t}^{\prime}$ are parameters resulting from a one-step
    gradient update (starting from $\boldsymbol{\theta}$) on task $t$. Here, the left
    term reflects the updated parameters chosen by the agent $(\boldsymbol{\theta}_{t})$,
    whereas the right term presents the minimum obtainable loss (in hindsight) from
    a single fixed set of parameters $\boldsymbol{\theta}$. Note that this setup assumes
    that the agent can make updates to its chosen parameters (transform its initial
    choice at time $t$ from $\boldsymbol{\theta}_{t}$ to $\boldsymbol{\theta}_{t}^{\prime}$).
  prefs: []
  type: TYPE_NORMAL
- en: Finn et al. ([2019](#bib.bib16)) propose FTML (Follow The Meta Leader), inspired
    by FTL (Follow The Leader) (Hannan, [1957](#bib.bib27); Kalai and Vempala, [2005](#bib.bib38)),
    to minimize the regret. The basic idea is to set the parameters for the next time
    step ($t+1$) equal to the best parameters in hindsight, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{t+1}:=argmin_{\boldsymbol{\theta}}\sum_{k=1}^{t}\mathcal{L}_{\mathcal{T}_{k}}(\boldsymbol{\theta}_{k}^{\prime}).$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: The gradient to perform meta-updates is then given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g_{t}(\boldsymbol{\theta}):=\nabla_{\boldsymbol{\theta}}\mathbb{E}_{\mathcal{T}_{k}\backsim
    p_{t}(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{k}}(\boldsymbol{\theta}_{k}^{\prime}),$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{t}(\mathcal{T})$ is a uniform distribution over tasks $1,\ldots,t$
    (at time $t$).
  prefs: []
  type: TYPE_NORMAL
- en: '[Algorithm 4](#alg4 "Algorithm 4 ‣ 5.10 Online MAML (FTML) ‣ 5 Optimization-based
    Meta-Learning ‣ A Survey of Deep Meta-Learning") contains the full pseudocode
    for FTML. In this algorithm, $\mathit{MetaUpdate}$ performs a few ($N_{meta}$)
    meta-steps. In each meta-step, a task is sampled from $B$, together with train
    and test mini-batches to compute the gradient $g_{t}$ in [Equation 37](#S5.E37
    "37 ‣ 5.10 Online MAML (FTML) ‣ 5 Optimization-based Meta-Learning ‣ A Survey
    of Deep Meta-Learning"). The initialization $\boldsymbol{\theta}$ is then updated
    ($\boldsymbol{\theta}:=\boldsymbol{\theta}-\beta g_{t}(\boldsymbol{\theta})$),
    where $\beta$ is the meta-learning rate. Note that the memory usage keeps increasing
    over time, as at every time step $t$, we append tasks to the buffer $B$, and keep
    task data sets in memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 FTML by Finn et al. ([2019](#bib.bib16))
  prefs: []
  type: TYPE_NORMAL
- en: 1:Performance threshold $\gamma$2:Initialize empty task buffer $B$3:for $t=1,\ldots$ do4:     Initialize
    data set $D_{t}=\emptyset$5:     Append $\mathcal{T}_{t}$ to B6:     while $|D_{t}|<N$ do7:         Append
    batch of data $\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{n}$ to $D_{t}$8:         $\boldsymbol{\theta}_{t}=\mathit{MetaUpdate}(\boldsymbol{\theta}_{t},B,t)$9:         Compute
    $\boldsymbol{\theta}^{\prime}_{t}$10:         if $\mathcal{L}_{D^{test}_{\mathcal{T}_{t}}}(\boldsymbol{\theta}^{\prime}_{t})<\gamma$ then11:              Save
    $|D_{t}|$ as the efficiency for task $\mathcal{T}_{t}$12:         end if13:     end while14:     Save
    final performance $\mathcal{L}_{D^{test}_{\mathcal{T}_{t}}}$($\boldsymbol{\theta}^{\prime}_{t}$)15:     $\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}$16:end for
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Online MAML is a robust technique for online-learning (Finn et al.,
    [2019](#bib.bib16)). A downside of this approach is the computational costs that
    keep growing over time, as all encountered data are stored. Reducing these costs
    is a direction for future work. Also, one could experiment with how well the approach
    works when more than one inner gradient update steps per task are used, as mentioned
    by Finn et al. ([2019](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.11 LLAMA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grant et al. ([2018](#bib.bib23)) mold MAML into a probabilistic framework,
    such that a probability distribution over task-specific parameters $\boldsymbol{\theta}_{j}^{\prime}$
    is learned, instead of a single one. In this way, multiple potential solutions
    can be obtained for a task. The resulting technique is called LLAMA (Laplace Approximation
    for Meta-Adaptation). Importantly, LLAMA is only developed for supervised learning
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: A key observation is that a neural network $f_{\boldsymbol{\theta}^{\prime}_{j}}$,
    parameterized by updated parameters $\boldsymbol{\theta}^{\prime}_{j}$ (obtained
    from few gradient updates using $D^{tr}_{\mathcal{T}_{j}}$), outputs class probabilities
    $p(y_{i}|\boldsymbol{x}_{i},\boldsymbol{\theta}^{\prime}_{j})$. To minimize the
    error on the query set $D^{test}_{\mathcal{T}_{j}}$, the model must output large
    probability scores for the true classes. This objective is captured in the maximum
    log-likelihood loss function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\theta}^{\prime}_{j})=-\sum_{\boldsymbol{x}_{i},y_{i}\in
    D^{test}_{\mathcal{T}_{j}}}log\,p(y_{i}&#124;\boldsymbol{x}_{i},\boldsymbol{\theta}^{\prime}_{j}).$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: Simply put, if we see a task $j$ as a probability distribution over examples
    $p_{\mathcal{T}_{j}}$, we wish to maximize the probability that the model predicts
    the correct class $y_{i}$, given an input $\boldsymbol{x}_{i}$. This can be done
    by plain gradient descent, as shown in [Algorithm 5](#alg5 "Algorithm 5 ‣ 5.11
    LLAMA ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning"),
    where $\beta$ is the meta-learning rate. Line 4 refers to ML-LAPLACE, which is
    a subroutine that computes task-specific updated parameters $\boldsymbol{\theta}^{\prime}_{j}$,
    and estimates the negative log likelihood (loss function) which is used to update
    the initialization $\boldsymbol{\theta}$, as shown in [Algorithm 6](#alg6 "Algorithm
    6 ‣ 5.11 LLAMA ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning").
    Grant et al. ([2018](#bib.bib23)) approximated the quadratic curvature matrix
    $\hat{H}$ using K-FAC (Martens and Grosse, [2015](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: The trick is that the initialization $\boldsymbol{\theta}$ defines a distribution
    $p(\boldsymbol{\theta}^{\prime}_{j}|\boldsymbol{\theta})$ over task-specific parameters
    $\boldsymbol{\theta}^{\prime}_{j}$. This distribution was taken to be a diagonal
    Gaussian (Grant et al., [2018](#bib.bib23)). Then, to sample solutions for a new
    task $\mathcal{T}_{j}$, one can simply generate possible solutions $\boldsymbol{\theta}^{\prime}_{j}$
    from the learned Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5 LLAMA by Grant et al. ([2018](#bib.bib23))
  prefs: []
  type: TYPE_NORMAL
- en: '1:Initialize $\boldsymbol{\theta}$ randomly2:while not converged do3:     Sample
    a batch of $J$ tasks: $B=\mathcal{T}_{1},\ldots,\mathcal{T}_{J}\backsim p(\mathcal{T})$4:     Estimate
    $\mathbb{E}_{(\boldsymbol{x}_{i},y_{i})\backsim p_{\mathcal{T}_{j}}}[-log\,p(y_{i}|\boldsymbol{x}_{i},\boldsymbol{\theta})]\,\forall\mathcal{T}_{j}\in
    B$ using ML-LAPLACE5:     $\boldsymbol{\theta}=\boldsymbol{\theta}-\beta\nabla_{\boldsymbol{\theta}}\sum_{j}\mathbb{E}_{(\boldsymbol{x}_{i},y_{i})\backsim
    p_{\mathcal{T}_{j}}}[-log\,p(y_{i}|\boldsymbol{x}_{i},\boldsymbol{\theta})$6:end while'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 6 ML-LAPLACE (Grant et al., [2018](#bib.bib23))
  prefs: []
  type: TYPE_NORMAL
- en: 1:$\boldsymbol{\theta}^{\prime}_{j}=\boldsymbol{\theta}$2:for $k=1,\ldots,K$ do3:     $\boldsymbol{\theta}^{\prime}_{j}=\boldsymbol{\theta}^{\prime}_{j}+\alpha\nabla_{\boldsymbol{\theta}^{\prime}_{j}}log\,p(y_{i}\in
    D^{tr}_{\mathcal{T}_{j}}|\boldsymbol{\theta}^{\prime}_{j},\boldsymbol{x}_{i}\in
    D^{tr}_{\mathcal{T}_{j}})$4:end for5:Compute curvature matrix $\hat{H}=\nabla_{\boldsymbol{\theta}^{\prime}_{j}}^{2}[-log\,p(y_{i}\in
    D^{test}_{\mathcal{T}_{j}}|\boldsymbol{\theta}^{\prime}_{j},\boldsymbol{x}_{i}\in
    D^{test}_{\mathcal{T}_{j}})]+\nabla_{\boldsymbol{\theta}^{\prime}_{j}}^{2}[-log\,p(\boldsymbol{\theta}^{\prime}_{j}|\boldsymbol{\theta})]$6:return
    $-log\,p(y_{i}\in D^{test}_{\mathcal{T}_{j}}|\boldsymbol{\theta}^{\prime}_{j},\boldsymbol{x}_{i}\in
    D^{test}_{\mathcal{T}_{j}})+\eta\,log[det(\hat{H})]$
  prefs: []
  type: TYPE_NORMAL
- en: In short, LLAMA extends MAML in a probabilistic fashion, such that one can obtain
    multiple solutions for a single task, instead of one. This does, however, increase
    the computational costs. On top of that, the used Laplace approximation (in ML-LAPLACE)
    can be quite inaccurate (Grant et al., [2018](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.12 PLATIPUS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PLATIPUS (Finn et al., [2018](#bib.bib15)) builds upon the probabilistic interpretation
    of LLAMA (Grant et al., [2018](#bib.bib23)), but learns a probability distribution
    over initializations $\boldsymbol{\theta}$, instead of task-specific parameters
    $\boldsymbol{\theta}_{j}^{\prime}$. Thus, PLATIPUS allows one to sample an initialization
    $\boldsymbol{\theta}\backsim p(\boldsymbol{\theta})$, which can be updated with
    gradient descent to obtain task-specific weights (fast weights) $\boldsymbol{\theta}_{j}^{\prime}$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 7 PLATIPUS training algorithm by Finn et al. ([2018](#bib.bib15))
  prefs: []
  type: TYPE_NORMAL
- en: 1:Initialize $\boldsymbol{\Theta}=\{\boldsymbol{\mu}_{\boldsymbol{\theta}},\boldsymbol{\sigma}^{2}_{\boldsymbol{\theta}},\boldsymbol{v}_{q},\boldsymbol{\gamma}_{p},\boldsymbol{\gamma}_{q}\}$2:while Not
    done do3:     Sample batch of tasks $B=\{\mathcal{T}_{j}\backsim p(\mathcal{T})\}_{i=1}^{m}$4:     for $\mathcal{T}_{j}\in
    B$ do5:         $D^{tr}_{\mathcal{T}_{j}},D^{test}_{\mathcal{T}_{j}}=\mathcal{T}_{j}$6:         Compute
    $\nabla_{\boldsymbol{\mu}_{\boldsymbol{\theta}}}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\mu}_{\boldsymbol{\theta}})$7:         Sample
    $\boldsymbol{\theta}\backsim q=N(\boldsymbol{\mu}_{\boldsymbol{\theta}}-\boldsymbol{\gamma}_{q}\nabla_{\boldsymbol{\mu}_{\boldsymbol{\theta}}}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\mu}_{\boldsymbol{\theta}}),\boldsymbol{v}_{q})$8:         Compute
    $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})$9:         Compute
    fast weights $\boldsymbol{\theta}^{\prime}_{i}=\boldsymbol{\theta}-\alpha\nabla_{\boldsymbol{\theta}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\theta})$10:     end for11:     $p(\boldsymbol{\theta}|D^{tr}_{\mathcal{T}_{j}})=N(\boldsymbol{\mu}_{\boldsymbol{\theta}}-\boldsymbol{\gamma}_{p}\nabla_{\boldsymbol{\mu}_{\boldsymbol{\theta}}}\mathcal{L}_{D^{tr}_{\mathcal{T}_{j}}}(\boldsymbol{\mu}_{\boldsymbol{\theta}}),\boldsymbol{\sigma}^{2}_{\boldsymbol{\theta}})$12:     Compute
    $\nabla_{\boldsymbol{\Theta}}\left[\sum_{\mathcal{T}_{j}}\mathcal{L}_{D^{test}_{\mathcal{T}_{j}}}(\boldsymbol{\phi}_{i})+D_{\mathit{KL}}(q(\boldsymbol{\theta}|D^{test}_{\mathcal{T}_{j}}),p(\boldsymbol{\theta}|D^{tr}_{\mathcal{T}_{j}}))\right]$13:     Update
    $\boldsymbol{\Theta}$ using the Adam optimizer14:end while
  prefs: []
  type: TYPE_NORMAL
- en: The approach is best explained by its pseudocode, as shown in [Algorithm 7](#alg7
    "Algorithm 7 ‣ 5.12 PLATIPUS ‣ 5 Optimization-based Meta-Learning ‣ A Survey of
    Deep Meta-Learning"). In contrast to the original MAML, PLATIPUS introduces five
    more parameter vectors (line 1). All of these parameters are used to facilitate
    the creation of Gaussian distributions over prior initializations (or simply priors)
    $\boldsymbol{\theta}$. That is, $\boldsymbol{\mu}_{\boldsymbol{\theta}}$ represents
    the vector mean of the distributions. $\boldsymbol{\sigma}^{2}_{\boldsymbol{q}}$,
    and $\boldsymbol{v}_{q}$ represent the covariances of train and test distributions
    respectively. $\boldsymbol{\gamma}_{x}$ for $x=q,p$ are learning rate vectors
    for performing gradient steps on distributions $q$ (line 6 and 7) and $P$ (line
    11).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key difference with the regular MAML is that instead of having a single
    initialization point $\boldsymbol{\theta}$, we now learn distributions over priors:
    $q$ and $P$, which are based on query and support data sets of task $\mathcal{T}_{j}$
    respectively. Since these data sets come from the same task, we want the distributions
    $q(\boldsymbol{\theta}|D^{test}_{\mathcal{T}_{j}})$, and $p(\boldsymbol{\theta}|D^{tr}_{\mathcal{T}_{j}})$
    to be close to each other. This is enforced by the Kullback–Leibler divergence
    ($D_{\mathit{KL}}$) loss term on line 12, which measures the distance between
    the two distributions. Importantly, note that $q$ (line 7) and $P$ (line 11) use
    vector means which are computed with one gradient update steps using the query
    and support data sets respectively. The idea is that the mean of the Gaussian
    distributions should be close to the updated mean $\boldsymbol{\mu}_{\boldsymbol{\theta}}$
    because we want to enable fast learning. As one can see, the training process
    is very similar to that of MAML (Finn et al., [2017](#bib.bib14)) ([Section 5.5](#S5.SS5
    "5.5 MAML ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning")),
    with some small adjustments to allow us to work with the probability distributions
    over $\boldsymbol{\theta}$.'
  prefs: []
  type: TYPE_NORMAL
- en: At test-time, one can simply sample a new initialization $\boldsymbol{\theta}$
    from the prior distribution $p(\boldsymbol{\theta}|D^{tr}_{\mathcal{T}_{j}})$
    (note that $q$ cannot be used at test-time as we do not have access to $D^{test}_{\mathcal{T}_{j}}$),
    and apply a gradient update on the provided support set $D^{tr}_{\mathcal{T}_{j}}$.
    Note that this allows us to sample multiple potential initializations $\boldsymbol{\theta}$
    for the given task.
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of PLATIPUS is that it is aware of its uncertainty, which
    greatly increases the applicability of Deep Meta-Learning in critical domains
    such as medical diagnosis (Finn et al., [2018](#bib.bib15)). Based on this uncertainty,
    it can ask for labels of some inputs it is unsure about (active learning). A downside
    to this approach, however, is the increased computational costs, and the fact
    that it is not applicable to reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.13 Bayesian MAML (BMAML)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayesian MAML (Yoon et al., [2018](#bib.bib92)) is another probabilistic variant
    of MAML that can generate multiple solutions. However, instead of learning a distribution
    over potential solutions, BMAML simply keeps $M$ possible solutions, and optimizes
    them in joint fashion. Recall that probabilistic MAMLs (e.g., PLATIPUS) attempt
    to maximize the data likelihood of task $\mathcal{T}_{j}$, i.e., $p(\boldsymbol{y}^{test}_{j}|\boldsymbol{\theta}^{\prime}_{j})$,
    where $\boldsymbol{\theta}^{\prime}_{j}$ are task-specific fast weights obtained
    by one or more gradient updates. Yoon et al. ([2018](#bib.bib92)) model this likelihood
    using Stein Variational Gradient Descent (SVGD) (Liu and Wang, [2016](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: To obtain $M$ solutions, or equivalently, parameter settings $\boldsymbol{\theta}^{m}$,
    SVGD keeps a set of $M$ particles $\boldsymbol{\Theta}=\{\boldsymbol{\theta}^{m}\}_{i=1}^{M}$.
    At iteration $t$, every $\boldsymbol{\theta}_{t}\in\boldsymbol{\Theta}$ is updated
    as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\epsilon(\phi(\boldsymbol{\theta}_{t}))$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{ where }\phi(\boldsymbol{\theta}_{t})=\frac{1}{M}\sum_{m=1}^{M}\left[k(\boldsymbol{\theta}^{m}_{t},\boldsymbol{\theta}_{t})\nabla_{\boldsymbol{\theta}^{m}_{t}}log\,p(\boldsymbol{\theta}_{t}^{m})+\nabla_{\boldsymbol{\theta}_{t}^{m}}k(\boldsymbol{\theta}^{m}_{t},\boldsymbol{\theta}_{t})\right].$
    |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: Here, $k(\boldsymbol{x},\boldsymbol{x}^{\prime})$ is a similarity kernel between
    $\boldsymbol{x}$ and $\boldsymbol{x}^{\prime}$. The authors used a radial basis
    function (RBF) kernel, but in theory, any other kernel could be used. Note that
    the update of one particle depends on the other gradients of particles. The first
    term in the summation ($k(\boldsymbol{\theta}^{m}_{t},\boldsymbol{\theta}_{t})\nabla_{\boldsymbol{\theta}^{m}_{t}}log\,p(\boldsymbol{\theta}_{t}^{m})$)
    moves the particle in the direction of the gradients of other particles, based
    on particle similarity. The second term ($\nabla_{\boldsymbol{\theta}_{t}^{m}}k(\boldsymbol{\theta}^{m}_{t},\boldsymbol{\theta}_{t})$)
    ensures that particles do not collapse (repulsive force) (Yoon et al., [2018](#bib.bib92)).
  prefs: []
  type: TYPE_NORMAL
- en: These particles can then be used to approximate the probability distribution
    of the test labels
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(\boldsymbol{y}^{test}_{j}&#124;\boldsymbol{\theta}^{\prime}_{j})\approx\frac{1}{M}\sum_{m=1}^{M}p(\boldsymbol{y}_{j}^{test}&#124;\boldsymbol{\theta}^{m}_{\mathcal{T}_{j}}),$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\theta}_{\mathcal{T}_{j}}^{m}$ is the $m$-th particle obtained
    by training on the support set $D^{tr}_{\mathcal{T}_{j}}$ of task $\mathcal{T}_{j}$.
  prefs: []
  type: TYPE_NORMAL
- en: Yoon et al. ([2018](#bib.bib92)) proposed a new meta-loss to train BMAML, called
    the Chaser Loss. This loss relies on the insight that we want the approximated
    parameter distribution (obtained from the support set $p^{n}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{\mathcal{T}_{j}}|D^{tr},\boldsymbol{\Theta}_{0})$)
    and true distribution $p^{\infty}_{\mathcal{T}_{j}}(\boldsymbol{\theta}_{\mathcal{T}_{j}}|D^{tr}\cup
    D^{test})$ to be close to each other (since the task is the same). Here, $n$ denotes
    the number of SVGD steps, and $\boldsymbol{\Theta}_{0}$ is the set of initial
    particles, in similar fashion to the initial parameters $\boldsymbol{\theta}$
    seen by MAML. Since the true distribution is unknown, Yoon et al. ([2018](#bib.bib92))
    approximate it by running SVGD for $s$ additional steps, granting us the leader
    $\boldsymbol{\Theta}^{n+s}_{\mathcal{T}_{j}}$, where the $s$ additional steps
    are performed on the combined support and query set. The intuition is that as
    the number of updates increases, the obtained distributions become more like the
    true ones. $\boldsymbol{\Theta}^{n}_{\mathcal{T}_{j}}$ in this context is called
    the chaser as it wants to get closer to the leader. The proposed meta-loss is
    then given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{BMAML}(\boldsymbol{\Theta}_{0})=\sum_{\mathcal{T}_{j}\in
    B}\sum_{m=1}^{M}&#124;&#124;\boldsymbol{\theta}_{\mathcal{T}_{j}}^{n,m}-\boldsymbol{\theta}_{\mathcal{T}_{j}}^{n+s,m}&#124;&#124;^{2}_{2}.$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: The full pseudocode of BMAML is shown in [Algorithm 8](#alg8 "Algorithm 8 ‣
    5.13 Bayesian MAML (BMAML) ‣ 5 Optimization-based Meta-Learning ‣ A Survey of
    Deep Meta-Learning"). Here, $\boldsymbol{\Theta}^{n}_{\mathcal{T}_{j}}(\boldsymbol{\Theta}_{0})$
    denotes the set of particles after $n$ updates on task $\mathcal{T}_{j}$, and
    $SG$ means “stop gradients" (we do not want the leader to depend on the initialization,
    as the leader must lead).
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 8 BMAML by Yoon et al. ([2018](#bib.bib92))
  prefs: []
  type: TYPE_NORMAL
- en: 1:Initialize $\boldsymbol{\Theta}_{0}$2:for $t=1,\ldots$ until convergence do3:     Sample
    a batch of tasks B from $p(\mathcal{T})$4:     for task $\mathcal{T}_{j}\in B$ do5:         Compute
    chaser $\boldsymbol{\Theta}^{n}_{\mathcal{T}_{j}}(\boldsymbol{\Theta}_{0})=SVGD_{n}(\boldsymbol{\Theta}_{0};D^{tr}_{\mathcal{T}_{j}},\alpha)$6:         Compute
    leader $\boldsymbol{\Theta}^{n+s}_{\mathcal{T}_{j}}(\boldsymbol{\Theta}_{0})=SVGD_{s}(\boldsymbol{\Theta}^{n}_{\mathcal{T}_{j}}(\boldsymbol{\Theta}_{0});D^{tr}_{\mathcal{T}_{j}}\cup
    D^{test}_{\mathcal{T}_{j}},\alpha)$7:     end for8:     $\boldsymbol{\Theta}_{0}=\boldsymbol{\Theta}_{0}-\beta\nabla_{\boldsymbol{\Theta}_{0}}\sum_{\mathcal{T}_{j}\in
    B}d(\boldsymbol{\Theta}^{n}_{\mathcal{T}_{j}}(\boldsymbol{\Theta}_{0}),SG(\boldsymbol{\Theta}^{n+s}_{\mathcal{T}_{j}}(\boldsymbol{\Theta}_{0})))$9:end for
  prefs: []
  type: TYPE_NORMAL
- en: In summary, BMAML is a robust optimization-based meta-learning technique that
    can propose $M$ potential solutions to a task. Additionally, it is applicable
    to reinforcement learning by using Stein Variational Policy Gradient instead of
    SVGD. A downside of this approach is that one has to keep $M$ parameter sets in
    memory, which does not scale well. Reducing the memory costs is a direction for
    future work (Yoon et al., [2018](#bib.bib92)). Furthermore, SVGD is sensitive
    to the selected kernel function, which was pre-defined in BMAML. However, Yoon
    et al. ([2018](#bib.bib92)) point out that it may be beneficial to learn the kernel
    function instead. This is another possibility for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 5.14 Simple Differentiable Solvers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bertinetto et al. ([2019](#bib.bib7)) take a quite different approach. That
    is, they pick simple base-learners that have an analytical closed-form solution.
    The intuition is that the existence of a closed-form solution allows for good
    learning efficiency. They propose two techniques using this principle, namely
    R2-D2 (Ridge Regression Differentiable Discriminator), and LR-D2 (Logistic Regression
    Differentiable Discriminator). We cover both in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Let $g_{\boldsymbol{\phi}}:X\rightarrow\mathbb{R}^{e}$ be a pre-trained input
    embedding model (e.g. a CNN), which outputs embeddings with a dimensionality of
    $e$. Furthermore, assume that we use a linear predictor function $f(g_{\boldsymbol{\phi}}(\boldsymbol{x}_{i}))=g_{\boldsymbol{\phi}}(\boldsymbol{x}_{i})W$,
    where $W$ is a $e\times o$ weight matrix, and $o$ is the output dimensionality
    (of the label). When using (regularized) Ridge Regression (done by R2-D2), one
    uses the optimal $W$, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle W^{*}$ | $\displaystyle=\operatorname*{arg\,min}_{W}\,&#124;&#124;XW-Y&#124;&#124;^{2}_{2}+\gamma&#124;&#124;W&#124;&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(X^{T}X+\gamma I)^{-1}X^{T}Y,$ |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: where $X\in\mathbb{R}^{n\times e}$ is the input matrix, containing $n$ rows
    (one for each embedded input $g_{\boldsymbol{\phi}}(\boldsymbol{x}_{i})$), $Y\in\mathbb{R}^{n\times
    o}$ is the output matrix with correct outputs corresponding to the inputs, and
    $\gamma$ is a regularization term to prevent overfitting. Note that the analytical
    solution contains the term $(X^{T}X)\in\mathbb{R}^{e\times e}$, which is quadratic
    in the size of the embeddings. Since $e$ can become quite large when using deep
    neural networks, Bertinetto et al. ([2019](#bib.bib7)) use Woodburry’s identity
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle W^{*}=X^{T}(XX^{T}+\gamma I)^{-1}Y,$ |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: where $XX^{T}\in\mathbb{R}^{n\times n}$ is linear in the embedding size, and
    quadratic in the number of examples, which is more manageable in few-shot settings,
    where $n$ is very small. To make predictions with this Ridge Regression based
    model, one can compute
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{Y}=\alpha X_{test}W^{*}+\beta,$ |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ and $\beta$ are hyperparameters of the base-learner that can
    be learned by the meta-learner, and $X_{test}\in\mathbb{R}^{m\times e}$ corresponds
    to the $m$ test inputs of a given task. Thus, the meta-learner needs to learn
    $\alpha,\beta,\gamma$, and $\boldsymbol{\phi}$ (embedding weights of the CNN).
  prefs: []
  type: TYPE_NORMAL
- en: The technique can also be applied to iterative solvers when the optimization
    steps are differentiable (Bertinetto et al., [2019](#bib.bib7)). LR-D2 uses the
    Logistic Regression objective and Newton’s method as solver. Outputs $\boldsymbol{y}\in\{-1,+1\}^{n}$
    are now binary. Let $\boldsymbol{w}$ denote a parameter row of our linear model
    (parameterized by $W$). Then, the $i$-th iteration of Newton’s method updates
    $\boldsymbol{w}_{i}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{w}_{i}=(X^{T}\mbox{diag}(\boldsymbol{s}_{i})X+\gamma
    I)^{-1}X^{T}\mbox{diag}(\boldsymbol{s}_{i})\boldsymbol{z}_{i},$ |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\mu}_{i}=\sigma(\boldsymbol{w}^{T}_{i-1}X)$, $\boldsymbol{s}_{i}=\boldsymbol{\mu}_{i}(1-\boldsymbol{\mu}_{i})$,
    $\boldsymbol{z}_{i}=\boldsymbol{w}^{T}_{i-1}X+(\boldsymbol{y}-\boldsymbol{\mu}_{i})/\boldsymbol{s}_{i}$,
    and $\sigma$ is the sigmoid function. Since the term $X^{T}\mbox{diag}(\boldsymbol{s}_{i})X$
    is a matrix of size $e\times e$, and thus again quadratic in the embedding size,
    Woodburry’s identity is also applied here to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{w}_{i}=X^{T}(XX^{T}+\lambda\mbox{diag}(\boldsymbol{s}_{i})^{-1})^{-1}\boldsymbol{z}_{i},$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: making it quadratic in the input size, which is not a big problem since $n$
    is small in the few-shot setting. The main difference compared to R2-D2 is that
    the base-solver has to be run for multiple iterations to obtain $W$.
  prefs: []
  type: TYPE_NORMAL
- en: In the few-shot setting, the base-level optimizers compute the weight matrix
    $W$ for a given task $\mathcal{T}_{i}$. The obtained loss on the query set of
    a task $\mathcal{L}_{D_{test}}$ is then used to update the parameters $\boldsymbol{\phi}$
    of the input embedding function (e.g. CNN) and the hyperparameters of the base-learner.
  prefs: []
  type: TYPE_NORMAL
- en: Lee et al. ([2019](#bib.bib45)) have done similar work to Bertinetto et al.
    ([2019](#bib.bib7)), but with linear Support Vector Machines (SVMs) as base-learner.
    Their approach is dubbed MetaOptNet and achieved state-of-the-art performance
    on few-shot image classification.
  prefs: []
  type: TYPE_NORMAL
- en: In short, simple differentiable solvers are simple, reasonably fast in terms
    of computation time, but limited to few-shot learning settings. Investigating
    the use of other simple base-learners is a direction for future work.
  prefs: []
  type: TYPE_NORMAL
- en: 5.15 Optimization-based Techniques, in conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimization-based aim to learn new tasks quickly through (learned) optimization
    procedures. Note that this closely resembles base-level learning, which also occurs
    through optimization (e.g., gradient descent). However, in contrast to base-level
    techniques, optimization-based meta-learners can learn the optimizer and/or are
    exposed to multiple tasks, which allows them to learn how to learn new tasks quickly.
    [Figure 28](#S5.F28 "Figure 28 ‣ 5.15 Optimization-based Techniques, in conclusion
    ‣ 5 Optimization-based Meta-Learning ‣ A Survey of Deep Meta-Learning") shows
    the relationships between the covered optimization-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da5278a5c5f3709afe144485b7f0251c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: The relationships between the covered optimization-based meta-learning
    techniques. As one can see, MAML has a central position in this graph of techniques
    as it has inspired many other works.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the LSTM optimizer (Andrychowicz et al., [2016](#bib.bib2)),
    which replaces hand-crafted optimization procedures such as gradient descent by
    a trainable LSTM, can be seen as the starting point for these optimization-based
    meta-learning techniques. Li and Malik ([2018](#bib.bib46)) also aim to learn
    the optimization procedure with reinforcement learning instead of gradient-based
    methods. The LSTM meta-learner (Ravi and Larochelle, [2017](#bib.bib65)) extends
    the LSTM optimizer to the few-shot setting by not only learning the optimization
    procedure but also a good set of initial weights. This way, it can be used across
    tasks. MAML (Finn et al., [2017](#bib.bib14)) is a simplification of the LSTM
    meta-learner as it replaces the trainable LSTM optimizer by hand-crafted gradient
    descent. MAML has received considerable attention within the field of deep meta-learning,
    and has, as one can see, inspired many other works.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-SGD is an enhancement of MAML that not only learns the initial parameters,
    but also the learning rates (Li et al., [2017](#bib.bib47)). LLAMA (Grant et al.,
    [2018](#bib.bib23)), PLATIPUS (Finn et al., [2018](#bib.bib15)), and online MAML
    (Finn et al., [2019](#bib.bib16)) extend MAML to the active and online learning
    settings. LLAMA and PLATIPUS are probabilistic interpretations of MAML, which
    allow them to sample multiple solutions for a given task and quantify their uncertainty.
    BMAML (Yoon et al., [2018](#bib.bib92)) takes a more discrete approach as it jointly
    optimizes a discrete set of $M$ initializations. iMAML (Rajeswaran et al., [2019](#bib.bib64))
    aims to overcome the computational expenses associated with the computation of
    second-order derivatives, which is needed by MAML. Through implicit differentiation,
    they also allow for the use of non-differentiable inner loop optimization procedures.
    Reptile (Nichol et al., [2018](#bib.bib58)) is an elegant first-order meta-learning
    algorithm for finding a set of initial parameters and removes the need for computing
    higher-order derivatives. LEO (Rusu et al., [2018](#bib.bib67)) tries to improve
    the robustness of MAML by optimizing in lower-dimensional parameter space through
    the use of an encoder-decoder architecture. Lastly, R2-D2, LR-D2 (Bertinetto et al.,
    [2019](#bib.bib7)), and Lee et al. ([2019](#bib.bib45)) use simple classical machine
    learning methods (ridge regression, logistic regression, SVM, respectively) as
    a classifier on top of a learned feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of optimization-based approaches is that they can achieve better
    performance on wider task distributions than, e.g., model-based approaches (Finn
    and Levine, [2018](#bib.bib13)). However, optimization-based techniques optimize
    a base-learner for every task that they are presented with and/or learn the optimization
    procedure, which is computationally expensive (Hospedales et al., [2020](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: Optimization-based meta-learning is a very active area of research. We expect
    future work to be done in order to reduce the computational demands of these methods
    and improve the solution quality and level of generalization. We think that benchmarking
    and reproducibility research will play an important role in these improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Concluding Remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we give a helicopter view of all that we discussed, and the
    field of Deep Meta-Learning in general. We will also discuss challenges and future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, there has been a shift in focus in the broad meta-learning
    community. Traditional algorithm selection and hyperparameter optimization for
    classical machine learning techniques (e.g. Support Vector Machines, Logistic
    Regression, Random Forests, etc.) have been augmented by Deep Meta-Learning, or
    equivalently, the pursuit of self-improving neural networks that can leverage
    prior learning experience to learn new tasks more quickly. Instead of training
    a new model from scratch for different tasks, we can use the same (meta-learning)
    model across tasks. As such, meta-learning can widen the applicability of powerful
    deep learning techniques to domains where fewer data are available and computational
    resources are limited.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Meta-Learning techniques are characterized by their meta-objective, which
    allows them to maximize performance across various tasks, instead of a single
    one, as is the case in base-level learning objectives. This meta-objective is
    reflected in the training procedure of meta-learning methods, as they learn on
    a set of different meta-training tasks. The few-shot setting lends itself nicely
    towards this end, as tasks consist of few data points. This makes it computationally
    feasible to train on many different tasks, and it allows us to evaluate whether
    a neural network can learn new concepts from few examples. Task construction for
    training and evaluation does require some special attention. That is, it has been
    shown beneficial to match training and test conditions (Vinyals et al., [2016](#bib.bib86)),
    and perhaps train in a more difficult setting than the one that will be used for
    evaluation (Snell et al., [2017](#bib.bib74)).
  prefs: []
  type: TYPE_NORMAL
- en: On a high level, there are three categories of Deep Meta-Learning techniques,
    namely i) metric-, ii) model-, and iii) optimization-based ones, which rely on
    i) computing input similarity, ii) task embeddings with states, and iii) task-specific
    updates, respectively. Each approach has strengths and weaknesses. Metric-learning
    techniques are simple and effective (Garcia and Bruna, [2017](#bib.bib17)) but
    are not readily applicable outside of the supervised learning setting (Hospedales
    et al., [2020](#bib.bib32)). Model-based techniques, on the other hand, can have
    very flexible internal dynamics, but lack generalization ability to more distant
    tasks than the ones used at meta-train time (Finn and Levine, [2018](#bib.bib13)).
    Optimization-based approaches have shown greater generalizability, but are in
    general computationally expensive, as they optimize a base-learner for every task
    (Finn and Levine, [2018](#bib.bib13); Hospedales et al., [2020](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2](#S2.T2 "Table 2 ‣ 2.4 Overview of the rest of this Work ‣ 2 Foundation
    ‣ A Survey of Deep Meta-Learning") provides a concise, tabular overview of these
    approaches. Many techniques have been proposed for each one of the categories,
    and the underlying ideas may vary greatly, even within the same category. [Table 3](#S2.T3
    "Table 3 ‣ 2.4 Overview of the rest of this Work ‣ 2 Foundation ‣ A Survey of
    Deep Meta-Learning"), therefore, provides an overview of all methods and key ideas
    that we have discussed in this work, together with their applicability to supervised
    learning (SL) and reinforcement learning (RL) settings, key ideas, and benchmarks
    that were used for testing them. [Table 5](#S6.T5 "Table 5 ‣ 6.1 Overview ‣ 6
    Concluding Remarks ‣ A Survey of Deep Meta-Learning") displays an overview of
    the 1- and 5-shot classification performances (reported by the original authors)
    of the techniques on the frequently used miniImageNet benchmark. Moreover, it
    displays the used backbone (feature extraction module) as well as the final classification
    mechanism. From this table, it becomes clear that the 5-shot performance is typically
    better than the 1-shot performance, indicating that data scarcity is a large bottleneck
    for achieving good performance. Moreover, there is a strong relationship between
    the expressivity of the backbone and the performance. That is, deeper backbones
    tend to give rise to better classification performance. The best performance is
    achieved by MetaOptNet, yielding a 1-shot accuracy of $64.09$% and a 5-shot accuracy
    of $80.00$%. Note however that MetaOptNet used a deeper backbone than most of
    the other techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Backbone | Classifier | 1-shot | 5-shot |'
  prefs: []
  type: TYPE_TB
- en: '| Metric-based |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Siamese nets | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| Matching nets | 64-64-64-64 | Cosine sim. | $43.56\pm 0.84$ | $55.31\pm 0.73$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prototypical nets | 64-64-64-64 | Euclidean dist. | $49.42\pm 0.78$ | $68.20\pm
    0.66$ |'
  prefs: []
  type: TYPE_TB
- en: '| Relation nets | 64-96-128-256 | Sim. network | $50.44\pm 0.82$ | $65.32\pm
    0.70$ |'
  prefs: []
  type: TYPE_TB
- en: '| ARC | - | 64-1 dense | $49.14\pm-$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN | 64-96-128-256 | Softmax | $50.33\pm 0.36$ | $66.41\pm 0.63$ |'
  prefs: []
  type: TYPE_TB
- en: '| Model-based |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| RMLs | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| MANNs | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| Meta nets | 64-64-64-64-64 | 64-Softmax | $49.21\pm 0.96$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| SNAIL | Adj. ResNet-12 | Softmax | $55.71\pm 0.99$ | $68.88\pm 0.92$ |'
  prefs: []
  type: TYPE_TB
- en: '| CNP | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| Neural stat. | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| Opt.-based |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM optimizer | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM ml. | 32-32-32-32 | Softmax | $43.44\pm 0.77$ | $60.60\pm 0.71$ |'
  prefs: []
  type: TYPE_TB
- en: '| RL optimizer | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| MAML | 32-32-32-32 | Softmax | $48.70\pm 1.84$ | $63.11\pm 0.92$ |'
  prefs: []
  type: TYPE_TB
- en: '| iMAML | 64-64-64-64 | Softmax | $49.30\pm 1.88$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Meta-SGD | 64-64-64-64 | Softmax | $50.47\pm 1.87$ | $64.03\pm 0.94$ |'
  prefs: []
  type: TYPE_TB
- en: '| Reptile | 32-32-32-32 | Softmax | $48.21\pm 0.69$ | $66.00\pm 0.62$ |'
  prefs: []
  type: TYPE_TB
- en: '| LEO | WRN-28-10 | Softmax | $61.76\pm 0.08$ | $77.59\pm 0.12$ |'
  prefs: []
  type: TYPE_TB
- en: '| Online MAML | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA | 64-64-64-64 | Softmax | $49.40\pm 1.83$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| PLATIPUS | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| BMAML | 64-64-64-64-64 | Softmax | $53.80\pm 1.46$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Diff. solvers |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|           R2-D2 | 96-192-384-512 | Ridge regr. | $51.8\pm 0.2$ | $68.4\pm
    0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '|           LR-D2 | 96-192-384-512 | Log. regr. | $51.90\pm 0.20$ | $68.70\pm
    0.20$ |'
  prefs: []
  type: TYPE_TB
- en: '|           MetaOptNet | ResNet-12 | SVM | $\boldsymbol{64.09\pm 0.62}$ | $\boldsymbol{80.00\pm
    0.45}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Comparison of the accuracy scores of the covered meta-learning techniques
    on 1- and 5-shot miniImageNet classification. Scores are taken from the original
    papers. The $\pm$ indicates the 95% confidence interval. The backbone is the used
    feature extraction module. The classifier column shows the final layer(s) that
    were used to transform the features into class predictions. Used abbreviations:
    “sim.": similarity, “Adj.": adjusted, and “dist.": distance, “log.": logistic,
    “regr.": regression, “ml.": meta-learner, “opt.": optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Open Challenges and Future Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the great potential of Deep Meta-Learning techniques, there are still
    open challenges, which we discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Meta-Learning")
    in Section [1](#S1 "1 Introduction ‣ A Survey of Deep Meta-Learning") displays
    the accuracy scores of the covered meta-learning techniques on 1-shot miniImageNet
    classification. Techniques that were not tested in this setting by the original
    authors are omitted. As we can see, the performance of the techniques is related
    to the expressivity of the used backbone (ordered in increasing order on the x-axis).
    For example, the best-performing techniques, LEO and MetaOptNet, use the largest
    network architectures. Moreover, the fact that different techniques use different
    backbones poses a problem as it is difficult to fairly compare their classification
    performance. An obvious question arises to which degree the difference in performance
    is due to methodological improvements, or due to the fact that a better backbone
    architecture was chosen. For this reason, we think that it would be useful to
    perform a large-scale benchmark test where techniques are compared when they use
    the same backbones. This would also allow us to get a more clear idea of how the
    expressivity of the feature extraction module affects the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge of Deep Meta-Learning techniques is that they can be susceptible
    to the memorization problem (meta-overfitting), where the neural network has memorized
    tasks seen at meta-training time and fails to generalize to new tasks. More research
    is required to better understand this problem. Clever task design and meta-regularization
    may prove useful to avoid such problems (Yin et al., [2020](#bib.bib91)).
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is that most of the meta-learning techniques discussed in this
    work are evaluated on narrow benchmark sets. This means that the data that the
    meta-learner used for training are not too distant from the data used for evaluating
    its performance. As such, one may wonder how well these techniques are able to
    adapt to more distant tasks. Chen et al. ([2019](#bib.bib9)) showed that the ability
    to adapt to new tasks decreases as they become more distant from the tasks seen
    at training time. Moreover, a simple non-meta-learning baseline (based on pre-training
    and fine-tuning) can outperform state-of-the-art meta-learning techniques when
    meta-test tasks come from a different data set than the one used for meta-training.
  prefs: []
  type: TYPE_NORMAL
- en: In reaction to these findings, Triantafillou et al. ([2020](#bib.bib81)) have
    recently proposed the Meta-Dataset benchmark, which consists of various previously
    used meta-learning benchmarks such as Omniglot (Lake et al., [2011](#bib.bib42))
    and ImageNet (Deng et al., [2009](#bib.bib10)). This way, meta-learning techniques
    can be evaluated in more challenging settings where tasks are diverse. Following
    Hospedales et al. ([2020](#bib.bib32)), we think that this new benchmark can prove
    to be a good means towards the investigation and development of meta-learning
    algorithms for such challenging scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier in this section, Deep Meta-Learning has the appealing prospect
    of widening the applicability of deep learning techniques to more real-world domains.
    For this, increasing the generalization ability of these techniques is very important.
    Additionally, the computational costs associated with the deployment of meta-learning
    techniques should be small. While these techniques can learn new tasks quickly,
    meta-training can be quite computationally expensive. Thus, decreasing the required
    computation time and memory costs of Deep Meta-Learning techniques remains an
    open challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Some real-world problems demand systems that can perform well in online, or
    active learning settings. The investigation of Deep Meta-Learning in these settings
    (Finn et al., [2018](#bib.bib15); Yoon et al., [2018](#bib.bib92); Finn et al.,
    [2019](#bib.bib16); Munkhdalai and Yu, [2017](#bib.bib55); Vuorio et al., [2018](#bib.bib87))
    remains an important direction for future work.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another direction for future research is the creation of compositional Deep
    Meta-Learning systems, which instead of learning flat and associative functions
    $\boldsymbol{x}\rightarrow y$, organize knowledge in a compositional manner. This
    would allow them to decompose an input $\boldsymbol{x}$ into several (already
    learned) components $c_{1}(\boldsymbol{x}),\ldots,c_{n}(\boldsymbol{x})$, which
    in turn could help the performance in low-data regimes (Tokmakov et al., [2019](#bib.bib80)).
  prefs: []
  type: TYPE_NORMAL
- en: The question has been raised whether contemporary Deep Meta-Learning techniques
    actually learn how to perform rapid learning, or simply learn a set of robust
    high-level features, which can be (re)used for many (new) tasks. Raghu et al.
    ([2020](#bib.bib63)) investigated this question for the most popular Deep Meta-Learning
    technique MAML and found that it largely relies on feature reuse. It would be
    interesting to see whether we can develop techniques that rely more upon fast
    learning, and what the effect would be on performance.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it may be useful to add more meta-abstraction levels, giving rise to,
    e.g., meta-meta-learning, meta-meta-…-learning (Hospedales et al., [2020](#bib.bib32);
    Schmidhuber, [1987](#bib.bib69)).
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thanks to Herke van Hoof for an insightful discussion on LLAMA. Thanks to Pavel
    Brazdil for his encouragement and feedback on a preliminary version of this work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anderson (2008) Anderson T (2008) The Theory and Practice of Online Learning.
    AU Press, Athabasca University
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrychowicz et al. (2016) Andrychowicz M, Denil M, Colmenarejo SG, Hoffman
    MW, Pfau D, Schaul T, Shillingford B, de Freitas N (2016) Learning to learn by
    gradient descent by gradient descent. In: Advances in Neural Information Processing
    Systems 29, Curran Associates Inc., NIPS’16, pp 3988–3996'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Antoniou et al. (2019) Antoniou A, Edwards H, Storkey A (2019) How to train
    your MAML. In: International Conference on Learning Representations, ICLR’19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barrett et al. (2018) Barrett DG, Hill F, Santoro A, Morcos AS, Lillicrap T
    (2018) Measuring abstract reasoning in neural networks. In: Proceedings of the
    35th International Conference on Machine Learning, JLMR.org, ICML’18, pp 4477–4486'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (1997) Bengio S, Bengio Y, Cloutier J, Gecsei J (1997) On the
    optimization of a synaptic learning rule. In: Optimality in Artificial and Biological
    Neural Networks, Lawrance Erlbaum Associates, Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (1991) Bengio Y, Bengio S, Cloutier J (1991) Learning a synaptic
    learning rule. In: International Joint Conference on Neural Networks, IEEE, IJCNN’91,
    vol 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bertinetto et al. (2019) Bertinetto L, Henriques JF, Torr PHS, Vedaldi A (2019)
    Meta-learning with differentiable closed-form solvers. In: International Conference
    on Learning Representations, ICLR’19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brazdil et al. (2008) Brazdil P, Carrier CG, Soares C, Vilalta R (2008) Metalearning:
    Applications to Data Mining. Springer-Verlag Berlin Heidelberg'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019) Chen WY, Liu YC, Kira Z, Wang YC, Huang JB (2019) A Closer
    Look at Few-shot Classification. In: International Conference on Learning Representations,
    ICLR’19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)
    ImageNet: A Large-Scale Hierarchical Image Database. In: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, IEEE, pp 248–255'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2016) Duan Y, Schulman J, Chen X, Bartlett PL, Sutskever I, Abbeel
    P (2016) RL²: Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv
    preprint arXiv:161102779'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Edwards and Storkey (2017) Edwards H, Storkey A (2017) Towards a Neural Statistician.
    In: International Conference on Learning Representations, ICLR’17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finn and Levine (2018) Finn C, Levine S (2018) Meta-Learning and Universality:
    Deep Representations and Gradient Descent can Approximate any Learning Algorithm.
    In: International Conference on Learning Representations, ICLR’18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finn et al. (2017) Finn C, Abbeel P, Levine S (2017) Model-agnostic Meta-learning
    for Fast Adaptation of Deep Networks. In: Proceedings of the 34th International
    Conference on Machine Learning, JMLR.org, ICML’17, pp 1126–1135'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finn et al. (2018) Finn C, Xu K, Levine S (2018) Probabilistic Model-Agnostic
    Meta-Learning. In: Advances in Neural Information Processing Systems 31, Curran
    Associates Inc., NIPS’18, pp 9516–9527'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finn et al. (2019) Finn C, Rajeswaran A, Kakade S, Levine S (2019) Online Meta-Learning.
    In: Chaudhuri K, Salakhutdinov R (eds) Proceedings of the 36th International Conference
    on Machine Learning, JLMR.org, ICML’19, pp 1920–1930'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garcia and Bruna (2017) Garcia V, Bruna J (2017) Few-Shot Learning with Graph
    Neural Networks. In: International Conference on Learning Representations, ICLR’17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garnelo et al. (2018) Garnelo M, Rosenbaum D, Maddison C, Ramalho T, Saxton
    D, Shanahan M, Teh YW, Rezende D, Eslami SMA (2018) Conditional neural processes.
    In: Dy J, Krause A (eds) Proceedings of the 35th International Conference on Machine
    Learning, JMLR.org, ICML’18, vol 80, pp 1704–1713'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goceri (2019a) Goceri E (2019a) Capsnet topology to classify tumours from brain
    images and comparative evaluation. IET Image Processing 14(5):882–889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goceri (2019b) Goceri E (2019b) Challenges and recent solutions for image segmentation
    in the era of deep learning. In: 2019 ninth international conference on image
    processing theory, tools and applications (IPTA), IEEE, pp 1–6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goceri (2020) Goceri E (2020) Convolutional neural network based desktop applications
    to classify dermatological diseases. In: 2020 IEEE 4th International Conference
    on Image Processing, Applications and Systems (IPAS), IEEE, pp 138–143'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goceri and Karakas (2020) Goceri E, Karakas AA (2020) Comparative evaluations
    of cnn based networks for skin lesion classification. In: 14th International Conference
    on Computer Graphics, Visualization, Computer Vision and Image Processing (CGVCVIP),
    Zagreb, Croatia, pp 1–6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grant et al. (2018) Grant E, Finn C, Levine S, Darrell T, Griffiths T (2018)
    Recasting Gradient-Based Meta-Learning as Hierarchical Bayes. In: International
    Conference on Learning Representations, ICLR’18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves et al. (2014) Graves A, Wayne G, Danihelka I (2014) Neural Turing Machines.
    arXiv preprint arXiv:14105401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2018) Gupta A, Mendonca R, Liu Y, Abbeel P, Levine S (2018) Meta-Reinforcement
    Learning of Structured Exploration Strategies. In: Advances in Neural Information
    Processing Systems 31, Curran Associates Inc., NIPS’18, pp 5302–5311'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hamilton et al. (2017) Hamilton WL, Ying R, Leskovec J (2017) Inductive representation
    learning on large graphs. In: Advances in Neural Information Processing Systems,
    Curran Associates Inc., NIPS’17, vol 30, p 1025–1035'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hannan (1957) Hannan J (1957) Approximation to bayes risk in repeated play.
    Contributions to the Theory of Games 3:97–139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hastie et al. (2009) Hastie T, Tibshirani R, Friedman J (2009) The Elements
    of Statistical Learning: Data Mining, Inference, and Prediction, 2nd edn. Springer,
    New York, NY'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2015) He K, Zhang X, Ren S, Sun J (2015) Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification. In: Proceedings
    of the IEEE International Conference on Computer Vision, pp 1026–1034'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton and Plaut (1987) Hinton GE, Plaut DC (1987) Using Fast Weights to Deblur
    Old Memories. In: Proceedings of the 9th Annual Conference of the Cognitive Science
    Society, pp 177–186'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter et al. (2001) Hochreiter S, Younger AS, Conwell PR (2001) Learning
    to Learn Using Gradient Descent. In: International Conference on Artificial Neural
    Networks, Springer, pp 87–94'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hospedales et al. (2020) Hospedales T, Antoniou A, Micaelli P, Storkey A (2020)
    Meta-Learning in Neural Networks: A Survey. arXiv preprint arXiv:200405439'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iqbal et al. (2018) Iqbal MS, Luo B, Khan T, Mehmood R, Sadiq M (2018) Heterogeneous
    transfer learning techniques for machine learning. Iran Journal of Computer Science
    1(1):31–46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iqbal et al. (2019a) Iqbal MS, El-Ashram S, Hussain S, Khan T, Huang S, Mehmood
    R, Luo B (2019a) Efficient cell classification of mitochondrial images by using
    deep learning. Journal of Optics 48(1):113–122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iqbal et al. (2019b) Iqbal MS, Luo B, Mehmood R, Alrige MA, Alharbey R (2019b)
    Mitochondrial organelle movement classification (fission and fusion) via convolutional
    neural network approach. IEEE Access 7:86570–86577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iqbal et al. (2020) Iqbal MS, Ahmad I, Bin L, Khan S, Rodrigues JJ (2020) Deep
    learning recognition of diseased and normal cell representation. Transactions
    on Emerging Telecommunications Technologies p e4017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jankowski et al. (2011) Jankowski N, Duch W, Grąbczewski K (2011) Meta-Learning
    in Computational Intelligence, vol 358\. Springer-Verlag Berlin Heidelberg
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalai and Vempala (2005) Kalai A, Vempala S (2005) Efficient algorithms for
    online decision problems. Journal of Computer and System Sciences 71(3):291–307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koch et al. (2015) Koch G, Zemel R, Salakhutdinov R (2015) Siamese Neural Networks
    for One-shot Image Recognition. In: Proceedings of the 32nd International Conference
    on Machine Learning, JMLR.org, ICML’15, vol 37'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2009) Krizhevsky A (2009) Learning Multiple Layers of Features from
    Tiny Images. Tech. rep., University of Toronto
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krizhevsky et al. (2012) Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet
    Classification with Deep Convolutional Neural Networks. In: Advances in Neural
    Information Processing Systems, pp 1097–1105'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lake et al. (2011) Lake B, Salakhutdinov R, Gross J, Tenenbaum J (2011) One
    shot learning of simple visual concepts. In: Proceedings of the annual meeting
    of the cognitive science society, vol 33, pp 2568–2573'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lake et al. (2017) Lake BM, Ullman TD, Tenenbaum JB, Gershman SJ (2017) Building
    machines that learn and think like people. Behavioral and brain sciences 40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun et al. (2010) LeCun Y, Cortes C, Burges C (2010) MNIST handwritten digit
    database. [http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist),
    accessed: 7-10-2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) Lee K, Maji S, Ravichandran A, Soatto S (2019) Meta-Learning
    with Differentiable Convex Optimization. In: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, IEEE, pp 10657–10665'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Malik (2018) Li K, Malik J (2018) Learning to Optimize Neural Nets. arXiv
    preprint arXiv:170300441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017) Li Z, Zhou F, Chen F, Li H (2017) Meta-SGD: Learning to Learn
    Quickly for Few-Shot Learning. arXiv preprint arXiv:170709835'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Wang (2016) Liu Q, Wang D (2016) Stein Variational Gradient Descent:
    A General Purpose Bayesian Inference Algorithm. In: Advances in neural information
    processing systems 29, Curran Associates Inc., NIPS’16, pp 2378–2386'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martens and Grosse (2015) Martens J, Grosse R (2015) Optimizing Neural Networks
    with Kronecker-factored Approximate Curvature. In: Proceedings of the 32th International
    Conference on Machine Learning, JMLR.org, ICML’15, pp 2408–2417'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miconi et al. (2018) Miconi T, Stanley K, Clune J (2018) Differentiable plasticity:
    training plastic neural networks with backpropagation. In: Dy J, Krause A (eds)
    Proceedings of the 35th International Conference on Machine Learning, JLMR.org,
    ICML’18, pp 3559–3568'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miconi et al. (2019) Miconi T, Rawal A, Clune J, Stanley KO (2019) Backpropamine:
    training self-modifying neural networks with differentiable neuromodulated plasticity.
    In: International Conference on Learning Representations, ICLR’19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishra et al. (2018) Mishra N, Rohaninejad M, Chen X, Abbeel P (2018) A Simple
    Neural Attentive Meta-Learner. In: International Conference on Learning Representations,
    ICLR’18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitchell (1980) Mitchell TM (1980) The need for biases in learning generalizations.
    Tech. Rep. CBM-TR-117, Rutgers University
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2013) Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I,
    Wierstra D, Riedmiller M (2013) Playing Atari with Deep Reinforcement Learning.
    arXiv preprint arXiv:13125602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Munkhdalai and Yu (2017) Munkhdalai T, Yu H (2017) Meta networks. In: Proceedings
    of the 34th International Conference on Machine Learning, JLMR.org, ICML’17, pp
    2554–2563'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nagabandi et al. (2019) Nagabandi A, Clavera I, Liu S, Fearing RS, Abbeel P,
    Levine S, Finn C (2019) Learning to Adapt in Dynamic, Real-World Environments
    Through Meta-Reinforcement Learning. In: International Conference on Learning
    Representations, ICLR’19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naik and Mammone (1992) Naik DK, Mammone RJ (1992) Meta-neural networks that
    learn by learning. In: International Joint Conference on Neural Networks, IEEE,
    IJCNN’92, vol 1, pp 437–442'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nichol et al. (2018) Nichol A, Achiam J, Schulman J (2018) On First-Order Meta-Learning
    Algorithms. arXiv preprint arXiv:180302999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oord et al. (2016) Oord Avd, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves
    A, Kalchbrenner N, Senior A, Kavukcuoglu K (2016) WaveNet: A Generative Model
    for Raw Audio. arXiv preprint arXiv:160903499'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oreshkin et al. (2018) Oreshkin B, López PR, Lacoste A (2018) Tadam: Task dependent
    adaptive metric for improved few-shot learning. In: Advances in Neural Information
    Processing Systems 31, Curran Associates Inc., NIPS’18, pp 721–731'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan and Yang (2009) Pan SJ, Yang Q (2009) A Survey on Transfer Learning. IEEE
    Transactions on knowledge and data engineering 22(10):1345–1359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2002) Peng Y, Flach PA, Soares C, Brazdil P (2002) Improved Dataset
    Characterisation for Meta-learning. In: International Conference on Discovery
    Science, Springer, Lecture Notes in Computer Science, vol 2534, pp 141–152'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raghu et al. (2020) Raghu A, Raghu M, Bengio S, Vinyals O (2020) Rapid Learning
    or Feature Reuse? Towards Understanding the Effectiveness of MAML. In: International
    Conference on Learning Representations, ICLR’20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajeswaran et al. (2019) Rajeswaran A, Finn C, Kakade SM, Levine S (2019) Meta-Learning
    with Implicit Gradients. In: Advances in Neural Information Processing Systems
    32, Curran Associates Inc., NIPS’19, pp 113–124'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ravi and Larochelle (2017) Ravi S, Larochelle H (2017) Optimization as a Model
    for Few-Shot Learning. In: International Conference on Learning Representations,
    ICLR’17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2018) Ren M, Triantafillou E, Ravi S, Snell J, Swersky K, Tenenbaum
    JB, Larochelle H, Zemel RS (2018) Meta-Learning for Semi-Supervised Few-Shot Classification.
    In: International Conference on Learning Representations, ICLR’18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rusu et al. (2018) Rusu AA, Rao D, Sygnowski J, Vinyals O, Pascanu R, Osindero
    S, Hadsell R (2018) Meta-Learning with Latent Embedding Optimization. In: International
    Conference on Learning Representations, ICLR’18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santoro et al. (2016) Santoro A, Bartunov S, Botvinick M, Wierstra D, Lillicrap
    T (2016) Meta-learning with Memory-augmented Neural Networks. In: Proceedings
    of the 33rd International Conference on International Conference on Machine Learning,
    JMLR.org, ICML’16, pp 1842–1850'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber (1987) Schmidhuber J (1987) Evolutionary principles in self-referential
    learning. Diploma Thesis, Technische Universität München
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber (1993) Schmidhuber J (1993) A neural network that embeds its own
    meta-levels. In: IEEE International Conference on Neural Networks, IEEE, pp 407–412'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber et al. (1997) Schmidhuber J, Zhao J, Wiering M (1997) Shifting Inductive
    Bias with Success-Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement.
    Machine Learning 28(1):105–130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shyam et al. (2017) Shyam P, Gupta S, Dukkipati A (2017) Attentive Recurrent
    Comparators. In: Proceedings of the 34th International Conference on Machine Learning,
    JLMR.org, ICML’17, pp 3173–3181'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2016) Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den
    Driessche G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M, Dieleman
    S, Grewe D, Nham J, Kalchbrenner N, Sutskever I, Lillicrap T, Leach M, Kavukcuoglu
    K, Graepel T, Hassabis D (2016) Mastering the game of Go with deep neural networks
    and tree search. Nature 529(7587):484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Snell et al. (2017) Snell J, Swersky K, Zemel R (2017) Prototypical Networks
    for Few-shot Learning. In: Advances in Neural Information Processing Systems 30,
    Curran Associates Inc., NIPS’17, pp 4077–4087'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2017) Sun C, Shrivastava A, Singh S, Gupta A (2017) Revisiting
    Unreasonable Effectiveness of Data in Deep Learning Era. In: Proceedings of the
    IEEE International Conference on Computer Vision, pp 843–852'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sung et al. (2018) Sung F, Yang Y, Zhang L, Xiang T, Torr PH, Hospedales TM
    (2018) Learning to Compare: Relation Network for Few-Shot Learning. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, IEEE, pp 1199–1208'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (2018) Sutton RS, Barto AG (2018) Reinforcement Learning:
    An Introduction, 2nd edn. MIT press'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taylor and Stone (2009) Taylor ME, Stone P (2009) Transfer Learning for Reinforcement
    Learning Domains: A Survey. Journal of Machine Learning Research 10(7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thrun (1998) Thrun S (1998) Lifelong Learning Algorithms. In: Learning to learn,
    Springer, pp 181–209'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokmakov et al. (2019) Tokmakov P, Wang YX, Hebert M (2019) Learning Compositional
    Representations for Few-Shot Recognition. In: Proceedings of the IEEE International
    Conference on Computer Vision, pp 6372–6381'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Triantafillou et al. (2020) Triantafillou E, Zhu T, Dumoulin V, Lamblin P,
    Evci U, Xu K, Goroshin R, Gelada C, Swersky K, Manzagol PA, Larochelle H (2020)
    Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. In:
    International Conference on Learning Representations, ICLR’20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vanschoren (2018) Vanschoren J (2018) Meta-Learning: A Survey. arXiv preprint
    arXiv:181003548'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vanschoren et al. (2014) Vanschoren J, van Rijn JN, Bischl B, Torgo L (2014)
    OpenML: Networked Science in Machine Learning. SIGKDD Explorations 15(2):49–60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention Is All You Need. In: Advances
    in Neural Information Processing Systems 30, Curran Associates Inc., NIPS’17,
    pp 5998–6008'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals (2017) Vinyals O (2017) Talk: Model vs optimization meta learning.
    [http://metalearning-symposium.ml/files/vinyals.pdf](http://metalearning-symposium.ml/files/vinyals.pdf),
    neural Information Processing Systems (NIPS’17); accessed 06-06-2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. (2016) Vinyals O, Blundell C, Lillicrap T, Kavukcuoglu K, Wierstra
    D (2016) Matching Networks for One Shot Learning. In: Advances in Neural Information
    Processing Systems 29, Curran Associates Inc., NIPS’16, pp 3637–3645'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vuorio et al. (2018) Vuorio R, Cho DY, Kim D, Kim J (2018) Meta Continual Learning.
    arXiv preprint arXiv:180606928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wah et al. (2011) Wah C, Branson S, Welinder P, Perona P, Belongie S (2011)
    The Caltech-UCSD Birds-200-2011 Dataset. Tech. Rep. CNS-TR-2011-001, California
    Institute of Technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ, Munos
    R, Blundell C, Kumaran D, Botvinick M (2016) Learning to reinforcement learn.
    arXiv preprint arXiv:161105763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2016) Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun
    M, Cao Y, Gao Q, Macherey K, Klingner J, Shah A, Johnson M, Liu X, Łukasz Kaiser,
    Gouws S, Kato Y, Kudo T, Kazawa H, Stevens K, Kurian G, Patil N, Wang W, Young
    C, Smith J, Riesa J, Rudnick A, Vinyals O, Corrado G, Hughes M, Dean J (2016)
    Google’s Neural Machine Translation System: Bridging the Gap between Human and
    Machine Translation. arXiv preprint arXiv:160908144'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2020) Yin M, Tucker G, Zhou M, Levine S, Finn C (2020) Meta-Learning
    without Memorization. In: International Conference on Learning Representations,
    ICLR’20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yoon et al. (2018) Yoon J, Kim T, Dia O, Kim S, Bengio Y, Ahn S (2018) Bayesian
    Model-Agnostic Meta-Learning. In: Advances in Neural Information Processing Systems
    31, Curran Associates Inc., NIPS’18, pp 7332–7342'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Younger et al. (2001) Younger AS, Hochreiter S, Conwell PR (2001) Meta-learning
    with backpropagation. In: International Joint Conference on Neural Networks, IEEE,
    IJCNN’01, vol 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019) Yu T, Quillen D, He Z, Julian R, Hausman K, Finn C, Levine
    S (2019) Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement
    Learning. arXiv preprint arXiv:191010897'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
