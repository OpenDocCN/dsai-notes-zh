- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:33:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2404.13830] A Comprehensive Survey and Taxonomy on Point Cloud Registration
    Based on Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13830](https://ar5iv.labs.arxiv.org/html/2404.13830)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on Deep
    Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yu-Xin Zhang¹    Jie Gui¹¹¹1Corresponding Author    Xiaofeng Cong¹    Xin Gong¹&Wenbing
    Tao²
  prefs: []
  type: TYPE_NORMAL
- en: ¹Southeast University
  prefs: []
  type: TYPE_NORMAL
- en: ²Huazhong University of Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: '{yuxinzhang, guijie}@seu.edu.cn, cxf_svip@163.com, xingong@seu.edu.cn, wenbingtao@hust.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Point cloud registration (PCR) involves determining a rigid transformation
    that aligns one point cloud to another. Despite the plethora of outstanding deep
    learning (DL)-based registration methods proposed, comprehensive and systematic
    studies on DL-based PCR techniques are still lacking. In this paper, we present
    a comprehensive survey and taxonomy of recently proposed PCR methods. Firstly,
    we conduct a taxonomy of commonly utilized datasets and evaluation metrics. Secondly,
    we classify the existing research into two main categories: supervised and unsupervised
    registration, providing insights into the core concepts of various influential
    PCR models. Finally, we highlight open challenges and potential directions for
    future research. A curated collection of valuable resources is made available
    at https://github.com/yxzhang15/PCR.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the progress of sensor technology, acquiring high-precision point cloud
    data has become more accessible and prevalent (Zhang et al., [2023b](#bib.bib77);
    Uy et al., [2019](#bib.bib52)). Point cloud registration (PCR), as a pivotal tool
    in point cloud data processing, aims to align the point cloud data with a common
    coordinate system, enabling precise three-dimensional (3D) modeling (Qin et al.,
    [2023](#bib.bib48); Chen et al., [2022b](#bib.bib9)). This registration process
    establishes a dependable foundation for point cloud analysis and various applications (Huang
    et al., [2021b](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the rapid advancements in this field, hundreds of deep learning (DL)-based
    methods have been proposed. There is an urgent need for thorough investigations
    to both inspire and steer future research endeavors. To address this necessity,
    we develop a comprehensive survey and establish a detailed taxonomy of PCR algorithms.
    This study categorizes these algorithms into two types: supervised and unsupervised.
    Supervised registration, leveraging labeled data that typically encompasses known
    transformations between point clouds, orchestrates the training process. In contrast,
    unsupervised registration hinges on the intrinsic geometric properties of the
    point clouds, independent of external labels. For supervised algorithms, the taxonomy
    is segmented into four crucial stages and two overarching concepts. The four stages
    include descriptor extraction, correspondence search, outlier filtering, and transformation
    parameter estimation, while the two concepts encompass optimization and multimodal.
    The supervised algorithms are systematically categorized based on their contributions
    to every stage or integration of concepts. Furthermore, for unsupervised algorithms,
    our taxonomy differentiates between two methodologies: the correspondence-free
    approaches, which align point clouds by minimizing feature discrepancies, and
    the correspondence-based approaches, which align point clouds by establishing
    correspondences.'
  prefs: []
  type: TYPE_NORMAL
- en: Goals of our survey. We aim to (i) classify commonly used datasets and metrics
    in PCR tasks; (ii) develop a taxonomy for DL-based registration algorithms, introducing
    core techniques employed across various methods; and (iii) identify open issues
    that could stimulate further research in PCR tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The differences between our survey and others. (Gu et al., [2020](#bib.bib26))
    only review traditional PCR algorithms, DL-based algorithms are not involved.
    (Zhang et al., [2020](#bib.bib71)) and (Huang et al., [2021b](#bib.bib28)) conduct
    a summary of DL-based PCR algorithms. However, recent advances in unsupervised
    algorithms are not elaborated. Additionally, they did not provide a comprehensive
    overview of the latest research developments in the PCR field. To address these
    gaps, our study conducts a comprehensive survey and taxonomy of DL-based supervised
    and unsupervised registration algorithms. The taxonomy is concisely summarized
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Definition ‣ 2 Related Work ‣ A Comprehensive
    Survey and Taxonomy on Point Cloud Registration Based on Deep Learning"), providing
    a clear and structured overview of the PCR algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of PCR is to find the optimal rotation $\bm{R}^{*}$ and translation
    $\bm{t}^{*}$ parameters that align source point cloud $\bm{X}\in\mathbb{R}^{N\times
    3}$ and target point cloud $\bm{Y}\in\mathbb{R}^{M\times 3}$. Here, $N$ and $M$
    represent the number of points in $\bm{X}$ and $\bm{Y}$, respectively. The mathematical
    objective of the PCR process is formulated by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(\bm{R}^{*},\bm{t}^{*})=\mathop{argmin}\limits_{\bm{R}\in SO(3),\bm{t}\in\mathbb{R}^{3}}\sum_{p=1}^{P}\&#124;(\bm{R}\bm{x}_{p}+\bm{t})-\bm{y}_{p}\&#124;^{2},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{x}_{p},\bm{y}_{p}\in\mathbb{R}^{1\times 3}$ are the p-th points in
    $\bm{X}$ and $\bm{Y}$, while $P$ denotes the number of correspondences between
    $\bm{X}$ and $\bm{Y}$.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F1.pic1" class="ltx_picture" height="572.59" overflow="visible"
    version="1.1" width="703.72"><g transform="translate(0,572.59) matrix(1 0 0 -1
    0 0) translate(16.98,0) translate(0,494.09)"><g stroke="#000000" fill="#000000"><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -11.81 -1.38)"><foreignobject width="23.62" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PCR <g stroke="#3C8C93" stroke-width="0.8pt"><path
    d="M 84.21 17.72 L 34.88 17.72 C 31.82 17.72 29.34 15.24 29.34 12.18 L 29.34 -12.18
    C 29.34 -15.24 31.82 -17.72 34.88 -17.72 L 84.21 -17.72 C 87.27 -17.72 89.75 -15.24
    89.75 -12.18 L 89.75 12.18 C 89.75 15.24 87.27 17.72 84.21 17.72 Z M 29.34 -17.72"
    style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 33.95 3.42)"><foreignobject width="51.18" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 168.16 29.52 L 108.2 29.52
    C 105.14 29.52 102.66 27.04 102.66 23.98 L 102.66 -23.98 C 102.66 -27.04 105.14
    -29.52 108.2 -29.52 L 168.16 -29.52 C 171.22 -29.52 173.7 -27.04 173.7 -23.98
    L 173.7 23.98 C 173.7 27.04 171.22 29.52 168.16 29.52 Z M 102.66 -29.52" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 15.22)"><foreignobject width="61.81" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Descriptor Extraction</foreignobject></g> <g
    stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 15.75 L 192.15 15.75 C
    189.09 15.75 186.62 13.27 186.62 10.21 L 186.62 -10.21 C 186.62 -13.27 189.09
    -15.75 192.15 -15.75 L 245.42 -15.75 C 248.48 -15.75 250.96 -13.27 250.96 -10.21
    L 250.96 10.21 C 250.96 13.27 248.48 15.75 245.42 15.75 Z M 186.62 -15.75" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 191.23 -1.38)"><foreignobject width="55.12" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Two-view</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 245.42 -62.13 L 192.15 -62.13 C 189.09 -62.13
    186.62 -64.61 186.62 -67.67 L 186.62 -88.38 C 186.62 -91.44 189.09 -93.92 192.15
    -93.92 L 245.42 -93.92 C 248.48 -93.92 250.96 -91.44 250.96 -88.38 L 250.96 -67.67
    C 250.96 -64.61 248.48 -62.13 245.42 -62.13 Z M 186.62 -93.92" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 191.23 -76.43)"><foreignobject width="55.12" height="22.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Multi-view</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 317.4 17.72 L 269.41 17.72 C 266.35 17.72 263.88
    15.24 263.88 12.18 L 263.88 -12.18 C 263.88 -15.24 266.35 -17.72 269.41 -17.72
    L 317.4 -17.72 C 320.45 -17.72 322.93 -15.24 322.93 -12.18 L 322.93 12.18 C 322.93
    15.24 320.45 17.72 317.4 17.72 Z M 263.88 -17.72" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 269.78 3.42)"><foreignobject width="47.24" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Keypoint-based</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 317.4 -28.67 L 269.41 -28.67 C 266.35 -28.67 263.88
    -31.15 263.88 -34.2 L 263.88 -58.57 C 263.88 -61.63 266.35 -64.11 269.41 -64.11
    L 317.4 -64.11 C 320.45 -64.11 322.93 -61.63 322.93 -58.57 L 322.93 -34.2 C 322.93
    -31.15 320.45 -28.67 317.4 -28.67 Z M 263.88 -64.11" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 269.78 -42.97)"><foreignobject width="47.24" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Keypoint-free</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 168.16 -111.33 L 108.2 -111.33 C 105.14 -111.33
    102.66 -113.81 102.66 -116.87 L 102.66 -157.84 C 102.66 -160.9 105.14 -163.38
    108.2 -163.38 L 168.16 -163.38 C 171.22 -163.38 173.7 -160.9 173.7 -157.84 L 173.7
    -116.87 C 173.7 -113.81 171.22 -111.33 168.16 -111.33 Z M 102.66 -163.38" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 -125.63)"><foreignobject width="61.81" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Correspondence Search</foreignobject></g> <g
    stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -120.19 L 192.15 -120.19
    C 189.09 -120.19 186.62 -122.66 186.62 -125.72 L 186.62 -148.99 C 186.62 -152.05
    189.09 -154.52 192.15 -154.52 L 245.42 -154.52 C 248.48 -154.52 250.96 -152.05
    250.96 -148.99 L 250.96 -125.72 C 250.96 -122.66 248.48 -120.19 245.42 -120.19
    Z M 186.62 -154.52" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -134.48)"><foreignobject
    width="55.12" height="25.12" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Full-object</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -158.39 L 192.15 -158.39
    C 189.09 -158.39 186.62 -160.87 186.62 -163.92 L 186.62 -190.61 C 186.62 -193.67
    189.09 -196.15 192.15 -196.15 L 245.42 -196.15 C 248.48 -196.15 250.96 -193.67
    250.96 -190.61 L 250.96 -163.92 C 250.96 -160.87 248.48 -158.39 245.42 -158.39
    Z M 186.62 -196.15" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -172.69)"><foreignobject
    width="55.12" height="28.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Partial-object</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 346.31 -159.55 L 269.41 -159.55
    C 266.35 -159.55 263.88 -162.03 263.88 -165.08 L 263.88 -189.45 C 263.88 -192.51
    266.35 -194.99 269.41 -194.99 L 346.31 -194.99 C 349.36 -194.99 351.84 -192.51
    351.84 -189.45 L 351.84 -165.08 C 351.84 -162.03 349.36 -159.55 346.31 -159.55
    Z M 263.88 -194.99" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 268.49 -173.85)"><foreignobject
    width="78.74" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Overlap
    Prediction</foreignobject></g> <g stroke="#3C8C93" stroke-width="0.8pt"><path
    d="M 346.31 -198.85 L 269.41 -198.85 C 266.35 -198.85 263.88 -201.33 263.88 -204.38
    L 263.88 -244.99 C 263.88 -248.04 266.35 -250.52 269.41 -250.52 L 346.31 -250.52
    C 349.36 -250.52 351.84 -248.04 351.84 -244.99 L 351.84 -204.38 C 351.84 -201.33
    349.36 -198.85 346.31 -198.85 Z M 263.88 -250.52" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 268.49 -213.15)"><foreignobject width="78.74" height="42.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Optimizing the Similarity Matrix</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 168.16 -238.1 L 108.2 -238.1
    C 105.14 -238.1 102.66 -240.58 102.66 -243.64 L 102.66 -270.7 C 102.66 -273.75
    105.14 -276.23 108.2 -276.23 L 168.16 -276.23 C 171.22 -276.23 173.7 -273.75 173.7
    -270.7 L 173.7 -243.64 C 173.7 -240.58 171.22 -238.1 168.16 -238.1 Z M 102.66
    -276.23" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 107.28 -252.4)"><foreignobject width="61.81"
    height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Outlier Filtering</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 168.75 -280.1 L 107.61 -280.1
    C 104.55 -280.1 102.07 -282.57 102.07 -285.63 L 102.07 -366.81 C 102.07 -369.86
    104.55 -372.34 107.61 -372.34 L 168.75 -372.34 C 171.81 -372.34 174.29 -369.86
    174.29 -366.81 L 174.29 -285.63 C 174.29 -282.57 171.81 -280.1 168.75 -280.1 Z
    M 102.07 -372.34" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 106.69 -294.39)"><foreignobject
    width="62.99" height="83.02" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformation
    Parameter Estimation</foreignobject></g> <g stroke="#3C8C93" stroke-width="0.8pt"><path
    d="M 168.16 -376.2 L 108.2 -376.2 C 105.14 -376.2 102.66 -378.68 102.66 -381.74
    L 102.66 -402.45 C 102.66 -405.51 105.14 -407.99 108.2 -407.99 L 168.16 -407.99
    C 171.22 -407.99 173.7 -405.51 173.7 -402.45 L 173.7 -381.74 C 173.7 -378.68 171.22
    -376.2 168.16 -376.2 Z M 102.66 -407.99" style="fill:none"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 107.28 -390.5)"><foreignobject
    width="61.81" height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Optimization</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -374.38 L 192.15 -374.38
    C 189.09 -374.38 186.62 -376.86 186.62 -379.91 L 186.62 -404.28 C 186.62 -407.34
    189.09 -409.82 192.15 -409.82 L 245.42 -409.82 C 248.48 -409.82 250.96 -407.34
    250.96 -404.28 L 250.96 -379.91 C 250.96 -376.86 248.48 -374.38 245.42 -374.38
    Z M 186.62 -409.82" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -388.68)"><foreignobject
    width="55.12" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ICP-based</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -413.68 L 192.15 -413.68
    C 189.09 -413.68 186.62 -416.16 186.62 -419.21 L 186.62 -460.18 C 186.62 -463.24
    189.09 -465.72 192.15 -465.72 L 245.42 -465.72 C 248.48 -465.72 250.96 -463.24
    250.96 -460.18 L 250.96 -419.21 C 250.96 -416.16 248.48 -413.68 245.42 -413.68
    Z M 186.62 -465.72" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -427.98)"><foreignobject
    width="55.12" height="42.82" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Probabilistic
    -based</foreignobject></g> <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M
    168.16 -447.68 L 108.2 -447.68 C 105.14 -447.68 102.66 -450.16 102.66 -453.21
    L 102.66 -477.58 C 102.66 -480.64 105.14 -483.12 108.2 -483.12 L 168.16 -483.12
    C 171.22 -483.12 173.7 -480.64 173.7 -477.58 L 173.7 -453.21 C 173.7 -450.16 171.22
    -447.68 168.16 -447.68 Z M 102.66 -483.12" style="fill:none"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 107.28 -461.98)"><foreignobject
    width="61.81" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multimodal</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 678.91 77.95
    L 341.38 77.95 C 338.33 77.95 335.85 75.47 335.85 72.41 L 335.85 -72.41 C 335.85
    -75.47 338.33 -77.95 341.38 -77.95 L 678.91 -77.95 C 681.97 -77.95 684.44 -75.47
    684.44 -72.41 L 684.44 72.41 C 684.44 75.47 681.97 77.95 678.91 77.95 Z M 335.85
    -77.95"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 340.46 63.65)"><foreignobject width="339.37" height="146.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DeepVCP (Lu et al., [2019a](#bib.bib38)), 3DSmoothNet
    (Gojcic et al., [2019](#bib.bib24)), Deng et al. (Deng et al., [2019](#bib.bib18)),
    HRegNet (Lu et al., [2021](#bib.bib40)), SpinNet (Ao et al., [2021](#bib.bib1)),
    StickyPillars (Fischer et al., [2021](#bib.bib21)), YOHO (Wang et al., [2022a](#bib.bib56)),
    RoReg (Wang et al., [2023b](#bib.bib59)), BUFFER (Ao et al., [2023](#bib.bib2)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 678.91 -9.95
    L 341.38 -9.95 C 338.33 -9.95 335.85 -12.43 335.85 -15.48 L 335.85 -77.29 C 335.85
    -80.35 338.33 -82.82 341.38 -82.82 L 678.91 -82.82 C 681.97 -82.82 684.44 -80.35
    684.44 -77.29 L 684.44 -15.48 C 684.44 -12.43 681.97 -9.95 678.91 -9.95 Z M 335.85
    -82.82"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 340.46 -24.25)"><foreignobject width="339.37" height="63.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">OIF-PCR (Yang et al., [2022](#bib.bib64)),
    GeDi (Poiesi and Boscaini, [2022](#bib.bib45)), GeoTransformer (Qin et al., [2023](#bib.bib48)),
    RoITr (Yu et al., [2023a](#bib.bib67)).</foreignobject></g> <g stroke="#3C8C93"
    fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -49.89 L 269.41 -49.89 C
    266.35 -49.89 263.88 -52.37 263.88 -55.42 L 263.88 -100.62 C 263.88 -103.68 266.35
    -106.16 269.41 -106.16 L 679.77 -106.16 C 682.83 -106.16 685.3 -103.68 685.3 -100.62
    L 685.3 -55.42 C 685.3 -52.37 682.83 -49.89 679.77 -49.89 Z M 263.88 -106.16"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 268.49 -64.19)"><foreignobject width="412.2" height="47.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MVDesc (Zhou et al., [2018](#bib.bib78)), Li
    et al. (Li et al., [2020b](#bib.bib36)), Gojcic et al. (Gojcic et al., [2020](#bib.bib25)),
    Wang et al. (Wang et al., [2023a](#bib.bib58)).</foreignobject></g> <g stroke="#3C8C93"
    fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -115.41 L 269.41 -115.41
    C 266.35 -115.41 263.88 -117.89 263.88 -120.94 L 263.88 -153.77 C 263.88 -156.82
    266.35 -159.3 269.41 -159.3 L 679.77 -159.3 C 682.83 -159.3 685.3 -156.82 685.3
    -153.77 L 685.3 -120.94 C 685.3 -117.89 682.83 -115.41 679.77 -115.41 Z M 263.88
    -159.3"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 268.49 -129.71)"><foreignobject width="412.2" height="34.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PointNetLK (Aoki et al., [2019](#bib.bib3)),
    DCP (Wang and Solomon, [2019a](#bib.bib54)), PointNetLK Revisited (Li et al.,
    [2021](#bib.bib37)).</foreignobject></g> <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path
    d="M 680.26 -130.41 L 370.29 -130.41 C 367.24 -130.41 364.76 -132.89 364.76 -135.95
    L 364.76 -218.59 C 364.76 -221.64 367.24 -224.12 370.29 -224.12 L 680.26 -224.12
    C 683.32 -224.12 685.79 -221.64 685.79 -218.59 L 685.79 -135.95 C 685.79 -132.89
    683.32 -130.41 680.26 -130.41 Z M 364.76 -224.12"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 369.37 -144.71)"><foreignobject
    width="311.81" height="84.48" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Predator
    (Huang et al., [2021a](#bib.bib27)), OMNet (Xu et al., [2021](#bib.bib62)), PCAM
    (Cao et al., [2021](#bib.bib6)), STORM (Wang et al., [2022b](#bib.bib57)), REGTR
    (Yew and Lee, [2022](#bib.bib66)).</foreignobject></g> <g stroke="#3C8C93" fill="#BBE0E3"
    stroke-width="0.8pt"><path d="M 680.26 -195.17 L 370.29 -195.17 C 367.24 -195.17
    364.76 -197.64 364.76 -200.7 L 364.76 -248.67 C 364.76 -251.73 367.24 -254.2 370.29
    -254.2 L 680.26 -254.2 C 683.32 -254.2 685.79 -251.73 685.79 -248.67 L 685.79
    -200.7 C 685.79 -197.64 683.32 -195.17 680.26 -195.17 Z M 364.76 -254.2"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 369.37 -209.46)"><foreignobject width="311.81" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PRNet (Wang and Solomon, [2019b](#bib.bib55)),
    RPMNet (Yew and Lee, [2020](#bib.bib65)), FIRE-Net (Wu et al., [2021](#bib.bib61))</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 680.07 -226.92
    L 192.15 -226.92 C 189.09 -226.92 186.62 -229.4 186.62 -232.45 L 186.62 -281.88
    C 186.62 -284.94 189.09 -287.42 192.15 -287.42 L 680.07 -287.42 C 683.13 -287.42
    685.6 -284.94 685.6 -281.88 L 685.6 -232.45 C 685.6 -229.4 683.13 -226.92 680.07
    -226.92 Z M 186.62 -287.42"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -241.22)"><foreignobject
    width="489.76" height="51.27" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3DRegNet
    (Pais et al., [2020](#bib.bib44)), DHVR (Lee et al., [2021](#bib.bib34)), PointDSC
    (Bai et al., [2021](#bib.bib5)), DLF (Zhang et al., [2022a](#bib.bib73)), Chen
    et al.(Chen et al., [2022b](#bib.bib9)), MAC (Zhang et al., [2023a](#bib.bib76)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 680.66 -310.47
    L 192.74 -310.47 C 189.69 -310.47 187.21 -312.95 187.21 -316 L 187.21 -336.43
    C 187.21 -339.49 189.69 -341.97 192.74 -341.97 L 680.66 -341.97 C 683.72 -341.97
    686.19 -339.49 686.19 -336.43 L 686.19 -316 C 686.19 -312.95 683.72 -310.47 680.66
    -310.47 Z M 187.21 -341.97"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.82 -326.87)"><foreignobject
    width="489.76" height="18.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DetarNet
    (Chen et al., [2022c](#bib.bib10)), FINet (Xu et al., [2022](#bib.bib63)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -372.26
    L 269.41 -372.26 C 266.35 -372.26 263.88 -374.74 263.88 -377.8 L 263.88 -406.39
    C 263.88 -409.45 266.35 -411.93 269.41 -411.93 L 679.77 -411.93 C 682.83 -411.93
    685.3 -409.45 685.3 -406.39 L 685.3 -377.8 C 685.3 -374.74 682.83 -372.26 679.77
    -372.26 Z M 263.88 -411.93"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 268.49 -386.56)"><foreignobject
    width="412.2" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCP
    (Wang and Solomon, [2019a](#bib.bib54)), PRNet (Wang and Solomon, [2019b](#bib.bib55)),
    IDAM (Li et al., [2020a](#bib.bib35)).</foreignobject></g> <g stroke="#3C8C93"
    fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -417.75 L 269.41 -417.75
    C 266.35 -417.75 263.88 -420.23 263.88 -423.29 L 263.88 -456.11 C 263.88 -459.17
    266.35 -461.65 269.41 -461.65 L 679.77 -461.65 C 682.83 -461.65 685.3 -459.17
    685.3 -456.11 L 685.3 -423.29 C 685.3 -420.23 682.83 -417.75 679.77 -417.75 Z
    M 263.88 -461.65"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 268.49 -432.05)"><foreignobject width="412.2"
    height="34.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DeepGMR
    (Yuan et al., [2020](#bib.bib69)), OGMM (Mei et al., [2023a](#bib.bib42)), Chen
    et al.(Chen et al., [2023a](#bib.bib11)), VBReg (Jiang et al., [2023](#bib.bib33)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 680.07 -437.26
    L 192.15 -437.26 C 189.09 -437.26 186.62 -439.74 186.62 -442.8 L 186.62 -488 C
    186.62 -491.05 189.09 -493.53 192.15 -493.53 L 680.07 -493.53 C 683.13 -493.53
    685.6 -491.05 685.6 -488 L 685.6 -442.8 C 685.6 -439.74 683.13 -437.26 680.07
    -437.26 Z M 186.62 -493.53"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -451.56)"><foreignobject
    width="489.76" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PCR-CG
    (Zhang et al., [2022c](#bib.bib75)), ImLoveNet (Chen et al., [2022a](#bib.bib8)),
    IMFNET (Huang et al., [2022c](#bib.bib31)), GMF (Huang et al., [2022b](#bib.bib30)),
    PEAL (Yu et al., [2023b](#bib.bib68)).</foreignobject></g> <g stroke-width="0.8pt"
    stroke="#E44057"><path d="M 84.21 -386.94 L 34.88 -386.94 C 31.82 -386.94 29.34
    -389.41 29.34 -392.47 L 29.34 -416.84 C 29.34 -419.89 31.82 -422.37 34.88 -422.37
    L 84.21 -422.37 C 87.27 -422.37 89.75 -419.89 89.75 -416.84 L 89.75 -392.47 C
    89.75 -389.41 87.27 -386.94 84.21 -386.94 Z M 29.34 -422.37" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 33.95 -401.23)"><foreignobject width="51.18" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Unsupervised</foreignobject></g> <g stroke-width="0.8pt"
    stroke="#E44057"><path d="M 168.16 -378.63 L 108.2 -378.63 C 105.14 -378.63 102.66
    -381.11 102.66 -384.17 L 102.66 -425.14 C 102.66 -428.2 105.14 -430.68 108.2 -430.68
    L 168.16 -430.68 C 171.22 -430.68 173.7 -428.2 173.7 -425.14 L 173.7 -384.17 C
    173.7 -381.11 171.22 -378.63 168.16 -378.63 Z M 102.66 -430.68" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 -392.93)"><foreignobject width="61.81" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Correspondence -free</foreignobject></g> <g
    stroke-width="0.8pt" fill="#FADBDF" stroke="#E44057"><path d="M 680.07 -376.52
    L 192.15 -376.52 C 189.09 -376.52 186.62 -379 186.62 -382.05 L 186.62 -427.25
    C 186.62 -430.31 189.09 -432.79 192.15 -432.79 L 680.07 -432.79 C 683.13 -432.79
    685.6 -430.31 685.6 -427.25 L 685.6 -382.05 C 685.6 -379 683.13 -376.52 680.07
    -376.52 Z M 186.62 -432.79"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -390.82)"><foreignobject
    width="489.76" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PPF-FoldNet (Deng
    et al., [2018](#bib.bib17)), PCRNet (Sarode et al., [2019](#bib.bib49)), UPCR (Zhang
    et al., [2021](#bib.bib72)), UGMM (Huang et al., [2022a](#bib.bib29)), Sun et
    al. (Sun et al., [2023](#bib.bib51)).</foreignobject></g> <g stroke-width="0.8pt"
    stroke="#E44057"><path d="M 168.16 -434.54 L 108.2 -434.54 C 105.14 -434.54 102.66
    -437.02 102.66 -440.07 L 102.66 -481.04 C 102.66 -484.1 105.14 -486.58 108.2 -486.58
    L 168.16 -486.58 C 171.22 -486.58 173.7 -484.1 173.7 -481.04 L 173.7 -440.07 C
    173.7 -437.02 171.22 -434.54 168.16 -434.54 Z M 102.66 -486.58" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 -448.84)"><foreignobject width="61.81" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Correspondence -based</foreignobject></g> <g
    stroke-width="0.8pt" fill="#FADBDF" stroke="#E44057"><path d="M 680.07 -440.73
    L 192.15 -440.73 C 189.09 -440.73 186.62 -443.2 186.62 -446.26 L 186.62 -474.86
    C 186.62 -477.91 189.09 -480.39 192.15 -480.39 L 680.07 -480.39 C 683.13 -480.39
    685.6 -477.91 685.6 -474.86 L 685.6 -446.26 C 685.6 -443.2 683.13 -440.73 680.07
    -440.73 Z M 186.62 -480.39"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -455.02)"><foreignobject
    width="489.76" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CEMNet (Jiang
    et al., [2021](#bib.bib32)), RIENet (Shen et al., [2022](#bib.bib50)), UDPReg (Mei
    et al., [2023b](#bib.bib43)).</foreignobject></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 16.98 0 L 19.69 0 L 19.69 0 L
    28.79 0" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 16.98 0 L 19.69 0 L 19.69 -404.65 L 28.79 -404.65"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 90.3 0 L 90.3 0" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04
    0 L 91.04 0 L 102.11 0" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 174.25 0 L 177.55 0 L 177.55 0
    L 186.06 0" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 174.25 0 L 177.55 0 L 177.55 -78.02 L 186.06 -78.02"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 251.51 0 L 258.16 0 L 258.16 0 L 263.32 0" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 0 L 258.16 0 L 258.16 -46.39 L 263.32 -46.39" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    323.48 -46.39 L 335.3 -46.39" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 323.48 0 L 335.3
    0" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 251.51 -78.02 L 263.32 -78.02" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    90.3 0 L 91.04 0 L 91.04 -137.35 L 102.11 -137.35" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -137.35 L 177.55 -137.35 L 177.55 -137.35 L 186.06 -137.35" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -137.35 L 177.55 -137.35 L 177.55 -177.27 L 186.06 -177.27" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 -137.35 L 263.32 -137.35" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 251.51 -177.27
    L 258.16 -177.27 L 258.16 -177.27 L 263.32 -177.27" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 -177.27 L 258.16 -177.27 L 258.16 -224.69 L 263.32 -224.69" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    352.39 -177.27 L 364.2 -177.27" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 352.39 -224.69
    L 364.2 -224.69" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -257.17
    L 102.11 -257.17" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -326.22
    L 101.52 -326.22" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -392.1
    L 102.11 -392.1" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -465.4
    L 102.11 -465.4" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 174.25 -257.17 L 186.06 -257.17"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 174.84 -326.22 L 186.65 -326.22" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -392.1 L 177.55 -392.1 L 177.55 -392.1 L 186.06 -392.1" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -392.1 L 177.55 -392.1 L 177.55 -439.7 L 186.06 -439.7" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 -392.1 L 263.32 -392.1" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 251.51 -439.7 L
    263.32 -439.7" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 174.25 -465.4 L 186.06 -465.4"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 90.3 -404.65 L 94.98 -404.65 L 94.98 -404.65 L 102.11
    -404.65" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 90.3 -404.65 L 94.98 -404.65 L 94.98 -460.56 L 102.11
    -460.56" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 174.25 -404.65 L 186.06 -404.65" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -460.56 L 186.06 -460.56" style="fill:none"></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: A taxonomy of PCR algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets for PCR can be broadly classified into two categories: artificially
    synthesized and acquired through real instruments. Each category exhibits unique
    characteristics and has different levels of applicability in PCR tasks. Synthesized
    point cloud datasets are typically composed of virtual models created by using
    computer graphics techniques to replicate real-world environments. These datasets
    contain the object-level ModelNet40 (Wu et al., [2015](#bib.bib60)) and ShapeNet
    (Chang et al., [2015](#bib.bib7)), which consist of data generated through computer-aided
    design, as well as the scene-level dataset ICL-NUIM (Choi et al., [2015](#bib.bib15))
    and FlyingShapes (Chen et al., [2023b](#bib.bib12)).'
  prefs: []
  type: TYPE_NORMAL
- en: However, while synthetic data are beneficial for certain applications, they
    often lack the complexity and variability found in real-world scenarios. Consequently,
    incorporating real-world data into training and validation processes is essential
    for obtaining robust algorithms. Datasets comprising realistic point cloud data
    include Stanford (Curless and Levoy, [1996](#bib.bib16)), ETH (Pomerleau et al.,
    [2012](#bib.bib46)), KITTI (Geiger et al., [2012](#bib.bib23)), Apollo-SouthBay
    (Lu et al., [2019b](#bib.bib39)), ScanObjectNN (Uy et al., [2019](#bib.bib52)),
    and WHU-TLS (Dong et al., [2020](#bib.bib19)). Furthermore, there is a 3DMatch
    (Zeng et al., [2017](#bib.bib70)) dataset that comprises both synthesized and
    realistic scans. The attributes of various datasets are summarized in Table [1](#S2.T1
    "Table 1 ‣ 2.3 Metrics ‣ 2 Related Work ‣ A Comprehensive Survey and Taxonomy
    on Point Cloud Registration Based on Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metrics play a pivotal role in evaluating and comparing the results of point
    cloud registration, aiding in the selection of optimal parameters. Consequently,
    the choice of an appropriate metric is vital for accurately assessing the quality
    of a registration algorithm. We categorize evaluation metrics based on their application
    scenarios. For object-level point clouds, the commonly employed metrics include
    root mean squared error, mean squared error, mean isotropic error, mean absolute
    error, Chamfer distance (CD), and coefficient of determination. For scene-level
    point clouds, the typical metrics are registration recall, inlier ratio, feature
    matching recall, relative rotation error, and relative translation error.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4efe31ba9de9c0593b8ed0d712925641.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The pipeline of the supervised algorithm. R-L represents the real
    label. PC represents point clouds. I denotes the images. S and C represent step
    and concept, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Type | Number | S/O |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford (Curless and Levoy, [1996](#bib.bib16)) | Real | 10 | O |'
  prefs: []
  type: TYPE_TB
- en: '| ETH (Pomerleau et al., [2012](#bib.bib46)) | Real | 36 | S |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI (Geiger et al., [2012](#bib.bib23)) | Real | 22 | S |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet40 (Wu et al., [2015](#bib.bib60)) | Syn | 12311 | O |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeNet (Chang et al., [2015](#bib.bib7)) | Syn | 55000+ | O |'
  prefs: []
  type: TYPE_TB
- en: '| ICL-NUIM (Choi et al., [2015](#bib.bib15)) | Syn | 8 | S |'
  prefs: []
  type: TYPE_TB
- en: '| 3DMatch (Zeng et al., [2017](#bib.bib70)) | Syn&Real | 62 | S |'
  prefs: []
  type: TYPE_TB
- en: '| Apollo-SouthBay (Lu et al., [2019b](#bib.bib39)) | Real | 6 | S |'
  prefs: []
  type: TYPE_TB
- en: '| ScanObjectNN (Uy et al., [2019](#bib.bib52)) | Real | 15000+ | O |'
  prefs: []
  type: TYPE_TB
- en: '| WHU-TLS (Dong et al., [2020](#bib.bib19)) | Real | 115 | S |'
  prefs: []
  type: TYPE_TB
- en: '| FlyingShapes (Chen et al., [2023b](#bib.bib12)) | Syn | 200 | S |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Datasets for PCR tasks. Syn means synthetic point clouds. Real means
    realistic point clouds. S and O denote scene-level and object-level, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Supervised Point Cloud Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised models for PCR typically rely on various forms of supervisory signals,
    such as ground-truth labels or transformation parameters, to guide the training
    process. To facilitate research on DL-based supervised methods, this section provides
    a structured categorization of the principal contributions made by various supervised
    algorithms across four key stages and two fundamental concepts. Such a taxonomy
    not only elucidates valuable technologies but also presents registration methods
    in a clear and concise manner. The four steps and two concepts of the supervised
    registration algorithm are shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Metrics
    ‣ 2 Related Work ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration
    Based on Deep Learning"). It is worth noting that not every algorithm framework
    contains the four steps and involves these two concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Descriptor Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In PCR tasks, descriptors are essential, and markedly influence the discriminability
    of features. Here, we describe the PCR algorithms that mainly contribute to descriptor
    extraction from two perspectives, which are two-view and multi-view algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Two-view
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first perspective involves two-view registration, which emerges as the
    prevalent approach in the field of PCR. We further classify these methods into
    two categories: keypoint-based and keypoint-free.'
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint-based needs to detect significant keypoints to obtain robust feature
    descriptors. To facilitate comprehension, this category is further segmented based
    on the type of input data employed, including points, patches, and voxel grids.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, points, serving as the fundamental elements of point clouds, are discrete
    and unlinked 3D entities. Consequently, extracting descriptors from points typically
    necessitates the construction of intricate local relationships. In DeepVCP (Lu
    et al., [2019a](#bib.bib38)), point weighting is incorporated into an end-to-end
    registration network to estimate point saliency scores, enabling the detection
    of keypoints. Subsequently, the $K$-nearest neighbors method is employed to establish
    neighborhoods around the keypoints, followed by a permutation-invariant network
    to extract more detailed descriptors. HRegNet (Lu et al., [2021](#bib.bib40))
    is a hierarchical network that leverages geometric features, descriptors, and
    similarity measures obtained through bilateral consensus and neighborhood consensus
    to establish correspondence between keypoints. The principles of bilateral consensus
    and neighborhood consensus suggest that within descriptor space, two correct corresponding
    points should not only be the nearest neighbors of each other but also exhibit
    similar neighborhoods. BUFFER (Ao et al., [2023](#bib.bib2)) designs a point-wise
    learner to enhance computational efficiency and feature representation capabilities
    by predicting keypoints and estimating point orientations.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, patches can directly represent the local neighborhood structure. In
    (Deng et al., [2019](#bib.bib18)), point cloud-FoldNet and point pair features-FoldNet
    are utilized to extract keypoints from the point cloud patch and obtain permutation-invariant
    descriptors. Additionally, a new pose estimation method in (Deng et al., [2019](#bib.bib18))
    is proposed that achieves faster and more robust results than random sample consensus
    (RANSAC) (Fischler and Bolles, [1981](#bib.bib22)). StickyPillars (Fischer et
    al., [2021](#bib.bib21)) integrates keypoint detection and descriptor extraction
    by jointly learning pixel-level and point-level feature descriptors. YOHO (Wang
    et al., [2022a](#bib.bib56)) and RoReg (Wang et al., [2023b](#bib.bib59)) employ
    advanced group equivariant feature learning techniques to achieve rotation invariance,
    enhancing robustness against variations in point density and noise. Furthermore,
    the rotation-equivariant component in YOHO and RoReg allows for estimation with
    just a single correspondence hypothesis, greatly reducing the search space for
    possible transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, voxel grids can achieve uniform sampling of point clouds by adopting
    grids of different, customizable sizes. 3DSmoothNet (Gojcic et al., [2019](#bib.bib24))
    adopts a voxelized smoothed density value technique, incorporating fully convolutional
    layers to model the local morphology of point clouds. It scrutinizes the local
    density estimates to accomplish PCR. SpinNet (Ao et al., [2021](#bib.bib1)) eliminates
    rotational variances by aligning with a reference axis and further reduces them
    through spherical voxelization and coordinate transformations. It then transforms
    point clouds into a manageable cylindrical volume and generates representative
    feature descriptors using cylindrical convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint-free involves considering all potential correspondences rather than
    detecting critical points. Within this perspective, there exist methods that utilize
    deep neural networks to directly obtain descriptors that encapsulate vital information.
    Subsequently, these descriptors are fed into a dedicated module responsible for
    estimating the transformation parameters. GeDi (Poiesi and Boscaini, [2022](#bib.bib45))
    operates by normalizing the local reference frame of the point cloud patch and
    then encoding it into the descriptors using a deep neural network. These descriptors
    are invariant to scale and rotation, making them effective for PCR across different
    application domains.
  prefs: []
  type: TYPE_NORMAL
- en: Other methodologies adopt a coarse-to-refine scheme, in which the matching outcomes
    are significantly influenced by the descriptors obtained in the initial coarse
    stage (Qin et al., [2023](#bib.bib48)). GeoTransformer (Qin et al., [2023](#bib.bib48))
    encodes the distance and angle information into the transformation representation,
    enabling effective capture of the geometric structure within individual point
    clouds and revealing the geometric consistency among the point clouds to be registered.
    From the viewpoint of considering point-wise and structure, OIF-PCR (Yang et al.,
    [2022](#bib.bib64)) employs an efficient and precise positional encoding strategy
    during the coarse stage, leveraging a limited number of correspondences. Simultaneously,
    a joint optimization approach is utilized to optimize the position encoding, progressively
    refining the point cloud features and reducing the reliance on initialization.
    RoITr (Yu et al., [2023a](#bib.bib67)) introduces an aggregation module using
    a rotation invariant Transformer (Vaswani et al., [2017](#bib.bib53)), which is
    strategically inserted between the encoder and decoder components. Its purpose
    is to facilitate the extraction of discriminative descriptors that are pose-agnostic
    and cross-frame position awareness.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint-based methods achieve precise matching with keypoint detection but
    face generalization challenges and are less efficient. Moreover, keypoint-free
    methods are robust in sparse, low-overlap point clouds but may lack detail accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Multi-view
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second perspective involves fusing information from multi-view. MVDesc (Zhou
    et al., [2018](#bib.bib78)) develops a multi-view local descriptor, which is derived
    from images captured from various viewpoints, specifically for characterizing
    the 3D keypoints. Subsequently, MVDesc advances a robust matching technique, aimed
    at rejecting outlier correspondences through efficient belief propagation inference
    within a defined graphical model. Li et al. (Li et al., [2020b](#bib.bib36)) integrate
    multi-view rendering into a neural network through a differentiable renderer,
    allowing the viewpoint to be an optimizable parameter for capturing more informative
    local context around the interest points. To obtain distinctive descriptors, Li
    et al. also design a soft view pooling module for fusing convolutional features
    from different views. Gojcic et al. (Gojcic et al., [2020](#bib.bib25)) utilize
    iteratively reweighted least squares (IRLS) as a global refinement technique to
    address the cycle consistency and alleviate the ambiguity of initial alignment
    in multi-view scanning. However, this approach relies on dense pairwise correspondences,
    which introduces significant computational overhead and increases the presence
    of outliers. Consequently, it becomes challenging for IRLS to accurately estimate
    the correct pose.
  prefs: []
  type: TYPE_NORMAL
- en: To address these limitations, Wang et al. (Wang et al., [2023a](#bib.bib58))
    propose a novel approach. They primarily concentrate on learning reliable initialization
    methods that consider the overlap between multiple point cloud pairs. This enables
    the construction of sparse yet reliable pose graphs. Furthermore, a history reweighting
    function is integrated into the IRLS framework, augmenting its generalization
    and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Correspondence Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, research has emerged for predicting correspondences between point
    clouds to be registered. These methods follow an end-to-end manner and often utilize
    existing point cloud feature extraction methods directly. Specifically, we further
    classify it into two categories according to the registration objects: full-object
    and partial-object.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Full-object
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The initial category involves full-object PCR, where each point is capable of
    identifying a unique counterpart in another point cloud. To address this problem,
    PointNetLK (Aoki et al., [2019](#bib.bib3)) and PointNetLK Revisited (Li et al.,
    [2021](#bib.bib37)), leverage the permutation-invariant network PointNet (Qi et
    al., [2017](#bib.bib47)) as adaptable imaging functions and integrate them into
    a recurrent Lucas-Kanade (Lucas and Kanade, [1981](#bib.bib41)) framework. DCP
    (Wang and Solomon, [2019a](#bib.bib54)) employs graph convolutional neural network
    and Transformer (Vaswani et al., [2017](#bib.bib53)) modeling to obtain feature
    representations and capture contextual information. Subsequently, the pointer
    generation mechanism is employed to estimate the correspondences.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Partial-object
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second category involves partial-object PCR, where not every point has a
    corresponding point in the other point cloud. Given the common scenario where
    only a subset of the point clouds to be registered exhibits correspondences, numerous
    noteworthy studies have emerged in the field of partial-to-partial PCR. These
    studies have a specific focus on overlap prediction and optimizing the similarity
    matrix accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Overlap prediction refers to estimating the overlap region between point clouds
    to be registered, and then directly finding correspondences in this region. To
    the best of our knowledge, Predator (Huang et al., [2021a](#bib.bib27)) is the
    pioneering model that introduces the concept of overlapping region prediction.
    Predator utilizes a joint encoder and decoder architecture, wherein a graph neural
    network and an overlap attention module are sequentially applied to enhance contextual
    relationships and predict the overlap score, respectively. Notably, the overlap
    attention module facilitates early-stage information interaction in the framework,
    which positively impacts the estimation of overlapping regions.
  prefs: []
  type: TYPE_NORMAL
- en: With reference to the above concept of information interaction, several approaches
    are proposed. OMNet (Xu et al., [2021](#bib.bib62)) introduces an innovative mask
    prediction module that possesses the capability to efficiently generate accurate
    overlapping masks. Moreover, OMNet establishes a direct connection between the
    intermediate layers of the mask prediction module and the transformation regression.
    This connection enables the simultaneous optimization of both the generation of
    overlapping masks and the estimation of transformation parameters. PCAM (Cao et
    al., [2021](#bib.bib6)) employs cross-attention matrices (CAM) to achieve feature
    augmentation. The CAM facilitates simultaneous focus on both shallow geometric
    information and deep contextual information, enabling the generation of more reliable
    matching features in overlapping regions.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, several methods enhance the prediction of overlapping regions by
    employing the Transformer for global modeling. STORM (Wang et al., [2022b](#bib.bib57))
    incorporates a differential sampling overlapping prediction module into dual Transformer
    (Vaswani et al., [2017](#bib.bib53)) layers, which facilitates information exchange
    between the before and after prediction phases. It employs a dedicated layer that
    iteratively applies the Gumbel-softmax technique, allowing for the independent
    sampling of points situated within overlapping regions. REGTR (Yew and Lee, [2022](#bib.bib66))
    leverages a main architecture composed of Transformer layers, which incorporate
    both self-attention and cross-attention mechanisms. These layers are proficient
    in facilitating the extraction of meaningful and enhanced features. Such an architectural
    selection empowers the network to accurately predict the probability of each point’s
    presence in overlapping regions and determine their corresponding positions in
    another point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the similarity matrix is a crucial aspect of fine-grained correspondence
    searching. The elements of the similarity matrix indicate the probability of correspondence
    between individual point pairs. Typically, a probability function is employed
    to compute the similarity matrix, followed by selecting the maximum value in each
    row or column to determine the most probable point pairs (Wang and Solomon, [2019a](#bib.bib54)).
    While softmax is frequently used as the probability function, it tends to produce
    a blurry correspondence map. To address this issue, numerous studies have emerged
    to mitigate the ambiguity. PRNet (Wang and Solomon, [2019b](#bib.bib55)) applies
    the Gumbel-softmax technique to obtain the similarity matrix, a method that finds
    hard correspondences and alleviates the ambiguity in correspondence search. In
    addition, to enhance the sharpness of the resultant similarity matrix, a temperature
    parameter is introduced into the Gumbel-softmax, which can be iteratively adjusted.
    RPMNet (Yew and Lee, [2020](#bib.bib65)) incorporates the optimal transport layer
    and annealing to learn a similarity matrix from a hybrid feature composed of spatial
    coordinates and geometric properties. FIRE-Net (Wu et al., [2021](#bib.bib61))
    facilitates feature interactions across various hierarchical levels of point clouds.
    Initially, FIRE-Net extracts structural features from the point cloud and fosters
    the interchange of feature information. This process permits points with high
    feature similarity to effectively perceive each other.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, full-object registration methods are impractical in real-world scenarios,
    as the point clouds subject to registration typically represent subset matches.
    The introduction of partial-object registration algorithms addresses this limitation,
    aligning more closely with practical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Outlier Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In PCR, outliers are defined as points lacking a corresponding counterpart.
    The principal objective of outlier filtering is to meticulously remove these outliers.
    Given their substantial influence on the outcomes of registration processes, the
    effective elimination of outliers is imperative to guarantee both robustness and
    accuracy. To identify outliers, 3DRegNet (Pais et al., [2020](#bib.bib44)) utilizes
    a deep neural network to estimate the probability of a point being classified
    as an outlier, which effectively minimizes the influence of hypothetical outliers
    during the registration procedure. DHVR (Lee et al., [2021](#bib.bib34)) places
    the initially predicted correspondences into a Hough voting module. This module
    casts votes in a deliberately sparse transformation parameter space, enhancing
    the accurate identification of inliers. Moreover, DLF (Zhang et al., [2022a](#bib.bib73))
    utilizes a classifier that combines the stacked order-aware modules to evaluate
    hypothesized outliers and determine the compatibility of hypothesized inliers.
  prefs: []
  type: TYPE_NORMAL
- en: The above methods directly estimate outliers after extracting features. However,
    during the feature extraction phase, they predominantly rely on methods like multilayer
    perceptron, inadvertently overlooking the critical aspect of the 3D spatial information.
    Furthermore, in classifying these features, each pair is assessed separately,
    ignoring the important consistency of inliers (Bai et al., [2021](#bib.bib5)).
    Based on the above thinking, PointDSC (Bai et al., [2021](#bib.bib5)) is proposed,
    which explicitly exploits the spatial compatibility inherently constructed by
    distance. It argues that not only should the relative distances of inliers between
    the point clouds to be registered remain consistent, but there also exists an
    inherent relationship among inliers within the single point cloud. Based on spatial
    compatibility, a second order spatial compatibility (Chen et al., [2022b](#bib.bib9))
    is proposed, which begins by converting the spatial compatibility matrix into
    a binary form and then calculates the similarity between two corresponding points
    based on the count of their mutually compatible points. This approach focusing
    on global rather than local compatibility, enhances early-stage differentiation
    between inliers and outliers. MAC (Zhang et al., [2023a](#bib.bib76)) loosens
    the maximum clique constraint and mines more local consistency information in
    the compatibility graph for accurate pose hypothesis generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Transformation Parameter Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The calculation of transformation parameters serves as the final step in PCR,
    with the widely adopted methods including RANSAC (Fischler and Bolles, [1981](#bib.bib22))
    and singular value decomposition (SVD) (Arun et al., [1987](#bib.bib4)). RANSAC
    is commonly employed during the coarse registration stage to mitigate the impact
    of outliers, and it requires a predetermined number of iterations to solve. Unlike
    RANSAC, SVD does not necessitate an iterative solution. It estimates the transformation
    parameters directly based on the pose difference between the two point clouds,
    thus requiring a reliable feature extraction network for accurate results (Zhang
    et al., [2022b](#bib.bib74)). The process of solving SVD reveals that the rotation
    matrix is computed prior to the calculation of the translation vector.
  prefs: []
  type: TYPE_NORMAL
- en: With the development of DL, some approaches strive to solve both rotation matrix
    and translation vector simultaneously using convolutional neural network (Deng
    et al., [2018](#bib.bib17); Pais et al., [2020](#bib.bib44)). The effectiveness
    of this idea is examined across multiple models. However, simultaneous resolution
    of transformation parameters can lead to mutual interference (Chen et al., [2022c](#bib.bib10)).
    To address this issue, DetarNet (Chen et al., [2022c](#bib.bib10)) employs Siamese
    networks to independently decouple transformation parameters in a two-step process.
    Initially, a regression network computes the translation vector, followed by the
    utilization of SVD to determine the rotation matrix. FINet (Xu et al., [2022](#bib.bib63))
    leverages point-wise and global features to enhance information association between
    point clouds to be registered at multiple stages. At the same time, a dual-branch
    structure containing a rotation regression branch and a translation regression
    branch is designed to predict the rotation matrix and translation vector, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Approaches to PCR optimization focus on enhancing the entire process. Based
    on the principles of their optimization, we divide them into two main categories:
    iterative closest point (ICP)-based methods and probability-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 ICP-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the classical ICP algorithm may be less effective than DL-based algorithms
    for PCR tasks, it still offers valuable advantages worth exploring. One such advantage
    is its iterative optimization thought, which has been widely adopted in various
    methods to refine estimation transformation parameters. In DCP (Wang and Solomon,
    [2019a](#bib.bib54)) and PRNet (Wang and Solomon, [2019b](#bib.bib55)), the entire
    network follows an iterative process to enhance the initial prediction of the
    rotation matrix and translation vector, progressively refining them from a coarse
    to a fine level. More specifically, before each iteration, the point cloud to
    be registered is updated with the transformation parameters estimated in the previous
    iteration. This incremental refinement process allows for the gradual improvement
    of the predicted transformation parameters. Contrasting with the previously discussed
    algorithms that iterate through the entire network, IDAM (Li et al., [2020a](#bib.bib35))
    distinctively positions the feature extraction component outside the iterative
    loop, which reduces the computational burden to a certain extent. Furthermore,
    it integrates distance information into the iterative network and incorporates
    a two-stage point elimination module. This design effectively filters out points
    that are detrimental to the registration process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Probabilistic-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Probability-based PCR algorithms integrate probabilistic knowledge within the
    registration framework to enhance the optimization process. These algorithms typically
    utilize probabilistic models to depict the matching relationship and inherent
    uncertainty between point clouds to be registered.
  prefs: []
  type: TYPE_NORMAL
- en: As a commonly used probability model, the Gaussian mixture model (GMM) finds
    optimal alignments by integrating an expectation-maximization (EM) method into
    a maximum likelihood framework (Eckart et al., [2018](#bib.bib20)). However, the
    EM process can be computationally intensive and potentially lead to incorrect
    data associations, especially in registrations with significant angular disparities
    (Yuan et al., [2020](#bib.bib69)). To address the aforementioned challenges, a
    technique called deep Gaussian mixture registration (DeepGMR) (Yuan et al., [2020](#bib.bib69))
    is proposed, which leverages a neural network to search correspondences between
    points and GMM parameters. Furthermore, two differentiable modules are employed
    to estimate the optimal transformation parameters. OGMM (Mei et al., [2023a](#bib.bib42))
    utilizes predictions of the overlapping area between two input point clouds for
    GMM estimation, framing the registration task as minimizing the variance between
    the two GMMs. In (Chen et al., [2023a](#bib.bib11)), GMM is formulated as a distribution
    that encompasses comprehensive representation capabilities, incorporating both
    global and local information.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to GMM, the Bayesian probabilistic model is also utilized for PCR.
    VBReg (Jiang et al., [2023](#bib.bib33)) introduces a variable non-local network
    architecture, which employs variational Bayesian inference for non-local feature
    learning. This approach enables the modeling of Bayesian-driven long-range dependencies
    and facilitates the acquisition of discriminative feature representations for
    inlier/outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Multimodal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original point cloud inherently possesses valuable structural information,
    which is crucial for accurate representation and analysis. The primary objective
    of current multimodal algorithms is to augment this structural data by incorporating
    texture information derived from images.
  prefs: []
  type: TYPE_NORMAL
- en: PCR-CG (Zhang et al., [2022c](#bib.bib75)) and PEAL (Yu et al., [2023b](#bib.bib68))
    employ a two-dimensional (2D) image matching technique to establish 2D correspondences,
    which are then projected onto point clouds using a 2D to 3D projection module,
    facilitating the identification of overlapping regions. ImLoveNet (Chen et al.,
    [2022a](#bib.bib8)) also utilizes images to enhance predictions in overlapping
    regions, directly employing cross-fusion technology to amalgamate the 3D features
    extracted directly from point clouds with the 3D features simulated from two-dimensional
    features derived from images. IMFNET (Huang et al., [2022c](#bib.bib31)) proposes
    an interpretable module to explain the contribution of the original points to
    the final descriptor. This approach significantly enhances both the transparency
    and effectiveness of the descriptor. GMF (Huang et al., [2022b](#bib.bib30)) integrates
    texture and structural information through a cross-attention fusion layer. Additionally,
    it incorporates a convolutional position encoding layer, which is instrumental
    in accentuating distinctions and focusing on neighboring information. Consequently,
    these enhancements contribute to improving correspondence quality and standard
    accuracy in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/555090e9b9ce93e5c84bc739ef7f0b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The pipeline of the unsupervised algorithm. The red arrows and green
    arrows represent correspondence-free and correspondence-based feature flows, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Unsupervised Point Cloud Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While supervised PCR algorithms demonstrate favorable outcomes, their success
    heavily depends on an extensive set of ground-truth transformations or correspondences
    as supervision signals during the model training process. Needless to say, acquiring
    such annotated data in real-world settings is often both challenging and costly,
    which limits the practical application scope of these supervised registration
    algorithms. Consequently, unsupervised PCR algorithms are explored. In this section,
    we divide unsupervised algorithms into two categories: correspondence-free and
    correspondence-based.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Correspondence-free
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, correspondence-based unsupervised registration methods first extract
    global features from the source and target point clouds and then minimize the
    difference between them to regress the transformation parameters. As depicted
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.6 Multimodal ‣ 3 Supervised Point Cloud Registration
    ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on Deep
    Learning"), the correspondence-free algorithms utilize the calculation of the
    distance between point clouds to define the loss function, termed distance loss.
    Typically, distance loss in the unsupervised methods uses CD, which measures the
    distance or feature differences between point pairs in two point clouds bidirectionally,
    with its formula is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{array}[]{rl}\mathcal{L}\left(\bm{X},\bm{Y}\right)=\frac{1}{N}\sum\limits_{\bm{x}\in\bm{X}}\min\limits_{\bm{y}\in\bm{Y}}\&#124;\bm{x}-\bm{y}\&#124;_{2}^{2}+\frac{1}{M}\sum\limits_{\bm{y}\in\bm{Y}}\min\limits_{\bm{x}\in\bm{X}}\&#124;\bm{y}-\bm{x}\&#124;_{2}^{2}.\end{array}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'In the field of correspondence-free unsupervised methods, an early significant
    contribution is PPF-FoldNet (Deng et al., [2018](#bib.bib17)), which starts by
    constructing four-dimensional (4D) point-pair features. These features are subsequently
    fed into an end-to-end architecture resembling a folded network, utilizing an
    encoder-decoder structure for reconstruction. The loss function involves comparing
    the CD between the 4D point pair features before and after reconstruction. Sun
    et al. (Sun et al., [2023](#bib.bib51)) have further developed the PointNetLK
    algorithm for use in cross-source PCR, employing global features for CD calculation.
    UGMM (Huang et al., [2022a](#bib.bib29)) presents a novel approach, redefining
    the PCR challenge as a clustering problem and estimating posterior probabilities
    through unsupervised learning. This method uses the CD between Gaussian mixtures
    derived from the point clouds as the loss function. UPCR (Zhang et al., [2021](#bib.bib72))
    introduces dual point cloud representations: pose-invariant and pose-related.
    The pose-related representations are leveraged to learn relative poses, which
    are essential for deriving transformation parameters. Moreover, the CD is also
    integrated into the loss function to evaluate the discrepancy between the source
    point cloud and the target point cloud. PCRNet (Sarode et al., [2019](#bib.bib49)),
    while following a similar approach as UPCR in designing its loss function, distinguishes
    itself by utilizing the earth mover’s distance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Correspondence-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared with the correspondence-free unsupervised method, the correspondence-based
    unsupervised method first extracts features, and then uses the correspondence
    relative step in Figure [3](#S3.F3 "Figure 3 ‣ 3.6 Multimodal ‣ 3 Supervised Point
    Cloud Registration ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration
    Based on Deep Learning") (including correspondence search or outlier filtering)
    to establish point-level, distribution-level, or cluster-level correspondences.
    Finally, the rigid transformation parameters are estimated from these correspondences.
    CEMNet (Jiang et al., [2021](#bib.bib32)) integrates the scaling estimator into
    the function that measures the registration error to weaken the negative impact
    of outliers on registration accuracy. CEMNet also uses CD as the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to CD, correspondence-based unsupervised algorithms also designed
    various other losses to refine aligned point clouds. RIENet (Shen et al., [2022](#bib.bib50))
    proposes a reliable inlier estimation module and designs the neighborhood consensus
    loss and spatial consistency loss to reduce the local differences and global differences
    of the point cloud to be registered. UDPReg (Mei et al., [2023b](#bib.bib43))
    finds correspondences from cluster-level and point-level, and designs self-consistency
    loss, cross-consistency loss, and local contrastive loss to enable unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges and Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Impressive outcomes have been yielded by the existing DL-based PCR algorithms.
    Here, we attempt to highlight the existing issues and identify open questions
    that may serve as a catalyst for future research.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Towards realistic data generation: A major challenge is bridging the gap between
    synthetic and real-world data. Most methods often rely on Gaussian noise to mimic
    realistic data, which fails to capture the complexity of actual data. Chen et
    al. (Chen et al., [2023c](#bib.bib13)) propose a new perspective that introduces
    the diffusion model to generate noisy training data. Future research can focus
    on integrating other generative models to simulate noise and occlusions, or developing
    data generation methodologies that can simulate realistic data independently of
    external networks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abundant multimodal information: Current multimodal PCR algorithms enhance
    feature representation by fusing image textures, which contributes to more accurate
    and detailed mapping. Future research could further enrich registration algorithms
    by integrating additional modalities information such as (i) topologically informed
    meshes, which offer advanced structural data, and (ii) semantic-level text labels
    embedded in large models, which provide contextual insights.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Designing new metrics: (Chen et al., [2023d](#bib.bib14)) designed a new metric
    that effectively achieves dual optimization in processing speed and registration
    accuracy. This advancement not only enhances the performance of existing registration
    networks but also opens new perspectives for PCR tasks. Future research can explore
    innovative evaluation metrics that comprehensively consider factors such as runtime
    speed, model size, and registration quality.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploiting pre-trained models: Many PCR algorithms are oriented towards the
    registration process to enhance the performance of registration. However, the
    integration of pre-trained models remains largely unexplored. Future research
    can (i) adapt existing pre-trained models for point cloud data, which could considerably
    reduce the data volume and computational resources needed for training models
    from scratch, and (ii) leverage features from pre-trained models, originally developed
    for other tasks, and apply them to PCR tasks, potentially leading to significant
    advancements and high efficiency.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper provides a comprehensive survey and taxonomy of the DL-based PCR
    algorithms. First, commonly used datasets and metrics are classified. Then, supervised
    and unsupervised registration algorithms are organized and analyzed from different
    technical perspectives. Finally, the issues worthy of attention in the future
    research of PCR are pointed out.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by the grant of the National Science Foundation
    of China under Grant 62172090.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ao et al. [2021] S. Ao, Q.g Hu, B. Yang, A. Markham, and Y. Guo. Spinnet: Learning
    a general surface descriptor for 3d point cloud registration. In CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ao et al. [2023] S. Ao, Q. Hu, H. Wang, K. Xu, and Y. Guo. Buffer: Balancing
    accuracy, efficiency, and generalizability in point cloud registration. In CVPR,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aoki et al. [2019] Y. Aoki, H. Goforth, R. Srivatsan, and L. Pointnetlk: Robust
    & efficient point cloud registration using pointnet. In CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arun et al. [1987] K. Arun, T. Huang, and S. Blostein. Least-squares fitting
    of two 3-d point sets. IEEE TPAMI, (5), 1987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. [2021] X. Bai, Z. Luo, L. Zhou, H. Chen, L. Li, Z. Hu, H. Fu, and
    C. Tai. Pointdsc: Robust point cloud registration using deep spatial consistency.
    In CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2021] A. Cao, G. Puy, A. Boulch, and R. Marlet. Pcam: Product of
    cross-attention matrices for rigid registration of point clouds. In ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. [2015] A. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,
    Z. Li, S. Savarese, M. Savva, S. Song, H. Su, Xiao. J, Yi L., and Yu F. Shapenet:
    An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022a] H. Chen, Z. Wei, Y. Xu, M. Wei, and J. Wang. Imlovenet:
    Misaligned image-supported registration network for low-overlap point cloud pairs.
    In SIGGRAPH, pages 1–9, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022b] Z. Chen, K. Sun, F. Yang, and W. Tao. Sc²-pcr: A second
    order spatial compatibility for efficient and robust point cloud registration.
    In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022c] Z. Chen, F. Yang, and W. Tao. Detarnet: Decoupling translation
    and rotation by siamese network for point cloud registration. In AAAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023a] H. Chen, B. Chen, Z. Zhao, and B. Song. Point cloud registration
    based on learning gaussian mixture models with global-weighted local representations.
    IEEE Geosci. Remote Sens. Lett., 20:1–5, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023b] S. Chen, H. Xu, R. Li, G. Liu, C. Fu, and S. Liu. Sira-pcr:
    Sim-to-real adaptation for 3d point cloud registration. In ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023c] Z. Chen, Y. Ren, T. Zhang, Z. Dang, W. Tao, S. Süsstrunk,
    and M. Salzmann. Diffusionpcr: Diffusion models for robust multi-step point cloud
    registration. arXiv preprint arXiv:2312.03053, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023d] Z. Chen, K. Sun, F. Yang, L. Guo, and W. Tao. Sc²-pcr++:
    Rethinking the generation and selection for efficient and robust boint cloud registration.
    IEEE TPAMI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. [2015] S. Choi, Q. Zhou, and V. Koltun. Robust reconstruction of
    indoor scenes. In CVPR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curless and Levoy [1996] B. Curless and M. Levoy. A volumetric method for building
    complex models from range images. In SIGGRAPH, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2018] H. Deng, T. Birdal, and S. Ilic. Ppf-foldnet: Unsupervised
    learning of rotation invariant 3d local descriptors. In ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2019] H. Deng, T. Birdal, and S. Ilic. 3d local features for direct
    pairwise registration. In CVPR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. [2020] Z. Dong, F. Liang, B. Yang, Yu. Xu, Y. Zang, J. Li, Y. Wang,
    W. Dai, H. Fan, J. Hyyppä, and Stilla U. Registration of large-scale terrestrial
    laser scanner point clouds: A review and benchmark. ISPRS J. Photogramm. Remote
    Sens., 163:327–342, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eckart et al. [2018] B. Eckart, K. Kim, and J. Kautz. Hgmr: Hierarchical gaussian
    mixtures for adaptive 3d registration. In ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fischer et al. [2021] K. Fischer, M. Simon, F. Olsner, S. Milz, H. Gross, and
    P. Mader. Stickypillars: Robust and efficient feature matching on point clouds
    using graph neural networks. In CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fischler and Bolles [1981] M. Fischler and R. Bolles. Random sample consensus:
    a paradigm for model fitting with applications to image analysis and automated
    cartography. Communications of the ACM, 24(6):381–395, 1981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geiger et al. [2012] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous
    driving? the kitti vision benchmark suite. In CVPR, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gojcic et al. [2019] Z. Gojcic, C. Zhou, J. Wegner, and A. Wieser. The perfect
    match: 3d point cloud matching with smoothed densities. In CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gojcic et al. [2020] Z. Gojcic, C. Zhou, J. D Wegner, L. Guibas, and T. Birdal.
    Learning multiview 3d point cloud registration. In CVPR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. [2020] X. Gu, X. Wang, and Y. Guo. A review of research on point
    cloud registration methods. In IOP Conf. Ser.: Mater. Sci. Eng., volume 782, page
    022070. IOP Publishing, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2021a] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler.
    Predator: Registration of 3d point clouds with low overlap. In CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2021b] X. Huang, G. Mei, J. Zhang, and R. Abbas. A comprehensive
    survey on point cloud registration. arXiv preprint arXiv:2103.02690, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2022a] X. Huang, S. Li, Y. Zuo, Y. Fang, J. Zhang, and X. Zhao.
    Unsupervised point cloud registration by learning unified gaussian mixture models.
    IEEE Robotics Autom. Lett., 7(3):7028–7035, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022b] X. Huang, W. Qu, Y. Zuo, Y. Fang, and X. Zhao. Gmf: General
    multimodal fusion framework for correspondence outlier rejection. IEEE Robotics
    Autom. Lett., 7(4):12585–12592, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022c] X. Huang, W. Qu, Y. Zuo, Y. Fang, and X. Zhao. Imfnet:
    Interpretable multimodal fusion for point cloud registration. IEEE Robotics Autom.
    Lett., 7(4):12323–12330, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2021] H. Jiang, Y. Shen, J. Xie, J. Li, Ji. Qian, and J. Yang.
    Sampling network guided cross-entropy method for unsupervised point cloud registration.
    In ICCV, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2023] H. Jiang, Z. Dang, Z. Wei, J. Xie, J. Yang, and M. Salzmann.
    Robust outlier rejection for 3d registration with variational bayes. In CVPR,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2021] J. Lee, S. Kim, M. Cho, and J. Park. Deep hough voting for
    robust global registration. In ICCV, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2020a] J. Li, C. Zhang, Z. Xu, H. Zhou, and C. Zhang. Iterative distance-aware
    similarity matrix convolution with mutual-supervised point elimination for efficient
    point cloud registration. In ECCV, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2020b] L. Li, S. Zhu, H. Fu, P. Tan, and C. Tai. End-to-end learning
    local multi-view descriptors for 3d point clouds. In CVPR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2021] X. Li, J. Pontes, and L. Pointnetlk revisited. In CVPR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2019a] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan, and S. Song. Deepvcp:
    An end-to-end deep neural network for point cloud registration. In CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2019b] W. Lu, Y. Zhou, G. Wan, S. Hou, and S. Song. L3-net: Towards
    learning based lidar localization for autonomous driving. In CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2021] F. Lu, G. Chen, Y. Liu, L. Zhang, S. Qu, S. Liu, and R. Gu.
    Hregnet: A hierarchical network for large-scale outdoor lidar point cloud registration.
    In ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lucas and Kanade [1981] B. Lucas and T. Kanade. An iterative image registration
    technique with an application to stereo vision. In IJCAI, 1981.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mei et al. [2023a] G. Mei, F. Poiesi, C. Saltori, J. Zhang, E. Ricci, and N. Sebe.
    Overlap-guided gaussian mixture models for point cloud registration. In WACV,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mei et al. [2023b] G. Mei, H. Tang, X. Huang, W. Wang, J. Liu, J. Zhang, Van
    G., and Q. Wu. Unsupervised deep probabilistic approach for partial point cloud
    registration. In CVPR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pais et al. [2020] G. Pais, S. Ramalingam, V. Govindu, J. Nascimento, R. Chellappa,
    and P. Miraldo. 3dregnet: A deep neural network for 3d point registration. In
    CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poiesi and Boscaini [2022] F. Poiesi and D. Boscaini. Learning general and distinctive
    3d local deep descriptors for point cloud registration. IEEE TPAMI, 45(3), 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pomerleau et al. [2012] F. Pomerleau, M. Liu, F. Colas, and R. Siegwart. Challenging
    data sets for point cloud registration algorithms. The International Journal of
    Robotics Research, 31(14):1705–1711, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. [2017] C. Qi, H. Su, K. Mo, and L. Guibas. Pointnet: Deep learning
    on point sets for 3d classification and segmentation. In CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. [2023] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, S. Ilic, D. Hu,
    and K. Xu. Geotransformer: Fast and robust point cloud registration with geometric
    transformer. IEEE TPAMI, 45(8):9806–9821, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarode et al. [2019] V. Sarode, X. Li, H. Goforth, Y. Aoki, R. Srivatsan, and
    S. Lucey. Pcrnet: Point cloud registration network using pointnet encoding. arXiv
    preprint arXiv:1908.07906, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. [2022] Y. Shen, L. Hui, H. Jiang, J. Xie, and J. Yang. Reliable
    inlier evaluation for unsupervised point cloud registration. In AAAI, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023] X. Sun, W. Li, J. Huang, D. Chen, and T. Jia. Research and
    application on cross-source point cloud registration method based on unsupervised
    learning. In CYBER, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Uy et al. [2019] M. Uy, Q. Pham, B. Hua, T. Nguyen, and S. Yeung. Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data. In ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Solomon [2019a] Y. Wang and J. Solomon. Deep closest point: Learning
    representations for point cloud registration. In ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Solomon [2019b] Y. Wang and J. Solomon. Prnet: Self-supervised learning
    for partial-to-partial registration. In NeurIPS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022a] H. Wang, Y. Liu, Z. Dong, and W. Wang. You only hypothesize
    once: Point cloud registration with rotation-equivariant descriptors. In ACM MM,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022b] Y. Wang, C. Yan, Y. Feng, S. Du, Q. Dai, and Y. Gao. Storm:
    Structure-based overlap matching for partial point cloud registration. IEEE TPAMI,
    45(1):1135–1149, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023a] H. Wang, Y. Liu, Z. Dong, Y. Guo, Y. Liu, W. Wang, and B. Yang.
    Robust multiview point cloud registration with reliable pose graph initialization
    and history reweighting. In CVPR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023b] H. Wang, Y. Liu, Q. Hu, B. Wang, J. Chen, Z. Dong, Y. Guo,
    W. Wang, and B. Yang. Roreg: Pairwise point cloud registration with oriented descriptors
    and local rotations. IEEE TPAMI, 45(8), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2015] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, and J. Tang, X.and Xiao.
    3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] B. Wu, J. Ma, G. Chen, and P. An. Feature interactive representation
    for point cloud registration. In ICCV, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng. Omnet: Learning
    overlapping mask for partial-to-partial point cloud registration. In ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2022] H. Xu, N. Ye, G. Liu, B. Zeng, and S. Liu. Finet: Dual branches
    feature interaction for partial-to-partial point cloud registration. In AAAI,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2022] F. Yang, L. Guo, Z. Chen, and W. Tao. One-inlier is first:
    Towards efficient position encoding for point cloud registration. In NeurIPS,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yew and Lee [2020] Z. Yew and G. Lee. Rpm-net: Robust point matching using
    learned features. In CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yew and Lee [2022] Z. Yew and G. Lee. Regtr: End-to-end point cloud correspondences
    with transformers. In CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. [2023a] H. Yu, Z. Qin, J. Hou, M. Saleh, D. Li, B. Busam, and S. Ilic.
    Rotation-invariant transformer for point cloud matching. In CVPR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2023b] J. Yu, L. Ren, Y. Zhang, W. Zhou, L. Lin, and G. Dai. Peal:
    Prior-embedded explicit attention learning for low-overlap point cloud registration.
    In CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2020] W. Yuan, B. Eckart, K. Kim, V. Jampani, D. Fox, and J. Kautz.
    Deepgmr: Learning latent gaussian mixture models for registration. In ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. [2017] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser.
    3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In CVPR,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Z. Zhang, Y. Dai, and J. Sun. Deep learning based point
    cloud registration: an overview. Virtual Real. Intell. Hardw., 2(3):222–246, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2021] Z. Zhang, J. Sun, Y. Dai, D. Zhou, X. Song, and M. He. A
    representation separation perspective to correspondence-free unsupervised 3-d
    point cloud registration. IEEE Geosci. Remote Sens. Lett., 19:1–5, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022a] Y. Zhang, Z. Sun, Z. Zeng, and K. Lam. Partial point cloud
    registration with deep local feature. IEEE TAI, 4(5):1317–1327, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022b] Y. Zhang, Z. Sun, Z. Zeng, and K. Lam. Point cloud registration
    using multiattention mechanism and deep hybrid features. IEEE Intell. Syst., 38(1):58–68,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022c] Y. Zhang, J. Yu, X. Huang, W. Zhou, and J. Hou. Pcr-cg:
    Point cloud registration via deep explicit color and geometry. In ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023a] X. Zhang, J. Yang, S. Zhang, and Y. Zhang. 3d registration
    with maximal cliques. In CVPR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023b] Z. Zhang, W. Sun, X. Min, Q. Zhou, J. He, Q. Wang, and
    G. Zhai. Mm-pcqa: Multi-modal learning for no-reference point cloud quality assessment.
    In IJCAI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2018] L. Zhou, S. Zhu, Z. Luo, T. Shen, R. Zhang, M. Zhen, T. Fang,
    and L. Quan. Learning and matching multi-view descriptors for registration of
    point clouds. In ECCV, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
