- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:33:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:33:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.13830] A Comprehensive Survey and Taxonomy on Point Cloud Registration
    Based on Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.13830] 基于深度学习的点云配准综合调查与分类'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13830](https://ar5iv.labs.arxiv.org/html/2404.13830)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13830](https://ar5iv.labs.arxiv.org/html/2404.13830)
- en: A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on Deep
    Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的点云配准综合调查与分类
- en: Yu-Xin Zhang¹    Jie Gui¹¹¹1Corresponding Author    Xiaofeng Cong¹    Xin Gong¹&Wenbing
    Tao²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张玉欣¹    桂杰¹¹¹1通讯作者    宋晓峰¹    龚鑫¹&陶文兵²
- en: ¹Southeast University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹东南大学
- en: ²Huazhong University of Science and Technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²华中科技大学
- en: '{yuxinzhang, guijie}@seu.edu.cn, cxf_svip@163.com, xingong@seu.edu.cn, wenbingtao@hust.edu.cn'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{yuxinzhang, guijie}@seu.edu.cn, cxf_svip@163.com, xingong@seu.edu.cn, wenbingtao@hust.edu.cn'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Point cloud registration (PCR) involves determining a rigid transformation
    that aligns one point cloud to another. Despite the plethora of outstanding deep
    learning (DL)-based registration methods proposed, comprehensive and systematic
    studies on DL-based PCR techniques are still lacking. In this paper, we present
    a comprehensive survey and taxonomy of recently proposed PCR methods. Firstly,
    we conduct a taxonomy of commonly utilized datasets and evaluation metrics. Secondly,
    we classify the existing research into two main categories: supervised and unsupervised
    registration, providing insights into the core concepts of various influential
    PCR models. Finally, we highlight open challenges and potential directions for
    future research. A curated collection of valuable resources is made available
    at https://github.com/yxzhang15/PCR.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 点云配准（PCR）涉及确定一个刚性变换，以使一个点云与另一个点云对齐。尽管提出了众多基于**深度学习（DL）**的优秀配准方法，但对基于DL的PCR技术的全面系统研究仍然不足。本文提供了对最近提出的PCR方法的全面调查和分类。首先，我们对常用的数据集和评价指标进行了分类。其次，我们将现有研究分为两大类：监督式和无监督式配准，提供了对各种有影响力的PCR模型核心概念的见解。最后，我们强调了开放性挑战和未来研究的潜在方向。一个精心整理的有价值资源集合可在[https://github.com/yxzhang15/PCR](https://github.com/yxzhang15/PCR)找到。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the progress of sensor technology, acquiring high-precision point cloud
    data has become more accessible and prevalent (Zhang et al., [2023b](#bib.bib77);
    Uy et al., [2019](#bib.bib52)). Point cloud registration (PCR), as a pivotal tool
    in point cloud data processing, aims to align the point cloud data with a common
    coordinate system, enabling precise three-dimensional (3D) modeling (Qin et al.,
    [2023](#bib.bib48); Chen et al., [2022b](#bib.bib9)). This registration process
    establishes a dependable foundation for point cloud analysis and various applications (Huang
    et al., [2021b](#bib.bib28)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着传感器技术的进步，获取高精度点云数据变得更加容易和普遍（张等，[2023b](#bib.bib77)；Uy等，[2019](#bib.bib52)）。点云配准（PCR）作为点云数据处理中的关键工具，旨在将点云数据对齐到一个共同的坐标系统中，实现精确的三维（3D）建模（秦等，[2023](#bib.bib48)；陈等，[2022b](#bib.bib9)）。这一配准过程为点云分析及其各种应用奠定了可靠的基础（黄等，[2021b](#bib.bib28)）。
- en: 'Given the rapid advancements in this field, hundreds of deep learning (DL)-based
    methods have been proposed. There is an urgent need for thorough investigations
    to both inspire and steer future research endeavors. To address this necessity,
    we develop a comprehensive survey and establish a detailed taxonomy of PCR algorithms.
    This study categorizes these algorithms into two types: supervised and unsupervised.
    Supervised registration, leveraging labeled data that typically encompasses known
    transformations between point clouds, orchestrates the training process. In contrast,
    unsupervised registration hinges on the intrinsic geometric properties of the
    point clouds, independent of external labels. For supervised algorithms, the taxonomy
    is segmented into four crucial stages and two overarching concepts. The four stages
    include descriptor extraction, correspondence search, outlier filtering, and transformation
    parameter estimation, while the two concepts encompass optimization and multimodal.
    The supervised algorithms are systematically categorized based on their contributions
    to every stage or integration of concepts. Furthermore, for unsupervised algorithms,
    our taxonomy differentiates between two methodologies: the correspondence-free
    approaches, which align point clouds by minimizing feature discrepancies, and
    the correspondence-based approaches, which align point clouds by establishing
    correspondences.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一领域的快速进展，已有数百种基于深度学习（DL）的方法被提出。迫切需要深入研究以启发和指导未来的研究工作。为满足这一需求，我们开发了一项全面的调查，并建立了
    PCR 算法的详细分类。本研究将这些算法分为两类：监督式和无监督式。监督式配准利用通常包含点云之间已知变换的标记数据来组织训练过程。相对而言，无监督式配准依赖于点云的内在几何特性，不依赖于外部标签。对于监督式算法，分类法分为四个关键阶段和两个总体概念。这四个阶段包括描述符提取、对应关系搜索、离群点过滤和变换参数估计，而两个概念则涵盖优化和多模态。监督式算法基于它们对每个阶段或概念集成的贡献进行系统分类。此外，对于无监督式算法，我们的分类法区分了两种方法：无对应方法通过最小化特征差异来对齐点云，而对应方法则通过建立对应关系来对齐点云。
- en: Goals of our survey. We aim to (i) classify commonly used datasets and metrics
    in PCR tasks; (ii) develop a taxonomy for DL-based registration algorithms, introducing
    core techniques employed across various methods; and (iii) identify open issues
    that could stimulate further research in PCR tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查的目标。我们旨在 (i) 分类 PCR 任务中常用的数据集和度量； (ii) 开发 DL 基于配准算法的分类法，引入各种方法中采用的核心技术；
    (iii) 识别可能激发进一步研究的开放问题。
- en: The differences between our survey and others. (Gu et al., [2020](#bib.bib26))
    only review traditional PCR algorithms, DL-based algorithms are not involved.
    (Zhang et al., [2020](#bib.bib71)) and (Huang et al., [2021b](#bib.bib28)) conduct
    a summary of DL-based PCR algorithms. However, recent advances in unsupervised
    algorithms are not elaborated. Additionally, they did not provide a comprehensive
    overview of the latest research developments in the PCR field. To address these
    gaps, our study conducts a comprehensive survey and taxonomy of DL-based supervised
    and unsupervised registration algorithms. The taxonomy is concisely summarized
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Definition ‣ 2 Related Work ‣ A Comprehensive
    Survey and Taxonomy on Point Cloud Registration Based on Deep Learning"), providing
    a clear and structured overview of the PCR algorithms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查与其他调查的区别在于。 (Gu et al., [2020](#bib.bib26)) 仅回顾了传统的 PCR 算法，而未涉及基于 DL 的算法。
    (Zhang et al., [2020](#bib.bib71)) 和 (Huang et al., [2021b](#bib.bib28)) 总结了基于
    DL 的 PCR 算法。然而，最近在无监督算法方面的进展没有详细阐述。此外，他们未提供 PCR 领域最新研究进展的全面概述。为解决这些问题，我们的研究对基于
    DL 的监督式和无监督式配准算法进行了全面调查和分类。该分类法在图 [1](#S2.F1 "Figure 1 ‣ 2.1 Definition ‣ 2 Related
    Work ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on
    Deep Learning") 中简要总结，为 PCR 算法提供了清晰且结构化的概述。
- en: 2 Related Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Definition
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 定义
- en: The goal of PCR is to find the optimal rotation $\bm{R}^{*}$ and translation
    $\bm{t}^{*}$ parameters that align source point cloud $\bm{X}\in\mathbb{R}^{N\times
    3}$ and target point cloud $\bm{Y}\in\mathbb{R}^{M\times 3}$. Here, $N$ and $M$
    represent the number of points in $\bm{X}$ and $\bm{Y}$, respectively. The mathematical
    objective of the PCR process is formulated by
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PCR 的目标是找到最佳的旋转 $\bm{R}^{*}$ 和平移 $\bm{t}^{*}$ 参数，使源点云 $\bm{X}\in\mathbb{R}^{N\times
    3}$ 和目标点云 $\bm{Y}\in\mathbb{R}^{M\times 3}$ 对齐。这里，$N$ 和 $M$ 分别表示 $\bm{X}$ 和 $\bm{Y}$
    中点的数量。PCR 过程的数学目标为
- en: '|  | $(\bm{R}^{*},\bm{t}^{*})=\mathop{argmin}\limits_{\bm{R}\in SO(3),\bm{t}\in\mathbb{R}^{3}}\sum_{p=1}^{P}\&#124;(\bm{R}\bm{x}_{p}+\bm{t})-\bm{y}_{p}\&#124;^{2},$
    |  | (1) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\bm{R}^{*},\bm{t}^{*})=\mathop{argmin}\limits_{\bm{R}\in SO(3),\bm{t}\in\mathbb{R}^{3}}\sum_{p=1}^{P}\&#124;(\bm{R}\bm{x}_{p}+\bm{t})-\bm{y}_{p}\&#124;^{2},$
    |  | (1) |'
- en: where $\bm{x}_{p},\bm{y}_{p}\in\mathbb{R}^{1\times 3}$ are the p-th points in
    $\bm{X}$ and $\bm{Y}$, while $P$ denotes the number of correspondences between
    $\bm{X}$ and $\bm{Y}$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x}_{p},\bm{y}_{p}\in\mathbb{R}^{1\times 3}$ 是 $\bm{X}$ 和 $\bm{Y}$ 中的第
    p 个点，而 $P$ 表示 $\bm{X}$ 和 $\bm{Y}$ 之间的对应点数量。
- en: <svg   height="572.59" overflow="visible" version="1.1" width="703.72"><g transform="translate(0,572.59)
    matrix(1 0 0 -1 0 0) translate(16.98,0) translate(0,494.09)"><g stroke="#000000"
    fill="#000000"><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -11.81 -1.38)"><foreignobject width="23.62" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PCR <g stroke="#3C8C93" stroke-width="0.8pt"><path
    d="M 84.21 17.72 L 34.88 17.72 C 31.82 17.72 29.34 15.24 29.34 12.18 L 29.34 -12.18
    C 29.34 -15.24 31.82 -17.72 34.88 -17.72 L 84.21 -17.72 C 87.27 -17.72 89.75 -15.24
    89.75 -12.18 L 89.75 12.18 C 89.75 15.24 87.27 17.72 84.21 17.72 Z M 29.34 -17.72"
    style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 33.95 3.42)"><foreignobject width="51.18" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 168.16 29.52 L 108.2 29.52
    C 105.14 29.52 102.66 27.04 102.66 23.98 L 102.66 -23.98 C 102.66 -27.04 105.14
    -29.52 108.2 -29.52 L 168.16 -29.52 C 171.22 -29.52 173.7 -27.04 173.7 -23.98
    L 173.7 23.98 C 173.7 27.04 171.22 29.52 168.16 29.52 Z M 102.66 -29.52" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 15.22)"><foreignobject width="61.81" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Descriptor Extraction</foreignobject></g> <g
    stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 15.75 L 192.15 15.75 C
    189.09 15.75 186.62 13.27 186.62 10.21 L 186.62 -10.21 C 186.62 -13.27 189.09
    -15.75 192.15 -15.75 L 245.42 -15.75 C 248.48 -15.75 250.96 -13.27 250.96 -10.21
    L 250.96 10.21 C 250.96 13.27 248.48 15.75 245.42 15.75 Z M 186.62 -15.75" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 191.23 -1.38)"><foreignobject width="55.12" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Two-view</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 245.42 -62.13 L 192.15 -62.13 C 189.09 -62.13
    186.62 -64.61 186.62 -67.67 L 186.62 -88.38 C 186.62 -91.44 189.09 -93.92 192.15
    -93.92 L 245.42 -93.92 C 248.48 -93.92 250.96 -91.44 250.96 -88.38 L 250.96 -67.67
    C 250.96 -64.61 248.48 -62.13 245.42 -62.13 Z M 186.62 -93.92" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 191.23 -76.43)"><foreignobject width="55.12" height="22.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Multi-view</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 317.4 17.72 L 269.41 17.72 C 266.35 17.72 263.88
    15.24 263.88 12.18 L 263.88 -12.18 C 263.88 -15.24 266.35 -17.72 269.41 -17.72
    L 317.4 -17.72 C 320.45 -17.72 322.93 -15.24 322.93 -12.18 L 322.93 12.18 C 322.93
    15.24 320.45 17.72 317.4 17.72 Z M 263.88 -17.72" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 269.78 3.42)"><foreignobject width="47.24" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Keypoint-based</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 317.4 -28.67 L 269.41 -28.67 C 266.35 -28.67 263.88
    -31.15 263.88 -34.2 L 263.88 -58.57 C 263.88 -61.63 266.35 -64.11 269.41 -64.11
    L 317.4 -64.11 C 320.45 -64.11 322.93 -61.63 322.93 -58.57 L 322.93 -34.2 C 322.93
    -31.15 320.45 -28.67 317.4 -28.67 Z M 263.88 -64.11" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 269.78 -42.97)"><foreignobject width="47.24" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Keypoint-free</foreignobject></g> <g stroke="#3C8C93"
    stroke-width="0.8pt"><path d="M 168.16 -111.33 L 108.2 -111.33 C 105.14 -111.33
    102.66 -113.81 102.66 -116.87 L 102.66 -157.84 C 102.66 -160.9 105.14 -163.38
    108.2 -163.38 L 168.16 -163.38 C 171.22 -163.38 173.7 -160.9 173.7 -157.84 L 173.7
    -116.87 C 173.7 -113.81 171.22 -111.33 168.16 -111.33 Z M 102.66 -163.38" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 -125.63)"><foreignobject width="61.81" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Correspondence Search</foreignobject></g> <g
    stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -120.19 L 192.15 -120.19
    C 189.09 -120.19 186.62 -122.66 186.62 -125.72 L 186.62 -148.99 C 186.62 -152.05
    189.09 -154.52 192.15 -154.52 L 245.42 -154.52 C 248.48 -154.52 250.96 -152.05
    250.96 -148.99 L 250.96 -125.72 C 250.96 -122.66 248.48 -120.19 245.42 -120.19
    Z M 186.62 -154.52" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -134.48)"><foreignobject
    width="55.12" height="25.12" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Full-object</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -158.39 L 192.15 -158.39
    C 189.09 -158.39 186.62 -160.87 186.62 -163.92 L 186.62 -190.61 C 186.62 -193.67
    189.09 -196.15 192.15 -196.15 L 245.42 -196.15 C 248.48 -196.15 250.96 -193.67
    250.96 -190.61 L 250.96 -163.92 C 250.96 -160.87 248.48 -158.39 245.42 -158.39
    Z M 186.62 -196.15" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -172.69)"><foreignobject
    width="55.12" height="28.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Partial-object</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 346.31 -159.55 L 269.41 -159.55
    C 266.35 -159.55 263.88 -162.03 263.88 -165.08 L 263.88 -189.45 C 263.88 -192.51
    266.35 -194.99 269.41 -194.99 L 346.31 -194.99 C 349.36 -194.99 351.84 -192.51
    351.84 -189.45 L 351.84 -165.08 C 351.84 -162.03 349.36 -159.55 346.31 -159.55
    Z M 263.88 -194.99" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 268.49 -173.85)"><foreignobject
    width="78.74" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Overlap
    Prediction</foreignobject></g> <g stroke="#3C8C93" stroke-width="0.8pt"><path
    d="M 346.31 -198.85 L 269.41 -198.85 C 266.35 -198.85 263.88 -201.33 263.88 -204.38
    L 263.88 -244.99 C 263.88 -248.04 266.35 -250.52 269.41 -250.52 L 346.31 -250.52
    C 349.36 -250.52 351.84 -248.04 351.84 -244.99 L 351.84 -204.38 C 351.84 -201.33
    349.36 -198.85 346.31 -198.85 Z M 263.88 -250.52" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 268.49 -213.15)"><foreignobject width="78.74" height="42.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Optimizing the Similarity Matrix</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 168.16 -238.1 L 108.2 -238.1
    C 105.14 -238.1 102.66 -240.58 102.66 -243.64 L 102.66 -270.7 C 102.66 -273.75
    105.14 -276.23 108.2 -276.23 L 168.16 -276.23 C 171.22 -276.23 173.7 -273.75 173.7
    -270.7 L 173.7 -243.64 C 173.7 -240.58 171.22 -238.1 168.16 -238.1 Z M 102.66
    -276.23" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 107.28 -252.4)"><foreignobject width="61.81"
    height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Outlier Filtering</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 168.75 -280.1 L 107.61 -280.1
    C 104.55 -280.1 102.07 -282.57 102.07 -285.63 L 102.07 -366.81 C 102.07 -369.86
    104.55 -372.34 107.61 -372.34 L 168.75 -372.34 C 171.81 -372.34 174.29 -369.86
    174.29 -366.81 L 174.29 -285.63 C 174.29 -282.57 171.81 -280.1 168.75 -280.1 Z
    M 102.07 -372.34" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 106.69 -294.39)"><foreignobject
    width="62.99" height="83.02" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformation
    Parameter Estimation</foreignobject></g> <g stroke="#3C8C93" stroke-width="0.8pt"><path
    d="M 168.16 -376.2 L 108.2 -376.2 C 105.14 -376.2 102.66 -378.68 102.66 -381.74
    L 102.66 -402.45 C 102.66 -405.51 105.14 -407.99 108.2 -407.99 L 168.16 -407.99
    C 171.22 -407.99 173.7 -405.51 173.7 -402.45 L 173.7 -381.74 C 173.7 -378.68 171.22
    -376.2 168.16 -376.2 Z M 102.66 -407.99" style="fill:none"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 107.28 -390.5)"><foreignobject
    width="61.81" height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Optimization</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -374.38 L 192.15 -374.38
    C 189.09 -374.38 186.62 -376.86 186.62 -379.91 L 186.62 -404.28 C 186.62 -407.34
    189.09 -409.82 192.15 -409.82 L 245.42 -409.82 C 248.48 -409.82 250.96 -407.34
    250.96 -404.28 L 250.96 -379.91 C 250.96 -376.86 248.48 -374.38 245.42 -374.38
    Z M 186.62 -409.82" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -388.68)"><foreignobject
    width="55.12" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ICP-based</foreignobject></g>
    <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M 245.42 -413.68 L 192.15 -413.68
    C 189.09 -413.68 186.62 -416.16 186.62 -419.21 L 186.62 -460.18 C 186.62 -463.24
    189.09 -465.72 192.15 -465.72 L 245.42 -465.72 C 248.48 -465.72 250.96 -463.24
    250.96 -460.18 L 250.96 -419.21 C 250.96 -416.16 248.48 -413.68 245.42 -413.68
    Z M 186.62 -465.72" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -427.98)"><foreignobject
    width="55.12" height="42.82" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Probabilistic
    -based</foreignobject></g> <g stroke="#3C8C93" stroke-width="0.8pt"><path d="M
    168.16 -447.68 L 108.2 -447.68 C 105.14 -447.68 102.66 -450.16 102.66 -453.21
    L 102.66 -477.58 C 102.66 -480.64 105.14 -483.12 108.2 -483.12 L 168.16 -483.12
    C 171.22 -483.12 173.7 -480.64 173.7 -477.58 L 173.7 -453.21 C 173.7 -450.16 171.22
    -447.68 168.16 -447.68 Z M 102.66 -483.12" style="fill:none"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 107.28 -461.98)"><foreignobject
    width="61.81" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multimodal</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 678.91 77.95
    L 341.38 77.95 C 338.33 77.95 335.85 75.47 335.85 72.41 L 335.85 -72.41 C 335.85
    -75.47 338.33 -77.95 341.38 -77.95 L 678.91 -77.95 C 681.97 -77.95 684.44 -75.47
    684.44 -72.41 L 684.44 72.41 C 684.44 75.47 681.97 77.95 678.91 77.95 Z M 335.85
    -77.95"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 340.46 63.65)"><foreignobject width="339.37" height="146.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DeepVCP (Lu et al., [2019a](#bib.bib38)), 3DSmoothNet
    (Gojcic et al., [2019](#bib.bib24)), Deng et al. (Deng et al., [2019](#bib.bib18)),
    HRegNet (Lu et al., [2021](#bib.bib40)), SpinNet (Ao et al., [2021](#bib.bib1)),
    StickyPillars (Fischer et al., [2021](#bib.bib21)), YOHO (Wang et al., [2022a](#bib.bib56)),
    RoReg (Wang et al., [2023b](#bib.bib59)), BUFFER (Ao et al., [2023](#bib.bib2)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 678.91 -9.95
    L 341.38 -9.95 C 338.33 -9.95 335.85 -12.43 335.85 -15.48 L 335.85 -77.29 C 335.85
    -80.35 338.33 -82.82 341.38 -82.82 L 678.91 -82.82 C 681.97 -82.82 684.44 -80.35
    684.44 -77.29 L 684.44 -15.48 C 684.44 -12.43 681.97 -9.95 678.91 -9.95 Z M 335.85
    -82.82"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 340.46 -24.25)"><foreignobject width="339.37" height="63.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">OIF-PCR (Yang et al., [2022](#bib.bib64)),
    GeDi (Poiesi and Boscaini, [2022](#bib.bib45)), GeoTransformer (Qin et al., [2023](#bib.bib48)),
    RoITr (Yu et al., [2023a](#bib.bib67)).</foreignobject></g> <g stroke="#3C8C93"
    fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -49.89 L 269.41 -49.89 C
    266.35 -49.89 263.88 -52.37 263.88 -55.42 L 263.88 -100.62 C 263.88 -103.68 266.35
    -106.16 269.41 -106.16 L 679.77 -106.16 C 682.83 -106.16 685.3 -103.68 685.3 -100.62
    L 685.3 -55.42 C 685.3 -52.37 682.83 -49.89 679.77 -49.89 Z M 263.88 -106.16"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 268.49 -64.19)"><foreignobject width="412.2" height="47.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MVDesc (Zhou et al., [2018](#bib.bib78)), Li
    et al. (Li et al., [2020b](#bib.bib36)), Gojcic et al. (Gojcic et al., [2020](#bib.bib25)),
    Wang et al. (Wang et al., [2023a](#bib.bib58)).</foreignobject></g> <g stroke="#3C8C93"
    fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -115.41 L 269.41 -115.41
    C 266.35 -115.41 263.88 -117.89 263.88 -120.94 L 263.88 -153.77 C 263.88 -156.82
    266.35 -159.3 269.41 -159.3 L 679.77 -159.3 C 682.83 -159.3 685.3 -156.82 685.3
    -153.77 L 685.3 -120.94 C 685.3 -117.89 682.83 -115.41 679.77 -115.41 Z M 263.88
    -159.3"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 268.49 -129.71)"><foreignobject width="412.2" height="34.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PointNetLK (Aoki et al., [2019](#bib.bib3)),
    DCP (Wang and Solomon, [2019a](#bib.bib54)), PointNetLK Revisited (Li et al.,
    [2021](#bib.bib37)).</foreignobject></g> <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path
    d="M 680.26 -130.41 L 370.29 -130.41 C 367.24 -130.41 364.76 -132.89 364.76 -135.95
    L 364.76 -218.59 C 364.76 -221.64 367.24 -224.12 370.29 -224.12 L 680.26 -224.12
    C 683.32 -224.12 685.79 -221.64 685.79 -218.59 L 685.79 -135.95 C 685.79 -132.89
    683.32 -130.41 680.26 -130.41 Z M 364.76 -224.12"></path></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 369.37 -144.71)"><foreignobject
    width="311.81" height="84.48" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Predator
    (Huang et al., [2021a](#bib.bib27)), OMNet (Xu et al., [2021](#bib.bib62)), PCAM
    (Cao et al., [2021](#bib.bib6)), STORM (Wang et al., [2022b](#bib.bib57)), REGTR
    (Yew and Lee, [2022](#bib.bib66)).</foreignobject></g> <g stroke="#3C8C93" fill="#BBE0E3"
    stroke-width="0.8pt"><path d="M 680.26 -195.17 L 370.29 -195.17 C 367.24 -195.17
    364.76 -197.64 364.76 -200.7 L 364.76 -248.67 C 364.76 -251.73 367.24 -254.2 370.29
    -254.2 L 680.26 -254.2 C 683.32 -254.2 685.79 -251.73 685.79 -248.67 L 685.79
    -200.7 C 685.79 -197.64 683.32 -195.17 680.26 -195.17 Z M 364.76 -254.2"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 369.37 -209.46)"><foreignobject width="311.81" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PRNet (Wang and Solomon, [2019b](#bib.bib55)),
    RPMNet (Yew and Lee, [2020](#bib.bib65)), FIRE-Net (Wu et al., [2021](#bib.bib61))</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 680.07 -226.92
    L 192.15 -226.92 C 189.09 -226.92 186.62 -229.4 186.62 -232.45 L 186.62 -281.88
    C 186.62 -284.94 189.09 -287.42 192.15 -287.42 L 680.07 -287.42 C 683.13 -287.42
    685.6 -284.94 685.6 -281.88 L 685.6 -232.45 C 685.6 -229.4 683.13 -226.92 680.07
    -226.92 Z M 186.62 -287.42"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -241.22)"><foreignobject
    width="489.76" height="51.27" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3DRegNet
    (Pais et al., [2020](#bib.bib44)), DHVR (Lee et al., [2021](#bib.bib34)), PointDSC
    (Bai et al., [2021](#bib.bib5)), DLF (Zhang et al., [2022a](#bib.bib73)), Chen
    et al.(Chen et al., [2022b](#bib.bib9)), MAC (Zhang et al., [2023a](#bib.bib76)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 680.66 -310.47
    L 192.74 -310.47 C 189.69 -310.47 187.21 -312.95 187.21 -316 L 187.21 -336.43
    C 187.21 -339.49 189.69 -341.97 192.74 -341.97 L 680.66 -341.97 C 683.72 -341.97
    686.19 -339.49 686.19 -336.43 L 686.19 -316 C 686.19 -312.95 683.72 -310.47 680.66
    -310.47 Z M 187.21 -341.97"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.82 -326.87)"><foreignobject
    width="489.76" height="18.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DetarNet
    (Chen et al., [2022c](#bib.bib10)), FINet (Xu et al., [2022](#bib.bib63)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -372.26
    L 269.41 -372.26 C 266.35 -372.26 263.88 -374.74 263.88 -377.8 L 263.88 -406.39
    C 263.88 -409.45 266.35 -411.93 269.41 -411.93 L 679.77 -411.93 C 682.83 -411.93
    685.3 -409.45 685.3 -406.39 L 685.3 -377.8 C 685.3 -374.74 682.83 -372.26 679.77
    -372.26 Z M 263.88 -411.93"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 268.49 -386.56)"><foreignobject
    width="412.2" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCP
    (Wang and Solomon, [2019a](#bib.bib54)), PRNet (Wang and Solomon, [2019b](#bib.bib55)),
    IDAM (Li et al., [2020a](#bib.bib35)).</foreignobject></g> <g stroke="#3C8C93"
    fill="#BBE0E3" stroke-width="0.8pt"><path d="M 679.77 -417.75 L 269.41 -417.75
    C 266.35 -417.75 263.88 -420.23 263.88 -423.29 L 263.88 -456.11 C 263.88 -459.17
    266.35 -461.65 269.41 -461.65 L 679.77 -461.65 C 682.83 -461.65 685.3 -459.17
    685.3 -456.11 L 685.3 -423.29 C 685.3 -420.23 682.83 -417.75 679.77 -417.75 Z
    M 263.88 -461.65"></path></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 268.49 -432.05)"><foreignobject width="412.2"
    height="34.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DeepGMR
    (Yuan et al., [2020](#bib.bib69)), OGMM (Mei et al., [2023a](#bib.bib42)), Chen
    et al.(Chen et al., [2023a](#bib.bib11)), VBReg (Jiang et al., [2023](#bib.bib33)).</foreignobject></g>
    <g stroke="#3C8C93" fill="#BBE0E3" stroke-width="0.8pt"><path d="M 680.07 -437.26
    L 192.15 -437.26 C 189.09 -437.26 186.62 -439.74 186.62 -442.8 L 186.62 -488 C
    186.62 -491.05 189.09 -493.53 192.15 -493.53 L 680.07 -493.53 C 683.13 -493.53
    685.6 -491.05 685.6 -488 L 685.6 -442.8 C 685.6 -439.74 683.13 -437.26 680.07
    -437.26 Z M 186.62 -493.53"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -451.56)"><foreignobject
    width="489.76" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PCR-CG
    (Zhang et al., [2022c](#bib.bib75)), ImLoveNet (Chen et al., [2022a](#bib.bib8)),
    IMFNET (Huang et al., [2022c](#bib.bib31)), GMF (Huang et al., [2022b](#bib.bib30)),
    PEAL (Yu et al., [2023b](#bib.bib68)).</foreignobject></g> <g stroke-width="0.8pt"
    stroke="#E44057"><path d="M 84.21 -386.94 L 34.88 -386.94 C 31.82 -386.94 29.34
    -389.41 29.34 -392.47 L 29.34 -416.84 C 29.34 -419.89 31.82 -422.37 34.88 -422.37
    L 84.21 -422.37 C 87.27 -422.37 89.75 -419.89 89.75 -416.84 L 89.75 -392.47 C
    89.75 -389.41 87.27 -386.94 84.21 -386.94 Z M 29.34 -422.37" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 33.95 -401.23)"><foreignobject width="51.18" height="26.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Unsupervised</foreignobject></g> <g stroke-width="0.8pt"
    stroke="#E44057"><path d="M 168.16 -378.63 L 108.2 -378.63 C 105.14 -378.63 102.66
    -381.11 102.66 -384.17 L 102.66 -425.14 C 102.66 -428.2 105.14 -430.68 108.2 -430.68
    L 168.16 -430.68 C 171.22 -430.68 173.7 -428.2 173.7 -425.14 L 173.7 -384.17 C
    173.7 -381.11 171.22 -378.63 168.16 -378.63 Z M 102.66 -430.68" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 -392.93)"><foreignobject width="61.81" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Correspondence -free</foreignobject></g> <g
    stroke-width="0.8pt" fill="#FADBDF" stroke="#E44057"><path d="M 680.07 -376.52
    L 192.15 -376.52 C 189.09 -376.52 186.62 -379 186.62 -382.05 L 186.62 -427.25
    C 186.62 -430.31 189.09 -432.79 192.15 -432.79 L 680.07 -432.79 C 683.13 -432.79
    685.6 -430.31 685.6 -427.25 L 685.6 -382.05 C 685.6 -379 683.13 -376.52 680.07
    -376.52 Z M 186.62 -432.79"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -390.82)"><foreignobject
    width="489.76" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PPF-FoldNet (Deng
    et al., [2018](#bib.bib17)), PCRNet (Sarode et al., [2019](#bib.bib49)), UPCR (Zhang
    et al., [2021](#bib.bib72)), UGMM (Huang et al., [2022a](#bib.bib29)), Sun et
    al. (Sun et al., [2023](#bib.bib51)).</foreignobject></g> <g stroke-width="0.8pt"
    stroke="#E44057"><path d="M 168.16 -434.54 L 108.2 -434.54 C 105.14 -434.54 102.66
    -437.02 102.66 -440.07 L 102.66 -481.04 C 102.66 -484.1 105.14 -486.58 108.2 -486.58
    L 168.16 -486.58 C 171.22 -486.58 173.7 -484.1 173.7 -481.04 L 173.7 -440.07 C
    173.7 -437.02 171.22 -434.54 168.16 -434.54 Z M 102.66 -486.58" style="fill:none"></path></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 107.28 -448.84)"><foreignobject width="61.81" height="42.82" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Correspondence -based</foreignobject></g> <g
    stroke-width="0.8pt" fill="#FADBDF" stroke="#E44057"><path d="M 680.07 -440.73
    L 192.15 -440.73 C 189.09 -440.73 186.62 -443.2 186.62 -446.26 L 186.62 -474.86
    C 186.62 -477.91 189.09 -480.39 192.15 -480.39 L 680.07 -480.39 C 683.13 -480.39
    685.6 -477.91 685.6 -474.86 L 685.6 -446.26 C 685.6 -443.2 683.13 -440.73 680.07
    -440.73 Z M 186.62 -480.39"></path></g><g stroke-width="0.8pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 191.23 -455.02)"><foreignobject
    width="489.76" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CEMNet (Jiang
    et al., [2021](#bib.bib32)), RIENet (Shen et al., [2022](#bib.bib50)), UDPReg (Mei
    et al., [2023b](#bib.bib43)).</foreignobject></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 16.98 0 L 19.69 0 L 19.69 0 L
    28.79 0" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 16.98 0 L 19.69 0 L 19.69 -404.65 L 28.79 -404.65"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 90.3 0 L 90.3 0" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04
    0 L 91.04 0 L 102.11 0" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 174.25 0 L 177.55 0 L 177.55 0
    L 186.06 0" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 174.25 0 L 177.55 0 L 177.55 -78.02 L 186.06 -78.02"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 251.51 0 L 258.16 0 L 258.16 0 L 263.32 0" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 0 L 258.16 0 L 258.16 -46.39 L 263.32 -46.39" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    323.48 -46.39 L 335.3 -46.39" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 323.48 0 L 335.3
    0" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 251.51 -78.02 L 263.32 -78.02" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    90.3 0 L 91.04 0 L 91.04 -137.35 L 102.11 -137.35" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -137.35 L 177.55 -137.35 L 177.55 -137.35 L 186.06 -137.35" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -137.35 L 177.55 -137.35 L 177.55 -177.27 L 186.06 -177.27" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 -137.35 L 263.32 -137.35" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 251.51 -177.27
    L 258.16 -177.27 L 258.16 -177.27 L 263.32 -177.27" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 -177.27 L 258.16 -177.27 L 258.16 -224.69 L 263.32 -224.69" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    352.39 -177.27 L 364.2 -177.27" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 352.39 -224.69
    L 364.2 -224.69" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -257.17
    L 102.11 -257.17" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -326.22
    L 101.52 -326.22" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -392.1
    L 102.11 -392.1" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 90.3 0 L 91.04 0 L 91.04 -465.4
    L 102.11 -465.4" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 174.25 -257.17 L 186.06 -257.17"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 174.84 -326.22 L 186.65 -326.22" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -392.1 L 177.55 -392.1 L 177.55 -392.1 L 186.06 -392.1" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -392.1 L 177.55 -392.1 L 177.55 -439.7 L 186.06 -439.7" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    251.51 -392.1 L 263.32 -392.1" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 251.51 -439.7 L
    263.32 -439.7" style="fill:none"></path></g><g stroke="#000000" fill="#000000"
    stroke-width="1.0pt" color="#000000"><path d="M 174.25 -465.4 L 186.06 -465.4"
    style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 90.3 -404.65 L 94.98 -404.65 L 94.98 -404.65 L 102.11
    -404.65" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 90.3 -404.65 L 94.98 -404.65 L 94.98 -460.56 L 102.11
    -460.56" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt"
    color="#000000"><path d="M 174.25 -404.65 L 186.06 -404.65" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M
    174.25 -460.56 L 186.06 -460.56" style="fill:none"></path></g>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: A taxonomy of PCR algorithms.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：PCR算法的分类。
- en: 2.2 Datasets
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据集
- en: 'Datasets for PCR can be broadly classified into two categories: artificially
    synthesized and acquired through real instruments. Each category exhibits unique
    characteristics and has different levels of applicability in PCR tasks. Synthesized
    point cloud datasets are typically composed of virtual models created by using
    computer graphics techniques to replicate real-world environments. These datasets
    contain the object-level ModelNet40 (Wu et al., [2015](#bib.bib60)) and ShapeNet
    (Chang et al., [2015](#bib.bib7)), which consist of data generated through computer-aided
    design, as well as the scene-level dataset ICL-NUIM (Choi et al., [2015](#bib.bib15))
    and FlyingShapes (Chen et al., [2023b](#bib.bib12)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PCR的数据集可以广泛分为两类：通过人工合成和通过真实仪器获取。每个类别都具有独特的特点，并在PCR任务中具有不同的适用性水平。合成的点云数据集通常由使用计算机图形技术创建的虚拟模型组成，以复制真实世界环境。这些数据集包含了通过计算机辅助设计生成的物体级ModelNet40（Wu等，[2015](#bib.bib60)）和ShapeNet（Chang等，[2015](#bib.bib7)）数据，以及场景级别的数据集ICL-NUIM（Choi等，[2015](#bib.bib15)）和FlyingShapes（Chen等，[2023b](#bib.bib12)）。
- en: However, while synthetic data are beneficial for certain applications, they
    often lack the complexity and variability found in real-world scenarios. Consequently,
    incorporating real-world data into training and validation processes is essential
    for obtaining robust algorithms. Datasets comprising realistic point cloud data
    include Stanford (Curless and Levoy, [1996](#bib.bib16)), ETH (Pomerleau et al.,
    [2012](#bib.bib46)), KITTI (Geiger et al., [2012](#bib.bib23)), Apollo-SouthBay
    (Lu et al., [2019b](#bib.bib39)), ScanObjectNN (Uy et al., [2019](#bib.bib52)),
    and WHU-TLS (Dong et al., [2020](#bib.bib19)). Furthermore, there is a 3DMatch
    (Zeng et al., [2017](#bib.bib70)) dataset that comprises both synthesized and
    realistic scans. The attributes of various datasets are summarized in Table [1](#S2.T1
    "Table 1 ‣ 2.3 Metrics ‣ 2 Related Work ‣ A Comprehensive Survey and Taxonomy
    on Point Cloud Registration Based on Deep Learning").
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管合成数据对某些应用程序有益，但它们通常缺乏现实场景中的复杂性和变化性。因此，将真实世界的数据纳入训练和验证过程对于获得稳健的算法至关重要。包含逼真点云数据的数据集包括Stanford（Curless和Levoy，[1996](#bib.bib16)），ETH（Pomerleau等，[2012](#bib.bib46)），KITTI（Geiger等，[2012](#bib.bib23)），Apollo-SouthBay（Lu等，[2019b](#bib.bib39)），ScanObjectNN（Uy等，[2019](#bib.bib52)）和WHU-TLS（Dong等，[2020](#bib.bib19)）。此外，还有一个包括合成和逼真扫描的3DMatch数据集（Zeng等，[2017](#bib.bib70)）。各个数据集的属性总结在表[1](#S2.T1
    "Table 1 ‣ 2.3 Metrics ‣ 2 Related Work ‣ A Comprehensive Survey and Taxonomy
    on Point Cloud Registration Based on Deep Learning")中。
- en: 2.3 Metrics
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 指标
- en: Metrics play a pivotal role in evaluating and comparing the results of point
    cloud registration, aiding in the selection of optimal parameters. Consequently,
    the choice of an appropriate metric is vital for accurately assessing the quality
    of a registration algorithm. We categorize evaluation metrics based on their application
    scenarios. For object-level point clouds, the commonly employed metrics include
    root mean squared error, mean squared error, mean isotropic error, mean absolute
    error, Chamfer distance (CD), and coefficient of determination. For scene-level
    point clouds, the typical metrics are registration recall, inlier ratio, feature
    matching recall, relative rotation error, and relative translation error.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 指标在评估和比较点云配准结果中起着至关重要的作用，有助于选择最佳参数。因此，选择适当的指标对准确评估配准算法的质量至关重要。根据应用场景，我们对评估指标进行分类。对于物体级点云，常用的指标包括均方根误差、均方误差、均匀误差、平均绝对误差、Chamfer距离（CD）和确定系数。对于场景级点云，典型的指标是配准回溯率、内点比率、特征匹配回溯率、相对旋转误差和相对平移误差。
- en: '![Refer to caption](img/4efe31ba9de9c0593b8ed0d712925641.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4efe31ba9de9c0593b8ed0d712925641.png)'
- en: 'Figure 2: The pipeline of the supervised algorithm. R-L represents the real
    label. PC represents point clouds. I denotes the images. S and C represent step
    and concept, respectively.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：受监督算法的流程。R-L表示真实标签。PC表示点云。I表示图像。S和C分别表示步骤和概念。
- en: '| Dataset | Type | Number | S/O |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 数量 | S/O |'
- en: '| Stanford (Curless and Levoy, [1996](#bib.bib16)) | Real | 10 | O |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Stanford（Curless和Levoy，[1996](#bib.bib16)） | 真实 | 10 | O |'
- en: '| ETH (Pomerleau et al., [2012](#bib.bib46)) | Real | 36 | S |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| ETH（Pomerleau等，[2012](#bib.bib46)） | 真实 | 36 | S |'
- en: '| KITTI (Geiger et al., [2012](#bib.bib23)) | Real | 22 | S |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| KITTI（Geiger等，[2012](#bib.bib23)） | 真实 | 22 | S |'
- en: '| ModelNet40 (Wu et al., [2015](#bib.bib60)) | Syn | 12311 | O |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40（Wu等，[2015](#bib.bib60)） | 合成 | 12311 | O |'
- en: '| ShapeNet (Chang et al., [2015](#bib.bib7)) | Syn | 55000+ | O |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet (Chang et al., [2015](#bib.bib7)) | Syn | 55000+ | O |'
- en: '| ICL-NUIM (Choi et al., [2015](#bib.bib15)) | Syn | 8 | S |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ICL-NUIM (Choi et al., [2015](#bib.bib15)) | Syn | 8 | S |'
- en: '| 3DMatch (Zeng et al., [2017](#bib.bib70)) | Syn&Real | 62 | S |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 3DMatch (Zeng et al., [2017](#bib.bib70)) | Syn&Real | 62 | S |'
- en: '| Apollo-SouthBay (Lu et al., [2019b](#bib.bib39)) | Real | 6 | S |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Apollo-SouthBay (Lu et al., [2019b](#bib.bib39)) | Real | 6 | S |'
- en: '| ScanObjectNN (Uy et al., [2019](#bib.bib52)) | Real | 15000+ | O |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ScanObjectNN (Uy et al., [2019](#bib.bib52)) | Real | 15000+ | O |'
- en: '| WHU-TLS (Dong et al., [2020](#bib.bib19)) | Real | 115 | S |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| WHU-TLS (Dong et al., [2020](#bib.bib19)) | Real | 115 | S |'
- en: '| FlyingShapes (Chen et al., [2023b](#bib.bib12)) | Syn | 200 | S |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| FlyingShapes (Chen et al., [2023b](#bib.bib12)) | Syn | 200 | S |'
- en: 'Table 1: Datasets for PCR tasks. Syn means synthetic point clouds. Real means
    realistic point clouds. S and O denote scene-level and object-level, respectively.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：PCR任务的数据集。Syn表示合成点云。Real表示真实点云。S和O分别表示场景级和对象级。
- en: 3 Supervised Point Cloud Registration
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 监督点云配准
- en: Supervised models for PCR typically rely on various forms of supervisory signals,
    such as ground-truth labels or transformation parameters, to guide the training
    process. To facilitate research on DL-based supervised methods, this section provides
    a structured categorization of the principal contributions made by various supervised
    algorithms across four key stages and two fundamental concepts. Such a taxonomy
    not only elucidates valuable technologies but also presents registration methods
    in a clear and concise manner. The four steps and two concepts of the supervised
    registration algorithm are shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Metrics
    ‣ 2 Related Work ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration
    Based on Deep Learning"). It is worth noting that not every algorithm framework
    contains the four steps and involves these two concepts.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 监督模型的PCR通常依赖于各种形式的监督信号，如真实标签或变换参数，以指导训练过程。为了促进对基于深度学习的监督方法的研究，本节提供了对不同监督算法在四个关键阶段和两个基本概念中的主要贡献的结构化分类。这种分类不仅阐明了有价值的技术，还以清晰简明的方式展示了配准方法。监督配准算法的四个步骤和两个概念如图[2](#S2.F2
    "Figure 2 ‣ 2.3 Metrics ‣ 2 Related Work ‣ A Comprehensive Survey and Taxonomy
    on Point Cloud Registration Based on Deep Learning")所示。值得注意的是，并不是每个算法框架都包含这四个步骤，并涉及这两个概念。
- en: 3.1 Descriptor Extraction
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 描述符提取
- en: In PCR tasks, descriptors are essential, and markedly influence the discriminability
    of features. Here, we describe the PCR algorithms that mainly contribute to descriptor
    extraction from two perspectives, which are two-view and multi-view algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCR任务中，描述符至关重要，并显著影响特征的区分能力。在这里，我们从两个角度描述了主要用于描述符提取的PCR算法，这两个角度分别是双视图和多视图算法。
- en: 3.1.1 Two-view
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 双视图
- en: 'The first perspective involves two-view registration, which emerges as the
    prevalent approach in the field of PCR. We further classify these methods into
    two categories: keypoint-based and keypoint-free.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个视角涉及双视图配准，这在PCR领域中成为一种流行的方法。我们进一步将这些方法分为两类：基于关键点的方法和非关键点方法。
- en: Keypoint-based needs to detect significant keypoints to obtain robust feature
    descriptors. To facilitate comprehension, this category is further segmented based
    on the type of input data employed, including points, patches, and voxel grids.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关键点的方法需要检测显著的关键点以获取稳健的特征描述符。为了便于理解，这一类别根据使用的输入数据类型进一步细分，包括点、补丁和体素网格。
- en: Firstly, points, serving as the fundamental elements of point clouds, are discrete
    and unlinked 3D entities. Consequently, extracting descriptors from points typically
    necessitates the construction of intricate local relationships. In DeepVCP (Lu
    et al., [2019a](#bib.bib38)), point weighting is incorporated into an end-to-end
    registration network to estimate point saliency scores, enabling the detection
    of keypoints. Subsequently, the $K$-nearest neighbors method is employed to establish
    neighborhoods around the keypoints, followed by a permutation-invariant network
    to extract more detailed descriptors. HRegNet (Lu et al., [2021](#bib.bib40))
    is a hierarchical network that leverages geometric features, descriptors, and
    similarity measures obtained through bilateral consensus and neighborhood consensus
    to establish correspondence between keypoints. The principles of bilateral consensus
    and neighborhood consensus suggest that within descriptor space, two correct corresponding
    points should not only be the nearest neighbors of each other but also exhibit
    similar neighborhoods. BUFFER (Ao et al., [2023](#bib.bib2)) designs a point-wise
    learner to enhance computational efficiency and feature representation capabilities
    by predicting keypoints and estimating point orientations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，点作为点云的基本元素，是离散的且未链接的三维实体。因此，从点中提取描述符通常需要构建复杂的局部关系。在 DeepVCP (Lu et al., [2019a](#bib.bib38))
    中，点加权被融入到端到端的配准网络中，以估计点的显著性分数，从而实现关键点的检测。随后，$K$-最近邻方法被用来在关键点周围建立邻域，接着使用不变排列网络提取更详细的描述符。HRegNet
    (Lu et al., [2021](#bib.bib40)) 是一个分层网络，它利用通过双边共识和邻域共识获得的几何特征、描述符和相似性度量来建立关键点之间的对应关系。双边共识和邻域共识的原则表明，在描述符空间中，两个正确的对应点不仅应彼此为最近邻，而且应展示类似的邻域。BUFFER
    (Ao et al., [2023](#bib.bib2)) 设计了一个点级学习器，通过预测关键点和估计点方向来提升计算效率和特征表示能力。
- en: Secondly, patches can directly represent the local neighborhood structure. In
    (Deng et al., [2019](#bib.bib18)), point cloud-FoldNet and point pair features-FoldNet
    are utilized to extract keypoints from the point cloud patch and obtain permutation-invariant
    descriptors. Additionally, a new pose estimation method in (Deng et al., [2019](#bib.bib18))
    is proposed that achieves faster and more robust results than random sample consensus
    (RANSAC) (Fischler and Bolles, [1981](#bib.bib22)). StickyPillars (Fischer et
    al., [2021](#bib.bib21)) integrates keypoint detection and descriptor extraction
    by jointly learning pixel-level and point-level feature descriptors. YOHO (Wang
    et al., [2022a](#bib.bib56)) and RoReg (Wang et al., [2023b](#bib.bib59)) employ
    advanced group equivariant feature learning techniques to achieve rotation invariance,
    enhancing robustness against variations in point density and noise. Furthermore,
    the rotation-equivariant component in YOHO and RoReg allows for estimation with
    just a single correspondence hypothesis, greatly reducing the search space for
    possible transformations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，补丁可以直接表示局部邻域结构。在 (Deng et al., [2019](#bib.bib18)) 中，点云-FoldNet 和点对特征-FoldNet
    被用于从点云补丁中提取关键点，并获得不变排列的描述符。此外，(Deng et al., [2019](#bib.bib18)) 中提出了一种新的姿态估计方法，其结果比随机样本一致性
    (RANSAC) (Fischler and Bolles, [1981](#bib.bib22)) 更快且更稳健。StickyPillars (Fischer
    et al., [2021](#bib.bib21)) 通过联合学习像素级和点级特征描述符来整合关键点检测和描述符提取。YOHO (Wang et al.,
    [2022a](#bib.bib56)) 和 RoReg (Wang et al., [2023b](#bib.bib59)) 采用先进的群等变特征学习技术，实现了旋转不变性，提高了对点密度和噪声变化的鲁棒性。此外，YOHO
    和 RoReg 中的旋转等变组件允许仅通过一个对应假设进行估计，大大减少了可能变换的搜索空间。
- en: Thirdly, voxel grids can achieve uniform sampling of point clouds by adopting
    grids of different, customizable sizes. 3DSmoothNet (Gojcic et al., [2019](#bib.bib24))
    adopts a voxelized smoothed density value technique, incorporating fully convolutional
    layers to model the local morphology of point clouds. It scrutinizes the local
    density estimates to accomplish PCR. SpinNet (Ao et al., [2021](#bib.bib1)) eliminates
    rotational variances by aligning with a reference axis and further reduces them
    through spherical voxelization and coordinate transformations. It then transforms
    point clouds into a manageable cylindrical volume and generates representative
    feature descriptors using cylindrical convolution layers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，体素网格通过采用不同的、自定义大小的网格来实现点云的均匀采样。3DSmoothNet (Gojcic et al., [2019](#bib.bib24))
    采用了体素化平滑密度值技术，结合全卷积层来建模点云的局部形态。它审查局部密度估计以实现 PCR。SpinNet (Ao et al., [2021](#bib.bib1))
    通过对齐参考轴来消除旋转变异，并通过球形体素化和坐标变换进一步减少这些变异。然后，它将点云转换为可管理的圆柱体积，并使用圆柱卷积层生成代表性的特征描述符。
- en: Keypoint-free involves considering all potential correspondences rather than
    detecting critical points. Within this perspective, there exist methods that utilize
    deep neural networks to directly obtain descriptors that encapsulate vital information.
    Subsequently, these descriptors are fed into a dedicated module responsible for
    estimating the transformation parameters. GeDi (Poiesi and Boscaini, [2022](#bib.bib45))
    operates by normalizing the local reference frame of the point cloud patch and
    then encoding it into the descriptors using a deep neural network. These descriptors
    are invariant to scale and rotation, making them effective for PCR across different
    application domains.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无关键点方法涉及考虑所有潜在的对应关系，而不是检测关键点。在这种视角下，存在一些方法利用深度神经网络直接获得 encapsulate 重要信息的描述符。随后，这些描述符被送入一个专门的模块，负责估计变换参数。GeDi
    (Poiesi and Boscaini, [2022](#bib.bib45)) 通过规范化点云补丁的局部参考框架，并使用深度神经网络将其编码为描述符。这些描述符对尺度和旋转不变，使其在不同应用领域的
    PCR 中有效。
- en: Other methodologies adopt a coarse-to-refine scheme, in which the matching outcomes
    are significantly influenced by the descriptors obtained in the initial coarse
    stage (Qin et al., [2023](#bib.bib48)). GeoTransformer (Qin et al., [2023](#bib.bib48))
    encodes the distance and angle information into the transformation representation,
    enabling effective capture of the geometric structure within individual point
    clouds and revealing the geometric consistency among the point clouds to be registered.
    From the viewpoint of considering point-wise and structure, OIF-PCR (Yang et al.,
    [2022](#bib.bib64)) employs an efficient and precise positional encoding strategy
    during the coarse stage, leveraging a limited number of correspondences. Simultaneously,
    a joint optimization approach is utilized to optimize the position encoding, progressively
    refining the point cloud features and reducing the reliance on initialization.
    RoITr (Yu et al., [2023a](#bib.bib67)) introduces an aggregation module using
    a rotation invariant Transformer (Vaswani et al., [2017](#bib.bib53)), which is
    strategically inserted between the encoder and decoder components. Its purpose
    is to facilitate the extraction of discriminative descriptors that are pose-agnostic
    and cross-frame position awareness.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法采用粗到精的方案，其中匹配结果在很大程度上受到初始粗略阶段获得的描述符的影响 (Qin et al., [2023](#bib.bib48))。GeoTransformer
    (Qin et al., [2023](#bib.bib48)) 将距离和角度信息编码到变换表示中，能够有效捕捉各个点云中的几何结构，并揭示要配准的点云之间的几何一致性。从考虑点级和结构的角度来看，OIF-PCR
    (Yang et al., [2022](#bib.bib64)) 在粗略阶段采用高效且精确的位置编码策略，利用有限数量的对应关系。同时，采用联合优化方法来优化位置编码，逐步细化点云特征，并减少对初始化的依赖。RoITr
    (Yu et al., [2023a](#bib.bib67)) 引入了一个使用旋转不变 Transformer (Vaswani et al., [2017](#bib.bib53))
    的聚合模块，该模块被战略性地插入在编码器和解码器组件之间。其目的是促进提取具有姿态无关性和跨帧位置感知的区分性描述符。
- en: Keypoint-based methods achieve precise matching with keypoint detection but
    face generalization challenges and are less efficient. Moreover, keypoint-free
    methods are robust in sparse, low-overlap point clouds but may lack detail accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关键点的方法通过关键点检测实现精确匹配，但面临泛化挑战，并且效率较低。此外，无关键点的方法在稀疏、低重叠的点云中具有鲁棒性，但可能缺乏细节准确性。
- en: 3.1.2 Multi-view
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 多视图
- en: The second perspective involves fusing information from multi-view. MVDesc (Zhou
    et al., [2018](#bib.bib78)) develops a multi-view local descriptor, which is derived
    from images captured from various viewpoints, specifically for characterizing
    the 3D keypoints. Subsequently, MVDesc advances a robust matching technique, aimed
    at rejecting outlier correspondences through efficient belief propagation inference
    within a defined graphical model. Li et al. (Li et al., [2020b](#bib.bib36)) integrate
    multi-view rendering into a neural network through a differentiable renderer,
    allowing the viewpoint to be an optimizable parameter for capturing more informative
    local context around the interest points. To obtain distinctive descriptors, Li
    et al. also design a soft view pooling module for fusing convolutional features
    from different views. Gojcic et al. (Gojcic et al., [2020](#bib.bib25)) utilize
    iteratively reweighted least squares (IRLS) as a global refinement technique to
    address the cycle consistency and alleviate the ambiguity of initial alignment
    in multi-view scanning. However, this approach relies on dense pairwise correspondences,
    which introduces significant computational overhead and increases the presence
    of outliers. Consequently, it becomes challenging for IRLS to accurately estimate
    the correct pose.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个视角涉及从多视角融合信息。MVDesc（Zhou et al., [2018](#bib.bib78)）开发了一种多视角局部描述符，该描述符来源于从不同视点捕获的图像，专门用于表征
    3D 关键点。随后，MVDesc 推进了一种鲁棒的匹配技术，旨在通过在定义的图模型内进行高效的信念传播推理来排除异常匹配。Li 等人（Li et al.,
    [2020b](#bib.bib36)）将多视角渲染集成到神经网络中，通过可微分渲染器使视点成为一个可优化的参数，从而捕捉到兴趣点周围更具信息量的局部上下文。为了获得具有区分性的描述符，Li
    等人还设计了一种软视角池化模块，用于融合来自不同视角的卷积特征。Gojcic 等人（Gojcic et al., [2020](#bib.bib25)）利用迭代重加权最小二乘（IRLS）作为全局精炼技术，以解决循环一致性问题并缓解多视角扫描中初始对齐的模糊性。然而，这种方法依赖于密集的成对对应关系，这带来了显著的计算开销，并增加了异常值的出现。因此，IRLS
    很难准确估计正确的姿态。
- en: To address these limitations, Wang et al. (Wang et al., [2023a](#bib.bib58))
    propose a novel approach. They primarily concentrate on learning reliable initialization
    methods that consider the overlap between multiple point cloud pairs. This enables
    the construction of sparse yet reliable pose graphs. Furthermore, a history reweighting
    function is integrated into the IRLS framework, augmenting its generalization
    and robustness.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些局限性，Wang 等人（Wang et al., [2023a](#bib.bib58)）提出了一种新颖的方法。他们主要集中于学习可靠的初始化方法，这些方法考虑了多个点云对之间的重叠。这使得可以构建稀疏但可靠的姿态图。此外，IRLS
    框架中集成了历史重加权函数，增强了其泛化能力和鲁棒性。
- en: 3.2 Correspondence Search
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 对应搜索
- en: 'Recently, research has emerged for predicting correspondences between point
    clouds to be registered. These methods follow an end-to-end manner and often utilize
    existing point cloud feature extraction methods directly. Specifically, we further
    classify it into two categories according to the registration objects: full-object
    and partial-object.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了用于预测待配准点云之间对应关系的研究。这些方法遵循端到端的方式，并且通常直接利用现有的点云特征提取方法。具体而言，我们根据配准对象进一步将其分类为两类：完全对象和部分对象。
- en: 3.2.1 Full-object
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 完全对象
- en: The initial category involves full-object PCR, where each point is capable of
    identifying a unique counterpart in another point cloud. To address this problem,
    PointNetLK (Aoki et al., [2019](#bib.bib3)) and PointNetLK Revisited (Li et al.,
    [2021](#bib.bib37)), leverage the permutation-invariant network PointNet (Qi et
    al., [2017](#bib.bib47)) as adaptable imaging functions and integrate them into
    a recurrent Lucas-Kanade (Lucas and Kanade, [1981](#bib.bib41)) framework. DCP
    (Wang and Solomon, [2019a](#bib.bib54)) employs graph convolutional neural network
    and Transformer (Vaswani et al., [2017](#bib.bib53)) modeling to obtain feature
    representations and capture contextual information. Subsequently, the pointer
    generation mechanism is employed to estimate the correspondences.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 初始类别涉及完全对象 PCR，其中每个点能够识别另一个点云中的唯一对应点。为了解决这个问题，PointNetLK（Aoki et al., [2019](#bib.bib3)）和
    PointNetLK Revisited（Li et al., [2021](#bib.bib37)）利用了不变排列的网络 PointNet（Qi et al.,
    [2017](#bib.bib47)）作为可适应的成像函数，并将其集成到递归的 Lucas-Kanade（Lucas 和 Kanade, [1981](#bib.bib41)）框架中。DCP（Wang
    和 Solomon, [2019a](#bib.bib54)）采用图卷积神经网络和 Transformer（Vaswani et al., [2017](#bib.bib53)）建模来获取特征表示和捕捉上下文信息。随后，使用指针生成机制来估计对应关系。
- en: 3.2.2 Partial-object
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 部分对象
- en: The second category involves partial-object PCR, where not every point has a
    corresponding point in the other point cloud. Given the common scenario where
    only a subset of the point clouds to be registered exhibits correspondences, numerous
    noteworthy studies have emerged in the field of partial-to-partial PCR. These
    studies have a specific focus on overlap prediction and optimizing the similarity
    matrix accordingly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类涉及部分对象的点云配准（PCR），在这种情况下，并不是每个点在另一个点云中都有对应点。鉴于通常只有部分需要配准的点云存在对应关系，部分到部分的PCR领域涌现出大量重要的研究。这些研究专注于重叠区域预测和相应的相似性矩阵优化。
- en: Overlap prediction refers to estimating the overlap region between point clouds
    to be registered, and then directly finding correspondences in this region. To
    the best of our knowledge, Predator (Huang et al., [2021a](#bib.bib27)) is the
    pioneering model that introduces the concept of overlapping region prediction.
    Predator utilizes a joint encoder and decoder architecture, wherein a graph neural
    network and an overlap attention module are sequentially applied to enhance contextual
    relationships and predict the overlap score, respectively. Notably, the overlap
    attention module facilitates early-stage information interaction in the framework,
    which positively impacts the estimation of overlapping regions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 重叠区域预测是指估计待配准点云之间的重叠区域，然后在该区域内直接寻找对应关系。根据我们的了解，Predator（Huang 等人，[2021a](#bib.bib27)）是首个引入重叠区域预测概念的模型。Predator
    采用联合编码器和解码器架构，其中图神经网络和重叠注意模块依次应用，以增强上下文关系并预测重叠分数。值得注意的是，重叠注意模块在框架中促进了早期的信息交互，这对重叠区域的估计有积极影响。
- en: With reference to the above concept of information interaction, several approaches
    are proposed. OMNet (Xu et al., [2021](#bib.bib62)) introduces an innovative mask
    prediction module that possesses the capability to efficiently generate accurate
    overlapping masks. Moreover, OMNet establishes a direct connection between the
    intermediate layers of the mask prediction module and the transformation regression.
    This connection enables the simultaneous optimization of both the generation of
    overlapping masks and the estimation of transformation parameters. PCAM (Cao et
    al., [2021](#bib.bib6)) employs cross-attention matrices (CAM) to achieve feature
    augmentation. The CAM facilitates simultaneous focus on both shallow geometric
    information and deep contextual information, enabling the generation of more reliable
    matching features in overlapping regions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参考上述信息交互的概念，提出了几种方法。OMNet（Xu 等人，[2021](#bib.bib62)）引入了一种创新的掩模预测模块，能够有效生成准确的重叠掩模。此外，OMNet
    在掩模预测模块的中间层和变换回归之间建立了直接连接。这种连接使得掩模生成和变换参数估计能够同时优化。PCAM（Cao 等人，[2021](#bib.bib6)）利用交叉注意矩阵（CAM）实现特征增强。CAM
    使得对浅层几何信息和深层上下文信息能够同时关注，从而生成更可靠的重叠区域匹配特征。
- en: In addition, several methods enhance the prediction of overlapping regions by
    employing the Transformer for global modeling. STORM (Wang et al., [2022b](#bib.bib57))
    incorporates a differential sampling overlapping prediction module into dual Transformer
    (Vaswani et al., [2017](#bib.bib53)) layers, which facilitates information exchange
    between the before and after prediction phases. It employs a dedicated layer that
    iteratively applies the Gumbel-softmax technique, allowing for the independent
    sampling of points situated within overlapping regions. REGTR (Yew and Lee, [2022](#bib.bib66))
    leverages a main architecture composed of Transformer layers, which incorporate
    both self-attention and cross-attention mechanisms. These layers are proficient
    in facilitating the extraction of meaningful and enhanced features. Such an architectural
    selection empowers the network to accurately predict the probability of each point’s
    presence in overlapping regions and determine their corresponding positions in
    another point cloud.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，几种方法通过使用Transformer进行全局建模来增强对重叠区域的预测。STORM（Wang et al., [2022b](#bib.bib57)）将差分采样重叠预测模块引入双Transformer（Vaswani
    et al., [2017](#bib.bib53)）层，这有助于在预测前后阶段之间的信息交换。它采用了一个专用层，该层迭代地应用Gumbel-softmax技术，允许在重叠区域内独立采样点。REGTR（Yew
    and Lee, [2022](#bib.bib66)）利用由Transformer层组成的主要架构，这些层包含自注意力和交叉注意力机制。这些层能够有效提取有意义的增强特征。这种架构选择使网络能够准确预测每个点在重叠区域中的存在概率，并确定其在另一个点云中的相应位置。
- en: Optimizing the similarity matrix is a crucial aspect of fine-grained correspondence
    searching. The elements of the similarity matrix indicate the probability of correspondence
    between individual point pairs. Typically, a probability function is employed
    to compute the similarity matrix, followed by selecting the maximum value in each
    row or column to determine the most probable point pairs (Wang and Solomon, [2019a](#bib.bib54)).
    While softmax is frequently used as the probability function, it tends to produce
    a blurry correspondence map. To address this issue, numerous studies have emerged
    to mitigate the ambiguity. PRNet (Wang and Solomon, [2019b](#bib.bib55)) applies
    the Gumbel-softmax technique to obtain the similarity matrix, a method that finds
    hard correspondences and alleviates the ambiguity in correspondence search. In
    addition, to enhance the sharpness of the resultant similarity matrix, a temperature
    parameter is introduced into the Gumbel-softmax, which can be iteratively adjusted.
    RPMNet (Yew and Lee, [2020](#bib.bib65)) incorporates the optimal transport layer
    and annealing to learn a similarity matrix from a hybrid feature composed of spatial
    coordinates and geometric properties. FIRE-Net (Wu et al., [2021](#bib.bib61))
    facilitates feature interactions across various hierarchical levels of point clouds.
    Initially, FIRE-Net extracts structural features from the point cloud and fosters
    the interchange of feature information. This process permits points with high
    feature similarity to effectively perceive each other.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 优化相似度矩阵是细粒度对应搜索的一个关键方面。相似度矩阵的元素表示个别点对之间的对应概率。通常，使用概率函数来计算相似度矩阵，然后选择每行或每列中的最大值以确定最可能的点对（Wang
    and Solomon, [2019a](#bib.bib54)）。尽管softmax经常作为概率函数使用，但它往往会产生模糊的对应图。为了解决这一问题，出现了许多研究来减轻模糊性。PRNet（Wang
    and Solomon, [2019b](#bib.bib55)）应用Gumbel-softmax技术来获取相似度矩阵，这种方法找到硬性对应关系并减轻了对应搜索中的模糊性。此外，为了增强相似度矩阵的清晰度，引入了一个温度参数到Gumbel-softmax中，该参数可以迭代调整。RPMNet（Yew
    and Lee, [2020](#bib.bib65)）结合最优传输层和退火技术，从由空间坐标和几何属性组成的混合特征中学习相似度矩阵。FIRE-Net（Wu
    et al., [2021](#bib.bib61)）促进了点云不同层次的特征交互。最初，FIRE-Net从点云中提取结构特征，并促进特征信息的交换。这个过程允许高特征相似度的点有效地相互感知。
- en: Notably, full-object registration methods are impractical in real-world scenarios,
    as the point clouds subject to registration typically represent subset matches.
    The introduction of partial-object registration algorithms addresses this limitation,
    aligning more closely with practical requirements.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，完全对象注册方法在现实场景中并不实际，因为待注册的点云通常表示的是子集匹配。部分对象注册算法的引入解决了这一局限，使其更贴近实际需求。
- en: 3.3 Outlier Filtering
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 异常值过滤
- en: In PCR, outliers are defined as points lacking a corresponding counterpart.
    The principal objective of outlier filtering is to meticulously remove these outliers.
    Given their substantial influence on the outcomes of registration processes, the
    effective elimination of outliers is imperative to guarantee both robustness and
    accuracy. To identify outliers, 3DRegNet (Pais et al., [2020](#bib.bib44)) utilizes
    a deep neural network to estimate the probability of a point being classified
    as an outlier, which effectively minimizes the influence of hypothetical outliers
    during the registration procedure. DHVR (Lee et al., [2021](#bib.bib34)) places
    the initially predicted correspondences into a Hough voting module. This module
    casts votes in a deliberately sparse transformation parameter space, enhancing
    the accurate identification of inliers. Moreover, DLF (Zhang et al., [2022a](#bib.bib73))
    utilizes a classifier that combines the stacked order-aware modules to evaluate
    hypothesized outliers and determine the compatibility of hypothesized inliers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PCR 中，离群点被定义为缺乏对应配对的点。离群点过滤的主要目标是仔细去除这些离群点。鉴于它们对配准过程结果的重大影响，有效去除离群点对于确保系统的鲁棒性和准确性至关重要。为了识别离群点，3DRegNet（Pais
    等， [2020](#bib.bib44)）利用深度神经网络估计点被分类为离群点的概率，从而有效地最小化在配准过程中假设的离群点的影响。DHVR（Lee 等，
    [2021](#bib.bib34)）将初步预测的对应关系放入霍夫投票模块。该模块在一个刻意稀疏的变换参数空间中投票，增强了对内点的准确识别。此外，DLF（Zhang
    等， [2022a](#bib.bib73)）使用一个分类器，该分类器结合了堆叠的顺序感知模块来评估假设的离群点，并确定假设内点的兼容性。
- en: The above methods directly estimate outliers after extracting features. However,
    during the feature extraction phase, they predominantly rely on methods like multilayer
    perceptron, inadvertently overlooking the critical aspect of the 3D spatial information.
    Furthermore, in classifying these features, each pair is assessed separately,
    ignoring the important consistency of inliers (Bai et al., [2021](#bib.bib5)).
    Based on the above thinking, PointDSC (Bai et al., [2021](#bib.bib5)) is proposed,
    which explicitly exploits the spatial compatibility inherently constructed by
    distance. It argues that not only should the relative distances of inliers between
    the point clouds to be registered remain consistent, but there also exists an
    inherent relationship among inliers within the single point cloud. Based on spatial
    compatibility, a second order spatial compatibility (Chen et al., [2022b](#bib.bib9))
    is proposed, which begins by converting the spatial compatibility matrix into
    a binary form and then calculates the similarity between two corresponding points
    based on the count of their mutually compatible points. This approach focusing
    on global rather than local compatibility, enhances early-stage differentiation
    between inliers and outliers. MAC (Zhang et al., [2023a](#bib.bib76)) loosens
    the maximum clique constraint and mines more local consistency information in
    the compatibility graph for accurate pose hypothesis generation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法在提取特征后直接估计离群点。然而，在特征提取阶段，它们主要依赖于多层感知器等方法，往往忽略了3D空间信息的关键方面。此外，在对这些特征进行分类时，每对特征是单独评估的，忽略了内点的一致性（Bai
    等， [2021](#bib.bib5)）。基于上述思路，提出了 PointDSC（Bai 等， [2021](#bib.bib5)），该方法明确利用距离固有构建的空间兼容性。它认为，不仅要保持待配准点云之间内点的相对距离一致，而且单个点云内的内点之间也存在固有关系。基于空间兼容性，提出了二阶空间兼容性（Chen
    等， [2022b](#bib.bib9)），该方法首先将空间兼容性矩阵转换为二进制形式，然后根据相互兼容点的数量计算两个对应点之间的相似性。该方法侧重于全局而非局部兼容性，增强了对内点和离群点的早期区分。MAC（Zhang
    等， [2023a](#bib.bib76)）放宽了最大团约束，并在兼容性图中挖掘更多的局部一致性信息，以生成准确的姿态假设。
- en: 3.4 Transformation Parameter Estimation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 变换参数估计
- en: The calculation of transformation parameters serves as the final step in PCR,
    with the widely adopted methods including RANSAC (Fischler and Bolles, [1981](#bib.bib22))
    and singular value decomposition (SVD) (Arun et al., [1987](#bib.bib4)). RANSAC
    is commonly employed during the coarse registration stage to mitigate the impact
    of outliers, and it requires a predetermined number of iterations to solve. Unlike
    RANSAC, SVD does not necessitate an iterative solution. It estimates the transformation
    parameters directly based on the pose difference between the two point clouds,
    thus requiring a reliable feature extraction network for accurate results (Zhang
    et al., [2022b](#bib.bib74)). The process of solving SVD reveals that the rotation
    matrix is computed prior to the calculation of the translation vector.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 转换参数的计算是 PCR 的最后一步，广泛采用的方法包括 RANSAC（Fischler 和 Bolles，[1981](#bib.bib22)）和奇异值分解（SVD）（Arun
    等，[1987](#bib.bib4)）。RANSAC 通常在粗配准阶段使用，以减轻离群点的影响，并且需要预定的迭代次数来解决。与 RANSAC 不同，SVD
    不需要迭代解法。它直接基于两个点云之间的姿态差异估计转换参数，因此需要一个可靠的特征提取网络以获得准确的结果（Zhang 等，[2022b](#bib.bib74)）。解决
    SVD 的过程表明，旋转矩阵是在计算平移向量之前计算的。
- en: With the development of DL, some approaches strive to solve both rotation matrix
    and translation vector simultaneously using convolutional neural network (Deng
    et al., [2018](#bib.bib17); Pais et al., [2020](#bib.bib44)). The effectiveness
    of this idea is examined across multiple models. However, simultaneous resolution
    of transformation parameters can lead to mutual interference (Chen et al., [2022c](#bib.bib10)).
    To address this issue, DetarNet (Chen et al., [2022c](#bib.bib10)) employs Siamese
    networks to independently decouple transformation parameters in a two-step process.
    Initially, a regression network computes the translation vector, followed by the
    utilization of SVD to determine the rotation matrix. FINet (Xu et al., [2022](#bib.bib63))
    leverages point-wise and global features to enhance information association between
    point clouds to be registered at multiple stages. At the same time, a dual-branch
    structure containing a rotation regression branch and a translation regression
    branch is designed to predict the rotation matrix and translation vector, respectively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的发展，一些方法尝试通过卷积神经网络（Deng 等，[2018](#bib.bib17)；Pais 等，[2020](#bib.bib44)）同时解决旋转矩阵和平移向量的问题。这个想法在多个模型中得到了验证。然而，同时解决转换参数可能会导致相互干扰（Chen
    等，[2022c](#bib.bib10)）。为了解决这个问题，DetarNet（Chen 等，[2022c](#bib.bib10)）采用了 Siamese
    网络以两步过程独立解耦转换参数。最初，回归网络计算平移向量，随后利用 SVD 确定旋转矩阵。FINet（Xu 等，[2022](#bib.bib63)）利用点对点和全局特征来增强在多个阶段要配准的点云之间的信息关联。同时，设计了一个包含旋转回归分支和平移回归分支的双分支结构，分别预测旋转矩阵和平移向量。
- en: 3.5 Optimization
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 优化
- en: 'Approaches to PCR optimization focus on enhancing the entire process. Based
    on the principles of their optimization, we divide them into two main categories:
    iterative closest point (ICP)-based methods and probability-based methods.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PCR 优化的方法侧重于提升整个过程。根据它们的优化原则，我们将它们分为两大类：基于 ICP 的方法和基于概率的方法。
- en: 3.5.1 ICP-based
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 基于 ICP 的方法
- en: While the classical ICP algorithm may be less effective than DL-based algorithms
    for PCR tasks, it still offers valuable advantages worth exploring. One such advantage
    is its iterative optimization thought, which has been widely adopted in various
    methods to refine estimation transformation parameters. In DCP (Wang and Solomon,
    [2019a](#bib.bib54)) and PRNet (Wang and Solomon, [2019b](#bib.bib55)), the entire
    network follows an iterative process to enhance the initial prediction of the
    rotation matrix and translation vector, progressively refining them from a coarse
    to a fine level. More specifically, before each iteration, the point cloud to
    be registered is updated with the transformation parameters estimated in the previous
    iteration. This incremental refinement process allows for the gradual improvement
    of the predicted transformation parameters. Contrasting with the previously discussed
    algorithms that iterate through the entire network, IDAM (Li et al., [2020a](#bib.bib35))
    distinctively positions the feature extraction component outside the iterative
    loop, which reduces the computational burden to a certain extent. Furthermore,
    it integrates distance information into the iterative network and incorporates
    a two-stage point elimination module. This design effectively filters out points
    that are detrimental to the registration process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然经典的ICP算法在PCR任务中可能不如基于深度学习的算法有效，但它仍然提供了值得探索的宝贵优势。其中一个优势是它的迭代优化思想，这一思想已被广泛应用于各种方法中，以细化估计变换参数。在DCP（Wang和Solomon，[2019a](#bib.bib54)）和PRNet（Wang和Solomon，[2019b](#bib.bib55)）中，整个网络遵循迭代过程来增强旋转矩阵和平移向量的初始预测，并将其从粗糙逐步细化得更精确。更具体地说，在每次迭代之前，要配准的点云会根据上一次迭代中估计的变换参数进行更新。这种渐进的细化过程允许逐步改进预测的变换参数。与之前讨论的那些遍历整个网络的算法相比，IDAM（Li等，[2020a](#bib.bib35)）将特征提取组件独特地置于迭代循环之外，从而在一定程度上减少了计算负担。此外，它将距离信息集成到迭代网络中，并引入了一个两阶段点消除模块。这种设计有效地过滤掉对配准过程有害的点。
- en: 3.5.2 Probabilistic-based
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 基于概率的
- en: Probability-based PCR algorithms integrate probabilistic knowledge within the
    registration framework to enhance the optimization process. These algorithms typically
    utilize probabilistic models to depict the matching relationship and inherent
    uncertainty between point clouds to be registered.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概率的PCR算法在配准框架中整合了概率知识，以增强优化过程。这些算法通常利用概率模型来描述点云配准之间的匹配关系及其固有的不确定性。
- en: As a commonly used probability model, the Gaussian mixture model (GMM) finds
    optimal alignments by integrating an expectation-maximization (EM) method into
    a maximum likelihood framework (Eckart et al., [2018](#bib.bib20)). However, the
    EM process can be computationally intensive and potentially lead to incorrect
    data associations, especially in registrations with significant angular disparities
    (Yuan et al., [2020](#bib.bib69)). To address the aforementioned challenges, a
    technique called deep Gaussian mixture registration (DeepGMR) (Yuan et al., [2020](#bib.bib69))
    is proposed, which leverages a neural network to search correspondences between
    points and GMM parameters. Furthermore, two differentiable modules are employed
    to estimate the optimal transformation parameters. OGMM (Mei et al., [2023a](#bib.bib42))
    utilizes predictions of the overlapping area between two input point clouds for
    GMM estimation, framing the registration task as minimizing the variance between
    the two GMMs. In (Chen et al., [2023a](#bib.bib11)), GMM is formulated as a distribution
    that encompasses comprehensive representation capabilities, incorporating both
    global and local information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种常用的概率模型，高斯混合模型（GMM）通过将期望最大化（EM）方法融入最大似然框架中来寻找最优对齐（Eckart等，[2018](#bib.bib20)）。然而，EM过程可能计算密集且可能导致数据关联错误，特别是在存在显著角度差异的配准中（Yuan等，[2020](#bib.bib69)）。为了应对上述挑战，提出了一种名为深度高斯混合配准（DeepGMR）（Yuan等，[2020](#bib.bib69)）的技术，该技术利用神经网络在点和GMM参数之间搜索对应关系。此外，还使用了两个可微分模块来估计最优的变换参数。OGMM（Mei等，[2023a](#bib.bib42)）利用两个输入点云之间重叠区域的预测进行GMM估计，将配准任务框定为最小化两个GMM之间的方差。在（Chen等，[2023a](#bib.bib11)）中，GMM被形式化为一种具有全面表示能力的分布，包含了全球和局部信息。
- en: In addition to GMM, the Bayesian probabilistic model is also utilized for PCR.
    VBReg (Jiang et al., [2023](#bib.bib33)) introduces a variable non-local network
    architecture, which employs variational Bayesian inference for non-local feature
    learning. This approach enables the modeling of Bayesian-driven long-range dependencies
    and facilitates the acquisition of discriminative feature representations for
    inlier/outlier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GMM外，贝叶斯概率模型也用于PCR。VBReg (Jiang et al., [2023](#bib.bib33)) 引入了一种变量非局部网络架构，采用变分贝叶斯推断进行非局部特征学习。这种方法能够建模贝叶斯驱动的长程依赖关系，并促进获得离群点/内点的区分特征表示。
- en: 3.6 Multimodal
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 多模态
- en: The original point cloud inherently possesses valuable structural information,
    which is crucial for accurate representation and analysis. The primary objective
    of current multimodal algorithms is to augment this structural data by incorporating
    texture information derived from images.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 原始点云本身具有宝贵的结构信息，这对于准确表示和分析至关重要。当前多模态算法的主要目标是通过结合从图像中提取的纹理信息来增强这些结构数据。
- en: PCR-CG (Zhang et al., [2022c](#bib.bib75)) and PEAL (Yu et al., [2023b](#bib.bib68))
    employ a two-dimensional (2D) image matching technique to establish 2D correspondences,
    which are then projected onto point clouds using a 2D to 3D projection module,
    facilitating the identification of overlapping regions. ImLoveNet (Chen et al.,
    [2022a](#bib.bib8)) also utilizes images to enhance predictions in overlapping
    regions, directly employing cross-fusion technology to amalgamate the 3D features
    extracted directly from point clouds with the 3D features simulated from two-dimensional
    features derived from images. IMFNET (Huang et al., [2022c](#bib.bib31)) proposes
    an interpretable module to explain the contribution of the original points to
    the final descriptor. This approach significantly enhances both the transparency
    and effectiveness of the descriptor. GMF (Huang et al., [2022b](#bib.bib30)) integrates
    texture and structural information through a cross-attention fusion layer. Additionally,
    it incorporates a convolutional position encoding layer, which is instrumental
    in accentuating distinctions and focusing on neighboring information. Consequently,
    these enhancements contribute to improving correspondence quality and standard
    accuracy in the model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PCR-CG (Zhang et al., [2022c](#bib.bib75)) 和 PEAL (Yu et al., [2023b](#bib.bib68))
    采用二维（2D）图像匹配技术来建立2D对应关系，然后通过2D到3D投影模块将这些对应关系投影到点云上，从而便于识别重叠区域。ImLoveNet (Chen
    et al., [2022a](#bib.bib8)) 也利用图像来增强重叠区域的预测，直接采用交叉融合技术将从点云直接提取的3D特征与从图像派生的二维特征模拟出的3D特征融合。IMFNET
    (Huang et al., [2022c](#bib.bib31)) 提出了一个可解释的模块，以解释原始点对最终描述符的贡献。这种方法显著提高了描述符的透明度和有效性。GMF
    (Huang et al., [2022b](#bib.bib30)) 通过交叉注意力融合层集成了纹理和结构信息。此外，它还结合了卷积位置编码层，这对于突出差异和关注邻近信息至关重要。因此，这些改进有助于提高模型中的对应质量和标准准确性。
- en: '![Refer to caption](img/555090e9b9ce93e5c84bc739ef7f0b3a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/555090e9b9ce93e5c84bc739ef7f0b3a.png)'
- en: 'Figure 3: The pipeline of the unsupervised algorithm. The red arrows and green
    arrows represent correspondence-free and correspondence-based feature flows, respectively.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：无监督算法的流程图。红色箭头和绿色箭头分别表示无对应关系和有对应关系的特征流。
- en: 4 Unsupervised Point Cloud Registration
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 无监督点云配准
- en: 'While supervised PCR algorithms demonstrate favorable outcomes, their success
    heavily depends on an extensive set of ground-truth transformations or correspondences
    as supervision signals during the model training process. Needless to say, acquiring
    such annotated data in real-world settings is often both challenging and costly,
    which limits the practical application scope of these supervised registration
    algorithms. Consequently, unsupervised PCR algorithms are explored. In this section,
    we divide unsupervised algorithms into two categories: correspondence-free and
    correspondence-based.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有监督的PCR算法表现良好，但其成功严重依赖于在模型训练过程中作为监督信号的大量真实变换或对应关系数据。不用说，在实际环境中获取这种注释数据往往既具有挑战性又昂贵，这限制了这些有监督配准算法的实际应用范围。因此，探索无监督PCR算法。在本节中，我们将无监督算法分为两类：无对应关系和有对应关系。
- en: 4.1 Correspondence-free
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 无对应关系
- en: In general, correspondence-based unsupervised registration methods first extract
    global features from the source and target point clouds and then minimize the
    difference between them to regress the transformation parameters. As depicted
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.6 Multimodal ‣ 3 Supervised Point Cloud Registration
    ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on Deep
    Learning"), the correspondence-free algorithms utilize the calculation of the
    distance between point clouds to define the loss function, termed distance loss.
    Typically, distance loss in the unsupervised methods uses CD, which measures the
    distance or feature differences between point pairs in two point clouds bidirectionally,
    with its formula is
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，基于对应关系的无监督配准方法首先从源点云和目标点云中提取全局特征，然后最小化它们之间的差异以回归变换参数。如图[3](#S3.F3 "Figure
    3 ‣ 3.6 Multimodal ‣ 3 Supervised Point Cloud Registration ‣ A Comprehensive Survey
    and Taxonomy on Point Cloud Registration Based on Deep Learning")所示，基于无对应关系的算法利用点云之间距离的计算来定义损失函数，这被称为距离损失。通常，无监督方法中的距离损失使用CD，它测量两点云中点对之间的距离或特征差异，公式为
- en: '|  | $\begin{array}[]{rl}\mathcal{L}\left(\bm{X},\bm{Y}\right)=\frac{1}{N}\sum\limits_{\bm{x}\in\bm{X}}\min\limits_{\bm{y}\in\bm{Y}}\&#124;\bm{x}-\bm{y}\&#124;_{2}^{2}+\frac{1}{M}\sum\limits_{\bm{y}\in\bm{Y}}\min\limits_{\bm{x}\in\bm{X}}\&#124;\bm{y}-\bm{x}\&#124;_{2}^{2}.\end{array}$
    |  | (2) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{array}[]{rl}\mathcal{L}\left(\bm{X},\bm{Y}\right)=\frac{1}{N}\sum\limits_{\bm{x}\in\bm{X}}\min\limits_{\bm{y}\in\bm{Y}}\&#124;\bm{x}-\bm{y}\&#124;_{2}^{2}+\frac{1}{M}\sum\limits_{\bm{y}\in\bm{Y}}\min\limits_{\bm{x}\in\bm{X}}\&#124;\bm{y}-\bm{x}\&#124;_{2}^{2}.\end{array}$
    |  | (2) |'
- en: 'In the field of correspondence-free unsupervised methods, an early significant
    contribution is PPF-FoldNet (Deng et al., [2018](#bib.bib17)), which starts by
    constructing four-dimensional (4D) point-pair features. These features are subsequently
    fed into an end-to-end architecture resembling a folded network, utilizing an
    encoder-decoder structure for reconstruction. The loss function involves comparing
    the CD between the 4D point pair features before and after reconstruction. Sun
    et al. (Sun et al., [2023](#bib.bib51)) have further developed the PointNetLK
    algorithm for use in cross-source PCR, employing global features for CD calculation.
    UGMM (Huang et al., [2022a](#bib.bib29)) presents a novel approach, redefining
    the PCR challenge as a clustering problem and estimating posterior probabilities
    through unsupervised learning. This method uses the CD between Gaussian mixtures
    derived from the point clouds as the loss function. UPCR (Zhang et al., [2021](#bib.bib72))
    introduces dual point cloud representations: pose-invariant and pose-related.
    The pose-related representations are leveraged to learn relative poses, which
    are essential for deriving transformation parameters. Moreover, the CD is also
    integrated into the loss function to evaluate the discrepancy between the source
    point cloud and the target point cloud. PCRNet (Sarode et al., [2019](#bib.bib49)),
    while following a similar approach as UPCR in designing its loss function, distinguishes
    itself by utilizing the earth mover’s distance.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在无对应关系的无监督方法领域，早期的一个重要贡献是PPF-FoldNet（Deng等，[2018](#bib.bib17)），该方法首先构造四维（4D）点对特征。这些特征随后被输入到一个类似于折叠网络的端到端架构中，利用编码器-解码器结构进行重建。损失函数涉及比较重建前后4D点对特征之间的CD。Sun等（Sun等，[2023](#bib.bib51)）进一步发展了用于跨源PCR的PointNetLK算法，采用全局特征进行CD计算。UGMM（Huang等，[2022a](#bib.bib29)）提出了一种新方法，将PCR挑战重新定义为聚类问题，并通过无监督学习估计后验概率。该方法使用从点云中得出的高斯混合体之间的CD作为损失函数。UPCR（Zhang等，[2021](#bib.bib72)）引入了双重点云表示：姿态不变和姿态相关。姿态相关的表示用于学习相对姿态，这对推导变换参数至关重要。此外，CD也被整合进损失函数中，以评估源点云和目标点云之间的差异。PCRNet（Sarode等，[2019](#bib.bib49)），虽然在设计其损失函数时采用了与UPCR类似的方法，但通过使用地球移动者距离来区别于UPCR。
- en: 4.2 Correspondence-based
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于对应关系的
- en: Compared with the correspondence-free unsupervised method, the correspondence-based
    unsupervised method first extracts features, and then uses the correspondence
    relative step in Figure [3](#S3.F3 "Figure 3 ‣ 3.6 Multimodal ‣ 3 Supervised Point
    Cloud Registration ‣ A Comprehensive Survey and Taxonomy on Point Cloud Registration
    Based on Deep Learning") (including correspondence search or outlier filtering)
    to establish point-level, distribution-level, or cluster-level correspondences.
    Finally, the rigid transformation parameters are estimated from these correspondences.
    CEMNet (Jiang et al., [2021](#bib.bib32)) integrates the scaling estimator into
    the function that measures the registration error to weaken the negative impact
    of outliers on registration accuracy. CEMNet also uses CD as the loss function.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与无对应关系的无监督方法相比，基于对应关系的无监督方法首先提取特征，然后使用图中的对应关系相对步骤（包括对应关系搜索或异常值过滤）建立点级、分布级或簇级对应关系。最后，从这些对应关系中估计刚性变换参数。CEMNet （Jiang等，[2021](#bib.bib32)）将缩放估计器集成到测量配准误差的函数中，以削弱异常值对配准精度的负面影响。CEMNet还使用CD作为损失函数。
- en: In addition to CD, correspondence-based unsupervised algorithms also designed
    various other losses to refine aligned point clouds. RIENet (Shen et al., [2022](#bib.bib50))
    proposes a reliable inlier estimation module and designs the neighborhood consensus
    loss and spatial consistency loss to reduce the local differences and global differences
    of the point cloud to be registered. UDPReg (Mei et al., [2023b](#bib.bib43))
    finds correspondences from cluster-level and point-level, and designs self-consistency
    loss, cross-consistency loss, and local contrastive loss to enable unsupervised
    learning.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了CD，基于对应关系的无监督算法还设计了各种其他损失函数来优化对齐的点云。RIENet （Shen等，[2022](#bib.bib50)）提出了一个可靠的内点估计模块，并设计了邻域共识损失和空间一致性损失，以减少待配准点云的局部差异和全局差异。UDPReg （Mei等，[2023b](#bib.bib43)）从簇级和点级寻找对应关系，并设计了自一致性损失、交叉一致性损失和局部对比损失，以实现无监督学习。
- en: 5 Challenges and Opportunities
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 挑战与机遇
- en: Impressive outcomes have been yielded by the existing DL-based PCR algorithms.
    Here, we attempt to highlight the existing issues and identify open questions
    that may serve as a catalyst for future research.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的基于DL的PCR算法已经取得了令人印象深刻的成果。在这里，我们尝试突出现有问题，并确定可能成为未来研究催化剂的开放问题。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Towards realistic data generation: A major challenge is bridging the gap between
    synthetic and real-world data. Most methods often rely on Gaussian noise to mimic
    realistic data, which fails to capture the complexity of actual data. Chen et
    al. (Chen et al., [2023c](#bib.bib13)) propose a new perspective that introduces
    the diffusion model to generate noisy training data. Future research can focus
    on integrating other generative models to simulate noise and occlusions, or developing
    data generation methodologies that can simulate realistic data independently of
    external networks.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 面向真实数据生成：主要挑战在于弥合合成数据和现实世界数据之间的差距。大多数方法通常依赖于高斯噪声来模拟真实数据，这未能捕捉实际数据的复杂性。Chen等（Chen
    et al., [2023c](#bib.bib13)）提出了一种新视角，引入扩散模型生成噪声训练数据。未来的研究可以关注于集成其他生成模型来模拟噪声和遮挡，或开发能够独立于外部网络模拟真实数据的数据生成方法。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Abundant multimodal information: Current multimodal PCR algorithms enhance
    feature representation by fusing image textures, which contributes to more accurate
    and detailed mapping. Future research could further enrich registration algorithms
    by integrating additional modalities information such as (i) topologically informed
    meshes, which offer advanced structural data, and (ii) semantic-level text labels
    embedded in large models, which provide contextual insights.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 丰富的多模态信息：当前的多模态PCR算法通过融合图像纹理来增强特征表示，这有助于更准确和详细的映射。未来的研究可以通过集成额外的模态信息，如（i）提供先进结构数据的拓扑信息网格，和（ii）嵌入大模型中的语义级文本标签，提供上下文洞察，进一步丰富配准算法。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Designing new metrics: (Chen et al., [2023d](#bib.bib14)) designed a new metric
    that effectively achieves dual optimization in processing speed and registration
    accuracy. This advancement not only enhances the performance of existing registration
    networks but also opens new perspectives for PCR tasks. Future research can explore
    innovative evaluation metrics that comprehensively consider factors such as runtime
    speed, model size, and registration quality.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设计新的度量标准: (Chen et al., [2023d](#bib.bib14)) 设计了一种新度量标准，有效地在处理速度和配准准确性上实现了双重优化。这一进展不仅提升了现有配准网络的性能，还为
    PCR 任务开辟了新的视角。未来的研究可以探索创新的评估度量，全面考虑运行时间、模型大小和配准质量等因素。'
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Exploiting pre-trained models: Many PCR algorithms are oriented towards the
    registration process to enhance the performance of registration. However, the
    integration of pre-trained models remains largely unexplored. Future research
    can (i) adapt existing pre-trained models for point cloud data, which could considerably
    reduce the data volume and computational resources needed for training models
    from scratch, and (ii) leverage features from pre-trained models, originally developed
    for other tasks, and apply them to PCR tasks, potentially leading to significant
    advancements and high efficiency.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '利用预训练模型: 许多 PCR 算法针对配准过程来提升性能。然而，预训练模型的集成仍然基本未被探索。未来的研究可以 (i) 适应现有的预训练模型用于点云数据，这可能大大减少训练模型时所需的数据量和计算资源，(ii)
    利用为其他任务开发的预训练模型中的特征，并将其应用于 PCR 任务，从而可能带来显著的进步和高效率。'
- en: 6 Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper provides a comprehensive survey and taxonomy of the DL-based PCR
    algorithms. First, commonly used datasets and metrics are classified. Then, supervised
    and unsupervised registration algorithms are organized and analyzed from different
    technical perspectives. Finally, the issues worthy of attention in the future
    research of PCR are pointed out.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了基于 DL 的 PCR 算法的全面调查和分类。首先，对常用数据集和度量标准进行了分类。然后，从不同的技术角度组织和分析了监督和无监督配准算法。最后，指出了
    PCR 未来研究中值得关注的问题。
- en: Acknowledgments
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported in part by the grant of the National Science Foundation
    of China under Grant 62172090.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了中国国家自然科学基金资助（资助号 62172090）。
- en: References
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ao et al. [2021] S. Ao, Q.g Hu, B. Yang, A. Markham, and Y. Guo. Spinnet: Learning
    a general surface descriptor for 3d point cloud registration. In CVPR, 2021.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ao et al. [2021] S. Ao, Q.g Hu, B. Yang, A. Markham, 和 Y. Guo. Spinnet: 学习用于
    3D 点云配准的通用表面描述符。发表于 CVPR, 2021。'
- en: 'Ao et al. [2023] S. Ao, Q. Hu, H. Wang, K. Xu, and Y. Guo. Buffer: Balancing
    accuracy, efficiency, and generalizability in point cloud registration. In CVPR,
    2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ao et al. [2023] S. Ao, Q. Hu, H. Wang, K. Xu, 和 Y. Guo. Buffer: 在点云配准中平衡准确性、效率和通用性。发表于
    CVPR, 2023。'
- en: 'Aoki et al. [2019] Y. Aoki, H. Goforth, R. Srivatsan, and L. Pointnetlk: Robust
    & efficient point cloud registration using pointnet. In CVPR, 2019.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aoki et al. [2019] Y. Aoki, H. Goforth, R. Srivatsan, 和 L. Pointnetlk: 使用 Pointnet
    的鲁棒且高效的点云配准。发表于 CVPR, 2019。'
- en: Arun et al. [1987] K. Arun, T. Huang, and S. Blostein. Least-squares fitting
    of two 3-d point sets. IEEE TPAMI, (5), 1987.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arun et al. [1987] K. Arun, T. Huang, 和 S. Blostein. 两个 3D 点集的最小二乘拟合。IEEE TPAMI,
    (5), 1987。
- en: 'Bai et al. [2021] X. Bai, Z. Luo, L. Zhou, H. Chen, L. Li, Z. Hu, H. Fu, and
    C. Tai. Pointdsc: Robust point cloud registration using deep spatial consistency.
    In CVPR, 2021.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. [2021] X. Bai, Z. Luo, L. Zhou, H. Chen, L. Li, Z. Hu, H. Fu, 和
    C. Tai. Pointdsc: 使用深度空间一致性的鲁棒点云配准。发表于 CVPR, 2021。'
- en: 'Cao et al. [2021] A. Cao, G. Puy, A. Boulch, and R. Marlet. Pcam: Product of
    cross-attention matrices for rigid registration of point clouds. In ICCV, 2021.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao et al. [2021] A. Cao, G. Puy, A. Boulch, 和 R. Marlet. Pcam: 交叉注意力矩阵的乘积，用于点云的刚性配准。发表于
    ICCV, 2021。'
- en: 'Chang et al. [2015] A. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,
    Z. Li, S. Savarese, M. Savva, S. Song, H. Su, Xiao. J, Yi L., and Yu F. Shapenet:
    An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang et al. [2015] A. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,
    Z. Li, S. Savarese, M. Savva, S. Song, H. Su, Xiao. J, Yi L., 和 Yu F. Shapenet:
    一个信息丰富的 3D 模型库。arXiv 预印本 arXiv:1512.03012, 2015。'
- en: 'Chen et al. [2022a] H. Chen, Z. Wei, Y. Xu, M. Wei, and J. Wang. Imlovenet:
    Misaligned image-supported registration network for low-overlap point cloud pairs.
    In SIGGRAPH, pages 1–9, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2022a] H. Chen, Z. Wei, Y. Xu, M. Wei, 和 J. Wang. Imlovenet: 低重叠点云对的图像支持配准网络。发表于
    SIGGRAPH, 页码 1–9, 2022。'
- en: 'Chen et al. [2022b] Z. Chen, K. Sun, F. Yang, and W. Tao. Sc²-pcr: A second
    order spatial compatibility for efficient and robust point cloud registration.
    In CVPR, 2022.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2022b] Z. Chen, K. Sun, F. Yang, 和 W. Tao. Sc²-pcr: 一种用于高效且鲁棒的点云注册的二阶空间兼容性。见
    CVPR, 2022。'
- en: 'Chen et al. [2022c] Z. Chen, F. Yang, and W. Tao. Detarnet: Decoupling translation
    and rotation by siamese network for point cloud registration. In AAAI, 2022.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2022c] Z. Chen, F. Yang, 和 W. Tao. Detarnet: 通过孪生网络解耦平移和旋转以进行点云注册。见
    AAAI, 2022。'
- en: Chen et al. [2023a] H. Chen, B. Chen, Z. Zhao, and B. Song. Point cloud registration
    based on learning gaussian mixture models with global-weighted local representations.
    IEEE Geosci. Remote Sens. Lett., 20:1–5, 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2023a] H. Chen, B. Chen, Z. Zhao, 和 B. Song. 基于学习高斯混合模型与全局加权局部表示的点云注册。IEEE
    Geosci. Remote Sens. Lett., 20:1–5, 2023。
- en: 'Chen et al. [2023b] S. Chen, H. Xu, R. Li, G. Liu, C. Fu, and S. Liu. Sira-pcr:
    Sim-to-real adaptation for 3d point cloud registration. In ICCV, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2023b] S. Chen, H. Xu, R. Li, G. Liu, C. Fu, 和 S. Liu. Sira-pcr: 3D
    点云注册的模拟到真实适配。见 ICCV, 2023。'
- en: 'Chen et al. [2023c] Z. Chen, Y. Ren, T. Zhang, Z. Dang, W. Tao, S. Süsstrunk,
    and M. Salzmann. Diffusionpcr: Diffusion models for robust multi-step point cloud
    registration. arXiv preprint arXiv:2312.03053, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2023c] Z. Chen, Y. Ren, T. Zhang, Z. Dang, W. Tao, S. Süsstrunk, 和
    M. Salzmann. Diffusionpcr: 用于鲁棒的多步点云注册的扩散模型。arXiv 预印本 arXiv:2312.03053, 2023。'
- en: 'Chen et al. [2023d] Z. Chen, K. Sun, F. Yang, L. Guo, and W. Tao. Sc²-pcr++:
    Rethinking the generation and selection for efficient and robust boint cloud registration.
    IEEE TPAMI, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2023d] Z. Chen, K. Sun, F. Yang, L. Guo, 和 W. Tao. Sc²-pcr++: 重新思考高效且鲁棒的点云注册的生成与选择。IEEE
    TPAMI, 2023。'
- en: Choi et al. [2015] S. Choi, Q. Zhou, and V. Koltun. Robust reconstruction of
    indoor scenes. In CVPR, 2015.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等人 [2015] S. Choi, Q. Zhou, 和 V. Koltun. 室内场景的鲁棒重建。见 CVPR, 2015。
- en: Curless and Levoy [1996] B. Curless and M. Levoy. A volumetric method for building
    complex models from range images. In SIGGRAPH, 1996.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Curless 和 Levoy [1996] B. Curless 和 M. Levoy. 一种基于体积的方法来构建复杂模型。见 SIGGRAPH, 1996。
- en: 'Deng et al. [2018] H. Deng, T. Birdal, and S. Ilic. Ppf-foldnet: Unsupervised
    learning of rotation invariant 3d local descriptors. In ECCV, 2018.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人 [2018] H. Deng, T. Birdal, 和 S. Ilic. Ppf-foldnet: 无监督学习旋转不变的 3D 局部描述符。见
    ECCV, 2018。'
- en: Deng et al. [2019] H. Deng, T. Birdal, and S. Ilic. 3d local features for direct
    pairwise registration. In CVPR, 2019.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人 [2019] H. Deng, T. Birdal, 和 S. Ilic. 直接配对注册的 3D 局部特征。见 CVPR, 2019。
- en: 'Dong et al. [2020] Z. Dong, F. Liang, B. Yang, Yu. Xu, Y. Zang, J. Li, Y. Wang,
    W. Dai, H. Fan, J. Hyyppä, and Stilla U. Registration of large-scale terrestrial
    laser scanner point clouds: A review and benchmark. ISPRS J. Photogramm. Remote
    Sens., 163:327–342, 2020.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人 [2020] Z. Dong, F. Liang, B. Yang, Yu. Xu, Y. Zang, J. Li, Y. Wang,
    W. Dai, H. Fan, J. Hyyppä, 和 Stilla U. 大规模地面激光扫描点云的注册：综述与基准测试。ISPRS J. Photogramm.
    Remote Sens., 163:327–342, 2020。
- en: 'Eckart et al. [2018] B. Eckart, K. Kim, and J. Kautz. Hgmr: Hierarchical gaussian
    mixtures for adaptive 3d registration. In ECCV, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eckart 等人 [2018] B. Eckart, K. Kim, 和 J. Kautz. Hgmr: 用于自适应 3D 注册的层次高斯混合模型。见
    ECCV, 2018。'
- en: 'Fischer et al. [2021] K. Fischer, M. Simon, F. Olsner, S. Milz, H. Gross, and
    P. Mader. Stickypillars: Robust and efficient feature matching on point clouds
    using graph neural networks. In CVPR, 2021.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fischer 等人 [2021] K. Fischer, M. Simon, F. Olsner, S. Milz, H. Gross, 和 P.
    Mader. Stickypillars: 使用图神经网络在点云上进行鲁棒且高效的特征匹配。见 CVPR, 2021。'
- en: 'Fischler and Bolles [1981] M. Fischler and R. Bolles. Random sample consensus:
    a paradigm for model fitting with applications to image analysis and automated
    cartography. Communications of the ACM, 24(6):381–395, 1981.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischler 和 Bolles [1981] M. Fischler 和 R. Bolles. 随机样本一致性：一种用于模型拟合的范例，应用于图像分析和自动制图。ACM
    通讯, 24(6):381–395, 1981。
- en: Geiger et al. [2012] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous
    driving? the kitti vision benchmark suite. In CVPR, 2012.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiger 等人 [2012] A. Geiger, P. Lenz, 和 R. Urtasun. 我们准备好自动驾驶了吗？KITTI 视觉基准套件。见
    CVPR, 2012。
- en: 'Gojcic et al. [2019] Z. Gojcic, C. Zhou, J. Wegner, and A. Wieser. The perfect
    match: 3d point cloud matching with smoothed densities. In CVPR, 2019.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gojcic 等人 [2019] Z. Gojcic, C. Zhou, J. Wegner, 和 A. Wieser. 完美匹配：具有平滑密度的 3D
    点云匹配。见 CVPR, 2019。
- en: Gojcic et al. [2020] Z. Gojcic, C. Zhou, J. D Wegner, L. Guibas, and T. Birdal.
    Learning multiview 3d point cloud registration. In CVPR, 2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gojcic 等人 [2020] Z. Gojcic, C. Zhou, J. D Wegner, L. Guibas, 和 T. Birdal. 学习多视角
    3D 点云注册。见 CVPR, 2020。
- en: 'Gu et al. [2020] X. Gu, X. Wang, and Y. Guo. A review of research on point
    cloud registration methods. In IOP Conf. Ser.: Mater. Sci. Eng., volume 782, page
    022070. IOP Publishing, 2020.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2020) 顾等人 X. 顾，X. 王 和 Y. 郭。A review of research on point cloud registration
    methods. In IOP Conf. Ser.: Mater. Sci. Eng., volume 782, page 022070. IOP Publishing,
    2020.'
- en: 'Huang et al. [2021a] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler.
    Predator: Registration of 3d point clouds with low overlap. In CVPR, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2021a) 黄等人 S. 黄，Z. 高齐奇，M. 乌斯维亚特索夫，A. 维塞尔 和 K. 辛德勒。Predator: Registration of
    3d point clouds with low overlap. In CVPR, 2021.'
- en: Huang et al. [2021b] X. Huang, G. Mei, J. Zhang, and R. Abbas. A comprehensive
    survey on point cloud registration. arXiv preprint arXiv:2103.02690, 2021.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2021b) 黄等人 X. 黄，G. 梅，J. 张 和 R. 阿巴斯。A comprehensive survey on point cloud registration.
    arXiv preprint arXiv:2103.02690, 2021.
- en: Huang et al. [2022a] X. Huang, S. Li, Y. Zuo, Y. Fang, J. Zhang, and X. Zhao.
    Unsupervised point cloud registration by learning unified gaussian mixture models.
    IEEE Robotics Autom. Lett., 7(3):7028–7035, 2022.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2022a) 黄等人 X. 黄，S. 李，Y. 左，Y. 方，J. 张 和 X. 赵。Unsupervised point cloud registration
    by learning unified gaussian mixture models. IEEE Robotics Autom. Lett., 7(3):7028–7035,
    2022.
- en: 'Huang et al. [2022b] X. Huang, W. Qu, Y. Zuo, Y. Fang, and X. Zhao. Gmf: General
    multimodal fusion framework for correspondence outlier rejection. IEEE Robotics
    Autom. Lett., 7(4):12585–12592, 2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2022b) 黄等人 X. 黄，W. 曲，Y. 左，Y. 方 和 X. 赵。Gmf: General multimodal fusion framework
    for correspondence outlier rejection. IEEE Robotics Autom. Lett., 7(4):12585–12592,
    2022.'
- en: 'Huang et al. [2022c] X. Huang, W. Qu, Y. Zuo, Y. Fang, and X. Zhao. Imfnet:
    Interpretable multimodal fusion for point cloud registration. IEEE Robotics Autom.
    Lett., 7(4):12323–12330, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2022c) 黄等人 X. 黄，W. 曲，Y. 左，Y. 方 和 X. 赵。Imfnet: Interpretable multimodal fusion
    for point cloud registration. IEEE Robotics Autom. Lett., 7(4):12323–12330, 2022.'
- en: Jiang et al. [2021] H. Jiang, Y. Shen, J. Xie, J. Li, Ji. Qian, and J. Yang.
    Sampling network guided cross-entropy method for unsupervised point cloud registration.
    In ICCV, 2021.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2021) 姜等人 H. 姜，Y. 沈，J. 谢，J. 李，季谦 和 J. 杨。Sampling network guided cross-entropy
    method for unsupervised point cloud registration. In ICCV, 2021.
- en: Jiang et al. [2023] H. Jiang, Z. Dang, Z. Wei, J. Xie, J. Yang, and M. Salzmann.
    Robust outlier rejection for 3d registration with variational bayes. In CVPR,
    2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2023) 姜等人 H. 姜，Z. 党，Z. 韦，J. 谢，J. 杨 和 M. 沙尔兹曼。Robust outlier rejection for 3d
    registration with variational bayes. In CVPR, 2023.
- en: Lee et al. [2021] J. Lee, S. Kim, M. Cho, and J. Park. Deep hough voting for
    robust global registration. In ICCV, 2021.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2021) 李等人 J. 李，S. 金，M. 曹 和 J. 朴。Deep hough voting for robust global registration.
    In ICCV, 2021.
- en: Li et al. [2020a] J. Li, C. Zhang, Z. Xu, H. Zhou, and C. Zhang. Iterative distance-aware
    similarity matrix convolution with mutual-supervised point elimination for efficient
    point cloud registration. In ECCV, 2020.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2020a) 李等人 J. 李，C. 张，Z. 徐，H. 周 和 C. 张。Iterative distance-aware similarity matrix
    convolution with mutual-supervised point elimination for efficient point cloud
    registration. In ECCV, 2020.
- en: Li et al. [2020b] L. Li, S. Zhu, H. Fu, P. Tan, and C. Tai. End-to-end learning
    local multi-view descriptors for 3d point clouds. In CVPR, 2020.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2020b) 李等人 L. 李，S. 朱，H. 符，P. 谭 和 C. 泰。End-to-end learning local multi-view
    descriptors for 3d point clouds. In CVPR, 2020.
- en: Li et al. [2021] X. Li, J. Pontes, and L. Pointnetlk revisited. In CVPR, 2021.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2021) 李等人 X. 李，J. Pontes 和 L. Pointnetlk revisited. In CVPR, 2021.
- en: 'Lu et al. [2019a] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan, and S. Song. Deepvcp:
    An end-to-end deep neural network for point cloud registration. In CVPR, 2019.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2019a) 卢等人 W. 卢，G. 万，Y. 周，X. 付，P. 袁 和 S. 宋。Deepvcp: An end-to-end deep neural
    network for point cloud registration. In CVPR, 2019.'
- en: 'Lu et al. [2019b] W. Lu, Y. Zhou, G. Wan, S. Hou, and S. Song. L3-net: Towards
    learning based lidar localization for autonomous driving. In CVPR, 2019.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2019b) 卢等人 W. 卢，Y. 周，G. 万，S. 侯 和 S. 宋。L3-net: Towards learning based lidar
    localization for autonomous driving. In CVPR, 2019.'
- en: 'Lu et al. [2021] F. Lu, G. Chen, Y. Liu, L. Zhang, S. Qu, S. Liu, and R. Gu.
    Hregnet: A hierarchical network for large-scale outdoor lidar point cloud registration.
    In ICCV, 2021.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2021) 卢等人 F. 卢，G. 陈，Y. 刘，L. 张，S. 曲，S. 刘 和 R. 顾。Hregnet: A hierarchical network
    for large-scale outdoor lidar point cloud registration. In ICCV, 2021.'
- en: Lucas and Kanade [1981] B. Lucas and T. Kanade. An iterative image registration
    technique with an application to stereo vision. In IJCAI, 1981.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1981) 卢卡斯和卡纳德 B. 卢卡斯 和 T. 卡纳德。An iterative image registration technique with
    an application to stereo vision. In IJCAI, 1981.
- en: Mei et al. [2023a] G. Mei, F. Poiesi, C. Saltori, J. Zhang, E. Ricci, and N. Sebe.
    Overlap-guided gaussian mixture models for point cloud registration. In WACV,
    2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2023a) 梅等人 G. 梅，F. 波伊西，C. 萨尔托里，J. 张，E. 里奇 和 N. 塞贝。Overlap-guided gaussian mixture
    models for point cloud registration. In WACV, 2023.
- en: Mei et al. [2023b] G. Mei, H. Tang, X. Huang, W. Wang, J. Liu, J. Zhang, Van
    G., and Q. Wu. Unsupervised deep probabilistic approach for partial point cloud
    registration. In CVPR, 2023.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2023b) 梅等人 G. 梅，H. 唐，X. 黄，W. 王，J. 刘，J. 张，Van G. 和 Q. 吴。Unsupervised deep probabilistic
    approach for partial point cloud registration. In CVPR, 2023.
- en: 'Pais et al. [2020] G. Pais, S. Ramalingam, V. Govindu, J. Nascimento, R. Chellappa,
    and P. Miraldo. 3dregnet: A deep neural network for 3d point registration. In
    CVPR, 2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pais et al. [2020] G. Pais, S. Ramalingam, V. Govindu, J. Nascimento, R. Chellappa,
    和 P. Miraldo. 3dregnet: 一种用于 3d 点配准的深度神经网络。在 CVPR, 2020。'
- en: Poiesi and Boscaini [2022] F. Poiesi and D. Boscaini. Learning general and distinctive
    3d local deep descriptors for point cloud registration. IEEE TPAMI, 45(3), 2022.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poiesi and Boscaini [2022] F. Poiesi 和 D. Boscaini. 学习通用且独特的 3d 本地深度描述符用于点云配准。IEEE
    TPAMI, 45(3), 2022。
- en: Pomerleau et al. [2012] F. Pomerleau, M. Liu, F. Colas, and R. Siegwart. Challenging
    data sets for point cloud registration algorithms. The International Journal of
    Robotics Research, 31(14):1705–1711, 2012.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pomerleau et al. [2012] F. Pomerleau, M. Liu, F. Colas, 和 R. Siegwart. 点云配准算法的挑战数据集。国际机器人研究杂志,
    31(14):1705–1711, 2012。
- en: 'Qi et al. [2017] C. Qi, H. Su, K. Mo, and L. Guibas. Pointnet: Deep learning
    on point sets for 3d classification and segmentation. In CVPR, 2017.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi et al. [2017] C. Qi, H. Su, K. Mo, 和 L. Guibas. Pointnet: 点集深度学习用于 3d 分类和分割。在
    CVPR, 2017。'
- en: 'Qin et al. [2023] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, S. Ilic, D. Hu,
    and K. Xu. Geotransformer: Fast and robust point cloud registration with geometric
    transformer. IEEE TPAMI, 45(8):9806–9821, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. [2023] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, S. Ilic, D. Hu,
    和 K. Xu. Geotransformer: 快速且稳健的点云配准方法，基于几何变换器。IEEE TPAMI, 45(8):9806–9821, 2023。'
- en: 'Sarode et al. [2019] V. Sarode, X. Li, H. Goforth, Y. Aoki, R. Srivatsan, and
    S. Lucey. Pcrnet: Point cloud registration network using pointnet encoding. arXiv
    preprint arXiv:1908.07906, 2019.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sarode et al. [2019] V. Sarode, X. Li, H. Goforth, Y. Aoki, R. Srivatsan, 和
    S. Lucey. Pcrnet: 使用 Pointnet 编码的点云配准网络。arXiv 预印本 arXiv:1908.07906, 2019。'
- en: Shen et al. [2022] Y. Shen, L. Hui, H. Jiang, J. Xie, and J. Yang. Reliable
    inlier evaluation for unsupervised point cloud registration. In AAAI, 2022.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. [2022] Y. Shen, L. Hui, H. Jiang, J. Xie, 和 J. Yang. 无监督点云配准的可靠内点评估。在
    AAAI, 2022。
- en: Sun et al. [2023] X. Sun, W. Li, J. Huang, D. Chen, and T. Jia. Research and
    application on cross-source point cloud registration method based on unsupervised
    learning. In CYBER, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2023] X. Sun, W. Li, J. Huang, D. Chen, 和 T. Jia. 基于无监督学习的跨源点云配准方法的研究与应用。在
    CYBER, 2023。
- en: 'Uy et al. [2019] M. Uy, Q. Pham, B. Hua, T. Nguyen, and S. Yeung. Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data. In ICCV, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Uy et al. [2019] M. Uy, Q. Pham, B. Hua, T. Nguyen, 和 S. Yeung. 重新审视点云分类: 基于真实世界数据的新基准数据集和分类模型。在
    ICCV, 2019。'
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS,
    2017.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. Gomez, Ł. Kaiser, 和 I. Polosukhin. 注意力机制即是你所需要的。在 NeurIPS, 2017。
- en: 'Wang and Solomon [2019a] Y. Wang and J. Solomon. Deep closest point: Learning
    representations for point cloud registration. In ICCV, 2019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang and Solomon [2019a] Y. Wang 和 J. Solomon. 深度最近点: 用于点云配准的表示学习。在 ICCV, 2019。'
- en: 'Wang and Solomon [2019b] Y. Wang and J. Solomon. Prnet: Self-supervised learning
    for partial-to-partial registration. In NeurIPS, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang and Solomon [2019b] Y. Wang 和 J. Solomon. Prnet: 用于部分到部分配准的自监督学习。在 NeurIPS,
    2019。'
- en: 'Wang et al. [2022a] H. Wang, Y. Liu, Z. Dong, and W. Wang. You only hypothesize
    once: Point cloud registration with rotation-equivariant descriptors. In ACM MM,
    2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2022a] H. Wang, Y. Liu, Z. Dong, 和 W. Wang. 你只需假设一次: 带有旋转等变描述符的点云配准。在
    ACM MM, 2022。'
- en: 'Wang et al. [2022b] Y. Wang, C. Yan, Y. Feng, S. Du, Q. Dai, and Y. Gao. Storm:
    Structure-based overlap matching for partial point cloud registration. IEEE TPAMI,
    45(1):1135–1149, 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2022b] Y. Wang, C. Yan, Y. Feng, S. Du, Q. Dai, 和 Y. Gao. Storm:
    基于结构的部分点云配准重叠匹配。IEEE TPAMI, 45(1):1135–1149, 2022。'
- en: Wang et al. [2023a] H. Wang, Y. Liu, Z. Dong, Y. Guo, Y. Liu, W. Wang, and B. Yang.
    Robust multiview point cloud registration with reliable pose graph initialization
    and history reweighting. In CVPR, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023a] H. Wang, Y. Liu, Z. Dong, Y. Guo, Y. Liu, W. Wang, 和 B. Yang.
    具有可靠姿态图初始化和历史加权的稳健多视角点云配准。在 CVPR, 2023。
- en: 'Wang et al. [2023b] H. Wang, Y. Liu, Q. Hu, B. Wang, J. Chen, Z. Dong, Y. Guo,
    W. Wang, and B. Yang. Roreg: Pairwise point cloud registration with oriented descriptors
    and local rotations. IEEE TPAMI, 45(8), 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023b] H. Wang, Y. Liu, Q. Hu, B. Wang, J. Chen, Z. Dong, Y. Guo,
    W. Wang, 和 B. Yang. Roreg: 具有方向描述符和局部旋转的成对点云配准。IEEE TPAMI, 45(8), 2023。'
- en: 'Wu et al. [2015] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, and J. Tang, X.and Xiao.
    3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. [2015] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, 和 J. Tang, X.
    和 Xiao. 3d shapenets: 一种用于体积形状的深度表示。在 CVPR, 2015。'
- en: Wu et al. [2021] B. Wu, J. Ma, G. Chen, and P. An. Feature interactive representation
    for point cloud registration. In ICCV, 2021.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2021] B. Wu, J. Ma, G. Chen, 和 P. An. 特征交互表示用于点云配准. 发表在 ICCV, 2021。
- en: 'Xu et al. [2021] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng. Omnet: Learning
    overlapping mask for partial-to-partial point cloud registration. In ICCV, 2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 [2021] H. Xu, S. Liu, G. Wang, G. Liu, 和 B. Zeng. Omnet: 学习用于部分到部分点云配准的重叠掩码.
    发表在 ICCV, 2021。'
- en: 'Xu et al. [2022] H. Xu, N. Ye, G. Liu, B. Zeng, and S. Liu. Finet: Dual branches
    feature interaction for partial-to-partial point cloud registration. In AAAI,
    2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 [2022] H. Xu, N. Ye, G. Liu, B. Zeng, 和 S. Liu. Finet: 双分支特征交互用于部分到部分点云配准.
    发表在 AAAI, 2022。'
- en: 'Yang et al. [2022] F. Yang, L. Guo, Z. Chen, and W. Tao. One-inlier is first:
    Towards efficient position encoding for point cloud registration. In NeurIPS,
    2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2022] F. Yang, L. Guo, Z. Chen, 和 W. Tao. 一次内点优先: 迈向高效的点云配准位置编码. 发表在
    NeurIPS, 2022。'
- en: 'Yew and Lee [2020] Z. Yew and G. Lee. Rpm-net: Robust point matching using
    learned features. In CVPR, 2020.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yew 和 Lee [2020] Z. Yew 和 G. Lee. Rpm-net: 使用学习特征的鲁棒点匹配. 发表在 CVPR, 2020。'
- en: 'Yew and Lee [2022] Z. Yew and G. Lee. Regtr: End-to-end point cloud correspondences
    with transformers. In CVPR, 2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yew 和 Lee [2022] Z. Yew 和 G. Lee. Regtr: 基于变换器的端到端点云对应. 发表在 CVPR, 2022。'
- en: Yu et al. [2023a] H. Yu, Z. Qin, J. Hou, M. Saleh, D. Li, B. Busam, and S. Ilic.
    Rotation-invariant transformer for point cloud matching. In CVPR, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2023a] H. Yu, Z. Qin, J. Hou, M. Saleh, D. Li, B. Busam, 和 S. Ilic. 对点云匹配的旋转不变变换器.
    发表在 CVPR, 2023。
- en: 'Yu et al. [2023b] J. Yu, L. Ren, Y. Zhang, W. Zhou, L. Lin, and G. Dai. Peal:
    Prior-embedded explicit attention learning for low-overlap point cloud registration.
    In CVPR, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2023b] J. Yu, L. Ren, Y. Zhang, W. Zhou, L. Lin, 和 G. Dai. Peal: 嵌入先验的显式注意学习用于低重叠点云配准.
    发表在 CVPR, 2023。'
- en: 'Yuan et al. [2020] W. Yuan, B. Eckart, K. Kim, V. Jampani, D. Fox, and J. Kautz.
    Deepgmr: Learning latent gaussian mixture models for registration. In ECCV, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等人 [2020] W. Yuan, B. Eckart, K. Kim, V. Jampani, D. Fox, 和 J. Kautz.
    Deepgmr: 学习潜在高斯混合模型用于配准. 发表在 ECCV, 2020。'
- en: 'Zeng et al. [2017] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser.
    3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In CVPR,
    2017.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等人 [2017] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, 和 T. Funkhouser.
    3dmatch: 从 RGB-D 重建中学习局部几何描述符. 发表在 CVPR, 2017。'
- en: 'Zhang et al. [2020] Z. Zhang, Y. Dai, and J. Sun. Deep learning based point
    cloud registration: an overview. Virtual Real. Intell. Hardw., 2(3):222–246, 2020.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2020] Z. Zhang, Y. Dai, 和 J. Sun. 基于深度学习的点云配准: 概述. 虚拟现实与智能硬件, 2(3):222–246,
    2020。'
- en: Zhang et al. [2021] Z. Zhang, J. Sun, Y. Dai, D. Zhou, X. Song, and M. He. A
    representation separation perspective to correspondence-free unsupervised 3-d
    point cloud registration. IEEE Geosci. Remote Sens. Lett., 19:1–5, 2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2021] Z. Zhang, J. Sun, Y. Dai, D. Zhou, X. Song, 和 M. He. 从表示分离的视角看无对应的无监督三维点云配准.
    IEEE 地球科学与遥感快报, 19:1–5, 2021。
- en: Zhang et al. [2022a] Y. Zhang, Z. Sun, Z. Zeng, and K. Lam. Partial point cloud
    registration with deep local feature. IEEE TAI, 4(5):1317–1327, 2022.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022a] Y. Zhang, Z. Sun, Z. Zeng, 和 K. Lam. 通过深度局部特征进行部分点云配准. IEEE
    TAI, 4(5):1317–1327, 2022。
- en: Zhang et al. [2022b] Y. Zhang, Z. Sun, Z. Zeng, and K. Lam. Point cloud registration
    using multiattention mechanism and deep hybrid features. IEEE Intell. Syst., 38(1):58–68,
    2022.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022b] Y. Zhang, Z. Sun, Z. Zeng, 和 K. Lam. 使用多注意力机制和深度混合特征进行点云配准.
    IEEE 智能系统, 38(1):58–68, 2022。
- en: 'Zhang et al. [2022c] Y. Zhang, J. Yu, X. Huang, W. Zhou, and J. Hou. Pcr-cg:
    Point cloud registration via deep explicit color and geometry. In ECCV, 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2022c] Y. Zhang, J. Yu, X. Huang, W. Zhou, 和 J. Hou. Pcr-cg: 通过深度显式颜色和几何进行点云配准.
    发表在 ECCV, 2022。'
- en: Zhang et al. [2023a] X. Zhang, J. Yang, S. Zhang, and Y. Zhang. 3d registration
    with maximal cliques. In CVPR, 2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023a] X. Zhang, J. Yang, S. Zhang, 和 Y. Zhang. 通过最大团进行三维配准. 发表在 CVPR,
    2023。
- en: 'Zhang et al. [2023b] Z. Zhang, W. Sun, X. Min, Q. Zhou, J. He, Q. Wang, and
    G. Zhai. Mm-pcqa: Multi-modal learning for no-reference point cloud quality assessment.
    In IJCAI, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2023b] Z. Zhang, W. Sun, X. Min, Q. Zhou, J. He, Q. Wang, 和 G. Zhai.
    Mm-pcqa: 多模态学习用于无参考点云质量评估. 发表在 IJCAI, 2023。'
- en: Zhou et al. [2018] L. Zhou, S. Zhu, Z. Luo, T. Shen, R. Zhang, M. Zhen, T. Fang,
    and L. Quan. Learning and matching multi-view descriptors for registration of
    point clouds. In ECCV, 2018.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2018] L. Zhou, S. Zhu, Z. Luo, T. Shen, R. Zhang, M. Zhen, T. Fang,
    和 L. Quan. 学习和匹配用于点云配准的多视图描述符. 发表在 ECCV, 2018。
