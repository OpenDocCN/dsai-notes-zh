- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:45:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:45:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2206.12478] Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2206.12478] 深度学习在Tomo-e Gozen瞬态调查中的真实/虚假分类'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.12478](https://ar5iv.labs.arxiv.org/html/2206.12478)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2206.12478](https://ar5iv.labs.arxiv.org/html/2206.12478)
- en: \Received
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \收到
- en: $\langle$reception date$\rangle$ \Accepted$\langle$acception date$\rangle$ \Published$\langle$publication
    date$\rangle$
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: $\langle$接收日期$\rangle$ \接受$\langle$接受日期$\rangle$ \发布$\langle$发布日期$\rangle$
- en: \KeyWords
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \关键字
- en: 'supernovae: general — methods: statistical — surveys'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 超新星：一般 — 方法：统计 — 调查
- en: Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在Tomo-e Gozen瞬态调查中的真实/虚假分类
- en: 'Ichiro Takahashi¹¹affiliation: Astronomical Institute, Tohoku University, Sendai,
    Miyagi 980-8578, Japan ^*^*affiliationmark:    Ryo Hamasaki²²affiliation: Department
    of Physics, Faculty of Science and Engineering, Konan University, 8-9-1 Okamoto,
    Kobe, Hyogo 658-8501, Japan    Naonori Ueda³³affiliation: NTT Communication Science
    Laboratories, 2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237,
    Japan    Masaomi Tanaka¹¹affiliationmark: ⁴⁴affiliation: Division for the Establishment
    of Frontier Sciences, Organization for Advanced Studies, Tohoku University, Sendai,
    Miyagi 980-8577, Japan ⁵⁵affiliation: Kavli Institute for the Physics and Mathematics
    of the Universe (WPI), The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba
    277-8583, Japan    Nozomu Tominaga⁶⁶affiliation: National Astronomical Observatory
    of Japan, 2-21-1 Osawa, Mitaka, Tokyo 181-8588, Japan ⁷⁷affiliation: Department
    of Astronomical Science, School of Physical Sciences, The Graduate University
    of Advanced Studies (SOKENDAI) 2-21-1 Osawa, Mitaka, Tokyo 181-8588, Japan ²²affiliationmark:
    ⁵⁵affiliationmark:    Shigeyuki Sako⁸⁸affiliation: Institute of Astronomy, Graduate
    School of Science, The University of Tokyo, 2-21-1 Osawa, Mitaka, Tokyo 181-0015,
    Japan ⁹⁹affiliation: UTokyo Organization for Planetary Space Science, The University
    of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan ^(10)^(10)affiliation: Collaborative
    Research Organization for Space Science and Technology, The University of Tokyo,
    Hongo, Bunkyo-ku, Tokyo 113-0033, Japan    Ryou Ohsawa⁸⁸affiliationmark:    and
    Naoki Yoshida^(11)^(11)affiliation: Department of Physics, Graduate School of
    Science, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
    ⁵⁵affiliationmark: ^(12)^(12)affiliation: Institute for Physics of Intelligence,
    The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan [ichiro.takahashi@astr.tohoku.ac.jp](mailto:ichiro.takahashi@astr.tohoku.ac.jp)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Ichiro Takahashi¹¹隶属机构：东北大学天文学研究所，日本宫城县仙台市980-8578 ^*^*隶属机构标记：    Ryo Hamasaki²²隶属机构：甲南大学物理系，科学与工程学院，日本兵库县神户市冈本8-9-1，658-8501
       Naonori Ueda³³隶属机构：NTT通信科学实验室，日本京都县京田边市光台2-4，619-0237    Masaomi Tanaka¹¹隶属机构标记：
    ⁴⁴隶属机构：前沿科学建设部，东北大学先进研究组织，日本宫城县仙台市980-8577 ⁵⁵隶属机构：宇宙物理与数学研究所（WPI），东京大学，日本千叶县柏市柏之叶5-1-5，277-8583
       Nozomu Tominaga⁶⁶隶属机构：日本国立天文台，日本东京市三鹰市大泽2-21-1，181-8588 ⁷⁷隶属机构：天文科学系，物理科学学院，高级研究生大学（SOKENDAI），日本东京市三鹰市大泽2-21-1，181-8588
    ²²隶属机构标记： ⁵⁵隶属机构标记：    Shigeyuki Sako⁸⁸隶属机构：东京大学天文学研究所，科学研究生院，日本东京市三鹰市大泽2-21-1，181-0015
    ⁹⁹隶属机构：UTokyo行星空间科学组织，东京大学，日本东京市本乡文京区113-0033 ^(10)^(10)隶属机构：东京大学空间科学与技术合作研究组织，日本东京市本乡文京区113-0033
       Ryou Ohsawa⁸⁸隶属机构标记：    和 Naoki Yoshida^(11)^(11)隶属机构：东京大学物理系，科学研究生院，日本东京市本乡文京区7-3-1，113-0033
    ⁵⁵隶属机构标记： ^(12)^(12)隶属机构：智能物理研究所，东京大学，日本东京市本乡文京区7-3-1，113-0033 [ichiro.takahashi@astr.tohoku.ac.jp](mailto:ichiro.takahashi@astr.tohoku.ac.jp)
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We present a deep neural network Real/Bogus classifier that improves classification
    performance in the Tomo-e Gozen transient survey by handling label errors in the
    training data. In the wide-field, high-frequency transient survey with Tomo-e
    Gozen, the performance of conventional convolutional neural network classifier
    is not sufficient as about $10^{6}$ bogus detections appear every night. In need
    of a better classifier, we have developed a new two-stage training method. In
    this training method, label errors in the training data are first detected by
    normal supervised learning classification, and then they are unlabeled and used
    for training of semi-supervised learning. For actual observed data, the classifier
    with this method achieves an area under the curve (AUC) of 0.9998 and a false
    positive rate (FPR) of 0.0002 at true positive rate (TPR) of 0.9. This training
    method saves relabeling effort by humans and works better on training data with
    a high fraction of label errors. By implementing the developed classifier in the
    Tomo-e Gozen pipeline, the number of transient candidates was reduced to $\sim$40
    objects per night, which is $\sim$1/130 of the previous version, while maintaining
    the recovery rate of real transients. This enables more efficient selection of
    targets for follow-up observations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种深度神经网络实/伪分类器，通过处理训练数据中的标签错误，提高了Tomo-e Gozen瞬态调查的分类性能。在Tomo-e Gozen的广角高频瞬态调查中，传统卷积神经网络分类器的表现不足，因为每晚出现约$10^{6}$个伪检测。为了获得更好的分类器，我们开发了一种新的两阶段训练方法。在这种训练方法中，首先通过正常的监督学习分类检测训练数据中的标签错误，然后对这些标签错误进行去标签处理，并用于半监督学习的训练。对于实际观察数据，这种方法的分类器在真实阳性率（TPR）为0.9时，曲线下面积（AUC）达到0.9998，假阳性率（FPR）为0.0002。这种训练方法节省了人工重新标记的工作，并且在标签错误比例高的训练数据上表现更好。通过在Tomo-e
    Gozen流程中实现开发的分类器，每晚的瞬态候选对象数量减少到$\sim$40个，相比之前版本减少了$\sim$1/130，同时保持了真实瞬态的回收率。这使得后续观察目标的选择更加高效。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Time-domain astronomy has become an active area in the modern astronomy. Studies
    of transient phenomena such as supernovae have been rapidly developing in recent
    years. To observe transients efficiently, transient surveys have become more wide-field,
    more sensitive, and more frequent. As a result, the number of discovered transients
    has dramatically increased; the reported number of transients reach tens of thousands
    per year. In the near future, hundreds of transient objects will be discovered
    every night with e.g., the Vera C. Rubin Observatory ([[Ivezić et al. (2019)](#bib.bib15)]).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 时域天文学已成为现代天文学中的一个活跃领域。对超新星等瞬态现象的研究近年来发展迅速。为了高效观察瞬态，瞬态调查变得更广角、更灵敏、频率更高。因此，发现的瞬态数量急剧增加；报告的瞬态数量每年达到数万。在不久的将来，每晚将发现数百个瞬态对象，例如，通过Vera
    C. Rubin天文台（[[Ivezić et al. (2019)](#bib.bib15)]）。
- en: To detect transients from large data, most of transient surveys implement image
    subtraction method. It detects transients by subtracting the past reference image
    from the observed new image. By image subtraction, only objects that change brightness,
    such as transients, can be extracted. The subtraction method can efficiently detect
    transients blended in galaxies. However, this method also has disadvantage, that
    is, it tends to generate a large amount of fake detections (hereafter called Bogus).
    Therefore, development of efficient methods to remove a large number of Bogus
    has become important. In order to select the target for follow-up observations,
    it is necessary to extract the real transients (hereafter called Real) from the
    detected candidates that include Bogus. However, with the increase of the scale
    of observations, the number of Bogus has increased to a level that is not feasible
    for the human eyes to check. For example, in Palomar Transient Factory (PTF; [[Law
    et al. (2009)](#bib.bib18)]), an order of $10^{6}$ potential candidates are detected
    per night ([[Brink et al. (2013)](#bib.bib6)]). Among these, the number of Bogus
    is estimated to be more than 1000 times greater than that of Real ([[Mahabal et al.
    (2019)](#bib.bib19)]). Thus, conventional selection methods, such as the parameter
    cutting, are no longer able to narrow down the candidates.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从大量数据中检测瞬态，大多数瞬态调查实施了图像减法方法。它通过从观察到的新图像中减去过去的参考图像来检测瞬态。通过图像减法，只有亮度发生变化的物体，如瞬态，才可以被提取。减法方法可以有效地检测与星系混合的瞬态。然而，这种方法也有一个缺点，即它倾向于产生大量虚假的检测（以下简称为Bogus）。因此，开发有效的方法来去除大量Bogus变得非常重要。为了选择跟踪观察的目标，有必要从包括Bogus在内的检测候选中提取真正的瞬态（以下简称为Real）。然而，随着观察规模的增加，Bogus的数量已增加到人工检查不可行的程度。例如，在Palomar
    Transient Factory (PTF; [[Law et al. (2009)](#bib.bib18)])，每晚检测到约$10^{6}$个潜在候选（[[Brink
    et al. (2013)](#bib.bib6)]）。其中，Bogus的数量估计是Real的1000倍以上（[[Mahabal et al. (2019)](#bib.bib19)]）。因此，传统的选择方法，如参数切割，已经无法缩小候选范围。
- en: Machine learning techniques are therefore gaining attention as an alternative
    method. In the case of Real/Bogus classification, by allowing the machine to learn
    the relationship between the data of detected objects and their classification
    results, the machine can classify transient candidates. If trained in advance,
    classification is fast and can be performed in real time for a large amount of
    data. Various methods for Real/Bogus classification by machine learning have been
    proposed and implemented in many transient surveys. In the early era, classification
    was performed by inputting features extracted from images into Random Forest or
    Neural Networks (e.g., [[Bloom et al. (2012)](#bib.bib5)]; [[Brink et al. (2013)](#bib.bib6)];
    [[Wright et al. (2015)](#bib.bib30)]; [[Morii et al. (2016)](#bib.bib21)]). Recently,
    the use of Convolutional Neural Networks (CNN), in which image data are directly
    input and the machine itself learns features, has become mainstream (e.g., [[Gieseke
    et al. (2017)](#bib.bib9)]; [[Turpin et al. (2020)](#bib.bib27)]; [[Killestein
    et al. (2021)](#bib.bib16)]; [[Hosenie et al. (2021)](#bib.bib13)]). For example,
    In the Zwicky Transient Facility (ZTF; [[Bellm et al. (2019)](#bib.bib4)]) survey,
    a CNN-based classifier, braai ([[Duev et al. (2019)](#bib.bib7)]) is applied.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习技术作为一种替代方法引起了关注。在Real/Bogus分类的情况下，通过让机器学习检测到的物体数据与其分类结果之间的关系，机器可以对瞬态候选进行分类。如果提前进行训练，分类速度很快，可以实时处理大量数据。各种机器学习方法用于Real/Bogus分类已经在许多瞬态调查中提出并实现。在早期，通过将从图像中提取的特征输入到随机森林或神经网络中进行分类（例如，[[Bloom
    et al. (2012)](#bib.bib5)]；[[Brink et al. (2013)](#bib.bib6)]；[[Wright et al.
    (2015)](#bib.bib30)]；[[Morii et al. (2016)](#bib.bib21)]）。最近，使用卷积神经网络（CNN），直接输入图像数据并由机器自身学习特征，已成为主流（例如，[[Gieseke
    et al. (2017)](#bib.bib9)]；[[Turpin et al. (2020)](#bib.bib27)]；[[Killestein et
    al. (2021)](#bib.bib16)]；[[Hosenie et al. (2021)](#bib.bib13)]）。例如，在Zwicky Transient
    Facility (ZTF; [[Bellm et al. (2019)](#bib.bib4)])调查中，应用了基于CNN的分类器braai（[[Duev
    et al. (2019)](#bib.bib7)]）。
- en: The Tomo-e Gozen transient survey is a time-domain survey project, which utilizes
    a wide-field Tomo-e Gozen camera with 84 CMOS sensors covering a field of view
    of 20 deg² per exposure ([[Sako et al. (2018)](#bib.bib24)]). The transient survey
    is performed with a high cadence of about 3-4 times per night with a typical sensitivity
    of 18 mag without filters. The survey is observing at a rate of $10^{5}$ images/day,
    and as many as $10^{6}$ transient candidates are detected every night. Although
    the CNN classifier was used to sort out real transients from these candidates,
    which are mostly Bogus, the classification performance was not sufficient. There
    was still a large amount of false positives (an order of $10^{3}$/day), i.e.,
    Bogus classified as Real. Thus, we needed a new classifier with a higher performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Tomo-e Gozen瞬态调查是一个时域调查项目，利用一个配备84个CMOS传感器的宽视场Tomo-e Gozen相机，每次曝光覆盖20平方度的视场（[[Sako
    et al. (2018)](#bib.bib24)]）。瞬态调查的高频率约为每晚3-4次，典型灵敏度为18等，无滤镜。调查的观察速度为$10^{5}$张图像/天，每晚检测到多达$10^{6}$个瞬态候选。尽管使用了CNN分类器将真实的瞬态与这些大多是虚假的候选者区分开来，但分类性能仍不够充分。仍有大量假阳性（每天约$10^{3}$个），即虚假的被分类为真实的。因此，我们需要一种性能更高的新分类器。
- en: To obtain a higher performance in classification, one can use more complex models.
    In general, training of complex models requires large amounts of training data.
    In such cases, the training is usually done by using simulated data rather than
    real data. However, with millions of samples of simulated data, it becomes infeasible
    to check their quality of simulation data manually. As a result, the training
    data can be contaminated by label errors, e.g., Real mislabeled as Bogus. When
    label errors are included in the training data, they cause an adverse impact on
    performance (e.g., [[Ayyar et al. (2022)](#bib.bib2)]).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更高的分类性能，可以使用更复杂的模型。通常，复杂模型的训练需要大量的训练数据。在这种情况下，训练通常是通过使用模拟数据而非真实数据进行的。然而，当模拟数据达到数百万个样本时，人工检查其质量变得不可行。因此，训练数据可能会被标签错误污染，例如，真实的被错误标记为虚假。当训练数据中包含标签错误时，会对性能产生负面影响（例如，[[Ayyar
    et al. (2022)](#bib.bib2)])。
- en: This paper describes improvements in the Real/Bogus classification of the Tomo-e
    Gozen transient survey by using complex machine learning model and by handling
    label errors. The structure of this paper is as follows. We first introduce the
    observational dataset used in this work in section 2. Then, the design of the
    new classifiers is described in section 3. The performance of these classifiers
    is presented in section 4. In section 5, we discuss key factors for improving
    the performance and show improvements in the actual operation. Finally, we give
    conclusions in section 6.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述了通过使用复杂的机器学习模型和处理标签错误来改进Tomo-e Gozen瞬态调查的真实/虚假分类。本文的结构如下：我们首先在第2节介绍用于本工作的观测数据集。然后，第3节描述了新分类器的设计。第4节展示了这些分类器的性能。在第5节，我们讨论了提高性能的关键因素，并展示了实际操作中的改进。最后，第6节给出了结论。
- en: 2 Observational data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 观测数据
- en: In this section, we describe our transient survey and dataset used to develop
    the machine learning classifier. We use the optical images from Tomo-e Gozen camera
    mounted on the 1.05m Kiso Schmidt telescope ([[Sako et al. (2018)](#bib.bib24)]).
    The Tomo-e Gozen camera is a mosaic camera equipped with 84 CMOS sensors. Thanks
    to the fast CMOS readout, the survey data are taken with a rate of 2 frames/sec.
    For the transient survey, 12 or 18 consecutive images (6 or 9 sec exposure) are
    taken with no filter, and these images are stacked. A typical limiting magnitude
    of the stacked image is 18 mag.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了用于开发机器学习分类器的瞬态调查和数据集。我们使用来自Tomo-e Gozen相机的光学图像，该相机安装在1.05米Kiso Schmidt望远镜上（[[Sako
    et al. (2018)](#bib.bib24)]）。Tomo-e Gozen相机是一个配备84个CMOS传感器的马赛克相机。由于快速的CMOS读出，调查数据以每秒2帧的速度采集。对于瞬态调查，拍摄12或18张连续图像（6或9秒曝光），不使用滤镜，并将这些图像叠加在一起。叠加图像的典型极限星等为18等。
- en: For transient detection, image subtraction between the stacked image and the
    reference image is performed. For image subtraction, we use the hotpants software
    ([[Becker (2015)](#bib.bib3)]), which is based on the method by Alard & Lupton
    ([1998](#bib.bib1)). We use Pan-STARRS1 (PS1) $r$-band data (Waters et al. ([2020](#bib.bib29));
    Flewelling et al. ([2020](#bib.bib8))) as reference images. Since PS1 data have
    better sensitivity and better seeing, the sensitivity of the Tomo-e Gozen image
    is not degraded even after the image subtraction, which enables the efficient
    transient detection. Also, as the Tomo-e Gozen camera is a newly developed camera,
    there were no deep stacked images at the beginning of the operation. Thus, the
    use of the existing PS1 data enables the transient survey even at the early stage
    of the operation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行瞬态检测，执行了堆叠图像与参考图像之间的图像减法。对于图像减法，我们使用了基于Alard & Lupton ([1998](#bib.bib1))
    方法的hotpants软件（[[Becker (2015)](#bib.bib3)]）。我们使用Pan-STARRS1 (PS1) $r$-带数据（Waters等人
    ([2020](#bib.bib29)); Flewelling等人 ([2020](#bib.bib8))) 作为参考图像。由于PS1数据具有更好的灵敏度和更好的视场，Tomo-e
    Gozen图像的灵敏度在图像减法后没有降低，从而实现了高效的瞬态检测。此外，由于Tomo-e Gozen相机是新开发的相机，在操作初期没有深度堆叠图像。因此，使用现有的PS1数据使得即使在操作的早期阶段也能够进行瞬态调查。
- en: Thanks to the high observing rate, as many as $10^{5}$ stacked images are taken
    every night in the Tomo-e Gozen transient survey. The number of transient candidates
    can reach $10^{6}$ in one night. As in other transient surveys, the detected candidates
    are by far dominated by Bogus detection. Also, image subtraction between different
    telescopes/instruments tend to cause more Bogus due to the differences in various
    factors of the data, for example, response functions or pixel scales. In this
    paper, we show that this difficulty can be overcome by developing the high-performance,
    complex machine learning classifier (section [3](#S3 "3 Method ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey")).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于观测率高，在Tomo-e Gozen瞬态调查中，每晚可拍摄多达$10^{5}$张堆叠图像。瞬态候选者的数量在一夜之间可达到$10^{6}$。与其他瞬态调查一样，检测到的候选者大多被假检测主导。此外，由于数据中各种因素的差异（例如响应函数或像素尺度），不同望远镜/仪器之间的图像减法往往会导致更多的假检测。在本文中，我们展示了通过开发高性能复杂的机器学习分类器（第[3](#S3
    "3 方法 ‣ 深度学习实时/假检测分类用于Tomo-e Gozen瞬态调查")节），可以克服这一难题。
- en: '![Refer to caption](img/47b4384a56ddd2158f064d63dbcd2ffc.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47b4384a56ddd2158f064d63dbcd2ffc.png)'
- en: 'Figure 1: An example of input images (new, reference, and subtracted images
    from left to right).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：输入图像的示例（从左到右为新图像、参考图像和减法图像）。
- en: 'We here describe the dataset used to develop the machine learning classifier.
    For the Real/Bogus classification, we use a set of three images: observed new
    image, reference image, and subtracted image. For each, a cutout image around
    the transient candidate (29 $\times$ 29 pixels) is used as a input of the classifier.
    Figure [1](#S2.F1 "Figure 1 ‣ 2 Observational data ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey") shows an example of cutout
    images.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此描述用于开发机器学习分类器的数据集。对于实时/假检测分类，我们使用了一组三张图像：观测的新图像、参考图像和减法图像。对于每张图像，使用了围绕瞬态候选者的裁剪图像（29
    $\times$ 29像素）作为分类器的输入。图[1](#S2.F1 "图 1 ‣ 2 观测数据 ‣ 深度学习实时/假检测分类用于Tomo-e Gozen瞬态调查")展示了裁剪图像的示例。
- en: 'Since we need large dataset for training of complex machine, we use artificial
    objects as Real for training dataset. For this purpose, we constructed a point
    spread function (PSF) for each image by measuring the shapes of stars in the image,
    and then embedded the constructed PSF into the observational images with various
    brightness. The artificial objects were embedded at two kinds of locations: (1)
    uniform distribution around galaxies, and (2) random distribution for the entire
    region of the images. Here, (1) and (2) mimic normal transients and hostless transients,
    respectively. We prepared about $6\times 10^{5}$ samples for each case. When embedding
    artificial sources around galaxies, we randomly selected objects registered as
    extended sources in the Pan-STARRS catalog.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要大规模的数据集来训练复杂的机器，我们使用人工对象作为训练数据集中的真实样本。为此，我们通过测量图像中星星的形状构建了每个图像的点扩散函数（PSF），然后将构建的PSF嵌入到具有不同亮度的观测图像中。人工对象被嵌入在两种位置：（1）均匀分布在银河系周围，和（2）在整个图像区域的随机分布。这里，（1）和（2）分别模拟了正常瞬态和无宿主瞬态。我们为每种情况准备了约$6\times
    10^{5}$个样本。在银河系周围嵌入人工源时，我们随机选择了在Pan-STARRS目录中登记为扩展源的对象。
- en: For Bogus samples of training data, we used actual Bogus objects which were
    detected in the subtracted images of Tomo-e Gozen. Figure [2](#S2.F2 "Figure 2
    ‣ 2 Observational data ‣ Deep-learning Real/Bogus classification for the Tomo-e
    Gozen transient survey") shows examples of Bogus in our dataset. The total number
    of the Bogus samples is $2\times 10^{6}$. The majority of them are false detections
    due to failed subtraction ((a) and (b) in figure [2](#S2.F2 "Figure 2 ‣ 2 Observational
    data ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey")). Other cases include the false detection due to the diffracted light
    from bright stars (c), hot pixels of the sensor (d), and artificial noise patterns
    due to malfunction of the data acquisition system ((e) and (f)). It is emphasized
    that a small amount of Real transients can be included in the Bogus samples since
    we assume that all the detected objects are Bogus (i.e., label error, see section [4.2](#S4.SS2
    "4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey")).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据的虚假样本，我们使用了在Tomo-e Gozen的减法图像中检测到的实际虚假对象。图[2](#S2.F2 "Figure 2 ‣ 2 Observational
    data ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey")展示了我们数据集中虚假的示例。虚假样本的总数为$2\times 10^{6}$。其中大多数是由于减法失败而导致的错误检测（图[2](#S2.F2
    "Figure 2 ‣ 2 Observational data ‣ Deep-learning Real/Bogus classification for
    the Tomo-e Gozen transient survey")中的（a）和（b））。其他情况包括由亮星的衍射光（c）、传感器的热像素（d）和由于数据采集系统故障导致的人工噪声模式（（e）和（f））。需要强调的是，少量真实瞬态可能会被包括在虚假样本中，因为我们假设所有检测到的对象都是虚假的（即标签错误，见章节
    [4.2](#S4.SS2 "4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey")）。
- en: '![Refer to caption](img/5785e21594a964aad5b3f23aa3054cc1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5785e21594a964aad5b3f23aa3054cc1.png)'
- en: 'Figure 2: Examples of Bogus detection.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：虚假检测示例。
- en: For the validation dataset, we used samples extracted from the actual observational
    data from January to April 2021. The Real dataset includes 125 objects reported
    to the Transient Name Server (TNS)¹¹1TNS $\langle$https://www.wis-tns.org$\rangle$.
    that were observed by Tomo-e Gozen. Since some of them were detected multiple
    times, the total number of Real samples is 363. As Bogus samples, we used Bogus
    objects in the same images where the Real objects are detected. The total number
    of Bogus is 255,777. The actual Bogus to Real ratio is much higher than this sample
    ratio (see section [5.2](#S5.SS2 "5.2 Performance in actual operations ‣ 5 Discussion
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")).
    Nevertheless, this sample ratio was adopted because the size of Bogus dataset
    would exceed that of the training dataset if we adopt the actual ratio ($\sim$
    1:$10^{6}$). The number of samples in training and validation dataset are summarized
    in table [2](#S2 "2 Observational data ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey").
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证数据集，我们使用了从2021年1月至4月的实际观测数据提取的样本。真实数据集包括125个被报告给瞬态名称服务器（TNS）¹¹1TNS $\langle$https://www.wis-tns.org$\rangle$的对象，这些对象由Tomo-e
    Gozen观测到。由于其中一些被多次检测，总的真实样本数量为363。作为虚假样本，我们使用了在检测到真实对象的相同图像中的虚假对象。虚假的总数为255,777。实际的虚假与真实比率远高于这个样本比率（见章节
    [5.2](#S5.SS2 "5.2 Performance in actual operations ‣ 5 Discussion ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey")）。尽管如此，采用这个样本比率是因为如果我们采用实际比率（$\sim$
    1:$10^{6}$），虚假数据集的大小将超过训练数据集的大小。训练和验证数据集中的样本数量总结在表[2](#S2 "2 Observational data
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")中。
- en: Finally, we test our classifiers in the actual operation of the Tomo-e Gozen
    transient survey (section [5.2](#S5.SS2 "5.2 Performance in actual operations
    ‣ 5 Discussion ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen
    transient survey")). For this purpose, we use the data taken for five nights,
    which correspond to about $5\times 10^{5}$ images and $5\times 10^{6}$ detections
    in total.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在Tomo-e Gozen瞬态调查的实际操作中测试了我们的分类器（见章节 [5.2](#S5.SS2 "5.2 Performance in
    actual operations ‣ 5 Discussion ‣ Deep-learning Real/Bogus classification for
    the Tomo-e Gozen transient survey")）。为此，我们使用了五晚的数据，总计约$5\times 10^{5}$张图像和$5\times
    10^{6}$次检测。
- en: \tbl
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \tbl
- en: Number of training and validation dataset. Dataset Number Note Training Real
    1,224,773 Artificial Bogus 2,031,193 Actual Validation Real 363 TNS Bogus 255,777
    Actual
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证数据集数量。数据集数量 说明 训练 真实 1,224,773 人工虚假 2,031,193 实际验证 真实 363 TNS 虚假 255,777
    实际
- en: 3 Method
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: To improve the performance of the conventional classifier, we modify the neural
    network into the one with a more complex structure (section [3.1](#S3.SS1 "3.1
    Model architecture ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the
    Tomo-e Gozen transient survey")). In addition, in order to take advantage of neural
    networks with complex structures, we propose a new training method²²2The source
    code of our proposed method is available at $\langle$https://github.com/ichiro-takahashi/tomoe-realbogus$\rangle$.
    devised for objective functions and dataset handling (section [3.2](#S3.SS2 "3.2
    Training methods ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the
    Tomo-e Gozen transient survey")).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高传统分类器的性能，我们将神经网络修改为具有更复杂结构的网络（见[3.1](#S3.SS1 "3.1 Model architecture ‣ 3
    Method ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey")）。此外，为了充分利用具有复杂结构的神经网络，我们提出了一种新的训练方法²²2我们提出的方法的源代码可以在$\langle$https://github.com/ichiro-takahashi/tomoe-realbogus$\rangle$中找到。专门针对目标函数和数据集处理（见[3.2](#S3.SS2
    "3.2 Training methods ‣ 3 Method ‣ Deep-learning Real/Bogus classification for
    the Tomo-e Gozen transient survey")）。
- en: 3.1 Model architecture
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 模型架构
- en: This subsection describes the model structure of a simple conventional model,
    which serves as a baseline, and a more complex model we propose. Hereafter, we
    call them as “Simple model” and “Complex model”, respectively. Both models perform
    binary classification (Real or Bogus) for a detected object by using three input
    images. Since Real class is important here, Real class and Bogus class are defined
    as positive class and negative class, respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节描述了一种简单传统模型的模型结构，该模型作为基线，以及我们提出的更复杂的模型。以后我们将其称为“简单模型”和“复杂模型”。这两个模型通过使用三个输入图像对检测到的对象进行二分类（真实或虚假）。由于真实类别在这里很重要，因此真实类别和虚假类别分别定义为正类和负类。
- en: 3.1.1 Simple model
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 简单模型
- en: As shown in figure [4](#footnote4 "footnote 4 ‣ Figure 3 ‣ 3.1.1 Simple model
    ‣ 3.1 Model architecture ‣ 3 Method ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey"), the Simple model consists of two convolutional
    layers in the first half and three fully connected layers in the second half.
    This structure follows the VGG model (Simonyan & Zisserman ([2014](#bib.bib25))),
    which is a basic model structure for image classification tasks using deep learning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](#footnote4 "footnote 4 ‣ Figure 3 ‣ 3.1.1 Simple model ‣ 3.1 Model architecture
    ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey")所示，简单模型由前半部分的两个卷积层和后半部分的三个全连接层组成。该结构遵循VGG模型（Simonyan & Zisserman ([2014](#bib.bib25)))，这是使用深度学习进行图像分类任务的基本模型结构。
- en: '![Refer to caption](img/ddfd0e2580a2c7ed1aca8ca403d88a7d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ddfd0e2580a2c7ed1aca8ca403d88a7d.png)'
- en: 'Figure 3: Architecture diagram of the Simple model. Since no padding is performed
    in the convolution layer, the spatial size of the features decreases with each
    pass through the convolution layer. The model adopts kernel_size$=$5 for the first
    convolutional layer and kernel_size$=$3 for the second convolutional layer. The
    flatten layer collapses the input (6,6,64) features into a vector with a size
    of 2304. In the dropout layer, the probability of the value being 0 is set to
    0.3. This figure was generated by PlotNeuralNet ⁴⁴4PlotNeuralNet $\langle$https://github.com/HarisIqbal88/PlotNeuralNet$\rangle$.
    and modified.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 简单模型的架构图。由于卷积层中没有进行填充，特征的空间尺寸在每次通过卷积层时都会减小。该模型在第一个卷积层中采用kernel_size$=$5，在第二个卷积层中采用kernel_size$=$3。展平层将输入的(6,6,64)特征压缩为一个大小为2304的向量。在dropout层中，值为0的概率设置为0.3。该图由PlotNeuralNet
    ⁴⁴4PlotNeuralNet $\langle$https://github.com/HarisIqbal88/PlotNeuralNet$\rangle$生成并修改。'
- en: '⁴⁴footnotetext: PlotNeuralNet $\langle$https://github.com/HarisIqbal88/PlotNeuralNet$\rangle$.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴⁴脚注文本：PlotNeuralNet $\langle$https://github.com/HarisIqbal88/PlotNeuralNet$\rangle$。
- en: 'In order to keep the range of values inside the network consistent, we normalize
    the input images. The normalization is performed on the original image $\mathbf{u}_{i,c}\in\mathcal{R}^{H\times
    W}$ of the $c$th channel in the $i$th sample ($H$ and $W$ are the height and width
    of each image) as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持网络内部值的范围一致，我们对输入图像进行归一化。归一化操作在第$i$个样本的第$c$通道的原始图像$\mathbf{u}_{i,c}\in\mathcal{R}^{H\times
    W}$上进行（$H$和$W$分别是每张图像的高度和宽度），具体如下：
- en: '|  | $\displaystyle\mathbf{x}_{i,c}=\frac{\mathbf{u}_{i,c}-\min\left(\mathbf{u}_{i,c}\right)}{\max\left(\mathbf{u}_{i,c}\right)-\min\left(\mathbf{u}_{i,c}\right)},$
    |  | (1) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x}_{i,c}=\frac{\mathbf{u}_{i,c}-\min\left(\mathbf{u}_{i,c}\right)}{\max\left(\mathbf{u}_{i,c}\right)-\min\left(\mathbf{u}_{i,c}\right)},$
    |  | (1) |'
- en: where $\max\left(\right)$ and $\min\left(\right)$ are functions that return
    the maximum and minimum values of the image, respectively. The output of the network
    is a two-dimensional vector. By normalizing the output vector with the softmax
    function, it can be interpreted as a probability that the object is Real.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\max\left(\right)$ 和 $\min\left(\right)$ 是分别返回图像的最大值和最小值的函数。网络的输出是一个二维向量。通过使用
    softmax 函数对输出向量进行归一化，可以将其解释为对象为 Real 的概率。
- en: 3.1.2 Complex model
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 复杂模型
- en: To achieve higher performance than the Simple model, we increase the number
    of layers in the new Complex model. However, increasing the number of layers makes
    training more difficult. One famous model that addresses this problem is ResNet
    (He et al. ([2016a](#bib.bib10)); He et al. ([2016b](#bib.bib11))), which has
    various extended versions. Among many extensions of ResNet, we have adopted SE-ResNet,
    which includes a channel and spatial Squeeze & Excitation (csSE) layer (Roy et al.
    ([2018](#bib.bib23))). Figure [4](#S3.F4 "Figure 4 ‣ 3.1.2 Complex model ‣ 3.1
    Model architecture ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the
    Tomo-e Gozen transient survey") shows the architecture diagram of the Complex
    model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现比简单模型更高的性能，我们在新的复杂模型中增加了层数。然而，增加层数会使训练变得更加困难。一个著名的解决这个问题的模型是 ResNet (He
    et al. ([2016a](#bib.bib10)); He et al. ([2016b](#bib.bib11)))，它有各种扩展版本。在众多 ResNet
    扩展中，我们采用了 SE-ResNet，它包括一个通道和空间的 Squeeze & Excitation (csSE) 层 (Roy et al. ([2018](#bib.bib23)))。图
    [4](#S3.F4 "图 4 ‣ 3.1.2 复杂模型 ‣ 3.1 模型架构 ‣ 3 方法 ‣ 深度学习 Real/Bogus 分类用于 Tomo-e Gozen
    瞬态调查") 显示了复杂模型的架构图。
- en: '![Refer to caption](img/01e800dffa49120035f56d439a2fb9b4.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/01e800dffa49120035f56d439a2fb9b4.png)'
- en: 'Figure 4: Architecture diagram of the Complex model. In the convolution layer,
    the kernel size is 5 only in the first layer after input and it is 3 in the remaining
    layers. Also, padding is performed to make the size in the spatial direction the
    same for input and output. Each region with a light blue background is a Residual
    Block. The third Residual Block from the input side has a convolution layer with
    stride$=$2 in the skip connection to adjust the feature size. The global averaging
    pooling calculates averages in the spatial direction for each channel and downsamples
    features of size (4, 4, 256) into a 256-dimensional vector. This figure was generated
    by PlotNeuralNet and modified.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：复杂模型的架构图。在卷积层中，卷积核大小在输入后的第一层为 5，剩余层为 3。此外，通过填充使得输入和输出在空间方向上的大小相同。每个背景为浅蓝色的区域是一个残差块。来自输入侧的第三个残差块在跳跃连接中有一个步幅$=$2的卷积层以调整特征大小。全局平均池化计算每个通道在空间方向上的平均值，并将大小为
    (4, 4, 256) 的特征下采样为 256 维的向量。该图由 PlotNeuralNet 生成并进行了修改。
- en: ResNet is a network structure consisting of stacked Residual Blocks, each of
    which consists of convolutional layers and a skip connection between the input
    and the output of the block. The skip connection eliminates the vanishing gradient
    problem and this facilitates performance of training. Since it is difficult to
    learn the identity map only with convolutional layers, there is a problem that
    too many convolutional layers degrade the performance. The skip connection makes
    it easier to learn the identity map in the entire Residual Block. Therefore, high
    performance can be achieved even in a deep network of stacked Residual Blocks.
    Since the csSE layer emphasizes the effective parts of the features for classification,
    SE-ResNet is expected to improve classification performance compared to the original
    ResNet.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 是一种由堆叠的残差块组成的网络结构，每个残差块包括卷积层和输入与输出之间的跳跃连接。跳跃连接消除了梯度消失问题，从而促进了训练性能。由于仅使用卷积层难以学习恒等映射，因此过多的卷积层会导致性能下降。跳跃连接使得在整个残差块中学习恒等映射变得更容易。因此，即使在堆叠的残差块的深层网络中也能实现高性能。由于
    csSE 层强调分类特征的有效部分，SE-ResNet 预计相比于原始 ResNet 能提高分类性能。
- en: One component of the Residual Block is the Batch Normalization layer (Ioffe
    & Szegedy ([2015](#bib.bib14))). It has the effect of stabilizing and speeding
    up training which contributes to the success in learning deep networks. However,
    Batch Normalization has several weaknesses, one of which is its performance degrading
    when the samples in a batch are highly correlated. This is a concern when there
    are many similar images with a high correlation, such as astronomical images.
    Therefore, instead of using Batch Normalization, we used Filter Response Normalization
    (Singh & Krishnan ([2020](#bib.bib26))), which is not affected by the correlation
    between samples because the normalization is done per channel and per sample.
    The input to the network, its normalization method, and the format of the output
    are the same as for the Simple model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Residual Block 的一个组件是 Batch Normalization 层（Ioffe & Szegedy ([2015](#bib.bib14)））。它具有稳定和加快训练的效果，这有助于深度网络的学习成功。然而，Batch
    Normalization 存在几个弱点，其中之一是当一个批次中的样本高度相关时，其性能会降低。这在存在许多高度相关的类似图像时是一个问题，例如天文图像。因此，我们使用了
    Filter Response Normalization（Singh & Krishnan ([2020](#bib.bib26)）），因为这种归一化是按通道和样本进行的，不受样本之间相关性的影响。网络的输入、归一化方法和输出格式与
    Simple 模型相同。
- en: 3.2 Training methods
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 训练方法
- en: 'This subsection describes the training methods for the conventional and new
    classifiers. We test multiple methods for three phases of training: (1) how to
    treat the training data, (2) which objective function is used, and (3) how to
    handle label errors.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节描述了传统分类器和新分类器的训练方法。我们测试了训练的三个阶段的多种方法：（1）如何处理训练数据，（2）使用哪个目标函数，以及（3）如何处理标签错误。
- en: 3.2.1 Treatment of training data
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 训练数据处理
- en: 'For the treatment of training data, two methods are tested: the first one is
    to prepare the training data for each CMOS sensor and prepare a classifier for
    each sensor; the other is to combine the training data of all the sensors and
    train a single classifier to classify the data of all the sensors. These tests
    are performed because it is not obvious which option gives a better performance,
    i.e, having each classifier specialized for each sensor to care the sensor diversity
    or having a unified classifier for all the sensors.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据的处理方面，测试了两种方法：第一种是为每个 CMOS 传感器准备训练数据，并为每个传感器准备一个分类器；另一种是将所有传感器的训练数据合并，并训练一个单一的分类器来分类所有传感器的数据。这些测试是因为不明显哪个选项提供更好的性能，即是否为每个传感器专门设置分类器以照顾传感器的多样性，或者是否为所有传感器使用统一的分类器。
- en: 3.2.2 Objective functions
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 目标函数
- en: 'For objective functions for training, three types of functions are used: the
    cross-entropy function, exp-Cross-hinge function (Kurora et al. ([2020](#bib.bib17))),
    and local distributional smoothness function (Miyato et al. ([2016](#bib.bib20))).
    They are used for the purpose of loss function for individual samples, loss function
    for the entire dataset, and loss function to make the classifier robust to input
    perturbation, respectively. Each training image $\mathbf{x}$ prepared in section
    [2](#S2 "2 Observational data ‣ Deep-learning Real/Bogus classification for the
    Tomo-e Gozen transient survey") is paired with a teacher label $y$ indicating
    whether the image is positive class or negative class. In this work, we dare to
    ignore the labels of some of the training data (we call this procedure “unlabel”).
    We describe why we ignore the labels and how to select them from the dataset in
    section [3.2.3](#S3.SS2.SSS3 "3.2.3 Handling of label errors ‣ 3.2 Training methods
    ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey"). The objective function is defined as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练的目标函数，使用了三种类型的函数：交叉熵函数、exp-Cross-hinge 函数（Kurora 等 ([2020](#bib.bib17)））和局部分布平滑函数（Miyato
    等 ([2016](#bib.bib20)））。它们分别用于单个样本的损失函数、整个数据集的损失函数，以及使分类器对输入扰动具有鲁棒性的损失函数。每个在第
    [2](#S2 "2 Observational data ‣ Deep-learning Real/Bogus classification for the
    Tomo-e Gozen transient survey") 节准备的训练图像 $\mathbf{x}$ 与一个教师标签 $y$ 配对，指示图像是正类还是负类。在这项工作中，我们敢于忽略一些训练数据的标签（我们称这个过程为“unlabel”）。我们在第
    [3.2.3](#S3.SS2.SSS3 "3.2.3 Handling of label errors ‣ 3.2 Training methods ‣
    3 Method ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey") 节中描述了为什么忽略标签以及如何从数据集中选择它们。目标函数定义如下：
- en: '|  | $\displaystyle L\left(\left\{\mathbf{x}_{l},y\right\},\left\{\mathbf{x}_{u}\right\};\boldsymbol{\theta}\right)$
    | $\displaystyle=$ | $\displaystyle\lambda_{\mathrm{ce}}L_{\mathrm{ce}}\left(\left\{\mathbf{x}_{l},y\right\};\boldsymbol{\theta}\right)$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L\left(\left\{\mathbf{x}_{l},y\right\},\left\{\mathbf{x}_{u}\right\};\boldsymbol{\theta}\right)$
    | $\displaystyle=$ | $\displaystyle\lambda_{\mathrm{ce}}L_{\mathrm{ce}}\left(\left\{\mathbf{x}_{l},y\right\};\boldsymbol{\theta}\right)$
    |  | (2) |'
- en: '|  |  |  | $\displaystyle+\lambda_{\mathrm{ech}}L_{\mathrm{ech}}\left(\left\{\mathbf{x}_{l},y\right\};\boldsymbol{\theta}\right)$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $\displaystyle+\lambda_{\mathrm{ech}}L_{\mathrm{ech}}\left(\left\{\mathbf{x}_{l},y\right\};\boldsymbol{\theta}\right)$
    |  |'
- en: '|  |  |  | $\displaystyle+\lambda_{\mathrm{lds}}L_{\mathrm{lds}}\left(\left\{\mathbf{x}_{l}\right\}\cup\left\{\mathbf{x}_{u}\right\};\boldsymbol{\theta}\right).$
    |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $\displaystyle+\lambda_{\mathrm{lds}}L_{\mathrm{lds}}\left(\left\{\mathbf{x}_{l}\right\}\cup\left\{\mathbf{x}_{u}\right\};\boldsymbol{\theta}\right).$
    |  |'
- en: Here, $\left\{\mathbf{x}_{l},y\right\}$ is a set of pairs of labeled image and
    its label, $\left\{\mathbf{x}_{u}\right\}$ is a set of unlabeled images, $\boldsymbol{\theta}$
    is a set of trainable variables of the neural network, and $L_{\mathrm{ce}}$,
    $L_{\mathrm{ech}}$, and $L_{\mathrm{lds}}$ are cross-entropy loss, exp-Cross-hinge
    loss, and local distributional smoothness (LDS) loss, respectively. The exp-Cross-hinge
    function is related to Area Under the Curve (AUC) maximization. The local distribution
    smoothness function is a key component of the Virtual Adversarial Training (VAT).
    The details of each term are described in the Appendix.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\left\{\mathbf{x}_{l},y\right\}$ 是一组标记图像及其标签的对，$\left\{\mathbf{x}_{u}\right\}$
    是一组未标记图像，$\boldsymbol{\theta}$ 是神经网络的可训练变量集合，$L_{\mathrm{ce}}$、$L_{\mathrm{ech}}$
    和 $L_{\mathrm{lds}}$ 分别是交叉熵损失、指数交叉铰链损失和局部分布光滑（LDS）损失。指数交叉铰链函数与曲线下面积（AUC）最大化相关。局部分布光滑函数是虚拟对抗训练（VAT）的关键组成部分。每项的详细说明在附录中给出。
- en: Three scalar hyper-parameters $\lambda_{\mathrm{ce}}$, $\lambda_{\mathrm{ech}}$,
    and $\lambda_{\mathrm{lds}}$ control the effect of each term. By setting one or
    two $\lambda$ values in equation ([2](#S3.E2 "In 3.2.2 Objective functions ‣ 3.2
    Training methods ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the
    Tomo-e Gozen transient survey")) to 0, we can create several variations of the
    objective functions. The objective function of the Simple model corresponds to
    the case $\left(\lambda_{\mathrm{ce}},\lambda_{\mathrm{ech}},\lambda_{\mathrm{lds}}\right)^{T}=\left(1,0,0\right)^{T}$,
    which equals to cross-entropy function. In section [4.2](#S4.SS2 "4.2 Effects
    of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey"), we test four patterns of objective functions
    for training the Complex model, where $\lambda_{\mathrm{ce}}$ is always non-zero
    and $\lambda_{\mathrm{ech}}$ and $\lambda_{\mathrm{lds}}$ can be zero or non-zero.
    It is necessary to tune the none zero element of the hyper-parameter $\left(\lambda_{\mathrm{ce}},\lambda_{\mathrm{ech}},\lambda_{\mathrm{lds}}\right)^{T}$.
    This tuning is performed by grid search, and multiple trials are made at each
    grid point to avoid possible variations in the results due to model initial values
    and other factors. The best combination of $\lambda$ values is the one that produces
    the model with the highest performance among the results of these trials.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 三个标量超参数 $\lambda_{\mathrm{ce}}$、$\lambda_{\mathrm{ech}}$ 和 $\lambda_{\mathrm{lds}}$
    控制每项的影响。通过将方程 ([2](#S3.E2 "In 3.2.2 Objective functions ‣ 3.2 Training methods
    ‣ 3 Method ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey")) 中一个或两个 $\lambda$ 值设置为 0，我们可以创建几个目标函数的变体。简单模型的目标函数对应于 $\left(\lambda_{\mathrm{ce}},\lambda_{\mathrm{ech}},\lambda_{\mathrm{lds}}\right)^{T}=\left(1,0,0\right)^{T}$
    的情况，即交叉熵函数。在 [4.2](#S4.SS2 "4.2 Effects of label error handling ‣ 4 Results ‣
    Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")
    节中，我们测试了四种目标函数模式用于训练复杂模型，其中 $\lambda_{\mathrm{ce}}$ 始终非零，$\lambda_{\mathrm{ech}}$
    和 $\lambda_{\mathrm{lds}}$ 可以是零或非零。必须调整超参数 $\left(\lambda_{\mathrm{ce}},\lambda_{\mathrm{ech}},\lambda_{\mathrm{lds}}\right)^{T}$
    的非零元素。这个调整是通过网格搜索完成的，并且在每个网格点上进行多次试验，以避免由于模型初始值和其他因素导致的结果变化。最佳的 $\lambda$ 值组合是那些在这些试验结果中产生最高性能的模型。
- en: In training, $\boldsymbol{\theta}$ is updated using the stochastic gradient
    descent to minimize the value of the objective function defined above.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，$\boldsymbol{\theta}$ 使用随机梯度下降法进行更新，以最小化上述定义的目标函数的值。
- en: 3.2.3 Handling of label errors
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 标签错误处理
- en: The training data we prepare are not always perfect. Some of them have incorrect
    labels or are difficult to label. It would be better to correct these label errors
    and train the classifiers with a clean training dataset. However, for large dataset,
    it is not practical to manually check and correct all of these samples as it requires
    an enormous amount of human effort. Therefore, we split the training into two
    stages to handle label errors. In the first stage of training, the machine finds
    samples that are likely to be mislabeled. Then, the second stage of training is
    performed by handling the samples found in the first stage. In both stages, the
    Complex model is used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备的训练数据并不总是完美的。其中一些有错误的标签或难以标记。更好的做法是纠正这些标签错误，并用干净的训练数据集训练分类器。然而，对于大规模数据集，手动检查和纠正所有这些样本是不切实际的，因为这需要大量的人力。因此，我们将训练分为两个阶段来处理标签错误。在训练的第一阶段，机器找到可能被标记错误的样本。然后，在第二阶段的训练中处理在第一阶段发现的样本。在两个阶段中，都使用复杂模型。
- en: 'To find label errors, we first train the classifier with the original training
    data by utilizing the fact that the ratio of label errors to the training data
    is sufficiently small. The classifier then classifies the training data themselves
    and identifies label errors. Specifically, the training dataset is divided into
    five categories: one is used for evaluation and the remaining four are used to
    train the classifier. Since there are five different ways to select data for evaluation,
    all the training data can be evaluated by five training and evaluation cycles
    in total. This method is the same as in Northcutt et al. ([2021](#bib.bib22)).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出标签错误，我们首先利用标签错误与训练数据的比例足够小这一事实，使用原始训练数据训练分类器。然后，分类器对训练数据进行分类，并识别标签错误。具体来说，训练数据集被分为五类：一类用于评估，其余四类用于训练分类器。由于有五种不同的选择数据进行评估的方式，因此所有训练数据可以通过五轮训练和评估周期来评估。这种方法与
    Northcutt 等人 ([2021](#bib.bib22)) 的方法相同。
- en: After classifying the training data themselves, we determine which samples are
    likely to have label errors based on the output of the classifier. Unlike Northcutt
    et al. ([2021](#bib.bib22)), we simply set the classification boundary to a probability
    of 0.5, and regard all misclassified samples as samples with potential label errors.
    Although the threshold value of 0.5 makes more samples to be potential label errors
    than the value used by Northcutt et al. ([2021](#bib.bib22)), the number of potential
    label errors is small ($\sim 1\%$) relative to the entire dataset in our case
    (section [4.2](#S4.SS2 "4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey")). Since the
    samples with potential label errors are used in semi-supervised learning as unlabeled
    samples, the effect of the overestimation is minor in the second stage of training
    described below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在对训练数据进行分类后，我们根据分类器的输出确定哪些样本可能存在标签错误。与 Northcutt 等人 ([2021](#bib.bib22)) 不同，我们仅将分类边界设置为
    0.5 的概率，并将所有分类错误的样本视为可能有标签错误的样本。尽管 0.5 的阈值使得更多样本被认为是潜在标签错误，相较于 Northcutt 等人 ([2021](#bib.bib22))
    使用的值，但在我们的案例中，相对于整个数据集，潜在标签错误的数量较少（$\sim 1\%$）（第 [4.2](#S4.SS2 "4.2 标签错误处理的影响
    ‣ 4 结果 ‣ Tomo-e Gozen 短暂调查的深度学习真实/虚假分类") 节）。由于潜在标签错误的样本在半监督学习中被用作未标记样本，因此在下面描述的第二阶段训练中的过度估计效果很小。
- en: In the second stage of training, we try two different methods to handle label
    errors. The first method is simply to remove the samples with potential label
    errors from the training dataset. The second method is setting the samples with
    potential label errors as “unlabeled” and then performing semi-supervised learning.
    The VAT does not use the labels in the computation of the objective function,
    which allows semi-supervised learning. Semi-supervised learning avoids the adverse
    effects of samples with label errors, while effectively utilizing them as training
    data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的第二阶段，我们尝试了两种不同的方法来处理标签错误。第一种方法是简单地从训练数据集中删除可能有标签错误的样本。第二种方法是将可能有标签错误的样本设置为“未标记”，然后进行半监督学习。VAT
    在目标函数的计算中不使用标签，这允许进行半监督学习。半监督学习避免了标签错误样本的负面影响，同时有效利用这些样本作为训练数据。
- en: 4 Results
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: In this section, we summarize the Real/Bogus classification performance for
    the validation dataset with the various combinations of the models and training
    methods described in the previous sections.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了前面各节所描述的各种模型和训练方法组合下的真实/虚假分类性能。
- en: 4.1 Effects of training data treatments
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 训练数据处理的效果
- en: 'We compare the performance of the three cases of the Simple models: (1) the
    model trained for each CMOS senor (Simple-each), (2) the model that has the same
    total number of samples as (1) but trained with samples of all sensors (Simple-mix),
    and (3) the model trained with the entire data for all sensors (Simple-all).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了Simple模型的三种情况的性能：（1）针对每个CMOS传感器训练的模型（Simple-each），（2）样本总数与（1）相同但使用所有传感器样本训练的模型（Simple-mix），以及（3）使用所有传感器的整个数据训练的模型（Simple-all）。
- en: First, we examine the impact of the sample diversity from multiple sensors on
    performance. The conventional classifier adopts the Simple-each approach and are
    trained using only cross-entropy loss as the objective function. Classification
    based on a dataset from a single sensor can cause overfitting, in which the unique
    “habits” of the dataset are used to classify the data. By training on datasets
    from multiple sensors, it is possible to learn more essential features that do
    not depend on unique “habits”, and thus to obtain the effect of data augmentation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查多个传感器的样本多样性对性能的影响。传统分类器采用Simple-each方法，并仅使用交叉熵损失作为目标函数进行训练。基于单个传感器数据集的分类可能导致过拟合，其中数据集的独特“习惯”被用于分类数据。通过在多个传感器的数据集上训练，有可能学习到不依赖于独特“习惯”的更重要的特征，从而实现数据增强的效果。
- en: Second, we study the effect of the size of the training data. In the Simple-all
    case, the total number of actual training data is larger than that of the Simple-each
    case by a factor of 84 (the number of sensors).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们研究训练数据规模的影响。在Simple-all情况下，实际训练数据的总量比Simple-each情况下多84倍（传感器数量）。
- en: The performance of the classifier can be evaluated by the Receiver Operating
    Characteristic (ROC) curve.⁵⁵5ROC curve shows the true positive rate (TPR) and
    false positive rate (FPR) values measured with different thresholds. The better
    the classification performance is, the closer the ROC curve is to the upper left
    corner of the plot. The AUC of the ROC curve indicates the overall performance.
    For the Simple-each case, since a classifier is prepared and tested for each sensor,
    we measure the performance of these cases by combining the results of all sensors.
    Figures [5](#S4.F5 "Figure 5 ‣ 4.1 Effects of training data treatments ‣ 4 Results
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")
    and [6](#S4.F6 "Figure 6 ‣ 4.1 Effects of training data treatments ‣ 4 Results
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")
    show the AUC and FPR for each Simple model, respectively. To see the variation
    of the results, we plot the results of five training runs with different initial
    seed values for each case. The FPR in figure [6](#S4.F6 "Figure 6 ‣ 4.1 Effects
    of training data treatments ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey") is defined at a threshold when the TPR
    is 0.9. Comparison between the Simple-each case and the Simple-mix case shows
    that data-mixing with different sensors gives better results thanks to the data
    augmentation effect for the same size training data. Furthermore, the Simple-all
    case achieved better results. This means that both the larger size of the training
    data and the mixing of data from multiple sensors contribute to the improvement.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的性能可以通过接收操作特性（ROC）曲线来评估。⁵⁵ROC曲线展示了在不同阈值下测量的真正率（TPR）和假正率（FPR）值。分类性能越好，ROC曲线越接近图的左上角。ROC曲线的AUC值表示整体性能。对于简单每个情况，由于为每个传感器准备和测试了分类器，我们通过结合所有传感器的结果来测量这些情况的性能。图[5](#S4.F5
    "图 5 ‣ 4.1 训练数据处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen过渡调查")和[6](#S4.F6 "图 6
    ‣ 4.1 训练数据处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen过渡调查")分别显示了每个简单模型的AUC和FPR。为了查看结果的变化，我们绘制了每种情况中五次训练运行的结果，使用不同的初始种子值。图[6](#S4.F6
    "图 6 ‣ 4.1 训练数据处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen过渡调查")中的FPR在TPR为0.9时定义。简单每个情况与简单混合情况的比较显示，使用不同传感器的数据混合由于数据增强效果，对于相同大小的训练数据取得了更好的结果。此外，简单全模型取得了更好的结果。这意味着训练数据的更大规模以及来自多个传感器的数据混合都对性能提升有所贡献。
- en: '![Refer to caption](img/29c2328de36a7e939aac4e47e838968d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/29c2328de36a7e939aac4e47e838968d.png)'
- en: 'Figure 5: Comparison of AUC for the Simple models. The five points in each
    model represent the performance variation with different initial seed values.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：简单模型的AUC比较。每个模型中的五个点表示在不同初始种子值下的性能变化。
- en: '![Refer to caption](img/134a3f60cb1258cf18b09bca776643f4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/134a3f60cb1258cf18b09bca776643f4.png)'
- en: 'Figure 6: Same as figure [5](#S4.F5 "Figure 5 ‣ 4.1 Effects of training data
    treatments ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e
    Gozen transient survey"), but for FPR at TPR=0.9.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：与图[5](#S4.F5 "图 5 ‣ 4.1 训练数据处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen过渡调查")相同，但为FPR在TPR=0.9时的情况。
- en: 4.2 Effects of label error handling
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 标签错误处理的影响
- en: We here investigate whether treatment of label errors improves the performance
    of the Complex model. For the baseline of the performance comparison, we use the
    performance of the Simple-all case. When multiple objective functions are combined,
    optimal values for the weight of the functions, i.e., $\lambda_{\mathrm{ce}}$,
    $\lambda_{\mathrm{ech}}$, and $\lambda_{\mathrm{lds}}$, are obtained from the
    parameter search for each case. The optimal values in each case are summarized
    in table [4.2](#S4.SS2 "4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey").
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此研究标签错误的处理是否能改善复杂模型的性能。作为性能比较的基线，我们使用简单全模型的性能。当多个目标函数被组合时，函数权重的最优值，即$\lambda_{\mathrm{ce}}$、$\lambda_{\mathrm{ech}}$和$\lambda_{\mathrm{lds}}$，是通过对每种情况的参数搜索获得的。每种情况的最优值总结在表[4.2](#S4.SS2
    "4.2 标签错误处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen过渡调查")中。
- en: First of all, we estimate the fraction of label errors in the training data.
    For the training data, we use the entire data from all the sensors, as in the
    Simple-all case. The left panel of figure [7](#S4.F7 "Figure 7 ‣ 4.2 Effects of
    label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification for
    the Tomo-e Gozen transient survey") shows the score distribution when the training
    data themselves are classified using a Complex model trained with cross-entropy
    loss for the label error identification. The score here is defined by the score
    function in equation ([9](#Ax1.E9 "In exp-Cross-hinge loss (AUC maximization)
    ‣ Details of objective function ‣ Deep-learning Real/Bogus classification for
    the Tomo-e Gozen transient survey")), not probability. We pay particular attention
    to samples that the classifier misclassifies, i.e., those in the tail of the Real
    and Bogus distributions. For these samples, we check the images by visual inspection.
    Since there are a large number of samples even in the tail, we conducted a sample
    survey to estimate the fraction of label errors. The fraction is evaluated by
    visually counting the label errors from randomly selected samples for each score
    bin. The right panel of figure [7](#S4.F7 "Figure 7 ‣ 4.2 Effects of label error
    handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e
    Gozen transient survey") shows the distribution of the scores and the fraction
    of label errors in each score bin. In fact, we find that the fraction of label
    errors also increases at the edges of the distributions. Based on the estimated
    fractions of label errors, the contamination ratio of label errors in the training
    data is about 0.6% for Bogus and 1% for Real.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们估计训练数据中的标签错误比例。对于训练数据，我们使用来自所有传感器的完整数据，如同在Simple-all案例中一样。图 [7](#S4.F7
    "图 7 ‣ 4.2 标签错误处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬变调查") 的左侧面板显示了当训练数据本身使用训练有交叉熵损失的复杂模型进行分类时的分数分布。这里的分数由方程
    ([9](#Ax1.E9 "在exp-Cross-hinge损失（AUC最大化） ‣ 目标函数的细节 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬变调查"))
    中的分数函数定义，而非概率。我们特别关注分类器错误分类的样本，即真实和虚假分布的尾部样本。对于这些样本，我们通过视觉检查图像。由于尾部样本数量众多，我们进行了样本调查以估计标签错误的比例。通过对每个分数区间随机选择的样本进行视觉计数来评估比例。图 [7](#S4.F7
    "图 7 ‣ 4.2 标签错误处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬变调查") 的右侧面板显示了各分数区间的分数分布和标签错误比例。实际上，我们发现标签错误的比例在分布的边缘也会增加。根据估计的标签错误比例，训练数据中虚假的标签错误污染率约为0.6%，真实的约为1%。
- en: '| ![Refer to caption](img/c0df7a0010f93eb42dfab0c60a5a1309.png)  ![Refer to
    caption](img/525d374c58d1b987a4d839aa7b4f6f0d.png)  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/c0df7a0010f93eb42dfab0c60a5a1309.png)  ![参见标题](img/525d374c58d1b987a4d839aa7b4f6f0d.png)  |'
- en: 'Figure 7: Left: Score distribution of the training dataset. Right: Score distribution
    (left axis) and fraction of label errors estimated from a sample survey by human
    eyes (right axis). The sample survey is performed at the edges of the score distribution.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：左侧：训练数据集的分数分布。右侧：分数分布（左轴）和通过人工视觉估计的标签错误比例（右轴）。样本调查在分数分布的边缘进行。
- en: Figure [8](#S4.F8 "Figure 8 ‣ 4.2 Effects of label error handling ‣ 4 Results
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")
    shows examples with label errors at the edge of the score distribution. Among
    these, those mislabeled as Bogus clearly show objects that appear to be transients.
    On the other hand, those mislabeled as Real have embedded artificial stars, but
    with a distinct Bogus detection. In other words, the labels are not correct for
    these samples and the machine actually classifies them correctly.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S4.F8 "图 8 ‣ 4.2 标签错误处理的影响 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬变调查") 显示了分数分布边缘的标签错误示例。其中，错误标记为虚假的样本明显显示出似乎是瞬变的对象。另一方面，错误标记为真实的样本包含了人工星体，但具有明显的虚假检测。换句话说，这些样本的标签不正确，机器实际上将其正确分类。
- en: '![Refer to caption](img/10e91f05d6b202d226da0680e500ecf5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/10e91f05d6b202d226da0680e500ecf5.png)'
- en: 'Figure 8: Examples of the samples with label errors.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：标签错误样本的示例。
- en: 'Then, we investigate the effect to the classification performance for two different
    ways to handle label errors. First, we examine the method in which we simply remove
    the samples with potential label errors. The AUC and FPR with potential label
    errors (indices 2 and 4) and without potential label errors (indices 3 and 5)
    are shown in figures [9](#S4.F9 "Figure 9 ‣ 4.2 Effects of label error handling
    ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey") and [10](#S4.F10 "Figure 10 ‣ 4.2 Effects of label error handling ‣ 4
    Results ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey"), respectively. Two classifiers are used in the comparison: one with CE
    only and the other with CE+AUC as the objective function. In figures  [9](#S4.F9
    "Figure 9 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey") and [10](#S4.F10 "Figure
    10 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey"), the CE, AUC, and VAT columns
    indicate whether or not $L_{\mathrm{ce}}$, $L_{\mathrm{ech}}$, and $L_{\mathrm{lds}}$
    are used, respectively. Both classifiers perform better when the samples with
    potential label errors are removed (indices 3 and 5). The FPR is significantly
    lower when the samples with potential label errors are removed while the AUC shows
    no significant difference due to high variation in each seed value. However, in
    all the cases, the classifiers removing potential label errors do not perform
    better than the Simple-all case.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调查了处理标签错误的两种不同方法对分类性能的影响。首先，我们检查了简单地移除可能存在标签错误的样本的方法。图[9](#S4.F9 "Figure
    9 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")和图[10](#S4.F10 "Figure 10
    ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey")分别展示了有潜在标签错误（索引2和4）和没有潜在标签错误（索引3和5）情况下的AUC和FPR。在比较中使用了两个分类器：一个仅使用CE，另一个使用CE+AUC作为目标函数。在图[9](#S4.F9
    "Figure 9 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")和图[10](#S4.F10 "Figure 10
    ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey")中，CE、AUC和VAT列表示是否使用了$L_{\mathrm{ce}}$、$L_{\mathrm{ech}}$和$L_{\mathrm{lds}}$。当移除潜在标签错误的样本（索引3和5）时，两个分类器的表现均有所提升。移除潜在标签错误的样本时，FPR显著降低，而AUC由于每个种子值的高变异性显示出没有显著差异。然而，在所有情况下，移除潜在标签错误的分类器表现不如Simple-all情况。
- en: Next, we examine the semi-supervised learning method in which all samples with
    potential label errors are unlabeled. The AUC and FPR for the model with CE+AUC+VAT
    as the objective function in each case are shown as indices 8 and 9 in figures [9](#S4.F9
    "Figure 9 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey") and [10](#S4.F10 "Figure
    10 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey"). In this comparison, the
    method that handles potential label errors by unlabeling and by performing semi-supervised
    learning (index 9) shows better results than the supervised learning with potential
    label errors remaining (index 8). Furthermore, the semi-supervised learning method
    (index 9) yields a lower FPR at TPR=0.9 than the method that removes potential
    label errors (indices 3 and 5) and the Simple-all case (index 1). This means that
    the semi-supervised learning method can achieve good performance even if the training
    data contain label errors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查了半监督学习方法，其中所有可能存在标签错误的样本被标记为未标记。在图[9](#S4.F9 "Figure 9 ‣ 4.2 Effects
    of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey")和图[10](#S4.F10 "Figure 10 ‣ 4.2 Effects
    of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey")中，每种情况下目标函数为CE+AUC+VAT的模型的AUC和FPR分别显示为索引8和9。在这次比较中，通过未标记处理潜在标签错误并执行半监督学习的方法（索引9）显示出比保留潜在标签错误的监督学习（索引8）更好的结果。此外，半监督学习方法（索引9）在TPR=0.9时的FPR低于移除潜在标签错误的方法（索引3和5）和Simple-all情况（索引1）。这意味着即使训练数据包含标签错误，半监督学习方法也能实现良好的性能。
- en: Finally, we compare the classification performance in all the cases with different
    objective function combinations and with/without handling label errors. AUC and
    FPR for all the cases are summarized in figures [9](#S4.F9 "Figure 9 ‣ 4.2 Effects
    of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey") and [10](#S4.F10 "Figure 10 ‣ 4.2 Effects
    of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey"). Comparing all the cases, the case combining
    the three objective functions and using semi-supervised learning achieved the
    best results (index 9). For comparison, the prediction distributions and confusion
    matrices for the Simple-each case, the Simple-all case, and the best classifier
    are shown in figure [11](#S4.F11 "Figure 11 ‣ 4.2 Effects of label error handling
    ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey"). In the best classifier, the number of Bogus misclassified as Real is
    further reduced as compared with the Simple-all case, and the false positive of
    the confusion matrix is 1/23 of that of the Simple-each case. The ROC curve of
    the best classifier is shown with the green line in figure [12](#S4.F12 "Figure
    12 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey"), where the AUC reaches
    to 0.9998. Similarly, the relationship between FPR and false negative rate (FNR$=1-$TPR)
    is plotted as figure [13](#S4.F13 "Figure 13 ‣ 4.2 Effects of label error handling
    ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient
    survey"). The FPR decreases to 0.0002 when FNR=0.1 (TPR=0.9).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们比较了在不同目标函数组合下以及处理/不处理标签错误的情况下的分类性能。所有情况下的AUC和FPR汇总在图[9](#S4.F9 "Figure
    9 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")和图[10](#S4.F10 "Figure 10
    ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification
    for the Tomo-e Gozen transient survey")中。比较所有情况后，结合三种目标函数并使用半监督学习的情况（索引9）取得了最佳结果。为了比较，图[11](#S4.F11
    "Figure 11 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")展示了Simple-each情况、Simple-all情况和最佳分类器的预测分布及混淆矩阵。在最佳分类器中，与Simple-all情况相比，被误分类为Real的Bogus数量进一步减少，混淆矩阵中的假阳性是Simple-each情况的1/23。最佳分类器的ROC曲线在图[12](#S4.F12
    "Figure 12 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")中以绿色线条展示，其中AUC达到0.9998。类似地，FPR与假阴性率（FNR$=1-$TPR）的关系绘制在图[13](#S4.F13
    "Figure 13 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")中。当FNR=0.1（TPR=0.9）时，FPR降至0.0002。
- en: '![Refer to caption](img/ad35f2f7c28981f4ea6bad82830ed572.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad35f2f7c28981f4ea6bad82830ed572.png)'
- en: 'Figure 9: Summary of AUC for all the classifiers. Five points in each case
    represent the performance variation with different initial seed values. The filled
    points indicate the training with the highest AUC for each case. The CE, AUC,
    and VAT columns indicate whether or not each term of the objective function is
    used.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：所有分类器的AUC汇总。每种情况下的五个点表示不同初始种子值下的性能变化。填充点表示每种情况下AUC最高的训练。CE、AUC和VAT列指示是否使用了目标函数的每一项。
- en: '![Refer to caption](img/eb2fa376c84f62346e09be9bed4d604e.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb2fa376c84f62346e09be9bed4d604e.png)'
- en: 'Figure 10: Same as figure [9](#S4.F9 "Figure 9 ‣ 4.2 Effects of label error
    handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e
    Gozen transient survey"), but for FPR at TPR=0.9.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：与图[9](#S4.F9 "Figure 9 ‣ 4.2 Effects of label error handling ‣ 4 Results
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")相同，但为TPR=0.9时的FPR。
- en: \tbl
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \tbl
- en: Summary of classification performance. Model type Data type CE AUC VAT $\lambda_{\mathrm{ce}}$
    $\lambda_{\mathrm{ech}}$ $\lambda_{\mathrm{lds}}$ AUC FPR@TPR=0.9 simple each
    $\checkmark$ 1.0 0.0 0.0 0.9916 $9.868\times 10^{-3}$ simple mix $\checkmark$
    1.0 0.0 0.0 0.9964 $5.028\times 10^{-3}$ simple all $\checkmark$ 1.0 0.0 0.0 0.9997
    $3.323\times 10^{-4}$ complex all $\checkmark$ 1.0 0.0 0.0 0.9993 $7.272\times
    10^{-4}$ complex removed $\checkmark$ 1.0 0.0 0.0 0.9993 $8.367\times 10^{-4}$
    complex all $\checkmark$ $\checkmark$ 0.3 0.7 0.0 0.9992 $1.306\times 10^{-3}$
    complex removed $\checkmark$ $\checkmark$ 0.4 0.6 0.0 0.9996 $5.083\times 10^{-4}$
    complex all $\checkmark$ $\checkmark$ 0.4 0.0 0.6 0.9997 $1.603\times 10^{-4}$
    complex unlabeled $\checkmark$ $\checkmark$ 0.6 0.0 0.4 0.9997 $1.838\times 10^{-4}$
    complex all $\checkmark$ $\checkmark$ $\checkmark$ 0.3 0.3 0.4 0.9997 $2.541\times
    10^{-4}$ complex unlabeled $\checkmark$ $\checkmark$ $\checkmark$ 0.9 0.05 0.05
    0.9998 $1.994\times 10^{-4}$
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 分类性能总结。 模型类型 数据类型 CE AUC VAT $\lambda_{\mathrm{ce}}$ $\lambda_{\mathrm{ech}}$
    $\lambda_{\mathrm{lds}}$ AUC FPR@TPR=0.9 简单 每个 $\checkmark$ 1.0 0.0 0.0 0.9916
    $9.868\times 10^{-3}$ 简单 混合 $\checkmark$ 1.0 0.0 0.0 0.9964 $5.028\times 10^{-3}$
    简单 全部 $\checkmark$ 1.0 0.0 0.0 0.9997 $3.323\times 10^{-4}$ 复杂 全部 $\checkmark$
    1.0 0.0 0.0 0.9993 $7.272\times 10^{-4}$ 复杂 去除 $\checkmark$ 1.0 0.0 0.0 0.9993
    $8.367\times 10^{-4}$ 复杂 全部 $\checkmark$ $\checkmark$ 0.3 0.7 0.0 0.9992 $1.306\times
    10^{-3}$ 复杂 去除 $\checkmark$ $\checkmark$ 0.4 0.6 0.0 0.9996 $5.083\times 10^{-4}$
    复杂 全部 $\checkmark$ $\checkmark$ 0.4 0.0 0.6 0.9997 $1.603\times 10^{-4}$ 复杂 未标记
    $\checkmark$ $\checkmark$ 0.6 0.0 0.4 0.9997 $1.838\times 10^{-4}$ 复杂 全部 $\checkmark$
    $\checkmark$ $\checkmark$ 0.3 0.3 0.4 0.9997 $2.541\times 10^{-4}$ 复杂 未标记 $\checkmark$
    $\checkmark$ $\checkmark$ 0.9 0.05 0.05 0.9998 $1.994\times 10^{-4}$
- en: '![Refer to caption](img/3d37a2d2151fd7b20d46e343b1a78f71.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d37a2d2151fd7b20d46e343b1a78f71.png)'
- en: 'Figure 11: Upper panels: distribution of output probability (to be Real) for
    each classifier. Red and blue colors show the histograms for Real and Bogus samples,
    respectively. Lower panels: Confusion matrix of each classifier. The predicted
    labels in all the classifiers are the ones with a threshold of 0.5\. The ratio
    in each row is normalized to 1\. The numbers in parentheses represent the raw
    numbers.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：上面面板：每个分类器的输出概率（为真实）的分布。红色和蓝色分别表示真实样本和虚假样本的直方图。下面面板：每个分类器的混淆矩阵。所有分类器中的预测标签都是阈值为
    0.5 的标签。每一行的比例归一化为 1。括号中的数字表示原始数字。
- en: '![Refer to caption](img/4f11d8e1951c12e81190c61402e86c59.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f11d8e1951c12e81190c61402e86c59.png)'
- en: 'Figure 12: ROC curves of the Simple-each classifier and the best classifier.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：Simple-each 分类器和最佳分类器的 ROC 曲线。
- en: '![Refer to caption](img/69498ca314e492406d16e8bed9266787.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/69498ca314e492406d16e8bed9266787.png)'
- en: 'Figure 13: FNR against FPR for the Simple-each classifier and the best classifier.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：Simple-each 分类器和最佳分类器的 FNR 与 FPR 对比。
- en: 5 Discussion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: In this section, we review the performance improvement in our Real/Bogus classification
    and discuss the key factors of the improvement, as well as the actual performance
    after implementing the best classifier in the Tomo-e Gozen transient pipeline.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们回顾了在我们的真实/虚假分类中的性能提升，并讨论了这一提升的关键因素，以及在 Tomo-e Gozen 瞬态管道中实施最佳分类器后的实际性能。
- en: 5.1 Performance analysis
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 性能分析
- en: 'The following is a summary of the improvement in the Tomo-e Gozen Real/Bogus
    classification performance. As shown in table [4.2](#S4.SS2 "4.2 Effects of label
    error handling ‣ 4 Results ‣ Deep-learning Real/Bogus classification for the Tomo-e
    Gozen transient survey"), the best performance is achieved when all of the following
    conditions are satisfied:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Tomo-e Gozen 真实/虚假分类性能提升的总结。如表[4.2](#S4.SS2 "4.2 标签错误处理的影响 ‣ 4 结果 ‣ Tomo-e
    Gozen 瞬态调查的深度学习真实/虚假分类")所示，当满足以下所有条件时，达到最佳性能：
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Training data for all the sensors are combined.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有传感器的训练数据已被合并。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All three objective functions are used.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了所有三个目标函数。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Semi-supervised learning is applied.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用了半监督学习。
- en: Compared to the Simple-each case, the AUC improves from 0.9916 to 0.9998 and
    FPR at TPR$=$0.9 decreases from 0.0099 to 0.0002. Comparing the performance with
    the ZTF Real/Bogus classifier (FPR$=$0.017 at TPR$=$0.983, Duev et al. ([2019](#bib.bib7))),
    our best classifier gives FPR$=$0.003 at the same TPR. Although exact comparisons
    cannot be made between Tomo-e Gozen and ZTF because the instruments, pipelines,
    and Real/Bogus ratio are different, we achieve FPR comparable to that of the ZTF
    classifier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与Simple-each情况相比，AUC从0.9916提高到0.9998，TPR$=$0.9时的FPR从0.0099下降到0.0002。与ZTF Real/Bogus分类器（TPR$=$0.983时FPR$=$0.017，Duev等人（[2019](#bib.bib7)））的性能进行比较，我们的最佳分类器在相同的TPR下提供了FPR$=$0.003。尽管由于仪器、管道和Real/Bogus比例的不同，无法对Tomo-e
    Gozen和ZTF进行准确的比较，但我们实现了与ZTF分类器相当的FPR。
- en: We then discuss key factors in improving classification performance. First,
    combining training data for different sensors significantly improved performance.
    This improvement is a result of the data augmentation effect by mixing data from
    multiple sensors with different characteristics. Also, the increasing the number
    of training data improves the performance, as expected. In addition, incorporating
    LDS loss into the objective function improves classification performance with
    the Complex model compared to the Simple-all case. The Complex model tends to
    overfit the training data because it can handle complex representations, resulting
    in lower performance on the validation data. On the other hand, when LDS loss
    is included in the objective function, the LDS-based regularization avoids overfitting
    and improves performance. We also show that, by setting the label error samples
    to unlabeled samples and by performing semi-supervised learning, the performance
    is further improved.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们讨论了提高分类性能的关键因素。首先，结合不同传感器的训练数据显著提升了性能。这一提升是由于通过混合具有不同特征的多个传感器的数据而产生的数据增强效果。此外，增加训练数据的数量也能如预期般提高性能。此外，将LDS损失纳入目标函数，与Simple-all情况相比，Complex模型的分类性能有所提升。由于Complex模型可以处理复杂的表示，它倾向于对训练数据进行过拟合，从而在验证数据上的性能较低。另一方面，当LDS损失被纳入目标函数时，基于LDS的正则化避免了过拟合，从而提高了性能。我们还展示了，通过将标签错误样本设置为未标记样本并进行半监督学习，性能得到了进一步提升。
- en: We here investigate whether the proposed method also works when there are more
    label errors in training data. The fraction of label errors in our original training
    dataset is about 1%. We artificially increase the fraction of label errors by
    inverting the labels based on the estimated fraction of label errors (figure [7](#S4.F7
    "Figure 7 ‣ 4.2 Effects of label error handling ‣ 4 Results ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey")). We then classify the
    data with our best method, semi-supervised learning with data containing unlabeled
    samples, and compare the results with those obtained by training with label errors
    remaining. Figures [14](#S5.F14 "Figure 14 ‣ 5.1 Performance analysis ‣ 5 Discussion
    ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen transient survey")
    and [15](#S5.F15 "Figure 15 ‣ 5.1 Performance analysis ‣ 5 Discussion ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey") compare the
    AUC and FPR when the ratio of label errors is increased to about 5% and 10%. It
    is found that the degree of improvement is higher as the fraction of label errors
    increases. When the fraction of label errors is 1%, by handling label errors,
    the average FPR decreases from 0.0004 to 0.0002, i.e., an improvement by a factor
    of about 2. On the other hand, when the fraction of label errors is 10%, the average
    FPR decreases from 0.0039 to 0.0003, which corresponds to the improvement by a
    factor of 13. This means that our method is more effective for datasets with higher
    fractions of label errors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里调查所提方法在训练数据中存在更多标签错误时是否仍然有效。我们原始训练数据集中的标签错误比例约为1%。我们通过根据估计的标签错误比例（图 [7](#S4.F7
    "图 7 ‣ 4.2 标签错误处理效果 ‣ 4 结果 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬态调查")）反转标签，人工增加标签错误比例。然后，我们使用包含未标记样本的数据的最佳方法——半监督学习，对数据进行分类，并将结果与训练时标签错误未消除的情况进行比较。图 [14](#S5.F14
    "图 14 ‣ 5.1 性能分析 ‣ 5 讨论 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬态调查") 和 [15](#S5.F15 "图 15
    ‣ 5.1 性能分析 ‣ 5 讨论 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬态调查") 比较了当标签错误比例增加到约5%和10%时的AUC和FPR。结果发现，标签错误比例增加时，改进程度更高。当标签错误比例为1%时，通过处理标签错误，平均FPR从0.0004降至0.0002，即改善约为2倍。另一方面，当标签错误比例为10%时，平均FPR从0.0039降至0.0003，对应于13倍的改善。这意味着我们的方法在标签错误比例较高的数据集上更为有效。
- en: '![Refer to caption](img/d4b00db1cbba66ff79a16dbfabb6a1b9.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d4b00db1cbba66ff79a16dbfabb6a1b9.png)'
- en: 'Figure 14: AUC for different fractions of label errors.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：不同标签错误比例下的AUC。
- en: '![Refer to caption](img/1071384b6f36ac5f2558f1dc690aded2.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1071384b6f36ac5f2558f1dc690aded2.png)'
- en: 'Figure 15: Same as figure [14](#S5.F14 "Figure 14 ‣ 5.1 Performance analysis
    ‣ 5 Discussion ‣ Deep-learning Real/Bogus classification for the Tomo-e Gozen
    transient survey") but for FPR at TPR$=$0.9.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：与图 [14](#S5.F14 "图 14 ‣ 5.1 性能分析 ‣ 5 讨论 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬态调查")
    相同，但用于FPR在TPR$=$0.9时。
- en: 5.2 Performance in actual operations
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实际操作中的性能
- en: Finally, we discuss the actual performance of our best classifier when implemented
    in the data analysis pipeline of Tomo-e Gozen. Prior to implementation, we determined
    the threshold for the classification in the actual operation. Figure [16](#S5.F16
    "Figure 16 ‣ 5.2 Performance in actual operations ‣ 5 Discussion ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey") shows the variation
    of each metric as a function of the threshold. We set the threshold score to be
    0.85, which gives the best precision while keeping TPR above 0.9.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了当我们的最佳分类器在Tomo-e Gozen的数据分析管道中实施时的实际性能。在实施之前，我们确定了实际操作中的分类阈值。图 [16](#S5.F16
    "图 16 ‣ 5.2 实际操作中的性能 ‣ 5 讨论 ‣ 深度学习真实/虚假分类用于Tomo-e Gozen瞬态调查") 显示了每个指标随阈值变化的情况。我们将阈值设置为0.85，这在保持TPR高于0.9的同时提供了最佳精度。
- en: '![Refer to caption](img/19eaca928eef55bb44f247c3e91f6b7a.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/19eaca928eef55bb44f247c3e91f6b7a.png)'
- en: 'Figure 16: Variation of each metric as a function of threshold for the best
    classifier.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：最佳分类器的每个指标随阈值变化的情况。
- en: We investigate the changes in the number of transient candidates registered
    in the Tomo-e Gozen transient database before and after the implementation. Figure [17](#S5.F17
    "Figure 17 ‣ 5.2 Performance in actual operations ‣ 5 Discussion ‣ Deep-learning
    Real/Bogus classification for the Tomo-e Gozen transient survey") shows the number
    of registrations to the database before and after implementation over a five-day
    period. As a rule for registration in the database, objects detected for the first
    time by the classifier are registered as “variable”. Among the variable candidates,
    those detected twice at the same coordinates are registered as “transient”. After
    the implementation of the new classifier, the average numbers of variable and
    transient registrations were reduced to 1/160 and 1/130, respectively.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了在实施前后Tomo-e Gozen瞬态数据库中登记的瞬态候选对象数量的变化。图[17](#S5.F17 "图17 ‣ 5.2 实际操作中的性能
    ‣ 5 讨论 ‣ Tomo-e Gozen瞬态调查中的深度学习真实/伪分类")显示了在五天期间实施前后数据库中的登记数量。作为数据库登记的规则，分类器首次检测到的对象被登记为“变量”。在变量候选对象中，那些在相同坐标上检测到两次的被登记为“瞬态”。新分类器实施后，变量和瞬态登记的平均数量分别减少到了1/160和1/130。
- en: '![Refer to caption](img/0fa3a2ff774e61f0a1a0f5fdb8fb38d6.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0fa3a2ff774e61f0a1a0f5fdb8fb38d6.png)'
- en: 'Figure 17: The number of objects registered in the Tomo-e transient database
    before (left panel) and after (right panel) implementation of the best classifier.
    The conventional classifier is trained in the same way as the Simple-each classifier.
    Horizontal dotted lines show five-day averages.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：在实施最佳分类器前（左侧面板）和实施后（右侧面板）Tomo-e瞬态数据库中登记的对象数量。传统分类器与Simple-each分类器的训练方式相同。水平虚线表示五天的平均值。
- en: To confirm that the new classifier does not miss real objects, we examined the
    recovery rate of real objects. By matching registered transients with TNS objects,
    we confirmed that the ratio of the number of matches to the number of TNS objects
    is comparable before and after implementation. Furthermore, the fraction of the
    TNS object rate among the registered transient candidates is 86 times higher after
    the implementation. This indicates that Bogus, which are incorrectly registered
    due to misclassification, are greatly reduced. The number of final transient candidates
    are reduced from about about 5,000 to 40 objects per day. This rate is a level
    at which human can check visually, and enables effective target selection for
    follow-up observations in a short time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认新分类器没有遗漏真实对象，我们检查了真实对象的恢复率。通过将登记的瞬态与TNS对象进行匹配，我们确认匹配数量与TNS对象数量的比例在实施前后相当。此外，登记的瞬态候选对象中TNS对象的比例在实施后提高了86倍。这表明由于错误分类而错误登记的Bogus大大减少。最终瞬态候选对象的数量从每天约5,000个减少到40个。这一数量级可以由人工进行视觉检查，并且可以在短时间内有效地选择后续观察的目标。
- en: 6 Conclusions
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this paper, we have presented a new Real/Bogus classification scheme by
    handling label errors in the training data for the Tomo-e Gozen transient survey.
    In the wide-field, high-frequency survey with Tomo-e Gozen, which mainly targets
    early supernovae and rapid transients, the performance of conventional CNN classifiers
    was not sufficient to extract follow-up targets, because the number of Bogus was
    an order of $10^{3}$ per day. Therefore, we developed a two-step training method:
    (1) normal supervised learning to detect label errors in the training data, and
    (2) semi-supervised learning with training data which include potential label
    errors as unlabeled samples.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新的真实/伪分类方案，通过处理Tomo-e Gozen瞬态调查的训练数据中的标签错误。在主要针对早期超新星和快速瞬态的宽场高频调查中，传统CNN分类器的性能不足以提取后续目标，因为Bogus的数量每天达到$10^{3}$级别。因此，我们开发了一种两步训练方法：（1）常规监督学习以检测训练数据中的标签错误；（2）半监督学习，使用包含潜在标签错误的训练数据作为未标记样本。
- en: The best classifier with this method achieves an AUC of 0.9998 and FPR of 0.0002
    at TPR$=$0.9 for validation data prepared from actual observations. Our training
    method does not require human effort to relabel the samples with potential label
    errors. We also show that our method gives a higher performance improvement when
    the fraction of label errors is higher. Finally, we implemented the developed
    classifier in the Tomo-e Gozen pipeline. After implementation, the number of registered
    transient candidates was reduced by a factor of about 100, to 40 candidates per
    day, while maintaining the recovery rate of real transients. This enables more
    efficient selection of follow-up targets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法的最佳分类器在基于实际观测准备的验证数据中，实现了 0.9998 的 AUC 和 0.0002 的 FPR，在 TPR$=$0.9 的情况下。我们的方法不需要人工重新标记可能存在标签错误的样本。我们还表明，当标签错误的比例较高时，我们的方法能够提供更高的性能提升。最后，我们在
    Tomo-e Gozen 流水线中实现了开发的分类器。实施后，注册的过渡候选者数量减少了大约 100 倍，降至每天 40 个候选者，同时保持了真实过渡体的恢复率。这使得后续目标选择更加高效。
- en: '{ack}'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '{ack}'
- en: We thank Yasuhiro Imoto for his significant contributions to the development
    of the classifiers. We are grateful to the anonymous referee for insightful suggestions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Yasuhiro Imoto 对分类器开发所作出的重要贡献。我们还对匿名审稿人提出的有见地的建议表示感谢。
- en: This work has been supported by Japan Science and Technology Agency (JST) AIP
    Acceleration Research Grant Number JP20317829 and the Japan Society for the Promotion
    of Science (JSPS) KAKENHI grants 21H04491, 18H05223, and 17H06363.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了日本科学技术振兴机构（JST）AIP 加速研究资助（资助号 JP20317829）以及日本学术振兴会（JSPS）KAKENHI 资助（21H04491、18H05223
    和 17H06363）的支持。
- en: This work is supported in part by the Optical and Near-Infrared Astronomy Inter-University
    Cooperation Program.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了光学和近红外天文学跨大学合作计划的支持。
- en: The Pan-STARRS1 Surveys (PS1) and the PS1 public science archive have been made
    possible through contributions by the Institute for Astronomy, the University
    of Hawaii, the Pan-STARRS Project Office, the Max-Planck Society and its participating
    institutes, the Max Planck Institute for Astronomy, Heidelberg and the Max Planck
    Institute for Extraterrestrial Physics, Garching, The Johns Hopkins University,
    Durham University, the University of Edinburgh, the Queen’s University Belfast,
    the Harvard-Smithsonian Center for Astrophysics, the Las Cumbres Observatory Global
    Telescope Network Incorporated, the National Central University of Taiwan, the
    Space Telescope Science Institute, the National Aeronautics and Space Administration
    under Grant No. NNX08AR22G issued through the Planetary Science Division of the
    NASA Science Mission Directorate, the National Science Foundation Grant No. AST-1238877,
    the University of Maryland, Eotvos Lorand University (ELTE), the Los Alamos National
    Laboratory, and the Gordon and Betty Moore Foundation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Pan-STARRS1 调查（PS1）和 PS1 公开科学档案的建立得到了以下机构的支持：天文学研究所、夏威夷大学、Pan-STARRS 项目办公室、马克斯·普朗克学会及其参与机构、马克斯·普朗克天文研究所（海德堡）和马克斯·普朗克外星物理研究所（加尔兴）、约翰·霍普金斯大学、达勒姆大学、爱丁堡大学、贝尔法斯特女王大学、哈佛-史密森天体物理中心、拉斯·昆布雷斯天文台全球望远镜网络公司、台湾中央大学、太空望远镜科学研究所、美国国家航空航天局（NASA）科学任务局行星科学部门资助的
    NNX08AR22G 号项目、国家科学基金会 AST-1238877 号资助、马里兰大学、厄尔特大学（ELTE）、洛斯阿拉莫斯国家实验室以及戈登与贝蒂·摩尔基金会。
- en: Details of objective function
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标函数的详细信息
- en: We here describe the details of the objective functions proposed in section [3.2.2](#S3.SS2.SSS2
    "3.2.2 Objective functions ‣ 3.2 Training methods ‣ 3 Method ‣ Deep-learning Real/Bogus
    classification for the Tomo-e Gozen transient survey").
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在此我们描述了第[3.2.2](#S3.SS2.SSS2 "3.2.2 目标函数 ‣ 3.2 训练方法 ‣ 3 方法 ‣ Tomo-e Gozen 瞬态调查的深度学习真假分类")节中提出的目标函数的详细信息。
- en: Cross-entropy
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交叉熵
- en: ': The cross-entropy function is an objective function commonly used in training
    classifiers. It works to match the output of the neural network to the teacher
    label for each sample. The cross-entropy function is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ': 交叉熵函数是一种在训练分类器中常用的目标函数。它的作用是将神经网络的输出与每个样本的教师标签进行匹配。交叉熵函数如下：'
- en: '|  | $\displaystyle L_{\mathrm{ce}}\left(\left\{\mathbf{x}_{l},y\right\};\boldsymbol{\theta}\right)$
    |  | (3) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{\mathrm{ce}}\left(\left\{\mathbf{x}_{l},y\right\};\boldsymbol{\theta}\right)$
    |  | (3) |'
- en: '|  |  | $\displaystyle=\frac{1}{N_{l}}\sum_{i=1}^{N_{l}}$ | $\displaystyle\left\{-I\left(y_{i}=c^{+}\right)\log
    p\left(y=c^{+}\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\right.$ |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{N_{l}}\sum_{i=1}^{N_{l}}$ | $\displaystyle\left\{-I\left(y_{i}=c^{+}\right)\log
    p\left(y=c^{+}\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\right.$ |  |'
- en: '|  |  |  | $\displaystyle\left.\;-I\left(y_{i}=c^{-}\right)\log p\left(y=c^{-}\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\right\},$
    |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | $\displaystyle\left.\;-I\left(y_{i}=c^{-}\right)\log p\left(y=c^{-}\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\right\},$
    |  |'
- en: '|  | $\displaystyle p\left(y=c\mid\mathbf{x};\boldsymbol{\theta}\right)$ |  |
    (4) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p\left(y=c\mid\mathbf{x};\boldsymbol{\theta}\right)$ |  |
    (4) |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\frac{\exp\left(f\left(y=c\mid\mathbf{x};\boldsymbol{\theta}\right)\right)}{\exp\left(f\left(y=c^{+}\mid\mathbf{x};\boldsymbol{\theta}\right)\right)+\exp\left(f\left(y=c^{-}\mid\mathbf{x};\boldsymbol{\theta}\right)\right)}.$
    |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\frac{\exp\left(f\left(y=c\mid\mathbf{x};\boldsymbol{\theta}\right)\right)}{\exp\left(f\left(y=c^{+}\mid\mathbf{x};\boldsymbol{\theta}\right)\right)+\exp\left(f\left(y=c^{-}\mid\mathbf{x};\boldsymbol{\theta}\right)\right)}.$
    |  |'
- en: Here $N_{l}$ is the number of labeled samples, $I\left(\right)$ is the indicator
    function, $c^{+}$ and $c^{-}$ are the labels of positive and negative examples,
    respectively, and $f\left(y=c\mid\mathbf{x};\boldsymbol{\theta}\right)$ is the
    output value corresponding to the label $c$ ($\mathbf{x}$ is input to the neural
    network).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $N_{l}$ 是标记样本的数量，$I\left(\right)$ 是指示函数，$c^{+}$ 和 $c^{-}$ 分别是正例和负例的标签，而 $f\left(y=c\mid\mathbf{x};\boldsymbol{\theta}\right)$
    是对应于标签 $c$ 的输出值（$\mathbf{x}$ 是神经网络的输入）。
- en: exp-Cross-hinge loss (AUC maximization)
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: exp-Cross-hinge 损失（AUC 最大化）
- en: ': If the sample ratio is highly biased, the apparent performance could be improved
    by always predicting the dominant class regardless of the input data. For example,
    in transient surveys, Bogus objects are always dominant over Real objects. In
    such a case, a classifier that classifies all the input to be Bogus can achieve
    a high score. However, obviously such a classifier is not useful to detect real
    transients. Approaches to handle imbalanced dataset include down-sampling of the
    majority class (e.g., Hosenie et al. ([2019](#bib.bib12))) and giving extra weight
    to the minority class (e.g., van Roestel et al. ([2021](#bib.bib28))). In our
    proposed method, we handle imbalanced data by incorporating exp-Cross-hinge loss
    (Kurora et al. ([2020](#bib.bib17))) into the objective function, as described
    below.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果样本比例高度偏斜，表观性能可以通过始终预测主要类别来提高，而不考虑输入数据。例如，在瞬态调查中，虚假对象始终占据主导地位，真实对象则相对较少。在这种情况下，将所有输入都分类为虚假对象的分类器可以获得高分。然而，显然这样的分类器对于检测真实瞬态并没有用处。处理不平衡数据集的方法包括对多数类进行下采样（例如，Hosenie
    等（[2019](#bib.bib12)））和给予少数类额外权重（例如，van Roestel 等（[2021](#bib.bib28)））。在我们提出的方法中，我们通过将
    exp-Cross-hinge 损失（Kurora 等（[2020](#bib.bib17)））纳入目标函数来处理不平衡数据，如下所述。
- en: 'The exp-Cross-hinge loss is a loss function for pairs of positive and negative
    examples in the dataset. When the score of the negative sample becomes larger
    than the score of the positive sample, a loss corresponding to the difference
    occurs. In contrast, when the score of the positive example is larger than the
    score of the negative example, the hinge function prevents the loss of the set
    from falling below a certain level. In addition, expanding the difference between
    the negative and positive scores with the exponential function enables learning
    even when the difference is small. For all pairs of positive and negative samples,
    this loss function is minimized when the score of the positive sample is greater
    than the score of the negative sample including the margin. The definition is
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: exp-Cross-hinge 损失是用于数据集中正例和负例对的损失函数。当负样本的得分大于正样本的得分时，会发生与差异相对应的损失。相反，当正例的得分大于负例的得分时，hinge
    函数会防止损失下降到一定水平。此外，通过指数函数扩大负样本和正样本得分之间的差异，即使差异较小也能进行学习。对于所有正例和负例对，当正样本的得分大于负样本的得分（包括边际）时，这个损失函数被最小化。其定义如下：
- en: '|  | $\displaystyle L_{\mathrm{ech}}\left(\left\{\mathbf{x}^{+}\right\},\left\{\mathbf{x}^{-}\right\};\boldsymbol{\theta}\right)$
    |  | (5) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{\mathrm{ech}}\left(\left\{\mathbf{x}^{+}\right\},\left\{\mathbf{x}^{-}\right\};\boldsymbol{\theta}\right)$
    |  | (5) |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\frac{1}{N^{+}N^{-}}\sum_{i=1}^{N^{+}}\sum_{j=1}^{N^{-}}\exp\left\{\left[s\left(\mathbf{x}_{j}^{-};\boldsymbol{\theta}\right)-s\left(\mathbf{x}_{i}^{+};\boldsymbol{\theta}\right)+\xi\right]_{+}\right\}.$
    |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\frac{1}{N^{+}N^{-}}\sum_{i=1}^{N^{+}}\sum_{j=1}^{N^{-}}\exp\left\{\left[s\left(\mathbf{x}_{j}^{-};\boldsymbol{\theta}\right)-s\left(\mathbf{x}_{i}^{+};\boldsymbol{\theta}\right)+\xi\right]_{+}\right\}.$
    |  |'
- en: Here, $s\left(\right)$ is the score function parameterized by $\boldsymbol{\theta}$,
    $\mathbf{x}^{+}$ and $\mathbf{x}^{-}$ are the positive and negative samples, $N^{+}$
    and $N^{-}$ are the numbers of positive and negative samples, respectively, and
    $\xi$ is the margin between the positive and negative sample scores. In the equation,
    $\left[\bullet\right]_{+}$ is the hinge function, defined as
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$s\left(\right)$ 是由 $\boldsymbol{\theta}$ 参数化的评分函数，$\mathbf{x}^{+}$ 和 $\mathbf{x}^{-}$
    分别是正样本和负样本，$N^{+}$ 和 $N^{-}$ 分别是正样本和负样本的数量，$\xi$ 是正样本和负样本得分之间的间隔。在方程中，$\left[\bullet\right]_{+}$
    是 hinge 函数，定义为
- en: '|  | $\displaystyle\left[z\right]_{+}=\left\{\begin{array}[]{ll}z&amp;(z\geq
    0)\\ 0&amp;(z<0)\end{array}\right..$ |  | (8) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left[z\right]_{+}=\left\{\begin{array}[]{ll}z&(z\geq 0)\\
    0&(z<0)\end{array}\right..$ |  | (8) |'
- en: 'We define the score function using the outputs of the neural network as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过神经网络的输出定义评分函数如下：
- en: '|  | $\displaystyle s\left(\mathbf{x};\boldsymbol{\theta}\right)=f\left(y=c^{+}\mid\mathbf{x};\boldsymbol{\theta}\right)-f\left(y=c^{-}\mid\mathbf{x};\boldsymbol{\theta}\right).$
    |  | (9) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s\left(\mathbf{x};\boldsymbol{\theta}\right)=f\left(y=c^{+}\mid\mathbf{x};\boldsymbol{\theta}\right)-f\left(y=c^{-}\mid\mathbf{x};\boldsymbol{\theta}\right).$
    |  | (9) |'
- en: The AUC is a measure for the performance of a binary classifier when we want
    to maximize the true positive rate and minimize the false positive rate. AUC is
    generally defined as the area under the ROC curve. Alternatively, it can be defined
    as the ratio of pairs in which the score of the positive sample is greater than
    the score of the negative sample among all pairs of positive and negative samples
    in the dataset. Since AUC is a discontinuous function, it is difficult to maximize
    AUC directly. On the other hand, since the exp-Cross-hinge function is a relaxation
    of the AUC function to a continuous function, we expect to obtain an approximate
    solution for AUC maximization by minimizing the exp-Cross-hinge function.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: AUC 是一种衡量二分类器性能的指标，当我们希望最大化真正例率并最小化假正例率时使用。AUC 通常定义为 ROC 曲线下的面积。或者，它可以定义为在数据集中所有正负样本对中，正样本得分大于负样本得分的对的比例。由于
    AUC 是一个不连续的函数，直接最大化 AUC 是困难的。另一方面，由于 exp-Cross-hinge 函数是对 AUC 函数的连续函数松弛，我们期望通过最小化
    exp-Cross-hinge 函数来获得 AUC 最大化的近似解。
- en: Virtual Adversarial Training (VAT)
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 虚拟对抗训练（VAT）
- en: ': We perform the Virtual Adversarial Training (VAT) (Miyato et al. ([2016](#bib.bib20))).
    The VAT is a training method with the local distributional smoothness (LDS) as
    a regularization term. The LDS is a new notion of smoothness for the outputs of
    models. In VAT, special perturbations that maximize the changes of the outputs
    of the neural network are added to input images. The neural network is trained
    to minimize the change of the outputs, thus smoothing regularization. Therefore,
    it is expected to be robust to the influence of input perturbation. The objective
    function is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ': 我们执行虚拟对抗训练（VAT）（Miyato 等人（[2016](#bib.bib20)））。VAT 是一种将局部分布平滑性（LDS）作为正则化项的训练方法。LDS
    是模型输出的新平滑性概念。在 VAT 中，向输入图像添加最大化神经网络输出变化的特殊扰动。神经网络被训练以最小化输出的变化，从而实现平滑正则化。因此，预计它对输入扰动的影响具有鲁棒性。目标函数如下：'
- en: '|  | $\displaystyle L_{\mathrm{lds}}\left(\left\{\mathbf{x}_{l}\right\}\cup\left\{\mathbf{x}_{u}\right\};\boldsymbol{\theta}\right)$
    |  | (10) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{\mathrm{lds}}\left(\left\{\mathbf{x}_{l}\right\}\cup\left\{\mathbf{x}_{u}\right\};\boldsymbol{\theta}\right)$
    |  | (10) |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\frac{1}{N_{l}+N_{u}}\sum_{i=1}^{N_{l}+N_{u}}\mathrm{KL}\left[p\left(y\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\middle&#124;\middle&#124;p\left(y\mid\mathbf{x}_{i}+\mathbf{r}_{i};\boldsymbol{\theta}\right)\right],$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\frac{1}{N_{l}+N_{u}}\sum_{i=1}^{N_{l}+N_{u}}\mathrm{KL}\left[p\left(y\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\middle&#124;\middle&#124;p\left(y\mid\mathbf{x}_{i}+\mathbf{r}_{i};\boldsymbol{\theta}\right)\right],$
    |  |'
- en: where $N_{l}$ and $N_{u}$ are the numbers of labeled samples and unlabeled samples,
    respectively, $\mathrm{KL}\left[\bullet\middle|\middle|\bullet\right]$ is KL divergence
    which is a measure of the difference between two distributions, and $\mathbf{r}_{i}$
    is the virtual adversarial perturbation of the $i$th sample. Here, $\mathbf{r}_{i}$
    is a tiny perturbation that maximizes the change in the classifier output and
    it is defined as
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{l}$ 和 $N_{u}$ 分别是标记样本和未标记样本的数量，$\mathrm{KL}\left[\bullet\middle|\middle|\bullet\right]$
    是KL散度，用于衡量两个分布之间的差异，而 $\mathbf{r}_{i}$ 是第 $i$ 个样本的虚拟对抗扰动。这里，$\mathbf{r}_{i}$ 是一个微小的扰动，能够最大化分类器输出的变化，其定义为
- en: '|  | $\displaystyle\mathbf{r}_{i}=\mathop{\rm arg~{}max}\limits_{\mathbf{r},\;\mathrm{w.r.t.}\left\&#124;\mathbf{r}\right\&#124;^{2}<\epsilon}\mathrm{KL}\left[p\left(y_{i}\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\middle&#124;\middle&#124;p\left(y_{i}\mid\mathbf{x}_{i}+\mathbf{r};\boldsymbol{\theta}\right)\right],$
    |  | (11) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{r}_{i}=\mathop{\rm arg~{}max}\limits_{\mathbf{r},\;\mathrm{w.r.t.}\left\|\mathbf{r}\right\|^{2}<\epsilon}\mathrm{KL}\left[p\left(y_{i}\mid\mathbf{x}_{i};\boldsymbol{\theta}\right)\middle\|\middle\|p\left(y_{i}\mid\mathbf{x}_{i}+\mathbf{r};\boldsymbol{\theta}\right)\right],$
    |  | (11) |'
- en: where $\epsilon$ is the size of the perturbation. It is not practical to obtain
    $\mathbf{r}_{i}$ exactly because it is computationally expensive. Instead, we
    efficiently obtain the approximate virtual adversarial perturbation by using the
    method shown in algorithm 1 of Miyato et al. ([2016](#bib.bib20)). Because no
    label information is needed to compute $\mathbf{r}_{i}$, the VAT objective function
    can be used even for samples without labels.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是扰动的大小。由于计算成本高，获得 $\mathbf{r}_{i}$ 的确切值不切实际。相反，我们通过使用 Miyato 等人（[2016](#bib.bib20)）的算法1中所示的方法有效地获得了近似的虚拟对抗扰动。由于计算
    $\mathbf{r}_{i}$ 不需要标签信息，因此即使对于没有标签的样本也可以使用VAT目标函数。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alard & Lupton (1998) Alard, C., & Lupton, R. H. 1998, ApJ, 503, 325
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alard & Lupton（1998）Alard, C., & Lupton, R. H. 1998年，ApJ，第503卷，第325页
- en: Ayyar et al. (2022) Ayyar, V., Knop, Robert, J., Awbrey, A., Anderson, A., &
    Nugent, P. 2022, arXiv:2203.09908
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ayyar 等人（2022）Ayyar, V., Knop, Robert, J., Awbrey, A., Anderson, A., & Nugent,
    P. 2022年，arXiv:2203.09908
- en: Becker (2015) Becker, A. 2015, Astrophysics Source Code Library, ascl:1504.004
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Becker（2015）Becker, A. 2015年，天体物理学源代码库，ascl:1504.004
- en: Bellm et al. (2019) Bellm, E. C., et al. 2019, PASP, 131, 018002
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellm 等人（2019）Bellm, E. C., 等人。2019年，PASP，第131卷，第018002页
- en: Bloom et al. (2012) Bloom, J. S., et al. 2012, PASP, 124, 1175
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bloom 等人（2012）Bloom, J. S., 等人。2012年，PASP，第124卷，第1175页
- en: Brink et al. (2013) Brink, H., Richards, J. W., Poznanski, D., Bloom, J. S.,
    Rice, J., Negahban, S., & Wainwright, M. 2013, MNRAS, 435, 1047
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brink 等人（2013）Brink, H., Richards, J. W., Poznanski, D., Bloom, J. S., Rice,
    J., Negahban, S., & Wainwright, M. 2013年，MNRAS，第435卷，第1047页
- en: Duev et al. (2019) Duev, D. A., et al. 2019, MNRAS, 489, 3582
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duev 等人（2019）Duev, D. A., 等人。2019年，MNRAS，第489卷，第3582页
- en: Flewelling et al. (2020) Flewelling, H. A., et al. 2020, ApJS, 251, 7
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flewelling 等人（2020）Flewelling, H. A., 等人。2020年，ApJS，第251卷，第7页
- en: Gieseke et al. (2017) Gieseke, F., et al. 2017, MNRAS, 472, 3101
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gieseke 等人（2017）Gieseke, F., 等人。2017年，MNRAS，第472卷，第3101页
- en: 'He et al. (2016a) He, K., Zhang, X., Ren, S., & Sun, J. 2016a, in 2016 IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR) (Los Alamitos, CA, USA:
    IEEE Computer Society), 770'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016a）He, K., Zhang, X., Ren, S., & Sun, J. 2016a年，在2016 IEEE计算机视觉与模式识别会议（CVPR）（洛杉矶，加州，美国：IEEE计算机学会），第770页
- en: 'He et al. (2016b) He, K., Zhang, X., Ren, S., & Sun, J. 2016b, in Computer
    Vision – ECCV 2016, ed. B. Leibe, J. Matas, N. Sebe, & M. Welling (Cham: Springer
    International Publishing), 630'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016b）He, K., Zhang, X., Ren, S., & Sun, J. 2016b年，在《计算机视觉 – ECCV 2016》，编辑
    B. Leibe, J. Matas, N. Sebe, & M. Welling（香农：Springer International Publishing），第630页
- en: Hosenie et al. (2019) Hosenie, Z., Lyon, R. J., Stappers, B. W., & Mootoovaloo,
    A. 2019, MNRAS, 488, 4858
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosenie 等人（2019）Hosenie, Z., Lyon, R. J., Stappers, B. W., & Mootoovaloo, A.
    2019年，MNRAS，第488卷，第4858页
- en: "Hosenie et al. (2021) Hosenie, Z., et al. 2021, Experimental Astronomy, 51,\
    \ 319â\x80\x93344"
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosenie 等人（2021）Hosenie, Z., 等人。2021年，《实验天文学》，第51卷，第319–344页
- en: Ioffe & Szegedy (2015) Ioffe, S., & Szegedy, C. 2015, in Proc. of the 32nd Int.
    Conf. on Machine Learning - Volume 37, ICML’15 (JMLR.org), 448
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe & Szegedy（2015）Ioffe, S., & Szegedy, C. 2015年，在第32届国际机器学习会议论文集 - 第37卷，ICML’15（JMLR.org），第448页
- en: Ivezić et al. (2019) Ivezić, Ž., et al. 2019, ApJ, 873, 111
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivezić 等人（2019）Ivezić, Ž., 等人。2019年，ApJ，第873卷，第111页
- en: Killestein et al. (2021) Killestein, T. L., et al. 2021, MNRAS, 503, 4838
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Killestein 等人（2021）Killestein, T. L., 等人。2021年，MNRAS，第503卷，第4838页
- en: Kurora et al. (2020) Kurora, S., Hachiya, H., Shimada, U., & Ueda, N. 2020,
    IBISML
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurora 等人（2020）Kurora, S., Hachiya, H., Shimada, U., & Ueda, N. 2020年，IBISML
- en: Law et al. (2009) Law, N. M., et al. 2009, PASP, 121, 1395
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Law 等人（2009）Law, N. M., 等人。2009年，PASP，第121卷，第1395页
- en: Mahabal et al. (2019) Mahabal, A., et al. 2019, PASP, 131, 038002
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahabal 等人（2019）Mahabal, A., 等人。2019年，PASP，第131卷，第038002页
- en: Miyato et al. (2016) Miyato, T., ichi Maeda, S., Koyama, M., Nakae, K., & Ishii,
    S. 2016, arXiv:1507.00677
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyato 等 (2016) Miyato, T., ichi Maeda, S., Koyama, M., Nakae, K., & Ishii,
    S. 2016, arXiv:1507.00677
- en: Morii et al. (2016) Morii, M., et al. 2016, PASJ, 68, 104
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morii 等 (2016) Morii, M., 等 2016, PASJ, 68, 104
- en: Northcutt et al. (2021) Northcutt, C. G., Jiang, L., & Chuang, I. L. 2021, Journal
    of Artificial Intelligence Research (JAIR), 70, 1373
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Northcutt 等 (2021) Northcutt, C. G., Jiang, L., & Chuang, I. L. 2021, 《人工智能研究期刊》
    (JAIR), 70, 1373
- en: 'Roy et al. (2018) Roy, A. G., Navab, N., & Wachinger, C. 2018, in Medical Image
    Computing and Computer Assisted Intervention – MICCAI 2018, ed. A. F. Frangi,
    J. A. Schnabel, C. Davatzikos, C. Alberola-López, & G. Fichtinger (Cham: Springer
    International Publishing), 421'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roy 等 (2018) Roy, A. G., Navab, N., & Wachinger, C. 2018, 在《医学图像计算与计算机辅助干预
    – MICCAI 2018》中, 主编 A. F. Frangi, J. A. Schnabel, C. Davatzikos, C. Alberola-López,
    & G. Fichtinger (Cham: Springer International Publishing), 421'
- en: Sako et al. (2018) Sako, S., et al. 2018, in Proc. SPIE Conf. Ser., Vol. 10702,
    Ground-based and Airborne Instrumentation for Astronomy VII, ed. C. J. Evans,
    L. Simard, & H. Takami, 107020J
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sako 等 (2018) Sako, S., 等 2018, 在 Proc. SPIE Conf. Ser., Vol. 10702, 《地面与空中天文仪器
    VII》中, 主编 C. J. Evans, L. Simard, & H. Takami, 107020J
- en: Simonyan & Zisserman (2014) Simonyan, K., & Zisserman, A. 2014, arXiv:1409.1556
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan & Zisserman (2014) Simonyan, K., & Zisserman, A. 2014, arXiv:1409.1556
- en: Singh & Krishnan (2020) Singh, S., & Krishnan, S. 2020, in Proc. of the IEEE/CVF
    Conf. on Computer Vision and Pattern Recognition (CVPR), 11234
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh & Krishnan (2020) Singh, S., & Krishnan, S. 2020, 在 Proc. of the IEEE/CVF
    Conf. on Computer Vision and Pattern Recognition (CVPR), 11234
- en: Turpin et al. (2020) Turpin, D., et al. 2020, MNRAS, 497, 2641
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turpin 等 (2020) Turpin, D., 等 2020, MNRAS, 497, 2641
- en: van Roestel et al. (2021) van Roestel, J., et al. 2021, AJ, 161, 267
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Roestel 等 (2021) van Roestel, J., 等 2021, AJ, 161, 267
- en: Waters et al. (2020) Waters, C. Z., et al. 2020, ApJS, 251, 4
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Waters 等 (2020) Waters, C. Z., 等 2020, ApJS, 251, 4
- en: Wright et al. (2015) Wright, D. E., et al. 2015, MNRAS, 449, 451
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wright 等 (2015) Wright, D. E., 等 2015, MNRAS, 449, 451
