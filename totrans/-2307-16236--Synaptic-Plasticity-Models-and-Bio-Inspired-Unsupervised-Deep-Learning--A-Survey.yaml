- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:37:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2307.16236] Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep
    Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2307.16236] 突触可塑性模型与生物启发的无监督深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.16236](https://ar5iv.labs.arxiv.org/html/2307.16236)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.16236](https://ar5iv.labs.arxiv.org/html/2307.16236)
- en: 'Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 突触可塑性模型与生物启发的无监督深度学习：综述
- en: Gabriele Lagani [gabriele.lagani@isti.cnr.it](mailto:gabriele.lagani@isti.cnr.it)
    ,  Fabrizio Falchi [fabrizio.falchi@isti.cnr.it](mailto:fabrizio.falchi@isti.cnr.it)
    ,  Claudio Gennaro [claudio.gennaro@isti.cnr.it](mailto:claudio.gennaro@isti.cnr.it)
     and  Giuseppe Amato [giuseppe.amato@isti.cnr.it](mailto:giuseppe.amato@isti.cnr.it)
    ISTI-CNRPisaItaly56124(2018)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Gabriele Lagani [gabriele.lagani@isti.cnr.it](mailto:gabriele.lagani@isti.cnr.it),
    Fabrizio Falchi [fabrizio.falchi@isti.cnr.it](mailto:fabrizio.falchi@isti.cnr.it),
    Claudio Gennaro [claudio.gennaro@isti.cnr.it](mailto:claudio.gennaro@isti.cnr.it)
    和 Giuseppe Amato [giuseppe.amato@isti.cnr.it](mailto:giuseppe.amato@isti.cnr.it)
    ISTI-CNR Pisa Italy 56124 (2018)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Recently emerged technologies based on Deep Learning (DL) achieved outstanding
    results on a variety of tasks in the field of Artificial Intelligence (AI). However,
    these encounter several challenges related to robustness to adversarial inputs,
    ecological impact, and the necessity of huge amounts of training data. In response,
    researchers are focusing more and more interest on biologically grounded mechanisms,
    which are appealing due to the impressive capabilities exhibited by biological
    brains. This survey explores a range of these biologically inspired models of
    synaptic plasticity, their application in DL scenarios, and the connections with
    models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired
    Deep Learning (BIDL) represents an exciting research direction, aiming at advancing
    not only our current technologies but also our understanding of intelligence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近出现的基于深度学习（DL）技术在人工智能（AI）领域的各种任务中取得了卓越的成果。然而，这些技术面临着诸如对抗性输入的鲁棒性、生态影响以及大量训练数据的需求等若干挑战。作为回应，研究人员越来越关注基于生物机制的研究，这些机制因生物大脑展现的令人印象深刻的能力而具有吸引力。本综述探讨了这些生物启发的突触可塑性模型的范围，它们在深度学习场景中的应用，以及与尖峰神经网络（SNNs）中可塑性模型的联系。总体而言，生物启发的深度学习（BIDL）代表了一个令人兴奋的研究方向，旨在推动我们当前技术的发展以及对智能的理解。
- en: 'Bio-Inspired, Hebbian, Deep Learning, Neural Networks, Spiking^†^†copyright:
    acmcopyright^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†ccs: Computing methodologies Bio-inspired
    approaches^†^†ccs: Computing methodologies Bio-inspired approaches'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生物启发的，赫布学习的，深度学习的，神经网络的，尖峰^†^†版权：acm 版权^†^†期刊年份：2018^†^†doi：XXXXXXX.XXXXXXX^†^†ccs：计算方法论
    生物启发的方法^†^†ccs：计算方法论 生物启发的方法
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: In the past decade, Deep Learning (DL) technologies have attained performance
    levels equivalent to, or even surpassing, human capabilities across a multitude
    of Artificial Intelligence (AI) applications, such as computer vision (He et al.,
    [2016](#bib.bib89)), reinforcement learning (Silver et al., [2016](#bib.bib220)),
    or language processing (Devlin et al., [2019](#bib.bib58)). Although Deep Neural
    Network (DNN) models were originally inspired by biological mechanisms, current
    technologies have observed a significant departure from their biological counterparts.
    For example, the biological plausibility of the error backpropagation algorithm
    (backprop) – the workhorse of DL – is questioned by neuroscientists (O’Reilly
    and Munakata, [2000](#bib.bib186); Marblestone et al., [2016](#bib.bib164); Hassabis
    et al., [2017](#bib.bib87); Lake et al., [2017](#bib.bib142); Richards et al.,
    [2019](#bib.bib200)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度学习（DL）技术在许多人工智能（AI）应用中，如计算机视觉（He et al., [2016](#bib.bib89)）、强化学习（Silver
    et al., [2016](#bib.bib220)）或语言处理（Devlin et al., [2019](#bib.bib58)），达到了相当于甚至超越人类能力的性能水平。尽管深度神经网络（DNN）模型最初受到生物机制的启发，但当前技术已经与其生物对应物显著偏离。例如，神经科学家对错误反向传播算法（backprop）——深度学习的核心——的生物学合理性提出了质疑（O’Reilly
    and Munakata, [2000](#bib.bib186); Marblestone et al., [2016](#bib.bib164); Hassabis
    et al., [2017](#bib.bib87); Lake et al., [2017](#bib.bib142); Richards et al.,
    [2019](#bib.bib200)）。
- en: In this survey, we highlight the connections of biological plausibility with
    a number of other challenges that current DL solutions still need to overcome,
    such as the lack of robustness of traditional DNN architectures to adversarially
    perturbed inputs (Goodfellow et al., [2014](#bib.bib83)), the necessity of huge
    amounts of labeled data (Roh et al., [2019](#bib.bib203)), or the ecological impact
    of neural network training (Badar et al., [2021](#bib.bib13)). For example, in
    (Badar et al., [2021](#bib.bib13)) it is shown how more complex and large-scale
    models allow to improve benchmark results but at a higher energy cost, which is
    typically not taken into account when comparing against other models. On the other
    hand, biological intelligence can exhibit proficient and robust behavior in a
    variety of tasks (Mainen and Sejnowski, [1995](#bib.bib159); Gerstner et al.,
    [1996](#bib.bib79)), while generalizing from little experience (Lake and Piantadosi,
    [2020](#bib.bib141)), with an exceptionally low energy expenditure (Javed et al.,
    [2010](#bib.bib109)). Therefore, it appears that drawing inspiration from biology
    could once again provide valuable insights toward addressing the challenges presented
    above. Indeed, in recent years, significant research efforts have been devoted
    to the development of bio-inspired solutions for DL.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们强调了生物学上的合理性与当前深度学习解决方案仍需克服的其他挑战之间的联系，例如传统深度神经网络架构对对抗性扰动输入的**鲁棒性不足**（Goodfellow
    et al., [2014](#bib.bib83)），对大量标注数据的**需求**（Roh et al., [2019](#bib.bib203)），以及神经网络训练的**生态影响**（Badar
    et al., [2021](#bib.bib13)）。例如，在（Badar et al., [2021](#bib.bib13)）中，展示了更复杂和大规模的模型如何提高基准测试结果，但代价是更高的**能量消耗**，而这通常在与其他模型比较时未被考虑。另一方面，生物智能可以在各种任务中展现出**高效**和**鲁棒**的行为（Mainen
    and Sejnowski, [1995](#bib.bib159)；Gerstner et al., [1996](#bib.bib79)），同时能从少量经验中进行**泛化**（Lake
    and Piantadosi, [2020](#bib.bib141)），且**能量消耗异常低**（Javed et al., [2010](#bib.bib109)）。因此，似乎从生物学中汲取灵感可能再次为解决上述挑战提供宝贵的见解。事实上，近年来，**生物启发的深度学习**解决方案的开发已投入了大量研究努力。
- en: In order to achieve a better understanding of the principles and mechanisms
    behind biological intelligence, scientific investigation moves from two different
    perspectives. On one hand, neuroscientists uncover the low-level working principles
    of biological intelligent systems and try to relate them to high-level intelligent
    behavior in a bottom-up fashion. On the other hand, computer scientists start
    from high-level abstractions to model AI problems and then work out the structures
    and architectural details needed to solve such problems. Unfortunately, finding
    the connections between the high-level and the low-level aspects is often difficult.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解生物智能背后的原则和机制，科学研究从两个不同的视角出发。一方面，神经科学家揭示生物智能系统的低级工作原理，并试图以自下而上的方式将其与高级智能行为联系起来。另一方面，计算机科学家从高级抽象开始，建模AI问题，然后解决这些问题所需的结构和架构细节。不幸的是，找到高级与低级方面之间的联系往往是困难的。
- en: '![Refer to caption](img/1245373a81a17301737a5391b4184229.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1245373a81a17301737a5391b4184229.png)'
- en: Figure 1. A schematic view of the topics of Bio-Inspired Deep Learning (BIDL)
    addressed in this work. We provide a comprehensive discussion on biologically
    grounded synaptic plasticity models, starting from Hebbian learning models for
    a single neuron to more complex models for populations of multiple neurons, such
    as competitive learning, subspace learning, etc. These approaches are related
    to pattern discovery mechanisms such as clustering, Principal Component Analysis
    (PCA), manifold learning, etc., thus providing interesting connections between
    neuroscientific models and computer science/engineering aspects of AI. One of
    the goals of this survey is to highlight the relationships between these two fields,
    showing how complex intelligent behavior can emerge from biologically-inspired
    principles.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 本工作中探讨的生物启发深度学习（BIDL）主题的示意图。我们对生物学基础的突触可塑性模型进行了全面讨论，从单一神经元的Hebbian学习模型开始，到多神经元群体的更复杂模型，如竞争学习、子空间学习等。这些方法与模式发现机制如聚类、主成分分析（PCA）、流形学习等相关联，从而提供了神经科学模型与计算机科学/工程AI方面之间的有趣联系。本次调查的一个目标是突出这两个领域之间的关系，展示如何从生物启发的原则中产生复杂的智能行为。
- en: 'The goal of this survey is to provide a comprehensive review of Bio-Inspired
    Deep Learning (BIDL), highlighting the connections between neuroscience and computer
    science viewpoints. We aim to provide a complete picture of the variety of perspectives,
    methods, and solutions that come together in this field, ranging from synaptic
    plasticity models to biologically realistic Spiking Neural Network (SNN) models.
    Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Synaptic Plasticity Models and
    Bio-Inspired Unsupervised Deep Learning: A Survey") provides a schematic summary
    of the various sub-topics embraced in this document.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的目标是提供关于生物启发深度学习（BIDL）的全面回顾，突出神经科学与计算机科学观点之间的联系。我们旨在呈现该领域中各种观点、方法和解决方案的完整图景，从突触可塑性模型到生物学上真实的脉冲神经网络（SNN）模型。图
    [1](#S1.F1 "图 1 ‣ 1\. 介绍 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 提供了本文档涵盖的各种子主题的示意性总结。
- en: This survey is intended both as a first approach for readers that are new to
    this field as well as a compact reference for a more experienced audience. The
    contents of this document should be easily accessible to computer scientists,
    as no prerequisite neuroscientific knowledge is expected, but they could also
    represent an interesting read for neuroscientists that are curious about the engineering
    aspects behind AI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查既适合作为新读者对该领域的初步了解，也作为经验丰富的读者的简明参考。文档内容应易于计算机科学家访问，因为不需要先验的神经科学知识，但对于那些对人工智能背后的工程学方面感兴趣的神经科学家来说，也可能是一个有趣的阅读材料。
- en: 'This document is structured as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文档的结构如下：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [2](#S2 "2\. Related Surveys ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey") discusses related surveys in the BIDL field.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [2](#S2 "2\. 相关调查 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 讨论了BIDL领域的相关调查。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [3](#S3 "3\. Synaptic Plasticity Models and Hebbian Learning in a Single
    Neuron ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey") describes biological synaptic plasticity models based on the Hebbian
    principle for a single neuron.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [3](#S3 "3\. 单神经元中的突触可塑性模型和赫布学习 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 描述了基于赫布原理的单神经元生物突触可塑性模型。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [4](#S4 "4\. Plasticity Models for Unsupervised Pattern Discovery with
    Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep
    Learning: A Survey") illustrates the extensions of plasticity mechanisms to populations
    of multiple neurons. It will be interesting to observe how certain biological
    aspects turn out to be related to unsupervised pattern discovery methods, thus
    showing a surprising connection between computer science and neuroscience.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [4](#S4 "4\. 多神经元无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 说明了将可塑性机制扩展到多个神经元群体的情况。观察某些生物学方面如何与无监督模式发现方法相关，将是非常有趣的，这显示了计算机科学与神经科学之间的意外联系。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [5](#S5 "5\. Synaptic Plasticity Models in Deep Learning ‣ Synaptic
    Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey") presents
    some experimental results from the literature regarding the application of synaptic
    plasticity models to DL contexts and the integration of traditional backprop-based
    learning and bio-inspired synaptic plasticity.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [5](#S5 "5\. 深度学习中的突触可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 介绍了一些关于突触可塑性模型在深度学习（DL）背景下应用的实验结果，以及传统的反向传播学习与生物启发突触可塑性的整合。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [6](#S6 "6\. Spiking Neural Networks ‣ Synaptic Plasticity Models and
    Bio-Inspired Unsupervised Deep Learning: A Survey") introduces biologically detailed
    models of neural computation based on SNNs, and highlights their technological
    potential for energy-efficient neuromorphic computing.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [6](#S6 "6\. 脉冲神经网络 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 介绍了基于脉冲神经网络（SNN）的生物详细神经计算模型，并强调了它们在节能神经形态计算中的技术潜力。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finally, we present our conclusions in Section [7](#S7 "7\. Concluding Remarks
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"),
    outlining open challenges and possible future research directions.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们在部分 [7](#S7 "7\. 结论 ‣ 突触可塑性模型和生物启发的无监督深度学习：调查") 中呈现我们的结论，概述了开放挑战和可能的未来研究方向。
- en: Moreover, in our companion paper (Amato et al., [2023](#bib.bib10)), we discuss
    biological models of spike coding and computation in greater detail, and we highlight
    the challenges of training such models with traditional backprop-based optimization.
    Therefore, we discuss recently proposed training algorithms, which pose themselves
    as alternatives to backprop, both for spiking and traditional architectures. These
    novel perspectives are promising to enhance the learning capabilities of current
    DL systems through biological insights and inspiration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们的配套论文（Amato et al., [2023](#bib.bib10)）中，我们更详细地讨论了脉冲编码和计算的生物模型，并强调了使用传统反向传播优化训练这些模型的挑战。因此，我们讨论了最近提出的训练算法，这些算法被视为反向传播的替代方案，适用于脉冲和传统架构。这些新颖的观点有望通过生物学的洞察和灵感来增强当前深度学习系统的学习能力。
- en: 2\. Related Surveys
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 相关调查
- en: Several aspects of BIDL have been reviewed in past contributions, each focused
    on a specific area or collection of methods. Some of these contributions are rooted
    in the neuroscientific viewpoint, while others explore more practical perspectives
    on DL system engineering. Our contribution aims at achieving a comprehensive presentation
    of the various domains and their interplay involved in the BIDL field, showing
    the connections between the neuroscientific aspects and the engineering abstractions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的贡献中回顾了BIDL的若干方面，每个贡献集中于特定领域或方法集。这些贡献有的根植于神经科学视角，而有的则探讨了深度学习系统工程的更实际的视角。我们的贡献旨在全面展示BIDL领域涉及的各种领域及其相互作用，展示神经科学方面与工程抽象之间的联系。
- en: An interesting treatment on the relationships between neuroscience and AI can
    be found in (Marblestone et al., [2016](#bib.bib164)), where the authors outline
    successful contributions in various aspects of DL, and propose directions of investigation
    for the neuroscientific field based on the insights provided by the high-level
    computational abstractions engineered by computer scientists. Similarly, the authors
    also suggest possible directions of inspiration that might come from neuroscience,
    towards the development of novel DL solutions. A similar discussion is presented
    in (Hassabis et al., [2017](#bib.bib87)), where the authors review historical
    interactions between computer science and neuroscience, showing how these interactions
    lead to novel results in these fields, and highlight possible shared themes for
    future development. Conversely, in (Richards et al., [2019](#bib.bib200)) it is
    outlined how architectures, objective functions, and learning rules developed
    for artificial learning systems can inspire further developments in system neuroscience.
    Focusing on biological and psychological inspiration, another work (Lake et al.,
    [2017](#bib.bib142)) suggests specific areas of exploration towards building more
    human-like DL systems, ranging from causal reasoning to compositional learning
    and program induction, as well as learning-to-learn approaches.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经科学与人工智能之间关系的有趣讨论可以在（Marblestone et al., [2016](#bib.bib164)）中找到，作者概述了深度学习各方面的成功贡献，并基于计算机科学家设计的高级计算抽象提供的洞察，提出了神经科学领域的研究方向。类似地，作者也建议了可能来自神经科学的灵感方向，旨在开发新的深度学习解决方案。类似的讨论在（Hassabis
    et al., [2017](#bib.bib87)）中也有呈现，作者回顾了计算机科学与神经科学之间的历史互动，展示了这些互动如何导致这些领域的新成果，并强调了未来发展的可能共享主题。相反，（Richards
    et al., [2019](#bib.bib200)）中概述了为人工学习系统开发的架构、目标函数和学习规则如何激发系统神经科学的进一步发展。关注生物学和心理学灵感的另一项工作（Lake
    et al., [2017](#bib.bib142)）建议了朝向构建更具人类特征的深度学习系统的具体探索领域，从因果推理到组合学习和程序诱导，以及学习如何学习的方法。
- en: Concerning the biologically grounded modeling of neural systems, in their book,
    Gerstner and Kistler (Gerstner and Kistler, [2002](#bib.bib80)) provides a comprehensive
    presentation of SNN models, as well as Hebbian plasticity and Spike Time Dependent
    Plasticity (STDP) models. A variety of Hebbian plasticity models are also reviewed
    in (Gorchetchnikov et al., [2011](#bib.bib84); Vasilkoski et al., [2011](#bib.bib231)),
    while recent SNN developments and applications are surveyed in (Pfeiffer and Pfeil,
    [2018](#bib.bib194); Tavanaei et al., [2019](#bib.bib228); Nunes et al., [2022](#bib.bib177)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生物学基础的神经系统建模，Gerstner 和 Kistler 在他们的书中（Gerstner 和 Kistler, [2002](#bib.bib80)）提供了对尖峰神经网络（SNN）模型的全面介绍，以及Hebbian可塑性和尖峰时间依赖可塑性（STDP）模型。各种Hebbian可塑性模型也在（Gorchetchnikov
    等, [2011](#bib.bib84); Vasilkoski 等, [2011](#bib.bib231)）中进行了回顾，而最近的SNN发展和应用则在（Pfeiffer
    和 Pfeil, [2018](#bib.bib194); Tavanaei 等, [2019](#bib.bib228); Nunes 等, [2022](#bib.bib177)）中进行了综述。
- en: 'Compared to previous surveys, we provide significant contributions in the following
    directions:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于之前的调查，我们在以下几个方向上提供了重要的贡献：
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared to works more focused on high-level perspectives about the interplay
    between neuroscience and computer science as a source of biological inspiration
    (Marblestone et al., [2016](#bib.bib164); Hassabis et al., [2017](#bib.bib87);
    Richards et al., [2019](#bib.bib200); Lake et al., [2017](#bib.bib142)), our work
    delves deeper into specific aspects where the connections between neuroscientific
    models and emergent computational properties arise;
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比于更关注神经科学和计算机科学之间相互作用作为生物学灵感来源的高层视角的研究（Marblestone 等, [2016](#bib.bib164);
    Hassabis 等, [2017](#bib.bib87); Richards 等, [2019](#bib.bib200); Lake 等, [2017](#bib.bib142)），我们的工作更深入地探讨了神经科学模型与新兴计算属性之间出现的具体方面；
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive description of bio-inspired synaptic plasticity models,
    showing the connections with learning principles that lead to autonomous pattern
    discovery as a resulting behavior, while other contributions only focus on the
    biological models of synaptic plasticity (Gerstner and Kistler, [2002](#bib.bib80);
    Gorchetchnikov et al., [2011](#bib.bib84); Vasilkoski et al., [2011](#bib.bib231));
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对生物启发的突触可塑性模型进行了全面的描述，展示了与学习原理的联系，这些学习原理导致了作为结果行为的自主模式发现，而其他研究则仅关注突触可塑性的生物学模型（Gerstner
    和 Kistler, [2002](#bib.bib80); Gorchetchnikov 等, [2011](#bib.bib84); Vasilkoski
    等, [2011](#bib.bib231)）。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide parallel perspectives on bio-inspired methods for traditional networks
    and those based on spiking models, compared to works dedicated only to low-level
    spike-based methods (Pfeiffer and Pfeil, [2018](#bib.bib194); Tavanaei et al.,
    [2019](#bib.bib228); Nunes et al., [2022](#bib.bib177)).
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对比生物启发方法在传统网络和基于尖峰模型网络中的并行视角，相比于仅专注于低级尖峰方法的研究（Pfeiffer 和 Pfeil, [2018](#bib.bib194);
    Tavanaei 等, [2019](#bib.bib228); Nunes 等, [2022](#bib.bib177)）。
- en: 3\. Synaptic Plasticity Models and Hebbian Learning in a Single Neuron
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 单神经元中的突触可塑性模型和Hebbian学习
- en: Throughout life, our brain is continually subject to modifications in order
    to incorporate knowledge from the environment and adapt to new tasks. This adaptation
    process is referred to as plasticity. The most prominent form of plasticity occurs
    in synapses, in the form of strengthening of synaptic efficacy, a.k.a. Long Term
    Potentiation (LTP), or weakening, a.k.a. Long Term Depression (LTD) (Bear, [1996](#bib.bib19);
    Gerstner and Kistler, [2002](#bib.bib80)). Given the crucial role that synaptic
    plasticity plays in neural systems, we begin this review by discussing synaptic
    plasticity models, starting from the simplest formulation of Hebbian plasticity,
    and then moving to more complex models that also recently found applications in
    DNN training. One of the challenges of formulating biologically plausible models
    of synaptic plasticity is the necessity to define local learning rules, i.e. using
    only information that is locally available at the neuron site for computing the
    synaptic updates.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个人生中，我们的大脑不断经历修改，以纳入环境中的知识并适应新任务。这一适应过程称为可塑性。最显著的可塑性形式发生在突触中，包括突触效能的增强，即长期增强（LTP），或减弱，即长期抑制（LTD）（Bear,
    [1996](#bib.bib19); Gerstner 和 Kistler, [2002](#bib.bib80)）。鉴于突触可塑性在神经系统中所发挥的关键作用，我们在本综述的开头讨论了突触可塑性模型，从最简单的Hebbian可塑性模型开始，然后转向更复杂的模型，这些模型也最近在深度神经网络（DNN）训练中找到了应用。制定生物学上可信的突触可塑性模型的挑战之一是必须定义局部学习规则，即仅使用在神经元位置上局部可用的信息来计算突触更新。
- en: '![Refer to caption](img/66810b66e54476ceafc0e5ce390e880a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/66810b66e54476ceafc0e5ce390e880a.png)'
- en: Figure 2. Representation of a neuron model which takes a vector $\mathbf{x}$
    as input. Inputs are modulated by synaptic weights $\mathbf{w}$, and then summed
    together, before a nonlinearity $f(\cdot)$ is applied. The resulting output is
    $y=f(\sum_{i}w_{i}\,x_{i})$ (where subscript $i$ indexes a specific vector entry).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 神经元模型的表示，该模型将向量 $\mathbf{x}$ 作为输入。输入由突触权重 $\mathbf{w}$ 调节，然后相加，最后应用非线性函数
    $f(\cdot)$。得到的输出是 $y=f(\sum_{i}w_{i}\,x_{i})$（其中下标 $i$ 索引特定的向量条目）。
- en: 'Let us consider a neuron model whose synaptic weights are described by a vector
    $\mathbf{w}$. The neuron takes as input a vector $\mathbf{x}$, and produces an
    output $y(\mathbf{x},\mathbf{w})=f(\mathbf{x}^{T}\,\mathbf{w})$ (Fig. [2](#S3.F2
    "Figure 2 ‣ 3\. Synaptic Plasticity Models and Hebbian Learning in a Single Neuron
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")),
    where $f$ is the activation function (optionally, a bias term can be implicitly
    modeled as a synapse connected to a constant input). In the following, we use
    boldface fonts to denote vectors, and normal fonts to denote scalars. In neuroscientific
    terms, input values $\mathbf{x}$ are also termed pre-synaptic activations, while
    the output $y$ is termed post-synaptic activation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们考虑一个神经元模型，其突触权重由向量 $\mathbf{w}$ 描述。神经元输入一个向量 $\mathbf{x}$，并生成输出 $y(\mathbf{x},\mathbf{w})=f(\mathbf{x}^{T}\,\mathbf{w})$（图
    [2](#S3.F2 "Figure 2 ‣ 3\. Synaptic Plasticity Models and Hebbian Learning in
    a Single Neuron ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep
    Learning: A Survey")），其中 $f$ 是激活函数（可选择性地，偏置项可以隐式建模为连接到恒定输入的突触）。接下来，我们使用粗体字体来表示向量，正常字体来表示标量。在神经科学术语中，输入值
    $\mathbf{x}$ 也称为前突触激活，而输出 $y$ 称为后突触激活。'
- en: 'In order to model synaptic plasticity, neuroscientists propose the Hebbian
    principle (Haykin, [2009](#bib.bib88); Gerstner and Kistler, [2002](#bib.bib80)):
    ”fire together, wire together”. According to this principle, the synaptic coupling
    between two neurons is reinforced when the two neurons are simultaneously active
    (Haykin, [2009](#bib.bib88); Gerstner and Kistler, [2002](#bib.bib80)). Mathematically,
    this learning rule, in its simplest ”vanilla” formulation, can be expressed as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模突触可塑性，神经科学家提出了赫布原则（Haykin，[2009](#bib.bib88)；Gerstner 和 Kistler，[2002](#bib.bib80)）：“共同激活，共同连接”。根据这一原则，当两个神经元同时激活时，它们之间的突触耦合会得到强化（Haykin，[2009](#bib.bib88)；Gerstner
    和 Kistler，[2002](#bib.bib80)）。在数学上，这一学习规则在其最简单的“香草”形式中可以表示为：
- en: '| (1) |  | $\Delta w_{i}=\eta\,y\,x_{i}$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\Delta w_{i}=\eta\,y\,x_{i}$ |  |'
- en: 'where $\eta$ is the learning rate, and the subscript $i$ refers to the i-th
    input/synapse. The effect of this learning rule is essentially to consolidate
    correlated activations between neural inputs and outputs, by reinforcing the synaptic
    couplings, so that, if a similar input will be observed again in the future, a
    similar response will likely be elicited from the neuron. We can already outline
    a first connection between the neuroscientific Hebbian learning theory and data
    science, specifically Principal Component Analysis (PCA): if multiple inputs are
    presented to a neuron, assuming that the activation function is linear and that
    the inputs have zero means, it can be shown (Gerstner and Kistler, [2002](#bib.bib80);
    Oja, [1982](#bib.bib178)) that Eq. [1](#S3.E1 "In 3\. Synaptic Plasticity Models
    and Hebbian Learning in a Single Neuron ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey") induces the weight vector to align towards
    the principal component of the data distribution.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\eta$ 是学习率，下标 $i$ 指第 $i$ 个输入/突触。这个学习规则的效果本质上是通过强化突触耦合来巩固神经输入和输出之间的相关激活，因此，如果未来再次观察到类似的输入，神经元可能会产生类似的响应。我们可以初步勾画出神经科学中的赫布学习理论与数据科学之间的联系，特别是主成分分析（PCA）：如果多个输入呈现给神经元，假设激活函数是线性的，并且输入均值为零，可以证明（Gerstner
    和 Kistler，[2002](#bib.bib80)；Oja，[1982](#bib.bib178)），方程 [1](#S3.E1 "In 3\. Synaptic
    Plasticity Models and Hebbian Learning in a Single Neuron ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey") 会使权重向量与数据分布的主成分对齐。'
- en: '![Refer to caption](img/9ad777d95a430ca7037e51ccb7c1359e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/9ad777d95a430ca7037e51ccb7c1359e.png)'
- en: (a) Weight vector subject to an update. Points are inputs (organized in a cluster),
    and the green point is the input currently being processed. The blue arrow represents
    the direction of the update that will affect the weight vector $w$, while the
    red arrow is the actual update.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 权重向量在更新过程中。点表示输入（组织成一个簇），绿色点是当前正在处理的输入。蓝色箭头表示将影响权重向量 $w$ 的更新方向，而红色箭头是实际更新。
- en: '![Refer to caption](img/60088c6b5dec89581342e290ed94ed42.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/60088c6b5dec89581342e290ed94ed42.png)'
- en: (b) Final position of the weight vector after training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 训练后的权重向量最终位置。
- en: Figure 3. Effect of Hebbian updates on a weight vector.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 赫布更新对权重向量的影响。
- en: 'The problem with Eq. [1](#S3.E1 "In 3\. Synaptic Plasticity Models and Hebbian
    Learning in a Single Neuron ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised
    Deep Learning: A Survey") is that there are no mechanisms to prevent the weights
    from growing unbounded, thus leading to possible instability. This issue can be
    counteracted by adding a weight decay term $\gamma(\mathbf{w},\mathbf{x})$ to
    the learning rule:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 等式 [1](#S3.E1 "在 3\. 突触可塑性模型和单神经元的赫布学习 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述") 的问题在于没有机制来防止权重无限增长，从而可能导致不稳定性。这个问题可以通过在学习规则中加入权重衰减项
    $\gamma(\mathbf{w},\mathbf{x})$ 来解决：
- en: '| (2) |  | $\Delta w_{i}=\eta\,y\,x_{i}-\gamma(\mathbf{w},\mathbf{x})$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\Delta w_{i}=\eta\,y\,x_{i}-\gamma(\mathbf{w},\mathbf{x})$ |  |'
- en: 'In particular, with an appropriate choice for this term, i.e. $\gamma(\mathbf{w},\mathbf{x})=\eta\,y(\mathbf{w},\mathbf{x})\,w_{i}$,
    we obtain a learning rule that has been widely applied to the context of competitive
    learning (which will be discussed more in detail later) (Grossberg, [1976](#bib.bib85);
    Rumelhart and Zipser, [1985](#bib.bib208); Kohonen, [1982](#bib.bib121)):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，通过对该项选择合适的值，即 $\gamma(\mathbf{w},\mathbf{x})=\eta\,y(\mathbf{w},\mathbf{x})\,w_{i}$，我们得到一个已广泛应用于竞争学习的学习规则（稍后将更详细讨论）（Grossberg,
    [1976](#bib.bib85); Rumelhart 和 Zipser, [1985](#bib.bib208); Kohonen, [1982](#bib.bib121)）：
- en: '| (3) |  | $\Delta w_{i}=\eta\,y\,x_{i}-\eta\,y\,w_{i}=\eta\,y\,(x_{i}-w_{i})$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\Delta w_{i}=\eta\,y\,x_{i}-\eta\,y\,w_{i}=\eta\,y\,(x_{i}-w_{i})$
    |  |'
- en: 'This rule has a simple intuitive interpretation, depicted in Fig. [3](#S3.F3
    "Figure 3 ‣ 3\. Synaptic Plasticity Models and Hebbian Learning in a Single Neuron
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"):
    when a collection of inputs is presented to the neuron, the weight vector evolves
    towards the centroid of the cluster formed by the inputs. In essence, the neuron
    is storing, in its synapses, a prototypical representation of the patterns observed
    during training. When a similar pattern is presented again, the neuron will produce
    a stronger response, thus becoming a sort of pattern-matching unit.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则有一个简单的直观解释，如图 [3](#S3.F3 "图 3 ‣ 3\. 突触可塑性模型和单神经元的赫布学习 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")
    所示：当一组输入呈现给神经元时，权重向量会朝着由输入形成的簇的质心演变。本质上，神经元在其突触中存储了训练过程中观察到的模式的典型表示。当类似的模式再次出现时，神经元将产生更强的响应，从而成为一种模式匹配单元。
- en: 'The learning rule above is a special case of post-synaptic gating (Gerstner
    and Kistler, [2002](#bib.bib80)):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上述学习规则是突触后门控的特例（Gerstner 和 Kistler, [2002](#bib.bib80)）：
- en: '| (4) |  | $\Delta w_{i}=\eta\,y\,(x_{i}-\theta)$ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\Delta w_{i}=\eta\,y\,(x_{i}-\theta)$ |  |'
- en: 'where the update step $x_{i}-\theta$ is ”gated” by the post-synaptic activation
    $y$. The parameter $\theta$ is a threshold parameter that determines the behavior
    of the updates: if the pre-synaptic stimulus is stronger than the threshold, LTP
    will be induced, otherwise we have LTD. Conversely, it is possible to define a
    pre-synaptic gating rule (Gerstner and Kistler, [2002](#bib.bib80)), by inverting
    the role of $x_{i}$ and $y$:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中更新步骤 $x_{i}-\theta$ 被突触后激活 $y$ “门控”。参数 $\theta$ 是一个阈值参数，决定了更新的行为：如果前突触刺激强于阈值，将引发
    LTP，否则我们有 LTD。相反，也可以定义一个前突触门控规则（Gerstner 和 Kistler, [2002](#bib.bib80)），通过反转 $x_{i}$
    和 $y$ 的角色：
- en: '| (5) |  | $\Delta w_{i}=\eta\,x_{i}\,(y-\theta)$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\Delta w_{i}=\eta\,x_{i}\,(y-\theta)$ |  |'
- en: The threshold parameter can be fixed, or it can depend on the weights, on the
    current pre/post-synaptic activations, or even on the history of past activations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值参数可以是固定的，或者可以依赖于权重、当前的前/后突触激活，甚至是过去激活的历史。
- en: 'A mixed approach is taken in the covariance rule (Sejnowski and Tesauro, [1989](#bib.bib215)),
    where thresholds are imposed both on the pre-synaptic and post-synaptic signals:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差规则（Sejnowski 和 Tesauro, [1989](#bib.bib215)）采取了一种混合方法，对预突触和后突触信号施加了阈值：
- en: '| (6) |  | $\Delta w_{i}=\eta\,(y-\theta_{y})\,(x_{i}-\theta_{x})$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\Delta w_{i}=\eta\,(y-\theta_{y})\,(x_{i}-\theta_{x})$ |  |'
- en: Specifically, $\theta_{x}$ and $\theta_{y}$ are running averages of pre- and
    post-synaptic activities over time, which are biologically supported by the concept
    of synaptic traces (Izhikevich, [2007](#bib.bib108); Yagishita et al., [2014](#bib.bib243);
    Shindou et al., [2019](#bib.bib218); Gerstner et al., [2018](#bib.bib81)). The
    covariance rule adapts vanilla Hebbian learning to the case of data with non-zero
    mean, tracking statistics online in the same spirit as batch normalization (Ioffe
    and Szegedy, [2015](#bib.bib106)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，$\theta_{x}$ 和 $\theta_{y}$ 是预突触和后突触活动随时间变化的滑动平均值，这在生物学上由突触痕迹的概念（Izhikevich,
    [2007](#bib.bib108); Yagishita et al., [2014](#bib.bib243); Shindou et al., [2019](#bib.bib218);
    Gerstner et al., [2018](#bib.bib81)）支持。协方差规则将普通的Hebbian学习调整到具有非零均值的数据情况，在线跟踪统计数据，具有与批量归一化相同的精神（Ioffe
    和 Szegedy, [2015](#bib.bib106)）。
- en: <svg   height="95.47" overflow="visible" version="1.1" width="141.35"><g transform="translate(0,95.47)
    matrix(1 0 0 -1 0 0) translate(23.34,0) translate(0,2.21) matrix(1.0 0.0 0.0 1.0
    -23.34 -2.21)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(23.34,0) translate(0,2.21)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g transform="matrix(1.0 0.0
    0.0 1.0 58.56 37.04)" fill="#000000" stroke="#000000"><foreignobject width="6.5"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\theta$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 15.4 32.47)" fill="#000000" stroke="#000000"><foreignobject
    width="28.06" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LTD</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 74.85 51.06)" fill="#000000" stroke="#000000"><foreignobject
    width="26.91" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LTP</foreignobject></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 105.56 54.07)" fill="#000000" stroke="#000000"><foreignobject
    width="7.28" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -8.35 24.72)" fill="#000000" stroke="#000000"><foreignobject
    width="43.54" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\phi(y-\theta)$</foreignobject></g></g></g></svg>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="95.47" overflow="visible" version="1.1" width="141.35"><g transform="translate(0,95.47)
    matrix(1 0 0 -1 0 0) translate(23.34,0) translate(0,2.21) matrix(1.0 0.0 0.0 1.0
    -23.34 -2.21)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(23.34,0) translate(0,2.21)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g transform="matrix(1.0 0.0
    0.0 1.0 58.56 37.04)" fill="#000000" stroke="#000000"><foreignobject width="6.5"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\theta$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 15.4 32.47)" fill="#000000" stroke="#000000"><foreignobject
    width="28.06" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LTD</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 74.85 51.06)" fill="#000000" stroke="#000000"><foreignobject
    width="26.91" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LTP</foreignobject></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 105.56 54.07)" fill="#000000" stroke="#000000"><foreignobject
    width="7.28" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -8.35 24.72)" fill="#000000" stroke="#000000"><foreignobject
    width="43.54" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\phi(y-\theta)$</foreignobject></g></g></g></svg>
- en: Figure 4. Non-linearity in BCM rule.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. BCM 规则中的非线性。
- en: 'The Bienenstock-Cooper-Munro (BCM) rule (Bienenstock et al., [1982](#bib.bib29))
    introduces a nonlinearity $\phi$ in the learning process:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Bienenstock-Cooper-Munro (BCM) 规则（Bienenstock et al., [1982](#bib.bib29)）在学习过程中引入了非线性函数
    $\phi$：
- en: '| (7) |  | $\Delta w_{i}=\eta\,x_{i}\,\phi(y-\theta)$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\Delta w_{i}=\eta\,x_{i}\,\phi(y-\theta)$ |  |'
- en: 'Fig. [4](#S3.F4 "Figure 4 ‣ 3\. Synaptic Plasticity Models and Hebbian Learning
    in a Single Neuron ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised
    Deep Learning: A Survey") shows a typical shape of the nonlinearity. A threshold
    is still applied to the post-synaptic activity, so that LTP occurs when the neuron
    activity is above the threshold, or LTD takes place otherwise. However, when the
    activity becomes too large, too small, or in proximity to the threshold, plasticity
    is inhibited. The idea is that, when neural activity is too large (or too small),
    LTP leads to a further increase in the activity (conversely, LTD leads to a further
    decrease), but the nonlinearity provides a stabilizing effect.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S3.F4 "图 4 ‣ 3\. 突触可塑性模型和单神经元中的赫布学习 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述") 显示了非线性的典型形状。阈值仍然应用于突触后活动，因此当神经元活动高于阈值时发生
    LTP，否则发生 LTD。然而，当活动变得过大、过小或接近阈值时，可塑性被抑制。其想法是，当神经活动过大（或过小）时，LTP 导致活动进一步增加（反之，LTD
    导致活动进一步减少），但非线性提供了稳定化效果。
- en: Another approach for setting bounds on the synaptic weights could be to adopt
    soft thresholds on the weights, i.e. explicit terms in the update equations that
    automatically limit the weight updates when a given threshold is approached (Gerstner
    and Kistler, [2002](#bib.bib80)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种设置突触权重界限的方法是采用权重的软阈值，即在更新方程中添加显式项，当接近给定阈值时自动限制权重更新（Gerstner 和 Kistler， [2002](#bib.bib80)）。
- en: '| (8) |  | $\Delta w_{i}=\eta\,\Delta w_{i}^{(base)}\,(w_{i}-\theta_{LB})\,(\theta_{UB}-w_{i})$
    |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\Delta w_{i}=\eta\,\Delta w_{i}^{(base)}\,(w_{i}-\theta_{LB})\,(\theta_{UB}-w_{i})$
    |  |'
- en: 'where $\theta_{LB}$ and $\theta_{UB}$ act as soft lower and upper thresholds
    for the weights, with $\theta_{LB}<\theta_{UB}$, and $\Delta w_{i}^{(base)}$ is
    a given weight update before soft-thresholding. A similar concept also arises
    in bi-stable synapse models (Fusi et al., [2000](#bib.bib71); Gerstner and Kistler,
    [2002](#bib.bib80)). In these models, synaptic weights are allowed to settle down
    only in one of two possible stable states (for example, 0 and 1). The learning
    rule is divided into two parts:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{LB}$ 和 $\theta_{UB}$ 作为权重的软下限和上限阈值，其中 $\theta_{LB}<\theta_{UB}$，$\Delta
    w_{i}^{(base)}$ 是在软阈值化之前给定的权重更新值。类似的概念也出现在双稳态突触模型中（Fusi 等， [2000](#bib.bib71)；Gerstner
    和 Kistler， [2002](#bib.bib80)）。在这些模型中，突触权重仅能在两个可能的稳定状态中的一个中稳定（例如，0 和 1）。学习规则分为两个部分：
- en: '| (9) |  | $\Delta w_{i}=H+R$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\Delta w_{i}=H+R$ |  |'
- en: 'where $H$ is a generic Hebbian term driving plasticity, while $R$ is a refresh
    term for synaptic stabilization:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 是驱动可塑性的通用赫布项，而 $R$ 是用于突触稳定化的刷新项：
- en: '| (10) |  | $R=\gamma\,w_{i}\,(1-w_{i})\,(w_{i}-\theta)$ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $R=\gamma\,w_{i}\,(1-w_{i})\,(w_{i}-\theta)$ |  |'
- en: where $\gamma$ is a constant. This term drives weight values towards 1, when
    the current weight value is above a threshold $\theta$, or to 0 otherwise. Thanks
    to this rule, transitions of the weight value from 0 to 1 can occur only when
    the driving Hebbian term provides a stimulation strong enough to bring the weight
    value beyond the threshold, and vice-versa for transitions from 1 to 0.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ 是一个常数。该项将权重值驱动到 1，当当前权重值高于阈值 $\theta$ 时，否则驱动到 0。由于这个规则，权重值从 0 到
    1 的过渡只能在驱动赫布项提供足够强的刺激使权重值超越阈值时发生，反之亦然，从 1 到 0 的过渡。
- en: 'A different approach to the weight instability problem has been proposed in
    (Oja, [1982](#bib.bib178)): the idea is to renormalize the weight vector after
    each update, so that its length is always kept constant, although the direction
    changes over time, aligning towards the data principal component (under the same
    assumptions as in the vanilla case). It can be shown that, under small learning
    rates, the learning rule with the renormalization step can be approximated by
    a first-order Taylor expansion around $\eta=0$, leading to the weight update named
    Oja’s rule:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (Oja, [1982](#bib.bib178)) 提出了一个不同的权重不稳定性问题的解决方法：即在每次更新后对权重向量进行重新归一化，以保持其长度恒定，尽管方向随时间变化，趋向数据主成分（在与普通情况下相同的假设下）。可以证明，在小学习率下，带有重新归一化步骤的学习规则可以通过在
    $\eta=0$ 处的一级泰勒展开近似，得到名为 Oja 规则的权重更新：
- en: '| (11) |  | $\Delta w_{i}=\eta\,y\,(x_{i}-y\,w_{i})$ |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\Delta w_{i}=\eta\,y\,(x_{i}-y\,w_{i})$ |  |'
- en: Note that this update also falls in the category of Hebbian updates with weight
    decay, which in this case is $\gamma(\mathbf{w},\mathbf{x})=\eta\,y(\mathbf{w},\mathbf{x})^{2}\,w_{i}$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种更新也属于希博更新与权重衰减的范畴，在这种情况下，$\gamma(\mathbf{w},\mathbf{x})=\eta\,y(\mathbf{w},\mathbf{x})^{2}\,w_{i}$。
- en: 'It is worth noting that the aforementioned learning rules can be interpreted
    as instances of a more general local synaptic update equation which, for a generic
    synaptic connection $i$, can be expressed as (Gerstner and Kistler, [2002](#bib.bib80)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，上述学习规则可以解释为更一般的局部突触更新方程的实例，对于一般的突触连接$i$，可以表示为(Gerstner and Kistler，[2002](#bib.bib80))：
- en: '| (12) |  | $\Delta w_{i}=a_{0}+a_{1}x_{i}+a_{2}y+a_{3}x_{i}y+a_{4}x_{i}^{2}+a_{5}y^{2}+...$
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\Delta w_{i}=a_{0}+a_{1}x_{i}+a_{2}y+a_{3}x_{i}y+a_{4}x_{i}^{2}+a_{5}y^{2}+...$
    |  |'
- en: where the coefficients $a_{i}$ may depend on the weights.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，系数$a_{i}$可能依赖于权重。
- en: 'Differential Hebbian Learning (DHL) (Kosko, [1986](#bib.bib125)) represents
    a departure from traditional Hebbian models. Instead of considering simply the
    pre- and post-synaptic activities at a given instant, this model proposes to consider
    also the rate of change of these activities over time:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 微分希博学习（DHL）(Kosko，[1986](#bib.bib125))代表了传统希博模型的一种离开。该模型不仅考虑给定时刻的前后突触活动，还提议考虑这些活动随时间的变化率：
- en: '| (13) |  | $\Delta w_{i}=\eta\,\frac{dy}{dt}\,x_{i}$ |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\Delta w_{i}=\eta\,\frac{dy}{dt}\,x_{i}$ |  |'
- en: This learning rule has interesting properties related to data decorrelation
    (Choi, [1998](#bib.bib42)), temporal difference learning (Kolodziejski et al.,
    [2009a](#bib.bib124), [b](#bib.bib123)), and encoding of error signals in STDP
    neurons (Roberts, [1999](#bib.bib202)) providing a biologically grounded mechanism
    for Contrastive Hebbian Learning (CHL) (Movellan, [1991](#bib.bib175)) in networks
    of spiking neurons (Bengio et al., [2015](#bib.bib26)) (we will come back to these
    topics in the following Sections).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习规则具有与数据去相关（Choi，[1998](#bib.bib42)），时间差分学习（Kolodziejski et al.，[2009a](#bib.bib124)，[b](#bib.bib123)）以及STDP神经元中错误信号的编码相关的有趣性质（Roberts，[1999](#bib.bib202)），为脉冲神经元网络中的对比性希博学习（CHL）（Movellan，[1991](#bib.bib175)）提供了生物学基础机制（Bengio
    et al.，[2015](#bib.bib26)）（我们将在下面的部分再回顾这些主题）。
- en: The learning rules presented so far involve only a single neuron. In the next
    subsection, we will consider more complex learning scenarios involving multiple
    neurons.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所呈现的学习规则仅涉及单个神经元。在下一小节中，我们将考虑涉及多个神经元的更复杂的学习场景。
- en: 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 用于多神经元无监督模式发现的可塑性模型
- en: So far, we have considered plasticity models for single neurons. When we are
    dealing with a population with multiple neurons, naively applying a synaptic update
    rule such as those from the previous subsection is not guaranteed to be effective
    for a learning task. In fact, if multiple neurons follow the same learning dynamics,
    it is easy for them to converge to similar configurations. It is instead desirable
    to achieve some form of decorrelation of neural activity, i.e. making sure that
    different neurons learn to encode different pieces of information for given inputs
    (Földiak, [1990](#bib.bib70); Olshausen and Field, [1996a](#bib.bib183)), in order
    to maximize the representation power. This subsection explores the strategies
    to achieve such decorrelation in neural populations with local synaptic plasticity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了单神经元的可塑性模型。当我们处理具有多个神经元的群体时，简单地应用前一小节中的突触更新规则并不能保证对于学习任务是有效的。实际上，如果多个神经元遵循相同的学习动态，它们很容易会收敛到类似的配置。相反，希望能够实现一定形式的神经活动去相关，即确保不同神经元学会为给定的输入编码不同的信息片段(Földiak，[1990](#bib.bib70);
    Olshausen和Field，[1996a](#bib.bib183))，以最大化表征能力。本小节探讨了如何在具有局部突触可塑性的神经元群体中实现这种去相关的策略。
- en: 4.1\. Background on Neural Cells
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 背景关于神经元
- en: Let us introduce some biological background on the various types of neural cells
    and their functions, from which it will be possible to draw relationships between
    the computational learning mechanisms that we are going to discuss in the following,
    and the biological substrate that supports such mechanisms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一些关于各种类型神经元及其功能的生物背景知识，从中我们将能够建立计算学习机制与支持此类机制的生物基础之间的关系。
- en: 'Neural cells can be classified into two main groups: pyramidal cells and non-pyramidal
    cells (White and Keller, [1989](#bib.bib238)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元可以分为两大类：金字塔形细胞和非金字塔形细胞（White and Keller，[1989](#bib.bib238)）。
- en: 'Pyramidal cells represent the fundamental computing unit of biological neural
    networks. They typically have a pyramid-like shape, and they are endowed with
    two types of dendrites (i.e. input connections): apical dendrites, which extend
    from the tip of the pyramid, and several basal dendrites, originating from the
    base. Apical dendrites extend through cortical layers, while basal dendrites extend
    mainly toward neighboring cells in the same region. The axon of pyramidal cells
    (i.e. the output connection) originates at the base of the pyramid and it immediately
    branches into a projection axon and several axon collaterals. The projection axon
    extends towards deeper layers. Axon collaterals can be local, i.e. extending for
    a short distance towards neighboring neurons, or they can extend for longer distances
    either in the same layer or towards other layers.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 锥体细胞是生物神经网络的基本计算单元。它们通常呈现锥体状，并且具有两种类型的树突（即输入连接）：顶端树突，从锥体的尖端延伸，和多个基底树突，起源于基部。顶端树突穿过皮层层次，而基底树突主要向同一区域的邻近细胞延伸。锥体细胞的轴突（即输出连接）起源于锥体的基部，并立即分支成一个投射轴突和几个轴突侧支。投射轴突向更深层延伸。轴突侧支可以是局部的，即向邻近神经元延伸短距离，或者它们可以延伸更长的距离，无论是在同一层还是向其他层。
- en: Concerning the non-pyramidal cells, they can be further divided into a variety
    of categories (Stefanis, [2020](#bib.bib223)), but the common features are a central
    body with a smaller size compared to pyramidal cells, a number of dendrites originating
    from it, and an axon which tends to branch into multiple ramifications. Dendrites
    and connections are mainly local, hence these cells tend to connect to neighboring
    neurons, such as pyramidal cells, thus transmitting information about the activity
    in the neighborhood. By virtue of this role, these cells are often labeled as
    interneurons. Axons of non-pyramidal cells tend to form mainly inhibitory connections
    with other neural elements, which play an important role in inhibitory interaction
    and shunting inhibition (Kubota et al., [2016](#bib.bib129)) between neurons.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 关于非锥体细胞，它们可以进一步分为多种类别（Stefanis，[2020](#bib.bib223)），但共同特点是其中央体积较小，相较于锥体细胞，拥有从中央体起源的多个树突，以及一个趋向于分支成多个分支的轴突。树突和连接主要是局部的，因此这些细胞倾向于与邻近神经元（如锥体细胞）连接，从而传递关于邻域活动的信息。由于这一角色，这些细胞通常被称为中间神经元。非锥体细胞的轴突主要与其他神经元形成抑制性连接，在神经元之间的抑制性交互和分流抑制（Kubota
    等，[2016](#bib.bib129)）中发挥重要作用。
- en: In the following, we will highlight the role played by artificial neurons, which
    correspond to pyramidal cells, and the mechanisms of inhibitory lateral interaction,
    mediated by non-pyramidal cells, which is essential to achieve the necessary decorrelation
    in neural activity, thus showing an interesting mapping between biological circuits
    and artificial learning systems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将强调人工神经元（即锥体细胞）所扮演的角色，以及由非锥体细胞介导的抑制性侧向交互机制，这对于实现神经活动的必要去相关性至关重要，从而展示了生物电路与人工学习系统之间的有趣映射。
- en: 4.2\. Competitive Learning
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 竞争学习
- en: '![Refer to caption](img/bfbcbb63f348884dd9fb0d203dad7a48.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bfbcbb63f348884dd9fb0d203dad7a48.png)'
- en: (a) Update step
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 更新步骤
- en: '![Refer to caption](img/91259a7b814cd02797fbab037fa8e1f5.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/91259a7b814cd02797fbab037fa8e1f5.png)'
- en: (b) Final position after convergence
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 收敛后的最终位置
- en: Figure 5. Hebbian updates with Winner-Takes-All competition.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 赫布更新与赢家通吃竞争。
- en: 'When multiple neurons are involved in a complex network, competitive learning
    can be adopted to force different neurons to learn different patterns. A possible
    strategy is Winner-Takes-All (WTA) (Grossberg, [1976](#bib.bib85); Rumelhart and
    Zipser, [1985](#bib.bib208)): when an input is presented to a WTA layer, the neuron
    whose weight vector is closest to the current input (e.g. in terms of angular
    or euclidean distance) is elected as winner. Only the winner is allowed to perform
    a weight update, according to Eq. [3](#S3.E3 "In 3\. Synaptic Plasticity Models
    and Hebbian Learning in a Single Neuron ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey"), thus moving its weight vector closer to
    the current input (Fig. [5](#S4.F5 "Figure 5 ‣ 4.2\. Competitive Learning ‣ 4\.
    Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic
    Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")). If
    a similar input will be presented again in the future, the same neuron will be
    more likely to win again. Competitive interaction between neurons is biologically
    motivated by the lateral inhibition mechanisms (Gabbott and Somogyi, [1986](#bib.bib73))
    that we have previously highlighted. This strategy allows a group of neurons to
    align their weight vectors towards the centroids of distinct clusters formed by
    the data points (Fig. [5](#S4.F5 "Figure 5 ‣ 4.2\. Competitive Learning ‣ 4\.
    Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic
    Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")), which
    shows another connection between a neuroscience-inspired learning theory, and
    a data analytic operation, namely clustering.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '当多个神经元参与一个复杂的网络时，可以采用竞争学习来强制不同的神经元学习不同的模式。一种可能的策略是胜者通吃（WTA）（Grossberg，[1976](#bib.bib85)；Rumelhart
    和 Zipser，[1985](#bib.bib208)）：当输入呈现给 WTA 层时，权重向量最接近当前输入的神经元（例如，在角度或欧几里得距离方面）被选为赢家。只有赢家被允许进行权重更新，依据公式
    [3](#S3.E3 "In 3\. Synaptic Plasticity Models and Hebbian Learning in a Single
    Neuron ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey")，从而将其权重向量向当前输入靠拢（图 [5](#S4.F5 "Figure 5 ‣ 4.2\. Competitive Learning
    ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")）。如果未来将再次呈现类似的输入，相同的神经元更有可能再次获胜。神经元之间的竞争互动在生物上受到我们之前提到的侧抑制机制（Gabbott
    和 Somogyi，[1986](#bib.bib73)）的激励。这一策略使一组神经元能够将它们的权重向量对齐到数据点形成的不同簇的质心上（图 [5](#S4.F5
    "Figure 5 ‣ 4.2\. Competitive Learning ‣ 4\. Plasticity Models for Unsupervised
    Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey")），展示了神经科学启发的学习理论与数据分析操作（即聚类）之间的另一种联系。'
- en: 'The following equation gives a mathematical description of Hebbian WTA learning:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程给出了赫布式 WTA 学习的数学描述：
- en: '| (14) |  | $\Delta w_{i,j}=\eta\,r_{j}\,(x_{i}-w_{i,j})$ |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\Delta w_{i,j}=\eta\,r_{j}\,(x_{i}-w_{i,j})$ |  |'
- en: 'Here, subscripts $i$ and $j$ refer to the i-th input/synapse and j-th neuron
    in the layer, respectively, and $r_{j}$ is the neuron activation after a competitive
    nonlinearity: it is $1$ for the winning neuron and $0$ otherwise. A variant of
    WTA is k-WTA (Majani et al., [1989](#bib.bib162)), where top-k neurons closest
    to the input are the winners, which means that $r_{j}=1$ for these neurons and
    $0$ for all the others.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，下标 $i$ 和 $j$ 分别指代第 i 个输入/突触和第 j 个神经元，$r_{j}$ 是在竞争非线性之后的神经元激活值：对于获胜的神经元，其值为
    $1$，否则为 $0$。WTA 的一个变体是 k-WTA（Majani 等，[1989](#bib.bib162)），其中最接近输入的前 k 个神经元是赢家，这意味着这些神经元的
    $r_{j}=1$，而其他神经元的 $r_{j}=0$。
- en: 'In contrast with the sharp competition provided by WTA, soft forms of competition
    are also possible. In these cases, instead of having sharp winning neurons, corresponding
    to $r_{j}$ being $0$ or $1$, we can also attribute intermediate values to $r_{j}$
    for each neuron. For example, soft-WTA (Nowlan, [1990](#bib.bib176)) allows all
    the neurons to receive a score based on their activations so that neurons with
    higher activation will receive a higher score. In the original work, the score
    was computed simply as an $L_{1}$ normalization of the activations of the neurons.
    In (Lagani et al., [2021c](#bib.bib136); Moraitis et al., [2021](#bib.bib174)),
    other soft-WTA variants were introduced where the score was computed as the $L_{p}$
    normalization or as the softmax of neural activations, i.e.:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与 WTA 提供的锐利竞争相比，也可能出现软形式的竞争。在这些情况下，除了具有明确的获胜神经元，即 $r_{j}$ 为 $0$ 或 $1$，我们还可以为每个神经元分配中间值的
    $r_{j}$。例如，soft-WTA（Nowlan，[1990](#bib.bib176)）允许所有神经元根据其激活接收一个分数，因此激活较高的神经元会收到更高的分数。在原始工作中，分数简单地计算为神经元激活的
    $L_{1}$ 归一化。在（Lagani 等，[2021c](#bib.bib136)；Moraitis 等，[2021](#bib.bib174)），介绍了其他
    soft-WTA 变体，其中分数计算为 $L_{p}$ 归一化或作为神经激活的 softmax，即：
- en: '| (15) |  | $r_{j}=\frac{y_{j}^{p}}{\sum_{k}y_{k}^{p}}$ |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $r_{j}=\frac{y_{j}^{p}}{\sum_{k}y_{k}^{p}}$ |  |'
- en: '| (16) |  | $r_{j}=\frac{e^{y_{j}/T}}{\sum_{k}e^{y_{k}/T}}$ |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $r_{j}=\frac{e^{y_{j}/T}}{\sum_{k}e^{y_{k}/T}}$ |  |'
- en: 'respectively. In the latter equation, T is the temperature parameter of the
    softmax (Gao and Pavel, [2017](#bib.bib75)), which serves to cope with the variance
    of the activations. Note that $r$ can be viewed as a step modulation coefficient:
    the higher the neuron activity, the larger the update step will be.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 分别。在后一方程中，T 是 softmax 的温度参数（Gao 和 Pavel，[2017](#bib.bib75)），用于应对激活值的变化。请注意，$r$
    可以视为步长调制系数：神经元活动越高，更新步长越大。
- en: '![Refer to caption](img/25aea9abb078d01b3aabb686ee32f3ec.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/25aea9abb078d01b3aabb686ee32f3ec.png)'
- en: (a) 1-dimensional lattice.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 一维格子。
- en: '![Refer to caption](img/52a5942a2c65ca45beecfba1fa5cd7dc.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/52a5942a2c65ca45beecfba1fa5cd7dc.png)'
- en: (b) 2-dimensional lattice.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 二维格子。
- en: Figure 6. Self-Organizing Maps with neurons arranged in different topologies.
    Some of the lateral feedback connections are highlighted in green.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. 自组织映射的神经元在不同拓扑结构中的排列。一些横向反馈连接以绿色突出显示。
- en: '![Refer to caption](img/94fe6db829734ae6e8aa7afb922a4827.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/94fe6db829734ae6e8aa7afb922a4827.png)'
- en: (a) 2-dimensional lattice with radius highlighted in green and distance $d_{j,i}$
    between neuron $j$ and neuron $i$ highlighted in blue.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 二维格子，其中半径以绿色突出显示，神经元 $j$ 和神经元 $i$ 之间的距离 $d_{j,i}$ 以蓝色突出显示。
- en: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 517.62 122.76)" fill="#000000"
    stroke="#000000"><foreignobject width="15.23" height="13.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d_{j,i}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="33.96"
    height="14.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h(d_{j,i})$</foreignobject></g></g></g></svg>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 517.62 122.76)" fill="#000000"
    stroke="#000000"><foreignobject width="15.23" height="13.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d_{j,i}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="33.96"
    height="14.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h(d_{j,i})$</foreignobject></g></g></g></svg>
- en: (b) Gaussian neighborhood function.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 高斯邻域函数。
- en: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 517.62 122.76)" fill="#000000"
    stroke="#000000"><foreignobject width="15.23" height="13.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d_{j,i}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="33.96"
    height="14.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h(d_{j,i})$</foreignobject></g></g></g></svg>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 517.62 122.76)" fill="#000000"
    stroke="#000000"><foreignobject width="15.23" height="13.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d_{j,i}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="33.96"
    height="14.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h(d_{j,i})$</foreignobject></g></g></g></svg>
- en: (c) Mexican hat neighborhood function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 墨西哥帽邻域函数。
- en: Figure 7. Lateral interaction with some neighborhood function profiles.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. 具有某些邻域函数轮廓的侧向交互。
- en: 'In (Kohonen, [1982](#bib.bib121)), the work on competitive learning was further
    extended with the introduction of Self-Organizing Maps (SOMs). A SOM is a layer
    of neurons arranged in an n-dimensional lattice (typically 1-dimensional or 2-dimensional,
    the latter being more common). After the competitive phase, but before the weight
    update, training is extended with a new cooperative phase, in which lateral interaction
    takes place in the form of a lateral feedback signal that is provided by the winning
    neuron to its neighbors in the lattice topology (fig. [6](#S4.F6 "Figure 6 ‣ 4.2\.
    Competitive Learning ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery
    with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised
    Deep Learning: A Survey")). The strength of this signal decreases with the distance
    from the winning neuron. Specifically, denoting with $i(x)$ the winning neuron
    on input $x$, the strength of the signal delivered to any neuron $j$, whose distance
    from $i(x)$ in the lattice topology is $d_{j,i(x)}$, is determined by the neighborhood
    function $h(d_{j,i(x)})$. This function should be equal to $1$ when $d_{j,i(x)}$
    is $0$ and should decrease with the distance. For instance, a possible choice
    for the neighborhood function can be a Gaussian function or a mexican hat function
    centered in zero (Fig. [7](#S4.F7 "Figure 7 ‣ 4.2\. Competitive Learning ‣ 4\.
    Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic
    Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")) (Haykin,
    [2009](#bib.bib88); Kohonen, [1993](#bib.bib122)). Other possible choices for
    the neighborhood function and further theoretical details about the SOMs are discussed
    in (Lo et al., [1991](#bib.bib152), [1993](#bib.bib153); Erwin et al., [1991](#bib.bib65),
    [1992a](#bib.bib66), [1992b](#bib.bib67); Cottrell et al., [2018](#bib.bib52)).
    The neighborhood function is characterized by a radius (the standard deviation
    in the Gaussian case) which is typically initialized to be equal to the radius
    of the lattice and then shrank over time. Once the cooperative phase is completed,
    the weight update takes place by applying eq. [14](#S4.E14 "In 4.2\. Competitive
    Learning ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple
    Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey"), in which $r$ is set to $h(d_{j,i(x)})$. Note that the WTA approach
    can be seen as a particular case of SOM in which the neighborhood function has
    a radius of zero.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '在（Kohonen, [1982](#bib.bib121)）中，竞争学习的工作通过引入自组织映射（SOMs）得到了进一步扩展。SOM 是一个神经元层，排列在一个
    n 维晶格中（通常是 1 维或 2 维，后者更为常见）。在竞争阶段之后，但在权重更新之前，训练会扩展到一个新的合作阶段，其中通过一个由获胜神经元向其在晶格拓扑中的邻居提供的横向反馈信号来进行横向互动（图
    [6](#S4.F6 "Figure 6 ‣ 4.2\. Competitive Learning ‣ 4\. Plasticity Models for
    Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models
    and Bio-Inspired Unsupervised Deep Learning: A Survey")）。该信号的强度随离获胜神经元的距离增加而减小。具体地，设
    $i(x)$ 为输入 $x$ 上的获胜神经元，则传递给任何神经元 $j$ 的信号强度，其在晶格拓扑中与 $i(x)$ 的距离为 $d_{j,i(x)}$，由邻域函数
    $h(d_{j,i(x)})$ 决定。该函数应在 $d_{j,i(x)}$ 为 $0$ 时等于 $1$，并随距离增加而减小。例如，邻域函数的一个可能选择是高斯函数或以零为中心的墨西哥帽函数（图
    [7](#S4.F7 "Figure 7 ‣ 4.2\. Competitive Learning ‣ 4\. Plasticity Models for
    Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models
    and Bio-Inspired Unsupervised Deep Learning: A Survey")）（Haykin, [2009](#bib.bib88);
    Kohonen, [1993](#bib.bib122)）。关于邻域函数的其他可能选择以及有关 SOM 的进一步理论细节，可以参考（Lo et al., [1991](#bib.bib152),
    [1993](#bib.bib153); Erwin et al., [1991](#bib.bib65), [1992a](#bib.bib66), [1992b](#bib.bib67);
    Cottrell et al., [2018](#bib.bib52)）。邻域函数的特点是一个半径（在高斯情况下是标准差），通常初始化为与晶格半径相等，然后随时间缩小。合作阶段完成后，通过应用方程
    [14](#S4.E14 "In 4.2\. Competitive Learning ‣ 4\. Plasticity Models for Unsupervised
    Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey") 进行权重更新，其中 $r$ 设置为 $h(d_{j,i(x)})$。请注意，WTA
    方法可以看作是 SOM 的一个特殊情况，其中邻域函数的半径为零。'
- en: 4.3\. Subspace Learning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 子空间学习
- en: According to the definition given above, WTA enforces a kind of quantized information
    encoding in neural network layers. Only one or a few neurons activate to encode
    the presence of a given pattern in the input. On the other hand, neural networks
    trained with backpropagation exhibit a distributed representation (Agrawal et al.,
    [2014](#bib.bib3)), where multiple neurons activate combinatorially to encode
    different properties of the input, resulting in an improved coding power. Similar
    distributed coding schemes have also been observed in biological neuron populations
    (Averbeck et al., [2006](#bib.bib12); Wohrer et al., [2013](#bib.bib239)). The
    importance of distributed representations was also highlighted in (Földiak, [1989](#bib.bib69);
    Olshausen and Field, [1996a](#bib.bib183)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述定义，WTA 强制在神经网络层中进行一种量化的信息编码。只有一个或少数神经元激活以编码输入中某个给定模式的存在。另一方面，使用反向传播训练的神经网络呈现出分布式表示（Agrawal
    等人，[2014](#bib.bib3)），其中多个神经元以组合方式激活以编码输入的不同属性，从而提高编码能力。类似的分布式编码方案也观察到在生物神经元群体中（Averbeck
    等人，[2006](#bib.bib12)；Wohrer 等人，[2013](#bib.bib239)）。分布式表示的重要性也在（Földiak，[1989](#bib.bib69)；Olshausen
    和 Field，[1996a](#bib.bib183)）中得到强调。
- en: A more distributed coding scheme could be obtained by representing data as linear
    combinations of some orthonormal basis of feature vectors. Data projection over
    an orthogonal basis of weight vectors can be easily implemented as a neural mapping.
    In order to capture as much information as possible from the data, this basis
    should span the principal subspace, i.e. the subspace capturing most of the data
    variance, for which the data principal components form an orthogonal basis (Haykin,
    [2009](#bib.bib88)).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将数据表示为某些正交基上的线性组合来获得更分布式的编码方案。通过将数据投影到权重向量的正交基上可以轻松实现作为神经映射的神经网络。为了尽可能从数据中捕获信息，此基应跨越主子空间，即捕获大多数数据方差的子空间，其数据主成分形成正交基（Haykin，[2009](#bib.bib88)）。
- en: A PCA-based neural network, PCANet, was proposed in (Chan et al., [2015](#bib.bib39)).
    Network filters were obtained by running PCA offline on the training dataset and
    using the extracted principal components as weights. Multiple processing layers
    were obtained by stacking PCA filters obtained from feature representations of
    the dataset extracted from previous layers, in a bottom-up fashion. The approach
    achieved 78% accuracy on the CIFAR-10 (Krizhevsky and Hinton, [2009](#bib.bib127))
    dataset with a relatively shallow network. Unfortunately, Offline PCA computation
    is quite expensive, and it becomes prohibitive when applied to larger inputs or
    deeper networks. However, there exists an extension to Oja’s rule that allows
    to perform PCA also in an online fashion, which is more appealing both in terms
    of efficiency and biological plausibility.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于主成分分析（PCA）的神经网络，即 PCANet，是由（Chan 等人，[2015](#bib.bib39)）提出的。网络滤波器通过在训练数据集上离线运行
    PCA 并使用提取的主成分作为权重来获得。通过以自下而上的方式在前一层提取的特征表示中叠加来自 PCA 滤波器得到的 PCA 滤波器，可以得到多个处理层。该方法在相对较浅的网络上实现了对
    CIFAR-10（Krizhevsky 和 Hinton，[2009](#bib.bib127)）数据集的 78% 的准确率。不幸的是，离线 PCA 运算非常昂贵，当应用于更大的输入或更深层的网络时，计算成本变得非常高。然而，存在一种扩展到
    Oja 规则的方法，可以以在线方式执行 PCA，此方法在效率和生物合理性方面更具吸引力。
- en: 'We have already observed how Oja’s rule provides a stable Hebbian mechanism
    for extracting the first principal component. Further extensions of such mechanism
    for multiple neurons exist, which allow the extraction of successive directions
    spanning the principal subspace (Sanger, [1989](#bib.bib209); Becker and Plumbley,
    [1996](#bib.bib22)). In order to perform Hebbian PCA, a set of weight vectors
    has to be determined, for the various neurons, that minimize the representation
    error, defined as:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经观察到 Oja 规则如何为提取第一个主成分提供了一种稳定的 Hebbian 机制。还存在针对多个神经元的此类机制的进一步扩展，可以提取构成主子空间的连续方向（Sanger，[1989](#bib.bib209)；Becker
    和 Plumbley，[1996](#bib.bib22)）。为了执行 Hebbian PCA，必须确定一组权重向量，用于各个神经元，以最小化表示误差，即定义为：
- en: '| (17) |  | $\mathcal{L}_{R}(\mathbf{w_{i}})=E[(\mathbf{x}-\sum_{j=1}^{i}y_{j}\,\mathbf{w_{j}})^{2}]$
    |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\mathcal{L}_{R}(\mathbf{w_{i}})=E[(\mathbf{x}-\sum_{j=1}^{i}y_{j}\,\mathbf{w_{j}})^{2}]$
    |  |'
- en: where the subscript $i$ refers to the $i^{th}$ neuron in a given layer and $E[\cdot]$
    is the mean value operator. It can be pointed out that, in the case of linear
    neurons and zero-centered data, this reduces to the classical PCA objective of
    maximizing the output variance, with the weight vectors subject to orthonormality
    constraints (Sanger, [1989](#bib.bib209); Becker and Plumbley, [1996](#bib.bib22);
    Karhunen and Joutsensalo, [1995](#bib.bib113)). From now on, let us assume that
    the input data are centered around zero. If this is not true, we just need to
    subtract the average from the inputs beforehand.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中下标$i$表示给定层中的第$i^{th}$个神经元，$E[\cdot]$是均值运算符。可以指出的是，在线性神经元和零中心数据的情况下，这就简化为最大化输出方差的经典PCA目标，权重向量受到正交约束（Sanger，[1989](#bib.bib209)；Becker和Plumbley，[1996](#bib.bib22)；Karhunen和Joutsensalo，[1995](#bib.bib113)）。从现在开始，我们假设输入数据以零为中心。如果不是这样，我们只需事先从输入中减去平均值。
- en: 'It can be shown that Sanger’s rule minimizes the objective in Eq. [17](#S4.E17
    "In 4.3\. Subspace Learning ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery
    with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised
    Deep Learning: A Survey") (Sanger, [1989](#bib.bib209)):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，Sanger的规则最小化了方程[17](#S4.E17 "在4.3\. 子空间学习 ‣ 4\. 无监督模式发现的可塑性模型 ‣ 突触可塑性模型与生物启发的无监督深度学习：综述")中的目标（Sanger，[1989](#bib.bib209)）：
- en: '| (18) |  | $\Delta\mathbf{w_{i}}=\eta y_{i}(\mathbf{x}-\sum_{j=1}^{i}y_{j}\mathbf{w_{j}})$
    |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\Delta\mathbf{w_{i}}=\eta y_{i}(\mathbf{x}-\sum_{j=1}^{i}y_{j}\mathbf{w_{j}})$
    |  |'
- en: 'The intuition behind this learning rule is the following: 1) for the first
    neuron, it simply corresponds to Oja’s rule, thus extracting the first principal
    component; 2) for a generic successive neuron $i$, the learning rule subtracts
    from the input the partial representation $\sum_{j=1}^{i-1}y_{j}\mathbf{w_{j}}$
    reconstructed from the previous neurons, thus canceling the subspace spanned by
    the first $i-1$ principal components; 3) neuron $i$ then applies Oja’s rule on
    the residual part of the input, which leads to the extraction of the $i$-th principal
    component. In the case of nonlinear neurons, a solution to the problem can still
    be found (Karhunen and Joutsensalo, [1995](#bib.bib113)). Calling $f()$ the neuron
    activation function, under mild conditions that include the monotonic increase
    of $f$, the representation error'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习规则的直觉是：1）对于第一个神经元，它简单地对应于Oja的规则，从而提取第一个主成分；2）对于一个通用的后续神经元$i$，学习规则从输入中减去由前面的神经元重建的部分表示$\sum_{j=1}^{i-1}y_{j}\mathbf{w_{j}}$，从而取消由前$i-1$个主成分生成的子空间；3）神经元$i$然后对输入的残差部分应用Oja的规则，这导致提取第$i$-th主成分。在非线性神经元的情况下，仍然可以找到问题的解决方案（Karhunen和Joutsensalo，[1995](#bib.bib113)）。称$f()$为神经元激活函数，在包括$f$单调增加的温和条件下，表示误差
- en: '| (19) |  | $\mathcal{L}_{R}(w_{i})=E[(\mathbf{x}-\sum_{j=1}^{i}f(y_{j})\,\mathbf{w_{j}})^{2}]$
    |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\mathcal{L}_{R}(w_{i})=E[(\mathbf{x}-\sum_{j=1}^{i}f(y_{j})\,\mathbf{w_{j}})^{2}]$
    |  |'
- en: 'can be minimized with the following nonlinear version of the previous learning
    rule for nonlinear Hebbian PCA:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下非线性版本的先前学习规则来最小化非线性Hebbian PCA：
- en: '| (20) |  | $\Delta\mathbf{w_{i}}=\eta f(y_{i})(\mathbf{x}-\sum_{j=1}^{i}f(y_{j})\mathbf{w_{j}})$
    |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\Delta\mathbf{w_{i}}=\eta f(y_{i})(\mathbf{x}-\sum_{j=1}^{i}f(y_{j})\mathbf{w_{j}})$
    |  |'
- en: 'Other variants of Hebbian PCA learning rules exist. The subspace learning rule,
    also due to Oja (Oja, [1989](#bib.bib179), [1992](#bib.bib180)), differs from
    Sanger’s rule in that each neuron subtracts from the input the same reconstruction
    vector: $\sum_{j=1}^{N}y_{j}w_{j}$ (mind the summation index running from 1 to
    N, where N is the number of neurons in the layer). The resulting learning rule
    is:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他变体的Hebbian PCA学习规则。子空间学习规则，也是Oja提出的（Oja，[1989](#bib.bib179)，[1992](#bib.bib180)），与Sanger的规则不同的是，每个神经元从输入中减去相同的重建向量：$\sum_{j=1}^{N}y_{j}w_{j}$（注意求和索引从1到N，其中N是层中的神经元数量）。得到的学习规则是：
- en: '| (21) |  | $\Delta w_{i}=\eta y_{i}(x-\sum_{j=1}^{N}y_{j}w_{j})$ |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\Delta w_{i}=\eta y_{i}(x-\sum_{j=1}^{N}y_{j}w_{j})$ |  |'
- en: With this variation in the learning scheme, the network is capable of extracting
    the principal subspace, i.e. the same space spanned by the PCA directions, but
    expressed in terms of some other orthonormal basis (which is going to be a rotated
    version of the PCA basis).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种学习方案的变体，网络能够提取主要子空间，即由PCA方向生成的相同空间，但以其他正交基的形式表示（这将是PCA基的旋转版本）。
- en: Strictly speaking, Sanger’s rule (as well as Oja’s subspace rule) is not biologically
    plausible, because the weight update of a synapse is computed using information
    about weights on other synapses and outputs from other neurons (i.e. the terms
    in the sum). Nonetheless, network models can be designed that are functionally
    equivalent to Sanger’s (or Oja’s), but they are also consistent with biological
    plausibility requirements, using only local information in the updates. One of
    these was proposed in (Földiak, [1989](#bib.bib69)), and similarly also in (Plumbley,
    [1993](#bib.bib196)), which consisted of a linear single-layer network with both
    feedforward and lateral connections. Feedforward connections were trained with
    Oja’s rule, while lateral connections were trained using an anti-Hebbian rule
    (i.e. with a minus sign in front), an update scheme known as Hebbian/anti-Hebbian
    (HaH). Thanks to the lateral interaction, neurons were able to decorrelate their
    activity, projecting the data onto the principal subspace, while also normalizing
    the variance of neural activations. In data analysis, the operation performed
    by this network configuration corresponds to the whitening transformation (Krizhevsky
    and Hinton, [2009](#bib.bib127)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，Sanger 规则（以及 Oja 的子空间规则）在生物学上并不完全可信，因为突触的权重更新是基于其他突触的权重和其他神经元的输出（即求和中的项）。尽管如此，可以设计出在功能上等同于
    Sanger（或 Oja）的网络模型，并且这些模型也符合生物学可信性的要求，只使用局部信息进行更新。其中之一在（Földiak, [1989](#bib.bib69)）中提出，类似地在（Plumbley,
    [1993](#bib.bib196)）中也有提出，该模型由一个具有前馈和侧向连接的线性单层网络组成。前馈连接使用 Oja 的规则进行训练，而侧向连接使用反Hebbian
    规则（即前面带有负号）进行训练，这种更新方案被称为 Hebbian/anti-Hebbian（HaH）。由于侧向相互作用，神经元能够解相关它们的活动，将数据投影到主要子空间，同时规范化神经激活的方差。在数据分析中，这种网络配置执行的操作对应于白化变换（Krizhevsky
    和 Hinton, [2009](#bib.bib127)）。
- en: A similar approach was taken in (Rubner and Tavan, [1989](#bib.bib207)), where
    purely Hebbian update was used for the feedforward connections and anti-Hebbian
    for the lateral ones, with explicit normalization performed after each update.
    Lateral connectivity was hierarchical, i.e. neuron i received lateral connections
    from neurons 1…i-1\. This organization makes the network equivalent to Sanger’s
    model, hence being capable of extracting principal components from data. The model
    presented in (Kung and Diamantaras, [1990](#bib.bib130)) also used hierarchical
    lateral connectivity, with Hebbian/anti-Hebbian updates. Normalization was not
    performed explicitly, but it was achieved by means of Oja-like weight decay terms.
    Again, the resulting model was able to perform PCA. Conversely, such an update
    scheme applied to networks with symmetric lateral connectivity, instead of hierarchical,
    as in (Leen, [1991](#bib.bib146)), provides a network that extracts the principal
    subspace. An interesting perspective on HaH is presented in (Seung and Zung, [2017](#bib.bib216)),
    which views Hebbian and anti-Hebbian learning parts as competing players in a
    game theoretic setting. Introducing nonlinearities in the learning process, with
    local learning rules in HaH networks, further enables the extraction of directions
    that maximize some generalized nonlinear moments of the data distribution (Karhunen
    and Joutsensalo, [1995](#bib.bib113)). Plasticity models such as BCM (Bienenstock
    et al., [1982](#bib.bib29)) or successive variants (Intrator and Cooper, [1992](#bib.bib105);
    Law and Cooper, [1994](#bib.bib143); Brito and Gerstner, [2016](#bib.bib32)),
    have been shown to be effective to model receptive field formation in these scenarios.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: （Rubner 和 Tavan, [1989](#bib.bib207)）中采取了类似的方法，其中前馈连接使用纯 Hebbian 更新，侧向连接使用 anti-Hebbian
    更新，并在每次更新后进行显式规范化。侧向连接是分层的，即神经元 i 从神经元 1…i-1 接收侧向连接。这种组织使得网络等同于 Sanger 的模型，因此能够从数据中提取主要成分。
    (Kung 和 Diamantaras, [1990](#bib.bib130)) 中提出的模型也使用了分层侧向连接，并且进行了 Hebbian/anti-Hebbian
    更新。虽然没有显式进行规范化，但通过类似 Oja 的权重衰减项实现了规范化。结果模型能够执行 PCA。相反，将这种更新方案应用于具有对称侧向连接的网络（如（Leen,
    [1991](#bib.bib146)）中）而非分层的，则提供了一个提取主要子空间的网络。关于 HaH 的一个有趣视角在（Seung 和 Zung, [2017](#bib.bib216)）中提出，该视角将
    Hebbian 和 anti-Hebbian 学习部分视为博弈论中的竞争者。在 HaH 网络中引入非线性，利用局部学习规则，进一步能够提取最大化数据分布某些广义非线性矩的方向（Karhunen
    和 Joutsensalo, [1995](#bib.bib113)）。如 BCM（Bienenstock 等人, [1982](#bib.bib29)）或后续变体（Intrator
    和 Cooper, [1992](#bib.bib105)；Law 和 Cooper, [1994](#bib.bib143)；Brito 和 Gerstner,
    [2016](#bib.bib32)），已经被证明在这些情况下有效地模拟了感受野的形成。
- en: A review of the above-mentioned approaches is provided in (Becker and Plumbley,
    [1996](#bib.bib22)). Although these methods provide biologically grounded mechanisms
    for subspace learning, their disadvantage is that, due to the recurrent nature
    of lateral connections, simulating these networks requires unfolding the recurrent
    dynamics in time. This requires a significant overhead compared to purely feedforward
    models such as Sanger’s or Oja’s. Nonetheless, we argue that the relationships
    between feedforward subspace learning models and HaH configurations have an important
    consequence, i.e. that the former are preferable for software simulation, while
    the correspondence with HaH models also provides biological support. It is also
    possible to build neural networks capable of extracting minor components from
    data, i.e. eigenvectors associated with the smallest eigenvalues of the data covariance
    matrix. This is useful, for example, when we need to recover a signal buried in
    white noise. Minor component extraction can be achieved by reversing the sign
    of Oja’s update rule, thus making it anti-Hebbian (Oja, [1992](#bib.bib180); Luo
    and Unbehauen, [1997](#bib.bib155)). HaH networks have also been derived from
    learning objectives related to Classical Multi-Dimensional Scaling (CMDS - a.k.a.
    strain loss, or similarity matching) (Pehlevan et al., [2015](#bib.bib193); Pehlevan
    and Chklovskii, [2015a](#bib.bib191), [b](#bib.bib192)), which are also shown
    to be related to subspace learning and principal component extraction. These approaches
    are also connected to manifold learning; therefore, they will be discussed in
    more detail in the next subsection.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对上述方法的综述可以参考（Becker 和 Plumbley，[1996](#bib.bib22)）。尽管这些方法提供了生物学基础的子空间学习机制，但它们的缺点是，由于横向连接的递归性质，模拟这些网络需要展开递归动态。这与纯粹的前馈模型如
    Sanger 的或 Oja 的相比，要求更多的开销。然而，我们认为前馈子空间学习模型与 HaH 配置之间的关系有一个重要的结论，即前者更适合软件模拟，而与
    HaH 模型的对应关系也提供了生物学支持。还可以构建能够从数据中提取小成分的神经网络，即与数据协方差矩阵的最小特征值相关的特征向量。例如，当我们需要恢复被白噪声掩盖的信号时，这一点非常有用。通过反转
    Oja 更新规则的符号，可以实现小成分提取，从而使其成为反 Hebbian 的（Oja，[1992](#bib.bib180)；Luo 和 Unbehauen，[1997](#bib.bib155)）。HaH
    网络也源于与经典多维尺度缩放（CMDS - 又名应变损失或相似性匹配）（Pehlevan 等，[2015](#bib.bib193)；Pehlevan 和
    Chklovskii，[2015a](#bib.bib191)，[b](#bib.bib192)）相关的学习目标，这些目标也与子空间学习和主成分提取有关。这些方法也与流形学习有关，因此将在下一小节中详细讨论。
- en: 4.4\. Manifold Learning Models
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 流形学习模型
- en: Manifold learning models aim at mapping samples into lower dimensional spaces
    by constraining the output to preserve certain properties about the geometric
    structure of the data, beyond the simple linear relationships captured by methods
    such as PCA. Popular manifold learning methods include Isomap embeddings (Tenenbaum
    et al., [2000](#bib.bib229)), Locally Linear Embeddings (LLE) (Roweis and Saul,
    [2000](#bib.bib204)), or Laplacian eigenmaps (Belkin and Niyogi, [2003](#bib.bib23)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学习模型旨在通过约束输出以保持数据几何结构的某些属性，将样本映射到低维空间，这超出了如 PCA 这类方法所捕捉的简单线性关系。流行的流形学习方法包括
    Isomap 嵌入（Tenenbaum 等，[2000](#bib.bib229)）、局部线性嵌入（LLE）（Roweis 和 Saul，[2000](#bib.bib204)）或拉普拉斯特征映射（Belkin
    和 Niyogi，[2003](#bib.bib23)）。
- en: 'An interesting manifold learning approach is represented by Classical Multi-Dimensional
    Scaling (CMDS) (Cox and Cox, [2008](#bib.bib53)). The reason is that recent work
    has derived HaH neural networks capable to optimize the CDMS objective (Pehlevan
    et al., [2015](#bib.bib193); Pehlevan and Chklovskii, [2015a](#bib.bib191), [b](#bib.bib192)).
    The particular form of the objective, known as similarity matching, or strain
    loss, is the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有趣的流形学习方法是经典的多维尺度缩放（CMDS）（Cox 和 Cox，[2008](#bib.bib53)）。原因在于最近的研究推导出了能够优化
    CDMS 目标的 HaH 神经网络（Pehlevan 等，[2015](#bib.bib193)；Pehlevan 和 Chklovskii，[2015a](#bib.bib191)，[b](#bib.bib192)）。这种目标的特定形式，被称为相似性匹配或应变损失，具体如下：
- en: '| (22) |  | $Y^{*}=\underset{Y}{arg\ min}\ \&#124;X^{T}X-Y^{T}Y\&#124;^{2}_{F}$
    |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $Y^{*}=\underset{Y}{arg\ min}\ \&#124;X^{T}X-Y^{T}Y\&#124;^{2}_{F}$
    |  |'
- en: 'Where $X$ is a matrix obtained by concatenating a set of input vectors and
    similarly $Y$ is the matrix of the output vectors, while $\|\cdot\|_{F}$ is the
    Frobenius norm. For linear mapping, this objective is equivalent to standard subspace
    learning (Pehlevan et al., [2015](#bib.bib193)). Let’s give an intuitive interpretation
    of what this loss represents: $X^{T}X$ is a matrix whose elements are the dot
    products of pairs of input vectors, hence they represent the similarity of input
    vectors with other input vectors, and the same holds for $Y^{T}Y$. Therefore,
    that difference represents how much the similarity metric gets distorted when
    moving from the input space to the output space, and this is what should be minimized.
    The authors show that the problem can be solved by applying the following biologically-grounded
    neural dynamics and learning rules:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X$ 是通过连接一组输入向量得到的矩阵，$Y$ 是输出向量的矩阵，而 $\|\cdot\|_{F}$ 是 Frobenius 范数。对于线性映射，这一目标等同于标准的子空间学习（Pehlevan
    等，[2015](#bib.bib193)）。我们给出一个直观的解释：$X^{T}X$ 是一个矩阵，其元素是输入向量对的点积，因此它们表示了输入向量与其他输入向量的相似度，$Y^{T}Y$
    同样适用。因此，这种差异表示了在从输入空间到输出空间的过程中，相似度度量的扭曲程度，而这正是需要最小化的。作者们展示了通过应用以下基于生物学的神经动力学和学习规则可以解决这个问题：
- en: '| (23) |  | <math   alttext="\begin{split}&amp;y=W\,x-M\,y\\ &amp;\Delta W_{i,j}=\frac{y_{i}\,(x_{j}-W_{i,j}\,y_{i})}{D_{i}}\\'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '| (23) |  | <math alttext="\begin{split}&amp;y=W\,x-M\,y\\ &amp;\Delta W_{i,j}=\frac{y_{i}\,(x_{j}-W_{i,j}\,y_{i})}{D_{i}}\\'
- en: '&amp;\Delta M_{i,j\neq i}=\frac{y_{i}\,(y_{j}-M_{i,j}\,y_{i})}{D_{i}},\ M_{i,i}=0\\'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\Delta M_{i,j\neq i}=\frac{y_{i}\,(y_{j}-M_{i,j}\,y_{i})}{D_{i}},\ M_{i,i}=0\\'
- en: '&amp;\Delta D_{i}=y_{i}^{2}\end{split}" display="block"><semantics ><mtable
    columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd  columnalign="left"
    ><mrow ><mi  >y</mi><mo >=</mo><mrow ><mrow ><mi  >W</mi><mo lspace="0.170em"
    rspace="0em"  >​</mo><mi >x</mi></mrow><mo >−</mo><mrow ><mi >M</mi><mo lspace="0.170em"
    rspace="0em"  >​</mo><mi >y</mi></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >W</mi><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow></msub></mrow><mo
    >=</mo><mfrac ><mrow ><msub ><mi >y</mi><mi >i</mi></msub><mo lspace="0.170em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><msub ><mi >x</mi><mi
    >j</mi></msub><mo >−</mo><mrow ><msub ><mi >W</mi><mrow ><mi >i</mi><mo >,</mo><mi
    >j</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >y</mi><mi
    >i</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><msub ><mi
    >D</mi><mi >i</mi></msub></mfrac></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >M</mi><mrow ><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow><mo
    >≠</mo><mi >i</mi></mrow></msub></mrow><mo >=</mo><mfrac ><mrow  ><msub ><mi >y</mi><mi
    >i</mi></msub><mo lspace="0.170em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msub ><mi >y</mi><mi >j</mi></msub><mo >−</mo><mrow ><msub ><mi >M</mi><mrow
    ><mi >i</mi><mo >,</mo><mi >j</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >y</mi><mi >i</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><msub
    ><mi >D</mi><mi >i</mi></msub></mfrac></mrow><mo rspace="0.667em" >,</mo><mrow
    ><msub ><mi  >M</mi><mrow ><mi >i</mi><mo >,</mo><mi >i</mi></mrow></msub><mo
    >=</mo><mn >0</mn></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mrow
    ><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >D</mi><mi >i</mi></msub></mrow><mo >=</mo><msubsup ><mi >y</mi><mi >i</mi><mn
    >2</mn></msubsup></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply  ><ci
    >𝑦</ci><apply ><apply  ><ci >𝑊</ci><ci >𝑥</ci></apply><apply ><ci  >𝑀</ci><ci
    >𝑦</ci><ci >Δ</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑊</ci><list
    ><ci >𝑖</ci><ci >𝑗</ci></list></apply></apply></apply></apply><apply ><apply ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑗</ci></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑊</ci><list ><ci >𝑖</ci><ci
    >𝑗</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑖</ci></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐷</ci><ci >𝑖</ci></apply></apply><ci >Δ</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑀</ci><apply ><list ><ci >𝑖</ci><ci >𝑗</ci></list><ci >𝑖</ci></apply></apply></apply></apply><apply
    ><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑗</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑀</ci><list ><ci >𝑖</ci><ci >𝑗</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑖</ci></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐷</ci><ci >𝑖</ci></apply></apply></apply></apply><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑀</ci><list ><ci >𝑖</ci><ci >𝑖</ci></list></apply><apply
    ><cn type="integer" >0</cn><ci >Δ</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐷</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><ci >𝑖</ci></apply><cn
    type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&y=W\,x-M\,y\\ &\Delta W_{i,j}=\frac{y_{i}\,(x_{j}-W_{i,j}\,y_{i})}{D_{i}}\\
    &\Delta M_{i,j\neq i}=\frac{y_{i}\,(y_{j}-M_{i,j}\,y_{i})}{D_{i}},\ M_{i,i}=0\\
    &\Delta D_{i}=y_{i}^{2}\end{split}</annotation></semantics></math> |  |'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\Delta D_{i}=y_{i}^{2}\end{split}" display="block"><semantics ><mtable
    columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd  columnalign="left"
    ><mrow ><mi  >y</mi><mo >=</mo><mrow ><mrow ><mi  >W</mi><mo lspace="0.170em"
    rspace="0em"  >​</mo><mi >x</mi></mrow><mo >−</mo><mrow ><mi >M</mi><mo lspace="0.170em"
    rspace="0em"  >​</mo><mi >y</mi></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >W</mi><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow></msub></mrow><mo
    >=</mo><mfrac ><mrow ><msub ><mi >y</mi><mi >i</mi></msub><mo lspace="0.170em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><msub ><mi >x</mi><mi
    >j</mi></msub><mo >−</mo><mrow ><msub ><mi >W</mi><mrow ><mi >i</mi><mo >,</mo><mi
    >j</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >y</mi><mi
    >i</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><msub ><mi
    >D</mi><mi >i</mi></msub></mfrac></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >M</mi><mrow ><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow><mo
    >≠</mo><mi >i</mi></mrow></msub></mrow><mo >=</mo><mfrac ><mrow  ><msub ><mi >y</mi><mi
    >i</mi></msub><mo lspace="0.170em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msub ><mi >y</mi><mi >j</mi></msub><mo >−</mo><mrow ><msub ><mi >M</mi><mrow
    ><mi >i</mi><mo >,</mo><mi >j</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >y</mi><mi >i</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><msub
    ><mi >D</mi><mi >i</mi></msub></mfrac></mrow><mo rspace="0.667em" >,</mo><mrow
    ><msub ><mi  >M</mi><mrow ><mi >i</mi><mo >,</mo><mi >i</mi></mrow></msub><mo
    >=</mo><mn >0</mn></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mrow
    ><mrow ><mi mathvariant="normal" >Δ</mi><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >D</mi><mi >i</mi></msub></mrow><mo >=</mo><msubsup ><mi >y</mi><mi >i</mi><mn
    >2</mn></msubsup></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply  ><ci
    >𝑦</ci><apply ><apply  ><ci >𝑊</ci><ci >𝑥</ci></apply><apply ><ci  >𝑀</ci><ci
    >𝑦</ci><ci >Δ</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑊</ci><list
    ><ci >𝑖</ci><ci >𝑗</ci></list></apply></apply></apply></apply><apply ><apply ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑗</ci></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑊</ci><list ><ci >𝑖</ci><ci
    >𝑗</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑖</ci></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐷</ci><ci >𝑖</ci></apply></apply><ci >Δ</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑀</ci><apply ><list ><ci >𝑖</ci><ci >𝑗</ci></list><ci >𝑖</ci></apply></apply></apply></apply><apply
    ><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑗</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑀</ci><list ><ci >𝑖</ci><ci >𝑗</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑖</ci></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐷</ci><ci >𝑖</ci></apply></apply></apply></apply><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑀</ci><list ><ci >𝑖</ci><ci >𝑖</ci></list></apply><apply
    ><cn type="integer" >0</cn><ci >Δ</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐷</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><ci >𝑖</ci></apply><cn
    type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&y=W\,x-M\,y\\ &\Delta W_{i,j}=\frac{y_{i}\,(x_{j}-W_{i,j}\,y_{i})}{D_{i}}\\
    &\Delta M_{i,j\neq i}=\frac{y_{i}\,(y_{j}-M_{i,j}\,y_{i})}{D_{i}},\ M_{i,i}=0\\
    &\Delta D_{i}=y_{i}^{2}\end{split}</annotation></semantics></math> |  |'
- en: where matrices $W$ and $M$ represent respectively the weights associated with
    the feed-forward and lateral interactions, while $D$ is a vector containing the
    cumulative squared activations of the neurons, which act in the equations as a
    dynamic learning rate.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵 $W$ 和 $M$ 分别表示与前馈和侧向交互相关的权重，而 $D$ 是一个包含神经元累计平方激活的向量，这些激活在方程中作为动态学习率作用。
- en: As in the case of CMDS, manifold learning objectives appear to be effective
    principles for deriving data transformations that can be suitably mapped to neural
    layers, with potential applications for neuromorphic computation, thus representing
    an interesting open research area.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CMDS 的情况一样，流形学习目标似乎是推导数据变换的有效原则，这些变换可以适当地映射到神经层，具有神经形态计算的潜在应用，因此代表了一个有趣的开放研究领域。
- en: 4.5\. Sparse Coding (SC)
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 稀疏编码 (SC)
- en: Another interesting feature observed in biological networks is sparsity in the
    neural activations, i.e. only a small percentage of neurons (around 1%) activate
    simultaneously to encode a given stimulus (Lennie, [2003](#bib.bib147)). This
    property might derive simply from metabolic/energetic constraints, but it is also
    possible that sparsity plays a relevant role to support effective information
    encoding strategies (Földiak, [1990](#bib.bib70); Olshausen and Field, [1996a](#bib.bib183)).
    Indeed, similar coding strategies are also observed in networks trained with backprop
    (Agrawal et al., [2014](#bib.bib3)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物网络中观察到的另一个有趣特征是神经激活的稀疏性，即只有小部分神经元（约 1%）同时激活以编码给定的刺激（Lennie，[2003](#bib.bib147)）。这一特性可能只是来源于代谢/能量约束，但稀疏性也可能在支持有效的信息编码策略方面发挥重要作用（Földiak，[1990](#bib.bib70)；Olshausen
    和 Field，[1996a](#bib.bib183)）。实际上，在使用反向传播训练的网络中也观察到类似的编码策略（Agrawal 等，[2014](#bib.bib3)）。
- en: The Sparse Coding (SC) principle explicitly introduces a sparsity constraint
    in the learning framework. Let $\mathcal{X}$ be a dataset of input vectors. SC
    assumes that the elements of $\mathcal{X}$ can be represented as linear combinations
    of few basis vectors $\mathbf{d}_{1},\mathbf{d}_{2},...\mathbf{d}_{N}$, also called
    words. For a compact representation, let $D$ be a matrix whose columns are the
    word vectors, which is called the dictionary matrix. The goal is to find, for
    each input $\mathbf{x}\in\mathcal{X}$, an encoding in terms of linear combinations
    of the dictionary vectors
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏编码 (SC) 原则在学习框架中明确引入了稀疏性约束。设 $\mathcal{X}$ 为输入向量的数据集。SC 假设 $\mathcal{X}$ 的元素可以表示为少数基向量
    $\mathbf{d}_{1},\mathbf{d}_{2},...\mathbf{d}_{N}$ 的线性组合，这些向量也称为字典。为了实现紧凑表示，设 $D$
    为一个其列为字典向量的矩阵，称为字典矩阵。目标是为每个输入 $\mathbf{x}\in\mathcal{X}$ 找到一种基于字典向量线性组合的编码。
- en: '| (24) |  | $\mathbf{\hat{x}}=\mathbf{\hat{x}}(\mathbf{y},D)=\sum_{i}y_{i}\mathbf{d}_{i}=D\,\mathbf{y}$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\mathbf{\hat{x}}=\mathbf{\hat{x}}(\mathbf{y},D)=\sum_{i}y_{i}\mathbf{d}_{i}=D\,\mathbf{y}$
    |  |'
- en: 'which minimizes the representation error, i.e. a distance measure between the
    original and the reconstructed input. The vector of linear combination coefficients
    $\mathbf{y}=(y_{1},y_{2},...y_{N})^{T}$ is also called the code vector. An additional
    constraint imposed by SC is the sparsity of the representation $\mathbf{y}$. For
    example, using Euclidean distance as an error metric, the SC objective can be
    expressed as:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其最小化表示误差，即原始输入和重建输入之间的距离度量。线性组合系数的向量 $\mathbf{y}=(y_{1},y_{2},...y_{N})^{T}$
    也称为编码向量。SC 施加的额外约束是表示 $\mathbf{y}$ 的稀疏性。例如，使用欧几里得距离作为误差度量，SC 目标可以表示为：
- en: '| (25) |  | $\mathcal{L}_{SC}(\mathbf{y},D)=\sum_{\frac{1}{2}\mathbf{x}\in\mathcal{X}}(\mathbf{x}-\mathbf{\hat{x}}(\mathbf{y},D))^{2}+\lambda\mathbf{C}(\mathbf{y}(\mathbf{x}))$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $\mathcal{L}_{SC}(\mathbf{y},D)=\sum_{\frac{1}{2}\mathbf{x}\in\mathcal{X}}(\mathbf{x}-\mathbf{\hat{x}}(\mathbf{y},D))^{2}+\lambda\mathbf{C}(\mathbf{y}(\mathbf{x}))$
    |  |'
- en: where $\mathbf{C}(\mathbf{y})=(C(y_{1}),...,C(y_{N}))^{T}$ is a cost function
    that penalizes dense codes, while $\lambda$ is a hyperparameter. In principle,
    we could choose function $C$ to simply count the number of non-zero elements of
    $y$, but this definition is not well suited for gradient-based optimization. Smother
    alternatives can be considered, such as $L_{1}$ or $L_{2}$ penalties on $\mathbf{y}$,
    but other forms are possible (Olshausen and Field, [1996a](#bib.bib183), [b](#bib.bib184)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{C}(\mathbf{y})=(C(y_{1}),...,C(y_{N}))^{T}$ 是一个惩罚稠密编码的代价函数，而 $\lambda$
    是一个超参数。原则上，我们可以选择函数 $C$ 来简单地计算 $y$ 的非零元素的数量，但这种定义不适合基于梯度的优化。可以考虑更平滑的替代方案，例如对 $\mathbf{y}$
    的 $L_{1}$ 或 $L_{2}$ 惩罚，但其他形式也是可能的（Olshausen 和 Field，[1996a](#bib.bib183)，[b](#bib.bib184)）。
- en: '![Refer to caption](img/080925ca8d94ad71aa87f57f919bc348.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/080925ca8d94ad71aa87f57f919bc348.png)'
- en: (a) Sparse coding layer with error recirculation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 稀疏编码层具有误差反馈。
- en: '![Refer to caption](img/b30c83d768c83afb1a59f07b564463c1.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b30c83d768c83afb1a59f07b564463c1.png)'
- en: (b) Sparse coding layer with feedforward and lateral connections, and a shrinkage-thresholding
    nonlinearity implementing the Locally Competitive Algorithm (LCA).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 稀疏编码层具有前馈和侧向连接，并且一个收缩阈值非线性实现了局部竞争算法 (LCA)。
- en: Figure 8. Neural architectures for sparse coding layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8. 稀疏编码层的神经网络结构。
- en: '![Refer to caption](img/0b9230c822f85814fb21667d182f4a1a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0b9230c822f85814fb21667d182f4a1a.png)'
- en: Figure 9. Shrink-thresholding nonlinearity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. 收缩阈值非线性。
- en: 'Given the dictionary, there are various algorithms to find sparse codes, such
    as Orthogonal Matching Pursuit (OMP) (Pati et al., [1993](#bib.bib190)), or Fast
    Iterative Shrinkage-Thresholding Algorithm (FISTA) (Beck and Teboulle, [2009](#bib.bib20)).
    If the dictionary is not given a priori, other algorithms can be used to find
    the dictionary vectors, for example, based on clustering (Li et al., [2003](#bib.bib150);
    He and Cichocki, [2006b](#bib.bib91)) or eigenvalue decomposition (He and Cichocki,
    [2006a](#bib.bib90); Aharon et al., [2006](#bib.bib4)). However, a neurally-grounded
    approach (Olshausen and Field, [1996a](#bib.bib183)) can be derived by considering
    SC as two nested optimization problems: first find optimal coding coefficients
    for a fixed dictionary, and then optimize the dictionary code vectors given the
    coding coefficients found before. In this scenario, the first optimization stage
    corresponds to unfolding the neural dynamics, while the second stage yields the
    synaptic dynamics. Specifically, starting from a code $\mathbf{y}$, we can minimize
    the objective in Eq. [25](#S4.E25 "In 4.5\. Sparse Coding (SC) ‣ 4\. Plasticity
    Models for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey") by iterating gradient
    descent steps w.r.t. $\mathbf{y}$.This leads to the following update for $\mathbf{y}$:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '给定字典，可以使用各种算法来找到稀疏编码，例如正交匹配追踪 (OMP) (Pati 等人，[1993](#bib.bib190))，或快速迭代收缩阈值算法
    (FISTA) (Beck 和 Teboulle，[2009](#bib.bib20))。如果字典事先未给出，可以使用其他算法来找到字典向量，例如基于聚类
    (Li 等人，[2003](#bib.bib150); He 和 Cichocki，[2006b](#bib.bib91)) 或特征值分解 (He 和 Cichocki，[2006a](#bib.bib90);
    Aharon 等人，[2006](#bib.bib4))。然而，通过将稀疏编码视为两个嵌套的优化问题，可以推导出一种神经基础的方法 (Olshausen 和
    Field，[1996a](#bib.bib183))：首先为固定的字典找到最优编码系数，然后在给定之前找到的编码系数的情况下优化字典编码向量。在这种情况下，第一个优化阶段对应于展开神经动态，而第二个阶段则产生突触动态。具体来说，从一个代码
    $\mathbf{y}$ 开始，我们可以通过对 $\mathbf{y}$ 进行梯度下降步骤迭代来最小化等式 [25](#S4.E25 "In 4.5\. Sparse
    Coding (SC) ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple
    Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey") 中的目标。这会导致 $\mathbf{y}$ 的以下更新：'
- en: '| (26) |  | $\Delta\mathbf{y}\propto D^{T}\,(\mathbf{x}-\mathbf{\hat{x}})-\mathbf{C}^{\prime}(\mathbf{y})$
    |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\Delta\mathbf{y}\propto D^{T}\,(\mathbf{x}-\mathbf{\hat{x}})-\mathbf{C}^{\prime}(\mathbf{y})$
    |  |'
- en: 'where $\mathbf{C}^{\prime}(\mathbf{y})=(C^{\prime}(y_{1}),...,C^{\prime}(y_{N}))^{T}$
    denotes the derivative of the sparsity-inducing cost function. This formulation
    corresponds to a neural layer (Fig. [8(a)](#S4.F8.sf1 "In Figure 8 ‣ 4.5\. Sparse
    Coding (SC) ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple
    Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey")) where the activations $\mathbf{y}$ represent the sparse code. The
    dictionary $D$ corresponds to feedback connections that map the code back to the
    reconstruction $\mathbf{\hat{x}}=D\,\mathbf{y}$, and the residual error $(\mathbf{x}-\mathbf{\hat{x}})$
    is computed. The transpose dictionary $D^{T}$ represents forward connections that
    modify the activations $\mathbf{y}$ based on the previous error, such that the
    updates follow a gradient descent direction. The residual error recirculates for
    a number of iterations until convergence is reached, while $-C^{\prime}(\mathbf{y})$
    contributes to a sparsity-inducing decay in the activations. Once convergence
    is reached, the second phase of optimization involves the dictionary vectors.
    Calling $\mathbf{y}^{*}$ the sparse code after convergence, the dictionary can
    be optimized by another gradient descent step, w.r.t. $D$:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{C}^{\prime}(\mathbf{y})=(C^{\prime}(y_{1}),...,C^{\prime}(y_{N}))^{T}$
    表示稀疏性诱导成本函数的导数。这个公式对应于一个神经层（见图 [8(a)](#S4.F8.sf1 "在图 8 ‣ 4.5\. 稀疏编码 (SC) ‣ 4\.
    多神经元无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")），其中激活值 $\mathbf{y}$ 代表稀疏编码。字典 $D$
    对应于反馈连接，将编码映射回重建 $\mathbf{\hat{x}}=D\,\mathbf{y}$，并计算残差误差 $(\mathbf{x}-\mathbf{\hat{x}})$。转置字典
    $D^{T}$ 表示前馈连接，根据先前的误差调整激活值 $\mathbf{y}$，使更新遵循梯度下降方向。残差误差会在若干次迭代中循环，直到收敛，而 $-C^{\prime}(\mathbf{y})$
    促使激活值的稀疏性衰减。一旦达到收敛，优化的第二阶段涉及字典向量。将收敛后的稀疏编码称为 $\mathbf{y}^{*}$，字典可以通过另一轮梯度下降步骤来优化，针对
    $D$：
- en: '| (27) |  | $\Delta D=\eta\,(\mathbf{x}-D\,\mathbf{y}^{*})\,(\mathbf{y}^{*})^{T}$
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\Delta D=\eta\,(\mathbf{x}-D\,\mathbf{y}^{*})\,(\mathbf{y}^{*})^{T}$
    |  |'
- en: Note that Hebbian PCA algorithms can be considered as a special case of SC with
    a single iteration of the recirculation process (which is sufficient for convergence,
    under the assumption that the dictionary is orthogonal) and no sparsity constraint.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Hebbian PCA 算法可以被视为 SC 的特例，其中只有一次递归过程（在假设字典正交的情况下，足以达到收敛）且没有稀疏性约束。
- en: 'Although there is no explicit neural circuitry to support the error recirculation
    process, it is possible to show that sparse coding can also be implemented using
    once again feedforward and lateral connections, supporting biologically plausible
    local processing and plasticity (Fig. [8(b)](#S4.F8.sf2 "In Figure 8 ‣ 4.5\. Sparse
    Coding (SC) ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple
    Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey")). By rewriting Eq. [26](#S4.E26 "In 4.5\. Sparse Coding (SC) ‣ 4\.
    Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic
    Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey") as'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有明确的神经电路支持误差递归过程，但可以证明稀疏编码也可以通过再次使用前馈和侧向连接来实现，从而支持生物学上合理的局部处理和可塑性（见图 [8(b)](#S4.F8.sf2
    "在图 8 ‣ 4.5\. 稀疏编码 (SC) ‣ 4\. 多神经元无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")）。通过将公式
    [26](#S4.E26 "在 4.5\. 稀疏编码 (SC) ‣ 4\. 多神经元无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")
    重写为
- en: '| (28) |  | $\Delta\mathbf{y}\propto D^{T}\,\mathbf{x}-D^{T}D\,\mathbf{y}-C^{\prime}(\mathbf{y})$
    |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $\Delta\mathbf{y}\propto D^{T}\,\mathbf{x}-D^{T}D\,\mathbf{y}-C^{\prime}(\mathbf{y})$
    |  |'
- en: 'we can observe two contributions in particular: $D^{T}\,\mathbf{x}$, which
    corresponds to a feedforward term, and $-D^{T}D\,\mathbf{y}$ which corresponds
    to a lateral interaction term, where $D^{T}$ and $-D^{T}D$ are respectively the
    feedforward and lateral connection weights, which replace the error recirculation
    process. An SC neural layer can also be formulated in terms of an Energy-Based
    Model (EBM) (Hopfield, [1982](#bib.bib95); Haykin, [2009](#bib.bib88)), as done
    in the Locally Competitive Algorithm (LCA) (Rozell et al., [2008](#bib.bib206)).
    An EMB is a dynamical system characterized by a state which evolves according
    to certain equations, in such a way that a certain energy function is progressively
    reduced. In the SC case, neurons maintain an internal state $\mathbf{u}$, represented
    by their membrane potential, in which stimuli are integrated over time. Outputs
    $\mathbf{y}$ are connected to $\mathbf{u}$ by a sparsifying monotonic nonlinearity:
    $\mathbf{y}=T(\mathbf{u})$. The evolution of the system is described by the following
    equations:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以特别观察到两个贡献：$D^{T}\,\mathbf{x}$，对应于前馈项，以及 $-D^{T}D\,\mathbf{y}$，对应于侧向交互项，其中
    $D^{T}$ 和 $-D^{T}D$ 分别是前馈和侧向连接权重，取代了误差再循环过程。SC 神经层也可以用能量基模型（EBM）（Hopfield， [1982](#bib.bib95);
    Haykin， [2009](#bib.bib88)）来表述，正如在局部竞争算法（LCA）（Rozell 等， [2008](#bib.bib206)）中所做的那样。EBM
    是一种动态系统，其状态根据某些方程演变，以使某个能量函数逐渐减少。在 SC 的情况下，神经元维持一个内部状态 $\mathbf{u}$，由其膜电位表示，其中刺激随着时间的推移被整合。输出
    $\mathbf{y}$ 通过稀疏化的单调非线性与 $\mathbf{u}$ 连接：$\mathbf{y}=T(\mathbf{u})$。系统的演变由以下方程描述：
- en: '| (29) |  | $\begin{split}\Delta\mathbf{u}&amp;\propto D^{T}\,\mathbf{x}-u-(D^{T}D-I)\,\mathbf{y}\\
    \mathbf{y}&amp;=T(\mathbf{u})\end{split}$ |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $\begin{split}\Delta\mathbf{u}&amp;\propto D^{T}\,\mathbf{x}-u-(D^{T}D-I)\,\mathbf{y}\\
    \mathbf{y}&amp;=T(\mathbf{u})\end{split}$ |  |'
- en: 'which can be shown to minimize Eq. [25](#S4.E25 "In 4.5\. Sparse Coding (SC)
    ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"),
    which is therefore the energy function of this EBM. Also in this case, the system
    is characterized by a feedforward interaction $D^{T}\,\mathbf{x}$, and a lateral
    interaction $-(D^{T}D-I)\,\mathbf{y}$, where $D^{T}$ and $-(D^{T}D-I)$ are respectively
    the feedforward and lateral connection weights. The specific form of the nonlinearity
    $T(\cdot)$ is related to the choice of the sparsity-inducing cost term $C(\mathbf{y})$
    in Eq. [25](#S4.E25 "In 4.5\. Sparse Coding (SC) ‣ 4\. Plasticity Models for Unsupervised
    Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey") as follows (Rozell et al., [2008](#bib.bib206)):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，这最小化了方程 [25](#S4.E25 "在 4.5\. 稀疏编码 (SC) ‣ 4\. 无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")，因此这是该
    EBM 的能量函数。在这种情况下，系统也由前馈交互 $D^{T}\,\mathbf{x}$ 和侧向交互 $-(D^{T}D-I)\,\mathbf{y}$
    特征，其中 $D^{T}$ 和 $-(D^{T}D-I)$ 分别是前馈和侧向连接权重。非线性 $T(\cdot)$ 的具体形式与方程 [25](#S4.E25
    "在 4.5\. 稀疏编码 (SC) ‣ 4\. 无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述") 中稀疏性诱导成本项 $C(\mathbf{y})$
    的选择有关（Rozell 等， [2008](#bib.bib206)）。
- en: '| (30) |  | $\lambda T^{-1}(y_{i})=C^{\prime}(y_{i})+y_{i}\qquad i=1,...,N$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\lambda T^{-1}(y_{i})=C^{\prime}(y_{i})+y_{i}\qquad i=1,...,N$
    |  |'
- en: 'A common choice is the shrinkage-threshold function (Fig. [9](#S4.F9 "Figure
    9 ‣ 4.5\. Sparse Coding (SC) ‣ 4\. Plasticity Models for Unsupervised Pattern
    Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey")), which corresponds to the $L_{1}$ sparsity
    penalty. This constitutes the LCA formulation of SC (Rozell et al., [2008](#bib.bib206)).
    As the name of the algorithm suggests, a form of local competition take place
    among neurons. Indeed, it can be noticed that the nonlinearity, together with
    the lateral connections, induces a competitive interaction, where activations
    below a threshold are suppressed, while activations above a threshold inhibit
    the others.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的选择是收缩阈值函数（图 [9](#S4.F9 "图 9 ‣ 4.5\. 稀疏编码 (SC) ‣ 4\. 无监督模式发现的可塑性模型 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")），它对应于
    $L_{1}$ 稀疏性惩罚。这构成了 SC 的 LCA 表述（Rozell 等， [2008](#bib.bib206)）。正如算法的名称所暗示的那样，神经元之间发生了某种形式的局部竞争。实际上，可以注意到，非线性以及侧向连接引发了竞争互动，其中低于阈值的激活被抑制，而高于阈值的激活则抑制其他激活。
- en: 4.6\. Independent Component Analysis (ICA)
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 独立成分分析（ICA）
- en: '![Refer to caption](img/7dbd711deab3cd3245860f994dc7bded.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7dbd711deab3cd3245860f994dc7bded.png)'
- en: Figure 10. Blind Source Separation (BSS) problem. A mixing process $M$ generates
    samples $\mathbf{x}$ from source variables $s_{1},...,s_{N}$. A demixer $W$ mapping
    samples $\mathbf{x}$ to outputs $y_{1},...,y_{N}$. The goal is to find a demixer
    capable of reconstructing the original sources, without using information about
    the sources themselves.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10. 盲源分离（BSS）问题。一个混合过程 $M$ 从源变量 $s_{1},...,s_{N}$ 生成样本 $\mathbf{x}$。一个解混器
    $W$ 将样本 $\mathbf{x}$ 映射到输出 $y_{1},...,y_{N}$。目标是找到一个能够在不使用源本身信息的情况下重构原始源的解混器。
- en: 'PCA looks for directions in the data space along have maximum variance while
    being maximally decorrelated. This idea can be generalized also to higher-order
    statistical moments. In particular, a stronger condition than decorrelation is
    represented by independence. Independent Component Analysis (ICA) (Hyvarinen et al.,
    [2002](#bib.bib99)) addresses the problem of finding data representations into
    a set of maximally independent variables. ICA has strong relationships with the
    Blind Source Separation (BSS) problem (Jutten and Herault, [1991](#bib.bib111)).
    In BSS, data are assumed to be generated by a mixing process as in Fig. [10](#S4.F10
    "Figure 10 ‣ 4.6\. Independent Component Analysis (ICA) ‣ 4\. Plasticity Models
    for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey"). A mixer generates
    data samples $\mathbf{x}$ from source variables $\mathbf{s}=(s_{1},...,s_{N})^{T}$
    (sampled from a problem-dependent distribution) through a mixing matrix $M$: $\mathbf{x}=M\,\mathbf{s}$.
    A demixer maps samples $\mathbf{x}$ to outputs $\mathbf{y}=(y_{1},...,y_{N})^{T}$,
    through a demixing matrix $W$: $\mathbf{y}=W\,\mathbf{x}$. The goal is to find
    a demixer that is capable of reconstructing the original sources using only information
    about the samples. The task is challenging because the information about the sources
    themselves is not available to guide the search for the desired demixer. Nonetheless,
    theoretical results show that the problem can be solved (Jutten and Herault, [1991](#bib.bib111);
    Comon et al., [1991](#bib.bib51); Comon, [1994](#bib.bib50); Cardoso and Laheld,
    [1996](#bib.bib37); Cardoso, [1997](#bib.bib34), [2001](#bib.bib35), [2003](#bib.bib36);
    Hyvarinen et al., [2002](#bib.bib99)), and the original sources can be correctly
    identified (up to a permutation), provided that the source distribution is non-Gaussian.
    Another scenario where the sources cannot be identified, without additional information,
    is represented by a nonlinear mixing process. Furthermore, depending on whether
    the number of sources is larger or smaller than the sample dimension (overcomplete
    or undercomplete mixtures, respectively), identifiability could be affected. Specifically,
    undercomplete mixtures are identifiable, while overcomplete mixtures require an
    additional constraint on the sparsity of the sources. This condition is reminiscent
    of the SC problem, and indeed it has been shown that ICA and SC are actually related
    (Olshausen, [1996](#bib.bib182)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'PCA 寻找数据空间中的方向，这些方向具有最大方差，同时尽可能地解相关。这个想法也可以推广到更高阶的统计矩。特别地，独立性比解相关性更强的条件。独立成分分析（ICA）
    (Hyvarinen et al., [2002](#bib.bib99)) 解决了将数据表示转换为一组最大独立变量的问题。ICA 与盲源分离（BSS）问题
    (Jutten and Herault, [1991](#bib.bib111)) 有着密切的关系。在 BSS 中，数据被假设由混合过程生成，如图 [10](#S4.F10
    "Figure 10 ‣ 4.6\. Independent Component Analysis (ICA) ‣ 4\. Plasticity Models
    for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey") 所示。一个混合器通过混合矩阵
    $M$ 从源变量 $\mathbf{s}=(s_{1},...,s_{N})^{T}$（从问题相关分布中采样）生成数据样本 $\mathbf{x}$: $\mathbf{x}=M\,\mathbf{s}$。一个解混器通过解混矩阵
    $W$ 将样本 $\mathbf{x}$ 映射到输出 $\mathbf{y}=(y_{1},...,y_{N})^{T}$: $\mathbf{y}=W\,\mathbf{x}$。目标是找到一个能够仅利用样本信息来重构原始源的解混器。这个任务具有挑战性，因为没有关于源本身的信息来指导对所需解混器的搜索。然而，理论结果表明这个问题可以解决
    (Jutten and Herault, [1991](#bib.bib111); Comon et al., [1991](#bib.bib51); Comon,
    [1994](#bib.bib50); Cardoso and Laheld, [1996](#bib.bib37); Cardoso, [1997](#bib.bib34),
    [2001](#bib.bib35), [2003](#bib.bib36); Hyvarinen et al., [2002](#bib.bib99))，并且原始源可以正确识别（最多有一个排列），前提是源分布是非高斯的。另一种无法在没有额外信息的情况下识别源的情况是非线性混合过程。此外，根据源的数量是否大于或小于样本维度（分别是过完备或欠完备混合），可识别性可能会受到影响。具体来说，欠完备混合是可识别的，而过完备混合需要对源的稀疏性施加额外约束。这个条件让人联想到
    SC 问题，确实，已证明 ICA 和 SC 实际上是相关的 (Olshausen, [1996](#bib.bib182))。'
- en: 'ICA approaches rely on information-theoretic methods to identify the correct
    demixer. A popular approach is the nautral gradient method (Amari et al., [1996](#bib.bib8)),
    which enforces the independence requirement by minimizing the Mutual Information
    (MI) of demixer outputs. This can be achieved by considering the distribution
    of the demixer outputs, say $\mathbf{q}_{\mathbf{Y}}(\mathbf{y})$, and observing
    that the output variables are independent only if their distribution is a factorial
    distribution, i.e. has the form $\mathbf{p}_{\mathbf{Y}}(\mathbf{y})=\prod_{i=1}^{N}p_{Y_{i}}(y_{i})$
    (we will come back later on the choice of the specific form of $p$). Therefore,
    an objective can be defined as the minimization of the Kullback-Leibler (KL) divergence
    between the joint distribution $\mathbf{q_{Y}}$ and target factorial distribution
    $\mathbf{p_{Y}}$:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 方法依赖于信息理论方法来识别正确的去混合器。一种流行的方法是自然梯度方法（Amari et al., [1996](#bib.bib8)），该方法通过最小化去混合器输出的互信息（MI）来强制独立性要求。这可以通过考虑去混合器输出的分布
    $\mathbf{q}_{\mathbf{Y}}(\mathbf{y})$，并观察到仅当输出变量的分布是因子分布时它们才是独立的，即具有形式 $\mathbf{p}_{\mathbf{Y}}(\mathbf{y})=\prod_{i=1}^{N}p_{Y_{i}}(y_{i})$（我们稍后将回到具体的
    $p$ 形式选择）。因此，可以定义一个目标，即最小化联合分布 $\mathbf{q_{Y}}$ 和目标因子分布 $\mathbf{p_{Y}}$ 之间的Kullback-Leibler
    (KL) 散度：
- en: '| (31) |  | $\mathcal{L}_{ICA}=D_{KL}(\mathbf{q_{Y}}(\mathbf{y})&#124;&#124;\mathbf{p_{Y}}(\mathbf{y}))$
    |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\mathcal{L}_{ICA}=D_{KL}(\mathbf{q_{Y}}(\mathbf{y})&#124;&#124;\mathbf{p_{Y}}(\mathbf{y}))$
    |  |'
- en: 'A gradient descent step on this objective w.r.t. the demixing matrix $W$ can
    be shown to lead to the following weight update equation:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标函数相对于去混合矩阵 $W$ 的梯度下降步骤，可以得到以下权重更新方程：
- en: '| (32) |  | $\delta W=\eta\,(W^{-T}-\mathbf{f}(\mathbf{y})\,x^{T})$ |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $\delta W=\eta\,(W^{-T}-\mathbf{f}(\mathbf{y})\,x^{T})$ |  |'
- en: 'where $W^{-T}$ denotes the inverse transpose of $W$, and $\mathbf{f}(\mathbf{y})=(f(y_{1}),...,f(y_{N}))^{T}$
    is a nonlinearity whose form is related to the choice of the target distribution
    $p$ as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W^{-T}$ 表示 $W$ 的逆转置，而 $\mathbf{f}(\mathbf{y})=(f(y_{1}),...,f(y_{N}))^{T}$
    是一个非线性函数，其形式与目标分布 $p$ 的选择相关，如下所示：
- en: '| (33) |  | $f(y)=\frac{d}{dy}\log p(y)=\frac{p^{\prime}(y)}{p(y)}$ |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $f(y)=\frac{d}{dy}\log p(y)=\frac{p^{\prime}(y)}{p(y)}$ |  |'
- en: The drawback of this learning rule is that it requires the computation of a
    matrix inversion $W^{-T}$ at each step, which is expensive. However, the method
    proposed in (Amari et al., [1996](#bib.bib8)) suggests resorting instead to a
    natural gradient, which is a modification of the ordinary gradient by taking into
    consideration the underlying geometric structure of the problem. In our case,
    the natural gradient is simply obtained by multiplying the ordinary gradient by
    $W^{T}\,W$. Notice that the resulting direction will still be a descent direction
    on the objective because we are multiplying the gradient by a positive semi-definite
    matrix (so that the resulting vector will have a positive scalar product with/be
    less the 90° away from the gradient). Luckily, this multiplication removes the
    undesired inverse, leading to the following updated equation for ICA.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习规则的缺点在于每一步都需要计算矩阵的逆 $W^{-T}$，这非常昂贵。然而，（Amari et al., [1996](#bib.bib8)）提出的方法建议改用自然梯度，这是一种通过考虑问题的几何结构对普通梯度的修正。在我们的情况下，自然梯度通过将普通梯度乘以
    $W^{T}\,W$ 获得。请注意，结果方向仍然是目标函数上的下降方向，因为我们将梯度乘以一个正半定矩阵（这样结果向量将与梯度的标量积为正，或与梯度的夹角小于90°）。幸运的是，这种乘法去除了不需要的逆，得到了ICA的以下更新方程。
- en: '| (34) |  | $\delta W=\eta\,(I-\mathbf{f}(\mathbf{y})\,y^{T})\,W$ |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\delta W=\eta\,(I-\mathbf{f}(\mathbf{y})\,y^{T})\,W$ |  |'
- en: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 525.57 121.39)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="31.22"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$p_{Y}(y)$</foreignobject></g></g></g></svg>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 525.57 121.39)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="31.22"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$p_{Y}(y)$</foreignobject></g></g></g></svg>
- en: (a) Super-Gaussian distribution (thick line) compared to Gaussian (dashed line).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 超高斯分布（实线）与高斯分布（虚线）进行比较。
- en: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 525.57 121.39)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="31.22"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$p_{Y}(y)$</foreignobject></g></g></g></svg>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 525.57 121.39)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="31.22"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$p_{Y}(y)$</foreignobject></g></g></g></svg>
- en: (b) Sub-Gaussian distribution (thick line) compared to Gaussian (dashed line).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 次高斯分布（实线）与高斯分布（虚线）进行比较。
- en: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 525.57 235.2)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="26.31"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(y)$</foreignobject></g></g></g></svg>
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="455.79" overflow="visible" version="1.1" width="538.29"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 525.57 235.2)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 273.76 439.97)" fill="#000000" stroke="#000000"><foreignobject width="26.31"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(y)$</foreignobject></g></g></g></svg>
- en: (c) Activation function for super-Gaussian distributions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 超高斯分布的激活函数。
- en: <svg   height="455.79" overflow="visible" version="1.1" width="538.28"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(-57.34,0) translate(0,-76.35)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 583.17 311.82)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 331.37 516.59)" fill="#000000" stroke="#000000"><foreignobject width="26.31"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(y)$</foreignobject></g></g></g></svg>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="455.79" overflow="visible" version="1.1" width="538.28"><g transform="translate(0,455.79)
    matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0
    -0.28 -0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(-57.34,0) translate(0,-76.35)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 583.17 311.82)" fill="#000000"
    stroke="#000000"><foreignobject width="7.28" height="8.65" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$y$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 331.37 516.59)" fill="#000000" stroke="#000000"><foreignobject width="26.31"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(y)$</foreignobject></g></g></g></svg>
- en: (d) Activation function for sub-Gaussian distributions.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 子高斯分布的激活函数。
- en: Figure 11. Types of distributions and corresponding nonlinearities in ICA.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11. ICA 中的分布类型及相应的非线性。
- en: 'Let’s go back to the choice of the distribution model for $p$. Ideally, the
    form of the target marginal distribution $p$ should correspond to the distribution
    of the sources, but this is often unknown in practice. However, domain expertise
    can help the expert to make assumptions about the distribution of the sources
    and choose $q$ accordingly. Distributions can be divided into two families: super-gaussian
    and sub-gaussian. Super-gaussian distributions are characterized by a sharp central
    peak and heavier tails compared to a Gaussian. Examples of super-gaussian distributions
    are the Laplacian distribution, whose log-density is $\log p_{Y}(y)=\alpha-|y|$
    (where $\alpha$ is a normalizing constant), or the distribution with log-density
    $\log p_{Y}(y)=\alpha-2\log\cosh(y)$. Sub-gaussian distributions are fat at the
    center and lighter-tailed compared to Gaussian, and some examples are the uniform
    distribution, or the distribution with log-density $\log p_{Y}(y)=\alpha-(\frac{1}{2}y^{2}-\log\cosh(y))$.
    Theoretical results (Hyvärinen, [1997](#bib.bib98); Hyvärinen and Oja, [1998](#bib.bib102);
    Hyvarinen et al., [2002](#bib.bib99); Haykin, [2009](#bib.bib88)) show that optimization
    will succeed as long as the assumed distribution belongs to the correct family
    as the true distribution. A simple condition to test whether a chosen nonlinearity
    is adequate for the source distribution of the given dataset is the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到对$p$的分布模型选择上。理想情况下，目标边际分布$p$的形式应该与源分布对应，但在实际中这通常是未知的。然而，领域专业知识可以帮助专家对源分布做出假设，并据此选择$q$。分布可以分为两类：超高斯分布和子高斯分布。超高斯分布的特点是中央峰值尖锐，相比高斯分布具有更重的尾部。超高斯分布的例子包括拉普拉斯分布，其对数密度为$\log
    p_{Y}(y)=\alpha-|y|$（其中$\alpha$是归一化常数），或者对数密度为$\log p_{Y}(y)=\alpha-2\log\cosh(y)$的分布。子高斯分布在中心部位比较胖，尾部比高斯分布轻，一些例子包括均匀分布，或对数密度为$\log
    p_{Y}(y)=\alpha-(\frac{1}{2}y^{2}-\log\cosh(y))$的分布。理论结果（Hyvärinen, [1997](#bib.bib98);
    Hyvärinen and Oja, [1998](#bib.bib102); Hyvarinen et al., [2002](#bib.bib99);
    Haykin, [2009](#bib.bib88)）表明，只要假设的分布属于与真实分布相同的正确类别，优化就会成功。测试所选非线性是否适合给定数据集的源分布的一个简单条件是：
- en: '| (35) |  | $\xi_{i}=\mathbb{E}[y_{i}\,f(y_{i})-f^{\prime}(y_{i})]>0\qquad
    i=1,...,N$ |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $\xi_{i}=\mathbb{E}[y_{i}\,f(y_{i})-f^{\prime}(y_{i})]>0\qquad
    i=1,...,N$ |  |'
- en: 'Note that, if $\xi_{i}$ is negative for a given $f(y_{i})$, then it will be
    positive for $g(y_{i})=y_{i}-f(y_{i})$ (assuming that data are normalized to zero
    mean and unit variance). For instance, the example distributions $\log p_{Y}(y)=\alpha-2\log\cosh(y)$
    and $\log p_{Y}(y)=\alpha-(\frac{1}{2}y^{2}-\log\cosh(y))$ lead to functions $f(y)=\tanh(y)$
    and $f(y)=y-\tanh(y)$, respectively. A plot of super-Gaussian and sub-Gaussian
    distributions, and related nonlinearities, is shown in Fig. [11](#S4.F11 "Figure
    11 ‣ 4.6\. Independent Component Analysis (ICA) ‣ 4\. Plasticity Models for Unsupervised
    Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，如果对于给定的$f(y_{i})$，$\xi_{i}$是负的，那么对于$g(y_{i})=y_{i}-f(y_{i})$（假设数据已归一化为零均值和单位方差），它将是正的。例如，分布$\log
    p_{Y}(y)=\alpha-2\log\cosh(y)$和$\log p_{Y}(y)=\alpha-(\frac{1}{2}y^{2}-\log\cosh(y))$分别导致函数$f(y)=\tanh(y)$和$f(y)=y-\tanh(y)$。超高斯和亚高斯分布及相关非线性函数的图示见图[11](#S4.F11
    "Figure 11 ‣ 4.6\. Independent Component Analysis (ICA) ‣ 4\. Plasticity Models
    for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey")。'
- en: 'The ICA approach is easily mapped into a feedforward neural layer (with architecture
    as Fig. [10](#S4.F10 "Figure 10 ‣ 4.6\. Independent Component Analysis (ICA) ‣
    4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple Neurons
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")),
    but alternative formulations also map into layers with feedforward and lateral
    connections (Jutten and Herault, [1991](#bib.bib111); Fyfe and Baddeley, [1995](#bib.bib72);
    Oja et al., [1996](#bib.bib181); Karhunen, [1996](#bib.bib112); Amari and Cichocki,
    [1998](#bib.bib7); Hyvärinen and Oja, [1998](#bib.bib102)). ICA is also related
    to nonlinear PCA methods (Karhunen and Joutsensalo, [1995](#bib.bib113); Becker
    and Plumbley, [1996](#bib.bib22)). The latter aims at canceling out some generalized
    higher-order cross-moments between the outputs (the specific form depends on the
    choice of the nonlinearity). Independence represents a stronger condition, as
    it implies the annulment of moments of all orders. However, nonlinear PCA approaches
    can also be effective in practice, as the minimization of higher-order moments
    often represents a good proxy to the maximization of independence.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 'ICA方法可以很容易地映射到一个前馈神经网络层中（其架构见图[10](#S4.F10 "Figure 10 ‣ 4.6\. Independent Component
    Analysis (ICA) ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with
    Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep
    Learning: A Survey")），但其他公式也可以映射到具有前馈和侧向连接的层中（Jutten 和 Herault，[1991](#bib.bib111)；Fyfe
    和 Baddeley，[1995](#bib.bib72)；Oja 等，[1996](#bib.bib181)；Karhunen，[1996](#bib.bib112)；Amari
    和 Cichocki，[1998](#bib.bib7)；Hyvärinen 和 Oja，[1998](#bib.bib102)）。ICA还与非线性PCA方法相关（Karhunen
    和 Joutsensalo，[1995](#bib.bib113)；Becker 和 Plumbley，[1996](#bib.bib22)）。后者旨在消除输出之间的一些广义高阶交叉矩（具体形式取决于非线性的选择）。独立性表示一种更强的条件，因为它意味着所有阶矩的消除。然而，非线性PCA方法在实践中也可以有效，因为高阶矩的最小化通常是独立性最大化的良好替代指标。'
- en: 'Other approaches for ICA have been proposed in the literature, such as InfoMax
    (Bell and Sejnowski, [1995](#bib.bib24)), i.e. maximizing the mutual information
    between the demixer output filtered by a nonlinearity $f$ (Fig. [10](#S4.F10 "Figure
    10 ‣ 4.6\. Independent Component Analysis (ICA) ‣ 4\. Plasticity Models for Unsupervised
    Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey")), whose shape depends on the source distribution,
    and samples $\mathbf{x}$, or maximum-likelihood approaches (Pham and Garat, [1997](#bib.bib195)).
    Indeed, these various formulations can be shown to be equivalent (Pham and Garat,
    [1997](#bib.bib195); Cardoso, [1997](#bib.bib34)). Another interesting aspect
    is the connection between ICA and SC (Olshausen, [1996](#bib.bib182)). The SC
    problem can be mapped to a maximum-likelihood formulation by considering the SC
    objective function $\mathcal{L}_{SC}$ (Eq. [25](#S4.E25 "In 4.5\. Sparse Coding
    (SC) ‣ 4\. Plasticity Models for Unsupervised Pattern Discovery with Multiple
    Neurons ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:
    A Survey")) as a log-likelihood of a probability density function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '文献中提出了其他ICA的方法，例如InfoMax（Bell和Sejnowski，[1995](#bib.bib24)），即最大化经过非线性函数$f$（图[10](#S4.F10
    "Figure 10 ‣ 4.6\. Independent Component Analysis (ICA) ‣ 4\. Plasticity Models
    for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey")）过滤的解混器输出之间的互信息，其形状取决于源分布，以及样本$\mathbf{x}$，或者最大似然方法（Pham和Garat，[1997](#bib.bib195)）。实际上，这些不同的公式可以证明是等价的（Pham和Garat，[1997](#bib.bib195)；Cardoso，[1997](#bib.bib34)）。另一个有趣的方面是ICA与SC之间的联系（Olshausen，[1996](#bib.bib182)）。SC问题可以通过将SC目标函数$\mathcal{L}_{SC}$（公式[25](#S4.E25
    "In 4.5\. Sparse Coding (SC) ‣ 4\. Plasticity Models for Unsupervised Pattern
    Discovery with Multiple Neurons ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey")）视为概率密度函数的对数似然来映射到最大似然公式：'
- en: '| (36) |  | $\ell_{\mathbf{Y}}(\mathbf{y},D)=\frac{1}{Z}\,e^{-\mathcal{L}_{SC}(\mathbf{y},D)}=\frac{1}{Z}\,e^{-\sum_{\frac{1}{2}\mathbf{x}\in\mathcal{X}}(\mathbf{x}-\mathbf{\hat{x}}(\mathbf{y},D))^{2}-\lambda\mathbf{C}(\mathbf{y}(\mathbf{x}))}$
    |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $\ell_{\mathbf{Y}}(\mathbf{y},D)=\frac{1}{Z}\,e^{-\mathcal{L}_{SC}(\mathbf{y},D)}=\frac{1}{Z}\,e^{-\sum_{\frac{1}{2}\mathbf{x}\in\mathcal{X}}(\mathbf{x}-\mathbf{\hat{x}}(\mathbf{y},D))^{2}-\lambda\mathbf{C}(\mathbf{y}(\mathbf{x}))}$
    |  |'
- en: where $Z$ is a normalizing constant. Without the sparsity-related term $\mathbf{C}$,
    this would just be a Gaussian density attributing higher probability density to
    lower reconstruction errors $(\mathbf{x}-\mathbf{\hat{x}})^{2}$. The sparsity
    term represents a prior that further penalizes dense codes by reducing the corresponding
    probability. Indeed, the sparsity condition in SC is related to the form of the
    marginal distribution in ICA. In particular, super-gaussian distributions induce
    sparse representations, being characterized by samples that are either close to
    zero (often), or very large (rarely).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z$是归一化常数。如果没有与稀疏性相关的项$\mathbf{C}$，这将只是一个高斯密度，将较低的重建误差$(\mathbf{x}-\mathbf{\hat{x}})^{2}$分配更高的概率密度。稀疏性项表示一个先验，通过减少相应的概率进一步惩罚密集编码。实际上，SC中的稀疏性条件与ICA中的边际分布形式有关。特别地，超高斯分布会引起稀疏表示，其特点是样本要么接近零（通常），要么非常大（很少）。
- en: If samples come from a temporal process, another class of BSS approaches exists,
    which is able to identify sources based on the temporal structure of the signals
    (Matsuoka et al., [1995](#bib.bib167); Belouchrani et al., [1997](#bib.bib25);
    Meyer-Base et al., [2001](#bib.bib170); Choi et al., [2002a](#bib.bib43), [b](#bib.bib44)).
    Finally, recent developments have provided a principled way to overcome previous
    theoretical limitations to the identifiability of nonlinear mixtures (Hyvarinen
    and Morioka, [2016](#bib.bib100), [2017](#bib.bib101); Hyvarinen et al., [2019](#bib.bib103)).
    These approaches are able to achieve nonlinear ICA by augmenting the data with
    additional information, such as temporal information, or any other auxiliary variable
    available, and then training a model to distinguish between the true augmented
    data and some negative data with a randomized auxiliary variable. The idea of
    extracting information by contrasting positive and negative views of the data
    has also emerged in various other domains (Lai and Fyfe, [2001](#bib.bib140);
    Sun et al., [2008](#bib.bib225); Andrew et al., [2013](#bib.bib11); Elmadany et al.,
    [2016](#bib.bib64); Dorfer et al., [2016](#bib.bib61); Gatto and dos Santos, [2017](#bib.bib76);
    Li et al., [2004](#bib.bib148); Dorfer et al., [2015](#bib.bib60); Koch et al.,
    [2015](#bib.bib120); Hoffer and Ailon, [2015](#bib.bib94); Ramachandran et al.,
    [2017](#bib.bib198); Baltrušaitis et al., [2018](#bib.bib16); Hossain et al.,
    [2019](#bib.bib96); Stefanini et al., [2022](#bib.bib222); Kaur et al., [2021](#bib.bib115);
    Oord et al., [2018](#bib.bib185); Löwe et al., [2019](#bib.bib154); Chen et al.,
    [2020](#bib.bib40)). This is the topic of multi-view or contrastive learning,
    which is discussed in the next subsection.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果样本来源于时间过程，还有另一类BSS方法可以基于信号的时间结构识别源（Matsuoka et al., [1995](#bib.bib167); Belouchrani
    et al., [1997](#bib.bib25); Meyer-Base et al., [2001](#bib.bib170); Choi et al.,
    [2002a](#bib.bib43), [b](#bib.bib44)）。最后，最近的发展提供了一种有原则的方法来克服以前理论上对非线性混合物可识别性的限制（Hyvarinen
    and Morioka, [2016](#bib.bib100), [2017](#bib.bib101); Hyvarinen et al., [2019](#bib.bib103)）。这些方法能够通过增加额外的信息，如时间信息或任何其他可用的辅助变量，来实现非线性ICA，然后训练模型以区分真实的增强数据和带有随机辅助变量的一些负数据。通过对比数据的正视图和负视图来提取信息的想法也在各种其他领域中出现（Lai
    and Fyfe, [2001](#bib.bib140); Sun et al., [2008](#bib.bib225); Andrew et al.,
    [2013](#bib.bib11); Elmadany et al., [2016](#bib.bib64); Dorfer et al., [2016](#bib.bib61);
    Gatto and dos Santos, [2017](#bib.bib76); Li et al., [2004](#bib.bib148); Dorfer
    et al., [2015](#bib.bib60); Koch et al., [2015](#bib.bib120); Hoffer and Ailon,
    [2015](#bib.bib94); Ramachandran et al., [2017](#bib.bib198); Baltrušaitis et
    al., [2018](#bib.bib16); Hossain et al., [2019](#bib.bib96); Stefanini et al.,
    [2022](#bib.bib222); Kaur et al., [2021](#bib.bib115); Oord et al., [2018](#bib.bib185);
    Löwe et al., [2019](#bib.bib154); Chen et al., [2020](#bib.bib40)）。这是多视图或对比学习的主题，将在下一小节中讨论。
- en: 4.7\. Multi-View Learning Models
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7. 多视图学习模型
- en: '![Refer to caption](img/d383b34fea025ee1a89a58daa1a3569f.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d383b34fea025ee1a89a58daa1a3569f.png)'
- en: Figure 12. Schematic representation of a multi-view learning module. The module
    generates representations for a pair of inputs, driving positive pair together,
    and negative pairs far apart.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. 多视图学习模块的示意图。该模块为一对输入生成表示，使正对样本靠近，而负对样本远离。
- en: 'In the field of machine learning, a variety of approaches exist which are based
    on the idea of processing data observed from multiple views. The concept of view
    is problem-dependent, but, for example, it can refer to multiple modalities through
    which the data are presented (visual, auditory, text, etc.), multiple copies of
    the same sample obtained by different transformations, multiple frames at different
    time instants, multiple features from a feature vector. Another example can be
    simply a sample and corresponding label information. The general idea is to map
    different related views (positives) to a common representation while making sure
    that unrelated views (negatives), which are typically obtained by a randomized
    pairing of views from different samples, are mapped to distinct representations
    (Fig. [12](#S4.F12 "Figure 12 ‣ 4.7\. Multi-View Learning Models ‣ 4\. Plasticity
    Models for Unsupervised Pattern Discovery with Multiple Neurons ‣ Synaptic Plasticity
    Models and Bio-Inspired Unsupervised Deep Learning: A Survey")).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，存在多种方法，它们基于从多个视角处理数据的理念。视角的概念依赖于具体问题，但例如，它可以指数据呈现的多种模态（视觉、听觉、文本等）、通过不同变换获得的相同样本的多个副本、在不同时间点的多个帧、来自特征向量的多个特征。另一个例子可以简单地是一个样本及其对应的标签信息。总体思路是将不同的相关视角（正样本）映射到一个共同的表示上，同时确保无关的视角（负样本），通常是通过随机配对来自不同样本的视角获得的，被映射到不同的表示上（见图
    [12](#S4.F12 "图 12 ‣ 4.7\. 多视角学习模型 ‣ 4\. 用于无监督模式发现的可塑性模型与多神经元 ‣ 突触可塑性模型和生物启发的无监督深度学习：综述")）。
- en: Canonical Correlation Analysis (CCA) (Lai and Fyfe, [2001](#bib.bib140); Sun
    et al., [2008](#bib.bib225); Andrew et al., [2013](#bib.bib11); Elmadany et al.,
    [2016](#bib.bib64); Dorfer et al., [2016](#bib.bib61); Gatto and dos Santos, [2017](#bib.bib76)),
    max-margin approaches (Mao and Jain, [1993](#bib.bib163); Pang et al., [2005](#bib.bib187);
    Demir and Ozmehmet, [2005](#bib.bib56); Li et al., [2004](#bib.bib148); Dorfer
    et al., [2015](#bib.bib60); Koch et al., [2015](#bib.bib120); Hoffer and Ailon,
    [2015](#bib.bib94)), multi-modal learning (Ramachandran et al., [2017](#bib.bib198);
    Baltrušaitis et al., [2018](#bib.bib16); Hossain et al., [2019](#bib.bib96); Stefanini
    et al., [2022](#bib.bib222); Kaur et al., [2021](#bib.bib115)), contrastive representation
    learning (Oord et al., [2018](#bib.bib185); Saunshi et al., [2019](#bib.bib212);
    Hénaff et al., [2019](#bib.bib92); Löwe et al., [2019](#bib.bib154); Chen et al.,
    [2020](#bib.bib40); Bardes et al., [2021](#bib.bib17)), can all be considered
    as multi-view learning approaches.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 典型相关分析（CCA）（Lai 和 Fyfe, [2001](#bib.bib140); Sun 等, [2008](#bib.bib225); Andrew
    等, [2013](#bib.bib11); Elmadany 等, [2016](#bib.bib64); Dorfer 等, [2016](#bib.bib61);
    Gatto 和 dos Santos, [2017](#bib.bib76)）、最大边际方法（Mao 和 Jain, [1993](#bib.bib163);
    Pang 等, [2005](#bib.bib187); Demir 和 Ozmehmet, [2005](#bib.bib56); Li 等, [2004](#bib.bib148);
    Dorfer 等, [2015](#bib.bib60); Koch 等, [2015](#bib.bib120); Hoffer 和 Ailon, [2015](#bib.bib94)）、多模态学习（Ramachandran
    等, [2017](#bib.bib198); Baltrušaitis 等, [2018](#bib.bib16); Hossain 等, [2019](#bib.bib96);
    Stefanini 等, [2022](#bib.bib222); Kaur 等, [2021](#bib.bib115)）、对比表示学习（Oord 等,
    [2018](#bib.bib185); Saunshi 等, [2019](#bib.bib212); Hénaff 等, [2019](#bib.bib92);
    Löwe 等, [2019](#bib.bib154); Chen 等, [2020](#bib.bib40); Bardes 等, [2021](#bib.bib17)），都可以被视为多视角学习方法。
- en: CCA (Lai and Fyfe, [2001](#bib.bib140); Sun et al., [2008](#bib.bib225); Gatto
    and dos Santos, [2017](#bib.bib76)) aims at finding linear projections of the
    different data modes so that the representations of related views are maximally
    correlated, while different views are uncorrelated. The IMax approach (Becker,
    [1996](#bib.bib21)) is similar, but it considers the Mutual Information (MI) between
    different views instead of simple correlation. Discriminative CCA is a particular
    instantiation in which the views that are considered are a data view and a target
    to be predicted (Kim et al., [2007](#bib.bib118); Sun et al., [2007](#bib.bib226);
    Elmadany et al., [2016](#bib.bib64); Dorfer et al., [2016](#bib.bib61)), which
    has been shown to be equivalent to linear regression methods (Shin and Park, [2011](#bib.bib217)).
    Nonlinear extensions of CCA through deep mappings were also recently explored
    (Andrew et al., [2013](#bib.bib11); Elmadany et al., [2016](#bib.bib64); Dorfer
    et al., [2016](#bib.bib61)). Moreover, Hebbian neural network implementations
    of CCA methods also exist (Lai and Fyfe, [2001](#bib.bib140); Gatto and dos Santos,
    [2017](#bib.bib76)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: CCA（Lai 和 Fyfe，[2001](#bib.bib140)；Sun 等，[2008](#bib.bib225)；Gatto 和 dos Santos，[2017](#bib.bib76)）旨在寻找不同数据模式的线性投影，使得相关视图的表示最大化相关，而不同视图之间不相关。IMax
    方法（Becker，[1996](#bib.bib21)）类似，但它考虑的是不同视图之间的互信息（MI），而不是简单的相关性。判别性 CCA 是一种特定的实例，其中考虑的视图是数据视图和待预测的目标（Kim
    等，[2007](#bib.bib118)；Sun 等，[2007](#bib.bib226)；Elmadany 等，[2016](#bib.bib64)；Dorfer
    等，[2016](#bib.bib61)），这已被证明等同于线性回归方法（Shin 和 Park，[2011](#bib.bib217)）。最近也探索了通过深度映射对
    CCA 的非线性扩展（Andrew 等，[2013](#bib.bib11)；Elmadany 等，[2016](#bib.bib64)；Dorfer 等，[2016](#bib.bib61)）。此外，Hebbian
    神经网络实现的 CCA 方法也存在（Lai 和 Fyfe，[2001](#bib.bib140)；Gatto 和 dos Santos，[2017](#bib.bib76)）。
- en: Max-margin approaches (Mao and Jain, [1993](#bib.bib163); Pang et al., [2005](#bib.bib187);
    Demir and Ozmehmet, [2005](#bib.bib56); Li et al., [2004](#bib.bib148); Dorfer
    et al., [2015](#bib.bib60); Koch et al., [2015](#bib.bib120); Hoffer and Ailon,
    [2015](#bib.bib94)) are supervised methods where label information is used to
    derive a mapping of samples into a feature space, in order to optimize class separability.
    Linear Discriminant Analysis (LDA) (Mao and Jain, [1993](#bib.bib163); Pang et al.,
    [2005](#bib.bib187); Demir and Ozmehmet, [2005](#bib.bib56)) is a classical method
    that minimizes the distance in feature space of same-class samples from the class
    centroid (intra-class distance), and maximizes the distance between different
    class centroids (inter-class distance). Equivalence between LDA and least-squares
    classification has been demonstrated (Ye, [2007](#bib.bib245)). While LDA focuses
    on linear mappings, max-margin methods were also extended to the nonlinear case
    (Santa Cruz and Dorronsoro, [1998](#bib.bib210); Mika et al., [1999](#bib.bib172);
    Kim and Kittler, [2005](#bib.bib117); Sugiyama, [2006](#bib.bib224); Dorfer et al.,
    [2015](#bib.bib60)). A similar principle is also pursued in metric learning with
    siamese networks (Koch et al., [2015](#bib.bib120)), and triplet loss learning
    (Hoffer and Ailon, [2015](#bib.bib94)), where the same class samples (positives)
    are mapped to nearby locations in feature space, while negative samples are far
    apart. Moreover, bio-inspired extensions of max-margin approaches for local learning
    have been proposed (Mao and Jain, [1993](#bib.bib163); Demir and Ozmehmet, [2005](#bib.bib56);
    Duan et al., [2021](#bib.bib62)). It is worth mentioning that the idea of augmenting
    unsupervised methods with label information has been explored in discriminative
    clustering (Kaski et al., [2005](#bib.bib114); Krause et al., [2010](#bib.bib126)),
    discriminative subspace learning (Bair et al., [2006](#bib.bib15); Barshan et al.,
    [2011](#bib.bib18); Li and Fu, [2015](#bib.bib149); Ritchie et al., [2019](#bib.bib201)),
    discriminative sparse coding (Mairal et al., [2008](#bib.bib160), [2009](#bib.bib161);
    Yang et al., [2011](#bib.bib244)), discriminative ICA (Akaho, [2002](#bib.bib5);
    Bressan and Vitrià, [2002](#bib.bib31); Dhir and Lee, [2011](#bib.bib59)), and
    discriminative manifold learning (De Ridder et al., [2003](#bib.bib55); Geng et al.,
    [2005](#bib.bib77); Zhang et al., [2008](#bib.bib250); Wang and Chen, [2009](#bib.bib236);
    Raducanu and Dornaika, [2012](#bib.bib197); Chien and Chen, [2016](#bib.bib41);
    Liu et al., [2019](#bib.bib151)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最大间隔方法（Mao 和 Jain, [1993](#bib.bib163)；Pang 等， [2005](#bib.bib187)；Demir 和 Ozmehmet,
    [2005](#bib.bib56)；Li 等， [2004](#bib.bib148)；Dorfer 等， [2015](#bib.bib60)；Koch
    等， [2015](#bib.bib120)；Hoffer 和 Ailon, [2015](#bib.bib94)）是监督方法，其中利用标签信息来推导样本到特征空间的映射，以优化类别的可分离性。线性判别分析（LDA）（Mao
    和 Jain, [1993](#bib.bib163)；Pang 等， [2005](#bib.bib187)；Demir 和 Ozmehmet, [2005](#bib.bib56)）是一种经典方法，它最小化同类别样本在特征空间中与类别中心的距离（类内距离），并最大化不同类别中心之间的距离（类间距离）。LDA
    与最小二乘分类的等价性已被证明（Ye, [2007](#bib.bib245)）。虽然 LDA 关注于线性映射，但最大间隔方法也扩展到了非线性情况（Santa
    Cruz 和 Dorronsoro, [1998](#bib.bib210)；Mika 等， [1999](#bib.bib172)；Kim 和 Kittler,
    [2005](#bib.bib117)；Sugiyama, [2006](#bib.bib224)；Dorfer 等， [2015](#bib.bib60)）。在度量学习中也追求类似的原则，包括使用孪生网络（Koch
    等， [2015](#bib.bib120)）和三元组损失学习（Hoffer 和 Ailon, [2015](#bib.bib94)），其中同一类别的样本（正样本）被映射到特征空间中的邻近位置，而负样本则远离。此外，还提出了基于生物启发的最大间隔方法在局部学习中的扩展（Mao
    和 Jain, [1993](#bib.bib163)；Demir 和 Ozmehmet, [2005](#bib.bib56)；Duan 等， [2021](#bib.bib62)）。值得一提的是，已探索通过标签信息增强无监督方法的想法，包括判别聚类（Kaski
    等， [2005](#bib.bib114)；Krause 等， [2010](#bib.bib126)）、判别子空间学习（Bair 等， [2006](#bib.bib15)；Barshan
    等， [2011](#bib.bib18)；Li 和 Fu, [2015](#bib.bib149)；Ritchie 等， [2019](#bib.bib201)）、判别稀疏编码（Mairal
    等， [2008](#bib.bib160)， [2009](#bib.bib161)；Yang 等， [2011](#bib.bib244)）、判别 ICA（Akaho,
    [2002](#bib.bib5)；Bressan 和 Vitrià, [2002](#bib.bib31)；Dhir 和 Lee, [2011](#bib.bib59)）和判别流形学习（De
    Ridder 等， [2003](#bib.bib55)；Geng 等， [2005](#bib.bib77)；Zhang 等， [2008](#bib.bib250)；Wang
    和 Chen, [2009](#bib.bib236)；Raducanu 和 Dornaika, [2012](#bib.bib197)；Chien 和 Chen,
    [2016](#bib.bib41)；Liu 等， [2019](#bib.bib151)）。
- en: Multi-modal learning (Ramachandran et al., [2017](#bib.bib198); Baltrušaitis
    et al., [2018](#bib.bib16); Hossain et al., [2019](#bib.bib96); Stefanini et al.,
    [2022](#bib.bib222)) aims at creating representations in DNNs that align different
    modalities of information, for example, images and text. Applications involve
    image captioning (Xu et al., [2015](#bib.bib242); Sarto et al., [2023](#bib.bib211)),
    text-to-image synthesis (Reed et al., [2016](#bib.bib199); Carrara et al., [2018](#bib.bib38);
    Zhang et al., [2017](#bib.bib249), [2018](#bib.bib248)), and cross-modal retrieval
    (Messina et al., [2021](#bib.bib169); Wang et al., [2016](#bib.bib235)). Hebbian
    learning could play a relevant role in reinforcing the observed correlations between
    different sensory pathways in the brain (Kaur et al., [2021](#bib.bib115)). Indeed,
    Hebbian approaches have been applied for cross-modal retrieval applications (Kaur
    et al., [2021](#bib.bib115)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习（Ramachandran 等，[2017](#bib.bib198)；Baltrušaitis 等，[2018](#bib.bib16)；Hossain
    等，[2019](#bib.bib96)；Stefanini 等，[2022](#bib.bib222)）旨在创建对齐不同信息模态的深度神经网络（DNN）表示，例如图像和文本。应用包括图像标注（Xu
    等，[2015](#bib.bib242)；Sarto 等，[2023](#bib.bib211)），文本到图像合成（Reed 等，[2016](#bib.bib199)；Carrara
    等，[2018](#bib.bib38)；Zhang 等，[2017](#bib.bib249)，[2018](#bib.bib248)），以及跨模态检索（Messina
    等，[2021](#bib.bib169)；Wang 等，[2016](#bib.bib235)）。Hebbian 学习在强化大脑中不同感官通路之间的观察到的相关性方面可能发挥相关作用（Kaur
    等，[2021](#bib.bib115)）。实际上，Hebbian 方法已经被应用于跨模态检索应用（Kaur 等，[2021](#bib.bib115)）。
- en: Contrastive representation learning (Oord et al., [2018](#bib.bib185); Saunshi
    et al., [2019](#bib.bib212); Hénaff et al., [2019](#bib.bib92); Löwe et al., [2019](#bib.bib154);
    Chen et al., [2020](#bib.bib40); Bardes et al., [2021](#bib.bib17)) is a popular
    approach for self-supervised learning in DNNs, which has shown promising results.
    Contrastive learning approaches aim at creating similar representations for elements
    that occur in similar contexts. This principle has been successfully used in the
    development of embedding models for language, such as the popular Word2Vec (Mikolov
    et al., [2013](#bib.bib173)), where words that occur in similar contexts are mapped
    to similar vectors. Contrastive learning approaches took inspiration from this
    principle and popularized it also for images. In this case, any two neighboring
    image patches – two views of the same input – are required to have consistent
    representations, while unrelated patches should have distinct representations.
    The SimCLR (Chen et al., [2020](#bib.bib40)) approach uses instead differently
    augmented versions of the same images, imposing consistency conditions over the
    corresponding representations. When data also have a temporal dimension available,
    consistency between neighboring frames can also be an effective approach, and
    it has been shown to yield feature representations that resemble those of biological
    brains (Watanabe et al., [2018](#bib.bib237); Zhuang et al., [2021](#bib.bib252)).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对比表示学习（Oord 等，[2018](#bib.bib185)；Saunshi 等，[2019](#bib.bib212)；Hénaff 等，[2019](#bib.bib92)；Löwe
    等，[2019](#bib.bib154)；Chen 等，[2020](#bib.bib40)；Bardes 等，[2021](#bib.bib17)）是一种流行的自监督学习方法，在
    DNN 中取得了良好的结果。对比学习方法旨在为出现在相似上下文中的元素创建相似的表示。这一原则已成功应用于语言嵌入模型的开发，例如流行的 Word2Vec（Mikolov
    等，[2013](#bib.bib173)），其中出现在相似上下文中的单词被映射到相似的向量。对比学习方法从这一原则中汲取灵感，并在图像处理领域也得到推广。在这种情况下，任何两个相邻的图像补丁——同一输入的两个视图——都要求具有一致的表示，而无关的补丁应具有不同的表示。SimCLR（Chen
    等，[2020](#bib.bib40)）方法使用的是相同图像的不同增强版本，对相应的表示施加一致性条件。当数据还有时间维度时，邻近帧之间的一致性也可以是一种有效的方法，并且已被证明能够产生类似于生物大脑的特征表示（Watanabe
    等，[2018](#bib.bib237)；Zhuang 等，[2021](#bib.bib252)）。
- en: 5\. Synaptic Plasticity Models in Deep Learning
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 深度学习中的突触可塑性模型
- en: '![Refer to caption](img/1d0896190311adf906369cb209dfa487.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1d0896190311adf906369cb209dfa487.png)'
- en: Figure 13. A given learning rule can be extended to the case of convolutional
    layers by applying it at different locations of the image, in a patch-wise fashion.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13. 通过在图像的不同位置以补丁方式应用给定的学习规则，可以将其扩展到卷积层的情况。
- en: 'Synaptic plasticity learning rules can be extended to the case of convolutional
    layers by applying them at different locations of the images, in a patch-wise
    fashion, as illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5\. Synaptic Plasticity
    Models in Deep Learning ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised
    Deep Learning: A Survey").'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '突触可塑性学习规则可以通过在图像的不同位置以补丁方式应用它们来扩展到卷积层的情况，如图[13](#S5.F13 "Figure 13 ‣ 5\. Synaptic
    Plasticity Models in Deep Learning ‣ Synaptic Plasticity Models and Bio-Inspired
    Unsupervised Deep Learning: A Survey")所示。'
- en: The idea of developing feature extractors for CNNs, based on pattern discovery
    mechanisms such as SC or clustering, was already used in the work from Coates
    et al. (Coates et al., [2011](#bib.bib46); Coates and Ng, [2011b](#bib.bib47),
    [a](#bib.bib45), [2012](#bib.bib48)). An SVM classifier stacked on top of multiple
    convolutional layers (up to 3) was able to achieve 82% accuracy on cifar-10 (Krizhevsky
    and Hinton, [2009](#bib.bib127)), 97% on NORB (LeCun et al., [2004](#bib.bib145)),
    72% con Caltech-101 (Fei-Fei et al., [2004](#bib.bib68)), 60% on STL-10 (Coates
    et al., [2011](#bib.bib46)). However, in order to achieve such results, these
    architectures require a significantly larger number of convolutional kernels (ranging
    from 1600 to 6000 in the proposed experiments) compared to traditional backprop-based
    architectures. For example, accuracy on CIFAR-10 drops to around 65% with k-means-based
    learning and 100 convolutional filters (Coates et al., [2011](#bib.bib46)). In
    these works, it has been shown how 1st layer neurons tend to develop simple filters
    such as edges with various orientations. This also happens in networks trained
    with backprop and in biological brains. Nonetheless, the computational power of
    Hebbian neural networks trained with competitive schemes is reduced by the fact
    that different neurons tend to learn shifted versions of the same filter. This
    creates a correlation in the coding scheme which is not detected over the channel
    dimension but over the height and width dimensions of the feature map. In order
    to overcome this problem, the authors of (Dundar et al., [2015](#bib.bib63)) introduced
    competition among neurons not only along the channel dimension but also with neighboring
    neurons in the height and width dimensions. The resulting 3-layer network achieved
    74.1% accuracy on the STL-10 (Coates et al., [2011](#bib.bib46)) dataset and 0.5%
    error rate on MNIST.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模式发现机制如SC或聚类开发CNN特征提取器的思想，已经在Coates等人的研究中使用过（Coates et al., [2011](#bib.bib46);
    Coates and Ng, [2011b](#bib.bib47), [a](#bib.bib45), [2012](#bib.bib48)）。在多个卷积层（最多3层）上叠加SVM分类器，能够在cifar-10上达到82%的准确率（Krizhevsky
    and Hinton, [2009](#bib.bib127)）、在NORB上达到97%（LeCun et al., [2004](#bib.bib145)）、在Caltech-101上达到72%（Fei-Fei
    et al., [2004](#bib.bib68)）、在STL-10上达到60%（Coates et al., [2011](#bib.bib46)）。然而，为了实现这些结果，这些架构需要比传统的基于反向传播的架构显著更多的卷积核（在提出的实验中范围从1600到6000）。例如，使用基于k-means的学习和100个卷积滤波器时，CIFAR-10上的准确率降至大约65%（Coates
    et al., [2011](#bib.bib46)）。在这些研究中，已显示出第一层神经元倾向于开发简单的滤波器，例如具有各种方向的边缘。这在使用反向传播训练的网络和生物大脑中也会发生。然而，使用竞争机制训练的Hebbian神经网络的计算能力会因不同的神经元倾向于学习相同滤波器的移位版本而减少。这在特征图的高度和宽度维度上创建了编码方案中的相关性，而不是在通道维度上检测到。为了克服这个问题，Dundar等人（[2015](#bib.bib63)）在通道维度之外，还引入了神经元之间在高度和宽度维度上的竞争。最终得到的3层网络在STL-10（Coates
    et al., [2011](#bib.bib46)）数据集上达到74.1%的准确率，在MNIST上达到0.5%的错误率。
- en: 'In (Wadhwa and Madhow, [2016b](#bib.bib234)), the authors propose a CNN architecture
    consisting of three convolutional layers, followed by an SVM classifier. The convolutional
    layers are trained, without supervision, to extract relevant features from the
    inputs. The proposed training algorithm, named Adaptive Hebbian Learning (AHL),
    combines Hebbian weight update with k-WTA, pre-synaptic competition (given two
    winning neurons $j$ and $k$, a pre-synaptic neuron $i$ and the connecting synaptic
    weights $w_{i,j}$ and $w_{i,k}$, only the highest between $w_{i,j}$ and $w_{i,k}$
    is updated) and dynamic recruiting/pruning of neurons. Additionally, a rule for
    learning bias terms, also used in previous works (Földiak, [1989](#bib.bib69)),
    is adopted: the bias term is important to balance the activations of different
    neurons; hence, the idea is to keep a running average of neuron activations as
    $r$, choose a target activation value $A_{bias}$ and increase or decrease the
    bias in order to make $r$ approach $A_{bias}$. The rule, which is biologically
    motivated by homeostatic mechanisms (Turrigiano, [2012](#bib.bib230)) for the
    stabilization of neural activity, is the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Wadhwa 和 Madhow，[2016b](#bib.bib234)）中，作者提出了一种由三层卷积层组成的 CNN 架构，后接一个 SVM 分类器。这些卷积层在没有监督的情况下进行训练，以从输入中提取相关特征。提出的训练算法名为自适应赫布学习（AHL），结合了赫布权重更新与
    k-WTA、突触前竞争（给定两个获胜的神经元 $j$ 和 $k$，一个突触前神经元 $i$ 以及连接的突触权重 $w_{i,j}$ 和 $w_{i,k}$，仅更新
    $w_{i,j}$ 和 $w_{i,k}$ 中的最高值）以及神经元的动态招募/修剪。此外，还采用了一种用于学习偏置项的规则，这也在之前的工作中使用过（Földiak，[1989](#bib.bib69)）：偏置项对平衡不同神经元的激活很重要；因此，想法是保持神经元激活的滚动平均值为
    $r$，选择一个目标激活值 $A_{bias}$ 并增加或减少偏置，以使 $r$ 接近 $A_{bias}$。这一规则是受到生物学上由稳态机制（Turrigiano，[2012](#bib.bib230)）对神经活动稳定化的启发，其规则如下：
- en: '| (37) |  | $\Delta b=\eta\,(r-A_{bias})$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $\Delta b=\eta\,(r-A_{bias})$ |  |'
- en: The authors applied these ideas to different image datasets and obtained an
    error rate of 0.65% on MNIST (LeCun et al., [1998](#bib.bib144)), an accuracy
    of 75.87% on CIFAR-10, and an error rate of 3.48% on NORB.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将这些思想应用于不同的图像数据集，并在 MNIST（LeCun 等，[1998](#bib.bib144)）上获得了 0.65% 的错误率，在 CIFAR-10
    上获得了 75.87% 的准确率，在 NORB 上获得了 3.48% 的错误率。
- en: Other works applied similar WTA-based approaches on fully connected networks.
    In (Krotov and Hopfield, [2019](#bib.bib128)), the authors achieved 98% accuracy
    on MNIST and 50% accuracy on CIFAR-10 with two fully-connected layers. In (Illing
    et al., [2019](#bib.bib104)) various Hebbian models for sparse coding, principal,
    and independent component analysis were compared, in similar settings as (Krotov
    and Hopfield, [2019](#bib.bib128)). ICA models achieved the best results, with
    53.9 % accuracy on CIFAR-10, and 98.8% mnist. PCA models achieved 50.8% accuracy
    on cifar-10, and 98.2% on MNIST, while SC achieved 50.2% accuracy on cifar-10,
    and 98.4% on MNIST. The HaH model derived from the similarity matching criterion
    (Pehlevan et al., [2015](#bib.bib193); Pehlevan and Chklovskii, [2015b](#bib.bib192))
    was applied to image classification tasks in (Bahroun and Soltoggio, [2017](#bib.bib14)),
    again on the CIFAR-10 dataset, achieving 80% accuracy with a single convolutional
    layer followed by an SVM classifier. Hebbian learning approaches were further
    investigated in(Miconi, [2021](#bib.bib171)), where it was shown that deeper network
    layers were not able to develop more abstract features without supervision. In
    order to achieve more complex feature representations, this work proposed to introduce
    sparsity in the weight configuration by pruning some selected weights. Experiments
    with a 3-layer CNN show 64% accuracy on CIFAR-10\. This work also introduced a
    loss function formulation of the learning rules investigated, which allows a more
    immediate integration with modern deep learning frameworks. The approaches in
    (Moraitis et al., [2021](#bib.bib174)), and layer (Journé et al., [2022](#bib.bib110)),
    use a soft-WTA training approach. In particular, the latter work shows for the
    first time increasing performance while going deeper with the number of layers,
    by resorting to very wide architectures where the number of neurons quadruples
    at each layer. It is shown that, as long as there are enough neurons, the network
    is able to disentangle the latent factors that describe the input, including those
    which provide the classification information. This work also contains experiments
    on ImageNet, but just for a single training epoch. A very recent work (Gupta et al.,
    [2022](#bib.bib86)) provides an interesting investigation of Hebbian learning
    algorithms compared to backprop, showing superior performance of the former in
    single epoch training and in contexts of data scarcity.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其他工作在全连接网络上应用了类似的WTA方法。在（Krotov 和 Hopfield, [2019](#bib.bib128)）中，作者在MNIST上达到了98%的准确率，在CIFAR-10上达到了50%的准确率，使用了两个全连接层。在（Illing
    等, [2019](#bib.bib104)）中，比较了各种用于稀疏编码、主成分分析和独立成分分析的Hebbian模型，与（Krotov 和 Hopfield,
    [2019](#bib.bib128)）中的设置类似。ICA模型取得了最佳结果，在CIFAR-10上准确率为53.9%，在MNIST上为98.8%。PCA模型在CIFAR-10上准确率为50.8%，在MNIST上为98.2%，而SC在CIFAR-10上准确率为50.2%，在MNIST上为98.4%。从相似性匹配准则（Pehlevan
    等, [2015](#bib.bib193); Pehlevan 和 Chklovskii, [2015b](#bib.bib192)）中衍生出的HaH模型被应用于（Bahroun
    和 Soltoggio, [2017](#bib.bib14)）的图像分类任务，再次使用CIFAR-10数据集，利用单个卷积层和SVM分类器达到了80%的准确率。Hebbian学习方法在（Miconi,
    [2021](#bib.bib171)）中得到了进一步的研究，结果显示更深的网络层在没有监督的情况下无法发展出更抽象的特征。为了实现更复杂的特征表示，该研究建议通过修剪一些选定的权重来引入权重配置的稀疏性。使用3层CNN的实验显示在CIFAR-10上的准确率为64%。该研究还引入了一种学习规则的损失函数公式，使其能够更直接地与现代深度学习框架集成。（Moraitis
    等, [2021](#bib.bib174)）和（Journé 等, [2022](#bib.bib110)）中的方法使用了soft-WTA训练方法。特别是，后者首次展示了在层数增加时性能提升，通过非常宽的架构，其中每层神经元数量增加四倍。结果表明，只要有足够的神经元，网络能够解开描述输入的潜在因子，包括那些提供分类信息的因子。这项工作还在ImageNet上进行了实验，但仅限于单次训练周期。一项非常近期的工作（Gupta
    等, [2022](#bib.bib86)）对Hebbian学习算法与反向传播进行了有趣的比较，显示出在单次训练周期和数据稀缺背景下，前者具有更优的表现。
- en: The above-mentioned approaches only applied synaptic plasticity models to relatively
    shallow network architectures (generally with 2-3 layers). A further step was
    taken in (Amato et al., [2019](#bib.bib9); Lagani, [2019](#bib.bib131); Lagani
    et al., [2021c](#bib.bib136), [2022b](#bib.bib137); Lagani, [2023](#bib.bib132)),
    where Hebbian WTA and PCA learning rules were investigated for training a 6-layer
    Convolutional Neural Network (CNN). Also, a supervised variant of Hebbian learning
    was proposed to train the final classification layer. Hybrid network models were
    also considered, in which some layers were trained using backprop and others using
    Hebbian learning. The results suggested that Hebbian learning is suitable for
    training early feature detectors, as well as higher network layers, but not very
    effective for training intermediate network layers. Furthermore, Hebbian learning
    was successfully used to retrain the higher layers of a pre-trained network, achieving
    results comparable to backprop, but requiring fewer training epochs, thus suggesting
    potential applications in the context of transfer learning (see also (Magotra
    and kim, [2019](#bib.bib157); Magotra and Kim, [2020](#bib.bib158); Canto, [2020](#bib.bib33))).
    Some contributions (Lagani et al., [2022a](#bib.bib133), [c](#bib.bib138)) showed
    promising results of unsupervised Hebbian algorithms for semi-supervised network
    training, in learning scenarios with scarce data availability, achieving superior
    results compared to other backprop-based unsupervised methods for semi-supervised
    training such as Variational Auto-Encoders (VAE) (Kingma and Welling, [2013](#bib.bib119)).
    In further developments (Lagani et al., [2022a](#bib.bib133), [c](#bib.bib138)),
    a more efficient formulation of Hebbian learning was also proposed, that enabled
    the scaling up of experiments to complex image recognition datasets, such as ImageNet
    (Deng et al., [2009](#bib.bib57)), large-scale image retrieval, and complex network
    architectures, improving training speed up to a factor of 50\. The solution, named
    FastHebb, leveraged some observations that allowed us to rewrite Hebbian update
    equations in terms of matrix multiplications, to better exploit GPU acceleration.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法仅将突触可塑性模型应用于相对较浅的网络结构（通常具有 2-3 层）。在 (Amato 等，[2019](#bib.bib9)；Lagani，[2019](#bib.bib131)；Lagani
    等，[2021c](#bib.bib136)，[2022b](#bib.bib137)；Lagani，[2023](#bib.bib132)）中，进一步研究了
    Hebbian WTA 和 PCA 学习规则用于训练 6 层卷积神经网络（CNN）。此外，还提出了一种监督式 Hebbian 学习变体，用于训练最终分类层。还考虑了混合网络模型，其中一些层使用反向传播训练，而其他层使用
    Hebbian 学习。结果表明，Hebbian 学习适用于训练早期特征检测器以及较高的网络层，但对训练中间网络层效果不佳。此外，Hebbian 学习成功用于重新训练预训练网络的较高层，取得了与反向传播相当的结果，但所需训练轮次较少，提示其在迁移学习中的潜在应用（参见
    (Magotra 和 kim，[2019](#bib.bib157)；Magotra 和 Kim，[2020](#bib.bib158)；Canto，[2020](#bib.bib33)））。一些贡献（Lagani
    等，[2022a](#bib.bib133)，[c](#bib.bib138)）显示了无监督 Hebbian 算法在半监督网络训练中的前景，特别是在数据稀缺的学习场景中，取得了比其他基于反向传播的无监督方法（如变分自编码器（VAE）（Kingma
    和 Welling，[2013](#bib.bib119)））更好的结果。在进一步发展中（Lagani 等，[2022a](#bib.bib133)，[c](#bib.bib138)），还提出了一种更高效的
    Hebbian 学习公式，使实验能够扩展到复杂的图像识别数据集，如 ImageNet（Deng 等，[2009](#bib.bib57)），大规模图像检索和复杂的网络结构，提高了训练速度，最高可提升
    50 倍。该解决方案名为 FastHebb，利用了一些观察结果，使我们能够将 Hebbian 更新方程重写为矩阵乘法形式，从而更好地利用 GPU 加速。
- en: Table 1. Experimental results of bio-inspired learning methods for deep learning
    applications on the CIFAR-10 dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 在 CIFAR-10 数据集上用于深度学习应用的生物启发学习方法的实验结果。
- en: '| Method | Description | CIFAR-10 Acc. (%) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 | CIFAR-10 准确率 (%) |'
- en: '| --- | --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| K-means features | 3 conv layers with thousands of filters. | 82% (Coates
    and Ng, [2012](#bib.bib48)) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| K-means 特征 | 3 个卷积层，具有数千个滤波器。 | 82%（Coates 和 Ng，[2012](#bib.bib48)） |'
- en: '| DHL | Competitive learning approach with 2 conv layers followed by SVM classifier,
    using label information to guide training. | 75.87% (Wadhwa and Madhow, [2016a](#bib.bib233))
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| DHL | 使用带有 2 个卷积层的竞争学习方法，随后是 SVM 分类器，利用标签信息指导训练。 | 75.87%（Wadhwa 和 Madhow，[2016a](#bib.bib233)）
    |'
- en: '| Similarity matching | HaH network based on the similarity matching criterion
    (Pehlevan et al., [2015](#bib.bib193)), with multi-scale filters, followed by
    an SVM classifier | 80% (Bahroun and Soltoggio, [2017](#bib.bib14)) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 相似性匹配 | 基于相似性匹配标准的 HaH 网络（Pehlevan 等，[2015](#bib.bib193)），具有多尺度滤波器，随后是 SVM
    分类器 | 80%（Bahroun 和 Soltoggio，[2017](#bib.bib14)） |'
- en: '| Krotov and Hopfield | Competitive learning approach with 2 fc layers. | 50%
    (Krotov and Hopfield, [2019](#bib.bib128)) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Krotov 和 Hopfield | 具有 2 个全连接层的竞争学习方法。 | 50% (Krotov 和 Hopfield, [2019](#bib.bib128))
    |'
- en: '| Shallow PCA | Analysis of bio-inspired methods on shallow networks: PCA on
    a single fc layer + final classifier. | 50.80% (Illing et al., [2019](#bib.bib104))
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 浅层 PCA | 对浅层网络中生物启发方法的分析：单一全连接层上的 PCA + 最终分类器。 | 50.80% (Illing et al., [2019](#bib.bib104))
    |'
- en: '| Shallow SC | Analysis of bio-inspired methods on shallow networks: PCA on
    a single fc layer + final classifier. | 50.20% (Illing et al., [2019](#bib.bib104))
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 浅层 SC | 对浅层网络中生物启发方法的分析：单一全连接层上的 PCA + 最终分类器。 | 50.20% (Illing et al., [2019](#bib.bib104))
    |'
- en: '| Shallow ICA | Analysis of bio-inspired methods on shallow networks: PCA on
    a single fc layer + final classifier. | 53.90% (Illing et al., [2019](#bib.bib104))
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 浅层 ICA | 对浅层网络中生物启发方法的分析：单一全连接层上的 PCA + 最终分类器。 | 53.90% (Illing et al., [2019](#bib.bib104))
    |'
- en: '| Hebbian learning with pruning | Objective function formulation of Hebbian
    approaches for gradient-based update computation. Hierarchical organization of
    Hebbian modules (3-layer CNN) with connection pruning and sparsification to induce
    more abstract features. | 64% (Miconi, [2021](#bib.bib171)) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Hebbian 学习与修剪 | Hebbian 方法的目标函数形式，用于基于梯度的更新计算。Hebbian 模块（3 层 CNN）的分层组织，连接修剪和稀疏化以引入更抽象的特征。
    | 64% (Miconi, [2021](#bib.bib171)) |'
- en: '| SoftHebb | Soft-WTA approach in deep CNNs (3 conv layers + final classifier).
    The number of filters is increased by a factor of 4 from each layer to the next,
    leading to very wide deep layers. | 80.31% (Journé et al., [2022](#bib.bib110))
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| SoftHebb | 在深度 CNN（3 个卷积层 + 最终分类器）中的 Soft-WTA 方法。过滤器的数量从每一层到下一层增加了 4 倍，导致非常宽的深层。
    | 80.31% (Journé et al., [2022](#bib.bib110)) |'
- en: '| FastHebb | Semi-supervised Hebbian-backprop training based on Hebbian WTA/PCA.
    | 85% (Lagani et al., [2021b](#bib.bib135), [a](#bib.bib134), [2022c](#bib.bib138))
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| FastHebb | 基于 Hebbian WTA/PCA 的半监督 Hebbian 反向传播训练。 | 85% (Lagani et al.,
    [2021b](#bib.bib135), [a](#bib.bib134), [2022c](#bib.bib138)) |'
- en: 'Tab. [1](#S5.T1 "Table 1 ‣ 5\. Synaptic Plasticity Models in Deep Learning
    ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")
    summarizes the main experimental results presented above on the CIFAR-10 dataset.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S5.T1 "Table 1 ‣ 5\. 突触可塑性模型在深度学习中的应用 ‣ 突触可塑性模型与生物启发的无监督深度学习：调查") 总结了上述
    CIFAR-10 数据集上的主要实验结果。
- en: 'We conclude this Section by mentioning some results related to the robustness
    properties of some bio-inspired models against adversarial perturbations (Szegedy
    et al., [2013](#bib.bib227); Akhtar and Mian, [2018](#bib.bib6); Yuan et al.,
    [2019](#bib.bib246)). Early studies on adversarial attacks and defenses (Goodfellow
    et al., [2014](#bib.bib83)) already noticed that Radial-Basis Function (RBF) networks
    exhibited strong robustness against adversarial settings (Vidnerová and Neruda,
    [2018](#bib.bib232); Zadeh et al., [2018](#bib.bib247); Goodfellow, [2017](#bib.bib82)).
    For example, a Gaussian RBF activation function has the form:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提及一些与生物启发模型在对抗扰动下的鲁棒性特性相关的结果来结束本节（Szegedy et al., [2013](#bib.bib227); Akhtar
    和 Mian, [2018](#bib.bib6); Yuan et al., [2019](#bib.bib246)）。早期关于对抗攻击和防御的研究（Goodfellow
    et al., [2014](#bib.bib83)）已经注意到，径向基函数（RBF）网络在对抗设置下表现出强大的鲁棒性（Vidnerová 和 Neruda,
    [2018](#bib.bib232); Zadeh et al., [2018](#bib.bib247); Goodfellow, [2017](#bib.bib82)）。例如，高斯
    RBF 激活函数的形式是：
- en: '| (38) |  | $y(\mathbf{x},\mathbf{w})=e^{-\frac{&#124;\mathbf{x}-\mathbf{w}&#124;^{2}}{2\sigma^{2}}}$
    |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $y(\mathbf{x},\mathbf{w})=e^{-\frac{|\mathbf{x}-\mathbf{w}|^{2}}{2\sigma^{2}}}$
    |'
- en: where the parameter $\sigma$ determines the width of the Gaussian, and the unit
    responds strongly only when the input $\mathbf{x}$ is within a certain distance
    from the reference weight vector $\mathbf{w}$. RBF-like activations could be biologically
    supported by frequency-dependent synaptic responses (Collingridge et al., [1988](#bib.bib49);
    Markram et al., [1998](#bib.bib165)). Although RBF networks are hard to train,
    due to gradient vanishing problems (Goodfellow et al., [2014](#bib.bib83)), gradient-free
    bio-inspired training methods could provide a useful mechanism to effectively
    leverage these types of models also in complex scenarios (Grossberg, [1976](#bib.bib85);
    Kohonen, [1982](#bib.bib121)). Lateral interaction and WTA-type nonlinearities
    have also proven useful to improve the adversarial robustness of DNN models (Kim
    et al., [2019](#bib.bib116); Xiao et al., [2019](#bib.bib241); Panousis et al.,
    [2021a](#bib.bib188), [b](#bib.bib189)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其中参数 $\sigma$ 决定了高斯的宽度，单位只有在输入 $\mathbf{x}$ 距离参考权重向量 $\mathbf{w}$ 在一定范围内时才会强烈响应。类似
    RBF 的激活可能通过频率依赖的突触响应得到生物学支持（Collingridge et al., [1988](#bib.bib49)；Markram et
    al., [1998](#bib.bib165)）。尽管由于梯度消失问题（Goodfellow et al., [2014](#bib.bib83)），RBF
    网络难以训练，但无梯度生物启发的训练方法可以提供有效利用这些类型模型的机制，即使在复杂场景中也能如此（Grossberg, [1976](#bib.bib85)；Kohonen,
    [1982](#bib.bib121)）。横向互动和 WTA 类型的非线性也被证明有助于提高 DNN 模型的对抗鲁棒性（Kim et al., [2019](#bib.bib116)；Xiao
    et al., [2019](#bib.bib241)；Panousis et al., [2021a](#bib.bib188), [b](#bib.bib189)）。
- en: 6\. Spiking Neural Networks
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 突触神经网络
- en: This Section introduces models of neural computation based on Spiking Neural
    Networks (SNNs) (Gerstner and Kistler, [2002](#bib.bib80)), which more faithfully
    resemble real neurons compared to traditional ANN models. We start by introducing
    the various neuron models for SNN simulation. We highlight the applications related
    to biological and neuromorphic computing, which are of strong practical interest
    thanks to the energy efficiency of the underlying computing paradigm, and we discuss
    the challenges related to SNN training. We describe the biological plasticity
    models for spiking neurons and the connections with Hebbian synaptic plasticity.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍基于突触神经网络（SNNs）（Gerstner 和 Kistler, [2002](#bib.bib80)）的神经计算模型，这些模型比传统的人工神经网络（ANN）模型更真实地模拟了实际神经元。我们首先介绍用于
    SNN 模拟的各种神经元模型。我们强调与生物计算和神经形态计算相关的应用，这些应用因其底层计算范式的能效而具有强烈的实际兴趣，并讨论了与 SNN 训练相关的挑战。我们描述了突触神经元的生物学可塑性模型以及与赫布突触可塑性的联系。
- en: 6.1\. Spiking Neuron Models
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 突触神经元模型
- en: Spiking Neural Networks (SNNs) are a realistic model of biological networks
    (Gerstner and Kistler, [2002](#bib.bib80); Maass, [1997](#bib.bib156)). While
    in traditional Artificial Neural Networks (ANNs), neurons communicate via real-valued
    signals, in SNNs they emit short pulses called spikes. All the spikes are equal
    to each other and values are encoded in the timing or in the frequency with which
    spikes are emitted.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 突触神经网络（SNNs）是生物网络的现实模型（Gerstner 和 Kistler, [2002](#bib.bib80)；Maass, [1997](#bib.bib156)）。在传统的人工神经网络（ANNs）中，神经元通过实值信号进行通信，而在
    SNNs 中，它们发出称为突触的短脉冲。所有突触都相等，值通过脉冲的时序或发射频率进行编码。
- en: 'Various spiking neuron models have been proposed in literature (Gerstner and
    Kistler, [2002](#bib.bib80)): from the classical neuron description due to Hodgkin
    and Huxley (HH) (Hodgkin and Huxley, [1952](#bib.bib93)), to more abstract but
    also computationally efficient models such as Izhikevich’s (Izhikevich, [2003](#bib.bib107)),
    Spike-Response Models (SRM) (Gerstner, [1995](#bib.bib78)), and Leaky Integrate
    and Fire (LIF) (Abbott and van Vreeswijk, [1993](#bib.bib2)). In particular, the
    LIF model is probably the highest-level description of spiking neurons, and also
    the most computationally efficient to simulate, which makes this model widely
    used in practice.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了各种突触神经元模型（Gerstner 和 Kistler, [2002](#bib.bib80)）：从 Hodgkin 和 Huxley（HH）（Hodgkin
    和 Huxley, [1952](#bib.bib93)）的经典神经元描述，到更抽象但也计算高效的模型，如 Izhikevich 的模型（Izhikevich,
    [2003](#bib.bib107)）、突触响应模型（SRM）（Gerstner, [1995](#bib.bib78)）和漏积分与发射（LIF）（Abbott
    和 van Vreeswijk, [1993](#bib.bib2)）。特别是，LIF 模型可能是突触神经元的最高级描述，也是模拟计算最有效的，这使得该模型在实际中被广泛使用。
- en: LIF neurons behave like integrators, summing up all the received spikes (weighted
    by the synaptic coefficients) until a threshold is exceeded. At this point, an
    output spike is emitted. In practice, this integration logic is implemented in
    terms of an electric potential that is accumulated on the neural membrane every
    time an input spike is received; when the threshold is reached and the output
    spike is released, the neural membrane discharges the accumulated potential and
    the process restarts. Immediately after the discharge, the neuron enters its refractory
    period, a time interval where it cannot spike regardless of its input. These units
    are leaky in the sense that, when no spikes are received in input, the membrane
    potential decays exponentially.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: LIF神经元表现得像积分器，汇总所有接收到的脉冲（由突触系数加权），直到超过阈值。在这一点上，输出脉冲被发射。实际上，这种积分逻辑是通过每次接收到输入脉冲时在神经膜上累积电位来实现的；当达到阈值并释放输出脉冲时，神经膜会释放累积的电位，过程重新开始。在放电之后，神经元进入其不应期，这是一个时间间隔，在此期间它不能产生脉冲，无论其输入如何。这些单元在没有接收到脉冲的情况下，膜电位会以指数方式衰减。
- en: 6.2\. Neuromorphic Computing
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 类脑计算
- en: Thanks to the spike-based communication paradigm, biological neurons are extremely
    efficient in terms of energy requirements (Javed et al., [2010](#bib.bib109)).
    Energy efficiency is an important issue in modern deep learning (Badar et al.,
    [2021](#bib.bib13)); hence, research is oriented toward different computing paradigms
    to support neural computation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于脉冲的通信范式，生物神经元在能源需求方面非常高效（Javed et al., [2010](#bib.bib109)）。能源效率是现代深度学习中的一个重要问题（Badar
    et al., [2021](#bib.bib13)）；因此，研究朝向不同的计算范式，以支持神经计算。
- en: 'Spiking neuron models represent a promising computing paradigm due to the possible
    applications in the implementation of computing hardware that reproduces the behavior
    of biological neurons in silico, with devices known as neuromorphic hardware (Roy
    et al., [2019](#bib.bib205); Zhu et al., [2020](#bib.bib251); Schuman et al.,
    [2022](#bib.bib214); Huynh et al., [2022](#bib.bib97); Shrestha et al., [2022](#bib.bib219)).
    By reproducing the spike-based computation in hardware, researchers were able
    to develop extremely energy-efficient neuromorphic chips (Gamrat et al., [2015](#bib.bib74);
    Wu et al., [2015](#bib.bib240)), such as Neurogrid (Benjamin et al., [2014](#bib.bib27)),
    TrueNorth (Merolla et al., [2014](#bib.bib168)), BrainScales (Schemmel et al.,
    [2010](#bib.bib213); Billaudelle et al., [2020](#bib.bib30)), Loihi (Davies et al.,
    [2018](#bib.bib54)). Despite the energy-efficient computing paradigm, SNN models
    have to face novel challenges, compared to traditional DNNs, related to the learning
    and optimization paradigms. In fact, traditional learning based on backprop is
    not adequate for SNN, because the spiking nonlinearity is not well suited for
    gradient-based optimization. Therefore, the following subsection is dedicated
    to the description of the counterpart model of Hebbian plasticity for SNNs: Spike
    Time Dependent Plasticity (STDP).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 脉冲神经元模型代表了一种有前景的计算范式，因为它可以用于实现能够在计算硬件中重现生物神经元行为的应用，这种设备被称为类脑硬件（Roy et al., [2019](#bib.bib205);
    Zhu et al., [2020](#bib.bib251); Schuman et al., [2022](#bib.bib214); Huynh et
    al., [2022](#bib.bib97); Shrestha et al., [2022](#bib.bib219)）。通过在硬件中重现基于脉冲的计算，研究人员能够开发出极其高效的类脑芯片（Gamrat
    et al., [2015](#bib.bib74); Wu et al., [2015](#bib.bib240)），如Neurogrid（Benjamin
    et al., [2014](#bib.bib27)）、TrueNorth（Merolla et al., [2014](#bib.bib168)）、BrainScales（Schemmel
    et al., [2010](#bib.bib213); Billaudelle et al., [2020](#bib.bib30)）、Loihi（Davies
    et al., [2018](#bib.bib54)）。尽管这种能源高效的计算范式存在，SNN模型仍需面对相对于传统DNN的新挑战，这些挑战涉及学习和优化范式。实际上，基于反向传播的传统学习方法并不适用于SNN，因为脉冲非线性不适合梯度优化。因此，以下小节专门描述了SNN的Hebbian可塑性对应模型：脉冲时间依赖可塑性（STDP）。
- en: 6.3\. Plasticity in SNNs
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. SNN中的可塑性
- en: 'In biological spiking neurons, learning occurs in the form of Spike Time Dependent
    Plasticity (STDP) (Bi and Poo, [1998](#bib.bib28); Song et al., [2000](#bib.bib221);
    Gerstner and Kistler, [2002](#bib.bib80)): when an input spike is received on
    a synapse and it is immediately followed by an output spike, then the weight on
    that synapse is increased. Specifically, a possible STDP rule can be expressed
    as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物脉冲神经元中，学习以脉冲时间依赖可塑性（STDP）的形式发生（Bi and Poo, [1998](#bib.bib28); Song et al.,
    [2000](#bib.bib221); Gerstner and Kistler, [2002](#bib.bib80)）：当输入脉冲在突触上被接收，并且随后立即发生输出脉冲时，那么该突触上的权重会增加。具体来说，一个可能的STDP规则可以表达如下：
- en: '| (39) |  | $\Delta w=\begin{cases}\eta^{+}\,e^{-&#124;\Delta t&#124;/\tau^{+}}&amp;\text{if
    $\Delta t>0$}\\ \eta^{-}\,e^{-&#124;\Delta t&#124;/\tau^{-}}&amp;\text{otherwise}\end{cases}$
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $\Delta w=\begin{cases}\eta^{+}\,e^{-&#124;\Delta t&#124;/\tau^{+}}&amp;\text{如果
    $\Delta t>0$}\\ \eta^{-}\,e^{-&#124;\Delta t&#124;/\tau^{-}}&amp;\text{否则}\end{cases}$
    |  |'
- en: 'where $w$ is the weight, $\Delta t$ is the time difference between the post-synaptic
    and the pre-synaptic spike, $\eta^{+}$ and $\eta^{-}$ are learning rate parameters
    ($\eta^{+}>0$ and $\eta^{-}<0$) and $\tau^{+}$ and $\tau^{-}$ are time constants.
    Weight strengthening occurs when a pre-synaptic spike has been a likely cause
    for a post-synaptic spike, hence pre-synaptic and post-synaptic activations are
    correlated. If instead, a pre-synaptic spike occurred right after a post-synaptic
    one, then the two activations are anti-correlated and a weight decrease occurs.
    In this perspective, STDP realizes the Hebbian principle in the context of spiking
    neurons. According to Eq. [39](#S6.E39 "In 6.3\. Plasticity in SNNs ‣ 6\. Spiking
    Neural Networks ‣ Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep
    Learning: A Survey") The dependency between $\Delta w$ and $\Delta t$ according
    to the STDP weight update rule follows a double exponential profile.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $w$ 是权重，$\Delta t$ 是突触后和突触前脉冲之间的时间差，$\eta^{+}$ 和 $\eta^{-}$ 是学习率参数（$\eta^{+}>0$
    和 $\eta^{-}<0$），$\tau^{+}$ 和 $\tau^{-}$ 是时间常数。当突触前脉冲很可能导致突触后脉冲时，会发生权重增强，因此突触前和突触后的激活是相关的。如果突触前脉冲紧接在突触后脉冲之后出现，那么这两种激活是反相关的，权重就会下降。从这个角度来看，STDP（时序依赖性突触可塑性）在脉冲神经元的背景下实现了希比安原则。根据方程
    [39](#S6.E39 "In 6.3\. Plasticity in SNNs ‣ 6\. Spiking Neural Networks ‣ Synaptic
    Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey")，$\Delta
    w$ 和 $\Delta t$ 之间的依赖关系根据 STDP 权重更新规则遵循双指数曲线。'
- en: 7\. Concluding Remarks
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: We wish to provide a conclusion to this survey with some final remarks about
    the limitations and potentials of the bio-inspired methods presented so far.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过一些关于目前介绍的生物启发方法的限制和潜力的最终评论，为这项调查提供结论。
- en: The main limitation of Hebbian and spiking models lies in the fact that their
    performance is not yet comparable to that of traditional DL approaches in terms
    of task performance. However, there exist several compelling factors that justify
    the study of biologically plausible learning models. Research on backprop-based
    models has witnessed extensive efforts, with the development of highly specialized
    hardware and solutions oriented to this type of optimization approach. Similarly,
    we anticipate a growth in efforts directed towards biologically plausible solutions
    in the near future, with an increasing interest in neuromorphic computing technologies
    (Roy et al., [2019](#bib.bib205); Zhu et al., [2020](#bib.bib251); Schuman et al.,
    [2022](#bib.bib214); Huynh et al., [2022](#bib.bib97); Shrestha et al., [2022](#bib.bib219)).
    At the same time, additional efforts may lead to promising results both in the
    refinement of bio-inspired algorithms and in their application to more complex
    network architectures, and this work hopes to stimulate further interest in this
    sense. Another disadvantage of biologically based models is related to the computational
    cost of simulating certain synaptic dynamics or unfolding the temporal evolution
    of complex neural circuitry. A recent work (Lagani et al., [2022c](#bib.bib138))
    addresses in part this problem by leveraging GPU parallelization more carefully.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 希比安（Hebbian）和脉冲神经网络（spiking）模型的主要限制在于它们的性能尚无法与传统深度学习（DL）方法在任务性能方面相比。然而，存在若干令人信服的因素，证明了生物学上可行的学习模型的研究是有必要的。基于反向传播的模型研究已见证了广泛的努力，包括高度专业化的硬件和针对这种优化方法的解决方案的开发。类似地，我们预计未来对生物学上可行的解决方案的努力将会增长，并且对神经形态计算技术的兴趣也会增加（Roy
    et al., [2019](#bib.bib205); Zhu et al., [2020](#bib.bib251); Schuman et al.,
    [2022](#bib.bib214); Huynh et al., [2022](#bib.bib97); Shrestha et al., [2022](#bib.bib219)）。与此同时，额外的努力可能会在生物启发算法的优化和对更复杂网络架构的应用方面取得令人鼓舞的结果，本研究希望能激发对此方面的进一步兴趣。生物学基础模型的另一个缺点与模拟某些突触动态或展开复杂神经电路的时间演变的计算成本有关。最近的研究（Lagani
    et al., [2022c](#bib.bib138)）部分解决了这一问题，通过更加仔细地利用GPU并行计算。
- en: A possible advantage of bio-inspired plasticity rules is locality, in the sense
    that each layer of neurons can perform an update without having to wait for the
    whole network to process the input (each layer is independent of the subsequent
    ones). This is well suited for highly parallelizable layerwise training. Additionally,
    local plasticity rules do not require gradient computations, which could make
    it easier to train deeper architectures without worrying about gradient vanishing
    problems; a possible application would be, for instance, efficient training of
    deep Radial Basis Function (RBF) networks, which are of great interest for their
    adversarial robustness properties (Goodfellow et al., [2014](#bib.bib83)), and
    whose computing model is biologically grounded (Collingridge et al., [1988](#bib.bib49);
    Markram et al., [1998](#bib.bib165)). Training RBF networks with backpropagation
    is challenging because the gradient vanishes quickly when going far from the center
    of the RBF kernel. Therefore, an effective gradient-free alternative training
    approach would be helpful in these scenarios.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 生物启发的可塑性规则的一个可能优势是局部性，即每一层神经元可以在不必等待整个网络处理输入的情况下进行更新（每一层独立于后续层）。这非常适合高度并行的逐层训练。此外，本地可塑性规则不需要梯度计算，这可能使得训练更深层次的架构变得更加容易，而无需担心梯度消失问题；一个可能的应用是，例如，高效训练深度径向基函数（RBF）网络，这些网络因其对抗鲁棒性而备受关注（Goodfellow
    et al., [2014](#bib.bib83)），其计算模型是生物学上有依据的（Collingridge et al., [1988](#bib.bib49);
    Markram et al., [1998](#bib.bib165)）。使用反向传播训练 RBF 网络具有挑战性，因为当远离 RBF 核心时梯度迅速消失。因此，在这些情况下，一个有效的无梯度替代训练方法将是有帮助的。
- en: A key aspect of SNN and STDP models is the possibility of realizing energy-efficient
    neural network implementations in neuromorphic hardware, which could also find
    applications in embedded devices. Towards SNN training, where the backpropagation
    algorithm is not directly applicable, exploration of alternatives to backprop
    training, inspired for instance by biological plasticity mechanisms, represents
    a promising direction.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: SNN 和 STDP 模型的一个关键方面是能够在神经形态硬件中实现节能的神经网络，这也可能在嵌入式设备中找到应用。在 SNN 训练方面，由于反向传播算法无法直接应用，因此探索替代反向传播训练的方法，例如受生物可塑性机制启发，代表了一条有前途的方向。
- en: Finally, neuroscience and engineering fields can mutually influence each other,
    as neuroscience can provide engineers with valuable inspiration for the design
    of AI solutions, and, in turn, technological advances can give insights to neuroscientists
    about what to look for in biological systems. Indeed, research efforts focused
    on the formulation of an algorithmic theory of the brain and further investigation
    of biologically plausible learning models are important to finally achieve a deeper
    understanding of how the human brain works, which could open possibilities of
    further advances both in technological and in medical fields (Lagani et al., [2021d](#bib.bib139);
    Markram et al., [2012](#bib.bib166)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，神经科学和工程领域可以相互影响，因为神经科学可以为工程师提供有价值的 AI 解决方案设计灵感，而技术进步则可以为神经科学家提供关于生物系统观察内容的见解。确实，专注于大脑算法理论的制定和生物学上可行的学习模型的进一步研究，对于最终实现对人脑工作原理的深入理解是重要的，这可能为技术和医疗领域的进一步进展打开了可能性（Lagani
    et al., [2021d](#bib.bib139); Markram et al., [2012](#bib.bib166)）。
- en: Acknowledgements.
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: 'This work was partially supported by:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到以下支持：
- en: '- Tuscany Health Ecosystem (THE) Project (CUP I53C22000780001), funded by the
    National Recovery and Resilience Plan (NRRP), within the NextGeneration Europe
    (NGEU) Program;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '- 托斯卡纳健康生态系统（THE）项目（CUP I53C22000780001），由国家复苏与韧性计划（NRRP）资助，属于 NextGeneration
    Europe（NGEU）计划；'
- en: '- Horizon Europe Research & Innovation Programme under Grant agreement N. 101092612
    (Social and hUman ceNtered XR - SUN project);'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '- Horizon Europe 研究与创新计划资助，资助协议号 101092612（社会和以人为本的 XR - SUN 项目）；'
- en: '- AI4Media project, funded by the European Commission (H2020 - Contract n.
    951911);'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '- AI4Media 项目，由欧洲委员会资助（H2020 - 合同号 951911）；'
- en: '- INAROS (INtelligenza ARtificiale per il mOnitoraggio e Supporto agli anziani)
    project co-funded by Tuscany Region POR FSE CUP B53D21008060008.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '- INAROS（用于监测和支持老年人的人工智能）项目由托斯卡纳地区 POR FSE CUP B53D21008060008 共同资助。'
- en: References
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abbott and van Vreeswijk (1993) LF Abbott and Carl van Vreeswijk. 1993. Asynchronous
    states in networks of pulse-coupled oscillators. *Physical Review E* 48, 2 (1993),
    1483.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbott 和 van Vreeswijk（1993）LF Abbott 和 Carl van Vreeswijk。1993年。《Asynchronous
    states in networks of pulse-coupled oscillators》。*Physical Review E* 48, 2（1993），1483。
- en: 'Agrawal et al. (2014) Pulkit Agrawal, Ross Girshick, and Jitendra Malik. 2014.
    Analyzing the performance of multilayer neural networks for object recognition.
    In *Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part VII 13*. Springer, 329–344.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agrawal 等（2014）Pulkit Agrawal, Ross Girshick 和 Jitendra Malik。2014年。《Analyzing
    the performance of multilayer neural networks for object recognition》。发表于 *Computer
    Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
    2014, Proceedings, Part VII 13*。Springer，329–344。'
- en: 'Aharon et al. (2006) Michal Aharon, Michael Elad, and Alfred Bruckstein. 2006.
    K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation.
    *IEEE Transactions on signal processing* 54, 11 (2006), 4311–4322.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aharon 等（2006）Michal Aharon, Michael Elad 和 Alfred Bruckstein。2006年。《K-SVD:
    An algorithm for designing overcomplete dictionaries for sparse representation》。*IEEE
    Transactions on signal processing* 54, 11（2006），4311–4322。'
- en: Akaho (2002) Shotaro Akaho. 2002. Conditionally independent component analysis
    for supervised feature extraction. *Neurocomputing* 49, 1-4 (2002), 139–150.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akaho（2002）Shotaro Akaho。2002年。《Conditionally independent component analysis
    for supervised feature extraction》。*Neurocomputing* 49, 1-4（2002），139–150。
- en: 'Akhtar and Mian (2018) Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial
    attacks on deep learning in computer vision: A survey. *IEEE Access* 6 (2018),
    14410–14430.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Akhtar 和 Mian（2018）Naveed Akhtar 和 Ajmal Mian。2018年。《Threat of adversarial
    attacks on deep learning in computer vision: A survey》。*IEEE Access* 6（2018），14410–14430。'
- en: Amari and Cichocki (1998) Shun-ichi Amari and Andrzej Cichocki. 1998. Adaptive
    blind signal processing-neural network approaches. *Proc. IEEE* 86, 10 (1998),
    2026–2048.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari 和 Cichocki（1998）Shun-ichi Amari 和 Andrzej Cichocki。1998年。《Adaptive blind
    signal processing-neural network approaches》。*Proc. IEEE* 86, 10（1998），2026–2048。
- en: Amari et al. (1996) Shun-ichi Amari, Andrzej Cichocki, and Howard Hua Yang.
    1996. A new learning algorithm for blind signal separation. In *Advances in neural
    information processing systems*. 757–763.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari 等（1996）Shun-ichi Amari, Andrzej Cichocki 和 Howard Hua Yang。1996年。《A new
    learning algorithm for blind signal separation》。发表于 *Advances in neural information
    processing systems*。757–763。
- en: Amato et al. (2019) Giuseppe Amato, Fabio Carrara, Fabrizio Falchi, Claudio
    Gennaro, and Gabriele Lagani. 2019. Hebbian Learning Meets Deep Convolutional
    Neural Networks. In *International Conference on Image Analysis and Processing*.
    Springer, 324–334.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amato 等（2019）Giuseppe Amato, Fabio Carrara, Fabrizio Falchi, Claudio Gennaro
    和 Gabriele Lagani。2019年。《Hebbian Learning Meets Deep Convolutional Neural Networks》。发表于
    *International Conference on Image Analysis and Processing*。Springer，324–334。
- en: 'Amato et al. (2023) Giuseppe Amato, Fabio Carrara, Fabrizio Falchi, Claudio
    Gennaro, and Gabriele Lagani. 2023. Spiking Neural Networks and Bio-Inspired Supervised
    Deep Learning: A Survey. (2023).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amato 等（2023）Giuseppe Amato, Fabio Carrara, Fabrizio Falchi, Claudio Gennaro
    和 Gabriele Lagani。2023年。《Spiking Neural Networks and Bio-Inspired Supervised Deep
    Learning: A Survey》。'
- en: Andrew et al. (2013) Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu.
    2013. Deep canonical correlation analysis. In *International conference on machine
    learning*. PMLR, 1247–1255.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew 等（2013）Galen Andrew, Raman Arora, Jeff Bilmes 和 Karen Livescu。2013年。《Deep
    canonical correlation analysis》。发表于 *International conference on machine learning*。PMLR，1247–1255。
- en: Averbeck et al. (2006) Bruno B Averbeck, Peter E Latham, and Alexandre Pouget.
    2006. Neural correlations, population coding and computation. *Nature reviews
    neuroscience* 7, 5 (2006), 358–366.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Averbeck 等（2006）Bruno B Averbeck, Peter E Latham 和 Alexandre Pouget。2006年。《Neural
    correlations, population coding and computation》。*Nature reviews neuroscience*
    7, 5（2006），358–366。
- en: Badar et al. (2021) Ahmed Badar, Arnav Varma, Adrian Staniec, Mahmoud Gamal,
    Omar Magdy, Haris Iqbal, Elahe Arani, and Bahram Zonooz. 2021. Highlighting the
    importance of reducing research bias and carbon emissions in cnns. In *International
    Conference of the Italian Association for Artificial Intelligence*. Springer,
    515–531.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badar 等（2021）Ahmed Badar, Arnav Varma, Adrian Staniec, Mahmoud Gamal, Omar Magdy,
    Haris Iqbal, Elahe Arani 和 Bahram Zonooz。2021年。《Highlighting the importance of
    reducing research bias and carbon emissions in cnns》。发表于 *International Conference
    of the Italian Association for Artificial Intelligence*。Springer，515–531。
- en: Bahroun and Soltoggio (2017) Yanis Bahroun and Andrea Soltoggio. 2017. Online
    representation learning with single and multi-layer Hebbian networks for image
    classification. In *International Conference on Artificial Neural Networks*. Springer,
    354–363.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahroun 和 Soltoggio（2017）Yanis Bahroun 和 Andrea Soltoggio。2017年。《Online representation
    learning with single and multi-layer Hebbian networks for image classification》。发表于
    *International Conference on Artificial Neural Networks*。Springer，354–363。
- en: Bair et al. (2006) Eric Bair, Trevor Hastie, Debashis Paul, and Robert Tibshirani.
    2006. Prediction by supervised principal components. *J. Amer. Statist. Assoc.*
    101, 473 (2006), 119–137.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bair 等 (2006) Eric Bair, Trevor Hastie, Debashis Paul, 和 Robert Tibshirani.
    2006. 通过有监督主成分进行预测。*美国统计学会杂志* 101, 473 (2006), 119–137。
- en: 'Baltrušaitis et al. (2018) Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe
    Morency. 2018. Multimodal machine learning: A survey and taxonomy. *IEEE transactions
    on pattern analysis and machine intelligence* 41, 2 (2018), 423–443.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baltrušaitis 等 (2018) Tadas Baltrušaitis, Chaitanya Ahuja, 和 Louis-Philippe
    Morency. 2018. 多模态机器学习：综述与分类。*IEEE 模式分析与机器智能汇刊* 41, 2 (2018), 423–443。
- en: 'Bardes et al. (2021) Adrien Bardes, Jean Ponce, and Yann LeCun. 2021. VICReg:
    Variance-Invariance-Covariance Regularization for Self-Supervised Learning. In
    *International Conference on Learning Representations*.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bardes 等 (2021) Adrien Bardes, Jean Ponce, 和 Yann LeCun. 2021. VICReg：用于自监督学习的方差-不变性-协方差正则化。在
    *国际学习表征会议* 上发表。
- en: 'Barshan et al. (2011) Elnaz Barshan, Ali Ghodsi, Zohreh Azimifar, and Mansoor Zolghadri
    Jahromi. 2011. Supervised principal component analysis: Visualization, classification
    and regression on subspaces and submanifolds. *Pattern Recognition* 44, 7 (2011),
    1357–1371.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barshan 等 (2011) Elnaz Barshan, Ali Ghodsi, Zohreh Azimifar, 和 Mansoor Zolghadri
    Jahromi. 2011. 有监督主成分分析：子空间和子流形上的可视化、分类和回归。*模式识别* 44, 7 (2011), 1357–1371。
- en: Bear (1996) Mark F Bear. 1996. A synaptic basis for memory storage in the cerebral
    cortex. *Proceedings of the National Academy of Sciences* 93, 24 (1996), 13453–13459.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bear (1996) Mark F Bear. 1996. 大脑皮层中记忆存储的突触基础。*美国国家科学院院刊* 93, 24 (1996), 13453–13459。
- en: Beck and Teboulle (2009) Amir Beck and Marc Teboulle. 2009. A fast iterative
    shrinkage-thresholding algorithm for linear inverse problems. *SIAM journal on
    imaging sciences* 2, 1 (2009), 183–202.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beck 和 Teboulle (2009) Amir Beck 和 Marc Teboulle. 2009. 一种用于线性逆问题的快速迭代收缩阈值算法。*SIAM
    成像科学期刊* 2, 1 (2009), 183–202。
- en: 'Becker (1996) Suzanna Becker. 1996. Mutual information maximization: models
    of cortical self-organization. *Network: Computation in neural systems* 7, 1 (1996),
    7–31.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Becker (1996) Suzanna Becker. 1996. 互信息最大化：皮层自组织模型。*神经系统中的网络计算* 7, 1 (1996),
    7–31。
- en: Becker and Plumbley (1996) Suzanna Becker and Mark Plumbley. 1996. Unsupervised
    neural network learning procedures for feature extraction and classification.
    *Applied Intelligence* 6, 3 (1996), 185–203.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Becker 和 Plumbley (1996) Suzanna Becker 和 Mark Plumbley. 1996. 无监督神经网络学习过程用于特征提取和分类。*应用智能*
    6, 3 (1996), 185–203。
- en: Belkin and Niyogi (2003) Mikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps
    for dimensionality reduction and data representation. *Neural computation* 15,
    6 (2003), 1373–1396.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin 和 Niyogi (2003) Mikhail Belkin 和 Partha Niyogi. 2003. 用于降维和数据表示的拉普拉斯特征映射。*神经计算*
    15, 6 (2003), 1373–1396。
- en: Bell and Sejnowski (1995) Anthony J Bell and Terrence J Sejnowski. 1995. An
    information-maximization approach to blind separation and blind deconvolution.
    *Neural computation* 7, 6 (1995), 1129–1159.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bell 和 Sejnowski (1995) Anthony J Bell 和 Terrence J Sejnowski. 1995. 信息最大化方法在盲源分离和盲去卷积中的应用。*神经计算*
    7, 6 (1995), 1129–1159。
- en: Belouchrani et al. (1997) Adel Belouchrani, Karim Abed-Meraim, J-F Cardoso,
    and Eric Moulines. 1997. A blind source separation technique using second-order
    statistics. *IEEE Transactions on signal processing* 45, 2 (1997), 434–444.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belouchrani 等 (1997) Adel Belouchrani, Karim Abed-Meraim, J-F Cardoso, 和 Eric
    Moulines. 1997. 一种使用二阶统计量的盲源分离技术。*IEEE 信号处理汇刊* 45, 2 (1997), 434–444。
- en: Bengio et al. (2015) Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard,
    and Zhouhan Lin. 2015. Towards biologically plausible deep learning. *arXiv preprint
    arXiv:1502.04156* (2015).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 (2015) Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard,
    和 Zhouhan Lin. 2015. 朝着生物学上合理的深度学习方向发展。*arXiv 预印本 arXiv:1502.04156* (2015)。
- en: 'Benjamin et al. (2014) Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh
    Choudhary, Anand R Chandrasekaran, Jean-Marie Bussat, Rodrigo Alvarez-Icaza, John V
    Arthur, Paul A Merolla, and Kwabena Boahen. 2014. Neurogrid: A mixed-analog-digital
    multichip system for large-scale neural simulations. *Proc. IEEE* 102, 5 (2014),
    699–716.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benjamin 等 (2014) Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh Choudhary,
    Anand R Chandrasekaran, Jean-Marie Bussat, Rodrigo Alvarez-Icaza, John V Arthur,
    Paul A Merolla, 和 Kwabena Boahen. 2014. Neurogrid：一种用于大规模神经模拟的混合模拟-数字多芯片系统。*IEEE
    会议录* 102, 5 (2014), 699–716。
- en: 'Bi and Poo (1998) Guo-qiang Bi and Mu-ming Poo. 1998. Synaptic modifications
    in cultured hippocampal neurons: dependence on spike timing, synaptic strength,
    and postsynaptic cell type. *Journal of neuroscience* 18, 24 (1998), 10464–10472.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 和 Poo (1998) Guo-qiang Bi 和 Mu-ming Poo. 1998. 培养的海马神经元中的突触修饰：对尖峰时序、突触强度和突触后细胞类型的依赖。*神经科学杂志*
    18, 24 (1998), 10464–10472。
- en: 'Bienenstock et al. (1982) Elie L Bienenstock, Leon N Cooper, and Paul W Munro.
    1982. Theory for the development of neuron selectivity: orientation specificity
    and binocular interaction in visual cortex. *Journal of Neuroscience* 2, 1 (1982),
    32–48.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bienenstock 等 (1982) Elie L Bienenstock, Leon N Cooper, 和 Paul W Munro. 1982.
    神经元选择性发展的理论：视觉皮层中的方向特异性和双眼交互。*神经科学杂志* 2, 1 (1982), 32–48。
- en: Billaudelle et al. (2020) Sebastian Billaudelle, Yannik Stradmann, Korbinian
    Schreiber, Benjamin Cramer, Andreas Baumbach, Dominik Dold, Julian Göltz, Akos F
    Kungl, Timo C Wunderlich, Andreas Hartel, et al. 2020. Versatile emulation of
    spiking neural networks on an accelerated neuromorphic substrate. In *2020 IEEE
    International Symposium on Circuits and Systems (ISCAS)*. IEEE, 1–5.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Billaudelle 等 (2020) Sebastian Billaudelle, Yannik Stradmann, Korbinian Schreiber,
    Benjamin Cramer, Andreas Baumbach, Dominik Dold, Julian Göltz, Akos F Kungl, Timo
    C Wunderlich, Andreas Hartel, 等. 2020. 在加速神经形态基底上多功能仿真尖峰神经网络。在*2020 IEEE 国际电路与系统研讨会
    (ISCAS)*。IEEE，1–5。
- en: Bressan and Vitrià (2002) Marco Bressan and Jordi Vitrià. 2002. Improving naive
    Bayes using class-conditional ICA. In *Ibero-American Conference on Artificial
    Intelligence*. Springer, 1–10.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bressan 和 Vitrià (2002) Marco Bressan 和 Jordi Vitrià. 2002. 使用类条件 ICA 改进朴素贝叶斯。在*伊比利亚美洲人工智能会议*。Springer，1–10。
- en: Brito and Gerstner (2016) Carlos SN Brito and Wulfram Gerstner. 2016. Nonlinear
    Hebbian learning as a unifying principle in receptive field formation. *PLoS computational
    biology* 12, 9 (2016), e1005070.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brito 和 Gerstner (2016) Carlos SN Brito 和 Wulfram Gerstner. 2016. 非线性 Hebbian
    学习作为接收场形成中的统一原则。*PLoS 计算生物学* 12, 9 (2016), e1005070。
- en: Canto (2020) Fernando Javier Aguilar Canto. 2020. Convolutional Neural Networks
    with Hebbian-Based Rules in Online Transfer Learning. In *Mexican International
    Conference on Artificial Intelligence*. Springer, 35–49.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Canto (2020) Fernando Javier Aguilar Canto. 2020. 基于 Hebbian 规则的卷积神经网络在在线迁移学习中的应用。在*墨西哥国际人工智能会议*。Springer，35–49。
- en: Cardoso (1997) J-F Cardoso. 1997. Infomax and maximum likelihood for blind source
    separation. *IEEE Signal processing letters* 4, 4 (1997), 112–114.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cardoso (1997) J-F Cardoso. 1997. Infomax 和最大似然用于盲源分离。*IEEE 信号处理信函* 4, 4 (1997),
    112–114。
- en: Cardoso (2001) Jean-François Cardoso. 2001. The three easy routes to independent
    component analysis; contrasts and geometry. In *Proc. ICA*, Vol. 2001\. 1–6.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cardoso (2001) Jean-François Cardoso. 2001. 进入独立成分分析的三条简单途径；对比和几何。 在*Proc. ICA*，Vol.
    2001. 1–6。
- en: Cardoso (2003) Jean-François Cardoso. 2003. Dependence, correlation and gaussianity
    in independent component analysis. *Journal of Machine Learning Research* 4, Dec
    (2003), 1177–1203.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cardoso (2003) Jean-François Cardoso. 2003. 独立成分分析中的依赖、相关性和高斯性。*机器学习研究杂志* 4,
    Dec (2003), 1177–1203。
- en: Cardoso and Laheld (1996) J-F Cardoso and Beate H Laheld. 1996. Equivariant
    adaptive source separation. *IEEE Transactions on signal processing* 44, 12 (1996),
    3017–3030.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cardoso 和 Laheld (1996) J-F Cardoso 和 Beate H Laheld. 1996. 等变自适应源分离。*IEEE 信号处理交易*
    44, 12 (1996), 3017–3030。
- en: 'Carrara et al. (2018) Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio
    Falchi, and Alejandro Moreo Fernández. 2018. Picture it in your mind: Generating
    high level visual representations from textual descriptions. *Information Retrieval
    Journal* 21 (2018), 208–229.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carrara 等 (2018) Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio Falchi,
    和 Alejandro Moreo Fernández. 2018. 在脑海中描绘：从文本描述生成高级视觉表示。*信息检索杂志* 21 (2018), 208–229。
- en: 'Chan et al. (2015) Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng,
    and Yi Ma. 2015. PCANet: A simple deep learning baseline for image classification?
    *IEEE transactions on image processing* 24, 12 (2015), 5017–5032.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan 等 (2015) Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng,
    和 Yi Ma. 2015. PCANet: 图像分类的简单深度学习基线？*IEEE 图像处理交易* 24, 12 (2015), 5017–5032。'
- en: Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. 2020. A simple framework for contrastive learning of visual representations.
    In *International conference on machine learning*. PMLR, 1597–1607.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, 和 Geoffrey Hinton.
    2020. 一种简单的对比学习视觉表示的框架。在*国际机器学习会议*。PMLR，1597–1607。
- en: Chien and Chen (2016) Jen-Tzung Chien and Ching-Huai Chen. 2016. Deep discriminative
    manifold learning. In *2016 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*. IEEE, 2672–2676.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chien 和 Chen（2016）Jen-Tzung Chien 和 Ching-Huai Chen. 2016. 深度判别流形学习。在*2016 IEEE国际声学、语音与信号处理会议（ICASSP）*中。IEEE,
    2672–2676。
- en: Choi (1998) Seungjin Choi. 1998. Differential Hebbian-type learning algorithms
    for decorrelation and independent component analysis. *Electronics Letters* 34,
    9 (1998), 900–900.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi（1998）Seungjin Choi. 1998. 用于去相关和独立成分分析的微分Hebbian类型学习算法。*电子快讯* 34, 9 (1998),
    900–900。
- en: Choi et al. (2002a) Seungjin Choi, Andrzej Cichocki, and Shunichi Amari. 2002a.
    Equivariant nonstationary source separation. *Neural networks* 15, 1 (2002), 121–130.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2002a）Seungjin Choi, Andrzej Cichocki 和 Shunichi Amari. 2002a. 等变非平稳源分离。*神经网络*
    15, 1 (2002), 121–130。
- en: Choi et al. (2002b) Seungjin Choi, Andrzej Cichocki, and Adel Beloucharni. 2002b.
    Second order nonstationary source separation. *Journal of VLSI signal processing
    systems for signal, image and video technology* 32, 1-2 (2002), 93–104.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi等（2002b）Seungjin Choi, Andrzej Cichocki 和 Adel Beloucharni. 2002b. 二阶非平稳源分离。*VLSI信号处理系统期刊：信号、图像和视频技术*
    32, 1-2 (2002), 93–104。
- en: Coates and Ng (2011a) Adam Coates and Andrew Ng. 2011a. Selecting receptive
    fields in deep networks. *Advances in neural information processing systems* 24
    (2011).
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coates 和 Ng（2011a）Adam Coates 和 Andrew Ng. 2011a. 深度网络中的感受野选择。*神经信息处理系统进展* 24
    (2011)。
- en: Coates et al. (2011) Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis
    of single-layer networks in unsupervised feature learning. In *Proceedings of
    the fourteenth international conference on artificial intelligence and statistics*.
    215–223.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coates 等（2011）Adam Coates, Andrew Ng 和 Honglak Lee. 2011. 单层网络在无监督特征学习中的分析。在*第十四届人工智能与统计国际会议*的会议记录中。215–223。
- en: Coates and Ng (2011b) Adam Coates and Andrew Y Ng. 2011b. The importance of
    encoding versus training with sparse coding and vector quantization. In *Proceedings
    of the 28th international conference on machine learning (ICML-11)*. 921–928.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coates 和 Ng（2011b）Adam Coates 和 Andrew Y Ng. 2011b. 稀疏编码与向量量化中编码与训练的重要性。在*第28届国际机器学习会议（ICML-11）*的会议记录中。921–928。
- en: 'Coates and Ng (2012) Adam Coates and Andrew Y Ng. 2012. Learning feature representations
    with k-means. In *Neural Networks: Tricks of the Trade: Second Edition*. Springer,
    561–580.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coates 和 Ng（2012）Adam Coates 和 Andrew Y Ng. 2012. 使用k均值学习特征表示。在*神经网络：交易技巧（第二版）*中。Springer,
    561–580。
- en: Collingridge et al. (1988) GL Collingridge, CE Herron, and RA Lester. 1988.
    Frequency-dependent N-methyl-D-aspartate receptor-mediated synaptic transmission
    in rat hippocampus. *The Journal of physiology* 399, 1 (1988), 301–312.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collingridge 等（1988）GL Collingridge, CE Herron 和 RA Lester. 1988. 大鼠海马体中频率依赖性N-甲基-D-天冬氨酸受体介导的突触传递。*生理学杂志*
    399, 1 (1988), 301–312。
- en: Comon (1994) Pierre Comon. 1994. Independent component analysis, a new concept?
    *Signal processing* 36, 3 (1994), 287–314.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comon（1994）Pierre Comon. 1994. 独立成分分析，新概念？*信号处理* 36, 3 (1994), 287–314。
- en: 'Comon et al. (1991) Pierre Comon, Christian Jutten, and Jeanny Herault. 1991.
    Blind separation of sources, Part II: Problems statement. *Signal processing*
    24, 1 (1991), 11–20.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comon 等（1991）Pierre Comon, Christian Jutten 和 Jeanny Herault. 1991. 源的盲分离，第二部分：问题陈述。*信号处理*
    24, 1 (1991), 11–20。
- en: Cottrell et al. (2018) Marie Cottrell, Madalina Olteanu, Fabrice Rossi, and
    Nathalie Villa-Vialaneix. 2018. Self-OrganizingMaps, theory and applications.
    *Revista de Investigacion Operacional* 39, 1 (2018), 1–22.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cottrell 等（2018）Marie Cottrell, Madalina Olteanu, Fabrice Rossi 和 Nathalie Villa-Vialaneix.
    2018. 自组织映射，理论与应用。*运筹学研究杂志* 39, 1 (2018), 1–22。
- en: Cox and Cox (2008) Michael AA Cox and Trevor F Cox. 2008. Multidimensional scaling.
    In *Handbook of data visualization*. Springer, 315–347.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cox 和 Cox（2008）Michael AA Cox 和 Trevor F Cox. 2008. 多维尺度分析。在*数据可视化手册*中。Springer,
    315–347。
- en: 'Davies et al. (2018) Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham
    Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil
    Imam, Shweta Jain, et al. 2018. Loihi: A neuromorphic manycore processor with
    on-chip learning. *Ieee Micro* 38, 1 (2018), 82–99.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davies等（2018）Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,
    Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta
    Jain 等. 2018. Loihi：一种具有芯片学习功能的类脑多核处理器。*IEEE微型计算机* 38, 1 (2018), 82–99。
- en: De Ridder et al. (2003) Dick De Ridder, Olga Kouropteva, Oleg Okun, Matti Pietikäinen,
    and Robert PW Duin. 2003. Supervised locally linear embedding. In *Artificial
    Neural Networks and Neural Information Processing—ICANN/ICONIP 2003*. Springer,
    333–341.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Ridder 等 (2003) Dick De Ridder、Olga Kouropteva、Oleg Okun、Matti Pietikäinen
    和 Robert PW Duin. 2003. 监督式局部线性嵌入。见 *人工神经网络与神经信息处理—ICANN/ICONIP 2003*. Springer,
    333–341.
- en: Demir and Ozmehmet (2005) Güleser Kalayci Demir and Kemal Ozmehmet. 2005. Online
    local learning algorithms for linear discriminant analysis. *Pattern Recognition
    Letters* 26, 4 (2005), 421–431.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demir 和 Ozmehmet (2005) Güleser Kalayci Demir 和 Kemal Ozmehmet. 2005. 用于线性判别分析的在线局部学习算法。*模式识别通讯*
    26, 4 (2005), 421–431.
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In *2009
    IEEE conference on computer vision and pattern recognition*. Ieee, 248–255.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2009) Jia Deng、Wei Dong、Richard Socher、Li-Jia Li、Kai Li 和 Li Fei-Fei.
    2009. Imagenet：一个大规模分层图像数据库。见 *2009 IEEE 计算机视觉与模式识别会议*. IEEE, 248–255.
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*. Association for Computational Linguistics,
    4171–4186.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等 (2019) Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova.
    2019. BERT：用于语言理解的深度双向变换器预训练。见 *2019年北美计算语言学协会会议论文集：人类语言技术，第1卷（长篇和短篇论文）*。计算语言学协会,
    4171–4186.
- en: Dhir and Lee (2011) Chandra Shekhar Dhir and Soo-Young Lee. 2011. Discriminant
    independent component analysis. *IEEE transactions on neural networks* 22, 6 (2011),
    845–857.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhir 和 Lee (2011) Chandra Shekhar Dhir 和 Soo-Young Lee. 2011. 判别独立成分分析。*IEEE
    神经网络交易* 22, 6 (2011), 845–857.
- en: Dorfer et al. (2015) Matthias Dorfer, Rainer Kelz, and Gerhard Widmer. 2015.
    Deep linear discriminant analysis. *arXiv preprint arXiv:1511.04707* (2015).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorfer 等 (2015) Matthias Dorfer、Rainer Kelz 和 Gerhard Widmer. 2015. 深度线性判别分析。*arXiv
    预印本 arXiv:1511.04707* (2015).
- en: Dorfer et al. (2016) Matthias Dorfer, Gerhard Widmer, and Gerhard Widmerajku
    At. 2016. Towards Deep and Discriminative Canonical Correlation Analysis. In *Proc.
    ICML Workshop on Multi-view Representaiton Learning*.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorfer 等 (2016) Matthias Dorfer、Gerhard Widmer 和 Gerhard Widmerajku At. 2016.
    朝向深度和判别典型相关分析。见 *ICML 多视角表示学习研讨会论文集*.
- en: Duan et al. (2021) Shiyu Duan, Shujian Yu, and José C Príncipe. 2021. Modularizing
    deep learning via pairwise learning with kernels. *IEEE Transactions on Neural
    Networks and Learning Systems* 33, 4 (2021), 1441–1451.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等 (2021) Shiyu Duan、Shujian Yu 和 José C Príncipe. 2021. 通过配对学习与核函数模块化深度学习。*IEEE
    神经网络与学习系统交易* 33, 4 (2021), 1441–1451.
- en: Dundar et al. (2015) Aysegul Dundar, Jonghoon Jin, and Eugenio Culurciello.
    2015. Convolutional clustering for unsupervised learning. *arXiv preprint arXiv:1511.06241*
    (2015).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dundar 等 (2015) Aysegul Dundar、Jonghoon Jin 和 Eugenio Culurciello. 2015. 用于无监督学习的卷积聚类。*arXiv
    预印本 arXiv:1511.06241* (2015).
- en: Elmadany et al. (2016) Nour El Din Elmadany, Yifeng He, and Ling Guan. 2016.
    Multiview learning via deep discriminative canonical correlation analysis. In
    *2016 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. IEEE, 2409–2413.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elmadany 等 (2016) Nour El Din Elmadany、Yifeng He 和 Ling Guan. 2016. 通过深度判别典型相关分析的多视角学习。见
    *2016 IEEE国际声学、语音和信号处理会议 (ICASSP)*. IEEE, 2409–2413.
- en: Erwin et al. (1991) Ed Erwin, Klaus Obermayer, and Klaus Schulten. 1991. Convergence
    properties of self-organizing maps.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erwin 等 (1991) Ed Erwin、Klaus Obermayer 和 Klaus Schulten. 1991. 自组织映射的收敛特性。
- en: 'Erwin et al. (1992a) Ed Erwin, Klaus Obermayer, and Klaus Schulten. 1992a.
    Self-organizing maps: ordering, convergence properties and energy functions. *Biological
    cybernetics* 67, 1 (1992), 47–55.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erwin 等 (1992a) Ed Erwin、Klaus Obermayer 和 Klaus Schulten. 1992a. 自组织映射：排序、收敛特性和能量函数。*生物网络学*
    67, 1 (1992), 47–55.
- en: 'Erwin et al. (1992b) Ed Erwin, Klaus Obermayer, and Klaus Schulten. 1992b.
    Self-organizing maps: Stationary states, metastability and convergence rate. *Biological
    Cybernetics* 67, 1 (1992), 35–45.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erwin 等 (1992b) Ed Erwin、Klaus Obermayer 和 Klaus Schulten. 1992b. 自组织映射：静态状态、亚稳定性和收敛速度。*生物网络学*
    67, 1 (1992), 35–45.
- en: 'Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learning
    generative visual models from few training examples: An incremental bayesian approach
    tested on 101 object categories. In *2004 conference on computer vision and pattern
    recognition workshop*. IEEE, 178–178.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei-Fei 等 (2004) Li Fei-Fei, Rob Fergus, 和 Pietro Perona. 2004. 从少量训练样本中学习生成视觉模型：一种在101个对象类别上测试的增量贝叶斯方法。见于*2004计算机视觉与模式识别研讨会*。IEEE,
    178–178。
- en: Földiak (1989) Peter Földiak. 1989. Adaptive network for optimal linear feature
    extraction. In *Proceedings of IEEE/INNS Int. Joint. Conf. Neural Networks*, Vol. 1.
    401–405.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Földiak (1989) Peter Földiak. 1989. 自适应网络用于最优线性特征提取。见于*IEEE/INNS 国际联合神经网络会议论文集*，第1卷，401–405。
- en: Földiak (1990) Peter Földiak. 1990. Forming sparse representations by local
    anti-Hebbian learning. *Biological cybernetics* 64, 2 (1990), 165–170.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Földiak (1990) Peter Földiak. 1990. 通过局部反Hebbian学习形成稀疏表示。*生物控制论* 64, 2 (1990),
    165–170。
- en: 'Fusi et al. (2000) Stefano Fusi, Mario Annunziato, Davide Badoni, Andrea Salamon,
    and Daniel J Amit. 2000. Spike-driven synaptic plasticity: theory, simulation,
    VLSI implementation. *Neural computation* 12, 10 (2000), 2227–2258.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fusi 等 (2000) Stefano Fusi, Mario Annunziato, Davide Badoni, Andrea Salamon,
    和 Daniel J Amit. 2000. 脉冲驱动的突触可塑性：理论、仿真、VLSI 实现。*神经计算* 12, 10 (2000), 2227–2258。
- en: Fyfe and Baddeley (1995) Colin Fyfe and Roland Baddeley. 1995. Non-linear data
    structure extraction using simple Hebbian networks. *Biological cybernetics* 72,
    6 (1995), 533–541.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fyfe 和 Baddeley (1995) Colin Fyfe 和 Roland Baddeley. 1995. 使用简单 Hebbian 网络的非线性数据结构提取。*生物控制论*
    72, 6 (1995), 533–541。
- en: Gabbott and Somogyi (1986) PLA Gabbott and P Somogyi. 1986. Quantitative distribution
    of GABA-immunoreactive neurons in the visual cortex (area 17) of the cat. *Experimental
    Brain Research* 61, 2 (1986), 323–331.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gabbott 和 Somogyi (1986) PLA Gabbott 和 P Somogyi. 1986. 猫视觉皮层 (17区) 中 GABA 免疫反应神经元的定量分布。*实验脑研究*
    61, 2 (1986), 323–331。
- en: Gamrat et al. (2015) C Gamrat, O Bichler, and D Roclin. 2015. Memristive based
    device arrays combined with Spike based coding can enable efficient implementations
    of embedded neuromorphic circuits. *2015 IEEE International Electron Devices Meeting
    (IEDM)* 2016 (2015), 4–5.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamrat 等 (2015) C Gamrat, O Bichler, 和 D Roclin. 2015. 基于忆阻器的器件阵列与基于脉冲的编码相结合可以实现嵌入式神经形态电路的高效实现。*2015
    IEEE 国际电子器件会议 (IEDM)* 2016 (2015), 4–5。
- en: Gao and Pavel (2017) Bolin Gao and Lacra Pavel. 2017. On the properties of the
    softmax function with application in game theory and reinforcement learning. *arXiv
    preprint arXiv:1704.00805* (2017).
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 和 Pavel (2017) Bolin Gao 和 Lacra Pavel. 2017. 软最大函数的性质及其在博弈论和强化学习中的应用。*arXiv
    预印本 arXiv:1704.00805* (2017)。
- en: Gatto and dos Santos (2017) Bernardo B Gatto and Eulanda M dos Santos. 2017.
    Discriminative canonical correlation analysis network for image classification.
    In *2017 IEEE International Conference on Image Processing (ICIP)*. IEEE, 4487–4491.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gatto 和 dos Santos (2017) Bernardo B Gatto 和 Eulanda M dos Santos. 2017. 用于图像分类的判别典型相关分析网络。见于*2017
    IEEE 国际图像处理会议 (ICIP)*。IEEE, 4487–4491。
- en: Geng et al. (2005) Xin Geng, De-Chuan Zhan, and Zhi-Hua Zhou. 2005. Supervised
    nonlinear dimensionality reduction for visualization and classification. *IEEE
    Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)* 35, 6 (2005),
    1098–1107.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng 等 (2005) Xin Geng, De-Chuan Zhan, 和 Zhi-Hua Zhou. 2005. 用于可视化和分类的监督非线性降维。*IEEE
    系统、管理和控制学报 B 部分 (控制论)* 35, 6 (2005), 1098–1107。
- en: Gerstner (1995) Wulfram Gerstner. 1995. Time structure of the activity in neural
    network models. *Physical review E* 51, 1 (1995), 738.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerstner (1995) Wulfram Gerstner. 1995. 神经网络模型中活动的时间结构。*物理评论 E* 51, 1 (1995),
    738。
- en: Gerstner et al. (1996) Wulfram Gerstner, Richard Kempter, J Leo Van Hemmen,
    and Hermann Wagner. 1996. A neuronal learning rule for sub-millisecond temporal
    coding. *Nature* 383, 6595 (1996), 76–78.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerstner 等 (1996) Wulfram Gerstner, Richard Kempter, J Leo Van Hemmen, 和 Hermann
    Wagner. 1996. 用于亚毫秒时间编码的神经学习规则。*自然* 383, 6595 (1996), 76–78。
- en: 'Gerstner and Kistler (2002) Wulfram Gerstner and Werner M Kistler. 2002. *Spiking
    neuron models: Single neurons, populations, plasticity*. Cambridge university
    press.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerstner 和 Kistler (2002) Wulfram Gerstner 和 Werner M Kistler. 2002. *脉冲神经元模型：单神经元、群体、可塑性*。剑桥大学出版社。
- en: 'Gerstner et al. (2018) Wulfram Gerstner, Marco Lehmann, Vasiliki Liakoni, Dane
    Corneil, and Johanni Brea. 2018. Eligibility traces and plasticity on behavioral
    time scales: experimental support of neohebbian three-factor learning rules. *Frontiers
    in neural circuits* 12 (2018), 53.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerstner 等 (2018) Wulfram Gerstner, Marco Lehmann, Vasiliki Liakoni, Dane Corneil,
    和 Johanni Brea. 2018. 行为时间尺度上的合格迹和可塑性：对新Hebbian三因素学习规则的实验支持。*神经回路前沿* 12 (2018),
    53。
- en: Goodfellow (2017) I. Goodfellow. 2017. CS231n Lecture 16 - Adversarial Examples
    and Adversarial Training. [https://youtu.be/CIfsB_EYsVI?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&t=2765](https://youtu.be/CIfsB_EYsVI?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&t=2765)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow (2017) I. Goodfellow. 2017. CS231n 第16讲 - 对抗样本与对抗训练。 [https://youtu.be/CIfsB_EYsVI?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&t=2765](https://youtu.be/CIfsB_EYsVI?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&t=2765)
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2014. Explaining and harnessing adversarial examples. *arXiv preprint arXiv:1412.6572*
    (2014).
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy.
    2014. 解释和利用对抗样本。*arXiv预印本 arXiv:1412.6572* (2014)。
- en: Gorchetchnikov et al. (2011) Anatoli Gorchetchnikov, Massimiliano Versace, Heather
    Ames, Ben Chandler, Jasmin Léveillé, Gennady Livitz, Ennio Mingolla, Greg Snider,
    Rick Amerson, Dick Carter, et al. 2011. Review and unification of learning framework
    in cog ex machina platform for memristive neuromorphic hardware. In *The 2011
    International Joint Conference on Neural Networks*. IEEE, 2601–2608.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gorchetchnikov et al. (2011) Anatoli Gorchetchnikov, Massimiliano Versace, Heather
    Ames, Ben Chandler, Jasmin Léveillé, Gennady Livitz, Ennio Mingolla, Greg Snider,
    Rick Amerson, Dick Carter, 等. 2011. 认知机器平台中学习框架的综述与统一，用于记忆电阻神经形态硬件。见 *2011年国际神经网络联合会议*。IEEE,
    2601–2608。
- en: 'Grossberg (1976) Stephen Grossberg. 1976. Adaptive pattern classification and
    universal recoding: I. Parallel development and coding of neural feature detectors.
    *Biological cybernetics* 23, 3 (1976), 121–134.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grossberg (1976) Stephen Grossberg. 1976. 自适应模式分类与通用编码：I. 神经特征检测器的并行发展与编码。*生物控制学*
    23, 3 (1976), 121–134。
- en: Gupta et al. (2022) Manas Gupta, Sarthak Ketanbhai Modi, Hang Zhang, Joon Hei
    Lee, and Joo Hwee Lim. 2022. Is Bio-Inspired Learning Better than Backprop? Benchmarking
    Bio Learning vs. Backprop. *arXiv preprint arXiv:2212.04614* (2022).
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta et al. (2022) Manas Gupta, Sarthak Ketanbhai Modi, Hang Zhang, Joon Hei
    Lee, 和 Joo Hwee Lim. 2022. 生物启发学习是否优于反向传播？生物学习与反向传播的基准测试。*arXiv预印本 arXiv:2212.04614*
    (2022)。
- en: Hassabis et al. (2017) Demis Hassabis, Dharshan Kumaran, Christopher Summerfield,
    and Matthew Botvinick. 2017. Neuroscience-inspired artificial intelligence. *Neuron*
    95, 2 (2017), 245–258.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassabis et al. (2017) Demis Hassabis, Dharshan Kumaran, Christopher Summerfield,
    和 Matthew Botvinick. 2017. 神经科学启发的人工智能。*Neuron* 95, 2 (2017), 245–258。
- en: Haykin (2009) Simon Haykin. 2009. *Neural networks and learning machines* (3
    ed.). Pearson.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haykin (2009) Simon Haykin. 2009. *神经网络与学习机器*（第3版）。Pearson。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2016.
    用于图像识别的深度残差学习。见 *IEEE计算机视觉与模式识别会议论文集*，770–778。
- en: He and Cichocki (2006a) Zhaoshui He and Andrzej Cichocki. 2006a. K-EVD clustering
    and its applications to sparse component analysis. In *International Conference
    on Independent Component Analysis and Signal Separation*. Springer, 90–97.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 Cichocki (2006a) Zhaoshui He 和 Andrzej Cichocki. 2006a. K-EVD 聚类及其在稀疏成分分析中的应用。见
    *国际独立成分分析与信号分离会议*。Springer, 90–97。
- en: He and Cichocki (2006b) Zhaoshui He and Andrzej Cichocki. 2006b. K-subspace
    clustering and its application in sparse component analysis. In *Proc. ESANN 2006*.
    Citeseer.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 Cichocki (2006b) Zhaoshui He 和 Andrzej Cichocki. 2006b. K-子空间聚类及其在稀疏成分分析中的应用。见
    *Proc. ESANN 2006*。Citeseer。
- en: Hénaff et al. (2019) Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali
    Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. 2019. Data-efficient
    image recognition with contrastive predictive coding. *arXiv preprint arXiv:1905.09272*
    (2019).
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hénaff et al. (2019) Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali
    Razavi, Carl Doersch, SM Eslami, 和 Aaron van den Oord. 2019. 使用对比预测编码的数据高效图像识别。*arXiv预印本
    arXiv:1905.09272* (2019)。
- en: Hodgkin and Huxley (1952) Alan L Hodgkin and Andrew F Huxley. 1952. A quantitative
    description of membrane current and its application to conduction and excitation
    in nerve. *The Journal of physiology* 117, 4 (1952), 500.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hodgkin 和 Huxley (1952) Alan L Hodgkin 和 Andrew F Huxley. 1952. 膜电流的定量描述及其在神经导电和兴奋中的应用。*生理学杂志*
    117, 4 (1952), 500。
- en: Hoffer and Ailon (2015) Elad Hoffer and Nir Ailon. 2015. Deep metric learning
    using triplet network. In *International Workshop on Similarity-Based Pattern
    Recognition*. Springer, 84–92.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffer 和 Ailon (2015) Elad Hoffer 和 Nir Ailon. 2015. 使用三元组网络的深度度量学习。见 *基于相似性的模式识别国际研讨会*。Springer,
    84–92。
- en: Hopfield (1982) John J Hopfield. 1982. Neural networks and physical systems
    with emergent collective computational abilities. *Proceedings of the national
    academy of sciences* 79, 8 (1982), 2554–2558.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield (1982) John J Hopfield. 1982. 神经网络和具有新兴集体计算能力的物理系统。*国家科学院学报* 79, 8
    (1982), 2554–2558。
- en: Hossain et al. (2019) MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin,
    and Hamid Laga. 2019. A comprehensive survey of deep learning for image captioning.
    *ACM Computing Surveys (CsUR)* 51, 6 (2019), 1–36.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hossain 等 (2019) MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, 和
    Hamid Laga. 2019. 深度学习在图像字幕生成中的综合调查。*ACM计算机调查 (CsUR)* 51, 6 (2019), 1–36。
- en: 'Huynh et al. (2022) Phu Khanh Huynh, M Lakshmi Varshika, Ankita Paul, Murat
    Isik, Adarsha Balaji, and Anup Das. 2022. Implementing spiking neural networks
    on neuromorphic architectures: A review. *arXiv preprint arXiv:2202.08897* (2022).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huynh 等 (2022) Phu Khanh Huynh, M Lakshmi Varshika, Ankita Paul, Murat Isik,
    Adarsha Balaji, 和 Anup Das. 2022. 在类脑架构上实现尖峰神经网络：综述。*arXiv预印本 arXiv:2202.08897*
    (2022)。
- en: Hyvärinen (1997) Aapo Hyvärinen. 1997. Independent component analysis by minimization
    of mutual information.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvärinen (1997) Aapo Hyvärinen. 1997. 通过最小化互信息进行独立成分分析。
- en: Hyvarinen et al. (2002) Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. 2002.
    Independent component analysis. *Studies in informatics and control* 11, 2 (2002),
    205–207.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvarinen 等 (2002) Aapo Hyvarinen, Juha Karhunen, 和 Erkki Oja. 2002. 独立成分分析。*信息与控制研究*
    11, 2 (2002), 205–207。
- en: Hyvarinen and Morioka (2016) Aapo Hyvarinen and Hiroshi Morioka. 2016. Unsupervised
    feature extraction by time-contrastive learning and nonlinear ICA. In *Advances
    in Neural Information Processing Systems*. 3765–3773.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvarinen 和 Morioka (2016) Aapo Hyvarinen 和 Hiroshi Morioka. 2016. 通过时间对比学习和非线性ICA进行无监督特征提取。在
    *神经信息处理系统进展*。3765–3773。
- en: Hyvarinen and Morioka (2017) AJ Hyvarinen and Hiroshi Morioka. 2017. Nonlinear
    ICA of temporally dependent stationary sources. In *Proceedings of Machine Learning
    Research*.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvarinen 和 Morioka (2017) AJ Hyvarinen 和 Hiroshi Morioka. 2017. 时间依赖的平稳源的非线性ICA。在
    *机器学习研究会议录*。
- en: Hyvärinen and Oja (1998) Aapo Hyvärinen and Erkki Oja. 1998. Independent component
    analysis by general nonlinear Hebbian-like learning rules. *signal processing*
    64, 3 (1998), 301–313.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvärinen 和 Oja (1998) Aapo Hyvärinen 和 Erkki Oja. 1998. 通过一般非线性Hebbian-like学习规则进行独立成分分析。*信号处理*
    64, 3 (1998), 301–313。
- en: Hyvarinen et al. (2019) Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner.
    2019. Nonlinear ICA using auxiliary variables and generalized contrastive learning.
    In *The 22nd International Conference on Artificial Intelligence and Statistics*.
    859–868.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvarinen 等 (2019) Aapo Hyvarinen, Hiroaki Sasaki, 和 Richard Turner. 2019. 使用辅助变量和广义对比学习的非线性ICA。在
    *第22届国际人工智能与统计会议*。859–868。
- en: Illing et al. (2019) Bernd Illing, Wulfram Gerstner, and Johanni Brea. 2019.
    Biologically plausible deep learning—But how far can we go with shallow networks?
    *Neural Networks* 118 (2019), 90–101.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Illing 等 (2019) Bernd Illing, Wulfram Gerstner, 和 Johanni Brea. 2019. 生物学上可行的深度学习——但我们能在浅层网络上走多远？*神经网络*
    118 (2019), 90–101。
- en: 'Intrator and Cooper (1992) Nathan Intrator and Leon N Cooper. 1992. Objective
    function formulation of the BCM theory of visual cortical plasticity: Statistical
    connections, stability conditions. *Neural Networks* 5, 1 (1992), 3–17.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Intrator 和 Cooper (1992) Nathan Intrator 和 Leon N Cooper. 1992. BCM理论视觉皮层可塑性的目标函数公式化：统计连接，稳定性条件。*神经网络*
    5, 1 (1992), 3–17。
- en: 'Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. 2015. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. *arXiv
    preprint arXiv:1502.03167* (2015).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe 和 Szegedy (2015) Sergey Ioffe 和 Christian Szegedy. 2015. 批量归一化：通过减少内部协变量转移来加速深度网络训练。*arXiv预印本
    arXiv:1502.03167* (2015)。
- en: Izhikevich (2003) Eugene M Izhikevich. 2003. Simple model of spiking neurons.
    *IEEE Transactions on neural networks* 14, 6 (2003), 1569–1572.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izhikevich (2003) Eugene M Izhikevich. 2003. 简单的尖峰神经元模型。*IEEE神经网络交易* 14, 6 (2003),
    1569–1572。
- en: Izhikevich (2007) Eugene M Izhikevich. 2007. Solving the distal reward problem
    through linkage of STDP and dopamine signaling. *Cerebral cortex* 17, 10 (2007),
    2443–2452.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izhikevich (2007) Eugene M Izhikevich. 2007. 通过STDP和多巴胺信号的关联解决远端奖励问题。*大脑皮层*
    17, 10 (2007), 2443–2452。
- en: 'Javed et al. (2010) Fahad Javed, Qing He, Lance E Davidson, John C Thornton,
    Jeanine Albu, Lawrence Boxt, Norman Krasnow, Marinos Elia, Patrick Kang, Stanley
    Heshka, et al. 2010. Brain and high metabolic rate organ mass: contributions to
    resting energy expenditure beyond fat-free mass. *The American journal of clinical
    nutrition* 91, 4 (2010), 907–912.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Javed et al. (2010) Fahad Javed, Qing He, Lance E Davidson, John C Thornton,
    Jeanine Albu, Lawrence Boxt, Norman Krasnow, Marinos Elia, Patrick Kang, Stanley
    Heshka 等. 2010. 大脑和高代谢率器官的质量：超越无脂质量对静息能量消耗的贡献。*美国临床营养学杂志* 91, 4 (2010)，907–912。
- en: Journé et al. (2022) Adrien Journé, Hector Garcia Rodriguez, Qinghai Guo, and
    Timoleon Moraitis. 2022. Hebbian deep learning without feedback. *arXiv preprint
    arXiv:2209.11883* (2022).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Journé et al. (2022) Adrien Journé, Hector Garcia Rodriguez, Qinghai Guo 和 Timoleon
    Moraitis. 2022. 无反馈的赫布深度学习。*arXiv预印本 arXiv:2209.11883* (2022)。
- en: 'Jutten and Herault (1991) Christian Jutten and Jeanny Herault. 1991. Blind
    separation of sources, part I: An adaptive algorithm based on neuromimetic architecture.
    *Signal processing* 24, 1 (1991), 1–10.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jutten 和 Herault (1991) Christian Jutten 和 Jeanny Herault. 1991. 源的盲分离，第一部分：基于神经仿生架构的自适应算法。*信号处理*
    24, 1 (1991)，1–10。
- en: Karhunen (1996) Juha Karhunen. 1996. Neural approaches to independent component
    analysis and source separation.. In *ESANN*, Vol. 96\. 249–266.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karhunen (1996) Juha Karhunen. 1996. 神经方法的独立成分分析和源分离。在 *ESANN*，第96卷。249–266。
- en: Karhunen and Joutsensalo (1995) Juha Karhunen and Jyrki Joutsensalo. 1995. Generalizations
    of principal component analysis, optimization problems, and neural networks. *Neural
    Networks* 8, 4 (1995), 549–562.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karhunen 和 Joutsensalo (1995) Juha Karhunen 和 Jyrki Joutsensalo. 1995. 主成分分析的泛化、优化问题和神经网络。*神经网络*
    8, 4 (1995)，549–562。
- en: Kaski et al. (2005) Samuel Kaski, Janne Sinkkonen, and Arto Klami. 2005. Discriminative
    clustering. *Neurocomputing* 69, 1-3 (2005), 18–41.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaski et al. (2005) Samuel Kaski, Janne Sinkkonen 和 Arto Klami. 2005. 判别聚类。*神经计算*
    69, 1-3 (2005)，18–41。
- en: Kaur et al. (2021) Parminder Kaur, Avleen Kaur Malhi, and Husanbir Singh Pannu.
    2021. Hybrid SOM based cross-modal retrieval exploiting Hebbian learning. *Knowledge-Based
    Systems* (2021), 108014.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur et al. (2021) Parminder Kaur, Avleen Kaur Malhi 和 Husanbir Singh Pannu.
    2021. 基于混合SOM的跨模态检索，利用赫布学习。*知识基系统* (2021)，108014。
- en: Kim et al. (2019) Edward Kim, Jessica Yarnall, Priya Shah, and Garrett T Kenyon.
    2019. A neuromorphic sparse coding defense to adversarial images. In *Proceedings
    of the International Conference on Neuromorphic Systems*. 1–8.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2019) Edward Kim, Jessica Yarnall, Priya Shah 和 Garrett T Kenyon.
    2019. 针对对抗图像的神经形态稀疏编码防御。在 *国际神经形态系统会议论文集*。1–8。
- en: Kim and Kittler (2005) Tae-Kyun Kim and Josef Kittler. 2005. Locally linear
    discriminant analysis for multimodally distributed classes for face recognition
    with a single model image. *IEEE transactions on pattern analysis and machine
    intelligence* 27, 3 (2005), 318–327.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Kittler (2005) Tae-Kyun Kim 和 Josef Kittler. 2005. 用于面部识别的局部线性判别分析，多模态分布类的单模型图像。*IEEE模式分析与机器智能期刊*
    27, 3 (2005)，318–327。
- en: Kim et al. (2007) Tae-Kyun Kim, Josef Kittler, and Roberto Cipolla. 2007. Discriminative
    learning and recognition of image set classes using canonical correlations. *IEEE
    Transactions on Pattern Analysis and Machine Intelligence* 29, 6 (2007), 1005–1018.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2007) Tae-Kyun Kim, Josef Kittler 和 Roberto Cipolla. 2007. 使用典型相关的图像集类的判别学习和识别。*IEEE模式分析与机器智能期刊*
    29, 6 (2007)，1005–1018。
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling (2013) Diederik P Kingma 和 Max Welling. 2013. 自编码变分贝叶斯。*arXiv预印本
    arXiv:1312.6114* (2013)。
- en: Koch et al. (2015) Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. 2015.
    Siamese neural networks for one-shot image recognition. In *ICML deep learning
    workshop*, Vol. 2\. Lille.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koch et al. (2015) Gregory Koch, Richard Zemel 和 Ruslan Salakhutdinov. 2015.
    用于一次性图像识别的孪生神经网络。在 *ICML深度学习研讨会*，第2卷。里尔。
- en: Kohonen (1982) Teuvo Kohonen. 1982. Self-organized formation of topologically
    correct feature maps. *Biological cybernetics* 43, 1 (1982), 59–69.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohonen (1982) Teuvo Kohonen. 1982. 自组织形成拓扑正确的特征图。*生物网络* 43, 1 (1982)，59–69。
- en: Kohonen (1993) Teuvo Kohonen. 1993. Things you haven’t heard about the Self-Organizing
    Map. *IEEE International Conference on Neural Networks* (1993), 1147–1156.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohonen (1993) Teuvo Kohonen. 1993. 关于自组织映射的未曾听闻之事。*IEEE国际神经网络会议* (1993)，1147–1156。
- en: Kolodziejski et al. (2009b) Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite,
    and Florentin Wörgötter. 2009b. On the asymptotic equivalence between differential
    Hebbian and temporal difference learning using a local third factor. In *Advances
    in Neural Information Processing Systems*. 857–864.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolodziejski 等（2009b）Christoph Kolodziejski、Bernd Porr、Minija Tamosiunaite 和
    Florentin Wörgötter。2009b。在使用局部第三因子的情况下，差分 Hebbian 学习与时间差分学习之间的渐近等价性。在 *《神经信息处理系统进展》*
    中，第 857–864 页。
- en: Kolodziejski et al. (2009a) Christoph Kolodziejski, Bernd Porr, and Florentin
    Wörgötter. 2009a. On the asymptotic equivalence between differential Hebbian and
    temporal difference learning. *Neural computation* 21, 4 (2009), 1173–1202.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolodziejski 等（2009a）Christoph Kolodziejski、Bernd Porr 和 Florentin Wörgötter。2009a。在差分
    Hebbian 学习与时间差分学习之间的渐近等价性。*《神经计算》* 21, 4（2009），第 1173–1202 页。
- en: Kosko (1986) Bart Kosko. 1986. Differential hebbian learning. In *AIP Conference
    proceedings*, Vol. 151\. American Institute of Physics, 277–282.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kosko（1986）Bart Kosko。1986。差分 Hebbian 学习。在 *《AIP 会议论文集》*，第 151 卷。美国物理学会，第 277–282
    页。
- en: Krause et al. (2010) Andreas Krause, Pietro Perona, and Ryan G Gomes. 2010.
    Discriminative clustering by regularized information maximization. In *Advances
    in neural information processing systems*. 775–783.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause 等（2010）Andreas Krause、Pietro Perona 和 Ryan G Gomes。2010。通过正则化信息最大化的判别聚类。在
    *《神经信息处理系统进展》* 中，第 775–783 页。
- en: Krizhevsky and Hinton (2009) Alex Krizhevsky and Geoffrey Hinton. 2009. Learning
    multiple layers of features from tiny images.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 和 Hinton（2009）Alex Krizhevsky 和 Geoffrey Hinton。2009。从微小图像中学习多层特征。
- en: Krotov and Hopfield (2019) Dmitry Krotov and John J Hopfield. 2019. Unsupervised
    learning by competing hidden units. *Proceedings of the National Academy of Sciences*
    116, 16 (2019), 7723–7731.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krotov 和 Hopfield（2019）Dmitry Krotov 和 John J Hopfield。2019。通过竞争隐藏单元进行无监督学习。*《美国国家科学院院刊》*
    116, 16（2019），第 7723–7731 页。
- en: Kubota et al. (2016) Yoshiyuki Kubota, Fuyuki Karube, Masaki Nomura, and Yasuo
    Kawaguchi. 2016. The diversity of cortical inhibitory synapses. *Frontiers in
    neural circuits* 10 (2016), 27.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubota 等（2016）Yoshiyuki Kubota、Fuyuki Karube、Masaki Nomura 和 Yasuo Kawaguchi。2016。皮层抑制性突触的多样性。*《神经回路前沿》*
    10（2016），第 27 页。
- en: Kung and Diamantaras (1990) Sun-Yuan Kung and KI Diamantaras. 1990. A neural
    network learning algorithm for adaptive principal component extraction (APEX).
    In *International Conference on Acoustics, Speech, and Signal Processing*. IEEE,
    861–864.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kung 和 Diamantaras（1990）Sun-Yuan Kung 和 KI Diamantaras。1990。一种用于自适应主成分提取（APEX）的神经网络学习算法。在
    *《国际声学、语音和信号处理会议》* 中。IEEE，第 861–864 页。
- en: Lagani (2019) Gabriele Lagani. 2019. *Hebbian learning algorithms for training
    convolutional neural networks*. Master’s thesis. School of Engineering, University
    of Pisa, Italy. [https://etd.adm.unipi.it/theses/available/etd-03292019-220853/](https://etd.adm.unipi.it/theses/available/etd-03292019-220853/)
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani（2019）Gabriele Lagani。2019。*《用于训练卷积神经网络的 Hebbian 学习算法》*。硕士论文。比萨大学工程学院，意大利。
    [https://etd.adm.unipi.it/theses/available/etd-03292019-220853/](https://etd.adm.unipi.it/theses/available/etd-03292019-220853/)
- en: 'Lagani (2023) Gabriele Lagani. 2023. *Bio-Inspired Approaches for Deep Learning:
    From Spiking Neural Networks to Hebbian Plasticity*. Ph. D. Dissertation. School
    of Engineering, University of Pisa, Italy. [https://etd.adm.unipi.it/t/etd-05022023-121539](https://etd.adm.unipi.it/t/etd-05022023-121539)'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani（2023）Gabriele Lagani。2023。*《深度学习的生物启发方法：从脉冲神经网络到 Hebbian 可塑性》*。博士论文。比萨大学工程学院，意大利。
    [https://etd.adm.unipi.it/t/etd-05022023-121539](https://etd.adm.unipi.it/t/etd-05022023-121539)
- en: Lagani et al. (2022a) Gabriele Lagani, Davide Bacciu, Claudio Gallicchio, Fabrizio
    Falchi, Claudio Gennaro, and Giuseppe Amato. 2022a. Deep Features for CBIR with
    Scarce Data using Hebbian Learning. *arXiv preprint arXiv:2205.08935* (2022).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani 等（2022a）Gabriele Lagani、Davide Bacciu、Claudio Gallicchio、Fabrizio Falchi、Claudio
    Gennaro 和 Giuseppe Amato。2022a。使用 Hebbian 学习进行稀疏数据的深度特征 CBIR。*arXiv 预印本 arXiv:2205.08935*（2022）。
- en: Lagani et al. (2021a) Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, and
    Giuseppe Amato. 2021a. Evaluating Hebbian Learning in a Semi-supervised Setting.
    In *International Conference on Machine Learning, Optimization, and Data Science*.
    Springer, 365–379.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani 等（2021a）Gabriele Lagani、Fabrizio Falchi、Claudio Gennaro 和 Giuseppe Amato。2021a。在半监督环境中评估
    Hebbian 学习。在 *《机器学习、优化与数据科学国际会议》* 中。Springer，第 365–379 页。
- en: Lagani et al. (2021b) Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, and
    Giuseppe Amato. 2021b. Hebbian semi-supervised learning in a sample efficiency
    setting. *Neural Networks* 143 (2021), 719–731.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani 等（2021b）Gabriele Lagani、Fabrizio Falchi、Claudio Gennaro 和 Giuseppe Amato。2021b。在样本效率设置中进行
    Hebbian 半监督学习。*《神经网络》* 143（2021），第 719–731 页。
- en: Lagani et al. (2021c) Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, and
    Giuseppe Amato. 2021c. Training Convolutional Neural Networks with Competitive
    Hebbian Learning Approaches. In *International Conference on Machine Learning,
    Optimization, and Data Science*. Springer, 25–40.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani et al. (2021c) Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, 和 Giuseppe
    Amato. 2021c. 使用竞争 Hebbian 学习方法训练卷积神经网络。在 *International Conference on Machine
    Learning, Optimization, and Data Science*。Springer, 25–40。
- en: Lagani et al. (2022b) Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, and
    Giuseppe Amato. 2022b. Comparing the performance of Hebbian against backpropagation
    learning using convolutional neural networks. *Neural Computing and Applications*
    34, 8 (2022), 6503–6519.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani et al. (2022b) Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, 和 Giuseppe
    Amato. 2022b. 比较 Hebbian 学习与反向传播学习在卷积神经网络中的性能。*Neural Computing and Applications*
    34, 8 (2022), 6503–6519。
- en: 'Lagani et al. (2022c) Gabriele Lagani, Claudio Gennaro, Hannes Fassold, and
    Giuseppe Amato. 2022c. FastHebb: Scaling Hebbian Training of Deep Neural Networks
    to ImageNet Level. In *Similarity Search and Applications: 15th International
    Conference, SISAP 2022, Bologna, Italy, October 5–7, 2022, Proceedings*. Springer,
    251–264.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lagani et al. (2022c) Gabriele Lagani, Claudio Gennaro, Hannes Fassold, 和 Giuseppe
    Amato. 2022c. FastHebb: 将 Hebbian 训练扩展到 ImageNet 级别。在 *Similarity Search and Applications:
    15th International Conference, SISAP 2022, Bologna, Italy, October 5–7, 2022,
    Proceedings*。Springer, 251–264。'
- en: Lagani et al. (2021d) Gabriele Lagani, Raffaele Mazziotti, Fabrizio Falchi,
    Claudio Gennaro, Guido Marco Cicchini, Tommaso Pizzorusso, Federico Cremisi, and
    Giuseppe Amato. 2021d. Assessing Pattern Recognition Performance of Neuronal Cultures
    through Accurate Simulation. In *2021 10th International IEEE/EMBS Conference
    on Neural Engineering (NER)*. IEEE, 726–729.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagani et al. (2021d) Gabriele Lagani, Raffaele Mazziotti, Fabrizio Falchi,
    Claudio Gennaro, Guido Marco Cicchini, Tommaso Pizzorusso, Federico Cremisi, 和
    Giuseppe Amato. 2021d. 通过准确模拟评估神经文化的模式识别性能。在 *2021 10th International IEEE/EMBS
    Conference on Neural Engineering (NER)*。IEEE, 726–729。
- en: Lai and Fyfe (2001) Pei Ling Lai and Colin Fyfe. 2001. A family of Canonical
    Correlation networks. *Neural processing letters* 14, 2 (2001), 93–105.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai and Fyfe (2001) Pei Ling Lai 和 Colin Fyfe. 2001. 一类典型相关网络。*Neural processing
    letters* 14, 2 (2001), 93–105。
- en: Lake and Piantadosi (2020) Brenden M Lake and Steven T Piantadosi. 2020. People
    infer recursive visual concepts from just a few examples. *Computational Brain
    & Behavior* 3 (2020), 54–65.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lake and Piantadosi (2020) Brenden M Lake 和 Steven T Piantadosi. 2020. 人们从少量例子中推断递归视觉概念。*Computational
    Brain & Behavior* 3 (2020), 54–65。
- en: Lake et al. (2017) Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J
    Gershman. 2017. Building machines that learn and think like people. *Behavioral
    and brain sciences* 40 (2017).
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lake et al. (2017) Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, 和 Samuel
    J Gershman. 2017. 构建像人类一样学习和思考的机器。*Behavioral and brain sciences* 40 (2017)。
- en: Law and Cooper (1994) C Charles Law and Leon N Cooper. 1994. Formation of receptive
    fields in realistic visual environments according to the Bienenstock, Cooper,
    and Munro (BCM) theory. *Proceedings of the National Academy of Sciences* 91,
    16 (1994), 7797–7801.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Law and Cooper (1994) C Charles Law 和 Leon N Cooper. 1994. 根据 Bienenstock, Cooper
    和 Munro (BCM) 理论在真实视觉环境中形成感受野。*Proceedings of the National Academy of Sciences*
    91, 16 (1994), 7797–7801。
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner,
    et al. 1998. Gradient-based learning applied to document recognition. *Proc. IEEE*
    86, 11 (1998), 2278–2324.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner,
    等. 1998. 应用于文档识别的基于梯度的学习。*Proc. IEEE* 86, 11 (1998), 2278–2324。
- en: LeCun et al. (2004) Yann LeCun, Fu Jie Huang, and Leon Bottou. 2004. Learning
    methods for generic object recognition with invariance to pose and lighting. In
    *Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition, 2004\. CVPR 2004.*, Vol. 2\. IEEE, II–104.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (2004) Yann LeCun, Fu Jie Huang, 和 Leon Bottou. 2004. 通用对象识别的学习方法，具有对姿态和光照的不变性。在
    *Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition, 2004. CVPR 2004.*，第2卷。IEEE, II–104。
- en: Leen (1991) Todd K Leen. 1991. Dynamics of learning in recurrent feature-discovery
    networks. In *Advances in neural information processing systems*. 70–76.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leen (1991) Todd K Leen. 1991. 递归特征发现网络中的学习动态。在 *Advances in neural information
    processing systems*。70–76。
- en: Lennie (2003) Peter Lennie. 2003. The cost of cortical computation. *Current
    biology* 13, 6 (2003), 493–497.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lennie (2003) Peter Lennie. 2003. 皮层计算的成本。*Current biology* 13, 6 (2003), 493–497。
- en: Li et al. (2004) Haifeng Li, Tao Jiang, and Keshu Zhang. 2004. Efficient and
    robust feature extraction by maximum margin criterion. In *Advances in neural
    information processing systems*. 97–104.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2004) Haifeng Li, Tao Jiang, 和 Keshu Zhang. 2004. 通过最大边际准则实现高效且鲁棒的特征提取。在
    *Advances in neural information processing systems*。97–104。
- en: Li and Fu (2015) Sheng Li and Yun Fu. 2015. Learning robust and discriminative
    subspace with low-rank constraints. *IEEE transactions on neural networks and
    learning systems* 27, 11 (2015), 2160–2173.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Fu (2015) Sheng Li 和 Yun Fu. 2015. 学习具有低秩约束的稳健判别子空间。*IEEE神经网络与学习系统汇刊* 27,
    11 (2015), 2160–2173。
- en: Li et al. (2003) Yuanqing Li, Andrzej Cichocki, and Shun-Ichi Amari. 2003. Sparse
    component analysis for blind source separation with less sensors than sources.
    In *Ica*, Vol. 2003. 89–94.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2003) Yuanqing Li, Andrzej Cichocki, 和 Shun-Ichi Amari. 2003. 少于源数量的盲源分离的稀疏分量分析。发表于
    *ICA*，第2003卷，89–94。
- en: Liu et al. (2019) Zhonghua Liu, Jingjing Wang, Gang Liu, and Lin Zhang. 2019.
    Discriminative low-rank preserving projection for dimensionality reduction. *Applied
    Soft Computing* 85 (2019), 105768.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019) Zhonghua Liu, Jingjing Wang, Gang Liu, 和 Lin Zhang. 2019. 低秩保持投影的判别性降维。*应用软计算*
    85 (2019), 105768。
- en: Lo et al. (1991) Zhen-Ping Lo, Masahiro Fujita, and Behnam Bavarian. 1991. Analysis
    of neighborhood interaction in Kohonen neural networks. *[1991] Proceedings. The
    Fifth International Parallel Processing Symposium* (1991), 246–249.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lo 等 (1991) Zhen-Ping Lo, Masahiro Fujita, 和 Behnam Bavarian. 1991. Kohonen
    神经网络中的邻域交互分析。*1991年第五届国际并行处理研讨会论文集* (1991), 246–249。
- en: Lo et al. (1993) Z-P Lo, Yaoqi Yu, and Behnam Bavarian. 1993. Analysis of the
    convergence properties of topology preserving neural networks. *IEEE Transactions
    on Neural Networks* 4, 2 (1993), 207–220.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lo 等 (1993) Z-P Lo, Yaoqi Yu, 和 Behnam Bavarian. 1993. 拓扑保持神经网络的收敛性质分析。*IEEE神经网络汇刊*
    4, 2 (1993), 207–220。
- en: 'Löwe et al. (2019) Sindy Löwe, Peter O’Connor, and Bastiaan Veeling. 2019.
    Putting an end to end-to-end: Gradient-isolated learning of representations. In
    *Advances in Neural Information Processing Systems*. 3039–3051.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Löwe 等 (2019) Sindy Löwe, Peter O’Connor, 和 Bastiaan Veeling. 2019. 终结端到端：隔离梯度的表示学习。发表于
    *神经信息处理系统进展*，3039–3051。
- en: Luo and Unbehauen (1997) Fa-Long Luo and Rolf Unbehauen. 1997. A generalized
    learning algorithm of minor component. In *1997 IEEE International Conference
    on Acoustics, Speech, and Signal Processing*, Vol. 4. IEEE, 3229–3232.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 和 Unbehauen (1997) Fa-Long Luo 和 Rolf Unbehauen. 1997. 一种广义的次成分学习算法。发表于
    *1997 IEEE国际声学、语音与信号处理会议*，第4卷。IEEE，3229–3232。
- en: 'Maass (1997) Wolfgang Maass. 1997. Networks of spiking neurons: the third generation
    of neural network models. *Neural networks* 10, 9 (1997), 1659–1671.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maass (1997) Wolfgang Maass. 1997. 脉冲神经网络：第三代神经网络模型。*神经网络* 10, 9 (1997), 1659–1671。
- en: Magotra and kim (2019) Arjun Magotra and Juntae kim. 2019. Transfer Learning
    for Image Classification Using Hebbian Plasticity Principles. In *Proceedings
    of the 2019 3rd International Conference on Computer Science and Artificial Intelligence*.
    233–238.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magotra 和 Kim (2019) Arjun Magotra 和 Juntae Kim. 2019. 基于赫布塑性原则的图像分类迁移学习。发表于
    *2019年第3届国际计算机科学与人工智能会议论文集*，233–238。
- en: Magotra and Kim (2020) Arjun Magotra and Juntae Kim. 2020. Improvement of Heterogeneous
    Transfer Learning Efficiency by Using Hebbian Learning Principle. *Applied Sciences*
    10, 16 (2020), 5631.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magotra 和 Kim (2020) Arjun Magotra 和 Juntae Kim. 2020. 通过使用赫布学习原则提高异质迁移学习效率。*应用科学*
    10, 16 (2020), 5631。
- en: Mainen and Sejnowski (1995) Zachary F Mainen and Terrence J Sejnowski. 1995.
    Reliability of spike timing in neocortical neurons. *Science* 268, 5216 (1995),
    1503–1506.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mainen 和 Sejnowski (1995) Zachary F Mainen 和 Terrence J Sejnowski. 1995. 新皮质神经元的尖峰时序可靠性。*科学*
    268, 5216 (1995), 1503–1506。
- en: Mairal et al. (2008) Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro,
    and Andrew Zisserman. 2008. Discriminative learned dictionaries for local image
    analysis. In *2008 IEEE Conference on Computer Vision and Pattern Recognition*.
    IEEE, 1–8.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairal 等 (2008) Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro, 和
    Andrew Zisserman. 2008. 用于局部图像分析的判别学习字典。发表于 *2008年IEEE计算机视觉与模式识别会议*。IEEE，1–8。
- en: Mairal et al. (2009) Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman,
    and Francis R Bach. 2009. Supervised dictionary learning. In *Advances in neural
    information processing systems*. 1033–1040.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairal 等 (2009) Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman,
    和 Francis R Bach. 2009. 监督字典学习。发表于 *神经信息处理系统进展*，1033–1040。
- en: Majani et al. (1989) E Majani, Ruth Erlanson, and Yaser S Abu-Mostafa. 1989.
    On the K-winners-take-all network. In *Advances in neural information processing
    systems*. 634–642.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majani 等 (1989) E Majani, Ruth Erlanson, 和 Yaser S Abu-Mostafa. 1989. 关于K-winners-take-all网络的研究。发表于
    *神经信息处理系统进展*，634–642。
- en: Mao and Jain (1993) Jianchang Mao and Anil K Jain. 1993. Discriminant analysis
    neural networks. In *IEEE International Conference on Neural Networks*. IEEE,
    300–305.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 和 Jain（1993）Jianchang Mao 和 Anil K Jain。1993年。判别分析神经网络。在 *IEEE国际神经网络会议*。IEEE，300–305。
- en: Marblestone et al. (2016) Adam H Marblestone, Greg Wayne, and Konrad P Kording.
    2016. Toward an integration of deep learning and neuroscience. *Frontiers in computational
    neuroscience* 10 (2016), 94.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marblestone 等（2016）Adam H Marblestone、Greg Wayne 和 Konrad P Kording。2016年。朝着深度学习与神经科学的整合迈进。*Frontiers
    in computational neuroscience* 10 (2016), 94。
- en: Markram et al. (1998) Henry Markram, Anirudh Gupta, Asher Uziel, Yun Wang, and
    Misha Tsodyks. 1998. Information processing with frequency-dependent synaptic
    connections. *Neurobiology of learning and memory* 70, 1-2 (1998), 101–112.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markram 等（1998）Henry Markram、Anirudh Gupta、Asher Uziel、Yun Wang 和 Misha Tsodyks。1998年。带有频率依赖突触连接的信息处理。*Neurobiology
    of learning and memory* 70, 1-2 (1998), 101–112。
- en: 'Markram et al. (2012) Henry Markram, K Meier, A Ailamaki, A Alvandpour, K Amunts,
    W Andreoni, et al. 2012. The human brain project: A report to the European Commission.
    *The HBP-PS Consortium, Lausanne* (2012).'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markram 等（2012）Henry Markram、K Meier、A Ailamaki、A Alvandpour、K Amunts、W Andreoni
    等。2012年。人脑工程计划：向欧盟委员会提交的报告。*The HBP-PS Consortium, Lausanne*（2012年）。
- en: Matsuoka et al. (1995) Kiyotoshi Matsuoka, Masahiro Ohoya, and Mitsuru Kawamoto.
    1995. A neural net for blind separation of nonstationary signals. *Neural networks*
    8, 3 (1995), 411–419.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matsuoka 等（1995）Kiyotoshi Matsuoka、Masahiro Ohoya 和 Mitsuru Kawamoto。1995年。用于盲分离非平稳信号的神经网络。*Neural
    networks* 8, 3 (1995), 411–419。
- en: Merolla et al. (2014) Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza,
    Andrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen
    Guo, Yutaka Nakamura, et al. 2014. A million spiking-neuron integrated circuit
    with a scalable communication network and interface. *Science* 345, 6197 (2014),
    668–673.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merolla 等（2014）Paul A Merolla、John V Arthur、Rodrigo Alvarez-Icaza、Andrew S Cassidy、Jun
    Sawada、Filipp Akopyan、Bryan L Jackson、Nabil Imam、Chen Guo、Yutaka Nakamura 等。2014年。一个拥有可扩展通信网络和接口的百万脉冲神经元集成电路。*Science*
    345, 6197 (2014), 668–673。
- en: Messina et al. (2021) Nicola Messina, Giuseppe Amato, Andrea Esuli, Fabrizio
    Falchi, Claudio Gennaro, and Stéphane Marchand-Maillet. 2021. Fine-grained visual
    textual alignment for cross-modal retrieval using transformer encoders. *ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM)* 17, 4 (2021),
    1–23.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Messina 等（2021）Nicola Messina、Giuseppe Amato、Andrea Esuli、Fabrizio Falchi、Claudio
    Gennaro 和 Stéphane Marchand-Maillet。2021年。使用变换器编码器进行跨模态检索的细粒度视觉文本对齐。*ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM)* 17, 4 (2021),
    1–23。
- en: Meyer-Base et al. (2001) Anke Meyer-Base, Yunmei Chen, and Scott McCullough.
    2001. Hebbian and anti-Hebbian learning for independent component analysis. In
    *IJCNN’01\. International Joint Conference on Neural Networks. Proceedings (Cat.
    No. 01CH37222)*, Vol. 2\. IEEE, 920–925.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyer-Base 等（2001）Anke Meyer-Base、Yunmei Chen 和 Scott McCullough。2001年。用于独立分量分析的Hebbian和反Hebbian学习。在
    *IJCNN’01\. 国际联合神经网络会议. 论文集（Cat. No. 01CH37222）*，第2卷。IEEE，920–925。
- en: 'Miconi (2021) Thomas Miconi. 2021. Hebbian learning with gradients: Hebbian
    convolutional neural networks with modern deep learning frameworks. *arXiv preprint
    arXiv:2107.01729* (2021).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miconi（2021）Thomas Miconi。2021年。具有梯度的Hebbian学习：结合现代深度学习框架的Hebbian卷积神经网络。*arXiv
    preprint arXiv:2107.01729*（2021年）。
- en: 'Mika et al. (1999) Sebastian Mika, Gunnar Ratsch, Jason Weston, Bernhard Scholkopf,
    and Klaus-Robert Mullers. 1999. Fisher discriminant analysis with kernels. In
    *Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal
    processing society workshop (cat. no. 98th8468)*. Ieee, 41–48.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mika 等（1999）Sebastian Mika、Gunnar Ratsch、Jason Weston、Bernhard Scholkopf 和
    Klaus-Robert Mullers。1999年。具有核函数的Fisher判别分析。在 *Neural networks for signal processing
    IX: Proceedings of the 1999 IEEE signal processing society workshop (cat. no.
    98th8468)*。IEEE，41–48。'
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. In *Advances in neural information processing systems*. 3111–3119.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等（2013）Tomas Mikolov、Ilya Sutskever、Kai Chen、Greg S Corrado 和 Jeff Dean。2013年。词语和短语的分布式表示及其组合性。在
    *Advances in neural information processing systems*。3111–3119。
- en: 'Moraitis et al. (2021) Timoleon Moraitis, Dmitry Toichkin, Yansong Chua, and
    Qinghai Guo. 2021. SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all
    networks. *arXiv preprint arXiv:2107.05747* (2021).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moraitis 等（2021）Timoleon Moraitis、Dmitry Toichkin、Yansong Chua 和 Qinghai Guo。2021年。SoftHebb：无监督Hebbian软赢家通吃网络中的贝叶斯推断。*arXiv
    preprint arXiv:2107.05747*（2021年）。
- en: Movellan (1991) Javier R Movellan. 1991. Contrastive Hebbian learning in the
    continuous Hopfield model. In *Connectionist models*. Elsevier, 10–17.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Movellan (1991) Javier R Movellan. 1991. 连续 Hopfield 模型中的对比Hebbian学习。在 *连接主义模型*
    中。Elsevier，10–17。
- en: Nowlan (1990) Steven J Nowlan. 1990. Maximum likelihood competitive learning.
    In *Advances in neural information processing systems*. 574–582.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nowlan (1990) Steven J Nowlan. 1990. 最大似然竞争学习。在 *神经信息处理系统的进展* 中，574–582。
- en: 'Nunes et al. (2022) Joao D Nunes, Marcelo Carvalho, Diogo Carneiro, and Jaime S
    Cardoso. 2022. Spiking neural networks: A survey. *IEEE Access* 10 (2022), 60738–60764.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nunes et al. (2022) Joao D Nunes, Marcelo Carvalho, Diogo Carneiro, 和 Jaime
    S Cardoso. 2022. 脉冲神经网络：综述。*IEEE Access* 10 (2022), 60738–60764。
- en: Oja (1982) Erkki Oja. 1982. Simplified neuron model as a principal component
    analyzer. *Journal of mathematical biology* 15, 3 (1982), 267–273.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oja (1982) Erkki Oja. 1982. 简化的神经元模型作为主成分分析器。*数学生物学杂志* 15, 3 (1982), 267–273。
- en: Oja (1989) Erkki Oja. 1989. Neural networks, principal components, and subspaces.
    *International journal of neural systems* 1, 01 (1989), 61–68.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oja (1989) Erkki Oja. 1989. 神经网络、主成分和子空间。*国际神经系统杂志* 1, 01 (1989), 61–68。
- en: Oja (1992) Erkki Oja. 1992. Principal components, minor components, and linear
    neural networks. *Neural networks* 5, 6 (1992), 927–935.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oja (1992) Erkki Oja. 1992. 主成分、次要成分和线性神经网络。*神经网络* 5, 6 (1992), 927–935。
- en: Oja et al. (1996) E Oja, J Karhunen, L Wang, and R Vigario. 1996. Principal
    and independent components in neural networks-recent developments. In *Proc. VII
    Italian Workshop Neural Networks WIRN*, Vol. 95. 16–35.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oja et al. (1996) E Oja, J Karhunen, L Wang, 和 R Vigario. 1996. 神经网络中的主成分和独立成分——近期发展。在
    *第七届意大利神经网络研讨会 WIRN* 中，Vol. 95，16–35。
- en: Olshausen (1996) Bruno A Olshausen. 1996. Learning linear, sparse, factorial
    codes. *Massachusetts Institute of Technology, AIM-1580* (1996).
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olshausen (1996) Bruno A Olshausen. 1996. 学习线性、稀疏、因子编码。*麻省理工学院，AIM-1580* (1996)。
- en: Olshausen and Field (1996a) Bruno A Olshausen and David J Field. 1996a. Emergence
    of simple-cell receptive field properties by learning a sparse code for natural
    images. *Nature* 381, 6583 (1996), 607.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olshausen 和 Field (1996a) Bruno A Olshausen 和 David J Field. 1996a. 通过学习自然图像的稀疏编码来产生简单细胞的接收场特性。*自然*
    381, 6583 (1996), 607。
- en: 'Olshausen and Field (1996b) Bruno A Olshausen and David J Field. 1996b. Natural
    image statistics and efficient coding. *Network: computation in neural systems*
    7, 2 (1996), 333–339.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olshausen 和 Field (1996b) Bruno A Olshausen 和 David J Field. 1996b. 自然图像统计和高效编码。*网络：神经系统中的计算*
    7, 2 (1996), 333–339。
- en: Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation
    learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*
    (2018).
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oord et al. (2018) Aaron van den Oord, Yazhe Li, 和 Oriol Vinyals. 2018. 通过对比预测编码进行表示学习。*arXiv
    预印本 arXiv:1807.03748* (2018)。
- en: 'O’Reilly and Munakata (2000) Randall C O’Reilly and Yuko Munakata. 2000. *Computational
    explorations in cognitive neuroscience: Understanding the mind by simulating the
    brain*. MIT press.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly 和 Munakata (2000) Randall C O’Reilly 和 Yuko Munakata. 2000. *认知神经科学中的计算探索：通过模拟大脑理解心智*。MIT
    出版社。
- en: Pang et al. (2005) Shaoning Pang, Seiichi Ozawa, and Nikola Kasabov. 2005. Incremental
    linear discriminant analysis for classification of data streams. *IEEE transactions
    on Systems, Man, and Cybernetics, part B (Cybernetics)* 35, 5 (2005), 905–914.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang et al. (2005) Shaoning Pang, Seiichi Ozawa, 和 Nikola Kasabov. 2005. 用于数据流分类的增量线性判别分析。*IEEE
    系统、人类与控制论杂志 B 部分 (控制论)* 35, 5 (2005), 905–914。
- en: Panousis et al. (2021a) Konstantinos Panousis, Sotirios Chatzis, Antonios Alexos,
    and Sergios Theodoridis. 2021a. Local competition and stochasticity for adversarial
    robustness in deep learning. In *International Conference on Artificial Intelligence
    and Statistics*. PMLR, 3862–3870.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panousis et al. (2021a) Konstantinos Panousis, Sotirios Chatzis, Antonios Alexos,
    和 Sergios Theodoridis. 2021a. 深度学习中的局部竞争和随机性以提高对抗鲁棒性。在 *国际人工智能与统计会议* 中，PMLR，3862–3870。
- en: Panousis et al. (2021b) Konstantinos P Panousis, Sotirios Chatzis, and Sergios
    Theodoridis. 2021b. Stochastic local winner-takes-all networks enable profound
    adversarial robustness. *arXiv preprint arXiv:2112.02671* (2021).
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panousis et al. (2021b) Konstantinos P Panousis, Sotirios Chatzis, 和 Sergios
    Theodoridis. 2021b. 随机局部赢家通吃网络实现深度对抗鲁棒性。*arXiv 预印本 arXiv:2112.02671* (2021)。
- en: 'Pati et al. (1993) Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy
    Krishnaprasad. 1993. Orthogonal matching pursuit: Recursive function approximation
    with applications to wavelet decomposition. In *Proceedings of 27th Asilomar conference
    on signals, systems and computers*. IEEE, 40–44.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pati et al. (1993) Yagyensh Chandra Pati, Ramin Rezaiifar, 和 Perinkulam Sambamurthy
    Krishnaprasad. 1993. 正交匹配追踪：递归函数逼近及其在小波分解中的应用。见于*第27届Asilomar信号、系统与计算机会议论文集*。IEEE，40–44。
- en: Pehlevan and Chklovskii (2015a) Cengiz Pehlevan and Dmitri Chklovskii. 2015a.
    A normative theory of adaptive dimensionality reduction in neural networks. In
    *Advances in neural information processing systems*. 2269–2277.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pehlevan and Chklovskii (2015a) Cengiz Pehlevan 和 Dmitri Chklovskii. 2015a.
    神经网络中自适应降维的规范理论。见于*神经信息处理系统进展*。2269–2277。
- en: Pehlevan and Chklovskii (2015b) Cengiz Pehlevan and Dmitri B Chklovskii. 2015b.
    Optimization theory of hebbian/anti-hebbian networks for pca and whitening. In
    *2015 53rd Annual Allerton Conference on Communication, Control, and Computing
    (Allerton)*. IEEE, 1458–1465.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pehlevan and Chklovskii (2015b) Cengiz Pehlevan 和 Dmitri B Chklovskii. 2015b.
    用于PCA和白化的Hebbian/反Hebbian网络的优化理论。见于*2015年第53届Allerton通信、控制与计算年度会议（Allerton）*。IEEE，1458–1465。
- en: 'Pehlevan et al. (2015) Cengiz Pehlevan, Tao Hu, and Dmitri B Chklovskii. 2015.
    A hebbian/anti-hebbian neural network for linear subspace learning: A derivation
    from multidimensional scaling of streaming data. *Neural computation* 27, 7 (2015),
    1461–1495.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pehlevan et al. (2015) Cengiz Pehlevan, Tao Hu, 和 Dmitri B Chklovskii. 2015.
    一种用于线性子空间学习的Hebbian/反Hebbian神经网络：基于流数据的多维缩放推导。*神经计算* 27, 7 (2015)，1461–1495。
- en: 'Pfeiffer and Pfeil (2018) Michael Pfeiffer and Thomas Pfeil. 2018. Deep learning
    with spiking neurons: Opportunities and challenges. *Frontiers in neuroscience*
    12 (2018), 774.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfeiffer and Pfeil (2018) Michael Pfeiffer 和 Thomas Pfeil. 2018. 使用脉冲神经元的深度学习：机遇与挑战。*前沿神经科学*
    12 (2018)，774。
- en: Pham and Garat (1997) Dinh Tuan Pham and Philippe Garat. 1997. Blind separation
    of mixture of independent sources through a quasi-maximum likelihood approach.
    *IEEE transactions on Signal Processing* 45, 7 (1997), 1712–1725.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pham and Garat (1997) Dinh Tuan Pham 和 Philippe Garat. 1997. 通过准最大似然方法进行独立源混合的盲分离。*IEEE信号处理学报*
    45, 7 (1997)，1712–1725。
- en: Plumbley (1993) Mark D Plumbley. 1993. A Hebbian/anti-Hebbian network which
    optimizes information capacity by orthonormalizing the principal subspace. In
    *1993 Third International Conference on Artificial Neural Networks*. IET, 86–90.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plumbley (1993) Mark D Plumbley. 1993. 一种通过正交化主子空间来优化信息容量的Hebbian/反Hebbian网络。见于*1993年第三届国际人工神经网络大会*。IET，86–90。
- en: Raducanu and Dornaika (2012) Bogdan Raducanu and Fadi Dornaika. 2012. A supervised
    non-linear dimensionality reduction approach for manifold learning. *Pattern Recognition*
    45, 6 (2012), 2432–2444.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raducanu and Dornaika (2012) Bogdan Raducanu 和 Fadi Dornaika. 2012. 一种用于流形学习的监督非线性降维方法。*模式识别*
    45, 6 (2012)，2432–2444。
- en: Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le.
    2017. Searching for activation functions. *arXiv preprint arXiv:1710.05941* (2017).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, 和 Quoc V Le. 2017.
    寻找激活函数。*arXiv预印本 arXiv:1710.05941* (2017)。
- en: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis.
    In *International conference on machine learning*. PMLR, 1060–1069.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, 和 Honglak Lee. 2016. 生成对抗文本到图像合成。见于*国际机器学习大会*。PMLR，1060–1069。
- en: Richards et al. (2019) Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin,
    Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, Rui Ponte Costa,
    Archy de Berker, Surya Ganguli, et al. 2019. A deep learning framework for neuroscience.
    *Nature neuroscience* 22, 11 (2019), 1761–1770.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richards et al. (2019) Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin,
    Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, Rui Ponte Costa,
    Archy de Berker, Surya Ganguli, 等. 2019. 神经科学的深度学习框架。*自然神经科学* 22, 11 (2019)，1761–1770。
- en: Ritchie et al. (2019) Alexander Ritchie, Clayton Scott, Laura Balzano, Daniel
    Kessler, and Chandra S Sripada. 2019. Supervised principal component analysis
    via manifold optimization. In *2019 IEEE Data Science Workshop (DSW)*. IEEE, 6–10.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritchie et al. (2019) Alexander Ritchie, Clayton Scott, Laura Balzano, Daniel
    Kessler, 和 Chandra S Sripada. 2019. 通过流形优化的监督主成分分析。见于*2019年IEEE数据科学研讨会（DSW）*。IEEE，6–10。
- en: 'Roberts (1999) Patrick D Roberts. 1999. Computational consequences of temporally
    asymmetric learning rules: I. Differential Hebbian learning. *Journal of computational
    neuroscience* 7, 3 (1999), 235–246.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts (1999) Patrick D Roberts. 1999. 时间不对称学习规则的计算后果：I. 差分Hebbian学习。*计算神经科学杂志*
    7, 3 (1999), 235–246。
- en: 'Roh et al. (2019) Yuji Roh, Geon Heo, and Steven Euijong Whang. 2019. A survey
    on data collection for machine learning: a big data-ai integration perspective.
    *IEEE Transactions on Knowledge and Data Engineering* 33, 4 (2019), 1328–1347.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roh 等 (2019) Yuji Roh, Geon Heo 和 Steven Euijong Whang. 2019. 机器学习数据收集的调查：大数据-人工智能集成视角。*IEEE知识与数据工程汇刊*
    33, 4 (2019), 1328–1347。
- en: Roweis and Saul (2000) Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality
    reduction by locally linear embedding. *science* 290, 5500 (2000), 2323–2326.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roweis 和 Saul (2000) Sam T Roweis 和 Lawrence K Saul. 2000. 通过局部线性嵌入进行非线性维度减少。*科学*
    290, 5500 (2000), 2323–2326。
- en: Roy et al. (2019) Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. 2019.
    Towards spike-based machine intelligence with neuromorphic computing. *Nature*
    575, 7784 (2019), 607–617.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 等 (2019) Kaushik Roy, Akhilesh Jaiswal 和 Priyadarshini Panda. 2019. 基于尖峰的机器智能与神经形态计算的进展。*自然*
    575, 7784 (2019), 607–617。
- en: Rozell et al. (2008) Christopher J Rozell, Don H Johnson, Richard G Baraniuk,
    and Bruno A Olshausen. 2008. Sparse coding via thresholding and local competition
    in neural circuits. *Neural computation* 20, 10 (2008), 2526–2563.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozell 等 (2008) Christopher J Rozell, Don H Johnson, Richard G Baraniuk 和 Bruno
    A Olshausen. 2008. 通过阈值化和局部竞争在神经回路中进行稀疏编码。*神经计算* 20, 10 (2008), 2526–2563。
- en: Rubner and Tavan (1989) Jeanne Rubner and Paul Tavan. 1989. A self-organizing
    network for principal-component analysis. *EPL (Europhysics Letters)* 10, 7 (1989),
    693.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubner 和 Tavan (1989) Jeanne Rubner 和 Paul Tavan. 1989. 用于主成分分析的自组织网络。*EPL (欧物理快报)*
    10, 7 (1989), 693。
- en: Rumelhart and Zipser (1985) David E Rumelhart and David Zipser. 1985. Feature
    discovery by competitive learning. *Cognitive science* 9, 1 (1985), 75–112.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 和 Zipser (1985) David E Rumelhart 和 David Zipser. 1985. 通过竞争学习进行特征发现。*认知科学*
    9, 1 (1985), 75–112。
- en: Sanger (1989) Terence D Sanger. 1989. Optimal unsupervised learning in a single-layer
    linear feedforward neural network. *Neural networks* 2, 6 (1989), 459–473.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanger (1989) Terence D Sanger. 1989. 单层线性前馈神经网络中的最优无监督学习。*神经网络* 2, 6 (1989),
    459–473。
- en: Santa Cruz and Dorronsoro (1998) Carlos Santa Cruz and Jose R Dorronsoro. 1998.
    A nonlinear discriminant algorithm for feature extraction and data classification.
    *IEEE transactions on neural networks* 9, 6 (1998), 1370–1376.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santa Cruz 和 Dorronsoro (1998) Carlos Santa Cruz 和 Jose R Dorronsoro. 1998.
    用于特征提取和数据分类的非线性判别算法。*IEEE神经网络汇刊* 9, 6 (1998), 1370–1376。
- en: Sarto et al. (2023) Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi,
    and Rita Cucchiara. 2023. Positive-Augmented Contrastive Learning for Image and
    Video Captioning Evaluation. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 6914–6924.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarto 等 (2023) Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi
    和 Rita Cucchiara. 2023. 用于图像和视频字幕评估的正向增强对比学习。发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*。6914–6924。
- en: Saunshi et al. (2019) Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail
    Khodak, and Hrishikesh Khandeparkar. 2019. A theoretical analysis of contrastive
    unsupervised representation learning. In *International Conference on Machine
    Learning*. PMLR, 5628–5637.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saunshi 等 (2019) Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak
    和 Hrishikesh Khandeparkar. 2019. 对比无监督表示学习的理论分析。发表于 *国际机器学习会议*。PMLR, 5628–5637。
- en: Schemmel et al. (2010) Johannes Schemmel, Daniel Briiderle, Andreas Griibl,
    Matthias Hock, Karlheinz Meier, and Sebastian Millner. 2010. A wafer-scale neuromorphic
    hardware system for large-scale neural modeling. In *Proceedings of 2010 IEEE
    International Symposium on Circuits and Systems*. IEEE, 1947–1950.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schemmel 等 (2010) Johannes Schemmel, Daniel Briiderle, Andreas Gruebl, Matthias
    Hock, Karlheinz Meier 和 Sebastian Millner. 2010. 用于大规模神经建模的晶圆级神经形态硬件系统。发表于 *2010
    IEEE国际电路与系统研讨会论文集*。IEEE, 1947–1950。
- en: Schuman et al. (2022) Catherine D Schuman, Shruti R Kulkarni, Maryam Parsa,
    J Parker Mitchell, Prasanna Date, and Bill Kay. 2022. Opportunities for neuromorphic
    computing algorithms and applications. *Nature Computational Science* 2, 1 (2022),
    10–19.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuman 等 (2022) Catherine D Schuman, Shruti R Kulkarni, Maryam Parsa, J Parker
    Mitchell, Prasanna Date 和 Bill Kay. 2022. 神经形态计算算法和应用的机会。*自然计算科学* 2, 1 (2022),
    10–19。
- en: 'Sejnowski and Tesauro (1989) Terrence J Sejnowski and Gerald Tesauro. 1989.
    Building network learning algorithms from hebbian synapses. *Brain organization
    and memory: cells, systems, and circuits. Oxford University Press, New York* (1989),
    338–355.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sejnowski and Tesauro (1989) Terrence J Sejnowski 和 Gerald Tesauro. 1989. 从赫布突触构建网络学习算法。*大脑组织与记忆：细胞、系统和电路。牛津大学出版社，纽约*
    (1989), 338–355.
- en: Seung and Zung (2017) H Sebastian Seung and Jonathan Zung. 2017. A correlation
    game for unsupervised learning yields computational interpretations of Hebbian
    excitation, anti-Hebbian inhibition, and synapse elimination. *arXiv preprint
    arXiv:1704.00646* (2017).
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seung and Zung (2017) H Sebastian Seung 和 Jonathan Zung. 2017. 一种用于无监督学习的相关性游戏提供了赫布激发、反赫布抑制和突触淘汰的计算解释。*arXiv预印本
    arXiv:1704.00646* (2017).
- en: Shin and Park (2011) Yong Shin and Cheong Park. 2011. Analysis of correlation
    based dimension reduction methods. *International Journal of Applied Mathematics
    and Computer Science* 21, 3 (2011), 549–558.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin and Park (2011) Yong Shin 和 Cheong Park. 2011. 基于相关性的降维方法分析。*应用数学与计算机科学国际期刊*
    21, 3 (2011), 549–558.
- en: Shindou et al. (2019) Tomomi Shindou, Mayumi Shindou, Sakurako Watanabe, and
    Jeffery Wickens. 2019. A silent eligibility trace enables dopamine-dependent synaptic
    plasticity for reinforcement learning in the mouse striatum. *European Journal
    of Neuroscience* 49, 5 (2019), 726–736.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shindou et al. (2019) Tomomi Shindou, Mayumi Shindou, Sakurako Watanabe 和 Jeffery
    Wickens. 2019. 静默的资格追踪使多巴胺依赖的突触可塑性在小鼠纹状体中得以实现。*欧洲神经科学杂志* 49, 5 (2019), 726–736.
- en: 'Shrestha et al. (2022) Amar Shrestha, Haowen Fang, Zaidao Mei, Daniel Patrick
    Rider, Qing Wu, and Qinru Qiu. 2022. A survey on neuromorphic computing: Models
    and hardware. *IEEE Circuits and Systems Magazine* 22, 2 (2022), 6–35.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shrestha et al. (2022) Amar Shrestha, Haowen Fang, Zaidao Mei, Daniel Patrick
    Rider, Qing Wu 和 Qinru Qiu. 2022. 神经形态计算的综述：模型与硬件。*IEEE电路与系统杂志* 22, 2 (2022),
    6–35.
- en: Silver et al. (2016) David Silver, Aja Huang, Chris J Maddison, Arthur Guez,
    Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of Go with
    deep neural networks and tree search. *nature* 529, 7587 (2016), 484.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver et al. (2016) David Silver, Aja Huang, Chris J Maddison, Arthur Guez,
    Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot 等. 2016. 使用深度神经网络和树搜索掌握围棋游戏。*自然* 529, 7587 (2016),
    484.
- en: Song et al. (2000) Sen Song, Kenneth D Miller, and Larry F Abbott. 2000. Competitive
    Hebbian learning through spike-timing-dependent synaptic plasticity. *Nature neuroscience*
    3, 9 (2000), 919.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2000) Sen Song, Kenneth D Miller 和 Larry F Abbott. 2000. 通过尖峰时序依赖的突触可塑性进行竞争性赫布学习。*自然神经科学*
    3, 9 (2000), 919.
- en: 'Stefanini et al. (2022) Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi,
    Silvia Cascianelli, Giuseppe Fiameni, and Rita Cucchiara. 2022. From show to tell:
    A survey on deep learning-based image captioning. *IEEE transactions on pattern
    analysis and machine intelligence* 45, 1 (2022), 539–559.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stefanini et al. (2022) Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi,
    Silvia Cascianelli, Giuseppe Fiameni 和 Rita Cucchiara. 2022. 从展示到讲述：基于深度学习的图像描述的综述。*IEEE模式分析与机器智能交易*
    45, 1 (2022), 539–559.
- en: Stefanis (2020) Costas Stefanis. 2020. Interneuronal mechanisms in the cortex.
    In *The interneuron*. University of California Press, 497–526.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stefanis (2020) Costas Stefanis. 2020. 皮层中的中间神经机制。在 *The interneuron* 中。加利福尼亚大学出版社,
    497–526.
- en: Sugiyama (2006) Masashi Sugiyama. 2006. Local fisher discriminant analysis for
    supervised dimensionality reduction. In *Proceedings of the 23rd international
    conference on Machine learning*. 905–912.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sugiyama (2006) Masashi Sugiyama. 2006. 用于监督性降维的局部Fisher判别分析。在 *第23届国际机器学习会议论文集*
    中。905–912.
- en: Sun et al. (2008) Liang Sun, Shuiwang Ji, and Jieping Ye. 2008. A least squares
    formulation for canonical correlation analysis. In *Proceedings of the 25th international
    conference on Machine learning*. 1024–1031.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2008) Liang Sun, Shuiwang Ji 和 Jieping Ye. 2008. 典型相关分析的最小二乘公式。在
    *第25届国际机器学习会议论文集* 中。1024–1031.
- en: Sun et al. (2007) Ting-Kai Sun, Song-Can Chen, Zhong Jin, and Jing-Yu Yang.
    2007. Kernelized discriminative canonical correlation analysis. In *2007 International
    Conference on Wavelet Analysis and Pattern Recognition*, Vol. 3\. IEEE, 1283–1287.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2007) Ting-Kai Sun, Song-Can Chen, Zhong Jin 和 Jing-Yu Yang. 2007.
    核化判别典型相关分析。在 *2007年国际小波分析与模式识别会议* 中，第3卷。IEEE, 1283–1287.
- en: Szegedy et al. (2013) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties
    of neural networks. *arXiv preprint arXiv:1312.6199* (2013).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2013) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, 和 Rob Fergus. 2013. 神经网络的引人入胜的特性。*arXiv
    预印本 arXiv:1312.6199* (2013)。
- en: Tavanaei et al. (2019) Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh,
    Timothée Masquelier, and Anthony Maida. 2019. Deep learning in spiking neural
    networks. *Neural Networks* 111 (2019), 47–63.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tavanaei et al. (2019) Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh,
    Timothée Masquelier, 和 Anthony Maida. 2019. 脉冲神经网络中的深度学习。*Neural Networks* 111
    (2019), 47–63。
- en: Tenenbaum et al. (2000) Joshua B Tenenbaum, Vin De Silva, and John C Langford.
    2000. A global geometric framework for nonlinear dimensionality reduction. *science*
    290, 5500 (2000), 2319–2323.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tenenbaum et al. (2000) Joshua B Tenenbaum, Vin De Silva, 和 John C Langford.
    2000. 用于非线性维度缩减的全球几何框架。*science* 290, 5500 (2000), 2319–2323。
- en: 'Turrigiano (2012) Gina Turrigiano. 2012. Homeostatic synaptic plasticity: local
    and global mechanisms for stabilizing neuronal function. *Cold Spring Harbor perspectives
    in biology* 4, 1 (2012), a005736.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turrigiano (2012) Gina Turrigiano. 2012. 自稳突触可塑性：稳定神经功能的局部和全球机制。*Cold Spring
    Harbor perspectives in biology* 4, 1 (2012), a005736。
- en: Vasilkoski et al. (2011) Zlatko Vasilkoski, Heather Ames, Ben Chandler, Anatoli
    Gorchetchnikov, Jasmin Léveillé, Gennady Livitz, Ennio Mingolla, and Massimiliano
    Versace. 2011. Review of stability properties of neural plasticity rules for implementation
    on memristive neuromorphic hardware. In *The 2011 International Joint Conference
    on Neural Networks*. IEEE, 2563–2569.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vasilkoski et al. (2011) Zlatko Vasilkoski, Heather Ames, Ben Chandler, Anatoli
    Gorchetchnikov, Jasmin Léveillé, Gennady Livitz, Ennio Mingolla, 和 Massimiliano
    Versace. 2011. 关于用于记忆电阻神经形态硬件实施的神经可塑性规则的稳定性属性综述。发表于 *2011年国际神经网络联合会议*。IEEE, 2563–2569。
- en: 'Vidnerová and Neruda (2018) Petra Vidnerová and Roman Neruda. 2018. Deep networks
    with rbf layers to prevent adversarial examples. In *Artificial Intelligence and
    Soft Computing: 17th International Conference, ICAISC 2018, Zakopane, Poland,
    June 3-7, 2018, Proceedings, Part I 17*. Springer, 257–266.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vidnerová 和 Neruda (2018) Petra Vidnerová 和 Roman Neruda. 2018. 使用 RBF 层的深度网络以防止对抗样本。发表于
    *人工智能与软计算：第17届国际会议，ICAISC 2018，波兰扎科帕内，2018年6月3-7日，论文集，第17部分*。Springer, 257–266。
- en: Wadhwa and Madhow (2016a) Aseem Wadhwa and Upamanyu Madhow. 2016a. Bottom-up
    Deep Learning using the Hebbian Principle.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wadhwa 和 Madhow (2016a) Aseem Wadhwa 和 Upamanyu Madhow. 2016a. 基于 Hebbian 原则的自下而上的深度学习。
- en: Wadhwa and Madhow (2016b) Aseem Wadhwa and Upamanyu Madhow. 2016b. Learning
    Sparse, Distributed Representations using the Hebbian Principle. *arXiv preprint
    arXiv:1611.04228* (2016).
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wadhwa 和 Madhow (2016b) Aseem Wadhwa 和 Upamanyu Madhow. 2016b. 使用 Hebbian 原则学习稀疏分布式表示。*arXiv
    预印本 arXiv:1611.04228* (2016)。
- en: Wang et al. (2016) Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang.
    2016. A comprehensive survey on cross-modal retrieval. *arXiv preprint arXiv:1607.06215*
    (2016).
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, 和 Liang Wang. 2016.
    跨模态检索的综合调查。*arXiv 预印本 arXiv:1607.06215* (2016)。
- en: Wang and Chen (2009) Ruiping Wang and Xilin Chen. 2009. Manifold discriminant
    analysis. In *2009 IEEE Conference on Computer Vision and Pattern Recognition*.
    IEEE, 429–436.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Chen (2009) Ruiping Wang 和 Xilin Chen. 2009. 流形判别分析。发表于 *2009 IEEE 计算机视觉与模式识别会议*。IEEE,
    429–436。
- en: Watanabe et al. (2018) Eiji Watanabe, Akiyoshi Kitaoka, Kiwako Sakamoto, Masaki
    Yasugi, and Kenta Tanaka. 2018. Illusory motion reproduced by deep neural networks
    trained for prediction. *Frontiers in psychology* 9 (2018), 345.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watanabe et al. (2018) Eiji Watanabe, Akiyoshi Kitaoka, Kiwako Sakamoto, Masaki
    Yasugi, 和 Kenta Tanaka. 2018. 通过训练预测的深度神经网络再现的错觉运动。*Frontiers in psychology* 9
    (2018), 345。
- en: 'White and Keller (1989) Edward L White and Asaf Keller. 1989. *Cortical circuits:
    synaptic organization of the cerebral cortex: structure, function, and theory*.
    Springer.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White 和 Keller (1989) Edward L White 和 Asaf Keller. 1989. *皮层回路：大脑皮层的突触组织：结构、功能和理论*。Springer。
- en: Wohrer et al. (2013) Adrien Wohrer, Mark D Humphries, and Christian K Machens.
    2013. Population-wide distributions of neural activity during perceptual decision-making.
    *Progress in neurobiology* 103 (2013), 156–193.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wohrer et al. (2013) Adrien Wohrer, Mark D Humphries, 和 Christian K Machens.
    2013. 感知决策过程中的神经活动的群体分布。*Progress in neurobiology* 103 (2013), 156–193。
- en: 'Wu et al. (2015) Xinyu Wu, Vishal Saxena, Kehan Zhu, and Sakkarapani Balagopal.
    2015. A CMOS Spiking Neuron for Brain-Inspired Neural Networks With Resistive
    Synapses andIn SituLearning. *IEEE Transactions on Circuits and Systems II: Express
    Briefs* 62, 11 (2015), 1088–1092.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2015) Xinyu Wu, Vishal Saxena, Kehan Zhu, and Sakkarapani Balagopal.
    2015. A CMOS Spiking Neuron for Brain-Inspired Neural Networks With Resistive
    Synapses and In Situ Learning. *IEEE 电路与系统 II: 快报* 62, 11 (2015), 1088–1092.'
- en: Xiao et al. (2019) Chang Xiao, Peilin Zhong, and Changxi Zheng. 2019. Enhancing
    adversarial defense by k-winners-take-all. *arXiv preprint arXiv:1905.10510* (2019).
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2019) Chang Xiao, Peilin Zhong, and Changxi Zheng. 2019. Enhancing
    adversarial defense by k-winners-take-all. *arXiv 预印本 arXiv:1905.10510* (2019).
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell:
    Neural image caption generation with visual attention. In *International conference
    on machine learning*. 2048–2057.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell:
    Neural image caption generation with visual attention. In *国际机器学习会议论文集*. 2048–2057.'
- en: Yagishita et al. (2014) Sho Yagishita, Akiko Hayashi-Takagi, Graham CR Ellis-Davies,
    Hidetoshi Urakubo, Shin Ishii, and Haruo Kasai. 2014. A critical time window for
    dopamine actions on the structural plasticity of dendritic spines. *Science* 345,
    6204 (2014), 1616–1620.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yagishita et al. (2014) Sho Yagishita, Akiko Hayashi-Takagi, Graham CR Ellis-Davies,
    Hidetoshi Urakubo, Shin Ishii, and Haruo Kasai. 2014. A critical time window for
    dopamine actions on the structural plasticity of dendritic spines. *科学* 345, 6204
    (2014), 1616–1620.
- en: Yang et al. (2011) Meng Yang, Lei Zhang, Xiangchu Feng, and David Zhang. 2011.
    Fisher discrimination dictionary learning for sparse representation. In *2011
    International Conference on Computer Vision*. IEEE, 543–550.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2011) Meng Yang, Lei Zhang, Xiangchu Feng, and David Zhang. 2011.
    Fisher discrimination dictionary learning for sparse representation. In *2011
    国际计算机视觉会议*. IEEE, 543–550.
- en: Ye (2007) Jieping Ye. 2007. Least squares linear discriminant analysis. In *Proceedings
    of the 24th international conference on Machine learning*. 1087–1093.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye (2007) Jieping Ye. 2007. Least squares linear discriminant analysis. In *第24届国际机器学习会议论文集*.
    1087–1093.
- en: 'Yuan et al. (2019) Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial
    examples: Attacks and defenses for deep learning. *IEEE transactions on neural
    networks and learning systems* 30, 9 (2019), 2805–2824.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. (2019) Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial
    examples: Attacks and defenses for deep learning. *IEEE 神经网络与学习系统汇刊* 30, 9 (2019),
    2805–2824.'
- en: 'Zadeh et al. (2018) Pourya Habib Zadeh, Reshad Hosseini, and Suvrit Sra. 2018.
    Deep-rbf networks revisited: Robust classification with rejection. *arXiv preprint
    arXiv:1812.03190* (2018).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zadeh et al. (2018) Pourya Habib Zadeh, Reshad Hosseini, and Suvrit Sra. 2018.
    Deep-rbf networks revisited: Robust classification with rejection. *arXiv 预印本
    arXiv:1812.03190* (2018).'
- en: 'Zhang et al. (2018) Hongyang Zhang, Susu Xu, Jiantao Jiao, Pengtao Xie, Ruslan
    Salakhutdinov, and Eric P Xing. 2018. Stackelberg gan: Towards provable minimax
    equilibrium via multi-generator architectures. *arXiv preprint arXiv:1811.08010*
    (2018).'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2018) Hongyang Zhang, Susu Xu, Jiantao Jiao, Pengtao Xie, Ruslan
    Salakhutdinov, and Eric P Xing. 2018. Stackelberg gan: Towards provable minimax
    equilibrium via multi-generator architectures. *arXiv 预印本 arXiv:1811.08010* (2018).'
- en: 'Zhang et al. (2017) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *Proceedings
    of the IEEE international conference on computer vision*. 5907–5915.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2017) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *IEEE 国际计算机视觉会议论文集*.
    5907–5915.'
- en: Zhang et al. (2008) Wei Zhang, Xiangyang Xue, Zichen Sun, Hong Lu, and Yue-Fei
    Guo. 2008. Metric learning by discriminant neighborhood embedding. *Pattern recognition*
    41, 6 (2008), 2086–2096.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2008) Wei Zhang, Xiangyang Xue, Zichen Sun, Hong Lu, and Yue-Fei
    Guo. 2008. Metric learning by discriminant neighborhood embedding. *模式识别* 41,
    6 (2008), 2086–2096.
- en: Zhu et al. (2020) Jiadi Zhu, Teng Zhang, Yuchao Yang, and Ru Huang. 2020. A
    comprehensive review on emerging artificial neuromorphic devices. *Applied Physics
    Reviews* 7, 1 (2020), 011312.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2020) Jiadi Zhu, Teng Zhang, Yuchao Yang, and Ru Huang. 2020. A
    comprehensive review on emerging artificial neuromorphic devices. *应用物理评论* 7,
    1 (2020), 011312.
- en: Zhuang et al. (2021) Chengxu Zhuang, Siming Yan, Aran Nayebi, Martin Schrimpf,
    Michael C Frank, James J DiCarlo, and Daniel LK Yamins. 2021. Unsupervised neural
    network models of the ventral visual stream. *Proceedings of the National Academy
    of Sciences* 118, 3 (2021).
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2021) Chengxu Zhuang, Siming Yan, Aran Nayebi, Martin Schrimpf,
    Michael C Frank, James J DiCarlo, and Daniel LK Yamins. 2021. Unsupervised neural
    network models of the ventral visual stream. *国家科学院学报* 118, 3 (2021).
