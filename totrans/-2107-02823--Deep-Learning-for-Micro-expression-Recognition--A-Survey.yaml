- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:53:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2107.02823] Deep Learning for Micro-expression Recognition: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.02823](https://ar5iv.labs.arxiv.org/html/2107.02823)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Micro-expression Recognition: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yante Li, Jinsheng Wei, Yang Liu, Janne Kauttonen and Guoying Zhao * represents
    the corresponding author. Y. Li, J. Wei, Y. Liu, G. Zhao are with the Center for
    Machine Vision and Signal Analysis, University of Oulu, Oulu, FI-90014, Finland.
    E-mail: firstname.lastname@oulu.fi J. Wei is with the School of Telecommunications
    and Information Engineering, Nanjing University of Posts and Telecommunications,
    Nanjing 210003 China; Email: 2018010217@njupt.edu.cn. J. Kauttonen is with the
    School of Digital Business, Haaga-Helia University of Applied Sciences, Helsinki,
    FI-00520, Finland; Email: Janne.Kauttonen@haaga-helia.fi.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Micro-expressions (MEs) are involuntary facial movements revealing people’s
    hidden feelings in high-stake situations and have practical importance in various
    fields. Early methods for Micro-expression Recognition (MER) are mainly based
    on traditional features. Recently, with the success of Deep Learning (DL) in various
    tasks, neural networks have received increasing interest in MER. Different from
    macro-expressions, MEs are spontaneous, subtle, and rapid facial movements, leading
    to difficult data collection and annotation, thus publicly available datasets
    are usually small-scale. Currently, various DL approaches have been proposed to
    solve the ME issues and improve MER performance. In this survey, we provide a
    comprehensive review of deep MER and define a new taxonomy for the field encompassing
    all aspects of MER based on DL, including datasets, each step of the deep MER
    pipeline, and performance comparisons of the most influential methods. The basic
    approaches and advanced developments are summarized and discussed for each aspect.
    Additionally, we conclude the remaining challenges and potential directions for
    the design of robust MER systems. Finally, ethical considerations in MER are discussed.
    To the best of our knowledge, this is the first survey of deep MER methods, and
    this survey can serve as a reference point for future MER research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Micro-expression recognition, Deep learning, Micro-expression dataset, Survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Facial expression (FE) is one of the most powerful and universal means for human
    communication, which is highly associated with human mental states, attitudes,
    and intentions. Besides ordinary FEs (also known as macro-expressions) that we
    see daily, emotions can also be expressed in a special format of Micro-expressions
    (MEs) under certain conditions. MEs are FEs revealing people’s hidden feelings
    in high-stake situations when people try to conceal their true feelings [[1](#bib.bib1)].
    Different from macro-expressions, MEs are spontaneous, subtle, and rapid (1/25
    to 1/3 second) facial movements reacting to emotional stimulus [[2](#bib.bib2),
    [3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The ME phenomenon was firstly discovered by Haggard and Isaacs [[4](#bib.bib4)]
    in 1966\. Three years later, Ekman and Friesen also declared the finding of MEs
    [[5](#bib.bib5)] during examining psychiatric patient’s videos for lie detection.
    In the following years, Ekman et al.  continued ME research and developed the
    Facial Action Coding System (FACS) [[6](#bib.bib6)] and Micro Expression Training
    Tool (METT) [[7](#bib.bib7)]. Specifically, FACS breaks down FEs into individual
    components of muscle movement, called Action Units (AUs) [[6](#bib.bib6)]. AU
    analysis can effectively resolve the ambiguity issue to represent individual expression
    and increase Facial Expression Recognition (FER) performance [[8](#bib.bib8)].
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Micro-expression
    Recognition: A Survey") shows the example of micro- and macro-expressions as well
    as activated AUs in each FE. On the other hand, METT is helpful for increasing
    people’s emotional awareness. It can promote manual ME detection performance which
    provides a potential chance to build reliable ME datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4cc143073913d256643fd8464f85f73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Examples of micro-expressions in CASME II [[9](#bib.bib9)] and macro-expressions
    in MMI [[10](#bib.bib10)], as well as the active AUs. The red arrow represents
    the muscle movement direction. AU4, AU6, AU7, AU9, AU12, AU15, and AU25 represent
    brow lowerer, cheek raise, lids tight, nose wrinkle, lip corner puller, lip corner
    depressor, and lips part, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: MER is the task of classifying ME clips into various emotion categories. In
    each ME clip, the frame starting facial movements is denoted as the onset frame,
    while the end frame is the offset frame. The frame with the largest intensity
    is the apex frame. Like FER, MER also classifies facial images/sequences into
    categories such as anger, surprise, and happiness. However, MER is more challenging
    as spontaneous MEs are involuntary, subtle, and fleeting. In addition, MEs can
    also be impacted by emotional context and cultural background [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]. Therefore, it is difficult to collect and
    annotate ME data, leading to small-scale ME datasets and existing methods are
    incapable of dealing with subtleness and fleetness.
  prefs: []
  type: TYPE_NORMAL
- en: 'MER has drawn increasing interest recently due to its practical importance
    in many human-computer interaction systems. The first spontaneous MER research
    can be traced to Pfister et al.’s work [[14](#bib.bib14)] which utilized a Local
    Binary Pattern from Three Orthogonal Planes (LBP-TOP) [[15](#bib.bib15)] on the
    first public spontaneous ME dataset: SMIC [[16](#bib.bib16)]. Following the work
    of [[15](#bib.bib15)], various approaches based on appearance and geometry features
     [[17](#bib.bib17), [18](#bib.bib18)] were proposed for improving the performance
    of MER.'
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, with the advance of Deep Learning (DL) and its successful extensions
    on object detection [[19](#bib.bib19)], human tracking [[20](#bib.bib20), [21](#bib.bib21)],
    image retrieval [[22](#bib.bib22), [23](#bib.bib23)] and FER [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], researchers have started to exploit MER with
    DL. Although MER with DL becomes challenging because of the limited ME samples
    and low intensity, great progress on MER has been made through designing effective
    shallow networks, exploring Generative Adversarial Net (GAN) [[27](#bib.bib27)]
    and so on. Currently, DL-based MER has achieved the state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we review the research on MER by DL since 2016 when the DL technology
    was firstly adopted in MER. Due to the page limitation, the representative works
    published in well-known journals and conferences, such as IEEE TPAMI, IEEE TAC,
    IEEE TIP, and ACM MM are specifically discussed. The ordinary FER approaches and
    MER with traditional learning methods are not considered in this survey. Although
    a few MER surveys have discussed the historical evolution and algorithmic pipelines
    for MER [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)], they mainly focus on traditional methods
    and only introduce some recent DL approaches. The DL-based MER has not been discussed
    systematically and specifically. As far as we know, this is the first survey of
    the DL-based MER. Different from previous surveys, we analyze the strengths and
    shortcomings of dynamic network inputs which are important for MER based on DL.
    Furthermore, the network blocks, architectures, training strategies, and losses
    are discussed and summarized in detail and future research directions are identified.
    The goal of this survey is to provide a DL-based MER dictionary that can serve
    as a reference point for future MER research.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a3d225eb9c50d4f1532bb770ec13264.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Examples of ME samples in ME datasets for MER.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper is organized as follows: Section [2](#S2 "2 Datasets ‣ Deep Learning
    for Micro-expression Recognition: A Survey") introduces spontaneous ME datasets.
    Section [3](#S3 "3 A taxonomy for MER based on DL ‣ Deep Learning for Micro-expression
    Recognition: A Survey") presents the taxonomy we defined for MER based on DL.
    Section [4](#S4 "4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") discusses the various inputs for deep MER. Section [5](#S5 "5 Deep networks
    for MER ‣ Deep Learning for Micro-expression Recognition: A Survey") provides
    a detailed review of neural networks for MER. The evaluation matrix, protocol,
    and the performance of representative DL-based MER are described in Section [6](#S6
    "6 Experiments ‣ Deep Learning for Micro-expression Recognition: A Survey"). Section
    [7](#S7 "7 Challenges AND FUTURE DIRECTIONS ‣ Deep Learning for Micro-expression
    Recognition: A Survey") summarizes current challenges and potential study directions.
    Finally, Section 8 discusses the ethical considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Spontaneous Datasets for MER'
  prefs: []
  type: TYPE_NORMAL
- en: '| Database | Resolution | Facial size | Frame rate | Samples | subjects | Expression
    | AU | Apex | Eth | Env |'
  prefs: []
  type: TYPE_TB
- en: '| SMIC HS/NIR/VIS [[16](#bib.bib16)] | $640\times 480$ | $190\times 230$ |
    100/25/25 | 164/71/71 | 16/8/8 | Pos (51) Neg (70) Sur (43) / Pos (28) Neg (23)
    Sur (20) / Pos (28) Neg (24) Sur (19) | $\circ$ | $\circ$ | 3 | L |'
  prefs: []
  type: TYPE_TB
- en: '| CASME [[34](#bib.bib34)] | $640\times 480$ $1280\times 720$ | $150\times
    90$ | 60 | 195 | 35 | Hap (5) Dis (88) Sad (6) Con (3) Fea (2) Ten (28) Sur (20)
    Rep (40) | $\bullet$ | $\bullet$ | 1 | L |'
  prefs: []
  type: TYPE_TB
- en: '| CASME II [[9](#bib.bib9)] | $640\times 480$ | $250\times 340$ | 200 | 247
    | 35 |  Hap (33) Sur (25) Dis (60) Rep (27) Oth (102) | $\bullet$ | $\bullet$
    | 1 | L |'
  prefs: []
  type: TYPE_TB
- en: '| CAS(ME)²  [[35](#bib.bib35)] | $640\times 480$ | - | 30 | Macro 300 Micro
    57 | 22 | Hap (51) Neg (70) Sur (43) Oth (19) | $\bullet$ | $\bullet$ | 1 | L
    |'
  prefs: []
  type: TYPE_TB
- en: '| SAMM [[36](#bib.bib36)] | $2040\times 1088$ | $400\times 400$ | 200 | 159
    | 32 | Hap (24) Ang (20) Sur (13) Dis (8) Fea (7) Sad (3) Oth (84) | $\bullet$
    | $\bullet$ | 13 | L |'
  prefs: []
  type: TYPE_TB
- en: '| MEVIEW [[37](#bib.bib37)] | $720\times 1280$ | - | 25 | 31 | 16 | Hap (6)
    Ang (2) Sur (9) Dis (1) Fea (3) Unc (13) Con(6) | $\bullet$ | $\circ$ | - | W
    |'
  prefs: []
  type: TYPE_TB
- en: '| MMEW [[32](#bib.bib32)] | $1920\times 1080$ | $400\times 400$ | 90 | 300
    | 36 | Hap (36) Ang (8) Sur (80) Dis (72) Fea (16) Sad (13) Oth (102) | $\bullet$
    | $\bullet$ | 1 | L |'
  prefs: []
  type: TYPE_TB
- en: '| Composite ME [[38](#bib.bib38)] | $640\times 480$ $1280\times 720$ $720\times
    1280$ | $150\times 90$ $250\times 340$ $400\times 400$ | 200 | 442 | 68 | Pos
    (109), Neg (250), and Sur (83) | $\circ\bullet$ | $\circ\bullet$ | 13 | L |'
  prefs: []
  type: TYPE_TB
- en: '| Compound ME [[39](#bib.bib39)] | $640\times 480$ $1280\times 720$ $720\times
    1280$ | $150\times 90$ | 200 | 1050 | 90 | Neg (233) Pos (82) Sur (70) PS (74)
    N S (236) PN (197) NN (158) | $\circ\bullet$ | $\circ\bullet$ | 13 | L |'
  prefs: []
  type: TYPE_TB
- en: '¹ Eth: Ethnicity; Env : Environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '² Pos: Positive; Neg: Negative; Sur: Surprise; Hap: Happiness; Dis: Disgust;
    Rep: Repression; Ang: Anger; Fea: Fear; Sad: Sadness; Con: Contempt; Unc: Unclear;
    Oth: Others; PS: Positively surprise; NS Negatively surprise; PN: Positively negative;
    NN: Negatively negative; L:Laboratory; W:In the wild.'
  prefs: []
  type: TYPE_NORMAL
- en: ³ $\circ$ represents unlabeled; $\bullet$ represents labeled and - represents
    unknown
  prefs: []
  type: TYPE_NORMAL
- en: 2 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Different from macro-expressions which can be easily captured in our daily
    life, MEs are involuntary brief FEs, particularly occurring under high stake situations.
    Four early databases appeared continuously around 2010: Canal9 [[40](#bib.bib40)],
    York-DDT [[41](#bib.bib41)], Polikvsky’s database [[41](#bib.bib41)] and USF-HD
    [[42](#bib.bib42)]. However, Canal9 and York-DDT are not aimed for ME research.
    Polikvsky’s database and USF-HD include only posed MEs which are collected by
    asking participants to intentionally pose or mimic a micro movement. The posed
    expressions contradict with the spontaneous nature of MEs. Currently, these databases
    are not used anymore for MER. In the recent years, several spontaneous ME databases
    were created, including: SMIC [[16](#bib.bib16)] and its extended version SMIC-E,
    CASME [[34](#bib.bib34)], CASME II [[9](#bib.bib9)], CAS(ME)² [[35](#bib.bib35)],
    SAMM [[36](#bib.bib36)], and micro-and-macro expression warehouse (MMEW) [[32](#bib.bib32)].
    In this survey, we focus on the spontaneous datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: In a general ME dataset collection procedure, participants are asked to keep
    a poker face while watching video clips to induce spontaneous MEs. The video clips
    are selected according to previous psychological studies, which can elicit strong
    emotions. Commonly, a high-speed camera is utilized to record facial videos. After
    one participant watched a video clip, he/she fills in a self-report questionnaire
    to report his/her true feelings about the video clip. As well, considering cultural
    backgrounds may have an impact on MEs [[43](#bib.bib43)], participants from different
    ethnicities could be recruited [[36](#bib.bib36)] for the potential study of cultural
    impact on MEs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the MEs are subtle and rapid, annotators are usually trained with FACS
    and certified facial action unit coders are employed to detect the MEs in the
    facial videos. The FACS helps people look precisely at the facial movements to
    make ME detection reliable. Specifically, when the duration of the facial action
    unit is less than 0.5s, the clip is regarded as a ME clip. The MEs are annotated
    into discrete categories. In SMIC [[16](#bib.bib16)], the emotions are labeled
    as ‘positive’, ‘negative’, and ‘surprise’ according to the participants’ self-reports.
    However, mixed emotions may be induced while the participants watch one video
    clip. Annotations based on the general emotion reported after watching the video,
    which usually allows one emotion, are not accurate. To this end, several datasets,
    such as CASME [[34](#bib.bib34)] and CASME II [[9](#bib.bib9)], consider AUs,
    self-reports, and the watched video clips to label the MEs. When there are ambiguities
    and conflicts in the emotion annotation, the emotion is annotated as ‘others’.
    Furthermore, to alleviate annotation bias caused by an individual annotator, the
    ME annotations are always carried out through cross-validation by multiple annotators.
    The specific details of datasets are introduced as followings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SMIC [[16](#bib.bib16)] is consisted of three subsets: SMIC-HS, SMIC-VIS and
    SMIC-NIR. SMIC-VIS and SMIC-NIR contain 71 samples recorded by normal speed cameras
    with 25 fps of visual (VIS) and near-inferred light range (NIR), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: CASME [[34](#bib.bib34)] contains spontaneous 159 ME clips from 19 subjects
    including frames from onset to offset. The emotions were labeled partly based
    on AUs and also taking account of participants’ self-reports and the content of
    the video episodes. Besides the onset and offset, the apex frames are also labeled.
    The shortcoming of CASME is the imbalanced sample distribution among classes.
  prefs: []
  type: TYPE_NORMAL
- en: CASME II [[9](#bib.bib9)] is an improved version of the CASME dataset. Samples
    in CASME II are increased to 247 MEs from 26 subjects and they are recorded by
    high-speed camera at 200 fps with face sizes cropped to $280\times 340$. Thus,
    it has a greater temporal and spatial resolution, compared with CASME.
  prefs: []
  type: TYPE_NORMAL
- en: CAS(ME)² [[35](#bib.bib35)] consists of spontaneous macro- and micro-expressions
    elicited from 22 subjects. CAS(ME)² has samples with longer durations which makes
    it suitable for ME spotting. Compared to the above datasets, the samples in CAS(ME)²
    were recorded with a relatively low frame rate in a relatively small number of
    ME samples, which makes it unsuitable for DL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: SAMM [[36](#bib.bib36)] collects 159 ME samples from 32 participants. The samples
    were collected by a gray-scale camera at 200 fps in controlled lighting conditions
    to prevent flickering. Unlike previous datasets that lack ethnic diversity, the
    participants are from 13 different ethnicities.
  prefs: []
  type: TYPE_NORMAL
- en: MEVIEW [[37](#bib.bib37)] is in-the-wild ME dataset. The samples in MEVIEW are
    collected from poker games and TV interviews on the Internet. In total, 31 videos
    from 16 individuals were annotated in the dataset and the average length of videos
    is three seconds.
  prefs: []
  type: TYPE_NORMAL
- en: MMEW [[32](#bib.bib32)] contains 300 ME and 900 macro-expression samples acted
    out by the same participants with a larger resolution ($1920\times 1080$ pixels).
    MEs and macro-expressions in MMEW were annotated to the same emotion classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The composite dataset [[38](#bib.bib38)] is proposed by the 2nd Micro-Expression
    Grand Challenge (MEGC2019). The composite dataset merges samples from three spontaneous
    facial ME datasets: CASME II [[9](#bib.bib9)], SAMM [[36](#bib.bib36)], and SMIC-HS
    [[16](#bib.bib16)]. This is to facilitate the evaluation of newly developed methods.
    As the annotations in the three datasets vary hugely, the composite dataset unifies
    emotion labels in all three datasets. The emotion labels are re-annotated as positive,
    negative, and surprise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The compound micro-expression dataset (CMED) [[39](#bib.bib39), [44](#bib.bib44)]
    is constructed by combining MEs from the CASME, CASME II, CAS(ME)², SMIC-HS, and
    SAMM datasets. Specifically, the MEs are divided into basic and compound emotional
    categories, as shown in Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning
    for Micro-expression Recognition: A Survey"). Psychological studies demonstrate
    that there are usually complex expressions in daily life. Multiple emotions co-exist
    in one FE, termed as “compound expressions” [[44](#bib.bib44)]. Compound expression
    analysis reflects more complex mental states and more abundant human facial emotions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific comparisons of the ME datasets are shown in Table [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ Deep Learning for Micro-expression Recognition: A Survey")
    and example samples are shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep Learning for Micro-expression Recognition: A Survey"). Although MEVIEW
    collects MEs in the wild, the number of ME samples is too small to learn robust
    ME features. The state-of-the-art approaches are commonly tested on the SMIC-HS,
    CASME [[34](#bib.bib34)], CASME II [[9](#bib.bib9)], and SAMM databases. As some
    emotions are difficult to trigger, such as fear and contempt, these categories
    have only a few samples and are not enough for learning. In most practical experiments,
    only the emotion categories with more than 10 samples are considered. Recently,
    the composite dataset is popular, because it can verify the generalization ability
    of the method on datasets with different natures. For further increasing the MER
    performance, MMEW collected micro- and macro-expressions from the same subjects
    which may be helpful for further cross-modal research.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg   height="1" overflow="visible" version="1.1"
    width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="0" height="0" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">\Tree</foreignobject> <foreignobject width="8.3"
    height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|  |
  prefs: []
  type: TYPE_NORMAL
- en: '|  |</foreignobject> <foreignobject width="-1.54" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">[45](#bib.bib45)[46](#bib.bib46)[47](#bib.bib47)[48](#bib.bib48)[49](#bib.bib49)[50](#bib.bib50)[51](#bib.bib51)[52](#bib.bib52)[48](#bib.bib48)[53](#bib.bib53)[54](#bib.bib54)[55](#bib.bib55),
    [56](#bib.bib56)[57](#bib.bib57)[58](#bib.bib58)[59](#bib.bib59)[60](#bib.bib60)[61](#bib.bib61)[58](#bib.bib58),
    [27](#bib.bib27)</foreignobject> <foreignobject width="40.55" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| [62](#bib.bib62)[17](#bib.bib17), [63](#bib.bib63)[64](#bib.bib64)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67) |</foreignobject> <foreignobject
    width="-2.31" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[59](#bib.bib59),
    [64](#bib.bib64), [68](#bib.bib68), [69](#bib.bib69)[61](#bib.bib61), [70](#bib.bib70),
    [58](#bib.bib58), [71](#bib.bib71), [72](#bib.bib72)[73](#bib.bib73)[74](#bib.bib74)[75](#bib.bib75)[76](#bib.bib76),
    [77](#bib.bib77)[78](#bib.bib78)</foreignobject> <foreignobject width="41.7" height="24.91"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| [79](#bib.bib79)[80](#bib.bib80)[60](#bib.bib60)[81](#bib.bib81)[82](#bib.bib82)
    |</foreignobject>  <foreignobject width="43.63" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| [83](#bib.bib83)[84](#bib.bib84)[71](#bib.bib71)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [85](#bib.bib85) |</foreignobject>  <foreignobject width="47.46" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| [86](#bib.bib86), [76](#bib.bib76)[81](#bib.bib81)[78](#bib.bib78)[83](#bib.bib83)[60](#bib.bib60)[87](#bib.bib87),
    [88](#bib.bib88), [61](#bib.bib61)[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [92](#bib.bib92) |</foreignobject> <foreignobject width="-6.92" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[64](#bib.bib64), [93](#bib.bib93),
    [74](#bib.bib74)[94](#bib.bib94), [88](#bib.bib88), [87](#bib.bib87)[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97)[98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101)[102](#bib.bib102)[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105)[106](#bib.bib106),
    [107](#bib.bib107)[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)[111](#bib.bib111)[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [105](#bib.bib105),
    [116](#bib.bib116)</foreignobject><g stroke="#000000" fill="#000000" color="#000000"><foreignobject
    width="-2.69" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[58](#bib.bib58)[117](#bib.bib117)[76](#bib.bib76)[91](#bib.bib91)[109](#bib.bib109)[87](#bib.bib87),
    [74](#bib.bib74)[118](#bib.bib118), [119](#bib.bib119), [69](#bib.bib69)[120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122)[123](#bib.bib123)[124](#bib.bib124), [125](#bib.bib125)[126](#bib.bib126),
    [127](#bib.bib127), [128](#bib.bib128)[129](#bib.bib129), [91](#bib.bib91)</foreignobject></g></g></svg>![Refer
    to caption](img/69e108b8a9d56412a08a729c1304a9f3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Taxonomy for MER based on deep learning. The studies cited on the
    branches are example approaches discussed in this paper. The future directions
    and corresponding approaches are shown on the right side. The future directions
    are annotated in brackets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 A taxonomy for MER based on DL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. 3 shows a taxonomy we summarize for MER based on DL, built along the important
    components including input and network. As the ME sequences have subtle movements
    and limited samples, different inputs have big impacts on MER performance. Thus
    the input plays an important role in MER. Firstly, the inputs need to be pre-processed
    for training a robust network. The specific pre-processing approaches and the
    strengths and shortcomings of various input modalities are discussed in Section
    [4](#S4 "4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey").
    Then, the networks introduced in Section [5](#S5 "5 Deep networks for MER ‣ Deep
    Learning for Micro-expression Recognition: A Survey") are utilized to discriminate
    between MEs. A common MER network can be described from four aspects: block, architecture,
    training strategy, and loss. Firstly, we introduce the special blocks designed
    to solve the ME challenges. Then, we describe the architecture in terms of single-stream,
    multi-stream, cascaded networks, and multi-task learning. Finally, the training
    strategies and loss functions for training networks are discussed. The future
    directions are annotated on the right side of Fig. 3\. All the methods discussed
    in this survey are face-based MER with DL.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Inputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Pre-processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like ordinary FEs, pre-processing involving face detection and alignment is
    required for robust MER. Compared with common FEs, MEs have low intensity, short
    duration, and small-scale datasets making MER more difficult. Therefore, besides
    traditional pre-processing steps, motion magnification, temporal normalization,
    regions-of-interest, and data augmentation have also been undertaken for better
    MER performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Face detection and registration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For processing MEs, face detection which removes the background and gets the
    facial region is the first step. One of the most widely used algorithms for face
    detection is Viola-Jones [[45](#bib.bib45)] based on a cascade of weak classifiers.
    However, this method can not deal with large pose variations and occlusions. Matsugu et
    al.  [[46](#bib.bib46)] firstly adopted CNN network for face detection with a
    rule-based algorithm, which is robust to translation, scale, and pose. Recently,
    face detectors based on DL have been utilized in popular open source libraries,
    such as dlib and OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Since spontaneous MEs involve muscle movements of low intensity, even little
    pose variations and movements may heavily affect MER performance. To this end,
    face registration is crucial for MER. It aligns the detected faces onto a reference
    face to handle varying head-pose issues for successful MER. Currently, one of
    the most used facial registration methods is the Active Shape Models (ASM) [[47](#bib.bib47)]
    encoding both geometry and intensity information. Then, the Active Appearance
    Models (AAM) [[48](#bib.bib48)] is presented for matching any face with any expression
    rapidly. With the fast development of DL, deep networks with cascaded regression
    [[49](#bib.bib49)] have become the state-of-the-art methods for face alignment
    due to their excellent performances.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Motion magnification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One challenge for MER is that the facial movements of MEs are too subtle to
    be distinguished. Therefore, motion magnification is important to enhance the
    ME intensity level. One of the commonly used methods is the Eulerian Video Magnification
    method (EVM) [[52](#bib.bib52)]. For MEs, the EVM is applied for facial motion
    magnification [[17](#bib.bib17)]. EVM magnifies either motion or color content
    across two consecutive frames in videos. However, a larger motion amplification
    level leads to a larger scale of motion amplification, which causes bigger artifacts.
    Different from EVM considering local magnification, Global Lagrangian Motion Magnification
    (GLMM) [[130](#bib.bib130)] was proposed for consistently tracking and exaggerating
    the FEs and global displacements across a whole video. Furthermore, the learning-based
    motion magnification [[53](#bib.bib53)] was firstly used in ME magnification by
    Lei et al.  [[89](#bib.bib89)] through extracting shape representations from the
    intermediate layers of networks. Compared with the traditional methods, the shape
    representations from the intermediate layers introduce less noise.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Temporal Normalization (TN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Besides the low intensity, the short and varied duration also increases the
    difficulty for robust MER. This problem is especially serious when the videos
    are filmed with relatively low frame rate. To solve this issue, the Temporal Interpolation
    Model[[50](#bib.bib50)] (TIM) was introduced to interpolate all ME sequences into
    the same specified length based on path graph between the frames. There are three
    strengths of applying TIM: 1) up-sampling ME clips with too few frames; 2) more
    stable features can be expected with a unified clip length; 3) extending ME clips
    to long sequences and sub-sampling to short clips for data augmentation. Additionally,
    CNN-based temporal interpolation [[51](#bib.bib51)] have been proposed to solve
    complex scenarios in reality.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6110fd5e3da0970c6dae548f57d4fc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Examples of regions of interest. (a) Equal block; (b) FACS-based
    RoIs [[55](#bib.bib55)]; (c) RoIs Masked eye and cheek [[93](#bib.bib93)]; (d)
    Eye and mouth [[131](#bib.bib131)]; (e) Difference-based ME datasets [[132](#bib.bib132)];
    (f) landmark-based local regions [[133](#bib.bib133)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Regions of interest (RoIs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FEs are formulated by basic facial movements [[134](#bib.bib134), [6](#bib.bib6)],
    which correspond to specific facial muscles and relate to different facial regions.
    In other words, not all facial regions contribute equally to FER. Especially for
    MEs, the MEs only trigger specific small regions, as MEs involve subtle facial
    movements. Moreover, the empirical experience and quantitative analysis in [[135](#bib.bib135)]
    found that the outliers such as eyeglass have a seriously negative impact on the
    performance of MER. Therefore, it is important to suppress the influence of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some studies alleviate the influence of regions without useful information
    by extracting features on the RoIs [[93](#bib.bib93)]. Several MER approaches
    [[17](#bib.bib17), [136](#bib.bib136)] divided the entire face into several equal
    blocks for better describing local changes (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3
    Temporal Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (a)). Davison et al.  [[55](#bib.bib55),
    [56](#bib.bib56)] selected RoIs from the face based on the FACS [[6](#bib.bib6)],
    shown in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (b). In
    addition, to eliminate the noise caused by the eye blinking and motion-less regions,
    Le et al.[[93](#bib.bib93)] proposed to mask the eye and cheek regions for each
    image (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1
    Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (c)). However, the motion of eyes has a big contribution to MER under
    certain situations, e.g. lid tighten refers to negative emotion. In work [[131](#bib.bib131)],
    Liong et al.  utilized the eyes and mouth regions for MER, as shown in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs
    ‣ Deep Learning for Micro-expression Recognition: A Survey") (d). Besides, Xia et
    al. [[132](#bib.bib132)] found that the regions around the eyes, nose, and mouth
    are mostly active for MEs and can be chosen as RoIs through analyzing difference
    heat maps of ME datasets, as shown in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal
    Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (e). Furthermore, Xie et al. [[58](#bib.bib58)] and Li et
    al. [[133](#bib.bib133)] proposed to extract features on small facial blocks located
    by facial landmarks (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization
    (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (f)). In this way, the dimension of learning space can be drastically
    reduced and helpful for deep model learning on small ME datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Data augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main challenge for MER with DL is the small-scale ME datasets. The current
    ME datasets are too limited to train a robust DL model from scratch, therefore
    data augmentation is necessary. The common way for data augmentation is random
    crop and rotation in terms of the spatial domain. Xia et al. augmented MEs through
    magnifying MEs with multiple ratios [[60](#bib.bib60)]. Fig. [5](#S4.F5 "Figure
    5 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (a) and (b) show the examples of magnified
    ME apex frames with different ratios on the basis of EVM [[52](#bib.bib52)] and
    learning-based magnification [[53](#bib.bib53)], respectively. Additionally, Generative
    Adversarial Network (GAN) [[137](#bib.bib137)] can augment data by producing synthetic
    images. Xie et al. [[58](#bib.bib58)] introduced the AU Intensity Controllable
    GAN (AU-ICGAN) to synthesize subtle MEs. As Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.5
    Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (d) shows, the ME sequences with continuous AU intensity
    can be synthesized through [[58](#bib.bib58)]. Yu et al. [[27](#bib.bib27)] proposed
    a Identity-aware and Capsule-Enhanced Generative Adversarial Network (ICE-GAN)
    to complete the ME synthesis and recognition tasks. ICE-GAN outperformed the winner
    of MEGC2019 by 7$\%$, demonstrating the effectiveness of GAN for ME augmentation
    and recognition. The synthesized images corresponding to different emotions are
    shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (c). Besides,
    Liong et al. [[138](#bib.bib138)] utilized conditional GAN to generate optical-flow
    images to improve the MER accuracy based on computed optical flow. For ME clips,
    sub-sampling MEs from extended ME sequences through TIM can augment ME sequences [[61](#bib.bib61)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3481bdd8aae98979687d886972e517d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Examples of magnified and synthesized MEs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d99e44617d9348160bd863b4c958d81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Examples of various inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Input modality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the MEs have low intensity, short duration, and limited data, it is challenging
    to recognize MEs based on DL and the MER performance varies with different inputs.
    In this section, we describe the various ME inputs and summarize their strengths
    and shortcomings, as shown in Table [II](#S4.T2 "TABLE II ‣ 4.2.2 Dynamic image
    sequence ‣ 4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Static image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For FER, a large volume of existing studies are conducted on static images
    without temporal information due to the availability of the massive facial images
    online and the convenience of data processing. Inspired by efficient FER with
    static images, some researchers [[64](#bib.bib64), [68](#bib.bib68)] explored
    the MER based on the apex frame with the largest intensity of facial movement
    among all frames (See Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1
    Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (a)). Li et al. [[59](#bib.bib59)] studied the contribution of the apex
    frame and verified that DL can achieve good MER performance with the single apex
    frame. Furthermore, the research of Sun et al.  [[69](#bib.bib69)] showed that
    the apex frame-based methods can effectively utilize the massive static images
    in macro-expression databases [[69](#bib.bib69)] and obtain better performance
    than onset-apex-offset sequences and the whole videos.'
  prefs: []
  type: TYPE_NORMAL
- en: Apex spotting is one of the key components for building a robust MER system
    based on apex frames. Patel et al. [[62](#bib.bib62)] computed the motion amplitude
    of optical flow shifted over time to locate the onset, apex, and offset frames
    of MEs, while other works [[17](#bib.bib17), [63](#bib.bib63)] exploited feature
    differences to detect MEs in long videos. However, optical flow-based approaches
    required complicated feature operation and the feature contrast-based methods
    ignored ME dynamic information. Different from above methods estimating the facial
    change in the spatio-temporal domain, Li et al. [[64](#bib.bib64)] proposed to
    locate the apex frame in rapid ME clips through exploring the information in the
    frequency domain which clearly describes the rate of change. Furthermore, SMEConvNet
    [[65](#bib.bib65)] firstly adopted CNN for ME spotting and a feature matrix processing
    was proposed for locating the apex frame in long videos. Following SMEConvNet,
    various CNN-based ME spotting methods [[66](#bib.bib66), [67](#bib.bib67)] were
    proposed. In general, the performance of CNN-based spotting method is limited
    because of the small-scale ME datasets and mixed macro- and micro-expressions
    clips in long videos. Further studies on reliable spotting methods are required
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Dynamic image sequence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the facial movements are subtle in the spatial domain, while change fast
    in the temporal domain, the temporal dynamics along the video sequences are essential
    in improving the MER performance. In this subsection, we describe the various
    dynamic inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence. Most ME researches utilize consecutive frames in video clips [[139](#bib.bib139),
    [54](#bib.bib54), [140](#bib.bib140)], as shown in Fig. [6](#S4.F6 "Figure 6 ‣
    4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (b). With the success of 3D CNN [[141](#bib.bib141)] and
    Recurrent Neural Network (RNN) [[142](#bib.bib142)] in video analysis [[143](#bib.bib143),
    [144](#bib.bib144)], MER based on sequence [[107](#bib.bib107), [61](#bib.bib61),
    [70](#bib.bib70), [58](#bib.bib58), [71](#bib.bib71), [72](#bib.bib72), [145](#bib.bib145)]
    is developed that considers the spatial and temporal information simultaneously.
    However, the computation cost is relatively high and the complex model tends to
    overfit the small-scale training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Frame aggregation. MEs are mostly collected with a high-speed camera (e.g. 200
    fps) to capture the rapid subtle changes. Liong et al. discovered that there is
    redundant information in ME clips recorded with high-speed cameras [[146](#bib.bib146)].
    The redundancy could decrease the performance of MER. The experimental results
    of [[146](#bib.bib146)] demonstrate that the onset, apex, and offset frames can
    provide enough spatial and temporal information to ME classification. Liong et
    al. [[73](#bib.bib73)] extracted features on onset and apex frames for MER, as
    shown in  Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (c). Furthermore,
    in order to avoid apex frame spotting, Liu et al. [[74](#bib.bib74)] and Kumar et
    al. [[75](#bib.bib75)] designed simple strategies to select aggregated frames
    automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image with dynamic information. Image with dynamic information [[147](#bib.bib147)]
    is a standard image that holds the dynamics of an entire video sequence in a single
    instance. The dynamic image generated by using the rank pooling algorithm has
    been successfully used in MER [[76](#bib.bib76), [93](#bib.bib93), [77](#bib.bib77),
    [148](#bib.bib148)] to summarize the subtle dynamics and appearance in an image.
    Similar to dynamic images, active images [[78](#bib.bib78)] encapsulated the spatial
    and temporal information of a video sequence into a single instance through estimating
    and accumulating the change of each pixel component (See Fig. [6](#S4.F6 "Figure
    6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (d)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: The comparisons of inputs for MER'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input modality | Strength | Shortcoming |'
  prefs: []
  type: TYPE_TB
- en: '| Static | Efficient; Take advantage of massive facial images | Require magnification
    and apex detection Without temporal information |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic | Sequence | Process directly | Not efficient; Information redundancy
    |'
  prefs: []
  type: TYPE_TB
- en: '| Frame aggregation | Efficiently leverage key temporal information | Require
    apex detection |'
  prefs: []
  type: TYPE_TB
- en: '| Image with dynamic information | Efficiently embed spatio-temporal information
    | Require dynamic information computation |'
  prefs: []
  type: TYPE_TB
- en: '| Optical flow | Remove identity to some degree; Movement considered | Optical
    flow computation is necessary |'
  prefs: []
  type: TYPE_TB
- en: '| Combination | Explore spatial and temporal information | High computation
    cost |'
  prefs: []
  type: TYPE_TB
- en: 'Optical flow. The motion between ME frames contributes important information
    for ME recognition. Optical flow approximates the local image motion, which has
    been verified to be helpful for motion representation [[149](#bib.bib149)]. It
    specifies the magnitude and direction of pixel motion in a given sequence of images
    with a two-dimension vector field (horizontal and vertical optical flows). In
    recent years, several novel methodologies have been presented to improve optical
    flow techniques [[79](#bib.bib79), [150](#bib.bib150), [151](#bib.bib151), [80](#bib.bib80),
    [152](#bib.bib152)], such as Farnebäck’s [[80](#bib.bib80)], Lucas-Kanade [[79](#bib.bib79)],
    TV-L1 [[152](#bib.bib152)], FlowNet [[82](#bib.bib82)], as shown in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (e). Currently, many MER approaches
    utilize optical flow to represent the micro-facial movement and reduce the identity
    characteristic [[60](#bib.bib60), [99](#bib.bib99), [153](#bib.bib153)]. Researches [[60](#bib.bib60),
    [99](#bib.bib99)] indicated that optical flow-based methods always outperform
    appearance-based methods. To further capture the subtle facial changes, multiple
    works [[81](#bib.bib81), [106](#bib.bib106), [91](#bib.bib91)] extracted features
    on computed optical flows on the onset and mid-frame/apex in horizontal and vertical
    directions separately.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Input combination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Considering the strengths of apex frame and dynamic image sequences, some works
    [[99](#bib.bib99), [71](#bib.bib71), [83](#bib.bib83), [84](#bib.bib84)] analyze
    multiple inputs to learn features from different cues in ME videos. Specifically,
    in Liu et al.’s work [[83](#bib.bib83)], the apex frames and optical flow are
    utilized to extract static-spatial and temporal features, respectively. Besides
    the above modalities, Song et al. [[99](#bib.bib99)] added local facial regions
    of the apex frame as inputs to embed the relationship of individual facial regions
    for increasing the robustness of MER. In addition, Sun et al. [[84](#bib.bib84)]
    employed optical flow and sequences for fully exploring the temporal ME information.
    Recently, inspired by the successful application of landmarks in facial analysis
    (See Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (f)),
    Kumar et al. [[85](#bib.bib85)] proposed to fuse the landmark graph and optical
    flow to enhance the discriminative ability of ME repression. Currently, the approaches
    with multiple inputs achieve the best MER performance through leveraging as much
    as ME information on limited ME datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In summary, the input is one of the key components to guarantee robust MER.
    The various ME inputs have different strengths and shortcomings. The comparisons
    of inputs are shown in Table [II](#S4.T2 "TABLE II ‣ 4.2.2 Dynamic image sequence
    ‣ 4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The input pre-processing is the first step in the MER system. Besides common
    face pre-processing approaches (face detection and registration), motion magnification,
    RoIs, and TIM also play important roles for robust MER, due to the subtle and
    rapid characteristics of MEs. Current motion magnification approaches always introduce
    noises and artifacts. More effective motion magnification approaches should be
    explored. Furthermore, considering the small-scale ME datasets are far from enough
    to train a robust deep model, data augmentation is necessary for MER. In the future,
    studying more robust GAN-based ME generation approaches is a promising research
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the static input, the apex-based MER can reduce computational complexity
    and take advantage of the massive FEs to resolve the small-dataset issue in some
    degree. But, magnification is necessary since all the temporal information is
    dropped in single apex-based methods and the motion intensity is still low in
    the apex frames. Moreover, as the apex label is absent in some ME datasets, the
    performance of apex-based MER severely relies on the apex detection algorithm.
    Currently, the apex frame detection in long videos is still challenging. The end-to-end
    framework for apex frame detection and MER needs to be further studied.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with the static image, the dynamic input is able to leverage spatial
    and temporal information for robust MER. The simplest dynamic input is ME sequence
    which doesn’t require extra operations. However, there is redundancy in ME sequences,
    and the complexity of the deep model is relatively high and tends to overfit on
    small-scale ME datasets. To solve the problem of redundancy, frame aggregation
    cascading multiple key frames is utilized. Besides, the dynamic image improves
    the computation efficiency through embedding the temporal and spatial information
    to a still image. It can simultaneously consider spatial and temporal information
    in one image without challenging apex frame detection. Furthermore, optical flow
    is widely used for MER as the optical flow describes the motions and removes the
    identity in some degree. However, most of the current optical flow-based MER methods
    are based on traditional optical flow, which is not end-to-end. In the future,
    more DL-based optical flow extraction can be further researched.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, combining various inputs is the inevitable trend to fully explore
    spatial and temporal information and leverage the merits of various inputs. Correspondingly,
    the combined inputs also inherit the shortcomings of the inputs. However, the
    multiple inputs could be complementary in some degree. So far, the method with
    various inputs has achieved the best performance. Considering the success of multiple
    inputs and limited ME samples, more combined modalities, such as optical flow,
    key frames, and landmarks can be promising research directions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9a52c0d2d2d1d09ab961a745d48b775.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Special blocks: (a) Residual block [[154](#bib.bib154)] (b) Inception
    module[[155](#bib.bib155)]; (c) RCN [[60](#bib.bib60)]; (d) Spatial attention
    of CBAM [[87](#bib.bib87)] (e) Channel attention of CBAM [[87](#bib.bib87)]; (f)
    Capsule module [[156](#bib.bib156)]; (g) Spatio-temporal attention [[94](#bib.bib94)];
    (h) GCN [[78](#bib.bib78)].'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Deep networks for MER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) have shown excellent performances for various
    computer vision tasks, such as action recognition [[157](#bib.bib157)] and FER
    [[24](#bib.bib24)]. In general, for image classification, CNNs employ two dimensional
    convolutional kernels (denoted as 2D CNN) to leverage spatial context across the
    height and width of the images to make predictions. Compared with 2D CNN, CNNs
    with three-dimensional convolutional kernels (denoted as 3D CNN) are verified
    more effective for exploring spatio-temporal information of videos [[158](#bib.bib158)].
    3D CNN can take advantage of spatio-temporal information to improve the performance
    but comes with a computational cost because of the increased number of parameters.
    Moreover, the 3D CNN only can deal with videos with the fixed length due to the
    pre-defined kernels. Recurrent Neural Network (RNN) [[142](#bib.bib142)] was proposed
    to process the time series data with various duration. Furthermore, Long Short-Term
    Memory (LSTM) was developed to settle the vanishing gradient problem that can
    be encountered when training RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike common video-based classification problems, for the recognition of subtle,
    fleeting, and involuntary MEs, various DL approaches have been proposed to boost
    MER performance. In this section, we introduced the approaches in the view of
    special blocks, network architecture, training strategy, and loss.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Network block
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In terms of solving the two main ME challenges: overfitting on small-scale
    ME datasets and low intensity of MEs, various effective network blocks have been
    utilized and designed, such as ResNet family with residual modules [[154](#bib.bib154),
    [159](#bib.bib159), [160](#bib.bib160)], and Inception module [[155](#bib.bib155)].
    In this subsection, we introduce the special network blocks utilized for MER improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the challenge of small-scale datasets, recent researches [[154](#bib.bib154)]
    demonstrate that residual blocks with shortcut connections (shown in Fig. [7](#S4.F7
    "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (a)) achieves easy optimization and reduces the effect of the vanishing
    gradient problem. Multiple MER works [[104](#bib.bib104), [161](#bib.bib161),
    [162](#bib.bib162), [89](#bib.bib89), [163](#bib.bib163)] employed residual blocks
    for robust recognition on small-scale ME datasets. Instead of directly applying
    the shortcut connection, [[164](#bib.bib164)] further designed a convolutionable
    shortcut to learn the important residual information and AffectiveNet [[165](#bib.bib165)]
    introduced an MFL module learning the low- and high-level feature parallelly to
    increase the discriminative capability between the inter and intra-class variations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the fully connected layer requires lots of parameters which makes it
    prone to extreme loss explosion and overfitting [[166](#bib.bib166)], the Inception
    module [[167](#bib.bib167)] aggregates different sizes of filters to compute multi-scale
    spatial information and assembles $1\times 1\times 1$ convolutional filters to
    reduce the dimension and parameter, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3
    Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey")
    (b). Multiple works [[81](#bib.bib81), [91](#bib.bib91)] utilized the Inception
    module for efficient MER. Inspired by the Inception structure, a Hybrid Feature
    (HyFeat) block [[168](#bib.bib168), [77](#bib.bib77), [78](#bib.bib78)] was proposed
    to preserve the domain knowledge features for expressive regions of MEs and enrich
    features of edge variations through using different scaled convolutional filters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, considering the fact that CNN with more convolutional layers has
    stronger representation ability, but easy to overfit on small-scale datasets,
    paper [[60](#bib.bib60)] and [[169](#bib.bib169)] introduced Recurrent Convolutional
    Network (RCN) which achieved a shallow architecture though recurrent connections,
    as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (c).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, MEs perform as the combination of multiple facial movements.
    The latent semantic information among subtle facial changes contributes important
    information for MER performance. Recent researches illustrate that the Graph Convolutional
    Network (GCN) is effective to model these semantic relationships and can be leveraged
    for face analysis tasks, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (h). Inspired
    by the successful application of GCN in FER, [[89](#bib.bib89), [58](#bib.bib58),
    [90](#bib.bib90), [91](#bib.bib91)] developed the GCN for MER to further improve
    the performance by modeling the relationship between the local facial movements.
    Lei et al. [[89](#bib.bib89), [92](#bib.bib92)] built graphs on the RoIs along
    facial landmarks contributing information to subtle MEs. The TCN residual blocks
    [[170](#bib.bib170), [89](#bib.bib89)] and transformer [[171](#bib.bib171), [92](#bib.bib92)]
    were applied for reasoning the relationships of RoIs. On the other hand, as the
    FE analysis can be benefited from the knowledge of AUs and FACS, the works [[58](#bib.bib58),
    [90](#bib.bib90), [91](#bib.bib91)] built graph on AU-level representations to
    boost the MER performance by inferring the AU relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides graph, Capsule Neural Network (CapsNet) [[156](#bib.bib156)] was employed
    to explore part-whole relationships on face to promote MER performance through
    better model hierarchical relationships by routing procedure [[68](#bib.bib68),
    [83](#bib.bib83), [27](#bib.bib27)], as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3
    Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey")
    (f).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, since MEs have specific muscular activations on the face, MEs are
    related with local regional changes [[172](#bib.bib172)]. Therefore, it is crucial
    to highlight the representation on RoIs [[8](#bib.bib8), [113](#bib.bib113)].
    Several approaches [[103](#bib.bib103), [173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)] have shown the benefit
    of enhancing spatial encoding with attention module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Except for spatial information, the temporal change also plays an important
    role for MER. As MEs have rapid changes, the frames have unequal contribution
    to MER. Wang et al. [[94](#bib.bib94)] explored a global spatial and temporal
    attention module (GAM) based on the non-local network [[178](#bib.bib178)] to
    encode wider spatial and temporal information to capture local high-level semantic
    information, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs
    ‣ Deep Learning for Micro-expression Recognition: A Survey") (g).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, Yao et al. [[179](#bib.bib179)] learned the weights of each feature
    channel adaptively through adding squeezeand-and-excitation blocks. Additionally,
    recent works [[87](#bib.bib87), [88](#bib.bib88), [61](#bib.bib61), [180](#bib.bib180)]
    encoded the spatio-temporal and channel attention simultaneously to further boost
    the representational power of MEs. Specifically, CBAMNet [[87](#bib.bib87)] presented
    a convolutional block attention module (CBAM) cascading the spatial attention
    module (see Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (d)) and channel attention module
    (see Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (e)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1392597ca9f53d7a987ce0ec0b3c9373.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: (a) GAM based on single stream [[94](#bib.bib94)]; (b) CNN cascaded
    with LSTM [[114](#bib.bib114)]; (c) TSCNN [[99](#bib.bib99)] based on three-stream
    network; (d) A dual-stream multi-task network incorporating gender detection designed
    by GEME [[76](#bib.bib76)]; (e) CNN cascaded with GCNs on the basis of AU-feature
    graph [[58](#bib.bib58)]; (f) MER based on knowledge distillation [[69](#bib.bib69)];
    and (g) The general concept of single-stream, multi-stream, cascaded networks,
    multi-task learning, and the training strategy of transferring knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, due to the special characteristics of MEs, many DL-based methods
    designed special blocks to extract discriminative ME representations from the
    latent semantic information. Recent MER researches indicate that attention and
    graph blocks are effective to model the semantic relationships. Current GCN-based
    MER are always based on the local facial regions and AU labels. In the future,
    more compact and concise representation, such as landmark location, can be further
    developed for efficient MER. Moreover, the transformer [[171](#bib.bib171)] has
    been verified effectively on modeling the relationship. For future MER research,
    transformers can be further applied to model the relationships between facial
    landmarks, AUs, RoIs and frames to enhance ME representation. On the other hand,
    other special blocks [[60](#bib.bib60)] targeted at learning discriminative ME
    features with less parameters to avoid overfitting. In the future, more efficient
    blocks should be studied to dig subtle ME movements on limited ME datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Network architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides designing special blocks for discriminative ME representation, the
    way of combining the blocks is also very important. The current network architecture
    of MER methods can be classified to five categories: single-stream, multi-stream,
    cascaded, multi-task learning and transfer learning. In this section, we will
    discuss the details of the five network architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Single-stream networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typical deep MER methods adopt single CNN with individual input [[181](#bib.bib181)].
    The apex frame, optical flow images and dynamic images are common inputs for single-stream
    2D CNNs, while single-stream 3D CNNs extract the spatial and temporal features
    from ME sequences directly. Considering the limited ME samples are far from enough
    to train a robust deep network, multiple works designed single-stream shallow
    CNNs for MER [[182](#bib.bib182), [140](#bib.bib140), [183](#bib.bib183)]. Belaiche et
    al. [[161](#bib.bib161)] achieved a shallow network through deleting multiple
    convolutional layers of the deep network Resnet. Zhao et al.  [[44](#bib.bib44)]
    proposed a 6-layer CNN in which the input is followed by an $1\times 1$ convolutional
    layer to increase the non-linear representation.
  prefs: []
  type: TYPE_NORMAL
- en: Besides designing shallow networks, many studies [[64](#bib.bib64), [74](#bib.bib74),
    [93](#bib.bib93)] fine-tuned deep networks pre-trained on large face datasets
    to avoid the overfitting problem. Li et al. [[64](#bib.bib64)] firstly adopted
    the 16-layer VGG-FACE model pre-trained on VGG-FACE dataset [[184](#bib.bib184)]
    for MER. Following [[64](#bib.bib64)], the MER with Resnet50, SEnet50 and VGG19
    pre-trained on Imagenet was explored in [[93](#bib.bib93)]. The results illustrate
    that VGG surpasses other architectures regarding the MER topic and is good at
    distinguishing the complex hidden information in data.
  prefs: []
  type: TYPE_NORMAL
- en: All of above works are based on 2D CNN with image input, while several works
    employed single 3D CNN to directly extract the spatial and temporal features from
    ME sequences. GAM [[94](#bib.bib94)], MERANet [[88](#bib.bib88)] and CBAMNet [[87](#bib.bib87)]
    combined attention modules to 3D CNN to enhance the representation in spatial
    and temporal dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Multi-stream network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Single stream is a basic model structure and only extracts features from the
    single view of MEs. However, MEs have subtle movements and limited samples, the
    single view is not able to provide sufficient information. As we discussed in
    Section [4.2](#S4.SS2 "4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey"), the various inputs from different views is able to effectively
    explore spatial and temporal information. Thus, the multi-stream network is adopted
    in MER to learn features from multiple inputs. The multi-stream structure allows
    the network extracting multi-view features through multi-path networks, as shown
    in Fig. [8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep
    Learning for Micro-expression Recognition: A Survey") (g). In general, multi-stream
    networks can be classified to networks with the same blocks, different blocks
    and handcrafted features.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-stream networks with the same blocks.* The Optical Flow Features from
    Apex frame Network (OFF-ApexNet) [[95](#bib.bib95)] and Dual-stream shallow network
    (DSSN) [[96](#bib.bib96)] built the dual-stream CNN for MER based on optical flow
    extracted from onset and apex. Furthermore, Liong et al. [[138](#bib.bib138)]
    extended OFF-ApexNet to multiple streams with various optical flow components
    as input data. The multi-stream CNN with optical flow [[86](#bib.bib86)] and Three-Stream
    CNN (TSCNN) [[98](#bib.bib98), [99](#bib.bib99)] designed three-stream CNN models
    for MER with three kinds of inputs (See Fig. [8](#S5.F8 "Figure 8 ‣ 5.1 Network
    block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (c)). Specifically, the former one utilized apex frame, optical flow
    and the apex frame masked by the optical flow threshold, while the latter approach
    employed the apex frames, optical flow between onset, apex, and offset frames
    to investigate the information of the static spatial, dynamic temporal and local
    information. In addition, She et al. [[102](#bib.bib102)] proposed a four-stream
    model considering three RoIs and global regions as each stream to explore the
    local and global information. Besides multi-stream 2D CNNs, 3DFCNN [[100](#bib.bib100)],
    SETFNet [[179](#bib.bib179)] and [[97](#bib.bib97)] applied 3D flow-based CNNs
    for video-based MER consisting of multiple sub-streams to extract features from
    frame sequences and optical flow, or RoIs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-stream networks with different blocks.* For enhancing the ME feature
    representation, some works [[106](#bib.bib106), [165](#bib.bib165), [102](#bib.bib102),
    [103](#bib.bib103), [107](#bib.bib107)] investigated the combination of different
    convolutions. Liong et al.  designed a Shallow Triple Stream Three-dimensional
    CNN (STSTNet) [[106](#bib.bib106)] adopting multiple 2D CNN with different kernels.
    Instead of utilizing different kernels, AffectiveNet [[165](#bib.bib165)] constructed
    a four-path network with four different receptive fields (RF) to obtain multi-scale
    features for better describing subtle MEs. On the other hand, Landmark Relations
    with Graph Attention Convolutional Network (LR-GACNN) [[75](#bib.bib75)] and MER-GCN
    [[90](#bib.bib90)] built two-stream graph networks to explore relationships between
    landmark points and the local patches, and AUs and sequence, respectively. Furthermore,
    [[103](#bib.bib103)] and [[107](#bib.bib107)] integrated 2D CNN and 3D CNN to
    extract spatio-temporal information.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-stream networks with handcrafted features.* Since the subtle facial
    movements of MEs are highly related to face textures, the handcrafted features
    for low-level representation also plays an important role in MER. Multiple works
    [[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)] combined deep features
    and handcrafted features to leverage the low-level and high-level information
    for robust MER. Specifically, in the works [[108](#bib.bib108)] and [[110](#bib.bib110)],
    the CNN features on apex frame and LBP-TOP were concatenated to represent MEs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Cascaded network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cascaded network combines various modules for different tasks sequentially
    to construct an effective network, as shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.1
    Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (g). Recent FE studies [[24](#bib.bib24)] demonstrate that learning
    a hierarchy of features gradually filters out the information unrelated to expressions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the FE studies [[24](#bib.bib24)], for further exploring the temporal
    information of MEs, Nistor et al. [[111](#bib.bib111)] cascaded CNN and RNN to
    extract features from individual frames of the sequence and capture the facial
    evolution during the sequence, respectively. Furthermore, Bai et al. [[114](#bib.bib114)]
    and Zhi et al. [[116](#bib.bib116)] combined CNN with LSTMs in series to deal
    with ME samples with various duration directly, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (b). Besides, in order to explore the AU semantics in
    MEs, Xie et al. proposed an AU-assisted Graph Attention Convolutional Network
    (AU-GACN) [[58](#bib.bib58)] cascading 3D CNN and GCN to infer MEs based on AU
    features (see Fig. [8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks
    for MER ‣ Deep Learning for Micro-expression Recognition: A Survey") (f)).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, multiple MER works combined multi-stream and cascaded structure
    to further explore the multi-view series information. VGGFace2+LSTM [[114](#bib.bib114)],
    Temporal Facial Micro-Variation Network (TFMVN) [[113](#bib.bib113)] and MER with
    Ternary Attentions (MERTA) [[101](#bib.bib101)] developed three stream VGGNets
    followed by LSTMs to extract multi-view spatio-temporal features. Different from
    above works, Khor et al.  [[112](#bib.bib112)] proposed an Enriched Long-term
    Recurrent Convolutional Network (ELRCN) adding one VGG+LSTM path with channel-wise
    stacking inputs for spatial enrichment. Besides, AT-Net [[104](#bib.bib104)] and
    SHCFNet [[105](#bib.bib105)] extracted spatial and temporal features by CNN and
    LSTM from the apex frame and optical-flow in parallel and concatenated them together
    to represent MEs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Multi-task network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most existing works for MER focus on learning features that are sensitive to
    expressions. However, MEs in the real world are intertwined with various factors,
    such as subject identity and AUs. The approaches aiming at a single MER task are
    incapable of making full use of the information on face. To address this issue,
    several multi-task learning-based MER approaches have been subsequently developed
    for better MER [[117](#bib.bib117), [76](#bib.bib76)]. Firstly, Li et al. [[117](#bib.bib117)]
    developed a multi-task network combining facial landmarks detection and optical
    flow extraction to refine the optical flow features for MER with SVM. Following
    [[117](#bib.bib117)], several end-to-end deep multi-task networks leveraging different
    side tasks were proposed. GEnder-based ME recognition (GEME) [[76](#bib.bib76)]
    designed a dual-stream multi-task network incorporating gender detection task
    with MER (see Fig [8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for
    MER ‣ Deep Learning for Micro-expression Recognition: A Survey") (d)), while feature
    refinement [[185](#bib.bib185)] and MER-auGCN [[91](#bib.bib91)] simultaneously
    detected AUs and MEs and further aggregated AU representation into ME representation.
    On the other hand, considering that a common feature representation can be learned
    from multiple tasks, Hu et al. [[109](#bib.bib109)] formulated MER as a multi-task
    classification problem in which each category classification can be regarded as
    one-against-all pairwise classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the network architecture can be roughly divided into single-stream,
    multi-stream, cascaded networks, and multi-task learning, as shown in Fig. [8](#S5.F8
    "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (g). Single stream is the simple basic model architecture.
    However, single-stream networks only consider the single view of MEs. To further
    leverage the ME information, the multi-stream network is proposed to learn features
    from multiple views for robust MER. Moreover, since learning a hierarchy of features
    can gradually filter out the information unrelated to expressions, the network
    cascades various modules, such as LSTMs and GCNs, sequentially to construct an
    effective MER network. In the future, more effective modules should be combined
    in multi-stream and cascaded ways to further boost the MER performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the tasks, multiple task learning [[186](#bib.bib186)] can share
    knowledge among tasks, introducing extra information and a low risk of overfitting
    in each task. Currently, most ME research only studied the contribution of landmarks
    detection, gender classification, and AU detection. Other tasks, such as face
    recognition and eye gaze tracking may also introduce useful knowledge for MER.
    Exploring and taking advantage of more face related-tasks is a practical way to
    further improve MER performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Training strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed before, DL-based MER suffers from a lack of adequate data.
    It is almost impossible to train a reliable deep model from scratch. Currently,
    there are large-scale FE datasets with labels. Leveraging the FE datasets by special
    training strategy, such as fine-tuning [[99](#bib.bib99)], knowledge distillation [[69](#bib.bib69)],
    and domain adaptation [[122](#bib.bib122)], is a reasonable way to solve the problem
    of a small amount of data. The knowledge of a pre-trained model for a related
    task can be transferred to MER to boost performance. The training strategy of
    transferring knowledge is shown in Fig [8](#S5.F8 "Figure 8 ‣ 5.1 Network block
    ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (g).'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning ME datasets on pre-trained models is widely used in MER [[99](#bib.bib99),
    [59](#bib.bib59), [87](#bib.bib87), [74](#bib.bib74)]. Patel et al. [[187](#bib.bib187)]
    provided two models pre-trained on ImageNet dataset and FE datasets, respectively.
    The feature selection method was also adopted to improve the model’s performance.
    It was found that features captured from the FE datasets performed better in terms
    of accuracy, as it is more similar to the ME datasets than object/face datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides fine-tuning, another effective transfer learning strategy is knowledge
    distillation. Knowledge distillation achieves small and fast networks through
    leveraging information from pre-trained high-capacity networks. Sun et al. [[69](#bib.bib69)]
    utilized Fitnets [[188](#bib.bib188)] to guide the shallow network learning for
    MER by mimicking the intermediate features of the deep network pre-trained for
    macro-expression recognition and AU detection, as shown in Fig [8](#S5.F8 "Figure
    8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (e). However, the appearances of MEs and macro-expressions
    are different due to the different intensity of facial movements. Thus, mimicking
    the macro-expression representation directly is not reasonable. Instead, SAAT
    [[121](#bib.bib121)] transferred attention on the style aggregated MEs generated
    by CycleGAN [[189](#bib.bib189)].'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, domain adaptation methods can obtain domain invariant representations
    by embedding domain adaptation in the pipeline of deep learning. In [[120](#bib.bib120)],
    [[122](#bib.bib122)], and [[190](#bib.bib190)], the gap between the MEs and macro-expressions
    was narrowed down by domain adaption based on adversarial learning strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In general, fine-tuning is most widely used in MER. To further effectively transfer
    meaningful information from massive FEs, knowledge distillation and domain adaptation
    are also applied to MER by distilling knowledge and extracting domain invariant
    representations, respectively. However, the domain adaptation with adversarial
    learning increases the learning complexity. There are significant differences
    both spatially and temporally between macro-expressions and MEs, therefore, directly
    transferring the knowledge is not able to fully leverage the macro-expression
    information. Considering that the facial muscle movements are consistent between
    MEs and macro-expressions, the attention and AUs can be further studied for transfer
    learning in future ME research. Moreover, semi-supervised and unsupervised learning
    could also be further developed to take advantage of unlabeled facial images.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from traditional methods, where the feature extraction and classification
    are independent, deep networks can perform end-to-end classification through loss
    functions by penalizing the deviation between the predicted and true labels during
    training. Most MER works directly applied the commonly used softmax cross-entropy
    loss [[123](#bib.bib123)]. The softmax loss is typically good at correctly classifying
    known categories. However, in practical classification tasks, the unknown samples
    need to be classified. Therefore, in order to obtain better-generalized ability,
    the inter-class difference and intra-class variation should be further optimized
    and reduced, respectively, especially for subtle and limited MEs. The metric learning
    techniques, such as contrastive loss [[124](#bib.bib124)] and triplet loss [[125](#bib.bib125)],
    was developed to ensure intra-class compactness and inter-class separability through
    measuring the relative distances between inputs. Xia et al. [[122](#bib.bib122)]
    adopted an adversarial learning approach and triplet loss with inequality regularization
    to converge the output of MicroNet efficiently. However, metric learning loss
    usually requires effective sample mining strategies for robust recognition performance.
    Metric learning alone is not enough for learning a discriminative metric space
    for MEs. Intensive experiments demonstrate that importing a large margin on softmax
    loss can increase the inter-class difference. Lalitha et al. [[191](#bib.bib191)]
    and Li et al. [[59](#bib.bib59)] combined softmax cross-entropy loss and center
    loss [[192](#bib.bib192)] to increase the compactness of intra-class variations
    and separable inter-class differences through penalizing the distance between
    deep features and their corresponding class centers.
  prefs: []
  type: TYPE_NORMAL
- en: Some special MEs are difficult to trigger, thus leading to data imbalance. Multiple
    MER works [[163](#bib.bib163), [76](#bib.bib76), [60](#bib.bib60), [91](#bib.bib91)]
    utilized the Focal loss to overcome the imbalance challenge by adding a factor
    to put more focus on misclassified and hard samples which are difficult to recognize.
    Moreover, MER-auGCN [[91](#bib.bib91)] designed an adaptive factor with the Focal
    loss to balance the proportion of the negative and positive samples in a given
    training batch.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, MER suffers from high intra-class variation, low inter-class differences,
    and imbalanced distribution because of the low intensity and spontaneous characteristics
    of MEs. Currently, most MER approaches are based on the basic softmax cross-entropy
    loss, but others utilized the triplet loss, center loss, or focal loss to encourage
    inter-class separability, intra-class compactness, and balanced learning. In the
    future, exploring more effective loss functions to learn discriminative representation
    for MEs can be a promising research direction. Considering the low intensity of
    facial movements leading to low inter-class differences, better metric space and
    larger margin loss for MER should be further studied. Recently, various methods
    [[193](#bib.bib193)] have been proposed for the classification of imbalanced long-tail
    distribution data. ME research can leverage the ideas for long-tail data to improve
    the MER performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MEs are involuntary, subtle, and brief facial movements. How to extract high-level
    discriminative representations on limited subtle ME samples is the main challenge
    for robust MER with DL. In order to extract discriminative ME representation,
    various blocks have been designed based on exploring the special characteristics
    of MEs with less parameters, such as the attention module and capsule module.
    In the future, more effective blocks, such as attention, GCN and transformer,
    should be further developed for MER performance improvement. On the other hand,
    considering the limited ME samples, more efficient blocks should be studied to
    learn discriminative ME features with less parameters for avoiding overfitting
    on small-scale ME datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the network architecture, compared with basic single stream networks,
    multi-stream networks can extract features from multi-view inputs to provide more
    information for MER. On the other hand, the cascaded network combines various
    modules for different tasks sequentially to construct an effective network and
    gradually filter out the information unrelated to MEs. Considering the strengths
    of multi-stream and cascaded networks, multi-stream cascaded networks have been
    developed to boost the MER performance further. In the future, exploring multi-stream
    cascaded networks combined with various efficient blocks is a promising research
    direction for MER. In addition, the multi-task learning framework achieves robust
    MER through leveraging information from related tasks. Multi-task learning is
    able to make use of more available information on the face. Current MER explored
    gender classification, landmark detection, and AU detection to take advantage
    of existing information as much as possible. In the future, more relevant tasks,
    such as identity classification and age estimation, could be studied.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is widely used in MER. Recent research [[69](#bib.bib69)] illustrated
    that borrowing information from large FE datasets through knowledge distillation
    and domain adaptation can achieve promising performance. For future ME research,
    how to effectively leverage massive face images will be a focus. Besides, the
    semi-supervised learning [[194](#bib.bib194)] and unsupervised learning [[195](#bib.bib195)]
    could be promising research directions.
  prefs: []
  type: TYPE_NORMAL
- en: For the losses, most DL-based MER employs the basic softmax cross-entropy loss.
    Several works utilized the metric learning loss and margin loss to increase the
    compactness of intra-class variations and separable inter-class differences. Furthermore,
    since the ME datasets are imbalanced, multiple works aimed to boost MER performance
    through Focal loss. However, current MER methods just employed the losses designed
    for common tasks, such as image classification and face recognition. MER is a
    special task due to the ME characteristics (low intensity and imbalanced small-scale
    ME datasets), effective losses aimed for MER should be explored in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Evaluation matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The common evaluation metrics for MER are accuracy and F1-score. In general,
    the accuracy metric measures the ratio of correct predictions over the total evaluated
    samples. However, the accuracy is susceptible to bias data. F1-score solves the
    bias problem by considering the total True Positives (TP), False Positives (FP)
    and False Negatives (FN) to reveal the true classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: For the composited dataset which combines multiple datasets leading to severe
    data imbalance, Unweighted F1-score (UF1) and Unweighted Average Recall (UAR)
    are utilized to measure the performance of various methods. UF1 is also known
    as macro-averaged F1-score which is determined by averaging the per-class F1-scores.
    UF1 provides equal emphasis on rare classes in imbalanced multi-class settings.
    UAR is defined as the average accuracy of each class divided by the number of
    classes without consideration of samples per class. UAR can reduce the bias caused
    by class imbalance and is known as balanced accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: MER on SMIC, CASME, CASME II, SAMM, and CMED datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Year | Pre-p. | Input | Network architecture | Block |
    Pre-train | Protocol | Cate. | F1 | ACC (%) |'
  prefs: []
  type: TYPE_TB
- en: '| SMIC | TSCNN [[99](#bib.bib99)] | 2019 | E, R | OF+Apex | 3S-CNN | - | FER2013
    [[196](#bib.bib196)] | LOSO | 3 | 0.7236 | 72.74 |'
  prefs: []
  type: TYPE_TB
- en: '| DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | - | - | LOSO | 3
    | 0.71 | 76.06 |'
  prefs: []
  type: TYPE_TB
- en: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+DA+GAN | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 3 | 0.744 | 76.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 3 | 0.778 | 78.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 3 | 0.78 | 77 |'
  prefs: []
  type: TYPE_TB
- en: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 3 | 0.77 | 79.87 |'
  prefs: []
  type: TYPE_TB
- en: '| CASME | TSCNN [[99](#bib.bib99)] | 2019 | E,R | OF+Apex | 3S-CNN | - | FER2013
    [[196](#bib.bib196)] | LOSO | 4 | 0.7270 | 73.88 |'
  prefs: []
  type: TYPE_TB
- en: '| DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | RES | - | LOSO |
    4 | 0.77 | 81.80 |'
  prefs: []
  type: TYPE_TB
- en: '| AffectiveNet [[165](#bib.bib165)] | 2020 | E | DI | 4S-CNN | MFL | - | LOSO
    | 4 | - | 72.64 |'
  prefs: []
  type: TYPE_TB
- en: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 4 | 0.75 | 78 |'
  prefs: []
  type: TYPE_TB
- en: '| CASME II | OFF-ApexNet [[95](#bib.bib95)] | 2019 | - | OF | 2S-CNN | - |
    - | LOSO | 3 | 0.8697 | 88.28 |'
  prefs: []
  type: TYPE_TB
- en: '| TSCNN [[99](#bib.bib99)] | 2019 | E, R | OF+Apex | 3S-CNN | - | FER2013 [[196](#bib.bib196)]
    | LOSO | 5 | 0.807 | 80.97 |'
  prefs: []
  type: TYPE_TB
- en: '| STSTNet [[106](#bib.bib106)] | 2019 | E | OF | 3S-3DCNN | - | - | LOSO |
    3 | 0.8382 | 86.86 |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-TCN [[89](#bib.bib89)] | 2020 | L, R | Apex | TCN+GCN | Graph | - |
    LOSO | 5 | 0.7246 | 73.98 |'
  prefs: []
  type: TYPE_TB
- en: '| SMA-STN [[74](#bib.bib74)] | 2020 | - | Snippet | CNN | Attention | WIDER
    FACE[[199](#bib.bib199)] | LOSO | 5 | 0.7946 | 82.59 |'
  prefs: []
  type: TYPE_TB
- en: '| GEME [[76](#bib.bib76)] | 2021 | - | DI | 2S-CNN+ML | RES | - | LOSO | 5
    | 0.7354 | 75.20 |'
  prefs: []
  type: TYPE_TB
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 5 | 0.759 | 79.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LR-GACNN [[75](#bib.bib75)] | 2021 | E | OF+Landmark | 2S-GACNN | Graph |
    - | LOSO | 5 | 0.7090 | 81.30 |'
  prefs: []
  type: TYPE_TB
- en: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 5 | 0.73 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 5 | 0.71 | 75.40 |'
  prefs: []
  type: TYPE_TB
- en: '| SAMM | DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | - | - | LOSO
    | 4 | 0.83 | 86.74 |'
  prefs: []
  type: TYPE_TB
- en: '| SMA-STN [[74](#bib.bib74)] | 2020 | - | Snippet | CNN | - | WIDER FACE[[199](#bib.bib199)]
    | LOSO | 5 | 0.7033 | 77.20 |'
  prefs: []
  type: TYPE_TB
- en: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+GAN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 5 | 0.736 | 74.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 5 | 0.764 | 76.7 |'
  prefs: []
  type: TYPE_TB
- en: '| LR-GACNN [[75](#bib.bib75)] | 2021 | E | OF+Landmark | 2S-GACNN | - | - |
    LOSO | 5 | 0.8279 | 88.24 |'
  prefs: []
  type: TYPE_TB
- en: '| GRAPH-AU [[92](#bib.bib92)] | 2021 | L | Apex | 2S-CNN+GCN | Graph, Transformer
    | - | LOSO | 5 | 0.7045 | 74.26 |'
  prefs: []
  type: TYPE_TB
- en: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 5 | 0.67 | 68.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CMED | Shallow CNN[[44](#bib.bib44)] | 2020 | E | OF | CNN | - | - | LOSO
    | 7 | 0.6353 | 66.06 |'
  prefs: []
  type: TYPE_TB
- en: '¹ Pre-p.: Pre-processing; E:EVM; R: RoI; T: Temporal normalization ; L: Learning-based
    magnification.'
  prefs: []
  type: TYPE_NORMAL
- en: '² OF: Optical flow; DI: Dynamic image.'
  prefs: []
  type: TYPE_NORMAL
- en: '³ nS-CNN: n-stream CNN; ML: Multi-task learning; DA: Domain adaption; KD: Knowledge
    distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: '⁴ Cate: Category; F1: F1-score; ACC: Accuracy; RES: Residual block.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Model evaluation protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-validation is the widely utilized protocol for evaluating the MER performance.
    In cross-validation, the dataset is splitted into multiple folds and the training
    and testing were evaluated on different folds. It regards a fair verification
    and prevents overfitting on the small-scale ME datasets. In the MER field, cross-validation
    includes leave-one-subject-out (LOSO), leave-one-video-out (LOVO), and K-Fold
    cross-validations.
  prefs: []
  type: TYPE_NORMAL
- en: In LOSO, every subject is taken as a test set in turn and the other subjects
    as the training data. This kind of subject-independent protocol can avoid subject
    bias and evaluate the generalization performance of various algorithms. LOSO is
    the most popular cross-validation in MER.
  prefs: []
  type: TYPE_NORMAL
- en: The LOVO takes each sample as the validation unit which enables more training
    data and alleviates the overfitting to some degree. However, it is not subject-independent,
    thus it can not well evaluate the generalization capability. Another problem is
    that the test number of LOVO is the sample size which may lead to huge time cost,
    not suitable for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: For K-fold cross-validation, the original samples are randomly partitioned into
    k equal-sized parts. Each part is taken as a test set in turn and the rest are
    the training data. Thus, the number of cross-validation tests is K. In practice,
    the evaluation time can be greatly reduced by setting an appropriate K. The typical
    K values are 5 or 10.
  prefs: []
  type: TYPE_NORMAL
- en: Since the MEs have small-scale datasets, the experiments on MER do not have
    reliable validation datasets. According to the released codes, some works [[120](#bib.bib120)]
    utilized the test datasets as the validation datasets directly and reserved the
    best epoch results on each fold as the final results. As the data is limited,
    even only two samples for some subjects, the final MER results will be greatly
    improved by regarding the test data as the validation data. According to [[169](#bib.bib169)],
    compared to the experiments based on the same epoch on all of the folds, the results
    can be increased by more than 10$\%$ by testing on the test datasets. But, in
    practice, the test data is unknown and it is not reasonable to reserve the best
    epoch results on each fold of the test data as the final results.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: MER on the Composite dataset (MECG2019)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pre-p. | Input | Network architecture | Block | Pre-train
    | Protocol | Cate. | UF1 | UAR |'
  prefs: []
  type: TYPE_TB
- en: '| NMER  [[120](#bib.bib120)] | 2019 | E, R | OF | CNN+DA | - | - | LOSO | 3
    | 0.7885 | 0.7824 |'
  prefs: []
  type: TYPE_TB
- en: '| Dual-Inception [[81](#bib.bib81)] | 2019 | - | OF | 2S-CNN | Inception |
    - | LOSO | 3 | 0.7322 | 0.7278 |'
  prefs: []
  type: TYPE_TB
- en: '| ICE-GAN [[27](#bib.bib27)] | 2020 | GAN | Apex | CNN+GAN | Capsule | ImageNet
    | LOSO | 3 | 0.845 | 0.841 |'
  prefs: []
  type: TYPE_TB
- en: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+GAN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 3 | 0.864 | 0.857 |'
  prefs: []
  type: TYPE_TB
- en: '| FR [[185](#bib.bib185)] | 2021 | - | OF | 2S-CNN+ML | Inception | - | LOSO
    | 3 | 0.7838 | 0.7832 |'
  prefs: []
  type: TYPE_TB
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 3 | 0.883 | 0.876 |'
  prefs: []
  type: TYPE_TB
- en: '| GRAPH-AU [[92](#bib.bib92)] | 2021 | L | Apex | 2S-CNN+GCN | Graph, Transformer
    | - | LOSO | 3 | 0.7914 | 0.7933 |'
  prefs: []
  type: TYPE_TB
- en: '| BDCNN [[136](#bib.bib136)] | 2022 | L | OF | 4S-CNN | - | - | LOSO | 3 |
    0.8509 | 0.8500 |'
  prefs: []
  type: TYPE_TB
- en: '¹ Pre-p.: Pre-processing; E: EVM; R: RoI; T: Temporal normalization ; L: Learning-based
    magnification.'
  prefs: []
  type: TYPE_NORMAL
- en: '² OF: Optical flow; DI: Dynamic image.'
  prefs: []
  type: TYPE_NORMAL
- en: '³ nS-CNN: n-stream CNN; ML: Multi-task learning; DA: Domain adaption; KD: Knowledge
    distillation'
  prefs: []
  type: TYPE_NORMAL
- en: '⁴ Cate.: Category; RES: Residual block.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tables [III](#S6.T3 "TABLE III ‣ 6.1 Evaluation matrix ‣ 6 Experiments ‣ Deep
    Learning for Micro-expression Recognition: A Survey") and [IV](#S6.T4 "TABLE IV
    ‣ 6.2 Model evaluation protocols ‣ 6 Experiments ‣ Deep Learning for Micro-expression
    Recognition: A Survey") list the reported performance of representative recent
    work of DL-based MER on popular ME datasets. As we discussed before, the evaluation
    protocol is varying and the practical training rule of each paper is ambiguous,
    we can not directly make a conclusion that which method performs best for MER.
    But, from the experimental results, the general trends of MER can be found.'
  prefs: []
  type: TYPE_NORMAL
- en: For the input, in general, the combined inputs can provide promising results
    on all of the datasets [[99](#bib.bib99), [180](#bib.bib180), [75](#bib.bib75)].
    This is because the different input modalities can contribute information from
    different views. On the basis of various input modalities, we can explore useful
    information on limited ME samples to the greatest extent. Since the combined inputs
    is a good choice for robust MER, the multi-stream network is recommended to learn
    effective representations from various inputs [[99](#bib.bib99), [180](#bib.bib180),
    [75](#bib.bib75)]. In contrast to the combined inputs, the sole sequence performs
    worse [[177](#bib.bib177)], due to the limited information and redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, from Tables [III](#S6.T3 "TABLE III ‣ 6.1 Evaluation matrix ‣ 6 Experiments
    ‣ Deep Learning for Micro-expression Recognition: A Survey") and [IV](#S6.T4 "TABLE
    IV ‣ 6.2 Model evaluation protocols ‣ 6 Experiments ‣ Deep Learning for Micro-expression
    Recognition: A Survey"), it can be seen that the learning strategy including fine-tuning
    [[74](#bib.bib74)], domain adaptation [[122](#bib.bib122), [190](#bib.bib190)]
    and knowledge distillation [[69](#bib.bib69)] can achieve state-of-the-art results
    on both the individual datasets and the composite dataset. This could be explained
    that the limited ME sample is the main challenge for MER and leveraging other
    related data sources is a reasonable and effective solution. In the future, domain
    adaption and knowledge distillation should be further researched to boost MER
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In some latest works [[92](#bib.bib92), [75](#bib.bib75), [89](#bib.bib89)],
    the GCN becomes a mainstream choice for MER and shows promising performance. Currently,
    the spatio-temporal graph representation combined with GCNs obtains more attention
    in MER studies. The possible reason is that the landmark and AU information are
    helpful and effective for locating and representing the facial muscle movements.
    However, the small-sample ME datasets limit the ability of graph representation.
    The combination of transfer learning and graph should be a promising direction
    for future ME studies.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Challenges AND FUTURE DIRECTIONS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MER has a wide range of potential applications in various fields, such as psychological
    disorders, education, business negotiation, and security control. More specific
    descriptions about applications of MER are introduced in APPENDIX A. Although
    MER could facilitate society in various fields, there are many challenges. In
    this section, we discuss the challenges and future directions of MER.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Dealing with small-scale dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning is a data-driven method, and successful training requires various
    large-scale data. Recent studies indicated that annotation bias, emotional contexts,
    and cultural backgrounds could affect the ME perception [[200](#bib.bib200)].
    They may mislead the model training and finally cause misclassification. Unfortunately,
    existing ME datasets are far from enough for training a robust model. To this
    end, more diverse ME datasets should be collected. Besides, effective deep-based
    data augmentation approaches should be further developed for ME analysis to avoid
    over-fitting. Semi-supervised and unsupervised learning could also be potential
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some emotions, such as fear, are challenging to be evoked and collected.
    The data imbalance causes the network to be biased towards classes in the majority.
    Therefore, effective imbalanced losses are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 3D ME sequence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, the main focus of MER is based on the 2D domain because of the data
    prevalence in the relevant modalities including images and videos. Although significant
    progress has been made for MER in recent years, most existing MER algorithms based
    on 2D facial images and sequences can not solve the challenging problems of illumination
    and pose variations in real-world applications. Recent research about FE analysis
    illustrates that the above issues can be addressed through 3D facial data [[201](#bib.bib201)].
    Inherent characteristics of 3D face make facial recognition robust to lighting
    and pose variations. Moreover, 3D geometry information may include important features
    for FER and provide more data for better training. Thanks to the benefits of 3D
    faces and the technological development of 3D scanning, MER based on 3D sequence
    could be a promising research direction. Special 3D blocks, such as 3D Graph and
    Transformer, should be studied in 3D MER.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 AU analysis in MEs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MEs reveal people’s hidden emotions in high-stake situations [[202](#bib.bib202),
    [3](#bib.bib3)] and have various applications such as clinic diagnosis and national
    security. However, ME interpretation suffers ambiguities  [[55](#bib.bib55)],
    e.g., the inner brow raiser may refer to surprise or sad. The FACS [[6](#bib.bib6)]
    has been verified to be effective for resolving the ambiguity issue. In FACS,
    action units (AUs) are defined as the basic facial movements, working as the building
    blocks to formulate multiple FEs [[6](#bib.bib6)]. Furthermore, the criteria for
    AU and FE correspondence is defined in FACS manual. Encoding AUs has been verified
    to benefit the MER [[69](#bib.bib69), [58](#bib.bib58), [203](#bib.bib203)] through
    embedding AU features. In the future, the relationship between AUs and MEs can
    be further explored to improve MER.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Multi-modal MER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the MER challenges is that the low-intensity and small-scale ME datasets
    provide very limited information for robust MER. Recent research demonstrated
    that utilizing multiple modalities can provide complementary information and enhance
    classification robustness. Different emotional expressions can produce different
    changes in autonomic activity, e.g. fear leads to increased heart rate and decreased
    skin temperature. Thus, the physiological signal can be utilized to incorporate
    complementary information for further improving MER. Besides, in recent years,
    new micro-gesture datasets [[204](#bib.bib204)] had been proposed. The micro-gesture
    is body movements that are elicited when hidden expressions are triggered in unconstrained
    situations. The hidden emotional states can be reflected through micro-gestures.
    How to combine multiple modalities to enhance MER performance is an important
    future direction. Lightweight multi-stream networks should be developed to learn
    multi-view ME information effectively and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 The explainanty of MER based on DL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The neural network is a brain-inspired model developed by neurobiologists and
    psychologists to test the computational analog of neurons [[205](#bib.bib205)].
    Naturally, it could be a tool to verify the theory in other disciplines, such
    as psychology, to enhance psychological and human communication study. In addition,
    the current DL is a “black box” algorithm [[205](#bib.bib205)] and focuses on
    learning features and recognizing patterns by updating the weights of networks.
    The interpretation and understanding of the inside DL process can get experts
    from cross disciplines involved in the internal state analysis and therefore facilitate
    building interpretable and reliable deep models.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 MEs in realistic situations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, most existing MER researches focus on classifying the basic MEs collected
    in controlled environments from the frontal view without any head movements, illumination
    variations or occlusion. However, it is almost impossible to reproduce such strict
    conditions in real-world applications. The approaches based on the constrained
    settings usually do not generalize well to videos recorded in-the-wild environment.
    Practical and robust algorithms for recognizing MEs in realistic situations with
    pose changes and illumination variations should be developed in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, most ME researches assume that there are just MEs in a video clip.
    However, in real life, MEs can appear with macro-expressions. Future studies should
    explore deep-based ME spotting methods to detect and distinguish the micro- and
    macro-expressions when they occur at the same time. Analyzing the macro and micro-expressions
    simultaneously would be helpful to understand people’s intentions and feelings
    in reality more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Ethical considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed above, MEs can help reveal people’s hidden feelings in high-stake
    situations and have practical applications in various fields, such as medical
    treatment and interrogations. MER, like many other computer vision and machine
    learning tasks, could be misused, especially when used in surveillance with predatory
    data collection practices [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] Therefore, ethical issues should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and data protection is the primarily and frequently discussed ethical
    issue in machine learning. For MER, the critical privacy concern is the privacy
    of personal data. Currently, data protection laws are well established to regulate
    data privacy, for example, the EU General Data Protection Regulation (GDPR) [[210](#bib.bib210)].
    The legislation defined rules for the protection of personal data, including international
    data protection agreements, privacy shields, transfer of participant names, record
    data, etc. In the research community, consent forms concerning data collection,
    processing, and sharing need to be signed when collecting ME data. In practical
    applications, consent forms should also be considered to regulate the usage, as
    people’s faces are present in the recorded images/videos with sensitive and biometric
    information that may be misused beyond the intended purpose. Pilot studies aim
    to remove sensitive information like identity while preserving facial properties
    [[211](#bib.bib211)], which could be further explored in MER.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, questions of reliability in MER systems are further pointed out together
    with privacy and data protection [[212](#bib.bib212)]. Results of a deep learning-based
    MER system usually depend on the quality of training data, which are difficult
    to ascertain because of possible data biases. Transparency of data and models
    should be aware and well-studied.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the Academy of Finland for Academy Professor project
    EmotionAI (grants 336116, 345122), by Ministry of Education and Culture of Finland
    for AI forum project and Infotech Oulu.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] P. Ekman, “Darwin, deception, and facial expression,” *Annals of the New
    York Academy of Sciences*, vol. 1000, no. 1, pp. 205–221, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Ekman and W. Friesen, “Constants across cultures in the face and emotion,”
    *Personality and Social Psychology*, vol. 17, no. 2, pp. 124–129, 1971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] P. Ekman, “Lie catching and microexpressions,” *The philosophy of deception*,
    pp. 118–133, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] E. A. Haggard and K. S. Isaacs, “Micromomentary facial expressions as indicators
    of ego mechanisms in psychotherapy,” in *Methods of research in psychotherapy*.   Springer,
    1966, pp. 154–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] P. Ekman and W. V. Friesen, “Nonverbal leakage and clues to deception,”
    *Psychiatry*, vol. 32, no. 1, pp. 88–106, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] W. V. Friesen and P. Ekman, “Facial action coding system: a technique for
    the measurement of facial movement,” *Palo Alto*, vol. 3, 1978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] P. Ekman, “Microexpression training tool (METT),” 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] X. Niu, H. Han, S. Yang, Y. Huang, and S. Shan, “Local relationship learning
    with person-specific shape regularization for facial action unit detection,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 11 917–11 926.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W. J. Yan, X. Li, S. J. Wang, G. Zhao, Y. J. Liu, Y. H. Chen, and X. Fu,
    “CASME II: an improved spontaneous micro-expression database and the baseline
    evaluation,” *Plos One*, vol. 9, no. 1, p. e86041, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, “Web-based database
    for facial expression analysis,” in *2005 IEEE international conference on multimedia
    and Expo*.   IEEE, 2005, pp. 5–pp.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] W. Merghani, A. K. Davison, and M. H. Yap, “A review on facial micro-expressions
    analysis: datasets, features and metrics,” *arXiv preprint arXiv:1805.02397*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] C. Crivelli and A. J. Fridlund, “Inside-out: From basic emotions theory
    to the behavioral ecology view,” *Journal of Nonverbal Behavior*, vol. 43, no. 2,
    pp. 161–194, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. M. Niedenthal, M. Rychlowska, F. Zhao, and A. Wood, “Historical migration
    patterns shape contemporary cultures of emotion,” *Perspectives on Psychological
    Science*, vol. 14, no. 4, pp. 560–573, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] T. Pfister, X. Li, G. Zhao, and M. Pietikäinen, “Differentiating spontaneous
    from posed facial expressions within a generic facial expression recognition framework,”
    in *International Conference Computer Vision*.   Springer, 2011, pp. 1449–1456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietikainen, “A spontaneous
    micro-expression database: Inducement, collection and baseline,” in *Proceedings
    of IEEE Conference Workshops on Automatic Face Gesture Recognition*, 2013, pp.
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietikäinen, “A spontaneous
    micro-expression database: Inducement, collection and baseline,” in *2013 10th
    IEEE International Conference and Workshops on Automatic Face and Gesture Recognition
    (FG)*.   IEEE, 2013, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] X. Li, X. Hong, A. Moilanen, X. Huang, T. Pfister, G. Zhao, and M. Pietikainen,
    “Towards reading hidden emotions: A comparative study of spontaneous micro-expression
    spotting and recognition methods,” *IEEE Transactions on Affective Computing*,
    vol. 9, no. 4, pp. 563 – 577, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Wei, G. Lu, and J. Yan, “A comparative study on movement feature in
    different directions for micro-expression recognition,” *Neurocomputing*, vol.
    449, pp. 159–171, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *International Journal
    of Computer Vision*, vol. 128, no. 2, pp. 261–318, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Brunetti, D. Buongiorno, G. F. Trotta, and V. Bevilacqua, “Computer
    vision and deep learning techniques for pedestrian detection and tracking: A survey,”
    *Neurocomputing*, vol. 300, pp. 17–33, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Liu, F. Yang, C. Zhong, Y. Tao, B. Dai, and M. Yin, “Visual tracking
    via salient feature extraction and sparse collaborative model,” *AEU-International
    Journal of Electronics and Communications*, vol. 87, pp. 134–143, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Z. Li, Y. Li, Y. Gao, and Y. Liu, “Fast cross-scenario clothing retrieval
    based on indexing deep features,” in *Pacific Rim Conference on Multimedia*.   Springer,
    2016, pp. 107–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z. Li, W. Tian, Y. Li, Z. Kuang, and Y. Liu, “A more effective method
    for image representation: Topic model based on latent dirichlet allocation,” in
    *2015 14th International Conference on Computer-Aided Design and Computer Graphics
    (CAD/Graphics)*.   IEEE, 2015, pp. 143–148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Li and W. Deng, “Deep facial expression recognition: A survey,” *IEEE
    Transactions on Affective Computing*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Liu, X. Zhang, J. Zhou, and L. Fu, “Sg-dsn: A semantic graph-based
    dual-stream network for facial expression recognition,” *Neurocomputing*, vol.
    462, pp. 320–330, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Liu, X. Zhang, J. Kauttonen, and G. Zhao, “Uncertain label correction
    via auxiliary action unit graphs for facial expression recognition,” *arXiv preprint
    arXiv:2204.11053*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] J. Yu, C. Zhang, Y. Song, and W. Cai, “ICE-GAN: Identity-aware and capsule-enhanced
    gan for micro-expression recognition and synthesis,” *arXiv preprint arXiv:2005.04370*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “An overview of facial
    micro-expression analysis: Data, methodology and challenge,” *arXiv preprint arXiv:2012.11307*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. M. Goh, C. H. Ng, L. L. Lim, and U. U. Sheikh, “Micro-expression recognition:
    an updated review of current trends, challenges and solutions,” *The Visual Computer*,
    vol. 36, no. 3, pp. 445–468, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Takalkar, M. Xu, Q. Wu, and Z. Chaczko, “A survey: facial micro-expression
    recognition,” *Multimedia Tools and Applications*, vol. 77, no. 15, pp. 19 301–19 325,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y.-H. Oh, J. See, A. C. Le Ngo, R. C.-W. Phan, and V. M. Baskaran, “A
    survey of automatic facial micro-expression analysis: databases, methods, and
    challenges,” *Frontiers in psychology*, vol. 9, p. 1128, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] X. Ben, Y. Ren, J. Zhang, S.-J. Wang, K. Kpalma, W. Meng, and Y.-J. Liu,
    “Video-based facial micro-expression analysis: A survey of datasets, features
    and algorithms,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] L. Zhou, X. Shao, and Q. Mao, “A survey of micro-expression recognition,”
    *Image and Vision Computing*, vol. 105, p. 104043, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] W. Yan, Q. Wu, Y. Liu, S. Wang, and X. Fu, “CASME database: A dataset
    of spontaneous micro-expressions collected from neutralized faces,” in *Proceedings
    of IEEE Conference Automatic Face Gesture Recognition*, 2013, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] F. Qu, S.-J. Wang, W.-J. Yan, H. Li, S. Wu, and X. Fu, “Cas (me) ²: A
    database for spontaneous macro-expression and micro-expression spotting and recognition,”
    *IEEE Transactions on Affective Computing*, vol. 9, no. 4, pp. 424–436, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap, “SAMM: A
    spontaneous micro-facial movement dataset,” *IEEE Transactions on Affective Computing*,
    vol. 9, no. 1, pp. 116–129, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] P. Husák, J. Cech, and J. Matas, “Spotting facial micro-expressions “in
    the wild”,” in *22nd Computer Vision Winter Workshop (RETZ)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. See, Y. Hoon, J. Li, X. Hong, and S. Wang, “MEGC 2019–the second facial
    micro-expressions grand challenge,” in *2019 14th IEEE International Conference
    on Automatic Face and Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Zhao and J. Xu, “A convolutional neural network for compound micro-expression
    recognition,” *Sensors*, vol. 19, no. 24, p. 5553, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Vinciarelli, A. Dielmann, S. Favre, and H. Salamin, “Canal9: A database
    of political debates for analysis of social interactions,” in *2009 3rd International
    Conference on Affective Computing and Intelligent Interaction and Workshops*.   IEEE,
    2009, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Polikovsky, Y. Kameda, and Y. Ohta, “Facial micro-expressions recognition
    using high speed camera and 3D-gradient descriptor,” 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Shreve, S. Godavarthy, D. Goldgof, and S. Sarkar, “Macro- and micro-expression
    spotting in long videos using spatio-temporal strain,” in *Proceedings of IEEE
    Conference Workshops on Automatic Face and Gesture Recognition*, 2011, pp. 51–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. Zhang, Q. Fu, Y.-H. Chen, and X. Fu, “Emotional context modulates micro-expression
    processing as reflected in event-related potentials,” *PsyCh journal*, vol. 7,
    no. 1, pp. 13–24, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Zhao and J. Xu, “Compound micro-expression recognition system,” in
    *2020 International Conference on Intelligent Transportation, Big Data and Smart
    City (ICITBS)*.   IEEE, 2020, pp. 728–733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] P. Viola, M. Jones *et al.*, “Robust real-time object detection,” *International
    journal of computer vision*, vol. 4, no. 34-47, p. 4, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Matsugu, K. Mori, Y. Mitari, and Y. Kaneda, “Subject independent facial
    expression recognition with robust face detection using a convolutional neural
    network,” *Neural Networks*, vol. 16, no. 5-6, pp. 555–559, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active shape
    models-their training and application,” *Computer vision and image understanding*,
    vol. 61, no. 1, pp. 38–59, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance models,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 23, no. 6,
    pp. 681–685, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment
    using multitask cascaded convolutional networks,” *IEEE Signal Processing Letters*,
    vol. 23, no. 10, pp. 1499–1503, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Zhou, G. Zhao, and M. Pietikäinen, “Towards a practical lipreading
    system,” in *CVPR 2011*.   IEEE, 2011, pp. 137–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Niklaus and F. Liu, “Context-aware synthesis for video frame interpolation,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 1701–1710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Wu, E. Shih, E. Shih, J. Guttag, and W. Freeman, “Eulerian video magnification
    for revealing subtle changes in the world,” *ACM Transactions on Graphics*, vol. 31,
    no. 4, pp. 1–8, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] T.-H. Oh, R. Jaroensri, C. Kim, M. Elgharib, F. Durand, W. T. Freeman,
    and W. Matusik, “Learning-based video motion magnification,” in *Proceedings of
    the European Conference on Computer Vision (ECCV)*, 2018, pp. 633–648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] G. Zhao and M. Pietikainen, “Dynamic texture recognition using local binary
    patterns with an application to facial expressions,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 29, no. 6, pp. 915–928, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Davison, W. Merghani, C. Lansley, C.-C. Ng, and M. H. Yap, “Objective
    micro-facial movement detection using facs-based regions and baseline evaluation,”
    in *2018 13th IEEE international conference on automatic face and gesture recognition
    (FG)*.   IEEE, 2018, pp. 642–649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] W. Merghani and M. H. Yap, “Adaptive mask for region-based facial micro-expression
    recognition,” in *2020 15th IEEE International Conference on Automatic Face and
    Gesture Recognition (FG)*.   IEEE Computer Society, 2020, pp. 428–433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Shreve, J. Brizzi, S. Fefilatyev, T. Luguev, D. Goldgof, and S. Sarkar,
    “Automatic expression spotting in videos,” *Image and Vision Computing*, vol. 32,
    no. 8, pp. 476–486, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “AU-assisted graph attention
    convolutional network for micro-expression recognition,” in *Proceedings of the
    28th ACM International Conference on Multimedia*, 2020, pp. 2871–2880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Li, X. Huang, and G. Zhao, “Joint local and global information learning
    with single apex frame detection for micro-expression recognition,” *IEEE Transactions
    on Image Processing*, vol. 30, pp. 249–263, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal recurrent
    convolutional networks for recognizing spontaneous micro-expressions,” *IEEE Transaction
    on Multimedia*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Li, X. Huang, and G. Zhao, “Micro-expression action unit detection
    with spatial and channel attention,” *Neurocomputing*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. Patel, G. Zhao, and M. Pietikäinen, “Spatiotemporal integration of
    optical flow vectors for micro-expression detection,” in *International Conference
    Advanced Concepts Intelligence Vision System*.   Springer, 2015, pp. 369–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Han, B. Li, Y.-K. Lai, and Y.-J. Liu, “CFD: A collaborative feature
    difference method for spontaneous micro-expression spotting,” in *2018 25th IEEE
    International Conference on Image Processing (ICIP)*.   IEEE, 2018, pp. 1942–1946.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Li, X. Huang, and G. Zhao, “Can micro-expression be recognized based
    on single apex frame?” in *IEEE International Conference Image Processing*.   IEEE,
    2018, pp. 3094–3098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Zhang, T. Chen, H. Meng, G. Liu, and X. Fu, “Smeconvnet: A convolutional
    neural network for spotting spontaneous facial micro-expression from long videos,”
    *IEEE Access*, vol. 6, pp. 71 143–71 151, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] T.-K. Tran, Q.-N. Vo, X. Hong, X. Li, and G. Zhao, “Micro-expression spotting:
    A new benchmark,” *Neurocomputing*, vol. 443, pp. 356–368, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Pan, L. Xie, and Z. Wang, “Local bilinear convolutional neural network
    for spotting macro-and micro-expression intervals in long video sequences,” in
    *2020 15th IEEE International Conference on Automatic Face and Gesture Recognition
    (FG)*.   IEEE Computer Society, 2020, pp. 343–347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] N. V. Quang, J. Chun, and T. Tokuyama, “Capsulenet for micro-expression
    recognition,” in *2019 14th IEEE International Conference on Automatic Face and
    Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] B. Sun, S. Cao, D. Li, J. He, and L. Yu, “Dynamic micro-expression recognition
    using knowledge distillation,” *IEEE Transactions on Affective Computing*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] R. Zhi, H. Xu, M. Wan, and T. Li, “Combining 3D convolutional neural networks
    with transfer learning by supervised pre-training for facial micro-expression
    recognition,” *IEICE Transactions on Information and Systems*, vol. 102, no. 5,
    pp. 1054–1064, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] D. Kim, W. J. Baddar, and Y. M. Ro, “Micro-expression recognition with
    expression-state constrained spatio-temporal feature representations,” in *Proceedings
    of the 24th ACM International Conference on Multimedia*, 2016, pp. 382–386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Li, Y. Wang, J. See, and W. Liu, “Micro-expression recognition based
    on 3D flow convolutional neural network,” *Pattern Analysis and Applications*,
    pp. 1–9, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S.-T. Liong and K. Wong, “Micro-expression recognition using apex frame
    with phase information,” in *2017 Asia-Pacific Signal and Information Processing
    Association Annual Summit and Conference (APSIPA ASC)*.   IEEE, 2017, pp. 534–537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Liu, W. Zheng, and Y. Zong, “SMA-STN: Segmented movement-attending
    spatiotemporal network for micro-expression recognition,” *arXiv preprint arXiv:2010.09342*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. J. R. Kumar and B. Bhanu, “Micro-expression classification based on
    landmark relations with graph attention convolutional network,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops*,
    June 2021, pp. 1511–1520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Nie, M. A. Takalkar, M. Duan, H. Zhang, and M. Xu, “GEME: Dual-stream
    multi-task gender-based micro-expression recognition,” *Neurocomputing*, vol.
    427, pp. 13–28, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M. Verma, S. K. Vipparthi, G. Singh, and S. Murala, “Learnet: Dynamic
    imaging network for micro expression recognition,” *IEEE Transactions on Image
    Processing*, vol. 29, pp. 1618–1627, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Verma, S. K. Vipparthi, and G. Singh, “Non-linearities improve originet
    based on active imaging for micro expression recognition,” in *2020 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] B. D. Lucas, “Generalized image matching by the method of differences.”
    1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] G. Farnebäck, “Two-frame motion estimation based on polynomial expansion,”
    in *Scandinavian conference on Image analysis*.   Springer, 2003, pp. 363–370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. Zhou, Q. Mao, and L. Xue, “Dual-inception network for cross-database
    micro-expression recognition,” in *2019 14th IEEE International Conference on
    Automatic Face and Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] D. Alexey, F. Philipp, I. H. Philip, H. Caner, G. Vladimir, V. Patrick,
    C. Daniel, and B. Thomas, “Flownet: Learning optical flow with convolutional networks,”
    in *Proceedings of IEEE International Conference of Computing Vision*, 2015, pp.
    2758–2766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] N. Liu, X. Liu, Z. Zhang, X. Xu, and T. Chen, “Offset or onset frame:
    A multi-stream convolutional neural network with capsulenet module for micro-expression
    recognition,” in *2020 5th International Conference on Intelligent Informatics
    and Biomedical Sciences (ICIIBMS)*.   IEEE, 2020, pp. 236–240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] B. Sun, S. Cao, J. He, and L. Yu, “Two-stream attention-aware network
    for spontaneous micro-expression movement spotting,” in *2019 IEEE 10th International
    Conference on Software Engineering and Service Science (ICSESS)*.   IEEE, 2019,
    pp. 702–705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. J. R. Kumar and B. Bhanu, “Micro-expression classification based on
    landmark relations with graph attention convolutional network,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 1511–1520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Liu, K. Li, B. Song, and L. Zhao, “A multi-stream convolutional neural
    network for micro-expression recognition using optical flow and EVM,” *arXiv preprint
    arXiv:2011.03756*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. Chen, Z. Zhang, N. Liu, Y. Tan, X. Liu, and T. Chen, “Spatiotemporal
    convolutional neural network with convolutional block attention module for micro-expression
    recognition,” *Information*, vol. 11, no. 8, p. 380, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] V. R. Gajjala, S. P. T. Reddy, S. Mukherjee, and S. R. Dubey, “MERANet:
    Facial micro-expression recognition using 3D residual attention network,” *arXiv
    preprint arXiv:2012.04581*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] L. Lei, J. Li, T. Chen, and S. Li, “A novel Graph-TCN with a graph structured
    representation for micro-expression recognition,” in *Proceedings of the 28th
    ACM International Conference on Multimedia*, 2020, pp. 2237–2245.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] L. Lo, H.-X. Xie, H.-H. Shuai, and W.-H. Cheng, “MER-GCN: Micro-expression
    recognition based on relation modeling with graph convolutional networks,” in
    *2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)*.   IEEE,
    2020, pp. 79–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] L. Zhou, Q. Mao, and M. Dong, “Objective class-based micro-expression
    recognition through simultaneous action unit detection and feature aggregation,”
    *arXiv preprint arXiv:2012.13148*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] L. Lei, T. Chen, S. Li, and J. Li, “Micro-expression recognition based
    on facial graph representation learning and facial action unit fusion,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops*,
    June 2021, pp. 1571–1580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] T. T. Q. Le, T.-K. Tran, and M. Rege, “Dynamic image for micro-expression
    recognition on region-based framework,” in *2020 IEEE 21st International Conference
    on Information Reuse and Integration for Data Science (IRI)*.   IEEE, 2020, pp.
    75–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Wang, H. Ma, X. Xing, and Z. Pan, “Eulerian motion based 3Dcnn architecture
    for facial micro-expression recognition,” in *International Conference on Multimedia
    Modeling*.   Springer, 2020, pp. 266–277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. Gan, S. Liong, W. Yau, Y. Huang, and L. Tan, “Off-apexnet on micro-expression
    recognition system,” *Signal Processing: Image Communication*, vol. 74, pp. 129–139,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H.-Q. Khor, J. See, S.-T. Liong, R. C. Phan, and W. Lin, “Dual-stream
    shallow networks for facial micro-expression recognition,” in *2019 IEEE International
    Conference on Image Processing (ICIP)*.   IEEE, 2019, pp. 36–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] H. Yan and L. Li, “Micro-expression recognition using enriched two stream
    3D convolutional network,” in *Proceedings of the 4th International Conference
    on Computer Science and Application Engineering*, 2020, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] K. Li, Y. Zong, B. Song, J. Zhu, J. Shi, W. Zheng, and L. Zhao, “Three-stream
    convolutional neural network for micro-expression recognition,” *Australian Journal
    of Intelligent Information Processing System*, vol. 15, no. 3, pp. 41–48, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] B. Song, K. Li, Y. Zong, J. Zhu, W. Zheng, J. Shi, and L. Zhao, “Recognizing
    spontaneous micro-expression using a three-stream convolutional neural network,”
    *IEEE Access*, vol. 7, pp. 184 537–184 551, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] J. Li, Y. Wang, J. See, and W. Liu, “Micro-expression recognition based
    on 3D flow convolutional neural network,” *Pattern Analysis and Applications*,
    vol. 22, no. 4, pp. 1331–1339, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] B. Yang, J. Cheng, Y. Yang, B. Zhang, and J. Li, “MERTA: micro-expression
    recognition with ternary attentions,” *Multimedia Tools and Applications*, vol. 80,
    no. 11, pp. 1–16, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] W. She, Z. Lv, J. Taoi, M. Niu *et al.*, “Micro-expression recognition
    based on multiple aggregation networks,” in *2020 Asia-Pacific Signal and Information
    Processing Association Annual Summit and Conference (APSIPA ASC)*.   IEEE, 2020,
    pp. 1043–1047.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Wang, J. Jia, and N. Mao, “Micro-expression recognition based on 2D
    -3D cnn,” in *2020 39th Chinese Control Conference (CCC)*.   IEEE, 2020, pp. 3152–3157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] M. Peng, C. Wang, T. Bi, Y. Shi, X. Zhou, and T. Chen, “A novel apex-time
    network for cross-dataset micro-expression recognition,” in *2019 8th International
    Conference on Affective Computing and Intelligent Interaction (ACII)*.   IEEE,
    2019, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] J. Huang, X. Zhao, and L. Zheng, “Shcfnet on micro-expression recognition
    system,” in *2020 13th International Congress on Image and Signal Processing,
    BioMedical Engineering and Informatics (CISP-BMEI)*.   IEEE, 2020, pp. 163–168.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] S. Liong, Y. Gan, J. See, H. Khor, and Y. Huang, “Shallow triple stream
    three-dimensional cnn (ststnet) for micro-expression recognition,” in *2019 14th
    IEEE International Conference on Automatic Face and Gesture Recognition (FG)*.   IEEE,
    2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] C. Wu and F. Guo, “TSNN: Three-stream combining 2D and 3D convolutional
    neural network for micro-expression recognition,” *IEEJ Transactions on Electrical
    and Electronic Engineering*, vol. 16, no. 1, pp. 98–107, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. A. Takalkar, M. Xu, and Z. Chaczko, “Manifold feature integration
    for micro-expression recognition,” *Multimedia Systems*, vol. 26, no. 5, pp. 535–551,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Hu, D. Jiang, H. Zou, X. Zuo, and Y. Shu, “Multi-task micro-expression
    recognition combining deep and handcrafted features,” in *2018 24th International
    Conference on Pattern Recognition (ICPR)*.   IEEE, 2018, pp. 946–951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. Pan, L. Xie, Z. Lv, J. Li, and Z. Wang, “Hierarchical support vector
    machine for facial micro-expression recognition,” *Multimedia Tools and Applications*,
    vol. 79, no. 41, pp. 31 451–31 465, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. C. Nistor, “Multi-staged training of deep neural networks for micro-expression
    recognition,” in *2020 IEEE 14th International Symposium on Applied Computational
    Intelligence and Informatics (SACI)*.   IEEE, 2020, pp. 000 029–000 034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] H.-Q. Khor, J. See, R. C. W. Phan, and W. Lin, “Enriched long-term recurrent
    convolutional network for facial micro-expression recognition,” in *2018 13th
    IEEE International Conference on Automatic Face and Gesture Recognition (FG)*.   IEEE,
    2018, pp. 667–674.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Zhang, Z. Huan, and L. Shang, “Micro-expression recognition using
    micro-variation boosted heat areas,” in *Chinese Conference on Pattern Recognition
    and Computer Vision (PRCV)*.   Springer, 2020, pp. 531–543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] M. Bai and R. Goecke, “Investigating lstm for micro-expression recognition,”
    in *Companion Publication of the 2020 International Conference on Multimodal Interaction*,
    2020, pp. 7–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] D. Y. Choi and B. C. Song, “Facial micro-expression recognition using
    two-dimensional landmark feature maps,” *IEEE Access*, vol. 8, pp. 121 549–121 563,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. Zhi, M. Liu, H. Xu, and M. Wan, “Facial micro-expression recognition
    using enhanced temporal feature-wise model,” in *Cyberspace Data and Intelligence,
    and Cyber-Living, Syndrome, and Health*.   Springer, 2019, pp. 301–311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Q. Li, S. Zhan, L. Xu, and C. Wu, “Facial micro-expression recognition
    based on the fusion of deep learning and enhanced optical flow,” *Multimedia Tools
    and Applications*, vol. 78, no. 20, pp. 29 307–29 322, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] N. Komodakis and S. Zagoruyko, “Paying more attention to attention: improving
    the performance of convolutional neural networks via attention transfer,” in *International
    Conference on Learning Representations (ICLR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y. Liu, H. Du, L. Zheng, and T. Gedeon, “A neural micro-expression recognizer,”
    in *2019 14th IEEE International Conference on Automatic Face and Gesture Recognition
    (FG)*.   IEEE, 2019, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] L. Zhou, Q. Mao, and L. Xue, “Cross-database micro-expression recognition:
    a style aggregated and attention transfer approach,” in *2019 IEEE International
    Conference on Multimedia and Expo Workshops (ICMEW)*.   IEEE, 2019, pp. 102–107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] B. Xia, W. Wang, S. Wang, and E. Chen, “Learning from macro-expression:
    a micro-expression recognition framework,” in *Proceedings of the 28th ACM International
    Conference on Multimedia*, 2020, pp. 2936–2944.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] D. M. Kline and V. L. Berardi, “Revisiting squared-error and cross-entropy
    functions for training neural network classifiers,” *Neural Computing and Applications*,
    vol. 14, no. 4, pp. 310–318, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by learning
    an invariant mapping,” in *IEEE Computing Society Conference on Computer Vision
    and Pattern Recognition*, vol. 2.   IEEE, 2006, pp. 1735–1742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2015, pp. 815–823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] W. Liu, Y. Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for convolutional
    neural networks.” in *International Conference on Machine Learning (ICML)*, vol. 2,
    2016, p. 7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
    margin loss for deep face recognition,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 4690–4699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu,
    “Cosface: Large margin cosine loss for deep face recognition,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    5265–5274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. C. Le Ngo, A. Johnston, R. C.-W. Phan, and J. See, “Micro-expression
    motion magnification: Global lagrangian vs. local eulerian approaches,” in *2018
    13th IEEE international conference on automatic face and gesture recognition (FG)*.   IEEE,
    2018, pp. 650–656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S.-T. Liong, J. See, R. C.-W. Phan, K. Wong, and S.-W. Tan, “Hybrid facial
    regions extraction for micro-expression recognition system,” *Journal of Signal
    Processing Systems*, vol. 90, no. 4, pp. 601–617, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal recurrent
    convolutional networks for recognizing spontaneous micro-expressions,” *IEEE Transactions
    on Multimedia*, vol. PP, pp. 1–1, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Li, C. Soladie, and R. Seguier, “Ltp-ml: Micro-expression detection
    by recognition of local temporal pattern of facial movements,” in *2018 13th IEEE
    international conference on automatic face and gesture recognition (FG)*.   IEEE,
    2018, pp. 634–641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] E. L. Rosenberg and P. Ekman, *What the face reveals: Basic and applied
    studies of spontaneous expression using the facial action coding system (FACS)*.   Oxford
    University Press, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] X. Huang, G. Zhao, W. Zheng, and M. Pietikäinen, “Towards a dynamic expression
    recognition system under facial occlusion,” *Pattern Recognition Letters*, vol. 33,
    no. 16, pp. 2181–2191, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] B. Chen, K.-H. Liu, Y. Xu, Q.-Q. Wu, and J.-F. Yao, “Block division convolutional
    network with implicit deep features augmentation for micro-expression recognition,”
    *IEEE Transactions on Multimedia*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] F. Zhang, T. Zhang, Q. Mao, and C. Xu, “Joint pose and expression modeling
    for facial expression recognition,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 3359–3368.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S.-T. Liong, Y. Gan, D. Zheng, S.-M. Li, H.-X. Xu, H.-Z. Zhang, R.-K.
    Lyu, and K.-H. Liu, “Evaluation of the spatio-temporal features and gan for micro-expression
    recognition system,” *Journal of Signal Processing Systems*, pp. 1–21, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] S.-J. W. et al., “Micro-expression recognition using color spaces,” *IEEE
    Transactions on Image Processing*, vol. 24, no. 12, pp. 6034–6047, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] V. Mayya, R. M. Pai, and M. M. Pai, “Combining temporal interpolation
    and dcnn for faster recognition of micro-expressions in video sequences,” in *2016
    International Conference on Advances in Computing, Communications and Informatics
    (ICACCI)*.   IEEE, 2016, pp. 699–703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] S. Ji, W. Xu, M. Yang, and K. Yu, “3D convolutional neural networks for
    human action recognition,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 35, no. 1, pp. 221–231, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] L. R. Medsker and L. Jain, “Recurrent neural networks,” *Design and Applications*,
    vol. 5, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Ullah, J. Ahmad, K. Muhammad, M. Sajjad, and S. W. Baik, “Action recognition
    in video sequences using deep bi-directional lstm with cnn features,” *IEEE access*,
    vol. 6, pp. 1155–1166, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] H. Yang, C. Yuan, B. Li, Y. Du, J. Xing, W. Hu, and S. J. Maybank, “Asymmetric
    3D convolutional neural networks for action recognition,” *Pattern recognition*,
    vol. 85, pp. 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Peng, C. Wang, Y. Gao, T. Bi, T. Chen, Y. Shi, and X.-D. Zhou, “Recognizing
    micro-expression in video clip with adaptive key-frame mining,” *arXiv preprint
    arXiv:2009.09179*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. Liong, J. See, K. S. Wong, and R. C. W. Phan, “Less is more: Micro-expression
    recognition from video using apex frame,” *Signal Processing: Image Communication*,
    vol. 62, pp. 82–92, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould, “Dynamic
    image networks for action recognition,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 3034–3042.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] H. Bilen, B. Fernando, E. Gavves, and A. Vedaldi, “Action recognition
    with dynamic image networks,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 40, no. 12, pp. 2799–2813, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. L. Barron, D. J. Fleet, and S. S. Beauchemin, “Performance of optical
    flow techniques,” *International journal of computer vision*, vol. 12, no. 1,
    pp. 43–77, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] B. K. Horn and B. G. Schunck, “Determining optical flow,” *Artificial
    intelligence*, vol. 17, no. 1-3, pp. 185–203, 1981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] T. Senst, V. Eiselein, and T. Sikora, “Robust local optical flow for
    feature tracking,” *IEEE Transactions on Circuits and Systems for Video Technology*,
    vol. 22, no. 9, pp. 1377–1387, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] A. Wedel, T. Pock, C. Zach, H. Bischof, and D. Cremers, “An improved
    algorithm for TV-L1 optical flow,” in *Statistical and geometrical approaches
    to visual motion analysis*.   Springer, 2009, pp. 23–45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] B. Allaert, I. R. Ward, I. M. Bilasco, C. Djeraba, and M. Bennamoun,
    “Optical flow techniques for facial expression analysis: Performance evaluation
    and improvements,” *arXiv preprint arXiv:1904.11592*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 31, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,”
    *arXiv preprint arXiv:1710.09829*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] P. Pareek and A. Thakkar, “A survey on video-based human action recognition:
    recent updates, datasets, challenges, and applications,” *Artificial Intelligence
    Review*, vol. 54, no. 3, pp. 2259–2322, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3D cnns retrace
    the history of 2D cnns and imagenet?” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 6546–6555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 1492–1500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Wu, C. Shen, and A. Van Den Hengel, “Wider or deeper: Revisiting the
    resnet model for visual recognition,” *Pattern Recognition*, vol. 90, pp. 119–133,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] R. Belaiche, Y. Liu, C. Migniot, D. Ginhac, and F. Yang, “Cost-effective
    cnns for real-time micro-expression recognition,” *Applied Sciences*, vol. 10,
    no. 14, p. 4959, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. Wen, W. Yang, L. Wang, W. Wei, S. Tan, and Y. Wu, “Cross-database
    micro expression recognition based on apex frame optical flow and multi-head self-attention,”
    in *International Symposium on Parallel Architectures, Algorithms and Programming*.   Springer,
    2020, pp. 128–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Z. Lai, R. Chen, J. Jia, and Y. Qian, “Real-time micro-expression recognition
    based on resnet and atrous convolutions,” *Journal of Ambient Intelligence and
    Humanized Computing*, pp. 1–12, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] G. Chinnappa and M. K. Rajagopal, “Residual attention network for deep
    face recognition using micro-expression image analysis,” *Journal of Ambient Intelligence
    and HumanizedComputing*, pp. 1–14, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Verma, S. K. Vipparthi, and G. Singh, “Affectivenet: Affective-motion
    feature learning for micro expression recognition,” *IEEE MultiMedia*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] W. Li, F. Abtahi, and Z. Zhu, “Action unit detection with region adaptation,
    multi-labeling learning and optimal temporal fusing,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2017, pp. 1841–1850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2015, pp.
    1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] M. Verma, S. K. Vipparthi, and G. Singh, “Hinet: Hybrid inherited feature
    learning network for facial expression recognition,” *IEEE Letters of the Computer
    Society*, vol. 2, no. 4, pp. 36–39, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Z. Xia, W. Peng, H.-Q. Khor, X. Feng, and G. Zhao, “Revealing the invisible
    with model and data shrinking for composite-database micro-expression recognition,”
    *IEEE Transactions on Image Processing*, vol. 29, pp. 8590–8605, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling,” *arXiv preprint arXiv:1803.01271*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and
    D. Tran, “Image transformer,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 4055–4064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] D. Acharya, Z. Huang, D. Pani Paudel, and L. Van Gool, “Covariance pooling
    for facial expression recognition,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition Workshops*, 2018, pp. 367–374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] M. A. Takalkar, S. Thuseethan, S. Rajasegarar, Z. Chaczko, M. Xu, and
    J. Yearwood, “LGAttNet: Automatic micro-expression detection using dual-stream
    local and global attentions,” *Knowledge-Based Systems*, vol. 212, p. 106566,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] M. Bai, “Detection of micro-expression recognition based on spatio-temporal
    modelling and spatial attention,” in *Proceedings of the 2020 International Conference
    on Multimodal Interaction*, 2020, pp. 703–707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] M. F. Hashmi, B. Ashish, V. Sharma, A. G. Keskar, N. D. Bokde, J. H.
    Yoon, and Z. W. Geem, “LARNet: Real-time detection of facial micro expression
    using lossless attention residual network,” *Sensors*, vol. 21, no. 4, p. 1098,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Y. Su, J. Zhang, J. Liu, and G. Zhai, “Key facial components guided micro-expression
    recognition based on first and second-order motion,” in *2021 IEEE International
    Conference on Multimedia and Expo (ICME)*.   IEEE, 2021, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] M. Wei, W. Zheng, Y. Zong, X. Jiang, C. Lu, and J. Liu, “A novel micro-expression
    recognition approach using attention-based magnification-adaptive networks,” in
    *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 2420–2424.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 7794–7803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] L. Yao, X. Xiao, R. Cao, F. Chen, and T. Chen, “Three stream 3D cnn with
    se block for micro-expression recognition,” in *2020 International Conference
    on Computer Engineering and Application (ICCEA)*, 2020, pp. 439–443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Y. Wang, Y. Huang, C. Liu, X. Gu, D. Yang, S. Wang, and B. Zhang, “Micro
    expression recognition via dual-stream spatiotemporal attention network,” *Journal
    of Healthcare Engineering*, vol. 2021, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Li, G. Wei, J. Wang, and Y. Zhou, “Multi-scale joint feature network
    for micro-expression recognition,” *Computational Visual Media*, pp. 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Y. Gan and S.-T. Liong, “Bi-directional vectors from apex in cnn for
    micro-expression recognition,” in *2018 IEEE 3rd International Conference on Image,
    Vision and Computing (ICIVC)*.   IEEE, 2018, pp. 168–172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] P. Gupta, “MERASTC: Micro-expression recognition using effective feature
    encodings and 2d convolutional neural network,” *IEEE Transactions on Affective
    Computing*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”
    in *British Machine Vision Conference*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] L. Zhou, Q. Mao, X. Huang, F. Zhang, and Z. Zhang, “Feature refinement:
    An expression-specific feature learning and fusion method for micro-expression
    recognition,” *arXiv preprint arXiv:2101.04838*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Transactions
    on Knowledge and Data Engineering*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] D. Patel, X. Hong, and G. Zhao, “Selective deep features for micro-expression
    recognition,” in *Proceedings of IEEE International Conference Pattern Recognition*,
    2017, pp. 2258–2263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] A. Romero, N. Ballas, E. K. Samira, A. Chassang, C. Gatta, and B. Yoshua,
    “Fitnets: Hints for thin deep nets,” *Proceedings of International Conference
    on Learning Representations (ICLR)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2017, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] B. Xia and S. Wang, “Micro-expression recognition enhanced by macro-expression
    from spatial-temporal domain,” in *Proceedings of the Thirtieth International
    Joint Conference on Artificial Intelligence*, 2021, pp. 1186–1193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] S. Lalitha and K. Thyagharajan, “Micro-facial expression recognition
    based on deep-rooted learning algorithm,” *arXiv preprint arXiv:2009.05778*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature learning
    approach for deep face recognition,” in *Proceedings of European Conference Computer
    Vision*.   Springer, 2016, pp. 499–515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Y. Li, T. Wang, B. Kang, S. Tang, C. Wang, J. Li, and J. Feng, “Overcoming
    classifier imbalance for long-tail object detection with balanced group softmax,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 10 991–11 000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised learning,”
    *Machine Learning*, vol. 109, no. 2, pp. 373–440, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] X. Wang, X. Wang, and Y. Ni, “Unsupervised domain adaptation for facial
    expression recognition using generative adversarial networks,” *Computational
    Intelligence and Neuroscience*, vol. 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Hamner,
    W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee *et al.*, “Challenges in representation
    learning: A report on three machine learning contests,” in *International conference
    on Neural Information Processing*.   Springer, 2013, pp. 117–124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,
    “The extended cohn-kanade dataset (CK+): A complete dataset for action unit and
    emotion-specified expression,” in *2010 IEEE Computer society Conference on Computer
    Vision and Pattern Recognition workshops*.   IEEE, 2010, pp. 94–101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. PietikäInen, “Facial expression
    recognition from near-infrared videos,” *Image and Vision Computing*, vol. 29,
    no. 9, pp. 607–619, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “Wider face: A face detection
    benchmark,” in *Proceedings of the IEEE*, 2016, pp. 5525–5533.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] J. Li, Z. Dong, S. Lu, S.-J. Wang, W.-J. Yan, Y. Ma, Y. Liu, C. Huang,
    and X. Fu, “Cas (me) 3: A third generation facial spontaneous micro-expression
    database with depth information and high ecological validity,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] G. Sandbach, S. Zafeiriou, M. Pantic, and L. Yin, “Static and dynamic
    3D facial expression recognition: A comprehensive survey,” *Image and Vision Computing*,
    vol. 30, no. 10, pp. 683–697, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] P. Ekman and W. Friesen, “Nonverbal leakage and clues to deception,”
    *Study Interpers*, vol. 32, pp. 88–106, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Y. Chen and J. Joo, “Understanding and mitigating annotation bias in
    facial expression recognition,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 14 980–14 991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] H. Chen, X. Liu, X. Li, H. Shi, and G. Zhao, “Analyze spontaneous gestures
    for emotional stress state recognition: A micro-gesture dataset and analysis with
    deep learning,” in *2019 14th IEEE International Conference on Automatic Face
    and Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *et al.*, “Explainable
    artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges
    toward responsible ai,” *Information fusion*, vol. 58, pp. 82–115, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] M. Kosinski, “Facial recognition technology can expose political orientation
    from naturalistic facial images,” *Scientific reports*, vol. 11, no. 1, pp. 1–7,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] S. Zuboff, *The age of surveillance capitalism: The fight for a human
    future at the new frontier of power: Barack Obama’s books of 2019*.   Profile
    books, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] I. D. Raji, T. Gebru, M. Mitchell, J. Buolamwini, J. Lee, and E. Denton,
    “Saving face: Investigating the ethical concerns of facial recognition auditing,”
    in *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, 2020,
    pp. 145–151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] S. Oviatt, “Technology as infrastructure for dehumanization: three hundred
    million people with the same face,” in *Proceedings of the 2021 International
    Conference on Multimodal Interaction*, 2021, pp. 278–287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Voigt and A. Von dem Bussche, “The eu general data protection regulation
    (gdpr),” *A Practical Guide, 1st Ed., Cham: Springer International Publishing*,
    vol. 10, no. 3152676, pp. 10–5555, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] H. Proença, “The uu-net: Reversible face de-identification for visual
    surveillance video footage,” *IEEE Transactions on Circuits and Systems for Video
    Technology*, vol. 32, no. 2, pp. 496–509, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] B. C. Stahl, “Ethical issues of ai,” in *Artificial Intelligence for
    a Better Future*.   Springer, 2021, pp. 35–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
