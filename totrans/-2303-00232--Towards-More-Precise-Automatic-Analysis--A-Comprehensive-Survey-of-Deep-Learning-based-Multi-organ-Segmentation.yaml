- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:41:15'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2303.00232] Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.00232](https://ar5iv.labs.arxiv.org/html/2303.00232)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Xiaoyu Liu, Linhao Qu, Ziyue Xie, Jiayue Zhao, Yonghong Shi, and Zhijian Song
    X. Liu, L. Qu, Z. Xie, J. Zhao, Y. Shi and Z. Song are with the Digital Medical
    Research Center, School of Basic Medical Sciences, Fudan University, and also
    with the Shanghai Key Laboratory of Medical Imaging Computing and Computer Assisted
    Intervention, Shanghai, 200032, China. (e-mail: (liuxiaoyu21, zyxie22, jiayuezhao22)
    @m.fudan.edu.cn; (lhqu20, yonghong.shi, zjsong) @fudan.edu.cn);X. Liu and L. Qu
    are co-first authors. Y. Shi and Z. Song are co-corresponding authors.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Accurate segmentation of multiple organs of the head, neck, chest, and abdomen
    from medical images is an essential step in computer-aided diagnosis, surgical
    navigation, and radiation therapy. In the past few years, with a data-driven feature
    extraction approach and end-to-end training, automatic deep learning-based multi-organ
    segmentation method has far outperformed traditional methods and become a new
    research topic. This review systematically summarizes the latest research in this
    field. For the first time, from the perspective of full and imperfect annotation,
    we comprehensively compile 161 studies on deep learning-based multi-organ segmentation
    in multiple regions such as the head and neck, chest, and abdomen, containing
    a total of 214 related references. The method based on full annotation summarizes
    the existing methods from four aspects: network architecture, network dimension,
    network dedicated modules, and network loss function. The method based on imperfect
    annotation summarizes the existing methods from two aspects: weak annotation-based
    methods and semi annotation-based methods. We also summarize frequently used datasets
    for multi-organ segmentation and discuss new challenges and new research trends
    in this field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: abdomen multi-organ, chest multi-organ, deep learning, head and neck multi-organ,
    multi-organ segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accurate segmentation of multiple organs of the head and neck, chest, abdomen
    as well as other parts from medical images is crucial in computer-aided diagnosis,
    surgical navigation, and radiotherapy [[1](#bib.bib1), [2](#bib.bib2)]. For example,
    radiotherapy, which targets tumour masses and microscopic areas with high risk
    of tumour proliferation, is a common treatment option for cancer patients [[3](#bib.bib3)].
    However, radiotherapy can bring great risk to the normal organs around the tumour,
    which are known as organs at risk (OARs). Thus, accurate segmentation of tumour
    contours and OARs is necessary [[4](#bib.bib4), [5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: The early segmentation process relied heavily on manual labelling by physicians,
    which is a labour-intensive and time-consuming process. For example, a trained
    specialist may spend more than four hours manually labelling a case, which not
    only places a heavy burden on the healthcare system but also likely causes a delay
    in the radiotherapy for a patient. Moreover, different physicians or hospitals
    will have different results of labelling [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. Therefore, accurate automatic multi-organ segmentation method
    is urgently needed in clinical practice.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-organ segmentation is a challenging task. First, the contour of the anatomical
    structure in image is highly variable, which is difficult to expressed by a unified
    mathematical rule. Second, the boundaries between different organs or tissue regions
    in an image are often blurred due to image noise and low intensity contrast, and
    these boundaries are difficult to identify using techniques of traditional digital
    image processing. Third, the use of different scanners, scanning protocols, and
    contrast agents will lead to different intensity distributions of organs in the
    obtained images, which poses a great challenge to the generalizability of the
    model. Finally, considering safety and ethical issues, many hospitals do not disclose
    their datasets. Many segmentation methods are trained and validated on private
    datasets, making it difficult to compare different methods. Therefore, designing
    accurate and robust multi-organ segmentation models is a very difficult and expensive
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional methods [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]
    usually utilize manually extracted image features for image segmentation, such
    as the threshold [[14](#bib.bib14)] method, graph cut [[15](#bib.bib15)] method,
    and region growth [[16](#bib.bib16)] method. Limited by a large number of manually
    extracted image features and the selection of non-robust thresholds or seeds,
    the segmentation results of these methods are usually unstable, and often yield
    only a rough segmentation result or only apply to specific organs. Knowledge-based
    methods can obtain anatomical information of different organs from labelled datasets,
    reduce the burden of manual feature extraction, and improve the robustness and
    accuracy of multi-organ segmentation, which commonly include multi-atlas label
    fusion [[17](#bib.bib17), [18](#bib.bib18)] and statistical shape models [[19](#bib.bib19),
    [20](#bib.bib20)]. The method based on multi-atlas label fusion-based uses image
    alignment to align predefined structural contours to the image to be segmented,
    and this method typically includes multiple steps. Therefore, the performance
    of this method may be influenced by various relevant factors involved in each
    step. The atlas-based method is still very popular, but due to the use of fixed
    atlases, it is difficult to handle the anatomical variation of organs between
    patients. In addition, it is computationally intensive and takes a long time to
    complete an alignment task. The statistical shape model uses the positional relationships
    between different organs, and uses the shape of the organs in the statistical
    space as a constraint to regularize the segmentation results. However, the accuracy
    of this approach is largely dependent on the reliability and extensibility of
    the shape model, and the model based on normal anatomical structures has very
    limited effect in the segmentation of irregular structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using data-driven feature extraction approach and end-to-end training, the
    methods based on deep learning (DL) have been widely studied in the fields of
    image classification [[21](#bib.bib21)], object detection [[22](#bib.bib22)] and
    image segmentation [[23](#bib.bib23), [24](#bib.bib24)], image fusion [[25](#bib.bib25)],
    image registration [[26](#bib.bib26)], etc. The segmentation method based on deep
    learning has become a mainstream method in the field of medical image processing.
    However, there are two main difficulties in multi-organ deep learning segmentation
    tasks. First, as shown in the head and neck in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"), the abdomen in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"), the chest in Fig. [3](#S1.F3 "Figure 3 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"), and the statistics of the multi-organ size in each
    part in Fig. [4](#S1.F4 "Figure 4 ‣ I Introduction ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation"),
    there are very large differences between the organs sizes, and the serious imbalances
    of different organs sizes will lead to a poor segmentation performance of the
    trained segmentation network for small organs. Second, due to the imaging principle
    of CT technology and the complex anatomical structure of the human body, the contrast
    between organs and their surrounding tissues is often low, which leads to the
    inaccurate segmentation of organ boundaries by segmentation networks. Therefore,
    it has become a new hot research topic to develop deep multi-organ segmentation
    methods that can accurately segment small and large organs at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d07a30c9687e01b624aa99e1a2c0d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Schematic diagram of the organs of the head and neck, where the numbers
    are arranged in order: (1) brainstem, (2) left eye, (3) right eye, (4) left lens,
    (5) right lens, (6) left optic nerve, (7) right optic nerve, (8) Optical chiasm,
    (9) left temporal lobe, (10) right temporal lobe, (11) pituitary gland, (12) left
    parotid gland, (13) right parotid gland, (14) left temporal bone rock, (15) right
    temporal bone rock, (16) left temporal bone, (17) right temporal bone, (18) left
    mandibular condyle, (19) right mandibular condyle, (20) spinal cord, (21) left
    mandible, (22) right mandible.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3f3143907aed61b1bd2b904bd15502e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Schematic diagram of the thoracic organs, where the numbers are arranged
    in order: (1) left lung, (2) right lung, (3) heart, (4) esophagus, (5) trachea,
    and (6) spinal cord.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8b11ae81f55dd3a86d9ddc6e9c37d81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Schematic diagram of the abdominal organs, where the numbers are
    arranged in order: (1) liver, (2) kidney, (3) spleen, (4) pancreas, (5) aorta,
    (6) inferior vena cava, (7) stomach, (8) gallbladder, (9) esophagus, (10) right
    adrenal gland, (11) left adrenal gland, and (12) celiac artery.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b458d3b44540ecab196982abf7294a8.png)![Refer to caption](img/e3b4373e1a597be439c4e89396ad4d7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of the percentage of voxels in each organ of the head
    and neck (a), chest (b), and abdomen (c), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, a large number of deep learning-based multi-organ segmentation methods
    with significantly improved performance have been proposed [[27](#bib.bib27)].
    Fu et al. [[28](#bib.bib28)] systematically reviewed the medical image multi-organ
    segmentation methods based on deep learning by 2020 according to the network architecture.
    However, with the rapid development of deep learning technology, more representative
    new techniques and methods have been proposed, such as transformer-based multi-organ
    segmentation methods and imperfect annotation-based methods. A more comprehensive
    review and summary of these techniques and methods are very important for the
    development of this field.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper reviews deep learning-based multi-organ segmentation method of the
    head, neck, chest and abdomen published from 2016 to 2022\. On Google Scholar,
    a search using the keywords ‘Multi Organ Segmentation’ and ‘Deep Learning’ yielded
    an initial 287 articles, 73 articles were removed according to abstract and keywords,
    and 161 highly relevant studies containing a total of 214 relevant references
    were obtained. Fig. [5](#S1.F5 "Figure 5 ‣ I Introduction ‣ Towards More Precise
    Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ
    Segmentation") summarizes all current state-of-the-art deep learning-based multi-organ
    segmentation methods according to full annotation and imperfect annotation architectures.
    In full annotation-based methods, we summarize the existing methods in four aspects:
    network architecture, network dimension, network dedicated modules, and network
    loss function. In imperfect annotation-based methods, we summarize the existing
    methods in two aspects, weak annotation and semi annotation, to investigate their
    innovation, contribution, and challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c66a3d0d23f66f8a0620d8f11abfb3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Framework diagram of the overview.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is organized as follows. Section [II](#S2 "II Definition and Evaluation
    Metrics ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation") expounds the mathematical definition
    of multi-organ segmentation and the corresponding evaluation metrics. Section
    [III](#S3 "III Multi-organ Segmentation Datasets ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    summarizes the multi-organ segmentation datasets. Section [IV](#S4 "IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation") describes the literature based on full
    annotation-based methods, involving four parts: network architecture (section
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").A), network
    dimension (section [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise
    Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ
    Segmentation").B), network dedicated modules (section [IV](#S4 "IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation").C), and network loss function (section
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").D). Section
    V analyzes the articles based on imperfect annotation methods, including two parts:
    weak annotation-based methods (section [V](#S5 "V Imperfect Annotation-based Methods
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation").A) and semi annotation-based methods (section [V](#S5
    "V Imperfect Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").B). We
    discuss the existing methods and their future outlooks in section [VI](#S6 "VI
    Discussion and Future Trends ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation"), and conclude the whole
    paper in section [VII](#S7 "VII Conclusion ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").'
  prefs: []
  type: TYPE_NORMAL
- en: II Definition and Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let $\boldsymbol{X}$ represent the union of multiple organ regions in the input
    images, $\boldsymbol{G}$ represent the union of ground truth labels of multiple
    organs in the input images, $\boldsymbol{P}$ represent the union of predicted
    labels of multiple organs in the output images, $\boldsymbol{x_{i}^{c}\in X,g_{i}^{c}\in
    G,p_{i}^{c}\in P,i=1,\cdots N}$, and $\boldsymbol{c=1,\cdots C}$, where $\boldsymbol{N}$
    represents the number of pixel in the image, $\boldsymbol{C}$ represents the number
    of categories to which the pixels belong, $\boldsymbol{f}$ represents the neural
    network, and $\boldsymbol{\theta}$ represents the parameters of the neural network
    optimization, where $\boldsymbol{P=f(X;\theta)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function represents the gap between the predicted and true values.
    In the multi-organ segmentation task, common loss functions include the cross-entropy
    loss and Dice loss. Section [IV-D](#S4.SS4 "IV-D Network Loss Function ‣ IV Full
    Annotation-based Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") provides specific details
    about the loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a multi-organ segmentation task, $\left\{\boldsymbol{\Psi}\right\}$ represents
    the represents the class set of organs to be segmented. $\left\{\boldsymbol{x}\right\}_{\ast}$
    represents the set of organs annotated in $\boldsymbol{x}$. According to the available
    annotations, multi-organ segmentation can be implemented according to three learning
    paradigms: full annotation-based learning, weak annotation-based learning, and
    semi annotation-based learning. The last two are called imperfect annotation-based
    methods, as shown in Fig. [6](#S2.F6 "Figure 6 ‣ II Definition and Evaluation
    Metrics ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation"). Full annotation-based learning means
    that the labels of all organ are given, which indicates that $\forall\boldsymbol{x}\in\boldsymbol{X},\{\boldsymbol{x}\}_{*}=\{\boldsymbol{\Psi}\}$
    . Weak annotation often means that the data come from $\boldsymbol{n}$ different
    datasets. However, each dataset provides the annotations of one or more organs
    but not all organs, which means that $\boldsymbol{X=X_{1}\cup X_{2}\cup\cdots\cup
    X_{n},\forall x_{k,i}\in X_{k},k=1,2,\ldots n}$, $\boldsymbol{\left\{x_{k,i}\right\}_{*}<\{\Psi\},\bigcup_{k=1}^{n}\left\{x_{k,i}\right\}_{*}=\{\Psi\}}$.
    Here, $\boldsymbol{x_{k,i}}$ denotes the $\boldsymbol{i}$-th image in $\boldsymbol{X_{k}}$.
    Semi annotation-based methods indicate that some of the training datasets are
    fully labelled and others are unlabelled, $\boldsymbol{X}=\boldsymbol{X}_{\boldsymbol{l}}\cup\boldsymbol{X_{u}}\cdot\boldsymbol{X}_{\boldsymbol{l}}$.
    $\boldsymbol{X_{l}}$ represents the fully labelled dataset, $\boldsymbol{X_{u}}$
    represents the unlabelled dataset, which indicates that $\forall\boldsymbol{x_{l}\in
    X}_{\boldsymbol{l}},\left\{\boldsymbol{x}_{\boldsymbol{l}}\right\}_{*}=\{\boldsymbol{\Psi}\}$
    and $\forall\boldsymbol{x}_{\boldsymbol{u}}\in\boldsymbol{X}_{\boldsymbol{u}},\left\{\boldsymbol{x}_{\boldsymbol{u}}\right\}_{*}=\boldsymbol{\phi}$,
    and the size of $\boldsymbol{X_{l}}$ is far less than the one of $\boldsymbol{X_{u}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6fdb4f766bb20227e2f8499f6019e74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: General overview of the learning paradigms reviewed in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually using the Dice similarity coefficient (DSC), 95% Hausdorff distance
    (HD95) and mean surface distance (MSD) to evaluate the performance of the segmentation
    methods. DSC is a measure of the volume overlap between the predicted labels and
    ground truth labels, HD95 and MSD are measures of the surface distance between
    the predicted labels and ground truth labels.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{DSC=\dfrac{2\times\left&#124;P^{c}\cap G^{c}\right&#124;}{\left&#124;P^{c}\right&#124;+\left&#124;G^{c}\right&#124;}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\boldsymbol{HD95=\max_{95\%}\left[d\left(P_{s}^{c},G_{s}^{c}\right),d\left(G_{s}^{c},P_{s}^{c}\right)\right]}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $MSD=\frac{1}{\left&#124;P_{s}^{c}\right&#124;+\left&#124;G_{s}^{c}\right&#124;}\left(\sum_{p_{s}^{c}\in
    P_{s}^{c}}d\left(p_{s}^{c},G_{s}^{c}\right)+\sum_{g_{s}^{c}\in G_{s}^{c}}d\left(g_{s}^{c},P_{s}^{c}\right)\right)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{P}^{\boldsymbol{c}}$ and $\boldsymbol{G}^{\boldsymbol{c}}$
    represent the set of predicted pixels and the set of real pixels of the $\boldsymbol{c}$
    class organ, respectively; $\boldsymbol{P_{s}^{c}}$ and $\boldsymbol{G_{s}^{c}}$
    represent the set of predicted pixels and the set of real pixels of the surface
    of the $\boldsymbol{c}$ class organ, respectively; and $\boldsymbol{d\left(p_{s}^{c},G_{s}^{c}\right)=\min_{g_{s}^{c}\in
    G_{s}^{c}}^{c}\left\|p_{s}^{c}-g_{s}^{c}\right\|_{2}}$ represents the minimal
    distance from point $\boldsymbol{p_{s}^{c}}$ to surface $\boldsymbol{G_{s}^{c}}$.
    The review reports various methods based on DSC values.
  prefs: []
  type: TYPE_NORMAL
- en: III Multi-organ Segmentation Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To obtain high-quality organ segmentation datasets, many research teams have
    undertaken several collaborations with medical organizations. Table [I](#S3.T1
    "TABLE I ‣ III Multi-organ Segmentation Datasets ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    summarizes the common head and neck, thorax, and abdomen datasets used for the
    development and validation of multi-organ segmentation method. Table I also shows
    that the quantity of annotated data available for deep learning studies is still
    very low.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Frequently Used Dataset for Multi-organ Segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Dataset | Modality | Part | Number of organs (specific organs) | Quantity
    | Labelling status | Image size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | MICCAI Multi-Atlas Labelling Beyond the Cranial Vault (BTCV) [[29](#bib.bib29)]
    | CT | Abdomen | 13 (spleen, right kidney, left kidney, gallbladder, esophagus,
    liver, stomach, aorta, inferior vena cava, portal and splenic veins, pancreas,
    right adrenal gland, left adrenal gland) | 50 (30 training and 20 testing) | The
    training set are labelled, the test set are not labelled | 512 × 512 × [85$\sim$198]
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | MICCAI head and neck Auto Segmentation Challenge (HNC) [[30](#bib.bib30)]
    | CT | Head and neck | 9 (brainstem, mandible, chiasm, left optic nerves, right
    optic nerves, left parotid glands, right parotid glands, left submandibular glands,
    right submandibular glands) | 35 (25 training, 10 off-site tests, 5 on-site tests)
    | Labelled | 512 × 512 × [110$\sim$190] |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Synapse multi- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; organ segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dataset (Synapse) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CT | Abdomen | 13 (spleen, right kidney, left kidney, gallbladder, esophagus,
    liver, stomach, aorta, inferior vena cava, portal vein and splenic vein, pancreas,
    right adrenal gland, left adrenal gland) | 50 (30 training, 20 testing) | Labelled
    | 512 × 512 × [85$\sim$198] |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Public Domain Database for Computational Anatomy (PDDCA) [[30](#bib.bib30)]
    | CT | Head and neck | 9 (brainstem, mandible, chiasm, left optic nerves, right
    optic nerves, left parotid glands, right parotid glands, left submandibular glands,
    right submandibular glands) | 48 (25 training, 8 additional training, 10 off-site
    and 5 on-site tests) | Labelled | 512 × 512 × [110$\sim$190] |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Thoracic Auto-segmentation Challenge (AAPM) [[31](#bib.bib31)] | CT
    | Thorax | 5 (left lung, right lung, heart, Esophagus, spinal cord) | 60 (36 training,
    12 off-site tests, 12 on-site tests) | Labelled | 512 × 512 × [103$\sim$279] |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Combined (CT-MR) Healthy Abdominal Organ Segmentation (CHAOS) [[32](#bib.bib32)]
    | CT | Abdomen | 4 (left kidney, right kidney, liver, spleen) | 40 (20 training
    and 20 testing) | Labelled training set and unlabeled test set | 512 × 512 × [78$\sim$294]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MR | 40 (20 training, 20 testing) × 3 sequences | Labelled training set and
    unlabeled test set | 256 × 256 × [26$\sim$50] |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | SegTHOR Challenge: Segmentation of Thoracic Organs at Risk in CT Images
    (SegTHOR) [[33](#bib.bib33)] | CT | Thorax | 5 (left and right lungs, heart, esophagus,
    spinal cord) | 60 (36 training, 12 off-site tests, 12 on-site tests) | Labelled
    | 512 × 512 × N |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Annotations for Body Organ Localization based on MICCAI LITS Dataset
    [[34](#bib.bib34)] | CT | Thorax | 11 (heart, left lung, right lung, liver, spleen,
    pancreas, left kidney, right kidney, bladder, left femoral head, right femoral
    head) | 201 (131 training and 70 testing) | Bounding boxes labelled | 512 × 512
    × N |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Automatic Structure Segmentation for Radiotherapy Planning Challenge
    2019 (StructSeg) | CT | Head and neck | 22 (left eye, right eye, left lens, right
    lens, left optical nerve, right optical nerve, chiasm, pituitary, brainstem, left
    temporal lobes, right temporal lobes, spinal cord, left parotid gland, right parotid
    gland, left inner ear, right inner ear, left middle ear, right middle ear, left
    temporomandibular joint, right temporomandibular joint, left mandible, right mandible)
    | 60 (50 training, 10 testing) | Labelled training set and unlabeled test set
    | 512 × 512 × [98$\sim$140] |'
  prefs: []
  type: TYPE_TB
- en: '| Thorax | 6 (left lung, right lung, spinal cord, esophagus, heart, trachea)
    | 60 (50 training, 10 testing) |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | OpenKBP: The open-access knowledge-based planning grand challenge
    and dataset [[35](#bib.bib35)] | CT | Head and neck | 7 (brainstem, spinal cord,
    right parotid, left parotid, larynx, esophagus, mandible) | 340 (200 training,
    40 validating, 100 testing) | Labelled | 128×128×128 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Abdomenct-1k [[36](#bib.bib36)] | CT | Abdomen | 5 (liver, right and
    left kidneys, spleen, pancreas) | 1112 | Labelled | 512 × 512 × N |'
  prefs: []
  type: TYPE_TB
- en: IV Full Annotation-based Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The method based on full annotation means that all organs of the multi-organ
    segmentation task are fully annotated. The existing methods can be analysed from
    four parts: network architecture, network dimension, network dedicated modules,
    and network loss function. Among these methods, the network architecture part
    summarizes the common neural network architectures and the combination or cascade
    of different architectures. In the network dimension part, the existing methods
    are classified into 2D, 3D, and multi-view methods according to the image dimension
    used. The part of network dedicated modules describes modules that are commonly
    used in multi-organ segmentation to improve the segmentation performance, The
    part of network loss function summarizes how common loss functions are innovated
    around multi-organ segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the design of the network architecture, multi-organ segmentation methods
    can be classified according to single-stage and multistage implementations. Single-stage
    methods include those based on CNN (Convolutional Neural Network), GAN (Generative
    Adversarial Network), transformer or hybrid networks. Multistage approaches include
    coarse-to-fine methods, localization and segmentation methods, or other cascade
    approaches. Tables [II](#Ax1.T2 "TABLE II ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")- [IV](#Ax1.T4
    "TABLE IV ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") summarize the literature related
    to single-stage methods for the segmentation of multi-organ in the head and neck,
    abdomen and chest based on DSC metrics. Since there are too many organs in the
    head and neck as well as abdomen, this paper mainly reports 9 organs in the head
    and neck and 7 organs in the abdomen. Tables [XI](#Ax1.T11 "TABLE XI ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")- [XII](#Ax1.T12 "TABLE XII ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    in the supplementary materials summarize the DSC values of other organs.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A1 CNN-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional Neural Network (CNN) is a feedforward neural network which can
    automatically extract deep features of the image. Multiple neurons are connected
    to each neuron in next layer, where each layer can perform complex tasks such
    as convolution, pooling, or loss computation [[37](#bib.bib37)]. CNNs have been
    successfully applied to medical images, such as brain [[38](#bib.bib38), [39](#bib.bib39)]
    and pancreas [[40](#bib.bib40)] segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Early CNN-Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Earlier CNN-based methods mainly used convolutional layers to extract features
    and then went through pooling layers and fully connected layers to obtain the
    final prediction results. Ibragimov and Xing [[41](#bib.bib41)] used deep learning
    methods to segment OARs in head and neck CT images for the first time, training
    13 CNNs for 13 OARs, and showed that the CNNs outperformed or were comparable
    to advanced algorithms in segmentation accuracy for organs such as the spinal
    cord, mandible, larynx, pharynx, eye, and optic nerve, but performed poorly in
    the segmentation of organs such as the parotid gland, submandibular gland, and
    optical chiasm. Fritscher et al. [[42](#bib.bib42)] combined the shape location
    as well as the intensity with CNN for segmentation of the parotid gland, submandibular
    gland and optic nerve. Moeskops et al. [[43](#bib.bib43)] investigated whether
    a single CNN can be used to segment six tissues in brain MR images, pectoral muscles
    in breast MR images, and coronary arteries in heart CTA images. The results showed
    that a single CNN can segment multiple organs not only on a single modality but
    also on multiple modalities.
  prefs: []
  type: TYPE_NORMAL
- en: FCN-Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Early CNN-based methods made some improvements in segmentation accuracy compared
    to traditional methods. However, CNN involves multiple identical computations
    of overlapping voxels during the convolution operation, which may cause some performance
    loss. Moreover, the spatial information of the image is lost when the convolutional
    features are input into the final fully connected network layer. Thus, Shelhamer
    et al. [[44](#bib.bib44)] proposed the Fully Convolutional Network (FCN), which
    enables end-to-end segmentation by using transposed convolutional layers that
    allow the size of the predicted image to match the size of the input image. Wang
    et al. [[45](#bib.bib45)] used FCN combined with a new sample selection strategy
    to segment 16 organs in the abdomen, and Trullo et al. [83] used a variant of
    FCN, SharpMask [[46](#bib.bib46)], to segment the esophagus, heart, trachea, and
    aorta in the thorax, which showed the segmentation results of all four organs
    were improved compared with the normal FCN.
  prefs: []
  type: TYPE_NORMAL
- en: U-Net-Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Based on FCN, Ronneberger et al. [[47](#bib.bib47)] proposed a classical U-Net
    architecture, which is consisted of an encoder for the down sampling layer and
    a decoder for the up-sampling layer, and connects them layer by layer with skip
    connections, so that the features extracted from the down sampling layer can be
    directly transmitted to the up-sampling layer to fuse multiscale features for
    segmentation. U-Net has become one of the most commonly used architectures in
    the field of multi-organ segmentation [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)]. Roth
    et al. [[52](#bib.bib52)] applied the U-Net architecture to segment the abdominal
    aorta, portal vein, liver, spleen, stomach, gallbladder, and pancreas. The advanced
    segmentation performance of multiple organs was achieved with an average Dice
    value of 0.893 for seven organs. Lambert et al. [[55](#bib.bib55)] proposed a
    simplified U-Net for segmenting the heart, trachea, aorta, and esophagus of the
    chest. The results showed that adding dropout and using bilinear interpolation
    can significantly improve the segmentation performance of the heart, aorta, and
    esophagus compared with the ordinary U-Net. In addition to U-Net, V-Net [[56](#bib.bib56)]
    proposes 3D image segmentation method based on volumetric, fully convolutional
    neural network. This method can directly process 3D medical data by introducing
    residual connections and using convolutional layers instead of pooling layers
    in the original U-Net. Gibson et al. [[57](#bib.bib57)] used dense V-Networks
    to segment the pancreas, esophagus, stomach, liver, spleen, gallbladder, left
    kidney, and duodenum of the abdomen. Xu et al. [[58](#bib.bib58)] proposed a new
    probabilistic V-Net model which combines a conditional variational autoencoder
    (cVAE) and hierarchical spatial feature transform (HSPT) for abdominal multi-organ
    segmentation. nnU-Net [101] is a novel framework based on U-Net architecture with
    the addition of adaptive pre-processing, data enhancement, and postprocessing
    techniques, and has shown state-of-the-art results on many publicly available
    datasets for different biomedical segmentation challenges [[59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]. Podobnik et al. [[59](#bib.bib59)] reported
    the results of segmentation of 31 OARs of the head and neck using the nnU-Net
    architecture combined with CT and MR images.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 GAN-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A typical Generative Adversarial Network (GAN) [[63](#bib.bib63)] includes a
    pair of competitive networks, which are generators and discriminators. The generator
    attempts to deceive the discriminator by generating the artificial data, and the
    discriminator strives to discriminate the artificial data without being deceived
    by the generator; after alternate optimization training, the performance of both
    networks can eventually be improved. In recent years, many GAN-based multi-organ
    segmentation methods have been proposed and achieved high segmentation accuracy
    [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68),
    [69](#bib.bib69), [70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: Dong et al. [[66](#bib.bib66)] jointly trained GAN with a set of U-Nets as a
    generator and a set of FCNs as a discriminator for segmenting the left lung, right
    lung, spinal cord, esophagus and heart from chest CT images. The results showed
    that the segmentation performance of most of the organs were improved with the
    help of adversarial networks, and the average DSC values of the above five OARs
    were finally obtained as 0.970, 0.970, 0.900, 0.750 and 0.870\. Tong et al. [[64](#bib.bib64)]
    proposed a Shape-constraint GAN for automatic head and neck OARs segmentation
    (SC-GAN) from CT and low-field MRI images. It uses DenseNet, a deep supervised
    fully convolutional network to segment organs for prediction, and uses a CNN as
    discriminator network to correct the error of prediction. The results show that
    the combination of GAN and DenseNet can further improve the segmentation performance
    of CNN based on the original shape constraints.
  prefs: []
  type: TYPE_NORMAL
- en: GAN can improve accuracy with its adversarial losses. However, the training
    of GAN network is difficult and time-consuming since the generator needs to achieve
    Nash equilibrium with the discriminator. And its adversarial loss as a shape modifier
    can only achieve higher segmentation accuracy when segmenting organs with regular
    and unique shapes (such as liver and heart), but may not work well for irregular
    or tubular structures (such as pancreas and aorta).
  prefs: []
  type: TYPE_NORMAL
- en: IV-A3 Transformer-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CNN-based methods can perform well for segmenting multiple organs in many tasks,
    but the inherent shortcomings of the perceptual field of the convolutional layers
    lead to the inability of CNNs to model global relationships, hindering the performance
    of the models. The self-attentive mechanism of the transformer [[71](#bib.bib71)]
    can solve the long-term dependency problem well, achieving better results than
    CNNs in many tasks such as natural language processing (NLP) or computer vision
    [[72](#bib.bib72)]. The performance of the medical image segmentation networks
    using transformer is also close or even better than the one of current state-of-the-art
    methods [[73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: Cao et al. [[77](#bib.bib77)] integrated the transformer with a U-shaped architecture
    to explore the potential of the pure transformer model for abdominal multi-organ
    segmentation. The results showed that the method has good segmentation accuracy.
    However, the method needs to initialize the network encoder and decoder using
    the training weights of the Swin transformer on ImageNet. Huang et al. [[78](#bib.bib78)]
    introduced an efficient and powerful medical image segmentation architecture,
    MISSFormer, where the proposed enhanced mixed block can effectively overcome the
    feature recognition limitation problem caused by convolution. Moreover, compared
    with Swin-UNet, this model does not require pre-training on large-scale datasets
    to achieve comparable segmentation results.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based approaches can capture long-range dependencies and achieve
    better performance than CNNs in many tasks. However, multi-organ segmentation
    problem involves the segmentation of many tiny organs, and the pure transformer
    network focuses on the global context modelling. This leads to the lack of detailed
    localization information of low-resolution features. Thus, a coarser segmentation
    result is usually obtained.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A4 Hybrid Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CNN convolution operation can extract local features well, but it is difficult
    to obtain global features. The self-attentive mechanism of the transformer can
    effectively capture feature dependencies over long distances, but it loses local
    feature details, which may obtain poor results for the segmentation accuracy of
    small organs. Therefore, some researchers have combined the CNN and transformer
    to overcome the limitations of both architectures [[79](#bib.bib79), [74](#bib.bib74),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)].
  prefs: []
  type: TYPE_NORMAL
- en: Suo et al. [[84](#bib.bib84)] proposed an intra-scale and inter-scale collaborative
    learning network (I2-Net) by combining features extracted by the CNN and transformer
    to segment multiple organs of the abdomen, which improved the segmentation performance
    of small and medium-sized organs by 4.19% and 1.83%-3.8%, respectively. Kan et
    al. [[85](#bib.bib85)] proposed ITUnet, which adds the features extracted by the
    transformer to the output of each block of the CNN-based encoder, which can obtain
    segmentation results provided by both the local and global information of the
    image. ITUnet has better accuracy and robustness than other methods, especially
    on difficult organs such as the lens. Chen et al. [[86](#bib.bib86)] proposed
    a new network architecture, TransUNet, which uses a transformer to further encode
    CNN encoders to build stronger encoders and report competitive results for multi-organ
    segmentation of the head and neck. Hatamizadeh et al. [[87](#bib.bib87)] proposed
    a new architecture U-net transformer (UNETR) using a transformer as an encoder
    and the CNN as a decoder, which achieves better segmentation accuracy by capturing
    global and local dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the methods combining CNN and transformer, there are some other
    hybrid frames. For example, Chen et al. [[88](#bib.bib88)] combined U-Net and
    long short-term memory (LSTM) to realize the segmentation of five organs in the
    chest, and the DSC values of all five organs were above 0.8\. Chakravarty et al.
    [[89](#bib.bib89)] proposed a hybrid architecture combining CNN and RNN to segment
    the optic disc, nucleus, and left atrium. The hybrid architecture-based approach
    can combine and utilize the advantages of the two architectures for the accurate
    segmentation of small and medium-sized organs, which is a key research direction
    for the future.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A5 Cascade Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Due to most organs occupy only a small volume in images, the segmentation models
    are easy to segment large organs and ignore small organs, which prompted researchers
    to propose cascade multistage methods. Multistage methods can be divided into
    two main categories, depending on the information provided by the primary network
    to the secondary network. The first category is called coarse-to-fine multi-organ
    segmentation method, where the first network performs coarse segmentation, and
    its results are passed to another network to achieve fine segmentation. The second
    category is called multi-organ segmentation method based on localization and segmentation,
    where candidate boxes for the location of each organ are identified by registration
    methods or localization networks, and then input into the second-level network
    for fine segmentation. In addition, the first network can provide other information,
    such as the shape location or proportion, to better guide the segmentation of
    the second network. Tables [V](#Ax1.T5 "TABLE V ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[X](#Ax1.T10
    "TABLE X ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") summarize the relevant literature
    of the cascade methods for head and neck, chest and abdomen based on DSC metrics,
    and tables [VIII](#Ax1.T8 "TABLE VIII ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[IX](#Ax1.T9
    "TABLE IX ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") in the supplementary materials
    summarize the DSC metrics of other organs.'
  prefs: []
  type: TYPE_NORMAL
- en: Coarse-to-Fine-Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The coarse-to-fine-based methods first inputs the original image and its corresponding
    labels into the first network. After training, the first-level network obtains
    the coarse segmentation probability map, which will be multiplied by the original
    image, and the results will be input into the second network to refine the rough
    segmentation. This process is shown in Fig. [7](#S4.F7 "Figure 7 ‣ Coarse-to-Fine-Based
    Methods ‣ IV-A5 Cascade Networks ‣ IV-A Network Architecture ‣ IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation"). In recent years, a number of coarse-to-fine
    methods have been proposed for multi-organ segmentation [[90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99)], and the references are
    shown in Tables [VI](#Ax1.T6 "TABLE VI ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[VIII](#Ax1.T8
    "TABLE VIII ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bfc1de36549d0a7b7727c028978d6c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Coarse-to-fine-based segmentation method.'
  prefs: []
  type: TYPE_NORMAL
- en: Trullo et al. [[100](#bib.bib100)] proposed two synergistic depth architectures
    to jointly segment all organs, including the esophagus, heart, aorta, and trachea.
    Probabilistic maps obtained in the first stage were passed to the second stage
    to learn anatomical constraints, and then four networks were trained for four
    structures in the second stage to distinguish the background from each target
    organ in separate refinements. Zhang et al. [[94](#bib.bib94)] proposed a new
    cascaded network model with Block Level Skip Connections (BLSC) between two cascaded
    networks. This architecture enabled the second-stage network to capture the features
    learned by each block in the first-stage network and accelerated the convergence
    of the second-stage network. Xie et al. [[95](#bib.bib95)] proposed a new framework
    called the Recurrent Saliency Transformation Network (RSTN). This framework enabled
    coarse scale segmentation masks to be passed to the fine stage as spatial weights,
    while gradients can be backpropagated from the loss layer to the whole network,
    so as to realize the joint optimization of the two stages, thus improving the
    segmentation accuracy of small targets. Ma et al. [[92](#bib.bib92)] proposed
    a new end-to-end coarse-to-fine segmentation model to automatically segment multiple
    OARs in head and neck CT images. This model used a predetermined threshold to
    classify the initial results of the coarse stage into large and small OARs, and
    then designed different modules to refine the segmentation results.
  prefs: []
  type: TYPE_NORMAL
- en: This coarse-to-fine approach effectively reduces the complexity of the background
    and enhances the discriminative information of the target structures. Compared
    with the single-stage approach, this coarse-to-fine-based method improves the
    segmentation results for small organs, but there are limitations in memory and
    training time because at least two networks need to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Localization and Segmentation Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The localization and segmentation methods are also multistage cascade methods.
    Here, the first-level network provides location information, returns a candidate
    frame, and crops the region of interest of the image according to the location
    information, and uses it as the input of the second network. In this way, when
    the second network performs segmentation, one organ can be targeted, excluding
    the interference of other organs or background noise and improving the segmentation
    accuracy. The process is shown in Fig. [8](#S4.F8 "Figure 8 ‣ Localization and
    Segmentation Based Methods ‣ IV-A5 Cascade Networks ‣ IV-A Network Architecture
    ‣ IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation"). The
    organ location in the first stage can be obtained through registration or localization
    network. The relevant literature of multi-stage method based on location and segmentation
    are listed in Tables [VIII](#Ax1.T8 "TABLE VIII ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[X](#Ax1.T10
    "TABLE X ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation"), and the DSC values of other organs
    are listed in tables [XV](#Ax1.T15 "TABLE XV ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[XVI](#Ax1.T16
    "TABLE XVI ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/acc452e4fe5899ccc3d80db4f9760314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Localization and segmentation based method.'
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. [[101](#bib.bib101)], Men et al. [[102](#bib.bib102)], Lei et al.
    [[103](#bib.bib103)], Francis et al. [[104](#bib.bib104)], Tang et al. [[105](#bib.bib105)]
    proposed decomposing OARs segmentation into two stages of localization and segmentation.
    The first stage localizes the target OARs using the bounding box, the second stage
    segments the target OARs within the bounding box, and both stages use neural networks.
    Among them, Wang et al. [[101](#bib.bib101)] and Francis et al. [[104](#bib.bib104)]
    used a 3D U-net in both stages. Lei et al. [[103](#bib.bib103)] used Faster RCNN
    to automatically locate the ROI of organs in the first stage. Korte et al. [[106](#bib.bib106)]
    demonstrated that the CNN is a suitable method for automatically segmenting parotid
    and submandibular glands in MRI images of HNC patients. The segmentation accuracy
    of the parotid and submandibular glands can be improved by cascading localizing
    CNNs, cropping and segmenting high-resolution CNNs. FocusNet [[69](#bib.bib69),
    [107](#bib.bib107)] presented a novel deep neural network to solve the class imbalance
    problem in the segmentation of head and neck OARs. The small organs are first
    localized by the organ localization network. Then, combined with the high-resolution
    information of each small organ, multiscale features are input to the segmentation
    network together to accurately segment the small organs.
  prefs: []
  type: TYPE_NORMAL
- en: The organ localization by Larsson et al. [[108](#bib.bib108)], Zhao et al. [[109](#bib.bib109)],
    Ren et al. [[110](#bib.bib110)] and Huang et al. [[111](#bib.bib111)] was obtained
    with registration method followed by the application of convolutional neural networks
    for segmentation. Among them, Ren et al. [[110](#bib.bib110)] designed interleaved
    cascades of 3D-CNNs to segment each structure of interest. Since adjacent tissues
    are usually highly correlated from a physiological and anatomical perspective,
    using the initial segmentation results of a specific tissue can help refine the
    segmentation of other neighbouring tissues. Zhao et al. [[109](#bib.bib109)] proposed
    a new flexible knowledge-assisted convolutional neural network which combine deep
    learning and traditional methods to improve the segmentation accuracy in the second
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: The vast majority of approaches require to determine the target areas prior
    to segmentation network training by different localization methods. For example,
    Ren et al. [[110](#bib.bib110)] localized organ regions through a multi-atlas-based
    method. Wang et al. [[101](#bib.bib101)] used separate CNNs to localize candidate
    areas. That is, their target organ region localization is constructed independently
    of organ segmentation, which will hinder the transmission of information between
    these two related learning tasks. On this basis, Liang et al. [[112](#bib.bib112)]
    proposed a multi-organ segmentation framework based on multi view spatial aggregation,
    which combines the learning of the organ localization subnetwork and the segmentation
    subnetwork to reduce the influence of background regions and neighbouring similar
    structures in the input data. Additionally, the proposed fine-grained representation
    based on ROIs can improve the segmentation accuracy of organs with different sizes,
    especially the segmentation results of small organs.
  prefs: []
  type: TYPE_NORMAL
- en: The type of multistage method improves the organ segmentation accuracy, especially
    for small organs, which largely reduces the interference of the background. However,
    this two-step process has certain requirements for memory and training time, and
    the segmentation accuracy also depends largely on the regional localization accuracy.
    Better localization of organs and improvement of segmentation accuracy are still
    directions to be investigated in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Other Cascade Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In addition to probability maps and localization information, the first network
    can also provide other types of information, such as scale information and shape
    priors. For example, Tong et al. [[113](#bib.bib113)] combines the FCNN and a
    shape representation model (SRM) for head and neck OARs segmentation. The first-level
    network is the SRM for learning highly representative shape features in head and
    neck organs. The direct comparison of the FCNN with and without SRM shows that
    the SRM significantly improves the segmentation accuracy of nine organs with different
    sizes, morphological complexity, and different CT contrasts. Roth et al. [[114](#bib.bib114)]
    proposed a multiscale 3D FCN approach which is accomplished by two cascaded FCNs,
    where low-resolution 3D FCN predictions are upsampled, cropped, and connected
    to higher-resolution 3D FCN inputs. In this case, the primary network provides
    scale information to the secondary network. And the method uses the scale space
    pyramid with automatic context to perform high-resolution semantic image segmentation,
    while considering large contextual information from the lower resolution levels.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Network Dimension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Considering the dimensionality of input images and convolutional kernels, multi-organ
    segmentation neural networks can be classified into 2D, 2.5D and 3D architectures,
    as shown in Fig. [9](#S4.F9 "Figure 9 ‣ IV-B Network Dimension ‣ IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation"), and the differences between the three
    architectures will be discussed in follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6d3e6157643d048984b03075d4e8000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Different network dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 2D- & 3D-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input of the 2D multi-organ segmentation neural network is slices from a
    three-dimensional medical image, and the convolution kernel is also two-dimensional.
    Men et al. [[115](#bib.bib115)], Trullo et al. [[100](#bib.bib100)], Gibson et
    al. [[57](#bib.bib57)], Chen et al. [[116](#bib.bib116)], Zhang et al. [[51](#bib.bib51)],
    Chen et al. [[117](#bib.bib117)] used 2D networks for multi-organ segmentation.
    2D architectures can reduce the GPU memory burden, but CT or MRI images are inherently
    3D. Moreover, slicing images into 2D tends to ignore the rich information in the
    entire image voxel, so 2D models are insufficient for analysing the complex 3D
    structures in medical images.
  prefs: []
  type: TYPE_NORMAL
- en: 3D multi-organ segmentation neural network architectures use 3D convolutional
    kernels, which can directly extract feature information from 3D medical images.
    Roth et al. [[52](#bib.bib52)], Zhu et al. [[48](#bib.bib48)], Gou et al. [[50](#bib.bib50)],
    and Jain et al. [[118](#bib.bib118)] used 3D architectures for multi-organ segmentation.
    However, due to GPU memory limitations, 3D architectures may face computationally
    intensive and memory shortage problems, so the majority of 3D network methods
    use sliding windows acting on patches. Zhu et al. [[48](#bib.bib48)] proposed
    a deep learning model called AnatomyNet, which receives full-volume head and neck
    CT images as the inputs and generates masks of all organs to be segmented at once.
    AnatomyNet only uses a down sampling layer in the first encoding block to consider
    the trade-off between GPU memory usage and network learning capability, which
    can occupy less GPU memory than other network structures while preserving information
    about small anatomical structures.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Multi-View-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In medical image segmentation, it is crucial to make good use of the spatial
    information between medical image slices. Directly input 3D images into the network,
    the 3D images will occupy huge memory, or convert 3D images to 2D images, the
    spatial information between medical image slices will be directly discarded. Thus,
    the idea of multiple views has appeared, which means using 2.5D neural networks
    with multiple 2D slices and combining 2D convolution and 3D convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 2.5D multi-organ segmentation neural network architecture still uses 2D
    convolutional kernels, but the input of the network is multiple slices, either
    a stack of adjacent slices using interslice information [[119](#bib.bib119), [120](#bib.bib120)],
    or slices along three orthogonal directions (axial, coronal, and sagittal) [[41](#bib.bib41),
    [42](#bib.bib42), [91](#bib.bib91), [121](#bib.bib121)]. This 2.5D approach saves
    computational resources and makes good use of spatial information. It is also
    widely used in semi supervised-based methods, which are reviewed in Section [V-B](#S5.SS2
    "V-B Semi Supervised-Based Methods ‣ V Imperfect Annotation-based Methods ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"). Zhou et al. [[122](#bib.bib122)] segmented each 2D
    slice using the FCN by sampling a 3D CT case on three orthogonally oriented slices
    (2D images) and then assembled the segmented output (i.e., 2D slice results) back
    into 3D. Chen et al. [[117](#bib.bib117)] developed a multi-view training method
    at the ratio of 4:1:1 on different views (axial, coronal, and sagittal) and applied
    a majority voting strategy to combine the three predictions into a final segmentation.
    The results show that the method can remove some wrong segmentation areas in the
    single-view output, especially for the small intestine and duodenum. Wang et al.
    [[123](#bib.bib123)] used a statistical fusion approach to combine segmentation
    results from three views and relate the structural similarity of 2D views to the
    original 3D image. Liang et al. [[121](#bib.bib121)] performed context-based iterative
    refinement training on each of the three views and aggregated all the predicted
    probability maps of the three orthogonal views in the last iteration to obtain
    the final segmentation results. Experiments show that this multi-view framework
    outperforms the segmentation results of the three separate views.'
  prefs: []
  type: TYPE_NORMAL
- en: Tang et al. [[124](#bib.bib124)] proposed a new framework for combining 3D and
    2D models, which implements segmentation through high-resolution 2D convolution
    and extracting spatial contextual information through low-resolution 3D convolution.
    The corresponding 3D features used to guide 2D segmentation are controlled by
    a self-attentive mechanism, and the results show that this method consistently
    outperforms existing 2D and 3D models. Chen et al. [[116](#bib.bib116)] proposed
    a hybrid convolutional neural network, OrganNet2.5D, which can make full use of
    3D image information to process different planar and depth image resolutions.
    OrganNet2.5D integrates 2D convolution and 3D convolution to extract both clear
    underlying edge features and rich high-level semantic features.
  prefs: []
  type: TYPE_NORMAL
- en: Some current studies only deal with 2D image, which avoids memory and computation
    problems but does not make full use of 3D image information. 2.5D methods can
    make better use of information from multiple views and improve single-view segmentation
    compared to 2D networks, but the spatial contextual information they can extract
    is still limited. Moreover, the current 2.5D methods using in multi-organ segmentation
    are the aggregation of three perspectives at the outcome level, and the intermediate
    processes are independent of each other; better use of the intermediate learning
    process is also the direction to be investigated [[125](#bib.bib125), [126](#bib.bib126),
    [127](#bib.bib127)]. Some studies have performed 3D convolution, but local patches
    need to be processed. For example, Networks that process full-volume 3D CT images,
    similar to AnatomyNet, use only a down sampling layer to preserve information
    about small anatomical structures, so the receptive field of these networks is
    limited. To solve this problem, DenseASPP with four expansion rates (3,6,12,18)
    is introduced into FocusNet [[107](#bib.bib107)]; however, when the expansion
    rates of the cascaded expanded convolution have a common factor relationship,
    grid problems affecting the segmentation accuracy may occur. Pure 3D networks
    also face the problem of increased parameter and computational burden, which limits
    the depth and performance of the network. Therefore, considering the memory and
    computational burden, better combination of multi-view information for more accurate
    multi-organ segmentation is still the future research direction.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Network Dedicated Modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network architecture is very important to improve the multi-organ segmentation
    accuracy, but its design process is complex. In multi-organ segmentation tasks,
    there are many special mechanisms to improve the accuracy of organ segmentation,
    such as the dilation convolution module, feature pyramid module, and attention
    module. They improve multi-organ segmentation accuracy by increasing the perceptual
    field, aggregating features of different scales, and focusing the network on the
    segmented region. Cheng et al. [[128](#bib.bib128)] studied the performance improvement
    of each module of the network compared with the basic U-Net network in the head
    and neck segmentation task.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Shape Prior Module
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shape prior is more suitable for medical images than natural images because
    the spatial relationships between internal structures in medical images are relatively
    fixed. Therefore, considering anatomical priors in a multi-organ segmentation
    task will significantly improve the performance of multi-organ segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: The current methods using anatomical priors fall into two main categories. One
    category is based on the idea of statistics, which calculates the average distribution
    of organs in a fully labelled dataset so that the prediction results can be as
    close as possible to the average distribution of organs [[42](#bib.bib42), [40](#bib.bib40),
    [66](#bib.bib66), [129](#bib.bib129), [130](#bib.bib130)]. The other is to train
    a shape representation model, which pretrains the shape representation model using
    the annotation of the training dataset, and then uses it as a regularization term
    to constrain the predictions of the segmentation network during training [[64](#bib.bib64),
    [113](#bib.bib113)]. It has also been shown that generative models can learn anatomical
    priors [[131](#bib.bib131)]. Therefore, it is a future research direction to consider
    using generative models (e.g., diffusion models, which are popular in the last
    two years [[132](#bib.bib132), [133](#bib.bib133)]) to better obtain anatomical
    prior knowledge to improve segmentation performance.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Dilated Convolutional Module
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In traditional CNNs, down sampling and pooling operation are usually used several
    times to reduce the computation and expand the field of perception, which will
    lose the spatial information and make image reconstruction difficult. Dilated
    convolution (also known as “Atrous”) introduces another parameter to the convolution
    layer, namely, the expansion rate, which can expand the field of perception to
    extract features across a larger spatial range without increasing the computational
    cost. Dilated convolution is a commonly used method in multi-organ segmentation
    tasks [[134](#bib.bib134), [40](#bib.bib40), [53](#bib.bib53), [135](#bib.bib135),
    [120](#bib.bib120)] that increases the size of the sampling space, allowing the
    neural network to extract features in a larger receptive field that captures multiscale
    contextual information. These contextual features can capture finer structural
    information, which is important for pinpointing organ location. Gibson et al.
    [[40](#bib.bib40)] used CNN networks with dilated convolution to accurately segment
    the liver, pancreas, stomach, and esophagus from abdominal CT. Men et al. [[115](#bib.bib115)]
    proposed a new method based on deep extended convolutional neural network (DDCNN)
    for fast and consistent automatic segmentation of clinical target volumes (CTVs)
    and OARs. Vesal et al. [[135](#bib.bib135)] introduced dilated convolution to
    2D U-Net for segmenting the esophagus, heart, aorta, and thoracic trachea.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Multiscale Module
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural networks extract the features of the target layer by layer. The lower
    layer networks have smaller perceptual fields and stronger representation of geometric
    detail information, and they have higher resolution but weaker representation
    of semantic information. The higher layer networks have larger perceptual fields
    and stronger representation of semantic information, but they have lower resolution
    of feature maps and weaker representation of geometric information, leading to
    the information loss of small targets. Common multiscale fusion modules include
    bottom-up, top-down, and laterally connected feature pyramids (FPNs) [[136](#bib.bib136)],
    spatial pooling pyramids (ASPPs) [[137](#bib.bib137)] combining dilated convolution
    and multiscale fusion, and others. In multi-organ segmentation tasks, multiscale
    feature fusion has been widely used in multi-organ segmentation due to the different
    sizes of the organs of interest. Jia and Wei [[53](#bib.bib53)] introduced the
    feature pyramid into the multi-organ segmentation network using two opposite feature
    pyramids, top-down and bottom-up forms, which can effectively handle multiscale
    changes and improve the segmentation accuracy of small targets. Shi et al. [[120](#bib.bib120)]
    used the pyramidal structure of lateral connections between encoders and decoders
    to capture contextual information at multiple scales. Srivastava et al. [[138](#bib.bib138)]
    proposed a new segmentation architecture named OARFocalFuseNet, which uses a focal
    modulation scheme to aggregate multiscale contexts in a specific resolution stream
    when performing multiscale fusion.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 Attention Module
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention module can highlight important features by dynamically weighting
    them. This novel attention mechanism allows exploring the inherent self-attentiveness
    of the network and is essential for multi-organ segmentation tasks [[65](#bib.bib65),
    [139](#bib.bib139)]. Common attention mechanisms include channel attention, spatial
    attention, and self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: Squeeze-and-excitation (SE) module [[140](#bib.bib140)] is a typical channel
    attention module which can focus on key parts of an image by generating a channel
    attention tensor. AnatomyNet [[48](#bib.bib48)] uses 3D SE residual blocks to
    segment the OARs of the head and neck, enabling the extraction of 3D features
    directly from CT images and adaptively calibrating the mapping of residual features
    within each feature channel. Liu et al. [[141](#bib.bib141)] proposed a new cross-layer
    spatial attentional map fusion network (CSAF-CNN) to segment multiple organs in
    the chest, which can effectively integrate the weights of different spatial attentional
    maps in the network, thus obtain more useful attentional maps. The average DSC
    of 22 organs in the head and neck was 72.50%, which was significantly better than
    U-Net (63.9%) and SE-UNet (67.9%). Gou et al. [[50](#bib.bib50)] designed a self-channel-spatial-attention
    neural network (SCSA-Net) for 3D head and neck OARs segmentation, which can adaptively
    enhance both channel and spatial features. Compared with SE-Res-Net and SE-Net,
    SCSA-Net improved the DSC of the optic nerve and submandibular gland by 0.06 and
    0.03 and 0.05 and 0.04, respectively. Lin et al. [[142](#bib.bib142)] suggested
    to embed the variance uncertainty into the attention architecture and proposed
    a variance-aware attention U-Net network to improve the attention to error-prone
    regions (e.g., boundary regions) in multi-organ segmentation. Compared with existing
    methods, the segmentation results of small organs and organs with irregular structures
    (e.g., duodenum, esophagus, gallbladder, and pancreas) are significantly improved.
    Zhang et al. [[51](#bib.bib51)] proposed a new hybrid network (Weaving attention
    U-Net, WAU-Net) with a U-Net++ [[143](#bib.bib143)] structure that uses CNNs to
    extract the underlying features, and uses axial attention blocks to efficiently
    model global relationships at different levels of the network, which achieve competitive
    performance in the head and neck multi-organ segmentation task.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C5 Other Modules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dense block [[144](#bib.bib144)] can efficiently use the information of
    the intermediate layer, and the residual block [[145](#bib.bib145)] can prevent
    gradient disappearance during backpropagation. These two modules are often embedded
    in the basic segmentation framework. The convolution kernel of the deformable
    convolution [[146](#bib.bib146)] can adapt itself to the actual situation and
    better extract the input features. Heinrich et al. [[147](#bib.bib147)] proposed
    a 3D abdominal multi-organ segmentation architecture with sparse deformable convolutions
    (OBELISK-Net) and showed that the combination with conventional CNNs can further
    improve the segmentation of small organs with large shape variations (e.g., pancreas,
    esophagus). The deformable convolutional block proposed by Shen et al. [[148](#bib.bib148)]
    can handle variations in the shape and size of different organs by generating
    reasonable receptive fields for different organs with additional trainable offsets.
    The strip pooling (strip pooling) [[149](#bib.bib149)] module can target long
    strip structures (e.g., esophagus and spinal cord) by using long pooling instead
    of traditional square pooling to avoid merging contaminated information from unrelated
    regions and better capture anisotropic and remote contextual information. For
    example, Zhang et al. [[150](#bib.bib150)] used a pool of anisotropic strips with
    three different directional receptive fields to capture the spatial relationships
    between multiple organs in the abdomen. Compared to network architectures, network
    modules have been widely utilized because of their relatively simple design process
    and the relative ease of embedding them into various architectures.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Network Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we all known, in addition to the network architecture or network modules,
    the segmentation accuracy also depends on the selected loss function. In multi-organ
    segmentation tasks, selecting a suitable loss function can reduce the class imbalance
    in deep learning and improve the segmentation accuracy of small organs.
  prefs: []
  type: TYPE_NORMAL
- en: Jadon [[151](#bib.bib151)] summarized the commonly used loss functions in semantic
    segmentation, which are classified into distribution-based loss functions, region-based
    loss functions, boundary-based loss functions, and compound-based loss functions.
    Common loss functions used for multi-organ segmentation include CE loss [[152](#bib.bib152)],
    Dice loss [[153](#bib.bib153)], Tversky loss [[154](#bib.bib154)], focal loss
    [[155](#bib.bib155)] and their combined loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D1 CE Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CE loss (cross-entropy loss function) [[152](#bib.bib152)] is an information
    theoretic measure that calculates the difference between the prediction of the
    network and the ground truth. Men et al. [[115](#bib.bib115)], Moeskops et al.
    [[43](#bib.bib43)], Zhang et al. [[51](#bib.bib51)] used CE loss for multi-organ
    segmentation. However, when the number of foreground pixels is much smaller than
    the background, CE loss will heavily bias the model towards the background, resulting
    in poor segmentation results. The weighted CE loss [[156](#bib.bib156)] adds weight
    parameters to each category based on CE loss. so that it can obtain better results
    in the case of unbalanced sample sizes compared to the original CE loss. Since
    there is a significant class imbalance problem in multi-organ segmentation, i.e.,
    a very large difference in the number of voxels in different organs, using weighted
    CE loss will achieve better results than using only the CE loss. Trullo et al.
    [[100](#bib.bib100)] used a weighted CE loss to segment the heart, esophagus,
    trachea, and aorta in thechest image; Roth et al. [[52](#bib.bib52)] applied a
    weighted CE loss to abdomen multi-organ segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D2 Dice Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Milletari et al. [[56](#bib.bib56)] proposed the Dice loss as a volume-based
    overlap measure, converting the voxel measure to the semantic label overlap measure,
    and becoming a commonly loss function in the segmentation task. Ibragimov and
    Xing [[41](#bib.bib41)] used the Dice loss to segment multiple organs of the head
    and neck. However, the use of the Dice loss alone does not eliminate the problem
    that the inherent nature of neural networks is beneficial to large volume organs.
    Inspired by the weighted CE loss, Sudre et al. [[153](#bib.bib153)] introduced
    the weighted Dice score (GDSC), which adaptively weighed its Dice values according
    to the current class size. Shen et al. [[157](#bib.bib157)] investigated three
    different types of GDSC based on class label frequencies (uniform, simple, and
    square) and evaluated their effects on segmentation accuracy. Gou et al. [[50](#bib.bib50)]
    used GDSC for head and neck multi-organ segmentation. Tappeiner et al. [[158](#bib.bib158)]
    introduced the class adaptive Dice loss to further compensate for high imbalances
    based on nnU-Net, and the results showed that the method could improve the performance
    of class imbalance segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D3 Other Losses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Tversky loss [[154](#bib.bib154)] is a generalization of the Dice loss and
    can be optimized by adjusting the parameters to control the balance between false
    positives and false negatives. The focal loss [[155](#bib.bib155)] was proposed
    in the field of object detection to enhance the attention on samples that are
    difficult to segment. Similar to the focal loss, the focal Tversky loss [[159](#bib.bib159)]
    focuses on segmenting difficult samples by reducing the weights of simple sample
    losses. Berzoini et al. [[54](#bib.bib54)] used the focal Tversky loss on smaller
    organs, thus balancing the indices between organs of different sizes, increasing
    the weights of small samples that are difficult to segment and finally solving
    the class imbalance problem caused by the kidney and bladder. Inspired by the
    exponential logarithmic loss (ELD-Loss) [[160](#bib.bib160)], Liu et al. [[141](#bib.bib141)]
    introduced the top-k exponential logarithmic loss (TELD-Loss) to solve the class
    imbalance problem in the head and neck. The results showed that using this loss
    function has a strong ability to handle mislabelling.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D4 Combined Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each type of loss function has its own advantages and disadvantages. Combining
    multiple functions can be used for multi-organ segmentation. A more common method
    is the weighted sum of the Dice loss and CE loss, which attempts to solve the
    class imbalance problem with the Dice loss while using the CE loss for curve smoothing.
    Isensee et al. [101] proposed combining the Dice loss and CE loss to measure the
    overlap of voxel-like predicted outcomes and ground truth. Isler et al. [[134](#bib.bib134)],
    Srivastava et al. [[138](#bib.bib138)], Xu et al. [[58](#bib.bib58)], Lin et al.
    [[142](#bib.bib142)], and Song et al. [[161](#bib.bib161)] used the weighted combination
    of the Dice loss and CE loss for multi-organ segmentation. When small objects
    are involved, using only the Dice loss leads to a lower accuracy; when the predicted
    region does not overlap with the labelled region, using the CE loss allows the
    prediction to be as close to the label as possible. Zhu et al. [[48](#bib.bib48)]
    specifically studied different loss functions for the unbalanced head and neck
    region, and pointed out that the combination of the Dice loss and focal loss was
    superior to the ordinary Dice loss. Both Cheng et al. [[128](#bib.bib128)] and
    Chen et al. [[116](#bib.bib116)] used this combined loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The conventional Dice loss is detrimental for smaller structures because a small
    amount of voxel misclassification leads to a large decrease in the Dice score.
    Applying the exponential logarithmic loss or combining the focal loss with the
    Dice loss can solve this problem. Using this kind of loss function does not require
    much adjustment to the network, however, it reduces the segmentation accuracy
    of the hard voxels in the region. On this basis, Lei et al. [[162](#bib.bib162)]
    proposed a new hardness-aware loss function that can focus more on hard voxels
    to achieve accurate segmentation. The ultimate goal of neural network optimization
    is the loss function, and designing a suitable loss function so that the network
    can improve the segmentation accuracy of various organs is still a research direction.
  prefs: []
  type: TYPE_NORMAL
- en: V Imperfect Annotation-based Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, most of the methods in the multi-organ segmentation field are based
    on fully annotated methods. However, medical image data is usually hard to acquire
    and annotate. In particular, for multi-organ segmentation tasks, obtaining fully
    annotated datasets is quite difficult, which inspired the idea of using imperfect
    annotation. In this paper, imperfect annotations are classified into two categories.
    The first category is weak annotation-based methods, where weak annotation indicates
    that the data annotation is incomplete or imprecise in each case. For example,
    in multi-organ segmentation, each image has only one kind of organ annotated;
    each image has no pixel-level annotation but only category annotation; or the
    annotation is scribbled or contains noise. Another category is semi supervised-based
    methods, where semi supervision indicates that only a small portion of the total
    data is annotated and most of the remaining is unannotated. In the following,
    we introduce the application of these two types of methods in multi-organ segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Weak Annotation-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In medical image segmentation, it is a difficult task to obtain the annotation
    of multiple organs simultaneously on the same set of images. For example, many
    existing single-organ datasets, such as LiTS [[163](#bib.bib163)], KiTS [[164](#bib.bib164)]
    (p19), and pancreas datasets [[165](#bib.bib165)], can only provide annotations
    for a single organ. However, multi-organ segmentation networks cannot be effectively
    trained solely based on these single-organ annotated datasets. Therefore, many
    studies have started to explore learning unified multi-organ segmentation networks
    from partially labelled datasets. Based on the implementation methods, we divide
    the current studies into model-based approaches and pseudo label-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Model-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea of the model-based approach is to realize a unified network for multiple
    partially labelled organs. Chen et al. [[166](#bib.bib166)] introduced a multi-branch
    decoder structure with a shared encoder and eight decoders to address the partial
    labelling problem. However, this structure is not flexible enough to be extended
    to new classes. Dmitriev and Kaufman [[167](#bib.bib167)] proposed conditional
    CNNs for learning multi-organ segmentation models, which integrate information
    of organ categories into the segmentation network. Zhang and Xie et al. [[168](#bib.bib168),
    [169](#bib.bib169)] proposed the idea of DoDNet. Similar to conditional CNN, they
    spliced the task encoding with the features extracted by the encoder, and introduced
    a dynamic parameter mechanism in the segmentation head. Zhang et al. [103] used
    the leading framework nn-UNet [[170](#bib.bib170)] as the backbone model, adding
    task encoding as supporting information to the decoder of nn-UNet, and combined
    the deep supervision mechanism to further refine the output of organs of different
    sizes. Wu et al. [[171](#bib.bib171)] proposed TGNet composed of task-guided attention
    module and task-guided residual block, which can highlight task-relevant features
    while suppressing task-irrelevant information during feature extraction. Liu et
    al. [[172](#bib.bib172)] first introduced incremental learning (IL) to aggregate
    partially labelled datasets in stages, and verified that the distribution of different
    partially labelled datasets misleads the process of IL. Xu and Yan [[173](#bib.bib173)]
    proposed a new federated multi-encoding U-Net (Fed-MENU) method that can effectively
    use independent datasets with different partial labels to train a unified model
    for multi-organ segmentation. The model outperformed any model trained on a single
    dataset as well as the model trained on all datasets combined. Fang and Yan [[174](#bib.bib174)]
    and Shi et al. [[175](#bib.bib175)] trained uniform models on partially labelled
    datasets by designing new network and proposing specific loss function.
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Pseudo Label-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pseudo label-based methods generate pseudo labels of unlabelled organs by
    using partial-organ segmentation models trained in partially labelled datasets,
    which can be converted to fully supervised methods. Zhou et al. [[129](#bib.bib129)]
    proposed an a Prior-aware Neural Network (PaNN), which utilized prior statistics
    obtained from a fully labelled dataset to guide the training process based on
    partially labelled datasets. Huang et al. [[176](#bib.bib176)] proposed a weight-averaging
    joint training framework, which can correct the noise in the pseudo labels, so
    as to learn a more robust model. Zhang et al. [[177](#bib.bib177)] proposed a
    multi-teacher knowledge distillation framework that utilizes pseudo labels predicted
    by teacher models trained on partially labelled datasets to train student models
    for multi-organ segmentation. Lian et al. [[130](#bib.bib130)] proposed a multi-organ
    segmentation model (PRIMP) based on single and multiple organs anatomical priors.
    The model first generates pseudo labels for each partially labelled dataset so
    as to obtain a set of multi-organ datasets with pseudo label. Then the multi-organ
    segmentation model is trained on this dataset, and tested on another new dataset.
    For the first time, this method considers the domain discrepancy between partially
    labelled datasets and the tested multi-organ datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to partial annotation, weak annotation also includes image-level
    annotation, sparse annotation, and noisy annotation [[178](#bib.bib178)]. Regarding
    multi-organ segmentation tasks, Kanavati et al. [[179](#bib.bib179)] proposed
    a weakly supervised organ segmentation method based on classification forests
    for the liver, spleen, and kidney, in which the labels are scribbled on the organs.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Semi Supervised-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semi supervised multi-organ segmentation methods make full use of unlabelled
    data to improve the segmentation performance, thus reducing the need for extensive
    annotation. In recent years, semi supervised learning has been widely used in
    medical image segmentation, such as heart segmentation [[180](#bib.bib180), [181](#bib.bib181),
    [182](#bib.bib182)], pancreas segmentation [[183](#bib.bib183)], and tumour target
    region segmentation [[184](#bib.bib184)]. A detailed review of semi supervised
    learning in medical images was presented by Jiao et al. [[185](#bib.bib185)],
    who classified semi supervised medical image segmentation methods into three paradigms:
    pseudo label-based methods, consistency regularization-based methods, and knowledge
    prior-based methods. In this review, we focus on semi supervised multi-organ segmentation
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Ma et al. [[36](#bib.bib36)] established a new benchmark for semi supervised
    abdominal multi-organ segmentation, which developed a method based on pseudo labelling.
    The teacher model was first trained on the labelled data, and generated the pseudo
    labels for the unlabelled data. Then, the student model was trained on both the
    real labelled and pseudo labelled data. Finally, the teacher model was substituted
    with the student model to complete the training. The results on the liver, kidney,
    spleen, and pancreas show that using unlabelled data can improve the performance
    of multi-organ segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-view methods are also widely used in semi supervised multi-organ segmentation,
    where the model is made to learn in a collaborative training manner to extract
    useful information from multiple planes (e.g., sagittal, coronal, and axial planes),
    and then use multi-plane fusion to generate more reliable pseudo labels, and thus
    train better segmentation networks. Zhou et al. [[186](#bib.bib186)] designed
    a system framework, DMPCT, for multi-organ segmentation of abdominal CT scans
    by fusing multi-planar information on unlabelled data during training. The framework
    uses a multi-planar fusion module to synthesize inferences and iteratively update
    pseudo labels for multiple configurations of unlabelled data. Xia et al. [[187](#bib.bib187)]
    proposed an uncertainty-aware multi-view collaborative training (UMCT) method
    based on uncertainty perception, which first obtains multiple views by spatial
    transformations such as rotation and alignment, then trains a 3D deep segmentation
    network on each view, and performs joint training by implementing multi-view consistency
    on unlabelled data.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the collaborative training approach, multi-organ segmentation
    is also suitable for consistency-based learning due to the large number of prospect
    categories and dense distribution of organs. Consistency learning encourages consistent
    output through networks with different parameters. Lai et al. [[188](#bib.bib188)]
    developed a semi supervised learning-based DLUNet for abdominal multi-organ segmentation,
    which consists of two lightweight U-Nets in the training phase. Moreover, regarding
    unlabelled data, the outputs obtained from two networks are used to supervise
    each other, which can improve the accuracy of these unlabelled data. It eventually
    achieves an average DSC of 0.8718 for 13 organs in the abdomen.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are other semi supervised multi-organ segmentation-based
    methods. Lee et al. [[189](#bib.bib189)] proposed a discriminator module based
    on human-in-the-loop quality assurance (QA) to supervise the learning of unlabelled
    data. They used QA scores as a loss function for unlabelled data. Raju Cheng et
    al. [[190](#bib.bib190)] proposed a powerful semi supervised organ segmentation
    method, CHASe, for liver and lesion segmentation. It integrates co-training and
    heteromodality learning into a co-heterogeneous training framework. The framework
    is trained on a small single-phase dataset and can be adapted to label-free multicentre
    and multiphase clinical data.
  prefs: []
  type: TYPE_NORMAL
- en: VI Discussion and Future Trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, a systematic review of deep learning methods for multi-organ
    segmentation is presented from the perspectives of both full annotation and imperfect
    annotation. The main innovations of the full annotation method focus on the design
    of network architectures, the combination of network dimensions, the innovation
    of network modules and the proposal of new loss functions. In terms of the network
    architecture design, with the development of the transformer [[75](#bib.bib75)]
    architectures, better utilization of these advanced architectures for multi-organ
    segmentation is a promising direction, as well as the automatic search for the
    optimal architecture for each organ through neural network architecture search
    (NAS) [[191](#bib.bib191)]. In the network dimension, optimally combining 2D and
    3D architectures is a worthwhile research direction. In terms of network module,
    more dedicated modules need to design to improve the segmentation accuracy according
    to the multi-organ segmentation task. In terms of the loss functions, targeting
    the class imbalance, geometric prior or introducing adversarial learning loss
    will have great potential for designing more comprehensive and diverse loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Full annotation methods rely on fully annotated and high-quality datasets. Many
    imperfect annotation-based methods have been proposed for medical image segmentation
    in the last two years, including the aforementioned multi-organ segmentation based
    on weak annotation-based methods and semi annotation-based methods. However, compared
    to full annotation-based methods, the imperfect annotation-based methods have
    been less studied. It is a future research focus if imperfect annotation-based
    methods can be used more adequately to achieve the performance close to that of
    the full annotation-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning has already played a significant role in multi-organ segmentation
    task, but many challenges remain to be explored in the future, which are summarized
    in follows:'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Higher Segmentation Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The current multi-organ segmentation method is more effective in solving the
    segmentation of large organs and organs with standard contours, such as the brainstem
    and mandible in the head and neck; the left and right lungs and heart in the chest;
    and the liver, spleen, and stomach in the abdomen. Moreover, the DSC of various
    methods can basically reach 0.8 or higher, while for small organs, such as the
    optical chiasm in the head and neck (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")(8)), the left and right optic nerves (see Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation")(6 and 7)), the DSC can
    only reach about 0.7; irregular organs such as the pancreas in the abdomen (Fig.
    [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")(4)),
    and long striped organs such as the spinal cord (Fig. [2](#S1.F2 "Figure 2 ‣ I
    Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation")(6)), the segmentation results
    are also not very satisfactory. The future research direction is to enhance the
    segmentation accuracy of these types of organs using more advanced automatic segmentation
    frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B More Comprehensive Public Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, public datasets covering multiple organs are not sufficient. And
    the vast majority of methods are validated on their private datasets, making it
    difficult to verify the generalizability of the models. Therefore, there is a
    need to establish multicentre public datasets of multi-organ segmentation with
    large data volumes, wide coverage, and strong clinical relevance in the future.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Better Use of Imperfect Annotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The vast majority of current methods are based on full annotation methods. Since
    medical image data are usually not easy to collect and annotating all the organs
    on the same image is a time-consuming and laborious work. Further studies can
    be performed to better utilize imperfect annotations [[192](#bib.bib192), [193](#bib.bib193)],
    including the use of weakly annotated datasets and semi annotated datasets.
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Study of Transfer Learning Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing deep learning models usually trained on one part of the body, which
    usually tend to obtain poor results when migrated to other datasets or applied
    to other parts of the body. Therefore, transfer learning models need to be explored
    in the future. For example, Fu et al. [[194](#bib.bib194)] proposed a new method
    called domain adaptive relational reasoning (DARR). It is used to generalize 3D
    multi-organ segmentation models to medical data from different domains. In addition,
    a very significant problem with medical images compared to other natural images
    is that many private datasets are not publicly available, and many hospitals only
    release trained models. Therefore, source free domain adaptation problem will
    be a very important research direction in the future. For example, Hong et al.
    [[195](#bib.bib195)] proposed a source free unsupervised domain adaptive cross-modal
    approach for abdomen multi-organ segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we systematically review 214 deep learning-based multi-organ
    segmentation studies in two broad categories, namely full annotation-based methods
    and imperfect annotation-based methods for multiple parts, including the head
    and neck, thorax and abdomen. In the fully labelled methods, we summarize the
    existing methods according to network architectures, network modules, network
    dimensions, and loss functions. In the imperfect annotation-based methods, we
    summarize both weak annotation-based methods and semi annotated-based methods.
    On this basis, we also put forward tailored solutions for some current difficulties
    and shortcomings in this field, and illustrate the future trends. The comprehensive
    survey shows that multi-organ segmentation algorithm based on deep learning is
    rapidly developing towards a new era of more accurate, more detailed and more
    automated analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Natural Science Foundation of China
    under grant 82072021\. This work was also supported by the medical-industrial
    integration project of Fudan University under grant XM03211181.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] B. Van Ginneken, C. M. Schaefer-Prokop, and M. Prokop, “Computer-aided
    diagnosis: how to move from the laboratory to the clinic,” *Radiology*, vol. 261,
    no. 3, pp. 719–732, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Sykes, “Reflections on the current status of commercial automated segmentation
    systems in clinical practice,” pp. 131–134, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. G. Pfister, S. Spencer, D. Adelstein, D. Adkins, Y. Anzai, D. M. Brizel,
    J. Y. Bruce, P. M. Busse, J. J. Caudell, A. J. Cmelak *et al.*, “Head and neck
    cancers, version 2.2020, nccn clinical practice guidelines in oncology,” *Journal
    of the National Comprehensive Cancer Network*, vol. 18, no. 7, pp. 873–898, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. K. Molitoris, T. Diwanji, J. W. Snider III, S. Mossahebi, S. Samanta,
    S. N. Badiyan, C. B. Simone, P. Mohindra *et al.*, “Advances in the use of motion
    management and image guidance in radiation therapy treatment for lung cancer,”
    *Journal of thoracic disease*, vol. 10, no. Suppl 21, pp. S2437–S2450, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. A. Vyfhuis, N. Onyeuku, T. Diwanji, S. Mossahebi, N. P. Amin, S. N.
    Badiyan, P. Mohindra, and C. B. Simone, “Advances in proton therapy in lung cancer,”
    *Therapeutic advances in respiratory disease*, vol. 12, p. 1753466618783878, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C. W. Hurkmans, J. H. Borger, B. R. Pieters, N. S. Russell, E. P. Jansen,
    and B. J. Mijnheer, “Variability in target volume delineation on ct scans of the
    breast,” *International Journal of Radiation Oncology Biology Physics*, vol. 50,
    no. 5, pp. 1366–1372, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Rasch, R. Steenbakkers, and M. van Herk, “Target definition in prostate,
    head, and neck,” in *Seminars in radiation oncology*, vol. 15, no. 3.   Elsevier,
    2005, pp. 136–145.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Van de Steene, N. Linthout, J. De Mey, V. Vinh-Hung, C. Claassens, M. Noppen,
    A. Bel, and G. Storme, “Definition of gross tumor volume in lung cancer: inter-observer
    variability,” *Radiotherapy and oncology*, vol. 62, no. 1, pp. 37–49, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] J. Breunig, S. Hernandez, J. Lin, S. Alsager, C. Dumstorf, J. Price, J. Steber,
    R. Garza, S. Nagda, E. Melian *et al.*, “A system for continual quality improvement
    of normal tissue delineation for radiation therapy treatment planning,” *International
    Journal of Radiation Oncology Biology Physics*, vol. 83, no. 5, pp. e703–e708,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Chen and L. Pan, “A survey of graph cuts/graph search based medical
    image segmentation,” *IEEE reviews in biomedical engineering*, vol. 11, pp. 112–124,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] I. El Naqa, D. Yang, A. Apte, D. Khullar, S. Mutic, J. Zheng, J. D. Bradley,
    P. Grigsby, and J. O. Deasy, “Concurrent multimodality image segmentation by active
    contours for radiotherapy treatment planning a,” *Medical physics*, vol. 34, no. 12,
    pp. 4738–4749, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Pratondo, C.-K. Chui, and S.-H. Ong, “Robust edge-stop functions for
    edge-based active contour models in medical image segmentation,” *IEEE Signal
    Processing Letters*, vol. 23, no. 2, pp. 222–226, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, W. E. Grimson,
    and A. Willsky, “A shape-based approach to the segmentation of medical imagery
    using level sets,” *IEEE transactions on medical imaging*, vol. 22, no. 2, pp.
    137–154, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. M. Saranathan and M. Parente, “Threshold based segmentation method
    for hyperspectral images,” in *2013 5Th workshop on hyperspectral image and signal
    processing: evolution in remote sensing (WHISPERS)*.   Gainesville: IEEE, 2013,
    pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Shi and J. Malik, “Normalized cuts and image segmentation,” *IEEE Transactions
    on pattern analysis and machine intelligence*, vol. 22, no. 8, pp. 888–905, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A. J. Vyavahare and R. Thool, “Segmentation using region growing algorithm
    based on clahe for medical images,” in *Fourth International Conference on Advances
    in Recent Technologies in Communication and Computing (ARTCom2012)*.   Bangalore,
    India: IET, 2012, pp. 182–185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] I. Isgum, M. Staring, A. Rutten, M. Prokop, M. A. Viergever, and B. Van Ginneken,
    “Multi-atlas-based segmentation with local decision fusion—application to cardiac
    and aortic segmentation in ct scans,” *IEEE transactions on medical imaging*,
    vol. 28, no. 7, pp. 1000–1010, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] P. Aljabar, R. A. Heckemann, A. Hammers, J. V. Hajnal, and D. Rueckert,
    “Multi-atlas based segmentation of brain images: atlas selection and its effect
    on accuracy,” *Neuroimage*, vol. 46, no. 3, pp. 726–738, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] O. Ecabert, J. Peters, H. Schramm, C. Lorenz, J. von Berg, M. J. Walker,
    M. Vembar, M. E. Olszewski, K. Subramanyan, G. Lavi *et al.*, “Automatic model-based
    segmentation of the heart in ct images,” *IEEE transactions on medical imaging*,
    vol. 27, no. 9, pp. 1189–1201, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. A. Qazi, V. Pekar, J. Kim, J. Xie, S. L. Breen, and D. A. Jaffray,
    “Auto-segmentation of normal and target structures in head and neck ct images:
    a feature-driven model-based approach,” *Medical physics*, vol. 38, no. 11, pp.
    6160–6170, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] E. A. Smirnov, D. M. Timoshenko, and S. N. Andrianov, “Comparison of regularization
    methods for imagenet classification with deep convolutional neural networks,”
    *Aasri Procedia*, vol. 6, pp. 89–94, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Mobiny and H. Van Nguyen, “Fast capsnet for lung cancer screening,”
    in *Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part II 11*.   Cham: Springer, 2018, pp. 741–749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Z. Alom, C. Yakopcic, M. Hasan, T. M. Taha, and V. K. Asari, “Recurrent
    residual u-net for medical image segmentation,” *Journal of Medical Imaging*,
    vol. 6, no. 1, p. 014006, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. Wang, T. Lei, R. Cui, B. Zhang, H. Meng, and A. K. Nandi, “Medical
    image segmentation using deep learning: A survey,” *IET Image Processing*, vol. 16,
    no. 5, pp. 1243–1267, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Huang, F. Yang, M. Yin, X. Mo, and C. Zhong, “A review of multimodal
    medical image fusion techniques,” *Computational and mathematical methods in medicine*,
    vol. 2020, p. 8279342, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang, “Deep learning
    in medical image registration: a review,” *Physics in Medicine & Biology*, vol. 65,
    no. 20, p. 20TR01, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Lei, Y. Fu, T. Wang, R. L. Qiu, W. J. Curran, T. Liu, and X. Yang,
    “Deep learning in multi-organ segmentation,” *arXiv preprint arXiv:2001.10619*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang, “A review of
    deep learning based methods for medical image multi-organ segmentation,” *Physica
    Medica*, vol. 85, pp. 107–122, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] B. Landman, Z. Xu, J. E. Igelsias, M. Styner, T. Langerak, and A. Klein,
    “Segmentation outside the cranial vault challenge,” *Synapse*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. F. Raudaschl, P. Zaffino, G. C. Sharp, M. F. Spadea, A. Chen, B. M.
    Dawant, T. Albrecht, T. Gass, C. Langguth, M. Lüthi *et al.*, “Evaluation of segmentation
    methods on head and neck ct: auto-segmentation challenge 2015,” *Medical physics*,
    vol. 44, no. 5, pp. 2020–2036, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Yang, H. Veeraraghavan, S. G. Armato III, K. Farahani, J. S. Kirby,
    J. Kalpathy-Kramer, W. van Elmpt, A. Dekker, X. Han, X. Feng *et al.*, “Autosegmentation
    for thoracic radiation treatment planning: a grand challenge at aapm 2017,” *Medical
    physics*, vol. 45, no. 10, pp. 4568–4581, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. E. Kavur, N. S. Gezer, M. Barış, S. Aslan, P.-H. Conze, V. Groza, D. D.
    Pham, S. Chatterjee, P. Ernst, S. Özkan *et al.*, “Chaos challenge-combined (ct-mr)
    healthy abdominal organ segmentation,” *Medical Image Analysis*, vol. 69, p. 101950,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] X. Feng, K. Qing, N. J. Tustison, C. H. Meyer, and Q. Chen, “Deep convolutional
    neural network for segmentation of thoracic organs-at-risk using cropped 3d images,”
    *Medical physics*, vol. 46, no. 5, pp. 2169–2180, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] X. Xu, F. Zhou, B. Liu, and X. Bai, “Annotations for body organ localization
    based on miccai lits dataset,” *IEEE Dataport*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Babier, B. Zhang, R. Mahmood, K. L. Moore, T. G. Purdie, A. L. McNiven,
    and T. C. Chan, “Openkbp: the open-access knowledge-based planning grand challenge
    and dataset,” *Medical Physics*, vol. 48, no. 9, pp. 5549–5561, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Ma, Y. Zhang, S. Gu, C. Zhu, C. Ge, Y. Zhang, X. An, C. Wang, Q. Wang,
    X. Liu *et al.*, “Abdomenct-1k: Is abdominal organ segmentation a solved problem?”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 44, no. 10,
    pp. 6695–6714, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural computation*, vol. 1, no. 4, pp. 541–551, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] R. Karthik, R. Menaka, A. Johnson, and S. Anand, “Neuroimaging and deep
    learning for brain stroke detection-a review of recent advancements and future
    prospects,” *Computer Methods and Programs in Biomedicine*, vol. 197, p. 105728,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Zhao, K. Chen, G. Wu, G. Zhang, X. Zhou, C. Lv, S. Wu, Y. Chen, G. Xie,
    and Z. Yao, “Deep learning shows good reliability for automatic segmentation and
    volume measurement of brain hemorrhage, intraventricular extension, and peripheral
    edema,” *European radiology*, vol. 31, no. 7, pp. 5012–5020, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. R.
    Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Towards image-guided
    pancreas and biliary endoscopy: automatic multi-organ segmentation on abdominal
    ct with dense dilated networks,” in *Medical Image Computing and Computer Assisted
    Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
    September 11-13, 2017, Proceedings, Part I 20*.   Cham, Switzerland: Springer,
    2017, pp. 728–736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] B. Ibragimov and L. Xing, “Segmentation of organs-at-risks in head and
    neck ct images using convolutional neural networks,” *Medical physics*, vol. 44,
    no. 2, pp. 547–557, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Fritscher, P. Raudaschl, P. Zaffino, M. F. Spadea, G. C. Sharp, and
    R. Schubert, “Deep neural networks for fast segmentation of 3d medical images,”
    in *Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th
    International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part
    II 19*.   Cham, Switzerland: Springer, 2016, pp. 158–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] P. Moeskops, J. M. Wolterink, B. H. Van Der Velden, K. G. Gilhuijs, T. Leiner,
    M. A. Viergever, and I. Išgum, “Deep learning for multi-task medical image segmentation
    in multiple modalities,” in *Medical Image Computing and Computer-Assisted Intervention–MICCAI
    2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings,
    Part II 19*.   Cham, Switzerland: Springer, 2016, pp. 478–486.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Long, Jonathan, Shelhamer, Evan, Darrell, and Trevor, “Fully convolutional
    networks for semantic segmentation,” *IEEE Transactions on Pattern Analysis &
    Machine Intelligence*, vol. 39, no. 4, pp. 640–651, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Y. Wang, Y. Zhou, P. Tang, W. Shen, E. K. Fishman, and A. L. Yuille, “Training
    multi-organ segmentation networks with sample selection by relaxed upper confident
    bound,” in *Medical Image Computing and Computer Assisted Intervention–MICCAI
    2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part IV 11*.   Cham, Switzerland: Springer, 2018, pp. 434–442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár, “Learning to refine
    object segments,” in *Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11–14, 2016, Proceedings, Part I 14*.   Cham, Switzerland:
    Springer, 2016, pp. 75–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18*.   Cham, Switzerland: Springer, 2015, pp.
    234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] W. Zhu, Y. Huang, L. Zeng, X. Chen, Y. Liu, Z. Qian, N. Du, W. Fan, and
    X. Xie, “Anatomynet: deep learning for fast and fully automated whole-volume segmentation
    of head and neck anatomy,” *Medical physics*, vol. 46, no. 2, pp. 576–589, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] W. van Rooij, M. Dahele, H. R. Brandao, A. R. Delaney, B. J. Slotman,
    and W. F. Verbakel, “Deep learning-based delineation of head and neck organs at
    risk: geometric and dosimetric evaluation,” *International Journal of Radiation
    Oncology Biology Physics*, vol. 104, no. 3, pp. 677–684, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Gou, N. Tong, S. Qi, S. Yang, R. Chin, and K. Sheng, “Self-channel-and-spatial-attention
    neural network for automated multi-organ segmentation on head and neck ct images,”
    *Physics in Medicine & Biology*, vol. 65, no. 24, p. 245034, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Zhang, T. Zhao, H. Gay, W. Zhang, and B. Sun, “Weaving attention u-net:
    A novel hybrid cnn and attention-based method for organs-at-risk segmentation
    in head and neck ct images,” *Medical physics*, vol. 48, no. 11, pp. 7052–7062,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. R. Roth, C. Shen, H. Oda, M. Oda, Y. Hayashi, K. Misawa, and K. Mori,
    “Deep learning and its application to medical image segmentation,” *Medical Imaging
    Technology*, vol. 36, no. 2, pp. 63–71, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. Jia and J. Wei, “Amo-net: abdominal multi-organ segmentation in mri
    with a extend unet,” in *2021 IEEE 4th Advanced Information Management, Communicates,
    Electronic and Automation Control Conference (IMCEC)*, vol. 4.   Chongqing, China:
    IEEE, 2021, pp. 1770–1775.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] R. Berzoini, A. A. Colombo, S. Bardini, A. Conelli, E. D’Arnese, and M. D.
    Santambrogio, “An optimized u-net for unbalanced multi-organ segmentation,” in
    *2022 44th Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC)*.   Glasgow, Scotland: IEEE, 2022, pp. 3764–3767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Lambert, C. Petitjean, B. Dubray, and S. Kuan, “Segthor: Segmentation
    of thoracic organs at risk in ct images,” in *2020 Tenth International Conference
    on Image Processing Theory, Tools and Applications (IPTA)*.   Paris, France: IEEE,
    2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
    neural networks for volumetric medical image segmentation,” in *2016 fourth international
    conference on 3D vision (3DV)*.   Stanford, CA: IEEE, 2016, pp. 565–571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. Davidson,
    S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Automatic multi-organ segmentation
    on abdominal ct with dense v-networks,” *IEEE transactions on medical imaging*,
    vol. 37, no. 8, pp. 1822–1834, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Xu, H. Guo, J. Zhang, K. Yan, and L. Lu, “A new probabilistic v-net
    model with hierarchical spatial feature transform for efficient abdominal multi-organ
    segmentation,” *arXiv preprint arXiv:2208.01382*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] G. Podobnik, B. Ibragimov, P. Strojan, P. Peterlin, and T. Vrtovec, “Segmentation
    of organs-at-risk from ct and mr images of the head and neck: Baseline results,”
    in *2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   Kolkata,
    India: IEEE, 2022, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] F. Isensee, P. F. Jäger, P. M. Full, P. Vollmuth, and K. H. Maier-Hein,
    “nnu-net for brain tumor segmentation,” in *Brainlesion: Glioma, Multiple Sclerosis,
    Stroke and Traumatic Brain Injuries: 6th International Workshop, BrainLes 2020,
    Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected
    Papers, Part II 6*.   Cham, Switzerland: Springer, 2021, pp. 118–132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Zhang, Z. Yang, B. Huo, S. Chai, and S. Jiang, “Multiorgan segmentation
    from partially labeled datasets with conditional nnu-net,” *Computers in Biology
    and Medicine*, vol. 136, p. 104658, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] N. Altini, A. Brunetti, V. P. Napoletano, F. Girardi, E. Allegretti, S. M.
    Hussain, G. Brunetti, V. Triggiani, V. Bevilacqua, and D. Buongiorno, “A fusion
    biopsy framework for prostate cancer based on deformable superellipses and nnu-net,”
    *Bioengineering*, vol. 9, no. 8, p. 343, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *arXiv preprint
    arXiv:1406.2661*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] N. Tong, S. Gou, S. Yang, M. Cao, and K. Sheng, “Shape constrained fully
    convolutional densenet with adversarial training for multiorgan segmentation on
    head and neck ct and low-field mr images,” *Medical physics*, vol. 46, no. 6,
    pp. 2669–2682, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] J. Cai, Y. Xia, D. Yang, D. Xu, L. Yang, and H. Roth, “End-to-end adversarial
    shape learning for abdomen organ deep segmentation,” in *Machine Learning in Medical
    Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI
    2019, Shenzhen, China, October 13, 2019, Proceedings 10*.   Cham, Switzerland:
    Springer, 2019, pp. 124–132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Dong, Y. Lei, T. Wang, M. Thomas, L. Tang, W. J. Curran, T. Liu, and
    X. Yang, “Automatic multiorgan segmentation in thorax ct images using u-net-gan,”
    *Medical physics*, vol. 46, no. 5, pp. 2157–2168, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Trullo, C. Petitjean, B. Dubray, and S. Ruan, “Multiorgan segmentation
    using distance-aware adversarial networks,” *Journal of Medical Imaging*, vol. 6,
    no. 1, p. 014001, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] F. Mahmood, D. Borders, R. J. Chen, G. N. McKay, K. J. Salimian, A. Baras,
    and N. J. Durr, “Deep adversarial training for multi-organ nuclei segmentation
    in histopathology images,” *IEEE transactions on medical imaging*, vol. 39, no. 11,
    pp. 3257–3267, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N. Metaxas,
    H. Li, and M. Chen, “Focusnetv2: Imbalanced large and small organ segmentation
    with adversarial shape constraint for head and neck ct images,” *Medical Image
    Analysis*, vol. 67, p. 101831, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Fang, Y. Fang, and X. Yang, “Multi-organ segmentation network with
    adversarial performance validator,” *arXiv preprint arXiv:2204.07850*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. Vaswani, N. Shazeer, and N. Parmar, “Attention is all uou need,” *arXiv:170603762*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] I. Bello, “Lambdanetworks: Modeling long-range interactions without attention,”
    *arXiv preprint arXiv:2102.08602*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Gao, M. Zhou, and D. N. Metaxas, “Utnet: a hybrid transformer architecture
    for medical image segmentation,” in *Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part III 24*.   Cham, Switzerland: Springer,
    2021, pp. 61–71.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. Yao, M. Hu, G. Zhai, and X. Zhang, “Transclaw u-net: Claw u-net with
    transformers for medical image segmentation,” *arXiv preprint arXiv:2107.05188*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical
    transformer: Gated axial-attention for medical image segmentation,” in *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    I 24*.   Springer, 2021, pp. 36–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Pan, Y. Lei, T. Wang, J. Wynne, C.-W. Chang, J. Roper, A. B. Jani,
    P. Patel, J. D. Bradley, T. Liu *et al.*, “Male pelvic multi-organ segmentation
    using token-based transformer vnet,” *Physics in Medicine & Biology*, vol. 67,
    no. 20, p. 205012, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, “Swin-unet:
    Unet-like pure transformer for medical image segmentation,” *arXiv preprint arXiv:2105.05537*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Huang, Z. Deng, D. Li, and X. Yuan, “Missformer: An effective medical
    image segmentation transformer,” *arXiv preprint arXiv:2109.07162*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Xie, J. Zhang, C. Shen, and Y. Xia, “Cotr: Efficiently bridging cnn
    and transformer for 3d medical image segmentation,” in *Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III 24*.   Cham,
    Switzerland: Springer, 2021, pp. 171–180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “Uctransnet: rethinking the
    skip connections in u-net from a channel-wise perspective with transformer,” in
    *Proceedings of the AAAI conference on artificial intelligence*, vol. 36, no. 3,
    2022, pp. 2441–2449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H. Wang, S. Xie, L. Lin, Y. Iwamoto, X.-H. Han, Y.-W. Chen, and R. Tong,
    “Mixed transformer u-net for medical image segmentation,” in *ICASSP 2022-2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   Singapore:
    IEEE, 2022, pp. 2390–2394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] G. Xu, X. Wu, X. Zhang, and X. He, “Levit-unet: Make faster encoders with
    transformer for medical image segmentation,” *arXiv preprint arXiv:2107.08623*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns
    for medical image segmentation,” in *Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part I 24*.   Cham, Switzerland: Springer, 2021,
    pp. 14–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C. Suo, X. Li, D. Tan, Y. Zhang, and X. Gao, “I2-net: Intra-and inter-scale
    collaborative learning network for abdominal multi-organ segmentation,” in *Proceedings
    of the 2022 International Conference on Multimedia Retrieval*, New York, NY, 2022,
    pp. 654–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] H. Kan, J. Shi, M. Zhao, Z. Wang, W. Han, H. An, Z. Wang, and S. Wang,
    “Itunet: Integration of transformers and unet for organs-at-risk segmentation,”
    in *2022 44th Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC)*.   IEEE, 2022, pp. 2123–2127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
    and Y. Zhou, “Transunet: Transformers make strong encoders for medical image segmentation,”
    *arXiv preprint arXiv:2102.04306*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R.
    Roth, and D. Xu, “Unetr: Transformers for 3d medical image segmentation,” in *Proceedings
    of the IEEE/CVF winter conference on applications of computer vision*, Waikoloa,
    HI, 2022, pp. 574–584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] P.-H. Chen, C.-H. Huang, S.-K. Hung, L.-C. Chen, H.-L. Hsieh, W.-Y. Chiou,
    M.-S. Lee, H.-Y. Lin, and W.-M. Liu, “Attention-lstm fused u-net architecture
    for organ segmentation in ct images,” in *2020 International Symposium on Computer,
    Consumer and Control (IS3C)*.   Taichung City, Taiwan: IEEE, 2020, pp. 304–307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Chakravarty and J. Sivaswamy, “Race-net: a recurrent neural network
    for biomedical image segmentation,” *IEEE journal of biomedical and health informatics*,
    vol. 23, no. 3, pp. 1151–1162, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] E. Tappeiner, S. Pröll, M. Hönig, P. F. Raudaschl, P. Zaffino, M. F. Spadea,
    G. C. Sharp, R. Schubert, and K. Fritscher, “Multi-organ segmentation of the head
    and neck area: an efficient hierarchical neural networks approach,” *International
    journal of computer assisted radiology and surgery*, vol. 14, no. 5, pp. 745–754,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Pu, S.-I. Kamata, and Y. Wang, “A coarse to fine framework for multi-organ
    segmentation in head and neck images,” in *2020 Joint 9th International Conference
    on Informatics, Electronics & Vision (ICIEV) and 2020 4th International Conference
    on Imaging, Vision & Pattern Recognition (icIVPR)*.   Kitakyushu, Japan: IEEE,
    2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Q. Ma, C. Zu, X. Wu, J. Zhou, and Y. Wang, “Coarse-to-fine segmentation
    of organs at risk in nasopharyngeal carcinoma radiotherapy,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24*.   Cham,
    Switzerland: Springer, 2021, pp. 358–368.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] P. Hu, F. Wu, J. Peng, Y. Bao, F. Chen, and D. Kong, “Automatic abdominal
    multi-organ segmentation using deep convolutional neural network and time-implicit
    level sets,” *International journal of computer assisted radiology and surgery*,
    vol. 12, no. 3, pp. 399–411, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] L. Zhang, J. Zhang, P. Shen, G. Zhu, P. Li, X. Lu, H. Zhang, S. A. Shah,
    and M. Bennamoun, “Block level skip connections across cascaded v-net for multi-organ
    segmentation,” *IEEE Transactions on Medical Imaging*, vol. 39, no. 9, pp. 2782–2793,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] L. Xie, Q. Yu, Y. Zhou, Y. Wang, E. K. Fishman, and A. L. Yuille, “Recurrent
    saliency transformation network for tiny target segmentation in abdominal ct scans,”
    *IEEE transactions on medical imaging*, vol. 39, no. 2, pp. 514–525, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. H. Lee, Y. Tang, S. Bao, R. G. Abramson, Y. Huo, and B. A. Landman,
    “Rap-net: Coarse-to-fine multi-organ segmentation with single random anatomical
    prior,” in *2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)*.   Nice,
    France: IEEE, 2021, pp. 1491–1494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
    P. Bilic, M. Rempfler, M. Armbruster, F. Hofmann, M. D’Anastasi *et al.*, “Automatic
    liver and lesion segmentation in ct using cascaded fully convolutional neural
    networks and 3d conditional random fields,” in *International conference on medical
    image computing and computer-assisted intervention*.   Cham, Switzerland: Springer,
    2016, pp. 415–423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] D. Lachinov, E. Vasiliev, and V. Turlapov, “Glioma segmentation with cascaded
    unet,” in *Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain
    Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with
    MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part
    II 4*.   Cham, Switzerland: Springer, 2019, pp. 189–198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Li, Y. Chen, S. Yang, and W. Luo, “Cascade dense-unet for prostate
    segmentation in mr images,” in *Intelligent Computing Theories and Application:
    15th International Conference, ICIC 2019, Nanchang, China, August 3–6, 2019, Proceedings,
    Part I 15*.   Cham, Switzerland: Springer, 2019, pp. 481–490.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] R. Trullo, C. Petitjean, S. Ruan, B. Dubray, D. Nie, and D. Shen, “Segmentation
    of organs at risk in thoracic ct images using a sharpmask architecture and conditional
    random fields,” in *2017 IEEE 14th international symposium on biomedical imaging
    (ISBI 2017)*.   Melbourne, Australia: IEEE, 2017, pp. 1003–1006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Wang, L. Zhao, M. Wang, and Z. Song, “Organ at risk segmentation in
    head and neck ct images using a two-stage segmentation framework based on 3d u-net,”
    *IEEE Access*, vol. 7, pp. 144 591–144 602, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] K. Men, H. Geng, C. Cheng, H. Zhong, M. Huang, Y. Fan, J. P. Plastaras,
    A. Lin, and Y. Xiao, “More accurate and efficient segmentation of organs-at-risk
    in radiotherapy with convolutional neural networks cascades,” *Medical physics*,
    vol. 46, no. 1, pp. 286–292, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Lei, J. Zhou, X. Dong, T. Wang, H. Mao, M. McDonald, W. J. Curran,
    T. Liu, and X. Yang, “Multi-organ segmentation in head and neck mri using u-faster-rcnn,”
    in *Medical Imaging 2020: Image Processing*, vol. 113133A.   Houston, TX: SPIE,
    2020, pp. 826–831.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Francis, P. Jayaraj, P. Pournami, M. Thomas, A. T. Jose, A. J. Binu,
    and N. Puzhakkal, “Thoraxnet: a 3d u-net based two-stage framework for oar segmentation
    on thoracic ct images,” *Physical and Engineering Sciences in Medicine*, vol. 45,
    no. 1, pp. 189–203, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Tang, X. Chen, Y. Liu, Z. Lu, J. You, M. Yang, S. Yao, G. Zhao, Y. Xu,
    T. Chen *et al.*, “Clinically applicable deep learning framework for organs at
    risk delineation in ct images,” *Nature Machine Intelligence*, vol. 1, no. 10,
    pp. 480–491, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. C. Korte, N. Hardcastle, S. P. Ng, B. Clark, T. Kron, and P. Jackson,
    “Cascaded deep learning-based auto-segmentation for head and neck cancer patients:
    Organs at risk on t2-weighted magnetic resonance imaging,” *Medical physics*,
    vol. 48, no. 12, pp. 7757–7772, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Gao, R. Huang, M. Chen, Z. Wang, J. Deng, Y. Chen, Y. Yang, J. Zhang,
    C. Tao, and H. Li, “Focusnet: Imbalanced large and small organ segmentation with
    an end-to-end deep neural network for head and neck ct images,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part III 22*.   Cham, Switzerland:
    Springer, 2019, pp. 829–838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. Larsson, Y. Zhang, and F. Kahl, “Robust abdominal organ segmentation
    using regional convolutional neural networks,” *Applied Soft Computing*, vol. 70,
    pp. 465–471, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Zhao, H. Li, S. Wan, A. Sekuboyina, X. Hu, G. Tetteh, M. Piraud, and
    B. Menze, “Knowledge-aided convolutional neural network for small organ segmentation,”
    *IEEE journal of biomedical and health informatics*, vol. 23, no. 4, pp. 1363–1373,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] X. Ren, L. Xiang, D. Nie, Y. Shao, H. Zhang, D. Shen, and Q. Wang, “Interleaved
    3d-cnn s for joint segmentation of small-volume structures in head and neck ct
    images,” *Medical physics*, vol. 45, no. 5, pp. 2063–2075, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] B. Huang, Y. Ye, Z. Xu, Z. Cai, Y. He, Z. Zhong, L. Liu, X. Chen, H. Chen,
    and B. Huang, “3d lightweight network for simultaneous registration and segmentation
    of organs-at-risk in ct images of head and neck cancer,” *IEEE Transactions on
    Medical Imaging*, vol. 41, no. 4, pp. 951–964, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Liang, F. Tang, X. Huang, K. Yang, T. Zhong, R. Hu, S. Liu, X. Yuan,
    and Y. Zhang, “Deep-learning-based detection and segmentation of organs at risk
    in nasopharyngeal carcinoma computed tomographic images for radiotherapy planning,”
    *European radiology*, vol. 29, no. 4, pp. 1961–1967, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] N. Tong, S. Gou, S. Yang, D. Ruan, and K. Sheng, “Fully automatic multi-organ
    segmentation for head and neck cancer radiotherapy using shape representation
    model constrained fully convolutional neural networks,” *Medical physics*, vol. 45,
    no. 10, pp. 4558–4567, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. R. Roth, H. Oda, Y. Hayashi, M. Oda, N. Shimizu, M. Fujiwara, K. Misawa,
    and K. Mori, “Hierarchical 3d fully convolutional networks for multi-organ segmentation,”
    *arXiv preprint arXiv:1704.06382*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] K. Men, J. Dai, and Y. Li, “Automatic segmentation of the clinical target
    volume and organs at risk in the planning ct for rectal cancer using deep dilated
    convolutional neural networks,” *Medical physics*, vol. 44, no. 12, pp. 6377–6389,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Z. Chen, C. Li, J. He, J. Ye, D. Song, S. Wang, L. Gu, and Y. Qiao, “A
    novel hybrid convolutional neural network for accurate organ segmentation in 3d
    head and neck ct images,” in *Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part I 24*.   Cham, Switzerland: Springer, 2021, pp. 569–578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf, W. Yang, D. Li,
    and Z. Fan, “Fully automated multiorgan segmentation in abdominal magnetic resonance
    imaging with deep neural networks,” *Medical physics*, vol. 47, no. 10, pp. 4971–4982,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Jain, A. Sutradhar, A. K. Dash, and S. Das, “Automatic multi-organ
    segmentation on abdominal ct scans using deep u-net model,” in *2021 19th OITS
    International Conference on Information Technology (OCIT)*.   Bhubaneswar, India:
    IEEE, 2021, pp. 48–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Y. Ahn, J. S. Yoon, S. S. Lee, H.-I. Suk, J. H. Son, Y. S. Sung, Y. Lee,
    B.-K. Kang, and H. S. Kim, “Deep learning algorithm for automated segmentation
    and volume measurement of the liver and spleen using portal venous phase computed
    tomography images,” *Korean journal of radiology*, vol. 21, no. 8, pp. 987–997,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Shi, K. Wen, X. Hao, X. Xue, H. An, and H. Zhang, “A novel u-like
    network for the segmentation of thoracic organs,” in *2020 IEEE 17th International
    Symposium on Biomedical Imaging Workshops (ISBI Workshops)*.   Iowa City, IA:
    IEEE, 2020, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Liang, K.-H. Thung, D. Nie, Y. Zhang, and D. Shen, “Multi-view spatial
    aggregation framework for joint localization and segmentation of organs at risk
    in head and neck ct images,” *IEEE Transactions on Medical Imaging*, vol. 39,
    no. 9, pp. 2794–2805, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] X. Zhou, R. Takayama, S. Wang, T. Hara, and H. Fujita, “Deep learning
    of the sectional appearances of 3d ct images for anatomical structure segmentation
    based on an fcn voting method,” *Medical physics*, vol. 44, no. 10, pp. 5221–5233,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Y. Wang, Y. Zhou, W. Shen, S. Park, E. K. Fishman, and A. L. Yuille,
    “Abdominal multi-organ segmentation with organ-attention networks and statistical
    fusion,” *Medical image analysis*, vol. 55, pp. 88–102, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Tang, X. Liu, K. Han, X. Xie, X. Chen, H. Qian, Y. Liu, S. Sun, and
    N. Bai, “Spatial context-aware self-attention model for multi-organ segmentation,”
    in *Proceedings of the IEEE/CVF winter conference on applications of computer
    vision*, Waikoloa, HI, 2021, pp. 939–949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] T. Qu, X. Wang, C. Fang, L. Mao, J. Li, P. Li, J. Qu, X. Li, H. Xue,
    Y. Yu *et al.*, “M³net: A multi-scale multi-view framework for multi-phase pancreas
    segmentation based on cross-phase non-local attention,” *Medical image analysis*,
    vol. 75, p. 102232, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Y. Ding, W. Zheng, J. Geng, Z. Qin, K.-K. R. Choo, Z. Qin, and X. Hou,
    “Mvfusfra: a multi-view dynamic fusion framework for multimodal brain tumor segmentation,”
    *IEEE Journal of Biomedical and Health Informatics*, vol. 26, no. 4, pp. 1570–1581,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Ouaknine, A. Newson, P. Pérez, F. Tupin, and J. Rebut, “Multi-view
    radar semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, Montreal, QC, 2021, pp. 15 671–15 680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Z. S. Cheng, T. Y. Zeng, S. J. Huang, and X. Yang, “A novel hybrid network
    for h&n organs at risk segmentation,” in *Proceedings of the 5th International
    Conference on Biomedical Signal and Image Processing*, Suzhou, China, 2020, pp.
    7–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han, E. Fishman, and A. L.
    Yuille, “Prior-aware neural network for partially-supervised multi-organ segmentation,”
    in *Proceedings of the IEEE/CVF international conference on computer vision*,
    Seoul, South Korea, 2019, pp. 10 672–10 681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Lian, L. Li, Z. Luo, Z. Zhong, B. Wang, and S. Li, “Learning multi-organ
    segmentation via partial-and mutual-prior from single-organ datasets,” *Biomedical
    Signal Processing and Control*, vol. 80, p. 104339, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,
    S. A. Cook, A. De Marvao, T. Dawes, D. P. O‘Regan *et al.*, “Anatomically constrained
    neural networks (acnns): application to cardiac image enhancement and segmentation,”
    *IEEE transactions on medical imaging*, vol. 37, no. 2, pp. 384–395, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Advances in Neural Information Processing Systems*, vol. 33, pp. 6840–6851, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,”
    *arXiv preprint arXiv:2010.02502*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] I. Isler, C. Lisle, J. Rineer, P. Kelly, D. Turgut, J. Ricci, and U. Bagci,
    “Enhancing organ at risk segmentation with improved deep neural networks,” in
    *Medical Imaging 2022: Image Processing*, vol. 12032.   San Diego, CA: SPIE, 2022,
    pp. 814–820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Vesal, N. Ravikumar, and A. Maier, “A 2d dilated residual u-net for
    multi-organ segmentation in thoracic ct,” *arXiv preprint arXiv:1905.07710*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, Honolulu, HI, 2017, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
    “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Srivastava, D. Jha, E. Keles, B. Aydogan, M. Abazeed, and U. Bagci,
    “An efficient multi-scale fusion network for 3d organ at risk (oar) segmentation,”
    *arXiv preprint arXiv:2208.07417*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,
    K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz *et al.*, “Attention u-net: Learning
    where to look for the pancreas,” *arXiv preprint arXiv:1804.03999*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] S. Hu *et al.*, “Hu j., shen l., albanie s., sun g., wu e,” *Squeeze-and-excitation
    networks, IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 42,
    no. 8, pp. 2011–2023, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Z. Liu, H. Wang, W. Lei, and G. Wang, “Csaf-cnn: cross-layer spatial
    attention map fusion network for organ-at-risk segmentation in head and neck ct
    images,” in *2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)*.   Iowa
    City, IA: IEEE, 2020, pp. 1522–1525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] H. Lin, Z. Li, Z. Yang, and Y. Wang, “Variance-aware attention u-net
    for multi-organ segmentation,” *Medical Physics*, vol. 48, no. 12, pp. 7864–7876,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
    A nested u-net architecture for medical image segmentation,” in *Deep Learning
    in Medical Image Analysis and Multimodal Learning for Clinical Decision Support:
    4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS
    2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018,
    Proceedings 4*.   Cham, Switzerland: Springer, 2018, pp. 3–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, Honolulu, HI, 2017, pp. 4700–4708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, San Juan, PR, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
    convolutional networks,” in *Proceedings of the IEEE international conference
    on computer vision*, Venice, Italy, 2017, pp. 764–773.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. P. Heinrich, O. Oktay, and N. Bouteldja, “Obelisk-net: Fewer layers
    to solve 3d multi-organ segmentation with sparse deformable convolutions,” *Medical
    image analysis*, vol. 54, pp. 1–9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] N. Shen, Z. Wang, J. Li, H. Gao, W. Lu, P. Hu, and L. Feng, “Multi-organ
    segmentation network for abdominal ct images based on spatial attention and deformable
    convolution,” *Expert Systems with Applications*, vol. 211, p. 118625, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, “Strip pooling: Rethinking
    spatial pooling for scene parsing,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, Seattle, WA, 2020, pp. 4003–4012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] F. Zhang, Y. Wang, and H. Yang, “Efficient context-aware network for
    abdominal multi-organ segmentation,” *arXiv preprint arXiv:2109.10601*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] S. Jadon, “A survey of loss functions for semantic segmentation,” in
    *2020 IEEE conference on computational intelligence in bioinformatics and computational
    biology (CIBCB)*.   Via del Mar, Chile: IEEE, 2020, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Yi-de, L. Qing, and Q. Zhi-Bai, “Automated image segmentation using
    improved pcnn model based on cross-entropy,” in *Proceedings of 2004 International
    Symposium on Intelligent Multimedia, Video and Speech Processing, 2004.*   Hong
    Kong, China: IEEE, 2004, pp. 743–746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. Jorge Cardoso,
    “Generalised dice overlap as a deep learning loss function for highly unbalanced
    segmentations,” in *Deep Learning in Medical Image Analysis and Multimodal Learning
    for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th
    International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec
    City, QC, Canada, September 14, Proceedings 3*.   Cham, Switzerland: Springer,
    2017, pp. 240–248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Tversky loss function
    for image segmentation using 3d fully convolutional deep networks,” in *Machine
    Learning in Medical Imaging: 8th International Workshop, MLMI 2017, Held in Conjunction
    with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017, Proceedings 8*.   Cham,
    Switzerland: Springer, 2017, pp. 379–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, Venice, Italy, 2017, pp. 2980–2988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] V. Pihur, S. Datta, and S. Datta, “Weighted rank aggregation of cluster
    validation measures: a monte carlo cross-entropy approach,” *Bioinformatics*,
    vol. 23, no. 13, pp. 1607–1615, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] C. Shen, H. R. Roth, H. Oda, M. Oda, Y. Hayashi, K. Misawa, and K. Mori,
    “On the influence of dice loss function in multi-class organ segmentation of abdominal
    ct using 3d fully convolutional networks,” *arXiv preprint arXiv:1801.05912*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] E. Tappeiner, M. Welk, and R. Schubert, “Tackling the class imbalance
    problem of deep learning-based head and neck organ segmentation,” *International
    Journal of Computer Assisted Radiology and Surgery*, vol. 17, no. 11, pp. 2103–2111,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] N. Abraham and N. M. Khan, “A novel focal tversky loss function with
    improved attention u-net for lesion segmentation,” in *2019 IEEE 16th international
    symposium on biomedical imaging (ISBI 2019)*.   Venice, Italy: IEEE, 2019, pp.
    683–687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. C. Wong, M. Moradi, H. Tang, and T. Syeda-Mahmood, “3d segmentation
    with exponential logarithmic loss for highly unbalanced object sizes,” in *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International
    Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part III 11*.   Cham,
    Switzerland: Springer, 2018, pp. 612–619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. Song, X. Chen, Q. Zhu, F. Shi, D. Xiang, Z. Chen, Y. Fan, L. Pan,
    and W. Zhu, “Global and local feature reconstruction for medical image segmentation,”
    *IEEE Transactions on Medical Imaging*, vol. 41, no. 9, pp. 2273–2284, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] W. Lei, H. Mei, Z. Sun, S. Ye, R. Gu, H. Wang, R. Huang, S. Zhang, S. Zhang,
    and G. Wang, “Automatic segmentation of organs-at-risk from head-and-neck ct using
    separable convolutional neural network with hard-region-weighted loss,” *Neurocomputing*,
    vol. 442, pp. 184–199, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] P. Bilic, P. Christ, H. B. Li, E. Vorontsov, A. Ben-Cohen, G. Kaissis,
    A. Szeskin, C. Jacobs, G. E. H. Mamani, G. Chartrand *et al.*, “The liver tumor
    segmentation benchmark (lits),” *Medical Image Analysis*, vol. 84, p. 102680,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak,
    J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich *et al.*, “The kits19 challenge
    data: 300 kidney tumor cases with clinical context, ct semantic segmentations,
    and surgical outcomes,” *arXiv preprint arXiv:1904.00445*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken,
    A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze *et al.*, “A large annotated
    medical image dataset for the development and evaluation of segmentation algorithms,”
    *arXiv preprint arXiv:1902.09063*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis,” *arXiv preprint arXiv:1904.00625*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] K. Dmitriev and A. E. Kaufman, “Learning multi-class segmentations from
    single-class datasets,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, Long Beach, CA, 2019, pp. 9501–9511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] J. Zhang, Y. Xie, Y. Xia, and C. Shen, “Dodnet: Learning to segment multi-organ
    and tumors from multiple partially labeled datasets,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, Nashville, TN, 2021, pp.
    1195–1204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Xie, J. Zhang, Y. Xia, and C. Shen, “Learning from partially labeled
    data for multi-organ and tumor segmentation,” *arXiv preprint arXiv:2211.06894*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,
    “nnu-net: a self-configuring method for deep learning-based biomedical image segmentation,”
    *Nature methods*, vol. 18, no. 2, pp. 203–211, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] H. Wu, S. Pang, and A. Sowmya, “Tgnet: A task-guided network architecture
    for multi-organ and tumour segmentation from partially labelled datasets,” in
    *2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   Kolkata,
    India: IEEE, 2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P. Liu, L. Xiao, and S. K. Zhou, “Incremental learning for multi-organ
    segmentation with partially labeled datasets,” *arXiv preprint arXiv:2103.04526*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] X. Xu and P. Yan, “Federated multi-organ segmentation with partially
    labeled data,” *arXiv preprint arXiv:2206.07156*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] X. Fang and P. Yan, “Multi-organ segmentation over partially labeled
    datasets with multi-scale feature abstraction,” *IEEE Transactions on Medical
    Imaging*, vol. 39, no. 11, pp. 3619–3629, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] G. Shi, L. Xiao, Y. Chen, and S. K. Zhou, “Marginal loss and exclusion
    loss for partially supervised multi-organ segmentation,” *Medical Image Analysis*,
    vol. 70, p. 101979, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] R. Huang, Y. Zheng, Z. Hu, S. Zhang, and H. Li, “Multi-organ segmentation
    via co-training weight-averaged models from few-organ datasets,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part IV 23*.   Cham, Switzerland:
    Springer, 2020, pp. 146–155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] L. Zhang, S. Feng, Y. Wang, Y. Wang, Y. Zhang, X. Chen, and Q. Tian,
    “Unsupervised ensemble distillation for multi-organ segmentation,” in *2022 IEEE
    19th International Symposium on Biomedical Imaging (ISBI)*.   Kolkata, India:
    IEEE, 2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Q. Wu, Y. Chen, N. Huang, and X. Yue, “Weakly-supervised cerebrovascular
    segmentation network with shape prior and model indicator,” in *Proceedings of
    the 2022 International Conference on Multimedia Retrieval*, Newark, NJ, 2022,
    pp. 668–676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] F. Kanavati, K. Misawa, M. Fujiwara, K. Mori, D. Rueckert, and B. Glocker,
    “Joint supervoxel classification forest for weakly-supervised organ segmentation,”
    in *Machine Learning in Medical Imaging: 8th International Workshop, MLMI 2017,
    Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017,
    Proceedings 8*.   Cham, Switzerland: Springer, 2017, pp. 79–87.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,
    A. King, P. M. Matthews, and D. Rueckert, “Semi-supervised learning for network-based
    cardiac mr image segmentation,” in *Medical Image Computing and Computer-Assisted
    Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
    September 11-13, 2017, Proceedings, Part II 20*.   Cham, Switzerland: Springer,
    2017, pp. 253–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Luo, M. Hu, T. Song, G. Wang, and S. Zhang, “Semi-supervised medical
    image segmentation via cross teaching between cnn and transformer,” in *International
    Conference on Medical Imaging with Deep Learning*.   Zurich, Switzerland: PMLR,
    2022, pp. 820–833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J. Chen, J. Zhang, K. Debattista, and J. Han, “Semi-supervised unpaired
    medical image segmentation through task-affinity consistency,” *IEEE Transactions
    on Medical Imaging*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Y. Wu, Z. Ge, D. Zhang, M. Xu, L. Zhang, Y. Xia, and J. Cai, “Mutual
    consistency learning for semi-supervised medical image segmentation,” *Medical
    Image Analysis*, vol. 81, p. 102530, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] X. Luo, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, N. Chen, G. Wang,
    and S. Zhang, “Efficient semi-supervised gross target volume of nasopharyngeal
    carcinoma segmentation via uncertainty rectified pyramid consistency,” in *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    II 24*.   Cham, Switzerland: Springer, 2021, pp. 318–329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] R. Jiao, Y. Zhang, L. Ding, R. Cai, and J. Zhang, “Learning with limited
    annotations: a survey on deep semi-supervised learning for medical image segmentation,”
    *arXiv preprint arXiv:2207.14191*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Zhou, Y. Wang, P. Tang, S. Bai, W. Shen, E. Fishman, and A. Yuille,
    “Semi-supervised 3d abdominal multi-organ segmentation via deep multi-planar co-training,”
    in *2019 IEEE Winter Conference on Applications of Computer Vision (WACV)*.   Waikoloa,
    HI: IEEE, 2019, pp. 121–140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Y. Xia, D. Yang, Z. Yu, F. Liu, J. Cai, L. Yu, Z. Zhu, D. Xu, A. Yuille,
    and H. Roth, “Uncertainty-aware multi-view co-training for semi-supervised medical
    image segmentation and domain adaptation,” *Medical image analysis*, vol. 65,
    p. 101766, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Lai, T. Wang, and S. Zhou, “Dlunet: Semi-supervised learning based
    dual-light unet for multi-organ segmentation,” in *Fast and Low-Resource Semi-supervised
    Abdominal Organ Segmentation: MICCAI 2022 Challenge, FLARE 2022, Held in Conjunction
    with MICCAI 2022, Singapore, September 22, 2022, Proceedings*.   Springer, 2023,
    pp. 64–73.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] H. H. Lee, Y. Tang, O. Tang, Y. Xu, Y. Chen, D. Gao, S. Han, R. Gao,
    M. R. Savona, R. G. Abramson *et al.*, “Semi-supervised multi-organ segmentation
    through quality assurance supervision,” in *Medical Imaging 2020: Image Processing*,
    vol. 11313.   Houston, TX: SPIE, 2020, pp. 363–369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. Raju, C.-T. Cheng, Y. Huo, J. Cai, J. Huang, J. Xiao, L. Lu, C. Liao,
    and A. P. Harrison, “Co-heterogeneous and adaptive segmentation from multi-source
    and multi-phase ct imaging data: A study on pathological liver and lesion segmentation,”
    in *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XXIII*.   Cham, Switzerland: Springer, 2020, pp. 448–465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] D. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao,
    and L. Lu, “Organ at risk segmentation for head and neck cancer using stratified
    learning and neural architecture search,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, Seattle, WA, 2020, pp. 4223–4232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding,
    “Embracing imperfect datasets: A review of deep learning solutions for medical
    image segmentation,” *Medical Image Analysis*, vol. 63, p. 101693, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Qu, S. Liu, X. Liu, M. Wang, and Z. Song, “Towards label-efficient
    automatic diagnosis and analysis: a comprehensive survey of advanced deep learning-based
    weakly-supervised, semi-supervised and self-supervised techniques in histopathological
    image analysis,” *Physics in Medicine & Biology*, vol. 67, no. 20, p. 20TR01,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, “Domain
    adaptive relational reasoning for 3d multi-organ segmentation,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part I 23*.   Cham, Switzerland: Springer,
    2020, pp. 656–666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Hong, Y.-D. Zhang, and W. Chen, “Source-free unsupervised domain adaptation
    for cross-modality abdominal multi-organ segmentation,” *Knowledge-Based Systems*,
    vol. 250, p. 109155, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Y. Liu, Y. Lei, Y. Fu, T. Wang, J. Zhou, X. Jiang, M. McDonald, J. J.
    Beitler, W. J. Curran, T. Liu *et al.*, “Head and neck multi-organ auto-segmentation
    on ct images aided by synthetic mri,” *Medical physics*, vol. 47, no. 9, pp. 4294–4302,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. Cros, E. Vorontsov, and S. Kadoury, “Managing class imbalance in multi-organ
    ct segmentation in head and neck cancer patients,” in *2021 IEEE 18th International
    Symposium on Biomedical Imaging (ISBI)*.   Nice, France: IEEE, 2021, pp. 1360–1364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] J. Jiang, S. Elguindi, S. L. Berry, I. Onochie, L. Cervino, J. O. Deasy,
    and H. Veeraraghavan, “Nested block self-attention multiple resolution residual
    network for multiorgan segmentation from ct,” *Medical Physics*, vol. 49, no. 8,
    pp. 5244–5257, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. Francis, G. Pooloth, S. B. S. Singam, N. Puzhakkal, P. Pulinthanathu Narayanan,
    and J. Pottekkattuvalappil Balakrishnan, “Sabos-net: Self-supervised attention
    based network for automatic organ segmentation of head and neck ct images,” *International
    Journal of Imaging Systems and Technology*, vol. 33, no. 1, pp. 175–191, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore,
    S. Phillips, D. Maffitt, M. Pringle *et al.*, “The cancer imaging archive (tcia):
    maintaining and operating a public information repository,” *Journal of digital
    imaging*, vol. 26, no. 6, pp. 1045–1057, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] H. R. Roth, L. Lu, A. Farag, H.-C. Shin, J. Liu, E. B. Turkbey, and R. M.
    Summers, “Deeporgan: Multi-level deep convolutional networks for automated pancreas
    segmentation,” in *Medical Image Computing and Computer-Assisted Intervention–MICCAI
    2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings,
    Part I 18*.   Cham, Switzerland: Springer, 2015, pp. 556–564.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] V. Kumar, M. K. Sharma, R. Jehadeesan, B. Venkatraman, and D. Sheet,
    “Adversarial training of deep convolutional neural network for multi-organ segmentation
    from multi-sequence mri of the abdomen,” in *2021 International Conference on
    Intelligent Technologies (CONIT)*.   Hubli, India: IEEE, 2021, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] B. Rister, D. Yi, K. Shivakumar, T. Nobashi, and D. L. Rubin, “Ct-org,
    a new dataset for multiple organ segmentation in computed tomography,” *Scientific
    Data*, vol. 7, no. 1, p. 381, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] C. C. Vu, Z. A. Siddiqui, L. Zamdborg, A. B. Thompson, T. J. Quinn, E. Castillo,
    and T. M. Guerrero, “Deep convolutional neural networks for automatic segmentation
    of thoracic organs-at-risk in radiation oncology–use of non-domain transfer learning,”
    *Journal of Applied Clinical Medical Physics*, vol. 21, no. 6, pp. 108–113, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. S. K. Gali, N. Garg, S. Vasamsetti *et al.*, “Dilated u-net based
    segmentation of organs at risk in thoracic ct images,” in *SegTHOR@ ISBI*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] H. Mahmood, S. M. S. Islam, J. Hill, and G. Tay, “Rapid segmentation
    of thoracic organs using u-net architecture,” in *2021 Digital Image Computing:
    Techniques and Applications (DICTA)*.   Gold Coast, Australia: IEEE, 2021, pp.
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] F. Zhang, Q. Wang, A. Yang, N. Lu, H. Jiang, D. Chen, Y. Yu, and Y. Wang,
    “Geometric and dosimetric evaluation of the automatic delineation of organs at
    risk (oars) in non-small-cell lung cancer radiotherapy based on a modified densenet
    deep learning network,” *Frontiers in Oncology*, vol. 12, p. 861857, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] J. Ma, Y. Zhang, S. Gu, X. An, Z. Wang, C. Ge, C. Wang, F. Zhang, Y. Wang,
    Y. Xu *et al.*, “Fast and low-gpu-memory abdomen ct organ segmentation: the flare
    challenge,” *Medical Image Analysis*, vol. 82, p. 102616, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] H. Kakeya, T. Okada, and Y. Oshiro, “3d u-japa-net: mixture of convolutional
    networks for abdominal multi-organ ct segmentation,” in *Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2018: 21st International Conference,
    Granada, Spain, September 16-20, 2018, Proceedings, Part IV 11*.   Cham, Switzerland:
    Springer, 2018, pp. 426–433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] R. Trullo, C. Petitjean, D. Nie, D. Shen, and S. Ruan, “Joint segmentation
    of multiple thoracic organs in ct images with two collaborative deep architectures,”
    in *Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
    Decision Support: Third International Workshop, DLMIA 2017, and 7th International
    Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC,
    Canada, September 14, Proceedings 3*, vol. 10553.   Springer, 2017, pp. 21–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Z. Cao, B. Yu, B. Lei, H. Ying, X. Zhang, D. Z. Chen, and J. Wu, “Cascaded
    se-resunet for segmentation of thoracic organs at risk,” *Neurocomputing*, vol.
    453, pp. 357–368, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Q. Yang, S. Zhang, X. Sun, J. Sun, and K. Yuan, “Automatic segmentation
    of head-neck organs by multi-mode cnns for radiation therapy,” in *2019 International
    Conference on Medical Imaging Physics and Engineering (ICMIPE)*.   Shenzhen, China:
    IEEE, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] C. E. Cardenas, A. S. Mohamed, J. Yang, M. Gooding, H. Veeraraghavan,
    J. Kalpathy-Cramer, S. P. Ng, Y. Ding, J. Wang, S. Y. Lai *et al.*, “Head and
    neck cancer patient images for determining auto-segmentation accuracy in t2-weighted
    magnetic resonance imaging through expert manual segmentations,” *Medical physics*,
    vol. 47, no. 5, pp. 2317–2322, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] O. Jimenez-del Toro, H. Müller, M. Krenn, K. Gruenberg, A. A. Taha, M. Winterstein,
    I. Eggel, A. Foncubierta-Rodríguez, O. Goksel, A. Jakab *et al.*, “Cloud-based
    evaluation of anatomical structure segmentation and landmark detection algorithms:
    Visceral anatomy benchmarks,” *IEEE transactions on medical imaging*, vol. 35,
    no. 11, pp. 2459–2475, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplementary Materials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE II: DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation
    Methods for the Head and Neck'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Datasets | Quantity | Organ type | Brainstem | Mandible
    | Parotid gland | Submandibular gland | Optic nerve | Chiasm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right | Left | Right | Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ibragimov and Xing [[41](#bib.bib41)] | 2.5D CNN | Private (CT) | 50 | 13
    | - | 0.895 | 0.766 | 0.779 | 0.697 | 0.730 | 0.639 | 0.645 | 0.374 |'
  prefs: []
  type: TYPE_TB
- en: '| Fritscher et al. [[42](#bib.bib42)] | 2.5D CNN | HNC (CT) [[30](#bib.bib30)]
    | 30 | 3 | - | - | 0.810 | - | 0.650 | - | - | - | 0.520 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. [[48](#bib.bib48)] | 3D U-Net | Private (CT) | 261 | 9 | 0.867
    | 0.925 | 0.881 | 0.874 | 0.814 | 0.813 | 0.721 | 0.706 | 0.532 |'
  prefs: []
  type: TYPE_TB
- en: '| Van Rooij et al. [[49](#bib.bib49)] | 3D U-Net | Private (CT) | 157 | 11
    | 0.640 | - | 0.830 | 0.830 | 0.820 | 0.810 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. [[64](#bib.bib64)] | 3D GAN | PDDCA (CT) [[30](#bib.bib30)] |
    48 | 9 | 0.867 | 0.939 | 0.855 | 0.858 | 0.807 | 0.819 | 0.664 | 0.699 | 0.592
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. [[64](#bib.bib64)] | 3D GAN | Private (MRI) | 25 | 9 | 0.916
    | 0.816 | 0.865 | 0.825 | - | - | 0.717 | 0.693 | 0.589 |'
  prefs: []
  type: TYPE_TB
- en: '| Gou et al. [[50](#bib.bib50)] | 3D U-Net | HNC (CT) [[30](#bib.bib30)] |
    48 | 9 | 0.880 | 0.940 | 0.870 | 0.860 | 0.780 | 0.810 | 0.720 | 0.710 | 0.610
    |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[196](#bib.bib196)] | 3D U-Net | Private (MRI & CT) | 45 | 19
    | 0.880 | 0.890 | 0.890 | 0.880 | - | - | 0.720 | 0.720 | 0.760 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[196](#bib.bib196)] | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.910 | 0.960 | 0.880 | 0.880 | 0.860 | 0.850 | 0.780 | 0.780 | 0.730
    |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | 2.5D U-Net | Private (CT) | 307 | 24 |
    - | - | - | - | - | - | 0.711 | 0.712 | 0.598 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | 2.5D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.872 | 0.922 | 0.867 | 0.858 | 0.821 | 0.821 | 0.750 | 0.741 | 0.663
    |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[141](#bib.bib141)] | 2D U-Net | StructSeg (CT) | 50 | 22 | 0.864
    | 0.906 | 0.802 | 0.826 | - | - | 0.770 | 0.647 | 0.712 |'
  prefs: []
  type: TYPE_TB
- en: '| Cros et al. [[197](#bib.bib197)] | 3D U-Net | Private (CT) | 200 | 12 | -
    | 0.900 | 0.760 | 0.760 | 0.740 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[162](#bib.bib162)] | 2.5D U-Net | StructSeg (CT) | 50 | 22 |
    0.897 | 0.914 | 0.857 | 0.873 | - | - | 0.680 | 0.663 | 0.566 |'
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[162](#bib.bib162)] | 2.5D U-Net | Hybrid HAN (CT) | 165 | 7
    | 0.874 | 0.900 | 0.847 | 0.846 | - | - | 0.624 | 0.621 | 0.290 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[51](#bib.bib51)] | 2D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.840 | 0.900 | 0.820 | 0.830 | 0.820 | 0.810 | 0.670 | 0.710 | 0.660
    |'
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | OpenKBP (CT) [[35](#bib.bib35)]
    | 188 | 5 | 0.803 | 0.883 | 0.799 | 0.773 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Podobnik et al. [[59](#bib.bib59)] | 2D nnU-net | Private (CT&MR) | 56 |
    31 | 0.836 | 0.898 | 0.817 | 0.765 | 0.716 | 0.670 | 0.572 | 0.604 | 0.387 |'
  prefs: []
  type: TYPE_TB
- en: '| Kan et al. [[85](#bib.bib85)] | 3D Transformer and U-Net | Private (CT) |
    94 | 18 | 0.871 | 0.925 | 0.821 | 0.844 | - | - | 0.717 | 0.679 | 0.328 |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[198](#bib.bib198)] | 2D U-Net | PDDCA (CT) [[30](#bib.bib30)]
    | 16 | 6 | 0.920 | 0.950 | 0.880 | 0.880 | 0.820 | 0.830 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Francis et al. [[199](#bib.bib199)] | 3D U-Net | Private (CT) | 232 | 7 |
    0.890 | 0.932 | 0.852 | 0.870 |  |  | 0.744 | 0.764 | 0.635 |'
  prefs: []
  type: TYPE_TB
- en: '| HNC (CT) [[30](#bib.bib30)] | 48 | 7 | 0.862 | 0.940 | 0.885 | 0.885 |  |  |
    0.728 | 0.723 | 0.620 |'
  prefs: []
  type: TYPE_TB
- en: '| Isler et al. [[134](#bib.bib134)] | 2D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 6 | 0.830 | - | 0.790 | 0.760 | - | - | 0.580 | 0.540 | 0.520 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenKBP (CT) [[35](#bib.bib35)] | 188 | 5 | 0.800 | 0.860 | 0.750 | 0.760
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: DSC-Based Summary of the Literature on Multi-Organ Single-Stage
    Segmentation Methods for the Abdomen'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Datasets | Quantity | Organ type | Liver | Spleen | Kidney
    | Pancreas | Gallbladder | Stomach |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[40](#bib.bib40)] | 3D CNN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV [[29](#bib.bib29)] (CT) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 72 | 4 | 0.920 | - | - | - | 0.660 | - | 0.830 |'
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[115](#bib.bib115)] | 2D CNN | Private (CT) | 278 | 5 | - | -
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[157](#bib.bib157)] | 3D U-Net | Private (CT) | 377 | 7 | 0.965
    | 0.947 | - | - | 0.847 | 0.808 | 0.963 |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[57](#bib.bib57)] | 3D V-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV [[29](#bib.bib29)] (CT) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 90 | 8 | 0.960 | 0.960 | 0.950 | - | 0.780 | 0.840 | 0.900 |'
  prefs: []
  type: TYPE_TB
- en: '| Roth et al. [[52](#bib.bib52)] | 3D U-Net | Private (CT) | 377 | 7 | 0.971
    | 0.977 | - | - | 0.849 | 0.851 | 0.961 |'
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[65](#bib.bib65)] | 2D FCN | Private (CT) | 120 | 16 | 0.96 |
    0.951 | 0.956 | 0.954 | 0.785 | 0.797 | 0.909 |'
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[65](#bib.bib65)] | 3D GAN | Private (CT) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 131(liver)+281(spleen) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +41(pancreas) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3 | 0.944 | 0.960 | - | - | 0.743 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Heinrich et al. [[147](#bib.bib147)] | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 43 | 8 | 0.954 | 0.944 | - | - | 0.702 | 0.753 | 0.868 |'
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 10 | 7 |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ahn et al. [[119](#bib.bib119)] | 2.5D CNN | Private (CT) | 813+150 | 2 |
    0.973 | 0.974 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 813+50 | 2 | 0.983 | 0.968 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. [[194](#bib.bib194)] | 3D V-Net | Synapse (CT) [[2](#bib.bib2)]
    | 90 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen
    and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; stomach average DSC: 0.698 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hatamizadeh et al. [[87](#bib.bib87)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 3D Transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| BTCV (CT) [[29](#bib.bib29)] | 30 | 13 | 0.983 | 0.972 | 0.954 | 0.942 |
    0.799 | 0.825 | 0.945 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[117](#bib.bib117)] | 2.5D U-Net | Private (MR) | 102 | 10 |
    0.963 | 0.946 | 0.954 | 0.954 | 0.880 | 0.732 | 0.923 |'
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[124](#bib.bib124)] | 2.5D U-Net | ABD-110 (CT) [[124](#bib.bib124)]
    | 110 | 11 | 0.964 | 0.959 | 0.960 | 0.957 | 0.821 | 0.822 | 0.875 |'
  prefs: []
  type: TYPE_TB
- en: '| Jia and Wei [[53](#bib.bib53)] | 3D U-Net | CHAOS (CT [[32](#bib.bib32)]
    | 20 | 4 | 0.934 | 0.896 | 0.937 | 0.949 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[142](#bib.bib142)] | 3D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV [[29](#bib.bib29)] (CT) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 90 | 8 | 0.953 | 0.920 | 0.902 | - | 0.742 | 0.760 | 0.862 |'
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. [[77](#bib.bib77)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.943 | 0.907 | 0.833 | 0.796 | 0.566 | 0.665 | 0.766 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[86](#bib.bib86)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2D Transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; And U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Synapse (CT) [[2](#bib.bib2)] | 30 | 8 | 0.941 | 0.851 | 0.819 | 0.770 |
    0.559 | 0.631 | 0.756 |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[161](#bib.bib161)] | 2D CNN | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.959 | 0.926 | 0.906 | 0.892 | 0.687 | 0.671 | 0.839 |'
  prefs: []
  type: TYPE_TB
- en: '| Kumar et al. [[202](#bib.bib202)] | 2D GAN | CHAOS (CT) [[32](#bib.bib32)]
    | 40 | 4 | Liver, left kidney, right kidney and spleen average DSC: 0.970 |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[78](#bib.bib78)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.944 | 0.919 | 0.852 | 0.820 | 0.657 | 0.687 | 0.808 |'
  prefs: []
  type: TYPE_TB
- en: '| Suo et al. [[84](#bib.bib84)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2D Transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; And U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Synapse (CT) [[2](#bib.bib2)] | 30 | 8 | 0.951 | 0.914 | 0.890 | 0.851 |
    0.699 | 0.720 | 0.826 |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[58](#bib.bib58)] | 3D V-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AbdomenCT-1K [[36](#bib.bib36)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +Private (CT) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1112+100 | 4 | 0.953 | 0.920 | 0.914 | 0.747 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AbdomenCT-1K [[36](#bib.bib36)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV (CT) [[29](#bib.bib29)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1112+90 | 4 | 0.961 | 0.954 | 0.918 | - | 0.784 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Berzoini et al. [[54](#bib.bib54)] | 2D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Open-source CT-org &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dataset [[203](#bib.bib203)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 140 | 5 | 0.922 | - | 0.837 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[148](#bib.bib148)] | 2D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 42 | 5 | 0.960 | - | - | - | 0.754 | 0.805 | 0.889 |'
  prefs: []
  type: TYPE_TB
- en: '| Hong et al. [[195](#bib.bib195)] | 2D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BTCV [[29](#bib.bib29)] & CHAOS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[32](#bib.bib32)] (CT) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 30+20 | 4 | 0.884 | 0.911 | 0.864 | 0.891 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[79](#bib.bib79)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 3D CNN And &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| BTCV [[29](#bib.bib29)] (CT) | 30 | 11 | 0.971 | 0.963 | 0.939 | 0.831 |
    0.666 | 0.882 |'
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.950 | 0.870 | 0.842 | 0.824 | 0.681 | 0.675 | 0.760 |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[198](#bib.bib198)] | 2D U-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 12 | 0.969 | 0.958 | 0.943 | 0.921 | 0.798 | 0.786 | 0.906 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation
    Methods for the Thorax'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Dataset | Quantity | Organ type | Heart | Esophagus | Trachea
    | Aorta | Spinal cord | Stomach |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| Trullo et al. [[100](#bib.bib100)] | 2D FCN | Private (CT) | 30 | 4 | 0.900
    | 0.670 | 0.820 | - | - | 0.860 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dong et al. [[66](#bib.bib66)] | 3D GAN | AAPM (CT) [[31](#bib.bib31)] |
    35 | 4 | 0.870 | 0.750 | - | 0.970 | 0.970 | - | 0.900 |'
  prefs: []
  type: TYPE_TB
- en: '| Vu et al. [[204](#bib.bib204)] | 2D U-Net | Private (CT) | 22411(2D) | 5
    | 0.910 | 0.630 | - | 0.960 | 0.960 | - | 0.710 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lambert et al. [[55](#bib.bib55)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gali et al. [[205](#bib.bib205)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2D U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2D U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SegTHOR (CT) [[33](#bib.bib33)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SegTHOR (CT) [[33](#bib.bib33)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.930 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.860 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.820 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.469 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.850 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.643 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.910 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.854 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vesal et al. [[135](#bib.bib135)] | 2D U-Net | SegTHOR (CT) [[33](#bib.bib33)]
    | 60 | 4 | 0.941 | 0.858 | 0.926 | - | - | 0.938 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Shi et al. [[120](#bib.bib120)] | 2.5D U-Net | StructSeg (CT) [[1](#bib.bib1)]
    | 50 | 5 | 0.941 | 0.821 | 0.882 | 0.968 | 0.971 | - | 0.902 |'
  prefs: []
  type: TYPE_TB
- en: '| Mahmood et al. [[206](#bib.bib206)] | 2D U-Net | AAPM (CT) [[31](#bib.bib31)]
    | 60 | 5 | 0.880 | 0.660 | - | 0.970 | 0.970 | - | 0.800 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[207](#bib.bib207)] | 2D FCN | Private (CT) | 36 | 6 | 0.860
    | 0.670 | 0.910 | 0.950 | 0.960 | - | 0.890 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Head and Neck'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Organ type | Brainstem | Mandible
    | Parotid gland | Submandibular gland | Optic Nerve | Chiasm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right | Left | Right | Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ren et al. [[110](#bib.bib110)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 3 | - | - | - | - | - | - | 0.720 | 0.700 | 0.580 |'
  prefs: []
  type: TYPE_TB
- en: '| Tappeiner et al. [[90](#bib.bib90)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 40 | 7 | 0.820 | 0.910 | 0.800 | 0.810 | - | - | 0.640 | 0.630 | 0.420 |'
  prefs: []
  type: TYPE_TB
- en: '| Pu et al. [[91](#bib.bib91)] | 2.5D U-Net | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.880 | 0.940 | 0.860 | 0.865 | 0.788 | 0.802 | 0.743 | 0.768 | 0.612
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ma et al. [[92](#bib.bib92)] | 3D U-Net |  | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.879 | 0.945 | 0.892 | 0.884 | 0.829 | 0.815 | 0.753 | 0.747 | 0.659
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. [[70](#bib.bib70)] | 2D FCN | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 32 | 9 | 0.849 | 0.924 | 0.842 | 0.849 | 0.734 | 0.782 | 0.676 | 0.684 | 0.547
    |'
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 56 | 14 | 0.863 | 0.905 | 0.582 | 0.687 | 0.668 | 0.575 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Abdomen'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Category | Liver | Spleen | Kidney
    | Pancreas | Gallbladder | Stomach |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. [[93](#bib.bib93)] | 3D FCN | Refinement Model | Private (CT) |
    140 | 4 | 0.960 | 0.942 | 0.954 |  | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Roth et al. [[114](#bib.bib114)] | 3D FCN | 3D FCN | Private (CT) | 331 |
    3 | 0.932 | 0.906 | - | - | 0.631 | 0.706 | 0.843 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[123](#bib.bib123)] | 2.5D FCN | 2.5D FCN | Private (CT) | 236
    | 13 | 0.980 | 0.971 | 0.968 | 0.984 | 0.878 | 0.905 | 0.952 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[94](#bib.bib94)] | 3D V-Net | 3D V-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | 0.945 | 0.915 | 0.909 | 0.919 | 0.694 | 0.682 | 0.784 |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[95](#bib.bib95)] | 2.5D FCN | 2.5D FCN | Private (CT) | 200
    | 16 | 0.969 | 0.968 | 0.962 | 0.960 | 0.877 | 0.894 | 0.951 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[150](#bib.bib150)] | 3D U-Net | 3D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; FLARE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2021 (CT) [[208](#bib.bib208)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 511 | 4 | 0.954 | 0.942 | 0.936 | 0.753 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. [[96](#bib.bib96)] | 3D U-Net | 3D U-Net | Private (CT) | 100
    | 13 | 0.960 | 0.965 | 0.945 | 0.920 | 0.766 | 0.793 | 0.833 |'
  prefs: []
  type: TYPE_TB
- en: '| Kakeya et al. [[209](#bib.bib209)] | 3D U-Net | 3D U-Net | Private (CT) |
    47 | 8 | 0.971 | 0.969 | 0.984 | 0.975 | 0.861 | 0.918 | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Thorax'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Category | Heart | Esophagus
    | Trachea | Lung | Aorta | Spinal cord |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Trullo et al. [[210](#bib.bib210)] | 2D FCN | 2D FCN | Private (CT) | 30
    | 4 | 0.900 | 0.690 | 0.870 | - | - | 0.89 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. [[211](#bib.bib211)] | 2D U-Net | 2D U-Net | SegTHOR (CT) [[33](#bib.bib33)]
    | 50 | 6 | 0.945 | 0.850 | 0.807 | 0.97 | 0.966 | - | 0.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[94](#bib.bib94)] | 3D V-Net | 3D V-Net | SegTHOR (CT) [[33](#bib.bib33)]
    | 50 | 4 | 0.930 | 0.785 | 0.890 | - | - | 0.916 | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: DSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION
    AND SEGMENTATION METHODS FOR THE HEAD AND NECK'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Datasets | Quantity | Category | Brainstem
    | Mandible | Parotid gland | Submandibular gland | Optic nerve | Chiasm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right | Left | Right | Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[101](#bib.bib101)] | 3D U-Net | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.875 | 0.930 | 0.864 | 0.848 | 0.758 | 0.733 | 0.737 | 0.736 | 0.451
    |'
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[102](#bib.bib102)] | 3D U-Net | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 100 | 7 | 0.900 | 0.920 | 0.860 | 0.860 | - | - | - | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[105](#bib.bib105)] | 3D U-Net | 3D U-Net | Private (CT) | 215
    | 28 | 0.863 | 0.931 | 0.849 | 0.849 | 0.807 | 0.825 | 0.757 | 0.761 | 0.642 |'
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[105](#bib.bib105)] | 3D U-Net | 3D U-Net | PDDCA (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.875 | 0.95 | 0.887 | 0.875 | 0.823 | 0.815 | 0.748 | 0.723 | 0.615
    |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[212](#bib.bib212)] | 3D CNN | 2D U-Net | Private (CT) | 88
    | 17 | 0.831 | 0.875 | 0.807 | 0.811 | - | - | 0.638 | 0.675 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[112](#bib.bib112)] | 2D CNN | 2D CNN | Private (CT) | 185
    | 18 | 0.896 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; left: 0.914; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; right: 0.912 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.852 | 0.85 | - | - | 0.661 | 0.717 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[107](#bib.bib107)] | 3D CNN | 3D CNN | Private (CT) | 50 | 18
    | 0.858 | - | 0.772 | 0.800 | - | - | 0.639 | 0.617 | 0.638 |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[107](#bib.bib107)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.875 | 0.935 | 0.863 | 0.879 | 0.798 | 0.801 | 0.735 | 0.744 | 0.596
    |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[121](#bib.bib121)] | 2.5D CNN | 2.5D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.923 | 0.941 | 0.876 | 0.808 | 0.736 | 0.713 |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[121](#bib.bib121)] | 2.5D CNN | 2.5D CNN | Private (CT) |
    96 | 11 | - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Left: 0.911; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; right: 0.914 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.883 | 0.868 | - | - | 0.871 | 0.874 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[103](#bib.bib103)] | 3D CNN | 3D U-Net | Private (CT) | 15 |
    8 | - | 0.850 | 0.820 | 0.810 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[111](#bib.bib111)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.879 | 0.916 | 0.884 | 0.878 | 0.801 | 0.776 | 0.677 | 0.706 | 0.643
    |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[111](#bib.bib111)] | 3D CNN | 3D CNN | StructSeg (CT) [[1](#bib.bib1)]
    | 15 | 7 | 0.769 | 0.807 | 0.802 | 0.802 | - | - | 0.499 | 0.534 | 0.211 |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[111](#bib.bib111)] | 3D CNN | 3D CNN | Private (CT) | 15 |
    9 | 0.957 | 0.848 | 0.962 | 0.946 | 0.846 | 0.808 | 0.824 | 0.843 | 0.434 |'
  prefs: []
  type: TYPE_TB
- en: '| Korte et al. [[106](#bib.bib106)] | 3D U-Net | 3D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Public RT-MAC dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (MRI) [[213](#bib.bib213)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 31 | 8 | - | - | 0.860 | 0.857 | 0.830 | 0.785 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Korte et al. [[106](#bib.bib106)] | 3D U-Net | 3D U-Net | Private (MRI) |
    10 | 8 | - | - | 0.730 | 0.775 | 0.537 | 0.435 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[69](#bib.bib69)] | 3D CNN | 3D CNN | Private (CT) | 1164 | 22
    | 0.891 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Left: 0.924; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; right: 0.925 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.846 | 0.87 | - | - | 0.713 | 0.753 | 0.612 |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[69](#bib.bib69)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.882 | 0.947 | 0.898 | 0.881 | 0.840 | 0.838 | 0.790 | 0.817 | 0.713
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: DSC-Based Summary of the Literature on Multi-Organ Localization and
    Segmentation Methods for the Abdomen'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Liver
    | Spleen | Kidney | Pancreas | Gallbladder | Stomach |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Larsson et al. [[108](#bib.bib108)] | Multi-Atlas | 3D FCN | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | 0.949 | 0.936 | 0.911 | 0.897 | 0.646 | 0.613 | 0.764 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[109](#bib.bib109)] | Registration | 2D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VISCERAL challenge dataset Nonenhanced &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTwb) [[214](#bib.bib214)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | - | - | - | - | 0.583 | 0.473 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[109](#bib.bib109)] | Registration | 2D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VISCERAL challenge dataset enhanced &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTce) [[214](#bib.bib214)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | - | - | - | - | 0.588 | 0.624 | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: DSC-Based Summary of the Literature on Multi-Organ Localization and
    Segmentation Methods for the Thorax'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Heart
    | Esophagus | Trachea | Lung | Aorta | Spinal cord |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Francis et al. [[211](#bib.bib211)] | 3D U-Net | Two 3D U-Net | AAPM (CT)
    [[31](#bib.bib31)] | 60 | 5 | 0.941 | 0.738 | - | 0.979 | 0.973 | - | 0.899 |'
  prefs: []
  type: TYPE_TB
- en: '| Feng et al. [[104](#bib.bib104)] | 3D U-Net | 3D U-Net | AAPM (CT) [[31](#bib.bib31)]
    | 60 | 5 | 0.925 | 0.726 | - | 0.979 | 0.972 | - | 0.893 |'
  prefs: []
  type: TYPE_TB
- en: '| Feng et al. [[104](#bib.bib104)] | 3D U-Net | 3D U-Net | Private (CT) | 30
    | 5 | 0.86 | 0.685 | - | 0.976 | 0.977 | - | 0.852 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation
    Methods for the Head and Neck-Supplementary Material'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Datasets | Quantity | Organ type | Other organs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ibragimov and Xing [[41](#bib.bib41)] | 2.5D CNN | Private (CT) | 50 | 13
    | Pharynx: 0.856; Left eyeball: 0.884; Right eyeball: 0.877; Spinal cord: 0.870;
    Larynx: 0.693 |'
  prefs: []
  type: TYPE_TB
- en: '| Van Rooij et al. [[49](#bib.bib49)] | 3D U-Net | Private (CT) | 157 | 11
    | Larynx: 0.780; Pharyngeal Constrictor: 0.680; Cricopharynx: 0.730; Upper esophageal
    sphincter: 0.810; esophagus: 0.600; Oral Cavity: 0.780 |'
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. [[64](#bib.bib64)] | 3D GAN | Private (MRI) | 25 | 9 | Pharynx:
    0.706; Larynx: 0.799 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[196](#bib.bib196)] | 3D U-Net | Private (MRI & CT) | 45 | 19
    | Pharynx: 0.740; spinal cord: 0.840; left cochlea: 0.760; right cochlea: 0.750;
    esophagus: 0.850; oral cavity: 0.900; left eye: 0.890; right eye: 0.870; left
    lens: 0.730; right lens: 0.730; larynx: 0.900; brain: 0.950 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | 2.5D U-Net | Private (CT) | 307 | 24 |
    Pituitary: 0.756; left middle ear: 0.869; right middle ear: 0.859; left lens:
    0.844; right lens: 0.839; left temporomandibular joint: 0.838; right temporomandibular
    joint: 0.829 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[141](#bib.bib141)] | 2D U-Net | StructSeg (CT) | 50 | 22 | Left
    eye: 0.858; right eye: 0.882; spinal cord: 0.804; pituitary: 0.503; left middle
    ear: 0.825; right middle ear: 0.717; left lens: 0.898; right lens: 0.786; left
    temporomandibular joint: 0.723; right temporomandibular joint: 0.824 |'
  prefs: []
  type: TYPE_TB
- en: '| Cros et al. [[197](#bib.bib197)] | 3D U-Net | Private (CT) | 200 | 12 | Medullary
    canal: 0.870; Outer medullary canal: 0.860; oral cavity: 0.660; esophagus: 0.600;
    trachea: 0.670; trunk: 0.670; outer trunk: 0.700; inner ears: 0.710; eyes: 0.770;
    sub-maxillary glands: 0.740 |'
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[162](#bib.bib162)] | 3D U-Net | StructSeg (CT) | 50 | 22 | Left
    eye: 0.886; right eye: 0.873; spinal cord: 0.830; pituitary: 0.661; left middle
    ear: 0.826; right middle ear: 0.783; left lens: 0.815; right lens: 0.754; left
    temporomandibular joint: 0.757; right temporomandibular joint: 0.772 |'
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | OpenKBP (CT) [[36](#bib.bib36)]
    | 188 | 5 | Spinal cord: 0.740 |'
  prefs: []
  type: TYPE_TB
- en: '| Podobnik et al. [[59](#bib.bib59)] | 2D nnU-net | Private (CT & MRI) | 56
    | 31 | Spinal cord: 0.812; Pharyngeal constrictor muscles: 0.617; oral cavity:
    0.845; Larynx-supraglottis: 0.728; Larynx-glottis:0.615; Lips:0.728; Thyroid:
    0.721; pituitary gland: 0.658; Lacrimal glands (left): 0.621; Lacrimal glands
    (right): 0.636; left eye: 0.887; right eye: 0.884; left lens: 0.723; right lens:
    0.763; Cervical esophagus: 0.559; Cricopharyngeal inlet: 0.517; Cochleae (left):0.558;
    Cochleae (right):0.514; Carotid arteries (left):0.624; Carotid arteries (right):0.618;
    Buccal mucosa: 0.661; Arytenoids:0.474 |'
  prefs: []
  type: TYPE_TB
- en: '| Kan et al. [[85](#bib.bib85)] | 3D Transformer and U-Net | Private (CT) |
    94 | 18 | Spinal cord: 0.897; pituitary gland: 0.608; oral cavity: 0.908; left
    eye: 0.907; right eye: 0.902; left lens: 0.724; right lens: 0.689; left TMJ: 0.789;
    right TMJ: 0.778; left temporal lobe: 0.803; right temporal lobe: 0.802 |'
  prefs: []
  type: TYPE_TB
- en: '| Isler et al. [[134](#bib.bib134)] | 2D U-Net | OpenKBP (CT) [[36](#bib.bib36)]
    | 188 | 5 | Spinal cord: 0.750 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XII: DSC-Based Summary of the Literature on Multi-Organ Single-State
    Segmentation Methods for the Abdomen-Supplementary Material'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Network | Datasets | Quantity | Organ type | Other organs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[40](#bib.bib40)] | 3D CNN | TCIA [[200](#bib.bib200), [201](#bib.bib201)]
    &BTCV [[29](#bib.bib29)] (CT) | 72 | 4 | Esophagus: 0.730 |'
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[115](#bib.bib115)] | 2D CNN | Private (CT) | 278 | 5 | Bladder:
    0.934; Intestine: 0.653; Left femoral head: 0.921; Right femoral head: 0.923;
    Colon: 0.618 |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[157](#bib.bib157)] | 3D U-Net | Private (CT) | 377 | 7 | Artery:
    0.892; Vein: 0.793 |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[57](#bib.bib57)] | 3D V-Net | TCIA [[200](#bib.bib200), [201](#bib.bib201)]
    &BTCV [[29](#bib.bib29)] (CT) | 90 | 8 | Duodenum 0.630; Esophagus: 0.760 |'
  prefs: []
  type: TYPE_TB
- en: '| Roth et al. [[52](#bib.bib52)] | 3D U-Net | Private (CT) | 377 | 7 | Artery:
    0.835; Vein 0.805 |'
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[65](#bib.bib65)] | 2D FCN | Private (CT) | 120 | 16 | Aorta:
    0.810; Adrenal gland: 0.368; Celiac AA: 0.385; Duodenum:0.649; Colon: 0.776; Inferior
    vena cava: 0.786; Superior mesenteric artery: 0.496; Small bowel: 0.729; Veins:
    0.651 |'
  prefs: []
  type: TYPE_TB
- en: '| Heinrich et al. [[147](#bib.bib147)] | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 43 | 8 | Left adrenal gland: 0.942; duodenum: 0.538; Esophagus:
    0.633 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Private (CT) | 10 | 7 | Spleen, pancreas, kidney, gallbladder, esophagus,
    liver, stomach and duodenum average DSC: 0.823 |'
  prefs: []
  type: TYPE_TB
- en: '| Hatamizadeh et al. [[87](#bib.bib87)] | 3D Transformer And U-Net | BTCV (CT)
    [[29](#bib.bib29)] | 30 | 13 | Esophagus: 0.864; aorta: 0.948; Inferior vena cava:
    0.890; vein: 0.858 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[117](#bib.bib117)] | 2.5D U-Net | Private (MR) | 102 | 10 |
    Duodenum: 0.801; Small Intestine: 0.870; Spinal Cord: 0.904; Vertebral Body: 0.900
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[124](#bib.bib124)] | 2.5D U-Net | ABD-110 (CT) [[124](#bib.bib124)]
    | 110 | 11 | Large intestine: 0.825; small intestine 0.765; duodenum 0.707; spinal
    cord 0.908 |'
  prefs: []
  type: TYPE_TB
- en: '| Jia and Wei [[53](#bib.bib53)] | 3D U-Net | CHAOS (CT) [[32](#bib.bib32)]
    | 20 | 4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[142](#bib.bib142)] | 3D U-Net | TCIA [[200](#bib.bib200), [201](#bib.bib201)]
    & BTCV(CT) [[29](#bib.bib29)] | 90 | 8 | Duodenum 0.637; esophagus 0.733 |'
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. [[77](#bib.bib77)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta 0.855 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[86](#bib.bib86)] | 2D Transformer And U-Net | Synapse (CT)
    [[2](#bib.bib2)] | 30 | 8 | Aorta 0.872 |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[161](#bib.bib161)] | 2D CNN | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta 0.903 |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[78](#bib.bib78)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta: 0.870 |'
  prefs: []
  type: TYPE_TB
- en: '| Suo et al. [[84](#bib.bib84)] | 2D Transformer And U-Net | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta: 0.881 |'
  prefs: []
  type: TYPE_TB
- en: '| Berzoini et al. [[54](#bib.bib54)] | 2D U-Net | Open-source CT-org dataset
    [[203](#bib.bib203)] | 140 | 6 | Lung: 0.967; bladder: 0.836; bone: 0.944 |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[148](#bib.bib148)] | 2D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 42 | 5 | Duodenum: 0.615 |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[79](#bib.bib79)] | 3D CNN And Transformer | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 11 | Esophagus: 0.780; aorta: 0.912; Inferior vena cava: 0.880; Portal
    vein and splenic vein: 0.781; |'
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta: 0.909 |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[198](#bib.bib198)] | 2D U-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 12 | Esophagus: 0.807; aorta: 0.913; Inferior vena cava: 0.850; Portal
    vein and splenic vein: 0.809; adrenal gland: 0.691 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIII: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Head and Neck-Supplementary Material'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Organ type | Other organs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. [[70](#bib.bib70)] | 2D FCN | 3D U-Net | Private (CT) | 56 |
    14 | Right eyeball: 0.634; Left eyeball: 0.636; Lips: 0.676; Oral Cavity: 0.829;
    throat: 0.389; Esophagus: 0.735; Thyroid gland: 0.642; spinal cord: 0.782 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIV: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Abdomen'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse network | Fine network | Dataset | Quantity | Category | Other
    organs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Roth [[103](#bib.bib103)] | 3D FCN | 3D FCN | Private (CT) | 331 | 3 | Artery:
    0.796; vein: 0.731 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang [[111](#bib.bib111)] | 2.5D FCN | 2.5D FCN | Private (CT) | 236 | 13
    | Aorta: 0.918; colon: 0.830; duodenum: 0.754; Inferior vena cava: 0.870; small
    intestine: 0.801; vein: 0.807 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang [[123](#bib.bib123)] | 3D V-Net | 3D V-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | Esophagus: 0.691; aorta: 0.877; Inferior vena cava: 0.865; Portal
    vein and splenic vein: 0.688; right adrenal gland: 0.651; left adrenal gland:
    0.619 |'
  prefs: []
  type: TYPE_TB
- en: '| Xie [[94](#bib.bib94)] | 2.5D FCN | 2.5D FCN | Private (CT) | 200 | 16 |
    Aorta: 0.937; adrenal gland: 0.630; abdominal cavity: 0.620; duodenum: 0.735;
    Inferior vena cava: 0.837; Vascular: 0.742; small intestine: 0.751; vein: 0.748;
    Colon: 0.800 |'
  prefs: []
  type: TYPE_TB
- en: '| Lee [[114](#bib.bib114)] | 3D U-Net | 3D U-Net | BTCV (CT) [[29](#bib.bib29)]
    | 47 | 8 | Esophagus: 0.783; aorta: 0.916; Inferior vena cava: 0.856; Portal vein
    and splenic vein: 0.762; RAD: 0.741; LAD: 0.746 |'
  prefs: []
  type: TYPE_TB
- en: '| Kakeya [[108](#bib.bib108)] | 3D U-Net | 3D U-Net | Private (CT) | 47 | 8
    | Inferior vena cava: 0.908; Aorta: 0.969 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XV: DSC-Based Summary of the Literature on Multi-Organ Localization and
    Segmentation Methods for the Head and Neck-Supplementary Material'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Other
    organs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[102](#bib.bib102)] | 3D U-Net | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 100 | 7 | Spinal cord: 0.910; Left eye: 0.930; Right eye:
    0.920 |'
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[105](#bib.bib105)] | 3D U-Net | 3D U-Net | Private (CT) | 215
    | 28 | Brachial plexus: 0.562; pharyngeal constrictor: 0.755; left ear: 0.773;
    right ear: 0.786; left eye: 0.925; right eye: 0.925; pituitary gland: 0.639; larynx:
    0.893; left lens: 0.819; right lens: 0.830; oral cavity: 0.908; spinal cord: 0.856;
    sublingual gland: 0.460; left temporal lobe: 0.848; right temporal lobe: 0.841;
    thyroid: 0.856; left temporomandibular joint: 0.880; right temporomandibular joint:
    0.869; trachea: 0.813 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[212](#bib.bib212)] | 3D CNN | 2D U-Net | Private (CT) | 88
    | 17 | Left eye: 0.875; right eye: 0.889; left lens: 0.747; right lens: 0.698;
    cerebellum: 0.936; pituitary: 0.672; thyroid: 0.844; Temporal lobe left: 0.762;
    Temporal lobe right: 0.784; brain: 0.976; head: 0.943 |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[112](#bib.bib112)] | 2D CNN | 2D CNN | Private (CT) | 185
    | 18 | Left eye: 0.932; right eye: 0.936; left lens: 0.930; right lens: 0.842;
    larynx: 0.870; oral cavity: 0.928; left mastoid: 0.821; right mastoid: 0.824;
    spinal cord: 0.884; left TMJ: 0.846; right TMJ: 0.844; |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[107](#bib.bib107)] | 3D CNN | 3D CNN | Private (CT) | 50 | 18
    | Left eye: 0.876; right eye: 0.912; oral cavity: 0.792; larynx: 0.658; spinal
    cord: 0.874; left lens: 0.808; right lens: 0.790; pituitary gland: 0.769; left
    middle ear: 0.567; right middle ear: 0.522; left TMJ: 0.584; right TMJ: 0.572
    |'
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 96 | 11 | Left eye: 0.930; right eye: 0.930; spinal cord:
    0.900; left lens: 0.872; right lens: 0.883; |'
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[103](#bib.bib103)] | 3D CNN | 3D U-Net | Private (CT) | 15 |
    8 | Esophagus: 0.840; Throat: 0.790; Oral: 0.890; Pharynx: 0.850; spinal cord:
    0.890 |'
  prefs: []
  type: TYPE_TB
- en: '| Korte et al. [[106](#bib.bib106)] | 3D U-Net | 3D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Public RT-MAC dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (MRI) [[213](#bib.bib213)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 43 | 8 | Secondary lymph nodes (left): 0.708; secondary lymph nodes (right):
    0.715; tertiary lymph nodes (left): 0.561; tertiary lymph nodes (right): 0.573;
    |'
  prefs: []
  type: TYPE_TB
- en: '| Private (MRI) | 10 | 8 | Secondary lymph nodes (left): 0.553; Secondary lymph
    nodes (right): 0.525; Tertiary lymph nodes (left): 0.304; Tertiary lymph nodes
    (right): 0.189; |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[69](#bib.bib69)] | 3D CNN | 3D CNN | Private (CT) | 1164 | 22
    | Left eye: 0.897; right eye: 0.895; left lens: 0.819; right lens: 0.825; pituitary
    gland: 0.722; left temporal lobe: 0.877; right temporal lobe: 0.883; spinal cord:
    0.831; left inner ear: 0.864; right inner ear: 0.855; left middle ear: 0.857;
    right middle ear: 0.843; left temporomandibular joint: 0.764; right temporomandibular
    joint: 0.789; |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XVI: DSC-Based Summary of the Literature on Multi-Organ Localization
    and Segmentation Methods for the Abdomen-Supplementary Material'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Other
    organs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Larsson et al. [[108](#bib.bib108)] | Multi-Atlas | 3D FCN | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | Esophagus: 0.588; aorta: 0.870; Inferior vena cava: 0.758; Portal
    vein and splenic vein: 0.715; Right adrenal gland: 0.630; Left adrenal gland:
    0.631 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[109](#bib.bib109)] | Registration | 2D U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VISCERAL challenge dataset Nonenhanced &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTwb) [[214](#bib.bib214)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | Left adrenal gland: 0.472; Right adrenal gland: 0.390 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VISCERAL challenge dataset enhanced &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTce) [[214](#bib.bib214)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | Left adrenal gland: 0.403; Right adrenal gland: 0.434 |'
  prefs: []
  type: TYPE_TB
