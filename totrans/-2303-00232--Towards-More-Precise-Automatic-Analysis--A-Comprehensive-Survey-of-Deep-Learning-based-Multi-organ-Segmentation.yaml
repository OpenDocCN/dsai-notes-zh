- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:41:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:41:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2303.00232] Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2303.00232] 朝着更精确的自动分析：深度学习基础的多脏器分割的综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.00232](https://ar5iv.labs.arxiv.org/html/2303.00232)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2303.00232](https://ar5iv.labs.arxiv.org/html/2303.00232)
- en: 'Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝着更精确的自动分析：深度学习基础的多脏器分割的综合调查
- en: 'Xiaoyu Liu, Linhao Qu, Ziyue Xie, Jiayue Zhao, Yonghong Shi, and Zhijian Song
    X. Liu, L. Qu, Z. Xie, J. Zhao, Y. Shi and Z. Song are with the Digital Medical
    Research Center, School of Basic Medical Sciences, Fudan University, and also
    with the Shanghai Key Laboratory of Medical Imaging Computing and Computer Assisted
    Intervention, Shanghai, 200032, China. (e-mail: (liuxiaoyu21, zyxie22, jiayuezhao22)
    @m.fudan.edu.cn; (lhqu20, yonghong.shi, zjsong) @fudan.edu.cn);X. Liu and L. Qu
    are co-first authors. Y. Shi and Z. Song are co-corresponding authors.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '刘晓宇、曲林浩、谢紫月、赵佳月、石永宏和宋智坚 X. 刘、L. 曲、Z. 谢、J. 赵、Y. 石和Z. 宋均隶属于复旦大学基础医学院数字医学研究中心，并与上海市医学影像计算与计算机辅助干预重点实验室（中国上海，200032）相关。(电子邮件:
    (liuxiaoyu21, zyxie22, jiayuezhao22) @m.fudan.edu.cn; (lhqu20, yonghong.shi, zjsong)
    @fudan.edu.cn); X. 刘和L. 曲为共同第一作者。Y. 石和Z. 宋为共同通讯作者。'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Accurate segmentation of multiple organs of the head, neck, chest, and abdomen
    from medical images is an essential step in computer-aided diagnosis, surgical
    navigation, and radiation therapy. In the past few years, with a data-driven feature
    extraction approach and end-to-end training, automatic deep learning-based multi-organ
    segmentation method has far outperformed traditional methods and become a new
    research topic. This review systematically summarizes the latest research in this
    field. For the first time, from the perspective of full and imperfect annotation,
    we comprehensively compile 161 studies on deep learning-based multi-organ segmentation
    in multiple regions such as the head and neck, chest, and abdomen, containing
    a total of 214 related references. The method based on full annotation summarizes
    the existing methods from four aspects: network architecture, network dimension,
    network dedicated modules, and network loss function. The method based on imperfect
    annotation summarizes the existing methods from two aspects: weak annotation-based
    methods and semi annotation-based methods. We also summarize frequently used datasets
    for multi-organ segmentation and discuss new challenges and new research trends
    in this field.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从医学图像中准确分割头部、颈部、胸部和腹部的多个脏器是计算机辅助诊断、外科导航和放射治疗中的一个重要步骤。在过去几年中，利用数据驱动的特征提取方法和端到端训练，自动深度学习基础的多脏器分割方法已经远远超越了传统方法，并成为一个新的研究课题。本综述系统地总结了该领域的最新研究。首次从完整和不完整注释的角度，我们全面编制了161项关于深度学习基础的多脏器分割研究，涵盖头部和颈部、胸部和腹部等多个区域，总共包含214个相关参考文献。基于完整注释的方法从网络架构、网络维度、网络专用模块和网络损失函数四个方面总结了现有的方法。基于不完整注释的方法从弱注释方法和半注释方法两个方面总结了现有的方法。我们还总结了多脏器分割中常用的数据集，并讨论了该领域的新挑战和新研究趋势。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: abdomen multi-organ, chest multi-organ, deep learning, head and neck multi-organ,
    multi-organ segmentation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 腹部多脏器，胸部多脏器，深度学习，头部和颈部多脏器，多脏器分割。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Accurate segmentation of multiple organs of the head and neck, chest, abdomen
    as well as other parts from medical images is crucial in computer-aided diagnosis,
    surgical navigation, and radiotherapy [[1](#bib.bib1), [2](#bib.bib2)]. For example,
    radiotherapy, which targets tumour masses and microscopic areas with high risk
    of tumour proliferation, is a common treatment option for cancer patients [[3](#bib.bib3)].
    However, radiotherapy can bring great risk to the normal organs around the tumour,
    which are known as organs at risk (OARs). Thus, accurate segmentation of tumour
    contours and OARs is necessary [[4](#bib.bib4), [5](#bib.bib5)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从医学图像中准确分割头部和颈部、胸部、腹部以及其他部位的多个脏器对于计算机辅助诊断、外科导航和放射治疗至关重要[[1](#bib.bib1), [2](#bib.bib2)]。例如，放射治疗针对肿瘤团块和高风险肿瘤扩散的微观区域，是癌症患者的常见治疗选择[[3](#bib.bib3)]。然而，放射治疗可能对肿瘤周围的正常脏器带来很大的风险，这些脏器被称为风险脏器（OARs）。因此，准确分割肿瘤轮廓和OARs是必要的[[4](#bib.bib4),
    [5](#bib.bib5)]。
- en: The early segmentation process relied heavily on manual labelling by physicians,
    which is a labour-intensive and time-consuming process. For example, a trained
    specialist may spend more than four hours manually labelling a case, which not
    only places a heavy burden on the healthcare system but also likely causes a delay
    in the radiotherapy for a patient. Moreover, different physicians or hospitals
    will have different results of labelling [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. Therefore, accurate automatic multi-organ segmentation method
    is urgently needed in clinical practice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的分割过程严重依赖医生手动标记，这是一项劳动密集且耗时的过程。例如，一位经过培训的专家可能需要花费超过四个小时手动标记一个病例，这不仅对医疗系统造成了沉重的负担，还可能导致患者放射治疗的延误。此外，不同的医生或医院会有不同的标记结果
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。因此，临床实践中迫切需要准确的自动化多脏器分割方法。
- en: Multi-organ segmentation is a challenging task. First, the contour of the anatomical
    structure in image is highly variable, which is difficult to expressed by a unified
    mathematical rule. Second, the boundaries between different organs or tissue regions
    in an image are often blurred due to image noise and low intensity contrast, and
    these boundaries are difficult to identify using techniques of traditional digital
    image processing. Third, the use of different scanners, scanning protocols, and
    contrast agents will lead to different intensity distributions of organs in the
    obtained images, which poses a great challenge to the generalizability of the
    model. Finally, considering safety and ethical issues, many hospitals do not disclose
    their datasets. Many segmentation methods are trained and validated on private
    datasets, making it difficult to compare different methods. Therefore, designing
    accurate and robust multi-organ segmentation models is a very difficult and expensive
    task.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多脏器分割是一项具有挑战性的任务。首先，图像中解剖结构的轮廓高度可变，这难以用统一的数学规则表达。其次，由于图像噪声和低强度对比度，图像中不同器官或组织区域之间的边界往往模糊，这些边界难以通过传统数字图像处理技术识别。第三，使用不同的扫描仪、扫描协议和对比剂会导致获得图像中器官强度分布的差异，这对模型的泛化能力提出了很大挑战。最后，考虑到安全和伦理问题，许多医院不公开其数据集。许多分割方法在私人数据集上进行训练和验证，使得不同方法的比较变得困难。因此，设计准确且稳健的多脏器分割模型是一项非常困难且昂贵的任务。
- en: Traditional methods [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]
    usually utilize manually extracted image features for image segmentation, such
    as the threshold [[14](#bib.bib14)] method, graph cut [[15](#bib.bib15)] method,
    and region growth [[16](#bib.bib16)] method. Limited by a large number of manually
    extracted image features and the selection of non-robust thresholds or seeds,
    the segmentation results of these methods are usually unstable, and often yield
    only a rough segmentation result or only apply to specific organs. Knowledge-based
    methods can obtain anatomical information of different organs from labelled datasets,
    reduce the burden of manual feature extraction, and improve the robustness and
    accuracy of multi-organ segmentation, which commonly include multi-atlas label
    fusion [[17](#bib.bib17), [18](#bib.bib18)] and statistical shape models [[19](#bib.bib19),
    [20](#bib.bib20)]. The method based on multi-atlas label fusion-based uses image
    alignment to align predefined structural contours to the image to be segmented,
    and this method typically includes multiple steps. Therefore, the performance
    of this method may be influenced by various relevant factors involved in each
    step. The atlas-based method is still very popular, but due to the use of fixed
    atlases, it is difficult to handle the anatomical variation of organs between
    patients. In addition, it is computationally intensive and takes a long time to
    complete an alignment task. The statistical shape model uses the positional relationships
    between different organs, and uses the shape of the organs in the statistical
    space as a constraint to regularize the segmentation results. However, the accuracy
    of this approach is largely dependent on the reliability and extensibility of
    the shape model, and the model based on normal anatomical structures has very
    limited effect in the segmentation of irregular structures.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法[[10](#bib.bib10)、[11](#bib.bib11)、[12](#bib.bib12)、[13](#bib.bib13)]通常利用手动提取的图像特征进行图像分割，例如阈值[[14](#bib.bib14)]方法、图割[[15](#bib.bib15)]方法和区域生长[[16](#bib.bib16)]方法。由于手动提取的图像特征数量庞大以及非鲁棒性阈值或种子的选择，这些方法的分割结果通常不稳定，往往只能得到粗略的分割结果或仅适用于特定器官。基于知识的方法可以从标注的数据集中获取不同器官的解剖信息，减少手动特征提取的负担，并提高多器官分割的鲁棒性和准确性，这些方法通常包括多图谱标签融合[[17](#bib.bib17)、[18](#bib.bib18)]和统计形状模型[[19](#bib.bib19)、[20](#bib.bib20)]。基于多图谱标签融合的方法使用图像配准将预定义的结构轮廓对齐到待分割图像上，该方法通常包括多个步骤。因此，该方法的性能可能受到每个步骤中涉及的各种相关因素的影响。基于图谱的方法仍然非常流行，但由于使用了固定的图谱，处理不同患者之间器官的解剖变异比较困难。此外，它计算密集且完成配准任务的时间较长。统计形状模型利用不同器官之间的位置关系，并将统计空间中器官的形状作为约束来规范化分割结果。然而，这种方法的准确性在很大程度上依赖于形状模型的可靠性和可扩展性，而基于正常解剖结构的模型在不规则结构的分割中效果非常有限。
- en: 'Using data-driven feature extraction approach and end-to-end training, the
    methods based on deep learning (DL) have been widely studied in the fields of
    image classification [[21](#bib.bib21)], object detection [[22](#bib.bib22)] and
    image segmentation [[23](#bib.bib23), [24](#bib.bib24)], image fusion [[25](#bib.bib25)],
    image registration [[26](#bib.bib26)], etc. The segmentation method based on deep
    learning has become a mainstream method in the field of medical image processing.
    However, there are two main difficulties in multi-organ deep learning segmentation
    tasks. First, as shown in the head and neck in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"), the abdomen in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"), the chest in Fig. [3](#S1.F3 "Figure 3 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"), and the statistics of the multi-organ size in each
    part in Fig. [4](#S1.F4 "Figure 4 ‣ I Introduction ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation"),
    there are very large differences between the organs sizes, and the serious imbalances
    of different organs sizes will lead to a poor segmentation performance of the
    trained segmentation network for small organs. Second, due to the imaging principle
    of CT technology and the complex anatomical structure of the human body, the contrast
    between organs and their surrounding tissues is often low, which leads to the
    inaccurate segmentation of organ boundaries by segmentation networks. Therefore,
    it has become a new hot research topic to develop deep multi-organ segmentation
    methods that can accurately segment small and large organs at the same time.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '采用数据驱动的特征提取方法和端到端训练的深度学习（DL）方法已在图像分类 [[21](#bib.bib21)]、目标检测 [[22](#bib.bib22)]、图像分割
    [[23](#bib.bib23), [24](#bib.bib24)]、图像融合 [[25](#bib.bib25)]、图像配准 [[26](#bib.bib26)]
    等领域得到广泛研究。基于深度学习的分割方法已成为医学图像处理领域的主流方法。然而，多脏器深度学习分割任务面临两大主要困难。首先，如图 [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") 所示的头部和颈部、图 [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") 的腹部、图 [3](#S1.F3 "Figure
    3 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") 的胸部以及图 [4](#S1.F4 "Figure
    4 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") 的各部分多脏器尺寸统计，器官尺寸之间差异非常大，不同器官尺寸的严重不平衡会导致训练的分割网络对小器官的分割性能差。其次，由于
    CT 技术的成像原理和人体复杂的解剖结构，器官与其周围组织之间的对比度通常较低，这导致分割网络对器官边界的分割不准确。因此，开发能够同时准确分割小器官和大器官的深度多脏器分割方法已成为新的研究热点。'
- en: '![Refer to caption](img/0d07a30c9687e01b624aa99e1a2c0d2f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d07a30c9687e01b624aa99e1a2c0d2f.png)'
- en: 'Figure 1: Schematic diagram of the organs of the head and neck, where the numbers
    are arranged in order: (1) brainstem, (2) left eye, (3) right eye, (4) left lens,
    (5) right lens, (6) left optic nerve, (7) right optic nerve, (8) Optical chiasm,
    (9) left temporal lobe, (10) right temporal lobe, (11) pituitary gland, (12) left
    parotid gland, (13) right parotid gland, (14) left temporal bone rock, (15) right
    temporal bone rock, (16) left temporal bone, (17) right temporal bone, (18) left
    mandibular condyle, (19) right mandibular condyle, (20) spinal cord, (21) left
    mandible, (22) right mandible.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：头部和颈部器官示意图，数字按顺序排列：（1）脑干，（2）左眼，（3）右眼，（4）左晶状体，（5）右晶状体，（6）左视神经，（7）右视神经，（8）视交叉，（9）左颞叶，（10）右颞叶，（11）垂体，（12）左腮腺，（13）右腮腺，（14）左颞骨岩部，（15）右颞骨岩部，（16）左颞骨，（17）右颞骨，（18）左下颌髁，（19）右下颌髁，（20）脊髓，（21）左下颌骨，（22）右下颌骨。
- en: '![Refer to caption](img/c3f3143907aed61b1bd2b904bd15502e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c3f3143907aed61b1bd2b904bd15502e.png)'
- en: 'Figure 2: Schematic diagram of the thoracic organs, where the numbers are arranged
    in order: (1) left lung, (2) right lung, (3) heart, (4) esophagus, (5) trachea,
    and (6) spinal cord.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：胸腔器官示意图，数字按顺序排列：（1）左肺，（2）右肺，（3）心脏，（4）食道，（5）气管，以及（6）脊髓。
- en: '![Refer to caption](img/c8b11ae81f55dd3a86d9ddc6e9c37d81.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8b11ae81f55dd3a86d9ddc6e9c37d81.png)'
- en: 'Figure 3: Schematic diagram of the abdominal organs, where the numbers are
    arranged in order: (1) liver, (2) kidney, (3) spleen, (4) pancreas, (5) aorta,
    (6) inferior vena cava, (7) stomach, (8) gallbladder, (9) esophagus, (10) right
    adrenal gland, (11) left adrenal gland, and (12) celiac artery.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：腹部器官示意图，其中数字按顺序排列：（1）肝脏，（2）肾脏，（3）脾脏，（4）胰腺，（5）主动脉，（6）下腔静脉，（7）胃，（8）胆囊，（9）食管，（10）右肾上腺，（11）左肾上腺，以及（12）腹腔动脉。
- en: '![Refer to caption](img/9b458d3b44540ecab196982abf7294a8.png)![Refer to caption](img/e3b4373e1a597be439c4e89396ad4d7f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b458d3b44540ecab196982abf7294a8.png)![参见说明](img/e3b4373e1a597be439c4e89396ad4d7f.png)'
- en: 'Figure 4: Illustration of the percentage of voxels in each organ of the head
    and neck (a), chest (b), and abdomen (c), respectively.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：头颈部（a）、胸部（b）和腹部（c）各器官体素百分比的示意图。
- en: Recently, a large number of deep learning-based multi-organ segmentation methods
    with significantly improved performance have been proposed [[27](#bib.bib27)].
    Fu et al. [[28](#bib.bib28)] systematically reviewed the medical image multi-organ
    segmentation methods based on deep learning by 2020 according to the network architecture.
    However, with the rapid development of deep learning technology, more representative
    new techniques and methods have been proposed, such as transformer-based multi-organ
    segmentation methods and imperfect annotation-based methods. A more comprehensive
    review and summary of these techniques and methods are very important for the
    development of this field.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，提出了大量基于深度学习的多脏器分割方法，这些方法性能显著提升[[27](#bib.bib27)]。Fu 等人[[28](#bib.bib28)]
    对2020年前基于深度学习的医学图像多脏器分割方法进行了系统综述，按网络架构分类。然而，随着深度学习技术的迅猛发展，提出了更多具有代表性的新技术和方法，如基于变换器的多脏器分割方法和不完美标注的方法。对这些技术和方法进行更全面的综述和总结对该领域的发展至关重要。
- en: 'This paper reviews deep learning-based multi-organ segmentation method of the
    head, neck, chest and abdomen published from 2016 to 2022\. On Google Scholar,
    a search using the keywords ‘Multi Organ Segmentation’ and ‘Deep Learning’ yielded
    an initial 287 articles, 73 articles were removed according to abstract and keywords,
    and 161 highly relevant studies containing a total of 214 relevant references
    were obtained. Fig. [5](#S1.F5 "Figure 5 ‣ I Introduction ‣ Towards More Precise
    Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ
    Segmentation") summarizes all current state-of-the-art deep learning-based multi-organ
    segmentation methods according to full annotation and imperfect annotation architectures.
    In full annotation-based methods, we summarize the existing methods in four aspects:
    network architecture, network dimension, network dedicated modules, and network
    loss function. In imperfect annotation-based methods, we summarize the existing
    methods in two aspects, weak annotation and semi annotation, to investigate their
    innovation, contribution, and challenges.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '本文回顾了2016至2022年间基于深度学习的头部、颈部、胸部和腹部的多脏器分割方法。在Google Scholar上，使用“Multi Organ
    Segmentation”和“Deep Learning”关键词的搜索初步获得了287篇文章，按摘要和关键词删除了73篇文章，最终获得了161篇高度相关的研究，并包含总计214条相关参考文献。图[5](#S1.F5
    "Figure 5 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation")总结了所有当前最先进的基于深度学习的多脏器分割方法，按照完全标注和不完美标注架构进行分类。在完全标注方法中，我们从网络架构、网络维度、网络专用模块和网络损失函数四个方面总结了现有方法。在不完美标注方法中，我们从弱标注和半标注两个方面总结了现有方法，以探讨它们的创新、贡献和挑战。'
- en: '![Refer to caption](img/c66a3d0d23f66f8a0620d8f11abfb3b3.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c66a3d0d23f66f8a0620d8f11abfb3b3.png)'
- en: 'Figure 5: Framework diagram of the overview.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：概览的框架图。
- en: 'This article is organized as follows. Section [II](#S2 "II Definition and Evaluation
    Metrics ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation") expounds the mathematical definition
    of multi-organ segmentation and the corresponding evaluation metrics. Section
    [III](#S3 "III Multi-organ Segmentation Datasets ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    summarizes the multi-organ segmentation datasets. Section [IV](#S4 "IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation") describes the literature based on full
    annotation-based methods, involving four parts: network architecture (section
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").A), network
    dimension (section [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise
    Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ
    Segmentation").B), network dedicated modules (section [IV](#S4 "IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation").C), and network loss function (section
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").D). Section
    V analyzes the articles based on imperfect annotation methods, including two parts:
    weak annotation-based methods (section [V](#S5 "V Imperfect Annotation-based Methods
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation").A) and semi annotation-based methods (section [V](#S5
    "V Imperfect Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").B). We
    discuss the existing methods and their future outlooks in section [VI](#S6 "VI
    Discussion and Future Trends ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation"), and conclude the whole
    paper in section [VII](#S7 "VII Conclusion ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '本文组织结构如下。第 [II](#S2 "II Definition and Evaluation Metrics ‣ Towards More Precise
    Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ
    Segmentation") 节阐述了多脏器分割的数学定义及相应的评估指标。第 [III](#S3 "III Multi-organ Segmentation
    Datasets ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") 节总结了多脏器分割的数据集。第 [IV](#S4 "IV Full
    Annotation-based Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") 节描述了基于完整注释方法的文献，涉及四个部分：网络架构（第
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").A 节）、网络维度（第
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").B 节）、网络专用模块（第
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").C 节）以及网络损失函数（第
    [IV](#S4 "IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation").D 节）。第
    V 节分析了基于不完全注释方法的文章，包括两个部分：弱注释方法（第 [V](#S5 "V Imperfect Annotation-based Methods
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation").A 节）和半注释方法（第 [V](#S5 "V Imperfect Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation").B 节）。我们在第 [VI](#S6 "VI Discussion and
    Future Trends ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation") 节讨论现有方法及其未来展望，并在第 [VII](#S7
    "VII Conclusion ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation") 节总结全文。'
- en: II Definition and Evaluation Metrics
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 定义与评估指标
- en: Let $\boldsymbol{X}$ represent the union of multiple organ regions in the input
    images, $\boldsymbol{G}$ represent the union of ground truth labels of multiple
    organs in the input images, $\boldsymbol{P}$ represent the union of predicted
    labels of multiple organs in the output images, $\boldsymbol{x_{i}^{c}\in X,g_{i}^{c}\in
    G,p_{i}^{c}\in P,i=1,\cdots N}$, and $\boldsymbol{c=1,\cdots C}$, where $\boldsymbol{N}$
    represents the number of pixel in the image, $\boldsymbol{C}$ represents the number
    of categories to which the pixels belong, $\boldsymbol{f}$ represents the neural
    network, and $\boldsymbol{\theta}$ represents the parameters of the neural network
    optimization, where $\boldsymbol{P=f(X;\theta)}$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\boldsymbol{X}$表示输入图像中多个器官区域的并集，$\boldsymbol{G}$表示输入图像中多个器官的真实标签的并集，$\boldsymbol{P}$表示输出图像中多个器官的预测标签的并集，$\boldsymbol{x_{i}^{c}\in
    X,g_{i}^{c}\in G,p_{i}^{c}\in P,i=1,\cdots N}$，以及$\boldsymbol{c=1,\cdots C}$，其中$\boldsymbol{N}$表示图像中的像素数量，$\boldsymbol{C}$表示像素所属的类别数量，$\boldsymbol{f}$表示神经网络，$\boldsymbol{\theta}$表示神经网络优化的参数，其中$\boldsymbol{P=f(X;\theta)}$。
- en: 'The loss function represents the gap between the predicted and true values.
    In the multi-organ segmentation task, common loss functions include the cross-entropy
    loss and Dice loss. Section [IV-D](#S4.SS4 "IV-D Network Loss Function ‣ IV Full
    Annotation-based Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation") provides specific details
    about the loss function.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '损失函数表示预测值与真实值之间的差距。在多脏器分割任务中，常见的损失函数包括交叉熵损失和Dice损失。第[IV-D](#S4.SS4 "IV-D Network
    Loss Function ‣ IV Full Annotation-based Methods ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")节提供了关于损失函数的具体细节。'
- en: 'Given a multi-organ segmentation task, $\left\{\boldsymbol{\Psi}\right\}$ represents
    the represents the class set of organs to be segmented. $\left\{\boldsymbol{x}\right\}_{\ast}$
    represents the set of organs annotated in $\boldsymbol{x}$. According to the available
    annotations, multi-organ segmentation can be implemented according to three learning
    paradigms: full annotation-based learning, weak annotation-based learning, and
    semi annotation-based learning. The last two are called imperfect annotation-based
    methods, as shown in Fig. [6](#S2.F6 "Figure 6 ‣ II Definition and Evaluation
    Metrics ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation"). Full annotation-based learning means
    that the labels of all organ are given, which indicates that $\forall\boldsymbol{x}\in\boldsymbol{X},\{\boldsymbol{x}\}_{*}=\{\boldsymbol{\Psi}\}$
    . Weak annotation often means that the data come from $\boldsymbol{n}$ different
    datasets. However, each dataset provides the annotations of one or more organs
    but not all organs, which means that $\boldsymbol{X=X_{1}\cup X_{2}\cup\cdots\cup
    X_{n},\forall x_{k,i}\in X_{k},k=1,2,\ldots n}$, $\boldsymbol{\left\{x_{k,i}\right\}_{*}<\{\Psi\},\bigcup_{k=1}^{n}\left\{x_{k,i}\right\}_{*}=\{\Psi\}}$.
    Here, $\boldsymbol{x_{k,i}}$ denotes the $\boldsymbol{i}$-th image in $\boldsymbol{X_{k}}$.
    Semi annotation-based methods indicate that some of the training datasets are
    fully labelled and others are unlabelled, $\boldsymbol{X}=\boldsymbol{X}_{\boldsymbol{l}}\cup\boldsymbol{X_{u}}\cdot\boldsymbol{X}_{\boldsymbol{l}}$.
    $\boldsymbol{X_{l}}$ represents the fully labelled dataset, $\boldsymbol{X_{u}}$
    represents the unlabelled dataset, which indicates that $\forall\boldsymbol{x_{l}\in
    X}_{\boldsymbol{l}},\left\{\boldsymbol{x}_{\boldsymbol{l}}\right\}_{*}=\{\boldsymbol{\Psi}\}$
    and $\forall\boldsymbol{x}_{\boldsymbol{u}}\in\boldsymbol{X}_{\boldsymbol{u}},\left\{\boldsymbol{x}_{\boldsymbol{u}}\right\}_{*}=\boldsymbol{\phi}$,
    and the size of $\boldsymbol{X_{l}}$ is far less than the one of $\boldsymbol{X_{u}}$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个多脏器分割任务，$\left\{\boldsymbol{\Psi}\right\}$ 表示要分割的器官类别集合。$\left\{\boldsymbol{x}\right\}_{\ast}$
    表示在 $\boldsymbol{x}$ 中标注的器官集合。根据现有的标注，可以通过三种学习范式实现多脏器分割：完全标注学习、弱标注学习和半标注学习。后两者称为不完全标注方法，如图
    [6](#S2.F6 "图 6 ‣ II 定义与评估指标 ‣ 迈向更精确的自动分析：基于深度学习的多脏器分割的全面调查") 所示。完全标注学习意味着所有器官的标签都已给出，这表示
    $\forall\boldsymbol{x}\in\boldsymbol{X},\{\boldsymbol{x}\}_{*}=\{\boldsymbol{\Psi}\}$。弱标注通常意味着数据来自
    $\boldsymbol{n}$ 个不同的数据集。然而，每个数据集提供一个或多个器官的标注，而不是所有器官，这意味着 $\boldsymbol{X=X_{1}\cup
    X_{2}\cup\cdots\cup X_{n},\forall x_{k,i}\in X_{k},k=1,2,\ldots n}$，$\boldsymbol{\left\{x_{k,i}\right\}_{*}<\{\Psi\},\bigcup_{k=1}^{n}\left\{x_{k,i}\right\}_{*}=\{\Psi\}}$。这里，$\boldsymbol{x_{k,i}}$
    表示 $\boldsymbol{X_{k}}$ 中的第 $\boldsymbol{i}$ 张图像。半标注方法表示一些训练数据集是完全标注的，而其他数据集是未标注的，$\boldsymbol{X}=\boldsymbol{X}_{\boldsymbol{l}}\cup\boldsymbol{X_{u}}\cdot\boldsymbol{X}_{\boldsymbol{l}}$。$\boldsymbol{X_{l}}$
    表示完全标注的数据集，$\boldsymbol{X_{u}}$ 表示未标注的数据集，这表示 $\forall\boldsymbol{x_{l}\in X}_{\boldsymbol{l}},\left\{\boldsymbol{x}_{\boldsymbol{l}}\right\}_{*}=\{\boldsymbol{\Psi}\}$
    和 $\forall\boldsymbol{x}_{\boldsymbol{u}}\in\boldsymbol{X}_{\boldsymbol{u}},\left\{\boldsymbol{x}_{\boldsymbol{u}}\right\}_{*}=\boldsymbol{\phi}$，且
    $\boldsymbol{X_{l}}$ 的规模远小于 $\boldsymbol{X_{u}}$。
- en: '![Refer to caption](img/a6fdb4f766bb20227e2f8499f6019e74.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6fdb4f766bb20227e2f8499f6019e74.png)'
- en: 'Figure 6: General overview of the learning paradigms reviewed in this paper.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：本文回顾的学习范式的总体概述。
- en: Usually using the Dice similarity coefficient (DSC), 95% Hausdorff distance
    (HD95) and mean surface distance (MSD) to evaluate the performance of the segmentation
    methods. DSC is a measure of the volume overlap between the predicted labels and
    ground truth labels, HD95 and MSD are measures of the surface distance between
    the predicted labels and ground truth labels.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用 Dice 相似系数 (DSC)、95% Hausdorff 距离 (HD95) 和均值表面距离 (MSD) 来评估分割方法的性能。DSC 是预测标签和真实标签之间体积重叠的度量，HD95
    和 MSD 是预测标签和真实标签之间表面距离的度量。
- en: '|  | $\boldsymbol{DSC=\dfrac{2\times\left&#124;P^{c}\cap G^{c}\right&#124;}{\left&#124;P^{c}\right&#124;+\left&#124;G^{c}\right&#124;}}$
    |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{DSC=\dfrac{2\times\left&#124;P^{c}\cap G^{c}\right&#124;}{\left&#124;P^{c}\right&#124;+\left&#124;G^{c}\right&#124;}}$
    |  | (1) |'
- en: '|  | $\boldsymbol{HD95=\max_{95\%}\left[d\left(P_{s}^{c},G_{s}^{c}\right),d\left(G_{s}^{c},P_{s}^{c}\right)\right]}$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{HD95=\max_{95\%}\left[d\left(P_{s}^{c},G_{s}^{c}\right),d\left(G_{s}^{c},P_{s}^{c}\right)\right]}$
    |  | (2) |'
- en: '|  | $MSD=\frac{1}{\left&#124;P_{s}^{c}\right&#124;+\left&#124;G_{s}^{c}\right&#124;}\left(\sum_{p_{s}^{c}\in
    P_{s}^{c}}d\left(p_{s}^{c},G_{s}^{c}\right)+\sum_{g_{s}^{c}\in G_{s}^{c}}d\left(g_{s}^{c},P_{s}^{c}\right)\right)$
    |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSD=\frac{1}{\left&#124;P_{s}^{c}\right&#124;+\left&#124;G_{s}^{c}\right&#124;}\left(\sum_{p_{s}^{c}\in
    P_{s}^{c}}d\left(p_{s}^{c},G_{s}^{c}\right)+\sum_{g_{s}^{c}\in G_{s}^{c}}d\left(g_{s}^{c},P_{s}^{c}\right)\right)$
    |  | （3） |'
- en: where $\boldsymbol{P}^{\boldsymbol{c}}$ and $\boldsymbol{G}^{\boldsymbol{c}}$
    represent the set of predicted pixels and the set of real pixels of the $\boldsymbol{c}$
    class organ, respectively; $\boldsymbol{P_{s}^{c}}$ and $\boldsymbol{G_{s}^{c}}$
    represent the set of predicted pixels and the set of real pixels of the surface
    of the $\boldsymbol{c}$ class organ, respectively; and $\boldsymbol{d\left(p_{s}^{c},G_{s}^{c}\right)=\min_{g_{s}^{c}\in
    G_{s}^{c}}^{c}\left\|p_{s}^{c}-g_{s}^{c}\right\|_{2}}$ represents the minimal
    distance from point $\boldsymbol{p_{s}^{c}}$ to surface $\boldsymbol{G_{s}^{c}}$.
    The review reports various methods based on DSC values.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{P}^{\boldsymbol{c}}$ 和 $\boldsymbol{G}^{\boldsymbol{c}}$ 分别表示
    $\boldsymbol{c}$ 类器官的预测像素集合和真实像素集合；$\boldsymbol{P_{s}^{c}}$ 和 $\boldsymbol{G_{s}^{c}}$
    分别表示 $\boldsymbol{c}$ 类器官表面的预测像素集合和真实像素集合；$\boldsymbol{d\left(p_{s}^{c},G_{s}^{c}\right)=\min_{g_{s}^{c}\in
    G_{s}^{c}}^{c}\left\|p_{s}^{c}-g_{s}^{c}\right\|_{2}}$ 表示点 $\boldsymbol{p_{s}^{c}}$
    到表面 $\boldsymbol{G_{s}^{c}}$ 的最小距离。评审报告了基于 DSC 值的各种方法。
- en: III Multi-organ Segmentation Datasets
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 多器官分割数据集
- en: 'To obtain high-quality organ segmentation datasets, many research teams have
    undertaken several collaborations with medical organizations. Table [I](#S3.T1
    "TABLE I ‣ III Multi-organ Segmentation Datasets ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    summarizes the common head and neck, thorax, and abdomen datasets used for the
    development and validation of multi-organ segmentation method. Table I also shows
    that the quantity of annotated data available for deep learning studies is still
    very low.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得高质量的器官分割数据集，许多研究团队与医疗组织进行了多次合作。表 [I](#S3.T1 "TABLE I ‣ III 多器官分割数据集 ‣ 朝向更精确的自动分析：基于深度学习的多器官分割的全面调查")
    总结了用于多器官分割方法开发和验证的常见头颈部、胸部和腹部数据集。表 I 还显示了用于深度学习研究的注释数据量仍然非常低。
- en: 'TABLE I: Frequently Used Dataset for Multi-organ Segmentation'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：常用的多器官分割数据集
- en: '| Year | Dataset | Modality | Part | Number of organs (specific organs) | Quantity
    | Labelling status | Image size |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 数据集 | 模态 | 部位 | 器官数量（具体器官） | 数量 | 标记状态 | 图像尺寸 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2015 | MICCAI Multi-Atlas Labelling Beyond the Cranial Vault (BTCV) [[29](#bib.bib29)]
    | CT | Abdomen | 13 (spleen, right kidney, left kidney, gallbladder, esophagus,
    liver, stomach, aorta, inferior vena cava, portal and splenic veins, pancreas,
    right adrenal gland, left adrenal gland) | 50 (30 training and 20 testing) | The
    training set are labelled, the test set are not labelled | 512 × 512 × [85$\sim$198]
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | MICCAI 脑颅外多图谱标注（BTCV） [[29](#bib.bib29)] | CT | 腹部 | 13（脾脏、右肾、左肾、胆囊、食道、肝脏、胃、主动脉、下腔静脉、门静脉和脾静脉、胰腺、右肾上腺、左肾上腺）
    | 50（30个用于训练，20个用于测试） | 训练集已标记，测试集未标记 | 512 × 512 × [85$\sim$198] |'
- en: '| 2015 | MICCAI head and neck Auto Segmentation Challenge (HNC) [[30](#bib.bib30)]
    | CT | Head and neck | 9 (brainstem, mandible, chiasm, left optic nerves, right
    optic nerves, left parotid glands, right parotid glands, left submandibular glands,
    right submandibular glands) | 35 (25 training, 10 off-site tests, 5 on-site tests)
    | Labelled | 512 × 512 × [110$\sim$190] |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | MICCAI 头颈部自动分割挑战（HNC） [[30](#bib.bib30)] | CT | 头颈部 | 9（脑干、下颌骨、视交叉、左侧视神经、右侧视神经、左侧腮腺、右侧腮腺、左侧下颌腺、右侧下颌腺）
    | 35（25个用于训练，10个离线测试，5个现场测试） | 已标记 | 512 × 512 × [110$\sim$190] |'
- en: '| 2015 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '&#124; Synapse multi- &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Synapse 多- &#124;'
- en: '&#124; organ segmentation &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 器官分割 &#124;'
- en: '&#124; dataset (Synapse) &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集（Synapse） &#124;'
- en: '| CT | Abdomen | 13 (spleen, right kidney, left kidney, gallbladder, esophagus,
    liver, stomach, aorta, inferior vena cava, portal vein and splenic vein, pancreas,
    right adrenal gland, left adrenal gland) | 50 (30 training, 20 testing) | Labelled
    | 512 × 512 × [85$\sim$198] |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| CT | 腹部 | 13（脾脏、右肾、左肾、胆囊、食道、肝脏、胃、主动脉、下腔静脉、门静脉和脾静脉、胰腺、右肾上腺、左肾上腺） | 50（30个用于训练，20个用于测试）
    | 已标记 | 512 × 512 × [85$\sim$198] |'
- en: '| 2015 | Public Domain Database for Computational Anatomy (PDDCA) [[30](#bib.bib30)]
    | CT | Head and neck | 9 (brainstem, mandible, chiasm, left optic nerves, right
    optic nerves, left parotid glands, right parotid glands, left submandibular glands,
    right submandibular glands) | 48 (25 training, 8 additional training, 10 off-site
    and 5 on-site tests) | Labelled | 512 × 512 × [110$\sim$190] |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 计算解剖学公共领域数据库（PDDCA） [[30](#bib.bib30)] | CT | 头部和颈部 | 9（脑干、下颌骨、视交叉、左侧视神经、右侧视神经、左腮腺、右腮腺、左颌下腺、右颌下腺）
    | 48（25 训练，8 额外训练，10 离线和 5 现场测试） | 标记 | 512 × 512 × [110$\sim$190] |'
- en: '| 2017 | Thoracic Auto-segmentation Challenge (AAPM) [[31](#bib.bib31)] | CT
    | Thorax | 5 (left lung, right lung, heart, Esophagus, spinal cord) | 60 (36 training,
    12 off-site tests, 12 on-site tests) | Labelled | 512 × 512 × [103$\sim$279] |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 胸部自动分割挑战（AAPM） [[31](#bib.bib31)] | CT | 胸部 | 5（左肺、右肺、心脏、食道、脊髓） |
    60（36 训练，12 离线测试，12 现场测试） | 标记 | 512 × 512 × [103$\sim$279] |'
- en: '| 2019 | Combined (CT-MR) Healthy Abdominal Organ Segmentation (CHAOS) [[32](#bib.bib32)]
    | CT | Abdomen | 4 (left kidney, right kidney, liver, spleen) | 40 (20 training
    and 20 testing) | Labelled training set and unlabeled test set | 512 × 512 × [78$\sim$294]
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 组合（CT-MR）健康腹部器官分割（CHAOS） [[32](#bib.bib32)] | CT | 腹部 | 4（左肾、右肾、肝脏、脾脏）
    | 40（20 训练和 20 测试） | 标记训练集和未标记测试集 | 512 × 512 × [78$\sim$294] |'
- en: '| MR | 40 (20 training, 20 testing) × 3 sequences | Labelled training set and
    unlabeled test set | 256 × 256 × [26$\sim$50] |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| MR | 40（20 训练，20 测试） × 3 序列 | 标记训练集和未标记测试集 | 256 × 256 × [26$\sim$50] |'
- en: '| 2019 | SegTHOR Challenge: Segmentation of Thoracic Organs at Risk in CT Images
    (SegTHOR) [[33](#bib.bib33)] | CT | Thorax | 5 (left and right lungs, heart, esophagus,
    spinal cord) | 60 (36 training, 12 off-site tests, 12 on-site tests) | Labelled
    | 512 × 512 × N |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | SegTHOR挑战：CT图像中的胸部风险器官分割（SegTHOR） [[33](#bib.bib33)] | CT | 胸部 | 5（左右肺、心脏、食道、脊髓）
    | 60（36 训练，12 离线测试，12 现场测试） | 标记 | 512 × 512 × N |'
- en: '| 2019 | Annotations for Body Organ Localization based on MICCAI LITS Dataset
    [[34](#bib.bib34)] | CT | Thorax | 11 (heart, left lung, right lung, liver, spleen,
    pancreas, left kidney, right kidney, bladder, left femoral head, right femoral
    head) | 201 (131 training and 70 testing) | Bounding boxes labelled | 512 × 512
    × N |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 基于MICCAI LITS数据集的身体器官定位标注 [[34](#bib.bib34)] | CT | 胸部 | 11（心脏、左肺、右肺、肝脏、脾脏、胰腺、左肾、右肾、膀胱、左股骨头、右股骨头）
    | 201（131 训练和 70 测试） | 边界框标记 | 512 × 512 × N |'
- en: '| 2019 | Automatic Structure Segmentation for Radiotherapy Planning Challenge
    2019 (StructSeg) | CT | Head and neck | 22 (left eye, right eye, left lens, right
    lens, left optical nerve, right optical nerve, chiasm, pituitary, brainstem, left
    temporal lobes, right temporal lobes, spinal cord, left parotid gland, right parotid
    gland, left inner ear, right inner ear, left middle ear, right middle ear, left
    temporomandibular joint, right temporomandibular joint, left mandible, right mandible)
    | 60 (50 training, 10 testing) | Labelled training set and unlabeled test set
    | 512 × 512 × [98$\sim$140] |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 2019年放射治疗规划挑战自动结构分割（StructSeg） | CT | 头部和颈部 | 22（左眼、右眼、左晶状体、右晶状体、左视神经、右视神经、视交叉、脑垂体、脑干、左颞叶、右颞叶、脊髓、左腮腺、右腮腺、左内耳、右内耳、左中耳、右中耳、左颞下颌关节、右颞下颌关节、左下颌骨、右下颌骨）
    | 60（50 训练，10 测试） | 标记训练集和未标记测试集 | 512 × 512 × [98$\sim$140] |'
- en: '| Thorax | 6 (left lung, right lung, spinal cord, esophagus, heart, trachea)
    | 60 (50 training, 10 testing) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 胸部 | 6（左肺、右肺、脊髓、食道、心脏、气管） | 60（50 训练，10 测试） |'
- en: '| 2020 | OpenKBP: The open-access knowledge-based planning grand challenge
    and dataset [[35](#bib.bib35)] | CT | Head and neck | 7 (brainstem, spinal cord,
    right parotid, left parotid, larynx, esophagus, mandible) | 340 (200 training,
    40 validating, 100 testing) | Labelled | 128×128×128 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | OpenKBP：开放获取的知识基础规划大挑战及数据集 [[35](#bib.bib35)] | CT | 头部和颈部 | 7（脑干、脊髓、右侧腮腺、左侧腮腺、喉、食道、下颌骨）
    | 340（200 训练，40 验证，100 测试） | 标记 | 128×128×128 |'
- en: '| 2021 | Abdomenct-1k [[36](#bib.bib36)] | CT | Abdomen | 5 (liver, right and
    left kidneys, spleen, pancreas) | 1112 | Labelled | 512 × 512 × N |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Abdomenct-1k [[36](#bib.bib36)] | CT | 腹部 | 5（肝脏、左右肾脏、脾脏、胰腺） | 1112
    | 标记 | 512 × 512 × N |'
- en: IV Full Annotation-based Methods
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 完全标注方法
- en: 'The method based on full annotation means that all organs of the multi-organ
    segmentation task are fully annotated. The existing methods can be analysed from
    four parts: network architecture, network dimension, network dedicated modules,
    and network loss function. Among these methods, the network architecture part
    summarizes the common neural network architectures and the combination or cascade
    of different architectures. In the network dimension part, the existing methods
    are classified into 2D, 3D, and multi-view methods according to the image dimension
    used. The part of network dedicated modules describes modules that are commonly
    used in multi-organ segmentation to improve the segmentation performance, The
    part of network loss function summarizes how common loss functions are innovated
    around multi-organ segmentation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于全标注的方法意味着多脏器分割任务的所有脏器都已完全标注。现有的方法可以从四个部分进行分析：网络架构、网络维度、网络专用模块和网络损失函数。在这些方法中，网络架构部分总结了常见的神经网络架构及不同架构的组合或级联。在网络维度部分，现有方法根据使用的图像维度分为2D、3D和多视角方法。网络专用模块部分描述了在多脏器分割中常用的模块，以提高分割性能。网络损失函数部分总结了如何围绕多脏器分割对常见损失函数进行创新。
- en: IV-A Network Architecture
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 网络架构
- en: 'Based on the design of the network architecture, multi-organ segmentation methods
    can be classified according to single-stage and multistage implementations. Single-stage
    methods include those based on CNN (Convolutional Neural Network), GAN (Generative
    Adversarial Network), transformer or hybrid networks. Multistage approaches include
    coarse-to-fine methods, localization and segmentation methods, or other cascade
    approaches. Tables [II](#Ax1.T2 "TABLE II ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")- [IV](#Ax1.T4
    "TABLE IV ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") summarize the literature related
    to single-stage methods for the segmentation of multi-organ in the head and neck,
    abdomen and chest based on DSC metrics. Since there are too many organs in the
    head and neck as well as abdomen, this paper mainly reports 9 organs in the head
    and neck and 7 organs in the abdomen. Tables [XI](#Ax1.T11 "TABLE XI ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")- [XII](#Ax1.T12 "TABLE XII ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    in the supplementary materials summarize the DSC values of other organs.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '根据网络架构的设计，多脏器分割方法可以分为单阶段和多阶段实现。单阶段方法包括基于CNN（卷积神经网络）、GAN（生成对抗网络）、transformer或混合网络的方法。多阶段方法包括粗到精的方法、定位和分割方法，或其他级联方法。表格[II](#Ax1.T2
    "TABLE II ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation")- [IV](#Ax1.T4 "TABLE IV ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")总结了基于DSC指标的头颈部、腹部和胸部多脏器分割的单阶段方法的文献。由于头颈部和腹部的脏器数量较多，本文主要报告了头颈部的9个脏器和腹部的7个脏器。补充材料中的表格[XI](#Ax1.T11
    "TABLE XI ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation")- [XII](#Ax1.T12 "TABLE XII ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")总结了其他脏器的DSC值。'
- en: IV-A1 CNN-Based Methods
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 基于CNN的方法
- en: Convolutional Neural Network (CNN) is a feedforward neural network which can
    automatically extract deep features of the image. Multiple neurons are connected
    to each neuron in next layer, where each layer can perform complex tasks such
    as convolution, pooling, or loss computation [[37](#bib.bib37)]. CNNs have been
    successfully applied to medical images, such as brain [[38](#bib.bib38), [39](#bib.bib39)]
    and pancreas [[40](#bib.bib40)] segmentation tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是一种前馈神经网络，能够自动提取图像的深层特征。多个神经元与下一层的每个神经元相连接，每一层可以执行复杂的任务，如卷积、池化或损失计算[[37](#bib.bib37)]。CNN已成功应用于医学图像，如脑部[[38](#bib.bib38)、[39](#bib.bib39)]和胰腺[[40](#bib.bib40)]的分割任务。
- en: Early CNN-Based Methods
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 早期的基于CNN的方法
- en: Earlier CNN-based methods mainly used convolutional layers to extract features
    and then went through pooling layers and fully connected layers to obtain the
    final prediction results. Ibragimov and Xing [[41](#bib.bib41)] used deep learning
    methods to segment OARs in head and neck CT images for the first time, training
    13 CNNs for 13 OARs, and showed that the CNNs outperformed or were comparable
    to advanced algorithms in segmentation accuracy for organs such as the spinal
    cord, mandible, larynx, pharynx, eye, and optic nerve, but performed poorly in
    the segmentation of organs such as the parotid gland, submandibular gland, and
    optical chiasm. Fritscher et al. [[42](#bib.bib42)] combined the shape location
    as well as the intensity with CNN for segmentation of the parotid gland, submandibular
    gland and optic nerve. Moeskops et al. [[43](#bib.bib43)] investigated whether
    a single CNN can be used to segment six tissues in brain MR images, pectoral muscles
    in breast MR images, and coronary arteries in heart CTA images. The results showed
    that a single CNN can segment multiple organs not only on a single modality but
    also on multiple modalities.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 早期基于CNN的方法主要使用卷积层提取特征，然后经过池化层和全连接层得到最终的预测结果。Ibragimov和Xing[[41](#bib.bib41)]首次使用深度学习方法对头颈部CT图像中的OAR进行分割，为13个OAR训练了13个CNN，并显示这些CNN在脊髓、下颌骨、喉部、咽部、眼睛和视神经等器官的分割准确度上优于或与先进算法相当，但在分割如腮腺、下颌腺和视交叉等器官时表现较差。Fritscher等人[[42](#bib.bib42)]结合了形状位置及强度与CNN用于腮腺、下颌腺和视神经的分割。Moeskops等人[[43](#bib.bib43)]探讨了是否可以使用单一的CNN对脑部MR图像中的六种组织、乳腺MR图像中的胸肌和心脏CTA图像中的冠状动脉进行分割。结果表明，单一的CNN不仅可以在单一模态上分割多个器官，还可以在多种模态上进行分割。
- en: FCN-Based Methods
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于FCN的方法
- en: Early CNN-based methods made some improvements in segmentation accuracy compared
    to traditional methods. However, CNN involves multiple identical computations
    of overlapping voxels during the convolution operation, which may cause some performance
    loss. Moreover, the spatial information of the image is lost when the convolutional
    features are input into the final fully connected network layer. Thus, Shelhamer
    et al. [[44](#bib.bib44)] proposed the Fully Convolutional Network (FCN), which
    enables end-to-end segmentation by using transposed convolutional layers that
    allow the size of the predicted image to match the size of the input image. Wang
    et al. [[45](#bib.bib45)] used FCN combined with a new sample selection strategy
    to segment 16 organs in the abdomen, and Trullo et al. [83] used a variant of
    FCN, SharpMask [[46](#bib.bib46)], to segment the esophagus, heart, trachea, and
    aorta in the thorax, which showed the segmentation results of all four organs
    were improved compared with the normal FCN.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 早期基于CNN的方法在分割准确度上相较于传统方法有所提升。然而，CNN在卷积操作中涉及多个相同的重叠体素计算，这可能导致一定的性能损失。此外，当卷积特征输入到最终的全连接网络层时，图像的空间信息会丢失。因此，Shelhamer等人[[44](#bib.bib44)]提出了全卷积网络（FCN），该网络通过使用转置卷积层使预测图像的大小与输入图像的大小匹配，从而实现端到端的分割。Wang等人[[45](#bib.bib45)]将FCN与一种新的样本选择策略相结合，分割了腹部的16个器官，而Trullo等人[83]则使用FCN的变体SharpMask[[46](#bib.bib46)]，对胸腔中的食管、心脏、气管和主动脉进行分割，结果显示所有四个器官的分割结果相比普通FCN有所改善。
- en: U-Net-Based Methods
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于U-Net的方法
- en: Based on FCN, Ronneberger et al. [[47](#bib.bib47)] proposed a classical U-Net
    architecture, which is consisted of an encoder for the down sampling layer and
    a decoder for the up-sampling layer, and connects them layer by layer with skip
    connections, so that the features extracted from the down sampling layer can be
    directly transmitted to the up-sampling layer to fuse multiscale features for
    segmentation. U-Net has become one of the most commonly used architectures in
    the field of multi-organ segmentation [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)]. Roth
    et al. [[52](#bib.bib52)] applied the U-Net architecture to segment the abdominal
    aorta, portal vein, liver, spleen, stomach, gallbladder, and pancreas. The advanced
    segmentation performance of multiple organs was achieved with an average Dice
    value of 0.893 for seven organs. Lambert et al. [[55](#bib.bib55)] proposed a
    simplified U-Net for segmenting the heart, trachea, aorta, and esophagus of the
    chest. The results showed that adding dropout and using bilinear interpolation
    can significantly improve the segmentation performance of the heart, aorta, and
    esophagus compared with the ordinary U-Net. In addition to U-Net, V-Net [[56](#bib.bib56)]
    proposes 3D image segmentation method based on volumetric, fully convolutional
    neural network. This method can directly process 3D medical data by introducing
    residual connections and using convolutional layers instead of pooling layers
    in the original U-Net. Gibson et al. [[57](#bib.bib57)] used dense V-Networks
    to segment the pancreas, esophagus, stomach, liver, spleen, gallbladder, left
    kidney, and duodenum of the abdomen. Xu et al. [[58](#bib.bib58)] proposed a new
    probabilistic V-Net model which combines a conditional variational autoencoder
    (cVAE) and hierarchical spatial feature transform (HSPT) for abdominal multi-organ
    segmentation. nnU-Net [101] is a novel framework based on U-Net architecture with
    the addition of adaptive pre-processing, data enhancement, and postprocessing
    techniques, and has shown state-of-the-art results on many publicly available
    datasets for different biomedical segmentation challenges [[59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]. Podobnik et al. [[59](#bib.bib59)] reported
    the results of segmentation of 31 OARs of the head and neck using the nnU-Net
    architecture combined with CT and MR images.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于FCN，Ronneberger等人[[47](#bib.bib47)] 提出了经典的U-Net架构，该架构由一个用于下采样的编码器和一个用于上采样的解码器组成，并通过跳跃连接逐层连接，使得从下采样层提取的特征可以直接传输到上采样层，以融合多尺度特征进行分割。U-Net已成为多脏器分割领域中最常用的架构之一[[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]。Roth等人[[52](#bib.bib52)] 将U-Net架构应用于腹主动脉、门静脉、肝脏、脾脏、胃、胆囊和胰腺的分割。通过对七个脏器的平均Dice值达到0.893，实现了多脏器的先进分割性能。Lambert等人[[55](#bib.bib55)]
    提出了一个简化版U-Net，用于分割胸部的心脏、气管、主动脉和食道。结果表明，与普通U-Net相比，添加dropout和使用双线性插值可以显著提高心脏、主动脉和食道的分割性能。除了U-Net，V-Net[[56](#bib.bib56)]
    提出了基于体积的3D图像分割方法，该方法基于完全卷积神经网络。通过引入残差连接并使用卷积层代替原U-Net中的池化层，该方法可以直接处理3D医学数据。Gibson等人[[57](#bib.bib57)]
    使用密集V-Net对腹部的胰腺、食道、胃、肝脏、脾脏、胆囊、左肾和十二指肠进行分割。Xu等人[[58](#bib.bib58)] 提出了一个新的概率V-Net模型，该模型结合了条件变分自编码器（cVAE）和层次空间特征变换（HSPT）用于腹部多脏器分割。nnU-Net
    [101] 是一个基于U-Net架构的新型框架，增加了自适应预处理、数据增强和后处理技术，并在许多公开数据集中对不同生物医学分割挑战展示了最先进的结果[[59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)]。Podobnik等人[[59](#bib.bib59)]
    报告了使用nnU-Net架构结合CT和MR图像对31个头颈部OARs进行分割的结果。
- en: IV-A2 GAN-Based Methods
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 基于GAN的方法
- en: A typical Generative Adversarial Network (GAN) [[63](#bib.bib63)] includes a
    pair of competitive networks, which are generators and discriminators. The generator
    attempts to deceive the discriminator by generating the artificial data, and the
    discriminator strives to discriminate the artificial data without being deceived
    by the generator; after alternate optimization training, the performance of both
    networks can eventually be improved. In recent years, many GAN-based multi-organ
    segmentation methods have been proposed and achieved high segmentation accuracy
    [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68),
    [69](#bib.bib69), [70](#bib.bib70)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的生成对抗网络（GAN） [[63](#bib.bib63)] 包括一对竞争网络，即生成器和判别器。生成器试图通过生成虚假数据来欺骗判别器，而判别器则努力区分虚假数据而不被生成器欺骗；经过交替优化训练，两个网络的性能最终可以得到提升。近年来，提出了许多基于
    GAN 的多器官分割方法，并实现了高分割准确性 [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]。
- en: Dong et al. [[66](#bib.bib66)] jointly trained GAN with a set of U-Nets as a
    generator and a set of FCNs as a discriminator for segmenting the left lung, right
    lung, spinal cord, esophagus and heart from chest CT images. The results showed
    that the segmentation performance of most of the organs were improved with the
    help of adversarial networks, and the average DSC values of the above five OARs
    were finally obtained as 0.970, 0.970, 0.900, 0.750 and 0.870\. Tong et al. [[64](#bib.bib64)]
    proposed a Shape-constraint GAN for automatic head and neck OARs segmentation
    (SC-GAN) from CT and low-field MRI images. It uses DenseNet, a deep supervised
    fully convolutional network to segment organs for prediction, and uses a CNN as
    discriminator network to correct the error of prediction. The results show that
    the combination of GAN and DenseNet can further improve the segmentation performance
    of CNN based on the original shape constraints.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Dong 等人 [[66](#bib.bib66)] 将 GAN 与一组 U-Nets 作为生成器和一组 FCNs 作为判别器联合训练，用于从胸部 CT
    图像中分割左肺、右肺、脊髓、食管和心脏。结果表明，大多数器官的分割性能在对抗网络的帮助下有所提高，以上五个 OAR 的平均 DSC 值最终分别为 0.970、0.970、0.900、0.750
    和 0.870。Tong 等人 [[64](#bib.bib64)] 提出了一个形状约束 GAN（SC-GAN）用于自动分割头颈部 OARs 从 CT 和低场
    MRI 图像中。它使用 DenseNet，一个深度监督全卷积网络进行器官分割预测，并使用 CNN 作为判别器网络来纠正预测误差。结果表明，GAN 和 DenseNet
    的结合可以进一步提高基于 CNN 的原始形状约束的分割性能。
- en: GAN can improve accuracy with its adversarial losses. However, the training
    of GAN network is difficult and time-consuming since the generator needs to achieve
    Nash equilibrium with the discriminator. And its adversarial loss as a shape modifier
    can only achieve higher segmentation accuracy when segmenting organs with regular
    and unique shapes (such as liver and heart), but may not work well for irregular
    or tubular structures (such as pancreas and aorta).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 可以通过其对抗损失提高准确性。然而，由于生成器需要与判别器达到纳什均衡，GAN 网络的训练既困难又耗时。其对抗损失作为形状修正器只能在分割形状规则且独特的器官（如肝脏和心脏）时实现更高的分割准确性，但可能对不规则或管状结构（如胰腺和主动脉）效果不佳。
- en: IV-A3 Transformer-Based Methods
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 基于 Transformer 的方法
- en: CNN-based methods can perform well for segmenting multiple organs in many tasks,
    but the inherent shortcomings of the perceptual field of the convolutional layers
    lead to the inability of CNNs to model global relationships, hindering the performance
    of the models. The self-attentive mechanism of the transformer [[71](#bib.bib71)]
    can solve the long-term dependency problem well, achieving better results than
    CNNs in many tasks such as natural language processing (NLP) or computer vision
    [[72](#bib.bib72)]. The performance of the medical image segmentation networks
    using transformer is also close or even better than the one of current state-of-the-art
    methods [[73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CNN 的方法在许多任务中可以很好地进行多器官分割，但卷积层感知领域的固有缺陷导致 CNN 无法建模全局关系，从而阻碍了模型的性能。Transformer
    的自注意力机制 [[71](#bib.bib71)] 可以很好地解决长期依赖问题，在许多任务中，如自然语言处理（NLP）或计算机视觉 [[72](#bib.bib72)]，取得了比
    CNN 更好的结果。使用 Transformer 的医学图像分割网络的性能也接近甚至优于当前最先进的方法 [[73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76)]。
- en: Cao et al. [[77](#bib.bib77)] integrated the transformer with a U-shaped architecture
    to explore the potential of the pure transformer model for abdominal multi-organ
    segmentation. The results showed that the method has good segmentation accuracy.
    However, the method needs to initialize the network encoder and decoder using
    the training weights of the Swin transformer on ImageNet. Huang et al. [[78](#bib.bib78)]
    introduced an efficient and powerful medical image segmentation architecture,
    MISSFormer, where the proposed enhanced mixed block can effectively overcome the
    feature recognition limitation problem caused by convolution. Moreover, compared
    with Swin-UNet, this model does not require pre-training on large-scale datasets
    to achieve comparable segmentation results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Cao等人[[77](#bib.bib77)] 将Transformer与U型架构集成，以探索纯Transformer模型在腹部多器官分割中的潜力。结果显示，该方法具有良好的分割精度。然而，该方法需要使用在ImageNet上训练的Swin
    Transformer的权重来初始化网络编码器和解码器。Huang等人[[78](#bib.bib78)] 介绍了一种高效且强大的医学图像分割架构MISSFormer，其中提出的增强混合块可以有效克服卷积引起的特征识别限制问题。此外，与Swin-UNet相比，该模型不需要在大规模数据集上进行预训练即可实现可比的分割结果。
- en: Transformer-based approaches can capture long-range dependencies and achieve
    better performance than CNNs in many tasks. However, multi-organ segmentation
    problem involves the segmentation of many tiny organs, and the pure transformer
    network focuses on the global context modelling. This leads to the lack of detailed
    localization information of low-resolution features. Thus, a coarser segmentation
    result is usually obtained.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的方法能够捕捉长期依赖关系，并在许多任务中比CNN实现更好的性能。然而，多脏器分割问题涉及许多微小器官的分割，而纯Transformer网络专注于全球上下文建模。这导致低分辨率特征缺乏详细的定位信息。因此，通常会得到较粗的分割结果。
- en: IV-A4 Hybrid Networks
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 混合网络
- en: CNN convolution operation can extract local features well, but it is difficult
    to obtain global features. The self-attentive mechanism of the transformer can
    effectively capture feature dependencies over long distances, but it loses local
    feature details, which may obtain poor results for the segmentation accuracy of
    small organs. Therefore, some researchers have combined the CNN and transformer
    to overcome the limitations of both architectures [[79](#bib.bib79), [74](#bib.bib74),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: CNN卷积操作可以很好地提取局部特征，但很难获得全球特征。Transformer的自注意机制可以有效捕捉长距离的特征依赖关系，但会丢失局部特征细节，这可能导致小器官的分割精度较差。因此，一些研究人员结合了CNN和Transformer，以克服这两种架构的局限性[[79](#bib.bib79),
    [74](#bib.bib74), [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)]。
- en: Suo et al. [[84](#bib.bib84)] proposed an intra-scale and inter-scale collaborative
    learning network (I2-Net) by combining features extracted by the CNN and transformer
    to segment multiple organs of the abdomen, which improved the segmentation performance
    of small and medium-sized organs by 4.19% and 1.83%-3.8%, respectively. Kan et
    al. [[85](#bib.bib85)] proposed ITUnet, which adds the features extracted by the
    transformer to the output of each block of the CNN-based encoder, which can obtain
    segmentation results provided by both the local and global information of the
    image. ITUnet has better accuracy and robustness than other methods, especially
    on difficult organs such as the lens. Chen et al. [[86](#bib.bib86)] proposed
    a new network architecture, TransUNet, which uses a transformer to further encode
    CNN encoders to build stronger encoders and report competitive results for multi-organ
    segmentation of the head and neck. Hatamizadeh et al. [[87](#bib.bib87)] proposed
    a new architecture U-net transformer (UNETR) using a transformer as an encoder
    and the CNN as a decoder, which achieves better segmentation accuracy by capturing
    global and local dependencies.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Suo等人[[84](#bib.bib84)] 提出了一个通过结合CNN和Transformer提取的特征进行内尺度和跨尺度协作学习的网络（I2-Net），用于分割腹部的多个器官，这提高了小型和中型器官的分割性能，分别提高了4.19%和1.83%-3.8%。Kan等人[[85](#bib.bib85)]
    提出了ITUnet，它将Transformer提取的特征添加到基于CNN的编码器的每个块的输出中，这样可以获得由图像的局部和全球信息提供的分割结果。ITUnet比其他方法具有更好的准确性和鲁棒性，尤其是在镜头等困难器官上。Chen等人[[86](#bib.bib86)]
    提出了一个新的网络架构，TransUNet，它使用Transformer进一步编码CNN编码器，以构建更强的编码器，并报告了头部和颈部多器官分割的竞争性结果。Hatamizadeh等人[[87](#bib.bib87)]
    提出了一个新的架构U-net transformer（UNETR），它使用Transformer作为编码器，CNN作为解码器，通过捕捉全球和局部依赖关系实现了更好的分割精度。
- en: In addition to the methods combining CNN and transformer, there are some other
    hybrid frames. For example, Chen et al. [[88](#bib.bib88)] combined U-Net and
    long short-term memory (LSTM) to realize the segmentation of five organs in the
    chest, and the DSC values of all five organs were above 0.8\. Chakravarty et al.
    [[89](#bib.bib89)] proposed a hybrid architecture combining CNN and RNN to segment
    the optic disc, nucleus, and left atrium. The hybrid architecture-based approach
    can combine and utilize the advantages of the two architectures for the accurate
    segmentation of small and medium-sized organs, which is a key research direction
    for the future.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了结合CNN和transformer的方法，还有一些其他的混合框架。例如，Chen等人[[88](#bib.bib88)]结合了U-Net和长短期记忆（LSTM）来实现胸部五个器官的分割，所有五个器官的DSC值均在0.8以上。Chakravarty等人[[89](#bib.bib89)]提出了一种结合CNN和RNN的混合架构来分割视盘、核和左心房。基于混合架构的方法可以结合并利用这两种架构的优点，以准确分割中小型器官，这是未来研究的一个关键方向。
- en: IV-A5 Cascade Networks
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A5 级联网络
- en: 'Due to most organs occupy only a small volume in images, the segmentation models
    are easy to segment large organs and ignore small organs, which prompted researchers
    to propose cascade multistage methods. Multistage methods can be divided into
    two main categories, depending on the information provided by the primary network
    to the secondary network. The first category is called coarse-to-fine multi-organ
    segmentation method, where the first network performs coarse segmentation, and
    its results are passed to another network to achieve fine segmentation. The second
    category is called multi-organ segmentation method based on localization and segmentation,
    where candidate boxes for the location of each organ are identified by registration
    methods or localization networks, and then input into the second-level network
    for fine segmentation. In addition, the first network can provide other information,
    such as the shape location or proportion, to better guide the segmentation of
    the second network. Tables [V](#Ax1.T5 "TABLE V ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[X](#Ax1.T10
    "TABLE X ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") summarize the relevant literature
    of the cascade methods for head and neck, chest and abdomen based on DSC metrics,
    and tables [VIII](#Ax1.T8 "TABLE VIII ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[IX](#Ax1.T9
    "TABLE IX ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation") in the supplementary materials
    summarize the DSC metrics of other organs.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '由于大多数器官在图像中只占有很小的体积，分割模型容易对大器官进行分割，而忽略小器官，这促使研究人员提出了级联多阶段方法。多阶段方法可以根据初级网络提供给次级网络的信息分为两大类。第一类被称为粗到细的多器官分割方法，其中第一个网络执行粗略分割，并将结果传递给另一个网络以实现精细分割。第二类被称为基于定位和分割的多器官分割方法，其中每个器官的候选框由配准方法或定位网络确定，然后输入到第二级网络中进行精细分割。此外，第一个网络还可以提供其他信息，如形状位置或比例，以更好地指导第二个网络的分割。表格[V](#Ax1.T5
    "TABLE V ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation")-[X](#Ax1.T10 "TABLE X ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")总结了基于DSC指标的头颈部、胸部和腹部级联方法的相关文献，而补充材料中的表格[VIII](#Ax1.T8
    "TABLE VIII ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation")-[IX](#Ax1.T9 "TABLE IX ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")总结了其他器官的DSC指标。'
- en: Coarse-to-Fine-Based Methods
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 粗到细的基于方法
- en: 'The coarse-to-fine-based methods first inputs the original image and its corresponding
    labels into the first network. After training, the first-level network obtains
    the coarse segmentation probability map, which will be multiplied by the original
    image, and the results will be input into the second network to refine the rough
    segmentation. This process is shown in Fig. [7](#S4.F7 "Figure 7 ‣ Coarse-to-Fine-Based
    Methods ‣ IV-A5 Cascade Networks ‣ IV-A Network Architecture ‣ IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation"). In recent years, a number of coarse-to-fine
    methods have been proposed for multi-organ segmentation [[90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99)], and the references are
    shown in Tables [VI](#Ax1.T6 "TABLE VI ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[VIII](#Ax1.T8
    "TABLE VIII ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于粗到细的方法首先将原始图像及其对应的标签输入到第一个网络中。经过训练后，第一级网络获得粗分割概率图，该图将与原始图像相乘，然后结果将输入到第二个网络中以细化粗略分割。这个过程如图
    [7](#S4.F7 "图 7 ‣ 基于粗到细的方法 ‣ IV-A5 级联网络 ‣ IV-A 网络架构 ‣ IV 完整注释方法 ‣ 朝着更精确的自动分析：基于深度学习的多器官分割的综合调查")
    所示。近年来，已经提出了许多用于多器官分割的粗到细方法 [[90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)]，参考文献见表 [VI](#Ax1.T6 "表 VI ‣ 朝着更精确的自动分析：基于深度学习的多器官分割的综合调查")-[VIII](#Ax1.T8
    "表 VIII ‣ 朝着更精确的自动分析：基于深度学习的多器官分割的综合调查")。
- en: '![Refer to caption](img/0bfc1de36549d0a7b7727c028978d6c3.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0bfc1de36549d0a7b7727c028978d6c3.png)'
- en: 'Figure 7: Coarse-to-fine-based segmentation method.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于粗到细的分割方法。
- en: Trullo et al. [[100](#bib.bib100)] proposed two synergistic depth architectures
    to jointly segment all organs, including the esophagus, heart, aorta, and trachea.
    Probabilistic maps obtained in the first stage were passed to the second stage
    to learn anatomical constraints, and then four networks were trained for four
    structures in the second stage to distinguish the background from each target
    organ in separate refinements. Zhang et al. [[94](#bib.bib94)] proposed a new
    cascaded network model with Block Level Skip Connections (BLSC) between two cascaded
    networks. This architecture enabled the second-stage network to capture the features
    learned by each block in the first-stage network and accelerated the convergence
    of the second-stage network. Xie et al. [[95](#bib.bib95)] proposed a new framework
    called the Recurrent Saliency Transformation Network (RSTN). This framework enabled
    coarse scale segmentation masks to be passed to the fine stage as spatial weights,
    while gradients can be backpropagated from the loss layer to the whole network,
    so as to realize the joint optimization of the two stages, thus improving the
    segmentation accuracy of small targets. Ma et al. [[92](#bib.bib92)] proposed
    a new end-to-end coarse-to-fine segmentation model to automatically segment multiple
    OARs in head and neck CT images. This model used a predetermined threshold to
    classify the initial results of the coarse stage into large and small OARs, and
    then designed different modules to refine the segmentation results.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Trullo 等人 [[100](#bib.bib100)] 提出了两种协同深度架构，以共同分割所有器官，包括食管、心脏、主动脉和气管。第一阶段获得的概率图被传递到第二阶段以学习解剖约束，然后在第二阶段训练四个网络来区分背景与每个目标器官，并进行单独的精细化。Zhang
    等人 [[94](#bib.bib94)] 提出了一个新的级联网络模型，在两个级联网络之间使用了块级跳跃连接（BLSC）。这种架构使第二阶段的网络能够捕捉到第一阶段网络中每个块学习到的特征，并加速了第二阶段网络的收敛。Xie
    等人 [[95](#bib.bib95)] 提出了一个名为递归显著性转换网络（RSTN）的新框架。该框架使粗尺度分割掩膜可以作为空间权重传递到细尺度阶段，同时梯度可以从损失层反向传播到整个网络，从而实现两个阶段的联合优化，提高了小目标的分割精度。Ma
    等人 [[92](#bib.bib92)] 提出了一个新的端到端粗到细分割模型，用于自动分割头颈部 CT 图像中的多个 OAR。该模型使用预定阈值将粗阶段的初步结果分类为大
    OAR 和小 OAR，然后设计了不同的模块来细化分割结果。
- en: This coarse-to-fine approach effectively reduces the complexity of the background
    and enhances the discriminative information of the target structures. Compared
    with the single-stage approach, this coarse-to-fine-based method improves the
    segmentation results for small organs, but there are limitations in memory and
    training time because at least two networks need to be trained.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种粗到细的方法有效减少了背景的复杂性，并增强了目标结构的区分信息。与单阶段方法相比，这种粗到细的方法改进了小器官的分割结果，但由于至少需要训练两个网络，因此在内存和训练时间上存在一定的限制。
- en: Localization and Segmentation Based Methods
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于定位和分割的方法
- en: 'The localization and segmentation methods are also multistage cascade methods.
    Here, the first-level network provides location information, returns a candidate
    frame, and crops the region of interest of the image according to the location
    information, and uses it as the input of the second network. In this way, when
    the second network performs segmentation, one organ can be targeted, excluding
    the interference of other organs or background noise and improving the segmentation
    accuracy. The process is shown in Fig. [8](#S4.F8 "Figure 8 ‣ Localization and
    Segmentation Based Methods ‣ IV-A5 Cascade Networks ‣ IV-A Network Architecture
    ‣ IV Full Annotation-based Methods ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation"). The
    organ location in the first stage can be obtained through registration or localization
    network. The relevant literature of multi-stage method based on location and segmentation
    are listed in Tables [VIII](#Ax1.T8 "TABLE VIII ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[X](#Ax1.T10
    "TABLE X ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation"), and the DSC values of other organs
    are listed in tables [XV](#Ax1.T15 "TABLE XV ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")-[XVI](#Ax1.T16
    "TABLE XVI ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of
    Deep Learning-based Multi-organ Segmentation").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '定位和分割方法也是多阶段级联方法。在这里，第一级网络提供位置相关信息，返回一个候选框，并根据位置信息裁剪图像的感兴趣区域，然后将其作为第二个网络的输入。通过这种方式，当第二个网络进行分割时，可以针对一个器官，排除其他器官或背景噪声的干扰，从而提高分割的准确性。该过程如图
    [8](#S4.F8 "Figure 8 ‣ Localization and Segmentation Based Methods ‣ IV-A5 Cascade
    Networks ‣ IV-A Network Architecture ‣ IV Full Annotation-based Methods ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation") 所示。第一个阶段的器官位置可以通过注册或定位网络获得。基于位置和分割的多阶段方法的相关文献列在表格 [VIII](#Ax1.T8
    "TABLE VIII ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation")-[X](#Ax1.T10 "TABLE X ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation") 中，其他器官的 DSC 值列在表格 [XV](#Ax1.T15 "TABLE XV ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")-[XVI](#Ax1.T16 "TABLE XVI ‣ Towards More Precise Automatic
    Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")
    中。'
- en: '![Refer to caption](img/acc452e4fe5899ccc3d80db4f9760314.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/acc452e4fe5899ccc3d80db4f9760314.png)'
- en: 'Figure 8: Localization and segmentation based method.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于定位和分割的方法。
- en: Wang et al. [[101](#bib.bib101)], Men et al. [[102](#bib.bib102)], Lei et al.
    [[103](#bib.bib103)], Francis et al. [[104](#bib.bib104)], Tang et al. [[105](#bib.bib105)]
    proposed decomposing OARs segmentation into two stages of localization and segmentation.
    The first stage localizes the target OARs using the bounding box, the second stage
    segments the target OARs within the bounding box, and both stages use neural networks.
    Among them, Wang et al. [[101](#bib.bib101)] and Francis et al. [[104](#bib.bib104)]
    used a 3D U-net in both stages. Lei et al. [[103](#bib.bib103)] used Faster RCNN
    to automatically locate the ROI of organs in the first stage. Korte et al. [[106](#bib.bib106)]
    demonstrated that the CNN is a suitable method for automatically segmenting parotid
    and submandibular glands in MRI images of HNC patients. The segmentation accuracy
    of the parotid and submandibular glands can be improved by cascading localizing
    CNNs, cropping and segmenting high-resolution CNNs. FocusNet [[69](#bib.bib69),
    [107](#bib.bib107)] presented a novel deep neural network to solve the class imbalance
    problem in the segmentation of head and neck OARs. The small organs are first
    localized by the organ localization network. Then, combined with the high-resolution
    information of each small organ, multiscale features are input to the segmentation
    network together to accurately segment the small organs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[101](#bib.bib101)]、Men 等人 [[102](#bib.bib102)]、Lei 等人 [[103](#bib.bib103)]、Francis
    等人 [[104](#bib.bib104)] 和 Tang 等人 [[105](#bib.bib105)] 提出了将 OARs 分割分解为定位和分割两个阶段的方法。第一阶段使用边界框定位目标
    OARs，第二阶段在边界框内分割目标 OARs，两阶段都使用神经网络。其中，Wang 等人 [[101](#bib.bib101)] 和 Francis 等人
    [[104](#bib.bib104)] 在两个阶段都使用了 3D U-net。Lei 等人 [[103](#bib.bib103)] 在第一阶段使用了 Faster
    RCNN 自动定位器官的 ROI。Korte 等人 [[106](#bib.bib106)] 证明了 CNN 是自动分割 HNC 患者 MRI 图像中腮腺和下颌腺的合适方法。通过级联定位
    CNNs、裁剪和分割高分辨率 CNNs 可以提高腮腺和下颌腺的分割精度。FocusNet [[69](#bib.bib69), [107](#bib.bib107)]
    提出了一个新颖的深度神经网络来解决头颈 OARs 分割中的类别不平衡问题。首先，通过器官定位网络定位小器官。然后，将每个小器官的高分辨率信息结合在一起，输入到分割网络中，从而准确分割小器官。
- en: The organ localization by Larsson et al. [[108](#bib.bib108)], Zhao et al. [[109](#bib.bib109)],
    Ren et al. [[110](#bib.bib110)] and Huang et al. [[111](#bib.bib111)] was obtained
    with registration method followed by the application of convolutional neural networks
    for segmentation. Among them, Ren et al. [[110](#bib.bib110)] designed interleaved
    cascades of 3D-CNNs to segment each structure of interest. Since adjacent tissues
    are usually highly correlated from a physiological and anatomical perspective,
    using the initial segmentation results of a specific tissue can help refine the
    segmentation of other neighbouring tissues. Zhao et al. [[109](#bib.bib109)] proposed
    a new flexible knowledge-assisted convolutional neural network which combine deep
    learning and traditional methods to improve the segmentation accuracy in the second
    stage.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Larsson 等人 [[108](#bib.bib108)]、Zhao 等人 [[109](#bib.bib109)]、Ren 等人 [[110](#bib.bib110)]
    和 Huang 等人 [[111](#bib.bib111)] 的器官定位是通过注册方法获得的，随后应用卷积神经网络进行分割。在这些研究中，Ren 等人 [[110](#bib.bib110)]
    设计了交错的 3D-CNNs 连续体来分割每个感兴趣的结构。由于相邻组织在生理和解剖上通常高度相关，使用特定组织的初始分割结果可以帮助优化其他邻近组织的分割。Zhao
    等人 [[109](#bib.bib109)] 提出了一个新的灵活的知识辅助卷积神经网络，它结合了深度学习和传统方法，以提高第二阶段的分割精度。
- en: The vast majority of approaches require to determine the target areas prior
    to segmentation network training by different localization methods. For example,
    Ren et al. [[110](#bib.bib110)] localized organ regions through a multi-atlas-based
    method. Wang et al. [[101](#bib.bib101)] used separate CNNs to localize candidate
    areas. That is, their target organ region localization is constructed independently
    of organ segmentation, which will hinder the transmission of information between
    these two related learning tasks. On this basis, Liang et al. [[112](#bib.bib112)]
    proposed a multi-organ segmentation framework based on multi view spatial aggregation,
    which combines the learning of the organ localization subnetwork and the segmentation
    subnetwork to reduce the influence of background regions and neighbouring similar
    structures in the input data. Additionally, the proposed fine-grained representation
    based on ROIs can improve the segmentation accuracy of organs with different sizes,
    especially the segmentation results of small organs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数方法要求在分割网络训练之前通过不同的定位方法确定目标区域。例如，Ren 等人 [[110](#bib.bib110)] 通过基于多图谱的方法定位器官区域。Wang
    等人 [[101](#bib.bib101)] 使用独立的 CNN 定位候选区域。也就是说，他们的目标器官区域定位与器官分割是独立构建的，这会阻碍这两个相关学习任务之间的信息传递。在此基础上，Liang
    等人 [[112](#bib.bib112)] 提出了一个基于多视角空间聚合的多器官分割框架，该框架结合了器官定位子网络和分割子网络的学习，以减少背景区域和输入数据中相邻相似结构的影响。此外，基于
    ROI 的细粒度表示可以提高不同大小器官的分割准确性，特别是对小器官的分割结果。
- en: The type of multistage method improves the organ segmentation accuracy, especially
    for small organs, which largely reduces the interference of the background. However,
    this two-step process has certain requirements for memory and training time, and
    the segmentation accuracy also depends largely on the regional localization accuracy.
    Better localization of organs and improvement of segmentation accuracy are still
    directions to be investigated in the future.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段方法提高了器官分割的准确性，特别是对小器官的分割，这大大减少了背景的干扰。然而，这种两步过程对内存和训练时间有一定要求，且分割准确性在很大程度上也依赖于区域定位的准确性。未来的研究方向仍然是更好地定位器官并提高分割准确性。
- en: Other Cascade Methods
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他级联方法
- en: In addition to probability maps and localization information, the first network
    can also provide other types of information, such as scale information and shape
    priors. For example, Tong et al. [[113](#bib.bib113)] combines the FCNN and a
    shape representation model (SRM) for head and neck OARs segmentation. The first-level
    network is the SRM for learning highly representative shape features in head and
    neck organs. The direct comparison of the FCNN with and without SRM shows that
    the SRM significantly improves the segmentation accuracy of nine organs with different
    sizes, morphological complexity, and different CT contrasts. Roth et al. [[114](#bib.bib114)]
    proposed a multiscale 3D FCN approach which is accomplished by two cascaded FCNs,
    where low-resolution 3D FCN predictions are upsampled, cropped, and connected
    to higher-resolution 3D FCN inputs. In this case, the primary network provides
    scale information to the secondary network. And the method uses the scale space
    pyramid with automatic context to perform high-resolution semantic image segmentation,
    while considering large contextual information from the lower resolution levels.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了概率图和定位信息外，第一个网络还可以提供其他类型的信息，如尺度信息和形状先验。例如，Tong 等人 [[113](#bib.bib113)] 将 FCNN
    和形状表示模型（SRM）结合用于头部和颈部 OARs 的分割。第一级网络是 SRM，用于学习头部和颈部器官的高度代表性形状特征。FCNN 与不带 SRM 的直接比较表明，SRM
    显著提高了九个不同大小、形态复杂性和 CT 对比度不同的器官的分割准确性。Roth 等人 [[114](#bib.bib114)] 提出了一个多尺度 3D
    FCN 方法，通过两个级联的 FCN 实现，其中低分辨率的 3D FCN 预测被上采样、裁剪，并连接到更高分辨率的 3D FCN 输入。在这种情况下，主网络为次网络提供尺度信息。该方法使用尺度空间金字塔和自动上下文来执行高分辨率语义图像分割，同时考虑来自较低分辨率级别的大量上下文信息。
- en: IV-B Network Dimension
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 网络维度
- en: 'Considering the dimensionality of input images and convolutional kernels, multi-organ
    segmentation neural networks can be classified into 2D, 2.5D and 3D architectures,
    as shown in Fig. [9](#S4.F9 "Figure 9 ‣ IV-B Network Dimension ‣ IV Full Annotation-based
    Methods ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep
    Learning-based Multi-organ Segmentation"), and the differences between the three
    architectures will be discussed in follows.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6d3e6157643d048984b03075d4e8000.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Different network dimensions.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 2D- & 3D-Based Methods
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input of the 2D multi-organ segmentation neural network is slices from a
    three-dimensional medical image, and the convolution kernel is also two-dimensional.
    Men et al. [[115](#bib.bib115)], Trullo et al. [[100](#bib.bib100)], Gibson et
    al. [[57](#bib.bib57)], Chen et al. [[116](#bib.bib116)], Zhang et al. [[51](#bib.bib51)],
    Chen et al. [[117](#bib.bib117)] used 2D networks for multi-organ segmentation.
    2D architectures can reduce the GPU memory burden, but CT or MRI images are inherently
    3D. Moreover, slicing images into 2D tends to ignore the rich information in the
    entire image voxel, so 2D models are insufficient for analysing the complex 3D
    structures in medical images.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 3D multi-organ segmentation neural network architectures use 3D convolutional
    kernels, which can directly extract feature information from 3D medical images.
    Roth et al. [[52](#bib.bib52)], Zhu et al. [[48](#bib.bib48)], Gou et al. [[50](#bib.bib50)],
    and Jain et al. [[118](#bib.bib118)] used 3D architectures for multi-organ segmentation.
    However, due to GPU memory limitations, 3D architectures may face computationally
    intensive and memory shortage problems, so the majority of 3D network methods
    use sliding windows acting on patches. Zhu et al. [[48](#bib.bib48)] proposed
    a deep learning model called AnatomyNet, which receives full-volume head and neck
    CT images as the inputs and generates masks of all organs to be segmented at once.
    AnatomyNet only uses a down sampling layer in the first encoding block to consider
    the trade-off between GPU memory usage and network learning capability, which
    can occupy less GPU memory than other network structures while preserving information
    about small anatomical structures.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Multi-View-Based Methods
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In medical image segmentation, it is crucial to make good use of the spatial
    information between medical image slices. Directly input 3D images into the network,
    the 3D images will occupy huge memory, or convert 3D images to 2D images, the
    spatial information between medical image slices will be directly discarded. Thus,
    the idea of multiple views has appeared, which means using 2.5D neural networks
    with multiple 2D slices and combining 2D convolution and 3D convolution.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The 2.5D multi-organ segmentation neural network architecture still uses 2D
    convolutional kernels, but the input of the network is multiple slices, either
    a stack of adjacent slices using interslice information [[119](#bib.bib119), [120](#bib.bib120)],
    or slices along three orthogonal directions (axial, coronal, and sagittal) [[41](#bib.bib41),
    [42](#bib.bib42), [91](#bib.bib91), [121](#bib.bib121)]. This 2.5D approach saves
    computational resources and makes good use of spatial information. It is also
    widely used in semi supervised-based methods, which are reviewed in Section [V-B](#S5.SS2
    "V-B Semi Supervised-Based Methods ‣ V Imperfect Annotation-based Methods ‣ Towards
    More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation"). Zhou et al. [[122](#bib.bib122)] segmented each 2D
    slice using the FCN by sampling a 3D CT case on three orthogonally oriented slices
    (2D images) and then assembled the segmented output (i.e., 2D slice results) back
    into 3D. Chen et al. [[117](#bib.bib117)] developed a multi-view training method
    at the ratio of 4:1:1 on different views (axial, coronal, and sagittal) and applied
    a majority voting strategy to combine the three predictions into a final segmentation.
    The results show that the method can remove some wrong segmentation areas in the
    single-view output, especially for the small intestine and duodenum. Wang et al.
    [[123](#bib.bib123)] used a statistical fusion approach to combine segmentation
    results from three views and relate the structural similarity of 2D views to the
    original 3D image. Liang et al. [[121](#bib.bib121)] performed context-based iterative
    refinement training on each of the three views and aggregated all the predicted
    probability maps of the three orthogonal views in the last iteration to obtain
    the final segmentation results. Experiments show that this multi-view framework
    outperforms the segmentation results of the three separate views.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Tang et al. [[124](#bib.bib124)] proposed a new framework for combining 3D and
    2D models, which implements segmentation through high-resolution 2D convolution
    and extracting spatial contextual information through low-resolution 3D convolution.
    The corresponding 3D features used to guide 2D segmentation are controlled by
    a self-attentive mechanism, and the results show that this method consistently
    outperforms existing 2D and 3D models. Chen et al. [[116](#bib.bib116)] proposed
    a hybrid convolutional neural network, OrganNet2.5D, which can make full use of
    3D image information to process different planar and depth image resolutions.
    OrganNet2.5D integrates 2D convolution and 3D convolution to extract both clear
    underlying edge features and rich high-level semantic features.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Some current studies only deal with 2D image, which avoids memory and computation
    problems but does not make full use of 3D image information. 2.5D methods can
    make better use of information from multiple views and improve single-view segmentation
    compared to 2D networks, but the spatial contextual information they can extract
    is still limited. Moreover, the current 2.5D methods using in multi-organ segmentation
    are the aggregation of three perspectives at the outcome level, and the intermediate
    processes are independent of each other; better use of the intermediate learning
    process is also the direction to be investigated [[125](#bib.bib125), [126](#bib.bib126),
    [127](#bib.bib127)]. Some studies have performed 3D convolution, but local patches
    need to be processed. For example, Networks that process full-volume 3D CT images,
    similar to AnatomyNet, use only a down sampling layer to preserve information
    about small anatomical structures, so the receptive field of these networks is
    limited. To solve this problem, DenseASPP with four expansion rates (3,6,12,18)
    is introduced into FocusNet [[107](#bib.bib107)]; however, when the expansion
    rates of the cascaded expanded convolution have a common factor relationship,
    grid problems affecting the segmentation accuracy may occur. Pure 3D networks
    also face the problem of increased parameter and computational burden, which limits
    the depth and performance of the network. Therefore, considering the memory and
    computational burden, better combination of multi-view information for more accurate
    multi-organ segmentation is still the future research direction.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Network Dedicated Modules
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network architecture is very important to improve the multi-organ segmentation
    accuracy, but its design process is complex. In multi-organ segmentation tasks,
    there are many special mechanisms to improve the accuracy of organ segmentation,
    such as the dilation convolution module, feature pyramid module, and attention
    module. They improve multi-organ segmentation accuracy by increasing the perceptual
    field, aggregating features of different scales, and focusing the network on the
    segmented region. Cheng et al. [[128](#bib.bib128)] studied the performance improvement
    of each module of the network compared with the basic U-Net network in the head
    and neck segmentation task.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Shape Prior Module
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shape prior is more suitable for medical images than natural images because
    the spatial relationships between internal structures in medical images are relatively
    fixed. Therefore, considering anatomical priors in a multi-organ segmentation
    task will significantly improve the performance of multi-organ segmentation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The current methods using anatomical priors fall into two main categories. One
    category is based on the idea of statistics, which calculates the average distribution
    of organs in a fully labelled dataset so that the prediction results can be as
    close as possible to the average distribution of organs [[42](#bib.bib42), [40](#bib.bib40),
    [66](#bib.bib66), [129](#bib.bib129), [130](#bib.bib130)]. The other is to train
    a shape representation model, which pretrains the shape representation model using
    the annotation of the training dataset, and then uses it as a regularization term
    to constrain the predictions of the segmentation network during training [[64](#bib.bib64),
    [113](#bib.bib113)]. It has also been shown that generative models can learn anatomical
    priors [[131](#bib.bib131)]. Therefore, it is a future research direction to consider
    using generative models (e.g., diffusion models, which are popular in the last
    two years [[132](#bib.bib132), [133](#bib.bib133)]) to better obtain anatomical
    prior knowledge to improve segmentation performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Dilated Convolutional Module
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In traditional CNNs, down sampling and pooling operation are usually used several
    times to reduce the computation and expand the field of perception, which will
    lose the spatial information and make image reconstruction difficult. Dilated
    convolution (also known as “Atrous”) introduces another parameter to the convolution
    layer, namely, the expansion rate, which can expand the field of perception to
    extract features across a larger spatial range without increasing the computational
    cost. Dilated convolution is a commonly used method in multi-organ segmentation
    tasks [[134](#bib.bib134), [40](#bib.bib40), [53](#bib.bib53), [135](#bib.bib135),
    [120](#bib.bib120)] that increases the size of the sampling space, allowing the
    neural network to extract features in a larger receptive field that captures multiscale
    contextual information. These contextual features can capture finer structural
    information, which is important for pinpointing organ location. Gibson et al.
    [[40](#bib.bib40)] used CNN networks with dilated convolution to accurately segment
    the liver, pancreas, stomach, and esophagus from abdominal CT. Men et al. [[115](#bib.bib115)]
    proposed a new method based on deep extended convolutional neural network (DDCNN)
    for fast and consistent automatic segmentation of clinical target volumes (CTVs)
    and OARs. Vesal et al. [[135](#bib.bib135)] introduced dilated convolution to
    2D U-Net for segmenting the esophagus, heart, aorta, and thoracic trachea.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Multiscale Module
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural networks extract the features of the target layer by layer. The lower
    layer networks have smaller perceptual fields and stronger representation of geometric
    detail information, and they have higher resolution but weaker representation
    of semantic information. The higher layer networks have larger perceptual fields
    and stronger representation of semantic information, but they have lower resolution
    of feature maps and weaker representation of geometric information, leading to
    the information loss of small targets. Common multiscale fusion modules include
    bottom-up, top-down, and laterally connected feature pyramids (FPNs) [[136](#bib.bib136)],
    spatial pooling pyramids (ASPPs) [[137](#bib.bib137)] combining dilated convolution
    and multiscale fusion, and others. In multi-organ segmentation tasks, multiscale
    feature fusion has been widely used in multi-organ segmentation due to the different
    sizes of the organs of interest. Jia and Wei [[53](#bib.bib53)] introduced the
    feature pyramid into the multi-organ segmentation network using two opposite feature
    pyramids, top-down and bottom-up forms, which can effectively handle multiscale
    changes and improve the segmentation accuracy of small targets. Shi et al. [[120](#bib.bib120)]
    used the pyramidal structure of lateral connections between encoders and decoders
    to capture contextual information at multiple scales. Srivastava et al. [[138](#bib.bib138)]
    proposed a new segmentation architecture named OARFocalFuseNet, which uses a focal
    modulation scheme to aggregate multiscale contexts in a specific resolution stream
    when performing multiscale fusion.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 Attention Module
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention module can highlight important features by dynamically weighting
    them. This novel attention mechanism allows exploring the inherent self-attentiveness
    of the network and is essential for multi-organ segmentation tasks [[65](#bib.bib65),
    [139](#bib.bib139)]. Common attention mechanisms include channel attention, spatial
    attention, and self-attention.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Squeeze-and-excitation (SE) module [[140](#bib.bib140)] is a typical channel
    attention module which can focus on key parts of an image by generating a channel
    attention tensor. AnatomyNet [[48](#bib.bib48)] uses 3D SE residual blocks to
    segment the OARs of the head and neck, enabling the extraction of 3D features
    directly from CT images and adaptively calibrating the mapping of residual features
    within each feature channel. Liu et al. [[141](#bib.bib141)] proposed a new cross-layer
    spatial attentional map fusion network (CSAF-CNN) to segment multiple organs in
    the chest, which can effectively integrate the weights of different spatial attentional
    maps in the network, thus obtain more useful attentional maps. The average DSC
    of 22 organs in the head and neck was 72.50%, which was significantly better than
    U-Net (63.9%) and SE-UNet (67.9%). Gou et al. [[50](#bib.bib50)] designed a self-channel-spatial-attention
    neural network (SCSA-Net) for 3D head and neck OARs segmentation, which can adaptively
    enhance both channel and spatial features. Compared with SE-Res-Net and SE-Net,
    SCSA-Net improved the DSC of the optic nerve and submandibular gland by 0.06 and
    0.03 and 0.05 and 0.04, respectively. Lin et al. [[142](#bib.bib142)] suggested
    to embed the variance uncertainty into the attention architecture and proposed
    a variance-aware attention U-Net network to improve the attention to error-prone
    regions (e.g., boundary regions) in multi-organ segmentation. Compared with existing
    methods, the segmentation results of small organs and organs with irregular structures
    (e.g., duodenum, esophagus, gallbladder, and pancreas) are significantly improved.
    Zhang et al. [[51](#bib.bib51)] proposed a new hybrid network (Weaving attention
    U-Net, WAU-Net) with a U-Net++ [[143](#bib.bib143)] structure that uses CNNs to
    extract the underlying features, and uses axial attention blocks to efficiently
    model global relationships at different levels of the network, which achieve competitive
    performance in the head and neck multi-organ segmentation task.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: IV-C5 Other Modules
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dense block [[144](#bib.bib144)] can efficiently use the information of
    the intermediate layer, and the residual block [[145](#bib.bib145)] can prevent
    gradient disappearance during backpropagation. These two modules are often embedded
    in the basic segmentation framework. The convolution kernel of the deformable
    convolution [[146](#bib.bib146)] can adapt itself to the actual situation and
    better extract the input features. Heinrich et al. [[147](#bib.bib147)] proposed
    a 3D abdominal multi-organ segmentation architecture with sparse deformable convolutions
    (OBELISK-Net) and showed that the combination with conventional CNNs can further
    improve the segmentation of small organs with large shape variations (e.g., pancreas,
    esophagus). The deformable convolutional block proposed by Shen et al. [[148](#bib.bib148)]
    can handle variations in the shape and size of different organs by generating
    reasonable receptive fields for different organs with additional trainable offsets.
    The strip pooling (strip pooling) [[149](#bib.bib149)] module can target long
    strip structures (e.g., esophagus and spinal cord) by using long pooling instead
    of traditional square pooling to avoid merging contaminated information from unrelated
    regions and better capture anisotropic and remote contextual information. For
    example, Zhang et al. [[150](#bib.bib150)] used a pool of anisotropic strips with
    three different directional receptive fields to capture the spatial relationships
    between multiple organs in the abdomen. Compared to network architectures, network
    modules have been widely utilized because of their relatively simple design process
    and the relative ease of embedding them into various architectures.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Network Loss Function
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we all known, in addition to the network architecture or network modules,
    the segmentation accuracy also depends on the selected loss function. In multi-organ
    segmentation tasks, selecting a suitable loss function can reduce the class imbalance
    in deep learning and improve the segmentation accuracy of small organs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Jadon [[151](#bib.bib151)] summarized the commonly used loss functions in semantic
    segmentation, which are classified into distribution-based loss functions, region-based
    loss functions, boundary-based loss functions, and compound-based loss functions.
    Common loss functions used for multi-organ segmentation include CE loss [[152](#bib.bib152)],
    Dice loss [[153](#bib.bib153)], Tversky loss [[154](#bib.bib154)], focal loss
    [[155](#bib.bib155)] and their combined loss functions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: IV-D1 CE Loss
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CE loss (cross-entropy loss function) [[152](#bib.bib152)] is an information
    theoretic measure that calculates the difference between the prediction of the
    network and the ground truth. Men et al. [[115](#bib.bib115)], Moeskops et al.
    [[43](#bib.bib43)], Zhang et al. [[51](#bib.bib51)] used CE loss for multi-organ
    segmentation. However, when the number of foreground pixels is much smaller than
    the background, CE loss will heavily bias the model towards the background, resulting
    in poor segmentation results. The weighted CE loss [[156](#bib.bib156)] adds weight
    parameters to each category based on CE loss. so that it can obtain better results
    in the case of unbalanced sample sizes compared to the original CE loss. Since
    there is a significant class imbalance problem in multi-organ segmentation, i.e.,
    a very large difference in the number of voxels in different organs, using weighted
    CE loss will achieve better results than using only the CE loss. Trullo et al.
    [[100](#bib.bib100)] used a weighted CE loss to segment the heart, esophagus,
    trachea, and aorta in thechest image; Roth et al. [[52](#bib.bib52)] applied a
    weighted CE loss to abdomen multi-organ segmentation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: IV-D2 Dice Loss
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Milletari et al. [[56](#bib.bib56)] proposed the Dice loss as a volume-based
    overlap measure, converting the voxel measure to the semantic label overlap measure,
    and becoming a commonly loss function in the segmentation task. Ibragimov and
    Xing [[41](#bib.bib41)] used the Dice loss to segment multiple organs of the head
    and neck. However, the use of the Dice loss alone does not eliminate the problem
    that the inherent nature of neural networks is beneficial to large volume organs.
    Inspired by the weighted CE loss, Sudre et al. [[153](#bib.bib153)] introduced
    the weighted Dice score (GDSC), which adaptively weighed its Dice values according
    to the current class size. Shen et al. [[157](#bib.bib157)] investigated three
    different types of GDSC based on class label frequencies (uniform, simple, and
    square) and evaluated their effects on segmentation accuracy. Gou et al. [[50](#bib.bib50)]
    used GDSC for head and neck multi-organ segmentation. Tappeiner et al. [[158](#bib.bib158)]
    introduced the class adaptive Dice loss to further compensate for high imbalances
    based on nnU-Net, and the results showed that the method could improve the performance
    of class imbalance segmentation tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: IV-D3 Other Losses
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Tversky loss [[154](#bib.bib154)] is a generalization of the Dice loss and
    can be optimized by adjusting the parameters to control the balance between false
    positives and false negatives. The focal loss [[155](#bib.bib155)] was proposed
    in the field of object detection to enhance the attention on samples that are
    difficult to segment. Similar to the focal loss, the focal Tversky loss [[159](#bib.bib159)]
    focuses on segmenting difficult samples by reducing the weights of simple sample
    losses. Berzoini et al. [[54](#bib.bib54)] used the focal Tversky loss on smaller
    organs, thus balancing the indices between organs of different sizes, increasing
    the weights of small samples that are difficult to segment and finally solving
    the class imbalance problem caused by the kidney and bladder. Inspired by the
    exponential logarithmic loss (ELD-Loss) [[160](#bib.bib160)], Liu et al. [[141](#bib.bib141)]
    introduced the top-k exponential logarithmic loss (TELD-Loss) to solve the class
    imbalance problem in the head and neck. The results showed that using this loss
    function has a strong ability to handle mislabelling.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: IV-D4 Combined Loss
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each type of loss function has its own advantages and disadvantages. Combining
    multiple functions can be used for multi-organ segmentation. A more common method
    is the weighted sum of the Dice loss and CE loss, which attempts to solve the
    class imbalance problem with the Dice loss while using the CE loss for curve smoothing.
    Isensee et al. [101] proposed combining the Dice loss and CE loss to measure the
    overlap of voxel-like predicted outcomes and ground truth. Isler et al. [[134](#bib.bib134)],
    Srivastava et al. [[138](#bib.bib138)], Xu et al. [[58](#bib.bib58)], Lin et al.
    [[142](#bib.bib142)], and Song et al. [[161](#bib.bib161)] used the weighted combination
    of the Dice loss and CE loss for multi-organ segmentation. When small objects
    are involved, using only the Dice loss leads to a lower accuracy; when the predicted
    region does not overlap with the labelled region, using the CE loss allows the
    prediction to be as close to the label as possible. Zhu et al. [[48](#bib.bib48)]
    specifically studied different loss functions for the unbalanced head and neck
    region, and pointed out that the combination of the Dice loss and focal loss was
    superior to the ordinary Dice loss. Both Cheng et al. [[128](#bib.bib128)] and
    Chen et al. [[116](#bib.bib116)] used this combined loss function.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The conventional Dice loss is detrimental for smaller structures because a small
    amount of voxel misclassification leads to a large decrease in the Dice score.
    Applying the exponential logarithmic loss or combining the focal loss with the
    Dice loss can solve this problem. Using this kind of loss function does not require
    much adjustment to the network, however, it reduces the segmentation accuracy
    of the hard voxels in the region. On this basis, Lei et al. [[162](#bib.bib162)]
    proposed a new hardness-aware loss function that can focus more on hard voxels
    to achieve accurate segmentation. The ultimate goal of neural network optimization
    is the loss function, and designing a suitable loss function so that the network
    can improve the segmentation accuracy of various organs is still a research direction.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: V Imperfect Annotation-based Methods
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, most of the methods in the multi-organ segmentation field are based
    on fully annotated methods. However, medical image data is usually hard to acquire
    and annotate. In particular, for multi-organ segmentation tasks, obtaining fully
    annotated datasets is quite difficult, which inspired the idea of using imperfect
    annotation. In this paper, imperfect annotations are classified into two categories.
    The first category is weak annotation-based methods, where weak annotation indicates
    that the data annotation is incomplete or imprecise in each case. For example,
    in multi-organ segmentation, each image has only one kind of organ annotated;
    each image has no pixel-level annotation but only category annotation; or the
    annotation is scribbled or contains noise. Another category is semi supervised-based
    methods, where semi supervision indicates that only a small portion of the total
    data is annotated and most of the remaining is unannotated. In the following,
    we introduce the application of these two types of methods in multi-organ segmentation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: V-A Weak Annotation-Based Methods
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In medical image segmentation, it is a difficult task to obtain the annotation
    of multiple organs simultaneously on the same set of images. For example, many
    existing single-organ datasets, such as LiTS [[163](#bib.bib163)], KiTS [[164](#bib.bib164)]
    (p19), and pancreas datasets [[165](#bib.bib165)], can only provide annotations
    for a single organ. However, multi-organ segmentation networks cannot be effectively
    trained solely based on these single-organ annotated datasets. Therefore, many
    studies have started to explore learning unified multi-organ segmentation networks
    from partially labelled datasets. Based on the implementation methods, we divide
    the current studies into model-based approaches and pseudo label-based approaches.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Model-Based Methods
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea of the model-based approach is to realize a unified network for multiple
    partially labelled organs. Chen et al. [[166](#bib.bib166)] introduced a multi-branch
    decoder structure with a shared encoder and eight decoders to address the partial
    labelling problem. However, this structure is not flexible enough to be extended
    to new classes. Dmitriev and Kaufman [[167](#bib.bib167)] proposed conditional
    CNNs for learning multi-organ segmentation models, which integrate information
    of organ categories into the segmentation network. Zhang and Xie et al. [[168](#bib.bib168),
    [169](#bib.bib169)] proposed the idea of DoDNet. Similar to conditional CNN, they
    spliced the task encoding with the features extracted by the encoder, and introduced
    a dynamic parameter mechanism in the segmentation head. Zhang et al. [103] used
    the leading framework nn-UNet [[170](#bib.bib170)] as the backbone model, adding
    task encoding as supporting information to the decoder of nn-UNet, and combined
    the deep supervision mechanism to further refine the output of organs of different
    sizes. Wu et al. [[171](#bib.bib171)] proposed TGNet composed of task-guided attention
    module and task-guided residual block, which can highlight task-relevant features
    while suppressing task-irrelevant information during feature extraction. Liu et
    al. [[172](#bib.bib172)] first introduced incremental learning (IL) to aggregate
    partially labelled datasets in stages, and verified that the distribution of different
    partially labelled datasets misleads the process of IL. Xu and Yan [[173](#bib.bib173)]
    proposed a new federated multi-encoding U-Net (Fed-MENU) method that can effectively
    use independent datasets with different partial labels to train a unified model
    for multi-organ segmentation. The model outperformed any model trained on a single
    dataset as well as the model trained on all datasets combined. Fang and Yan [[174](#bib.bib174)]
    and Shi et al. [[175](#bib.bib175)] trained uniform models on partially labelled
    datasets by designing new network and proposing specific loss function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Pseudo Label-Based Methods
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pseudo label-based methods generate pseudo labels of unlabelled organs by
    using partial-organ segmentation models trained in partially labelled datasets,
    which can be converted to fully supervised methods. Zhou et al. [[129](#bib.bib129)]
    proposed an a Prior-aware Neural Network (PaNN), which utilized prior statistics
    obtained from a fully labelled dataset to guide the training process based on
    partially labelled datasets. Huang et al. [[176](#bib.bib176)] proposed a weight-averaging
    joint training framework, which can correct the noise in the pseudo labels, so
    as to learn a more robust model. Zhang et al. [[177](#bib.bib177)] proposed a
    multi-teacher knowledge distillation framework that utilizes pseudo labels predicted
    by teacher models trained on partially labelled datasets to train student models
    for multi-organ segmentation. Lian et al. [[130](#bib.bib130)] proposed a multi-organ
    segmentation model (PRIMP) based on single and multiple organs anatomical priors.
    The model first generates pseudo labels for each partially labelled dataset so
    as to obtain a set of multi-organ datasets with pseudo label. Then the multi-organ
    segmentation model is trained on this dataset, and tested on another new dataset.
    For the first time, this method considers the domain discrepancy between partially
    labelled datasets and the tested multi-organ datasets.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In addition to partial annotation, weak annotation also includes image-level
    annotation, sparse annotation, and noisy annotation [[178](#bib.bib178)]. Regarding
    multi-organ segmentation tasks, Kanavati et al. [[179](#bib.bib179)] proposed
    a weakly supervised organ segmentation method based on classification forests
    for the liver, spleen, and kidney, in which the labels are scribbled on the organs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: V-B Semi Supervised-Based Methods
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semi supervised multi-organ segmentation methods make full use of unlabelled
    data to improve the segmentation performance, thus reducing the need for extensive
    annotation. In recent years, semi supervised learning has been widely used in
    medical image segmentation, such as heart segmentation [[180](#bib.bib180), [181](#bib.bib181),
    [182](#bib.bib182)], pancreas segmentation [[183](#bib.bib183)], and tumour target
    region segmentation [[184](#bib.bib184)]. A detailed review of semi supervised
    learning in medical images was presented by Jiao et al. [[185](#bib.bib185)],
    who classified semi supervised medical image segmentation methods into three paradigms:
    pseudo label-based methods, consistency regularization-based methods, and knowledge
    prior-based methods. In this review, we focus on semi supervised multi-organ segmentation
    methods.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Ma et al. [[36](#bib.bib36)] established a new benchmark for semi supervised
    abdominal multi-organ segmentation, which developed a method based on pseudo labelling.
    The teacher model was first trained on the labelled data, and generated the pseudo
    labels for the unlabelled data. Then, the student model was trained on both the
    real labelled and pseudo labelled data. Finally, the teacher model was substituted
    with the student model to complete the training. The results on the liver, kidney,
    spleen, and pancreas show that using unlabelled data can improve the performance
    of multi-organ segmentation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Multi-view methods are also widely used in semi supervised multi-organ segmentation,
    where the model is made to learn in a collaborative training manner to extract
    useful information from multiple planes (e.g., sagittal, coronal, and axial planes),
    and then use multi-plane fusion to generate more reliable pseudo labels, and thus
    train better segmentation networks. Zhou et al. [[186](#bib.bib186)] designed
    a system framework, DMPCT, for multi-organ segmentation of abdominal CT scans
    by fusing multi-planar information on unlabelled data during training. The framework
    uses a multi-planar fusion module to synthesize inferences and iteratively update
    pseudo labels for multiple configurations of unlabelled data. Xia et al. [[187](#bib.bib187)]
    proposed an uncertainty-aware multi-view collaborative training (UMCT) method
    based on uncertainty perception, which first obtains multiple views by spatial
    transformations such as rotation and alignment, then trains a 3D deep segmentation
    network on each view, and performs joint training by implementing multi-view consistency
    on unlabelled data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the collaborative training approach, multi-organ segmentation
    is also suitable for consistency-based learning due to the large number of prospect
    categories and dense distribution of organs. Consistency learning encourages consistent
    output through networks with different parameters. Lai et al. [[188](#bib.bib188)]
    developed a semi supervised learning-based DLUNet for abdominal multi-organ segmentation,
    which consists of two lightweight U-Nets in the training phase. Moreover, regarding
    unlabelled data, the outputs obtained from two networks are used to supervise
    each other, which can improve the accuracy of these unlabelled data. It eventually
    achieves an average DSC of 0.8718 for 13 organs in the abdomen.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are other semi supervised multi-organ segmentation-based
    methods. Lee et al. [[189](#bib.bib189)] proposed a discriminator module based
    on human-in-the-loop quality assurance (QA) to supervise the learning of unlabelled
    data. They used QA scores as a loss function for unlabelled data. Raju Cheng et
    al. [[190](#bib.bib190)] proposed a powerful semi supervised organ segmentation
    method, CHASe, for liver and lesion segmentation. It integrates co-training and
    heteromodality learning into a co-heterogeneous training framework. The framework
    is trained on a small single-phase dataset and can be adapted to label-free multicentre
    and multiphase clinical data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: VI Discussion and Future Trends
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, a systematic review of deep learning methods for multi-organ
    segmentation is presented from the perspectives of both full annotation and imperfect
    annotation. The main innovations of the full annotation method focus on the design
    of network architectures, the combination of network dimensions, the innovation
    of network modules and the proposal of new loss functions. In terms of the network
    architecture design, with the development of the transformer [[75](#bib.bib75)]
    architectures, better utilization of these advanced architectures for multi-organ
    segmentation is a promising direction, as well as the automatic search for the
    optimal architecture for each organ through neural network architecture search
    (NAS) [[191](#bib.bib191)]. In the network dimension, optimally combining 2D and
    3D architectures is a worthwhile research direction. In terms of network module,
    more dedicated modules need to design to improve the segmentation accuracy according
    to the multi-organ segmentation task. In terms of the loss functions, targeting
    the class imbalance, geometric prior or introducing adversarial learning loss
    will have great potential for designing more comprehensive and diverse loss functions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Full annotation methods rely on fully annotated and high-quality datasets. Many
    imperfect annotation-based methods have been proposed for medical image segmentation
    in the last two years, including the aforementioned multi-organ segmentation based
    on weak annotation-based methods and semi annotation-based methods. However, compared
    to full annotation-based methods, the imperfect annotation-based methods have
    been less studied. It is a future research focus if imperfect annotation-based
    methods can be used more adequately to achieve the performance close to that of
    the full annotation-based methods.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning has already played a significant role in multi-organ segmentation
    task, but many challenges remain to be explored in the future, which are summarized
    in follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Higher Segmentation Accuracy
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The current multi-organ segmentation method is more effective in solving the
    segmentation of large organs and organs with standard contours, such as the brainstem
    and mandible in the head and neck; the left and right lungs and heart in the chest;
    and the liver, spleen, and stomach in the abdomen. Moreover, the DSC of various
    methods can basically reach 0.8 or higher, while for small organs, such as the
    optical chiasm in the head and neck (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based
    Multi-organ Segmentation")(8)), the left and right optic nerves (see Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive
    Survey of Deep Learning-based Multi-organ Segmentation")(6 and 7)), the DSC can
    only reach about 0.7; irregular organs such as the pancreas in the abdomen (Fig.
    [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Towards More Precise Automatic Analysis:
    A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation")(4)),
    and long striped organs such as the spinal cord (Fig. [2](#S1.F2 "Figure 2 ‣ I
    Introduction ‣ Towards More Precise Automatic Analysis: A Comprehensive Survey
    of Deep Learning-based Multi-organ Segmentation")(6)), the segmentation results
    are also not very satisfactory. The future research direction is to enhance the
    segmentation accuracy of these types of organs using more advanced automatic segmentation
    frameworks.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: VI-B More Comprehensive Public Datasets
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, public datasets covering multiple organs are not sufficient. And
    the vast majority of methods are validated on their private datasets, making it
    difficult to verify the generalizability of the models. Therefore, there is a
    need to establish multicentre public datasets of multi-organ segmentation with
    large data volumes, wide coverage, and strong clinical relevance in the future.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Better Use of Imperfect Annotations
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The vast majority of current methods are based on full annotation methods. Since
    medical image data are usually not easy to collect and annotating all the organs
    on the same image is a time-consuming and laborious work. Further studies can
    be performed to better utilize imperfect annotations [[192](#bib.bib192), [193](#bib.bib193)],
    including the use of weakly annotated datasets and semi annotated datasets.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Study of Transfer Learning Models
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing deep learning models usually trained on one part of the body, which
    usually tend to obtain poor results when migrated to other datasets or applied
    to other parts of the body. Therefore, transfer learning models need to be explored
    in the future. For example, Fu et al. [[194](#bib.bib194)] proposed a new method
    called domain adaptive relational reasoning (DARR). It is used to generalize 3D
    multi-organ segmentation models to medical data from different domains. In addition,
    a very significant problem with medical images compared to other natural images
    is that many private datasets are not publicly available, and many hospitals only
    release trained models. Therefore, source free domain adaptation problem will
    be a very important research direction in the future. For example, Hong et al.
    [[195](#bib.bib195)] proposed a source free unsupervised domain adaptive cross-modal
    approach for abdomen multi-organ segmentation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we systematically review 214 deep learning-based multi-organ
    segmentation studies in two broad categories, namely full annotation-based methods
    and imperfect annotation-based methods for multiple parts, including the head
    and neck, thorax and abdomen. In the fully labelled methods, we summarize the
    existing methods according to network architectures, network modules, network
    dimensions, and loss functions. In the imperfect annotation-based methods, we
    summarize both weak annotation-based methods and semi annotated-based methods.
    On this basis, we also put forward tailored solutions for some current difficulties
    and shortcomings in this field, and illustrate the future trends. The comprehensive
    survey shows that multi-organ segmentation algorithm based on deep learning is
    rapidly developing towards a new era of more accurate, more detailed and more
    automated analysis.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Natural Science Foundation of China
    under grant 82072021\. This work was also supported by the medical-industrial
    integration project of Fudan University under grant XM03211181.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] B. Van Ginneken, C. M. Schaefer-Prokop, and M. Prokop, “Computer-aided
    diagnosis: how to move from the laboratory to the clinic,” *Radiology*, vol. 261,
    no. 3, pp. 719–732, 2011.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Sykes, “Reflections on the current status of commercial automated segmentation
    systems in clinical practice,” pp. 131–134, 2014.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. G. Pfister, S. Spencer, D. Adelstein, D. Adkins, Y. Anzai, D. M. Brizel,
    J. Y. Bruce, P. M. Busse, J. J. Caudell, A. J. Cmelak *et al.*, “Head and neck
    cancers, version 2.2020, nccn clinical practice guidelines in oncology,” *Journal
    of the National Comprehensive Cancer Network*, vol. 18, no. 7, pp. 873–898, 2020.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. K. Molitoris, T. Diwanji, J. W. Snider III, S. Mossahebi, S. Samanta,
    S. N. Badiyan, C. B. Simone, P. Mohindra *et al.*, “Advances in the use of motion
    management and image guidance in radiation therapy treatment for lung cancer,”
    *Journal of thoracic disease*, vol. 10, no. Suppl 21, pp. S2437–S2450, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. A. Vyfhuis, N. Onyeuku, T. Diwanji, S. Mossahebi, N. P. Amin, S. N.
    Badiyan, P. Mohindra, and C. B. Simone, “Advances in proton therapy in lung cancer,”
    *Therapeutic advances in respiratory disease*, vol. 12, p. 1753466618783878, 2018.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C. W. Hurkmans, J. H. Borger, B. R. Pieters, N. S. Russell, E. P. Jansen,
    and B. J. Mijnheer, “Variability in target volume delineation on ct scans of the
    breast,” *International Journal of Radiation Oncology Biology Physics*, vol. 50,
    no. 5, pp. 1366–1372, 2001.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Rasch, R. Steenbakkers, and M. van Herk, “Target definition in prostate,
    head, and neck,” in *Seminars in radiation oncology*, vol. 15, no. 3.   Elsevier,
    2005, pp. 136–145.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Van de Steene, N. Linthout, J. De Mey, V. Vinh-Hung, C. Claassens, M. Noppen,
    A. Bel, and G. Storme, “Definition of gross tumor volume in lung cancer: inter-observer
    variability,” *Radiotherapy and oncology*, vol. 62, no. 1, pp. 37–49, 2002.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] J. Breunig, S. Hernandez, J. Lin, S. Alsager, C. Dumstorf, J. Price, J. Steber,
    R. Garza, S. Nagda, E. Melian *et al.*, “A system for continual quality improvement
    of normal tissue delineation for radiation therapy treatment planning,” *International
    Journal of Radiation Oncology Biology Physics*, vol. 83, no. 5, pp. e703–e708,
    2012.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Chen and L. Pan, “A survey of graph cuts/graph search based medical
    image segmentation,” *IEEE reviews in biomedical engineering*, vol. 11, pp. 112–124,
    2018.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] I. El Naqa, D. Yang, A. Apte, D. Khullar, S. Mutic, J. Zheng, J. D. Bradley,
    P. Grigsby, and J. O. Deasy, “Concurrent multimodality image segmentation by active
    contours for radiotherapy treatment planning a,” *Medical physics*, vol. 34, no. 12,
    pp. 4738–4749, 2007.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Pratondo, C.-K. Chui, and S.-H. Ong, “Robust edge-stop functions for
    edge-based active contour models in medical image segmentation,” *IEEE Signal
    Processing Letters*, vol. 23, no. 2, pp. 222–226, 2015.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, W. E. Grimson,
    and A. Willsky, “A shape-based approach to the segmentation of medical imagery
    using level sets,” *IEEE transactions on medical imaging*, vol. 22, no. 2, pp.
    137–154, 2003.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. M. Saranathan and M. Parente, “Threshold based segmentation method
    for hyperspectral images,” in *2013 5Th workshop on hyperspectral image and signal
    processing: evolution in remote sensing (WHISPERS)*.   Gainesville: IEEE, 2013,
    pp. 1–4.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Shi and J. Malik, “Normalized cuts and image segmentation,” *IEEE Transactions
    on pattern analysis and machine intelligence*, vol. 22, no. 8, pp. 888–905, 2000.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A. J. Vyavahare and R. Thool, “Segmentation using region growing algorithm
    based on clahe for medical images,” in *Fourth International Conference on Advances
    in Recent Technologies in Communication and Computing (ARTCom2012)*.   Bangalore,
    India: IET, 2012, pp. 182–185.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] I. Isgum, M. Staring, A. Rutten, M. Prokop, M. A. Viergever, and B. Van Ginneken,
    “Multi-atlas-based segmentation with local decision fusion—application to cardiac
    and aortic segmentation in ct scans,” *IEEE transactions on medical imaging*,
    vol. 28, no. 7, pp. 1000–1010, 2009.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] P. Aljabar, R. A. Heckemann, A. Hammers, J. V. Hajnal, and D. Rueckert,
    “Multi-atlas based segmentation of brain images: atlas selection and its effect
    on accuracy,” *Neuroimage*, vol. 46, no. 3, pp. 726–738, 2009.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] O. Ecabert, J. Peters, H. Schramm, C. Lorenz, J. von Berg, M. J. Walker,
    M. Vembar, M. E. Olszewski, K. Subramanyan, G. Lavi *et al.*, “Automatic model-based
    segmentation of the heart in ct images,” *IEEE transactions on medical imaging*,
    vol. 27, no. 9, pp. 1189–1201, 2008.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. A. Qazi, V. Pekar, J. Kim, J. Xie, S. L. Breen, and D. A. Jaffray,
    “Auto-segmentation of normal and target structures in head and neck ct images:
    a feature-driven model-based approach,” *Medical physics*, vol. 38, no. 11, pp.
    6160–6170, 2011.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] E. A. Smirnov, D. M. Timoshenko, and S. N. Andrianov, “Comparison of regularization
    methods for imagenet classification with deep convolutional neural networks,”
    *Aasri Procedia*, vol. 6, pp. 89–94, 2014.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Mobiny and H. Van Nguyen, “Fast capsnet for lung cancer screening,”
    in *Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st
    International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part II 11*.   Cham: Springer, 2018, pp. 741–749.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Z. Alom, C. Yakopcic, M. Hasan, T. M. Taha, and V. K. Asari, “Recurrent
    residual u-net for medical image segmentation,” *Journal of Medical Imaging*,
    vol. 6, no. 1, p. 014006, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. Wang, T. Lei, R. Cui, B. Zhang, H. Meng, and A. K. Nandi, “Medical
    image segmentation using deep learning: A survey,” *IET Image Processing*, vol. 16,
    no. 5, pp. 1243–1267, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Huang, F. Yang, M. Yin, X. Mo, and C. Zhong, “A review of multimodal
    medical image fusion techniques,” *Computational and mathematical methods in medicine*,
    vol. 2020, p. 8279342, 2020.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang, “Deep learning
    in medical image registration: a review,” *Physics in Medicine & Biology*, vol. 65,
    no. 20, p. 20TR01, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Lei, Y. Fu, T. Wang, R. L. Qiu, W. J. Curran, T. Liu, and X. Yang,
    “Deep learning in multi-organ segmentation,” *arXiv preprint arXiv:2001.10619*,
    2020.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang, “A review of
    deep learning based methods for medical image multi-organ segmentation,” *Physica
    Medica*, vol. 85, pp. 107–122, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] B. Landman, Z. Xu, J. E. Igelsias, M. Styner, T. Langerak, and A. Klein,
    “Segmentation outside the cranial vault challenge,” *Synapse*, 2015.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. F. Raudaschl, P. Zaffino, G. C. Sharp, M. F. Spadea, A. Chen, B. M.
    Dawant, T. Albrecht, T. Gass, C. Langguth, M. Lüthi *et al.*, “Evaluation of segmentation
    methods on head and neck ct: auto-segmentation challenge 2015,” *Medical physics*,
    vol. 44, no. 5, pp. 2020–2036, 2017.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Yang, H. Veeraraghavan, S. G. Armato III, K. Farahani, J. S. Kirby,
    J. Kalpathy-Kramer, W. van Elmpt, A. Dekker, X. Han, X. Feng *et al.*, “Autosegmentation
    for thoracic radiation treatment planning: a grand challenge at aapm 2017,” *Medical
    physics*, vol. 45, no. 10, pp. 4568–4581, 2018.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. E. Kavur, N. S. Gezer, M. Barış, S. Aslan, P.-H. Conze, V. Groza, D. D.
    Pham, S. Chatterjee, P. Ernst, S. Özkan *et al.*, “Chaos challenge-combined (ct-mr)
    healthy abdominal organ segmentation,” *Medical Image Analysis*, vol. 69, p. 101950,
    2021.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] X. Feng, K. Qing, N. J. Tustison, C. H. Meyer, and Q. Chen, “Deep convolutional
    neural network for segmentation of thoracic organs-at-risk using cropped 3d images,”
    *Medical physics*, vol. 46, no. 5, pp. 2169–2180, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] X. Xu, F. Zhou, B. Liu, and X. Bai, “Annotations for body organ localization
    based on miccai lits dataset,” *IEEE Dataport*, 2018.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Babier, B. Zhang, R. Mahmood, K. L. Moore, T. G. Purdie, A. L. McNiven,
    and T. C. Chan, “Openkbp: the open-access knowledge-based planning grand challenge
    and dataset,” *Medical Physics*, vol. 48, no. 9, pp. 5549–5561, 2021.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Ma, Y. Zhang, S. Gu, C. Zhu, C. Ge, Y. Zhang, X. An, C. Wang, Q. Wang,
    X. Liu *et al.*, “Abdomenct-1k: Is abdominal organ segmentation a solved problem?”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 44, no. 10,
    pp. 6695–6714, 2021.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural computation*, vol. 1, no. 4, pp. 541–551, 1989.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] R. Karthik, R. Menaka, A. Johnson, and S. Anand, “Neuroimaging and deep
    learning for brain stroke detection-a review of recent advancements and future
    prospects,” *Computer Methods and Programs in Biomedicine*, vol. 197, p. 105728,
    2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Zhao, K. Chen, G. Wu, G. Zhang, X. Zhou, C. Lv, S. Wu, Y. Chen, G. Xie,
    and Z. Yao, “Deep learning shows good reliability for automatic segmentation and
    volume measurement of brain hemorrhage, intraventricular extension, and peripheral
    edema,” *European radiology*, vol. 31, no. 7, pp. 5012–5020, 2021.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. R.
    Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Towards image-guided
    pancreas and biliary endoscopy: automatic multi-organ segmentation on abdominal
    ct with dense dilated networks,” in *Medical Image Computing and Computer Assisted
    Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
    September 11-13, 2017, Proceedings, Part I 20*.   Cham, Switzerland: Springer,
    2017, pp. 728–736.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] B. Ibragimov and L. Xing, “Segmentation of organs-at-risks in head and
    neck ct images using convolutional neural networks,” *Medical physics*, vol. 44,
    no. 2, pp. 547–557, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Fritscher, P. Raudaschl, P. Zaffino, M. F. Spadea, G. C. Sharp, and
    R. Schubert, “Deep neural networks for fast segmentation of 3d medical images,”
    in *Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th
    International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part
    II 19*.   Cham, Switzerland: Springer, 2016, pp. 158–165.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] P. Moeskops, J. M. Wolterink, B. H. Van Der Velden, K. G. Gilhuijs, T. Leiner,
    M. A. Viergever, and I. Išgum, “Deep learning for multi-task medical image segmentation
    in multiple modalities,” in *Medical Image Computing and Computer-Assisted Intervention–MICCAI
    2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings,
    Part II 19*.   Cham, Switzerland: Springer, 2016, pp. 478–486.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Long, Jonathan, Shelhamer, Evan, Darrell, and Trevor, “Fully convolutional
    networks for semantic segmentation,” *IEEE Transactions on Pattern Analysis &
    Machine Intelligence*, vol. 39, no. 4, pp. 640–651, 2017.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Y. Wang, Y. Zhou, P. Tang, W. Shen, E. K. Fishman, and A. L. Yuille, “Training
    multi-organ segmentation networks with sample selection by relaxed upper confident
    bound,” in *Medical Image Computing and Computer Assisted Intervention–MICCAI
    2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings,
    Part IV 11*.   Cham, Switzerland: Springer, 2018, pp. 434–442.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár, “Learning to refine
    object segments,” in *Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11–14, 2016, Proceedings, Part I 14*.   Cham, Switzerland:
    Springer, 2016, pp. 75–91.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18*.   Cham, Switzerland: Springer, 2015, pp.
    234–241.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] W. Zhu, Y. Huang, L. Zeng, X. Chen, Y. Liu, Z. Qian, N. Du, W. Fan, and
    X. Xie, “Anatomynet: deep learning for fast and fully automated whole-volume segmentation
    of head and neck anatomy,” *Medical physics*, vol. 46, no. 2, pp. 576–589, 2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] W. van Rooij, M. Dahele, H. R. Brandao, A. R. Delaney, B. J. Slotman,
    and W. F. Verbakel, “Deep learning-based delineation of head and neck organs at
    risk: geometric and dosimetric evaluation,” *International Journal of Radiation
    Oncology Biology Physics*, vol. 104, no. 3, pp. 677–684, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Gou, N. Tong, S. Qi, S. Yang, R. Chin, and K. Sheng, “Self-channel-and-spatial-attention
    neural network for automated multi-organ segmentation on head and neck ct images,”
    *Physics in Medicine & Biology*, vol. 65, no. 24, p. 245034, 2020.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Zhang, T. Zhao, H. Gay, W. Zhang, and B. Sun, “Weaving attention u-net:
    A novel hybrid cnn and attention-based method for organs-at-risk segmentation
    in head and neck ct images,” *Medical physics*, vol. 48, no. 11, pp. 7052–7062,
    2021.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. R. Roth, C. Shen, H. Oda, M. Oda, Y. Hayashi, K. Misawa, and K. Mori,
    “Deep learning and its application to medical image segmentation,” *Medical Imaging
    Technology*, vol. 36, no. 2, pp. 63–71, 2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. Jia and J. Wei, “Amo-net: abdominal multi-organ segmentation in mri
    with a extend unet,” in *2021 IEEE 4th Advanced Information Management, Communicates,
    Electronic and Automation Control Conference (IMCEC)*, vol. 4.   Chongqing, China:
    IEEE, 2021, pp. 1770–1775.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] R. Berzoini, A. A. Colombo, S. Bardini, A. Conelli, E. D’Arnese, and M. D.
    Santambrogio, “An optimized u-net for unbalanced multi-organ segmentation,” in
    *2022 44th Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC)*.   Glasgow, Scotland: IEEE, 2022, pp. 3764–3767.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Lambert, C. Petitjean, B. Dubray, and S. Kuan, “Segthor: Segmentation
    of thoracic organs at risk in ct images,” in *2020 Tenth International Conference
    on Image Processing Theory, Tools and Applications (IPTA)*.   Paris, France: IEEE,
    2020, pp. 1–6.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
    neural networks for volumetric medical image segmentation,” in *2016 fourth international
    conference on 3D vision (3DV)*.   Stanford, CA: IEEE, 2016, pp. 565–571.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. Davidson,
    S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Automatic multi-organ segmentation
    on abdominal ct with dense v-networks,” *IEEE transactions on medical imaging*,
    vol. 37, no. 8, pp. 1822–1834, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Xu, H. Guo, J. Zhang, K. Yan, and L. Lu, “A new probabilistic v-net
    model with hierarchical spatial feature transform for efficient abdominal multi-organ
    segmentation,” *arXiv preprint arXiv:2208.01382*, 2022.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] G. Podobnik, B. Ibragimov, P. Strojan, P. Peterlin, and T. Vrtovec, “Segmentation
    of organs-at-risk from ct and mr images of the head and neck: Baseline results,”
    in *2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   Kolkata,
    India: IEEE, 2022, pp. 1–4.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] F. Isensee, P. F. Jäger, P. M. Full, P. Vollmuth, and K. H. Maier-Hein,
    “nnu-net for brain tumor segmentation,” in *Brainlesion: Glioma, Multiple Sclerosis,
    Stroke and Traumatic Brain Injuries: 6th International Workshop, BrainLes 2020,
    Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected
    Papers, Part II 6*.   Cham, Switzerland: Springer, 2021, pp. 118–132.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Zhang, Z. Yang, B. Huo, S. Chai, and S. Jiang, “Multiorgan segmentation
    from partially labeled datasets with conditional nnu-net,” *Computers in Biology
    and Medicine*, vol. 136, p. 104658, 2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] N. Altini, A. Brunetti, V. P. Napoletano, F. Girardi, E. Allegretti, S. M.
    Hussain, G. Brunetti, V. Triggiani, V. Bevilacqua, and D. Buongiorno, “A fusion
    biopsy framework for prostate cancer based on deformable superellipses and nnu-net,”
    *Bioengineering*, vol. 9, no. 8, p. 343, 2022.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *arXiv preprint
    arXiv:1406.2661*, 2014.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] N. Tong, S. Gou, S. Yang, M. Cao, and K. Sheng, “Shape constrained fully
    convolutional densenet with adversarial training for multiorgan segmentation on
    head and neck ct and low-field mr images,” *Medical physics*, vol. 46, no. 6,
    pp. 2669–2682, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] J. Cai, Y. Xia, D. Yang, D. Xu, L. Yang, and H. Roth, “End-to-end adversarial
    shape learning for abdomen organ deep segmentation,” in *Machine Learning in Medical
    Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI
    2019, Shenzhen, China, October 13, 2019, Proceedings 10*.   Cham, Switzerland:
    Springer, 2019, pp. 124–132.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Dong, Y. Lei, T. Wang, M. Thomas, L. Tang, W. J. Curran, T. Liu, and
    X. Yang, “Automatic multiorgan segmentation in thorax ct images using u-net-gan,”
    *Medical physics*, vol. 46, no. 5, pp. 2157–2168, 2019.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Trullo, C. Petitjean, B. Dubray, and S. Ruan, “Multiorgan segmentation
    using distance-aware adversarial networks,” *Journal of Medical Imaging*, vol. 6,
    no. 1, p. 014001, 2019.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] F. Mahmood, D. Borders, R. J. Chen, G. N. McKay, K. J. Salimian, A. Baras,
    and N. J. Durr, “Deep adversarial training for multi-organ nuclei segmentation
    in histopathology images,” *IEEE transactions on medical imaging*, vol. 39, no. 11,
    pp. 3257–3267, 2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N. Metaxas,
    H. Li, and M. Chen, “Focusnetv2: Imbalanced large and small organ segmentation
    with adversarial shape constraint for head and neck ct images,” *Medical Image
    Analysis*, vol. 67, p. 101831, 2021.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Fang, Y. Fang, and X. Yang, “Multi-organ segmentation network with
    adversarial performance validator,” *arXiv preprint arXiv:2204.07850*, 2022.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. Vaswani, N. Shazeer, and N. Parmar, “Attention is all uou need,” *arXiv:170603762*,
    2021.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] I. Bello, “Lambdanetworks: Modeling long-range interactions without attention,”
    *arXiv preprint arXiv:2102.08602*, 2021.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Gao, M. Zhou, and D. N. Metaxas, “Utnet: a hybrid transformer architecture
    for medical image segmentation,” in *Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part III 24*.   Cham, Switzerland: Springer,
    2021, pp. 61–71.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. Yao, M. Hu, G. Zhai, and X. Zhang, “Transclaw u-net: Claw u-net with
    transformers for medical image segmentation,” *arXiv preprint arXiv:2107.05188*,
    2021.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical
    transformer: Gated axial-attention for medical image segmentation,” in *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    I 24*.   Springer, 2021, pp. 36–46.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Pan, Y. Lei, T. Wang, J. Wynne, C.-W. Chang, J. Roper, A. B. Jani,
    P. Patel, J. D. Bradley, T. Liu *et al.*, “Male pelvic multi-organ segmentation
    using token-based transformer vnet,” *Physics in Medicine & Biology*, vol. 67,
    no. 20, p. 205012, 2022.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, “Swin-unet:
    Unet-like pure transformer for medical image segmentation,” *arXiv preprint arXiv:2105.05537*,
    2021.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Huang, Z. Deng, D. Li, and X. Yuan, “Missformer: An effective medical
    image segmentation transformer,” *arXiv preprint arXiv:2109.07162*, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Xie, J. Zhang, C. Shen, and Y. Xia, “Cotr: Efficiently bridging cnn
    and transformer for 3d medical image segmentation,” in *Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III 24*.   Cham,
    Switzerland: Springer, 2021, pp. 171–180.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “Uctransnet: rethinking the
    skip connections in u-net from a channel-wise perspective with transformer,” in
    *Proceedings of the AAAI conference on artificial intelligence*, vol. 36, no. 3,
    2022, pp. 2441–2449.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H. Wang, S. Xie, L. Lin, Y. Iwamoto, X.-H. Han, Y.-W. Chen, and R. Tong,
    “Mixed transformer u-net for medical image segmentation,” in *ICASSP 2022-2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   Singapore:
    IEEE, 2022, pp. 2390–2394.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] G. Xu, X. Wu, X. Zhang, and X. He, “Levit-unet: Make faster encoders with
    transformer for medical image segmentation,” *arXiv preprint arXiv:2107.08623*,
    2021.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns
    for medical image segmentation,” in *Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part I 24*.   Cham, Switzerland: Springer, 2021,
    pp. 14–24.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C. Suo, X. Li, D. Tan, Y. Zhang, and X. Gao, “I2-net: Intra-and inter-scale
    collaborative learning network for abdominal multi-organ segmentation,” in *Proceedings
    of the 2022 International Conference on Multimedia Retrieval*, New York, NY, 2022,
    pp. 654–660.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] H. Kan, J. Shi, M. Zhao, Z. Wang, W. Han, H. An, Z. Wang, and S. Wang,
    “Itunet: Integration of transformers and unet for organs-at-risk segmentation,”
    in *2022 44th Annual International Conference of the IEEE Engineering in Medicine
    & Biology Society (EMBC)*.   IEEE, 2022, pp. 2123–2127.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
    and Y. Zhou, “Transunet: Transformers make strong encoders for medical image segmentation,”
    *arXiv preprint arXiv:2102.04306*, 2021.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R.
    Roth, and D. Xu, “Unetr: Transformers for 3d medical image segmentation,” in *Proceedings
    of the IEEE/CVF winter conference on applications of computer vision*, Waikoloa,
    HI, 2022, pp. 574–584.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] P.-H. Chen, C.-H. Huang, S.-K. Hung, L.-C. Chen, H.-L. Hsieh, W.-Y. Chiou,
    M.-S. Lee, H.-Y. Lin, and W.-M. Liu, “Attention-lstm fused u-net architecture
    for organ segmentation in ct images,” in *2020 International Symposium on Computer,
    Consumer and Control (IS3C)*.   Taichung City, Taiwan: IEEE, 2020, pp. 304–307.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Chakravarty and J. Sivaswamy, “Race-net: a recurrent neural network
    for biomedical image segmentation,” *IEEE journal of biomedical and health informatics*,
    vol. 23, no. 3, pp. 1151–1162, 2018.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] E. Tappeiner, S. Pröll, M. Hönig, P. F. Raudaschl, P. Zaffino, M. F. Spadea,
    G. C. Sharp, R. Schubert, and K. Fritscher, “Multi-organ segmentation of the head
    and neck area: an efficient hierarchical neural networks approach,” *International
    journal of computer assisted radiology and surgery*, vol. 14, no. 5, pp. 745–754,
    2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Pu, S.-I. Kamata, and Y. Wang, “A coarse to fine framework for multi-organ
    segmentation in head and neck images,” in *2020 Joint 9th International Conference
    on Informatics, Electronics & Vision (ICIEV) and 2020 4th International Conference
    on Imaging, Vision & Pattern Recognition (icIVPR)*.   Kitakyushu, Japan: IEEE,
    2020, pp. 1–6.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Q. Ma, C. Zu, X. Wu, J. Zhou, and Y. Wang, “Coarse-to-fine segmentation
    of organs at risk in nasopharyngeal carcinoma radiotherapy,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,
    Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24*.   Cham,
    Switzerland: Springer, 2021, pp. 358–368.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] P. Hu, F. Wu, J. Peng, Y. Bao, F. Chen, and D. Kong, “Automatic abdominal
    multi-organ segmentation using deep convolutional neural network and time-implicit
    level sets,” *International journal of computer assisted radiology and surgery*,
    vol. 12, no. 3, pp. 399–411, 2017.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] L. Zhang, J. Zhang, P. Shen, G. Zhu, P. Li, X. Lu, H. Zhang, S. A. Shah,
    and M. Bennamoun, “Block level skip connections across cascaded v-net for multi-organ
    segmentation,” *IEEE Transactions on Medical Imaging*, vol. 39, no. 9, pp. 2782–2793,
    2020.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] L. Xie, Q. Yu, Y. Zhou, Y. Wang, E. K. Fishman, and A. L. Yuille, “Recurrent
    saliency transformation network for tiny target segmentation in abdominal ct scans,”
    *IEEE transactions on medical imaging*, vol. 39, no. 2, pp. 514–525, 2019.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. H. Lee, Y. Tang, S. Bao, R. G. Abramson, Y. Huo, and B. A. Landman,
    “Rap-net: Coarse-to-fine multi-organ segmentation with single random anatomical
    prior,” in *2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)*.   Nice,
    France: IEEE, 2021, pp. 1491–1494.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
    P. Bilic, M. Rempfler, M. Armbruster, F. Hofmann, M. D’Anastasi *et al.*, “Automatic
    liver and lesion segmentation in ct using cascaded fully convolutional neural
    networks and 3d conditional random fields,” in *International conference on medical
    image computing and computer-assisted intervention*.   Cham, Switzerland: Springer,
    2016, pp. 415–423.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] D. Lachinov, E. Vasiliev, and V. Turlapov, “Glioma segmentation with cascaded
    unet,” in *Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain
    Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with
    MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part
    II 4*.   Cham, Switzerland: Springer, 2019, pp. 189–198.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Li, Y. Chen, S. Yang, and W. Luo, “Cascade dense-unet for prostate
    segmentation in mr images,” in *Intelligent Computing Theories and Application:
    15th International Conference, ICIC 2019, Nanchang, China, August 3–6, 2019, Proceedings,
    Part I 15*.   Cham, Switzerland: Springer, 2019, pp. 481–490.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] R. Trullo, C. Petitjean, S. Ruan, B. Dubray, D. Nie, and D. Shen, “Segmentation
    of organs at risk in thoracic ct images using a sharpmask architecture and conditional
    random fields,” in *2017 IEEE 14th international symposium on biomedical imaging
    (ISBI 2017)*.   Melbourne, Australia: IEEE, 2017, pp. 1003–1006.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Wang, L. Zhao, M. Wang, and Z. Song, “Organ at risk segmentation in
    head and neck ct images using a two-stage segmentation framework based on 3d u-net,”
    *IEEE Access*, vol. 7, pp. 144 591–144 602, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] K. Men, H. Geng, C. Cheng, H. Zhong, M. Huang, Y. Fan, J. P. Plastaras,
    A. Lin, and Y. Xiao, “More accurate and efficient segmentation of organs-at-risk
    in radiotherapy with convolutional neural networks cascades,” *Medical physics*,
    vol. 46, no. 1, pp. 286–292, 2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Lei, J. Zhou, X. Dong, T. Wang, H. Mao, M. McDonald, W. J. Curran,
    T. Liu, and X. Yang, “Multi-organ segmentation in head and neck mri using u-faster-rcnn,”
    in *Medical Imaging 2020: Image Processing*, vol. 113133A.   Houston, TX: SPIE,
    2020, pp. 826–831.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Francis, P. Jayaraj, P. Pournami, M. Thomas, A. T. Jose, A. J. Binu,
    and N. Puzhakkal, “Thoraxnet: a 3d u-net based two-stage framework for oar segmentation
    on thoracic ct images,” *Physical and Engineering Sciences in Medicine*, vol. 45,
    no. 1, pp. 189–203, 2022.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Tang, X. Chen, Y. Liu, Z. Lu, J. You, M. Yang, S. Yao, G. Zhao, Y. Xu,
    T. Chen *et al.*, “Clinically applicable deep learning framework for organs at
    risk delineation in ct images,” *Nature Machine Intelligence*, vol. 1, no. 10,
    pp. 480–491, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. C. Korte, N. Hardcastle, S. P. Ng, B. Clark, T. Kron, and P. Jackson,
    “Cascaded deep learning-based auto-segmentation for head and neck cancer patients:
    Organs at risk on t2-weighted magnetic resonance imaging,” *Medical physics*,
    vol. 48, no. 12, pp. 7757–7772, 2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Gao, R. Huang, M. Chen, Z. Wang, J. Deng, Y. Chen, Y. Yang, J. Zhang,
    C. Tao, and H. Li, “Focusnet: Imbalanced large and small organ segmentation with
    an end-to-end deep neural network for head and neck ct images,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
    Shenzhen, China, October 13–17, 2019, Proceedings, Part III 22*.   Cham, Switzerland:
    Springer, 2019, pp. 829–838.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. Larsson, Y. Zhang, and F. Kahl, “Robust abdominal organ segmentation
    using regional convolutional neural networks,” *Applied Soft Computing*, vol. 70,
    pp. 465–471, 2018.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Zhao, H. Li, S. Wan, A. Sekuboyina, X. Hu, G. Tetteh, M. Piraud, and
    B. Menze, “Knowledge-aided convolutional neural network for small organ segmentation,”
    *IEEE journal of biomedical and health informatics*, vol. 23, no. 4, pp. 1363–1373,
    2019.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] X. Ren, L. Xiang, D. Nie, Y. Shao, H. Zhang, D. Shen, and Q. Wang, “Interleaved
    3d-cnn s for joint segmentation of small-volume structures in head and neck ct
    images,” *Medical physics*, vol. 45, no. 5, pp. 2063–2075, 2018.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] B. Huang, Y. Ye, Z. Xu, Z. Cai, Y. He, Z. Zhong, L. Liu, X. Chen, H. Chen,
    and B. Huang, “3d lightweight network for simultaneous registration and segmentation
    of organs-at-risk in ct images of head and neck cancer,” *IEEE Transactions on
    Medical Imaging*, vol. 41, no. 4, pp. 951–964, 2021.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Liang, F. Tang, X. Huang, K. Yang, T. Zhong, R. Hu, S. Liu, X. Yuan,
    and Y. Zhang, “Deep-learning-based detection and segmentation of organs at risk
    in nasopharyngeal carcinoma computed tomographic images for radiotherapy planning,”
    *European radiology*, vol. 29, no. 4, pp. 1961–1967, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] N. Tong, S. Gou, S. Yang, D. Ruan, and K. Sheng, “Fully automatic multi-organ
    segmentation for head and neck cancer radiotherapy using shape representation
    model constrained fully convolutional neural networks,” *Medical physics*, vol. 45,
    no. 10, pp. 4558–4567, 2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. R. Roth, H. Oda, Y. Hayashi, M. Oda, N. Shimizu, M. Fujiwara, K. Misawa,
    and K. Mori, “Hierarchical 3d fully convolutional networks for multi-organ segmentation,”
    *arXiv preprint arXiv:1704.06382*, 2017.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] K. Men, J. Dai, and Y. Li, “Automatic segmentation of the clinical target
    volume and organs at risk in the planning ct for rectal cancer using deep dilated
    convolutional neural networks,” *Medical physics*, vol. 44, no. 12, pp. 6377–6389,
    2017.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Z. Chen, C. Li, J. He, J. Ye, D. Song, S. Wang, L. Gu, and Y. Qiao, “A
    novel hybrid convolutional neural network for accurate organ segmentation in 3d
    head and neck ct images,” in *Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part I 24*.   Cham, Switzerland: Springer, 2021, pp. 569–578.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf, W. Yang, D. Li,
    and Z. Fan, “Fully automated multiorgan segmentation in abdominal magnetic resonance
    imaging with deep neural networks,” *Medical physics*, vol. 47, no. 10, pp. 4971–4982,
    2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Jain, A. Sutradhar, A. K. Dash, and S. Das, “Automatic multi-organ
    segmentation on abdominal ct scans using deep u-net model,” in *2021 19th OITS
    International Conference on Information Technology (OCIT)*.   Bhubaneswar, India:
    IEEE, 2021, pp. 48–53.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Y. Ahn, J. S. Yoon, S. S. Lee, H.-I. Suk, J. H. Son, Y. S. Sung, Y. Lee,
    B.-K. Kang, and H. S. Kim, “Deep learning algorithm for automated segmentation
    and volume measurement of the liver and spleen using portal venous phase computed
    tomography images,” *Korean journal of radiology*, vol. 21, no. 8, pp. 987–997,
    2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Shi, K. Wen, X. Hao, X. Xue, H. An, and H. Zhang, “A novel u-like
    network for the segmentation of thoracic organs,” in *2020 IEEE 17th International
    Symposium on Biomedical Imaging Workshops (ISBI Workshops)*.   Iowa City, IA:
    IEEE, 2020, pp. 1–4.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Liang, K.-H. Thung, D. Nie, Y. Zhang, and D. Shen, “Multi-view spatial
    aggregation framework for joint localization and segmentation of organs at risk
    in head and neck ct images,” *IEEE Transactions on Medical Imaging*, vol. 39,
    no. 9, pp. 2794–2805, 2020.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] X. Zhou, R. Takayama, S. Wang, T. Hara, and H. Fujita, “Deep learning
    of the sectional appearances of 3d ct images for anatomical structure segmentation
    based on an fcn voting method,” *Medical physics*, vol. 44, no. 10, pp. 5221–5233,
    2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Y. Wang, Y. Zhou, W. Shen, S. Park, E. K. Fishman, and A. L. Yuille,
    “Abdominal multi-organ segmentation with organ-attention networks and statistical
    fusion,” *Medical image analysis*, vol. 55, pp. 88–102, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Tang, X. Liu, K. Han, X. Xie, X. Chen, H. Qian, Y. Liu, S. Sun, and
    N. Bai, “Spatial context-aware self-attention model for multi-organ segmentation,”
    in *Proceedings of the IEEE/CVF winter conference on applications of computer
    vision*, Waikoloa, HI, 2021, pp. 939–949.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] T. Qu, X. Wang, C. Fang, L. Mao, J. Li, P. Li, J. Qu, X. Li, H. Xue,
    Y. Yu *et al.*, “M³net: A multi-scale multi-view framework for multi-phase pancreas
    segmentation based on cross-phase non-local attention,” *Medical image analysis*,
    vol. 75, p. 102232, 2022.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Y. Ding, W. Zheng, J. Geng, Z. Qin, K.-K. R. Choo, Z. Qin, and X. Hou,
    “Mvfusfra: a multi-view dynamic fusion framework for multimodal brain tumor segmentation,”
    *IEEE Journal of Biomedical and Health Informatics*, vol. 26, no. 4, pp. 1570–1581,
    2021.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Ouaknine, A. Newson, P. Pérez, F. Tupin, and J. Rebut, “Multi-view
    radar semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, Montreal, QC, 2021, pp. 15 671–15 680.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Z. S. Cheng, T. Y. Zeng, S. J. Huang, and X. Yang, “A novel hybrid network
    for h&n organs at risk segmentation,” in *Proceedings of the 5th International
    Conference on Biomedical Signal and Image Processing*, Suzhou, China, 2020, pp.
    7–13.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han, E. Fishman, and A. L.
    Yuille, “Prior-aware neural network for partially-supervised multi-organ segmentation,”
    in *Proceedings of the IEEE/CVF international conference on computer vision*,
    Seoul, South Korea, 2019, pp. 10 672–10 681.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Lian, L. Li, Z. Luo, Z. Zhong, B. Wang, and S. Li, “Learning multi-organ
    segmentation via partial-and mutual-prior from single-organ datasets,” *Biomedical
    Signal Processing and Control*, vol. 80, p. 104339, 2023.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,
    S. A. Cook, A. De Marvao, T. Dawes, D. P. O‘Regan *et al.*, “Anatomically constrained
    neural networks (acnns): application to cardiac image enhancement and segmentation,”
    *IEEE transactions on medical imaging*, vol. 37, no. 2, pp. 384–395, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Advances in Neural Information Processing Systems*, vol. 33, pp. 6840–6851, 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,”
    *arXiv preprint arXiv:2010.02502*, 2020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] I. Isler, C. Lisle, J. Rineer, P. Kelly, D. Turgut, J. Ricci, and U. Bagci,
    “Enhancing organ at risk segmentation with improved deep neural networks,” in
    *Medical Imaging 2022: Image Processing*, vol. 12032.   San Diego, CA: SPIE, 2022,
    pp. 814–820.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Vesal, N. Ravikumar, and A. Maier, “A 2d dilated residual u-net for
    multi-organ segmentation in thoracic ct,” *arXiv preprint arXiv:1905.07710*, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, Honolulu, HI, 2017, pp. 2117–2125.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
    “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Srivastava, D. Jha, E. Keles, B. Aydogan, M. Abazeed, and U. Bagci,
    “An efficient multi-scale fusion network for 3d organ at risk (oar) segmentation,”
    *arXiv preprint arXiv:2208.07417*, 2022.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,
    K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz *et al.*, “Attention u-net: Learning
    where to look for the pancreas,” *arXiv preprint arXiv:1804.03999*, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] S. Hu *et al.*, “Hu j., shen l., albanie s., sun g., wu e,” *Squeeze-and-excitation
    networks, IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 42,
    no. 8, pp. 2011–2023, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Z. Liu, H. Wang, W. Lei, and G. Wang, “Csaf-cnn: cross-layer spatial
    attention map fusion network for organ-at-risk segmentation in head and neck ct
    images,” in *2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)*.   Iowa
    City, IA: IEEE, 2020, pp. 1522–1525.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] H. Lin, Z. Li, Z. Yang, and Y. Wang, “Variance-aware attention u-net
    for multi-organ segmentation,” *Medical Physics*, vol. 48, no. 12, pp. 7864–7876,
    2021.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
    A nested u-net architecture for medical image segmentation,” in *Deep Learning
    in Medical Image Analysis and Multimodal Learning for Clinical Decision Support:
    4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS
    2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018,
    Proceedings 4*.   Cham, Switzerland: Springer, 2018, pp. 3–11.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, Honolulu, HI, 2017, pp. 4700–4708.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, San Juan, PR, 2016, pp. 770–778.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
    convolutional networks,” in *Proceedings of the IEEE international conference
    on computer vision*, Venice, Italy, 2017, pp. 764–773.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. P. Heinrich, O. Oktay, and N. Bouteldja, “Obelisk-net: Fewer layers
    to solve 3d multi-organ segmentation with sparse deformable convolutions,” *Medical
    image analysis*, vol. 54, pp. 1–9, 2019.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] N. Shen, Z. Wang, J. Li, H. Gao, W. Lu, P. Hu, and L. Feng, “Multi-organ
    segmentation network for abdominal ct images based on spatial attention and deformable
    convolution,” *Expert Systems with Applications*, vol. 211, p. 118625, 2023.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, “Strip pooling: Rethinking
    spatial pooling for scene parsing,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, Seattle, WA, 2020, pp. 4003–4012.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] F. Zhang, Y. Wang, and H. Yang, “Efficient context-aware network for
    abdominal multi-organ segmentation,” *arXiv preprint arXiv:2109.10601*, 2021.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] S. Jadon, “A survey of loss functions for semantic segmentation,” in
    *2020 IEEE conference on computational intelligence in bioinformatics and computational
    biology (CIBCB)*.   Via del Mar, Chile: IEEE, 2020, pp. 1–7.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Yi-de, L. Qing, and Q. Zhi-Bai, “Automated image segmentation using
    improved pcnn model based on cross-entropy,” in *Proceedings of 2004 International
    Symposium on Intelligent Multimedia, Video and Speech Processing, 2004.*   Hong
    Kong, China: IEEE, 2004, pp. 743–746.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. Jorge Cardoso,
    “Generalised dice overlap as a deep learning loss function for highly unbalanced
    segmentations,” in *Deep Learning in Medical Image Analysis and Multimodal Learning
    for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th
    International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec
    City, QC, Canada, September 14, Proceedings 3*.   Cham, Switzerland: Springer,
    2017, pp. 240–248.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Tversky loss function
    for image segmentation using 3d fully convolutional deep networks,” in *Machine
    Learning in Medical Imaging: 8th International Workshop, MLMI 2017, Held in Conjunction
    with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017, Proceedings 8*.   Cham,
    Switzerland: Springer, 2017, pp. 379–387.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, Venice, Italy, 2017, pp. 2980–2988.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] V. Pihur, S. Datta, and S. Datta, “Weighted rank aggregation of cluster
    validation measures: a monte carlo cross-entropy approach,” *Bioinformatics*,
    vol. 23, no. 13, pp. 1607–1615, 2007.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] C. Shen, H. R. Roth, H. Oda, M. Oda, Y. Hayashi, K. Misawa, and K. Mori,
    “On the influence of dice loss function in multi-class organ segmentation of abdominal
    ct using 3d fully convolutional networks,” *arXiv preprint arXiv:1801.05912*,
    2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] E. Tappeiner, M. Welk, and R. Schubert, “Tackling the class imbalance
    problem of deep learning-based head and neck organ segmentation,” *International
    Journal of Computer Assisted Radiology and Surgery*, vol. 17, no. 11, pp. 2103–2111,
    2022.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] N. Abraham and N. M. Khan, “A novel focal tversky loss function with
    improved attention u-net for lesion segmentation,” in *2019 IEEE 16th international
    symposium on biomedical imaging (ISBI 2019)*.   Venice, Italy: IEEE, 2019, pp.
    683–687.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. C. Wong, M. Moradi, H. Tang, and T. Syeda-Mahmood, “3d segmentation
    with exponential logarithmic loss for highly unbalanced object sizes,” in *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International
    Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part III 11*.   Cham,
    Switzerland: Springer, 2018, pp. 612–619.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. Song, X. Chen, Q. Zhu, F. Shi, D. Xiang, Z. Chen, Y. Fan, L. Pan,
    and W. Zhu, “Global and local feature reconstruction for medical image segmentation,”
    *IEEE Transactions on Medical Imaging*, vol. 41, no. 9, pp. 2273–2284, 2022.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] W. Lei, H. Mei, Z. Sun, S. Ye, R. Gu, H. Wang, R. Huang, S. Zhang, S. Zhang,
    and G. Wang, “Automatic segmentation of organs-at-risk from head-and-neck ct using
    separable convolutional neural network with hard-region-weighted loss,” *Neurocomputing*,
    vol. 442, pp. 184–199, 2021.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] P. Bilic, P. Christ, H. B. Li, E. Vorontsov, A. Ben-Cohen, G. Kaissis,
    A. Szeskin, C. Jacobs, G. E. H. Mamani, G. Chartrand *et al.*, “The liver tumor
    segmentation benchmark (lits),” *Medical Image Analysis*, vol. 84, p. 102680,
    2023.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak,
    J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich *et al.*, “The kits19 challenge
    data: 300 kidney tumor cases with clinical context, ct semantic segmentations,
    and surgical outcomes,” *arXiv preprint arXiv:1904.00445*, 2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken,
    A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze *et al.*, “A large annotated
    medical image dataset for the development and evaluation of segmentation algorithms,”
    *arXiv preprint arXiv:1902.09063*, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis,” *arXiv preprint arXiv:1904.00625*, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] K. Dmitriev and A. E. Kaufman, “Learning multi-class segmentations from
    single-class datasets,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, Long Beach, CA, 2019, pp. 9501–9511.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] J. Zhang, Y. Xie, Y. Xia, and C. Shen, “Dodnet: Learning to segment multi-organ
    and tumors from multiple partially labeled datasets,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, Nashville, TN, 2021, pp.
    1195–1204.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Xie, J. Zhang, Y. Xia, and C. Shen, “Learning from partially labeled
    data for multi-organ and tumor segmentation,” *arXiv preprint arXiv:2211.06894*,
    2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,
    “nnu-net: a self-configuring method for deep learning-based biomedical image segmentation,”
    *Nature methods*, vol. 18, no. 2, pp. 203–211, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] H. Wu, S. Pang, and A. Sowmya, “Tgnet: A task-guided network architecture
    for multi-organ and tumour segmentation from partially labelled datasets,” in
    *2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)*.   Kolkata,
    India: IEEE, 2022, pp. 1–5.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P. Liu, L. Xiao, and S. K. Zhou, “Incremental learning for multi-organ
    segmentation with partially labeled datasets,” *arXiv preprint arXiv:2103.04526*,
    2021.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] X. Xu and P. Yan, “Federated multi-organ segmentation with partially
    labeled data,” *arXiv preprint arXiv:2206.07156*, 2022.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] X. Fang and P. Yan, “Multi-organ segmentation over partially labeled
    datasets with multi-scale feature abstraction,” *IEEE Transactions on Medical
    Imaging*, vol. 39, no. 11, pp. 3619–3629, 2020.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] G. Shi, L. Xiao, Y. Chen, and S. K. Zhou, “Marginal loss and exclusion
    loss for partially supervised multi-organ segmentation,” *Medical Image Analysis*,
    vol. 70, p. 101979, 2021.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] R. Huang, Y. Zheng, Z. Hu, S. Zhang, and H. Li, “Multi-organ segmentation
    via co-training weight-averaged models from few-organ datasets,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part IV 23*.   Cham, Switzerland:
    Springer, 2020, pp. 146–155.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] L. Zhang, S. Feng, Y. Wang, Y. Wang, Y. Zhang, X. Chen, and Q. Tian,
    “Unsupervised ensemble distillation for multi-organ segmentation,” in *2022 IEEE
    19th International Symposium on Biomedical Imaging (ISBI)*.   Kolkata, India:
    IEEE, 2022, pp. 1–5.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Q. Wu, Y. Chen, N. Huang, and X. Yue, “Weakly-supervised cerebrovascular
    segmentation network with shape prior and model indicator,” in *Proceedings of
    the 2022 International Conference on Multimedia Retrieval*, Newark, NJ, 2022,
    pp. 668–676.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] F. Kanavati, K. Misawa, M. Fujiwara, K. Mori, D. Rueckert, and B. Glocker,
    “Joint supervoxel classification forest for weakly-supervised organ segmentation,”
    in *Machine Learning in Medical Imaging: 8th International Workshop, MLMI 2017,
    Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017,
    Proceedings 8*.   Cham, Switzerland: Springer, 2017, pp. 79–87.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,
    A. King, P. M. Matthews, and D. Rueckert, “Semi-supervised learning for network-based
    cardiac mr image segmentation,” in *Medical Image Computing and Computer-Assisted
    Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada,
    September 11-13, 2017, Proceedings, Part II 20*.   Cham, Switzerland: Springer,
    2017, pp. 253–260.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Luo, M. Hu, T. Song, G. Wang, and S. Zhang, “Semi-supervised medical
    image segmentation via cross teaching between cnn and transformer,” in *International
    Conference on Medical Imaging with Deep Learning*.   Zurich, Switzerland: PMLR,
    2022, pp. 820–833.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J. Chen, J. Zhang, K. Debattista, and J. Han, “Semi-supervised unpaired
    medical image segmentation through task-affinity consistency,” *IEEE Transactions
    on Medical Imaging*, 2022.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Y. Wu, Z. Ge, D. Zhang, M. Xu, L. Zhang, Y. Xia, and J. Cai, “Mutual
    consistency learning for semi-supervised medical image segmentation,” *Medical
    Image Analysis*, vol. 81, p. 102530, 2022.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] X. Luo, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, N. Chen, G. Wang,
    and S. Zhang, “Efficient semi-supervised gross target volume of nasopharyngeal
    carcinoma segmentation via uncertainty rectified pyramid consistency,” in *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
    Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part
    II 24*.   Cham, Switzerland: Springer, 2021, pp. 318–329.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] R. Jiao, Y. Zhang, L. Ding, R. Cai, and J. Zhang, “Learning with limited
    annotations: a survey on deep semi-supervised learning for medical image segmentation,”
    *arXiv preprint arXiv:2207.14191*, 2022.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Zhou, Y. Wang, P. Tang, S. Bai, W. Shen, E. Fishman, and A. Yuille,
    “Semi-supervised 3d abdominal multi-organ segmentation via deep multi-planar co-training,”
    in *2019 IEEE Winter Conference on Applications of Computer Vision (WACV)*.   Waikoloa,
    HI: IEEE, 2019, pp. 121–140.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Y. Xia, D. Yang, Z. Yu, F. Liu, J. Cai, L. Yu, Z. Zhu, D. Xu, A. Yuille,
    and H. Roth, “Uncertainty-aware multi-view co-training for semi-supervised medical
    image segmentation and domain adaptation,” *Medical image analysis*, vol. 65,
    p. 101766, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Lai, T. Wang, and S. Zhou, “Dlunet: Semi-supervised learning based
    dual-light unet for multi-organ segmentation,” in *Fast and Low-Resource Semi-supervised
    Abdominal Organ Segmentation: MICCAI 2022 Challenge, FLARE 2022, Held in Conjunction
    with MICCAI 2022, Singapore, September 22, 2022, Proceedings*.   Springer, 2023,
    pp. 64–73.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] H. H. Lee, Y. Tang, O. Tang, Y. Xu, Y. Chen, D. Gao, S. Han, R. Gao,
    M. R. Savona, R. G. Abramson *et al.*, “Semi-supervised multi-organ segmentation
    through quality assurance supervision,” in *Medical Imaging 2020: Image Processing*,
    vol. 11313.   Houston, TX: SPIE, 2020, pp. 363–369.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. Raju, C.-T. Cheng, Y. Huo, J. Cai, J. Huang, J. Xiao, L. Lu, C. Liao,
    and A. P. Harrison, “Co-heterogeneous and adaptive segmentation from multi-source
    and multi-phase ct imaging data: A study on pathological liver and lesion segmentation,”
    in *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XXIII*.   Cham, Switzerland: Springer, 2020, pp. 448–465.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] D. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao,
    and L. Lu, “Organ at risk segmentation for head and neck cancer using stratified
    learning and neural architecture search,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, Seattle, WA, 2020, pp. 4223–4232.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding,
    “Embracing imperfect datasets: A review of deep learning solutions for medical
    image segmentation,” *Medical Image Analysis*, vol. 63, p. 101693, 2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Qu, S. Liu, X. Liu, M. Wang, and Z. Song, “Towards label-efficient
    automatic diagnosis and analysis: a comprehensive survey of advanced deep learning-based
    weakly-supervised, semi-supervised and self-supervised techniques in histopathological
    image analysis,” *Physics in Medicine & Biology*, vol. 67, no. 20, p. 20TR01,
    2022.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, “Domain
    adaptive relational reasoning for 3d multi-organ segmentation,” in *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
    Lima, Peru, October 4–8, 2020, Proceedings, Part I 23*.   Cham, Switzerland: Springer,
    2020, pp. 656–666.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Hong, Y.-D. Zhang, and W. Chen, “Source-free unsupervised domain adaptation
    for cross-modality abdominal multi-organ segmentation,” *Knowledge-Based Systems*,
    vol. 250, p. 109155, 2022.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Y. Liu, Y. Lei, Y. Fu, T. Wang, J. Zhou, X. Jiang, M. McDonald, J. J.
    Beitler, W. J. Curran, T. Liu *et al.*, “Head and neck multi-organ auto-segmentation
    on ct images aided by synthetic mri,” *Medical physics*, vol. 47, no. 9, pp. 4294–4302,
    2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. Cros, E. Vorontsov, and S. Kadoury, “Managing class imbalance in multi-organ
    ct segmentation in head and neck cancer patients,” in *2021 IEEE 18th International
    Symposium on Biomedical Imaging (ISBI)*.   Nice, France: IEEE, 2021, pp. 1360–1364.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] J. Jiang, S. Elguindi, S. L. Berry, I. Onochie, L. Cervino, J. O. Deasy,
    and H. Veeraraghavan, “Nested block self-attention multiple resolution residual
    network for multiorgan segmentation from ct,” *Medical Physics*, vol. 49, no. 8,
    pp. 5244–5257, 2022.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. Francis, G. Pooloth, S. B. S. Singam, N. Puzhakkal, P. Pulinthanathu Narayanan,
    and J. Pottekkattuvalappil Balakrishnan, “Sabos-net: Self-supervised attention
    based network for automatic organ segmentation of head and neck ct images,” *International
    Journal of Imaging Systems and Technology*, vol. 33, no. 1, pp. 175–191, 2023.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore,
    S. Phillips, D. Maffitt, M. Pringle *et al.*, “The cancer imaging archive (tcia):
    maintaining and operating a public information repository,” *Journal of digital
    imaging*, vol. 26, no. 6, pp. 1045–1057, 2013.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] H. R. Roth, L. Lu, A. Farag, H.-C. Shin, J. Liu, E. B. Turkbey, and R. M.
    Summers, “Deeporgan: Multi-level deep convolutional networks for automated pancreas
    segmentation,” in *Medical Image Computing and Computer-Assisted Intervention–MICCAI
    2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings,
    Part I 18*.   Cham, Switzerland: Springer, 2015, pp. 556–564.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] V. Kumar, M. K. Sharma, R. Jehadeesan, B. Venkatraman, and D. Sheet,
    “Adversarial training of deep convolutional neural network for multi-organ segmentation
    from multi-sequence mri of the abdomen,” in *2021 International Conference on
    Intelligent Technologies (CONIT)*.   Hubli, India: IEEE, 2021, pp. 1–6.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] B. Rister, D. Yi, K. Shivakumar, T. Nobashi, and D. L. Rubin, “Ct-org,
    a new dataset for multiple organ segmentation in computed tomography,” *Scientific
    Data*, vol. 7, no. 1, p. 381, 2020.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] C. C. Vu, Z. A. Siddiqui, L. Zamdborg, A. B. Thompson, T. J. Quinn, E. Castillo,
    and T. M. Guerrero, “Deep convolutional neural networks for automatic segmentation
    of thoracic organs-at-risk in radiation oncology–use of non-domain transfer learning,”
    *Journal of Applied Clinical Medical Physics*, vol. 21, no. 6, pp. 108–113, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. S. K. Gali, N. Garg, S. Vasamsetti *et al.*, “Dilated u-net based
    segmentation of organs at risk in thoracic ct images,” in *SegTHOR@ ISBI*, 2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] H. Mahmood, S. M. S. Islam, J. Hill, and G. Tay, “Rapid segmentation
    of thoracic organs using u-net architecture,” in *2021 Digital Image Computing:
    Techniques and Applications (DICTA)*.   Gold Coast, Australia: IEEE, 2021, pp.
    1–6.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] F. Zhang, Q. Wang, A. Yang, N. Lu, H. Jiang, D. Chen, Y. Yu, and Y. Wang,
    “Geometric and dosimetric evaluation of the automatic delineation of organs at
    risk (oars) in non-small-cell lung cancer radiotherapy based on a modified densenet
    deep learning network,” *Frontiers in Oncology*, vol. 12, p. 861857, 2022.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] J. Ma, Y. Zhang, S. Gu, X. An, Z. Wang, C. Ge, C. Wang, F. Zhang, Y. Wang,
    Y. Xu *et al.*, “Fast and low-gpu-memory abdomen ct organ segmentation: the flare
    challenge,” *Medical Image Analysis*, vol. 82, p. 102616, 2022.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] H. Kakeya, T. Okada, and Y. Oshiro, “3d u-japa-net: mixture of convolutional
    networks for abdominal multi-organ ct segmentation,” in *Medical Image Computing
    and Computer Assisted Intervention–MICCAI 2018: 21st International Conference,
    Granada, Spain, September 16-20, 2018, Proceedings, Part IV 11*.   Cham, Switzerland:
    Springer, 2018, pp. 426–433.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] R. Trullo, C. Petitjean, D. Nie, D. Shen, and S. Ruan, “Joint segmentation
    of multiple thoracic organs in ct images with two collaborative deep architectures,”
    in *Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
    Decision Support: Third International Workshop, DLMIA 2017, and 7th International
    Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC,
    Canada, September 14, Proceedings 3*, vol. 10553.   Springer, 2017, pp. 21–29.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Z. Cao, B. Yu, B. Lei, H. Ying, X. Zhang, D. Z. Chen, and J. Wu, “Cascaded
    se-resunet for segmentation of thoracic organs at risk,” *Neurocomputing*, vol.
    453, pp. 357–368, 2021.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Q. Yang, S. Zhang, X. Sun, J. Sun, and K. Yuan, “Automatic segmentation
    of head-neck organs by multi-mode cnns for radiation therapy,” in *2019 International
    Conference on Medical Imaging Physics and Engineering (ICMIPE)*.   Shenzhen, China:
    IEEE, 2019, pp. 1–5.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] C. E. Cardenas, A. S. Mohamed, J. Yang, M. Gooding, H. Veeraraghavan,
    J. Kalpathy-Cramer, S. P. Ng, Y. Ding, J. Wang, S. Y. Lai *et al.*, “Head and
    neck cancer patient images for determining auto-segmentation accuracy in t2-weighted
    magnetic resonance imaging through expert manual segmentations,” *Medical physics*,
    vol. 47, no. 5, pp. 2317–2322, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] O. Jimenez-del Toro, H. Müller, M. Krenn, K. Gruenberg, A. A. Taha, M. Winterstein,
    I. Eggel, A. Foncubierta-Rodríguez, O. Goksel, A. Jakab *et al.*, “Cloud-based
    evaluation of anatomical structure segmentation and landmark detection algorithms:
    Visceral anatomy benchmarks,” *IEEE transactions on medical imaging*, vol. 35,
    no. 11, pp. 2459–2475, 2016.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplementary Materials
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE II: DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation
    Methods for the Head and Neck'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Datasets | Quantity | Organ type | Brainstem | Mandible
    | Parotid gland | Submandibular gland | Optic nerve | Chiasm |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| Left | Right | Left | Right | Left | Right |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| Ibragimov and Xing [[41](#bib.bib41)] | 2.5D CNN | Private (CT) | 50 | 13
    | - | 0.895 | 0.766 | 0.779 | 0.697 | 0.730 | 0.639 | 0.645 | 0.374 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| Fritscher et al. [[42](#bib.bib42)] | 2.5D CNN | HNC (CT) [[30](#bib.bib30)]
    | 30 | 3 | - | - | 0.810 | - | 0.650 | - | - | - | 0.520 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. [[48](#bib.bib48)] | 3D U-Net | Private (CT) | 261 | 9 | 0.867
    | 0.925 | 0.881 | 0.874 | 0.814 | 0.813 | 0.721 | 0.706 | 0.532 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| Van Rooij et al. [[49](#bib.bib49)] | 3D U-Net | Private (CT) | 157 | 11
    | 0.640 | - | 0.830 | 0.830 | 0.820 | 0.810 | - | - | - |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. [[64](#bib.bib64)] | 3D GAN | PDDCA (CT) [[30](#bib.bib30)] |
    48 | 9 | 0.867 | 0.939 | 0.855 | 0.858 | 0.807 | 0.819 | 0.664 | 0.699 | 0.592
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. [[64](#bib.bib64)] | 3D GAN | Private (MRI) | 25 | 9 | 0.916
    | 0.816 | 0.865 | 0.825 | - | - | 0.717 | 0.693 | 0.589 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| Gou et al. [[50](#bib.bib50)] | 3D U-Net | HNC (CT) [[30](#bib.bib30)] |
    48 | 9 | 0.880 | 0.940 | 0.870 | 0.860 | 0.780 | 0.810 | 0.720 | 0.710 | 0.610
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[196](#bib.bib196)] | 3D U-Net | Private (MRI & CT) | 45 | 19
    | 0.880 | 0.890 | 0.890 | 0.880 | - | - | 0.720 | 0.720 | 0.760 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[196](#bib.bib196)] | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.910 | 0.960 | 0.880 | 0.880 | 0.860 | 0.850 | 0.780 | 0.780 | 0.730
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | 2.5D U-Net | Private (CT) | 307 | 24 |
    - | - | - | - | - | - | 0.711 | 0.712 | 0.598 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | 2.5D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.872 | 0.922 | 0.867 | 0.858 | 0.821 | 0.821 | 0.750 | 0.741 | 0.663
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[141](#bib.bib141)] | 2D U-Net | StructSeg (CT) | 50 | 22 | 0.864
    | 0.906 | 0.802 | 0.826 | - | - | 0.770 | 0.647 | 0.712 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| Cros et al. [[197](#bib.bib197)] | 3D U-Net | Private (CT) | 200 | 12 | -
    | 0.900 | 0.760 | 0.760 | 0.740 | - | - | - | - |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[162](#bib.bib162)] | 2.5D U-Net | StructSeg (CT) | 50 | 22 |
    0.897 | 0.914 | 0.857 | 0.873 | - | - | 0.680 | 0.663 | 0.566 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[162](#bib.bib162)] | 2.5D U-Net | Hybrid HAN (CT) | 165 | 7
    | 0.874 | 0.900 | 0.847 | 0.846 | - | - | 0.624 | 0.621 | 0.290 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[51](#bib.bib51)] | 2D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.840 | 0.900 | 0.820 | 0.830 | 0.820 | 0.810 | 0.670 | 0.710 | 0.660
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | OpenKBP (CT) [[35](#bib.bib35)]
    | 188 | 5 | 0.803 | 0.883 | 0.799 | 0.773 | - | - | - | - | - |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| Podobnik et al. [[59](#bib.bib59)] | 2D nnU-net | Private (CT&MR) | 56 |
    31 | 0.836 | 0.898 | 0.817 | 0.765 | 0.716 | 0.670 | 0.572 | 0.604 | 0.387 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| Kan et al. [[85](#bib.bib85)] | 3D Transformer and U-Net | Private (CT) |
    94 | 18 | 0.871 | 0.925 | 0.821 | 0.844 | - | - | 0.717 | 0.679 | 0.328 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[198](#bib.bib198)] | 2D U-Net | PDDCA (CT) [[30](#bib.bib30)]
    | 16 | 6 | 0.920 | 0.950 | 0.880 | 0.880 | 0.820 | 0.830 |  |  |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| Francis et al. [[199](#bib.bib199)] | 3D U-Net | Private (CT) | 232 | 7 |
    0.890 | 0.932 | 0.852 | 0.870 |  |  | 0.744 | 0.764 | 0.635 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| HNC (CT) [[30](#bib.bib30)] | 48 | 7 | 0.862 | 0.940 | 0.885 | 0.885 |  |  |
    0.728 | 0.723 | 0.620 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| Isler et al. [[134](#bib.bib134)] | 2D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 6 | 0.830 | - | 0.790 | 0.760 | - | - | 0.580 | 0.540 | 0.520 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| OpenKBP (CT) [[35](#bib.bib35)] | 188 | 5 | 0.800 | 0.860 | 0.750 | 0.760
    | - | - | - | - | - |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: DSC-Based Summary of the Literature on Multi-Organ Single-Stage
    Segmentation Methods for the Abdomen'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Datasets | Quantity | Organ type | Liver | Spleen | Kidney
    | Pancreas | Gallbladder | Stomach |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[40](#bib.bib40)] | 3D CNN |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '&#124; TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV [[29](#bib.bib29)] (CT) &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '| 72 | 4 | 0.920 | - | - | - | 0.660 | - | 0.830 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[115](#bib.bib115)] | 2D CNN | Private (CT) | 278 | 5 | - | -
    | - | - | - | - | - |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[157](#bib.bib157)] | 3D U-Net | Private (CT) | 377 | 7 | 0.965
    | 0.947 | - | - | 0.847 | 0.808 | 0.963 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[57](#bib.bib57)] | 3D V-Net |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '&#124; TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV [[29](#bib.bib29)] (CT) &#124;'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '| 90 | 8 | 0.960 | 0.960 | 0.950 | - | 0.780 | 0.840 | 0.900 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| Roth et al. [[52](#bib.bib52)] | 3D U-Net | Private (CT) | 377 | 7 | 0.971
    | 0.977 | - | - | 0.849 | 0.851 | 0.961 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[65](#bib.bib65)] | 2D FCN | Private (CT) | 120 | 16 | 0.96 |
    0.951 | 0.956 | 0.954 | 0.785 | 0.797 | 0.909 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[65](#bib.bib65)] | 3D GAN | Private (CT) |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '&#124; 131(liver)+281(spleen) &#124;'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +41(pancreas) &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '| 3 | 0.944 | 0.960 | - | - | 0.743 | - | - |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| Heinrich et al. [[147](#bib.bib147)] | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 43 | 8 | 0.954 | 0.944 | - | - | 0.702 | 0.753 | 0.868 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 10 | 7 |  |  |  |  |  |  |  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| Ahn et al. [[119](#bib.bib119)] | 2.5D CNN | Private (CT) | 813+150 | 2 |
    0.973 | 0.974 | - | - | - | - | - |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 813+50 | 2 | 0.983 | 0.968 | - | - | - | - | - |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. [[194](#bib.bib194)] | 3D V-Net | Synapse (CT) [[2](#bib.bib2)]
    | 90 | 8 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '&#124; Aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen
    and &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; stomach average DSC: 0.698 &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '| Hatamizadeh et al. [[87](#bib.bib87)] |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '&#124; 3D Transformer &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and U-Net &#124;'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '| BTCV (CT) [[29](#bib.bib29)] | 30 | 13 | 0.983 | 0.972 | 0.954 | 0.942 |
    0.799 | 0.825 | 0.945 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[117](#bib.bib117)] | 2.5D U-Net | Private (MR) | 102 | 10 |
    0.963 | 0.946 | 0.954 | 0.954 | 0.880 | 0.732 | 0.923 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[124](#bib.bib124)] | 2.5D U-Net | ABD-110 (CT) [[124](#bib.bib124)]
    | 110 | 11 | 0.964 | 0.959 | 0.960 | 0.957 | 0.821 | 0.822 | 0.875 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| Jia and Wei [[53](#bib.bib53)] | 3D U-Net | CHAOS (CT [[32](#bib.bib32)]
    | 20 | 4 | 0.934 | 0.896 | 0.937 | 0.949 | - | - | - |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[142](#bib.bib142)] | 3D U-Net |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '&#124; TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV [[29](#bib.bib29)] (CT) &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '| 90 | 8 | 0.953 | 0.920 | 0.902 | - | 0.742 | 0.760 | 0.862 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. [[77](#bib.bib77)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.943 | 0.907 | 0.833 | 0.796 | 0.566 | 0.665 | 0.766 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[86](#bib.bib86)] |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '&#124; 2D Transformer &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; And U-Net &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '| Synapse (CT) [[2](#bib.bib2)] | 30 | 8 | 0.941 | 0.851 | 0.819 | 0.770 |
    0.559 | 0.631 | 0.756 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[161](#bib.bib161)] | 2D CNN | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.959 | 0.926 | 0.906 | 0.892 | 0.687 | 0.671 | 0.839 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| Kumar et al. [[202](#bib.bib202)] | 2D GAN | CHAOS (CT) [[32](#bib.bib32)]
    | 40 | 4 | Liver, left kidney, right kidney and spleen average DSC: 0.970 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[78](#bib.bib78)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.944 | 0.919 | 0.852 | 0.820 | 0.657 | 0.687 | 0.808 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| Suo et al. [[84](#bib.bib84)] |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '&#124; 2D Transformer &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; And U-Net &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '| Synapse (CT) [[2](#bib.bib2)] | 30 | 8 | 0.951 | 0.914 | 0.890 | 0.851 |
    0.699 | 0.720 | 0.826 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[58](#bib.bib58)] | 3D V-Net |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '&#124; AbdomenCT-1K [[36](#bib.bib36)] &#124;'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +Private (CT) &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '| 1112+100 | 4 | 0.953 | 0.920 | 0.914 | 0.747 | - | - |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AbdomenCT-1K [[36](#bib.bib36)] &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +TCIA [[200](#bib.bib200), [201](#bib.bib201)] & &#124;'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTCV (CT) [[29](#bib.bib29)] &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '| 1112+90 | 4 | 0.961 | 0.954 | 0.918 | - | 0.784 | - | - |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| Berzoini et al. [[54](#bib.bib54)] | 2D U-Net |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '&#124; Open-source CT-org &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dataset [[203](#bib.bib203)] &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '| 140 | 5 | 0.922 | - | 0.837 | - | - | - |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[148](#bib.bib148)] | 2D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 42 | 5 | 0.960 | - | - | - | 0.754 | 0.805 | 0.889 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| Hong et al. [[195](#bib.bib195)] | 2D U-Net |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '&#124; BTCV [[29](#bib.bib29)] & CHAOS &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[32](#bib.bib32)] (CT) &#124;'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '| 30+20 | 4 | 0.884 | 0.911 | 0.864 | 0.891 | - | - | - |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[79](#bib.bib79)] |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '&#124; 3D CNN And &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transformer &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '| BTCV [[29](#bib.bib29)] (CT) | 30 | 11 | 0.971 | 0.963 | 0.939 | 0.831 |
    0.666 | 0.882 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | 0.950 | 0.870 | 0.842 | 0.824 | 0.681 | 0.675 | 0.760 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[198](#bib.bib198)] | 2D U-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 12 | 0.969 | 0.958 | 0.943 | 0.921 | 0.798 | 0.786 | 0.906 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation
    Methods for the Thorax'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Dataset | Quantity | Organ type | Heart | Esophagus | Trachea
    | Aorta | Spinal cord | Stomach |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| Trullo et al. [[100](#bib.bib100)] | 2D FCN | Private (CT) | 30 | 4 | 0.900
    | 0.670 | 0.820 | - | - | 0.860 | - |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| Dong et al. [[66](#bib.bib66)] | 3D GAN | AAPM (CT) [[31](#bib.bib31)] |
    35 | 4 | 0.870 | 0.750 | - | 0.970 | 0.970 | - | 0.900 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| Vu et al. [[204](#bib.bib204)] | 2D U-Net | Private (CT) | 22411(2D) | 5
    | 0.910 | 0.630 | - | 0.960 | 0.960 | - | 0.710 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lambert et al. [[55](#bib.bib55)] &#124;'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gali et al. [[205](#bib.bib205)] &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2D U-Net &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2D U-Net &#124;'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SegTHOR (CT) [[33](#bib.bib33)] &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SegTHOR (CT) [[33](#bib.bib33)] &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4 &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4 &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.930 &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.860 &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.820 &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.469 &#124;'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.850 &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.643 &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.910 &#124;'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.854 &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; - &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '| Vesal et al. [[135](#bib.bib135)] | 2D U-Net | SegTHOR (CT) [[33](#bib.bib33)]
    | 60 | 4 | 0.941 | 0.858 | 0.926 | - | - | 0.938 | - |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
- en: '| Shi et al. [[120](#bib.bib120)] | 2.5D U-Net | StructSeg (CT) [[1](#bib.bib1)]
    | 50 | 5 | 0.941 | 0.821 | 0.882 | 0.968 | 0.971 | - | 0.902 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
- en: '| Mahmood et al. [[206](#bib.bib206)] | 2D U-Net | AAPM (CT) [[31](#bib.bib31)]
    | 60 | 5 | 0.880 | 0.660 | - | 0.970 | 0.970 | - | 0.800 |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[207](#bib.bib207)] | 2D FCN | Private (CT) | 36 | 6 | 0.860
    | 0.670 | 0.910 | 0.950 | 0.960 | - | 0.890 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Head and Neck'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Organ type | Brainstem | Mandible
    | Parotid gland | Submandibular gland | Optic Nerve | Chiasm |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
- en: '| Left | Right | Left | Right | Left | Right |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '| Ren et al. [[110](#bib.bib110)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 3 | - | - | - | - | - | - | 0.720 | 0.700 | 0.580 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '| Tappeiner et al. [[90](#bib.bib90)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 40 | 7 | 0.820 | 0.910 | 0.800 | 0.810 | - | - | 0.640 | 0.630 | 0.420 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '| Pu et al. [[91](#bib.bib91)] | 2.5D U-Net | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.880 | 0.940 | 0.860 | 0.865 | 0.788 | 0.802 | 0.743 | 0.768 | 0.612
    |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '| Ma et al. [[92](#bib.bib92)] | 3D U-Net |  | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.879 | 0.945 | 0.892 | 0.884 | 0.829 | 0.815 | 0.753 | 0.747 | 0.659
    |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. [[70](#bib.bib70)] | 2D FCN | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 32 | 9 | 0.849 | 0.924 | 0.842 | 0.849 | 0.734 | 0.782 | 0.676 | 0.684 | 0.547
    |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 56 | 14 | 0.863 | 0.905 | 0.582 | 0.687 | 0.668 | 0.575 |  |  |  |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Abdomen'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Category | Liver | Spleen | Kidney
    | Pancreas | Gallbladder | Stomach |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. [[93](#bib.bib93)] | 3D FCN | Refinement Model | Private (CT) |
    140 | 4 | 0.960 | 0.942 | 0.954 |  | - | - | - |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '| Roth et al. [[114](#bib.bib114)] | 3D FCN | 3D FCN | Private (CT) | 331 |
    3 | 0.932 | 0.906 | - | - | 0.631 | 0.706 | 0.843 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[123](#bib.bib123)] | 2.5D FCN | 2.5D FCN | Private (CT) | 236
    | 13 | 0.980 | 0.971 | 0.968 | 0.984 | 0.878 | 0.905 | 0.952 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[94](#bib.bib94)] | 3D V-Net | 3D V-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | 0.945 | 0.915 | 0.909 | 0.919 | 0.694 | 0.682 | 0.784 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[95](#bib.bib95)] | 2.5D FCN | 2.5D FCN | Private (CT) | 200
    | 16 | 0.969 | 0.968 | 0.962 | 0.960 | 0.877 | 0.894 | 0.951 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[150](#bib.bib150)] | 3D U-Net | 3D U-Net |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '&#124; FLARE &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2021 (CT) [[208](#bib.bib208)] &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '| 511 | 4 | 0.954 | 0.942 | 0.936 | 0.753 | - | - |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. [[96](#bib.bib96)] | 3D U-Net | 3D U-Net | Private (CT) | 100
    | 13 | 0.960 | 0.965 | 0.945 | 0.920 | 0.766 | 0.793 | 0.833 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| Kakeya et al. [[209](#bib.bib209)] | 3D U-Net | 3D U-Net | Private (CT) |
    47 | 8 | 0.971 | 0.969 | 0.984 | 0.975 | 0.861 | 0.918 | - |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Thorax'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Category | Heart | Esophagus
    | Trachea | Lung | Aorta | Spinal cord |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| Trullo et al. [[210](#bib.bib210)] | 2D FCN | 2D FCN | Private (CT) | 30
    | 4 | 0.900 | 0.690 | 0.870 | - | - | 0.89 | - |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. [[211](#bib.bib211)] | 2D U-Net | 2D U-Net | SegTHOR (CT) [[33](#bib.bib33)]
    | 50 | 6 | 0.945 | 0.850 | 0.807 | 0.97 | 0.966 | - | 0.91 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[94](#bib.bib94)] | 3D V-Net | 3D V-Net | SegTHOR (CT) [[33](#bib.bib33)]
    | 50 | 4 | 0.930 | 0.785 | 0.890 | - | - | 0.916 | - |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: DSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION
    AND SEGMENTATION METHODS FOR THE HEAD AND NECK'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Datasets | Quantity | Category | Brainstem
    | Mandible | Parotid gland | Submandibular gland | Optic nerve | Chiasm |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| Left | Right | Left | Right | Left | Right |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[101](#bib.bib101)] | 3D U-Net | 3D U-Net | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.875 | 0.930 | 0.864 | 0.848 | 0.758 | 0.733 | 0.737 | 0.736 | 0.451
    |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[102](#bib.bib102)] | 3D U-Net | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 100 | 7 | 0.900 | 0.920 | 0.860 | 0.860 | - | - | - | -
    | - |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[105](#bib.bib105)] | 3D U-Net | 3D U-Net | Private (CT) | 215
    | 28 | 0.863 | 0.931 | 0.849 | 0.849 | 0.807 | 0.825 | 0.757 | 0.761 | 0.642 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[105](#bib.bib105)] | 3D U-Net | 3D U-Net | PDDCA (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.875 | 0.95 | 0.887 | 0.875 | 0.823 | 0.815 | 0.748 | 0.723 | 0.615
    |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[212](#bib.bib212)] | 3D CNN | 2D U-Net | Private (CT) | 88
    | 17 | 0.831 | 0.875 | 0.807 | 0.811 | - | - | 0.638 | 0.675 | - |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[112](#bib.bib112)] | 2D CNN | 2D CNN | Private (CT) | 185
    | 18 | 0.896 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
- en: '&#124; left: 0.914; &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; right: 0.912 &#124;'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.852 | 0.85 | - | - | 0.661 | 0.717 | - |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[107](#bib.bib107)] | 3D CNN | 3D CNN | Private (CT) | 50 | 18
    | 0.858 | - | 0.772 | 0.800 | - | - | 0.639 | 0.617 | 0.638 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[107](#bib.bib107)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.875 | 0.935 | 0.863 | 0.879 | 0.798 | 0.801 | 0.735 | 0.744 | 0.596
    |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[121](#bib.bib121)] | 2.5D CNN | 2.5D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.923 | 0.941 | 0.876 | 0.808 | 0.736 | 0.713 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[121](#bib.bib121)] | 2.5D CNN | 2.5D CNN | Private (CT) |
    96 | 11 | - |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
- en: '&#124; Left: 0.911; &#124;'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; right: 0.914 &#124;'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.883 | 0.868 | - | - | 0.871 | 0.874 | - |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[103](#bib.bib103)] | 3D CNN | 3D U-Net | Private (CT) | 15 |
    8 | - | 0.850 | 0.820 | 0.810 | - | - | - | - | - |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[111](#bib.bib111)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.879 | 0.916 | 0.884 | 0.878 | 0.801 | 0.776 | 0.677 | 0.706 | 0.643
    |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[111](#bib.bib111)] | 3D CNN | 3D CNN | StructSeg (CT) [[1](#bib.bib1)]
    | 15 | 7 | 0.769 | 0.807 | 0.802 | 0.802 | - | - | 0.499 | 0.534 | 0.211 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[111](#bib.bib111)] | 3D CNN | 3D CNN | Private (CT) | 15 |
    9 | 0.957 | 0.848 | 0.962 | 0.946 | 0.846 | 0.808 | 0.824 | 0.843 | 0.434 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: '| Korte et al. [[106](#bib.bib106)] | 3D U-Net | 3D U-Net |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
- en: '&#124; Public RT-MAC dataset &#124;'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (MRI) [[213](#bib.bib213)] &#124;'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: '| 31 | 8 | - | - | 0.860 | 0.857 | 0.830 | 0.785 | - | - | - |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| Korte et al. [[106](#bib.bib106)] | 3D U-Net | 3D U-Net | Private (MRI) |
    10 | 8 | - | - | 0.730 | 0.775 | 0.537 | 0.435 | - | - | - |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[69](#bib.bib69)] | 3D CNN | 3D CNN | Private (CT) | 1164 | 22
    | 0.891 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '&#124; Left: 0.924; &#124;'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; right: 0.925 &#124;'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.846 | 0.87 | - | - | 0.713 | 0.753 | 0.612 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[69](#bib.bib69)] | 3D CNN | 3D CNN | HNC (CT) [[30](#bib.bib30)]
    | 48 | 9 | 0.882 | 0.947 | 0.898 | 0.881 | 0.840 | 0.838 | 0.790 | 0.817 | 0.713
    |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: DSC-Based Summary of the Literature on Multi-Organ Localization and
    Segmentation Methods for the Abdomen'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Liver
    | Spleen | Kidney | Pancreas | Gallbladder | Stomach |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| Larsson et al. [[108](#bib.bib108)] | Multi-Atlas | 3D FCN | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | 0.949 | 0.936 | 0.911 | 0.897 | 0.646 | 0.613 | 0.764 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[109](#bib.bib109)] | Registration | 2D U-Net |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '&#124; VISCERAL challenge dataset Nonenhanced &#124;'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTwb) [[214](#bib.bib214)] &#124;'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | - | - | - | - | 0.583 | 0.473 | - |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[109](#bib.bib109)] | Registration | 2D U-Net |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: '&#124; VISCERAL challenge dataset enhanced &#124;'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTce) [[214](#bib.bib214)] &#124;'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | - | - | - | - | 0.588 | 0.624 | - |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: DSC-Based Summary of the Literature on Multi-Organ Localization and
    Segmentation Methods for the Thorax'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Heart
    | Esophagus | Trachea | Lung | Aorta | Spinal cord |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: '| Left | Right |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| Francis et al. [[211](#bib.bib211)] | 3D U-Net | Two 3D U-Net | AAPM (CT)
    [[31](#bib.bib31)] | 60 | 5 | 0.941 | 0.738 | - | 0.979 | 0.973 | - | 0.899 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| Feng et al. [[104](#bib.bib104)] | 3D U-Net | 3D U-Net | AAPM (CT) [[31](#bib.bib31)]
    | 60 | 5 | 0.925 | 0.726 | - | 0.979 | 0.972 | - | 0.893 |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: '| Feng et al. [[104](#bib.bib104)] | 3D U-Net | 3D U-Net | Private (CT) | 30
    | 5 | 0.86 | 0.685 | - | 0.976 | 0.977 | - | 0.852 |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: DSC-Based Summary of the Literature on Multi-Organ Single-State Segmentation
    Methods for the Head and Neck-Supplementary Material'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Backbone | Datasets | Quantity | Organ type | Other organs |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
- en: '| Ibragimov and Xing [[41](#bib.bib41)] | 2.5D CNN | Private (CT) | 50 | 13
    | Pharynx: 0.856; Left eyeball: 0.884; Right eyeball: 0.877; Spinal cord: 0.870;
    Larynx: 0.693 |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
- en: '| Van Rooij et al. [[49](#bib.bib49)] | 3D U-Net | Private (CT) | 157 | 11
    | Larynx: 0.780; Pharyngeal Constrictor: 0.680; Cricopharynx: 0.730; Upper esophageal
    sphincter: 0.810; esophagus: 0.600; Oral Cavity: 0.780 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
- en: '| Tong et al. [[64](#bib.bib64)] | 3D GAN | Private (MRI) | 25 | 9 | Pharynx:
    0.706; Larynx: 0.799 |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[196](#bib.bib196)] | 3D U-Net | Private (MRI & CT) | 45 | 19
    | Pharynx: 0.740; spinal cord: 0.840; left cochlea: 0.760; right cochlea: 0.750;
    esophagus: 0.850; oral cavity: 0.900; left eye: 0.890; right eye: 0.870; left
    lens: 0.730; right lens: 0.730; larynx: 0.900; brain: 0.950 |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[116](#bib.bib116)] | 2.5D U-Net | Private (CT) | 307 | 24 |
    Pituitary: 0.756; left middle ear: 0.869; right middle ear: 0.859; left lens:
    0.844; right lens: 0.839; left temporomandibular joint: 0.838; right temporomandibular
    joint: 0.829 |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[141](#bib.bib141)] | 2D U-Net | StructSeg (CT) | 50 | 22 | Left
    eye: 0.858; right eye: 0.882; spinal cord: 0.804; pituitary: 0.503; left middle
    ear: 0.825; right middle ear: 0.717; left lens: 0.898; right lens: 0.786; left
    temporomandibular joint: 0.723; right temporomandibular joint: 0.824 |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
- en: '| Cros et al. [[197](#bib.bib197)] | 3D U-Net | Private (CT) | 200 | 12 | Medullary
    canal: 0.870; Outer medullary canal: 0.860; oral cavity: 0.660; esophagus: 0.600;
    trachea: 0.670; trunk: 0.670; outer trunk: 0.700; inner ears: 0.710; eyes: 0.770;
    sub-maxillary glands: 0.740 |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[162](#bib.bib162)] | 3D U-Net | StructSeg (CT) | 50 | 22 | Left
    eye: 0.886; right eye: 0.873; spinal cord: 0.830; pituitary: 0.661; left middle
    ear: 0.826; right middle ear: 0.783; left lens: 0.815; right lens: 0.754; left
    temporomandibular joint: 0.757; right temporomandibular joint: 0.772 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | OpenKBP (CT) [[36](#bib.bib36)]
    | 188 | 5 | Spinal cord: 0.740 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
- en: '| Podobnik et al. [[59](#bib.bib59)] | 2D nnU-net | Private (CT & MRI) | 56
    | 31 | Spinal cord: 0.812; Pharyngeal constrictor muscles: 0.617; oral cavity:
    0.845; Larynx-supraglottis: 0.728; Larynx-glottis:0.615; Lips:0.728; Thyroid:
    0.721; pituitary gland: 0.658; Lacrimal glands (left): 0.621; Lacrimal glands
    (right): 0.636; left eye: 0.887; right eye: 0.884; left lens: 0.723; right lens:
    0.763; Cervical esophagus: 0.559; Cricopharyngeal inlet: 0.517; Cochleae (left):0.558;
    Cochleae (right):0.514; Carotid arteries (left):0.624; Carotid arteries (right):0.618;
    Buccal mucosa: 0.661; Arytenoids:0.474 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
- en: '| Kan et al. [[85](#bib.bib85)] | 3D Transformer and U-Net | Private (CT) |
    94 | 18 | Spinal cord: 0.897; pituitary gland: 0.608; oral cavity: 0.908; left
    eye: 0.907; right eye: 0.902; left lens: 0.724; right lens: 0.689; left TMJ: 0.789;
    right TMJ: 0.778; left temporal lobe: 0.803; right temporal lobe: 0.802 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '| Isler et al. [[134](#bib.bib134)] | 2D U-Net | OpenKBP (CT) [[36](#bib.bib36)]
    | 188 | 5 | Spinal cord: 0.750 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: 'TABLE XII: DSC-Based Summary of the Literature on Multi-Organ Single-State
    Segmentation Methods for the Abdomen-Supplementary Material'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Network | Datasets | Quantity | Organ type | Other organs |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[40](#bib.bib40)] | 3D CNN | TCIA [[200](#bib.bib200), [201](#bib.bib201)]
    &BTCV [[29](#bib.bib29)] (CT) | 72 | 4 | Esophagus: 0.730 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[115](#bib.bib115)] | 2D CNN | Private (CT) | 278 | 5 | Bladder:
    0.934; Intestine: 0.653; Left femoral head: 0.921; Right femoral head: 0.923;
    Colon: 0.618 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[157](#bib.bib157)] | 3D U-Net | Private (CT) | 377 | 7 | Artery:
    0.892; Vein: 0.793 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
- en: '| Gibson et al. [[57](#bib.bib57)] | 3D V-Net | TCIA [[200](#bib.bib200), [201](#bib.bib201)]
    &BTCV [[29](#bib.bib29)] (CT) | 90 | 8 | Duodenum 0.630; Esophagus: 0.760 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| Roth et al. [[52](#bib.bib52)] | 3D U-Net | Private (CT) | 377 | 7 | Artery:
    0.835; Vein 0.805 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
- en: '| Cai et al. [[65](#bib.bib65)] | 2D FCN | Private (CT) | 120 | 16 | Aorta:
    0.810; Adrenal gland: 0.368; Celiac AA: 0.385; Duodenum:0.649; Colon: 0.776; Inferior
    vena cava: 0.786; Superior mesenteric artery: 0.496; Small bowel: 0.729; Veins:
    0.651 |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
- en: '| Heinrich et al. [[147](#bib.bib147)] | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 43 | 8 | Left adrenal gland: 0.942; duodenum: 0.538; Esophagus:
    0.633 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
- en: '|  |  | Private (CT) | 10 | 7 | Spleen, pancreas, kidney, gallbladder, esophagus,
    liver, stomach and duodenum average DSC: 0.823 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
- en: '| Hatamizadeh et al. [[87](#bib.bib87)] | 3D Transformer And U-Net | BTCV (CT)
    [[29](#bib.bib29)] | 30 | 13 | Esophagus: 0.864; aorta: 0.948; Inferior vena cava:
    0.890; vein: 0.858 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[117](#bib.bib117)] | 2.5D U-Net | Private (MR) | 102 | 10 |
    Duodenum: 0.801; Small Intestine: 0.870; Spinal Cord: 0.904; Vertebral Body: 0.900
    |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[124](#bib.bib124)] | 2.5D U-Net | ABD-110 (CT) [[124](#bib.bib124)]
    | 110 | 11 | Large intestine: 0.825; small intestine 0.765; duodenum 0.707; spinal
    cord 0.908 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
- en: '| Jia and Wei [[53](#bib.bib53)] | 3D U-Net | CHAOS (CT) [[32](#bib.bib32)]
    | 20 | 4 | - |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[142](#bib.bib142)] | 3D U-Net | TCIA [[200](#bib.bib200), [201](#bib.bib201)]
    & BTCV(CT) [[29](#bib.bib29)] | 90 | 8 | Duodenum 0.637; esophagus 0.733 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. [[77](#bib.bib77)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta 0.855 |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[86](#bib.bib86)] | 2D Transformer And U-Net | Synapse (CT)
    [[2](#bib.bib2)] | 30 | 8 | Aorta 0.872 |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[161](#bib.bib161)] | 2D CNN | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta 0.903 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[78](#bib.bib78)] | 2D Transformer | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta: 0.870 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| Suo et al. [[84](#bib.bib84)] | 2D Transformer And U-Net | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta: 0.881 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| Berzoini et al. [[54](#bib.bib54)] | 2D U-Net | Open-source CT-org dataset
    [[203](#bib.bib203)] | 140 | 6 | Lung: 0.967; bladder: 0.836; bone: 0.944 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[148](#bib.bib148)] | 2D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 42 | 5 | Duodenum: 0.615 |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. [[79](#bib.bib79)] | 3D CNN And Transformer | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 11 | Esophagus: 0.780; aorta: 0.912; Inferior vena cava: 0.880; Portal
    vein and splenic vein: 0.781; |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| Srivastava et al. [[138](#bib.bib138)] | 3D U-Net | Synapse (CT) [[2](#bib.bib2)]
    | 30 | 8 | Aorta: 0.909 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[198](#bib.bib198)] | 2D U-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 12 | Esophagus: 0.807; aorta: 0.913; Inferior vena cava: 0.850; Portal
    vein and splenic vein: 0.809; adrenal gland: 0.691 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIII: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Head and Neck-Supplementary Material'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse | Fine | Datasets | Quantity | Organ type | Other organs |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. [[70](#bib.bib70)] | 2D FCN | 3D U-Net | Private (CT) | 56 |
    14 | Right eyeball: 0.634; Left eyeball: 0.636; Lips: 0.676; Oral Cavity: 0.829;
    throat: 0.389; Esophagus: 0.735; Thyroid gland: 0.642; spinal cord: 0.782 |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIV: DSC-Based Summary of the Literature on Multi-Organ Coarse-to-Fine
    Segmentation Methods for the Abdomen'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Coarse network | Fine network | Dataset | Quantity | Category | Other
    organs |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| Roth [[103](#bib.bib103)] | 3D FCN | 3D FCN | Private (CT) | 331 | 3 | Artery:
    0.796; vein: 0.731 |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: '| Wang [[111](#bib.bib111)] | 2.5D FCN | 2.5D FCN | Private (CT) | 236 | 13
    | Aorta: 0.918; colon: 0.830; duodenum: 0.754; Inferior vena cava: 0.870; small
    intestine: 0.801; vein: 0.807 |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
- en: '| Zhang [[123](#bib.bib123)] | 3D V-Net | 3D V-Net | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | Esophagus: 0.691; aorta: 0.877; Inferior vena cava: 0.865; Portal
    vein and splenic vein: 0.688; right adrenal gland: 0.651; left adrenal gland:
    0.619 |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
- en: '| Xie [[94](#bib.bib94)] | 2.5D FCN | 2.5D FCN | Private (CT) | 200 | 16 |
    Aorta: 0.937; adrenal gland: 0.630; abdominal cavity: 0.620; duodenum: 0.735;
    Inferior vena cava: 0.837; Vascular: 0.742; small intestine: 0.751; vein: 0.748;
    Colon: 0.800 |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| Lee [[114](#bib.bib114)] | 3D U-Net | 3D U-Net | BTCV (CT) [[29](#bib.bib29)]
    | 47 | 8 | Esophagus: 0.783; aorta: 0.916; Inferior vena cava: 0.856; Portal vein
    and splenic vein: 0.762; RAD: 0.741; LAD: 0.746 |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| Kakeya [[108](#bib.bib108)] | 3D U-Net | 3D U-Net | Private (CT) | 47 | 8
    | Inferior vena cava: 0.908; Aorta: 0.969 |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: 'TABLE XV: DSC-Based Summary of the Literature on Multi-Organ Localization and
    Segmentation Methods for the Head and Neck-Supplementary Material'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Other
    organs |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Men et al. [[102](#bib.bib102)] | 3D U-Net | 3D U-Net | TCIA (CT) [[200](#bib.bib200),
    [201](#bib.bib201)] | 100 | 7 | Spinal cord: 0.910; Left eye: 0.930; Right eye:
    0.920 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. [[105](#bib.bib105)] | 3D U-Net | 3D U-Net | Private (CT) | 215
    | 28 | Brachial plexus: 0.562; pharyngeal constrictor: 0.755; left ear: 0.773;
    right ear: 0.786; left eye: 0.925; right eye: 0.925; pituitary gland: 0.639; larynx:
    0.893; left lens: 0.819; right lens: 0.830; oral cavity: 0.908; spinal cord: 0.856;
    sublingual gland: 0.460; left temporal lobe: 0.848; right temporal lobe: 0.841;
    thyroid: 0.856; left temporomandibular joint: 0.880; right temporomandibular joint:
    0.869; trachea: 0.813 |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[212](#bib.bib212)] | 3D CNN | 2D U-Net | Private (CT) | 88
    | 17 | Left eye: 0.875; right eye: 0.889; left lens: 0.747; right lens: 0.698;
    cerebellum: 0.936; pituitary: 0.672; thyroid: 0.844; Temporal lobe left: 0.762;
    Temporal lobe right: 0.784; brain: 0.976; head: 0.943 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[112](#bib.bib112)] | 2D CNN | 2D CNN | Private (CT) | 185
    | 18 | Left eye: 0.932; right eye: 0.936; left lens: 0.930; right lens: 0.842;
    larynx: 0.870; oral cavity: 0.928; left mastoid: 0.821; right mastoid: 0.824;
    spinal cord: 0.884; left TMJ: 0.846; right TMJ: 0.844; |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[107](#bib.bib107)] | 3D CNN | 3D CNN | Private (CT) | 50 | 18
    | Left eye: 0.876; right eye: 0.912; oral cavity: 0.792; larynx: 0.658; spinal
    cord: 0.874; left lens: 0.808; right lens: 0.790; pituitary gland: 0.769; left
    middle ear: 0.567; right middle ear: 0.522; left TMJ: 0.584; right TMJ: 0.572
    |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| Private (CT) | 96 | 11 | Left eye: 0.930; right eye: 0.930; spinal cord:
    0.900; left lens: 0.872; right lens: 0.883; |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| Lei et al. [[103](#bib.bib103)] | 3D CNN | 3D U-Net | Private (CT) | 15 |
    8 | Esophagus: 0.840; Throat: 0.790; Oral: 0.890; Pharynx: 0.850; spinal cord:
    0.890 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| Korte et al. [[106](#bib.bib106)] | 3D U-Net | 3D U-Net |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '&#124; Public RT-MAC dataset &#124;'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (MRI) [[213](#bib.bib213)] &#124;'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: '| 43 | 8 | Secondary lymph nodes (left): 0.708; secondary lymph nodes (right):
    0.715; tertiary lymph nodes (left): 0.561; tertiary lymph nodes (right): 0.573;
    |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
- en: '| Private (MRI) | 10 | 8 | Secondary lymph nodes (left): 0.553; Secondary lymph
    nodes (right): 0.525; Tertiary lymph nodes (left): 0.304; Tertiary lymph nodes
    (right): 0.189; |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[69](#bib.bib69)] | 3D CNN | 3D CNN | Private (CT) | 1164 | 22
    | Left eye: 0.897; right eye: 0.895; left lens: 0.819; right lens: 0.825; pituitary
    gland: 0.722; left temporal lobe: 0.877; right temporal lobe: 0.883; spinal cord:
    0.831; left inner ear: 0.864; right inner ear: 0.855; left middle ear: 0.857;
    right middle ear: 0.843; left temporomandibular joint: 0.764; right temporomandibular
    joint: 0.789; |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
- en: 'TABLE XVI: DSC-Based Summary of the Literature on Multi-Organ Localization
    and Segmentation Methods for the Abdomen-Supplementary Material'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref | Localization | Segmentation | Dataset | Quantity | Category | Other
    organs |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
- en: '| Larsson et al. [[108](#bib.bib108)] | Multi-Atlas | 3D FCN | BTCV (CT) [[29](#bib.bib29)]
    | 30 | 13 | Esophagus: 0.588; aorta: 0.870; Inferior vena cava: 0.758; Portal
    vein and splenic vein: 0.715; Right adrenal gland: 0.630; Left adrenal gland:
    0.631 |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[109](#bib.bib109)] | Registration | 2D U-Net |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
- en: '&#124; VISCERAL challenge dataset Nonenhanced &#124;'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTwb) [[214](#bib.bib214)] &#124;'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | Left adrenal gland: 0.472; Right adrenal gland: 0.390 |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VISCERAL challenge dataset enhanced &#124;'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CT (CTce) [[214](#bib.bib214)] &#124;'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: '| 20 | 4 | Left adrenal gland: 0.403; Right adrenal gland: 0.434 |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
